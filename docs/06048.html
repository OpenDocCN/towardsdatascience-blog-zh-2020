<html>
<head>
<title>A logistic regression from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-logistic-regression-from-scratch-3824468b1f88?source=collection_archive---------18-----------------------#2020-05-17">https://towardsdatascience.com/a-logistic-regression-from-scratch-3824468b1f88?source=collection_archive---------18-----------------------#2020-05-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f17e2d4a1579d8d500370b94c8cc6d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-UWDCFfm9KQ_Xu9OoQadw.png"/></div></div></figure><div class=""/><div class=""><h2 id="28fd" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">将张量流转换为普通数字</h2></div><p id="7982" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我之前的博文中，我们研究了人工神经网络，开发了一个类来构建任意层数和神经元数的网络。虽然博客引用了前面的笔记本来解释先决条件，但是还没有附带的文章，也就是这篇博客文章。</p><p id="ecbc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这篇文章也可以在我的 Github 上作为<a class="ae lp" href="https://github.com/dennisbakhuis/Tutorials/tree/master/Logistic_Regression" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>获得，所以你可以边阅读边编码。</p><p id="4cfe" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果你是 Python 和 Jupyter 的新手，<a class="ae lp" href="https://github.com/dennisbakhuis/Tutorials/tree/master/Python_Conda_Pip_Environments" rel="noopener ugc nofollow" target="_blank">这里有一个关于我如何管理我的 Python 环境和包的简短解释</a>。</p><p id="1076" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">我们将讨论的主题的简短概述:</strong></p><ol class=""><li id="1514" class="lq lr je kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">神经网络和逻辑回归之间的联系</li><li id="85f0" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">退一步:线性回归</li><li id="b656" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">从线性回归到(二元)逻辑回归</li><li id="7de4" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">围捕</li></ol><h1 id="9575" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">神经网络与逻辑回归之间的联系</h1><p id="a0c4" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">当我们听到或读到<em class="nb">深度学习</em>时，我们通常指的是使用<em class="nb">人工神经网络</em> (ANN)的机器学习子领域。这些计算系统在解决各种领域的复杂问题方面相当成功，例如图像识别、语言建模和语音识别。虽然 ANN 这个名字意味着它们与我们大脑的内部工作有关，但事实是它们主要共享一些术语。人工神经网络通常由多个相互连接的层组成，这些层本身是使用神经元(也称为节点)构建的。图 1 显示了一个例子。</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/d4503cffa3c8b834533c72b19b5c6944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SalbZTS_nqs7dnQw9gIHfw.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">图 1:一个典型的神经网络的例子，借用于我关于神经网络的后续文章。</p></figure><p id="562e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这个例子中，我们有一个输入层，由四个独立的输入节点组成。该输入层“完全连接”到第一个隐藏层，即完全连接意味着每个输入连接到每个节点。第一个隐藏层再次完全连接到另一个“隐藏”层。术语“隐藏”表示我们没有直接与这些层交互，这些层对用户来说有点模糊。第二个隐藏层依次完全连接两个最终输出层，它由两个节点组成。因此，在本例中，我们向模型提供四个输入，我们将接收两个输出。</p><p id="52cb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在让我们把注意力集中在前面例子中的单个神经元上。这个神经元仍然连接到所有的输入，也称为特征。使用这些特征，神经元计算单个响应(或输出)。这种系统的示意图如图 2 所示。</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/908b88081ab8d61187d9cf56f90e4d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xuidFWX1Qfk5GWPqKkhZw.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">图 2:具有四个输入特征的单个神经元。神经元有两个操作:线性部分和激活函数。</p></figure><p id="f44f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">输入特征被命名为<em class="nb"> 𝑓 </em>、<em class="nb"> 𝑓 </em>、<em class="nb"> 𝑓 </em>和<em class="nb"> 𝑓⁴ </em>，并且都连接到单个神经元。这个神经元执行两个操作。第一个操作是将特征与权重<em class="nb">向量𝑊 </em>相乘，并添加偏置项<em class="nb"> 𝑏 </em>。第二个操作是所谓的<em class="nb">激活功能</em>，这里由<em class="nb"> 𝜎 </em>表示。神经元的输出是介于零和一之间的概率。单个神经元的行为就像一个小的逻辑回归模型，因此，人工神经网络可以被视为一堆相互连接的逻辑回归模型堆叠在一起。虽然这个想法很简洁，但潜在的事实却有点微妙。人工神经网络有许多不同的体系结构，它们可以使用各种不同的构建模块，其行为与本例中的完全不同。</p><p id="67ab" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们单个神经元中的线性运算只不过是线性回归。因此，要理解逻辑回归，第一步是了解线性回归是如何工作的。下一节将展示一个逐步的例子作为回顾。</p><h1 id="556e" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">退一步:线性回归</h1><h2 id="7d90" class="nm mf je bd mg nn no dn mk np nq dp mo lc nr ns mq lg nt nu ms lk nv nw mu nx bi translated">什么是线性回归来着？</h2><p id="141b" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">最简单形式的线性回归(也称为简单线性回归)，使用单个自变量<em class="nb"> 𝑥 </em>对单个因变量<em class="nb"> 𝑦 </em>建模。这听起来可能令人望而生畏，但这意味着我们要解下面的方程:</p><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="3b7f" class="nm mf je nz b gy od oe l of og"><em class="nb">𝑦 </em>= <em class="nb">a𝑥 </em>+ <em class="nb">𝑏</em></span></pre><p id="5c01" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在<em class="nb">机器学习</em>的背景下，<em class="nb"> 𝑥 </em>代表我们的输入数据，<em class="nb"> 𝑦 </em>代表我们的输出数据，通过求解我们的意思是找到最佳权重(或参数)，在线性回归方程中用<em class="nb"> 𝑤 </em>和<em class="nb"> 𝑏 </em>来表示。计算机可以帮助我们找到𝑤<em class="nb">和𝑏</em>和<em class="nb">的最佳值，使用输入变量<em class="nb"> 𝑥 </em>得到𝑦 </em>的最接近匹配<em class="nb">。</em></p><p id="fbbc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">对于接下来的例子，让我们为<em class="nb"> x </em>和<em class="nb"> y </em>定义以下值:</p><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="d1a4" class="nm mf je nz b gy od oe l of og"><em class="nb">𝑥 </em>= [−2,−1,0,1,2,3,4,5]<br/><em class="nb">𝑦 </em>= [−3,−1,1,3,5,7,9,11]</span></pre><p id="99c9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><em class="nb"> 𝑥 </em>和<em class="nb"> 𝑦 </em>的值具有线性关系，因此我们可以使用线性回归找到(或拟合)最佳权重并解决问题。也许，通过长时间盯着这些值，我们可以发现它们之间的关系，然而，使用计算机来找到答案要容易得多。</p><p id="095b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果你已经盯得够久了，或者只是想知道答案，那么<em class="nb"> 𝑥 </em>和<em class="nb"> 𝑦 </em>的关系如下:</p><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="82b1" class="nm mf je nz b gy od oe l of og"><em class="nb">𝑦 </em>= 2<em class="nb">𝑥 </em>+ 1</span></pre><p id="3aaa" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在下一节中，我们将使用 Tensorflow 来创建我们的单神经元模型，并尝试“求解”该方程。</p><h2 id="712c" class="nm mf je bd mg nn no dn mk np nq dp mo lc nr ns mq lg nt nu ms lk nv nw mu nx bi translated">Tensorflow 中的实现</h2><p id="01ac" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">在我们开始使用 Tensorflow 之前，我们应该首先组织我们的输入数据(<em class="nb"> 𝑥 </em>)和输出数据(<em class="nb"> 𝑦 </em>)。为此，我们将使用 Numpy:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="db3f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">下一步，我们将导入 Tensorflow。检查我们正在使用的版本始终是一个好的做法:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="4c71" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们可以使用 Keras 创建一个模型，它现在是 Tensorflow 的一部分。为了做到这一点，我们将使用 Sequential 类，它可以将不同的层一个接一个地“顺序”堆叠起来。我们使用 Keras 中的密集类来创建一个“完全连接”层，它由单个神经元(单元)组成。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="7c31" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Dense 函数用于创建许多完全连接的神经元(逻辑单元)的层。参数 units 用于设置神经元的数量。我们只使用一个单位，因此，我们将它设置为 1。由于这是我们的模型的第一个“层”，我们需要告诉 Tensorflow 它可以预期什么形状作为输入。这只对第一层是必要的。</p><p id="fe6e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">既然我们已经定义了模型，我们需要使用 Compile()方法来配置模型以进行训练。该方法需要至少两个参数，损失函数和优化器。损失函数是模型预测实际值的程度的度量。对于这个例子，我们将使用均方误差(𝑦 的预测值和实际值之间的平方差的平均值)。“<em class="nb">学习算法</em>将通过调整(优化)每一步的参数(权重和偏差)来尽量减少损失。优化器定义了一个方法来执行这个优化步骤，一个常用的方法是梯度下降，或者在我们的例子中是随机梯度下降(SGD)。在下一节中，我们将用简单的 Numpy 实现这个方法，这个方法将变得更加清晰。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="8883" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">接下来，我们使用 Fit()方法让算法学习最佳参数。虽然 Tensorflow 有许多聪明的方法来解决这个问题，但秘诀或多或少是这样的:</p><ol class=""><li id="6cbd" class="lq lr je kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">使用当前参数(<em class="nb"> 𝑊 </em>和<em class="nb"> 𝑏 </em>)计算预测<em class="nb"> ŷ </em></li><li id="a3cf" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">计算当前值的损失</li><li id="0464" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">计算损失函数相对于参数的梯度</li><li id="a2a3" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">使用渐变调整权重(优化)。</li><li id="4114" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">重复上述操作，以确定出现的次数，即浏览所提供的示例(数据集)的次数。</li></ol><p id="1fb5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果这不是完全清楚的，不要太担心。我们将用简单的 Numpy 编码每一步，并对下一节中使用的所有数学进行彻底的推导。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="fc27" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们已经使用我们的训练数据在 Tensorflow 中训练了该模型，并且我们准备尝试一下。现在，我们可以使用我们的模型来“预测”它从未见过的值。这有时也被称为推理。我们来试试 12 的值。我们知道应该是 25。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="6841" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为什么值不正好是 25？</p><p id="8bca" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">该模型计算实际值和预测值之间的差异，并慢慢向实际值靠拢。长时间运行 fit 方法会让你更接近 25。</p><p id="91ce" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这并不难，但可能感觉像某种黑暗绝地的力量。因此，在下一节中，我们将用普通 Python 实现这个算法(借助 Numpy)。</p><h2 id="4612" class="nm mf je bd mg nn no dn mk np nq dp mo lc nr ns mq lg nt nu ms lk nv nw mu nx bi translated">“引擎盖下”到底发生了什么</h2><p id="1a0f" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">在上一节中，我们大致介绍了 Tensorflow 正在做的事情:</p><ol class=""><li id="18ef" class="lq lr je kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">使用当前参数(<em class="nb"> 𝑊 </em>和<em class="nb"> 𝑏 </em>)计算预测<em class="nb"> ŷ </em></li><li id="d490" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">计算当前值的损失</li><li id="5cb0" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">计算损失函数相对于参数的梯度</li><li id="a5ba" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">使用渐变调整权重(优化)。</li><li id="9ff8" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">重复上述操作，以确定出现的次数，即浏览所提供的示例(数据集)的次数。</li></ol><p id="f5d0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们现在将在普通 Python 中实现这一点，并希望得到与 Tensorflow 类似的结果。</p><p id="2710" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">首先，我们定义模型参数。这些是权重<em class="nb"> 𝑊 </em>，这只是一个单一的值，因为我们只有一个单一的输入。我们还需要定义偏差项<em class="nb"> 𝑏 </em>。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="e457" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这是我们模型中所有可训练的参数，单个标量用于权重，单个标量用于偏差。</p><p id="d6aa" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">接下来，我们将定义一个使用当前模型参数进行预测的函数。在深度学习术语中，这被称为向前传递。预测值的变量一般命名为<em class="nb"> ŷ </em>(或者<em class="nb"> A </em>但你可以在我的下一篇文章中了解到这一点)。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="6d0e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">该函数名为 forward，使用输入向量<em class="nb"> 𝑋 </em>并将其与权重参数<em class="nb"> 𝑊 </em>相乘，并添加偏差项<em class="nb"> 𝑏 </em>。正如我们在前面提到的等式中描述的那样。</p><p id="7947" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们现在可以用当前参数测试函数。同样，我们将输入一个值 12.0，但是当然，它将返回乱码，因为权重是随机初始化的。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="b9e0" class="nm mf je nz b gy od oe l of og">-0.2122614846691141</span></pre><p id="07d3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">事实上，-0.212261…并不是我们真正想要的，但我们仍然需要训练我们的模型，然后它才能做出正确的预测。在此之前，我们需要计算当前的损失。作为损失，我们使用预测值<em class="nb"> ŷ </em>和实际值<em class="nb"> 𝑦 </em>的均方误差。</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ca"><img src="../Images/876f0bdc33665bdd79fc12d818dcb857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RIYAp_UBaesJko1Emx8qjg.png"/></div></div></figure><p id="3932" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">希望数学不会吓到你，但是如果你花时间，它并不难。变量<em class="nb"> 𝑚 </em>这里是例子的数量(数据集中的点)。我们的<em class="nb"> 𝑋 </em>有八个值，因此，<em class="nb"> 𝑚 </em> =8。当我们有一个 1/ <em class="nb"> 𝑚 </em>后跟 as sum(≘)超过所有<em class="nb"> 𝑚 </em>值时，它无非是这些值的平均值，即<em class="nb"> 𝑖𝑛𝑠𝑖𝑑𝑒 </em>的总和。这里，我们取所有(<em class="nb">𝑦</em>—<em class="nb">ŷ</em>)<em class="nb"/>的平均值，这是真实值<em class="nb"> 𝑦 </em>和预测值<em class="nb"> ŷ </em>的平方之差。平方很重要，因为如果我们不把差平方，负差和正差就会互相抵消。既然我们完全理解了<em class="nb">均方误差</em>，我们可以用代码实现它:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="66cd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">要查看我们之前计算的值之间的损失，我们可以这样做:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="729d" class="nm mf je nz b gy od oe l of og">149.13933056993267</span></pre><p id="31e1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">正如我们所看到的，损失相当大，我们的重量相当大。因此，我们需要更新我们的可训练参数，以做出更好的预测。为此，我们需要首先计算梯度<em class="nb"> 𝛿 </em>损失/ <em class="nb"> 𝛿𝑊 </em>和<em class="nb"> 𝛿 </em>损失/ <em class="nb"> 𝛿𝑏 </em>。也许你的鉴别能力有点生疏了。诀窍是应用链式法则。另一个好处是平均算子(和)是线性的，因此，我们可以在微分中忽略它，以后再放回去。</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/baa1d729f7404d9333fb4d4dd0f8b7cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4_W5gcAmFnhNCPKwsumFjA.png"/></div></div></figure><p id="5420" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了计算<em class="nb"> 𝛿 </em>损失/ <em class="nb"> 𝛿𝑏 </em>我们只需要重复最后一步，关于<em class="nb"> 𝑏 </em>:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ca"><img src="../Images/b80c75ca2f6befba824ebdc314570ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*srtCubhUNdBDb4t2CVueLw.png"/></div></div></figure><p id="83fb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们将在“反向传递”函数中实现这一点。由于变量名变得有点长，我们也有点懒，我们就称它们为<em class="nb"> 𝑑𝑊 </em>和<em class="nb"> 𝑑𝑏 </em>。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="dfce" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，我们可以得到之前测试示例的梯度:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="7017" class="nm mf je nz b gy od oe l of og">(-605.0942756320587, -50.42452296933823)</span></pre><p id="38ba" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在构建训练循环之前，我们需要的最后一个函数是更新函数。该功能将一步“优化”我们的权重。这是实际的梯度下降，其中我们从当前权重中减去(下降)梯度。梯度下降定义如下:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/8d698a939166ca7f5d42e4ae976b942a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HD3RP1XzQLD63J2rNzBdSg.png"/></div></div></figure><p id="c5f8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这里我们有一个新的参数<em class="nb"> 𝛼 </em>，它被称为学习率。这个值将设置我们试图收敛到最小损失的速度。对于这个例子，我们将它设置为 0.01。学习率没有黄金值，这个值需要针对每个问题进行微调。我们的更新函数代码如下:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="2775" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">要更新我们当前的模型参数，我们只需:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="3d31" class="nm mf je nz b gy od oe l of og">(array([6.0332543]), 0.5042452296933823)</span></pre><p id="ba41" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">好的，我们第一次更新了我们的权重。为了提高重量，我们必须重复这个过程很多次。为此，我们将编写一个循环:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="0c2a" class="nm mf je nz b gy od oe l of og">129.7037237445872<br/>0.06819476739792682<br/>0.00458155318441586<br/>0.0003078050915820409<br/>2.067944441333496e-05</span></pre><p id="37f0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们很幸运，损失，即我们的模型预测和实际值之间的差异，正在减少。当我们输入值 12.0 时，我们现在如何预测？</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="5e92" class="nm mf je nz b gy od oe l of og">array([25.00101652])</span></pre><p id="9fcd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我希望 Tensorflow 中的这种黑魔法现在更加清晰了。在下一节中，我们将使用我们新学到的知识来解决一个二元逻辑回归问题。</p><h1 id="e89d" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">从线性回归到(二元)逻辑回归</h1><h2 id="5229" class="nm mf je bd mg nn no dn mk np nq dp mo lc nr ns mq lg nt nu ms lk nv nw mu nx bi translated">到底有什么区别？</h2><p id="90bd" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">线性回归和逻辑回归之间的差别不是很大。与我们之前创建的代码有两个不同之处。首先，我们的线性回归模型只有一个单一的特征，我们用<em class="nb"> 𝑥 </em>输入，这意味着我们只有一个单一的权重。在逻辑回归中，通常输入多个特征，每个特征都有自己的权重。从技术上讲，你可以有一个单一的特性，但是它只不过是一个 if 语句(想想看)。增加功能的数量会将之前的<em class="nb">简单乘法</em>变为<em class="nb">矩阵乘法</em>(点积)。其次，我们将添加一个所谓的<em class="nb">激活函数</em>来映射 0 或 1 之间的值。让我们再次提醒自己我们的简单模型:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/908b88081ab8d61187d9cf56f90e4d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xuidFWX1Qfk5GWPqKkhZw.png"/></div></div></figure><p id="4826" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">按照 Tensorflow 中的惯例(根据我的理解)，输入向量有用于特征的列和用于示例的行。如果我们有 2 个数据点，输入矩阵将如下所示:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ca"><img src="../Images/40f4e5d213554da3229242fe610b3671.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*27iy3ZQME8pcVxDNgTJulw.png"/></div></div></figure><p id="400f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">上标表示特征号，下标表示例子。</p><p id="8dd6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这些输入中的每一个都与它们自己的权重相关联。节点本身有两个不同的操作。第一个是权重向量和输入向量之间的点积。第二个是 Sigmoid 函数。这个例子中的权重向量<em class="nb"> 𝑊 </em>具有四个权重:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ca"><img src="../Images/1f0f5013997b3b4792f5f1e56a9e8588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KJWXbJgxCPl4_S0uLD__cw.png"/></div></div></figure><p id="044f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在节点中，我们首先计算线性部分:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/35286a8b51d905dd0e7aad32686a6723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*exV1B0VD-3NhqDPU_5qLgw.png"/></div></div></figure><p id="fc1e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我们的例子中，它看起来像这样:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/4c9cbb92b604b81232d1a38068d862a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fbKWmGdEXNaTyJS0rmIXyg.png"/></div></div></figure><p id="70e0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">请注意，每个示例的结果都只有一个值。</p><p id="fcc7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">对于<em class="nb"> 𝑥 </em>的所有值，可以从∞到+∞(全部为实数)，Sigmoid 函数将<em class="nb"> 𝑥 </em>映射到 0 和 1 之间。接近零的<em class="nb"> 𝑥 </em>的值具有最大的影响，因为它们处于“线性状态”。非常大或非常小仅分别被限制为 1 和 0。Sigmoid 函数的数学定义是:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/d42f6cbee052cd091eb0e7dbb072c590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dy4nEvZA8hl0CfRq0xZZLg.png"/></div></div></figure><p id="e181" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这就是后勤单位的全部情况。Sigmoid 函数赋予节点非线性特征。许多这样的单位一起可以做几乎神奇的事情。在下一节中，我们将首先在 Tensorflow 中制作一个逻辑回归模型。</p><h2 id="07a1" class="nm mf je bd mg nn no dn mk np nq dp mo lc nr ns mq lg nt nu ms lk nv nw mu nx bi translated">Tensorflow 中的实现</h2><p id="4798" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">在我们开始之前，我们首先需要一些数据来做逻辑回归。我从 Kaggle 的 Azeem Bootwala 下载了 titantic 数据集，以演示这个例子。可以从这里下载:<br/><a class="ae lp" href="https://www.kaggle.com/azeembootwala/titanic" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/azeembootwala/titanic</a></p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="f7b4" class="nm mf je nz b gy od oe l of og">(792, 17)</span></pre><p id="68de" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">首先，始终检查列和数据类型:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="f2b3" class="nm mf je nz b gy od oe l of og">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 792 entries, 0 to 791<br/>Data columns (total 17 columns):<br/> #   Column       Non-Null Count  Dtype  <br/>---  ------       --------------  -----  <br/> 0   Unnamed: 0   792 non-null    int64  <br/> 1   PassengerId  792 non-null    int64  <br/> 2   Survived     792 non-null    int64  <br/> 3   Sex          792 non-null    int64  <br/> 4   Age          792 non-null    float64<br/> 5   Fare         792 non-null    float64<br/> 6   Pclass_1     792 non-null    int64  <br/> 7   Pclass_2     792 non-null    int64  <br/> 8   Pclass_3     792 non-null    int64  <br/> 9   Family_size  792 non-null    float64<br/> 10  Title_1      792 non-null    int64  <br/> 11  Title_2      792 non-null    int64  <br/> 12  Title_3      792 non-null    int64  <br/> 13  Title_4      792 non-null    int64  <br/> 14  Emb_1        792 non-null    int64  <br/> 15  Emb_2        792 non-null    int64  <br/> 16  Emb_3        792 non-null    int64  <br/>dtypes: float64(3), int64(14)<br/>memory usage: 105.3 KB</span></pre><p id="3bf3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我认为 Azeem 没有使用 index=False 选项保存集合，因此我们有一个“未命名:0”列。这个对于我们当前的索引来说是多余的，所以我们可以删除它。此外，PassengerId 对于我们的模型来说不是很有用，让我们也删除该列。之后，让我们对数据集进行采样，以获得一个概念:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/05b4e17ac89f0a5cd25f866e260c0ed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xfQffkFzORf4XW186aWBQw.png"/></div></div></figure><p id="b032" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Azeem 已经做了一些预处理。目标变量<em class="nb"> 𝑌 </em>是‘幸存’，所有其他列都是特征。功能简介:<strong class="kv jf"> <br/> - </strong>性别:0 或 1 - &gt;男或女<br/> -年龄:值在 0 和 1 之间重新调整<br/> -票价:票价在 0 和 1 之间重新调整<br/> - Pclass_1..Pclass_3:一键编码乘客类别<br/> -家庭规模:家庭规模的 0 到 1 之间的重新调整值。<br/> -标题 _1..title_4:先生、夫人、主人、小姐 one-hot encoded <br/> - emb_1..emb_3:登船位置一-热编码。</p><p id="152e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">总共我们将有 14 个功能。</p><p id="57cb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">对于这个例子来说，数据就足够了，我不会详细讨论这些数据是如何变成现在这个样子的。老实说，我不知道我自己，刚刚从 Kaggle 下载；-).</p><p id="e6c4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们把这些变量放入我们之前定义的格式中(<em class="nb"> 𝑋 </em>和<em class="nb"> 𝑌 </em>)。这里<em class="nb"> 𝑌 </em>对应一个人是否幸存的标签。我们总共有 792 个例子。因此，<em class="nb"> 𝑌 </em>的形状为(<em class="nb"> 𝑚 </em>，1)其中<em class="nb"> 𝑚 </em> = 792。对于<em class="nb"> 𝑋 </em>我们期望(<em class="nb"> 𝑚，14 </em>)，其中列是特性。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="31cb" class="nm mf je nz b gy od oe l of og">((792, 14), (792,))</span></pre><p id="9915" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们已经准备好了数据，我们可以在 Tensorflow 中创建一个模型:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="c629" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Tensorflow 中的模型与我们的线性回归模型非常相似。输入从 1 个特征变为 14 个特征，并且我们增加了 Sigmoid 激活功能。接下来，我们必须再次编译我们的模型:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="30dd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这个例子中，我将损失改为“二元交叉熵”。这是另一个对二元逻辑回归问题更有效的损失函数。如果你对内部工作感兴趣，我推荐维基百科。<br/>我们增加了一个额外的指标，叫做“准确度”,现在是为每个历元计算的。现在我们准备训练我们的模型:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="d7f1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这些都是在 Tensorflow 中训练二元逻辑分类器所需的步骤。我们达到了大约 80%的准确率，对于我们付出的努力来说，这已经算不错了。</p><p id="197a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们可以从我们的模型中提取权重<em class="nb"> 𝑊 </em>和偏差<em class="nb"> 𝑏 </em>。我们稍后可以将这些值与我们自己的逻辑回归实现进行比较:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="08b6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">策划损失也很容易:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/53be1fbd44df284108115083a7552727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oUcd5BItECXxWCh8y9_ALA.png"/></div></div></figure><p id="a674" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">不介意好看的 XKCD 抖动:-)。令人印象深刻的是，只需几个步骤就能达到这样的结果。在最后一部分，我们将揭开由 Tensorflow 表演的黑暗绝地艺术。</p><h2 id="fcd7" class="nm mf je bd mg nn no dn mk np nq dp mo lc nr ns mq lg nt nu ms lk nv nw mu nx bi translated">实际发生了什么？</h2><p id="188c" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">嗯，总的配方没有变:</p><ol class=""><li id="2211" class="lq lr je kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">前进传球</li><li id="8412" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">计算损失</li><li id="c83e" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">偶数道次</li><li id="118b" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">更新权重</li><li id="2500" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">重复</li></ol><p id="7631" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这些步骤本身需要稍加修改。正向传递将是更一般的点积，我们需要添加激活函数。损失函数是二元交叉熵，当然和均方差不同。反向传递将计算新损失函数相对于<em class="nb"> 𝑊 </em>和<em class="nb"> 𝑏 </em>的梯度。由于我们现在也有一个激活功能，我们将有一个额外的步骤。update-weights 函数不变，最终的循环也非常相似。</p><p id="026e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">据我所知，在 Numpy 和一般数学中，点积需要向量(和矩阵)的形状兼容:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/ce35b32d7904640b930e0e16ac2f67b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mPQa0oMU1XjwDimKGn4x6w.png"/></div></div></figure><p id="67e2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这意味着<em class="nb"> 𝑋 </em>的列数必须等于<em class="nb"> 𝑌 </em>的行数。为了让我们自己简单一点，我们将转置我们的输入向量<em class="nb"> 𝑋 </em>并翻转向量。这将导致行成为特征，列成为示例。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="8e4d" class="nm mf je nz b gy od oe l of og">(14, 792)</span></pre><p id="ac81" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了便于测试，我们只选择两个例子，让它更具可读性:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="c253" class="nm mf je nz b gy od oe l of og">(14, 2)</span></pre><p id="c917" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们定义我们的重量。因为我们有 14 个特征，我们的向量<em class="nb"> 𝑊 </em>将有 14 个值。偏差对于整个节点是常数，并且只有一个值。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="b657" class="nm mf je nz b gy od oe l of og">(14, 1)</span></pre><p id="04c5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">接下来，我们需要定义 Sigmoid 函数:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="2fab" class="nm mf je nz b gy od oe l of og">array([3.72007598e-44, 5.00000000e-01, 5.24979187e-01, 1.00000000e+00])</span></pre><p id="4127" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在让我们重新定义我们的转发函数，让它使用点积和激活函数。我们可以把这些分成两步:<br/><em class="nb">𝑍</em>=<em class="nb">𝑊𝑋</em>+<em class="nb">𝑏<br/>a =𝜎</em>(<em class="nb">𝑍</em>)</p><p id="1a23" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">注意<em class="nb"> 𝑊𝑋 </em>是一个点积。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="fe14" class="nm mf je nz b gy od oe l of og">array([[0.48951622, 0.50149394]])</span></pre><p id="41a8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们有了实际的预测，我们可以写出损失函数来衡量我们的预测有多好。这是通过二元交叉熵完成的，对此我将简单地给出等式:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/67979a8b51b5c1daab14df89a8b2ac9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fRmVoq03OMCA_wAQiwl7GQ.png"/></div></div></figure><p id="81ca" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">可能会发生这样的情况，我们试图计算一个 log(0 ),当然这是没有定义的。为了避免警告，我们将在损失中增加一个微小的值。由于它非常小，差异并不明显，但确实有助于抑制警告。一天少一个警告，保持…</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="cf58" class="nm mf je nz b gy od oe l of og">1.3625601508159457</span></pre><p id="8022" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">接下来是向后传球。为此，我们需要用<em class="nb"> 𝑊 </em>和<em class="nb"> 𝑏 </em>区分损失函数。为了不让你们觉得无聊，我已经提供了这些函数，但是我不会阻止你们自己计算微分:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/43649f8b21ee88253a31ec11bbc61975.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wS4ndi54Ce17DkXGW7NpaA.png"/></div></div></figure><p id="8922" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在 Python 中，这看起来像这样:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="5504" class="nm mf je nz b gy od oe l of og">(array([[ 0.06118953],<br/>        [-0.01277168],<br/>        [-0.0078041 ],<br/>        [-0.06231326],<br/>        [ 0.        ],<br/>        [ 0.06118953],<br/>        [-0.00011237],<br/>        [-0.00112373],<br/>        [ 0.        ],<br/>        [ 0.        ],<br/>        [ 0.        ],<br/>        [-0.06231326],<br/>        [ 0.        ],<br/>        [ 0.06118953]]),<br/> -0.0011237299781017493)</span></pre><p id="14d0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">差不多了，接下来我们需要更新权重。该函数与我们之前的示例没有变化:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="704c" class="nm mf je nz b gy od oe l of og">(array([[-0.01830035],<br/>        [ 0.00088324],<br/>        [-0.01122826],<br/>        [-0.00589117],<br/>        [-0.00893116],<br/>        [-0.01335291],<br/>        [-0.00061042],<br/>        [ 0.00065638],<br/>        [ 0.00410113],<br/>        [-0.00572882],<br/>        [-0.00801334],<br/>        [ 0.01374348],<br/>        [ 0.01274699],<br/>        [-0.01275547]]),<br/> 1.1237299781017494e-05)</span></pre><p id="5313" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了比较结果，我们可以计算准确度。然而，我们的激活函数返回一个介于 0 和 1 之间的概率。根据定义，值&lt;= 0.5 are rounded to 0 and values &gt; 0.5 被四舍五入为 1。这与常规的 round 函数略有不同，因此我们将为此创建自己的函数:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="e817" class="nm mf je nz b gy od oe l of og">array([[0, 1]], dtype=uint8)</span></pre><p id="06ad" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们有了创建训练循环的一切。我们将存储一些指标，以便以后绘制:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="35b2" class="nm mf je nz b gy od oe l of og">loss: 17.16646271874957 	accuracy: 64.4%<br/>loss: 12.757526841623513 	accuracy: 75.9%<br/>loss: 11.688361064742441 	accuracy: 79.5%<br/>loss: 11.150776615503466 	accuracy: 79.5%<br/>loss: 10.83231532525657 	accuracy: 79.8%<br/>loss: 10.622380143288218 	accuracy: 79.8%<br/>loss: 10.473049466548982 	accuracy: 79.8%<br/>loss: 10.36043263717339 	accuracy: 80.8%</span></pre><p id="054a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们对网络进行了训练，得到了略高于 80%的最终准确率，与使用 Tensorflow 时的结果非常相似。当然，多一点努力，也多一点乐趣。让我们绘制我们的指标:</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/637a5139a2dcc88419ddcced70447be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KYIf8J8V2l9Lkjl5IQcLQA.png"/></div></div></figure><p id="72ca" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当然，我们已经计算了训练数据的指标。为了对模型有一个正确的认识，这应该使用测试数据来完成。这一点，我留给读者；-).</p><h1 id="7766" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">围捕</h1><p id="95ec" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">好了，这就是关于逻辑回归的教程。希望你对逻辑回归工作原理有所了解，张量流不仅仅是魔法。我发现从头开始写这些是一个很好的练习，正如你所看到的，也不是很难。</p><p id="965d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果您有任何意见或建议，请告诉我。</p></div></div>    
</body>
</html>