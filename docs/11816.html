<html>
<head>
<title>Text Generation with Bi-LSTM in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 PyTorch 中的双 LSTM 生成文本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c?source=collection_archive---------5-----------------------#2020-08-16">https://towardsdatascience.com/text-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c?source=collection_archive---------5-----------------------#2020-08-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="845f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过使用 PyTorch 的 LSTMCells 从头开始创建双 LSTM 模型来构建文本生成模型的分步指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3a6e80fa10027a2e16cfba8063dd4d8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9x875YMgaOY1l_V9"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">帕特里克·托马索在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="kz"><p id="cb13" class="la lb it bd lc ld le lf lg lh li lj dk translated">“没有规定怎么写。有时它来得容易而完美:有时它就像钻岩石，然后用炸药把它炸出来”——欧内斯特·海明威</p></blockquote><p id="fa2c" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me lj im bi translated">这篇博客的目的是解释如何通过实现一个基于 LSTMs 的强大架构来构建一个用于文本生成的端到端模型。</p><p id="a15d" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">博客分为以下几个部分:</p><ul class=""><li id="4d92" class="mk ml it lm b ln mf lq mg lt mm lx mn mb mo lj mp mq mr ms bi translated"><strong class="lm iu">简介</strong></li><li id="45eb" class="mk ml it lm b ln mt lq mu lt mv lx mw mb mx lj mp mq mr ms bi translated"><strong class="lm iu">文本预处理</strong></li><li id="e1e9" class="mk ml it lm b ln mt lq mu lt mv lx mw mb mx lj mp mq mr ms bi translated"><strong class="lm iu">序列生成</strong></li><li id="52f7" class="mk ml it lm b ln mt lq mu lt mv lx mw mb mx lj mp mq mr ms bi translated"><strong class="lm iu">模型架构</strong></li><li id="6941" class="mk ml it lm b ln mt lq mu lt mv lx mw mb mx lj mp mq mr ms bi translated"><strong class="lm iu">培训阶段</strong></li><li id="3bdf" class="mk ml it lm b ln mt lq mu lt mv lx mw mb mx lj mp mq mr ms bi translated"><strong class="lm iu">文本生成</strong></li></ul><p id="ba21" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">您可以在以下网址找到完整的代码:<a class="ae ky" href="https://github.com/FernandoLpz/Text-Generation-BiLSTM-PyTorch" rel="noopener ugc nofollow" target="_blank">https://github . com/FernandoLpz/Text-Generation-BiLSTM-py torch</a></p><h1 id="7987" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">介绍</h1><p id="56cb" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">这些年来，各种各样的提案被推出来<em class="nv">模拟自然语言</em>，但是这是怎么回事呢？“<em class="nv">建模自然语言</em>的想法指的是什么？我们可以认为“<em class="nv">建模自然语言</em>”指的是对组成语言的语义和语法进行推理，本质上是这样，但它走得更远。</p><p id="18d6" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">如今，<strong class="lm iu">自然语言处理</strong> ( <strong class="lm iu"> NLP </strong>)领域通过不同的方法和技术处理涉及语言的推理、理解和建模的不同任务。NLP(自然语言处理)领域在过去的十年中发展非常迅速。许多模型从不同的角度提出了不同的 NLP 任务。同样，最受欢迎的提议中的共同点是实现基于<strong class="lm iu">深度学习</strong>的模型。</p><p id="df02" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">正如已经提到的，<strong class="lm iu"> NLP </strong>领域解决了大量的问题，特别是在这篇博客中，我们将通过利用基于<em class="nv">深度学习的模型</em>来解决<em class="nv">文本生成</em>的问题，例如<em class="nv">递归神经网络</em> <strong class="lm iu"> LSTM </strong>和<strong class="lm iu">双 LSTM </strong>。同样，我们将使用当今最复杂的框架之一来开发深度学习模型，具体来说，我们将使用来自<strong class="lm iu"> PyTorch </strong>的<strong class="lm iu">LSTMCell</strong>T14】类来开发所提议的架构。</p><blockquote class="nw nx ny"><p id="0419" class="lk ll nv lm b ln mf ju lp lq mg jx ls nz mh lv lw oa mi lz ma ob mj md me lj im bi translated">如果你想深入了解<strong class="lm iu"> LSTM </strong>的机制，以及它是如何在<strong class="lm iu"> PyTorch </strong>中实现的，看一看这个令人惊叹的解释:<a class="ae ky" rel="noopener" target="_blank" href="/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3"> <em class="it">从 LSTM 细胞到带有 PyTorch </em> </a>的多层 LSTM 网络</p></blockquote><h2 id="105b" class="oc mz it bd na od oe dn ne of og dp ni lt oh oi nk lx oj ok nm mb ol om no on bi translated"><strong class="ak">问题陈述</strong></h2><p id="e293" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">给定一个文本，神经网络将通过字符序列来学习给定文本的语义和语法。随后，将随机抽取一个字符序列，并预测下一个字符。</p><p id="d3fb" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">所以，让我们开始吧！</p><h1 id="e67d" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">文本预处理</h1><p id="f125" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">首先，我们需要一个我们将要使用的文本。有不同的资源在那里你可以找到不同的纯文本文本，我推荐你看一看<a class="ae ky" href="https://www.gutenberg.org/" rel="noopener ugc nofollow" target="_blank"> <em class="nv">古腾堡项目</em> </a>。</p><p id="9d51" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">在这种情况下，我将使用由<em class="nv">乔治·伯德·格林内尔</em>所著的《印第安人中的<em class="nv">杰克》这本书，你可以在这里找到这本书:<a class="ae ky" href="https://www.gutenberg.org/cache/epub/46205/pg46205.txt" rel="noopener ugc nofollow" target="_blank"> <em class="nv">链接到本书</em> </a>。所以，第一章的第一行看起来像:</em></p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="a9b8" class="oc mz it op b gy ot ou l ov ow">The train rushed down the hill, with a long shrieking whistle, and then began to go more and more slowly. Thomas had brushed Jack off and thanked him for the coin that he put in his hand, and with the bag in one hand and the stool in the other now went out onto the platform and down the steps, Jack closely following.</span></pre><p id="92bb" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">如您所见，文本包含大写、小写、换行符、标点符号等。建议做的是尝试将文本改编为一种形式，这种形式允许我们以更好的方式处理它，并且主要降低我们将要开发的模型的复杂性。所以我们将把每个字符转换成小写形式。同样，最好将文本作为一个字符列表来处理<em class="nv">，也就是说，我们将拥有一个字符列表，而不是一个“<em class="nv">大字符串</em>”。将文本作为一个字符序列的目的是为了在生成序列时更好地处理模型(我们将在下一节中详细讨论)。</em></p><p id="328e" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">所以让我们开始吧！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 1。预处理</p></figure><p id="6777" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">正如我们所看到的，在第 2 行中，我们定义了要使用的字符，所有其他符号都将被丢弃，我们只保留“<em class="nv">空白字符</em>”符号。在第 6 行和第 10 行，我们正在读取<em class="nv">原始文件</em>，并将其转换成小写形式。在第 14 行和第 19 行的循环中，我们创建了一个表示整本书的字符串，并生成了一个字符列表。在第 23 行，我们通过只保留第 2 行定义的字母来过滤<em class="nv">文本列表</em>。</p><p id="2d08" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">因此，一旦文本被加载和预处理，我们将从这样的文本:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="fb1a" class="oc mz it op b gy ot ou l ov ow">text = "The train rushed down the hill."</span></pre><p id="09a4" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">有这样一个字符列表:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="205f" class="oc mz it op b gy ot ou l ov ow">text = ['t','h','e',' ','t','r','a','i','n',' ','r','u','s','h','e','d',' ','d','o','w','n',<br/>' ','t','h','e',' ','h','i','l','l']</span></pre><p id="76a5" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">我们已经有了完整的文本，作为一个字符列表。众所周知，我们不能将<em class="nv">原始字符</em>直接引入神经网络，我们需要一个<em class="nv">数字表示</em>，因此，我们需要将每个字符转换成一个数字表示。为此，我们将创建一个字典来帮助我们保存等价的"<em class="nv">字符索引</em>"和" i <em class="nv">索引字符</em>"。</p><p id="1225" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">所以，让我们开始吧！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 2。词典创作</p></figure><p id="e4cf" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">我们可以注意到，在第 11 行和第 12 行创建了"<em class="nv"> char-index </em>"和"<em class="nv"> index-char </em>"字典。</p><p id="28af" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">到目前为止，我们已经展示了如何加载文本并以字符列表的形式保存它，我们还创建了几个字典来帮助我们对每个字符进行编码解码。现在，是时候看看我们将如何生成将被引入模型的序列了。那么，让我们进入下一部分吧！</p><h1 id="0d31" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">序列生成</h1><p id="9228" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">序列生成的方式完全取决于我们将要实现的模型的类型。如前所述，我们将使用<strong class="lm iu"> LSTM </strong>类型的递归神经网络，它顺序接收数据(时间步长)。</p><p id="d3e0" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">对于我们的模型，我们需要形成给定长度的序列，我们称之为“<strong class="lm iu"> <em class="nv">窗口</em> </strong>”，其中要预测的字符(<strong class="lm iu"> <em class="nv">目标</em> </strong>)将是<strong class="lm iu"> <em class="nv">窗口</em> </strong>旁边的字符。每个序列将由<strong class="lm iu"> <em class="nv">窗口</em> </strong>中包含的字符组成。为了形成一个序列，<strong class="lm iu"> <em class="nv">窗口</em> </strong>被一次向右切割一个字符。要预测的字符将始终是跟随<strong class="lm iu"> <em class="nv">窗口</em> </strong>的字符。我们可以在图 1 中清楚地看到这个过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/148585f10754b80dd5534180f268c8a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-4Y_FY9Zz5cvA3T3KS6nBQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。序列生成。在这个例子中，窗口大小为 4，这意味着它将包含 4 个字符。目标是作者的窗口|图像旁边的第一个字符</p></figure><p id="9251" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">到目前为止，我们已经看到了如何以简单的方式生成字符序列。现在我们需要将每个字符转换成其各自的数字格式，为此我们将使用预处理阶段生成的字典。这个过程可以在图 2 中看到。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/f06774325da6eea788575cd18fe380b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KBhXEXAuYCAIaW9ruELbBA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。作者将字符转换为数字格式|图片</p></figure><p id="2971" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">很好，现在我们知道了如何使用一次滑动一个字符的<strong class="lm iu"><em class="nv"/></strong>窗口生成字符序列，以及如何将字符转换成数字格式，下面的代码片段显示了所描述的过程。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 3。序列生成</p></figure><p id="41ae" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">太棒了，现在我们知道如何预处理原始文本，如何将它转换成字符列表，以及如何生成数字格式的序列。现在我们进入最有趣的部分，模型架构。</p><h1 id="b663" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">模型架构</h1><p id="1bfd" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">正如你已经在这篇博客的标题中读到的，我们将利用<strong class="lm iu">双 LSTM </strong>递归神经网络和标准<strong class="lm iu"> LSTMs </strong>。本质上，我们利用这种类型的神经网络，因为它在处理序列数据(如文本类型数据)时具有巨大的潜力。同样，也有大量的文章提到了基于递归神经网络的架构的使用(例如<strong class="lm iu"> RNN、</strong>T10、LSTM、<strong class="lm iu"> GRU、</strong>、<strong class="lm iu">比-LSTM </strong>等)。)进行文本建模，具体为文本生成[1，2]。</p><blockquote class="nw nx ny"><p id="4158" class="lk ll nv lm b ln mf ju lp lq mg jx ls nz mh lv lw oa mi lz ma ob mj md me lj im bi translated">所提出的神经网络的架构包括一个<strong class="lm iu">嵌入</strong>层，接着是一个<strong class="lm iu">双 LSTM </strong>以及一个<strong class="lm iu"> LSTM </strong>层。紧接着，后者<strong class="lm iu"> LSTM </strong>连接到一个<strong class="lm iu">线性层</strong>上。</p></blockquote><h2 id="6fa0" class="oc mz it bd na od oe dn ne of og dp ni lt oh oi nk lx oj ok nm mb ol om no on bi translated">方法学</h2><p id="8af3" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">该方法包括将每个字符序列传递到<strong class="lm iu">嵌入</strong>层，这将为组成该序列的每个元素生成一个矢量形式的表示，因此我们将形成一个嵌入字符的<em class="nv">序列。随后，嵌入字符</em>的<em class="nv">序列的每个元素将被传递到<strong class="lm iu">双 LSTM </strong> <strong class="lm iu">层</strong>。随后，将产生组成<strong class="lm iu">双 LSTM</strong><strong class="lm iu">正向 LSTM </strong>和<strong class="lm iu">反向 LSTM </strong>的<strong class="lm iu">lstm 的每个输出的级联。紧接着，每个<em class="nv">向前+向后</em>连接的向量将被传递到<strong class="lm iu"> LSTM </strong> <strong class="lm iu">层</strong>，从该层中<strong class="lm iu">最后一个隐藏状态</strong>将被用于馈送到<strong class="lm iu">线性层</strong>。这个最后的<strong class="lm iu">线性层</strong>将具有作为激活函数的<strong class="lm iu"> Softmax 函数</strong>，以便表示每个字符的概率。图 3 显示了所描述的方法。</strong></em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/b39995bdcce3b6522f6f6773eaf5e716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EiOZtPtYUkInaq1HBefJwQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3。比尔斯特姆-LSTM 模型。在这张图片中，作者将单词“熊”通过比尔斯特姆-LSTM 模型进行文本生成</p></figure><p id="0e2f" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">太棒了，到目前为止，我们已经解释了文本生成模型的架构以及实现的方法。现在我们需要知道如何用<strong class="lm iu"> PyTorch 框架</strong>完成所有这些，但是首先，我想简单解释一下<strong class="lm iu"> Bi-LSTM </strong>和<strong class="lm iu"> LSTM </strong>是如何一起工作的，以便稍后看到我们如何在代码中完成，所以让我们看看<strong class="lm iu"> Bi-LSTM </strong> <strong class="lm iu">网络</strong>是如何工作的。</p><h2 id="0df6" class="oc mz it bd na od oe dn ne of og dp ni lt oh oi nk lx oj ok nm mb ol om no on bi translated">双 LSTM &amp; LSTM</h2><p id="b18a" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">一辆标准<strong class="lm iu"> LSTM </strong>和一辆<strong class="lm iu">比 LSTM </strong>的关键区别在于，这辆<strong class="lm iu"> <em class="nv">比 LSTM </em> </strong> <em class="nv">是由</em> <strong class="lm iu"> <em class="nv"> 2 个 lstm</em></strong>组成的，更好的说法是“<em class="nv">向前</em><strong class="lm iu"><em class="nv">【LSTM</em></strong>和“<em class="nv">向后</em> <strong class="lm iu"> <em class="nv"> LSTM </em> </strong>”。基本上，<em class="nv">前进</em> <strong class="lm iu"> <em class="nv"> LSTM </em> </strong>按原顺序接收，而<em class="nv">后退</em><strong class="lm iu"><em class="nv">LSTM</em></strong><em class="nv"/>按相反顺序接收。随后，根据要做的事情，两个<strong class="lm iu">lstm</strong>的每个时间步的每个<strong class="lm iu">隐藏状态</strong>可以被加入，或者只有两个<strong class="lm iu">lstm</strong>的<strong class="lm iu">最后状态</strong>将被操作。在提议的模型中，我们建议为每个时间步加入两个隐藏状态<strong class="lm iu"/>。</p><p id="5735" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">太好了，现在我们明白了<strong class="lm iu">双 LSTM </strong>和<strong class="lm iu"> LSTM </strong>的主要区别。回到我们正在开发的例子，图 4 显示了每个字符序列在通过模型时的演变。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/8505845c44c8e0602e83d96cf8dccdcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bW3tP9CBiWqCeLFDwv-xDQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4。比尔斯特姆-LSTM 模型。一个简单的例子显示了每个角色在通过作者的模型|图像时的演变</p></figure><p id="d59b" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">很好，一旦关于<strong class="lm iu"> Bi-LSTM </strong>和<strong class="lm iu"> LSTM </strong>之间的交互的一切都清楚了，让我们看看如何仅使用来自伟大的<strong class="lm iu"> PyTorch </strong>框架的<strong class="lm iu"> LSTMCells </strong>在代码中实现这一点。</p><p id="8183" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">所以，首先让我们了解一下我们如何构造<strong class="lm iu"> TextGenerator </strong>类的构造函数，让我们看看下面的代码片段:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 4。文本生成器类的构造函数</p></figure><p id="e6e4" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">正如我们所看到的，从第 6 行到第 10 行，我们定义了用于初始化神经网络每一层的参数。需要特别提到的是<em class="nv"> input_size </em>等于词汇表的<em class="nv">大小(也就是我们的字典在预处理中生成的包含的元素个数)。同样，要预测的<em class="nv">类的数量</em>也与词汇表的大小相同，并且<em class="nv"> sequence_length </em>指的是<em class="nv">窗口</em>的大小。</em></p><p id="a602" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">另一方面，在第 20 行和第 21 行，我们定义了两个<strong class="lm iu">lstmcell</strong>，它们组成了<strong class="lm iu">双 LSTM </strong> ( <em class="nv">向前</em>和<em class="nv">向后</em>)。在第 24 行中，我们定义了<strong class="lm iu"> LSTMCell </strong>，它将被馈送以<strong class="lm iu">双 LSTM </strong>的输出。值得一提的是<strong class="lm iu">隐藏状态</strong>的大小是<strong class="lm iu">双 LSTM </strong>的两倍，这是因为<strong class="lm iu">双 LSTM </strong>的输出是级联的。稍后在第 27 行我们定义了<strong class="lm iu">线性层</strong>，稍后将被<strong class="lm iu"> softmax </strong>函数过滤。</p><p id="c3dc" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">一旦定义了构造函数，我们需要为每个<strong class="lm iu"> LSTM </strong>创建包含<strong class="lm iu">单元格状态</strong> ( <em class="nv"> cs </em>)和<strong class="lm iu">隐藏状态</strong> ( <em class="nv"> hs </em>)的张量。因此，我们按如下方式进行:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 5。权重初始化</p></figure><p id="f6f2" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">太棒了，一旦定义了将包含<strong class="lm iu">隐藏状态</strong>和<strong class="lm iu">单元状态</strong>的张量，就该展示整个架构的组装是如何完成的了，让我们开始吧！</p><p id="bd08" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">首先，让我们看看下面的代码片段:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 6。BiLSTM + LSTM +线性层</p></figure><p id="b1ae" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">为了更好的理解，我们将用一些定义的值来解释这个集合，这样我们就可以理解每个张量是如何从一层传递到另一层的。假设我们有:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="4986" class="oc mz it op b gy ot ou l ov ow">batch_size = 64<br/>hidden_size = 128<br/>sequence_len = 100<br/>num_classes = 27</span></pre><p id="587e" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">因此,<em class="nv"> x </em>输入张量将有一个形状:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="d5dc" class="oc mz it op b gy ot ou l ov ow"># torch.Size([batch_size, sequence_len])<br/>x : torch.Size([64, 100])</span></pre><p id="9d5e" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">然后，在第 2 行中，通过嵌入层传递了<em class="nv"> x </em>张量，因此输出的大小为:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="7a40" class="oc mz it op b gy ot ou l ov ow"># torch.Size([batch_size, sequence_len, hidden_size])<br/>x_embedded : torch.Size([64, 100, 128])</span></pre><p id="d2ce" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">值得注意的是，在第 5 行，我们正在对<em class="nv"> x 嵌入张量</em>进行<strong class="lm iu">整形</strong>。这是因为我们需要将<em class="nv">序列长度</em>作为第一维度，本质上是因为在<strong class="lm iu">双 LSTM </strong>中，我们将迭代每个序列，因此整形后的张量将具有一个形状:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="c839" class="oc mz it op b gy ot ou l ov ow"># torch.Size([sequence_len, batch_size, hidden_size])<br/>x_embedded_reshaped : torch.Size([100, 64, 128])</span></pre><p id="3d05" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">紧接着，在第 7 行和第 8 行中定义了<em class="nv">向前</em>和<em class="nv">向后</em>列表。在那里我们将存储<strong class="lm iu">双 LSTM 的<strong class="lm iu">隐藏状态</strong>。</strong></p><p id="e843" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">所以是时候给双 LSTM 喂食了。首先，在第 12 行我们迭代了<strong class="lm iu"> <em class="nv">前进的 LSTM </em> </strong>，我们还保存了每个<em class="nv">时间步</em> ( <strong class="lm iu"> hs_forward </strong>)的<strong class="lm iu">隐藏状态</strong>。在第 19 行，我们正在迭代向后的<em class="nv"/><strong class="lm iu"><em class="nv">LSTM</em></strong>，同时我们正在保存每个<em class="nv">时间步</em> ( <strong class="lm iu"> hs_backward </strong>)的<strong class="lm iu">隐藏状态</strong>。你可以注意到循环是以同样的顺序进行的，不同的是它是以相反的形式读出的。每个<strong class="lm iu">隐藏状态</strong>将具有以下形状:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="4552" class="oc mz it op b gy ot ou l ov ow"># hs_forward : torch.Size([batch_size, hidden_size])<br/>hs_forward : torch.Size([64, 128])</span><span id="ce07" class="oc mz it op b gy pd ou l ov ow"># hs_backward : torch.Size([batch_size, hidden_size])<br/>hs_backward: torch.Size([64, 128])</span></pre><p id="5a8b" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">太好了，现在让我们看看如何喂养最新的<strong class="lm iu"> LSTM 层</strong>。为此，我们利用<em class="nv">前进</em>和<em class="nv">后退</em>列表。在第 26 行中，我们正在遍历每个<strong class="lm iu">隐藏状态</strong>，对应于第 27 行中<strong class="lm iu">连接的<em class="nv">向前</em>和<em class="nv">向后</em>。值得注意的是，通过<strong class="lm iu">连接</strong>两个<strong class="lm iu">隐藏状态</strong>，张量的维数将增加 2X，即张量将具有以下形状:</strong></p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="ec77" class="oc mz it op b gy ot ou l ov ow"># input_tesor : torch.Size([bathc_size, hidden_size * 2])<br/>input_tensor : torch.Size([64, 256])</span></pre><p id="8f62" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">最后，LSTM 将返回一个<strong class="lm iu">大小的隐藏状态</strong>:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="209f" class="oc mz it op b gy ot ou l ov ow"># last_hidden_state: torch.Size([batch_size, num_classes])<br/>last_hidden_state: torch.Size([64, 27])</span></pre><p id="8fb7" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">最后，<strong class="lm iu"> LSTM </strong>的<strong class="lm iu">最后隐藏状态</strong>将通过 l <strong class="lm iu">线性层</strong>，如第 31 行所示。因此，完整的转发函数如下面的代码片段所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 7。正向功能</p></figure><p id="61e9" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">恭喜你！至此，我们已经知道如何在<strong class="lm iu"> PyTorch </strong>中使用<strong class="lm iu"> LSTMCell </strong>组装神经网络。现在是时候看看我们如何进行培训了，让我们进入下一部分。</p><h1 id="ea31" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">培训阶段</h1><p id="0df3" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">太好了，我们来参加<strong class="lm iu">培训</strong>。为了执行<strong class="lm iu">训练</strong>我们需要初始化<em class="nv">模型</em>和<em class="nv">优化器</em>，稍后我们需要为每个<em class="nv">时期</em>和每个<em class="nv">小批量</em>进行迭代，让我们开始吧！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 8。培训阶段</p></figure><p id="31d9" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">一旦模型被<strong class="lm iu">训练</strong>，我们将需要<strong class="lm iu">保存神经网络的权重</strong>，以便稍后使用它们<strong class="lm iu">生成文本</strong>。为此我们有两个选择，第一个是定义一个固定数量的历元 s，然后保存权重，第二个是确定一个停止函数<em class="nv">以获得模型的最佳版本。在这种特殊情况下，我们将选择第一个选项。在一定数量的时期下训练模型之后，我们如下保存权重:</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 9。节省重量</p></figure><p id="eecf" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">完美，到目前为止我们已经看到了如何<strong class="lm iu">训练文本生成器</strong>以及如何<strong class="lm iu">保存权重</strong>，现在我们将进入这个博客的顶部，文本生成<strong class="lm iu">！那么让我们进入下一部分。</strong></p><h1 id="5aaf" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">文本生成</h1><p id="d1fa" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">太棒了，我们已经到了博客的最后一部分，<strong class="lm iu">文本生成</strong>。为此，我们需要做两件事:第一件是<strong class="lm iu">加载训练好的权重</strong>，第二件是<strong class="lm iu">从一组序列</strong>中随机抽取一个样本作为开始生成下一个字符的模式。所以让我们来看看下面的代码片段:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 10。文本生成器</p></figure><p id="a735" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">因此，通过在以下特征下训练模型:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="2d0e" class="oc mz it op b gy ot ou l ov ow"><strong class="op iu">window : 100<br/>epochs : 50<br/>hidden_dim : 128<br/>batch_size : 128<br/>learning_rate : 0.001</strong></span></pre><p id="7923" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">我们可以生成以下内容:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="81d8" class="oc mz it op b gy ot ou l ov ow"><strong class="op iu">Seed:<br/></strong><em class="nv">one of the prairie swellswhich gave a little wider view than most of them jack saw quite close to the</em></span><span id="f944" class="oc mz it op b gy pd ou l ov ow"><strong class="op iu">Prediction:<br/></strong><em class="nv">one of the prairie swellswhich gave a little wider view than most of them jack saw quite close to the</em> wnd banngessejang boffff <strong class="op iu">we</strong> outheaedd <strong class="op iu">we band</strong> r hes tller a reacarof t t alethe ngothered uhe th wengaco ack fof ace ca  e s alee bin  cacotee tharss th <strong class="op iu">band</strong> fofoutod <strong class="op iu">we we</strong> ins sange trre anca y w farer <strong class="op iu">we</strong> sewigalfetwher d e  <strong class="op iu">we</strong> n s shed <strong class="op iu">pack</strong> wngaingh tthe <strong class="op iu">we the we</strong> javes t supun f <strong class="op iu">the</strong> har <strong class="op iu">man</strong> bllle s ng ou   y anghe ond <strong class="op iu">we</strong> nd ba a  <strong class="op iu">she</strong> t t anthendwe wn <strong class="op iu">me</strong> anom ly tceaig t i isesw arawns t d ks wao thalac tharr jad  d anongive <strong class="op iu">where</strong> <strong class="op iu">the</strong> awe w we he is ma mie cack seat sesant sns t imes hethof riges <strong class="op iu">we he d</strong> ooushe <strong class="op iu">he hang out</strong> f t thu inong bll llveco <strong class="op iu">we see</strong> s <strong class="op iu">the he</strong> haa <strong class="op iu">is</strong> s igg merin ishe d t san wack owhe o or th we sbe se <strong class="op iu">we we</strong> inange t ts wan br seyomanthe harntho thengn  th me ny <strong class="op iu">we</strong> ke in acor offff  of wan  s arghe <strong class="op iu">we</strong> t angorro <strong class="op iu">the</strong> wand <strong class="op iu">be</strong> <strong class="op iu">thing</strong> a sth t tha alelllll willllsse of s wed w brstougof bage orore <strong class="op iu">he</strong> anthesww <strong class="op iu">were</strong> ofawe ce qur <strong class="op iu">the he</strong> sbaing tthe bytondece nd t llllifsffo acke o t <strong class="op iu">in</strong> ir <strong class="op iu">me</strong> hedlff scewant pi t bri pi owasem <strong class="op iu">the</strong> awh thorathas th <strong class="op iu">we</strong> hed ofainginictoplid <strong class="op iu">we me</strong></span></pre><p id="3ce1" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">正如我们所看到的，生成的文本可能没有任何意义，但是有一些单词和短语似乎形成了一个想法，例如:</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="fe05" class="oc mz it op b gy ot ou l ov ow">we, band, pack, the, man, where, he, hang, out, be, thing, me, were</span></pre><p id="d0ce" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">恭喜你，我们已经到了博客的结尾！</p></div><div class="ab cl pe pf hx pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="im in io ip iq"><h1 id="0220" class="my mz it bd na nb pl nd ne nf pm nh ni jz pn ka nk kc po kd nm kf pp kg no np bi translated">结论</h1><p id="efc4" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">在这篇博客中，我们展示了如何使用<strong class="lm iu"> PyTorch 的 LSTMCell </strong>和基于递归神经网络<strong class="lm iu"> LSTM </strong>和<strong class="lm iu">双 LSTM 实现一个架构来制作一个端到端的文本生成模型。</strong></p><p id="8435" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">重要的是要说明，文本生成的建议模型可以通过不同的方式进行改进。一些建议的想法将是增加<em class="nv">待训练的文本语料库</em>的大小，<em class="nv">增加历元</em>的数量以及每个<strong class="lm iu"> LSTM 的<em class="nv">存储器大小</em>。</strong>另一方面，我们可以想到一个基于<strong class="lm iu">卷积 LSTM </strong>的有趣架构(可能是另一个博客的主题)。</p><h2 id="d24b" class="oc mz it bd na od oe dn ne of og dp ni lt oh oi nk lx oj ok nm mb ol om no on bi translated">参考</h2><p id="2cb5" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">[1] <a class="ae ky" href="https://arxiv.org/pdf/1908.04332.pdf" rel="noopener ugc nofollow" target="_blank"> LSTM 对 GRU 对双向 RNN 的脚本生成</a></p><p id="7f53" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">[2] <a class="ae ky" href="https://www.sciencedirect.com/science/article/pii/S1319157820303360" rel="noopener ugc nofollow" target="_blank">调查:深度学习中的文本生成模型</a></p></div></div>    
</body>
</html>