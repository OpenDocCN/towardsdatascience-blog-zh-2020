<html>
<head>
<title>Fine Tuning a T5 transformer for any Summarization Task</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为任何摘要任务微调 T5 变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81?source=collection_archive---------5-----------------------#2020-09-09">https://towardsdatascience.com/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81?source=collection_archive---------5-----------------------#2020-09-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="093f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用数据做很酷的事情！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f5035773fb92f2f9ae60101bf8c3f9e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r_KsjHpbg5WLVW4Myp_RVg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">借用自 pexels.com<a class="ae ky" href="https://www.pexels.com/photo/book-eyeglasses-eyewear-page-261857/" rel="noopener ugc nofollow" target="_blank">的免版税图片</a></p></figure><h1 id="ede1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="fff9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我对<a class="ae ky" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank"> T5 变压器型号</a>的威力感到惊讶！T5 代表文本到文本转换转换器，它使得在任何文本到文本任务上微调转换器模型变得容易。任何 NLP 任务事件如果是分类任务，都可以被框定为输入文本到输出文本的问题。</p><p id="5679" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇博客中，我展示了如何在<em class="ms">的任何数据集</em>上调优这个模型。特别是，我演示了如何在汇总数据集上实现这一点。我在 CNN-每日邮报和维基百科数据集上亲自测试过。代码可以在我的 Github <a class="ae ky" href="https://github.com/priya-dwivedi/Deep-Learning/blob/master/wikihow-fine-tuning-T5/Tune_T5_WikiHow-Github.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>公开获得。</p><p id="7453" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">T5-small 接受过 Wikihow 培训，能够写出令人惊叹的摘要。请参见下面的实际文本、实际摘要和预测摘要的片段。这个模型也可以在 HuggingFace 变形金刚模型中枢<a class="ae ky" href="https://huggingface.co/deep-learning-analytics/wikihow-t5-small?text=+Bring+1%2F2+cup+water+to+the+boil.Add+the+fresh+or+dried+rosemary+to+the+water.Remove+from+the+heat.+Set+aside+for+1%2F2+an+hour+to+infuse.+Added+flavour+can+be+released+by+pressing+down+on+the+rosemary+leaves+with+a+spoon.+Add+the+pieces+to+the+blender+or+food+processor+with+the+elderflower+cordial.+Blend+or+process+to+a+pur%C3%A9e.%2C%2C+Add+the+lemon+or+lime+juice+and+stir+to+combine.%2C+Add+a+cover+and+place+in+the+freezer.After+2+hours%2C+remove+from+the+freezer+and+break+up+with+a+fork.+This+helps+the+ice+crystals+to+form+properly.Continue+doing+this+every+hour+until+the+granita+freezes+properly.+Scoop+the+granita+into+dessert+bowls+and+serve.+Garnish+with+a+cucumber+curl+or+a+small+sprig+of+rosemary." rel="noopener ugc nofollow" target="_blank">这里</a>。该链接提供了一种在输入文本和 JSON 端点上测试模型的便捷方式。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="0625" class="my la it mu b gy mz na l nb nc">WikiHow Text: Make sure you've got all the cables disconnected from the back of your console,<br/>especially the power cord., You'll need the straight end to be about 2-3 inches long.You will need a<br/>large size paper clip for this method because it will need to go in about 1 and a half inches to<br/>push the disc out., It's located on the left side of the console, right behind the vents.The eject<br/>hole on an Xbox One S is located at the second hole on the left from the right corner and the third<br/>hole up from the bottom. It can be more difficult to spot, so it's best to have a good amount of<br/>light available. Doing so will cause the disc to pop out a little bit., Carefully pull the disc the<br/>rest of the way out with your fingers. It might be a good idea to use a cloth or soft fabric to<br/>protect the disc from fingerprints and scratching.<br/><br/>Actual Summary: Unplug all cables from your Xbox One.Bend a paper clip into a straight line.Locate the orange circle.Insert the paper clip into the eject hole.Use your fingers to pull the disc out.<br/><br/>Predicted Summary: Gather the cables.Place the disc on your console.Section the eject hole on the left side of the console.Pull out the disc.Remove from the back of the console.</span></pre><p id="9580" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我经营一家机器学习咨询，<a class="ae ky" href="https://deeplearninganalytics.org/" rel="noopener ugc nofollow" target="_blank">深度学习分析</a>。在深度学习分析公司，我们非常热衷于使用数据科学和机器学习来解决现实世界的问题。如果您正在为您的业务项目寻找 NLP 专业知识，请联系我们。原文全文发表在<a class="ae ky" href="https://deeplearninganalytics.org/fine-tuning-a-t5-transformer-for-any-summarization-task/" rel="noopener ugc nofollow" target="_blank">我们的网站这里</a>。</p><h1 id="f934" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">T5 变压器型号</h1><p id="2a6b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">google research 发布的 T5 模型在现有研究的基础上增加了以下内容:</p><ol class=""><li id="a3ff" class="nd ne it lt b lu mn lx mo ma nf me ng mi nh mm ni nj nk nl bi translated">它创建了一个大规模通用爬网数据集的干净版本，称为巨大干净通用爬网(C4)。这个数据集是 s <em class="ms">比维基百科</em>大两个数量级。</li><li id="171a" class="nd ne it lt b lu nm lx nn ma no me np mi nq mm ni nj nk nl bi translated">它在普通爬行上训练 T5</li><li id="e621" class="nd ne it lt b lu nm lx nn ma no me np mi nq mm ni nj nk nl bi translated">它建议将所有的自然语言处理任务作为输入文本重新组织为输出文本公式</li><li id="d50c" class="nd ne it lt b lu nm lx nn ma no me np mi nq mm ni nj nk nl bi translated">它表明，使用预训练的 T5 和文本-文本公式化对不同任务(摘要、QnA、阅读理解)的微调产生了最先进的结果</li><li id="47bd" class="nd ne it lt b lu nm lx nn ma no me np mi nq mm ni nj nk nl bi translated">T5 团队还进行了系统研究，以了解预培训和微调的最佳实践。他们的论文详述了哪些参数对获得好的结果最重要。</li></ol><p id="3839" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">来自 T5 论文的下图解释了这个输入文本到输出文本的问题公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/f0d65c376f779affb920f15836b52db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wyY4MYzwv-gTpTi60b_-0Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">T5 模型任务制定。图来自<a class="ae ky" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank"> T5 论文</a></p></figure><p id="f14a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">谷歌的这篇博客也很好地解释了这篇论文。现在让我们深入研究代码吧！</p><h1 id="7e79" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">T5 微调管道</h1><p id="969a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将使用 T5 模型的 HuggingFace Transformers 实现来完成这项任务。非常感谢 Suraj 的这个<a class="ae ky" href="https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb" rel="noopener ugc nofollow" target="_blank">棒极了的作品</a>，我用它作为我代码的起点。</p><h2 id="3a1b" class="my la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">获取数据</h2><p id="5d12" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了简单地将这个管道扩展到任何 NLP 任务，我使用了<a class="ae ky" href="https://github.com/huggingface/nlp/tree/master/datasets/wikihow" rel="noopener ugc nofollow" target="_blank"> HuggingFace NLP </a>库来获取数据集。这使得加载许多支持数据集变得容易。HuggingFace NLP 库也支持许多指标。我已经为我的模型使用了它 rouge score 实现。</p><p id="6b3e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我的<a class="ae ky" href="https://github.com/priya-dwivedi/Deep-Learning/blob/master/wikihow-fine-tuning-T5/Tune_T5_WikiHow-Github.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上有完整的代码。在这个演示中，我将展示如何处理<a class="ae ky" href="https://www.tensorflow.org/datasets/catalog/wikihow" rel="noopener ugc nofollow" target="_blank"> WikiHow 数据集。该代码可以灵活地扩展到任何摘要任务。</a></p><p id="40c1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">涉及的主要步骤是:</p><ol class=""><li id="87cf" class="nd ne it lt b lu mn lx mo ma nf me ng mi nh mm ni nj nk nl bi translated">加载 Wikihow 数据。请注意，对于此数据集，需要将两个文件下载到本地数据文件夹</li><li id="e42a" class="nd ne it lt b lu nm lx nn ma no me np mi nq mm ni nj nk nl bi translated">NLP 库创建的 dataset 对象可用于查看示例</li><li id="2f0f" class="nd ne it lt b lu nm lx nn ma no me np mi nq mm ni nj nk nl bi translated">我们希望查看文本的平均长度，以决定输入是否可以标记为最大长度 512</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="310f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于 Wikihow 数据集，文本的平均长度是 660 个单词，摘要的平均长度是 49 个单词。下图显示了文本长度的分布</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f90fdc264280de0948030427379b7b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*S3pzDYsaPWevyYVtOv9mSA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">WikiHow 上的文本长度分布</p></figure><p id="398a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">WikiHow 文本通常是一个主题的 1-2 段说明性文本。下面分享一个例子</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="c08b" class="my la it mu b gy mz na l nb nc">WikiHow Text: Each airline has a different seat design, but you should find a lever on the side of<br/>your seat in many cases. Pull it up to bring the seat back up. If you can't find the lever, ask a<br/>flight attendant for help., Most airlines still use seat belts that only go across your lap. Locate<br/>the buckle on one side and the latching device on the other. Straighten out each side, if necessary.<br/>Insert the buckle into the latching device. Make sure you hear a click. Pull the belt until it's<br/>snug across the tops of your thighs., Do this even if the captain turns off the “Fasten Seat Belts”<br/>sign. If you decide to recline, make sure the belt stays snug across your lap. If you're using a<br/>blanket, place it between the belt and your body.</span></pre><h2 id="4402" class="my la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">为数据创建 Pytorch 数据集类</h2><p id="d181" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">接下来，我们定义一个 Pytorch 数据集类，它可以用于任何 NLP 数据集类型。对于文本到文本 T5，我们必须定义输入文本和目标文本的字段。这里，文章的“文本”是输入文本，“标题”是其摘要。</p><p id="28f3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我使用了 512 个令牌的输入目标长度和 150 个输出摘要长度。wikihow 数据集类的输出是:</p><ul class=""><li id="ad2b" class="nd ne it lt b lu mn lx mo ma nf me ng mi nh mm og nj nk nl bi translated">source_ids:标记化的输入文本长度被截断/填充到最大长度 512</li><li id="8691" class="nd ne it lt b lu nm lx nn ma no me np mi nq mm og nj nk nl bi translated">source_mask:对应于输入令牌 id 的注意掩码</li><li id="64fe" class="nd ne it lt b lu nm lx nn ma no me np mi nq mm og nj nk nl bi translated">target_ids:标记化的目标(摘要)文本长度被截断/填充到最大长度 150</li><li id="db0a" class="nd ne it lt b lu nm lx nn ma no me np mi nq mm og nj nk nl bi translated">target_mask:对应于目标令牌 id 的注意掩码</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="1a81" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我在<a class="ae ky" href="https://github.com/priya-dwivedi/Deep-Learning/blob/master/wikihow-fine-tuning-T5/Tune_T5_WikiHow-Github.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上的笔记本有示例代码，您可以用它来玩数据集类，以检查输入是否被正确编码和解码。</p><h2 id="f980" class="my la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">定义 T5 调谐器</h2><p id="15b2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">T5 调谐器是一个 pytorch lightning 类，它定义了数据加载器、模型前向传递、一步训练、一步验证以及时期结束时的验证。</p><p id="becc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我在这里添加了一些功能，以便更容易地使用它进行总结:</p><ol class=""><li id="e786" class="nd ne it lt b lu mn lx mo ma nf me ng mi nh mm ni nj nk nl bi translated">我已经使用了<a class="ae ky" href="https://github.com/huggingface/nlp" rel="noopener ugc nofollow" target="_blank"> NLP 库</a>来导入 rouge_metric</li><li id="b64c" class="nd ne it lt b lu nm lx nn ma no me np mi nq mm ni nj nk nl bi translated">我扩展了代码以在验证步骤生成预测，并使用这些预测来计算 rouge 指标</li><li id="af11" class="nd ne it lt b lu nm lx nn ma no me np mi nq mm ni nj nk nl bi translated">将 WANDB 添加为记录器</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><h1 id="ae85" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">训练模型</h1><p id="7ccb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我决定训练一个 T5 小模型。我对 train 和 val 都使用了 4 的批量大小，可以在大约 4 小时内在 GTX 1080Ti 上训练这个模型。该模型被训练了 2 个时期，并且 WANDB logger 在模型被训练时显示出 Rouge1 分数和 Val 损失的良好改善。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e406a7a385acfd9945dd5aec86139fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*TXKSBJSrjogc75qjL142lQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Rouge1 分数— Wikihow T5 小型 WandB 记录器</p></figure><p id="3b5b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该模型的完整报告在这里<a class="ae ky" href="https://app.wandb.ai/pdwivedi/wikohow-t5/reports/Wikihow-T5-small-model--VmlldzoyMjk4ODY" rel="noopener ugc nofollow" target="_blank">分享。</a></p><h1 id="3113" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">测试模型</h1><p id="9443" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我已经把这个模型上传到 Huggingface Transformers 模型中心，并在这里测试。要在本地测试模型，可以使用 HuggingFace AutoModelWithLMHeadand 和 AutoTokenizer 特性来加载它。下面分享了这样做的示例脚本。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="3ec0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">当前模型的主要缺点是输入文本长度被设置为最大 512 个标记。这对于许多总结问题来说可能是不够的。为了克服这个限制，我正在研究一个基于<a class="ae ky" href="https://github.com/allenai/longformer" rel="noopener ugc nofollow" target="_blank"> Longformer </a>的摘要模型。我会很快分享我的博客！</p><h1 id="24d2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="3050" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">T5 是一款很棒的车型。有了足够的数据，针对任何 NLP 问题微调转换器变得很容易。在这篇博客中，我创建了一个代码外壳，可以适应任何摘要问题。</p><p id="3599" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我希望您尝试一下代码，并训练自己的模型。请在下面的评论中分享你的经历。</p><p id="9128" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在<a class="ae ky" href="https://deeplearninganalytics.org/" rel="noopener ugc nofollow" target="_blank">深度学习分析</a>，我们非常热衷于使用机器学习来解决现实世界的问题。我们已经帮助许多企业部署了创新的基于人工智能的解决方案。如果您看到合作的机会，请通过我们的网站<a class="ae ky" href="https://deeplearninganalytics.org/contact-us/" rel="noopener ugc nofollow" target="_blank">这里</a>联系我们。</p></div></div>    
</body>
</html>