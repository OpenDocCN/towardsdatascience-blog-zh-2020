<html>
<head>
<title>PySpark and SparkSQL Basics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark和SparkSQL基础知识</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53?source=collection_archive---------0-----------------------#2020-01-10">https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53?source=collection_archive---------0-----------------------#2020-01-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="721d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">使用PySpark实施大数据</h2><div class=""/><div class=""><h2 id="0402" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">如何用Python编程实现Spark</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/c266c8a7481f180cb461b0210b57cd1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HFmaIJiZ7BehRryMgLDvxQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://pixabay.com/illustrations/analytics-information-innovation-3088958/" rel="noopener ugc nofollow" target="_blank"> <strong class="bd lf">【来源】</strong> </a></p></figure><p id="bc51" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">Python通过Spark Python API揭示了Spark编程模型来处理结构化数据，该API被称为<a class="ae le" href="https://spark.apache.org/docs/0.9.1/api/pyspark/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja"> PySpark </strong> </a>。</p><p id="1be2" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这篇文章的目的是演示如何用PySpark运行Spark并执行常见的函数。</p><p id="d854" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">Python编程语言需要安装IDE。使用Python和Anaconda的最简单的方法，因为它安装了足够的IDE和重要的包。</p><h1 id="1ad7" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">1.下载Anaconda并安装PySpark</h1><p id="a2bd" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">借助这个<a class="ae le" href="https://www.anaconda.com/distribution/" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja">链接</strong> </a>，可以下载Anaconda。下载合适的Anaconda版本后，点击它继续安装程序，安装程序在<a class="ae le" href="https://docs.anaconda.com/anaconda/install/" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja"> Anaconda文档</strong> </a>中有详细说明。</p><p id="314b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">安装完成后，将会打开Anaconda Navigator主页。要使用Python，只需点击“笔记本”模块的“启动”按钮。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mz"><img src="../Images/846e008ccc9a298278544efa34ca3df6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRFG5QRJcud_trRCsKGJOQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Anaconda Navigator主页(图片由作者提供)</p></figure><p id="6085" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">为了能够通过Anaconda使用Spark，应该遵循下面的包安装步骤。</p><blockquote class="na nb nc"><p id="1442" class="lg lh nd li b lj lk ka ll lm ln kd lo ne lq lr ls nf lu lv lw ng ly lz ma mb ij bi translated">Anaconda提示终端</p><p id="81b7" class="lg lh nd li b lj lk ka ll lm ln kd lo ne lq lr ls nf lu lv lw ng ly lz ma mb ij bi translated">康达安装pyspark</p><p id="c783" class="lg lh nd li b lj lk ka ll lm ln kd lo ne lq lr ls nf lu lv lw ng ly lz ma mb ij bi translated">康达安装<a class="ae le" href="https://pypi.org/project/pyarrow/" rel="noopener ugc nofollow" target="_blank"> pyarrow </a></p></blockquote><p id="4d92" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">PySpark和PyArrow包安装完成后，只需关闭终端，返回Jupyter Notebook，在代码顶部导入所需的包。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="ebdb" class="nm md iq ni b gy nn no l np nq">import pandas as pd<br/>from pyspark.sql import SparkSession<br/>from pyspark.context import SparkContext<br/>from pyspark.sql.functions <br/>import *from pyspark.sql.types <br/>import *from datetime import date, timedelta, datetime<br/>import time</span></pre><h1 id="748d" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">2.正在初始化SparkSession</h1><p id="42c3" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">首先，需要初始化一个Spark会话。在SparkSession的帮助下，可以创建DataFrame并将其注册为表。而且执行SQL表，可以缓存表，可以读取parquet/JSON/CSV/Avro数据格式的文件。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="42ce" class="nm md iq ni b gy nn no l np nq">sc = SparkSession.builder.appName("PysparkExample")\    <br/>.config ("spark.sql.shuffle.partitions", "50")\    .config("spark.driver.maxResultSize","5g")\    <br/>.config ("spark.sql.execution.arrow.enabled", "true")\    .getOrCreate()</span></pre><p id="f131" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">关于SparkSession各个参数的详细解释，敬请访问<a class="ae le" href="https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.SparkSession" rel="noopener ugc nofollow" target="_blank">py spark . SQL . spark session</a>。</p><h1 id="a196" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">3.创建数据框</h1><p id="66f9" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">数据帧可以被接受为标题列的分布式列表集合，类似于关系数据库中的表。在本文中，我们将在PySpark API上使用DataFrame操作来处理数据集。</p><p id="8615" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">你可以从<a class="ae le" href="https://www.kaggle.com/cmenca/new-york-times-hardcover-fiction-best-sellers" rel="noopener ugc nofollow" target="_blank">这个链接</a>下载Kaggle数据集。</p><p id="2418" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">3.1<em class="nd">。来自星火数据来源</em> </strong></p><p id="f9ad" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">可以通过读取文本、CSV、JSON和拼花文件格式来创建数据帧。在我们的例子中，我们将使用一个. json格式的文件。您还可以通过使用如下所示的相关读取函数来查找和读取文本、CSV和拼花文件格式。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="28f5" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja">#Creates a spark data frame called as raw_data.</strong></span><span id="48da" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja">#JSON</strong><br/>dataframe = sc.read.json('dataset/nyt2.json')</span><span id="3f78" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja">#TXT FILES# </strong><br/>dataframe_txt = sc.read.text('text_data.txt')</span><span id="d2a8" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja">#CSV FILES# </strong><br/>dataframe_csv = sc.read.csv('csv_data.csv')</span><span id="539d" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja">#PARQUET FILES# </strong><br/>dataframe_parquet = sc.read.load('parquet_data.parquet')</span></pre><h1 id="d556" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">4.重复值</h1><p id="0ac2" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">使用dropDuplicates()函数可以消除表中的重复值。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="c1f2" class="nm md iq ni b gy nn no l np nq">dataframe = sc.read.json('dataset/nyt2.json') <br/>dataframe.show(10)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/a35dbe760b2fe2401dcb37d0af9453f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9SSJp3h_H9vyiLrAia79ow.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(图片由作者提供)</p></figure><p id="aa93" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在应用dropDuplicates()函数之后，我们可以观察到重复项被从数据集中删除了。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="1a48" class="nm md iq ni b gy nn no l np nq">dataframe_dropdup = dataframe.dropDuplicates() dataframe_dropdup.show(10)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/62eb346380013146c3a1106f3cce1535.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-alcYFLxw20bP7DnIOFEg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="a3da" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">5.问题</h1><p id="d7d9" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">查询操作可以用于各种目的，例如用<em class="nd"> "select" </em>子集化列，用<em class="nd"> "when" </em>添加条件，用<em class="nd"> "like "过滤列内容。</em>下面举例说明一些最常用的操作。有关查询操作的完整列表，请参见<a class="ae le" href="https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja"> Apache Spark文档</strong> </a>。</p><p id="197b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">5.1<em class="nd">。【选择】操作</em> </strong></p><p id="e7a9" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">可以通过属性(“作者”)或索引(dataframe['author'])来获取列。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="868a" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja">#Show all entries in title column</strong><br/>dataframe.select("author").show(10)</span><span id="4829" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja">#Show all entries in title, author, rank, price columns</strong><br/>dataframe.select("author", "title", "rank", "price").show(10)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/2a495d0be2a5c003dd258d86ff680108.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*-O05JNTy0F2MotorM2D1tg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">第一个结果表只显示“作者”选择，第二个结果表显示多列(作者的图像)</p></figure><p id="c143" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> <em class="nd"> 5.2。【当】操作</em> </strong></p><p id="bafb" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在第一个示例中，选择了“title”列，并添加了一个带有“when”条件的条件。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="f728" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Show title and assign 0 or 1 depending on title</strong></span><span id="c337" class="nm md iq ni b gy nr no l np nq">dataframe.select("title",when(dataframe.title != 'ODD HOURS', <br/>1).otherwise(0)).show(10)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/e5d0d8a68ca52648cd50ccfca429d2dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Wh-afg6mOh6Ttb_hw4vm1w.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">显示10行指定条件(图片由作者提供)</p></figure><p id="f23b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在第二个示例中，应用了“isin”操作，而不是“when ”,后者也可用于为行定义一些条件。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="f700" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Show rows with specified authors if in the given options</strong></span><span id="1684" class="nm md iq ni b gy nr no l np nq">dataframe [dataframe.author.isin("John Sandford", <br/>"Emily Giffin")].show(5)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/1c8969176a99910e73ae503334fb8ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TBFwFumnuN-hpa8Av7BbsQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">结果集显示5行指定的条件(图片由作者提供)</p></figure><p id="86ab" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">5.3<em class="nd">。【喜欢】操作</em> </strong></p><p id="8337" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在“Like”函数的括号中,%字符用于过滤掉所有包含“the”单词的标题。如果我们正在寻找的条件是完全匹配的，那么不应该使用%字符。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="c389" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Show author and title is TRUE if title has " THE " word in titles</strong></span><span id="3263" class="nm md iq ni b gy nr no l np nq">dataframe.select("author", "title",<br/>dataframe.title.like("% THE %")).show(15)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/502cbc210794516bcfff55923d16f846.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*sfeHyss4REhNeJev0PVcwg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">包含“THE”单词的标题的结果集。(图片由作者提供)</p></figure><p id="3ae9" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">5.4<em class="nd">。</em> </strong></p><p id="7937" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">从单词/内容的开头开始扫描，并在括号中指定标准。并行地，EndsWith从末尾开始处理单词/内容。这两个函数都区分大小写。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="b291" class="nm md iq ni b gy nn no l np nq">dataframe.select("author", "title", dataframe.title.startswith("THE")).show(5)</span><span id="60e8" class="nm md iq ni b gy nr no l np nq">dataframe.select("author", "title", dataframe.title.endswith("NT")).show(5)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/12623b1887ca6cf2cc152b79e1e31a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*OKC6LV6w7O0ezqyrie_-4Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">结果集有5行startsWith和endsWith操作。(图片由作者提供)</p></figure><p id="4bf6" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> <em class="nd"> 5.5。【子串】操作</em> </strong></p><p id="a302" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">Substring函数提取指定索引之间的文本。在以下示例中，从索引号(1，3)、(3，6)和(1，6)中提取文本。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="2215" class="nm md iq ni b gy nn no l np nq">dataframe.select(dataframe.author.substr(1, 3).alias("title")).show(5)</span><span id="38a8" class="nm md iq ni b gy nr no l np nq">dataframe.select(dataframe.author.substr(3, 6).alias("title")).show(5)</span><span id="6393" class="nm md iq ni b gy nr no l np nq">dataframe.select(dataframe.author.substr(1, 6).alias("title")).show(5)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/532ed04897ab4b4f0ba4e190d7a728f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*FlskqEygPq2b5RZigDjW5Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">分别显示子字符串(1，3)、(3，6)、(1，6)的结果。(图片由作者提供)</p></figure><h1 id="622c" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">6.添加、更新和删除列</h1><p id="51c6" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">DataFrame API中也提供了数据操作函数。下面，您可以找到添加/更新/删除列操作的示例。</p><p id="0d35" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> <em class="nd"> 6.1。添加</em>列</strong></p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="8f41" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Lit() is required while we are creating columns with exact values.</strong></span><span id="7817" class="nm md iq ni b gy nr no l np nq">dataframe = dataframe.withColumn('new_column', <br/>F.lit('This is a new column'))</span><span id="6e6c" class="nm md iq ni b gy nr no l np nq">display(dataframe)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/2cb466d80616e635189364ee2688a930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D7CKydP0JAhDXcFFDruTPQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">新列被添加到数据集的末尾(图片由作者提供)</p></figure><p id="db0c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">6.2<em class="nd">。更新列</em> </strong></p><p id="6972" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">对于DataFrame API的更新操作，withColumnRenamed()函数使用两个参数。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="867b" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Update column 'amazon_product_url' with 'URL'</strong></span><span id="462e" class="nm md iq ni b gy nr no l np nq">dataframe = dataframe.withColumnRenamed('amazon_product_url', 'URL')</span><span id="c49f" class="nm md iq ni b gy nr no l np nq">dataframe.show(5)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nz"><img src="../Images/6b91898a8168f10ec4ab1d4f07380b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lHjHr1CIuVm1kfIqBNxBmw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">“Amazon_Product_URL”列名更新为“URL”(图片由作者提供)</p></figure><p id="5806" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">6.3<em class="nd">。移除列</em> </strong></p><p id="67f7" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">可以通过两种方式删除列:在drop()函数中添加列名列表，或者通过在drop函数中指向来指定列。两个示例如下所示。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="0490" class="nm md iq ni b gy nn no l np nq">dataframe_remove = dataframe.drop("publisher", "published_date").show(5)</span><span id="ace7" class="nm md iq ni b gy nr no l np nq">dataframe_remove2 = dataframe \ .drop(dataframe.publisher).drop(dataframe.published_date).show(5)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/677cbbc6c72084db04743afd6c126f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wR0rf9AR1Qlbj8fbDboixA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">“publisher”和“published_date”列以两种不同的方法删除。(图片由作者提供)</p></figure><h1 id="b262" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">7.检查数据</h1><p id="a2e3" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">存在几种类型的函数来检查数据。下面，你可以找到一些常用的。要深入了解，请访问<a class="ae le" href="https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja"> Apache Spark文档</strong> </a>。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="60fc" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Returns dataframe column names and data types</strong><br/>dataframe.dtypes</span><span id="ccfe" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Displays the content of dataframe</strong><br/>dataframe.show()</span><span id="2d5f" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Return first n rows</strong><br/>dataframe.head()</span><span id="3760" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Returns first row</strong><br/>dataframe.first()</span><span id="7dd3" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Return first n rows</strong><br/>dataframe.take(5)</span><span id="9f8d" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Computes summary statistics</strong><br/>dataframe.describe().show()</span><span id="b6f3" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Returns columns of dataframe</strong><br/>dataframe.columns</span><span id="3793" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Counts the number of rows in dataframe</strong><br/>dataframe.count()</span><span id="bd67" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Counts the number of distinct rows in dataframe</strong><br/>dataframe.distinct().count()</span><span id="5825" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Prints plans including physical and logical</strong><br/>dataframe.explain(4)</span></pre><h1 id="d6c1" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">8.“分组”操作</h1><p id="e9b1" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">通过在函数中添加列名，使用GroupBy()函数应用分组过程。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="17cf" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Group by author, count the books of the authors in the groups</strong></span><span id="b3ab" class="nm md iq ni b gy nr no l np nq">dataframe.groupBy("author").count().show(10)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/433c18299bbebe2653165aecc0ac7dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*sDgCgaeSpQIYsXwua03VYA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者按出版的书籍数量分组(图片由作者提供)</p></figure><h1 id="d9ca" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">9.“过滤”操作</h1><p id="1e1c" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">通过使用filter()函数并在其中添加一个条件参数来应用过滤。该函数区分大小写。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="a811" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Filtering entries of title<br/># Only keeps records having value 'THE HOST'</strong></span><span id="b200" class="nm md iq ni b gy nr no l np nq">dataframe.filter(dataframe["title"] == 'THE HOST').show(5)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/54bbf057be0793c61d3bae1fc476349f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LAsWvTaGfoO3eWoL1o4k8Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">标题列被过滤，内容只有“主机”并显示5个结果。(图片由作者提供)</p></figure><h1 id="3c77" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">10.缺少和替换值</h1><p id="8859" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">对于每个数据集，在数据预处理阶段总是需要替换现有值、删除不必要的列和填充缺失值。<a class="ae le" href="http://pyspark.sql.DataFrameNaFunction" rel="noopener ugc nofollow" target="_blank"><strong class="li ja">py spark . SQL . dataframenafunction</strong></a>库帮助我们在这方面操作数据。下面补充一些例子。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="a61f" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Replacing null values</strong><br/>dataframe.na.fill()<br/>dataFrame.fillna()<br/>dataFrameNaFunctions.fill()</span><span id="3b6b" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Returning new dataframe restricting rows with null </strong>valuesdataframe.na.drop()<br/>dataFrame.dropna()<br/>dataFrameNaFunctions.drop()</span><span id="e081" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Return new dataframe replacing one value with another</strong><br/>dataframe.na.replace(5, 15)<br/>dataFrame.replace()<br/>dataFrameNaFunctions.replace()</span></pre><h1 id="c637" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">11.分配</h1><p id="683f" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">可以增加或减少RDD中现有的分区级别。增加可以通过使用<em class="nd"> repartition(self，numPartitions) </em>函数来实现，该函数会产生一个获得更多分区数量的新RDD。减少可以用<em class="nd"> coalesce(self，numPartitions，shuffle=False) </em>函数处理，该函数产生一个新的RDD，其分区数量减少到指定的数量。更多信息请访问<strong class="li ja"/><a class="ae le" href="https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html" rel="noopener ugc nofollow" target="_blank"><strong class="li ja">Apache Spark docs</strong></a>。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="da66" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Dataframe with 10 partitions</strong><br/>dataframe.repartition(10).rdd.getNumPartitions()</span><span id="7896" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Dataframe with 1 partition</strong><br/>dataframe.coalesce(1).rdd.getNumPartitions()</span></pre><h1 id="4d86" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">12.以编程方式运行SQL查询</h1><p id="0f2f" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">还可以通过在SparkSession上启用“sql”操作来使用原始SQL查询，从而以编程方式运行SQL查询，并将结果集作为DataFrame结构返回。更多详细信息，敬请访问<a class="ae le" href="https://spark.apache.org/docs/2.2.0/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank"><strong class="li ja">Apache Spark docs</strong></a>。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="dbd1" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Registering a table</strong><br/>dataframe.registerTempTable("df")</span><span id="e2db" class="nm md iq ni b gy nr no l np nq">sc.sql("select * from df").show(3)</span><span id="70d4" class="nm md iq ni b gy nr no l np nq">sc.sql("select \               <br/>CASE WHEN description LIKE '%love%' THEN 'Love_Theme' \               WHEN description LIKE '%hate%' THEN 'Hate_Theme' \               WHEN description LIKE '%happy%' THEN 'Happiness_Theme' \               WHEN description LIKE '%anger%' THEN 'Anger_Theme' \               WHEN description LIKE '%horror%' THEN 'Horror_Theme' \               WHEN description LIKE '%death%' THEN 'Criminal_Theme' \               WHEN description LIKE '%detective%' THEN 'Mystery_Theme' \               ELSE 'Other_Themes' \               END Themes \       <br/>from df").groupBy('Themes').count().show()</span></pre><h1 id="2875" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">13.输出</h1><p id="6dea" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated"><strong class="li ja">13.1<em class="nd">。数据结构</em> </strong></p><p id="3c67" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">DataFrame API使用RDD作为基础，它将SQL查询转换为低级RDD函数。通过使用。rdd操作，一个数据帧可以转换成RDD。也可以将Spark Dataframe转换成一串RDD和熊猫格式。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="ddd9" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Converting dataframe into an RDD</strong><br/>rdd_convert = dataframe.rdd</span><span id="e06e" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Converting dataframe into a RDD of string </strong>dataframe.toJSON().first()</span><span id="7f6a" class="nm md iq ni b gy nr no l np nq"><strong class="ni ja"># Obtaining contents of df as Pandas</strong> <br/>dataFramedataframe.toPandas()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/dbbb931f48240ca5058793fa72d9db0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EQwkZbw7pZZLoOuuEW_6rA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">不同数据结构的结果(图片由作者提供)</p></figure><p id="340a" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">13.2<em class="nd">。写&amp;保存到</em>文件</strong></p><p id="79f4" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">作为数据帧加载到我们代码中的任何数据源类型都可以很容易地转换并保存为其他类型，包括。parquet和. json .更多保存、加载、写函数细节请访问<a class="ae le" href="https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html" rel="noopener ugc nofollow" target="_blank"><strong class="li ja">Apache Spark doc</strong></a>。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="9d87" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Write &amp; Save File in .parquet format</strong><br/>dataframe.select("author", "title", "rank", "description") \<br/>.write \<br/>.save("Rankings_Descriptions.parquet")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/3ba162903176642be236b8f1dd65a0a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*q1JyYTmJXhHK4ZmrIVEQwg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">拼花文件是在。写吧。save()函数被处理。(图片由作者提供)</p></figure><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="0a53" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># Write &amp; Save File in .json format</strong><br/>dataframe.select("author", "title") \<br/>.write \<br/>.save("Authors_Titles.json",format="json")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/1a349b43dd06865c838528edb35a9fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*g07arMM0ibE0pvx3lDZvKA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">JSON文件是在。写吧。save()函数被处理。(图片由作者提供)</p></figure><p id="8356" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">13.3<em class="nd">。停止火花会话</em>T29】</strong></p><p id="a5dd" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">可以通过运行<em class="nd"> stop() </em>功能来停止Spark会话，如下所示。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="1270" class="nm md iq ni b gy nn no l np nq"><strong class="ni ja"># End Spark Session</strong><br/>sc.stop()</span></pre><p id="1b56" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">代码和Jupyter笔记本都在我的<a class="ae le" href="https://github.com/pinarersoy/PySpark_SparkSQL_MLib" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja"> GitHub上。</strong> </a></p><p id="cceb" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">非常感谢您的提问和评论！</p></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><p id="2e74" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">参考文献:</strong></p><ol class=""><li id="c359" class="on oo iq li b lj lk lm ln lp op lt oq lx or mb os ot ou ov bi translated"><a class="ae le" href="http://spark.apache.org/docs/latest/" rel="noopener ugc nofollow" target="_blank">http://spark.apache.org/docs/latest/</a></li><li id="2cec" class="on oo iq li b lj ow lm ox lp oy lt oz lx pa mb os ot ou ov bi translated"><a class="ae le" href="https://docs.anaconda.com/anaconda/" rel="noopener ugc nofollow" target="_blank">https://docs.anaconda.com/anaconda/</a></li></ol></div></div>    
</body>
</html>