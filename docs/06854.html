<html>
<head>
<title>Top 10 Binary Classification Algorithms [a Beginner’s Guide]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">十大二元分类算法[初学者指南]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/top-10-binary-classification-algorithms-a-beginners-guide-feeacbd7a3e2?source=collection_archive---------5-----------------------#2020-05-28">https://towardsdatascience.com/top-10-binary-classification-algorithms-a-beginners-guide-feeacbd7a3e2?source=collection_archive---------5-----------------------#2020-05-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d08d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何用几行Python代码实现10个最重要的二进制分类算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a0b1653ced2d79c17a226b1b2a56be76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*C2Pz_KAS6qE3B8IA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Javier Allegue Barros 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="d415" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="400d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi mn translated"><span class="l mo mp mq bm mr ms mt mu mv di"> B </span>一元分类问题可以通过从朴素贝叶斯到深度学习网络的各种机器学习算法来解决。哪种解决方案在运行时间和准确性方面表现最佳取决于数据量(样本和要素的数量)和数据质量(异常值、不平衡数据)。</p><p id="4de5" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">本文提供了一个概述和代码示例，您可以很容易地自己尝试。目标是快速获得Python的第一个工作结果。在这里，我将保持事情简短，并将详细解释每个算法。我已经添加了对每个算法的参考，以防你想了解更多关于算法，它的基本理论和如何调整它的参数。</p><p id="b551" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">在本文中，我们将重点介绍10种最常见的二进制分类算法:</p><ol class=""><li id="bfcd" class="nb nc it lt b lu mw lx mx ma nd me ne mi nf mm ng nh ni nj bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯</a></li><li id="808f" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/understanding-logistic-regression-9b02c2aec102">逻辑回归</a></li><li id="fa27" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761">K-最近邻</a></li><li id="6291" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47">支持向量机</a></li><li id="aa94" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/decision-trees-in-machine-learning-641b9c4e8052">决策树</a></li><li id="6e39" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><a class="ae ky" href="https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f" rel="noopener"> Bagging决策树(集成学习I) </a></li><li id="fbf2" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><a class="ae ky" href="https://medium.com/ml-research-lab/boosting-ensemble-meta-algorithm-for-reducing-bias-5b8bfdce281" rel="noopener">增强决策树(集成学习II) </a></li><li id="993f" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><a class="ae ky" href="https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920" rel="noopener">随机森林(集成学习III) </a></li><li id="fe86" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html" rel="noopener ugc nofollow" target="_blank">投票分类(集成学习IV) </a></li><li id="23b1" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><a class="ae ky" href="https://medium.com/botsupply/a-beginners-guide-to-deep-learning-5ee814cf7706" rel="noopener">神经网络(深度学习)</a></li></ol><p id="8a01" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">为了让事情尽可能简单，我们在本教程中将只使用三个Python库:<a class="ae ky" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> <em class="np"> Numpy </em> </a>、<a class="ae ky" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> <em class="np"> Sklearn </em> </a>和<a class="ae ky" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> <em class="np"> Keras </em> </a>。</p><p id="d994" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">在代码示例中，我总是在代码片段的顶部导入必要的Python模块，以明确接下来会用到它。你可以在你的脚本开始的时候加载它们。</p><p id="d06f" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">我创建了一个Jupyter笔记本，里面有你能在我的Github资源库中找到的所有代码:<a class="ae ky" href="https://github.com/alexortner/teaching/tree/master/binary_classification" rel="noopener ugc nofollow" target="_blank">https://Github . com/alexortner/teaching/tree/master/binary _ classification</a></p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="815b" class="kz la it bd lb lc nx le lf lg ny li lj jz nz ka ll kc oa kd ln kf ob kg lp lq bi translated">资料组</h1><p id="08d0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我们将对电影评论进行二元情感分析，这是自然语言处理中的一个常见问题。</p><p id="adca" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">我们使用高度极性电影评论的<a class="ae ky" href="https://keras.io/api/datasets/imdb/" rel="noopener ugc nofollow" target="_blank"> IMDB数据集</a>，以对不同电影的文本评论和正面或负面评分的形式。</p><p id="e8c9" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">目标是通过分析评论文本中的情感来训练可以预测评论的模型。</p><h2 id="2b4c" class="oc la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">数据导入</h2><p id="090e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们直接从<em class="np"> Keras </em>加载数据集。这样做的好处是，我们不需要下载文件，也几乎不需要在代码中准备数据。Keras网站的描述称:</p><blockquote class="oo op oq"><p id="b06f" class="lr ls np lt b lu mw ju lw lx mx jx lz or my mc md os mz mg mh ot na mk ml mm im bi translated">这是来自IMDB的25，000个电影评论的数据集，由情绪(正面/负面)标记。评论已经过预处理，每个评论都被编码成一个单词索引(整数)列表。为方便起见，单词按数据集中的总频率进行索引，例如，整数“3”编码数据中第三个最频繁出现的单词。这允许快速过滤操作，例如:“仅考虑前10，000个最常用的单词，但排除前20个最常用的单词”。</p><p id="a003" class="lr ls np lt b lu mw ju lw lx mx jx lz or my mc md os mz mg mh ot na mk ml mm im bi translated">按照惯例，“0”不代表特定的单词，而是用于编码任何未知的单词。</p><p id="77db" class="lr ls np lt b lu mw ju lw lx mx jx lz or my mc md os mz mg mh ot na mk ml mm im bi translated">(引自:<a class="ae ky" href="https://keras.io/api/datasets/imdb/" rel="noopener ugc nofollow" target="_blank">https://keras.io/api/datasets/imdb/</a>)</p></blockquote><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="b56f" class="oc la it ov b gy oz pa l pb pc">import keras.datasets as keras_data</span><span id="d6f8" class="oc la it ov b gy pd pa l pb pc">(imdb_train_data,imdb_train_labels),(imdb_test_data,imdb_test_labels) = keras_data.imdb.load_data(num_words=10000)</span></pre><p id="9a01" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><em class="np"> num_words </em>将文本导入限制为10，000个最常用的单词。</p><h2 id="c382" class="oc la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">数据的快速概览</h2><p id="f6f3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">训练和测试数据各包含25000个样本。来自<em class="np"> Keras </em>源的文本已经被标记化，这意味着它被编码成一个整数序列，其中每个数字对应一个单词。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="e8b4" class="oc la it ov b gy oz pa l pb pc">print(imdb_train_data.shape)<br/>print(imdb_test_data.shape)</span><span id="3c63" class="oc la it ov b gy pd pa l pb pc">print(imdb_train_data[5])</span><span id="2132" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu"># output<br/></strong>(25000,)<br/>(25000,)<br/>[1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 2, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 2, 10, 10]</span></pre><p id="e282" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">如果您想以明文形式查看评论，可以使用数据集中的<em class="np"> get_word_index </em>函数将索引映射回原始单词。请记住，标点和空格已经从数据集中移除，因此无法恢复。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="b81b" class="oc la it ov b gy oz pa l pb pc">word_index = keras_data.imdb.get_word_index()</span><span id="618e" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu"># map index to word mapping into a Python dict</strong><br/>reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])</span><span id="c9da" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu"># map the word index to one of the tokenized comments</strong><br/>decoded_word_index = ''.join([reverse_word_index.get(i-3,'?') for i in imdb_train_data[5]])</span></pre><p id="f946" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">打印单词索引列表中最常用的10个单词:</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="8b58" class="oc la it ov b gy oz pa l pb pc">for key in sorted(reverse_word_index.keys()):<br/>    if(key&lt;=10): <br/>        print("%s: %s" % (key, reverse_word_index[key]))</span><span id="d27a" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu"># output</strong><br/>1: the<br/>2: and<br/>3: a<br/>4: of<br/>5: to<br/>6: is<br/>7: br<br/>8: in<br/>9: it<br/>10: i</span></pre><p id="194e" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">打印解码的注释:</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="b83f" class="oc la it ov b gy oz pa l pb pc">print(decoded_word_index)</span><span id="c435" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu"># output</strong><br/>beginsbetterthanitendsfunnythattherussiansubmarinecrewoutperformsallotheractorsit'slikethosesceneswheredocumentaryshotsbrbrspoilerpartthemessagedechiferedwascontrarytothewholestoryitjustdoesnotmeshbrbr</span></pre><h2 id="5090" class="oc la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">数据矢量化和一键编码</h2><p id="860f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">目前，每个样本都是一个整数数组，长度取决于注释的文本长度。所使用的算法需要一个张量，其中每个样本具有相同数量的特征。为此，我们通过将每个数字替换为长度为10，000的向量来对整数列表进行一次性编码，该向量在索引位置的值为1，在所有其他位置的值为0。所以我们基本上是把整数序列编码成二进制矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/1242098ff43edd62968c8301e8a091c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*agWomje6HiNyuFpXsdQ5hA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文本的符号化和矢量化示意图(图片由作者提供)</p></figure><p id="95b2" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">对于向量化，我们写一个简单的函数</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="b9fb" class="oc la it ov b gy oz pa l pb pc">import numpy as np</span><span id="588c" class="oc la it ov b gy pd pa l pb pc">def vectorize_sequence(sequences,dimensions):<br/>    results=np.zeros((len(sequences),dimensions))<br/>    for i, sequence in enumerate(sequences):<br/>        results[i,sequence] = 1.<br/>    return results</span></pre><p id="2321" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">并用它将我们的测试和训练数据转换成张量</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="3829" class="oc la it ov b gy oz pa l pb pc">x_train=vectorize_sequence(imdb_train_data,10000)<br/>x_test=vectorize_sequence(imdb_test_data,10000)</span></pre><p id="7023" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">标签数据已经是一个二进制整数，我们只需将它转换成一个<em class="np"> Numpy </em>数组和浮点类型，因为<em class="np"> Keras </em>只接受这种数据类型</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="cfe1" class="oc la it ov b gy oz pa l pb pc">y_train=np.asarray(imdb_train_labels).astype('float32')<br/>y_test=np.asarray(imdb_test_labels).astype('float32')</span></pre><p id="3837" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">对张量形状的检查表明，对于训练和测试数据集，我们现在有25000个样本和10000个特征</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="873a" class="oc la it ov b gy oz pa l pb pc">print(x_train.shape)<br/>print(x_test.shape)</span><span id="305c" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu">#output</strong><br/>(25000, 10000)<br/>(25000, 10000)</span></pre></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="32a3" class="kz la it bd lb lc nx le lf lg ny li lj jz nz ka ll kc oa kd ln kf ob kg lp lq bi translated">1.朴素贝叶斯</h1><p id="8234" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">朴素贝叶斯方法是一种监督学习算法，它基于应用贝叶斯定理和给定类变量值的每对要素之间条件独立性的“朴素”假设。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="bca0" class="oc la it ov b gy oz pa l pb pc">from sklearn.naive_bayes import MultinomialNB</span><span id="25ae" class="oc la it ov b gy pd pa l pb pc">mnb = MultinomialNB().fit(x_train, y_train)</span><span id="9fdf" class="oc la it ov b gy pd pa l pb pc">print("score on test: " + str(mnb.score(x_test, y_test)))<br/>print("score on train: "+ str(mnb.score(x_train, y_train)))</span></pre><p id="f611" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><strong class="lt iu">总结:</strong>朴素贝叶斯算法对于这个特征丰富的数据集非常快(记住我们有一个具有10，000个特征向量的张量)，并且已经提供了80%以上的好结果。训练和测试数据上的得分彼此接近，这表明我们没有过度拟合。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="6c71" class="oc la it ov b gy oz pa l pb pc"><strong class="ov iu">Results Naive Bayes</strong><br/>+-------------+------+<br/>| <strong class="ov iu">run time</strong>    | 2s   |<br/>+-------------+------+<br/>| <strong class="ov iu">train score </strong>| 0.87 |<br/>+-------------+------+<br/>| <strong class="ov iu">test score </strong> | 0.84 |<br/>+-------------+------+</span></pre></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="fa4a" class="kz la it bd lb lc nx le lf lg ny li lj jz nz ka ll kc oa kd ln kf ob kg lp lq bi translated">2.逻辑回归</h1><p id="5b1a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/understanding-logistic-regression-9b02c2aec102">逻辑回归</a>是解决分类问题的最古老、最基本的算法之一:</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="07c2" class="oc la it ov b gy oz pa l pb pc">from sklearn.linear_model import LogisticRegression</span><span id="8812" class="oc la it ov b gy pd pa l pb pc">lr=LogisticRegression(max_iter=1000)<br/>lr.fit(x_train, y_train)</span><span id="ac15" class="oc la it ov b gy pd pa l pb pc">print("score on test: " + str(lr.score(x_test, y_test)))<br/>print("score on train: "+ str(lr.score(x_train, y_train)))</span></pre><p id="c8ff" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><strong class="lt iu">总结:</strong>逻辑回归需要相当长的训练时间，而且会过拟合。从训练数据得分(98%)与测试数据得分(86%)的偏差中可以看出算法过拟合。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="c50f" class="oc la it ov b gy oz pa l pb pc"><strong class="ov iu">Results Logistic Regression<br/></strong>+-------------+------+<br/>| <strong class="ov iu">run time</strong>    | 60s  |<br/>+-------------+------+<br/>| <strong class="ov iu">train score </strong>| 0.98 |<br/>+-------------+------+<br/>| <strong class="ov iu">test score </strong> | 0.86 |<br/>+-------------+------+</span></pre></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="6503" class="kz la it bd lb lc nx le lf lg ny li lj jz nz ka ll kc oa kd ln kf ob kg lp lq bi translated">3.k-最近邻</h1><p id="4031" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761">k-最近邻(KNN) </a>算法是一种监督机器学习算法，可用于解决分类和回归问题。对于KNN，众所周知，它不太适用于大型数据集(高样本量)，尤其是许多特征(高维)。我们具有25000个样本和10000个特征的数据集对于该算法来说已经不是最优的了。我必须设置两个参数，<em class="np"> algorithm = "brute "和n_jobs=-1 </em>来让分类器在最近运行。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="2333" class="oc la it ov b gy oz pa l pb pc">from sklearn.neighbors import KNeighborsClassifier</span><span id="ee6b" class="oc la it ov b gy pd pa l pb pc">knn = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)</span><span id="ba8d" class="oc la it ov b gy pd pa l pb pc">knn.fit(x_train, y_train)</span><span id="b0d8" class="oc la it ov b gy pd pa l pb pc">print("train shape: " + str(x_train.shape))<br/>print("score on test: " + str(knn.score(x_test, y_test)))<br/>print("score on train: "+ str(knn.score(x_train, y_train)))</span></pre><p id="ddfd" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><strong class="lt iu">总结:</strong>不出所料，这个算法不太适合这类预测问题。它需要12分钟，预测非常差，只有62%，并显示出过度拟合的趋势。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="e8e3" class="oc la it ov b gy oz pa l pb pc"><strong class="ov iu">Results K-Nearest Neighbours<br/></strong>+-------------+------+<br/>| <strong class="ov iu">run time</strong>    | 12m  |<br/>+-------------+------+<br/>| <strong class="ov iu">train score </strong>| 0.79 |<br/>+-------------+------+<br/>| <strong class="ov iu">test score </strong> | 0.62 |<br/>+-------------+------+</span></pre></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="8197" class="kz la it bd lb lc nx le lf lg ny li lj jz nz ka ll kc oa kd ln kf ob kg lp lq bi translated">4.支持向量机</h1><p id="7840" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47">支持向量机</a>是用于分类和回归任务的简单算法。它能以较小的计算能力非常快地提供高精度。由于大量的特性，我们使用<em class="np"> LinearSVC。</em>结果表明，设置正则化参数<em class="np"> C=0.0001 </em>提高了预测质量，减少了过拟合。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="b410" class="oc la it ov b gy oz pa l pb pc">from sklearn.svm import LinearSVC</span><span id="0235" class="oc la it ov b gy pd pa l pb pc">svm=LinearSVC(C=0.0001)<br/>svm.fit(x_train, y_train)</span><span id="73fc" class="oc la it ov b gy pd pa l pb pc">print("score on test: " + str(svm.score(x_test, y_test)))<br/>print("score on train: "+ str(svm.score(x_train, y_train)))</span></pre><p id="f6e5" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><strong class="lt iu">总结:</strong>支持向量机速度非常快，预测得分高，没有过拟合问题。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="3abf" class="oc la it ov b gy oz pa l pb pc"><strong class="ov iu">Results Support Vector Machine<br/></strong>+-------------+------+<br/>| <strong class="ov iu">run time</strong>    | 2s   |<br/>+-------------+------+<br/>| <strong class="ov iu">train score </strong>| 0.86 |<br/>+-------------+------+<br/>| <strong class="ov iu">test score </strong> | 0.85 |<br/>+-------------+------+</span></pre></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="b899" class="kz la it bd lb lc nx le lf lg ny li lj jz nz ka ll kc oa kd ln kf ob kg lp lq bi translated">5.决策图表</h1><p id="4413" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/decision-trees-in-machine-learning-641b9c4e8052">决策树</a>是一种用于分类和回归的非参数监督学习方法。目标是通过学习从数据特征推断的简单决策规则(if-else)来创建预测目标变量的值的模型。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="4a4b" class="oc la it ov b gy oz pa l pb pc">from sklearn.tree import DecisionTreeClassifier</span><span id="74d3" class="oc la it ov b gy pd pa l pb pc">clf = DecisionTreeClassifier()<br/>clf.fit(x_train, y_train)</span><span id="f8c2" class="oc la it ov b gy pd pa l pb pc">print("score on test: "  + str(clf.score(x_test, y_test)))<br/>print("score on train: " + str(clf.score(x_train, y_train)))</span></pre><p id="bd80" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><strong class="lt iu">总结:</strong>对这个特征丰富的数据集应用单个决策树会导致大量的过度拟合。事实上，100%的准确率意味着它准确地记住了训练数据集，因此对测试数据的概括很差。我们在这里看到的是单一决策树的缺点之一，它不能处理太多特征的数据。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="35de" class="oc la it ov b gy oz pa l pb pc"><strong class="ov iu">Results Decision Tree<br/></strong>+-------------+------+<br/>| <strong class="ov iu">run time</strong>    | 4m   |<br/>+-------------+------+<br/>| <strong class="ov iu">train score </strong>| 1.0  |<br/>+-------------+------+<br/>| <strong class="ov iu">test score </strong> | 0.70 |<br/>+-------------+------+</span></pre><p id="1516" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">然而，这个问题可以通过调整算法的一些参数或引入集成学习技术来解决。在这里可以找到关于决策树分类器参数调优的深度文章:<a class="ae ky" href="https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3" rel="noopener">https://medium . com/@ mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118 a03c 3</a>。接下来，我们将关注一些常见的集成学习方法。</p><h1 id="ee92" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">6.Bagging决策树(集成学习I)</h1><p id="7204" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当决策树过度拟合时，应用像bagging 这样的<a class="ae ky" href="https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f" rel="noopener">集成学习算法可能会提高预测模型的质量。在打包中，通过从训练数据中提取引导来增加训练数据。这意味着从训练数据中获取多个样本(替换)，并在这些子数据集上训练模型。最终预测是每个引导样本所有预测的平均值。</a></p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="60f5" class="oc la it ov b gy oz pa l pb pc">from sklearn.ensemble import BaggingClassifier<br/>from sklearn.tree import DecisionTreeClassifier</span><span id="3a46" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu"># max_samples: </strong>maximum size 0.5=50% of each sample taken from the full dataset</span><span id="811f" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu"># max_features:</strong> maximum of features 1=100% taken here all 10K <br/><strong class="ov iu"># n_estimators:</strong> number of decision trees </span><span id="5490" class="oc la it ov b gy pd pa l pb pc">bg=BaggingClassifier(DecisionTreeClassifier(),max_samples=0.5,max_features=1.0,n_estimators=10)<br/>bg.fit(x_train, y_train)</span><span id="2737" class="oc la it ov b gy pd pa l pb pc">print("score on test: " + str(bg.score(x_test, y_test)))<br/>print("score on train: "+ str(bg.score(x_train, y_train)))</span></pre><p id="17cc" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><strong class="lt iu">总结:</strong>Bagging分类器要慢得多，因为它基本上运行10个决策树，但人们可以看到我们在单个决策树上看到的过度拟合的减少和测试分数的增加。调整参数以进一步改善这个结果。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="57de" class="oc la it ov b gy oz pa l pb pc"><strong class="ov iu">Results Bagging Decision Tree<br/></strong>+-------------+------+<br/>| <strong class="ov iu">run time</strong>    | 10m  |<br/>+-------------+------+<br/>| <strong class="ov iu">train score </strong>| 0.93 |<br/>+-------------+------+<br/>| <strong class="ov iu">test score </strong> | 0.77 |<br/>+-------------+------+</span></pre></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="b8b3" class="kz la it bd lb lc nx le lf lg ny li lj jz nz ka ll kc oa kd ln kf ob kg lp lq bi translated">7.推进决策树(集成学习II)</h1><p id="2b5e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一般来说，我们不能用<a class="ae ky" href="https://medium.com/ml-research-lab/boosting-ensemble-meta-algorithm-for-reducing-bias-5b8bfdce281" rel="noopener">提升</a>来改善一个<em class="np">分值= 1 </em>的完全过拟合的模型。要应用Boosting，我们首先必须稍微调整一下决策树分类器。我花了一些试验和错误，直到我得到了决策树和AdaBoost分类器的最佳参数。我确信你可以通过更多的比赛进一步提高。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="67d2" class="oc la it ov b gy oz pa l pb pc">from sklearn.ensemble import AdaBoostClassifier<br/>from sklearn.tree import DecisionTreeClassifier</span><span id="7a07" class="oc la it ov b gy pd pa l pb pc">adb = AdaBoostClassifier(DecisionTreeClassifier(min_samples_split=10,max_depth=4),n_estimators=10,learning_rate=0.6)<br/>adb.fit(x_train, y_train)</span><span id="e0ed" class="oc la it ov b gy pd pa l pb pc">print("score on test: " + str(adb.score(x_test, y_test)))<br/>print("score on train: "+ str(adb.score(x_train, y_train)))</span></pre><p id="fa37" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><strong class="lt iu">总结:</strong>通过Boosting和改进的底层决策树，我们摆脱了过拟合问题。尽管我们只能以大约80%的把握进行预测，但我们可以对所有的测试数据进行可靠的预测。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="eed0" class="oc la it ov b gy oz pa l pb pc"><strong class="ov iu">Results Boosting Decision Tree<br/></strong>+-------------+------+<br/>| <strong class="ov iu">run time</strong>    | 5m   |<br/>+-------------+------+<br/>| <strong class="ov iu">train score </strong>| 0.80 |<br/>+-------------+------+<br/>| <strong class="ov iu">test score </strong> | 0.78 |<br/>+-------------+------+</span></pre><h1 id="9cf9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">8.随机森林(集成学习III)</h1><p id="8600" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920" rel="noopener">随机森林算法</a>是另一种常用的集成学习分类器，它使用多个决策树。随机森林分类器基本上是决策树的改进的bagging算法，它不同地选择子集。我发现<em class="np"> max_depth=9 </em>对于这个特性丰富的数据集来说是一个很好的值。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="5cc7" class="oc la it ov b gy oz pa l pb pc">from sklearn.ensemble import RandomForestClassifier</span><span id="d3d8" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu"># n_estimators = number of decision trees<br/></strong>rf = RandomForestClassifier(n_estimators=30, max_depth=9)<br/>rf.fit(x_train, y_train)</span><span id="0ae4" class="oc la it ov b gy pd pa l pb pc">print("score on test: " + str(rf.score(x_test, y_test)))<br/>print("score on train: "+ str(rf.score(x_train, y_train)))</span></pre><p id="23f1" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><strong class="lt iu">总结:</strong>随机森林不会过拟合，其预测分数与增强决策树的预测分数相当，但它的性能要好得多，因为它比增强决策树快一个数量级以上(快15倍)。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="acb5" class="oc la it ov b gy oz pa l pb pc"><strong class="ov iu">Results Random Forest<br/></strong>+-------------+------+<br/>| <strong class="ov iu">run time</strong>    | 20s  |<br/>+-------------+------+<br/>| <strong class="ov iu">train score </strong>| 0.83 |<br/>+-------------+------+<br/>| <strong class="ov iu">test score </strong> | 0.80 |<br/>+-------------+------+</span></pre><h1 id="54f0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">9.投票分类器(集成学习IV)</h1><p id="c306" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这个来自集成学习工具箱的分类器评估不同的分类器并从中选择最好的</p><blockquote class="oo op oq"><p id="77e6" class="lr ls np lt b lu mw ju lw lx mx jx lz or my mc md os mz mg mh ot na mk ml mm im bi translated"><code class="fe pf pg ph ov b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">VotingClassifier</strong></a></code>背后的想法是结合概念上不同的机器学习分类器，并使用多数投票或平均预测概率来预测类别标签。这种分类器可以用于一组同样表现良好的模型，以平衡它们各自的弱点。</p><p id="116d" class="lr ls np lt b lu mw ju lw lx mx jx lz or my mc md os mz mg mh ot na mk ml mm im bi translated">(引用自:<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . ensemble . voting classifier . html</a>)</p></blockquote><p id="1fd1" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">因此，让我们使用这个分类器来组合我们到目前为止的一些模型，并应用投票分类器</p><ul class=""><li id="e761" class="nb nc it lt b lu mw lx mx ma nd me ne mi nf mm pi nh ni nj bi translated">朴素贝叶斯(84%，2秒)</li><li id="d87b" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm pi nh ni nj bi translated">逻辑回归(86%，60秒，过度拟合)</li><li id="c108" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm pi nh ni nj bi translated">随机森林(80%，20秒)</li><li id="f3ef" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm pi nh ni nj bi translated">支持向量机(85%，10s)</li></ul><p id="ed78" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">请注意，对于这个代码片段，所有使用的模型定义都必须加载到Python内核中。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="6636" class="oc la it ov b gy oz pa l pb pc">from sklearn.ensemble import VotingClassifier</span><span id="19ed" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu"># 1)</strong> naive bias = mnb<br/><strong class="ov iu"># 2)</strong> logistic regression =lr<br/><strong class="ov iu"># 3)</strong> random forest =rf<br/><strong class="ov iu"># 4)</strong> support vector machine = svm</span><span id="8187" class="oc la it ov b gy pd pa l pb pc">evc=VotingClassifier(estimators=[('mnb',mnb),('lr',lr),('rf',rf),('svm',svm)],voting='hard')<br/>evc.fit(x_train, y_train)</span><span id="02f0" class="oc la it ov b gy pd pa l pb pc">print("score on test: " + str(evc.score(x_test, y_test)))<br/>print("score on train: "+ str(evc.score(x_train, y_train)))</span></pre><ul class=""><li id="4437" class="nb nc it lt b lu mw lx mx ma nd me ne mi nf mm pi nh ni nj bi translated"><strong class="lt iu">总结:</strong>即使有点过拟合，但它在测试数据上得到了迄今为止最好的预测分数。</li></ul><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="e246" class="oc la it ov b gy oz pa l pb pc"><strong class="ov iu">Results Voting Classifier<br/></strong>+-------------+------+<br/>| <strong class="ov iu">run time</strong>    | 2s   |<br/>+-------------+------+<br/>| <strong class="ov iu">train score </strong>| 0.91 |<br/>+-------------+------+<br/>| <strong class="ov iu">test score </strong> | 0.86 |<br/>+-------------+------+</span></pre><h1 id="424e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">10.神经网络(深度学习)</h1><p id="f3b2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://medium.com/botsupply/a-beginners-guide-to-deep-learning-5ee814cf7706" rel="noopener">深度学习</a>使用人工神经网络，该网络使用多层从训练数据中逐步提取更高级别的特征。我们使用一个简单的三层网络，没有任何优化，除了使用一个小的验证数据集。这里我们用的是<em class="np"> Keras </em>而不是<em class="np"> Sklearn。</em></p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="9adb" class="oc la it ov b gy oz pa l pb pc">from keras import layers<br/>from keras import models<br/>from keras import optimizers<br/>from keras import losses<br/>from keras import metrics</span><span id="ee25" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu"># split an additional validation dataset</strong><br/>x_validation=x_train[:1000]<br/>x_partial_train=x_train[1000:]<br/>y_validation=y_train[:1000]<br/>y_partial_train=y_train[1000:]</span><span id="6492" class="oc la it ov b gy pd pa l pb pc">model=models.Sequential()<br/>model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))<br/>model.add(layers.Dense(16,activation='relu'))<br/>model.add(layers.Dense(1,activation='sigmoid'))<br/>model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])</span><span id="a4c8" class="oc la it ov b gy pd pa l pb pc">model.fit(x_partial_train,y_partial_train,epochs=4,batch_size=512,validation_data=(x_validation,y_validation))</span><span id="7ddc" class="oc la it ov b gy pd pa l pb pc">print("score on test: " + str(model.evaluate(x_test,y_test)[1]))<br/>print("score on train: "+ str(model.evaluate(x_train,y_train)[1]))</span></pre><p id="0fa8" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated"><strong class="lt iu">总结:</strong>使用神经网络得到了迄今为止我们所达到的最好的测试分数，大约为88%。然而，标准设置导致训练数据的过度拟合。</p><p id="75b0" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">这个问题可以通过添加一些较小的调整参数来轻松解决，这些参数会将训练分数降低到0.90。</p><p id="7e91" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">在下文中，我应用了在神经网络中处理过拟合的<a class="ae ky" rel="noopener" target="_blank" href="/handling-overfitting-in-deep-learning-models-c760ee047c6e"> 3个最佳实践:</a></p><ol class=""><li id="5d4b" class="nb nc it lt b lu mw lx mx ma nd me ne mi nf mm ng nh ni nj bi translated">缩小网络的规模</li><li id="ba9f" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated">增加一些重量调整</li><li id="8913" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated">添加辍学</li></ol><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="5dd5" class="oc la it ov b gy oz pa l pb pc">from keras import layers<br/>from keras import models<br/>from keras import optimizers<br/>from keras import losses<br/>from keras import regularizers<br/>from keras import metrics</span><span id="4cf7" class="oc la it ov b gy pd pa l pb pc"># add validation dataset<br/>validation_split=1000<br/>x_validation=x_train[:validation_split]<br/>x_partial_train=x_train[validation_split:]<br/>y_validation=y_train[:validation_split]<br/>y_partial_train=y_train[validation_split:]</span><span id="ff6d" class="oc la it ov b gy pd pa l pb pc">model=models.Sequential()<br/>model.add(layers.Dense(<strong class="ov iu">8</strong>,<strong class="ov iu">kernel_regularizer=regularizers.l2(0.003)</strong>,activation='relu',input_shape=(10000,)))<br/><strong class="ov iu">model.add(layers.Dropout(0.5))<br/></strong>model.add(layers.Dense(<strong class="ov iu">8</strong>,<strong class="ov iu">kernel_regularizer=regularizers.l2(0.003)</strong>,activation='relu'))<br/><strong class="ov iu">model.add(layers.Dropout(0.6))<br/></strong>model.add(layers.Dense(1,activation='sigmoid'))<br/>model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])</span><span id="220a" class="oc la it ov b gy pd pa l pb pc">model.fit(x_partial_train,y_partial_train,epochs=4,batch_size=512,validation_data=(x_validation,y_validation))</span><span id="0ab2" class="oc la it ov b gy pd pa l pb pc">print("score on test: " + str(model.evaluate(x_test,y_test)[1]))<br/>print("score on train: "+ str(model.evaluate(x_train,y_train)[1]))</span><span id="ac22" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu">Results Deep Learning with Neural Network standard<br/></strong>+-------------+------+<br/>| <strong class="ov iu">run time</strong>    | 8s   |<br/>+-------------+------+<br/>| <strong class="ov iu">train score </strong>| 0.95 |<br/>+-------------+------+<br/>| <strong class="ov iu">test score </strong> | 0.88 |<br/>+-------------+------+</span><span id="904c" class="oc la it ov b gy pd pa l pb pc"><strong class="ov iu">Results Deep Learning with Neural Network optimised<br/></strong>+-------------+------+<br/>| <strong class="ov iu">run time</strong>    | 8s   |<br/>+-------------+------+<br/>| <strong class="ov iu">train score </strong>| 0.90 |<br/>+-------------+------+<br/>| <strong class="ov iu">test score </strong> | 0.88 |<br/>+-------------+------+</span></pre><h1 id="cff0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><p id="902f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们现在在同一数据集上训练了10种不同的机器学习算法来解决同一任务，根据对文本段落的情感分析预测正面或负面的评论。让我们看看谁在预测质量和运行时间方面胜出。参数过拟合(OF)就是训练分数与测试分数之间的差异。如果它很大，这意味着我们有很高的过度拟合。</p><pre class="kj kk kl km gt ou ov ow ox aw oy bi"><span id="9fc0" class="oc la it ov b gy oz pa l pb pc">+----+-------------------+----------+-------+------+---------+<br/><strong class="ov iu">| No | Algorithm         | Time [s] | Train | Test | OF      |</strong><br/>+----+-------------------+----------+-------+------+---------+<br/>| 1  | Naive Bayes       | <strong class="ov iu">2 </strong>       | 0.87  | 0.84 | 0.03    |<br/>+----+-------------------+----------+-------+------+---------+<br/>| 2  | Logistic Reg.     | 60       | 0.98  | <strong class="ov iu">0.86</strong> | <strong class="ov iu">0.12 !!</strong> |<br/>+----+-------------------+----------+-------+------+---------+<br/>| 3  | KNN               | 720      | 0.79  | 0.62 | <strong class="ov iu">0.17</strong> !! |<br/>+----+-------------------+----------+-------+------+---------+<br/>| 4  | SVM               | <strong class="ov iu">2</strong>        | 0.86  | 0.85 | 0.01    |<br/>+----+-------------------+----------+-------+------+---------+<br/>| 5  | Decision Tree     | 240      | 1.0   | 0.70 | <strong class="ov iu">0.3 !! </strong> |<br/>+----+-------------------+----------+-------+------+---------+<br/>| 6  | Bagging DT        | 600      | 0.93  | 0.77 | <strong class="ov iu">0.16 !!</strong> |<br/>+----+-------------------+----------+-------+------+---------+<br/>| 7  | Boosting DT       | 300      | 0.80  | 0.78 | 0.02    |<br/>+----+-------------------+----------+-------+------+---------+<br/>| 8  | Random Forest     | 20       | 0.83  | 0.80 | 0.03    |<br/>+----+-------------------+----------+-------+------+---------+<br/>| 9  | Voting Classifier | <strong class="ov iu">2 </strong>       | 0.91  | <strong class="ov iu">0.86</strong> | 0.04    |<br/>+----+-------------------+----------+-------+------+---------+<br/>| 10 |<strong class="ov iu"> Neuronal Network</strong>  | <strong class="ov iu">8</strong>        | 0.90  | <strong class="ov iu">0.88</strong> | 0.02    |<br/>+----+-------------------+----------+-------+------+---------+</span></pre><h2 id="6cc5" class="oc la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated"><strong class="ak">最佳运行时间:</strong></h2><p id="8bcd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在运行时间方面，最快的算法是<strong class="lt iu">朴素贝叶斯、支持向量机、投票分类器</strong>和<strong class="lt iu">神经网络。</strong></p><h2 id="153a" class="oc la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated"><strong class="ak">最佳预测得分:</strong></h2><p id="b1b0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">就测试数据集的最佳预测而言，最佳算法是<strong class="lt iu">逻辑回归、投票分类器</strong>和<strong class="lt iu">神经网络。</strong></p><h2 id="392a" class="oc la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated"><strong class="ak">最差过拟合:</strong></h2><p id="9ad3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">过拟合最多的算法是<strong class="lt iu">逻辑回归、K近邻</strong>、<strong class="lt iu">决策树</strong>和<strong class="lt iu"> Bagging决策树。</strong></p><h2 id="5044" class="oc la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated"><strong class="ak">最佳算法:</strong></h2><p id="bfea" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">解决该文本情感分析任务的总体最佳方法是<strong class="lt iu">神经网络、</strong>，其速度快、精度高且低过拟合。速度稍快但预测质量稍差的是<strong class="lt iu">投票分类器</strong>。</p><h1 id="9193" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Jupyter笔记本</h1><p id="8227" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">你可以在我的Github资源库中找到所有作为Jupyter笔记本的代码:<a class="ae ky" href="https://github.com/alexortner/teaching/tree/master/binary_classification" rel="noopener ugc nofollow" target="_blank">https://Github . com/alexortner/teaching/tree/master/binary _ classification</a></p><p id="4a1a" class="pw-post-body-paragraph lr ls it lt b lu mw ju lw lx mx jx lz ma my mc md me mz mg mh mi na mk ml mm im bi translated">在这里，您会发现应用于不同机器学习问题和数据集的相同的前10个二进制分类算法。</p><ol class=""><li id="c631" class="nb nc it lt b lu mw lx mx ma nd me ne mi nf mm ng nh ni nj bi translated">IMDB数据集— <em class="np">自然语言处理</em> —二元情感分析</li><li id="771f" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated">FashionMNIST数据集— <em class="np">计算机视觉</em> —二值图像分类</li><li id="6d37" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated">威斯康星州乳腺癌数据集-简单二元分类法</li></ol><h1 id="30d9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">感谢阅读！</h1><p id="dbe7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我希望你喜欢这篇文章。如果您看到任何需要纠正的地方，请发表评论，以便我尽快修复。</p></div></div>    
</body>
</html>