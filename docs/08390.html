<html>
<head>
<title>The curious case of developmental BERTology</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">发展型BERTology的奇特案例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-curious-case-of-developmental-bertology-d601ec52f69d?source=collection_archive---------57-----------------------#2020-06-18">https://towardsdatascience.com/the-curious-case-of-developmental-bertology-d601ec52f69d?source=collection_archive---------57-----------------------#2020-06-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="84c9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">稀疏性、迁移学习、概括和大脑</h2></div><p id="7be8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章是为机器学习研究人员和神经科学家写的(将使用这两个领域的一些术语)。虽然这不是一个全面的文献综述，我们将通过一系列主题的经典作品和新成果的选择，试图发展以下论点:</p><blockquote class="lb"><p id="09db" class="lc ld iq bd le lf lg lh li lj lk la dk translated">就像表征学习和感知/认知神经生理学之间富有成效的相互作用一样，迁移/持续学习、高效深度学习和发展神经生物学之间也存在类似的协同作用。</p></blockquote><p id="31aa" class="pw-post-body-paragraph kf kg iq kh b ki lm jr kk kl ln ju kn ko lo kq kr ks lp ku kv kw lq ky kz la ij bi translated">希望它能以一种或两种方式激励读者，或者至少在全球疫情期间消除一些无聊。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/010e4e8c6eb9b01a8f50f52c75da7a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7UsLCJZaJ9MDHTrd"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">由<a class="ae mh" href="https://unsplash.com/@fredasem?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">弗瑞德·卡尼</a>在<a class="ae mh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="53ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将通过大型语言模型的镜头触及以下主题:</p><ul class=""><li id="8bef" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la mn mo mp mq bi translated">过度参数化的深度神经网络如何泛化？</li><li id="637f" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">迁移学习如何帮助概括？</li><li id="19dd" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">我们如何让深度学习在实践中计算高效？</li><li id="1270" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">在解决这些问题的过程中，深度学习研究<em class="mw">会如何使</em>和<em class="mw">受益于对发育和老化大脑的</em>科学研究？</li></ul><h1 id="878c" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">哲学序言</h1><p id="9f9e" class="pw-post-body-paragraph kf kg iq kh b ki np jr kk kl nq ju kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">在我们开始之前，谨慎的做法是说几句关于<em class="mw">大脑隐喻</em>的话，以澄清作者在这个问题上的立场，因为它经常在辩论中出现。</p><p id="bbbc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度学习和神经科学的融合可以说早在人工神经网络的概念出现时就已经发生了，因为人工神经元抽象了生物神经元的特征行为。然而，截然不同的学习机制和智能功能种类的差异在这两个屹立了几十年的人之间竖起了一道难以逾越的障碍。近年来现代深度学习的成功重新点燃了另一股融合的潮流，结出了新的果实。除了设计受大脑启发的AI系统(例如[ <a class="ae mh" href="https://www.cell.com/neuron/fulltext/S0896-6273(17)30509-3" rel="noopener ugc nofollow" target="_blank"> 2 </a> ])，深度神经网络最近也被提议作为一个有用的模型系统来理解大脑如何工作(例如[ <a class="ae mh" href="https://www.nature.com/articles/s41593-019-0520-2" rel="noopener ugc nofollow" target="_blank"> 3 </a> ])。好处是相互的。学习机制的协调正在取得进展，但在不止一个重要方面，智力差距仍然顽固存在。</p><p id="a2cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，对于一个深度学习研究人员或从业者来说，看看今天这个复杂的景观，大脑类比<em class="mw">是有益的</em>还是误导的？基于信仰给出答案当然简单，双方都有大量信徒。但是现在，让我们不要凭信念选择立场。相反，让我们完全通过其实际分支来评估其独特上下文中的每个类比:<strong class="kh ir"><em class="mw"/></strong><em class="mw">，它只有在做出实验上可验证/可证伪的预测时才是有用的，而</em> <strong class="kh ir"> <em class="mw">对于工程</em> </strong> <em class="mw">来说，它只有在生成可以经受坚实基准测试的候选特征时才是有用的</em>。因此，对于我们将在本文其余部分提出的所有大脑类比，无论它们看起来多么合适或牵强，我们都将超越任何先前的原则，努力阐明可以在实践中指导未来科学和工程工作的假设，无论是在这些页面的限制之内还是之外。</p><h1 id="64d1" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">工作类比</h1><p id="905c" class="pw-post-body-paragraph kf kg iq kh b ki np jr kk kl nq ju kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">当把深度神经网络比作大脑时，我们通常会想到什么？</p><p id="0feb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对大多数人来说，网络结构映射到大脑区域的大体解剖结构(如感觉通路)和它们的相互连接，即连接体，单元映射到神经元或细胞组件，连接权重映射到突触强度。因此，神经生理学执行模型推理的计算。</p><p id="02be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度神经网络的学习通常以在训练数据集上优化目标函数的形式，在给定预定义的网络架构的情况下进行。(一个主要的困难在于人工学习算法的生物学合理性，这是我们在本文中没有触及的主题——在这里，尽管机制不同，我们只是接受功能的相似性。)因此，通过优化的数据驱动学习类似于基于经验的神经发展，即<em class="mw">培育</em>，而网络架构以及很大程度上的初始化和一些超参数是作为进化的结果而被遗传编程的，即<em class="mw">本性</em>。</p><blockquote class="nu nv nw"><p id="5841" class="kf kg mw kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">备注</em> </strong> <em class="iq">:需要注意的是，现代的深度网络架构，无论是手工隐式设计的，还是通过神经架构搜索(NAS)显式优化的[</em><a class="ae mh" href="http://arxiv.org/abs/1905.01392" rel="noopener ugc nofollow" target="_blank"><em class="iq">7</em></a><em class="iq">]，都是数据驱动优化的结果，产生了</em>归纳偏差<em class="iq">——免费的午餐由所有未能在自然选择中幸存的不适合者买单。</em></p></blockquote><p id="d639" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于数据和计算能力的快速增长，2010年的十年见证了深度神经网络物种的寒武纪大爆发，在机器学习领域迅速传播。</p><h1 id="d003" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">伯特学</h1><p id="bf31" class="pw-post-body-paragraph kf kg iq kh b ki np jr kk kl nq ju kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">随着现代深度学习的进化在过去两年里产生了一群新物种，情况变得更加复杂。他们在自然语言理解的大陆(NLU)，在承载着巨大计算能力的大河肥沃的三角洲上茁壮成长，比如谷歌和微软。这些非凡的生物有一些关键的共性:它们都有一个被称为<em class="mw">变压器</em> [ <a class="ae mh" href="http://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> 8 </a> ]的典型皮质微电路，拥有快速增加的脑容量，创造了历史记录(例如[ <a class="ae mh" href="http://arxiv.org/abs/1909.08053" rel="noopener ugc nofollow" target="_blank"> 9 </a>，<a class="ae mh" href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" rel="noopener ugc nofollow" target="_blank"> 10 </a>，<a class="ae mh" href="http://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> 11 </a>)，并且经常以其中一个布偶的科学名称命名。但是这些物种对其进化成功至关重要的最突出的共同特征是转移学习的能力。</p><p id="c1c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是什么意思？嗯，这些生物有两个阶段的神经发展:一个漫长的，自我监督的幼虫阶段，称为<em class="mw">预训练</em>，随后是一个快速的，受监督的成熟阶段，称为<em class="mw">微调</em>。在自我监督的预训练过程中，大量未标记文本的语料库被呈现给受试者，受试者通过优化某些目标来自娱自乐，这些目标非常类似于解决给人类孩子的语言测验，例如完成句子，填写缺失的单词，说出句子的逻辑顺序，以及找出语法错误。然后，在微调过程中，经过良好预训练的受试者可以通过监督训练快速学会执行特定的语言理解任务。</p><p id="34c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">transformers (BERT) [ <a class="ae mh" href="http://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> 12 </a> ]的双向编码器表示的出现标志着迁移学习对NLU土地的彻底征服。BERT和它的变体已经在相当大的范围内推进了最先进的技术。他们非凡的成功激起了人们对这些模型内部运作的极大兴趣，创造了对“机器人学”的研究。与神经生物学家不同，BERTologists将电极插入模型大脑，以记录解释神经代码的活动(即激活和注意模式)，对大脑区域(即编码层和注意头)进行有针对性的损伤，以了解它们的功能，并研究早期发展的经验(即训练前的目标)如何有助于成熟的行为(即在NLU任务中的良好表现)。</p><h1 id="07e8" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">网络压缩</h1><p id="17ca" class="pw-post-body-paragraph kf kg iq kh b ki np jr kk kl nq ju kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">与此同时，在深度学习的世界中，多阶段发展(如迁移学习)发生在不止一个动物王国。特别是，在生产中，人们经常需要将一个训练好的庞大神经网络压缩成一个紧凑的网络，以便有效地部署。</p><p id="557a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">网络压缩的实践源自深度神经网络的一个非常令人困惑的特性:<em class="mw">过度参数化不仅有助于泛化，还有助于优化</em>。也就是说，训练一个小网络往往不仅不如训练一个大网络(如果一个人当然有能力这样做的话)[ <a class="ae mh" href="https://www.pnas.org/content/116/32/15849" rel="noopener ugc nofollow" target="_blank"> 14 </a> ]，而且不如将一个训练好的大网络压缩到同样小的规模。在实践中，压缩可以通过稀疏化(剪枝)、蒸馏等来实现。</p><blockquote class="nu nv nw"><p id="27b2" class="kf kg mw kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">备注:</em> </strong> <em class="iq">值得注意的是，对密集网络进行优化再压缩所产生的最佳稀疏网络的现象(见例如【</em> <a class="ae mh" href="http://arxiv.org/abs/1710.01878" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> 15 </em> </a> <em class="iq">，</em> <a class="ae mh" href="http://arxiv.org/abs/1902.09574" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> 16 </em> </a> <em class="iq">)很像正在发育的大脑，其中过度产生的连接被逐渐修剪掉【</em> <a class="ae mh" href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(17)30200-0" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> 17 </em></a></p></blockquote><p id="0c78" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，模型压缩中的多阶段开发类型与迁移学习非常不同。迁移学习的两个阶段看到相同的模型针对不同的目标进行优化，而在模型压缩中，原始模型变形为不同的模型，以保持相同目标的最优性。如果说前者类似于获得新技能的成熟，那么后者更像是优雅的衰老，而不会失去已经学到的技能。</p><h1 id="70df" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">学习权重与学习结构:二元性？</h1><p id="3db8" class="pw-post-body-paragraph kf kg iq kh b ki np jr kk kl nq ju kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">当一个网络被压缩时，它的<em class="mw">结构</em>经常会发生变化。这可能意味着<em class="mw">网络架构</em>(例如在提取的情况下)或者<em class="mw">参数稀疏性</em>(例如在修剪的情况下)。这些结构上的改变通常是由启发法或正则化方法强加的，这些方法限制了原本已经有效的优化。</p><p id="00db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是<em class="mw">结构</em>能超越仅仅是一个效率限制，成为一个有效的学习手段吗？越来越多的新兴研究似乎表明了这一点。</p><p id="6432" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个有趣的例子是权重不可知网络。这些类似水母的生物在一生中不需要学习，但仍然非常适应它们的生态位，因为进化为它们选择了有效的大脑结构。</p><p id="123d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">即使是自然选择的固定架构，<em class="mw">学习稀疏结构仍然可以和学习突触权重</em>一样有效。最近，Ramanujan等人[ <a class="ae mh" href="http://arxiv.org/abs/1911.13299" rel="noopener ugc nofollow" target="_blank"> 19 </a> ]设法找到了初始化卷积网络的稀疏化版本，如果足够宽和足够深，其泛化能力不会比经历权重训练的密集网络差。理论研究还表明，如果模型被充分过度参数化，随机权重的稀疏化与优化参数一样有效[ <a class="ae mh" href="http://arxiv.org/abs/2002.00585" rel="noopener ugc nofollow" target="_blank"> 20 </a>，<a class="ae mh" href="http://arxiv.org/abs/2003.01794" rel="noopener ugc nofollow" target="_blank"> 21 </a> ]。</p><p id="b1ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，在现代深度学习严重过度参数化的制度中，我们有一把双刃剑:权重的优化和结构的优化<em class="mw"/>。这让人想起作为生物学习和记忆基础机制的<em class="mw">突触</em>和<em class="mw">结构可塑性</em>(例如，参见[ <a class="ae mh" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3181802/" rel="noopener ugc nofollow" target="_blank"> 22 </a>、<a class="ae mh" href="https://www.cell.com/current-biology/pdf/S0960-9822(06)02620-0.pdf" rel="noopener ugc nofollow" target="_blank"> 23 </a>)。</p><blockquote class="nu nv nw"><p id="614c" class="kf kg mw kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">备注</em> </strong> <em class="iq">:描述参数稀疏性的一种正式方式是通过参数掩码的公式化(图1)。学习可以通过优化固定结构中的连续权重来实现，也可以通过优化给定一组固定权重的离散结构来实现(图2)。</em></p></blockquote><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/d67f1e1fdb6316bcfc25b8f3e5524918.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*PRi1sw4OdBgTnBYAdYkLkA.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated"><strong class="bd ob">图一。模型参数结构稀疏性的参数掩模公式。</strong></p></figure><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/44826410ff251831608161ff0da89f36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*NDJBph1-rDAXLzFWZM1rqA.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated"><strong class="bd ob">图二。学习权重与学习结构。</strong></p></figure><h1 id="97e6" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">通过稀疏化进行微调</h1><p id="0738" class="pw-post-body-paragraph kf kg iq kh b ki np jr kk kl nq ju kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">既然结构就像权重一样可以优化学习，那么这个机制是否可以用来让迁移学习变得更好？</p><p id="d3f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是的，确实可以。最近，Radiya-Dixit &amp; Wang[<a class="ae mh" href="http://arxiv.org/abs/2004.14129" rel="noopener ugc nofollow" target="_blank">24</a>]让BERT捡起这个新基因，进化出新的东西。他们表明，可以通过稀疏化预训练的权重来有效地微调BERT，而不改变它们的值，正如通用语言理解评估(GLUE)任务所系统展示的那样[ <a class="ae mh" href="https://arxiv.org/abs/1804.07461" rel="noopener ugc nofollow" target="_blank"> 25 </a> ]。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/55db132bfaf5131efabad8d3f9ce65e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*maJPoRLUjI9AiirkbeOfCA.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated"><strong class="bd ob">图3。通过稀疏化对BERT进行微调</strong> [ <a class="ae mh" href="http://arxiv.org/abs/2004.14129" rel="noopener ugc nofollow" target="_blank"> 24 </a> ]。</p></figure><blockquote class="nu nv nw"><p id="2037" class="kf kg mw kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">备注</em> </strong> <em class="iq">:注意，类似的稀疏化微调已经成功应用于计算机视觉，例如【</em> <a class="ae mh" href="http://arxiv.org/abs/1801.06519" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> 26 </em> </a> <em class="iq">】。还要注意预训练期间稀疏化BERT的现有工作[</em><a class="ae mh" href="http://arxiv.org/abs/2002.08307" rel="noopener ugc nofollow" target="_blank"><em class="iq">27</em></a><em class="iq">]。</em></p></blockquote><p id="13f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过稀疏化进行的微调具有<em class="mw">有利的实际意义</em>。一方面，预训练的参数值在学习多个任务时保持不变，将特定于任务的参数存储减少到只有一个二进制掩码；另一方面，稀疏化压缩了模型，通过适当的硬件加速，潜在地避免了许多“乘零累加”操作。一石二鸟。</p><p id="4fcc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，除了实际的好处，通过稀疏化进行微调的可能性带来了一些新的机会，有助于更深入地理解语言预训练及其与生物大脑的潜在联系。让我们在接下来的部分中看看它们。</p><h1 id="4ba3" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">不同彩票的中奖彩票</h1><p id="7d85" class="pw-post-body-paragraph kf kg iq kh b ki np jr kk kl nq ju kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">首先我们从最优化的角度研究语言预训练的本质。</p><p id="5782" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">似乎语言预训练元学习是学习下游NLU任务的良好初始化。正如Hao等人[ <a class="ae mh" href="http://arxiv.org/abs/1908.05620" rel="noopener ugc nofollow" target="_blank"> 28 </a> ]最近所表明的，预训练的BERT权重具有良好的特定于任务的最优值，在损失情况下更接近且更平坦。这意味着预训练使微调更容易，微调后的解决方案更通用。</p><p id="6c5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">类似地，预训练也使得发现微调的稀疏子网络更容易[ <a class="ae mh" href="http://arxiv.org/abs/2004.14129" rel="noopener ugc nofollow" target="_blank"> 24 </a> ]。因此，有趣的是，预先训练的语言模型具有弗兰克尔和卡宾[ <a class="ae mh" href="http://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank"> 29 </a> ]制定的“中奖彩票”的所有关键属性，但鉴于优化权重与结构的二重性，它们恰恰是<em class="mw">互补</em>类型(图3、4):</p><ul class=""><li id="06be" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la mn mo mp mq bi translated"><strong class="kh ir"> Frankle-Carbin中奖票</strong>是<em class="mw">一种特定的稀疏结构</em>，使得<em class="mw">便于权重优化</em>。对重量初始化 [ <a class="ae mh" href="http://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank"> 29 </a>敏感<em class="mw">。这是<em class="mw">跨视觉任务<a class="ae mh" href="http://arxiv.org/abs/1906.02773" rel="noopener ugc nofollow" target="_blank"> 30 </a>的潜在转移</em>。</em></li><li id="da61" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">一个<strong class="kh ir">预训练的语言模型</strong>就是<em class="mw">一组特定的权重</em>使得<em class="mw">便于结构优化</em>。对结构初始化 [ <a class="ae mh" href="http://arxiv.org/abs/2004.14129" rel="noopener ugc nofollow" target="_blank"> 24 </a>敏感<em class="mw">。它是<em class="mw">跨NLU任务</em>转移<a class="ae mh" href="http://arxiv.org/abs/2004.14129" rel="noopener ugc nofollow" target="_blank"> 24 </a>。</em></li></ul><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e377ca2f7ecb57e6624b1b0b690878c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*qE1MEebG4xFZ-emEjlaf7g.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated"><strong class="bd ob">图4。弗兰克尔-卡宾赢票</strong> [ <a class="ae mh" href="http://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank"> 29 </a>，<em class="ll">比照</em>通过稀疏化进行微调(图3)。</p></figure><blockquote class="nu nv nw"><p id="e2e5" class="kf kg mw kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">备注</em> </strong> <em class="iq">:注意预训练BERT的“中奖票”属性与[19]中的宽深政权不同。基于transformer的大型语言模型，如果做得足够宽和足够深(如果它们已经很大，那么一定会非常大)，是否可以在没有预训练的情况下，从随机初始化中有效地进行微调，这仍然是一个悬而未决的问题。</em></p></blockquote><p id="282c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然学习中奖彩票的权重和在预训练的权重内搜索子网会导致相同的结果——一个紧凑、稀疏的网络，可以很好地推广，但这两种方法的生物学合理性截然不同:找到一张弗兰克尔-卡宾彩票需要重复的时间倒带和重新训练，如果早期的状态可以被遗传编码，然后在下一代中复制以实现倒带，这一过程只有在多个生物代中才有可能。但是在结构稀疏化之后的重量预训练类似于发育和老化，都发生在一代人的时间内。因此，密集预训练和稀疏微调可能是神经发育的有用模型。</p><h1 id="e245" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">健壮性:不同结构的相同功能</h1><p id="bcc4" class="pw-post-body-paragraph kf kg iq kh b ki np jr kk kl nq ju kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">伯特和大脑的另一个不可思议的相似之处是其结构的坚固性。</p><p id="42dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在各种稀疏水平下，似乎有大量良好的预训练BERT子网络[ <a class="ae mh" href="http://arxiv.org/abs/2004.14129" rel="noopener ugc nofollow" target="_blank"> 24 </a> ]:一个典型的胶合任务可以通过消除预训练权重的百分之几到一半以上来学习，良好的稀疏解决方案存在于其间的任何地方(图5，左)。这让人想起了在成熟和衰老的大脑中发挥作用的结构可塑性——其获得的功能保持不变，而底层结构随着时间的推移不断发生变化。这与易碎的<em class="mw">点方案</em>乘<em class="mw"> </em>的传统工程截然不同。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi od"><img src="../Images/ffebd5ad8c31f5602427b1e6725a275e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ScroY6YenSh4EhSMMSqXGQ.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated"><strong class="bd ob">图五。通过稀疏化进行微调的语言模型的结构鲁棒性</strong>。<strong class="bd ob"> </strong>(左)存在许多预训练BERT的良好子网络，这些子网络跨越大范围的稀疏性(从百分之几到一半以上)[ <a class="ae mh" href="http://arxiv.org/abs/2004.14129" rel="noopener ugc nofollow" target="_blank"> 24 </a> ]。(右)持续稀疏化过程中损失景观的漫画视图。密集训练(实心洋红色和橙色箭头)找到位于连续流形上的低损耗解决方案(类似于[ <a class="ae mh" href="http://arxiv.org/abs/1803.00885" rel="noopener ugc nofollow" target="_blank"> 31 </a> ]的图1的黄色虚线框)。只要通过重量消除的任何结构扰动(紫色虚线箭头和圆圈)没有偏离低损失歧管太远，快速的结构微调(洋红色虚线箭头和圆圈)可以连续地恢复最佳性。蓝色网格表示稀疏参数的离散集合。</p></figure><p id="dccc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种现象主要源于深度神经网络的过度参数化。在总体过度参数化的现代体制中，损失景观中的最优值通常是高维连续非凸流形[ <a class="ae mh" href="http://arxiv.org/abs/1803.00885" rel="noopener ugc nofollow" target="_blank"> 31 </a>，<a class="ae mh" href="http://arxiv.org/abs/1906.04724" rel="noopener ugc nofollow" target="_blank"> 32 </a> ]。这与生物学有着奇怪的相似之处，在生物学中，完全相同的网络行为可以从非常不同的底层参数配置中产生，在参数空间中形成非凸集，例如参见[ <a class="ae mh" href="https://www.pnas.org/content/108/Supplement_3/15542" rel="noopener ugc nofollow" target="_blank"> 33 </a> ]。</p><p id="4f10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在有趣的部分来了。就像生物学中的终身体内平衡调整一样，类似的机制可能支持过度参数化的深度网络中的持续学习(如图5所示):密集连接的早期学习发现了一个好的解流形，沿着它存在大量好的稀疏解；随着网络的老化，网络的持续和逐渐稀疏化可以通过结构可塑性快速微调(就像保持终身可塑性的大脑)。</p><p id="f25c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从神经生物学的角度来看，如果接受<em class="mw"> </em> <strong class="kh ir"> <em class="mw">优化假说</em> </strong> [ <a class="ae mh" href="https://www.nature.com/articles/s41593-019-0520-2" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]，那么终生可塑性必然在整个生命周期中不断进行某种功能优化。按照这个逻辑，这个过程出现偏差而导致的神经发育障碍，本质上应该是<strong class="kh ir"> <em class="mw">优化疾病</em> </strong>，具有初始化不良、优化器动态不稳定等病因学特征。</p><p id="a684" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述假设是否对深层神经网络普遍适用，是否足以作为神经发育和病理生理学的良好模型，这是未来研究的开放问题。</p><h1 id="03c4" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">伯特学到了多少？</h1><p id="f72f" class="pw-post-body-paragraph kf kg iq kh b ki np jr kk kl nq ju kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">最后，让我们将一些神经科学的思维应用于BERTology。</p><p id="a2a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们问这样一个问题:在<em class="mw">预先训练的</em> BERT参数中存储了多少与解决NLU任务相关的信息？这不是一个容易回答的问题，因为在预训练和微调期间参数值的连续变化会相互混淆。</p><p id="3784" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在通过稀疏化进行微调的BERT的情况下，这种限制不再存在，其中预训练仅学习权重值，微调仅学习结构。对生物学家来说，如果两个发展阶段涉及完全不同的生理过程，这总是好消息，在这种情况下，其中一个可以用来研究另一个。</p><p id="7e4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们这样做。让我们扰动预先训练的权重值，并研究下游结果。在这个实验中，我们不进行生理扰动(例如破坏注意力头)，而是进行药理学扰动:全身应用一种影响整个大脑中每一个突触的物质。这种药物是量子化的。表1总结了一些初步的剂量反应:尽管BERT和相关物种已经发展出了巨大的大脑，但似乎在语言预训练期间学到的知识可能只用每个突触的几个比特来描述。</p><p id="8505" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在实践中，这意味着，由于预训练的权重在通过稀疏化进行微调的过程中不会改变值，因此可能只需要存储所有BERT参数的低精度整数版本，而不会产生任何不利后果——显著的压缩。结果:<strong class="kh ir">你所需要的是所有任务共享的预训练参数的量化整数版本，以及为每个任务微调的二进制掩码</strong>。</p><blockquote class="nu nv nw"><p id="85bf" class="kf kg mw kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">备注</em> </strong> <em class="iq">:注意现有的关于BERT权重的量化的工作是量化微调的权重(例如Q-BERT[</em><a class="ae mh" href="http://arxiv.org/abs/1909.05840" rel="noopener ugc nofollow" target="_blank"><em class="iq">34</em></a><em class="iq">])而不是预训练的权重。</em></p></blockquote><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi oe"><img src="../Images/1fdb7022cdbebf28ae09a718f3807522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yzL-KNOBYuKL7dfZIGNZ6g.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated"><strong class="bd ob">表1。MRPC微调BERT和相关模型的F1分数。</strong>多亏了<a class="ae mh" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">抱紧脸的变形金刚</a>，像这样的实验轻而易举。</p></figure><h1 id="9d2a" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">收场白</h1><p id="c75d" class="pw-post-body-paragraph kf kg iq kh b ki np jr kk kl nq ju kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">深度神经网络和大脑有明显的区别:在最底层，在学习算法上，在最高层，在一般智力上。然而，中级水平的深刻相似性已被证明有利于深度学习和神经科学的进步。</p><p id="6d0f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，感知和认知<em class="mw">神经生理学</em>已经启发了<em class="mw">有效的深度网络架构</em>，这反过来又为理解大脑提供了一个有用的模型。在这篇文章中，我们提出了另一个交叉点:生物<em class="mw">神经发育</em>可能会激发<em class="mw">高效和稳健的优化程序</em>，这反过来又会成为大脑成熟和老化的有用模型。</p><blockquote class="nu nv nw"><p id="df88" class="kf kg mw kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">备注</em> </strong> <em class="iq">:需要注意的是，传统联结主义语境下的神经发展是在20世纪90年代提出的(例如参见[</em><a class="ae mh" href="https://mitpress.mit.edu/books/rethinking-innateness" rel="noopener ugc nofollow" target="_blank"><em class="iq">35</em></a><em class="iq">)</em>。</p></blockquote><p id="b5e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">具体来说，我们回顾了最近关于权重学习和结构学习作为优化的补充手段的一些结果，以及它们如何结合起来，在大型语言模型中实现有效的迁移学习。</p><p id="0d37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着结构学习在深度学习中变得越来越重要，我们将看到相应的硬件加速器出现(例如，Nvidia的Ampère架构支持稀疏权重[ <a class="ae mh" href="https://blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/" rel="noopener ugc nofollow" target="_blank"> 36 </a> ])。这可能会带来专用硬件架构多样化的新浪潮——加速结构学习需要适应特定计算的智能数据移动，这是一个新的探索前沿。</p></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><h2 id="4715" class="om my iq bd mz on oo dn nd op oq dp nh ko or os nj ks ot ou nl kw ov ow nn ox bi translated">参考</h2><p id="2ea5" class="pw-post-body-paragraph kf kg iq kh b ki np jr kk kl nq ju kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">[1] W. S .麦卡洛克和w .皮茨，“<a class="ae mh" href="https://link.springer.com/article/10.1007/BF02478259" rel="noopener ugc nofollow" target="_blank">神经活动中固有观念的逻辑演算</a>”，1943年。<br/> [2] D .哈萨比斯等，“<a class="ae mh" href="https://www.cell.com/neuron/fulltext/S0896-6273(17)30509-3" rel="noopener ugc nofollow" target="_blank">神经科学启发的人工智能</a>”，2017。<br/> [3] B. A. Richards等人，“<a class="ae mh" href="https://www.nature.com/articles/s41593-019-0520-2" rel="noopener ugc nofollow" target="_blank">神经科学的深度学习框架</a>”，2019。<br/> [4] T. P. Lillicrap等人，“<a class="ae mh" href="https://www.nature.com/articles/s41583-020-0277-3" rel="noopener ugc nofollow" target="_blank">反向传播与大脑</a>”，2020。<br/>【5】g . Marcus，《深度学习:<a class="ae mh" href="http://arxiv.org/abs/1801.00631" rel="noopener ugc nofollow" target="_blank">一个批判性的鉴定</a>，2018。<br/> [6] G. Marcus，“人工智能的下一个十年:迈向强大人工智能的四个步骤”，2020年。<br/> [7] M. Wistuba等著《<a class="ae mh" href="http://arxiv.org/abs/1905.01392" rel="noopener ugc nofollow" target="_blank">关于神经架构搜索的调查</a>》，2019。<br/> [8] A. Vaswani等人，“<a class="ae mh" href="http://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>”，2017。<br/> [9] M. Shoeybi等，“<a class="ae mh" href="http://arxiv.org/abs/1909.08053" rel="noopener ugc nofollow" target="_blank">威震天-LM:利用模型并行性训练数十亿参数语言模型</a>”，2019。<br/>【10】micro sift Research，“<a class="ae mh" href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" rel="noopener ugc nofollow" target="_blank">图灵-NLG:微软</a>的170亿参数语言模型”，2020年。<br/>【11】t . b . Brown等.<a class="ae mh" href="http://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank">语言模型是很少出手的学习者</a>，2020。<br/>【12】j . Devlin等人，“<a class="ae mh" href="http://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向变形器的预训练</a>”，2018。<br/>【13】A . Rogers等著《<a class="ae mh" href="http://arxiv.org/abs/2002.12327" rel="noopener ugc nofollow" target="_blank">伯特学入门:我们所知道的伯特如何工作</a>》，2020年。<br/> [14] M. Belkin等人，“<a class="ae mh" href="https://www.pnas.org/content/116/32/15849" rel="noopener ugc nofollow" target="_blank">调和现代机器学习实践与古典-方差权衡</a>”，2019。<br/>【15】m . Zhu，S. Gupta，“<a class="ae mh" href="http://arxiv.org/abs/1710.01878" rel="noopener ugc nofollow" target="_blank">修剪，还是不修剪:探索模型压缩中修剪的功效</a>”，2017。<br/>【16】t . Gale等，“<a class="ae mh" href="http://arxiv.org/abs/1902.09574" rel="noopener ugc nofollow" target="_blank">深度神经网络中的稀疏状态</a>”，2019。<br/>【17】s . Navlakha等人，“<a class="ae mh" href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(17)30200-0" rel="noopener ugc nofollow" target="_blank">网络设计与大脑</a>”，2018。<br/>【18】a . Gaier和D. Ha，“<a class="ae mh" href="http://arxiv.org/abs/1906.04358" rel="noopener ugc nofollow" target="_blank">权重不可知神经网络</a>”，2019。<br/>【19】v . Ramanujan等人《<a class="ae mh" href="http://arxiv.org/abs/1911.13299" rel="noopener ugc nofollow" target="_blank">随机加权的神经网络中隐藏着什么？</a>》，2019。<br/>【20】e .马拉奇等著《<a class="ae mh" href="http://arxiv.org/abs/2002.00585" rel="noopener ugc nofollow" target="_blank">证明彩票假说:修剪是你所需要的全部</a>》，2020。<br/>【21】m .叶等，“<a class="ae mh" href="http://arxiv.org/abs/2003.01794" rel="noopener ugc nofollow" target="_blank">良好子网络可证明存在:通过贪婪正向选择进行剪枝</a>”，2020。<br/>【22】f . h . Gage，“成人大脑的<a class="ae mh" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3181802/" rel="noopener ugc nofollow" target="_blank">结构可塑性</a>”，2004。<br/>【23】h . Johansen-Berg，“<a class="ae mh" href="https://www.cell.com/current-biology/pdf/S0960-9822(06)02620-0.pdf" rel="noopener ugc nofollow" target="_blank">结构可塑性:重新连接大脑</a>”，2007。<br/>【24】e . Radiya-Dixit和X. Wang，“<a class="ae mh" href="http://arxiv.org/abs/2004.14129" rel="noopener ugc nofollow" target="_blank">微调能有多细？学习高效的语言模型</a>”，2020。<br/>【25】A .王等，“<a class="ae mh" href="https://arxiv.org/abs/1804.07461" rel="noopener ugc nofollow" target="_blank"> GLUE:自然语言理解的多任务基准与分析平台</a>”，2019。<br/>【26】a . Mallya等人，“<a class="ae mh" href="http://arxiv.org/abs/1801.06519" rel="noopener ugc nofollow" target="_blank">捎带:通过学习任务权重</a>使单个网络适应多项任务”，2018。<br/>【27】m . a . Gordon等，“<a class="ae mh" href="http://arxiv.org/abs/2002.08307" rel="noopener ugc nofollow" target="_blank">压缩BERT:研究权重剪枝对迁移学习的影响</a>”，2020。<br/>【28】y .郝等《<a class="ae mh" href="http://arxiv.org/abs/1908.05620" rel="noopener ugc nofollow" target="_blank">可视化与理解BERT </a>的有效性》，2020。<br/>【29】j . Frankle和M. Carbin，“<a class="ae mh" href="http://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank">彩票假说:寻找小的、可训练的神经网络</a>”，2018年。<br/> [30] A. S. Morcos等人，“<a class="ae mh" href="http://arxiv.org/abs/1906.02773" rel="noopener ugc nofollow" target="_blank">一票全赢:跨数据集和优化器的一般化彩票初始化</a>”，2019。<br/> [31] F. Draxler等人，“<a class="ae mh" href="http://arxiv.org/abs/1803.00885" rel="noopener ugc nofollow" target="_blank">神经网络能源格局中本质上没有壁垒</a>”，2018。<br/> [32] S. Fort和S. Jastrzebski，“神经网络损失景观的<a class="ae mh" href="http://arxiv.org/abs/1906.04724" rel="noopener ugc nofollow" target="_blank">大尺度结构</a>”，2019。<br/>【33】e . Marder，“神经元和电路中的<a class="ae mh" href="https://www.pnas.org/content/108/Supplement_3/15542" rel="noopener ugc nofollow" target="_blank">可变性、补偿和调制</a>”，2011年。<br/> [34] S .沈等，“<a class="ae mh" href="http://arxiv.org/abs/1909.05840" rel="noopener ugc nofollow" target="_blank"> Q-BERT:基于Hessian的超低精度量化的BERT </a>”，2019。<br/> [35] J. Elman等.<a class="ae mh" href="https://mitpress.mit.edu/books/rethinking-innateness" rel="noopener ugc nofollow" target="_blank">反思天赋:发展的联结主义观点</a>，1996年(ISBN 978–0–262–55030–7)。<br/>【36】英伟达博客，“<a class="ae mh" href="https://blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/" rel="noopener ugc nofollow" target="_blank">什么是AI推理中的稀疏性？</a>”，2020。</p></div></div>    
</body>
</html>