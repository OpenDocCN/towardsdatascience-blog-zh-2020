<html>
<head>
<title>First Steps in Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的第一步</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/first-steps-in-reinforcement-learning-20cffd1c0b92?source=collection_archive---------32-----------------------#2020-03-20">https://towardsdatascience.com/first-steps-in-reinforcement-learning-20cffd1c0b92?source=collection_archive---------32-----------------------#2020-03-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/9ca020ac00ce406b637e1e16f517b2b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZIpG0vX5umuCHKpX7J7ndg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">插图作者:<a class="ae jd" href="https://www.behance.net/gallery/93893223/Infographic-and-cover-for-the-AI-article" rel="noopener ugc nofollow" target="_blank">伊洛娜·塞雷达</a></p></figure><div class=""/><div class=""><h2 id="90a7" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">从简单的政策到加强使用旋转上升</h2></div><p id="4412" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你曾经使用OpenAI创建的<a class="ae jd" href="https://spinningup.openai.com/en/latest/" rel="noopener ugc nofollow" target="_blank">旋转资源</a>研究过强化学习，你会注意到他们关于学习强化学习的建议。也就是说，从强化(或VPG，即普通政策梯度)开始，尝试从头开始实施RL算法。我也是，结果证明这是一项相当艰巨的任务，所以我决定先研究它们的实现。</p><p id="05e7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该算法看起来很简单，但是他们实现<a class="ae jd" href="https://github.com/DmytroSytro/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py" rel="noopener ugc nofollow" target="_blank">简单策略</a>和<a class="ae jd" href="https://github.com/DmytroSytro/spinningup/blob/master/spinup/algos/pytorch/vpg/vpg.py" rel="noopener ugc nofollow" target="_blank"> VPG </a>的复杂度有很大差距。简单的策略看起来像是专门为学习而设计的易于理解的代码，而VPG看起来像是一个具有并行化、模块、类和大量参数的专业库。我意识到两全其美会很棒，因此，我决定让VPG摆脱简单的政策执行。这个想法是引入最小的必要的变化，使VPG。在这个过程中，我必须仔细研究简单策略和VPG实现。我认为这是一个很好的练习，比从零开始的实现要求更少，并且仔细研究和理解了OpenAI的实现。</p><h1 id="a404" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">简单政策</h1><p id="efdf" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">如果您不熟悉<a class="ae jd" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html" rel="noopener ugc nofollow" target="_blank">旋转启动简介</a>，以下描述对于理解代码是必需的。否则，您可能会对我对Spinning Up中介绍的概念的理解感兴趣。</p><p id="12b4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在继续之前阅读<a class="ae jd" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html" rel="noopener ugc nofollow" target="_blank"> Spinning Up intro to RL </a>是一种建议，但实际上，如果你熟悉深度学习的基础知识，直观地理解简单的策略梯度是非常容易的。</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/7a8f8fe1e649593e05182eb35d697db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N4Ki0Dt6FH1nuaHJoFXA8w.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">由<a class="ae jd" href="https://www.behance.net/gallery/93893223/Infographic-and-cover-for-the-AI-article" rel="noopener ugc nofollow" target="_blank">伊洛娜·塞雷达</a></p></figure><p id="787a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你所需要做的就是拥有一个环境，一个可以在其中行动的代理，收集关于环境的观察，根据一些政策(即规则或功能)行动，接收奖励，计算通过优化政策功能使奖励最大化的损失。因此，基本上，策略可以是一个神经网络，它接收观察结果作为输入，并产生动作概率作为输出:</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">get_policy(obs)看起来像一个标准的深度神经网络(完整代码<a class="ae jd" href="https://github.com/DmytroSytro/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py" rel="noopener ugc nofollow" target="_blank">此处</a>)</p></figure><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="14c1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">区别在于损失函数。为了获得关于我们政策的真实损失的反馈，我们需要根据政策功能给出的行动概率对行动进行抽样，采取行动，获得奖励，根据抽样的行动概率对奖励进行加权。实际上，代理可以确定性地行动(即没有随机抽样)，只选择概率最高的行动。但这应该会更糟，因为代理人应该设法探索其行动空间，而不是停留在局部成功。看看梯度政策最简单的公式:</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/62516e8c46c38e787b7957ac201d1128.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*1UXUYeNohaihGOykBeCbaw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">从这里旋转起来<a class="ae jd" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="b1e7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以不理会求和符号和<strong class="kx jh"> <em class="mv"> D </em> </strong>。这些是平均和求和运算。重要的一点是，策略<strong class="kx jh"> <em class="mv"> (π) </em> </strong>参数<em class="mv">θ</em>在优化中是变化的，因为它们决定了依赖于状态<strong class="kx jh"> <em class="mv"> s </em> </strong>的动作<strong class="kx jh"> <em class="mv"> a </em> </strong>，动作带来回报<strong class="kx jh"> <em class="mv"> R </em> </strong>。奖励确定为每个轨迹<strong class="kx jh"> <em class="mv"> τ </em> </strong>末端奖励之和(试)。</p><p id="7fa1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以，这是一种奇怪的损失函数，因为回报取决于政策的输出，而且回报必须通过梯度上升来优化。最后一个问题实际上可以很容易地通过改变符号来解决。但最奇怪的是，损失实际上并不能说明什么。损失函数只是指示在具有特定策略的环境的特定状态下的特定损失。在优化策略后，代理可以采取不同的行动，这可以将环境带入不同的状态，从而带来不同的回报。所以模型改进的实际指标是收到的奖励，而不是损失。</p><h1 id="a394" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">VPG(增援)</h1><p id="4e50" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">VGP有什么新鲜事？首先，代理人得到了一个分裂的人格，因为我们都是，这样，代理人看起来更真实。这意味着经纪人现在同时是演员和评论家。用更通俗的话来说，这意味着除了决定如何行动的政策函数，我们还有试图在行动前预测回报的价值函数。</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/5117a0e787170ba6376fe09d5140523c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0EKHBp1KU0re9auM-j7uJw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">伊洛娜·塞里达</p></figure><p id="358e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有什么帮助？好吧，简单政策中的政策函数仍然隐含地说明了回报，只是因为回报在它的损失函数中。不过，奇怪的是，政策必须预测行动，但它优化了回报。比简单的职责划分更有意义的是，获得的奖励通常是不稳定的，即差异很大。所以，用各种方法计算的收益来代替损失函数中的回报是有意义的。</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/366eea56dbc9d89f75a5ef883678ec40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*yM0_yeaMtL2_QR5rzNw85w.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">从<a class="ae jd" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html" rel="noopener ugc nofollow" target="_blank">旋转起来</a></p></figure><p id="694c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">行动优势的基本思想是比较特定状态下的特定行动后接当前最优政策下的行动(<strong class="kx jh"> <em class="mv"> Q </em> </strong>)与该状态及后续状态下的最优政策行动在预期报酬(<strong class="kx jh"> <em class="mv"> V </em> </strong>)方面的差异。因此，在VPG，重要的是采取行动后获得的奖励，而不是整个轨迹(审判中的状态、行动和奖励的路径)。将这个回报与当前最优策略的预期(平均)回报进行比较也很重要。看看VPG的梯度:</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/68354636203e1e82ab23f043fc25481a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*QjSjtC7VppQZYLScD2iaKQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">从<a class="ae jd" href="https://spinningup.openai.com/en/latest/algorithms/vpg.html" rel="noopener ugc nofollow" target="_blank">旋转起来</a></p></figure><p id="bfcc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，计算期望轨迹<strong class="kx jh"><em class="mv"/></strong>的梯度，其中根据最优策略<strong class="kx jh"><em class="mv"/></strong>对策略动作进行采样，并且对于每个状态<strong class="kx jh"><em class="mv">s</em>通过动作<strong class="kx jh"><em class="mv"/></strong>给定状态<strong class="kx jh"><em class="mv">s</em><em class="mv">s</em>的概率对优势<strong class="kx jh"><em class="mv"/>A</strong>进行加权</strong></strong></p><p id="0e90" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">就现实而言，而不是数学(除非你相信数学是现实)，我们在一个轨迹中所拥有的只是一系列的状态观察、最优政策下采取的行动、奖励以及价值函数对这些奖励的评估。除了动作实际上是从最优策略分布中采样的，并且在某些状态下，出于探索的目的，可能会对不具有最高概率的动作进行采样。这很有趣，因为价值函数将根据以前的许多试验来评估该状态的奖励，当然，在这些试验中，更高概率的行为被采样得更频繁。</p><p id="969d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，价值函数评估在最优策略下，当前试验的平均回报。另一方面，当前的轨迹也是在最优政策下做出的，但有些决策是冒险的探索性决策。因此，这个想法基本上是收集奖励，在未来从当前行动中获取奖励，并将其与当前政策的平均奖励进行比较。在OpenAI的实现中，使用了一般优势估计(GAE ),我发现这是理解VPG最具挑战性的部分。这里不欢迎你。如果你看看OpenAI的advantage的代码实现，就不难了:</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="b18e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其思想是将<strong class="kx jh"> <em class="mv"> Q </em> </strong>计算为下一步奖励的贴现和贴现值(这是下一步奖励的评估)并将<strong class="kx jh"> <em class="mv"> V </em> </strong>用作基线以减少优势的方差。</p><p id="956a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，算法与简单策略的不同之处在于，在每次尝试后，我们需要计算价值和优势，在策略优化后，我们需要优化价值函数。</p><h1 id="12d6" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">代码更改</h1><p id="9ecf" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">首先，我们需要添加预测奖励的<em class="mv"> values_net </em>:</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="06bc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">添加<em class="mv"> value_loss </em>也很重要，它使用MSE和优化奖励，为使用advantage的策略模型添加新的损失函数，还有<em class="mv"> discount_cumsum </em>帮助计算advantage，以及归一化advantage的归一化函数:</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="fe86" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">除此之外，价值函数的优化器也是必须的。在进行试验时，与简单策略的第一个区别是，OpenAI的VPG实现不会将剧集收集到一个批处理中，而是相反地使用单集进行优化，并剪辑过长的剧集:</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="097a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">大部分批量变量看起来很肤浅，因为批量只是一个插曲。我只是让它们更容易与简单策略进行比较。一集的奖励不加总，奖励收集到n-1步，因为这是优势函数应该使用的。计算所有观察值。因此，奖励是用gamma参数贴现计算的。优势是按照OpenAI的实现来计算的。<br/>最后，需要做的是优化:</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="eb1d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">值得注意的是，对于策略优化中的每一步，我们在价值函数优化中都采取了许多步骤。这是合理的，因为，如前所述，政策损失并不真正表明改善，但价值函数应该根据迄今为止的最优政策预测每个州的平均回报。</p><p id="5c5d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">奇怪的是，每次观察的批量大小都不同，这取决于典型深度学习的剧集长度。</p><p id="9453" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将学习速率和最大批量长度的默认值设置为与OpenAI相同，我们最终可以测试该算法。对于相同的环境，它的工作原理非常相似。例如:</p><pre class="mo mp mq mr gt mx my mz na aw nb bi"><span id="73f9" class="nc ls jg my b gy nd ne l nf ng">epoch: 145   loss: -0.078   return: -78.000   ep_len: 79.000<br/>epoch: 146   loss: 0.098   return: -110.000   ep_len: 111.000<br/>epoch: 147   loss: -0.017   return: -96.000   ep_len: 97.000<br/>epoch: 148   loss: 0.058   return: -92.000   ep_len: 93.000<br/>epoch: 149   los: -0.033   return: -103.000   ep_len: 104.000</span></pre><p id="a654" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，对于OpenAI gym的“Acrobot-v1”环境，它转换为大约相同的平均回报。或者对于“CartPole-v1”:</p><pre class="mo mp mq mr gt mx my mz na aw nb bi"><span id="94f9" class="nc ls jg my b gy nd ne l nf ng">epoch: 695   loss: 0.003   return: 500.000   ep_len: 500.000<br/>epoch: 696   loss: -0.032   return: 377.000   ep_len: 377.000<br/>epoch: 697   loss: -0.006   return: 500.000   ep_len: 500.000<br/>epoch: 698   loss: -0.008   return: 500.000   ep_len: 500.000<br/>epoch: 699   loss: -0.010   return: 500.000   ep_len: 500.000</span></pre><p id="7665" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它只适用于离散的操作空间，没有并行化，但那是我的任务——进行将简单策略转换为VPG所需的最小更改。</p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><h1 id="a42c" class="lr ls jg bd lt lu no lw lx ly np ma mb km nq kn md kp nr kq mf ks ns kt mh mi bi translated">结论</h1><p id="739f" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">总而言之，很明显，从零开始实现VPG比像我一样将两个实现结合起来更具挑战性。但是，我仍然认为这是理解OpenAI的简单策略和VPG实现的一个非常有用的练习。我希望最终的脚本更容易理解，但代价是不那么高效和优雅。我认为它让你从零开始为更具挑战性的RL算法的实现做更充分的准备。我打算继续做下去，做成一个系列。你可以在这里找到这篇文章<a class="ae jd" href="https://github.com/DmytroSytro/spinningup/tree/master/spinup/algos/pytorch/vpg" rel="noopener ugc nofollow" target="_blank">的所有代码。</a></p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><p id="5d3c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">特别感谢<a class="ae jd" href="https://www.linkedin.com/in/ilona-sereda-539bb515a/" rel="noopener ugc nofollow" target="_blank"> Ilona Sereda为本文制作了精彩的插图和信息图！</a></p><h1 id="35f9" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">参考</h1><p id="b642" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">[1] J .舒尔曼，p .莫里茨，s .莱文，m .乔丹，p .阿贝耳(2016)，<a class="ae jd" href="https://arxiv.org/pdf/1506.02438.pdf" rel="noopener ugc nofollow" target="_blank">利用广义优势估计的高维连续控制，</a> ICLR 2016</p><div class="ip iq gp gr ir nt"><a href="https://spinningup.openai.com/en/latest/" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd jh gy z fp ny fr fs nz fu fw jf bi translated">欢迎来到极速旋转！-编制文件</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">编辑描述</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">spinningup.openai.com</p></div></div><div class="oc l"><div class="od l oe of og oc oh ix nt"/></div></div></a></div></div></div>    
</body>
</html>