<html>
<head>
<title>Text Summarization with GloVe Embeddings..</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有手套嵌入的文本摘要..</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-summarization-with-glove-embeddings-1e969ef9a452?source=collection_archive---------21-----------------------#2020-05-10">https://towardsdatascience.com/text-summarization-with-glove-embeddings-1e969ef9a452?source=collection_archive---------21-----------------------#2020-05-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d219" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于注意力的LSTM编解码器模型的手套嵌入文本摘要。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f03575e35bc224b7cbee83cd4fe36df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nX4A3oJIA_8VpfW7"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安妮·斯普拉特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="6663" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个故事是我们之前的<a class="ae ky" rel="noopener" target="_blank" href="/lets-give-some-attention-to-summarising-texts-d0af2c4061d1">博客文章</a>的延续，在那里我们已经讨论了文本摘要的基础，各种方法以及我们如何实现一个编码器-解码器模型(注意)来解决手头的问题。</p><p id="c9ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单回顾一下，文本摘要是从多种文本资源(如书籍、新闻文章、博客帖子、研究论文、电子邮件和推文)中生成简明而有意义的文本摘要的过程。</p><p id="8fca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经看到编码器-解码器(seqtoseq)模型是摘要任务的完美选择，所以我们将继续使用该架构。除此之外，我们将使用<strong class="lb iu"> GloVe预训练单词嵌入</strong>让我们的模型领先一步，并检查它在理解语言语义和总结方面是否真的表现得更好。</p><h1 id="09c5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是单词嵌入？</h1><p id="936a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">“单词嵌入”是一组旨在将语义映射到几何空间的自然语言处理技术。这是通过将数字向量与字典中的每个单词相关联来实现的，使得任何两个向量之间的距离(例如，L2距离或更常见的余弦距离)将捕获两个相关单词之间的部分语义关系。这些向量形成的几何空间称为<em class="ms">嵌入空间</em>。</p><p id="ddc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，“<strong class="lb iu"> <em class="ms">椰子</em> </strong>”和“<strong class="lb iu"> <em class="ms">北极熊”</em> </strong>是语义完全不同的单词，因此合理的嵌入空间会将它们表示为相距很远的向量。但是“<strong class="lb iu"> <em class="ms">厨房</em> </strong>”和“<strong class="lb iu"> <em class="ms">晚餐</em> </strong>”是关联词，所以应该是嵌得比较近的。</p><p id="5899" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理想情况下，在一个好的嵌入空间中，从“厨房”到“晚餐”的“路径”(一个向量)将精确地捕捉这两个概念之间的语义关系。在这种情况下，关系是“x出现的地方”，因此您会期望向量<code class="fe mt mu mv mw b">kitchen - dinner</code>(两个嵌入向量的差，即从晚餐到厨房的路径)捕捉这种“x出现的地方”关系。基本上，我们应该有向量恒等式:<code class="fe mt mu mv mw b">dinner + (where x occurs) = kitchen</code>(至少近似)。如果事实确实如此，那么我们可以用这样一个关系向量来回答问题。例如，从一个新的向量开始，例如“工作”，并应用这个关系向量，我们应该得到一些有意义的东西，例如<code class="fe mt mu mv mw b">work + (where x occurs) = office</code>，回答“工作发生在哪里？”。</p><p id="b04d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过对文本语料库中单词之间的共现统计数据集应用降维技术来计算单词嵌入。这可以通过神经网络(“word2vec”技术)或矩阵分解来完成。</p><p id="ff42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以玩这个漂亮的<a class="ae ky" href="https://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">张量流投影仪</a>，更好地理解单词嵌入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/a3abfd1ed9a67f8f756cec01d93692ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*A0TXCTFQUhKFYg7cqRHseg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">所有与厨房相关的单词。</p></figure><h1 id="3503" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">为什么要嵌入手套？</h1><p id="47be" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">两个最常见的单词嵌入是:<strong class="lb iu"> Word2Vec </strong>和<strong class="lb iu"> GloVe，</strong>而且这两个都同样受欢迎。但是<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>(单词表示的全局向量)顾名思义更适合保存<strong class="lb iu">全局上下文</strong>，因为它通过估计给定单词与其他单词共现的概率来创建全局共现矩阵。在这里，为了进行总结，全局环境是必要的，所以我们继续使用GloVe，但是在大多数用例中，两者之间很少有选择。</p><p id="1564" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，我们将使用在2014年英语维基百科转储上计算的40万个单词的100维手套嵌入。你可以在这里下载它们<a class="ae ky" href="http://nlp.stanford.edu/data/glove.6B.zip" rel="noopener ugc nofollow" target="_blank">(警告:点击这个链接将开始一个822MB的下载)。</a></p><h1 id="85b3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">编码时间:</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/442dbe5f2621027348be12d211c31f64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fFv2EQ_IvLvA-fWS"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@arianismmm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Arian Darvishi </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d03a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是LSTM编码器-解码器模型的完整<a class="ae ky" href="https://gist.github.com/sayakmisra/9f12e5c86c38c69b2678b8b58e719082" rel="noopener ugc nofollow" target="_blank">代码</a>，添加了注意力和手套嵌入。</p><p id="e57f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不讨论模型架构的细节，因为我们在之前的<a class="ae ky" rel="noopener" target="_blank" href="/lets-give-some-attention-to-summarising-texts-d0af2c4061d1">博客文章</a>中已经讨论过了，并转而关注于向其添加手套嵌入和评估性能。</p><h2 id="7677" class="mz lw it bd lx na nb dn mb nc nd dp mf li ne nf mh lm ng nh mj lq ni nj ml nk bi translated">下载并解压手套</h2><p id="2f0d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">首先，让我们下载并解压缩手套嵌入。</p><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="17c3" class="mz lw it mw b gy np nq l nr ns">!wget 'http://nlp.stanford.edu/data/glove.6B.zip'</span><span id="facb" class="mz lw it mw b gy nt nq l nr ns">!unzip '/content/glove.6B.zip'</span></pre><h2 id="f5b0" class="mz lw it bd lx na nb dn mb nc nd dp mf li ne nf mh lm ng nh mj lq ni nj ml nk bi translated">制备嵌入层</h2><p id="8ec1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">接下来，我们通过解析预训练嵌入的数据转储来计算将单词映射到已知嵌入的索引:</p><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="87b8" class="mz lw it mw b gy np nq l nr ns">embeddings_index = {}<br/>f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))<br/>for line in f:<br/>    values = line.split()<br/>    word = values[0]<br/>    coefs = np.asarray(values[1:], dtype='float32')<br/>    embeddings_index[word] = coefs<br/>f.close()<br/><br/>print('Found %s word vectors.' % len(embeddings_index))</span></pre><p id="df96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此时，我们可以利用我们的<code class="fe mt mu mv mw b">embedding_index</code>字典和<code class="fe mt mu mv mw b">word_index</code>来计算我们的嵌入矩阵:</p><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="91b9" class="mz lw it mw b gy np nq l nr ns">EMBEDDING_DIM = 100<br/>embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))<br/>for word, i in word_index.items():<br/>    embedding_vector = embeddings_index.get(word)<br/>    if embedding_vector is not None:<br/>        # words not found in embedding index will be all-zeros.<br/>        embedding_matrix[i] = embedding_vector</span></pre><p id="2e8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将这个嵌入矩阵加载到一个<code class="fe mt mu mv mw b">Embedding</code>层中。注意，我们设置<code class="fe mt mu mv mw b">trainable=False</code>是为了防止权重在训练过程中被更新。以前我们使用默认的<code class="fe mt mu mv mw b">Embedding</code>层，带有<code class="fe mt mu mv mw b">trainable=True</code>，它通过训练过程学习嵌入。</p><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="f0b0" class="mz lw it mw b gy np nq l nr ns">embedding_layer = Embedding(len(word_index) + 1,<br/>                            EMBEDDING_DIM,<br/>                            weights=[embedding_matrix],<br/>                            input_length=MAX_SEQUENCE_LENGTH,<br/>                            trainable=False)</span></pre><p id="5f02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们使用100维手套嵌入，嵌入保存在glove.6B.100d.txt中。</p><p id="5bff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个<code class="fe mt mu mv mw b">Embedding</code>层应该被输入整数序列，即形状<code class="fe mt mu mv mw b">(samples, indices)</code>的2D输入。这些输入序列应该被填充，以便它们在一批输入数据中具有相同的长度(尽管如果您没有向层传递一个明确的<code class="fe mt mu mv mw b">input_length</code>参数，<code class="fe mt mu mv mw b">Embedding</code>层能够处理不同长度的序列)。</p><p id="e82c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">Embedding</code>层所做的只是将整数输入映射到嵌入矩阵中相应索引处的向量，即序列<code class="fe mt mu mv mw b">[1, 2]</code>将被转换为<code class="fe mt mu mv mw b">[embeddings[1], embeddings[2]]</code>。这意味着<code class="fe mt mu mv mw b">Embedding</code>层的输出将是一个形状为<code class="fe mt mu mv mw b">(samples, sequence_length, embedding_dim)</code>的3D张量。</p><h2 id="7bcf" class="mz lw it bd lx na nb dn mb nc nd dp mf li ne nf mh lm ng nh mj lq ni nj ml nk bi translated">培训绩效:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/a2375c4a8de68665b32bac75f1c7a4de.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*yts5nU-GTrqNNUmfXJ2rHg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">培训和测试损失</p></figure><p id="d107" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们得到大约2.25的验证损失，这与我们在没有手套嵌入的情况下得到的损失相似。</p><h2 id="547a" class="mz lw it bd lx na nb dn mb nc nd dp mf li ne nf mh lm ng nh mj lq ni nj ml nk bi translated">测试:</h2><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="6084" class="mz lw it mw b gy np nq l nr ns"><strong class="mw iu">Review</strong>: tried several tassimo discs gevalia best coffee ever tasted coffee snob love coffee  <br/><strong class="mw iu">Original summary:</strong> great coffee  <br/><strong class="mw iu">Predicted summary:  </strong>best coffee ever</span><span id="2e45" class="mz lw it mw b gy nt nq l nr ns"><strong class="mw iu">Review</strong>: almonds unbelievable amount salt love salt top bought brand stores never salty maybe machine went wacko processing cannot eat unless take time try wipe salt talks forever  <br/><strong class="mw iu">Original summary:</strong> too salty to eat  <br/><strong class="mw iu">Predicted summary:  </strong>not the best</span><span id="5e1d" class="mz lw it mw b gy nt nq l nr ns"><strong class="mw iu">Review:</strong> sure much nourishment hair getting shampoo dandruff nice job cleaning hair leaving tangle free somewhat conditioned using pantene pro shampoo compares favorably pantene like way bottle made easy open dispensing far like way hair coming shampooing seem little better started used bottle seen little improvement really much improvement would say shampoo would worth trying nice job cleaning smells nice bottle easy use relatively inexpensive keep mind make many shampoos one person likes another hate  <br/><strong class="mw iu">Original summary</strong>: seems like an excellent shampoo  <br/><strong class="mw iu">Predicted summary:</strong>  great for hair</span></pre><p id="3ec2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的例子中我们可以看到，它在大多数用例中表现得相当好。</p><p id="cb1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里是完整实现的<a class="ae ky" href="https://gist.github.com/sayakmisra/9f12e5c86c38c69b2678b8b58e719082" rel="noopener ugc nofollow" target="_blank"> colab-notebook </a>。</p><h1 id="0feb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">使用手套嵌入真的能提高性能吗？</h1><p id="3d9c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">长话短说，<strong class="lb iu">号</strong></p><p id="9eed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然该模型可能在某些情况下产生更好的摘要，但不能肯定它在所有示例中都有更好的结果。这背后的主要原因是我们在这里有足够的记录(准确地说是100000条)供我们在编码器-解码器模型中的嵌入层学习语言的语义，所以即使没有预先训练的嵌入，它也表现得相当好。</p><h1 id="e983" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">那么，我们什么时候应该使用预先训练好的嵌入呢？</h1><p id="b80d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如果我们有一个短的数据集，预训练模型(如GloVe)可以在结果上有很大的改进。如果数据集中的记录数量较少，嵌入层将无法生成自己的正确嵌入，在这种情况下，使用预先训练的嵌入将提高性能准确性。</p><h1 id="bc42" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="0d1c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这里，我们已经看到了如何将预训练嵌入添加到现有的LSTM编码器-解码器架构中，虽然结果没有出现太多尖峰，但它可以在较小的数据集上表现出色。</p><p id="b7af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们能够利用<strong class="lb iu">迁移学习</strong>的真正精髓，通过使用预训练语言模型的状态，如:<strong class="lb iu"/>，<strong class="lb iu">巴特</strong>，<strong class="lb iu"> T5 </strong>，我们可以进一步大幅改进总结过程。在我们的下一个故事中，我们将深入研究这些模型的细节，以及它们如何精彩地概括文本。</p><h1 id="079f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><ol class=""><li id="6488" class="nv nw it lb b lc mn lf mo li nx lm ny lq nz lu oa ob oc od bi translated"><a class="ae ky" href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/snap/amazon-fine-food-reviews</a></li><li id="7cb9" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><a class="ae ky" href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html" rel="noopener ugc nofollow" target="_blank">https://blog . keras . io/using-pre-trained-word-embedding-in-a-keras-model . html</a></li><li id="f101" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><a class="ae ky" href="https://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">https://projector.tensorflow.org/</a></li></ol></div></div>    
</body>
</html>