<html>
<head>
<title>Are you still using sklearn for Regression Analysis?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你还在用sklearn做回归分析吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/are-you-still-using-sklearn-for-regression-analysis-fb06bb06ce96?source=collection_archive---------24-----------------------#2020-05-25">https://towardsdatascience.com/are-you-still-using-sklearn-for-regression-analysis-fb06bb06ce96?source=collection_archive---------24-----------------------#2020-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fda0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当谈到Python中的经典机器学习算法时，sklearn是第一个要去的包——还有其他你应该知道的。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cf79022ba3bcf204fd9a95d97f0af57b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tD8NOMaPMSPlQO4t"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安东·达利斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="b0c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于大多数数据科学家来说，当谈到Python中的经典机器学习算法时，sklearn是第一个首选包。但是这并不意味着没有其他优秀的软件包可以用来建模。我在本文中展示的软件包主要与回归分析有关，许多数据科学家不合理地将回归分析与简单化的建模联系起来。</p><p id="71da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">这里有几个你可能会感兴趣的链接:</strong></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="36b0" class="ma mb it lw b gy mc md l me mf">- <a class="ae ky" href="https://www.humanfirst.ai/" rel="noopener ugc nofollow" target="_blank">Labeling and Data Engineering for Conversational AI and Analytics</a></span><span id="4cac" class="ma mb it lw b gy mg md l me mf">- <a class="ae ky" href="https://imp.i115008.net/c/2402645/880006/11298" rel="noopener ugc nofollow" target="_blank">Data Science for Business Leaders</a> [Course]</span><span id="dff9" class="ma mb it lw b gy mg md l me mf">- <a class="ae ky" href="https://imp.i115008.net/c/2402645/788201/11298" rel="noopener ugc nofollow" target="_blank">Intro to Machine Learning with PyTorch</a> [Course]</span><span id="2a1c" class="ma mb it lw b gy mg md l me mf">- <a class="ae ky" href="https://imp.i115008.net/c/2402645/803127/11298" rel="noopener ugc nofollow" target="_blank">Become a Growth Product Manager</a> [Course]</span><span id="cf8c" class="ma mb it lw b gy mg md l me mf">- <a class="ae ky" href="https://amzn.to/3ncTG7D" rel="noopener ugc nofollow" target="_blank">Deep Learning (Adaptive Computation and ML series)</a> [Ebook]</span><span id="c588" class="ma mb it lw b gy mg md l me mf">- <a class="ae ky" href="https://aigents.co/skills" rel="noopener ugc nofollow" target="_blank">Free skill tests for Data Scientists &amp; Machine Learning Engineers</a></span></pre><p id="3897" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的一些链接是附属链接，如果你通过它们进行购买，我会赚取佣金。请记住，我链接课程是因为它们的质量，而不是因为我从你的购买中获得的佣金。</p><blockquote class="mi"><p id="f517" class="mj mk it bd ml mm mn mo mp mq mr lu dk translated">大多数认为他们需要高级人工智能/人工智能的公司实际上只需要对清理后的数据进行线性回归。罗宾·汉森发推文。</p></blockquote><p id="dd37" class="pw-post-body-paragraph kz la it lb b lc ms ju le lf mt jx lh li mu lk ll lm mv lo lp lq mw ls lt lu im bi translated">虽然我在上面的推文中感觉到了讽刺，但掌握线性回归的主要假设远远不是简单的(例如，多重共线性，多元正态性，同方差性等)。).许多数据科学家不知道的另一件事是，线性回归和逻辑回归是同一算法家族的一部分——广义线性模型(GLM)。</p><h1 id="cc5b" class="mx mb it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">认识统计模型</h1><p id="d8d9" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">我一直很喜欢R在训练一个回归模型后给你一个详细的输出。一个提供类似输出的Python包是<a class="ae ky" href="http://statsmodels" rel="noopener ugc nofollow" target="_blank">stats models</a>——一个用于估计许多不同统计模型，以及进行统计测试和统计数据探索的包。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/04303d20d72a3fcd0cdcb177ad9fcdee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*w1MnEZz8nGnpjFSK.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">r中训练的回归模型的详细摘要。</p></figure><p id="3056" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下图中，我们可以观察到用statsmodels训练的回归模型的摘要。看起来和r里面的差不多，甚至还会显示警告，这是相对于sklearn的另一个优势。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/ad51d8d3e752ac0a6f8c0445fec21638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xw2do0Rd80M2NYkF-IzSVw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用statsmodels训练的回归模型的摘要。</p></figure><h1 id="1613" class="mx mb it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">广义线性模型</h1><p id="290f" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">如上所述，逻辑回归和线性回归是广义线性模型的一部分。GLM通过链接函数将线性模型与响应变量相关联，并允许每个测量值的方差大小是其预测值的函数，从而推广了线性回归。广义线性模型由约翰·内尔德和罗伯特·威德伯恩提出，作为一种统一各种其他统计模型的方式，包括线性回归、逻辑回归和泊松回归。</p><p id="29bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想更多地了解GLM，请听一听伟大的吴恩达的演讲:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">吴恩达关于广义线性模型的讲座</p></figure><p id="56b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GLM让你使用概率分布作为建模的基础。让我们看一个使用statsmodels的逻辑回归的例子:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c72f" class="ma mb it lw b gy mc md l me mf">import statsmodels.api as sm</span><span id="ac24" class="ma mb it lw b gy mg md l me mf">model = sm.GLM(y_train, x_train, family=sm.families.Binomial(link=sm.families.links.logit()))</span></pre><p id="5bb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，逻辑回归是用二项式概率分布和Logit链接函数定义的。注意，y_train是一个带有目标变量的数组，x_train表示一个特征数组。</p><p id="fca4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">默认情况下，statsmodels中不添加截距列(1的列)。我们可以添加以下内容:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="83b3" class="ma mb it lw b gy mc md l me mf">sm.add_constant(x_train)</span></pre><p id="ec9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要用线性回归(普通最小二乘回归)代替逻辑回归，我们只需要改变家族分布:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="eda6" class="ma mb it lw b gy mc md l me mf">model = sm.GLM(y_train, x_train, family=sm.families.Gaussian(link=sm.families.links.identity()))</span></pre><p id="b3e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种常用的回归是泊松回归，它假设目标变量具有泊松分布。它被用来模拟计数变量，比如在学校患感冒的人数。同样，使用statsmodels，我们只需要更改家族分布:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6f57" class="ma mb it lw b gy mc md l me mf">model = sm.GLM(y_train, x_train, family=sm.families.Poisson())</span></pre><p id="d368" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然sklearn中有<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank">线性回归</a>、<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>和<a class="ae ky" href="https://scikit-learn.org/dev/modules/generated/sklearn.linear_model.PoissonRegressor.html#sklearn.linear_model.PoissonRegressor" rel="noopener ugc nofollow" target="_blank">泊松回归</a>，但我真的很喜欢statsmodels API的实现方式。它还提供了许多定制，我将在下一篇关于statsmodels的文章中谈到这些。</p><h1 id="62a7" class="mx mb it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">下降趋势</h1><p id="df7a" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">在sklearn上使用statsmodels的一个缺点是它不支持稀疏矩阵。这使得用许多稀疏特征来训练模型是不可行的，例如用单词包方法转换的文本特征。</p><p id="a0f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">sklearn文档优于statsmodels文档。</p><p id="819b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总的来说，sklearn似乎比statsmodels更优化。</p><h1 id="2da4" class="mx mb it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">参考</h1><ul class=""><li id="e99c" class="nx ny it lb b lc no lf np li nz lm oa lq ob lu oc od oe of bi translated">[1] <a class="ae ky" href="https://en.wikipedia.org/wiki/Generalized_linear_model" rel="noopener ugc nofollow" target="_blank">广义线性模型</a></li></ul><h1 id="d68c" class="mx mb it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">在你走之前</h1><p id="dea3" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">在<a class="ae ky" href="https://twitter.com/romanorac" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我，在那里我定期<a class="ae ky" href="https://twitter.com/romanorac/status/1328952374447267843" rel="noopener ugc nofollow" target="_blank">发布关于数据科学和机器学习的</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/b5d426b68cc5a21b1a35d0a157ebc4f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*69rP1pwjJi9mLSFE"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@cmhedger?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">考特尼·海杰</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure></div></div>    
</body>
</html>