<html>
<head>
<title>Hyperparameter Tuning with Informed Searching</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于信息搜索的超参数调整</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-with-informed-searching-37ed58d331f8?source=collection_archive---------31-----------------------#2020-03-11">https://towardsdatascience.com/hyperparameter-tuning-with-informed-searching-37ed58d331f8?source=collection_archive---------31-----------------------#2020-03-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/eeae0994c60147c60a28dd83f10349c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-2aVPLO2efOKE50-P_EDUQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">威尔·弗朗西斯在<a class="ae jg" href="https://unsplash.com/s/photos/radio?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><p id="6d0c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">咻…已经几个星期了，但是很高兴赶上了！这些帖子的目标是希望找到一些对我来说是新的东西，这样我就可以“向前支付”。你们中的许多人可能听说过GridSearchCV，甚至可能听说过RandomSearchCV，但是知情搜索呢？</p><p id="dd87" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用信息搜索技术的优势在于超参数是通过顺序学习来调整的。GridSearch和RandomSearch都很棒，但它们只深入一层。知情搜索从先前的超参数调整中学习，以优化调整过程。有三种方法是我知道的，请随意分享其他方法！</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi le"><img src="../Images/08f00d15fbf143ac5e3b67f72b407ba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zS7yR4o3pmYef3JHUgSW2g.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">伊恩·施耐德在<a class="ae jg" href="https://unsplash.com/s/photos/sanding?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="548c" class="lj lk jj bd ll lm ln dn lo lp lq dp lr kr ls lt lu kv lv lw lx kz ly lz ma mb bi translated">粗调至微调</h2><p id="abf7" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这是微调超参数最明显的方法。只有四个步骤:</p><ol class=""><li id="1e3c" class="mh mi jj ki b kj kk kn ko kr mj kv mk kz ml ld mm mn mo mp bi translated">执行随机搜索(或网格搜索)。</li><li id="9555" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld mm mn mo mp bi translated">查看结果。</li><li id="f32f" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld mm mn mo mp bi translated">根据检查结果定义新的参数范围。</li><li id="3bd6" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld mm mn mo mp bi translated">继续，直到获得最佳得分。</li></ol><p id="43d4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，这是非常明显的，但最容易实现，无需安装任何软件包。这也是最耗时的。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mv"><img src="../Images/f1ceed11c117958b28e53885015151ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ioDQpI7W2LaqR8Ay5BQ5A.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">贝叶斯定理</p></figure><h2 id="57b0" class="lj lk jj bd ll lm ln dn lo lp lq dp lr kr ls lt lu kv lv lw lx kz ly lz ma mb bi translated">贝叶斯调谐</h2><p id="0793" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">是的，贝氏又来了！该方法将采用一系列超参数，并利用贝叶斯信念原则来迭代超参数，以提供最佳结果。<a class="ae jg" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank">hyperpt</a>套装提供您需要的一切！Parzen估计器的<a class="ae jg" href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf" rel="noopener ugc nofollow" target="_blank">树</a> (tpe.suggest)是函数中的算法。此外，还创建了一个目标函数来迭代参数并测量损失。</p><p id="2b18" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是一个例子:</p><pre class="lf lg lh li gt mw mx my mz aw na bi"><span id="8d57" class="lj lk jj mx b gy nb nc l nd ne">from hyperopt import hp, space, fmin, tpe<br/># Set up space dictionary with specified hyperparameters<br/>space = {'max_depth': hp.quniform('max_depth', 2, 10, 2),'learning_rate': hp.uniform('learning_rate', 0.001,0.9)}<br/><br/>  # Set up objective function<br/>def objective(params):<br/>    params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}</span><span id="a52b" class="lj lk jj mx b gy nf nc l nd ne">    # model can be set - scoring must be 'accuracy'<br/>    gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) <br/>      best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()<br/>    loss = 1 - best_score<br/>    return loss</span><span id="f92c" class="lj lk jj mx b gy nf nc l nd ne"># Run the algorithm - test max evals<br/>best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)<br/>  print(best)</span><span id="2e40" class="lj lk jj mx b gy nf nc l nd ne">#Sample output:<br/> 0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]<br/>  5%|5         | 1/20 [00:00&lt;00:04,  4.16it/s, best loss: 0.26759418985474637]<br/> 10%|#         | 2/20 [00:00&lt;00:04,  4.32it/s, best loss: 0.2549063726593165] <br/> 15%|#5        | 3/20 [00:00&lt;00:03,  4.60it/s, best loss: 0.2549063726593165]<br/> 20%|##        | 4/20 [00:00&lt;00:03,  4.82it/s, best loss: 0.2549063726593165]<br/> 25%|##5       | 5/20 [00:01&lt;00:04,  3.64it/s, best loss: 0.2549063726593165]<br/> 30%|###       | 6/20 [00:01&lt;00:03,  3.71it/s, best loss: 0.2549063726593165]<br/> 35%|###5      | 7/20 [00:01&lt;00:03,  4.09it/s, best loss: 0.2549063726593165]<br/> 40%|####      | 8/20 [00:01&lt;00:02,  4.29it/s, best loss: 0.2549063726593165]<br/> 45%|####5     | 9/20 [00:02&lt;00:02,  4.49it/s, best loss: 0.2549063726593165]<br/> 50%|#####     | 10/20 [00:02&lt;00:02,  4.69it/s, best loss: 0.2549063726593165]<br/> 55%|#####5    | 11/20 [00:02&lt;00:01,  4.77it/s, best loss: 0.2549063726593165]<br/> 60%|######    | 12/20 [00:02&lt;00:01,  4.53it/s, best loss: 0.2549063726593165]<br/> 65%|######5   | 13/20 [00:03&lt;00:01,  4.16it/s, best loss: 0.2549063726593165]<br/> 70%|#######   | 14/20 [00:03&lt;00:02,  2.81it/s, best loss: 0.2525688142203555]<br/> 75%|#######5  | 15/20 [00:03&lt;00:01,  3.29it/s, best loss: 0.2525688142203555]<br/> 80%|########  | 16/20 [00:04&lt;00:01,  3.57it/s, best loss: 0.2525688142203555]<br/> 85%|########5 | 17/20 [00:04&lt;00:01,  2.41it/s, best loss: 0.24246856171404285]<br/> 90%|######### | 18/20 [00:05&lt;00:00,  2.41it/s, best loss: 0.24246856171404285]<br/> 95%|#########5| 19/20 [00:05&lt;00:00,  2.46it/s, best loss: 0.24246856171404285]<br/>100%|##########| 20/20 [00:05&lt;00:00,  2.69it/s, best loss: 0.24246856171404285]<br/>100%|##########| 20/20 [00:05&lt;00:00,  3.40it/s, best loss: 0.24246856171404285]<br/>{'learning_rate': 0.11310589268581149, 'max_depth': 6.0}</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ng"><img src="../Images/cef899e726840da0fce1dd6739a8470f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gyZXlRHmXu-BoZiTt1mACw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Johannes Plenio 在<a class="ae jg" href="https://unsplash.com/s/photos/evolution?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="9014" class="lj lk jj bd ll lm ln dn lo lp lq dp lr kr ls lt lu kv lv lw lx kz ly lz ma mb bi translated">遗传调谐</h2><p id="90f0" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">最后，我们将看看基因调整。这是最有趣的概念，因为它遵循达尔文的进化过程:</p><ol class=""><li id="cb87" class="mh mi jj ki b kj kk kn ko kr mj kv mk kz ml ld mm mn mo mp bi translated">不同种类(不同型号)</li><li id="ac96" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld mm mn mo mp bi translated">最强的生存下来(最好的得分被选出)</li><li id="99bc" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld mm mn mo mp bi translated">复制(创建与最佳模型相似的新模型)</li><li id="fb06" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld mm mn mo mp bi translated">遗传随机性发生在繁殖过程中(增加随机性，这样就不会达到局部最优)</li><li id="99ed" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld mm mn mo mp bi translated">重复</li></ol><p id="cbdf" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个过程可以用<a class="ae jg" href="https://epistasislab.github.io/tpot/" rel="noopener ugc nofollow" target="_blank"> TPOT </a>包来执行。有了TPOT，你可以设置所有这些“遗传”参数:</p><ul class=""><li id="4b0f" class="mh mi jj ki b kj kk kn ko kr mj kv mk kz ml ld nh mn mo mp bi translated">世代—周期数</li><li id="6413" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld nh mn mo mp bi translated">population_size =要保留的模型数量</li><li id="3879" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld nh mn mo mp bi translated">后代大小=每个中后代的数量</li><li id="35dc" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld nh mn mo mp bi translated">突变率=应用随机性的管道比例</li><li id="90f1" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld nh mn mo mp bi translated">crossover_rate =每次迭代的管道比例</li></ul><p id="fff6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TPOT是建立在几个库之上的，所以一定要查看文档以确保正确安装。(有一个链接附在第一次提到TPOT的地方)</p><p id="7aaf" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一个TPOT的例子:</p><pre class="lf lg lh li gt mw mx my mz aw na bi"><span id="1a1d" class="lj lk jj mx b gy nb nc l nd ne"># Assign the values outlined to the inputs<br/>number_generations = 3<br/>population_size = 4<br/>offspring_size = 3<br/>scoring_function = 'accuracy'<br/><br/># Create the tpot classifier<br/>tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size,<br/>                            offspring_size=offspring_size, scoring=scoring_function,<br/>                            verbosity=2, random_state=2, cv=2)</span><span id="116b" class="lj lk jj mx b gy nf nc l nd ne"># Fit the classifier to the training data<br/>tpot_clf.fit(X_train, y_train)<br/><br/># Score on the test set<br/>print(tpot_clf.score(X_test, y_test))</span><span id="5f2a" class="lj lk jj mx b gy nf nc l nd ne">#sample output:<br/>    Generation 1 - Current best internal CV score: 0.7549688742218555<br/>    Generation 2 - Current best internal CV score: 0.7549688742218555<br/>    <br/>    Best pipeline: DecisionTreeClassifier(input_matrix, criterion=gini, max_depth=7, min_samples_leaf=11, min_samples_split=12)<br/>    0.75</span></pre><p id="7a35" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你有它！在寻找最佳超参数的过程中，需要实施三个(基本上是两个)新过程。</p><p id="97a2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些过程不会凭空出现，我确实通过<a class="ae jg" href="https://datacamp.com/" rel="noopener ugc nofollow" target="_blank">数据营</a>的课程学到了很多。DataCamp是一个保持练习、学习新技术、巩固已学知识和发现新程序的好方法。强烈推荐。</p><p id="d9cb" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">保重，下次再见！</p></div></div>    
</body>
</html>