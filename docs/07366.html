<html>
<head>
<title>Logistic Regression from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-from-scratch-69db4f587e17?source=collection_archive---------4-----------------------#2020-06-04">https://towardsdatascience.com/logistic-regression-from-scratch-69db4f587e17?source=collection_archive---------4-----------------------#2020-06-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6b7c1496912b6c099afb85938b684f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPI0uutvHKDFGhYe8UCuVw.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">鸣谢:法比奥·罗斯</p></figure><h1 id="9380" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">介绍</h1><p id="1ef0" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在这篇文章中，我们将使用梯度下降从头开始构建我们自己的逻辑回归模型。为了测试我们的模型，我们将使用来自<strong class="lc ir"> sklearn </strong>包的“乳腺癌威斯康星州数据集”,并以超过 95%的准确度预测肿块是良性还是恶性。GitHub 回购在这里是<a class="ae ly" href="https://github.com/arseniyturin/logistic-regression" rel="noopener ugc nofollow" target="_blank"/>。所以让我们开始吧。</p><h1 id="b9d7" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">模型核心</h1><p id="fb09" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">实质上，逻辑回归模型由两个部分组成:sigmoid 函数和具有权重的特征:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lz"><img src="../Images/b204f1bbdbb1e5474f4330d462ebf4e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HwoyV8PkILVCSQdlc-VrrA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">Sigmoid 函数</p></figure><p id="f282" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">sigmoid 函数<strong class="lc ir"> <em class="mj"> g(z) </em> </strong>将特征和权重<strong class="lc ir"> <em class="mj"> z </em> </strong>作为输入，并返回介于 0 和 1 之间的结果。sigmoid 函数的输出是实际预测<strong class="lc ir"> <em class="mj"> ŷ </em> </strong> <em class="mj">。</em></p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/b976339637b35eef460e197a8e57c7ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWyU3Xo_pjJpDBnnp789Sw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">特征和重量</p></figure><p id="ce2b" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">在模型做出预测之后，我们可以用交叉熵损失函数来评估结果:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/0dd50d710e2711a91fdae43dfc30010e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b34ODnrbJpAfOpy2PQcUJQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">二元交叉熵损失函数</p></figure><p id="51f9" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">自然对数在这里对我们有利，因为如果预测值与真实值相差很远，它会受到很大的惩罚。例如，如果模型预测值<strong class="lc ir"><em class="mj"/></strong>和真实值<strong class="lc ir"> <em class="mj"> y=1 </em> </strong>，则误差高，反之亦然:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mm"><img src="../Images/93fc154a41205ee4159343dcf515f2b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ohzMSQSEMHkcMt2Mz7GdNw.png"/></div></div></figure><p id="54fb" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">损失函数由两部分组成，但我们可以将它们合并成一个等式:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/13ca5f9b51d9fcedad9b5b24a89805ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h9idzK6YjMKKxfCl-Zq8eA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">二元交叉熵损失函数</p></figure><p id="1278" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">这里我们添加了<strong class="lc ir"> <em class="mj"> y </em> </strong>和<strong class="lc ir"> <em class="mj"> (1 - y) </em> </strong>来根据输出抵消一部分。我们将在我们的模型中使用该函数来计算损失，并在模型训练的梯度下降部分使用该函数。</p><h1 id="499b" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">模特培训</h1><p id="022b" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">模型训练本质上是损失函数的最小化。我们通过梯度下降技术实现了这一点，该技术可以分为几个步骤:</p><ol class=""><li id="bbe5" class="mo mp iq lc b ld me lh mf ll mq lp mr lt ms lx mt mu mv mw bi translated">首先，我们找到损失函数相对于每个权重的导数。导数可以告诉我们应该向哪个方向改变权重，以及改变多少，以使模型损失更小一些。</li><li id="2972" class="mo mp iq lc b ld mx lh my ll mz lp na lt nb lx mt mu mv mw bi translated">根据导数更新每个权重，直到找到局部最小值，即模型不再改进，因此我们可以停止。</li></ol><h2 id="3523" class="nc kd iq bd ke nd ne dn ki nf ng dp km ll nh ni kq lp nj nk ku lt nl nm ky nn bi translated">派生物</h2><p id="7cc1" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这是最关键也是最难的部分。没有导数，模型就不能训练，所以我们将在这一部分详细讨论。</p><p id="7724" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">此时，我们可以放下求和函数，专注于内部内容:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/b46fac324fa515831501cb79642ccad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RMHdxX2NKc5tpCe97CPjYw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">1 个样本的损失函数</p></figure><p id="f2db" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">符号:<strong class="lc ir"> <em class="mj"> y </em> </strong> —真值，<strong class="lc ir"><em class="mj"/></strong>—预测值(sigmoid)</p><p id="0b9b" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">为了找到导数，我们将使用<strong class="lc ir">链式法则</strong>:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/fc72cdaa63f04e40e1e14a1e0f753043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lm78vLno4CLJezjHtpSznA.png"/></div></div></figure><p id="3c8e" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">当我们需要找到一个包含另一个函数的函数的导数时，我们使用链式法则，等等。在我们的例子中，我们有一个包含 sigmoid 函数的损失函数，该函数包含特征和权重。所以有三个<strong class="lc ir">函数，我们将一个接一个地推导它们。</strong></p><h2 id="3f43" class="nc kd iq bd ke nd ne dn ki nf ng dp km ll nh ni kq lp nj nk ku lt nl nm ky nn bi translated">1.链条中的一阶导数</h2><p id="9e7e" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">自然对数的导数很容易计算:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/56e944963d6d651807a0c0744a173dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*el-aYa96OrA3MuIIbe7BRg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">自然对数的导数</p></figure><p id="b846" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">由于等式中有两个部分，我们可以分别导出它们:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/796d2f57483d4c3a401238f69837835f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WwXWsfvWxH_Ds5uaArNq_w.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">对数函数的导数</p></figure><p id="e75b" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">现在，我们可以将两部分重新组合在一起，并进行简化:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/9dd9846db9e29e00061e200f10159363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SS7Q3_hp3tmFqfC8A14CiA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">对数导数的简化</p></figure><p id="1bf9" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">这看起来很好，现在我们把注意力放在 sigmoid 函数上。</p><h2 id="d883" class="nc kd iq bd ke nd ne dn ki nf ng dp km ll nh ni kq lp nj nk ku lt nl nm ky nn bi translated">2.链中的二阶导数</h2><p id="28b6" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">根据链式法则，我们要找到<strong class="lc ir"><em class="mj">【ŷ】</em></strong><em class="mj">的导数。你可以在网上找到它的导数的详细解释，因为它在机器学习模型中经常使用，所以我只写下最终结果:</em></p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/fd94a5444de99815a366554f1dfeedd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_fhDH9l6YFkZzQkYsPymgg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">Sigmoid 导数</p></figure><h2 id="ff4f" class="nc kd iq bd ke nd ne dn ki nf ng dp km ll nh ni kq lp nj nk ku lt nl nm ky nn bi translated">3.链条中的三阶导数</h2><p id="c269" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">链中的最后一个函数是包含在<strong class="lc ir"> <em class="mj"> z </em> </strong>中的内容——我们的特征和权重。对于每个权重的导数(<strong class="lc ir"> <em class="mj"> w </em> </strong>)将是其特征值(<strong class="lc ir"> <em class="mj"> x </em> </strong>)，例如:<em class="mj"> (x1 * w1)' = x1 </em>，因为<strong class="lc ir"> <em class="mj"> w </em> </strong>的导数是 1。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/f8136ff2975e97df4a0cb9d3b807f093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XI6bDHDkN3713CM11jNe8w.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">x*w 的导数</p></figure><p id="b6fb" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">根据链式法则，我们将每个衍生函数相乘，因此得到以下结果:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/1e1a059857f0f66e7507f7d9d4b08144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YZrH4piWgKSm0F_iC2baYQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">最终方程</p></figure><p id="f40a" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">我们完了。最后，我们得到了一个非常紧凑的函数，我们将在梯度下降中使用它。</p><h2 id="f78b" class="nc kd iq bd ke nd ne dn ki nf ng dp km ll nh ni kq lp nj nk ku lt nl nm ky nn bi translated">梯度下降</h2><p id="f8ce" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">与寻找导数相比，这部分是轻而易举的。在程序员的语言中，这只是循环的<strong class="lc ir">,在这里我们不断地更新权重。</strong></p><p id="6bd6" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">在此过程中，我们将每个导出值乘以学习率参数，然后从权重中减去该值:</p><pre class="ma mb mc md gt nw nx ny nz aw oa bi"><span id="4b4e" class="nc kd iq nx b gy ob oc l od oe"># example of updating one weight</span><span id="28e1" class="nc kd iq nx b gy of oc l od oe">epochs = 50<br/>lr = 0.1</span><span id="eef8" class="nc kd iq nx b gy of oc l od oe">for _ in range(epochs):<br/>    <br/>    w1 -= lr * x1 * (y_hat - y) # y_hat is predicted value</span></pre><p id="7560" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">学习率通常是一个很小的数字，用来控制我们向最小化移动的快慢。让我们来看看完整的代码:</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="og oh l"/></div></figure><h1 id="a25f" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">估价</h1><p id="d4ca" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">乳腺癌威斯康星数据集 569 个样本和 30 个特征:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oi"><img src="../Images/d1c98499601287db67af624eff695408.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kKFN6ibeyy36pKSyYcxRSQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">乳腺癌数据集</p></figure><p id="47c2" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">经过训练测试后，分裂模型以 97%和 95%的准确度预测了恶性和良性肿块，这是一个不错的结果。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/ebc3ff46d4187bb51c172026a6bb2408.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OZdLTStaX3o6wK1VNhDeKA.png"/></div></div></figure><h1 id="7156" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">结论</h1><p id="e570" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我们创建的模型可以用于现实生活中的应用，而且它非常适合教育目的。虽然物流回收会更快更准确，但我们可以不断优化现有资源，最终达到类似的效果。如果你有任何问题或建议，请在评论中告诉我。</p><p id="00cf" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">感谢您的阅读，</p><p id="af49" class="pw-post-body-paragraph la lb iq lc b ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt mi lv lw lx ij bi translated">阿瑟尼。</p></div></div>    
</body>
</html>