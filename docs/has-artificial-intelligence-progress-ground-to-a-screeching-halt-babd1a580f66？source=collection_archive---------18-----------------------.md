# 人工智能的进步已经嘎然而止了吗？

> 原文：<https://towardsdatascience.com/has-artificial-intelligence-progress-ground-to-a-screeching-halt-babd1a580f66?source=collection_archive---------18----------------------->

## 还有希望让它重新加速吗？

![](img/d617a25bf48aa98cb9c0bbbb3483bcbf.png)

[https://images.app.goo.gl/4VCmrSEiri3LkGbVA](https://images.app.goo.gl/4VCmrSEiri3LkGbVA)

我们刚刚进入 2020 年，这个世界看起来并不像拥有飞行汽车和机器人女佣的杰森一家。随着周围所有关于人工智能接管我们的生活，用机器人取代我们的谈论和炒作，我们应该都失业了，无家可归了，但我们没有。

为什么？

根据 OpenAI 的一项研究，人工智能训练中使用的计算能力每 3.4 个月翻一倍。这是一个巨大的，几乎不可能的加速度，标准级数是不习惯的。

计算中使用的一个概念称为摩尔定律，它估计计算能力每两年翻一番，上述计算产生的结果是摩尔定律的七倍。

**七次！！！**

OpenAI 认为，人工智能需要将计算能力提高 30 万倍才能利用这项技术。这种趋势是不可持续的，人工智能的进步正因此受到打击。研究表明，计算能力越强，人工智能系统的性能就越好。

由于计算能力的实际限制，人工智能的发展受到了阻碍。OpenAI 指出，简单的物理学将限制用于驱动和训练人工智能系统的芯片的潜在效率。

那么，这是否意味着人工智能将很快在某个时候完全停止发展？也许除非研究人员发现绕过这一限制的新方法。下面我们来看看几个潜在的突破性发现。

> “这种发展速度是不可持续的，如果你看看顶级实验，每年的成本都会上涨 10 倍。现在，一项实验可能要 7 位数，但不会达到 9 或 10 位数，这是不可能的，没人能负担得起。”詹姆斯·派森提，脸书人工智能的负责人。

# 量子计算

所有的计算机都依靠一种基本的能力来存储和处理单个比特的信息。这些位是使用 0 或 1 编码的长字符串。量子计算机使用更小的量子比特。它们基于一种叫做叠加的原理运行，这种原理允许同时处于多种状态。

那么，这对 AI 有什么影响呢？

标准计算机可以使用 0 和 1 来执行计算，而量子计算机的优势是通过叠加使用 0、1 和量子位来同时进行计算。

所有的大玩家都在利用量子计算的力量，他们相信这是解决当前人工智能局限性的未来。谷歌甚至宣布，它已经实现了“量子优势”,开发了一种能够解决标准计算机需要 10，000 多年才能完成的计算的量子处理器。

亚马逊宣布了一项提供量子计算服务的计划，类似于它提供亚马逊网络服务的方式。

这两家公司都可能代表着人工智能克服当前限制和能力的潜在进展。有朝一日，人们可能会在人工智能机器中发展出像真人一样思考、感受和行为的能力。诚然，我们离人类的这种反应还很远，但这并非不可能。

# 元学习

这个概念最初是由唐纳德·b·莫德斯利在 1979 年描述为一个过程，在这个过程中，学习者意识到并越来越多地控制他们已经内化的感知、探究、学习和成长的习惯。

换句话说，学习者学习他们如何学习。他们适应新环境，并通过一些培训实例学习新技能。实现这一点的三种最常见的方法是:

1.学习有效的距离度量。(基于公制)

2.使用带有外部或内部存储器的循环网络(基于模型)

3.显式优化模型参数以实现快速学习。(基于优化)

适应过程发生在测试过程中，但是对新任务配置的暴露是有限的。适应的模型在学会如何学习后，在运行多次后将完成新的任务。

任务可以是任何明确定义的机器学习问题，包括监督学习和强化学习。

一个好的元学习模型应该在各种学习任务上进行训练，并在任务分配上进行优化以获得最佳性能。每个任务都与包含特征向量和真实标签的数据集相关联。

# 结论

人工智能在进步方面开始放缓，需要一种新的方式来保持跳跃式增长。缺乏足够的计算能力是人工智能受到阻碍的主要原因之一。脸书人工智能负责人 Jerome Pesenti 告诉《连线》杂志，深度学习在可以扩大规模并给予更多操作空间时效果最佳。他还理论说，人工智能和机器学习的发展即将‘碰壁’。

我们之前讨论的两种有前途的技术被视为解决这个问题的最佳方案。量子计算将增加计算能力，比标准计算机能进行更多的计算。元学习将通过自学新技能或快速适应新环境来使人工智能变得更聪明。这些潜在突破中的任何一个都将改变人类的游戏规则。我们只能等着看哪一种能够被开发出来，并提供最好的机会来解决世界上最大的问题。