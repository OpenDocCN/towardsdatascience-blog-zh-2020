<html>
<head>
<title>Predicting Boston House prices using Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用线性回归预测波士顿房价</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-boston-house-prices-using-linear-regression-3c6107c662e5?source=collection_archive---------25-----------------------#2020-05-02">https://towardsdatascience.com/predicting-boston-house-prices-using-linear-regression-3c6107c662e5?source=collection_archive---------25-----------------------#2020-05-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="050a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">线性、多元回归和正则化(LASSO、岭回归)</h2><div class=""/><div class=""><h2 id="18df" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">线性回归:理论与应用</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d57bfe876d32d95053a98341c36591e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqQ-2Q2RdkK_bxIVVFfdAw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://www.pexels.com/@pixabay" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>经【pexels.com】T2</p></figure><h1 id="e928" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="8944" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">什么是线性回归？它是一种预测性建模技术，可以发现自变量和因变量(连续变量)之间的关系。<em class="mw">自变量</em>(<em class="mw">iv)</em>s可以是<em class="mw">分类的</em>(如美国，英国，0/1)或<em class="mw">连续的</em> (1729，3.141等)，而<em class="mw">因变量(dv) </em> s是连续的。底层函数映射<em class="mw"> iv </em>和<em class="mw"> dv </em>可以是线性、二次、多项式或其他非线性函数(如逻辑回归中的sigmoid函数)，但本文讨论的是线性技术。</p><blockquote class="mx my mz"><p id="4dd5" class="ma mb mw mc b md na kd mf mg nb kg mi nc nd ml mm ne nf mp mq ng nh mt mu mv im bi translated">回归技术在房地产价格预测、金融预测、交通到达时间(ETA)预测中被大量使用。</p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/cf07e3ef802db937f7865e9f5862ea3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*15DDrvDIlV0otIZ5J375cw.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://towardsdatascience.com/@NotAyushXD" rel="noopener" target="_blank">阿尤什·潘特</a>经<a class="ae lh" rel="noopener" target="_blank" href="/introduction-to-linear-regression-and-polynomial-regression-f8adc96f31cb">TowardsDataScience.com</a></p></figure><h1 id="0cce" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">变量的类型</h1><p id="0b61" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><em class="mw">分类</em>:取不同的值:垃圾邮件/非垃圾邮件、糖尿病+ve/-ve</p><p id="ed98" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated"><em class="mw">连续</em>:可以取无限个值，如金钱、时间、重量。</p><p id="4b2c" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated"><em class="mw">依赖</em>:实验结果，本博客房屋价值</p><p id="b93c" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">独立变量:与研究者的行为无关的变量。面积、位置、卧室数量等。</p><p id="adae" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">如需更多参考，请查看:<a class="ae lh" href="https://www.statisticshowto.com/probability-and-statistics/types-of-variables/" rel="noopener ugc nofollow" target="_blank"> statistichowto </a>。</p><h1 id="093e" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">线性回归技术</h1><h2 id="1c30" class="nj lj it bd lk nk nl dn lo nm nn dp ls mj no np lu mn nq nr lw mr ns nt ly iz bi translated">1.普通最小二乘法(OLS)</h2><p id="4889" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在<em class="mw"> OLS </em>中，目标是通过数据点找到最佳拟合线。<em class="mw">最佳拟合线</em>通过最小化距离预测线的数据平方距离的总和获得。这是一个<em class="mw">无偏估计</em>(尽管方差没有最小化)，因为它最小化了该观察空间中数据集的偏差。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/bb3b94b1115f9acc4cdc3ab5eb48f334.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*IsGdUYLwFAPjo3Ok1vxy5w.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">最小二乘法的目标</p></figure><p id="13c0" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated"><em class="mw">关键假设</em>:</p><p id="2abc" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">(一)OLS假设<em class="mw"> dv </em>和<em class="mw"> iv成线性关系。</em></p><p id="4ad7" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">(ii)假设<em class="mw">同质性</em>。因此，它患上了<a class="ae lh" href="http://www.statsmakemecry.com/smmctheblog/confusing-stats-terms-explained-heteroscedasticity-heteroske.html" rel="noopener ugc nofollow" target="_blank"><em class="mw"/></a><em class="mw">。</em>简而言之，就是随着<em class="mw"> iv </em>的值的增加，因变量相对于自变量的可变性。数据散点图中的圆锥形表示<em class="mw">异方差</em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/af5b6c825cbe989752280b8c617767ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*LGxkEnfHCfSsc3dm3W_1bA.jpeg"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">异方差，来源:<a class="ae lh" href="http://www.statsmakemecry.com/smmctheblog/confusing-stats-terms-explained-heteroscedasticity-heteroske.html" rel="noopener ugc nofollow" target="_blank"> statsmakemecry </a></p></figure><p id="51eb" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">例如，仅通过查看房屋面积(1个变量)、房屋面积和位置(2个预测变量)来预测房价。</p><h2 id="917b" class="nj lj it bd lk nk nl dn lo nm nn dp ls mj no np lu mn nq nr lw mr ns nt ly iz bi translated">2.多元案例</h2><p id="fb82" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在多变量<em class="mw"> OLS </em>中，目标函数与单变量OLS相似。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0b6c03eead382f7b8709b168150bd850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*2qxO-M6ziN1lGXOE0jEEvw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">多元最小二乘法的目标</p></figure><p id="80d2" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">多元OLS的关键问题是<a class="ae lh" href="https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-multiple-regression/multicollinearity.html" rel="noopener ugc nofollow" target="_blank"> <em class="mw">多重共线性</em> </a>。这是两个或更多预测因子彼此高度相关的情况(1表示100%相关，0表示不相关)。</p><blockquote class="mx my mz"><p id="31f1" class="ma mb mw mc b md na kd mf mg nb kg mi nc nd ml mm ne nf mp mq ng nh mt mu mv im bi translated">在具有<em class="it">多重共线性的数据中，OLS </em>不会产生一个好的估计值，因为方差项的误差很大，而<em class="it"> OLS </em>只会最小化<em class="it">偏差误差</em>而不是方差导致的<em class="it">误差。因此，我们采用正则化技术来最小化由于方差引起的误差，并改进我们的模型。</em></p></blockquote><h2 id="138b" class="nj lj it bd lk nk nl dn lo nm nn dp ls mj no np lu mn nq nr lw mr ns nt ly iz bi translated">3.里脊回归</h2><p id="a1b1" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">如上所述，当数据遭受多重共线性时，像OLS这样的无偏估计量由于方差项而具有较高的误差。岭回归通过引入具有<em class="mw"> L2 </em>正则化项的参数α来解决这个问题，从而除了最小平方损失之外还缩小了权重<em class="mw"> W </em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/883a3c22c9790565f731b7a95a310164.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*5C1H3pHirDH8Ad23pXIAIw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">岭回归的目标</p></figure><p id="c58d" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">除了<em class="mw">同方差</em>之外，它与<em class="mw"> OLS </em>有相似的假设。岭回归缩小了不高度相关的系数的值<strong class="mc jd">(不完全为零)</strong>。</p><h2 id="e4d3" class="nj lj it bd lk nk nl dn lo nm nn dp ls mj no np lu mn nq nr lw mr ns nt ly iz bi translated">4.套索回归</h2><p id="13ae" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">类似于岭回归，它通过正则化来解决多重共线性问题。将具有<em class="mw"> L1范数</em>的收缩参数引入权重W <em class="mw"> </em>有助于减少LASSO回归中的方差误差。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7e32af97a833cc3970377f470d1024b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*ZqAhBk9jLIn6IVCcdzNN-g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">套索回归的目标</p></figure><p id="0532" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">拉索做了与OLS相似的假设，除了<em class="mw">的同质性。</em> LASSO将非高度相关的系数值<strong class="mc jd">缩小到零</strong>。</p><h1 id="c4e8" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">预测波士顿房价</h1><p id="81ba" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">让我们开始编写线性回归模型的代码。在本帖中，我们将使用波士顿房价数据集。它由506个样本组成，有13个特征，价格从5.0到50.0不等</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h2 id="9ac3" class="nj lj it bd lk nk nl dn lo nm nn dp ls mj no np lu mn nq nr lw mr ns nt ly iz bi translated">使用最小二乘法的单变量模型</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">具有一个特征(13个特征中的第5个)的线性回归，MSE为54.926</p></figure><p id="fecc" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">让我们画出最佳拟合线</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/6c929a92a542bea461d1584844ba537d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9NOfBqoHU7viCix54jbmlw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">最小二乘法最佳拟合直线</p></figure><p id="b624" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">你可以清楚地看到，我们有一个使用sklearn和几行代码的预测模型。对于一个特性来说还不错。不过，我们可以显著提高54.926的均方误差(MSE)。</p><h2 id="05b3" class="nj lj it bd lk nk nl dn lo nm nn dp ls mj no np lu mn nq nr lw mr ns nt ly iz bi translated">多元最小二乘法</h2><p id="49a5" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">类似于单变量方法，除了我们在<em class="mw"> X </em>的所有特征上训练，而不是一个特征。所以只需复制前面的代码片段和注释行3。我们需要检查MSE和回归系数来确定最佳拟合线，因为绘制13维变量还不是主流:/</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="7004" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">显然，我们的模型将MSE指标从54.926提高到37.894。由于有13个特征，很有可能出现<em class="mw">多重共线性</em>。为了验证这一点，我们将看到一个新的统计数据，叫做<em class="mw">方差膨胀因子(VIF) </em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/b686697f680088b796f91bb5e1398877.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/1*qR3hI_7FO6dHNBl9WCE-Jw.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">VIF什么via <a class="ae lh" href="https://giphy.com/gifs/emma-mila-stauffer-VeB9ieebylsaN5Jw8p" rel="noopener ugc nofollow" target="_blank"> GIPHY </a></p></figure><p id="a60f" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated"><a class="ae lh" href="https://www.statisticshowto.com/variance-inflation-factor/" rel="noopener ugc nofollow" target="_blank"> VIF </a>估计由于多重共线性引起的回归系数的膨胀。Vif的计算方法是采用一个预测值，并将其与模型中的所有其他预测值进行回归。VIF的数值给出了每个系数的方差膨胀的百分比。根据经验，VIF大于5.0表明变量高度相关。因此，对于一个好的预测模型，必须去除那些高度相关的变量。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">计算VIF以检查多重共线性</p></figure><p id="c2d9" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">除了3个特征外，所有特征的VIF都非常高(VIF超过5.0被视为高)。因此，我们使用岭和套索回归的正则化模型应该比普通的最小二乘法更好地处理这些数据。</p><h2 id="d3b9" class="nj lj it bd lk nk nl dn lo nm nn dp ls mj no np lu mn nq nr lw mr ns nt ly iz bi translated">里脊回归</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="699e" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated"><em class="mw">两个要点</em> : <em class="mw"> MSE </em>减小，岭回归将X的系数挤压到接近零。</p><h2 id="b79e" class="nj lj it bd lk nk nl dn lo nm nn dp ls mj no np lu mn nq nr lw mr ns nt ly iz bi translated">套索回归</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="1d05" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">MSE从普通最小二乘法中的38.894进一步提高到21.669。那是相当大的胜利。很明显，LASSO将几个特征的系数推到了零。</p><h1 id="6ff2" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">结论</h1><p id="a8b9" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">总而言之，我们看到了如何使用sklearn库在几行代码中实现线性回归。</p><ol class=""><li id="41ef" class="od oe it mc b md na mg nb mj of mn og mr oh mv oi oj ok ol bi translated">获得最佳拟合线的普通最小二乘法在许多情况下效果很好，而且非常直观。</li><li id="edb2" class="od oe it mc b md om mg on mj oo mn op mr oq mv oi oj ok ol bi translated">然而，当数据遭受多重共线性或异方差时，我们需要使用正则化技术来执行回归。</li><li id="3d14" class="od oe it mc b md om mg on mj oo mn op mr oq mv oi oj ok ol bi translated">VIF是一种可用于检测预测变量中多重共线性的度量。</li><li id="02bd" class="od oe it mc b md om mg on mj oo mn op mr oq mv oi oj ok ol bi translated">山脊线和套索线在执行回归建模方面做得很好。</li></ol><p id="ad74" class="pw-post-body-paragraph ma mb it mc b md na kd mf mg nb kg mi mj nd ml mm mn nf mp mq mr nh mt mu mv im bi translated">快乐学习:)</p><h2 id="9b3e" class="nj lj it bd lk nk nl dn lo nm nn dp ls mj no np lu mn nq nr lw mr ns nt ly iz bi translated">附加源</h2><ol class=""><li id="3912" class="od oe it mc b md me mg mh mj or mn os mr ot mv oi oj ok ol bi translated"><a class="ae lh" href="https://etav.github.io/python/vif_factor_python.html" rel="noopener ugc nofollow" target="_blank">https://etav.github.io/python/vif_factor_python.html</a></li></ol></div></div>    
</body>
</html>