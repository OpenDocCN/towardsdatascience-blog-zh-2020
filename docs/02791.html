<html>
<head>
<title>Learning Deep Features for Discriminative Localization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于鉴别定位的深度特征学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-deep-features-for-discriminative-localization-class-activation-mapping-2a653572be7f?source=collection_archive---------35-----------------------#2020-03-17">https://towardsdatascience.com/learning-deep-features-for-discriminative-localization-class-activation-mapping-2a653572be7f?source=collection_archive---------35-----------------------#2020-03-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2219" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">可解释人工智能的全局平均池和类激活映射</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a15a73308ec608b69ae31747ede8b17c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7DELoLuCuX8X0u4473o01Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">类激活映射[1]</p></figure><p id="d5bd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">今天，我将重温CVPR 2016年的论文“学习歧视性本地化的深层特征”。该论文由麻省理工学院研究人员<em class="lr">周等人</em><strong class="kx ir">【1】</strong>撰写，在可解释人工智能和弱监督检测/定位方面做出了有价值的贡献。其贡献可归纳如下:</p><ol class=""><li id="659c" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated"><strong class="kx ir">全球平均池使康文网络保持其本地化能力</strong></li><li id="a6c7" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><strong class="kx ir">引入弱监督、端到端、单路目标定位的类激活映射技术</strong></li><li id="3bdc" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><strong class="kx ir">在ILSVRC 2014上获得37.1%的前5名本地化误差，接近完全监督的AlexNet的34.2% </strong></li></ol><p id="0521" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本白皮书基于之前对ConvNets本地化功能的研究，以及全球平均池(GAP)的优势。广泛应用于计算机视觉中的ConvNets具有固有的金字塔结构和固有的平移不变性，允许它们通过以分层的方式从空间特征图中提取语义代表性增加的特征。虽然之前的研究<strong class="kx ir">【2】</strong>表明，ConvNets的卷积单元在没有监督的情况下学习位置信息，但当全连接(FC)层(其中神经元密集地连接到前一层的激活值)用于分类时，这种能力就丧失了。在这项工作中，<em class="lr">周等人</em>提出了全局平均池作为一种机制来保持ConvNets的本地化能力，直到最后一层。</p><p id="6286" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">全球平均统筹(缺口)</strong></p><p id="ae79" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">尽管GAP并不是一项新技术——已经作为防止过度拟合的规则进行了研究，但直到这项工作，其潜力才被充分认识到。作者在GAP和<strong class="kx ir">【3】</strong>的全局最大池(GMP)之间进行了显著的比较，目的是弱监督的对象定位。使用GMP，<strong class="kx ir">【3】</strong>仅限于定位对象的边界，而GAP则使<em class="lr">周等人</em>能够识别整个对象的位置(类似于一个遮罩)。这是由这样的认识提供的，即平均池激励对所有区别区域(有助于成功识别对象的特征)的识别，而最大池仅仅激励对图像的最大区别区域的识别。</p><p id="4c09" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">类激活图(CAM) </strong></p><p id="28e7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">CAMs可以被描述为最后卷积特征映射的点积和FC层的类权重(在GAP之后应用)的总和。这被正式描述为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mg"><img src="../Images/8194649549ed4b62dd242b83ca446191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_g6k-5csLcnDgsV1lJSQwg.png"/></div></div></figure><p id="aba8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，作者将cam描述为“在不同空间位置存在的(这些)视觉模式的加权总和”。根据最后一个卷积特征图的分辨率，对CAM进行上采样，以显示输入图像中的区别区域，并通过softmax/sigmoid输出函数，以生成图像级分类标签。</p><p id="ec7d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">实验</strong></p><p id="c904" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">周等人</em>在8个分类数据集上评估了3个ConvNet基线上的cam，即AlexNet、VGGNet和GoogLeNet，以及用于弱监督包围盒定位的ILSVRC 2014和CUB-200–2011数据集。对分类数据集的实验产生了两个主要见解:</p><ul class=""><li id="1b1b" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq mh ly lz ma bi translated">首先，这种差距不会严重影响基线模型的分类性能(除了vanilla AlexNet，他们通过添加Conv层进行了调整)，同时保留了本地化信息。</li><li id="53cb" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq mh ly lz ma bi translated">第二，在空间和语义特征之间存在折衷，其中移除Conv层(以获得更高的映射分辨率，即，最后的卷积特征图较少下采样)有益于定位，但是有害于分类。</li></ul><p id="0d64" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这由表1中的前5和前1分类误差在去除Conv层之后的轻微增加所证明:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mi"><img src="../Images/ac4852bb92a4527b4045b0e9834ef7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9oq21Z--PU6nh18HZ6KuKg.png"/></div></div></figure><p id="f048" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在ILSVRC的本地化数据集上进一步评估CAM基线。<em class="lr">周等人</em>使用在最大的对象特征集群(定义为CAM中值&gt;为CAM中最大值的20%的区域)上拟合边界框的阈值技术，生成边界框预测。表2和表3证明了CAM方法比以前的方法(包括<strong class="kx ir">【4】</strong>的反向传播和<strong class="kx ir">【5】</strong>的网络中的网络)以及vanilla(在图像级标签上训练的)ConvNet基线定位得更好。特别地，GoogLeNet-GAP(使用启发式)获得了接近完全监督(在边界框注释上训练)的AlexNet的结果，尽管已经在图像级分类标签上进行了训练。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mj"><img src="../Images/496414b01105d844e1334affc5ea60ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l66m39c-0JTYCjISUiZwjA.png"/></div></div></figure><p id="144a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对CUB-200–2011数据集的进一步评估用于细粒度识别和边界框定位。作者报告了GoogLeNet-GAP在3种设置下的结果，其中训练使用1)具有图像级标签的弱监督方法，2)具有裁剪的逐代方法，3)具有边界框注释的完全监督方法。分类结果分别为63.0%，67.8%，70.5%，超过了大多数以前的方法，表明由GAP识别的特征有助于分类性能，因此在定位任务之外是有用的。最后，使用边界框0.5交集/并集(IoU)标准的评估给出了41.0%的准确度(相比之下，机会性能为5.5%)。通过在CUB-200–2011数据集上的实验，<em class="lr">周等人</em>表明GAP方法识别深度的可定位特征，适用于两种分类&amp;定位任务。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/dccc94fad470051769016183101f7d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fc9ty7S8fE-z6jxPes2Vfw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对CUB-200–2011数据集的评估[1]</p></figure><p id="953a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">应用</strong></p><p id="f51a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">作者进一步探讨了GAP方法的模式识别和可视化能力。通过SUN<strong class="kx ir">【6】</strong>和SVT<strong class="kx ir">【7】</strong>数据集，作者展示了由GoogLeNet-GAP生成的特征具有从图像中发现、定位和在某种程度上解释高级概念的潜力，尽管缺乏明确的监督。此外，cam能够根据查询的感兴趣类别(称为特定类别单元)突出显示不同的区分区域。通过对视觉问答的初步实验，<em class="lr">周等人</em>证明了CAMs中可能存在的高层次理解能力。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/8924d3fd90a6dac5c2db9b7d12572e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N41jUwCZmMtClyjznfQjug.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">VQA的凸轮[1]</p></figure><p id="7440" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">结论</strong></p><p id="d73d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">“学习区分性定位的深度特征”介绍了一种在分类和定位中都有用的弱监督方法，并在基准数据集上进行严格的实验以验证其结果。通过对全局平均池函数的新颖见解，以及类激活图的开发，这项工作极大地推进了对ConvNets内部工作方式的理解。</p><p id="3344" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> <em class="lr"> </em> </strong> <em class="lr"> SUN397、MIT Indoor67、Scene15、SUN Attribute、Caltech101、Caltech256、斯坦福Action40、UIUC Event8 </em></p><p id="6f8e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">参考文献</strong></p><p id="8bcd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[1] B .周、a .科斯拉、a .拉皮德里扎、a .奥利瓦和a .托拉尔巴。<a class="ae mm" href="https://arxiv.org/pdf/1512.04150.pdf" rel="noopener ugc nofollow" target="_blank">学习深度特征进行鉴别定位。</a>2016年CVPR。[2] B .周、V. Jagadeesh和R. Piramuthu。<a class="ae mm" href="https://arxiv.org/pdf/1411.5328.pdf" rel="noopener ugc nofollow" target="_blank"> Conceptlearner:从弱标记图像集合中发现视觉概念。</a>2015年，CVPR。<br/>[3]奥夸布、博图、拉普捷夫和西维奇。对象本地化是免费的吗？卷积神经网络的弱监督学习。2015年，CVPR。<br/> [4] K. Simonyan、A. Vedaldi和A. Zisserman。<a class="ae mm" href="https://arxiv.org/pdf/1312.6034.pdf" rel="noopener ugc nofollow" target="_blank">深层卷积网络:可视化图像分类模型和显著图。</a>2014年ICLR研讨会。<br/>[5]m . Lin、Q. Chen和S. Yan。<a class="ae mm" href="https://arxiv.org/pdf/1312.4400.pdf" rel="noopener ugc nofollow" target="_blank">网络中的网络。</a>2014年ICLR。<br/> [6]肖、海斯、埃林格、奥利瓦和托雷巴。<a class="ae mm" href="https://www.cc.gatech.edu/~hays/papers/sun.pdf" rel="noopener ugc nofollow" target="_blank">孙数据库:从修道院到动物园的大规模场景识别。</a>2010年，CVPR。<br/>[7]k .王、b .巴本科和s .贝隆吉。<a class="ae mm" href="https://vision.cornell.edu/se3/wp-content/uploads/2014/09/wang_iccv2011.pdf" rel="noopener ugc nofollow" target="_blank">端到端的场景文本识别。</a>2011年，ICCV。</p></div></div>    
</body>
</html>