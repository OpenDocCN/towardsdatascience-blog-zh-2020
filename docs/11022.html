<html>
<head>
<title>Predicting Sentiment of Employee Reviews</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测员工评论的情绪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-sentiment-of-employee-reviews-ec0c0c837328?source=collection_archive---------33-----------------------#2020-07-31">https://towardsdatascience.com/predicting-sentiment-of-employee-reviews-ec0c0c837328?source=collection_archive---------33-----------------------#2020-07-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d929" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对来自 Indeed.com 的员工评论的正面和负面情绪进行分类</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e29b20ba2a915af9a81e29788d67d46a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IWftw8Lo5OTnZ1x6"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">奥利维尔·科莱在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="57db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我之前的文章中，我们学习了如何从 Indeed.com 的<a class="ae ky" rel="noopener" target="_blank" href="/scraping-the-web-using-beautifulsoup-and-python-5df8e63d9de3">收集</a>、<a class="ae ky" rel="noopener" target="_blank" href="/preprocessing-text-data-using-python-576206753c28">流程</a>、<a class="ae ky" rel="noopener" target="_blank" href="/nlp-part-3-exploratory-data-analysis-of-text-data-1caa8ab3f79d">分析</a>员工评价。请随意查看并提供反馈。我很想听听你会如何改进代码。特别是，如何动态地克服网站的 HTML 的变化。</p><p id="4ca6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我想把我们的数据集进一步解决情感分类问题。具体来说，我们将为每个评论分配情感目标，然后使用二元分类算法来预测这些目标。</p><p id="5308" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将导入从 Indeed.com 收集的原始员工评论。我建议你回顾一下我以前的文章<a class="ae ky" rel="noopener" target="_blank" href="/scraping-the-web-using-beautifulsoup-and-python-5df8e63d9de3">来了解我们是如何获得这个数据集的。</a></p><p id="b307" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们导入数据集并进行处理。对于此分析，我们只需要“评级”和“评级 _ 描述”列。关于文本预处理更详细的解释，请阅读我的 nlp 预处理文章<a class="ae ky" rel="noopener" target="_blank" href="/preprocessing-text-data-using-python-576206753c28">。</a></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="45f3" class="ma mb it lw b gy mc md l me mf">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt <br/>import seaborn as sns<br/>import contractions<br/>import random<br/>import fasttext<br/>from autocorrect import spell<br/>from nltk.probability import FreqDist<br/>from nltk.tokenize import word_tokenize<br/>import nltk<br/>from nltk.corpus import stopwords, wordnet<br/>from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS<br/>from nltk.stem import WordNetLemmatizer<br/>from imblearn.over_sampling import SMOTE<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.model_selection import StratifiedKFold<br/>from imblearn.pipeline import make_pipeline</span><span id="c683" class="ma mb it lw b gy mg md l me mf">from sklearn.svm import SVC<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.naive_bayes import GaussianNB</span><span id="8ad3" class="ma mb it lw b gy mg md l me mf">from sklearn.metrics import confusion_matrix<br/>from sklearn.metrics import classification_report<br/>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score<br/>from sklearn.metrics import precision_recall_curve<br/>from sklearn.metrics import roc_auc_score<br/>from sklearn.metrics import auc<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.model_selection import GridSearchCV</span><span id="7118" class="ma mb it lw b gy mg md l me mf">from warnings import simplefilter<br/>simplefilter(action='ignore', category=FutureWarning)<br/>pd.set_option('display.max_colwidth', 200)</span><span id="c620" class="ma mb it lw b gy mg md l me mf">with open('apple_scrape.csv') as f:<br/>    df = pd.read_csv(f)<br/>f.close()</span><span id="5bfa" class="ma mb it lw b gy mg md l me mf">print(df.head())</span><span id="b3f7" class="ma mb it lw b gy mg md l me mf">df = df[['rating', 'rating_description']]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mh"><img src="../Images/6bbd8cf6decb9509938b34f2959be08d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7uyfj0eOFn0vRfpe46G1JQ.png"/></div></div></figure><h2 id="15b2" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">消除收缩</h2><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="99a0" class="ma mb it lw b gy mc md l me mf">df['no_contract'] = df['rating_description'].apply(lambda x: [contractions.fix(word) for word in x.split()])</span></pre><h2 id="4ffa" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">将单词列表转换为字符串</h2><p id="b39a" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">应用“fix”函数的结果是一个单词列表，我们需要将该列表转换回字符串。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c6a8" class="ma mb it lw b gy mc md l me mf">df['rating_desc_str'] = [' '.join(map(str, l)) for l in df['no_contract']]</span></pre><h2 id="ab5a" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">删除非英语评论</h2><p id="cf85" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">首先，我们将使用“快速文本”库来识别每个评论的语言。使用此<a class="ae ky" href="https://fasttext.cc/docs/en/language-identification.html" rel="noopener ugc nofollow" target="_blank">链接</a>下载预训练模型。一旦每个评论被标记，我们只需过滤数据框，只包括英文评论。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="bed4" class="ma mb it lw b gy mc md l me mf">pretrained_model = "lid.176.bin" <br/>model = fasttext.load_model(pretrained_model)</span><span id="d460" class="ma mb it lw b gy mg md l me mf">langs = []<br/>for sent in df['rating_desc_str']:<br/>    lang = model.predict(sent)[0]<br/>    langs.append(str(lang)[11:13])</span><span id="e900" class="ma mb it lw b gy mg md l me mf">df['langs'] = langs</span><span id="6a36" class="ma mb it lw b gy mg md l me mf">df.head()</span><span id="d6ed" class="ma mb it lw b gy mg md l me mf">df = df[df['langs'] == 'en']</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/0d13770ce0cf678ad2da1aa08123b628.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E0i3CJsTaKkr79da6SMk0Q.png"/></div></div></figure><h2 id="5cfc" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">符号化</h2><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4504" class="ma mb it lw b gy mc md l me mf">df['tokenized'] = df['rating_desc_str'].apply(word_tokenize)</span></pre><h2 id="2ea6" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">删除标点符号和特殊字符</h2><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="953f" class="ma mb it lw b gy mc md l me mf">df['no_punc'] = df['tokenized'].apply(lambda x: [word for word in x if word.isalnum()])</span></pre><h2 id="864e" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">转换成小写</h2><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7e97" class="ma mb it lw b gy mc md l me mf">df['lower'] = df['no_punc'].apply(lambda x: [word.lower() for word in x])</span></pre><h2 id="6e21" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">拼写</h2><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="3d45" class="ma mb it lw b gy mc md l me mf">df['spell'] = df['lower'].apply(lambda x: [spell(word) for word in x])</span></pre><h2 id="2f68" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">移除任何数字字符(即整数、浮点数)</h2><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="b890" class="ma mb it lw b gy mc md l me mf">df['no_num'] = df['spell'].apply(lambda x: [word for word in x if word.isalpha()])</span></pre><h2 id="1cd8" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">词汇化</h2><p id="11bc" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">为了将词汇化应用于我们的记号，我们首先需要识别每个单词的词性。接下来，我们需要将从 nltk 获得的标签转换为 wordnet 标签，因为这些是我们的 lemmatizer (WordNetLemmatizer())唯一接受的标签。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="1762" class="ma mb it lw b gy mc md l me mf">df['pos_tags'] = df['no_num'].apply(nltk.tag.pos_tag)</span><span id="987c" class="ma mb it lw b gy mg md l me mf">def get_wordnet_pos(tag):<br/>    if tag.startswith('J'):<br/>        return wordnet.ADJ<br/>    elif tag.startswith('V'):<br/>        return wordnet.VERB<br/>    elif tag.startswith('N'):<br/>        return wordnet.NOUN<br/>    elif tag.startswith('R'):<br/>        return wordnet.ADV<br/>    else:<br/>        return wordnet.NOUN</span><span id="965f" class="ma mb it lw b gy mg md l me mf">df['wordnet_pos'] = df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])</span><span id="b069" class="ma mb it lw b gy mg md l me mf">wnl = WordNetLemmatizer()<br/>df['lemma'] = df['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for (word, tag) in x])</span></pre><h2 id="4610" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">删除停用词</h2><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6fdf" class="ma mb it lw b gy mc md l me mf">stop_words = set(stopwords.words('english'))<br/>df['stop_removed'] = df['lemma'].apply(lambda x: [word for word in x if word not in stop_words])</span></pre><h2 id="10a8" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">删除长度少于 3 个字符的单词</h2><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="00ef" class="ma mb it lw b gy mc md l me mf">def short_words(words):<br/>    return [word for word in words if len(word) &gt; 2]</span><span id="3b82" class="ma mb it lw b gy mg md l me mf">df['length'] = df['stop_removed'].apply(short_words)</span></pre><p id="af20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们剩下的是一个大的 dataframe，它包含我们执行的每个预处理步骤的一列。我更喜欢以这种方式工作，因为我们可以很容易地看到每一步前后的变化。如果你问我的话，会使数据验证更容易。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/2a16b77bd0a9a602efc80c787733b205.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sr4eDiRQe-ENSPzuq9G24w.png"/></div></div></figure><h2 id="e71f" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">决定情绪</h2><p id="a9b5" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我们可以利用像“TextBlob”这样的库。情绪”来计算每个评论的情绪，但由于我们有实际的员工评级，我们可以使用此列来代替。</p><p id="89b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该员工用 5 分制的李克特量表给公司评分，超过 75%的评分是正面的(即 5 或 4)。此外，5 分制的评分为 3 分(中性)，可以是正面的，也可以是负面的。为简单起见，我们将删除中性评级。介于 1 和 2 之间的评分为负，介于 4 和 5 之间的评分为正。</p><p id="0e99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将筛选数据帧，使其只包含训练模型所需的列。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2500" class="ma mb it lw b gy mc md l me mf">def sent(df):<br/>    if df['rating'] &lt;= 2:<br/>        val = 1.0<br/>    elif df['rating'] &gt;= 4:<br/>        val = 0.0<br/>    else:<br/>        val = -1.0<br/>    return val</span><span id="5d3f" class="ma mb it lw b gy mg md l me mf">df['sentiment'] = df.apply(sent, axis=1)</span><span id="98a6" class="ma mb it lw b gy mg md l me mf">df = df.loc[(df['sentiment']==1) | (df['sentiment']==0)]</span><span id="aca1" class="ma mb it lw b gy mg md l me mf">df = df[['rating', 'length', 'sentiment']]<br/>df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/5133fce7ca1d17750af90fb937ecdc4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lSaD6WCQxRWXB9XLyeOeyw.png"/></div></div></figure><h2 id="97c8" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">将数据可视化</h2><p id="7d65" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">让我们花一点时间，根据每个单词的积极或消极程度(即感悟)。为了完成这项任务，我们需要计算每个独特的单词在积极情感评论和消极情感评论中出现的频率。 最终结果将类似于下面的列表，其中“工作”一词在负面评价中出现了 425 次，在正面评价中出现了 4674 次。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/5428cc97bf06b1e1083e843720731019.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*XPtnMUsflmV8nY9PD9mGfw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">下面是代码将产生的结果</p></figure><p id="24c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了计算上面的列表，我们首先按照情绪降序排列评论，并重置索引。接下来，我们可以看到，我们有 3570 个正面和 1039 个负面的情感评论。现在，我们将数据集分为正面和负面评论，然后创建一个由 3570 个 1 和 1039 个 0 组成的数组。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="976c" class="ma mb it lw b gy mc md l me mf">df.sort_values(by='sentiment', ascending=False, inplace=True)<br/>df.reset_index(inplace=True, drop=True)</span><span id="be6b" class="ma mb it lw b gy mg md l me mf">df_pos = df[df['sentiment'] == 0.0]<br/>df_neg = df[df['sentiment'] == 1.0]</span><span id="9298" class="ma mb it lw b gy mg md l me mf">targets = np.append(np.ones(len(df_neg)), np.zeros(len(df_pos)))</span><span id="1ac5" class="ma mb it lw b gy mg md l me mf">df['sentiment'].value_counts()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/c800ccf5a079ec069c41a3dbc3145e02.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*kOa7cVVAMx8PHaQS-0_Cvg.png"/></div></figure><p id="fe9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们创建一个名为“frequencies”的空字典来存放输出。现在我们迭代 zipped(即。元组)评论和目标数组(3，570 个零和 404 个一)。当我们迭代每个评论中的每个单词时，我们以由单词及其情感组成的元组的形式创建一个对(即。1 或 0)。我们知道有 3，570 个正面评价，因此，只有前 3，570 个评价中的单词将被分配为正面的。剩余的 404 个评论中的所有单词将被指定为负面的。最终结果是单词和情感的元组键的字典，以及作为值的正面和负面词频。</p><p id="5232" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，“工作”这个词在所有正面评价中出现了 425 次。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="dd82" class="ma mb it lw b gy mc md l me mf">frequencies = {}<br/>for y, words in zip(targets, df['length']):<br/>    for word in words:<br/>        pair = (word, y)<br/>        if pair in frequencies:<br/>            frequencies[pair] += 1<br/>        else:<br/>            frequencies[pair] = 1<br/>            <br/>print(frequencies)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/f7e74852836bd7219577b7d6812e99dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*98vgNEYYVXz0nhHRayE-KQ.png"/></div></div></figure><p id="b6d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们需要做的最后一件事是对“频率”字典中的每个单词的正计数和负计数求和。</p><p id="d393" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们在频率字典键中索引这个单词，创建一个所有单词的列表(存储在“words”中)。接下来，我们初始化 pos_count 和 neg_count 的计数变量。如果元组关键字“单词 1.0”出现在频率字典中(即 work '，1.0)然后我们索引该关键字的值，“neg_count”变量成为该词在负面评论中出现的次数。出现在正面评论中的单词也是如此。最后，我们将单词、pos_count 和 neg_count 添加到“数据”列表中。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d461" class="ma mb it lw b gy mc md l me mf">words = [word[0] for word in frequencies]<br/>data = []</span><span id="af88" class="ma mb it lw b gy mg md l me mf">for word in words:<br/>    pos_count = 0<br/>    neg_count = 0<br/>    <br/>    if (word, 1) in frequencies:<br/>        neg_count = frequencies[(word,1)]<br/>    if (word, 0) in frequencies:<br/>        pos_count = frequencies[(word, 0)]<br/>    <br/>    data.append([word, neg_count, pos_count])</span><span id="4b34" class="ma mb it lw b gy mg md l me mf">print(data)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/60e806c0c11e991bcd9fee58becd625e.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*ksAB1NE1ohxagFq0uKwp1g.png"/></div></figure><p id="8c4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，如果我们把所有的单词都画出来，情节会变得有点乱，因此，让我们随机选择几个单词。因为有更多积极的评论，所以情节偏向积极的一面。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="aeb3" class="ma mb it lw b gy mc md l me mf">fig, ax = plt.subplots(figsize=(20, 15))</span><span id="e534" class="ma mb it lw b gy mg md l me mf">x = np.log([x[1] + 1 for x in data])<br/>y = np.log([x[2] + 1 for x in data])</span><span id="8b7f" class="ma mb it lw b gy mg md l me mf">ax.scatter(x, y)</span><span id="ab1a" class="ma mb it lw b gy mg md l me mf">plt.xlabel('Negative Count')<br/>plt.ylabel('Positive Count')</span><span id="07c1" class="ma mb it lw b gy mg md l me mf">for i in range(0, len(data)): <br/>    ax.annotate(data[i][0], (x[i], y[i]), fontsize=15)    <br/>ax.plot([0,7], [0,7], color='red')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/37567d39dbd46acc7f718d4c3d8650fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lv7H-NR-2T4ZPUk14eHkUg.png"/></div></div></figure><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4511" class="ma mb it lw b gy mc md l me mf">fig, ax = plt.subplots(figsize=(20, 15))</span><span id="b75c" class="ma mb it lw b gy mg md l me mf">data_rand = random.sample(data, 50)</span><span id="853f" class="ma mb it lw b gy mg md l me mf">x = np.log([x[1] + 1 for x in data_rand])<br/>y = np.log([x[2] + 1 for x in data_rand])</span><span id="c9a9" class="ma mb it lw b gy mg md l me mf">ax.scatter(x, y)</span><span id="2142" class="ma mb it lw b gy mg md l me mf">plt.xlabel('Negative Count')<br/>plt.ylabel('Positive Count')</span><span id="52f8" class="ma mb it lw b gy mg md l me mf">for i in range(0, len(data_rand)): <br/>    ax.annotate(data_rand[i][0], (x[i], y[i]), fontsize=15)    <br/>ax.plot([0,7], [0,7], color='red')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/ece3ebe0697d8f14b950a45848c73668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aM26XJL2GH2wTZDTqVblgA.png"/></div></div></figure><h2 id="ee5d" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">特征抽出</h2><p id="6235" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我们将创建一个只有两个特征的矢量化特征集，而不是走一步到位的编码或计数矢量化路线，这将创建一个巨大的稀疏矩阵，(想要可视化一个稀疏矩阵<a class="ae ky" rel="noopener" target="_blank" href="/visualizing-a-sparse-matrix-1c4c807ea6c9">看看这个</a>)。<strong class="lb iu"> <em class="nh">第一个特征将是来自“频率”字典的所有负频率的总和，用于评论中的每个唯一单词。第二个特征将是来自“频率”字典的所有频率的总和，用于评论中的每个独特的正面词。</em> </strong></p><p id="4b84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比如看第一个特征(即。负数总计)，让我们假设我们有一个包含以下单词的评论['工作'，'苹果'，'承包商']。查看之前计算的“频率”字典，该评论的特征 1 值将是 739 或(425+279+35=739)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/28bcab3e93e074ec881d6c7cdf943d85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n4qPZbb_Shj4DgflMSDlGQ.png"/></div></div></figure><p id="6a7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们创建一个备用的 1x2 numpy 阵列。在评论中循环每个单词，如果(word，1)(即。work '，1)作为一个关键字出现在频率字典中，我们将对其值进行索引。该值被赋给“x”数组中的第一列。如果这个单词不存在，那么“x”数组中的第一列将被赋一个零。然后 for 循环查看是否相同的单词但是具有积极的情绪(即 apple '，0)出现在频率字典中。如果是这样，索引值被分配给“x”数组中的第二列。当 for 循环完成其第一次迭代时，它将 x[0，0]和 x[0，1]值赋给“X”数组中相应列的第一行。</p><p id="b2c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以在下面的结果数据框中看到，第一篇评论中的负面词汇总数为 6790 个，正面词汇总数为 54693 个。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="3708" class="ma mb it lw b gy mc md l me mf">def features(review, frequencies):<br/>    x = np.zeros((1,2))<br/>    <br/>    for word in review:<br/>        if (word, 1) in frequencies:<br/>            x[0,0] += frequencies.get((word, 1.0),0)<br/>        if (word, 0) in frequencies:<br/>            x[0,1] += frequencies.get((word, 0.0),0)<br/>    assert(x.shape == (1, 2))<br/>    return x</span><span id="e7c4" class="ma mb it lw b gy mg md l me mf">X = np.zeros((len(df), 2))</span><span id="819a" class="ma mb it lw b gy mg md l me mf">for i in range(len(df['length'])):<br/>    X[i, :] = features(df['length'][i], frequencies)</span><span id="d026" class="ma mb it lw b gy mg md l me mf">X_df = pd.DataFrame(X, columns=['neg_count', 'pos_count'])<br/>reviews_df = pd.concat([X_df, df['sentiment']], axis=1)</span><span id="ddde" class="ma mb it lw b gy mg md l me mf">print(reviews_df.head())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/be0ca14cae83e72f495e9df33d8d193a.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*QXFh3PSk0cvNDSo9RhO9_Q.png"/></div></figure><h2 id="1298" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">列车测试分离</h2><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="78cf" class="ma mb it lw b gy mc md l me mf">X_train, X_test, y_train, y_test = train_test_split(<br/>    reviews_df.drop('sentiment', axis=1),<br/>    reviews_df['sentiment'], <br/>    test_size=0.25)</span></pre><h2 id="dcd8" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">分类器评估</h2><p id="60c7" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我们将评估六种分类器，逻辑回归、随机森林、KNN、朴素贝叶斯、支持向量分类器和梯度推进分类器。我们肯定希望我们的模型有利于对负面评论(真正的正面评论)的准确预测，因为这些评论给了我们对组织问题的洞察力。也就是说，我们不想要一个过于“挑剔”的模型，它只会预测一个负面评论是负面的，如果它是绝对确定的。我们最终想要一个更平衡的模型，它有利于准确的负面评论预测(真正的正面)，但也能很好地正确分类正面评论。因此，从度量的角度来看，我们需要高召回率来正确预测尽可能多的真实负面评论，但也需要良好的精确度来最小化错误的负面评论。此外，高 AUC 分数将指示该模型具有正确分类正面和负面评论的高概率。</p><p id="ae14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了获得更准确的模型评估并避免过度拟合，我们将使用分层 K-Fold 交叉验证。这样，每个模型将在来自我们原始训练数据的训练和测试数据的不同分割上被评估 k 次。将在每个折叠处计算指标，并对所有折叠进行总体平均。我们将比较每一层的平均训练和测试指标，以确定哪个分类器可能过拟合。此外，由于训练数据中存在少数类的类不平衡，我们将在每个折叠处使用 SMOTE 技术来过采样和平衡目标。让我们看看我们的分类器表现如何。</p><p id="8e33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，兰登森林和 KNN 严重过度拟合数据。朴素贝叶斯和 SVC 具有很高的测试召回率，但模型的精度很低，导致了很高的假阴性率。最后，逻辑回归和梯度推进分类器具有非常相似的分数，但是后者具有稍高的精度和 AUC。这似乎是一个稍微平衡的模型。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a9a5" class="ma mb it lw b gy mc md l me mf">models = []<br/>models.append(('log_reg', LogisticRegression(max_iter=10000, random_state=42)))<br/>models.append(('rf_classifer', RandomForestClassifier(random_state=42)))<br/>models.append(('knn', KNeighborsClassifier()))<br/>models.append(('bayes', GaussianNB()))<br/>models.append(('svc', SVC()))<br/>models.append(('gbc', GradientBoostingClassifier()))</span><span id="d327" class="ma mb it lw b gy mg md l me mf">scoring = {'accuracy': 'accuracy',<br/>           'precision': 'precision',<br/>           'recall': 'recall',<br/>           'f1_score' : 'f1',<br/>            'auc': 'roc_auc'}</span><span id="fc5d" class="ma mb it lw b gy mg md l me mf">sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)<br/>skf = StratifiedKFold(n_splits=10)</span><span id="0c2b" class="ma mb it lw b gy mg md l me mf">for name, classifier in models:<br/>    pipeline = make_pipeline(sm, classifier)<br/>    scores = cross_validate(pipeline, X_train, y_train, cv=skf, scoring=scoring, return_train_score=True)    <br/>    for score in list(scores)[2:]:<br/>        print('{} Avg CV Recall {} is {} | std is (+/- {})'.format(name, score[:-6], <br/>            np.mean(scores[score]).round(3), <br/>            np.std(scores[score]).round(3)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/27fdd7dc8a66d73292f3d7793d609ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*LYakkX7kMFsY9wxf6fv15Q.png"/></div></figure><h2 id="de55" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">超参数调谐</h2><p id="3a0b" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">梯度推进分类器有很大范围的超参数，我们可以调整。通常，n _ 估计量(即树的数量)、最大深度和学习速率被认为是最重要的参数。我们将检查学习率、n 估计量、最大深度、最小样本分裂和最小样本叶。</p><p id="36fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，这些超参数范围是多次迭代的结果。我们将从大范围开始(即范围(10，1000，100)并缩小到更具体的范围(即。800，1000，1)基于获得的分数。它似乎是 0.01 的学习率和 n _ 估计量(即 959 的树数)是最好的。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="38a1" class="ma mb it lw b gy mc md l me mf">learning_rate = [0.3,0.2,0.15,0.1,0.05,0.01]<br/>n_estimators = range(20,1000,1)</span><span id="eb3e" class="ma mb it lw b gy mg md l me mf">param_grid = dict(gradientboostingclassifier__learning_rate=learning_rate,<br/>                 gradientboostingclassifier__n_estimators=n_estimators)</span><span id="c5ed" class="ma mb it lw b gy mg md l me mf">gbc = GradientBoostingClassifier(random_state=42)<br/>skf = StratifiedKFold(n_splits=10)<br/>sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)<br/>pipeline = make_pipeline(sm, gbc)</span><span id="bd6c" class="ma mb it lw b gy mg md l me mf">grid = GridSearchCV(pipeline, <br/>                    param_grid=param_grid, <br/>                    scoring='roc_auc', <br/>                    verbose=1, <br/>                    n_jobs=-1, <br/>                    cv=skf,<br/>                    return_train_score=True<br/>                   )<br/>grid_results = grid.fit(X_train, y_train)</span><span id="606d" class="ma mb it lw b gy mg md l me mf">print('Best Score: ', grid_results.best_score_)<br/>print('Best Params: ', grid_results.best_params_)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/001a8d641010ffb8c801117cde38e6d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wuge-iROZnYy9QkXu8BTQQ.png"/></div></div></figure><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4965" class="ma mb it lw b gy mc md l me mf">scores = grid_results.cv_results_['mean_test_score'].reshape(len(learning_rate),len(n_estimators))</span><span id="ecd1" class="ma mb it lw b gy mg md l me mf">plt.figure(figsize=(30, 15))<br/>plt.pcolor(scores, cmap='coolwarm', alpha=0.7)<br/>plt.xlabel('n_estimators')<br/>plt.ylabel('learning_rate')<br/>plt.colorbar()<br/>plt.xticks(np.arange(len(n_estimators)), n_estimators)<br/>plt.yticks(np.arange(len(learning_rate)), learning_rate)<br/>plt.title('GridSearchCV Test AUC Score')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/370401e27c226c70e8cd1884adc4a315.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*97OKR5bzOoeKojR_tQHh7A.png"/></div></div></figure><p id="bada" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们转向 max_depth，即每个决策树可以构建的深度。增加深度使模型能够捕捉更多的信息(增加复杂性)，但是有一个收益递减的水平，因为太多的级别模型将开始过度拟合。</p><p id="84cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">似乎 max_depth 为 2 是最佳的，增加深度我们可以看到模型很快开始过度拟合。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="f489" class="ma mb it lw b gy mc md l me mf">max_depth = [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]</span><span id="1f13" class="ma mb it lw b gy mg md l me mf">param_grid = dict(gradientboostingclassifier__max_depth=max_depth)</span><span id="e560" class="ma mb it lw b gy mg md l me mf">gbc = GradientBoostingClassifier(random_state=42,<br/>                                 learning_rate = 0.01,<br/>                                 n_estimators = 959)<br/>skf = StratifiedKFold(n_splits=10)<br/>sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)<br/>pipeline = make_pipeline(sm, gbc)</span><span id="4047" class="ma mb it lw b gy mg md l me mf">grid = GridSearchCV(pipeline, <br/>                    param_grid=param_grid,<br/>                    scoring='roc_auc', <br/>                    verbose=1, <br/>                    n_jobs=-1, <br/>                    cv=skf,<br/>                    return_train_score=True)<br/>grid_results = grid.fit(X_train, y_train)</span><span id="a255" class="ma mb it lw b gy mg md l me mf">print('Best Score: ', grid_results.best_score_)<br/>print('Best Params: ', grid_results.best_params_)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/39697136d68c244186af7acce470d859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*5s7Ij6oMXN7e6EIXCGGmXw.png"/></div></figure><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0a35" class="ma mb it lw b gy mc md l me mf">cv_results_df = pd.DataFrame(grid_results.cv_results_)<br/>cv_results_df.rename(columns={'param_gradientboostingclassifier__max_depth': 'max_depth'}, inplace=True)<br/>train_scores_mean = cv_results_df["mean_train_score"]<br/>test_scores_mean = cv_results_df["mean_test_score"]<br/>max_depths = cv_results_df['max_depth']</span><span id="c3ae" class="ma mb it lw b gy mg md l me mf">plt.figure(figsize=(10,5))<br/>plt.plot(max_depths, train_scores_mean, label='Avg Training AUC Scores')<br/>plt.plot(max_depths, test_scores_mean, label='Avg Test AUC Scores')<br/>plt.xlabel('max_depths')<br/>plt.ylabel('Avg AUC Score')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/054835f4e761d4e3bd8b4fa5a9e26646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*kypPBhbjaVIajCc5485vvw.png"/></div></figure><p id="713c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Min_samples_split 或拆分内部节点所需的最小样本数。换句话说，如果我们将该参数设置为 2，那么该节点将需要至少两个记录/审查，以便拆分成两个节点。较高的值有助于过度拟合，因为它迫使决策树在拆分之前需要更多的记录。更多的分割=更多的深度=增加的复杂性=过度拟合。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="69ab" class="ma mb it lw b gy mc md l me mf">min_samples_split = [2,3,4,5]</span><span id="1e42" class="ma mb it lw b gy mg md l me mf">param_grid = dict(gradientboostingclassifier__min_samples_split = min_samples_split)</span><span id="2893" class="ma mb it lw b gy mg md l me mf">gbc = GradientBoostingClassifier(random_state=42,<br/>                                 learning_rate=0.01,<br/>                                 n_estimators=959,<br/>                                 max_depth=2)<br/>skf = StratifiedKFold(n_splits=10)<br/>sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)<br/>pipeline = make_pipeline(sm, gbc)</span><span id="ccd0" class="ma mb it lw b gy mg md l me mf">grid = GridSearchCV(pipeline, <br/>                    param_grid=param_grid,<br/>                    scoring='roc_auc', <br/>                    verbose=1, <br/>                    n_jobs=-1, <br/>                    cv=skf,<br/>                    return_train_score=True)<br/>grid_results = grid.fit(X_train, y_train)</span><span id="0d03" class="ma mb it lw b gy mg md l me mf">print('Best Score: ', grid_results.best_score_)<br/>print('Best Params: ', grid_results.best_params_)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/5255784df4aca71ed8f1511c5273b334.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*aWFG_Z0olV1he3ixqeRTKg.png"/></div></figure><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="5be7" class="ma mb it lw b gy mc md l me mf">cv_results_df = pd.DataFrame(grid_results.cv_results_)<br/>cv_results_df.rename(columns={'param_gradientboostingclassifier__min_samples_split': 'min_samples_split'}, inplace=True)<br/>train_scores_mean = cv_results_df["mean_train_score"]<br/>test_scores_mean = cv_results_df["mean_test_score"]<br/>min_samples_splits = cv_results_df['min_samples_split']</span><span id="e59e" class="ma mb it lw b gy mg md l me mf">plt.figure(figsize=(10,5))<br/>plt.plot(min_samples_splits, train_scores_mean, label='Avg Training AUC Scores')<br/>plt.plot(min_samples_splits, test_scores_mean, label='Avg Test AUC Scores')<br/>plt.xlabel('min_samples_splits')<br/>plt.ylabel('Avg AUC Score')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/9d6e50039b5225269d81b8c052c739c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*5fhOJUAdB7euYAqKH6zsrw.png"/></div></figure><p id="f4b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Min_samples_leaf 是形成叶节点所需的最小样本数。换句话说，每个叶子必须至少有 min_samples_leaf 评论，它将其分类为正面或负面。看来 26 条评论是最佳数字。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2a00" class="ma mb it lw b gy mc md l me mf">min_samples_leaf = range(10,30,1)</span><span id="f6e5" class="ma mb it lw b gy mg md l me mf">param_grid = dict(gradientboostingclassifier__min_samples_leaf = min_samples_leaf)</span><span id="a084" class="ma mb it lw b gy mg md l me mf">gbc = GradientBoostingClassifier(random_state=42,<br/>                                 learning_rate=0.01,<br/>                                 n_estimators=959,<br/>                                 max_depth=2,<br/>                                 min_samples_split=2<br/>                                )<br/>skf = StratifiedKFold(n_splits=10)<br/>sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)<br/>pipeline = make_pipeline(sm, gbc)</span><span id="8e47" class="ma mb it lw b gy mg md l me mf">grid = GridSearchCV(pipeline, <br/>                    param_grid=param_grid,<br/>                    scoring='roc_auc', <br/>                    verbose=1, <br/>                    n_jobs=-1, <br/>                    cv=skf,<br/>                    return_train_score=True)<br/>grid_results = grid.fit(X_train, y_train)</span><span id="628f" class="ma mb it lw b gy mg md l me mf">print('Best Score: ', grid_results.best_score_)<br/>print('Best Params: ', grid_results.best_params_)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/307c5beef37b7ac9aee74ffaacc04e80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*elkJg7hyVeQjLqLWPzu2GQ.png"/></div></figure><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="f867" class="ma mb it lw b gy mc md l me mf">cv_results_df = pd.DataFrame(grid_results.cv_results_)<br/>cv_results_df.rename(columns={'param_gradientboostingclassifier__min_samples_leaf': 'min_samples_leaf'}, inplace=True)<br/>train_scores_mean = cv_results_df["mean_train_score"]<br/>test_scores_mean = cv_results_df["mean_test_score"]<br/>min_samples_leafs = cv_results_df['min_samples_leaf']</span><span id="c03a" class="ma mb it lw b gy mg md l me mf">plt.figure(figsize=(10,5))<br/>plt.plot(min_samples_leafs, train_scores_mean, label='Avg Training AUC Scores')<br/>plt.plot(min_samples_leafs, test_scores_mean, label='Avg Test AUC Scores')<br/>plt.xlabel('min_samples_leafs')<br/>plt.ylabel('Avg AUC Score')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/894388f3962177805e35f42d53903799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*pnMeB74cwA5WEWMct65qxQ.png"/></div></figure><h2 id="33fa" class="ma mb it bd mi mj mk dn ml mm mn dp mo li mp mq mr lm ms mt mu lq mv mw mx my bi translated">模型评估</h2><p id="be4c" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">将我们的优化模型应用到我们坚持的测试数据中，我们可以看到边际改进。总体而言，AUC 略有上升(0.728 至 0.734)，同时积极评价类别的召回率也有所上升(74%至 76%)。我们能够通过 11 个评论增加我们的真实正面计数。我们的模型仍然将大量正面评论错误分类为负面评论(217 个误报)。也就是说，总的来说，它在正确分类真实的正面评价(76%)和真实的负面评价(71%)方面做得相当不错。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="143b" class="ma mb it lw b gy mc md l me mf">model_one = GradientBoostingClassifier(random_state=42,<br/>                                       learning_rate=0.01,<br/>                                       n_estimators=959,<br/>                                       max_depth=2,<br/>                                       min_samples_split=2,<br/>                                       min_samples_leaf=26)</span><span id="a613" class="ma mb it lw b gy mg md l me mf">sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)<br/>X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)</span><span id="3542" class="ma mb it lw b gy mg md l me mf">model_one.fit(X_train_sm, y_train_sm)<br/>train_auc = cross_val_score(model_one, X_train_smote, y_train_smote, cv=10, scoring='roc_auc')<br/>y_pred = model_one.predict(X_test)<br/>cnf_matrix = confusion_matrix(y_test, y_pred)<br/>roc_auc = roc_auc_score(y_test, y_pred).round(3)</span><span id="dfdf" class="ma mb it lw b gy mg md l me mf">print(classification_report(y_test, y_pred))<br/>print(cnf_matrix)<br/>print('Training CV AUC:', train_auc)<br/>print('Training Avg AUC:', train_auc.mean().round(3))<br/>print('Test Auc:', roc_auc)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/a9137bb27751b69e822d5d61af3e8429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qDpp7DGZC6R3GDp9xy07Og.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">优化模型</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/cc75a924ff02e9ecdddf01b9830104ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aiHCdb6uOL7nQ4lKbkDcYQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基础模型</p></figure><p id="29ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章对您的数据科学事业有所帮助和启发。一如既往，我欢迎任何和所有的反馈。</p></div></div>    
</body>
</html>