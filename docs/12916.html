<html>
<head>
<title>Delta Lake in Action: Upsert &amp; Time Travel</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">三角洲湖在行动:Upsert &amp;时间旅行</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/delta-lake-in-action-upsert-time-travel-3bad4d50725f?source=collection_archive---------11-----------------------#2020-09-05">https://towardsdatascience.com/delta-lake-in-action-upsert-time-travel-3bad4d50725f?source=collection_archive---------11-----------------------#2020-09-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1b90" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在 Apache spark 中使用 Delta lake 的初学者指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/afccadae5c1abdfdc43a902b4025ac48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BEUXxFlYbie1n8r_FouF3A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@goumbik?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">卢卡斯·布拉塞克</a>在<a class="ae kv" href="https://unsplash.com/s/photos/time?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0865" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我介绍 Delta Lake with Apache Spark <a class="ae kv" rel="noopener" target="_blank" href="/delta-lake-with-spark-what-and-why-6d08bef7b963">文章</a>的后续文章，请继续阅读，了解如何使用 Delta Lake with Apache Spark 来执行操作，如更新现有数据、检查以前版本的数据、将数据转换为 Delta 表等。</p><p id="7511" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在深入研究代码之前，让我们试着理解<strong class="ky ir">什么时候</strong>将 Delta Lake 与 Spark 一起使用，因为这并不像我某天醒来就将 Delta Lake 包含在架构中:P</p><p id="fdbe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">可使用三角洲湖泊:</em></p><ul class=""><li id="16e2" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">当处理同一个数据集的<em class="ls">【覆盖】</em>时，这是我处理过的最头疼的问题，Delta Lake 在这种情况下真的很有帮助。</li><li id="1eae" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">当处理有更新的数据时，Delta Lake 的<em class="ls">“merge”</em>功能有助于处理数据中的更新(再见，混乱的连接/过滤操作！！)</li></ul><p id="b4c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，有许多使用 Delta Lake 的用例，但这是两个场景，它们真正帮助了我。</p><p id="b945" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">说够了，让我们深入代码！</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="4825" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">如何用 Apache Spark 运行 Delta Lake</h1><p id="29eb" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">首先，要开始使用 Delta Lake，需要将其添加为 Spark 应用程序的一个依赖项，可以这样做:</p><ul class=""><li id="58ba" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">对于<code class="fe nl nm nn no b">pyspark</code>外壳</li></ul><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="0514" class="nt mp iq no b gy nu nv l nw nx">pyspark --packages io.delta:delta-core_2.11:0.6.1 --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"</span></pre><ul class=""><li id="8405" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">作为 maven 的依赖项，delta lake 可以包含在<code class="fe nl nm nn no b">pom.xml</code>中，如下所示</li></ul><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="2472" class="nt mp iq no b gy nu nv l nw nx">&lt;dependency&gt;<br/>  &lt;groupId&gt;io.delta&lt;/groupId&gt;<br/>  &lt;artifactId&gt;delta-core_2.11&lt;/artifactId&gt;<br/>  &lt;version&gt;0.6.1&lt;/version&gt;<br/>&lt;/dependency&gt;</span></pre><p id="02ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">注意事项:</em></p><ul class=""><li id="044e" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">在这里，<code class="fe nl nm nn no b">2.11</code>是<code class="fe nl nm nn no b">scala</code>版本，如果使用 scala <code class="fe nl nm nn no b">2.12</code>相应地修改版本。</li><li id="d35c" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><code class="fe nl nm nn no b">0.6.1</code>是 Delta Lake 版本，是支持<code class="fe nl nm nn no b">Spark 2.4.4</code>的版本。</li><li id="b891" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">截止到<code class="fe nl nm nn no b">20200905</code>，delta lake 的最新版本是<code class="fe nl nm nn no b">0.7.0</code>，支持<code class="fe nl nm nn no b">Spark 3.0</code></li><li id="710b" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><em class="ls"> AWS EMR 特定:</em>不要将 delta lake 与<code class="fe nl nm nn no b">EMR 5.29.0</code>一起使用，它有已知问题。建议升级或降级 EMR 版本，以便与 Delta Lake 配合使用。</li></ul><h1 id="fc36" class="mo mp iq bd mq mr ny mt mu mv nz mx my jw oa jx na jz ob ka nc kc oc kd ne nf bi translated">如何将数据写成 Delta 表？</h1><p id="0ece" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">Delta lake 使用存储为 Delta 表的数据，因此数据需要以 Delta 表的形式写入。</p><ul class=""><li id="5ceb" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">要将现有数据转换为增量表，需要一次完成。</li></ul><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="65fc" class="nt mp iq no b gy nu nv l nw nx">df = spark.read.parquet("path/of/some/data") # Read existing data</span><span id="fecf" class="nt mp iq no b gy od nv l nw nx">df.coalesce(5).write.format("delta").save("path/of/some/deltaTable")</span></pre><ul class=""><li id="6474" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">将新数据写入增量表</li></ul><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="8f86" class="nt mp iq no b gy nu nv l nw nx">df.coalesce(5).write.format("delta").save("path/of/some/deltaTable")</span></pre><p id="5004" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">注意事项:</em></p><ul class=""><li id="1bae" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">当数据保存为增量表时，该路径包含跟踪增量表元数据的<code class="fe nl nm nn no b">_delta_log</code>目录。</li><li id="da3b" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">编写使用某些列进行分区的 DeltaTable，如下所示:</li></ul><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="6844" class="nt mp iq no b gy nu nv l nw nx">df.coalesce(5).write.partitionBy("country").format("delta").save("path/of/some/deltaTable")</span></pre><h1 id="2272" class="mo mp iq bd mq mr ny mt mu mv nz mx my jw oa jx na jz ob ka nc kc oc kd ne nf bi translated">如何读取增量表？</h1><ul class=""><li id="ea09" class="lt lu iq ky b kz ng lc nh lf oe lj of ln og lr ly lz ma mb bi translated">作为火花数据帧:</li></ul><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="b8d9" class="nt mp iq no b gy nu nv l nw nx">df = spark.read.format("delta").load("path/of/some/deltaTable")As DeltaTable:</span></pre><p id="9822" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将 DeltaTable 作为 Spark 数据帧读取允许对 DeltaTable 进行所有数据帧操作。</p><ul class=""><li id="d43e" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">作为增量表:</li></ul><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="ac8d" class="nt mp iq no b gy nu nv l nw nx">deltaTable = DeltaTable.<em class="ls">forPath</em>("path/of/some/deltaTable")</span></pre><p id="b029" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将数据读取为 DeltaTable 仅支持 DeltaTable 特定函数。此外，DeltaTable 可以转换为 DataFrame，如下所示:</p><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="d7ba" class="nt mp iq no b gy nu nv l nw nx">df = deltaTable.toDF</span></pre><h1 id="9ece" class="mo mp iq bd mq mr ny mt mu mv nz mx my jw oa jx na jz ob ka nc kc oc kd ne nf bi translated">阿帕奇斯帕克的三角洲湖</h1><p id="03ec" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">DeltaTable 上的 UPSERT 操作允许数据更新，这意味着如果 DeltaTable 可以与一些新的数据集合并，并且在一些<code class="fe nl nm nn no b">join key</code>的基础上，可以在 delta 表中插入修改后的数据。这是在 DeltaTable 上进行向上插入的方法:</p><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="8082" class="nt mp iq no b gy nu nv l nw nx">val todayData = # new data that needs to be written to deltaTable</span><span id="d6f8" class="nt mp iq no b gy od nv l nw nx">val deltaTable = DeltaTable.<em class="ls">forPath</em>("path/of/some/deltaTable")</span><span id="958e" class="nt mp iq no b gy od nv l nw nx">deltaTable.alias("oldData")<br/>  .merge(<br/>    todayData.alias("newData"),<br/>    "oldData.city_id = newData.city_id")<br/>  .whenMatched()<br/>  .updateAll()<br/>  .whenNotMatched()<br/>  .insertAll()<br/>  .execute()</span></pre><p id="24d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">似乎很简单，对吗？就是这样！所以接下来会发生的是，如果城市已经存在于<code class="fe nl nm nn no b">deltaTable</code>中，那么<code class="fe nl nm nn no b">todayData</code>中带有城市更新信息的行将在<code class="fe nl nm nn no b">deltaTable</code>中被修改，如果城市不存在，那么行将被插入到<code class="fe nl nm nn no b">deltaTable</code>中，这是我们的基本更新！</p><p id="4805" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">注意事项:</em></p><ul class=""><li id="3118" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">可以使用分区来修改 Upsert 操作，例如，如果需要合并特定的分区，可以指定它来优化合并操作，如:</li></ul><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="920f" class="nt mp iq no b gy nu nv l nw nx">deltaTable.alias("oldData")<br/>  .merge(<br/>    todayData.alias("newData"),<br/>    "(country = 'India') AND oldData.city_id = newData.city_id")<br/>  .whenMatched()<br/>  .updateAll()<br/>  .whenNotMatched()<br/>  .insertAll()<br/>  .execute()</span></pre><ul class=""><li id="6214" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">当<code class="fe nl nm nn no b">deltaTable</code>中的行与连接键匹配或不匹配时，要执行的操作是可配置的，详情请参考文档。</li><li id="f52f" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">默认情况下，合并操作会写很多小文件，为了控制小文件的数量，相应地设置下面的<code class="fe nl nm nn no b">spark conf</code>属性，详细信息请参考文档。</li></ul><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="26b2" class="nt mp iq no b gy nu nv l nw nx">spark.delta.merge.repartitionBeforeWrite true<br/>spark.sql.shuffle.partitions 10</span></pre><ul class=""><li id="e273" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">在成功的合并操作之后，将在<code class="fe nl nm nn no b">_delta_log</code>目录中创建一个日志提交，浏览其内容以了解 Delta Lake 的工作方式可能会很有趣。</li><li id="f27c" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">合并只支持一对一的映射，即只有一行应该尝试更新<code class="fe nl nm nn no b">DeltaTable</code>中的一行，如果多行尝试更新<code class="fe nl nm nn no b">DeltaTable</code>中的同一行，则合并操作失败。预处理数据来解决这个问题。</li></ul><h1 id="90f9" class="mo mp iq bd mq mr ny mt mu mv nz mx my jw oa jx na jz ob ka nc kc oc kd ne nf bi translated">阿帕奇星火中的三角洲湖时光旅行</h1><p id="b8d8" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">Delta Lake 支持返回到先前数据版本的功能，同样可以通过指定要读取的数据版本来实现:</p><pre class="kg kh ki kj gt np no nq nr aw ns bi"><span id="3e98" class="nt mp iq no b gy nu nv l nw nx">df= spark.read.format("delta").option("versionAsOf", 0).load("path/of/some/deltaTable")</span></pre><p id="99b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">版本化从<code class="fe nl nm nn no b">0</code>开始，在<code class="fe nl nm nn no b">delta_log </code>目录中有多少日志提交，就会有多少数据版本。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="6b0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">似乎，简单吧？</p><p id="31a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">供参考:</p><div class="oh oi gp gr oj ok"><a href="https://docs.delta.io/latest/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">欢迎来到三角洲湖文档</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">学习如何使用三角洲湖。</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">文档增量 io</p></div></div></div></a></div><p id="3ab2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">下次见，<br/> Ciao。</em></p></div></div>    
</body>
</html>