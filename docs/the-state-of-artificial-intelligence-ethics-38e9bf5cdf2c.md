# 人工智能伦理的现状

> 原文：<https://towardsdatascience.com/the-state-of-artificial-intelligence-ethics-38e9bf5cdf2c?source=collection_archive---------26----------------------->

# 一些清晰的信号来自刺耳的噪音

![](img/1186008f9cc75bcdb0026fb9426a9821.png)

面部识别等使用人工智能/人工智能的工具可能会变成控制和压迫的机制。[图片来自 Pexels 的陈堃]

在他的博客文章“人脸识别和人工智能的伦理”中，Benedict Evans 谈到了在 20 世纪 70 年代和 80 年代初，人们对关系数据库有两种类似的担忧，就像我们今天对人工智能系统的担忧一样——首先，这些数据库将包含坏数据或坏假设(因为我们担心我们的偏见被融入人工智能系统)，其次，这些数据库将被故意用来制造坏事来伤害人们(因为我们担心人工智能系统的邪恶应用)。担心是否有效，担心是否无效。

我们对人工智能系统的担忧在两个重要方面与我们对数据库的担忧略有不同——首先，人工智能系统正在拾取人类不知道存在的信号(棘手的复杂性)，其次，先进的人工智能系统可以提供正确的声音答案，这些答案在非显而易见的方面是错误的(无意的欺骗)。该领域的人们越来越清楚，我们需要解决这两种行为。

谨慎乐观是好事。一些组织和政府为理解和解决这些担忧做出了合理的努力。在过去的几年里，关于人工智能伦理的文章激增，似乎来自所有与技术有关的组织，无论是国家政府还是大型科技公司。它们以指导方针、政策文件、原则或战略的形式出现，并伴随着一些委员会或董事会来监督其实施。哈佛大学伯克曼·克莱恩中心的研究人员撰写了一篇像样的论文，总结了对 36 份此类文件的分析，可以在这里找到。出现了八个关键主题——隐私、问责制、安全和安保、透明度和可解释性、公平和不歧视、人类对技术的控制、职业责任、促进人类价值观。值得注意的是，人类控制技术和促进人类价值是这 36 份文件中最少提及的原则。作者以一句警告的话结束了“在这些高层次概念的表达和它们在现实世界中的实际成就之间存在着广泛而棘手的差距”，即写关于伦理的政策文件固然很好，但如何将其转化为行动呢？

嗯，这很难。伦理是本土化的。试图将自己的本土伦理观点强加于一个全球性现象会带来挑战。有一些善意的努力将人工智能伦理原则转化为行动。我将它们大致分为技术、政策、人员和监管类别，尽管所有类别都错综复杂地联系在一起。

# **技术**

技术工作包括实现允许更好的机器学习可解释性的技术工具(例如，使用本地可解释的模型不可知解释，或 LIME，一种模型不可知工具，用于提供对分类器为什么预测某些事物的解释；或使用深度学习重要特征，或 DeepLIFT，一种用于识别神经网络中的哪些输入在使用反向传播预测某个输出时最重要的方法)；算法可解释性(使用逐层相关性传播，LRP 或类似)；以及算法去偏置(例如，通过使用模型对抗去偏置、变分自动编码器，或者通过训练数据的动态上采样，或者分布鲁棒优化)。单独实施技术措施来建立道德人工智能是有限的，因为社会偏见更加深刻和系统化——因此需要与政策、人员和法规紧密结合。

# **政策**

拥有通过设计倡导算法公平的政策(拥有问责制、透明度和责任)是实现人工智能伦理原则的一个重要里程碑。问责制意味着证明一个人的设计决策的能力——这将需要在人工智能系统的操作背景下更深入地钻研道德价值观和社会规范——并需要涵盖指导功能(我们选择这样做，而不是那样)和解释功能(这是我们为什么这样设计系统)。问责制是对组织内的其他人、外部机构(监管机构)或受人工智能系统影响的个人(或团体)负责。创建一个透明的人工智能系统意味着能够描述这些系统做什么，复制它，并解释它们如何做出决策和适应它们的环境，并展示它们使用或创建的数据治理机制。责任意味着能够为组成人工智能系统的设计决策指定一个人类所有者(或一组人类所有者)。

# **人**

了解设计决策的制定者和受这些决策影响的人非常重要，了解他们的能力和局限性也很重要。构建人工智能系统的团队需要仔细挑选——确保这些团队是多样化的，并且对人工智能解决方案所针对的问题有良好的接触，这一点很重要。当构建一个人工智能系统时，人们的偏好、偏见和历史偏见应该被讨论并至少被仔细承认，如果不能避免的话(因为有时避免这些既不可行也不合适)。在发展的同时，应该进行公平透明的协商，征求所有利益攸关方的意见。需要授权参与端到端流程的人员提供充分的监督，并且应该仔细研究对所有相关利益方的影响和益处。

# **监管**

法规无法阻止人工智能实施中的错误——这些错误将不可避免地发生。然而，他们可以要求一个审计过程来发现这些错误，并通过处罚或禁止来帮助纠正这些错误。监管必须在适当的焦点水平上执行——太宽泛的话，你就有成为极权主义者的风险。强制开发人员遵循软件工程中的最佳实践——记录责任意味着我们将需要在检查这些最佳实践方面具有专业知识的监管机构。人权应该被视为道德人工智能系统的核心——这些机构应该能够提供保护，防止非法歧视、不公平做法、失去自由、增加监控、刻板印象强化、要人伤害和社会污名化等。如果治理是由再分配定义的，我们需要治理人类对技术的应用。

![](img/c49004c2a9dfd74c2aeed99e5fda06e5.png)

人工智能伦理警告需要转化为一个强大的监管框架[图片来自 Pexels 的 Fernando Arcos

在过去的几十年里，许多工作场所已经意识到他们招聘系统中的偏见。他们已经采取了系统的措施来最大限度地减少这些偏见——在内部建立员工的招聘意识，使用经过验证的评估，标准化面试流程，优先考虑多样性和包容性，改变广告渠道和外联。除了同质的工作场所比多样化的工作场所效率低这一事实之外，雇佣歧视还有严重的法律后果。在美国，1964 年民权法案第七章和相关法律保护免受年龄、性/性别、性取向、种族、宗教、残疾、怀孕(或其他病史)和基因歧视。同样，在欧盟,《就业平等框架指令》和《种族平等指令》为维持公平公正的招聘程序提供了法律支持。结构是存在的，我们不需要从头开始。

问题是——如果招聘是使用人工智能系统自动化的，偏见(以及歧视行为)会变得更难检测吗？招聘算法是由人类设计、开发、训练、测试和部署的——如果歧视被确定，我们需要追究建立、拥有和操作这个系统的人的责任，而不是把我们的手扔到空中，声称“是人工智能干的”。还有一个相反的论点——机器学习算法将会进化——现在适用于一套人工智能系统的一套去偏置工具将会随着新的算法集的进化而迅速过时。今天的算法还处于起步阶段。随着算法越来越多地模仿人类水平的思维，我们转移到这些算法上的固有偏见也将变得更加复杂，反映出当前现有的社会偏见。

通过我们的监管流程，我们可以强制要求审计流程承认并避免这些偏见。然而，我认为我们必须超越偏见。算法是有偏差的，因为人是有偏差的——偏差不是 AI 的特殊特征，而是人性固有的一部分。当我们将我们的知识转移到人工智能系统上时，我们也将我们的偏好和刻板印象转移到它身上。这些偏好和刻板印象反映在我们的社会结构中，其中一些我们认识到需要改变，因此集体同意改变它们。规范、政策和法律是谈判的产物，是这种变化的工具——它们清楚地表明什么行为被认为是不好的，以及实施这些行为的后果。这些规范、政策和法律需要适用于设计人工智能系统的人类，就像它们适用于设计药物的人类一样。即使是作为法律实体的公司也是人类意图的延伸，它们的行为可以(也确实)与其人类代表联系在一起。创建一个人工智能系统需要人类来设计这些系统。这些设计决策可以被文档化，进而可以被审计。这种设计和意图的标准是可以设定和执行的。所有人类活动都发生在某种监管框架的背景下；因此，大多数人工智能系统已经受到监管，因为绝大多数人工智能已经并将可能继续由商业驱动。

在我开始提到的八个主题中，人类价值的提升对人工智能伦理的影响最小。将这一点带入中心舞台可以打破平衡，有利于一个伦理驱动的人工智能采用时代。