<html>
<head>
<title>Simple Linear Regression: What’s inside?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单线性回归:里面是什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-linear-regression-whats-inside-702475055ad5?source=collection_archive---------35-----------------------#2020-07-03">https://towardsdatascience.com/simple-linear-regression-whats-inside-702475055ad5?source=collection_archive---------35-----------------------#2020-07-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c324" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让我们深入研究简单线性回归的数学意义并实现它。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/af5ad3efe7e1367ea236222895955f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bLX4iybhhh8tnB-Xwcpokg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">第 0 步:开始</strong></p></figure><p id="e39f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回归是一种统计方法，建议在其他自变量(数据)的帮助下预测因变量(目标特征)。回归是最广为人知和理解的统计方法之一。</p><p id="37dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归是一种假设因变量和自变量之间存在线性关系的模型。线性回归进一步分为简单线性回归(SLR)和多元线性回归(MLR)。我们将探讨一元线性回归，即一个因变量和一个自变量的回归，因为它简单。SLR 的数学是许多其他机器学习模型的基础。</p><p id="5ef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我将详细阐述简单的线性回归，以获得关于它如何工作的直觉。我将使用一个 NBA 比赛得分数据集(下面的链接)来演示单反，并最终将其与 Scikit-learn 的线性回归模型进行比较。</p><h2 id="9d45" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><em class="mo">简单线性回归</em></h2><p id="9e40" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">要理解单反，我们来分解一下必须要经历的概念</p><ul class=""><li id="3426" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">SLR 线及其系数</li><li id="214d" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">损失函数</li><li id="6ffc" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">梯度下降</li><li id="1de9" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">导出系数(可选)</li></ul><h2 id="2351" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">SLR 线及其系数</h2><p id="5612" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">直线的斜率截距形式为 Y= MX+B。</p><p id="5ca9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">y 是因变量(目标)，X 是自变量(数据)，M 和 B 是直线的特征。斜率(M)给出了相关变量 X 和 Y 的关系，截距(B)给出了当变化率被消除时因变量的值的信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/a252ef3e67cbe610446779f14882b774.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*9AdmI78JqW2IFDB3bmNwRg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae nj" href="https://www.onlinemath4all.com/images/slopeinterceptform1.png" rel="noopener ugc nofollow" target="_blank"> onlinemath4all </a></p></figure><p id="7d4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在单反中，等式写成<strong class="lb iu"> y = b0 + x b1。</strong> b0 和 b1 分别是截距和斜率。它们由下面给出的公式确定，以找到最佳拟合线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/d91bd5a8d90d1f48501f2bd16c367d61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0SbFLgwdaNrgsJ6wz6sKbw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/6fec87e1b5ecce3cca71c56609837837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Ohp6oqE7swRRWRmMBhA1w.png"/></div></div></figure><p id="fb1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，在回归中情况并不总是如此。让我们在梯度下降部分了解一下为什么。如果你对我们如何偶然发现这些感到好奇，请查看可选部分。</p><h2 id="35bb" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">损失函数</h2><p id="78fb" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">损失函数是表明预测值与实际值偏离程度的一种度量。有很多损失函数可用，我们将着眼于均方差(MSE)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/d4073771c4874e5270ac5914d9f71cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-M7qSa31k_Y2YWoZoxP7Q.png"/></div></div></figure><p id="4f73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MSE，顾名思义，对每条记录的实际值和预测值之差求平方，求和，然后除以记录数。我们的目标是找到一个损失最小的模型。</p><h2 id="0f50" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">梯度下降</h2><p id="562f" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">梯度下降是一种优化算法，它迭代地更新参数，以找到损失最小的模型。具有一个或两个参数的模型的损失函数可以被部分微分以找到最小值。但是随着维度的增加，很难将参数可视化，更不用说每个解的特征值了。由于局部最小值的多次出现，我们将不得不遍历特征值的所有组合，以确保我们找到了全局最小值。尽管全局最小值问题没有完全解决，梯度下降有助于找到高阶模型的最小值。</p><p id="efb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们还没有探索问题的基础:损失函数。MSE(作为二次函数)保证曲线上总有一个点的梯度为零，但有些损失函数不能保证梯度为零的点，或者梯度为零的点可能不总是全局最小值。为了克服这个问题，采用了梯度下降法。</p><p id="d537" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体到我们的情况，我们可以选择通过上面给出的公式来找到系数，或者我们可以从随机非零值开始，让它以最佳方式工作。梯度下降算法的数学意义本身就值得写一篇文章。现在，我将通过直觉来实现这个算法。数学方法类似于系数，我认为把它包括进来是多余的(如果你好奇，我会在最后把它联系起来)。</p><p id="cedc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们看看梯度下降是怎么回事。想象一个人在没有视觉的情况下徒步下山；这个人的目标是到达谷底。凭直觉，他向前迈了一步，如果坡度是向下的，他将继续前进，直到遇到坡度的变化。一旦这个人在移动中感觉不到提升，他/她就会停下来。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/05a7b1de4f018ea495f76e7adc9bc2bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nAUzoWNPMkIYUeDlDRfdGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae nj" href="https://www.kdnuggets.com/2018/06/intuitive-introduction-gradient-descent.html" rel="noopener ugc nofollow" target="_blank"> kdnuggets </a></p></figure><p id="6a93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是如上所述，采取确定长度的步骤然后评估路线修正是没有意义的，因为这个人可能已经通过了最小值，却意识到他/她向错误的方向移动。这就是学习率发挥作用的地方。它惩罚大步，以确保该人不采取一步超过最低限度。</p><p id="a7bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人如何选择学习速度？</p><p id="4bcd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，学习率没有“万能”的标准。我们可以粗略估计这个值的一种方法是通过反复试验。我们必须注意的问题是学习率的高低。这两种方法的计算量都很大，所以在开始时总是要运行几次模型来检查损失的变化。浪费的计算能力和时间。</p><p id="096e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个适合高学习率的类比是金属球轴承和碗。当一个球从碗的边缘被释放出来时，它的速度会随着力的方向趋向最小值而不断增加。当它经过最小值时，球的方向会远离最小值，但作用在球上的力的方向会相反。这样，通过一些有损耗的振荡，它最终在碗的底部达到稳定的平衡。有损耗的振荡是计算能力和时间的浪费。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/52ada1a7b4913af5ef856ce52c8a8e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yVa3ptGCkTkNewMyw0u1jw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae nj" href="https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/images/lr1.png" rel="noopener ugc nofollow" target="_blank">深度学习向导</a></p></figure><p id="be4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们把这个和单反合并一下。我们所指的人是 b0 和 b1 系数。我们看到的谷是 MSE 相对于 b0 和 b1 参数绘制的曲线。学习率(alpha)为我们提供了修改参数的步长，而不必在每次通过时跳过最小值。</p><p id="0071" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">推导系数(可选)</strong></p><p id="06a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然不是关键，但探索模型的机制很有趣。让我们首先建立基本方程和推导方程的术语。在我们开始之前，确保你熟悉微积分的基础知识。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/22cf105daf39cccde6c6e5d333e8e613.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I0KpcJ-pCX1tbWljJmFz9g.png"/></div></div></figure><p id="6372" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们都有了方程，让我们开始研究 b0 和 b1。现在，如果我们绘制任何一个系数(保持另一个不变)对 E 的曲线，它看起来会像这样</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/206668040784518346823297053ad23e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*ellw5xGIE-dCHff_UGFr0w.png"/></div></figure><p id="5bec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数的最小值可以通过对函数相对于系数进行部分微分并使梯度等于零来找到。现在我们的目标是</p><ul class=""><li id="dce3" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">为了找到两个系数的梯度为零(近似)的点</li><li id="a677" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">找出一个仅是给定数据的函数的方程。</li></ul><p id="ff57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">找到 b0 </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/d44cac632e94940c6878f26b292c1bb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vaR2rkpbWph4t9i3zZ7wgw.png"/></div></div></figure><p id="94d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在求最小值，将梯度等于零。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/bfe416d9d565014c171345806ce4a496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9T4jA-MGx66-GjV_94ZM1g.png"/></div></div></figure><p id="b2ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然我们确实找到了 b0 的值，但是它依赖于 b1。</p><p id="e7a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">找到 b1 </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/9d6578c478edc1770f7fcdae520e5013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EVI7Oqy4IzmxsMzMALxBrQ.png"/></div></div></figure><p id="6ce7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代入 b0 的值并部分微分</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/8aadf07124f01c395944ee173eee5dc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CzePgHR7BHvkqakvhZZm_Q.png"/></div></div></figure><p id="aa2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似于 b0，将梯度等于零</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/58addb0d3e6981f7c7f0ba5fd5e4ac81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3aCJQYUuvP0My_guawweIw.png"/></div></div></figure><p id="e1e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这检验了我们的目标；我们找到了 b1 的方程式，它只依赖于我们现有的数据。我们能够利用曲线的基本性质找到这些值。</p><p id="ad2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nv">实现</em> </strong></p><p id="a5a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们处理数据，然后使用</p><ul class=""><li id="bbc2" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">系数</li><li id="7a36" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">梯度下降</li><li id="db64" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">Scikit-learn 线性回归</li></ul><p id="0f92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">数据</strong></p><p id="38ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用标准 python 库，让我们导入数据并可视化分布。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="509f" class="lv lw it nx b gy ob oc l od oe"><strong class="nx iu">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/></strong>%matplotlib inline</span><span id="aebb" class="lv lw it nx b gy of oc l od oe"><em class="nv">#load data</em></span><span id="64a7" class="lv lw it nx b gy of oc l od oe">df=<strong class="nx iu">pd.read_csv</strong>('teampts_fg.csv')<br/>X=df['TeamPoints'].<strong class="nx iu">values</strong><br/>y=df['FieldGoals'].<strong class="nx iu">values</strong></span><span id="126b" class="lv lw it nx b gy of oc l od oe"><em class="nv">#plot data</em></span><span id="84d1" class="lv lw it nx b gy of oc l od oe"><strong class="nx iu">plt.rcParams</strong>["figure.figsize"] = (12,8)<br/><strong class="nx iu">plt.xlim</strong>(40,160)<br/><strong class="nx iu">sns.scatterplot</strong>(x='TeamPoints', y='FieldGoals', data=df);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/9bbd27728a28b1129947d3f4dfece004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hwm8gyFi7F-UOoRURdX3ZQ.png"/></div></div></figure><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="95ef" class="lv lw it nx b gy ob oc l od oe"><em class="nv">#define the loss function</em></span><span id="15f0" class="lv lw it nx b gy of oc l od oe"><strong class="nx iu">def</strong> loss(data, data_pred):<br/>    N=data.<strong class="nx iu">shape</strong>[0]<br/>    loss=<strong class="nx iu">np.sum</strong>(<strong class="nx iu">np.square</strong>(data-data_pred))/N<br/>    <strong class="nx iu">return</strong> loss</span></pre><p id="e983" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用系数实现模型</strong></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="a0e3" class="lv lw it nx b gy ob oc l od oe">Bc1=<strong class="nx iu">sum</strong>((X-<strong class="nx iu">np.mean</strong>(X)) * (y - <strong class="nx iu">np.mean</strong>(y)))/<strong class="nx iu">sum</strong>((X-<strong class="nx iu">np.mean</strong>(X))**2)<br/>Bc0=np.mean(y) - Bc1*(np.mean(X))<br/>x_coeff_model=np.linspace(40,160,1000)<br/>y_coeff_model= Bc1*x_coeff_model + Bc0</span><span id="e6f6" class="lv lw it nx b gy of oc l od oe"><em class="nv">#plot the line with the original data</em></span><span id="f93a" class="lv lw it nx b gy of oc l od oe"><strong class="nx iu">plt.rcParams</strong>["figure.figsize"] = (12,8)<br/><strong class="nx iu">plt.xlim</strong>(40,160)<br/><strong class="nx iu">sns.scatterplot</strong>(x='TeamPoints', y='FieldGoals', data=df);<br/><strong class="nx iu">plt.plot</strong>(x_coeff_model, y_coeff_model,c='r');<br/><strong class="nx iu">plt.show</strong>()<br/><strong class="nx iu">print</strong>("The value of b0 is {} and b1 is {}".format(Bc0, Bc1))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/f7ea720547d7b17a1435a3eacdc8db79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*meK_CrkuXkuXWThzO2n91Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用系数的最佳拟合线</p></figure><p id="0a6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用梯度下降实现模型</strong></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="96f8" class="lv lw it nx b gy ob oc l od oe">loss_history=[]</span><span id="77bb" class="lv lw it nx b gy of oc l od oe"><em class="nv">#define gradient descent</em></span><span id="8b64" class="lv lw it nx b gy of oc l od oe"><strong class="nx iu">def</strong> gradient_descent(epochs, X , y , alpha):<br/>    B0=0.0001<br/>    B1=0.0001<br/>    N=X.<strong class="nx iu">shape</strong>[0]<br/>    for i in np.arange(epochs):<br/>        y_pred= B0 + B1*X<br/>        loss_history.append(loss(y,y_pred))</span><span id="69db" class="lv lw it nx b gy of oc l od oe">        dB0=(-2/N)*<strong class="nx iu">np.sum</strong>(y-y_pred)        <br/>        dB1=(-2/N)*<strong class="nx iu">np.sum</strong>(X*(y-y_pred))</span><span id="8be4" class="lv lw it nx b gy of oc l od oe">        B0= B0- alpha*dB0<br/>        B1= B1- alpha*dB1</span><span id="9a91" class="lv lw it nx b gy of oc l od oe">    <strong class="nx iu">return </strong>[B0,B1]</span><span id="2894" class="lv lw it nx b gy of oc l od oe"><em class="nv">#call the gradient_descent function for 30 iterations on the data</em></span><span id="25bc" class="lv lw it nx b gy of oc l od oe">Bgd0,Bgd1=<strong class="nx iu">gradient_descent</strong>(30, X, y, 0.00001)</span></pre><p id="f568" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在根据获得的参数绘制直线。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="4dd0" class="lv lw it nx b gy ob oc l od oe">x_gd_model=<strong class="nx iu">np.linspace</strong>(40,160,1000)<br/>y_gd_model= Bgd1*x_coeff_model + Bgd0<br/><strong class="nx iu">plt.rcParams</strong>["figure.figsize"] = (12,8)<br/><strong class="nx iu">plt.xlim</strong>(40,160)<br/><strong class="nx iu">sns.scatterplot</strong>(x='TeamPoints', y='FieldGoals', data=df);<br/><strong class="nx iu">plt.plot</strong>(x_gd_model, y_gd_model,c='r')<br/><strong class="nx iu">plt.show</strong>()<br/><strong class="nx iu">print</strong>("The value of b0 is {} and b1 is {}".format(Bc0, Bc1))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/a50d81cd95ba64ee0a24fdb47a1b3698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-K0vn-GYBhDAaKSZsTgSrg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用梯度下降的最佳拟合线</p></figure><p id="930b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我以很小的学习率存储了几百次迭代的每个值，以可视化模型如何成熟。使用 pyplot，我保存了所有的图来制作一个 gif。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0384a62b10e65df73eeccde63f03bd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*yHW_QybK7UmWr4fkbGgCBQ.gif"/></div></div></figure><p id="a219" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们绘制模型的损失历史。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="6bcb" class="lv lw it nx b gy ob oc l od oe"><strong class="nx iu">plt.plot</strong>(loss_history);<br/><strong class="nx iu">plt.xlabel</strong>("Number of iterations")<br/><strong class="nx iu">plt.ylabel</strong>("Loss")<br/><strong class="nx iu">plt.show</strong>()<br/><strong class="nx iu">print</strong>("Loss at iteration 25 is {}".format(loss_history[24]))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/23e3641d8cee5347a9e015ad9712ef58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lE_aWx4he5B36ABDWejbbQ.png"/></div></div></figure><p id="1df0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用 scikit-learn 实现模型</strong></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="c661" class="lv lw it nx b gy ob oc l od oe"><strong class="nx iu">from</strong> sklearn.linear_model <strong class="nx iu">import</strong> LinearRegression<br/>lr= <strong class="nx iu">LinearRegression</strong>()<br/>lr.<strong class="nx iu">fit</strong>(X.<strong class="nx iu">reshape</strong>(-1,1),y.<strong class="nx iu">reshape</strong>(-1,1))<br/>x_lr_model=<strong class="nx iu">np.linspace</strong>(40,160,1000)<br/>y_lr_model=lr.<strong class="nx iu">predict</strong>(x_lr_model.<strong class="nx iu">reshape</strong>(-1,1)).<strong class="nx iu">reshape</strong>(1,-1)[0]</span><span id="09a4" class="lv lw it nx b gy of oc l od oe"><em class="nv">#plot the line</em></span><span id="42d9" class="lv lw it nx b gy of oc l od oe"><strong class="nx iu">plt.rcParams</strong>["figure.figsize"] = (12,8)<br/><strong class="nx iu">plt.xlim</strong>(40,160)<br/><strong class="nx iu">sns.scatterplot</strong>(x='TeamPoints', y='FieldGoals', data=df);<br/><strong class="nx iu">plt.plot</strong>(x_lr_model, y_lr_model,c='r')<br/><strong class="nx iu">plt.show</strong>()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/8b2bf23dc6bee5420e4ef6c54e27eaea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z7LAwh1ktY0vzd7JmagOEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用 scikit-learn 的最佳拟合线</p></figure><p id="58a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">对比</strong></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="80f9" class="lv lw it nx b gy ob oc l od oe"><strong class="nx iu">plt.plot</strong>(x_lr_model, y_lr_model,c='b', label="LinearRegression");<br/><strong class="nx iu">plt.plot</strong>(x_gd_model, y_gd_model, c='r', label="Gradient Descent");<br/><strong class="nx iu">plt.plot</strong>(x_coeff_model, y_coeff_model, c='g', label="Coefficient");<br/><strong class="nx iu">plt.legend</strong>()<br/><strong class="nx iu">plt.show</strong>()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/8edcf428247043828380a63b74cd521d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WDovx3KAXASXv_YVF1vR6Q.png"/></div></div></figure><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="bfa2" class="lv lw it nx b gy ob oc l od oe"><em class="nv">#let's print the loss between lines to see the difference</em></span><span id="e9e7" class="lv lw it nx b gy of oc l od oe"><strong class="nx iu">print</strong>("The loss between Coeff model and Gradient descent {}".format(loss(y_coeff_model,y_gd_model)))<br/><strong class="nx iu">print</strong>("The loss between Coeff model and Linear Regression Model {}".format(loss(y_coeff_model,y_lr_model)))<br/><strong class="nx iu">print</strong>("The loss between Gradient descent and Linear Regression Model {}".format(loss(y_gd_model,y_lr_model)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/ebc56fef399da95f741eb634cf998c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aj-XiAxcfE5gSGHw9cXfnw.png"/></div></div></figure><p id="1be4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的比较图中，我们可以看到线性回归线和系数线重叠。但是另一方面，梯度下降线稍微偏向线性回归线。这是什么意思？我们来调查一下。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="9297" class="lv lw it nx b gy ob oc l od oe">loss_gd=loss_history[-1]<br/>loss_lr=loss(Bc0+Bc1*X, y)<br/><strong class="nx iu">print</strong>("The loss of the LinearRegression line is {}".format(loss_lr))<br/><strong class="nx iu">print</strong>("The loss of the GradientDescent line is {}".format(loss_gd))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/510ca66af6d6184933912f8be1dc35a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45eGDts3vMnewbsKXlAkcw.png"/></div></div></figure><p id="a966" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管线的损失有所不同，但所有模型的预测值和实际值之间的损失是非常相似的。我们可以有把握地推断梯度下降线——尽管它没有相同的参数——已经以近似等于理想状态的方式定位了自己。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/21400222ede67783b568c7c540d67473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wD66nhPnr3vsZqAv_WEOCQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">步骤 steps_count[-1]:结束</strong></p></figure><p id="1b09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，这是简单线性回归的内容。谢谢你。</p></div><div class="ab cl on oo hx op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="im in io ip iq"><blockquote class="ou ov ow"><p id="a1ff" class="kz la nv lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated">参考</p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pa pb l"/></div></figure><div class="pc pd gp gr pe pf"><a href="https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">如何用 Python-Machine Learning mastering 实现简单的线性回归</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">线性回归是一种已有 200 多年历史的预测方法。简单的线性回归是一个很好的开端…</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">machinelearningmastery.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt ks pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a rel="noopener follow" target="_blank" href="/linear-regression-using-gradient-descent-97a6c8700931"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">使用梯度下降的线性回归</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">在本教程中，你可以学习梯度下降算法的工作原理，并从头开始用 python 实现它。首先…</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pu l pq pr ps po pt ks pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a href="https://www.kaggle.com/ionaskel/nba-games-stats-from-2014-to-2018" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">2014 年至 2018 年 NBA 球队比赛统计</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">2014 - 2018 nba 期间每场比赛的统计数据</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">www.kaggle.com</p></div></div></div></a></div><blockquote class="ou ov ow"><p id="5d8f" class="kz la nv lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated">除引用图片外，所有图片均由作者制作</p></blockquote></div></div>    
</body>
</html>