<html>
<head>
<title>K-means and PCA for Image Clustering: a Visual Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-means 和 PCA 在图像聚类中的可视化分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-and-pca-for-image-clustering-a-visual-analysis-8e10d4abba40?source=collection_archive---------9-----------------------#2020-06-03">https://towardsdatascience.com/k-means-and-pca-for-image-clustering-a-visual-analysis-8e10d4abba40?source=collection_archive---------9-----------------------#2020-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a6b4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们可以使用 K-means 和主成分分析(PCA)对时尚 MNIST 数据集上的图像进行聚类。我们还将使用<a class="ae ki" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"> matplotlib </a>和<a class="ae ki" href="https://plotly.com/" rel="noopener ugc nofollow" target="_blank"> plotly </a>对结果进行可视化分析。</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/436a215b63a7a15e3c47bd2429246865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ogbAotjStIKLG4TyLzzDtQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">时尚 MNIST 数据集。图片来自 researchgate.net<a class="ae ki" href="https://www.researchgate.net/figure/Examples-from-the-Fashion-MNIST-dataset_fig6_333997546" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="5703" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里要做的是在 f-MNIST 数据上训练一个 K-means 聚类模型，以便它能够相对准确地对数据集的图像进行聚类，并且这些聚类具有一些我们可以理解和解释的逻辑。然后，我们将参考实际标签(y)使用 matplotlib 和 plotly 直观地分析聚类的结果，并得出关于 k-means 聚类如何在图像数据集上执行的粗略结论。最后的代码可以在最后的链接中找到。</p><p id="df15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，<em class="lv">功能</em>和<em class="lv">组件</em>可互换使用。</p><p id="4e39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ki" href="https://github.com/sunman91/Data-Science-stuff/blob/master/k-means-and-pca-on-f-mnist-dataset.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="lv">直奔 Github 上的代码。</em> </strong> </a></p><h1 id="a2e9" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">介绍</h1><p id="1608" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">聚类</strong>是一种无监督的机器学习算法，它识别没有特定标签的模式，并根据特征对数据进行聚类。在我们的例子中，我们将看到一个聚类算法(k-means)是否可以在没有标签(y)的情况下在 f-MNIST 中找到不同服装图像之间的模式。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mt"><img src="../Images/31929f92a50da839ef38449ca7072c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/1*JUm9BrH21dEiGpHg76AImw.gif"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">说明 K-means 如何工作的 gif。每个红点是一个质心，每种不同的颜色代表一个不同的集群。每一帧都是质心被重新定位的迭代。(来源:<a class="ae ki" href="https://gfycat.com/softenragedhypsilophodon" rel="noopener ugc nofollow" target="_blank"> gyfcat </a>)</p></figure><p id="9d96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> K-means </strong>聚类的工作原理是根据给定的聚类数分配多个质心。每个数据点被分配给质心最接近它的聚类。该算法旨在最小化观测值与其所属聚类质心之间的平方欧氏距离。</p><p id="647e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">主成分分析或 PCA </strong>是一种减少给定数据集的维度，同时仍然保留其大部分方差的方法。维基百科将其定义为，“<em class="lv"> PCA 被定义为一种正交线性变换，它将数据变换到一个新的坐标系，使得数据的一些标量投影的最大方差位于第一个坐标上(称为第一主分量)，第二个最大方差位于第二个坐标上，以此类推。”</em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mu"><img src="../Images/ea81306d3cd9a904a92c387ab7e7e0b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*UpFltkN-kT9aGqfLhOR9xg.gif"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">PCA 可视化。最好的 PC(黑色移动线)是那些红线的总长度最小的时候。它将被用来代替水平和垂直分量(来源:<a class="ae ki" href="https://giphy.com/gifs/pca-Lyejb62QjQepG" rel="noopener ugc nofollow" target="_blank"> giphy </a></p></figure><p id="03fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基本上，PCA 降低了数据集的维度，同时保留了大部分信息。例如，如果一个数据集有 500 个特征，根据保留的特定方差量，它会减少到 200 个特征。保留的方差越大，保存的信息就越多，但是得到的维数也就越多。</p><p id="8823" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">维度越少，训练和测试模型的时间就越少。在某些情况下，使用数据集和 PCA 的模型比原始数据集表现更好。PCA 的概念以及它通过改变保留方差而在图像上引起的变化在这里<a class="ae ki" rel="noopener" target="_blank" href="/pca-using-python-scikit-learn-e653f8989e60">处</a>得到了很好的展示。</p><h1 id="7d44" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">这个计划</h1><p id="b877" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">因此，计划是在数据集上执行 k-means，但只是在对其应用 PCA 之后。</p><ol class=""><li id="5901" class="mv mw it lb b lc ld lf lg li mx lm my lq mz lu na nb nc nd bi translated">从 keras 加载数据集</li><li id="edbb" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">预处理数据，展平数据(从 60000 x 28 x 28 数组到 60000 x 784 数组)</li><li id="68ca" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">对其应用 PCA 以减少维度(使用 0.98 方差从 784 到 420)</li><li id="1ef5" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">在 PC 数据集上应用 K 均值聚类(10 个聚类)</li><li id="67dc" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">使用 matplotlib 和 plotly 观察和分析结果</li></ol><h1 id="d5d6" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">数据集</h1><p id="c27d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">与 MNIST 数字数据集相比，f-MNIST 数据集是一个稍微高级的数据集，它由不同种类服装的 28 x 28 像素图像组成，数字代表以下服装类型。</p><pre class="kk kl km kn gt nj nk nl nm aw nn bi"><span id="01b4" class="no lx it nk b gy np nq l nr ns">#Loading required libraries<strong class="nk iu"><br/>import keras<br/>from keras.datasets import fashion_mnist <br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.cluster import KMeans</strong><br/></span><span id="8a2e" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">#Loading the dataset<br/>(X_train,y_train), (X_test,y_test) = fashion_mnist.load_data()</strong></span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/436a215b63a7a15e3c47bd2429246865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ogbAotjStIKLG4TyLzzDtQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">时尚 MNIST 数据集</p></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nu"><img src="../Images/d7c17234ddf42bdde76e111c720fba39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iLss7dBAEt9zgNWB.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">每张图片都是 28×28 像素，上面有各种各样的衣服、鞋子和包包</p></figure><h1 id="a346" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">数据预处理</strong></h1><p id="8d6a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们通过将特征展平成一维来将数据从 3D 阵列重新成形为 2D 阵列。28×28 阵列的每个图像现在是 784 个特征的单个阵列。所以我们的数据集从之前的 60000，28，28 变成了 60000，784 数组。</p><pre class="kk kl km kn gt nj nk nl nm aw nn bi"><span id="ec47" class="no lx it nk b gy np nq l nr ns">#Reshaping X to a 2D array for PCA and then k-means<br/><strong class="nk iu">X = X_train.reshape(-1,X_train.shape[1]*X_train.shape[2])</strong> #We will only be using X for clustering. No need of y.</span><span id="861e" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">print ("The shape of X is " + str(X.shape))<br/>print ("The shape of y is " + str(y.shape))</strong> #We will be using y only to check our clustering</span><span id="30b8" class="no lx it nk b gy nt nq l nr ns"><br/><strong class="nk iu">Output:</strong><br/>The shape of X is (60000, 784)<br/>The shape of y is (60000,)</span></pre><h1 id="34ab" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">主成分分析</h1><p id="7177" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们现在将在数据集上使用 PCA 来降低维数。我们将选择保留方差 0.98(通过反复试验选择的值)，并将其用于我们的数据集。</p><pre class="kk kl km kn gt nj nk nl nm aw nn bi"><span id="2cb9" class="no lx it nk b gy np nq l nr ns"><strong class="nk iu">from sklearn.decomposition import PCA</strong><br/># Make an instance of the Model</span><span id="d77b" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">variance = 0.98</strong> #The higher the explained variance the more accurate the model will remain, but more dimensions will be present</span><span id="3ddd" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">pca = PCA(variance)<br/></strong><br/><strong class="nk iu">pca.fit(Clus_dataSet)</strong> #fit the data according to our PCA instance</span><span id="27fa" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">print("Number of components before PCA  = " + str(X.shape[1]))<br/>print("Number of components after PCA 0.98 = " + str(pca.n_components_))</strong> <br/>#dimension reduced from 784</span><span id="74e3" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">Output:</strong><br/>Number of components before PCA  = 784<br/>Number of components after PCA 0.98 = 420</span></pre><p id="6ff6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在根据 PCA 实例转换数据</p><pre class="kk kl km kn gt nj nk nl nm aw nn bi"><span id="466a" class="no lx it nk b gy np nq l nr ns"><strong class="nk iu">Clus_dataSet = pca.transform(Clus_dataSet)</strong></span><span id="f441" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">print(“Dimension of our data after PCA = “ + str(Clus_dataSet.shape))</strong></span><span id="655f" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">Output:</strong><br/>Dimension of our data after PCA  = (60000, 420)</span></pre><p id="b5e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以对数据进行逆变换，以查看数据因 PCA 而发生的变化。</p><pre class="kk kl km kn gt nj nk nl nm aw nn bi"><span id="4d17" class="no lx it nk b gy np nq l nr ns"><strong class="nk iu">approximation = pca.inverse_transform(Clus_dataSet)</strong></span><span id="ec49" class="no lx it nk b gy nt nq l nr ns">#image reconstruction using the less dimensioned data<br/><strong class="nk iu">plt.figure(figsize=(8,4));</strong></span><span id="02df" class="no lx it nk b gy nt nq l nr ns">n = 500 #index value, change to view different data</span><span id="20a9" class="no lx it nk b gy nt nq l nr ns"># Original Image<br/><strong class="nk iu">plt.subplot(1, 2, 1);<br/>plt.imshow(X[n].reshape(X_train.shape[1], X_train.shape[2]),<br/> cmap = plt.cm.gray,);<br/>plt.xlabel(str(X.shape[1])+’ components’, fontsize = 14)<br/>plt.title(‘Original Image’, fontsize = 20);</strong></span><span id="1356" class="no lx it nk b gy nt nq l nr ns"># 196 principal components<br/><strong class="nk iu">plt.subplot(1, 2, 2);<br/>plt.imshow(approximation[n].reshape(X_train.shape[1], X_train.shape[2]),<br/> cmap = plt.cm.gray,);<br/>plt.xlabel(str(Clus_dataSet.shape[1]) +’ components’, fontsize = 14)<br/>plt.title(str(variance * 100) + ‘% of Variance Retained’, fontsize = 20);</strong></span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/32a10947d9c0b4ca411eb606527cd105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*CPby1PT9kCFLkkwjCkyrPw.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">左边的数据是原始数据(784 个分量)，右边的数据是从 PCA 逆变换而来的(420 个分量)。两者都有 28 x 28 的尺寸。</p></figure><h1 id="9db2" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">k 均值聚类</h1><p id="5dba" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">构建 k 均值模型时，我们需要以下参数的值。</p><p id="ab00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> init </strong>:质心的初始化方法。值将为:“k-means++”。k-means++以一种智能的方式为 k-mean 聚类选择初始聚类中心，以加速收敛。</p><p id="cec4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> n_clusters </strong>:要形成的簇的数量以及要生成的质心的数量。值将是:10(根据索引，我们有 10 个类，可能不是最好的，但对我们的上下文足够好)</p><p id="708d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> n_init </strong>:使用不同质心种子运行 k-means 算法的次数。就惯性而言，最终结果将是 n_init 次连续运行的最佳输出。基于我们的惯性结果，值将是:35(可能不是最好的，但对我们的环境来说足够好了)</p><p id="109f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了自己的模型，并使它适合我们的数据集。首先尝试使用提到的参数来检查本文的结果。您可以稍后更改它们，并看到不同的结果。</p><pre class="kk kl km kn gt nj nk nl nm aw nn bi"><span id="bf71" class="no lx it nk b gy np nq l nr ns"><strong class="nk iu">k_means = KMeans(init = “k-means++”, n_clusters = 10, n_init = 35)<br/>k_means.fit(Clus_dataSet)</strong></span></pre><p id="959b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，该模型已经适合我们的图像数据集，我们已经执行了主成分分析。数据现在被分成 10 个簇。现在，我们必须检查哪种类型的图像出现在每个集群中，并查看是否有任何模式。请记住，分布是相似的，但是在执行此操作时，集群的数量(标签)可能会有所不同。</p><h1 id="cad8" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">可视化和分析</h1><p id="222a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">拟合之后，我们使用下面的代码来可视化我们的集群。</p><pre class="kk kl km kn gt nj nk nl nm aw nn bi"><span id="aa62" class="no lx it nk b gy np nq l nr ns"><strong class="nk iu">G = len(np.unique(k_means_labels)) </strong>#Number of labels</span><span id="b906" class="no lx it nk b gy nt nq l nr ns">#2D matrix  for an array of indexes of the given label<br/><strong class="nk iu">cluster_index= [[] for i in range(G)]<br/>for i, label in enumerate(k_means_labels,0):<br/>    for n in range(G):<br/>        if label == n:<br/>            cluster_index[n].append(i)<br/>        else:<br/>            continue</strong></span><span id="8efc" class="no lx it nk b gy nt nq l nr ns">#Visualisation for clusters = clust</span><span id="8fd6" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">plt.figure(figsize=(20,20));<br/>clust = 3 #</strong>enter label number to visualise<strong class="nk iu"><br/>num = 100 </strong>#num of data to visualize from the cluster<strong class="nk iu"><br/>for i in range(1,num): <br/>    plt.subplot(10, 10, i); </strong>#(Number of rows, Number of column per row, item number)<strong class="nk iu"><br/>    plt.imshow(X[cluster_index[clust][i+500]].reshape(X_train.shape[1], X_train.shape[2]), cmap = plt.cm.binary);<br/>    <br/>plt.show()</strong></span></pre><p id="3e81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你在下面看到的，这个特殊的集群(在我的例子中是 3 个)在集群踝靴方面做得很好。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nw"><img src="../Images/02ffe663603e859ff180c52d201c3320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NGXCrG_zt_cWUbBe2QS4Gg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">第三组似乎有短靴(略短)、运动鞋和一些凉鞋</p></figure><p id="8941" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的条形图可视化代码可以在最后的链接中找到。这使用来自数据集的<em class="lv"> y_train </em>的类别标签，并检查给定聚类中某个类别的数量。该图将是相同的(如果您使用相同的参数),而当您执行它时，集群的数量(标签)可能会有所不同。<em class="lv">例如，在我的示例中，标记为 0 的聚类似乎是凉鞋聚类，而在你的示例中，该聚类可能标记为 4。</em></p><p id="5949" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">注:簇名为条形图上方</em><strong class="lb iu"/><em class="lv">。</em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nx"><img src="../Images/b1f3ccdaac4e72fdafcf5d5371d358f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HaIGwi5H9Qlyn4tOFkaGnA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">簇 0~ 3 的条形图</p></figure><p id="f309" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">集群 0 </strong>好像以凉鞋居多。</p><p id="c75b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">集群 1 </strong>看似随机但大多只有上身衣服。<em class="lv"> (T 恤、套头衫、连衣裙、外套和衬衫)</em></p><p id="cb3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">集群 2 </strong>也有上身衣服但是种类较少。<em class="lv">(套头衫、衬衫和外套)</em></p><p id="b601" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">集群 3 似乎大多是短靴，很少有运动鞋和凉鞋。基本上都是鞋子。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ny"><img src="../Images/293394a0fef54da0f120ce2088ea6d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TlJbr7t6ii6OE5EymHelgQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">簇 4 ~ 7 的条形图。集群 6 和 7 似乎都有包。</p></figure><p id="2031" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，我们可以观察到星团 4 ~7。在群集 6 和 7(在我的情况下)中，似乎有大多数的包。但是在使用上述集群的可视化(如下所示)时，我们看到了一个合理的模式。一个集群的提手被提起(以及其他)，另一个没有。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nz"><img src="../Images/d410687d5204a795f822d038de9bac6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8myhP794EK4WVt1cqEULgw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">第六组似乎有提手的袋子。</p></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oa"><img src="../Images/2ffef899f479845729dcbbe315fff219.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GSpkG9taEYjJZ4noDQUhgQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">第 7 组似乎有把手朝下的包。</p></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ob"><img src="../Images/f90c47773782a1f90ebe5156aa0189ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*katGtNTrzkiUaVr71W9vJQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">看起来有鞋子的分类 8 和分类 9 的条形图。</p></figure><p id="7ef0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面显示的群集 8 和 9 似乎是鞋。聚类 8 有一些凉鞋，大部分是运动鞋，聚类 9 似乎有一些凉鞋，大部分是短靴。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oc"><img src="../Images/0919e2a2ac2f65fdb778753f569595ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XNDNW3RdV0frCtRmltmj5Q.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">第八组好像有运动鞋和凉鞋。两个都是鞋。</p></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi od"><img src="../Images/bd09170c16d60ab313ec41bd1a62acec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9j4xVxlR59bldB8-mgpLxw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">集群 9 似乎主要是踝靴和几双凉鞋。两个都是鞋。</p></figure><h1 id="ab33" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">集群的 3D 可视化</h1><p id="d8a1" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们将使用<a class="ae ki" href="https://plotly.com/" rel="noopener ugc nofollow" target="_blank"> plotly 在 3D 中可视化星团。</a> Plotly 是 python 的高级可视化库。使用以下代码获得聚类数据的 3D 散点图。我们将仅使用数据集中 420 个要素中的 3 个要素。这种可视化有助于了解集群的形成情况，以及单个集群扩散到其他集群的程度。</p><pre class="kk kl km kn gt nj nk nl nm aw nn bi"><span id="8534" class="no lx it nk b gy np nq l nr ns">#install these if you haven’t<br/><strong class="nk iu">!pip install chart_studio <br/>!pip install plotly</strong></span><span id="3d64" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">import plotly as py<br/>import plotly.graph_objs as go<br/>import plotly.express as px</strong></span><span id="c49e" class="no lx it nk b gy nt nq l nr ns">#3D Plotly Visualization of Clusters using go</span><span id="8cd4" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">layout = go.Layout(<br/>    title='&lt;b&gt;Cluster Visualisation&lt;/b&gt;',<br/>    yaxis=dict(<br/>        title='&lt;i&gt;Y&lt;/i&gt;'<br/>    ),<br/>    xaxis=dict(<br/>        title='&lt;i&gt;X&lt;/i&gt;'<br/>    )<br/>)</strong></span><span id="be90" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">colors = ['red','green' ,'blue','purple','magenta','yellow','cyan','maroon','teal','black']<br/>trace = [ go.Scatter3d() for _ in range(11)]<br/>for i in range(0,10):<br/>    my_members = (k_means_labels == i)<br/>    index = [h for h, g in enumerate(my_members) if g]<br/>    trace[i] = go.Scatter3d(<br/>            x=Clus_dataSet[my_members, 0],</strong># 0 is a component among the 420 components. Feel free to change it<strong class="nk iu"><br/>            y=Clus_dataSet[my_members, 1],</strong># 1 is a component among the 420 components. Feel free to change it<strong class="nk iu"><br/>            z=Clus_dataSet[my_members, 2],</strong># 2 is a component among the 420 components. Feel free to change it<strong class="nk iu"><br/>            mode='markers',<br/>            marker = dict(size = 2,color = colors[i]),<br/>            hovertext=index,<br/>            name='Cluster'+str(i),<br/>   <br/>            )</strong></span><span id="1277" class="no lx it nk b gy nt nq l nr ns"><strong class="nk iu">fig = go.Figure(data=[trace[0],trace[1],trace[2],trace[3],trace[4],trace[5],trace[6],trace[7],trace[8],trace[9]], layout=layout)<br/>    <br/>py.offline.iplot(fig)</strong></span></pre><p id="8573" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">输出:</em> </strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oe"><img src="../Images/92a4fc4247e32c443c77a727dd6b01ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AnZKU8pp9AnLBKGhzfDJhw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">使用 PCA 对 k 均值聚类的 f-MNIST 数据集进行 3D plotly 散点图可视化。每种颜色代表一个不同的集群。</p></figure><p id="421e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于数据集(具有 PCA)实际上具有 420 个维度，因此该可视化仅在散点图中显示了这些特征中的 3 个。要绘制的组件可以在代码中更改，以了解不同组件的情况。</p><p id="f0fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">(可进行进一步的主成分分析，将所有 784 个分量减少到 3 个分量，以在 3D 图中完全表示，但这样做会丢失大量信息)</em></p><h1 id="3b4d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">结论</strong></h1><p id="85a5" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">聚类似乎将相似的项目分组在一起。一个集群或者包含上身衣服<em class="lv"> (T 恤/上衣、套衫、连衣裙、外套、衬衫)</em>或者鞋子<em class="lv">(凉鞋/运动鞋/短靴)</em>或者包。然而，该聚类在裤子上表现不佳，并且似乎将其与连衣裙归为一组。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi of"><img src="../Images/8a5a5c9cb9cd9398545e55935e3c61a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*OL-ca_Ln74IF7zeloeNMfQ.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">一群穿着裤子和裙子的人</p></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi og"><img src="../Images/59417f82a37382b25f6aedc9d734005d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OOkz3E0TCOCJqu4ApNTqlw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">前述集群中的服装和裤子的数据可视化</p></figure><p id="31f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但它很好地区分了这两种包。这种模式可以由我们来解释。一组是提手凸起的包，另一组是提手未凸起的包。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/cc483cd456f1571f7377db4e8b4cd061.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*Pp7By9MTvaY7wzEovgbW4A.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">两个集群都有包。但是该算法能够区分手柄被提起和未被提起</p></figure><p id="4f2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，在下面显示的两个集群中，两个集群都以短靴为主。但是短靴看起来大小不一，小一点的看起来和同一组的运动鞋很相似。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7d58132db7bc43426cb4ddc40b49d68a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*TIJvZZDvDil_UM54CvEpdg.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">两个集群都有最大的踝靴。左边的比右边的短</p></figure><p id="b03a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">观察上面的结果，我们可以看到 k -means 算法基于一个合理的模式来区分它的聚类。因此，我们可以得出一个粗略的结论，K-means 聚类方法在进行主成分分析后可以得到一个不错的分类结果。</p><p id="55ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ki" href="https://github.com/sunman91/Data-Science-stuff/blob/master/k-means-and-pca-on-f-mnist-dataset.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="lv">点击此处链接到</em> </strong> </a></p><p id="5f49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">参考文献</em></p><p id="898c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] A. Ng，斯坦福大学的机器学习。(未注明)。从 https://www.coursera.org/learn/machine-learning<a class="ae ki" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="2814" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] <a class="ae ki" href="https://medium.com/@joel_34096?source=post_page-----a648f28bdc47----------------------" rel="noopener"> S Joel Franklin </a>，<a class="ae ki" href="https://medium.com/@joel_34096/k-means-clustering-for-image-classification-a648f28bdc47" rel="noopener"> K-Means 聚类用于图像分类</a> (2020)，<a class="ae ki" href="https://medium.com" rel="noopener">中</a></p><p id="fc6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] <a class="ae ki" href="https://github.com/mGalarnyk" rel="noopener ugc nofollow" target="_blank">迈克尔·加拉内克</a>，<a class="ae ki" href="https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/PCA/PCA_Image_Reconstruction_and_such.ipynb" rel="noopener ugc nofollow" target="_blank"> PCA + Logistic 回归(MNIST)</a>(2018)<a class="ae ki" href="https://github.com" rel="noopener ugc nofollow" target="_blank">github</a></p></div></div>    
</body>
</html>