<html>
<head>
<title>Gaussian Mixture Models:
implemented from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高斯混合模型:从零开始实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gaussian-mixture-models-implemented-from-scratch-1857e40ea566?source=collection_archive---------12-----------------------#2020-08-31">https://towardsdatascience.com/gaussian-mixture-models-implemented-from-scratch-1857e40ea566?source=collection_archive---------12-----------------------#2020-08-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/5dda92ee06930f65fa36f291eada6751.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yRmKvmk35WF6KYjxr9I7vg.png"/></div></div></figure><p id="b3b5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">从机器学习和人工智能领域的兴起，概率论是一个强大的工具，它允许我们处理许多应用中的不确定性，从分类到预测任务。今天，我想和你们讨论更多关于概率和高斯分布在聚类问题中的应用，以及 GMM 模型的实现。所以让我们开始吧</p><h2 id="ec34" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated">什么是 GMM？</h2><p id="c9b7" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">GMM(或高斯混合模型)是一种算法，它使用对数据集密度的估计来将数据集分成初步定义数量的聚类。为了更好地理解，我将同时解释这个理论，并展示实现它的代码。</p><p id="462b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于这个实现，我将使用<a class="ae lx" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank"> EM(期望最大化)</a>算法。</p><h2 id="8576" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated">理论和代码是最好的结合。</h2><p id="8e10" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">首先让我们导入所有需要的库:</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="06b7" class="kz la it md b gy mh mi l mj mk">import numpy as np<br/>import pandas as pd</span></pre><p id="da08" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我强烈建议在自己实现模型时遵循 sci-kit learn library 的标准。这就是为什么我们将 GMM 作为一个类来实现。让我们也来看看 __init_function。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="7379" class="kz la it md b gy mh mi l mj mk">class GMM:<br/>    def __init__(self, n_components, max_iter = 100, comp_names=None):<br/>        self.n_componets = n_components<br/>        self.max_iter = max_iter<br/>        if comp_names == None:<br/>            self.comp_names = [f"comp{index}" for index in range(self.n_componets)]<br/>        else:<br/>            self.comp_names = comp_names<br/>        # pi list contains the fraction of the dataset for every cluster<br/>        self.pi = [1/self.n_componets for comp in range(self.n_componets)]</span></pre><p id="ad41" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">简而言之，<strong class="kd iu"> n_components </strong>是我们想要拆分数据的集群数量。<strong class="kd iu"> max_iter </strong>表示算法进行的迭代次数，comp_names 是具有 n_components 个元素的字符串列表，这些元素被解释为聚类的名称。</p><h2 id="d960" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated">拟合函数。</h2><p id="b99b" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">因此，在我们使用 EM 算法之前，我们必须分割我们的数据集。之后，我们必须启动 2 个列表。一个包含每个子集的平均向量(向量的每个元素都是列的平均值)的列表。第二个列表包含每个子集的协方差矩阵。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="2be9" class="kz la it md b gy mh mi l mj mk">def fit(self, X):<br/>        # Spliting the data in n_componets sub-sets<br/>        new_X = np.array_split(X, self.n_componets)<br/>        # Initial computation of the mean-vector and covarience matrix<br/>        self.mean_vector = [np.mean(x, axis=0) for x in new_X]<br/>        self.covariance_matrixes = [np.cov(x.T) for x in new_X]<br/>        # Deleting the new_X matrix because we will not need it anymore<br/>        del new_X</span></pre><p id="1c93" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我们可以讨论 EM 算法了。</p><h2 id="05ac" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated">EM 算法。</h2><p id="739f" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">顾名思义，EM 算法分为两步——E 和 m。</p><h2 id="2ec1" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated">电子步骤:</h2><p id="bc62" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">在估计步骤中，我们计算 r 矩阵。它是使用下面的公式计算的。</p><figure class="ly lz ma mb gt ju gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/dbc07b65ca5ef27b49c6615189319efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/0*wRAioLy_mwyVfYt8"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">计算 r(责任)矩阵的公式(来源——mathematis for Machine Learning Book)</p></figure><p id="c9bf" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="mq"> r 矩阵</em>也被称为<strong class="kd iu">‘职责’</strong>，可以用以下方式解释。行是来自数据集的样本，而列代表每个聚类，该矩阵的元素解释如下:rnk 是样本 n 成为聚类 k 的一部分的概率。当算法收敛时，我们将使用该矩阵来预测点聚类。</p><p id="946d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">同样，我们计算 N 个列表，其中每个元素基本上是 r 矩阵中对应列的和。下面的代码就是这样做的。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="bd10" class="kz la it md b gy mh mi l mj mk">for iteration in range(self.max_iter):<br/>    ''' ----------------   E - STEP   ------------------ '''<br/>    # Initiating the r matrix, evrey row contains the probabilities<br/>    # for every cluster for this row<br/>    self.r = np.zeros((len(X), self.n_componets))<br/>    # Calculating the r matrix<br/>    for n in range(len(X)):<br/>        for k in range(self.n_componets):<br/>            self.r[n][k] = self.pi[k] * self.multivariate_normal(X[n], self.mean_vector[k], self.covariance_matrixes[k])<br/>            self.r[n][k] /= sum([self.pi[j]*self.multivariate_normal(X[n], self.mean_vector[j], self.covariance_matrixes[j]) for j in range(self.n_componets)])<br/>    # Calculating the N<br/>    N = np.sum(self.r, axis=0)</span></pre><p id="4d85" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">指出多元正态只是应用于向量的正态分布公式，它用于计算向量在正态分布中的概率。</p><figure class="ly lz ma mb gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mr"><img src="../Images/bffa7a501c77d00b6d291ddfc68781da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*i7m3ZCRzA2OnhdH8"/></div></div></figure><p id="22a8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面的代码实现了它，采用行向量、均值向量和协方差矩阵。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="e429" class="kz la it md b gy mh mi l mj mk">def multivariate_normal(self, X, mean_vector, covariance_matrix):<br/>    return (2*np.pi)**(-len(X)/2)*np.linalg.det(covariance_matrix)**(-1/2)*np.exp(-np.dot(np.dot((X-mean_vector).T, np.linalg.inv(covariance_matrix)), (X-mean_vector))/2)</span></pre><p id="3060" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">看起来有点乱，但是你可以在那里找到完整的代码。</p><h2 id="411d" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated">m 步:</h2><p id="d051" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">在最大化步骤中，我们将逐步设置均值向量和协方差矩阵的值，用它们来描述聚类。为此，我们将使用以下公式。</p><figure class="ly lz ma mb gt ju gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/99736ff12d61f86e9310aba486272d9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*L9uVNWuLFQVjqtIu"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">M-step 的公式(来源-mathematis for Machine Learning Book)</p></figure><p id="1703" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在代码中，我希望:</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="a276" class="kz la it md b gy mh mi l mj mk">''' ---------------   M - STEP   --------------- '''<br/># Initializing the mean vector as a zero vector<br/>self.mean_vector = np.zeros((self.n_componets, len(X[0])))<br/># Updating the mean vector<br/>for k in range(self.n_componets):<br/>    for n in range(len(X)):<br/>        self.mean_vector[k] += self.r[n][k] * X[n]<br/>        self.mean_vector = [1/N[k]*self.mean_vector[k] for k in range(self.n_componets)]<br/># Initiating the list of the covariance matrixes<br/>self.covariance_matrixes = [np.zeros((len(X[0]), len(X[0]))) for k in range(self.n_componets)]<br/># Updating the covariance matrices<br/>for k in range(self.n_componets):<br/>    self.covariance_matrixes[k] = np.cov(X.T, aweights=(self.r[:, k]), ddof=0)<br/>self.covariance_matrixes = [1/N[k]*self.covariance_matrixes[k] for k in range(self.n_componets)]<br/># Updating the pi list<br/>self.pi = [N[k]/len(X) for k in range(self.n_componets)]</span></pre><p id="86f0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们已经完成了拟合函数。创造性地应用 EM 算法将使 GMM 最终收敛。</p><h2 id="ddc9" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated">预测函数。</h2><p id="17d4" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">预测函数实际上非常简单，我们只需使用多元正态函数，该函数使用每个聚类的最佳均值向量和协方差矩阵，以找出哪个给出最大值。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="ab7e" class="kz la it md b gy mh mi l mj mk">def predict(self, X):<br/>        probas = []<br/>        for n in range(len(X)):<br/>            probas.append([self.multivariate_normal(X[n], self.mean_vector[k], self.covariance_matrixes[k])<br/>                           for k in range(self.n_componets)])<br/>        cluster = []<br/>        for proba in probas:<br/>            cluster.append(self.comp_names[proba.index(max(proba))])<br/>        return cluster</span></pre><h2 id="7ca2" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">结果。</strong></h2><p id="5b73" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">为了测试该模型，我选择将其与 sci-kit 库中实现的 GMM 进行比较。我使用 sci-kit 学习数据集生成函数生成了 2 个数据集——使用不同的设置生成 _ blobs。这就是结果。</p><figure class="ly lz ma mb gt ju gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/0d83aff6a7d4937c19d0aa6685a89f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*96Hdf0KZcbRiEp9BNRNm6g.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">我们的模型与 sci-kit learn 模型的比较。</p></figure><p id="dd1a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们的模型和 sci-kit 模型的聚类几乎相同。很好的结果。完整的代码你可以在<a class="ae lx" href="https://github.com/ScienceKot/mysklearn/tree/master/Gaussian%20Mixture%20Models" rel="noopener ugc nofollow" target="_blank">那里找到</a>。</p><figure class="ly lz ma mb gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/20f73cb96e2fa41d8d3ff82eaee5bd83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/1*UEuA3iIXXDXC-QuXwD_MhA.gif"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">来源——tenor.com</p></figure><p id="da32" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这篇文章是西格蒙德和❤一起写的。</p><p id="c80f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有用的链接:</p><ul class=""><li id="b347" class="mv mw it kd b ke kf ki kj km mx kq my ku mz ky na nb nc nd bi translated"><a class="ae lx" href="https://github.com/ScienceKot/mysklearn/tree/master/Gaussian%20Mixture%20Models?fbclid=IwAR0oexiLBSatnB9ukeo_gLL6dR_Dd2T_uO7K7VHmCkavcRfrXKbyhyBIJbc" rel="noopener ugc nofollow" target="_blank">https://github . com/science kot/mysklearn/tree/master/Gaussian % 20 mixture % 20 models</a></li><li id="0ce7" class="mv mw it kd b ke ne ki nf km ng kq nh ku ni ky na nb nc nd bi translated"><a class="ae lx" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Expectation % E2 % 80% 93 最大化 _ 算法</a></li><li id="767f" class="mv mw it kd b ke ne ki nf km ng kq nh ku ni ky na nb nc nd bi translated">用于机器学习的数学由 Cheng Soon Ong、Marc Peter Deisenroth 和 A. Aldo Faisal 编写</li></ul></div></div>    
</body>
</html>