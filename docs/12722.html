<html>
<head>
<title>GPT-3: Just Another Language Model But Bigger</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPT-3:只是另一个语言模型，但是更大</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gpt-3-just-another-language-model-but-bigger-1add6e9277fa?source=collection_archive---------35-----------------------#2020-09-01">https://towardsdatascience.com/gpt-3-just-another-language-model-but-bigger-1add6e9277fa?source=collection_archive---------35-----------------------#2020-09-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4cac" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解 GPT-3 研究论文</h2></div><p id="80da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GPT-3 在很短的时间内接管了 NLP 世界。证明了增加参数个数会提高模型精度的理论。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="lj lk l"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">Sam Altman 的推文，公共领域</p></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lq"><img src="../Images/31407f492f8834d2a8c1ac8769725678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1W6tu3wlCO9FjUK9TeGfyQ.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">由作者在<a class="ae lp" href="https://www.befunky.com/create/designer/" rel="noopener ugc nofollow" target="_blank"> befunky </a>上创建</p></figure><h1 id="e1e7" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">什么是语言模型？</h1><p id="e127" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated"><a class="ae lp" href="https://en.wikipedia.org/wiki/Language_model" rel="noopener ugc nofollow" target="_blank">语言模型</a>试图预测给定 m 个单词的下一个单词。它将概率分配给下一个单词，使句子更有可能。</p><h1 id="d5c7" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">什么是 GPT-3？</h1><p id="7b83" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">它是 GPT-2 语言模型的继承者，具有最多的待训练参数。OpenAI 创建了 GPT-3 模型，它证明了语言模型的大小与准确度成正比，因此，我们越增加模型的大小，准确度越高。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mu"><img src="../Images/6eb035feb22f222e4a450b88bc6b1844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fheIOvGVhg5nweJ1g6iOYA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="0446" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们从图表中看到的，语言模型数量的增加成比例地提高了准确性。因此，可以通过向模型添加更多的参数来获得更高的精度。</p><p id="b685" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>是目前最大的语言模型，拥有 1750 亿个参数，比拥有 170 亿个参数的图灵-NLG 模型大 10 倍。</p><p id="1c34" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个博客中，我们将浏览<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>的研究论文，并推断为什么它只是另一种语言模型，为什么它不能被称为可以模仿任何水平的人类的模型。这是目前最好的语言模型，但不是能够理解和学习人类语言的模型。</p><h1 id="7c6f" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">测试模型的设置:</h1><ul class=""><li id="1d8b" class="mv mw it kk b kl mp ko mq kr mx kv my kz mz ld na nb nc nd bi translated"><strong class="kk iu">少镜头(FS) </strong>指的是很少的例子与任务描述一起给出给模型。因此，不是为特定任务提供完整的数据集，而是给模型提供描述以及一些 k 示例，然后向模型提出问题。k 示例在 10 到 100 的范围内，并且不允许模型基于该示例进行任何权重改变。优点是该模型不需要大的数据集来学习特定的任务，但是精度仍然不能与最新的任务特定模型相比。</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/c06291cbb1353a7655b2e23dc09d1213.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*qKHi9B6a1xSxl6CpPYwfQg.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">少数镜头设置示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><ul class=""><li id="b930" class="mv mw it kk b kl km ko kp kr nf kv ng kz nh ld na nb nc nd bi translated"><strong class="kk iu">单镜头(OS) </strong>类似于 k 等于 1 的少镜头。所以模型只需要理解基于一个例子的任务。这种学习方式类似于人类的学习方式。这一次也不允许模特改变体重。</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/7ddd5fe1210eab6f4b4a44de59e66f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*s7bFtZhEdw4VQxTPjm6S-A.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">一次性设置示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><ul class=""><li id="49c4" class="mv mw it kk b kl km ko kp kr nf kv ng kz nh ld na nb nc nd bi translated"><strong class="kk iu">零拍(ZS)</strong>k 值为零。没有向模型提供示例。模型需要从任务描述中理解模型需要做什么。</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/f6406b35195d6d13ff329be470bdd4e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*NHZFNs97EVTzWoWoTIESbA.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">零射击设置示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="009d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型的准确性与最新的任务特定模型(也称为微调模型)进行了比较。这种模型的缺点是他们需要庞大的特定任务数据集来进行训练。这种类型的模型是特定于任务的，在其他任务上表现不佳。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/ae4bc6131197051380aaa94419b3072a.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*UFeJx3L1DPU0Pm-TOwcRYw.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">微调模型|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><h1 id="eca6" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">训练数据集</h1><p id="a9cd" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">GPT 3 号使用的数据集非常庞大，几乎包含了互联网上的所有内容。对训练数据集进行了模糊搜索，以删除与测试和验证数据集相关的任何内容，这将有助于提供更准确的结果。他们通过合并各种数据集和爬行创建了这个数据集。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nl"><img src="../Images/c341ecd13fcc17c51c1c6262f8fab873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mAfVy6JsZBWw2IGpcpf6Lg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GPT-3 使用的数据集|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><h1 id="a0f9" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">培训过程</h1><p id="89dc" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">他们使用梯度噪声标度来找出要使用的正确批量。架构与 GPT-2 相似，但稍有改动。</p><h1 id="023f" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">结果</h1><p id="1ac5" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">在各种自然语言任务和各种数据集上评估了 GPT-3。</p><h2 id="ec54" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">语言建模、完形填空和完成任务</h2><h2 id="79f1" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1606.06031.pdf" rel="noopener ugc nofollow" target="_blank">λ</a>数据集</h2><pre class="le lf lg lh gt ny nz oa ob aw oc bi"><span id="93c3" class="nm ly it nz b gy od oe l of og">Context: “Yes, I thought I was going to lose the baby.” “I was scared too,” he stated, sincerity flooding his eyes. “You were ?” “Yes, of course. Why do you even ask?” “This baby wasn’t exactly planned for.” </span><span id="9bb2" class="nm ly it nz b gy oh oe l of og">Target sentence: “Do you honestly think that I would want you to have a _______ ?” </span><span id="e0d9" class="nm ly it nz b gy oh oe l of og">Target word: miscarriage</span></pre><p id="7124" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个<a class="ae lp" href="https://arxiv.org/pdf/1606.06031.pdf" rel="noopener ugc nofollow" target="_blank">数据集</a>中，上下文被给出，然后句子被给出给模型，空白需要由模型完成。单词的选择应该基于给定的上下文，而不是过去的知识模型。这个数据集证明了模型能够理解上下文，并在此基础上提供答案。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oi"><img src="../Images/57c39b550fa37728a293ab2a674a46a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBDOUWf5YPwCKis-D5JSrQ.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GPT-3 在兰巴达数据集上的准确性|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="778a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，在这个数据集上，GPT-3 的所有设置都超过了 SOTA 的精度。GPT-3 的单发精度低于零发精度。我们可以暗示，GPT-3 用任何一个例子都比用一个例子预测得更准确。<em class="oj">GPT-3 可能通过训练集获得了数据集的知识，并且在少击方法中，模型可以更准确地知道使用哪个权重。因为当你在谷歌上搜索</em> <code class="fe ok ol om nz b"><em class="oj">Yes, of course. Why do you even ask?"This baby wasn't planned for.”</em></code> <em class="oj">的时候，你可以找到各种链接来回答这个问题。</em></p><h2 id="3e56" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1905.07830.pdf" rel="noopener ugc nofollow" target="_blank"> HellaSwag </a>数据集</h2><pre class="le lf lg lh gt ny nz oa ob aw oc bi"><span id="f8cf" class="nm ly it nz b gy od oe l of og">A woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath. She …  </span><span id="152b" class="nm ly it nz b gy oh oe l of og">A. rinses the bucket off with soap and blow dry the dog’s head. <br/>B. uses a hose to keep it from getting soapy. <br/>C. gets the dog wet, then it runs away again. <br/>D. gets into a bath tub with the dog.</span></pre><p id="ccbc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些是选择题，其中模型需要选择最合适的选项。该数据集用于测试模型的常识。GPT-3 FS 达到了 79.3%的精度，比 SOTA 低 6%，后者是经过微调的多任务模型。</p><h2 id="b1c0" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated"><a class="ae lp" href="https://www.cs.rochester.edu/~nasrinm/files/Papers/lsdsem17-shared-task.pdf" rel="noopener ugc nofollow" target="_blank">故事完形填空</a>数据集</h2><pre class="le lf lg lh gt ny nz oa ob aw oc bi"><span id="a5bc" class="nm ly it nz b gy od oe l of og"><strong class="nz iu">Context</strong>: Sammy’s coffee grinder was broken. He needed something to crush up his coffee beans. He put his coffee beans in a plastic bag. He tried crushing them with a hammer. </span><span id="e27c" class="nm ly it nz b gy oh oe l of og"><strong class="nz iu">Right Ending</strong>: It worked for Sammy. </span><span id="7f2f" class="nm ly it nz b gy oh oe l of og"><strong class="nz iu">Wrong Ending</strong>: Sammy was not that much into coffee.</span></pre><p id="8bff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些数据集也用于常识测试。在这种情况下，模型被赋予上下文，并且模型必须从提供的两个选项中预测正确的结局。GPT-3 FS 已达到 87.7%的精度，远低于 SOTA，但在 GPT-3 ZS 显示了 10%的巨大改进。</p><h2 id="61c1" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">闭卷问答</h2><p id="25c7" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">开卷问答应用于搜索引擎中，搜索引擎可以搜索相关信息并从中提取答案。在<code class="fe ok ol om nz b">closed book</code>中，不允许模型进行搜索，也没有提供任何上下文或摘录来了解模型实际上在询问什么。</p><p id="2a00" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://homes.cs.washington.edu/~eunsol/papers/acl17jcwz.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> TriviaQA </strong> </a></p><p id="ab1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个数据集是基于事实的，也就是说答案是事实，而不是依赖于任何上下文。因此，我们可以说，这个数据集测试了模型的记忆，因为 GPT-3 也是在维基百科文本上训练的。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi on"><img src="../Images/aec0f403959e0eed1b8f17d6a7bfb719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tewW1_UDOg4UiJZwnH2i4A.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">TriviaQA 示例|参考:<a class="ae lp" href="https://homes.cs.washington.edu/~eunsol/papers/acl17jcwz.pdf" rel="noopener ugc nofollow" target="_blank"> TriviaQA 研究论文</a>，公共领域</p></figure><p id="1cd6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">模型被提问，并期望得到基于事实的答案。摘录仅供我们参考，不提供给模型。也允许多个答案，例如，如果答案是姓名比尔盖茨，答案可以是比尔，盖茨，比尔盖茨，微软创始人比尔盖茨。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oo"><img src="../Images/493be050af1639b470618e14f5509442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1U9lnhE4niKiN1FHbqDjVA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">TriviaQA 准确性|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="d037" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，GPT-3 已经超过了微调开放图书 SOTA。因此，我们可以说，GPT-3 是更有效的信息检索引擎的事实。</p><p id="9ada" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1607.06275.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">WebQuestions</strong></a><strong class="kk iu">(WebQA)</strong></p><p id="c043" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个数据集类似于 TriviaQA，但是这个数据集是由真实世界中的真实用户提出的真实世界的问题组成的。因此，它在真实世界场景中测试模型。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi op"><img src="../Images/6de2d8f34d07c960198e005741704cf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dpcqqC9kN8p7ZmD-gtliQg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">WebQA 实例|参考:<a class="ae lp" href="https://arxiv.org/pdf/1607.06275.pdf" rel="noopener ugc nofollow" target="_blank"> WebQA 研究论文</a>，公共领域</p></figure><p id="39ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GPT-3 FS 与微调过的 SOTA 型号具有相似的精度。</p><p id="536e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/1f7b46b5378d757553d3e92ead36bda2e4254244.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">自然题</strong> </a> <strong class="kk iu"> (NQs) </strong></p><p id="001b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该数据集由 Google 提供，用于在现实世界中测试问答模型。所有的问题都是用户在谷歌搜索上搜索出来的。在模型中，试图从维基百科页面中预测可能包含问题答案的段落。模型还预测了这个问题的精确答案。但是短答案必须出现在所选段落中，也称为长答案。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oq"><img src="../Images/a536501e49f15a0c7bf525e231450b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*niA06St0mk4mrpKHsIbgrA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">自然题例|参考:<a class="ae lp" href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/1f7b46b5378d757553d3e92ead36bda2e4254244.pdf" rel="noopener ugc nofollow" target="_blank"> NQs 研究论文</a>，公共领域</p></figure><p id="04fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GPT-3 FS 设置的精度比<a class="ae lp" href="http://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15791880.pdf" rel="noopener ugc nofollow" target="_blank">微调过的</a> SOTA 低 29.9%。这个数据集测试了模型保留维基百科信息的极限。</p><blockquote class="or os ot"><p id="8ffd" class="ki kj oj kk b kl km ju kn ko kp jx kq ou ks kt ku ov kw kx ky ow la lb lc ld im bi translated">通过这种测试，我们可以说，模型越大，可以保留的信息就越多，因此准确度就越高。</p></blockquote></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><h2 id="9cc4" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">翻译</h2><p id="a601" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">GPT-3 在语言翻译方面的准确性令人惊讶。因为训练集的 93%的单词是英语。所以，这个模型会更偏向于英语而不是其他语言。GPT-3 是 GPT-2 的升级版，测试了除英语之外的法语、德语和罗马尼亚语的机器翻译。</p><p id="7b62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器翻译任务的模型效率是用<a class="ae lp" href="https://en.wikipedia.org/wiki/BLEU" rel="noopener ugc nofollow" target="_blank"> BLEU </a>来衡量的，而不是准确性。我们改天会调查它。</p><p id="965b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://www.aclweb.org/anthology/P02-1040.pdf" rel="noopener ugc nofollow" target="_blank"> BLEU </a>在 6 个语言对上计算:</p><ol class=""><li id="7b52" class="mv mw it kk b kl km ko kp kr nf kv ng kz nh ld pe nb nc nd bi translated">英语到法语(英语→法语)</li><li id="e150" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">法语到英语(法语→英语)</li><li id="1960" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">英语到德语(英语→德语)</li><li id="56e7" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">德语到英语(德→恩)</li><li id="22ae" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">英语到罗马尼亚语(En→Ro)</li><li id="175e" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">罗马尼亚语到英语(Ro→En)</li></ol><p id="3910" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练数据集具有大约 92.5%的英语单词、1.8%的法语单词、1.5%的德语单词和 0.16%的罗马尼亚语单词。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pk"><img src="../Images/4c896b7a786a3d5b2cf94e98340058f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2O1Pb37WhqfWXI1d2l19CA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">6 种语言对的 BLEU 评分|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="4005" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，在将任何文本翻译成英语时，GPT-3 模型几乎比 SOTA 模型更好。</p><p id="3d6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">零拍示例:</strong></p><pre class="le lf lg lh gt ny nz oa ob aw oc bi"><span id="3b73" class="nm ly it nz b gy od oe l of og">Q: What is the {language} translation of {sentence} </span><span id="7676" class="nm ly it nz b gy oh oe l of og">A: {translation}.</span></pre><p id="8b6c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，只有描述文本与句子和目标语言一起被提供给模型。</p><p id="c0fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">单镜头和少镜头示例:</strong></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pl"><img src="../Images/542817bbcf2eeb92e9da76d8e33d4ecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ObDT_geBj6tGzZ7m1o8Cmw.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">一次性和少量学习的格式|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="df00" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于少数镜头，在提问之前，给模型提供了大约 64 个例子。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pm"><img src="../Images/2b8b36443f14328307c22b9009fa23aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3IdzaxAvuowBqjB7uvQ6TQ.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">少数镜头 BLEU 评分|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="27f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们可以看到的，随着数据的增加，模型的 BLEU 分数增加。英语-&gt;法语的分数大于英语-&gt;德语，德语大于英语-&gt;罗马尼亚语，并且与数据集中每种语言的单词百分比成比例。</p></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><h2 id="5bf7" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">Winograd 风格的任务</h2><p id="20a0" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">这类任务包括确定句子中使用的代词是什么样的。这个代词对计算机来说是模糊的，但对人类来说却是明确的。部分评估方法用于评估，您可以在此处阅读<a class="ae lp" href="https://arxiv.org/pdf/1811.01778.pdf" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="le lf lg lh gt ny nz oa ob aw oc bi"><span id="6da1" class="nm ly it nz b gy od oe l of og">"<strong class="nz iu">sentence</strong>": "The city councilmen refused the demonstrators a permit because [they] feared violence.", </span><span id="fbb8" class="nm ly it nz b gy oh oe l of og">"<strong class="nz iu">answer1</strong>": "The demonstrators"<br/>"<strong class="nz iu">answer0</strong>": "The city councilmen"</span><span id="65a3" class="nm ly it nz b gy oh oe l of og">"<strong class="nz iu">sentence_switched</strong>": "The demonstrators refused the city councilmen a permit because [they] feared violence."</span></pre><p id="adf5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">句子的答案必须是<code class="fe ok ol om nz b">answer1</code>，但是一旦我们转换句子，答案也应该变成<code class="fe ok ol om nz b">answer0</code></p><p id="cf73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://aaai.org/ocs/index.php/KR/KR12/paper/view/4492" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">威诺格拉</strong> </a> <strong class="kk iu">数据集</strong></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pn"><img src="../Images/8bfb036e6274700cc8e818c01c29a4c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ebbmBaQZl842ltJo6MUhDA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">Winograd 示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="b51f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GPT 3 号在这个数据集上的表现低于 SOTA。在训练数据集中也发现了一些测试集的例子。</p><p id="f263" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1907.10641.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">wino grande</strong></a><strong class="kk iu">数据集</strong></p><p id="8286" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该数据集类似于 winograde 数据集，但更高级、更困难。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi po"><img src="../Images/0f90617ce84ee2328785b06e415974c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PAqaAUCw5aSSdtE1d-QRjA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">Winogrande 数据集|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/9ed6fe7f39dc44b8273689df99f7f316.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*QZVf2GL173huiBZUP0mK7w.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GPT-3 在 Winograde 式任务上的准确性|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><h2 id="dcfb" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">常识推理</h2><p id="9a24" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">该测试旨在测试模型如何将常识应用到现实世界的问题中。例如，如果我们问人类几点了？人类会回应当前时间，如下午 5:30，而不是解释时间的概念。我们用常识来检验这个模型是否能回答这样的问题。以前的所有任务大多依赖于数据和概率，而不是模型实际理解内部概念的程度。在这里，我们测试模型理解人类语言的程度。</p><p id="a8f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1911.11641.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> PhysicalQA </strong> </a> <strong class="kk iu">数据集</strong></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/5b57dbfc4290dadc75d510de12381e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*Lz1xZFhNsGEmz4j6r_uCMA.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">PIQA 范例|参考:<a class="ae lp" href="https://arxiv.org/pdf/1911.11641.pdf" rel="noopener ugc nofollow" target="_blank"> PhysicalQA 研究论文</a>，公共领域</p></figure><p id="ab73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于所有类型的调优，GPT-3 在这个数据集上以很小的优势击败了 SOTA。但是由于可能的数据污染，对这一结果的信心不大。</p><blockquote class="or os ot"><p id="7922" class="ki kj oj kk b kl km ju kn ko kp jx kq ou ks kt ku ov kw kx ky ow la lb lc ld im bi translated">我们的分析将 PIQA 标记为潜在的数据污染问题(尽管隐藏了测试标签)，因此我们保守地用星号标记结果。</p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pr"><img src="../Images/824130a309891424448e5c357117306a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZvwvL5kCnkyZHzve1SoVA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GPT-3 在 PIQA 数据集上的表现|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="e3b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1803.05457.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> AI2 推理挑战数据集(ARC) </strong> </a></p><p id="6b78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该数据集由 3 至 9 年级的多项选择题组成。数据集分为几个部分:1 .简易版 2。挑战版。挑战集由基于检索的算法错误回答的问题组成。</p><p id="5764" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GPT-3 在这个数据集上的性能远远低于由 UnifiedQA 实现的 SOTA。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ps"><img src="../Images/bb6e50d83222ea68e141b3d1eb627885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fWIl1tq971Klkph949CGHQ.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">ARC 挑战集示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="d65e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1809.02789.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">OpenBookQA</strong></a><strong class="kk iu">数据集</strong></p><p id="ffdc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个数据集类似于开卷测试，问题的答案是事实和常识的结合。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/bc8dda153abffb882555e897a238112a.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*S2tPDMG7MmYYHAHWRyTAwQ.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">OpenBookQA 实例|参考:<a class="ae lp" href="https://arxiv.org/pdf/1809.02789.pdf" rel="noopener ugc nofollow" target="_blank"> OpenBookQA 研究论文</a>，公共领域</p></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pu"><img src="../Images/8d8923edd44b9d8bd4ebdf748f484478.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0sPUXMvbMcuQVNxDkMpPKA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GPT-3 常识推理任务中的准确性|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="c645" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于 GPT-3 是在互联网上出现的所有文本上训练的，所以回答 GPT-3 是否应用常识来回答这个问题是一个好问题，而不仅仅是检索训练数据集中某处可用的信息。</p></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><h2 id="4031" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">阅读理解</h2><p id="080c" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">在这里，我们测试模型理解文章和根据文章中提供的信息回答问题的能力。</p><p id="b0ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://stanfordnlp.github.io/coqa/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> CoQA 数据集</strong> </a></p><p id="a1df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个数据集中，提供了段落，所提的问题基于人类之间的对话。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pv"><img src="../Images/16c647e529162fb56af0f1f1be897506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eAB4WYIq2-yEvwbSbtryOQ.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">少量调整的 CoQA 示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="6eba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://quac.ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> QuAC 数据集</strong> </a></p><p id="b8c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个数据集非常有趣，因为学生并没有看到文章，而是问了一些问题，这些问题由老师来回答。学生问基本问题，教师回答一些额外的上下文，学生再问另一个问题。如果老师不能回答太多的问题，或者如果学生没有问任何相关的问题，互动就结束了。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pw"><img src="../Images/b8d7098194b8502801d75760c060dce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d5iDXNRqzpc_3EXFxjIxWg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">QuAC 数据集示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="8f57" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://allennlp.org/drop" rel="noopener ugc nofollow" target="_blank">删除数据集</a></p><p id="6120" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此数据集中，模型必须解析问题中的引用，可能是对多个输入位置的引用，并对它们执行离散操作，如加法、计数或排序。</p><pre class="le lf lg lh gt ny nz oa ob aw oc bi"><span id="bbdb" class="nm ly it nz b gy od oe l of og"><strong class="nz iu">Passage</strong>: Although the movement initially gathered some 60,000    adherents, the subsequent establishment of the Bulgarian Exarchate reduced their number by some 75%.</span><span id="44e0" class="nm ly it nz b gy oh oe l of og"><strong class="nz iu">Question</strong>: How many adherents were left after the establishment of the Bulgarian Exarchate?</span><span id="b191" class="nm ly it nz b gy oh oe l of og"><strong class="nz iu">Correct Answer</strong>: 15,000</span></pre><p id="1620" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们在这里看到的，模型需要理解文章和问题，以便正确地回答它，它还应该对它执行一些操作，而不仅仅是检索信息。</p><p id="947e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">小队 2.0 数据集</strong> </a></p><p id="40bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除了在文章中有答案的问题，在文章中没有答案的问题也被提出。对于文章中没有答案的问题，该模型不会给出答案。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi px"><img src="../Images/259f087632116ee1682257e274d4c269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E5655AJkEVKF95g551rzJg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">SQuAD 2.0 示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="77b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://www.aclweb.org/anthology/D17-1082/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">种族数据集</strong> </a></p><p id="b183" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个数据集由 12 到 18 岁的中国学生参加的初中和高中英语考试的短文组成，旨在评估学生的推理能力。数据集分为两部分:1 .RACE-m 数据集由中学 2 的段落组成。RACE-h 数据集由高中的段落组成。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi py"><img src="../Images/972f0540e2ac8b13828aea86e3c743e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*57Oz-biF5vNXaKvKinZsyA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">RACE-m 示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pz"><img src="../Images/db73cfa4a2e61c5a75617f55a2caea6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kj95yqHWubR_MJWEFCwf7w.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GPT-3 对阅读理解的准确性|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><h2 id="f18f" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated"><a class="ae lp" href="https://super.gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank">强力胶水</a></h2><p id="3a56" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">它可以被认为是所有能够执行多种语言建模任务的 NLP 模型的标准化测试。它根据模型在不同任务中的表现给出单一结果。由于结果是单一值，因此很容易在相同的规模上比较不同的模型。</p><p id="726a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该基准测试包括以下任务:</p><ol class=""><li id="4569" class="mv mw it kk b kl km ko kp kr nf kv ng kz nh ld pe nb nc nd bi translated">问题回答</li><li id="1179" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">自然语言推理(NLI)</li><li id="9f57" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated"><a class="ae lp" href="https://demo.allennlp.org/coreference-resolution" rel="noopener ugc nofollow" target="_blank">共指消解(coref。)</a></li><li id="eaa6" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated"><a class="ae lp" href="http://www.scholarpedia.org/article/Word_sense_disambiguation" rel="noopener ugc nofollow" target="_blank">词义消歧(WSD) </a></li></ol><p id="d581" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用的数据集:</p><ol class=""><li id="59ff" class="mv mw it kk b kl km ko kp kr nf kv ng kz nh ld pe nb nc nd bi translated"><a class="ae lp" href="https://arxiv.org/abs/1905.10044" rel="noopener ugc nofollow" target="_blank"> BoolQ </a></li><li id="095d" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated"><a class="ae lp" href="https://github.com/mcdm/CommitmentBank" rel="noopener ugc nofollow" target="_blank">承诺银行</a></li><li id="92cc" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">科帕</li><li id="caaa" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated"><a class="ae lp" href="https://github.com/CogComp/multirc" rel="noopener ugc nofollow" target="_blank"> MultiRC </a></li><li id="fae2" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated"><a class="ae lp" href="https://sheng-z.github.io/ReCoRD-explorer/" rel="noopener ugc nofollow" target="_blank">记录</a></li><li id="af5d" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated"><a class="ae lp" href="https://aclweb.org/aclwiki/Textual_Entailment_Resource_Pool" rel="noopener ugc nofollow" target="_blank"> RTE </a>(使用 RTE1、RTE2、RTE3 和 RTE5)</li><li id="3c55" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated"><a class="ae lp" href="https://pilehvar.github.io/wic/" rel="noopener ugc nofollow" target="_blank"> WiC </a></li><li id="4cd4" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated"><a class="ae lp" href="https://aaai.org/ocs/index.php/KR/KR12/paper/view/4492" rel="noopener ugc nofollow" target="_blank"> WSC </a></li></ol><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qa"><img src="../Images/3c2c38384f89e743dd26eda97201ee80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zztZU_sFvH563NexIN3BbA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">强力胶基准|参考:<a class="ae lp" href="https://super.gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank">强力胶研究论文</a>，公共领域</p></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/e3da4198ee6c8993a094a934b87859ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*fecbjKvvuAqmoVkd_3kCLQ.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GPT-3 在强力胶上的表现|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="65ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于 GPT-3，在上下文中使用了 32 个例子。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/f1c0b58f5bbc5fb7584d57a837a85875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*fu2Wht03d_cF3XWXbDIOHw.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GPT-3 FS 强力胶基准测试示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="9cc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们可以看到更多的例子，GPT-3 的准确性增加，但在非常小的数量。当在上下文中给出 8 个例子时，GPT-3 FS 超过了微调的 BERT-Large。</p></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><h2 id="7fa4" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">自然语言推理(NLI)</h2><p id="8d5a" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">在这种类型的任务中，模型决定两个句子之间的关系。这是一个分类问题，模型将第二个句子分为三类:1。支持第一句 2。与第一句相矛盾。中立的</p><p id="b681" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RTE 数据集用于在 NLI 任务上测试模型。与 RTE 一起，更难对抗的 NLI (ANLI)被用于测试 GPT-3 模型。在<a class="ae lp" href="https://adversarialnli.com/#" rel="noopener ugc nofollow" target="_blank">安立</a> GPT-3 的表现略好于随机机遇，但远低于 SOTA。</p><p id="0937" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1910.14599.pdf" rel="noopener ugc nofollow" target="_blank">安力</a>根据难度分为三组:第一轮、第二轮、第三轮。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi py"><img src="../Images/79561ec355b73a8d21686c580d5f0f00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hT7VkZ_ptjzxfhQ1JWiVTQ.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">安理 R1 举例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qd"><img src="../Images/2d9be357cb79c440fa2962b15b602fff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3rXH4rqx1atfiHwQwJt4Lw.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">安理 R2 举例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qe"><img src="../Images/a9092fa48022611e51b7a4f8a86a64cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mU7ChdWKMHZbXl1hoxn5bw.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">安立 R3 举例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><h1 id="40c3" class="lx ly it bd lz ma qf mc md me qg mg mh jz qh ka mj kc qi kd ml kf qj kg mn mo bi translated">综合和定性任务</h1><p id="418e" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">在这项测试中，GPT-3 执行各种综合任务，如算术运算，重新排列和解读单词中的字母，解决 SAT 式的类比问题，在句子中使用新单词，纠正英语语法，以及生成新闻文章。</p><h2 id="4c0a" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">算术</h2><p id="ad05" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">在这个测试中，GPT-3 执行算术运算的能力得到了检验。各种算术运算，如加法，减法和乘法测试。</p><p id="a201" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">算术问题包括以下类型:</p><ol class=""><li id="36f5" class="mv mw it kk b kl km ko kp kr nf kv ng kz nh ld pe nb nc nd bi translated">两位数加法(2D+)</li><li id="459e" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">两位数减法(2D-)</li><li id="fea0" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">3 位数加法(3D+)</li><li id="02e1" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">3 位数减法(3D-)</li><li id="c9d6" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">4 位数加法(4D+)</li><li id="54f6" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">4 位数减法(4D-)</li><li id="5782" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">5 位数加法(5D+)</li><li id="5bde" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">5 位数减法(5D-)</li><li id="20a8" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">两位数乘法(2Dx)</li><li id="2260" class="mv mw it kk b kl pf ko pg kr ph kv pi kz pj ld pe nb nc nd bi translated">一位数复合(1DC)</li></ol><p id="355c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与其他类型的设置相比，该模型在少拍设置中表现更好。而且，随着位数的增加，精度会大大降低。GPT-3 在这一系列上比小型号有了巨大的飞跃。</p><pre class="le lf lg lh gt ny nz oa ob aw oc bi"><span id="7644" class="nm ly it nz b gy od oe l of og">Examples of the questions asked to GPT-3:</span><span id="b123" class="nm ly it nz b gy oh oe l of og">Q: What is 17 minus 14?<br/>Q: What is (2 * 4) * 6?<br/>Q: What is 95 times 45?<br/>Q: What is 98 plus 45?</span></pre><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qk"><img src="../Images/83d43c79bd190aa6a63f6633780f9a0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pf3gZ9wUxQNuM42ZtaBJZg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GPT-3 关于算术运算的准确性|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ql"><img src="../Images/daf184a300749ddddc93d0f56ddd97bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DSxvMtn51NAGIDADS-rZRw.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GPT-3 FS 结果|参考:GPT-3 研究论文</p></figure><p id="4328" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着数字的增加，模型的精度呈指数下降。和复合方程等复杂运算越多，模型性能越差。</p><p id="ee10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="oj">该模型可能已经记住了较小的数字算术运算，因为它们在互联网上比大数字更常见。在训练集中搜索“&lt; NUM1 &gt; + &lt; NUM2 &gt; =”和“&lt; NUM1 &gt; - &lt; NUM2 &gt; =”，对于测试用例只找到 0.8%和 0.1%的匹配。但是，训练集可能具有“四十二+二十三等于六十五”或“四十二加二十三等于六十五”类型的等式，并且模型可能已经将整数和运算映射到该串。</em></p><p id="8188" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据错误的结果检查该模型，发现当操作包括进位时，该模型工作不正确。</p><h2 id="1cbb" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">单词打乱和操作任务</h2><p id="6f9e" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">在这种情况下，对单词进行各种操作，并将其作为模型的输入。模型需要预测原始单词。</p><p id="5517" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该任务包括 5 种类型的任务:</p><ol class=""><li id="e803" class="mv mw it kk b kl km ko kp kr nf kv ng kz nh ld pe nb nc nd bi translated"><strong class="kk iu">循环单词(CL) </strong>中的字母——循环单词的字母，并要求模型找到原始单词。</li></ol><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qm"><img src="../Images/f6e7eca4d638fbd006b10fd61ba193d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HGruYL0UeumEwo12rSB01g.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">循环信函示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="0570" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.<strong class="kk iu">除第一个和最后一个字符之外的所有字符的变位词(A1) </strong> —除了第一个和最后一个字母之外，单词的所有字母都被打乱。模型需要输出原始单词。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qn"><img src="../Images/ed732b1b8c77727f08efecee59cff263.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uBe5AKx0qc6pf6ESa-uFFg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">A1 字谜示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="65c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.<strong class="kk iu">除了前两个和后两个字符之外的所有字符的变位词(A2) </strong> —除了前两个和后两个字母之外，单词的所有字母都被打乱。该模型需要输出原始单词。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qo"><img src="../Images/2d250b36bce3589ab77bc7ad30ff67a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mfNvUfouSHKAwpYaW9RhyA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">A2 字谜示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="4a90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.<strong class="kk iu">单词中的随机插入(R1) </strong> —在字母之间插入随机标点或空格，模型需要预测原始单词。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qo"><img src="../Images/13273ff8d566874372615d1f655c1b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0EJh1KKdNRh5oJFt17joGg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">随机插入(R1)示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="1d4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5.<strong class="kk iu">反向单词(RW) </strong> —单词反向拼写，模型需要预测原始单词。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qp"><img src="../Images/ebe198cc21e023155d0329dd42b68709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KYEUdUeoJx_R6uNqF9bbkg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">反向单词(RW)示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="a962" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型在零触发设置中表现不佳，在单触发设置中表现一般。该模型给出了 100 个少镜头设置的例子，表现相当好。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/7c1e266246c93f68e5cf0e46541de637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ntGrUJmwqpPDKMP0utx6NA.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">解读任务的准确性|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><h2 id="4520" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated"><a class="ae lp" href="https://arxiv.org/pdf/cs/0309035.pdf" rel="noopener ugc nofollow" target="_blank"> SAT 类比</a></h2><p id="0552" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">在这种情况下，模型需要回答基于类比的问题。这道题是高考 SAT 中问的。随机猜测的准确率是 20%，而大学生的平均分是 57%。GPT-3 少数几次设置实现了 65.2%的准确性，一次设置实现了 59.1%的准确性，远高于学生的平均水平。零炮设置的准确率为 53.7%。对于少拍设置，给出的示例数量为 20 个。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/52ec4b8bc98b2adcf3bb26ca341b3dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*I-mId2etLEXBqPW-drWIJg.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">SAT 类比示例|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><h2 id="eb4a" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">新闻文章生成</h2><p id="bc10" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">在这个模型中，提供了标题和副标题，它需要用它们来写一篇 200 字左右的文章。该模型只在少数镜头设置上进行了测试，因为模型无法理解它是否需要写一篇文章或需要作为后续推文进行响应。</p><p id="9b14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将 GPT-3 的性能与故意制造次品的控制模型进行了比较。人被用来区分物品是人做的还是模型做的。人类以 86%的准确度准确地检测到文章是由控制模型生成的。但是随着模型大小的增加，人类能够以 52%的准确度检测到文章是由模型生成的。</p><p id="4e76" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">被该模型检测到的大多数文章包含错误的事实信息，或者有重复的句子和不寻常的措辞。</p><p id="c195" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型也在具有 500 个单词的文章上进行测试，但是人类仍然只能区分 52%的人类文章。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qs"><img src="../Images/56f2d55ba88f2603da95fc34108d3749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lq-Y-n6ihtYWAhY_-rp63w.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">由 GPT-3 生成的文章被检测到只有 12%的准确性|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="ff2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当你用谷歌搜索上述文章中的任何一句话时，你会找到一些有着相似句子的不同主题的文章。很有可能，GPT-3 在整个互联网上接受的训练可能只是用不到 500 个字总结了相关主题的所有文章。而不是生成文章，它可能只是合并不同文章的各种句子，从而使文章类似于人类。如果我们提供非常独特的标题，我们将知道 GPT-3 是否能生成文章。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qt"><img src="../Images/9fb82389477b680cf1c1e23b5634ef75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9SD8-bcM2OenzUm9T6Zq2A.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">人类检测文章 500 字的准确率是人类写的还是模型写的|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><h2 id="4518" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">学习和使用新单词</h2><p id="ae0c" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">在这种情况下，测试模型在定义单词一次后是否可以在句子中使用该单词。</p><p id="9488" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，为了测试这个任务的模型，单词定义被给定一次，所以一次性设置，但是提供了每个单词定义的例子，因此，关于任务描述的少量设置。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qu"><img src="../Images/6d4f7fe706124428d9af49be18b2778c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5uuy-E5NgIEqtOU4Zcf8Hg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GPT 新协议中定义和使用新词的示例|参考:GPT 新协议研究论文</p></figure><p id="b463" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些单词是人类造的，因此，模特事先对此一无所知。我们可以看到模型使用定义的单词生成了所有符合逻辑的句子。</p><h2 id="3741" class="nm ly it bd lz nn no dn md np nq dp mh kr nr ns mj kv nt nu ml kz nv nw mn nx bi translated">纠正英语语法</h2><p id="206b" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">测试该模型是否能够预测错误的语法并能够用正确的语法修改句子。该模型在少拍设置上进行了测试。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qv"><img src="../Images/fdaab951ae3692788379396244590b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y33bYgogTCiYjEvBPQYjtA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">纠正英语语法|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="ec24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，该模型是在非常基本的语法上进行测试的，而不是在复杂的语法上，如子句。很高兴看到模型在如此糟糕的语法上的表现，因为即使对人类来说，这也是一项复杂的任务。</p></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><h1 id="00cf" class="lx ly it bd lz ma qf mc md me qg mg mh jz qh ka mj kc qi kd ml kf qj kg mn mo bi translated">测量和防止基准记忆</h1><p id="8ace" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">由于训练数据集是通过爬取互联网生成的，因此训练数据集可能包含各种测试集示例。如果发生这种情况，那么模型在各种任务上报告的准确性的置信度可能是不可靠的。</p><p id="31f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在训练数据集中搜索测试集示例，如果两个集中的 N 个单词相似，则认为文档匹配。n 等于每组单词中第 5 个百分位数的示例长度。较小的 N 值会导致大量不符合逻辑的匹配。因此，N 的最小值保持为 8，最大值保持为 13。根据最小长度匹配 n 个单词或整个句子。如果找到匹配，则该示例被认为是脏的，否则是干净的示例。</p><p id="6a41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 clean 数据集上对 GPT-3 进行了各种任务的测试，并将精确度与原始分数进行了比较。如果精度几乎匹配，则意味着即使存在污染也不会影响模型精度。如果准确度低于原始分数，则意味着污染夸大了结果。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qw"><img src="../Images/6a048606a47ddbfb4610a2727f4420e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8NzFmtweNuitHB8tbpx_w.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">基准污染分析|参考:<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 研究论文</a>，公共领域</p></figure><p id="c001" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于数据集 QuAC，数据集内存在 0%的干净数据，但精度仍正向变化 20%，这肯定意味着它有问题。他们发现大约 90%的数据集被污染。但是，经过进一步的分析推断，训练集只包括段落，而不包括问题和答案。</p></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><p id="0012" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GPT-3 有各种局限性，在文本合成等领域仍有改进的机会。GPT-3 是基于训练数据集，因此，不能认为它自己的。如果大多数人对性别、种族、宗教有偏见，那么这个模型也会代表这些偏见。更多的限制和偏见，你可以在这里阅读<a class="ae lp" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"/>。<a class="ae lp" href="https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/" rel="noopener ugc nofollow" target="_blank">麻省理工科技评论</a>也发表了一篇关于 GPT-3 局限性的博客。</p></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><p id="0cc0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GPT-3 模型只是预测下一个单词应该出现。它不理解上下文，也不理解单词的实际含义。该模型缺乏逻辑推理和常识推理。模型输出可以通过调整数据集来改变。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="lj lk l"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">这条推文显示了 GPT 的偏见-3 |推文由脸书公共领域副总裁 Jerome Pesenti 发布</p></figure></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><p id="a760" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是您了解 GPT-3 更多信息的重要资源:</p><div class="qx qy gp gr qz ra"><a rel="noopener follow" target="_blank" href="/you-can-understand-gpt-3-with-these-youtube-videos-6a30887c928b"><div class="rb ab fo"><div class="rc ab rd cl cj re"><h2 class="bd iu gy z fp rf fr fs rg fu fw is bi translated">你可以通过这些 YouTube 视频了解 GPT 3</h2><div class="rh l"><h3 class="bd b gy z fp rf fr fs rg fu fw dk translated">通过这些 YouTube 视频，在不到 3 分钟的时间内对 GPT 3 号有一个初步的了解</h3></div><div class="ri l"><p class="bd b dl z fp rf fr fs rg fu fw dk translated">towardsdatascience.com</p></div></div><div class="rj l"><div class="rk l rl rm rn rj ro lv ra"/></div></div></a></div><div class="qx qy gp gr qz ra"><a href="https://analyticsindiamag.com/top-free-resources-to-learn-gpt-3/" rel="noopener  ugc nofollow" target="_blank"><div class="rb ab fo"><div class="rc ab rd cl cj re"><h2 class="bd iu gy z fp rf fr fs rg fu fw is bi translated">了解 GPT-3 -分析杂志的顶级免费资源</h2><div class="rh l"><h3 class="bd b gy z fp rf fr fs rg fu fw dk translated">随着开放人工智能发布其前卫的预训练语言模型——GPT-3 突然成为了…</h3></div><div class="ri l"><p class="bd b dl z fp rf fr fs rg fu fw dk translated">analyticsindiamag.com</p></div></div><div class="rj l"><div class="rp l rl rm rn rj ro lv ra"/></div></div></a></div><p id="9e4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的阅读，祝您有美好的一天。</p><p id="edbe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="oj">*斜体文字仅为个人观点</em></p></div></div>    
</body>
</html>