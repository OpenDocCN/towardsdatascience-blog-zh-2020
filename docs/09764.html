<html>
<head>
<title>Mathematics behind the training of Word2vec Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2vec 模型训练背后的数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mathematics-behind-the-training-of-word2vec-model-62d7e0ff76e4?source=collection_archive---------15-----------------------#2020-07-11">https://towardsdatascience.com/mathematics-behind-the-training-of-word2vec-model-62d7e0ff76e4?source=collection_archive---------15-----------------------#2020-07-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="95ed" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本文中，我们将了解 Word2Vec 算法训练背后涉及的数学。</h2></div><p id="a0a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我的上一篇文章中，我们介绍了单词嵌入的基本思想及其使用 genism 库的实现。不了解单词嵌入基础知识的人，我强烈建议他们看看我之前的文章。</p><div class="le lf gp gr lg lh"><a rel="noopener follow" target="_blank" href="/visualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354"><div class="li ab fo"><div class="lj ab lk cl cj ll"><h2 class="bd iu gy z fp lm fr fs ln fu fw is bi translated">基于 Gensim 和 PCA 的单词嵌入向量可视化</h2><div class="lo l"><h3 class="bd b gy z fp lm fr fs ln fu fw dk translated">在本文中，我们将学习如何使用单词嵌入来训练文本数据，并进一步使用…</h3></div><div class="lp l"><p class="bd b dl z fp lm fr fs ln fu fw dk translated">towardsdatascience.com</p></div></div><div class="lq l"><div class="lr l ls lt lu lq lv lw lh"/></div></div></a></div><p id="840c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Word2Vec 模型的训练可以使用两种算法来完成</p><p id="4fc6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单跳格模型</p><p id="0561" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2-连续单词包(CBOW)</p><p id="039a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将涉及 SG 模型背后的数学。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/b91bf89ac3f8f6eefc8102a74875b4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*poF6DXCz2_J7c-4I"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">约翰·莫塞斯·鲍恩在<a class="ae mm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="8d75" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">跳格模型:</strong></h1><p id="7141" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">该模型假设在文本语料库中，一个单词可以用来预测其周围的单词。该算法背后的主要思想是，给定一个中心词(<strong class="kk iu"> Vc </strong>)，它试图预测相邻词(<strong class="kk iu"> Vw </strong>)的条件概率，并进一步试图最大化该出现概率。要考虑的相邻单词的数量取决于窗口大小 m。下图使用窗口大小=2 来解释这一想法。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nk"><img src="../Images/76ad38c5535fcc94bf03eab2b1d784a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zHtbpwTU981ZxORWMZqjsg.jpeg"/></div></div></figure><p id="11e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们假设，给定中心单词，上下文单词彼此独立地生成。在这种情况下，所有单词同时出现的概率公式如下</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nl"><img src="../Images/3ba58729209b21f66de2b9cd0e0c2751.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kz7UiXHOMXWugTfPvSf5bw.jpeg"/></div></div></figure><h1 id="2c35" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">定义损失函数:</strong></h1><p id="d9f4" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">上面的概率计算只是在整个语料库中以某个随机词为中心。但是我们将不得不遍历整个语料库<strong class="kk iu"> (T) </strong>以一次一个地将所有单词视为中心单词，并针对 m 的窗口大小来预测其各自的周围单词。最大似然函数可以写成如下</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nm"><img src="../Images/cbc7429bfaafa467301e8e9ab6931f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sxGj0rRUWpcwZ2aROTue-g.jpeg"/></div></div></figure><p id="45d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所知，在梯度计算方面使用对数似然函数总是很容易的，因此我们在两边取对数，并将其转换成对数形式。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nn"><img src="../Images/9691460bbb39658e30889dc48bd06f0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-fLikrkCovcnDFNRUICWiA.jpeg"/></div></div></figure><p id="ea26" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们定义一些符号，因为我们将从这里开始使用它们-</p><p id="e5de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">Vc</strong>——中心词的向量表示。</p><p id="b5ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">Uo</strong>-上下文单词的矢量表示。</p><h1 id="7174" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">概率计算:</strong></h1><p id="4ccd" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">可以使用 softmax 函数来获得为给定的中心目标单词<strong class="kk iu"> c </strong>生成上下文单词<strong class="kk iu"> o </strong>的条件概率。为了说服你自己，你可以认为如果两个词是相同的，那么相似度是 1，概率是 1。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi no"><img src="../Images/48fe809ad4196b3f9d1cf1e8fde90e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GSx4ky5LS9vsOC0JQ2EI8g.jpeg"/></div></div></figure><p id="f815" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">softmax 通过对整个词汇 v 进行归一化，将其转换为概率。点积表示两个单词向量之间的相似度。</p><p id="32e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们在将表达式代入概率后，写出最终的成本函数。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi np"><img src="../Images/90ddd297d30a7feeb16e8f0bc7fdf59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1IyaZU_SwjwsoCmJgL9oVg.jpeg"/></div></div></figure><h1 id="b2bd" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">梯度计算:</strong></h1><p id="894a" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">我们的主要目标是在一个<strong class="kk iu">降维空间</strong>中找到文本中每个单词的向量表示。这里的诀窍是每个单词<strong class="kk iu"> w </strong>将有两种不同的表示，当单词 w 是中心单词时有一个<strong class="kk iu"> Vw </strong>，当单词 w 是上下文单词时有另一个<strong class="kk iu"> Uw </strong>。因此，我们之前讨论过的参数<strong class="kk iu">θ</strong>将是<strong class="kk iu"> 2V x d 维向量</strong>，其中每行代表词汇表中的一个不同单词，其跨列的 d 维向量表示。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nq"><img src="../Images/b0bec9e98504dfde399977773c3fec55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dfi1peB0xTlJP5B7tqu4-w.jpeg"/></div></div></figure><p id="6bea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在每个单词有两个向量表示。为了训练我们的模型，我们将对<strong class="kk iu"> Vw 和 Uw 求导，并使用梯度下降更新这些向量。</strong> Vc 和 Vw 基本相同。我们刚刚改变了符号，以避免任何混淆。根据矩阵<strong class="kk iu">θ，我们只有两个参数 Vc 和 Uw。</strong></p><h1 id="c5a1" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">相对于 Vc 的梯度:</strong></h1><p id="ebd1" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated"><strong class="kk iu">第一步:</strong></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nr"><img src="../Images/df77e0f82376340a97152b3598354cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DvrJNOHzoYinjMd6Dynvsw.jpeg"/></div></div></figure><p id="498f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二步:对第二项应用链式法则。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ns"><img src="../Images/85774f93eaaa30a9a4ff8737376c4023.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nCtTeBEyXN9QqEq-USNhng.jpeg"/></div></div></figure><p id="1411" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第三步:</strong></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nt"><img src="../Images/fe1e22b935cb5b96b3e9e72d766ebe97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rAPQVi_GfIgWiY0ow3w0NQ.jpeg"/></div></div></figure><p id="a119" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第四步:</strong></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nu"><img src="../Images/3a5ca0cd21d4d0495d2dc8c6bd539fe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*moPZUaXd8yYe_LQUQ_GDjQ.jpeg"/></div></div></figure><h1 id="64a2" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">相对于 Uw 的梯度:</h1><p id="db27" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">这里 w 是上下文词。因此，可能有两种情况，o 是上下文单词 w，而 o 不是上下文单词 w。因此，我们将梯度分成两部分来计算相对于 Uw 的梯度。按照和上面 Vc 一样的计算方法，我们可以用对称性写出梯度 w.r.t Uw。</p><p id="17bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">什么时候，</p><h2 id="a667" class="nv mo it bd mp nw nx dn mt ny nz dp mx kr oa ob mz kv oc od nb kz oe of nd og bi translated">O ≠ W:</h2><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi oh"><img src="../Images/073c294268e30b04fa7499c51db99c07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q4s-INKLg42IdzzbGJfI4A.jpeg"/></div></div></figure><h2 id="70f2" class="nv mo it bd mp nw nx dn mt ny nz dp mx kr oa ob mz kv oc od nb kz oe of nd og bi translated">O = W:</h2><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi oi"><img src="../Images/eddc74dad2162c3c958a310d763e60cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OYXANy6ZKVftkiS1tHsLrg.jpeg"/></div></div></figure><h1 id="283a" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">梯度下降:</strong></h1><p id="157a" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">现在，我们将使用梯度下降算法更新整个语料库的计算梯度。该算法的伪代码如下所示</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi oj"><img src="../Images/d5bf96e4248b9c312e455e73a00d9f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eXTl43CM-SWjCwHtt5cTrA.jpeg"/></div></div></figure><p id="39a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，这种方法有一个问题。正如我们所见，在梯度的分母中，我们必须取所有单词点积的指数，当我们有大量词汇时，这非常耗时。我们将需要训练数十亿个权重，这在计算上是昂贵的。因此，我们使用一种叫做负采样的技术来解决这个问题。在这种技术中，我们只从基于单字母分布的随机选择的词汇表中抽取少量单词。我不打算在这篇文章中讨论这些细节。</p><h1 id="5b57" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">结论:</strong></h1><p id="7600" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">我希望这篇文章能让您对 Word2vec 模型背后的数学有一个基本的了解。由于嵌入了太多的图片，这篇文章可能看起来有点令人沮丧，但是如果你一步一步地去做，我相信它会非常有帮助。如果你们能给我一些反馈，那就太好了，这样我下次就能以更好的方式展示了。</p><h1 id="b299" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">参考文献:</strong></h1><p id="2558" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated"><a class="ae mm" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">1-https://papers . nips . cc/paper/5021-单词和短语的分布式表示及其组成性. pdf </a></p><h1 id="a7f7" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">感谢您的阅读！！！！</h1><p id="0fea" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">如果你喜欢我的工作并想支持我:</p><p id="cb96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1-支持我的最好方式是跟随我上<a class="ae mm" href="https://medium.com/@saketthavananilindan" rel="noopener"> <strong class="kk iu">中</strong> </a> <strong class="kk iu">。</strong></p><p id="9f3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2-在<a class="ae mm" href="https://www.linkedin.com/in/saket-thavanani-b1a149a0/" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">LinkedIn</strong></a><strong class="kk iu">上关注我。</strong></p></div></div>    
</body>
</html>