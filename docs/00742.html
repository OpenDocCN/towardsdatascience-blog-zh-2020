<html>
<head>
<title>Recommending news articles based on already read articles</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于已阅读的文章推荐新闻文章</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recommending-news-articles-based-on-already-read-articles-627695221fe8?source=collection_archive---------18-----------------------#2020-01-21">https://towardsdatascience.com/recommending-news-articles-based-on-already-read-articles-627695221fe8?source=collection_archive---------18-----------------------#2020-01-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="719b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python中基于内容的推荐</h2></div><p id="c44e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于在你最喜欢的在线平台上很容易获得大量的商品(服务)，如<em class="le">电子商务</em>、<em class="le">求职门户</em>、<em class="le">送餐</em>、<em class="le">音乐或视频流</em>，快速找到你选择的所需商品是非常困难和耗时的。这些平台可以通过分析你过去与系统的互动或行为，根据你的兴趣和偏好推荐<strong class="kk iu">商品来帮助你。</strong></p><p id="5e1a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从<strong class="kk iu">亚马逊</strong>到<strong class="kk iu"> Linkedin </strong>，<strong class="kk iu">优步吃</strong>到<strong class="kk iu"> Spotify </strong>，<strong class="kk iu">网飞</strong>到<strong class="kk iu">脸书</strong>，<strong class="kk iu">推荐系统</strong>被最广泛地用于向他们的用户推荐“相似的项目”、“相关的工作”、“喜欢的食物”、“感兴趣的电影”等等。</p><p id="12aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">推荐系统</em> </strong>提供适当的商品建议，有助于促进销售、增加收入、留住客户并增加竞争优势。<em class="le">推荐</em>方法基本有两种。</p><ol class=""><li id="5751" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated"><strong class="kk iu">基于内容的推荐</strong></li><li id="392f" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld lk ll lm ln bi translated"><strong class="kk iu">协同过滤</strong></li></ol><p id="64c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">基于内容的推荐</em> </strong>基于通过用户<strong class="kk iu"> <em class="le">属性</em> </strong>获得的用户/项目之间的相似度。它使用关于<strong class="kk iu"> <em class="le">用户</em> </strong>或<strong class="kk iu"> <em class="le">条目</em> </strong>的附加信息(元数据)，即它依赖于什么样的<strong class="kk iu"> <em class="le">内容</em> </strong>已经可用。该元数据可以是用户的<strong class="kk iu"> <em class="le">人口统计信息</em> </strong>，如<em class="le">年龄</em>、<em class="le">性别</em>、<em class="le">工作</em>、<em class="le">位置</em>、<em class="le">技能集、</em>等。同样，对于<strong class="kk iu"> <em class="le">项目</em>、</strong>可以是<em class="le">项目名称</em>、<em class="le">规格</em>、<em class="le">类别</em>、<em class="le">登记日期、</em>等。</p><p id="f895" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以其核心思想就是根据物品的<strong class="kk iu"><em class="le"/></strong>属性，通过找到与关注的<strong class="kk iu"> <em class="le">物品/用户</em> </strong>相似的物品/用户来推荐物品。</p><p id="c336" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇博客中，我们将使用<strong class="kk iu">新闻类别</strong>数据集来讨论<em class="le">基于内容的推荐</em>。目标是通过使用诸如文章标题<em class="le">、<em class="le">类别</em>、<em class="le">作者</em>和<em class="le">出版日期</em>等属性来推荐与已阅读文章相似的<em class="le">新闻文章</em>。</em></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/d473e14dc08780370397da590cd32a8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*E33jvgZcZX2Ib8HP2SUClQ.png"/></div></figure><p id="90e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么让我们开始使用Kaggle上可用的<a class="ae mb" href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="le">新闻类别数据集</em></strong></a><strong class="kk iu"><em class="le"/></strong>。数据集包含大约20万个对6个不同特征的观察，如<em class="le">新闻标题</em>、<em class="le">类别</em>、<em class="le">作者</em>、<em class="le">简短描述</em>、<em class="le">出版日期、</em>等。</p><blockquote class="mc md me"><p id="cfb6" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">附加说明:为了访问完整的python代码，请访问kaggle内核<a class="ae mb" href="https://www.kaggle.com/vikashrajluhaniwal/recommending-news-articles-based-on-read-articles" rel="noopener ugc nofollow" target="_blank">这里</a>(<a class="ae mb" href="https://www.kaggle.com/vikashrajluhaniwal/recommending-news-articles-based-on-read-articles" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/vikashrajluhaniwal/recommending-news-articles-based-on-read-articles</a>)。</p></blockquote><h2 id="6286" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">1.数据预处理</h2><ul class=""><li id="0704" class="lf lg it kk b kl nb ko nc kr nd kv ne kz nf ld ng ll lm ln bi translated"><strong class="kk iu"> <em class="le">仅获取2018年</em> </strong>的文章——由于数据集大小相当大，因此处理整个数据集可能会耗费太多时间。为了避免这种情况，我们只考虑2018年的最新文章。</li><li id="dfef" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld ng ll lm ln bi translated"><strong class="kk iu"> <em class="le">移除所有短标题文章</em></strong><em class="le">——</em><strong class="kk iu"><em class="le"/></strong>从标题中移除停用词后，标题非常短的文章可能会变成空白标题文章。所以我们把标题里字数少(&lt; 5)的文章都去掉吧。</li><li id="4bee" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld ng ll lm ln bi translated"><strong class="kk iu"> <em class="le">检查并删除所有重复的</em></strong><em class="le">——</em>由于有些文章的标题完全相同，所以让我们删除所有标题重复的文章。</li></ul><h2 id="7fe0" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated"><strong class="ak"> 2。基础数据探索</strong></h2><p id="0920" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated"><strong class="kk iu"> a .基本统计—文章数量、作者、类别</strong></p><p id="862e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">经过数据预处理后，共有892位作者的8485篇新闻文章，分属26个不同的类别。</p><p id="f1f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> b .文章类别分布</strong></p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="ffaf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从条形图中，我们可以观察到<code class="fe nm nn no np b">politics</code>类别的<em class="le">文章数量最高</em>文章数量次之<code class="fe nm nn no np b">entertainment</code>文章数量最高，以此类推。</p><p id="ac2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> c .每月文章数量</strong></p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="7395" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从柱状图中，我们可以观察到<code class="fe nm nn no np b">January </code>月的<em class="le">文章数量最高</em>，然后是<code class="fe nm nn no np b">March</code>等等。</p><p id="5c0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> d. PDF为标题长度</strong></p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="7062" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">标题</em> <code class="fe nm nn no np b">length</code>的<em class="le">概率分布</em>函数几乎类似于<em class="le">高斯分布</em>，其中<em class="le">标题</em>的大部分长度为58到80个字。</p><h2 id="8ba7" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">3.文本预处理</h2><ul class=""><li id="70e0" class="lf lg it kk b kl nb ko nc kr nd kv ne kz nf ld ng ll lm ln bi translated"><strong class="kk iu"> <em class="le">停用词移除</em> </strong> <em class="le"> — </em>停用词对分析没有太大帮助，而且包含停用词会在处理过程中耗费大量时间，所以让我们移除它们。</li><li id="b758" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld ng ll lm ln bi translated"><strong class="kk iu"> <em class="le">引理化— </em> </strong>让我们找到单词的基本形式(引理)来考虑与引理相同的单词的不同屈折。</li></ul><h1 id="13ff" class="nq mj it bd mk nr ns nt mn nu nv nw mq jz nx ka mt kc ny kd mw kf nz kg mz oa bi translated">4.基于标题的新文章相似度</h1><p id="c18e" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">通常，我们根据<em class="le">距离</em>来评估<em class="le">相似度</em>。如果<em class="le">距离</em>最小，则高<em class="le">相似度</em>，如果最大，则低<em class="le">相似度</em>。为了计算<em class="le">距离</em>，我们需要将标题表示为一个<em class="le"> d维</em>向量。然后我们可以根据向量之间的<em class="le">距离</em>找出<em class="le">相似度</em>。</p><p id="5fd1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有多种方法将一个<em class="le">文本</em>表示为一个<em class="le"> d维</em>向量，如<em class="le">单词包</em>、<em class="le"> TF-IDF方法</em>、<em class="le"> Word2Vec嵌入、</em>等。每种方法都有自己的优缺点。</p><p id="b4e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们一个一个的通过所有的方法来看头条的特征表现。</p><h2 id="af12" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated"><strong class="ak"> a .使用包字法</strong></h2><p id="f434" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated"><em class="le">单词包(BoW) </em>方法表示单词在<em class="le">文档中的出现。</em>在这里，每个标题可以被认为是一个<em class="le">文档</em>，所有标题的集合形成一个<em class="le">语料库。</em></p><p id="3b95" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<em class="le">弓</em>的方法，每个<em class="le">文档</em>由一个<strong class="kk iu"> <em class="le">维</em> </strong>向量表示，其中<em class="le">维</em> <strong class="kk iu"> </strong>是语料库中<strong class="kk iu"> </strong> <em class="le">唯一词</em>的总数。这一组独特的单词构成了词典。</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ob nl l"/></div></figure><p id="44ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以上功能基于标题推荐10篇<em class="le">与<em class="le">查询(阅读)</em>文章相似的</em>文章。它接受两个参数——已读文章的索引和要推荐的文章总数。</p><p id="0fa2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基于<em class="le">欧几里德距离</em>它找出10个最近的<em class="le">邻居</em>并推荐。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a545f62004d54d4d85b385383812d1a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*DYGga3m-2EGmws7PFR0miw.png"/></div></figure><p id="b074" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点</strong></p><ol class=""><li id="fa9b" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated">它对<em class="le">语料库</em>中<em class="le">不太频繁</em>观察到的单词赋予很低的<em class="le">重要性</em>。在整个<em class="le">语料库</em>中，<code class="fe nm nn no np b">employer</code>、<code class="fe nm nn no np b">flip</code>、<code class="fe nm nn no np b">fire</code>等被查询文章中的几个词出现<em class="le">的频率较低</em>，因此<em class="le"> BoW </em>方法<em class="le">不会向</em>推荐<em class="le"> headline </em>包含这些词的任何文章。由于<code class="fe nm nn no np b">trump</code>是<em class="le">语料库</em>中常见的词，所以它推荐标题包含<code class="fe nm nn no np b">trump</code>的文章。</li><li id="5e76" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld lk ll lm ln bi translated"><em class="le">鞠躬</em>方法不保留单词的顺序。</li></ol><p id="1e23" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了克服第一个缺点，我们使用<strong class="kk iu"> <em class="le"> TF-IDF </em> </strong>方法进行特征表示。</p><h2 id="f559" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated"><strong class="ak"> b .使用TF-IDF方法</strong></h2><p id="9632" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated"><em class="le"> TF-IDF </em>方法是一种加权方法，它赋予<em class="le">语料库</em>中不太常用的词更多的重要性。它根据<em class="le">术语频率(TF) </em>和<em class="le">逆文档频率(IDF)为文档中的每个<em class="le">术语(单词)</em>分配一个权重。</em></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi od"><img src="../Images/effa63cfacb60c9b2b0f18b32b971a1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*T-K3qKOMmMNoXB3m4mf0Cw.png"/></div></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/38a9849a508766661ec1e62225bc6007.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*6rDFP44AqQCmD6ma1NDnww.png"/></div></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi of"><img src="../Images/44cc6410e5f87b82989e31793dabc6bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*cq6F2MYVhM5HH1GRvz_wXQ.png"/></div></figure><p id="3066" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，如果一个单词在一个文档中出现的次数更多，但在所有其他文档中出现的次数更少，那么它的<em class="le"> TF-IDF </em>值将会很高。</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ob nl l"/></div></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi og"><img src="../Images/bce9b7e869bb316e2d0487c54d45101f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*zcgeyG3WWe6CxkPKKfwJlw.png"/></div></figure><p id="d581" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相比于<em class="le"> BoW </em>方法，这里<em class="le"> TF-IDF </em>方法推荐标题<em class="le">的文章</em>中包含类似<code class="fe nm nn no np b">employer</code>、<code class="fe nm nn no np b">fire</code>、<code class="fe nm nn no np b">flip</code>的词在前5位推荐，这些词在<em class="le">语料库</em>中出现频率<em class="le">较少</em>。</p><p id="45c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点</strong></p><ul class=""><li id="99c5" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld ng ll lm ln bi translated"><em class="le"> Bow </em>和<em class="le"> TF-IDF </em>方法不捕获给定单词与其他单词的<em class="le">语义</em>和<em class="le">句法</em>相似性，但这可以使用嵌入 的<strong class="kk iu"> <em class="le">单词来捕获。例如，<code class="fe nm nn no np b">trump </code>和<code class="fe nm nn no np b">white house</code>、<code class="fe nm nn no np b">office</code>和<code class="fe nm nn no np b">employee</code>、<code class="fe nm nn no np b">tiger</code>和<code class="fe nm nn no np b">leopard</code>、<code class="fe nm nn no np b">USA</code>和<code class="fe nm nn no np b">Washington D.C</code>等词之间就有很好的联想。使用<em class="le">单词嵌入</em>技术可以捕获这种<em class="le">语义</em>相似性。<br/> <em class="le">单词嵌入</em>技术像<em class="le"> Word2Vec </em>、<em class="le"> GloVe </em>和<em class="le"> fastText </em>利用<em class="le">语义</em>单词</em>之间的相似性。</strong></li></ul><h2 id="6cc4" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated"><strong class="ak"> c .使用Word2Vec嵌入</strong></h2><p id="4e37" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated"><em class="le"> Word2Vec </em>是<em class="le">语义</em>相似度的技术之一，由<em class="le"> Google </em>于2013年发明。对于给定的<em class="le">语料库</em>，在训练过程中，它观察模式，并用一个d <em class="le">维</em>向量来表示每个单词。为了得到更好的结果，我们需要一个相当大的语料库。</p><p id="6f4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于我们的语料库规模很小，所以让我们在<em class="le">谷歌新闻</em>文章上使用谷歌的预训练模型。这个标准模型包含通过对数百万篇新文章进行训练而获得的数十亿个单词的向量表示。在这里，每个单词用一个300维的<em class="le">密集</em>向量来表示。</p><p id="ace9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于模型给出了每个<em class="le">单词</em>的矢量表示，但是我们计算了<em class="le">标题</em>之间的距离，因此我们需要获得每个<em class="le">标题</em>的矢量表示。一种方法是首先将标题中所有可用单词的矢量表示相加，然后计算平均值。也被称为<em class="le">平均Word2Vec </em>型号。</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ob nl l"/></div></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/930e3a731b4cc4d4642071c02391d803.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*D3DVF1eYMvGUHxEsb6H39A.png"/></div></figure><p id="1fa6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，基于<em class="le"> Word2Vec </em>的表示推荐包含与被查询<em class="le">文章</em>中的<code class="fe nm nn no np b">trump</code>相关联的<code class="fe nm nn no np b">white house</code>的<em class="le">标题</em>。类似地，它推荐带有与被查询的<em class="le">标题</em>中的<code class="fe nm nn no np b">employer</code>、<code class="fe nm nn no np b">sue</code>具有语义相似性的<code class="fe nm nn no np b">official</code>、<code class="fe nm nn no np b">insist</code>的标题。</p><blockquote class="mc md me"><p id="7364" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">注意:到目前为止，我们只推荐使用一个特征，即<em class="it">标题</em>，但是为了使<em class="it">推荐系统更加健壮</em>，我们需要一次考虑<em class="it">多个</em>特征。基于商业利益和规则，我们可以决定每个特征的权重。</p></blockquote><p id="a358" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看不同的型号与不同的<em class="le">功能组合</em>的文章相似性。</p><h2 id="2b0f" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">d.基于标题和类别的加权相似度</h2><p id="e127" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">我们先来看看基于<em class="le">标题</em>和<em class="le">类别</em>的文章相似度。我们使用<em class="le">一键编码</em>来获得<em class="le">类别</em>的特征表示。</p><p id="87fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有时根据业务需要，我们可能需要更多地优先考虑来自同一<em class="le">类别</em>的文章。在这种情况下，我们可以在推荐时给<em class="le">类别</em>分配更多的权重。权重越高，意义越大。同样，权重越小，特定特征的重要性就越小。</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ob nl l"/></div></figure><p id="0c43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述函数采用两个额外的参数<em class="le"> w1 </em>和<em class="le"> w2 </em>作为对应于<em class="le">标题</em>和<em class="le">类别</em>的权重。在从<em class="le"> 0到1 </em>的范围内传递<em class="le">重量</em>始终是一个好的做法，其中接近1的值表示重量大，而接近0的值表示重量小。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/77cedf11f6fb4ffe0aa1c95990f053ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*11c7HsyGR5Qo7EbWmKx_KQ.png"/></div></figure><p id="df5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们可以观察到推荐的文章与查询的文章<em class="le">类别</em>来自同一个<em class="le">类别</em>。这是由于高值传递到<em class="le"> w2 </em>。</p><h2 id="2d0e" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">e.基于标题、类别和作者的加权相似度</h2><p id="b5f3" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">现在让我们来看看基于<em class="le">作者</em>以及<em class="le">标题</em>和<em class="le">类别</em>计算文章相似度。同样，我们通过<em class="le">一次性编码</em>对<em class="le">作者</em>进行编码。</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ob nl l"/></div></figure><p id="6203" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的函数为<em class="le">作者</em>多了一个权重参数<em class="le"> w3 </em>。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/114ddf3e32f49800f1a2823702925c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*mtGYMlcfM9wEoQZIfXvflQ.png"/></div></figure><p id="c692" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在输出中，我们可以观察到，由于对<em class="le"> w3 </em>的高权重，推荐的文章来自与查询的文章<em class="le">作者</em>相同的<em class="le">作者</em>。</p><h2 id="ecf2" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">f.基于标题、类别、作者和出版日期的加权相似度</h2><p id="5307" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">现在让我们来看看基于发布<em class="le">工作日</em>作者<em class="le">以及<em class="le">标题</em>、<em class="le">类别、</em>和<em class="le">作者</em>计算文章相似度。同样，我们通过<em class="le">一键编码</em>来编码这个新特性。</em></p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ob nl l"/></div></figure><p id="2a93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述函数为<em class="le">日和月</em>额外增加了一个权重参数<em class="le"> w4 </em>。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="ol om di on bf oo"><div class="gh gi ok"><img src="../Images/15fa639ec2c08e79e7ec8c894b270efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AqU9gRAlEsoFJwwOeCgt9Q.png"/></div></div></figure><p id="3077" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在输出中，我们可以看到，由于对<em class="le"> w4 </em>的高权重，推荐的文章与查询的文章来自同一个<em class="le">日和月</em>。</p><h2 id="1f95" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">结束注释</h2><p id="baeb" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">在本文中，我们讨论了不同类型的<em class="le">单词嵌入</em>技术，以及针对<em class="le">基于内容的推荐</em>的<em class="le">特征</em>的不同组合，以及与每种技术相关的常见问题。</p></div></div>    
</body>
</html>