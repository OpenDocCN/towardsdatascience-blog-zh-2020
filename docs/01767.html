<html>
<head>
<title>A guide to Collaborative Topic Modeling recommender systems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">协作主题建模推荐系统指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-collaborative-topic-modeling-recommender-systems-49fd576cc871?source=collection_archive---------13-----------------------#2020-02-18">https://towardsdatascience.com/a-guide-to-collaborative-topic-modeling-recommender-systems-49fd576cc871?source=collection_archive---------13-----------------------#2020-02-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fbf7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">具有矩阵外预测能力的推荐系统的理论与实现。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e0828b8437f6151f4a9aaae0bb797a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7UbI2D7w9kJ0ZftA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·斯皮斯克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7edf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">推荐系统是一大类机器学习模型，其目的是预测用户<em class="lv"> u </em>会给一个项目<em class="lv"> i </em>的未观察到的评级。</p><p id="858d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本指南中，我们将讨论由<a class="ae ky" href="http://www.cs.columbia.edu/~blei/papers/WangBlei2011.pdf" rel="noopener ugc nofollow" target="_blank"> Wang 和 Blei(2011)</a>【3】介绍的<strong class="lb iu">协作主题建模/回归(CTM/CTR) </strong>，这是一个基于文本的项目推荐系统，具有增强的准确性和矩阵外预测能力。我们还将为那些不关心模型背后的数学细节的人提供 Python3 实现。</p><p id="b7b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CTM 建立在两个众所周知的模型上，即概率矩阵分解(PMF) [1]和潜在狄利克雷分配(LDA) [2]，因此需要对上述模型有初步的了解。对于那些不熟悉它们的人来说，网上有很多非常好的指南，其中包括:</p><ul class=""><li id="f044" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158">潜在的狄利克雷分配</a>，作者 Thushan Gangedara</li><li id="a9b2" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/probabilistic-matrix-factorization-b7852244a321">概率矩阵分解</a>，作者 Benjamin Draves</li></ul><h1 id="574d" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">为什么要合作主题建模</h1><p id="0935" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">概率矩阵分解(PMF)是一种非常简单而强大的方法，当我们唯一可用的数据是一个<em class="lv"> (U X I) </em>稀疏评级矩阵时，它允许我们推断项目和用户潜在的特征。</p><p id="8701" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，这种方法的简单性也导致了它的消亡，因为 PMF 无法对新的、完全未评级的项目进行归纳:因为没有对项目<em class="lv"> j </em>进行评级，所以该模型无法导出其潜在的质量向量。用技术术语来说，我们说 PMF 无法进行矩阵外预测。<br/> 在未评级的项目因为没有评级而没有得到推荐，并且因为没有得到推荐而一直没有评级的情况下，这个问题特别麻烦。这种情况的一个例子是科学出版物领域，在这一领域，不太知名的作者的新论文尽管质量很好，却无法到达潜在感兴趣的读者手中。</p><h1 id="a07f" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">直觉</h1><p id="a77d" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">CTM 背后的核心思想是我们可以将文档<em class="lv"> i、Qᵢ </em>的潜在质量向量表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/f304fa93d4946c59fc7853be99363a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SkvAnSCmm8K-Ze-XjzpG6g.png"/></div></div></figure><p id="7a6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> θᵢ </em>是从传统 LDA 估计中获得的项目<em class="lv"> i </em>的主题比例的<em class="lv">(k×1)</em>向量，<em class="lv"> εᵢ </em>是调整主题比例的<em class="lv">(k×1)</em><strong class="lb iu">偏移向量</strong>。偏移向量的基本原理是，它通过结合来自观察到的评级的信息来调整主题比例。<br/>举个例子:假设你有两篇科学论文，而且都是 50%关于“机器学习”，50%关于“生物学”。现在，根据观察到的评级，可能第一篇论文对机器学习研究人员更有吸引力，而第二篇论文对生物学研究人员更有吸引力。使用偏移向量εᵢ，我们能够校准主题比例，以反映不同主题对不同个人的吸引力。</p><h1 id="72a1" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">模型</h1><p id="f1fb" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">就像它赖以建立的 PMF 和 LDA 一样，CTM 也有自己的生成过程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/a4c09cceb8a6ed4c53bfc4b63811dfec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NNuNrPh8r1d6RCScQyiaSQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">概率矩阵分解的生成过程。注意，步骤 1.1、3.1 与 PMF 相同，步骤 2.1、2.3 与 LDA 相同。唯一建模不同的是步骤 2.2。</p></figure><p id="bdcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> σ _P </em>和<em class="lv"> σ _Q </em>表示我们对 P 和 Q 中的向量元素的分布先验地施加<em class="lv">的方差。类似地，<em class="lv"> σ </em>表示我们对评级分布先验地施加的方差。对于那些不熟悉贝叶斯统计的人来说，这些值可以简单地视为我们模型的正则化超参数。</em></p><p id="b508" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用平板符号表示生成过程，我们可以清楚地看到，CTM 只不过是 LDA 模型在 PMF 模型之上的堆叠版本，其中主题比例向量θᵢ被用作<em class="lv"> Qᵢ </em>的生成分布的平均值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/bbfa62b272de8dca19afa936ebc122d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OCpLo4cpRJq-KwwIfeeXLg.png"/></div></div></figure><h1 id="5573" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">学习参数</h1><p id="0bd9" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在推荐系统设置中，我们感兴趣的是获得对所有潜在变量的估计，这些变量有助于确定评级 rᵤᵢ.因此，我们会对<em class="lv"> θᵢ </em>、<em class="lv"> Qᵢ </em>、<em class="lv">、</em>和<em class="lv"> Pᵤ.感兴趣<br/> </em>由于完全后验概率<em class="lv"> P(θ，Q，P) </em>在分析上是难以处理的，我们借助于最大似然估计，其中我们数据的似然定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/0dbbf2defb47606bf1fe5d337db43204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xFGn6SVqSo-0OVzo3DDqSw.png"/></div></div></figure><p id="a95a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像在大多数 MLE 场景中一样，使用对数似然法很方便:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/4418abda5d6c41e26131e2cc7719d4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u1ZLQ-KXR-A9TmgTTrXgSg.png"/></div></div></figure><p id="af73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单独导出四个分量中的每一个是很方便的。</p><p id="4aa4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第一部分:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/90e116852fa466583ad83b7087c94a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qpldExRGokD5wIBIEikLCw.png"/></div></div></figure><p id="abec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第二部分:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/91c46322c30080c31f081673c422417f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RZbUhKhMR1NB1i629BGThA.png"/></div></div></figure><p id="65ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第三十部:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/9bc7236429839b5348adf2151d512b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fd6ZNa0liTcBx0omMLcynw.png"/></div></div></figure><p id="714f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为方便起见，假设α=1，上述分布变为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/f2568586ae97792f8bbf14e5090ff900.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bT857aFRR99ejPS-h9KZ1Q.png"/></div></div></figure><p id="8c9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第四部分:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/86f012b21c0390878f8c3d81a9aed7c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5vh92POSEa9BrxmODXEUWw.png"/></div></div></figure><p id="f33d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的对数可能性将是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/a13d68ff483e137fbe633d4af30bb341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0omr0mHgd3ntNyJfnTAf_g.png"/></div></div></figure><p id="4060" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文最初采用的策略是使用坐标上升，在优化<em class="lv">【P，Q】</em>和<em class="lv"> θ之间迭代交替。</em>但正如同一作者指出的，使用通过标准 LDA 估计获得的对<em class="lv"> θ </em>的估计，然后仅对<em class="lv"> [P，Q] </em>进行优化，给出了可比较的结果，并在训练期间节省了大量时间。因此，我们将假设我们已经从 vanilla LDA 获得了我们的<em class="lv"> θ </em>估计，并且我们将通过<strong class="lb iu">梯度上升</strong>来优化<em class="lv"> [P，Q】</em>。很容易得出我们的对数似然相对于<em class="lv">【P，Q】</em>的梯度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/35706ea133b5c8192d528daf9df0b26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ejkh745g6yXXSYQmX3HBGA.png"/></div></div></figure><p id="66d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的公式是本节的主要结果，我们将在下面的 Python3 中实现它。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="af52" class="mk ml it bd mm mn oa mp mq mr ob mt mu jz oc ka mw kc od kd my kf oe kg na nb bi translated">代码实现</h1><p id="e09b" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在本节中，我们将为 Steam 游戏创建一个推荐系统。我们将使用两个数据集:</p><ul class=""><li id="a660" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><a class="ae ky" href="https://www.kaggle.com/tamber/steam-video-games/version/1" rel="noopener ugc nofollow" target="_blank"> Steam 200k </a>，包含关于超过 200k 用户-游戏交互的每个游戏游戏时间的信息；</li><li id="95ed" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" href="https://www.kaggle.com/trolukovich/steam-games-complete-dataset" rel="noopener ugc nofollow" target="_blank"> Steam games 完整数据集</a>，包含超过 40k 款不同 Steam 游戏的各种信息，包括标题和描述</li></ul><p id="4e1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注</strong>:此处报告的代码并不详尽，应作为参考。您可以在此访问包含所有代码<a class="ae ky" href="https://github.com/RussoMarioDamiano/Collaborative-Topic-Modeling" rel="noopener ugc nofollow" target="_blank">的完整功能笔记本。</a></p><h2 id="f911" class="of ml it bd mm og oh dn mq oi oj dp mu li ok ol mw lm om on my lq oo op na oq bi translated">预处理</h2><p id="914a" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">首先，我们从打开 Steam games 完整数据集开始，只保留每个游戏的名称和描述。我们也放弃那些没有文字描述的游戏。</p><pre class="kj kk kl km gt or os ot ou aw ov bi"><span id="c903" class="of ml it os b gy ow ox l oy oz">games = pd.read_csv("steam_games.csv")</span><span id="adcb" class="of ml it os b gy pa ox l oy oz"># keep only the columns of interest<br/>games = games.loc[:, ["name", "game_description"]]</span><span id="d1b7" class="of ml it os b gy pa ox l oy oz"># Drop NaN game descriptions and names<br/>games = games[~games.game_description.isna()]<br/>games = games[~games.name.isna()]</span><span id="710a" class="of ml it os b gy pa ox l oy oz"># Drop the introductory " About This Game " text<br/>games.game_description = games.game_description.apply(lambda x: x.replace(" About This Game ", ""))</span><span id="26d5" class="of ml it os b gy pa ox l oy oz"># drop single-space descriptions<br/>games = games[games.game_description != " "]</span><span id="cc40" class="of ml it os b gy pa ox l oy oz">games.head(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/8e8adfbeeec329f5dec866fdb3c9e820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rQ6XJRTWtq_PuDdVOLaaWg.png"/></div></div></figure><p id="1626" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们打开 Steam 200k 数据集。我们将尝试预测的评分变量是<strong class="lb iu">以小时计的总播放时间</strong>，它将被调整到 0 到 1 之间。</p><pre class="kj kk kl km gt or os ot ou aw ov bi"><span id="e55e" class="of ml it os b gy ow ox l oy oz">ratings = pd.read_csv("steam-200k.csv", header = None)</span><span id="e575" class="of ml it os b gy pa ox l oy oz"># drop last empty column<br/>ratings.drop(4, axis = 1, inplace=True)</span><span id="9418" class="of ml it os b gy pa ox l oy oz"># rename columns<br/>ratings.columns = ["UserID", "Title", "Action", "Value"]</span><span id="e83d" class="of ml it os b gy pa ox l oy oz"># keep only "play" variables<br/>ratings = ratings[ratings.Action != "purchase"]<br/># and drop the "Action" column, as now it is all "play"s<br/>ratings.drop("Action", axis = 1, inplace = True)</span><span id="ef29" class="of ml it os b gy pa ox l oy oz"># to ease computations, constrain PlayTime to be between 0 and 1<br/>pt = ratings.Value<br/>pt_scaled = (pt - pt.min()) / (pt.max() - pt.min())<br/>ratings.Value = pt_scaled</span><span id="5e04" class="of ml it os b gy pa ox l oy oz">ratings.head(3)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/ed2ddf87817e4822868fd499506eb3fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1MIjSTUS07h5-JwRhjFoDA.png"/></div></div></figure><p id="4361" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一点上，我们需要找到既有稀疏的观察评分向量又有描述的游戏。我们小写所有标题，并删除特殊字符[！,.-"?:]以增加标题之间的兼容性。为了避免不匹配，我们也放弃了所有非唯一标题的游戏。我们以 1982 年的标题结束，我们有评级和描述。</p><p id="2771" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在转向通过组合我们预处理的两个数据集来创建<em class="lv"> (U X I) </em>评级矩阵。</p><pre class="kj kk kl km gt or os ot ou aw ov bi"><span id="50b0" class="of ml it os b gy ow ox l oy oz">R = pd.pivot_table(data=ratings, values = ["Value"], index=["UserID"], columns=["Title"])</span><span id="d6d4" class="of ml it os b gy pa ox l oy oz"># remove the level on top of game names called "Value"<br/>R.columns = R.columns.droplevel()</span><span id="60ee" class="of ml it os b gy pa ox l oy oz"># remove leftover columns name from pivot operation<br/>R.columns.name = ""</span><span id="f151" class="of ml it os b gy pa ox l oy oz"># lastly, fill in the NaNs with 0's<br/>R.fillna(0, inplace=True)</span><span id="3d21" class="of ml it os b gy pa ox l oy oz">R.head(3)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/766229d0caf19be582188a87dda1839f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0dNfXRJl5WrOcBP7J-ZAuw.png"/></div></div></figure><p id="c09d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这个矩阵中，我们给出了数据集中第 1105 款游戏<strong class="lb iu">“极品飞车卧底”</strong>的收视率向量。我们将用它来测试我们矩阵外预测能力的有效性。</p><p id="70bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到 1981 年，我们最终的收视率矩阵将为 10058 个用户，只有 0.23%的收视率被观察到。</p><h2 id="0794" class="of ml it bd mm og oh dn mq oi oj dp mu li ok ol mw lm om on my lq oo op na oq bi translated">基于 LDA 的主题建模</h2><p id="68ad" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在训练我们的 CTM 模型之前，我们需要通过训练 LDA 模型来提取每个游戏描述中的主题及其比例。我们要做的第一件事是对游戏描述进行词汇化，以减少词汇中的差异并提高 LDA 估计。</p><pre class="kj kk kl km gt or os ot ou aw ov bi"><span id="dbe9" class="of ml it os b gy ow ox l oy oz">nlp = spacy.load("en")</span><span id="0c4f" class="of ml it os b gy pa ox l oy oz"># lemmatize game descriptions<br/>games["lemmas"] = [[[token.lemma_ if token.lemma_ != "-PRON-" else token.text.lower() for token in sentence if token.pos_ in {"NOUN", "VERB", "ADJ", "ADV", "X"}] for sentence in nlp(speech).sents] for speech in games.game_description]</span></pre><p id="aeaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们训练我们的 LDA 模型，找出所有游戏描述中的<em class="lv"> K </em> =15 个主题，并确定每个主题在每个描述中出现的百分比。</p><pre class="kj kk kl km gt or os ot ou aw ov bi"><span id="d8a1" class="of ml it os b gy ow ox l oy oz">## Train LDA model ##<br/>ldacorpus = [dictionary.doc2bow(text) for text in instances]<br/>tfidfmodel = TfidfModel(ldacorpus)<br/>model_corpus = tfidfmodel[ldacorpus]</span><span id="05bf" class="of ml it os b gy pa ox l oy oz">num_topics = 15<br/>num_passes = 30<br/>chunk_size = len(model_corpus) * num_passes/200</span><span id="cb30" class="of ml it os b gy pa ox l oy oz">model = LdaMulticore(num_topics=num_topics,<br/>                    corpus=model_corpus,<br/>                    id2word=dictionary,<br/>                    workers=multiprocessing.cpu_count()-1,<br/>                    chunksize=chunk_size,<br/>                    passes=num_passes,<br/>                    alpha=0.1)<br/>## ##</span><span id="565d" class="of ml it os b gy pa ox l oy oz"><br/>## obtain the matrix of topic proportions per document ##<br/>all_topics = model.get_document_topics(model_corpus, per_word_topics=True, minimum_probability=0.0)</span><span id="d309" class="of ml it os b gy pa ox l oy oz">corpus_topics = []</span><span id="09db" class="of ml it os b gy pa ox l oy oz">for doc_topics, word_topics, phi_values in all_topics:<br/>    corpus_topics.append([topic[1] for topic in doc_topics])<br/>    <br/>corpus_topics = np.array(corpus_topics)</span><span id="093f" class="of ml it os b gy pa ox l oy oz">theta = corpus_topics.copy().T<br/>## ##</span><span id="0343" class="of ml it os b gy pa ox l oy oz">## remove the heldout game from the theta matrix ##<br/>thet = pd.DataFrame(theta)<br/>heldout_topics = thet.iloc[:, heldout_idx]<br/>thet.drop(heldout_idx, axis = 1, inplace=True)<br/>theta = thet.values<br/>## ##</span></pre><p id="cd17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> theta </em>是<em class="lv"> (K X I) </em>矩阵，它告诉我们<em class="lv"> K </em> =15 个主题中的每一个出现在每一个<em class="lv"> I </em>游戏中的比例。</p><p id="93be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们有了评级矩阵和每项主题比例矩阵，我们就可以建立我们的 CTM 模型。</p><h2 id="75cc" class="of ml it bd mm og oh dn mq oi oj dp mu li ok ol mw lm om on my lq oo op na oq bi translated"><strong class="ak">培训我们的 CTM 模型</strong></h2><p id="8ba4" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们首先将我们的评级矩阵分成 X_train 和 X_val。</p><pre class="kj kk kl km gt or os ot ou aw ov bi"><span id="6b2c" class="of ml it os b gy ow ox l oy oz"># train - test split</span><span id="9077" class="of ml it os b gy pa ox l oy oz">def train_test_split(ratings, percs = [0.8, 0.2]):<br/>    <br/>    validation = np.zeros(ratings.shape)<br/>    train = ratings.copy()<br/>    <br/>    for user in np.arange(ratings.shape[0]):<br/>        val_ratings = np.random.choice(ratings[user,:].nonzero()[0],<br/>                                        size = round(len(ratings[user,:].nonzero()[0]) * percs[1]),<br/>                                        replace=False<br/>                                        )<br/>        train[user, val_ratings] = 0<br/>        validation[user, val_ratings] = ratings[user, val_ratings]<br/>    <br/>    return train, validation</span><span id="ffe5" class="of ml it os b gy pa ox l oy oz">X_train, X_val = train_test_split(R.values)</span></pre><p id="fea3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还为我们的预测评级定义了一个 MSE 函数。</p><pre class="kj kk kl km gt or os ot ou aw ov bi"><span id="a53b" class="of ml it os b gy ow ox l oy oz">from sklearn.metrics import mean_squared_error</span><span id="f863" class="of ml it os b gy pa ox l oy oz">def mse(prediction, ground_truth):<br/>    prediction = prediction[ground_truth.nonzero()].flatten()<br/>    ground_truth = ground_truth[ground_truth.nonzero()].flatten()<br/>    return mean_squared_error(prediction, ground_truth)</span></pre><p id="b865" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们建立我们的 CTM 模型:</p><pre class="kj kk kl km gt or os ot ou aw ov bi"><span id="a572" class="of ml it os b gy ow ox l oy oz">from tqdm import trange<br/>import sys</span><span id="5a30" class="of ml it os b gy pa ox l oy oz">class CTR():<br/>    """<br/>    Collaborative Topic Regression Model as developed by Wang and Blei (2012).<br/>    Leverages topic proportions obtained from LDA model to improve predictions<br/>    and allow for out-of-matrix predictions.<br/>    <br/>    Parameters:<br/>        - sigma2: expected variance of ratings <br/>                  (variance of the ratings Normal prior)<br/>        - sigma2_P: expected variance of the elements of the<br/>                    preference vector<br/>        - sigma2_Q: expected variance of the elements of the<br/>                    quality vector<br/>    """<br/>    def __init__(self, epochs=200, learning_rate=0.001, sigma2=10, sigma2_P=10, sigma2_Q=10):<br/>        self.epochs = epochs<br/>        self.learning_rate = learning_rate<br/>        self.sigma2 = sigma2<br/>        self.sigma2_P = sigma2_P<br/>        self.sigma2_Q = sigma2_Q<br/>    <br/>    <br/>    def fit(self, theta, X_train, X_val):<br/>        """<br/>        Fit a CTR model.<br/>        <br/>        Parameters:<br/>            - theta: (K X I) matrix of topic proportions obtained via LDA.<br/>            - X_train: (U X I) ratings matrix to train the model on.<br/>            - X_test: (U X I) ratings matrix to validate the model on.<br/>        """<br/>        <br/>        K = theta.shape[0]<br/>        U, I = X_train.shape<br/>        <br/>        #initialize P and Q matrices.<br/>        # P is initialized randomly<br/>        self.P = np.random.randint(0, 10) * np.random.rand(K, U)<br/>        # Q is initialized to be equal to theta<br/>        self.Q = theta.copy()<br/>        <br/>        self.train_error = []<br/>        self.val_error = []<br/>        <br/>        # obtain the pairs of (u, i) indices for which we observe a rating<br/>        users, items = X_train.nonzero()<br/>        <br/>        <br/>        # begin training<br/>        for iteration in trange(self.epochs, file=sys.stdout, desc='CTR'):<br/>            for u, i in zip(users, items):<br/>                error = X_train[u, i] - np.dot(self.P[:, u].T, self.Q[:, i])</span><span id="5179" class="of ml it os b gy pa ox l oy oz"># we are MAXIMIZING the likelihood via gradient ascent<br/>                self.P[:, u] += self.learning_rate * (-self.P[:, u]/self.sigma2_P + (self.P[:, u] * error)/self.sigma2)<br/>                self.Q[:, i] += self.learning_rate * (-(self.Q[:, i] - theta[:, i])/self.sigma2_Q + (self.Q[:, i] * error)/self.sigma2)</span><span id="9f7f" class="of ml it os b gy pa ox l oy oz">self.train_error.append(mse(np.dot(self.P.T, self.Q), X_train))<br/>            self.val_error.append(mse(np.dot(self.P.T, self.Q), X_val))<br/>    <br/>    <br/>    <br/>    def predict_ratings(self):<br/>        """<br/>        Returns the matrix of predicted ratings.<br/>        """<br/>        return np.dot(self.P.T, self.Q)<br/>    <br/>    <br/>    <br/>    def predict_out_of_matrix(self, topics):<br/>        """<br/>        Returns the (U X 1) vector of predicted ratings <br/>        for an unrated item, using the item's topic proportions.<br/>        <br/>        Parameters:<br/>            - topics: (K X 1) array of topic proportions<br/>                      for the unrated item.<br/>        """<br/>        return np.dot(self.P.T, topics)</span></pre><p id="97df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并训练模型:</p><pre class="kj kk kl km gt or os ot ou aw ov bi"><span id="fd97" class="of ml it os b gy ow ox l oy oz">ctr = ctr = CTR(sigma2_P=5, sigma2_Q=5, sigma2=1)<br/>ctr.fit(theta, X_train, X_val)</span></pre><p id="6f9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练结束时，我们的<code class="fe pe pf pg os b">ctr</code>对象将已经学习了潜在矩阵 P 和 Q，并将能够通过它们的点积来预测评分矩阵中缺失的值。</p><p id="d892" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是 100 个时期的整个训练的 MSE 性能，在最后一个训练时期记录的验证集上有 0.126 MSE。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/b305f4c06bdd7f7891f27628cc0c7487.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQyrD_YPq72BIz6L91F3Pw.png"/></div></div></figure><p id="1106" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对单个保持观察的性能具有 0.35 的 MSE。当然，这没有矩阵内预测记录的高，但如果我们考虑到我们推荐的是矩阵外的项目，这是相当令人印象深刻的。</p><p id="a87e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们对我们的结果感到满意，但是在这种情况下，协作主题回归与传统的概率矩阵分解相比表现如何呢？显然很好。标准因式分解方法达到了 0.32 MSE，而 CTM 达到了 0.126。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/4841b31ed3cac23affa33cd11162daf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JFeo0pNagvYJ0HswL2ApcA.png"/></div></div></figure><h2 id="1132" class="of ml it bd mm og oh dn mq oi oj dp mu li ok ol mw lm om on my lq oo op na oq bi translated">附加功能</h2><p id="dffa" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">CTM 特有的潜在主题结构增加了进一步的可解释性，以做出推论。首先，CTM 相对于传统 PMF 的额外好处是，我们能够标记学习到的潜在维度，看到每个主题在每个项目中有多突出。注意:这些不是从 LDA 学习的主题比例，而是通过 CTM 模型学习的主题相关性！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/685955cc8b6c330d6523afcfde2329bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qu9nIfCwI0TsXyP9GqaOHA.png"/></div></div></figure><p id="4031" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以通过简单的<em class="lv"> P </em>矩阵的水平求和来研究用户偏好在<em class="lv"> K </em>潜在维度上的分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/d6aa82e50babe9462624e5c82c31a418.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*MciuUNF5sARby_QdouGh1Q.png"/></div></figure><p id="6bc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以对项目而不是用户进行同样的操作，以查看类型如何在我们的项目中分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/fe54f0724a431425dd0b24556f626db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*-v-l8lc4Rzil-c0Ez2c4tA.png"/></div></figure></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="5385" class="mk ml it bd mm mn oa mp mq mr ob mt mu jz oc ka mw kc od kd my kf oe kg na nb bi translated">结论</h1><p id="62d1" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在本文中，我们提出了协同主题建模，如王和 Blei (2011)所述，这是一种改进的基于文本项目的矩阵分解推荐系统。该模型的性能通常优于传统的矩阵分解方法，并具有额外的优势，如<strong class="lb iu">矩阵外预测</strong>和潜在维度可解释性。</p><p id="5335" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一般来说，校准 CTM 模型的最大挑战是找到超参数的最佳组合，使我们的数据表现最佳。事实上，我们需要调整:</p><ul class=""><li id="971c" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><em class="lv"> alpha </em>，来自 LDA 模型。</li><li id="07ea" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv">话题数量 K </em></li><li id="3f7d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv">学习率</em></li><li id="7de3" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv"> sigma2，sigma2_P </em>，<em class="lv"> sigma2_Q，</em>分别关于 R，P，Q 的先验分布的超参数。</li></ul><p id="9650" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二，由于模型的随机性质，在 LDA 阶段和 CTM 训练阶段，在单独的训练中模型的性能可能有很多可变性。</p><p id="906d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管不能保证比 PMF 有所改进，但 CTM 确实是一种在处理基于文本的文档推荐任务时应该考虑的方法，因为它通常具有更高的性能和矩阵外预测能力。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="367b" class="mk ml it bd mm mn oa mp mq mr ob mt mu jz oc ka mw kc od kd my kf oe kg na nb bi translated">参考</h1><p id="317e" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">[1] Mnih、Andriy 和 Russ R. Salakhutdinov。"概率矩阵分解."<em class="lv">神经信息处理系统的进展</em>。2008.</p><p id="47ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]布莱、戴维·m、安德鲁·Ng 和迈克尔·乔丹。"潜在的狄利克雷分配."<em class="lv">机器学习研究杂志</em> 3。一月(2003):993–1022。</p><p id="ac33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]王、钟和戴维·布雷。"用于推荐科学文章的协作主题建模."第 17 届 ACM SIGKDD 知识发现和数据挖掘国际会议论文集<em class="lv">。2011.</em></p></div></div>    
</body>
</html>