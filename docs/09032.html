<html>
<head>
<title>LSTM Gradients</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM 梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lstm-gradients-b3996e6a0296?source=collection_archive---------9-----------------------#2020-06-29">https://towardsdatascience.com/lstm-gradients-b3996e6a0296?source=collection_archive---------9-----------------------#2020-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="abc1" class="pw-subtitle-paragraph jr is it bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">LSTM 单元梯度的详细数学推导。</h2></div></div><div class="ab cl kj kk hx kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="im in io ip iq"><p id="323d" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">LSTM 或长短期记忆是复杂和最先进的神经网络结构的非常重要的构建块。这篇文章背后的主要思想是解释背后的数学。为了对 LSTM 有一个初步的了解，我推荐下面的博客。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/c8606544577e15c64b81fa3d691418da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*FPTmBD8GY9ZvkdsUBYXC9Q.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated"><a class="ae ly" href="https://commons.wikimedia.org/wiki/File:LSTM.png" rel="noopener ugc nofollow" target="_blank">学分</a></p></figure><div class="lz ma gp gr mb mc"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd iu gy z fp mh fr fs mi fu fw is bi translated">了解 LSTM 网络</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">2015 年 8 月 27 日发布人类不是每秒钟都从零开始思考。当你读这篇文章时，你…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">colah.github.io</p></div></div></div></a></div><h1 id="b5d2" class="ml mm it bd mn mo mp mq mr ms mt mu mv ka mw kb mx kd my ke mz kg na kh nb nc bi translated">内容:</h1><h1 id="ff3b" class="ml mm it bd mn mo mp mq mr ms mt mu mv ka mw kb mx kd my ke mz kg na kh nb nc bi translated">概念</h1><ul class=""><li id="1822" class="nd ne it ks b kt nf kw ng kz nh ld ni lh nj ll nk nl nm nn bi translated">介绍</li><li id="0b2c" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">说明</li><li id="1080" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">推导先决条件</li></ul><h1 id="cc07" class="ml mm it bd mn mo mp mq mr ms mt mu mv ka mw kb mx kd my ke mz kg na kh nb nc bi translated"><strong class="ak"> B —推导</strong></h1><ul class=""><li id="c3bb" class="nd ne it ks b kt nf kw ng kz nh ld ni lh nj ll nk nl nm nn bi translated">LSTM 的产量</li><li id="f092" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">隐藏状态</li><li id="495c" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">输出门</li><li id="4624" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">细胞状态</li><li id="c3f0" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">输入门</li><li id="6731" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">忘记大门</li><li id="35b8" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">对 LSTM 的投入</li><li id="91cd" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">权重和偏差</li></ul><h1 id="68a3" class="ml mm it bd mn mo mp mq mr ms mt mu mv ka mw kb mx kd my ke mz kg na kh nb nc bi translated">C —随时间反向传播</h1><h1 id="e133" class="ml mm it bd mn mo mp mq mr ms mt mu mv ka mw kb mx kd my ke mz kg na kh nb nc bi translated">D —结论</h1><h1 id="a4e9" class="ml mm it bd mn mo mp mq mr ms mt mu mv ka mw kb mx kd my ke mz kg na kh nb nc bi translated">概念</h1><h2 id="f163" class="nt mm it bd mn nu nv dn mr nw nx dp mv kz ny nz mx ld oa ob mz lh oc od nb oe bi translated">介绍</h2><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi of"><img src="../Images/e1e17f49b49485fe1f1e04fff8c7030f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xzCim1w_4LdEVqbq_2fk6Q.jpeg"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">图 1 : LSTM 细胞</p></figure><p id="5509" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">上图是单个 LSTM 细胞的示意图。我知道这看起来很吓人😰，但是我们将一个接一个地浏览它，希望在文章结束时，它会变得非常清楚。</p><h2 id="b619" class="nt mm it bd mn nu nv dn mr nw nx dp mv kz ny nz mx ld oa ob mz lh oc od nb oe bi translated">说明</h2><p id="c972" class="pw-post-body-paragraph kq kr it ks b kt nf jv kv kw ng jy ky kz ok lb lc ld ol lf lg lh om lj lk ll im bi translated">基本上，一个 LSTM 细胞有 4 个不同的组成部分。忘记门、输入门、输出门和单元状态。我们将首先简要讨论这些部分的用法(详细解释请参考上面的博客)，然后深入研究其中的数学部分。</p><p id="d294" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> <em class="on">忘门</em> </strong></p><p id="1e07" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">顾名思义，这个部分负责决定最后一步要丢弃或保留哪些信息。这是由第一个乙状结肠层完成的。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi of"><img src="../Images/7191f6dcb642bca7ed0f2f8f85c8d81a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B47Q-N_SItx0E-ZWnJydrg.jpeg"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">图 2:蓝色标记的忘记门</p></figure><p id="b636" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">基于 h_t-1(先前的隐藏状态)和 x_t(在时间步长 t 的当前输入)，这为单元状态 C_t-1 中的每个值决定 0 和 1 之间的值。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/efe29128dae789bc7545327ba4802474.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*LXd013Hu3jUxyyS70pS9AQ.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">图 3:忘记门和以前的细胞状态</p></figure><p id="9ea2" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">对于所有的 1，所有的信息都保持原样，对于所有的 0，所有的信息都被丢弃，而对于其他值，它决定了有多少来自前一状态的信息将被携带到下一状态。</p><p id="4ff8" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> <em class="on">输入门</em> </strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi op"><img src="../Images/49bfb18ecbbdf29eaf4da4255d0bca2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4SbHINJMts1kcOKjEk-Iw.jpeg"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">图 4:用蓝色标记的输入门</p></figure><p id="d061" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Christopher Olah 对输入门发生的事情有一个漂亮的解释。引用他的博客:</p><blockquote class="oq or os"><p id="0f2e" class="kq kr on ks b kt ku jv kv kw kx jy ky ot la lb lc ou le lf lg ov li lj lk ll im bi translated"><a class="ae ly" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">下一步是决定我们要在单元格状态中存储什么新信息。这有两个部分。首先，一个称为“输入门层”的 sigmoid 层决定我们要更新哪些值。接下来，tanh 层创建一个新的候选值向量，C~t，可以添加到状态中。在下一步中，我们将结合这两者来创建状态更新。</a></p></blockquote><p id="b101" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，这两个值，即 i_t 和 c~t 结合起来决定将什么新的输入馈送到单元状态。</p><p id="bd94" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> <em class="on">单元格状态</em> </strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi op"><img src="../Images/ba97f995bc6a72a47eb2a0ac35a552a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uRIJyh7Jaj4HS9hgDVogtA.jpeg"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">图 5:蓝色标记的电池状态</p></figure><p id="a099" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">细胞状态作为 LSTM 的记忆。这是他们在处理更长的输入序列时比香草 RNN 表现更好的地方。在每个时间步长，先前的单元状态(C_t-1)与遗忘门结合，以决定哪些信息将被结转，该信息又与输入门(i_t 和 c~t)结合，以形成新的单元状态或单元的新存储器。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi ow"><img src="../Images/4d6f4d0cd8c85945278fb3d1b9407b55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x_pELQ2zuIq2IssM12YTPw.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">图 6:新的细胞状态方程</p></figure><p id="adb3" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> <em class="on">输出门</em> </strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi op"><img src="../Images/86374c6cbeb9333759e78a4480daba0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IPZ4Su7E0S6lJJt1gbsuZQ.jpeg"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">图 7:标有蓝色的输出门</p></figure><p id="12f0" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最后，LSTM 细胞必须给出一些输出。从上面获得的单元状态通过一个称为 tanh 的双曲线函数传递，以便在-1 和 1 之间过滤单元状态值。详情进入不同的激活功能，<a class="ae ly" rel="noopener" target="_blank" href="/activation-functions-neural-networks-1cbd9f8d91d6">这个</a>是个不错的博客。</p><p id="38d4" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，我希望 LSTM 单元的基本单元结构是清楚的，我们可以继续推导我们将在实现中使用的方程。</p><h2 id="d357" class="nt mm it bd mn nu nv dn mr nw nx dp mv kz ny nz mx ld oa ob mz lh oc od nb oe bi translated">推导先决条件</h2><ol class=""><li id="ed82" class="nd ne it ks b kt nf kw ng kz nh ld ni lh nj ll ox nl nm nn bi translated"><strong class="ks iu">要求</strong>:推导方程的核心概念是基于反向传播、成本函数和损失。如果你不熟悉这些，这几个链接将有助于更好地理解。本文还假设对高中微积分有基本的了解(计算导数和规则)。</li></ol><div class="lz ma gp gr mb mc"><a rel="noopener follow" target="_blank" href="/understanding-backpropagation-algorithm-7bb3aa2f95fd"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd iu gy z fp mh fr fs mi fu fw is bi translated">理解反向传播算法</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">了解神经网络最重要组成部分的具体细节</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd ls mc"/></div></div></a></div><div class="lz ma gp gr mb mc"><a rel="noopener follow" target="_blank" href="/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd iu gy z fp mh fr fs mi fu fw is bi translated">寻找神经网络的成本函数</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">一步一步:神经网络背后的数学</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="pe l pa pb pc oy pd ls mc"/></div></div></a></div><p id="5c1b" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">2.<strong class="ks iu">变量</strong>:对于每个门，我们有一组权重和偏差，表示为:</p><ul class=""><li id="47eb" class="nd ne it ks b kt ku kw kx kz pf ld pg lh ph ll nk nl nm nn bi translated">W_f，b_f-&gt;忘记门重和偏差</li><li id="190a" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">W_i，b_i-&gt;输入门权重和偏置</li><li id="71b3" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">候选小区状态权重和偏差</li><li id="83cd" class="nd ne it ks b kt no kw np kz nq ld nr lh ns ll nk nl nm nn bi translated">W_o，b_o-&gt;输出门权重和偏置</li></ul><p id="2a3e" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">W_v，b_v -&gt;与 Softmax 层关联的权重和偏差。</p><p id="5297" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">f_t，i_t，c_tilede_t，o_t -&gt;激活函数的输出</p><p id="cbb3" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">a_f，a_i，a_c，a_o -&gt;激活函数的输入</p><p id="9646" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">j 是成本函数，我们将根据它来计算导数。注意(下划线(_)后的字符是下标)</p><p id="e3dd" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">3.<strong class="ks iu">正向推进方程式</strong>:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pi"><img src="../Images/a7811b87a9050ec84dd3a29c5f14e8b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bDJPAZZWA9-RTTyuKedWug.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">图 8:门方程</p></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pj"><img src="../Images/2cbc3efa227ef68c378d3ee835aa8f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h330Kvu--nUdSY9_4O9aTQ.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">图 9:电池状态和输出方程</p></figure><p id="3231" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">4.<strong class="ks iu">计算过程</strong>:让我们以忘记门为例来说明导数的计算。我们需要遵循下图中红色箭头的路径。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pk"><img src="../Images/980e77579c86796a19f12a5f3875e7d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iod84m1IkkIGhIfB8LarnQ.png"/></div></div></figure><p id="daa3" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">所以我们画出一条从 f_t 到成本函数 J 的路径，即</p><p id="56df" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">f_t →C_t →h_t →J。</p><p id="87f2" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">反向传播恰好发生在同一步骤中，但方向相反</p><p id="1164" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">f_t ←C_t ←h_t ←J。</p><p id="166d" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">j 相对于 h_t 微分，h_t 相对于 _C_t 微分，C_t 相对于 f_t 微分。</p><p id="89e2" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">因此，如果我们在这里观察，J 和 h_t 是单元格的最后一步，如果我们计算 dJ/dh_t，那么它可以用于类似 dJ/dC_t 的计算，因为:</p><p id="a3a9" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dC_t = dJ/dh_t * dh_t/dC_t ( <a class="ae ly" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review" rel="noopener ugc nofollow" target="_blank">链式法则</a>)</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pk"><img src="../Images/980e77579c86796a19f12a5f3875e7d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iod84m1IkkIGhIfB8LarnQ.png"/></div></div></figure><p id="1e02" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">类似地，将计算第 1 点中提到的所有变量的导数。</p><p id="bdaf" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">既然我们已经准备好了变量，也清楚了正向推进方程，是时候通过反向传播来推导导数了。我们将从输出方程开始，因为我们看到在其他方程中使用了相同的导数。这就是链式法则的用武之地。所以现在就开始吧。</p><h1 id="2258" class="ml mm it bd mn mo mp mq mr ms mt mu mv ka mw kb mx kd my ke mz kg na kh nb nc bi translated">衍生物</h1><h2 id="15a9" class="nt mm it bd mn nu nv dn mr nw nx dp mv kz ny nz mx ld oa ob mz lh oc od nb oe bi translated"><strong class="ak"><em class="jq">lstm</em>T5】的输出</strong></h2><p id="08a1" class="pw-post-body-paragraph kq kr it ks b kt nf jv kv kw ng jy ky kz ok lb lc ld ol lf lg lh om lj lk ll im bi translated">输出有两个我们需要计算的值。</p><ol class=""><li id="39ae" class="nd ne it ks b kt ku kw kx kz pf ld pg lh ph ll ox nl nm nn bi translated">Softmax:对于使用 Softmax 的交叉熵损失的导数，我们将直接使用最终方程。</li></ol><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pl"><img src="../Images/6287d7ffe86c4884543aa8c8f44bf7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sWrC6poVznPdlzbgwau74g.jpeg"/></div></div></figure><p id="e97c" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">详细的推导过程如下:</p><div class="lz ma gp gr mb mc"><a href="https://sefiks.com/2017/12/17/a-gentle-introduction-to-cross-entropy-loss-function/#:~:text=Cross%20Entropy%20Error%20Function&amp;text=If%20loss%20function%20were%20MSE,error%20function%20is%20cross%20entropy.&amp;text=c%20refers%20to%20one%20hot,refers%20to%20softmax%20applied%20probabilities." rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd iu gy z fp mh fr fs mi fu fw is bi translated">交叉熵损失函数的温和介绍</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">神经网络在多类分类问题中产生多个输出。但是，他们没有能力…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">sefiks.com</p></div></div><div class="oy l"><div class="pm l pa pb pc oy pd ls mc"/></div></div></a></div><h2 id="39e6" class="nt mm it bd mn nu nv dn mr nw nx dp mv kz ny nz mx ld oa ob mz lh oc od nb oe bi translated">隐藏状态</h2><p id="e114" class="pw-post-body-paragraph kq kr it ks b kt nf jv kv kw ng jy ky kz ok lb lc ld ol lf lg lh om lj lk ll im bi translated">我们有 h_t 这样的隐藏态，h_t 对 w . r . t . j 求导，根据链式法则，推导可以在下图中看到。我们使用图 9 等式 7 中提到的 V_t 值，即:</p><p id="4268" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">v t = W v . h t+b v</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pn"><img src="../Images/8835484f50ac566fbe24185294778a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E9ViLoFoh-QfQl2tQkGmeg.jpeg"/></div></div></figure><h2 id="a3f8" class="nt mm it bd mn nu nv dn mr nw nx dp mv kz ny nz mx ld oa ob mz lh oc od nb oe bi translated">输出门</h2><p id="fdd2" class="pw-post-body-paragraph kq kr it ks b kt nf jv kv kw ng jy ky kz ok lb lc ld ol lf lg lh om lj lk ll im bi translated">相关变量:a_o 和 o_t。</p><p id="bb80" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> o_t </strong>:下图显示了 o_t 和 J 之间的路径。根据箭头，微分的完整方程如下:</p><p id="813f" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dV_t * dV_t/dh_t * dh_t/dO_t</p><p id="22d2" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dV_t * dV_t/dh_t 可以写成 dJ/dh_t(我们从隐藏状态得到这个值)。</p><p id="4c2a" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">h_t 的值= o_t * tanh(c_t) -&gt;图 9 等式 6。<strong class="ks iu"> <em class="on">所以我们只需要对 h_t w.r.t o_t. </em> </strong>进行区分，区分如下:-</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi po"><img src="../Images/5939888fc8b4687e160f7701792956db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EiHnlDMKi5MJ3w5q8ao17A.jpeg"/></div></div></figure><p id="e95a" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> a_o </strong>:同样，显示 a_o 和 J 之间的路径。根据箭头，微分的完整方程如下:</p><p id="c2be" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dV _ t * dV _ t/DH _ t * DH _ t/dO _ t * dO _ t/da _ o</p><p id="58af" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dV_t * dV_t/dh_t * dh_t/dO_t 可以写成 dJ/dO_t(上面 O_t 我们有这个值)。</p><p id="d440" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">o_t = sigmoid (a_o) -&gt;图 8 等式 4。<strong class="ks iu"> <em class="on">所以我们只需要区分 o _ T w . r . T a _ o .</em></strong><em class="on">T</em>他的区分将为:-</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pp"><img src="../Images/ffe7faca7c13f798b3caa2875538ba50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N3pWD47cVgUmF_COH3-WrA.jpeg"/></div></div></figure><h2 id="0fc9" class="nt mm it bd mn nu nv dn mr nw nx dp mv kz ny nz mx ld oa ob mz lh oc od nb oe bi translated">细胞状态</h2><p id="b2ea" class="pw-post-body-paragraph kq kr it ks b kt nf jv kv kw ng jy ky kz ok lb lc ld ol lf lg lh om lj lk ll im bi translated">C_t 是单元的单元状态。除此之外，我们还在这里处理候选单元状态 a_c 和 c~_t。</p><p id="5595" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">C_t:</strong>C _ t 的推导非常简单，因为从 C _ t 到 J 的路径非常简单。C_t → h_t → V_t → J .由于我们已经有了 dJ/dh_t，所以直接微分 h_t w.r.t C_t。</p><p id="db21" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">h_t = o_t * tanh(c_t) -&gt;图 9 等式 6。<strong class="ks iu"> <em class="on">所以我们只需要区分 h_t w.r.t C_t. </em> </strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pq"><img src="../Images/83f850ef2df1fdace8a8cabc248fb420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P-yxD6pfD6OORZy7OfdhQw.jpeg"/></div></div></figure><p id="d7c9" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">注意:单元状态 clubbed 会在文末解释。</p><p id="c53b" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> c~_t </strong>:下图显示了 c~_t 和 J 之间的路径。根据箭头，微分的完整方程如下:</p><p id="63d9" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dh_t * dh_t/dC_t * dC_t/dc~_t</p><p id="230e" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dh_t * dh_t/dC_t 可以写成 dJ/dC_t(上面我们有这个值)。</p><p id="d7dd" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">C_t 的值如图 9 等式 5 所示(下图第 3 行最后一个 c_t 中缺少波浪号(~)符号-&gt;书写错误)。<strong class="ks iu"> <em class="on">所以我们只需要区分 C_t w.r.t c~_t. </em> </strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pr"><img src="../Images/e5be148d04f0268e55faba7619aba5dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WwN7P-ZE__Y_HCLqmGpMWQ.jpeg"/></div></div></figure><p id="5783" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> a_c : </strong>下图显示了 a_c 和 J 之间的路径。根据箭头，微分的完整方程如下:</p><p id="4f8c" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/DH _ t * DH _ t/dC _ t * dC _ t/dC ~ _ t * dC ~ _ t/da _ c</p><p id="aa8a" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dh_t * dh_t/dC_t * dC_t/dc~_t 可以写成 dJ/dc~_t(我们从上面有这个值)。</p><p id="77bd" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">c \u t 的值如图 8 等式 3 所示。<strong class="ks iu"> <em class="on">所以我们只需要区分 c~_t w.r.t a_c </em> </strong>。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi ps"><img src="../Images/934e556817349a5cff660ef8125c3831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SLLBIC8OteKZe0ja40rBRg.jpeg"/></div></div></figure><h2 id="b4b6" class="nt mm it bd mn nu nv dn mr nw nx dp mv kz ny nz mx ld oa ob mz lh oc od nb oe bi translated">输入门</h2><p id="c054" class="pw-post-body-paragraph kq kr it ks b kt nf jv kv kw ng jy ky kz ok lb lc ld ol lf lg lh om lj lk ll im bi translated">相关变量:i_t 和 a_i</p><p id="773b" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> i_t </strong>:下图显示了 i_t 和 J 之间的路径。根据箭头，微分的完整方程如下:</p><p id="e473" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dh_t * dh_t/dC_t * dC_t/di_t</p><p id="84ab" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dh_t * dh_t/dC_t 可以写成 dJ/dC_t(我们从 cell state 得到这个值)。<strong class="ks iu">所以我们只需要区分 C_t w.r.t i_t. </strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pt"><img src="../Images/83eddc0f750ac746d92e3e66464f0590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cflZasWkg4mpaX9y6t4pFQ.jpeg"/></div></div></figure><p id="0bce" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">C_t 的值如图 9 等式 5 所示。因此，区别如下:-</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pu"><img src="../Images/006c211812561681f3bb09191f59acb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4FMGMgm3x1zue7IiB0F5gg.jpeg"/></div></div></figure><p id="9bfe" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> a_i : </strong>下图显示了 a_i 和 J 之间的路径。根据箭头，微分的完整方程如下:</p><p id="693e" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/DH _ t * DH _ t/dC _ t * dC _ t/di _ t * di _ t/da _ I</p><p id="145c" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dh_t * dh_t/dC_t * dC_t/di_t 可以写成 dJ/di_t(我们从上面有这个值)。<strong class="ks iu">所以我们只需要区分 i_t w.r.t a_i. </strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pv"><img src="../Images/69493648ce01c9ceb336a62b64f10f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rfDe2Yuym3dVwYMMVal11Q.jpeg"/></div></div></figure><h2 id="98df" class="nt mm it bd mn nu nv dn mr nw nx dp mv kz ny nz mx ld oa ob mz lh oc od nb oe bi translated">忘记大门</h2><p id="aaf2" class="pw-post-body-paragraph kq kr it ks b kt nf jv kv kw ng jy ky kz ok lb lc ld ol lf lg lh om lj lk ll im bi translated">相关变量:f_t 和 a_f</p><p id="f8cb" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> f_t </strong>:下图显示了 f_t 和 J 之间的路径。根据箭头，微分的完整方程如下:</p><p id="cfa9" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dh_t * dh_t/dC_t * dC_t/df_t</p><p id="da32" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dh_t * dh_t/dC_t 可以写成 dJ/dC_t(我们从 cell state 得到这个值)。所以我们只需要区分 C_t w.r.t f_t. </p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pk"><img src="../Images/980e77579c86796a19f12a5f3875e7d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iod84m1IkkIGhIfB8LarnQ.png"/></div></div></figure><p id="4784" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">C_t 的值如图 9 等式 5 所示。因此，区别如下:-</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pw"><img src="../Images/284468c5227d2548835f91f1501e22d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NC-eMhEZ2-B-pQX32FxNcA.jpeg"/></div></div></figure><p id="4cec" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> a_f </strong>:下图显示了 f_t 和 J 之间的路径。根据箭头，微分的完整方程如下:</p><p id="c8d0" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/DH _ t * DH _ t/dC _ t * dC _ t/df _ t * df _ t/da _ t</p><p id="e8a5" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/dh_t * dh_t/dC_t * dC_t/df_t 可以写成 dJ/df_t(我们从上面有这个值)。所以我们只需要区分 f_t w.r.t a_f</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi px"><img src="../Images/40c4aac64ef7e8e674ccf8950507d601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_55vkNprAy0xDZxJ0RRRw.jpeg"/></div></div></figure><h2 id="8fd2" class="nt mm it bd mn nu nv dn mr nw nx dp mv kz ny nz mx ld oa ob mz lh oc od nb oe bi translated">Lstm 的输入</h2><p id="d601" class="pw-post-body-paragraph kq kr it ks b kt nf jv kv kw ng jy ky kz ok lb lc ld ol lf lg lh om lj lk ll im bi translated">有 2 个变量与每个单元的输入相关联，即先前的单元状态 C_t-1 和与当前输入连接的先前的隐藏状态，即</p><p id="d180" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">[h_t-1，x_t] -&gt; Z_t</p><p id="2c35" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> C_t-1 : </strong>这是 Lstm 细胞的记忆。图 5 显示了单元状态。C_t-1 的推导相当简单，因为只涉及 C_t-1 和 C_t。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi py"><img src="../Images/88a381b71dba3109fffdb8f7ab151b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*icwLFOBPzUeRnpbl_s726Q.jpeg"/></div></div></figure><p id="4dcd" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> Z_t </strong>:如下图所示，Z_t 进入 4 条不同的路径，a_f，a_i，a_o，a_c</p><p id="cbc1" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Z_t → a_f → f_t → C_t → h_t → J . -&gt;忘记门</p><p id="779a" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Z_t → a_i→ i_t → C_t → h_t → J . -&gt;输入门</p><p id="b1f5" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Z_t → a_c → c~_t → C_t → h_t → J . -&gt;候选细胞态</p><p id="dacc" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Z_t → a_o → o_t → C_t → h_t → J . -&gt;输出门</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi pz"><img src="../Images/7831b37566d916402b8b122763824371.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dvPvD8PgsgSAaltqbbJ4-g.jpeg"/></div></div></figure><h2 id="4b5f" class="nt mm it bd mn nu nv dn mr nw nx dp mv kz ny nz mx ld oa ob mz lh oc od nb oe bi translated">权重和偏差</h2><p id="c025" class="pw-post-body-paragraph kq kr it ks b kt nf jv kv kw ng jy ky kz ok lb lc ld ol lf lg lh om lj lk ll im bi translated">W 和 b 的推导非常简单。下面的推导是针对 Lstm 的输出门。对于其余的门，对权重和偏差进行类似的处理。</p><div class="ln lo lp lq gt ab cb"><figure class="qa lr qb qc qd qe qf paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><img src="../Images/558f8743aa4e4f1dc61489bf48717d0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*mLCakT4qav15zGNSl5Efew.jpeg"/></div></figure><figure class="qa lr qg qc qd qe qf paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><img src="../Images/fcdc4bd842897c5baba78b914d67c06d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*CgAr46Ltgm28tWBIt3w57g.jpeg"/></div><p class="lu lv gj gh gi lw lx bd b be z dk qh di qi qj translated">输入和遗忘门的权重和偏差</p></figure></div><div class="ab cb"><figure class="qa lr qk qc qd qe qf paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><img src="../Images/9f2c5bc8dde9a210452a938ed389fda0.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*Vj9O1GRWTxnZOp30tEfXjQ.jpeg"/></div></figure><figure class="qa lr ql qc qd qe qf paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><img src="../Images/4310ab8a2e9e846b1d4589b2a32cf02f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*HxmUEYtlwWOlANFt8JDQMA.jpeg"/></div><p class="lu lv gj gh gi lw lx bd b be z dk qm di qn qj translated">输出和输出门的权重和偏置</p></figure></div><p id="27b8" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/d_W_f = dJ/da_f . da_f / d_W_f -&gt;忘记门</p><p id="8d9a" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/d_W_i = dJ/da_i . da_i / d_W_i -&gt;输入门</p><p id="085f" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/d_W_v = dJ/dV_t . dV_t/ d_W_v -&gt;输出</p><p id="ca78" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dJ/d_W_o = dJ/da_o . da_o / d_W_o -&gt;输出门</p><p id="011e" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最后我们完成了所有的推导。现在我们只需要澄清几点。</p><h1 id="173a" class="ml mm it bd mn mo mp mq mr ms mt mu mv ka mw kb mx kd my ke mz kg na kh nb nc bi translated">时间反向传播</h1><p id="07f7" class="pw-post-body-paragraph kq kr it ks b kt nf jv kv kw ng jy ky kz ok lb lc ld ol lf lg lh om lj lk ll im bi translated">到目前为止，我们所做的是一个单一的时间步骤。现在我们必须进行一次迭代。</p><p id="ef78" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">因此，如果我们有总共 T 个时间步长，那么每个时间步长的梯度将在 T 个时间步长结束时相加，因此每次迭代结束时的累积梯度将为:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="og oh di oi bf oj"><div class="gh gi qo"><img src="../Images/2b866c6e1c069dcf3942b0f457d2fb90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GPqNo-wvniteAqLDesa0Iw.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">图 10:每次迭代结束时的累积梯度</p></figure><p id="258a" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在这些将用于更新权重。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/3b8451cd57e58212cd4143942b69bc89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*MH3gyZSo5nbuPVU25S2k3g.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">图 11:重量更新</p></figure><h1 id="9492" class="ml mm it bd mn mo mp mq mr ms mt mu mv ka mw kb mx kd my ke mz kg na kh nb nc bi translated">结论</h1><p id="c6dc" class="pw-post-body-paragraph kq kr it ks b kt nf jv kv kw ng jy ky kz ok lb lc ld ol lf lg lh om lj lk ll im bi translated">LSTMs 是非常复杂的结构，但是它们也工作得很好。主要有两种类型的 RNN 具有这种特征:LSTM 和格鲁</p><p id="09b9" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">LSTMs 的训练也是一项棘手的任务，因为有许多超参数，正确组合通常是一项困难的任务。</p><p id="fe36" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">所以，到现在为止，我希望 LSTM 的数学部分已经很清楚了，我建议你把手弄脏，以便更清楚、更好地理解它。以下是一些可能有帮助的链接:</p><div class="lz ma gp gr mb mc"><a rel="noopener follow" target="_blank" href="/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd iu gy z fp mh fr fs mi fu fw is bi translated">理解 LSTM 及其在情感分析 keras 中的快速实现。</h2><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="qq l pa pb pc oy pd ls mc"/></div></div></a></div><div class="lz ma gp gr mb mc"><a href="https://adventuresinmachinelearning.com/keras-lstm-tutorial/" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd iu gy z fp mh fr fs mi fu fw is bi translated">Keras LSTM 教程-如何轻松建立一个强大的深度学习语言模型</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">在以前的帖子中，我介绍了用于构建卷积神经网络和执行单词嵌入的 Keras。的…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">adventuresinmachinelearning.com</p></div></div><div class="oy l"><div class="qr l pa pb pc oy pd ls mc"/></div></div></a></div><p id="d266" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">感谢你阅读这篇文章，希望你喜欢。</p><p id="ea4d" class="pw-post-body-paragraph kq kr it ks b kt ku jv kv kw kx jy ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">快乐学习😃！！！</p></div></div>    
</body>
</html>