<html>
<head>
<title>Understanding Graph Convolutional Networks for Node Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解用于节点分类的图卷积网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b?source=collection_archive---------0-----------------------#2020-06-10">https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b?source=collection_archive---------0-----------------------#2020-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/df7c3c7a6bafabc9181e467da565d3a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3IcRT75O6f2NC9BFGhq3g.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图解卷积网络(图片由作者提供)</p></figure><p id="4e53" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">神经网络在过去十年中取得了巨大的成功。然而，神经网络的早期变体只能使用常规或欧几里德数据来实现，而现实世界中的许多数据具有非欧几里德的底层图形结构。数据结构的不规则性导致了最近图形神经网络的发展。在过去的几年中，图形神经网络的不同变体正在被开发，图形卷积网络(GCN)是其中之一。GCNs 也被认为是基本图形神经网络的变体之一。</p><p id="9fac" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在本文中，我们将深入探讨由托马斯·基普夫和马克斯·韦林开发的图卷积网络。我也将给出一些关于使用<a class="ae ld" href="https://networkx.github.io/documentation/networkx-2.3/index.html" rel="noopener ugc nofollow" target="_blank">网络 X </a>构建我们的第一张图的非常基本的例子。通过这篇文章的结尾，我希望我们能对图卷积网络内部的机制有更深入的了解。</p><p id="53df" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="le">如果你不熟悉图形神经网络的基本概念，我推荐你在这里阅读我之前的文章</em><a class="ae ld" href="https://medium.com/analytics-vidhya/getting-the-intuition-of-graph-neural-networks-a30a2c34280d" rel="noopener"><em class="le"/></a><em class="le">。</em></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="bdd9" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated"><strong class="ak"> <em class="mk">图中卷积神经网络</em> </strong></h1><p id="21c1" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq mn ks kt ku mo kw kx ky mp la lb lc im bi translated">如果你熟悉卷积神经网络中的<a class="ae ld" href="https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">卷积层，那么 GCNs 中的‘卷积’基本上也是同样的操作。它指的是将输入神经元乘以一组通常称为<em class="le">滤波器</em>或<em class="le">内核的权重。</em>过滤器在整个图像中充当滑动窗口，使 CNN 能够从相邻细胞中学习特征。在同一层内，<em class="le"> </em>相同的滤波器将被用于整个图像，这被称为<strong class="kh iu">权重共享</strong>。例如，使用 CNN 对猫与非猫的图像进行分类，将在同一层中使用相同的过滤器来检测猫的鼻子和耳朵。</a></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/c2fdc18240ed005d5fc0a28e2fc96880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-J7R1c0SgR81QmJlehh5qA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">相同的权重(或核心，或 CNN 中的过滤器)应用于整个图像(作者的图像)</p></figure><p id="289b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">gcn 执行类似的操作，其中模型通过检查相邻节点来学习特征。CNN 和 GNNs 之间的主要区别在于，CNN 是专门构建来对规则(欧几里德)结构化数据进行操作的，而 GNNs 是 CNN 的一般化版本，其中节点连接的数量是变化的，并且节点是无序的(在非欧几里德结构化数据上是不规则的)。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/59eeaef1aba92bc49d7b939539f3ccb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*BWcCJNNUUafx44joEigIYg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">2D 卷积神经网络(左)和图卷积网络(右)的图解，通过<a class="ae ld" href="https://arxiv.org/pdf/1901.00596.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="a107" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">GCNs 本身可以分为两种主要算法，<strong class="kh iu">空间图卷积网络</strong>和<strong class="kh iu">谱图卷积网络</strong>。在本文中，我们将关注 F <a class="ae ld" href="https://arxiv.org/pdf/1609.02907.pdf" rel="noopener ugc nofollow" target="_blank"> ast 近似基于谱的图卷积网络</a>。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="207f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在深入研究 GCNs 内部发生的计算之前，让我们先简要回顾一下神经网络中前向传播的概念。如果你熟悉的话，可以跳过下面的部分。</p><h1 id="dcce" class="lm ln it bd lo lp mw lr ls lt mx lv lw lx my lz ma mb mz md me mf na mh mi mj bi translated">神经网络正向传播简要概述</h1><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/e1a18b0e03e34ac3d68ba08832830e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*MCgKxTNwL7L8WJN3JJ6tcQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">全连接神经网络的图示(图片由作者提供)</p></figure><p id="044b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在神经网络中，为了将特征表示传播到下一层(正向传递)，我们执行下面的等式:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/0bbf81ba848ca1af153b13bad69b6dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*9LlQmajRQeCQfLgas5vTRg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">等式 1 —神经网络中的正向传递</p></figure><p id="6b28" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这基本上相当于<strong class="kh iu">线性回归</strong>中的<strong class="kh iu"> y = mx+b </strong>，其中:</p><p id="18fc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> m </strong>相当于<strong class="kh iu">重量</strong></p><p id="f92e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> x </strong>是<strong class="kh iu">输入特性</strong></p><p id="d3ea" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> b </strong>是<strong class="kh iu">偏向</strong></p><p id="6147" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上述正向传递方程与线性回归的区别在于，神经网络应用<strong class="kh iu">非线性</strong> <a class="ae ld" rel="noopener" target="_blank" href="/activation-functions-neural-networks-1cbd9f8d91d6"> <strong class="kh iu">激活函数</strong> </a>来表示潜在维度中的非线性特征。</p><p id="6d25" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">回头看上面的等式，对于第一个隐藏层(i = 0)，我们可以简单地将等式重写为如下:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/38da7e861ada28e0ae0b799854d76092.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*t6nDnpJoGpXJSV0-jjPRVQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">等式 2 —第一层神经网络中的正向传递</p></figure><p id="dca8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">其中第 0 层的<strong class="kh iu">特征表示</strong>基本上是<strong class="kh iu">输入特征(X) </strong>。</p><p id="6a17" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="le">这个等式在图卷积网络中有什么不同？</em></p><h1 id="b656" class="lm ln it bd lo lp mw lr ls lt mx lv lw lx my lz ma mb mz md me mf na mh mi mj bi translated">快速近似谱图卷积网络</h1><p id="41b4" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq mn ks kt ku mo kw kx ky mp la lb lc im bi translated">频谱 GCN 背后的最初想法受到信号/波传播的启发。我们可以把频谱 GCN 中的信息传播看作是沿着节点的信号传播。谱 gcn 利用图拉普拉斯矩阵的特征分解来实现这种信息传播方法。简而言之，特征分解帮助我们理解图的结构，从而对图的节点进行分类。这有点类似于主成分分析(PCA)和线性判别分析(LDA)的基本概念，其中我们使用特征分解来降低维度并执行聚类。如果你从未听说过特征分解和拉普拉斯矩阵，不要担心！在这个快速近似方法中，我们不打算显式地使用它们。</p><p id="9a31" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在这种方法中，除了节点特征(或所谓的输入特征)之外，我们将在前向传播方程中考虑<strong class="kh iu">邻接矩阵(A) </strong>。<strong class="kh iu"> A </strong>是表示正向传播方程中节点之间的边或连接的矩阵。在前向传递方程中插入<strong class="kh iu"> A </strong>使得<strong class="kh iu"> </strong>模型能够学习基于节点连通性的特征表示。为了简单起见，偏置<strong class="kh iu"> b </strong>被省略<strong class="kh iu"> </strong>。由此产生的 GCN 可以被视为以消息传递网络形式的谱图卷积的一阶近似，其中信息沿着图中的相邻节点传播。</p><p id="2b0e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">通过添加邻接矩阵作为附加元素，前向传递方程将是:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f073e42b75d39bc6d8297acfa5ef9aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*2cT063K_PIvJVRqFn8c5gg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">等式 3—图形卷积网络中的前向传递</p></figure><p id="6f2c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">等待..你说 A，什么是 <strong class="kh iu"> <em class="le"> A* </em> </strong> <em class="le">？</em></p><p id="e7c1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> A* </strong>是<strong class="kh iu"> A </strong>的规格化版本。为了更好地理解为什么我们需要规范化<strong class="kh iu"> A </strong>以及在 GCNs 中向前传递时会发生什么，让我们做一个实验。</p><h1 id="64d3" class="lm ln it bd lo lp mw lr ls lt mx lv lw lx my lz ma mb mz md me mf na mh mi mj bi translated">构建图形卷积网络</h1><h2 id="84be" class="nf ln it bd lo ng nh dn ls ni nj dp lw kq nk nl ma ku nm nn me ky no np mi nq bi translated">初始化图形 G</h2><p id="01eb" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq mn ks kt ku mo kw kx ky mp la lb lc im bi translated">让我们首先使用<a class="ae ld" href="https://networkx.github.io/documentation/networkx-2.3/index.html" rel="noopener ugc nofollow" target="_blank"> NetworkX </a>构建一个简单的无向图(<strong class="kh iu"> G </strong>)。图<strong class="kh iu"> G </strong>将由 6 个节点组成，并且每个节点的特征将对应于特定的节点号。例如，节点 1 的节点特征为 1，节点 2 的节点特征为 2，依此类推。为了简化，在这个实验中我们不打算指定边缘特征。</p><figure class="mr ms mt mu gt ju"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="7037" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">输出:</strong></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/377ba2492181a5991a9527123f248656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ALLECdSyZPCjaSrCXV5aRA.png"/></div></div></figure><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/bce554b5d17069cd19a557ab5595d4a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*RUQp4X6rHWGI7UX1yX5PeQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图形 G 可视化</p></figure><p id="1468" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">由于我们只有 1 个图，这个数据配置是一个<a class="ae ld" href="https://medium.com/analytics-vidhya/getting-the-intuition-of-graph-neural-networks-a30a2c34280d" rel="noopener"> <strong class="kh iu">单模</strong> </a> <strong class="kh iu"> </strong>表示的例子。我们将构建一个学习结点要素表示的 GCN。</p><h2 id="6521" class="nf ln it bd lo ng nh dn ls ni nj dp lw kq nk nl ma ku nm nn me ky no np mi nq bi translated">将邻接矩阵<strong class="ak"> (A)插入前向传递方程</strong></h2><p id="cd91" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq mn ks kt ku mo kw kx ky mp la lb lc im bi translated">下一步是从图<strong class="kh iu"> G </strong>中获得邻接矩阵<strong class="kh iu"> (A) </strong>和节点特征矩阵<strong class="kh iu"> (X) </strong>。</p><figure class="mr ms mt mu gt ju"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="11f8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">输出:</strong></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/fd5fb33751f378393a19cf59094bcb50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZHOSiNqygw9NVBZKfZr6Zw.png"/></div></div></figure><p id="cd4c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，让我们研究如何通过将<strong class="kh iu">和</strong>插入正向传递方程来增加模型更丰富的特征表示。我们要对<strong class="kh iu"> A </strong>和<strong class="kh iu">x</strong>进行点积运算，在本文中我们把这个点积运算的结果称为<em class="le"> AX </em>。</p><figure class="mr ms mt mu gt ju"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="72db" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">输出:</strong></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/475fcf5f1f193da4d3387a7cc5e46cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UckfxHh6pA_TkHkgkIWw4g.png"/></div></div></figure><p id="195b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">从结果可以明显看出，<em class="le"> AX </em> <strong class="kh iu"> </strong>代表<strong class="kh iu">相邻节点特征</strong>的总和。例如，<em class="le"> AX </em>的第一行对应于连接到节点 0 的节点特征的总和，节点 0 是节点 1、2 和 3。这让我们了解了 GCNs 中的传播机制以及节点连接性如何影响 GCNs 所看到的隐藏特征表示。</p><blockquote class="nw nx ny"><p id="c77b" class="kf kg le kh b ki kj kk kl km kn ko kp nz kr ks kt oa kv kw kx ob kz la lb lc im bi translated">邻接矩阵和节点特征矩阵的点积表示相邻节点特征的总和。</p></blockquote><p id="9a71" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">但是，如果我们想得更多，我们会意识到，虽然<em class="le"> AX </em>总结了<em class="le"> </em>，<strong class="kh iu">相邻节点的特征，但它没有考虑节点本身的特征</strong>。</p><p id="432a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="le">糟糕，检测到问题！怎么解决？</em></p><h2 id="86e3" class="nf ln it bd lo ng nh dn ls ni nj dp lw kq nk nl ma ku nm nn me ky no np mi nq bi translated"><strong class="ak">插入自循环并规格化一个</strong></h2><p id="5722" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq mn ks kt ku mo kw kx ky mp la lb lc im bi translated">为了解决这个问题，我们现在向<strong class="kh iu"> A 的每个节点添加<a class="ae ld" href="https://en.wikipedia.org/wiki/Loop_(graph_theory)" rel="noopener ugc nofollow" target="_blank">自循环</a>。</strong>添加<strong class="kh iu"> </strong>自循环基本上是一种将节点连接到自身的机制。也就是说，邻接矩阵<strong class="kh iu"> A </strong>的所有对角元素现在都将变成 1，因为每个节点都与自身相连。我们把<strong class="kh iu"> A </strong>加上自循环叫做<strong class="kh iu"> A_hat </strong>重新计算<em class="le"> AX </em>，现在就是<strong class="kh iu"> A_hat </strong>和<strong class="kh iu"> X </strong>的点积:</p><figure class="mr ms mt mu gt ju"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="83a0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">输出:</strong></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/d67f5be57cc6ed76a78e5ffcae8ea251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MBYZB6i0IoGmyuVFOUSJFw.png"/></div></div></figure><p id="4763" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="le">太好了！一个问题解决了！</em></p><p id="82fa" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，你可能会意识到另一个问题。<em class="le"> AX </em>的元素是<strong class="kh iu">未规格化</strong>。类似于任何神经网络操作的数据预处理，我们需要归一化特征以防止数值不稳定性和消失/爆炸梯度，从而使模型收敛。在 GCNs 中，我们通过<strong class="kh iu">计算</strong> <a class="ae ld" href="https://en.wikipedia.org/wiki/Degree_matrix" rel="noopener ugc nofollow" target="_blank"> <strong class="kh iu">度矩阵</strong> </a> <strong class="kh iu"> (D)并执行 D 的</strong> <a class="ae ld" href="https://mathworld.wolfram.com/MatrixInverse.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kh iu">逆</strong> </a> <strong class="kh iu">与</strong> <em class="le"> AX </em>的点积运算来规范化我们的数据</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/829b5e74a138c57f77b4d67f02b9065d.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*DKWEsqvmm74KwLS-eM1G8w.png"/></div></figure><p id="33c3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在本文中我们称之为<em class="le"> DAX </em>。在图形术语中，术语“度”是指一个节点所连接的边的数量。</p><figure class="mr ms mt mu gt ju"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="1c78" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">输出:</strong></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/dacfbab72b7b39e958c80f2cc53999f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VhRv3ng2Ixat2_w-pS_9IA.png"/></div></div></figure><p id="9411" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果我们比较<em class="le"> DAX </em>和<em class="le"> AX </em>，我们会注意到:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi of"><img src="../Images/32b8a9890c165782c0da063eaad57476.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*ltJAgs2TYVIW3j16H3oWzg.png"/></div></figure><p id="036a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们可以看到规范化对 DAX 的影响，其中对应于节点 3 的元素与节点 4 和 5 相比具有较低的值。但是，如果节点 3 与节点 4 和 5 具有相同的初始值，为什么在规范化后会有不同的值呢？</p><p id="a529" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们回头看看我们的图表。节点 3 有<strong class="kh iu"> 3 条入射边</strong>，而节点 4 和 5 只有<strong class="kh iu"> 2 条入射边</strong>。<strong class="kh iu">节点 3 比节点 4 和 5 具有更高的度</strong>的事实导致<strong class="kh iu">节点 3 的特征在<em class="le"> DAX </em> </strong>中的权重更低。换句话说，节点的度越低，节点属于某个组或簇的能力就越强。</p><p id="0200" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在<a class="ae ld" href="https://arxiv.org/pdf/1609.02907.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中，Kipf 和 Welling 指出，进行对称归一化将使动力学更有趣，因此，归一化方程修改为:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f294cb62fe9e0d22a2c2cb02863fb362.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*yd8uL8Ewj_C4ES5faZVUxg.png"/></div></figure><p id="6a43" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们使用新的对称归一化方程来计算归一化值:</p><figure class="mr ms mt mu gt ju"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="7b54" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">输出:</strong></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/55efb4e5ef1410bf7b7159a362101f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KKNXFIcVoa60wq0RKp61yQ.png"/></div></div></figure><p id="8622" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">回头看看上一节中的等式 3，我们会意识到我们现在有了什么是<strong class="kh iu"> A* </strong>的答案！文中将<strong class="kh iu"> A* </strong>称为<em class="le">重正化诡计</em>。</p><p id="b1e2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">完成了特性处理之后，是时候完成我们的 GCN 了。</p><h2 id="7909" class="nf ln it bd lo ng nh dn ls ni nj dp lw kq nk nl ma ku nm nn me ky no np mi nq bi translated">添加权重和激活函数</h2><p id="2f49" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq mn ks kt ku mo kw kx ky mp la lb lc im bi translated">我们将使用<a class="ae ld" href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank"> ReLu </a>作为激活函数来构建一个 2 层 GCN。为了初始化权重，我们将使用随机种子，这样我们就可以复制结果。请记住，权重初始化不能为 0。在这个实验中，我们将为隐藏层设置 4 个神经元。因为我们将绘制二维特征表示，所以将有 2 个输出神经元。</p><p id="a9b8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了更简单，我们将使用 numpy 重写<em class="le">重正化技巧</em>方程，只是为了更简单。</p><figure class="mr ms mt mu gt ju"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="b05e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">输出:</strong></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/ce478980f82011720191042a543e83a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-ti0vK_HlZ5fg8ffphlSw.png"/></div></div></figure><p id="e936" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">搞定了。我们刚刚建立了我们的第一个前馈 GCN 模型！</p><h2 id="2124" class="nf ln it bd lo ng nh dn ls ni nj dp lw kq nk nl ma ku nm nn me ky no np mi nq bi translated">绘制要素制图表达</h2><p id="98a6" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq mn ks kt ku mo kw kx ky mp la lb lc im bi translated">GCN 的“魔力”在于它<a class="ae ld" href="https://arxiv.org/pdf/1810.11908.pdf" rel="noopener ugc nofollow" target="_blank">可以学习特征表现，即使没有训练</a>。让我们在通过 2 层 GCN 后可视化要素制图表达。</p><figure class="mr ms mt mu gt ju"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="ec04" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">输出:</strong></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/68eb493c5cd67d9f30a7b989544a7261.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*QUa95OfVcS3X8BiIh0Fa0Q.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来自前馈 GCN 的特征表示</p></figure><p id="5a9b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">从上面的图中，可以清楚地看到有 2 个主要组，其中左边的组由节点 0、1、2 组成，右边的组由节点 3、4、5 组成。我们可以推断出<strong class="kh iu"> GCNs 已经可以学习特征表示，即使没有训练或者</strong> <a class="ae ld" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank"> <strong class="kh iu">反向传播</strong> </a>。</p><h1 id="0e80" class="lm ln it bd lo lp mw lr ls lt mx lv lw lx my lz ma mb mz md me mf na mh mi mj bi translated">关键要点</h1><ul class=""><li id="297d" class="oj ok it kh b ki ml km mm kq ol ku om ky on lc oo op oq or bi translated">图卷积网络中的术语“卷积”在<em class="le">权重共享方面类似于卷积神经网络。</em>主要区别在于<em class="le"> </em>数据结构，其中 gcn 是 CNN 的一般化版本，可以处理底层非规则结构的数据。</li><li id="5cf3" class="oj ok it kh b ki os km ot kq ou ku ov ky ow lc oo op oq or bi translated">在 GCNs 的前向传递方程中插入邻接矩阵(<strong class="kh iu"> A </strong>，使得模型能够学习相邻节点的特征。这种机制可以看作是沿着图中的节点传递消息的操作。</li><li id="b938" class="oj ok it kh b ki os km ot kq ou ku ov ky ow lc oo op oq or bi translated"><em class="le">重正化技巧</em>由<a class="ae ld" href="https://arxiv.org/pdf/1609.02907.pdf" rel="noopener ugc nofollow" target="_blank"> Thomas Kipf 和 Max Welling (2017) </a>用于归一化快速近似基于谱图卷积网络中的特征。</li><li id="a15f" class="oj ok it kh b ki os km ot kq ou ku ov ky ow lc oo op oq or bi translated">gcn 甚至可以在训练之前学习特征表示。</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="83a5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="le">感谢阅读！如果您想了解如何使用 CORA 数据集在节点分类任务上训练 GCN，可以阅读本系列的下一篇文章</em><a class="ae ld" rel="noopener" target="_blank" href="/graph-convolutional-networks-on-node-classification-2b6bbec1d042"><em class="le"/></a><em class="le">。</em></p><p id="17d5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="le">有什么意见、反馈或想讨论的吗？请给我留言。你可以在</em> <a class="ae ld" href="https://www.linkedin.com/in/inneke-mayachita-34023877/" rel="noopener ugc nofollow" target="_blank"> <em class="le"> LinkedIn </em> </a> <em class="le">上联系我。</em></p><p id="5e39" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="le">可以在</em><a class="ae ld" href="https://github.com/imayachita/Explore_GCN/blob/master/Building_GCN.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="le">GitHub</em></a><em class="le">上获取完整代码。</em></p><h1 id="cbd3" class="lm ln it bd lo lp mw lr ls lt mx lv lw lx my lz ma mb mz md me mf na mh mi mj bi translated">参考</h1><p id="0e0c" class="pw-post-body-paragraph kf kg it kh b ki ml kk kl km mm ko kp kq mn ks kt ku mo kw kx ky mp la lb lc im bi translated">[1] T. Kipf 和 M. Welling，<a class="ae ld" href="https://arxiv.org/pdf/1609.02907.pdf" rel="noopener ugc nofollow" target="_blank">利用图卷积网络的半监督分类</a> (2017)。arXiv 预印本 arXiv:1609.02907。ICLR 2017</p><p id="d776" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[2] T .基普夫，<a class="ae ld" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">https://tkipf.github.io/graph-convolutional-networks/</a></p><p id="ca50" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[3]吴，等。艾尔。，<a class="ae ld" href="https://arxiv.org/pdf/1901.00596.pdf" rel="noopener ugc nofollow" target="_blank">图神经网络综合研究</a> (2019)。</p><p id="07fa" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[4] T. S. Jepsen，<a class="ae ld" rel="noopener" target="_blank" href="/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780">https://towardsdatascience . com/how-do-deep-learning-on-graphs-with-graph-convolutionary-networks-7d 2250723780</a></p></div></div>    
</body>
</html>