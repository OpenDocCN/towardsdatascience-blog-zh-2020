<html>
<head>
<title>Understanding Regularization in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解机器学习中的正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-regularization-in-machine-learning-d7dd0729dde5?source=collection_archive---------10-----------------------#2020-06-10">https://towardsdatascience.com/understanding-regularization-in-machine-learning-d7dd0729dde5?source=collection_archive---------10-----------------------#2020-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="323c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过防止过度拟合来优化预测模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b0121c1c62d2d9c22b8c8d4861f717d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_CgtoLd1UAzJkL74"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">杰克逊·乔斯特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="f7c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> W </span>在训练机器学习模型时，一个主要方面就是评估模型是否过拟合数据。过度拟合通常发生在模型试图拟合所有的数据点时，在此过程中捕获噪声，导致模型的不准确发展。</p><p id="055d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习模型的性能可以通过成本函数来评估。通常，成本函数由实际值和预测值之差的平方和表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/2a6e7f8918c831377294a7051b5ff1a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*iO-U6wi1-Ok9SaNREbA6iw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这里“y”代表实际值，而“ŷ”代表预测值。</p></figure><p id="2687" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这也被称为“<strong class="lb iu">残差平方和</strong>或“<strong class="lb iu">误差平方和</strong>”。预测模型在被训练时试图以最小化该成本函数的方式来拟合数据。</p><p id="9ad7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当一个模型通过所有的数据点时，它开始过度拟合。在这种情况下，尽管成本函数的值等于零，但是考虑了数据集中的噪声的模型并不代表实际的函数。在这种情况下，根据训练数据计算的误差较小。然而，在测试数据上，误差仍然很大。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mf"><img src="../Images/be2a5b51d47c6849a6fc8e52006aae32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UGWRKn-mvzOK52ksGvRThw.png"/></div></div></figure><p id="f69b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，模型通过使用高度复杂的曲线来过度拟合数据，这些曲线具有自由度大的项以及为每一项提供权重的相应系数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mg"><img src="../Images/4bedaad9a5646edd7320454520c02b52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bD7ac0TP0za5Cy69XLPDJg.png"/></div></div></figure><p id="394f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以从方程中清楚地观察到曲线越来越复杂。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mh"><img src="../Images/7d4fd1dc994b4fdebbb33a6d49f54f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rTbl-lFa7Entl2vocS2N2A.png"/></div></div></figure><p id="1f9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图可以看出，对于较高的自由度，测试集误差比训练集误差大。</p><p id="b73e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正则化是一个概念，通过它可以防止机器学习算法过度拟合数据集。正则化通过在成本函数中引入惩罚项来实现这一点，该惩罚项为复杂曲线分配更高的惩罚。</p><p id="750b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基本上有两种类型的正则化技术</p><ol class=""><li id="3460" class="mi mj it lb b lc ld lf lg li mk lm ml lq mm lu mn mo mp mq bi translated"><strong class="lb iu"> L1正则化或拉索回归</strong></li><li id="00f3" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated"><strong class="lb iu"> L2正则化或岭回归</strong></li></ol><p id="8bd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们首先从理解L2正则化或岭回归开始。</p><h1 id="0982" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">L2正则化或岭回归</h1><p id="f944" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">岭回归的成本函数由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/00942b73120820a9c5e445d5c88c6dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*m8RukANxfFcue6NghGC28Q.png"/></div></figure><p id="900d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里λ(<em class="nu">𝜆</em>)是一个超参数，它决定了惩罚的严厉程度。λ的值可以从0变化到无穷大。可以观察到，当λ的值为零时，罚项不再影响成本函数的值，因此成本函数降回误差平方和。</p><p id="8149" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解惩罚项的意义，让我们深入研究一个例子。</p><p id="04a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们仅仅根据“误差平方和”来评估我们模型的性能，我们会得到下图中左侧图形所表示的曲线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/0508b53f7ef42537a9e9913f84d90fce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*ODhXW4H9ZRTKTzXOAyStAg.png"/></div></figure><p id="9b17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，惩罚项不再影响成本函数的值。因此，我们得到相同的过拟合曲线。然而，当lambda的值增加时，我们会得到一条更简单的曲线，如上图右侧的图表所示。</p><p id="ce73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比较两个模型的均方误差，我们观察到，对于过拟合曲线，训练集上的误差最小，但是对于更简单的曲线，测试集上观察到的误差显著下降。</p><p id="e9ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，通过简化我们的曲线，我们在训练集中引入了一些误差，但这使我们能够走向更一般化的模型。</p><p id="1d18" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要强调的一个重要方面是，通过改变λ的值并将我们的复杂曲线转换为简单曲线，我们正在处理相同的15次多项式模型。直到15度的项仍然存在于简单模型的方程中，但是模型的复杂性已经降低。</p><h1 id="7140" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">这是如何实现的？</h1><p id="33fb" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">答案在于刑罚本身的机制。我们再来看看成本函数。</p><p id="a703" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">λ是一个超参数，决定惩罚的严厉程度。随着罚值的增加，系数的值收缩，以便最小化成本函数。由于这些系数也充当多项式项的权重，缩小这些系数将减少分配给它们的权重，并最终减少其影响。因此，对于上述情况，分配给高次多项式项的系数已经缩小到这样的程度，即这些项的值不再像以前那样严重地影响模型，因此我们有一条简单的曲线。</p><p id="f7fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在确定了λ的最佳值后，我们将其应用到模型中，得到下面的曲线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ce4149ba60a8cdc2e083d0730cd20106.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*nDS7icEHoPYJh2LD90hTCg.png"/></div></figure><h1 id="738d" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">改变λ值的影响</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/c7f981f6d5bb08688af60a9a21a1d3b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9XZ_c6gMmYpIlVqLyauRmQ.png"/></div></div></figure><p id="b42f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们观察到，随着λ值的增加，模型变得更加简单，直到它渐近平行于x轴。换句话说，对于很高的λ值，我们有一个高度偏差的模型。</p><h1 id="8ed2" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">λ的值如何选择？</h1><p id="584b" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">这让我们左右为难。对于非常低的λ值，获得过拟合曲线，而对于非常高的λ值，获得欠拟合或高度偏差的模型。那么怎样才能达到λ的最佳值呢？</p><p id="87f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个问题的答案是交叉验证。通常，10倍交叉验证可以帮助我们确定最佳λ值。</p><h1 id="f04a" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">多维数据集</h1><p id="ef23" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">假设我们试图预测一种动物的身体大小。假设这个尺寸取决于它的体重和年龄。在这种情况下，我们模型的功能可以表示为:-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/7acb54cc78732c623a6086c4f69d4277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C9-ebkli1O2H_Mx5Kcv0Dw.png"/></div></div></figure><p id="7b78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我们有多种取决于尺寸的功能，成本可由下式给出:-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/c0defa161a9190a1bdd2bebda54842c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vj_m1eF13WilcWV1PQoaQw.png"/></div></div></figure><p id="4ed7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们的训练集中的数据点数量有限，岭回归可以通过减少方差并使模型对“训练数据”分布不太敏感，从而使曲线对噪声不太敏感，来改进根据数据做出的预测。</p><h1 id="8b84" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">L1正则化或套索回归</h1><p id="13cb" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">LASSO代表<strong class="lb iu">最小绝对收缩和选择算子</strong>。</p><p id="b963" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Lasso回归的成本函数由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/4137a4a5848c67bf6a2703123398f234.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*rRJZqUszR-O-3EVCWzDypg.png"/></div></figure><p id="cf2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与岭回归相似，lambda也是一个超参数，它决定惩罚的严厉程度。成本函数的区别在于岭回归采用斜率的平方，而套索回归采用斜率的绝对值。</p><h1 id="c60d" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">改变λ的影响</h1><p id="3114" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">随着λ值的增加，系数的值将越来越接近0，直到该值最终为0。</p><p id="92db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，在岭回归中，当λ增加时，系数的值收缩，直到模型渐近平行于x轴。在lasso回归中，对于较大的λ值，模型实际上会变得与x轴平行。</p><p id="a2e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解这一点，让我们举一个例子。</p><p id="bec3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设动物的大小由以下等式给出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/7f3bf8ca080c044aee5e15c5b4cd90d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HjpJ4UAA9I4KfC8_bf5eJw.png"/></div></div></figure><p id="6a70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">套索回归的工作方式是保留最相关的要素，而缩小其他要素。</p><p id="0200" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述等式的成本函数可以给出如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/1da96c80b104abf1e6c56d6e4120bf11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CDttT4Atp6DX0cciigxYSQ.png"/></div></div></figure><p id="e35f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中,“体重”和“食物卡路里含量”与体型最相关，而“星座”和“风速”最不相关。因此，“coeff1”和“coeff2”会缩小一点，而“coeff3”和“coeff4”会一直缩小到零。</p><p id="0782" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们只剩下等式:-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/e0faf51bdf1fb373d956857682c3a509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qdsX7JsRLF_EtfdYW7WEmA.png"/></div></div></figure><p id="7a08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于lasso回归可以从方程中排除无关紧要的变量，因此在减少包含大量无用变量的模型中的方差方面，它比岭回归稍好一些。换句话说，套索回归有助于特征选择。</p><p id="978b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相比之下，当大多数变量都有用时，岭回归往往做得更好一点。</p><h1 id="abdf" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">套索和岭回归的另一个有趣的区别</h1><p id="fa4d" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">前面提到过，随着岭回归中λ值的不断增加，模型曲线变得越来越平坦，直到它渐近平行于x轴。</p><p id="145e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解这一点，为了简单起见，我们将参考线性回归模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/15af25c69d5e132c06d79c250018c299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WuAH6ufpExhN0V8RYe-KJQ.png"/></div></div></figure><p id="1c56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">点代表数据点，线代表回归模型。因为这是一条直线，所以它有一个斜率和一个y截距。</p><p id="0caf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着λ值的增加，线性回归模型的斜率将持续降低。</p><p id="917d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在较大的λ值下，我们观察到回归模型的以下曲线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/69be0d5c95219b86e5c8f1104d5db8bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i5C0axn8cJXuw7w3Qh1asw.png"/></div></div></figure><p id="2f1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然看起来这条线平行于x轴，但实际上，线性模型的斜率值略大于零。</p><p id="1b19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种行为可以通过下图直观地观察到。x轴表示模型的斜率，y轴表示模型的成本函数值。每个图的左侧是λ值的范围。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/72f7781d3f2830bc45d2507bb2b8eb4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ADKa24xDiIYaTVin_Ks7fw.png"/></div></div></figure><p id="c4e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">曲线中的蓝点代表模型成本函数的最小值。随着lambda值的增加，岭回归模型的成本函数的最低值不断向零斜率值移动，但最低值永远不会与零重合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/5d9e9ec385af18ef09d7ca18fe9f26f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F58htEl3nUYzEBfcaIhPiQ.png"/></div></div></figure><p id="40d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，在lasso回归模型中，随着λ值的增加，我们观察到类似的趋势，其中成本函数的最低值逐渐向零斜率值移动。然而，在这种情况下，对于大的λ值，当斜率的值与零一致时，实现了成本函数的最低值。特别是对于等于40、60和80的λ值，我们在曲线图中观察到一个明显的拐点，其中斜率值等于零，这恰好也是λ等于80时成本函数的最低值。</p><p id="6a0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，对于较大的λ值，lasso回归模型可以具有等于零的斜率。</p><h1 id="42b6" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">套索回归对特征选择有什么帮助？</h1><p id="1b83" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">让我们看看每种正则化技术的成本函数。</p><p id="3c2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于套索回归:-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/4137a4a5848c67bf6a2703123398f234.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*rRJZqUszR-O-3EVCWzDypg.png"/></div></figure><p id="deb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于岭回归:-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/00942b73120820a9c5e445d5c88c6dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*m8RukANxfFcue6NghGC28Q.png"/></div></figure><p id="a1dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">套索回归的一种解释方式是解一个系数的模之和小于或等于常数“c”的方程。类似地，岭回归可以解释为求解一个系数的平方和小于等于常数“c”的方程。</p><p id="d1b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们的模型结合了两个特征来预测某个实体，并且这些特征的系数由β1和β2给出。</p><p id="33b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，岭回归可以表示为:-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d7902cf47debab54ad91a0175d0fb5a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*EjbMVpVe6F7-YR6Zd-o4PA.png"/></div></figure><p id="c305" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，套索回归可以表示为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/06d422f611118530cb09c14796c20be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*aawhcA2VfwVX6387Jb5kVg.png"/></div></figure><p id="45f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">岭回归方程类似于圆的方程，因此约束区域位于圆的圆周内和圆周上，类似地，套索回归方程类似于菱形方程，约束区域位于该形状的内部和圆周上。</p><p id="67d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些方程在下面的图片中被可视化了。</p><p id="d7d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">约束区域由浅蓝色区域表示，而红色椭圆是表示“误差平方和”的轮廓。在任何特定的给定轮廓中,“误差平方和”的值都是相同的。轮廓离中心越远，“误差平方和”的值越高。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/046fda04f91017d824cf5fc0e34399ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*GMi7-FUvb9NhuGqxV-FGUw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">资料来源:统计学习导论，作者:加雷斯·詹姆斯，丹妮拉·威滕，特雷弗·哈斯蒂，罗伯特·蒂布拉尼</p></figure><p id="bc1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果“c”的值足够大，约束区域将包含由β (hat)表示的轮廓的中心，因此岭回归和套索回归的值将与“误差平方和”的最小值相同。这对应于λ= 0的情况。</p><p id="d63e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">岭和套索回归的系数估计由轮廓接触约束区域的第一个点给出。由于岭回归具有圆形约束区域，接触点通常不会出现在轴上。因此，系数估计将主要是非零的。</p><p id="3adc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，由于套索回归约束区域有突出的角，所以轮廓与轴上的约束区域相交的可能性更大。在这种情况下，其中一个系数的值将等于零。在高维空间中，我们有两个以上的特征，许多系数估计可能同时等于零。</p><h1 id="1096" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">结论</h1><p id="8a33" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">正则化是防止模型过度拟合的有效技术。它允许我们减少模型中的方差，而不会显著增加模型的偏差。这种方法允许我们开发一个更一般化的模型，即使我们的数据集中只有几个数据点。</p><p id="0ce4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在确定模型的参数或特征已知的情况下，岭回归有助于缩小模型的系数。</p><p id="9211" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，套索回归可以有效地从模型方程中排除无关紧要的变量。换句话说，套索回归有助于特征选择。</p><p id="ba5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总的来说，这是一项重要的技术，可以极大地提高我们模型的性能。</p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="b6ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您阅读这篇博客。我希望听到你对此的想法。</p><p id="f856" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nu">如果你渴望在一群志同道合的人的陪伴下踏上高质量的学习之旅，我强烈推荐你加入最有吸引力的社区:</em> <a class="ae ky" href="https://www.linkedin.com/company/30951747/" rel="noopener ugc nofollow" target="_blank"> <em class="nu">共同学习休息室</em> </a> <em class="nu">关于</em> <a class="ae ky" href="https://t.me/ColearningLounge_AIRoom" rel="noopener ugc nofollow" target="_blank"> <em class="nu">电报</em> </a> <em class="nu">和访问</em><a class="ae ky" href="http://colearninglounge.com/" rel="noopener ugc nofollow" target="_blank"><em class="nu">colearninglounge.com</em></a><em class="nu">了解更多信息。</em></p><h1 id="b1a9" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">参考</h1><ol class=""><li id="2892" class="mi mj it lb b lc no lf np li os lm ot lq ou lu mn mo mp mq bi translated">Josh Starmer，<em class="nu"> StatQuest:正则化第一部分:岭回归</em>。<a class="ae ky" href="https://youtu.be/Q81RR3yKn30" rel="noopener ugc nofollow" target="_blank">https://youtu.be/Q81RR3yKn30</a></li><li id="298b" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">Josh Starmer，<em class="nu"> StatQuest:正则化第二部分:Lasso回归。</em><a class="ae ky" href="https://youtu.be/NGf0voTMlcs" rel="noopener ugc nofollow" target="_blank">https://youtu.be/NGf0voTMlcs</a></li><li id="e263" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">Josh Starmer，<em class="nu"> StatQuest: Ridge vs Lasso回归，可视化！！！https://youtu.be/Xm2C_gTAl8</em>T2</li><li id="61e8" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">普拉尚·古普塔，<em class="nu">机器学习中的正则化</em>。<a class="ae ky" rel="noopener" target="_blank" href="/regularization-in-machine-learning-76441ddcf99a">https://towards data science . com/regulation-in-machine-learning-76441 ddcf 99 a</a></li><li id="86c1" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">统计学习导论</li></ol></div></div>    
</body>
</html>