<html>
<head>
<title>Bias and Variance: Two Important Machine Learning Concepts to Improve Every Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">偏差和方差:改进每个模型的两个重要的机器学习概念</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/two-important-machine-learning-concepts-to-improve-every-model-62fd058916b?source=collection_archive---------19-----------------------#2020-04-16">https://towardsdatascience.com/two-important-machine-learning-concepts-to-improve-every-model-62fd058916b?source=collection_archive---------19-----------------------#2020-04-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/4c1015af1424a4ce76627411d93540be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qsV7ZWucAvgD_O2K"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">克里斯蒂娜·特里普科维奇在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="c10b" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">了解偏差和方差如何提高模型的准确性</h2></div><p id="e0fb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你不明白自己做得对或错，那么训练任何新模型都是困难的。大多数时候，模型是黑匣子，它吸入数据，吐出精确的数字。理解<strong class="kx jh">为什么</strong>你的模型表现不佳是知道<strong class="kx jh">你如何</strong>改进它的关键。</p><ol class=""><li id="4bdc" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">通过<strong class="kx jh">识别偏差和方差，了解<strong class="kx jh">为什么</strong>你的模型表现不佳。</strong></li><li id="afa5" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">了解<strong class="kx jh">如何通过<strong class="kx jh">减少偏差和方差</strong>来</strong>改进您的模型。</li></ol><h1 id="2f54" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">识别偏差和差异</h1><p id="7291" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">先说误差。误差是你的模型在测试数据上有多不准确。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/e887619d9caac273c83b82c908d89192.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*8L4JDA-xgiTwfMRKDvQN9w.png"/></div></figure><p id="98a4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你的模型在测试集上达到86%的准确率，那么就有14%的误差。这种误差一部分是偏差，一部分是方差。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/68a091a86f4da57d779a8a50a51060d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v1Vzho1V53s96dqUNan0mQ.png"/></div></div></figure><p id="7882" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上图两个要点:<br/> 1。<strong class="kx jh">偏差</strong>是训练集<br/> 2的误差。<strong class="kx jh">方差</strong>是训练和测试准确度之间的差距</p><h2 id="987c" class="ni mg jg bd mh nj nk dn ml nl nm dp mp le nn no mr li np nq mt lm nr ns mv nt bi translated">偏见</h2><p id="0967" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">偏差描述了模型从训练数据中学习的能力。较大的偏差意味着模型很难从训练数据中学习。<br/>如果模型对训练数据有90%的准确度，那么模型有10%的偏差。这种偏见有些是可以避免的，有些是不可避免的。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/02d14d4c53ec56e4f75574cb8d1f7098.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*DRMxInhZ34RbGTX-kI298A.png"/></div></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/0f39208621e6c7363cdeb071a2b97bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4bJbBp3RGEdfoVy8d5uUZQ.png"/></div></div></figure><h2 id="e72b" class="ni mg jg bd mh nj nk dn ml nl nm dp mp le nn no mr li np nq mt lm nr ns mv nt bi translated"><a class="ae jd" href="https://d2wvfoqc9gyqzf.cloudfront.net/content/uploads/2018/09/Ng-MLY01-13.pdf#page=46" rel="noopener ugc nofollow" target="_blank">不可避免与可避免的偏差</a></h2><p id="90d0" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated"><strong class="kx jh">不可避免的偏差</strong>被称为<strong class="kx jh">最优误差率</strong>。这是模型性能的上限。它认识到，一些任务，如字幕或股票预测，是不可能100%准确预测的，即使对人类来说也是如此。因此，我们可以预期，即使在一个完美的世界中，我们的模型至少在某些时候是错误的。</p><p id="e5e7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你决定你的模型至少有4%的时间是错误的，那么这个模型就有4%不可避免的偏差。</p><p id="e455" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">可避免偏差</strong>是最优错误率与训练误差之差。这是我们可以尝试减少的误差，以实现最佳误差率。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/e93fe11ab145eee3bc1e4c95ac943e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hv4hamCuiwHu7fQC8aDqGA.png"/></div></div></figure><h2 id="4cc6" class="ni mg jg bd mh nj nk dn ml nl nm dp mp le nn no mr li np nq mt lm nr ns mv nt bi translated">差异</h2><p id="55e5" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">方差描述了您的模型对其尚未见过的数据的泛化能力。我们将方差定义为训练精度和测试精度之间的差异。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/9b1fd724d654a88234bda68f74286127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*vEE4nBWeDhr2MfSCzqkfRA.png"/></div></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/0a170f3cec50faba94da37e53ee52d6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hr_Zu47mrzjIsAzn78svNg.png"/></div></div></figure><h2 id="e470" class="ni mg jg bd mh nj nk dn ml nl nm dp mp le nn no mr li np nq mt lm nr ns mv nt bi translated"><a class="ae jd" href="https://d2wvfoqc9gyqzf.cloudfront.net/content/uploads/2018/09/Ng-MLY01-13.pdf#page=50" rel="noopener ugc nofollow" target="_blank">偏差与方差的权衡</a></h2><p id="ec38" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">大多数用来减少偏差或方差的方法都是以牺牲一个为代价来减少另一个。有一些例外，但是大多数时候构建最佳模型意味着最小化偏差和方差。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/8b70279081019041de7d4eeced8774d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZhx-iO6cjFBg8qreLuINg.png"/></div></div></figure><h1 id="0e08" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">减少偏差和差异</h1><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/d7662cb5344e6cd1bba32ff3da204e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PjsmMnBbCUplz-a8vCR7vg.png"/></div></div></figure><h2 id="ca40" class="ni mg jg bd mh nj nk dn ml nl nm dp mp le nn no mr li np nq mt lm nr ns mv nt bi translated"><a class="ae jd" href="https://d2wvfoqc9gyqzf.cloudfront.net/content/uploads/2018/09/Ng-MLY01-13.pdf#page=51" rel="noopener ugc nofollow" target="_blank">减少可避免的偏差</a></h2><ul class=""><li id="0a63" class="lr ls jg kx b ky mx lb my le ob li oc lm od lq oe lx ly lz bi translated"><strong class="kx jh">增加模型尺寸<br/> </strong>增加模型尺寸是减少可避免偏差的一种方法。<br/>模型越大，需要调整的参数越多。更多的参数允许模型学习更复杂的关系。您可以通过向模型添加更多的层或节点来增加模型的大小。模型从数据中学习得越好，就越接近最佳错误率。</li></ul><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi of"><img src="../Images/1de1bd9cb27e3e5a83f59b46073841dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UsQoEcB6wDSnB7uye9ssig.png"/></div></div></figure><ul class=""><li id="1664" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq oe lx ly lz bi translated"><strong class="kx jh">减少规则<br/> </strong>减少模型的规则允许模型更好地拟合训练数据。然而，较少的正则化意味着您的模型不会同样概化，从而增加了方差。这是偏倚与方差权衡的经典例子。</li></ul><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi og"><img src="../Images/02c7861cb0e430dbab138ce024cc47d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mvxSxQmjpq_sBamGnZ8P7g.png"/></div></div></figure><ul class=""><li id="adc2" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq oe lx ly lz bi translated"><strong class="kx jh">更改模型架构<br/> </strong>更改模型架构可以帮助它更好地适应数据集。这类似于增加模型的大小，但是有更多的自由度。您可以更改以下任何内容，但要谨慎。</li><li id="914d" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq oe lx ly lz bi translated">这些技术可以改变<strong class="kx jh">偏差和</strong>方差。<br/> 1。层激活函数(tanh，relu，sigmoid，…) <br/> 2。模型正在学习什么(安，CNN，RNN，KNN…)<br/>3。模型是如何学习的(Adam，SGD，RMSprop，…) <br/> 4。更改其他超参数(学习率、图像大小等)</li></ul><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/b16c27bf7980262aac944dfde84c1bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GZla1ZTQzMRyF-efj5EBAA.png"/></div></div></figure><ul class=""><li id="161a" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq oe lx ly lz bi translated"><strong class="kx jh">添加新特征<br/> </strong>向训练数据添加新特征可以向模型提供更多信息，模型可以利用这些信息进行学习。这可以通过称为<a class="ae jd" href="https://en.wikipedia.org/wiki/Feature_engineering" rel="noopener ugc nofollow" target="_blank">特征工程</a>的过程来完成。在此过程中，您还可以将在开发早期剪切的功能添加回去。</li></ul><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/5841780ea2f7f0afef423881e293a4ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fmHd-MeiuDdoVJ05e-VOog.png"/></div></div></figure><h2 id="cdea" class="ni mg jg bd mh nj nk dn ml nl nm dp mp le nn no mr li np nq mt lm nr ns mv nt bi translated"><a class="ae jd" href="https://d2wvfoqc9gyqzf.cloudfront.net/content/uploads/2018/09/Ng-MLY01-13.pdf#page=53" rel="noopener ugc nofollow" target="_blank">减少差异</a></h2><ul class=""><li id="d32d" class="lr ls jg kx b ky mx lb my le ob li oc lm od lq oe lx ly lz bi translated"><strong class="kx jh">添加更多的数据<br/> </strong>添加更多的数据是最简单的方法，<em class="oj">几乎</em>总是，增加你的模型的性能。添加更多数据的效果可以在Andrej Karpathy的文章<a class="ae jd" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">U<em class="oj">n reasonable Effectiveness of Data</em></a>中看到。此<em class="oj">通常</em>不会<em class="oj"> </em>影响偏差，因此是减少方差的首选方法。</li></ul><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/5a0cab752949b483c90683b6750b6c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4YC2KiL3_LOpSDystN9NKw.png"/></div></div></figure><ul class=""><li id="ce0f" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq oe lx ly lz bi translated"><strong class="kx jh">增加正则化<br/> </strong>增加正则化可以防止模型在数据上过度拟合。虽然这减少了方差，但总会增加偏差。除了减少方差之外，加入正则化还可以产生显著的积极影响。我最喜欢的是使用drop out来实现<a class="ae jd" href="https://medium.com/@ahmdtaha/dropout-as-a-bayesian-approximation-representing-model-uncertainty-in-deep-learning-7a2e49e64a15" rel="noopener">蒙特卡罗辍学</a>。</li></ul><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/1c562c40b3ab6a90342e15d8200e57a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NuSd9oNZMBS_6jJFcazPTA.png"/></div></div></figure><ul class=""><li id="1be1" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq oe lx ly lz bi translated"><strong class="kx jh">减小模型尺寸<br/> </strong>减小模型尺寸有助于减少训练数据的过度拟合。尽管这种技术是最简单的，但它降低了模型学习数据集复杂模式的能力。通过添加正则化通常可以看到相同的结果，因此该方法是更优选的。</li></ul><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/80cdc75e21ded350a823a14315aa59d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjhvYidJGfrud84D6jvfaA.png"/></div></div></figure><ul class=""><li id="9e13" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq oe lx ly lz bi translated"><strong class="kx jh">特征选择<br/> </strong>通过删除不需要的特征来减少数据集的维数是减少模型方差的一个好方法。你可以用<a class="ae jd" href="http://www.iro.umontreal.ca/~pift6080/H09/documents/papers/pca_tutorial.pdf" rel="noopener ugc nofollow" target="_blank">主成分分析(PCA) </a>过滤掉特征或者组合成几个主成分。</li></ul><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/b7f79105fbddb1e9711d8443915f3051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tHtvvxqQCIOKXP5tJQvDag.png"/></div></div></figure><h1 id="790e" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">完整的画面</h1><p id="faeb" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">将所有这些放在一起，你应该能够识别偏差和方差，并知道如何减少它。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/aee65cebdc07359140f0a09a2c6bbff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dPsfWuAvNJm29val0Uek9w.png"/></div></div></figure><h1 id="1618" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">摘要备忘单</h1><h2 id="d46f" class="ni mg jg bd mh nj nk dn ml nl nm dp mp le nn no mr li np nq mt lm nr ns mv nt bi translated">减少偏差</h2><ul class=""><li id="aaf1" class="lr ls jg kx b ky mx lb my le ob li oc lm od lq oe lx ly lz bi translated"><strong class="kx jh">增加模型尺寸</strong></li><li id="45e6" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq oe lx ly lz bi translated"><strong class="kx jh">减少正规化</strong></li><li id="7947" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq oe lx ly lz bi translated"><strong class="kx jh">改变模型架构</strong></li><li id="9d4e" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq oe lx ly lz bi translated"><strong class="kx jh">添加功能</strong></li></ul><h2 id="7ea6" class="ni mg jg bd mh nj nk dn ml nl nm dp mp le nn no mr li np nq mt lm nr ns mv nt bi translated">减少方差</h2><ul class=""><li id="2b5a" class="lr ls jg kx b ky mx lb my le ob li oc lm od lq oe lx ly lz bi translated"><strong class="kx jh">添加更多数据</strong></li><li id="d20c" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq oe lx ly lz bi translated"><strong class="kx jh">减小模型尺寸</strong></li><li id="53be" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq oe lx ly lz bi translated"><strong class="kx jh">添加正规化</strong></li><li id="ea24" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq oe lx ly lz bi translated"><strong class="kx jh">功能选择</strong></li></ul><h1 id="ffbb" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">资源</h1><p id="c11b" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">所有这些概念以及更多内容都包含在<a class="ae jd" href="https://d2wvfoqc9gyqzf.cloudfront.net/content/uploads/2018/09/Ng-MLY01-13.pdf" rel="noopener ugc nofollow" target="_blank">吴恩达的《机器学习的渴望》一书中</a>。它可以免费阅读、打印和分发。我强烈建议你去看看。</p><p id="d5bd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所有的图表都是作者创作的:</p><div class="ip iq gp gr ir op"><a href="https://medium.com/@mikianmusser" rel="noopener follow" target="_blank"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jh gy z fp ou fr fs ov fu fw jf bi translated">米基安·穆塞—中号</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">阅读米基安·穆瑟在媒体上的文章。数据科学家https://mm909.github.io/Mikian/.·天天、米基安·穆塞尔和…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">medium.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd ix op"/></div></div></a></div></div></div>    
</body>
</html>