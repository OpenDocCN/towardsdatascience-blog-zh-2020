<html>
<head>
<title>The Guide to Multi-Tasking with the T5 Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">T5 变压器多任务处理指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b?source=collection_archive---------10-----------------------#2020-06-19">https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b?source=collection_archive---------10-----------------------#2020-06-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0fa7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">T5 变压器可以执行任何 NLP 任务。它可以在同一时间使用同一型号执行多项任务。祝您身体健康</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0bcf3e3cae67f3f55dc1bf6ae02857d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K7eSIQM5HcssfIYf"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@mbeero?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马特·贝罗</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="bc7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">T5(文本到文本迁移转换器)模型是一项大规模研究的产物(<a class="ae ky" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">论文</a>)，旨在探索迁移学习的局限性。它建立在流行的架构之上，如 GPT、伯特和罗伯塔(仅举几个例子)模型，这些模型利用迁移学习取得了令人难以置信的成功。虽然可以对类似 BERT 的模型进行微调以执行各种任务，但体系结构的约束意味着每个模型只能执行一项任务。</p><p id="dbab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，这是通过在 Transformer 模型之上添加一个特定于任务的层来实现的。例如，通过添加具有两个输出神经元(对应于每个类别)的全连接层，可以使 BERT 变换器适用于二进制分类。T5 模型通过将所有的自然语言处理任务重新组织为文本到文本的任务而背离了这一传统。这导致任何 NLP 任务的共享框架作为模型的输入，而模型的输出总是字符串。在二进制分类的例子中，T5 模型将简单地输出该类的字符串表示(即<code class="fe lw lx ly lz b">"0"</code>或<code class="fe lw lx ly lz b">"1"</code>)。</p><p id="bbaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于任何 NLP 任务的输入和输出格式都是相同的，相同的 T5 模型可以被训练来执行多个<em class="lv">任务！为了指定应该执行哪个任务，我们可以简单地在模型的输入前加上一个前缀(字符串)。谷歌人工智能博客文章中的动画(如下所示)展示了这一概念。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/33280ba1458e932920134893d25e0624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*iK5VVgPA2z_WgvwT.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自文章<a class="ae ky" href="http://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">探索 T5 的迁移学习:文本到文本的迁移转换器</a></p></figure><p id="fa4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将使用这种技术来训练一个能够执行 3 个 NLP 任务、二进制分类、多标签分类和回归的 T5 模型。</p><p id="00ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">所有代码也可以在</em><a class="ae ky" href="https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/t5/mixed_tasks" rel="noopener ugc nofollow" target="_blank"><em class="lv">Github</em></a><em class="lv">上找到。</em></p><h1 id="85fc" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">任务说明</h1><h2 id="ef6b" class="mt mc it bd md mu mv dn mh mw mx dp ml li my mz mn lm na nb mp lq nc nd mr ne bi translated">二元分类</h2><p id="c469" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">自然语言处理中二元分类的目标是将给定的文本序列分成两类。在我们的任务中，我们将使用 Yelp 评论数据集将文本的情感分为正面(<code class="fe lw lx ly lz b">"1"</code>)或负面(<code class="fe lw lx ly lz b">"0"</code>)。</p><h2 id="43c0" class="mt mc it bd md mu mv dn mh mw mx dp ml li my mz mn lm na nb mp lq nc nd mr ne bi translated">多标签分类</h2><p id="77c4" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在多标签分类中，给定的文本序列应该用一组预定义标签的正确子集来标记(注意，该子集可以包括<em class="lv">空集</em>和标签本身的完整集)。为此，我们将使用有毒评论数据集，其中每个文本都可以用标签的任何子集来标记<code class="fe lw lx ly lz b">toxic, severe_toxic, obscene, threat, insult, identity_hate</code>。</p><h2 id="cecd" class="mt mc it bd md mu mv dn mh mw mx dp ml li my mz mn lm na nb mp lq nc nd mr ne bi translated">回归</h2><p id="4812" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在回归任务中，目标变量是一个连续值。在我们的任务中，我们将使用 STS-B(语义文本相似性基准)数据集，其目标是预测两个句子的相似性。相似性由<code class="fe lw lx ly lz b">0</code>和<code class="fe lw lx ly lz b">5</code>之间的连续值表示。</p><h1 id="e7ac" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">数据准备</h1><p id="d19b" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">因为我们将使用 3 个数据集，所以我们将它们放在<code class="fe lw lx ly lz b">data</code>目录中的 3 个单独的子目录中。</p><ul class=""><li id="8062" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">data/binary_classification</code></li><li id="7b11" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">data/multilabel_classification</code></li><li id="07f4" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">data/regression</code></li></ul><h2 id="46c7" class="mt mc it bd md mu mv dn mh mw mx dp ml li my mz mn lm na nb mp lq nc nd mr ne bi translated">下载</h2><ol class=""><li id="88b1" class="nk nl it lb b lc nf lf ng li ny lm nz lq oa lu ob nq nr ns bi translated">下载<a class="ae ky" href="https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz" rel="noopener ugc nofollow" target="_blank"> Yelp 评论数据集</a>。</li><li id="a68e" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu ob nq nr ns bi translated">抽出<code class="fe lw lx ly lz b">train.csv</code>和<code class="fe lw lx ly lz b">test.csv</code>至<code class="fe lw lx ly lz b">data/binary_classification</code>。</li><li id="feaa" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu ob nq nr ns bi translated">下载<a class="ae ky" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/" rel="noopener ugc nofollow" target="_blank">有毒评论数据集</a>。</li><li id="b356" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu ob nq nr ns bi translated">将<code class="fe lw lx ly lz b">csv</code>文件解压到<code class="fe lw lx ly lz b">data/multilabel_classification</code>。</li><li id="fae0" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu ob nq nr ns bi translated">下载<a class="ae ky" href="http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark" rel="noopener ugc nofollow" target="_blank"> STS-B 数据集</a>。</li><li id="8f1e" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu ob nq nr ns bi translated">将<code class="fe lw lx ly lz b">csv</code>文件解压到<code class="fe lw lx ly lz b">data/regression</code>。</li></ol><h2 id="5e17" class="mt mc it bd md mu mv dn mh mw mx dp ml li my mz mn lm na nb mp lq nc nd mr ne bi translated">组合数据集</h2><p id="42d8" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">如前所述，T5 模型的输入和输出总是文本。通过使用前缀为<em class="lv">的文本来指定一个特定的任务，让模型知道它应该对输入做什么。</em></p><p id="b7a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单变压器中 T5 模型的输入数据格式反映了这一事实。输入是一个熊猫数据帧，有 3 列— <code class="fe lw lx ly lz b">prefix</code>、<code class="fe lw lx ly lz b">input_text</code>和<code class="fe lw lx ly lz b">target_text</code>。这使得在多个任务上训练模型变得非常容易，因为你只需要改变<code class="fe lw lx ly lz b">prefix</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="986a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的笔记本加载每个数据集，为 T5 对它们进行预处理，最后将它们组合成一个统一的数据帧。</p><p id="0f6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这给了我们一个具有 3 个唯一前缀的数据帧，即<code class="fe lw lx ly lz b">binary classification</code>、<code class="fe lw lx ly lz b">multilabel classification</code>和<code class="fe lw lx ly lz b">similarity</code>。注意前缀本身是相当随意的，重要的是确保每个任务都有自己唯一的前缀。模型的输入将采用以下格式:</p><pre class="kj kk kl km gt oe lz of og aw oh bi"><span id="1c51" class="mt mc it lz b gy oi oj l ok ol">&lt;prefix&gt;: &lt;input_text&gt;</span></pre><p id="15e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"/><code class="fe lw lx ly lz b"><em class="lv">": "</em></code><em class="lv">是训练时自动添加的。</em></p><p id="6831" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其他一些需要注意的事项:</p><ul class=""><li id="58a1" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated">多标签分类任务的输出是预测标签的逗号分隔列表(<code class="fe lw lx ly lz b">toxic, severe_toxic, obscene, threat, insult, identity_hate</code>)。如果没有预测到标签，输出应该是<code class="fe lw lx ly lz b">clean</code>。</li><li id="f90d" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">相似性任务的<code class="fe lw lx ly lz b">input_text</code>包括两个句子，如下例所示；<br/> <code class="fe lw lx ly lz b">sentence1: A man plays the guitar. sentence2: The man sang and played his guitar.</code></li><li id="b262" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">相似性任务的输出是一个介于 0.0 和 5.0 之间的数字(字符串)，增量为 0.2。(如<code class="fe lw lx ly lz b">0.0</code>、<code class="fe lw lx ly lz b">0.4</code>、<code class="fe lw lx ly lz b">3.0</code>、<code class="fe lw lx ly lz b">5.0</code>)。这与 T5 论文作者使用的格式相同。</li></ul><p id="4106" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从不同输入和输出的表示方式可以看出，T5 模型的文本到文本方法在表示各种任务和我们可以执行的实际任务方面为我们提供了很大的灵活性。</p><p id="bb4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">例如；</em></p><div class="om on gp gr oo op"><a rel="noopener follow" target="_blank" href="/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd iu gy z fp ou fr fs ov fu fw is bi translated">问正确的问题:在新任务中训练 T5 变压器模型</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">T5 转换器将任何 NLP 任务构造为文本到文本的任务，使其能够轻松学习新任务。让我们来教…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd ks op"/></div></div></a></div><p id="b2e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">唯一的限制就是想象力！<em class="lv">(嗯，想象力和计算资源，但那是另一回事了)</em>😅</p><p id="d257" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回到数据，运行笔记本应该会给您一个<code class="fe lw lx ly lz b">train.tsv</code>和一个<code class="fe lw lx ly lz b">eval.tsv</code>文件，我们将在下一节中使用它们来训练我们的模型！</p><h1 id="4af6" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">设置</h1><p id="4144" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们将使用<a class="ae ky" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank">简单变形金刚</a>库(基于拥抱脸<a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>)来训练 T5 模型。</p><p id="6c6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面给出的说明将安装所有的要求。</p><ol class=""><li id="4f60" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu ob nq nr ns bi translated">从<a class="ae ky" href="https://www.anaconda.com/distribution/" rel="noopener ugc nofollow" target="_blank">这里</a>安装 Anaconda 或 Miniconda 包管理器。</li><li id="92f0" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu ob nq nr ns bi translated">创建新的虚拟环境并安装软件包。<br/><code class="fe lw lx ly lz b">conda create -n simpletransformers python</code><br/><code class="fe lw lx ly lz b">conda activate simpletransformers</code><br/>T2】</li><li id="6534" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu ob nq nr ns bi translated">安装简单变压器。<br/>T3】</li></ol><p id="637f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">参见安装</em> <a class="ae ky" href="https://simpletransformers.ai/docs/installation/#installation-steps" rel="noopener ugc nofollow" target="_blank"> <em class="lv">文档</em> </a></p><h1 id="1239" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">培训 T5 模型</h1><p id="f54f" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">和往常一样，用简单的变形金刚训练模型非常简单。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="a90d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里使用的大多数参数都相当标准。</p><ul class=""><li id="b1cd" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">max_seq_length</code>:选择时不截断大多数样本。增加序列长度会极大地影响模型的内存消耗，因此通常最好尽可能地缩短序列长度(最好不要截断输入序列)。</li><li id="2dfd" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">train_batch_size</code>:越大越好(只要适合你的 GPU)</li><li id="421e" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">eval_batch_size</code>:同<code class="fe lw lx ly lz b">train_batch_size</code></li><li id="80cf" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">num_train_epochs</code>:超过 1 个历元的训练可能会提高模型的性能，但它显然也会增加训练时间(在 RTX 泰坦上每个历元大约 7 个小时)。</li><li id="5b1e" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">我们将根据测试数据定期测试模型，看看它是如何学习的。</li><li id="a01a" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">evaluate_during_training_steps</code>:前述测试模型的时期。</li><li id="9331" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">evaluate_during_training_verbose</code>:测试完成后显示结果。</li><li id="e6e7" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">use_multiprocessing</code>:使用多处理大大减少了标记化所花费的时间(在训练开始前完成)，但是，这目前会导致 T5 实现的问题。所以，暂时没有多重处理。😢</li><li id="21eb" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">fp16</code> : FP16 或混合精度训练减少了训练模型的内存消耗(意味着更大的批量是可能的)。不幸的是，<code class="fe lw lx ly lz b">fp16</code>训练目前与 T5 不稳定，所以也被关闭了。</li><li id="5001" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">save_steps</code>:设置为<code class="fe lw lx ly lz b">-1</code>表示不保存检查点。</li><li id="fb41" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">save_eval_checkpoints</code>:默认情况下，在训练过程中进行评估时，会保存一个模型检查点。因为这个实验只是为了演示，所以我们也不要浪费空间来保存这些检查点。</li><li id="32d8" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">我们只有一个纪元，所以没有。也不需要这个。</li><li id="5ff0" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">reprocess_input_data</code>:控制是否从缓存加载特征(保存到磁盘)或是否对输入序列再次进行标记化。只有在多次运行时才真正重要。</li><li id="bedf" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">overwrite_output_dir</code>:这将覆盖任何先前保存的模型，如果它们在相同的输出目录中。</li><li id="094a" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><code class="fe lw lx ly lz b">wandb_project</code>:用于培训进度的可视化。</li></ul><p id="4107" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">说到可视化，你可以在这里查看我的训练进度<a class="ae ky" href="https://app.wandb.ai/thilina/T5%20mixed%20tasks%20-%20Binary,%20Multi-Label,%20Regression?workspace=user-thilina" rel="noopener ugc nofollow" target="_blank">。为他们令人敬畏的图书馆大声喊出来！</a></p><h1 id="5670" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">测试 T5 型号</h1><p id="9f37" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">考虑到我们正在处理多个任务的事实，使用合适的度量来评估每个任务是一个好主意。考虑到这一点，我们将使用以下指标:</p><ul class=""><li id="0a39" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated">二元分类:<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" rel="noopener ugc nofollow" target="_blank"> F1 得分</a>和<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html" rel="noopener ugc nofollow" target="_blank">准确度得分</a></li><li id="5866" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">多标签分类:F1 分数(拥抱脸队指标实现)和精确匹配(拥抱脸队指标实现)</li><li id="007c" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">相似度:<a class="ae ky" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html" rel="noopener ugc nofollow" target="_blank">皮尔逊相关系数</a>和<a class="ae ky" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html" rel="noopener ugc nofollow" target="_blank">斯皮尔曼相关</a></li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="b575" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，在准备数据时，在<code class="fe lw lx ly lz b">prefix</code>和<code class="fe lw lx ly lz b">input_text</code>之间插入了一个<code class="fe lw lx ly lz b">": “</code>。这是在训练时自动完成的，但需要手动处理以进行预测。</p><p id="ed85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">如果你想了解更多关于解码论点的内容(</em> <code class="fe lw lx ly lz b"><em class="lv">num_beams</em></code> <em class="lv">，</em> <code class="fe lw lx ly lz b"><em class="lv">do_sample</em></code> <em class="lv">，</em> <code class="fe lw lx ly lz b"><em class="lv">max_length</em></code> <em class="lv">，</em> <code class="fe lw lx ly lz b"><em class="lv">top_k</em></code> <em class="lv">，</em> <code class="fe lw lx ly lz b"><em class="lv">top_p</em></code> <em class="lv">)，请参考本文</em><a class="ae ky" rel="noopener" target="_blank" href="/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c"><em class="lv"/></a><em class="lv">。</em></p><p id="7a6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是时候看看我们的模型做得如何了！</p><pre class="kj kk kl km gt oe lz of og aw oh bi"><span id="ffca" class="mt mc it lz b gy oi oj l ok ol">-----------------------------------<br/>Results: <br/>Scores for binary classification:<br/>F1 score: 0.96044512420231<br/>Accuracy Score: 0.9605263157894737</span><span id="ab85" class="mt mc it lz b gy pe oj l ok ol">Scores for multilabel classification:<br/>F1 score: 0.923048001002632<br/>Exact matches: 0.923048001002632</span><span id="5365" class="mt mc it lz b gy pe oj l ok ol">Scores for similarity:<br/>Pearson Correlation: 0.8673017763553101<br/>Spearman Correlation: 0.8644328787107548</span></pre><p id="b586" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管在 3 个独立的任务上进行训练，该模型在每个任务上表现得相当好！在下一节中，我们将快速了解一下如何进一步提高模型的性能。</p><h1 id="e393" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">结束语</h1><h2 id="da5d" class="mt mc it bd md mu mv dn mh mw mx dp ml li my mz mn lm na nb mp lq nc nd mr ne bi translated">可能的改进</h2><p id="c56f" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">混合任务时出现的一个潜在问题是用于每个任务的数据集大小之间的差异。通过查看训练样本计数，我们可以在我们的数据集中看到这个问题。</p><pre class="kj kk kl km gt oe lz of og aw oh bi"><span id="c93f" class="mt mc it lz b gy oi oj l ok ol">binary classification        560000<br/>multilabel classification    143613<br/>similarity                     5702</span></pre><p id="0c07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集在本质上是不平衡的，任务<code class="fe lw lx ly lz b">similarity</code>的困境看起来尤其可怕！这可以在评估分数中清楚地看到，其中<code class="fe lw lx ly lz b">similarity</code>任务落后于其他任务(尽管重要的是要注意，我们<strong class="lb iu">而不是</strong>在任务之间查看相同的指标)。</p><p id="3e5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对这个问题的一个可能的补救方法是<em class="lv">对<code class="fe lw lx ly lz b">similarity</code>任务进行过采样</em>，以便模型。</p><p id="104c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除此之外，增加训练历元的数量(以及调整其他超参数)也有可能改进模型。</p><p id="f478" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，调整解码参数也可以得到更好的结果。</p><h2 id="ef8b" class="mt mc it bd md mu mv dn mh mw mx dp ml li my mz mn lm na nb mp lq nc nd mr ne bi translated">包扎</h2><p id="4f1e" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">T5 模型的文本到文本格式为将 Transformers 和 NLP 应用于各种各样的任务铺平了道路，几乎不需要定制。T5 型号性能强劲，即使在使用同一型号执行多项任务时也是如此！</p><p id="cbe0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望在不久的将来，这将导致许多创新的应用。</p><h1 id="af70" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">参考</h1><ol class=""><li id="57d7" class="nk nl it lb b lc nf lf ng li ny lm nz lq oa lu ob nq nr ns bi translated">用统一的文本到文本转换器探索迁移学习的极限—<a class="ae ky" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1910.10683</a></li><li id="9305" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu ob nq nr ns bi translated">谷歌人工智能博客—<a class="ae ky" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">https://AI . Google Blog . com/2020/02/exploring-transfer-learning-with-t5 . html</a></li></ol></div></div>    
</body>
</html>