<html>
<head>
<title>Let’s give some ‘Attention’ to Summarising Texts..</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们对摘要文本给予一些“关注”..</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lets-give-some-attention-to-summarising-texts-d0af2c4061d1?source=collection_archive---------39-----------------------#2020-05-09">https://towardsdatascience.com/lets-give-some-attention-to-summarising-texts-d0af2c4061d1?source=collection_archive---------39-----------------------#2020-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2f20" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用LSTM编解码器模型的文本摘要。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cfb7898f6e43a75cbb1d3c67dba15f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8A4oKl5er9kxjUDY"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">罗曼·维涅斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="5efd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文本摘要是自然语言处理领域中最具挑战性和最有趣的问题之一。这是一个从多种文本资源(如书籍、新闻文章、博客帖子、研究论文、电子邮件和推文)中生成简明而有意义的文本摘要的过程。现在，随着大量文本语料库的可用性，摘要是一项更加重要的任务。</p><h1 id="b7a9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">那么有哪些不同的方法呢？</h1><h2 id="adfd" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">摘录摘要</h2><p id="1fca" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">这些方法依赖于从一段文本中提取几个部分，如短语和句子，并将它们堆叠在一起以创建摘要。因此，识别用于摘要的正确句子在抽取方法中是至关重要的。我们用一个例子来理解这个。</p><p id="5a4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">正文:</strong> <em class="ne">梅西和c罗的战绩都比他们的同行好。在所有比赛中表现出色。他们被认为是我们这一代中的佼佼者。</em></p><p id="4a66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">摘录总结:</strong> <em class="ne">梅西和c罗的战绩都比同行好。我们这一代最好的。</em></p><p id="80e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你在上面看到的，粗体字已经被提取出来并连接在一起形成了一个摘要——尽管有时候这个摘要在语法上很奇怪。</p><h2 id="c686" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated"><strong class="ak">抽象概括</strong></h2><p id="37e1" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">这些方法使用先进的NLP技术来生成全新的摘要。本摘要的某些部分甚至可能不会出现在原文中。我们用一个例子来理解这个。</p><p id="2cb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">正文:</strong> <em class="ne">梅西和c罗的战绩都比同行好。在所有比赛中表现出色。他们被认为是我们这一代中的佼佼者。</em></p><p id="982a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">抽象总结:</strong> <em class="ne">梅西和c罗的战绩都比同行好，所以被认为是我们这一代的佼佼者。</em></p><p id="e784" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">抽象文本摘要算法创建新的短语和句子，从原始文本中传递最有用的信息——就像人类一样。</p><p id="5a71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将关注<strong class="lb iu">抽象概括</strong>技术，并且我们将使用<strong class="lb iu">编码器-解码器架构</strong>来解决这个问题。</p><h1 id="11ab" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是编码器-解码器架构？</h1><p id="b2e8" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">常用的序列对序列模型(编码器-解码器)的总体结构如下所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/0879762464c7c8ca9cd5eae25f191449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lF1bEEE1VvSF4xRy-zLI9A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基本的编码器-解码器架构</p></figure><p id="b88a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型由三部分组成:<strong class="lb iu">E<em class="ne">n编码器、中间向量和解码器。</em>T3】</strong></p><h2 id="5fc2" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated"><strong class="ak">E<em class="ng">n编码器</em>T7】</strong></h2><ul class=""><li id="ef34" class="nh ni it lb b lc mz lf na li nj lm nk lq nl lu nm nn no np bi translated">编码器基本上由一系列LSTM/GRU单元组成(请阅读<a class="ae ky" href="https://keras.io/api/layers/recurrent_layers/lstm/" rel="noopener ugc nofollow" target="_blank"> LSTM </a> / <a class="ae ky" href="https://keras.io/api/layers/recurrent_layers/gru/" rel="noopener ugc nofollow" target="_blank"> GRU </a>文档以更好地理解该架构)。</li><li id="af7b" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">编码器接收输入序列，并将信息封装为内部状态向量。</li><li id="3998" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">解码器使用编码器的输出和内部状态。</li><li id="5693" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">在我们的文本摘要问题中，输入序列是来自文本的所有单词的集合，需要对其进行摘要。每个单词表示为<em class="ne"> x_i </em>，其中<em class="ne"> i </em>是该单词的顺序。</li></ul><h2 id="70d4" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">中间(编码器)向量</h2><ul class=""><li id="8235" class="nh ni it lb b lc mz lf na li nj lm nk lq nl lu nm nn no np bi translated">这是从模型的编码器部分产生的最终隐藏状态。它是用上面的公式计算的。</li><li id="dfac" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">该向量旨在封装所有输入元素的信息，以帮助解码器做出准确的预测。</li><li id="e59d" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">它充当模型解码器部分的初始隐藏状态。</li></ul><h2 id="de24" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">解码器</h2><ul class=""><li id="ff85" class="nh ni it lb b lc mz lf na li nj lm nk lq nl lu nm nn no np bi translated">几个循环单元的堆栈，每个循环单元在时间步长<em class="ne"> t </em>预测一个输出<em class="ne"> y_t </em>。</li><li id="5f4a" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">每个递归单元接受来自前一个单元的隐藏状态，并产生和输出它自己的隐藏状态。</li><li id="0395" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">在摘要问题中，输出序列是来自摘要文本的所有单词的集合。每个单词都表示为<em class="ne"> y_i </em>，其中<em class="ne"> i </em>是该单词的顺序。</li><li id="9840" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">任何隐藏状态<em class="ne"> h_i </em>都是使用以下公式计算的<em class="ne"> </em>:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/599e37e482b6d8ad96b66440d6d1bb8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*n4q-OxH2i1lykzCE"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/8b9b39e94cf06b5dde677682d5d966c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KQazpbnAprdIbCzr.png"/></div></div></figure><p id="8722" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，我们只是使用前一个隐藏状态来计算下一个。</p><ul class=""><li id="97d7" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">使用以下公式计算时间步长<em class="ne"> t </em>的输出<em class="ne"> y_t </em>:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/ccdd36f430d46b93a21753cec1ace331.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*GHd6nTa_ZiWC34b3"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/6a839ccee600a1ad08646b01fd88fbfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AasJEbfQxNeMi_ii.png"/></div></div></figure><p id="a20e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用当前时间步长的隐藏状态以及相应的权重W(S)来计算输出。<a class="ae ky" href="https://www.youtube.com/watch?v=LLux1SW--oM" rel="noopener ugc nofollow" target="_blank"> Softmax </a>用于创建一个概率向量，该向量将帮助我们确定最终输出(如问答问题中的单词)。</p><h1 id="83be" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">请注意:</h1><p id="d981" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">首先，我们需要明白什么是注意力。</p><blockquote class="ob oc od"><p id="2187" class="kz la ne lb b lc ld ju le lf lg jx lh oe lj lk ll of ln lo lp og lr ls lt lu im bi translated"><em class="it">在时间步长</em> <strong class="lb iu"> <em class="it"> t </em> </strong> <em class="it">生成一个单词，我们需要对输入序列中的每一个单词投入多大的注意力？这是注意力机制概念背后的关键直觉。</em></p></blockquote><p id="bba3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用一个简单的例子来理解这一点:</p><p id="e9a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">问题</strong>:近十年来，<strong class="lb iu"> <em class="ne">谁是最好的</em></strong><strong class="lb iu"><em class="ne">足球运动员</em> </strong>？</p><p id="7521" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">回答</strong> : <strong class="lb iu"> <em class="ne">梅西</em> </strong>是最好的<strong class="lb iu"> <em class="ne">球员</em> </strong>。</p><p id="95a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，问题<strong class="lb iu">中的第五个字</strong>与谁相关<strong class="lb iu">莱昂内尔</strong> <strong class="lb iu">梅西</strong>和第九个字<strong class="lb iu">足球运动员</strong>与第六个字<strong class="lb iu">球员</strong>相关。</p><p id="8f79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们可以增加源序列中产生目标序列的特定部分的重要性，而不是查看源序列中的所有单词。这是注意力机制背后的基本思想。</p><p id="670d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据所关注的上下文向量的导出方式，存在2种不同类别的注意机制:</p><h2 id="b1c1" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">全球关注</h2><p id="4ada" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">这里，注意力放在所有的源位置上。换句话说，<strong class="lb iu">编码器的所有隐藏状态都被考虑用于导出关注上下文向量。</strong>在这个总结任务中，我们将使用<strong class="lb iu">全球注意力。</strong></p><h2 id="a0c7" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">当地的关注</h2><p id="5ea2" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">这里，注意力仅放在几个源位置上。<strong class="lb iu">仅考虑编码器的几个隐藏状态来导出关注上下文向量。</strong></p><p id="070d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们了解这种<strong class="lb iu">注意力</strong>到底是如何运作的:</p><ul class=""><li id="4d77" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">编码器输出源序列中每个时间步长<strong class="lb iu"> j </strong>的隐藏状态(<strong class="lb iu"> hj </strong></li><li id="ab02" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">类似地，解码器输出目标序列中每个时间步长<strong class="lb iu"> i </strong>的隐藏状态(<strong class="lb iu"> si </strong></li><li id="00f3" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">我们计算一个称为<strong class="lb iu">对齐分数(eij </strong>)的分数，基于该分数，使用分数函数将源单词与目标单词对齐。使用score函数从源隐藏状态<strong class="lb iu"> hj </strong>和目标隐藏状态<strong class="lb iu"> si </strong>计算校准分数。这由下式给出:</li></ul><blockquote class="oh"><p id="82e0" class="oi oj it bd ok ol om on oo op oq lu dk translated"><strong class="ak"> eij=得分(si，hj ) </strong></p></blockquote><p id="4866" class="pw-post-body-paragraph kz la it lb b lc or ju le lf os jx lh li ot lk ll lm ou lo lp lq ov ls lt lu im bi translated">其中<strong class="lb iu"> eij </strong>表示目标时间步长<strong class="lb iu"> i </strong>和源时间步长<strong class="lb iu">j</strong>的对齐分数</p><ul class=""><li id="e1ec" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">我们使用softmax函数归一化比对分数，以检索注意力权重(<strong class="lb iu"> aij </strong>):</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/63588c65d419dd919da2538a3f7fbd42.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/format:webp/0*091bZxxhzOad5uwk.jpg"/></div></figure><ul class=""><li id="38ba" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">我们计算关注权重<strong class="lb iu"> aij </strong>和编码器隐藏状态<strong class="lb iu"> hj </strong>的乘积的线性和，以产生关注上下文向量(<strong class="lb iu"> Ci </strong>):</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/9b6f524d74db02b2750dcb2bcb7a2b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:156/format:webp/0*pTvDNrDBhC8gKLgt.jpg"/></div></figure><ul class=""><li id="ca3d" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">关注的上下文向量和解码器在时间步<strong class="lb iu"> i </strong>的目标隐藏状态被连接以产生关注的隐藏向量<strong class="lb iu"> Si，</strong>其中，<strong class="lb iu"> </strong> <em class="ne"> Si=连接(【Si；Ci]) </em></li><li id="4dc8" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">然后将关注的隐向量<strong class="lb iu"> Si </strong>送入稠密层，产生<strong class="lb iu"> yi，</strong> <em class="ne"> yi=稠密(Si)。</em></li></ul><p id="a918" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们借助一个例子来理解以上的注意机制步骤。考虑源文本序列为[x1，x2，x3，x4]，目标摘要序列为[y1，y2]。</p><ul class=""><li id="8796" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">编码器读取整个源序列，并输出每个时间步长的隐藏状态，比如说<strong class="lb iu"> h1、h2、h3、h4 </strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/60ebc8d1ab2b04fa6205e6d06a39bdfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/0*A7WGrIpv5Je2mh1c.jpg"/></div></figure><ul class=""><li id="766c" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">解码器读取偏移了一个时间步长的整个目标序列，并输出每个时间步长的隐藏状态，例如<strong class="lb iu"> s1、s2、s3 </strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/8ff2035c3d21c9d4a54da6b8f1371c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/0*4sI3TMFknBAKxpDC.jpg"/></div></figure><p id="5926" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">目标时间步长i=1 </strong></p><ul class=""><li id="0fb8" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">使用score函数从源隐藏状态<strong class="lb iu"> hi </strong>和目标隐藏状态<strong class="lb iu"> s1 </strong>计算校准分数<strong class="lb iu"> e1j </strong>:</li></ul><pre class="kj kk kl km gt pa pb pc pd aw pe bi"><span id="a06d" class="mn lw it pb b gy pf pg l ph pi">e11= score(s1, h1)<br/>e12= score(s1, h2)<br/>e13= score(s1, h3)<br/>e14= score(s1, h4)</span></pre><ul class=""><li id="027d" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">使用softmax标准化比对分数<strong class="lb iu"> e1j </strong>产生注意力权重<strong class="lb iu"> a1j </strong>:</li></ul><pre class="kj kk kl km gt pa pb pc pd aw pe bi"><span id="ba10" class="mn lw it pb b gy pf pg l ph pi">a11= exp(e11)/((exp(e11)+exp(e12)+exp(e13)+exp(e14))<br/>a12= exp(e12)/(exp(e11)+exp(e12)+exp(e13)+exp(e14))<br/>a13= exp(e13)/(exp(e11)+exp(e12)+exp(e13)+exp(e14))<br/>a14= exp(e14)/(exp(e11)+exp(e12)+exp(e13)+exp(e14))</span></pre><ul class=""><li id="9908" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">参与上下文向量<strong class="lb iu"> C1 </strong>由编码器隐藏状态<strong class="lb iu"> hj </strong>和对齐分数<strong class="lb iu"> a1j </strong>的乘积的线性和得到:</li></ul><pre class="kj kk kl km gt pa pb pc pd aw pe bi"><span id="62d3" class="mn lw it pb b gy pf pg l ph pi">C1= h1 * a11 + h2 * a12 + h3 * a13 + h4 * a14</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/e53219cf44fab391b0c5cae58f8247dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/0*5zrqRsG8h884Z3I7.jpg"/></div></figure><ul class=""><li id="6383" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">关联上下文向量<strong class="lb iu"> C1 </strong>和目标隐藏状态<strong class="lb iu"> s1 </strong>被连接以产生关联隐藏向量<strong class="lb iu"> S1 </strong></li></ul><pre class="kj kk kl km gt pa pb pc pd aw pe bi"><span id="21e2" class="mn lw it pb b gy pf pg l ph pi">S1= concatenate([s1; C1])</span></pre><ul class=""><li id="a7fb" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">注意力隐藏向量<strong class="lb iu"> S1 </strong>然后被送入密集层产生<strong class="lb iu"> y1 </strong></li></ul><pre class="kj kk kl km gt pa pb pc pd aw pe bi"><span id="a390" class="mn lw it pb b gy pf pg l ph pi">y1= dense(S1)</span></pre><p id="ce90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，我们可以计算Y2。</p><p id="a968" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Keras没有提供注意力层，所以我们可以自己写或者使用别人提供的注意力层。这里我们使用<a class="ae ky" href="https://github.com/thushv89/attention_keras/blob/master/layers/attention.py" rel="noopener ugc nofollow" target="_blank">这个</a>实现关注层。</p><h1 id="aa32" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">实施时间:</h1><p id="88a7" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">这个汇总任务的全部代码可以在这里找到<a class="ae ky" href="https://gist.github.com/sayakmisra/6133be0554ce916d8cae4cdb83d475d8" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="5706" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">显示数据:</h2><p id="a86c" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我们将在本文中使用<a class="ae ky" href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener ugc nofollow" target="_blank">亚马逊食品评论数据集</a>。让我们看一下数据的快照:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/49c224ee0f94ea35caeb14ed1b6798ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2NjYxonoimL3meWezKjxLA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集的快照。</p></figure><h2 id="fc7e" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">清理数据:</h2><p id="39bb" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我们首先需要清理我们的数据，因此我们需要遵循的步骤是:</p><ul class=""><li id="c0cf" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated">将所有内容转换为小写</li><li id="4d43" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">删除HTML标签</li><li id="03a1" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">收缩映射</li><li id="8919" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">移除(' s)</li><li id="40ef" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">删除括号( )内的任何文本</li><li id="8f86" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">消除标点符号和特殊字符</li><li id="86b4" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">删除停用词。</li><li id="3510" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">删除短词</li></ul><h2 id="aba3" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">数据的分布:</h2><p id="992c" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">然后，我们将分析评论和摘要的长度，从而对文章 t <strong class="lb iu">长度的<strong class="lb iu">分布有一个总体的了解。</strong>这将帮助我们确定序列的最大长度。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/6121acff73e078ef143a2990f0a06333.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*-vzC5pn_S2XtWqkXZ7TynQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">x轴:字数，Y轴:句子数量。</p></figure><h2 id="6df0" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">将数据标记化:</h2><p id="826a" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">记号赋予器构建词汇表并将单词序列转换成整数序列。我们将使用Keras的分词器对句子进行分词。</p><h2 id="1a31" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">模型构建:</h2><p id="3ba3" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我们终于进入了模型构建阶段。但在此之前，我们需要熟悉一些术语，这些术语是构建模型之前所必需的。</p><ul class=""><li id="6e57" class="nh ni it lb b lc ld lf lg li nx lm ny lq nz lu nm nn no np bi translated"><strong class="lb iu"> Return Sequences = True: </strong>当Return Sequences参数设置为<strong class="lb iu"> True </strong>时，LSTM为每个时间步长产生隐藏状态和单元格状态</li><li id="b27e" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><strong class="lb iu">返回状态=真:</strong>当返回状态= <strong class="lb iu">真</strong>时，LSTM只产生最后一个时间步长的隐藏状态和单元格状态</li><li id="5ddf" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><strong class="lb iu">初始状态:</strong>用于初始化第一个时间步长的LSTM内部状态</li><li id="4707" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><strong class="lb iu">堆叠的LSTM: </strong>堆叠的LSTM有多层LSTM堆叠在一起。这导致序列的更好的表示。我鼓励你尝试将LSTM的多层叠加在一起(这是学习的好方法)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/1e5bd251dd59285f39439643202598c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tY2nh0pMrKfIQNt9w07UMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型总结。</p></figure><h2 id="7ed0" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">训练和提前停止:</h2><p id="678b" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">这是在训练期间损失是如何减少的，我们可以推断在纪元10之后验证损失略有增加。因此，我们将在这个时代之后停止训练模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/81e8e5106ec73a8d6cc701872bb012c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*PEXg_f6u1VnCgZfoYTn4qQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">培训和测试损失</p></figure><h2 id="f9bc" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated"><strong class="ak">推论:</strong></h2><p id="c1cf" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">现在，我们将为编码器和解码器设置推理。这里编码器和解码器将一起工作，产生一个摘要。解码器将被堆叠在编码器之上，解码器的输出将再次被馈送到解码器以产生下一个字。</p><h2 id="fa48" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">测试:</h2><p id="a0af" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在这里，最后，我们可以用我们的定制输入来测试我们的模型。</p><pre class="kj kk kl km gt pa pb pc pd aw pe bi"><span id="89ff" class="mn lw it pb b gy pf pg l ph pi"><strong class="pb iu">Review</strong>: right quantity japanese green tea able either drink one sitting save later tastes great sweet  <br/><strong class="pb iu">Original summary:</strong> great japanese product  <br/><strong class="pb iu">Predicted summary:</strong>  great tea</span><span id="2303" class="mn lw it pb b gy po pg l ph pi"><strong class="pb iu">Review:</strong> love body wash smells nice works great feels great skin add fact subscribe save deal great value sold<br/><strong class="pb iu">Original summary:</strong> great product and value  <br/><strong class="pb iu">Predicted summary:</strong>  great product</span><span id="6bcb" class="mn lw it pb b gy po pg l ph pi"><strong class="pb iu">Review</strong>: look like picture include items pictured buy gift recipient disappointed  <br/><strong class="pb iu">Original summary:</strong> very disappointed  <br/><strong class="pb iu">Predicted summary:</strong>  not what expected<br/></span></pre><p id="5dc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是这篇文章的笔记本。</p><h1 id="b340" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">包装它</h1><p id="f76e" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在本文中，我们已经看到了如何使用序列到序列模型来总结文本。我们可以通过增加数据集，使用<strong class="lb iu">双向LSTM </strong>，使用<strong class="lb iu">波束搜索策略</strong>等来进一步改进这个模型。</p><p id="162d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的<a class="ae ky" rel="noopener" target="_blank" href="/text-summarization-with-glove-embeddings-1e969ef9a452">下一个故事</a>中，我们将看到如何用迁移学习来实现它。我们将使用预训练的<strong class="lb iu"> GloVe </strong>单词嵌入，并观察我们的模型如何表现，以及它是否能够通过预训练的嵌入更好地理解语义。在那里见。</p><h1 id="c20b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><ol class=""><li id="e130" class="nh ni it lb b lc mz lf na li nj lm nk lq nl lu pp nn no np bi translated"><a class="ae ky" href="https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2019/06/comprehensive-guide-text-summary-using-deep-learning-python/</a></li><li id="c94a" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu pp nn no np bi translated"><a class="ae ky" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank">https://blog . keras . io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras . html</a></li><li id="db49" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu pp nn no np bi translated">【https://www.kaggle.com/snap/amazon-fine-food-reviews T4】</li></ol></div></div>    
</body>
</html>