<html>
<head>
<title>Cross-Entropy Method Performance Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">交叉熵方法性能分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cross-entropy-method-performance-analysis-161a5faef5fc?source=collection_archive---------37-----------------------#2020-06-08">https://towardsdatascience.com/cross-entropy-method-performance-analysis-161a5faef5fc?source=collection_archive---------37-----------------------#2020-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9e5e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/deep-r-l-explained" rel="noopener" target="_blank">深度强化学习讲解— 07 </a></h2><div class=""/><div class=""><h2 id="1f8e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">交叉熵训练循环的实现</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fa0f6596bcb8f7d913d0517d5a751d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDhcpkgdzkisARJU93i5xw.png"/></div></div></figure><p id="c0b3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在这篇文章中，我们将详细描述交叉熵方法的训练循环，这是我们在上一篇文章中跳过的，同时看看我们如何在考虑更复杂的神经网络的情况下改进代理的学习。此外，我们将介绍该方法的改进变体，它为训练过程的多次迭代保留“精英”片段。最后，我们将展示交叉熵方法激励其他方法的局限性。</p><h1 id="594d" class="lz ma it bd mb mc md me mf mg mh mi mj ki mk kj ml kl mm km mn ko mo kp mp mq bi translated">1.训练循环概述</h1><p id="d417" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">接下来，我们将详细介绍构成我们在上一篇文章中介绍的训练循环的代码。</p><blockquote class="mw mx my"><p id="270b" class="ld le mz lf b lg lh kd li lj lk kg ll na ln lo lp nb lr ls lt nc lv lw lx ly im bi translated"><em class="it">本帖的</em> <a class="ae nd" href="https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="it">完整代码可以在 GitHub </em> </a> <em class="it">和</em> <a class="ae nd" href="https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="it">上找到，使用此链接</em> </a> <em class="it">可以作为 Colab 谷歌笔记本运行。</em></p></blockquote><h2 id="aa7e" class="ne ma it bd mb nf ng dn mf nh ni dp mj lm nj nk ml lq nl nm mn lu nn no mp iz bi translated">主要变量</h2><p id="374a" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">代码从定义方法的主要参数开始。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="1b37" class="ne ma it nq b gy nu nv l nw nx">BATCH_SIZE = 100<br/>GAMMA = 0.9</span><span id="2e28" class="ne ma it nq b gy ny nv l nw nx">PERCENTILE = 30<br/>REWARD_GOAL = 0.8</span></pre><h2 id="b2c5" class="ne ma it bd mb nf ng dn mf nh ni dp mj lm nj nk ml lq nl nm mn lu nn no mp iz bi translated">助手类</h2><p id="5260" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">我们建议使用两个助手类来简化代码的可解释性:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="2916" class="ne ma it nq b gy nu nv l nw nx">from collections import namedtuple</span><span id="a04b" class="ne ma it nq b gy ny nv l nw nx">Episode = namedtuple(‘Episode’, field_names=[‘reward’, ‘steps’])</span><span id="8f44" class="ne ma it nq b gy ny nv l nw nx">EpisodeStep = namedtuple(‘EpisodeStep’, <br/>                          field_names=[‘observation’, ‘action’])</span></pre><p id="9e9d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这里我们将定义两个助手类，它们被命名为标准库中的<code class="fe nz oa ob nq b">collections</code>包中的元组:</p><ul class=""><li id="a282" class="oc od it lf b lg lh lj lk lm oe lq of lu og ly oh oi oj ok bi translated"><code class="fe nz oa ob nq b">EpisodeStep</code>:这将用来表示我们的代理在剧集中做的一个单独的步骤，它存储了从环境中观察到的<strong class="lf jd">状态</strong>以及代理完成了什么<strong class="lf jd">动作</strong>。奖励没有被记录，因为它总是<code class="fe nz oa ob nq b">0.0</code>，除了最后一个过渡。请记住，我们将使用“精英”剧集中的剧集步骤作为训练数据。</li><li id="c4aa" class="oc od it lf b lg ol lj om lm on lq oo lu op ly oh oi oj ok bi translated"><code class="fe nz oa ob nq b">Episode</code>:这是储存了总折扣奖励和 EpisodeStep 集合的单集。</li></ul><h2 id="efd2" class="ne ma it bd mb nf ng dn mf nh ni dp mj lm nj nk ml lq nl nm mn lu nn no mp iz bi translated">变量的初始化</h2><p id="f530" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">此时，我们将在训练循环中使用的一组变量被初始化。我们将按照循环中的要求逐一介绍:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="7612" class="ne ma it nq b gy nu nv l nw nx">iter_no = 0</span><span id="5aee" class="ne ma it nq b gy ny nv l nw nx">reward_mean = 0</span><span id="6914" class="ne ma it nq b gy ny nv l nw nx">full_batch = []<br/>batch = []</span><span id="e277" class="ne ma it nq b gy ny nv l nw nx">episode_steps = []<br/>episode_reward = 0.0</span><span id="c8e1" class="ne ma it nq b gy ny nv l nw nx">state = env.reset()</span></pre><h2 id="a66f" class="ne ma it bd mb nf ng dn mf nh ni dp mj lm nj nk ml lq nl nm mn lu nn no mp iz bi translated">训练循环</h2><p id="6928" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">我们在上一篇文章中了解到，实现交叉熵算法的代理的训练循环重复 4 个主要步骤，直到我们对结果满意为止:</p><p id="12df" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">1 —播放<em class="mz"> N </em>集数</p><p id="15bf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">2-计算每集的预期回报，并确定回报界限</p><p id="19e9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">3-丢弃所有回报低于界限的剧集。</p><p id="a8cf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">4-使用“精英”剧集中的剧集步骤训练神经网络</p><p id="c9eb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们已经决定，代理必须接受培训，直到达到一定的奖励阈值。具体来说，我们决定了一个 80%的阈值，如变量<code class="fe nz oa ob nq b">REWARD_GOAL</code>所示:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="a721" class="ne ma it nq b gy nu nv l nw nx">while reward_mean &lt; REWARD_GOAL:</span></pre><h2 id="94c5" class="ne ma it bd mb nf ng dn mf nh ni dp mj lm nj nk ml lq nl nm mn lu nn no mp iz bi translated">步骤 1-播放 N 集</h2><p id="c6fb" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">下一段代码是生成包含剧集的批处理的代码:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="7f9d" class="ne ma it nq b gy nu nv l nw nx">action = select_action(state)<br/>next_state, reward, episode_is_done, _ = env.step(action)</span><span id="f1c4" class="ne ma it nq b gy ny nv l nw nx">episode_steps.append(EpisodeStep(observation=state,action=action))episode_reward += reward</span><span id="09e3" class="ne ma it nq b gy ny nv l nw nx">if episode_is_done: # Episode finished<br/>    batch.append(Episode(reward=episode_reward,<br/>                         steps=episode_steps))<br/>    next_state = env.reset()<br/>    episode_steps = []<br/>    episode_reward = 0.0</span><span id="7835" class="ne ma it nq b gy ny nv l nw nx">    &lt;STEP 2&gt;</span><span id="3f17" class="ne ma it nq b gy ny nv l nw nx">    &lt;STEP 3&gt;</span><span id="ee90" class="ne ma it nq b gy ny nv l nw nx">    &lt;STEP 4&gt;</span><span id="ceb0" class="ne ma it nq b gy ny nv l nw nx">state = next_state</span></pre><p id="2130" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将使用的主要变量是:</p><ul class=""><li id="8ba6" class="oc od it lf b lg lh lj lk lm oe lq of lu og ly oh oi oj ok bi translated"><code class="fe nz oa ob nq b">batch</code>累积<code class="fe nz oa ob nq b">Episode</code>实例的列表(<code class="fe nz oa ob nq b">BATCH_SIZE=100</code>)。</li><li id="dfc2" class="oc od it lf b lg ol lj om lm on lq oo lu op ly oh oi oj ok bi translated"><code class="fe nz oa ob nq b">episode_steps</code>累积当前剧集中的步骤列表。</li><li id="6855" class="oc od it lf b lg ol lj om lm on lq oo lu op ly oh oi oj ok bi translated"><code class="fe nz oa ob nq b">episode_reward</code>为当前剧集维护一个奖励计数器(在我们的例子中，我们只在剧集的结尾有奖励，但是该算法是针对更一般的情况描述的，其中我们不仅可以在最后一步有奖励)。</li></ul><p id="9df2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">情节步骤列表增加了(观察、行动)对。值得注意的是，我们保存了用于选择动作的观察结果<code class="fe nz oa ob nq b">state</code>(而不是作为动作结果由环境返回的观察结果<code class="fe nz oa ob nq b">next_state</code>):</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="43ce" class="ne ma it nq b gy nu nv l nw nx">episode_steps.append(EpisodeStep(observation=state,action=action))</span></pre><p id="3d06" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">奖励将添加到当前剧集的总奖励中:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="36ad" class="ne ma it nq b gy nu nv l nw nx">episode_reward += reward</span></pre><p id="ce56" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当当前剧集结束时(洞或目标状态)，我们需要将最终完成的剧集添加到批处理中，保存我们已采取的总奖励和步骤。然后，我们重置环境以重新开始，我们重置变量<code class="fe nz oa ob nq b">episode_steps</code>和<code class="fe nz oa ob nq b">episode_reward</code>以开始跟踪下一集:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="1b6f" class="ne ma it nq b gy nu nv l nw nx">batch.append(Episode(reward=episode_reward, steps=episode_steps))</span><span id="2d7f" class="ne ma it nq b gy ny nv l nw nx">next_obs = env.reset()<br/>episode_steps = []<br/>episode_reward = 0.0</span></pre><h2 id="7ce2" class="ne ma it bd mb nf ng dn mf nh ni dp mj lm nj nk ml lq nl nm mn lu nn no mp iz bi translated">第二步——计算每集的回报，并确定回报界限</h2><p id="48a7" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">下一段代码实现了步骤 2:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="ad09" class="ne ma it nq b gy nu nv l nw nx">if len(batch) == BATCH_SIZE:<br/>    reward_mean = float(np.mean(list(map(lambda s: <br/>                  s.reward, batch))))<br/>    elite_candidates= batch<br/>    ExpectedReturn = list(map(lambda s: s.reward * (GAMMA **          <br/>                     len(s.steps)), elite_candidates))<br/>    reward_bound = np.percentile(ExpectedReturn, PERCENTILE)</span></pre><p id="b879" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当运行了等于<code class="fe nz oa ob nq b">BATCH_SIZE</code>的播放次数时，训练循环执行该步骤:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="03f5" class="ne ma it nq b gy nu nv l nw nx">if len(batch) == BATCH_SIZE:</span></pre><p id="b39c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，代码计算当前批次中所有剧集的预期回报:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="40e5" class="ne ma it nq b gy nu nv l nw nx">elite_candidates= batch<br/>ExpectedReturn = list(map(lambda s: s.reward * (GAMMA **          <br/>                 len(s.steps)), elite_candidates))</span></pre><p id="ab4e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在此步骤中，根据给定的剧集批次和百分点值，我们计算边界奖励，该奖励将用于过滤“精英”剧集以训练代理神经网络:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="9d9b" class="ne ma it nq b gy nu nv l nw nx">reward_bound = np.percentile(ExpectedReturn, PERCENTILE)</span></pre><p id="974e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了获得边界奖励，我们将使用 NumPy 的 percentile 函数，该函数从值列表和所需的百分位数中计算百分位数的值。在这段代码中，我们将使用前 30%的剧集(由变量<code class="fe nz oa ob nq b">PERCENTILE</code>表示)来创建“精英”剧集。</p><p id="a8e8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在此步骤中，我们计算用于决定何时结束训练循环的<code class="fe nz oa ob nq b">reward_mean</code>:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="b59e" class="ne ma it nq b gy nu nv l nw nx">reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))</span></pre><h2 id="07e8" class="ne ma it bd mb nf ng dn mf nh ni dp mj lm nj nk ml lq nl nm mn lu nn no mp iz bi translated">第三步——扔掉所有回报低于界限的剧集</h2><p id="a0b1" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">接下来，我们将使用以下代码过滤掉我们的剧集:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="3d68" class="ne ma it nq b gy nu nv l nw nx">train_obs = []<br/>train_act = []<br/>elite_batch = []</span><span id="2a90" class="ne ma it nq b gy ny nv l nw nx">for example, discounted_reward in zip(elite_candidates, <br/>                                      ExpectedReturn):<br/>    if discounted_reward &gt; reward_bound:<br/>       train_obs.extend(map(lambda step: step.observation, <br/>                            example.steps))<br/>       train_act.extend(map(lambda step: step.action, <br/>                            example.steps))<br/>       elite_batch.append(example)</span><span id="2300" class="ne ma it nq b gy ny nv l nw nx">full_batch=elite_batch<br/>state=train_obs<br/>acts=train_act</span></pre><p id="f866" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于批次中的每一集:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="ce2f" class="ne ma it nq b gy nu nv l nw nx">for example, discounted_reward in zip(elite_candidates,<br/>                                      ExpectedReturn):</span></pre><p id="e09f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将检查这一集的总回报是否高于我们的界限:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="229c" class="ne ma it nq b gy nu nv l nw nx">if discounted_reward &gt; reward_bound:</span></pre><p id="e974" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果有，我们将填充我们将训练的观察状态和动作的列表，并跟踪精华片段:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="8c53" class="ne ma it nq b gy nu nv l nw nx">train_obs.extend(map(lambda step: step.observation,example.steps))<br/>train_act.extend(map(lambda step: step.action, example.steps))<br/>elite_batch.append(example)</span></pre><p id="a63c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后，我们将使用“精英”剧集更新该树变量，这些剧集是我们用来训练神经网络的状态和动作列表:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="fc6a" class="ne ma it nq b gy nu nv l nw nx">full_batch=elite_batch<br/>state=train_obs<br/>acts=train_act</span></pre><h2 id="ed82" class="ne ma it bd mb nf ng dn mf nh ni dp mj lm nj nk ml lq nl nm mn lu nn no mp iz bi translated">步骤 4——使用“精英”剧集中的剧集步骤训练神经网络</h2><p id="f298" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">每当我们的循环累积足够的片段(<code class="fe nz oa ob nq b">BATCH_SIZE</code>)时，我们计算“精英”片段，并且在相同的迭代中，循环用以下代码训练代理的神经网络:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="f888" class="ne ma it nq b gy nu nv l nw nx">state_t = torch.FloatTensor(state)<br/>acts_t = torch.LongTensor(acts)</span><span id="52c3" class="ne ma it nq b gy ny nv l nw nx">optimizer.zero_grad()<br/>action_scores_t = net(state_t)<br/>loss_t = objective(action_scores_t, acts_t)<br/>loss_t.backward()<br/>optimizer.step()<br/><br/>iter_no += 1<br/>batch = []</span></pre><p id="ab1a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这段代码使用“精英”剧集中的剧集步骤训练神经网络<strong class="lf jd">，使用状态<strong class="lf jd"> <em class="mz"> s </em> </strong>作为输入，发出动作<strong class="lf jd"> <em class="mz"> a </em> </strong>作为标签(期望输出)。让我们更详细地注释所有代码行:</strong></p><p id="c9db" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，我们将变量转换成张量:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="ee98" class="ne ma it nq b gy nu nv l nw nx">state_t = torch.FloatTensor(state)<br/>acts_t = torch.LongTensor(acts)</span></pre><p id="95a9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将神经网络的梯度归零</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="33e9" class="ne ma it nq b gy nu nv l nw nx">optimizer.zero_grad()</span></pre><p id="23ee" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">并将观察到的状态传递给神经网络，获得其动作得分:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="06b8" class="ne ma it nq b gy nu nv l nw nx">action_scores_t = net(state_t)</span></pre><p id="e6c8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这些分数被传递给目标函数，该函数将计算神经网络输出和代理采取的动作之间的交叉熵</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="c6db" class="ne ma it nq b gy nu nv l nw nx">loss_t = objective(action_scores_t, acts_t)</span></pre><p id="71bd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请记住，我们只考虑“精英”行动。这种想法是为了加强我们的神经网络，以执行那些带来良好回报的“精英”行动。</p><p id="99d1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后，我们需要使用<code class="fe nz oa ob nq b">backward</code>方法计算损失的梯度，并使用优化器的<code class="fe nz oa ob nq b">step</code>方法调整神经网络的参数:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="9b04" class="ne ma it nq b gy nu nv l nw nx">loss_t.backward()<br/>optimizer.step()</span></pre><h2 id="3409" class="ne ma it bd mb nf ng dn mf nh ni dp mj lm nj nk ml lq nl nm mn lu nn no mp iz bi translated">监控代理的进度</h2><p id="5f93" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">为了监控代理的学习进度，我们在培训循环中加入了以下内容:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="32ae" class="ne ma it nq b gy nu nv l nw nx">print(“%d: loss=%.3f, reward_mean=%.3f” % <br/>      (iter_no, loss_t.item(), reward_mean))</span></pre><p id="8b75" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们用它来显示迭代次数、损失和批次的平均回报(在下一节中，我们也将相同的值写入 TensorBoard 以获得一个漂亮的图表):</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="9da1" class="ne ma it nq b gy nu nv l nw nx">0: loss=1.384, reward_mean=0.020 <br/>1: loss=1.353, reward_mean=0.040  <br/>2: loss=1.332, reward_mean=0.010  <br/>3: loss=1.362, reward_mean=0.020  <br/>4: loss=1.337, reward_mean=0.020   <br/>5: loss=1.378, reward_mean=0.020 </span><span id="7e8a" class="ne ma it nq b gy ny nv l nw nx">. . .</span><span id="d4e0" class="ne ma it nq b gy ny nv l nw nx">639: loss=0.471, reward_mean=0.730  <br/>640: loss=0.511, reward_mean=0.730 <br/>641: loss=0.472, reward_mean=0.760 <br/>642: loss=0.481, reward_mean=0.650 <br/>643: loss=0.472, reward_mean=0.750 <br/>644: loss=0.492, reward_mean=0.720 <br/>645: loss=0.480, reward_mean=0.660 <br/>646: loss=0.479, reward_mean=0.740 <br/>647: loss=0.474, reward_mean=0.660  <br/>648: loss=0.517, reward_mean=0.830 <br/></span></pre><p id="4e13" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以检查<code class="fe nz oa ob nq b">reward_mean</code>变量的最后一个值是允许完成训练循环的值。</p><h1 id="69d5" class="lz ma it bd mb mc md me mf mg mh mi mj ki mk kj ml kl mm km mn ko mo kp mp mq bi translated">2.用更好的神经网络改进智能体</h1><p id="3ae6" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">在之前的<a class="ae nd" rel="noopener" target="_blank" href="/pytorch-performance-analysis-with-tensorboard-7c61f91071aa">帖子</a>中，我们已经介绍了 TensorBoard，这是一个有助于数据可视化过程的工具。取而代之的是上一节中使用的“打印”，我们可以用这两个句子来描绘这两个变量的行为:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="aeb2" class="ne ma it nq b gy nu nv l nw nx">writer.add_scalar(“loss”, loss_t.item(), iter_no)<br/>writer.add_scalar(“reward_mean”, reward_mean, iter_no)</span></pre><p id="0541" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在这种情况下，输出是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/9f55b7c0d4ddca30c41181637e11b85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ew081AvGU2uvlKnJIA3KZQ.png"/></div></div></figure><h2 id="830d" class="ne ma it bd mb nf ng dn mf nh ni dp mj lm nj nk ml lq nl nm mn lu nn no mp iz bi translated">更复杂的神经网络</h2><p id="48eb" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">出现的一个问题是，我们是否可以改进代理的神经网络。例如，如果我们考虑一个具有更多神经元的隐藏层，比如说 128 个神经元，会发生什么情况:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="d7d4" class="ne ma it nq b gy nu nv l nw nx">HIDDEN_SIZE = 128<br/>net= nn.Sequential(<br/>           nn.Linear(obs_size, HIDDEN_SIZE),<br/>           nn.Sigmoid(),<br/>           nn.Linear(HIDDEN_SIZE, n_actions)<br/>           )</span><span id="9ec7" class="ne ma it nq b gy ny nv l nw nx">objective = nn.CrossEntropyLoss()<br/>optimizer = optim.Adam(params=net.parameters(), lr=0.001)</span><span id="895d" class="ne ma it nq b gy ny nv l nw nx">train_loop()</span></pre><p id="fbe9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">结果可以在这里显示(或者执行<a class="ae nd" href="https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub 代码</a>):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/590cb2f70571bd7aaef39cd2c4118f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*69jG1ol6dNH3Ambvo6aR8w.png"/></div></div></figure><p id="7cb1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以看到，这个网络比前一个网络学习得更快。</p><h2 id="9432" class="ne ma it bd mb nf ng dn mf nh ni dp mj lm nj nk ml lq nl nm mn lu nn no mp iz bi translated">ReLU 激活功能</h2><p id="b050" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">如果我们改变激活函数会发生什么？例如，用 ReLU 代替乙状结肠？</p><p id="2588" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下面你可以看到发生了什么:网络收敛得更早，仅仅 200 次迭代就已经完成了。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/8193e0a58bebbeef84e6c2ac14574475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KQlwhrfuLoqlFewxgSEnZg.png"/></div></div></figure><h1 id="1462" class="lz ma it bd mb mc md me mf mg mh mi mj ki mk kj ml kl mm km mn ko mo kp mp mq bi translated">3.交叉熵算法的改进</h1><p id="ed0d" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">到目前为止，我们已经展示了如何改进神经网络架构。但我们也可以改进算法本身:我们可以将“精英”剧集保留更长时间。该算法的前一版本从环境中采样剧集，对最佳剧集进行训练，然后将其丢弃。然而，当成功剧集的数量很少时，可以将“精英”剧集保持更长时间，保持它们几次迭代以在其上进行训练。我们只需要修改代码中的一行:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="14a4" class="ne ma it nq b gy nu nv l nw nx">elite_candidates= full_batch + batch</span><span id="5f12" class="ne ma it nq b gy ny nv l nw nx">#elite_candidates= batch</span></pre><p id="62bb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">通过 TensorBoard 看到的结果是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/854a4e8d174254aa218fb273c9c6e1c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p5WasRn3NdOz5Tq4km4MXA.png"/></div></div></figure><p id="3f04" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以看到所需的迭代次数再次减少。</p><h1 id="3efc" class="lz ma it bd mb mc md me mf mg mh mi mj ki mk kj ml kl mm km mn ko mo kp mp mq bi translated">4.交叉熵方法的局限性</h1><p id="0512" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">到目前为止，我们已经看到，通过所提出的改进，通过很少的训练循环迭代，我们可以找到一个好的神经网络。但这是因为我们在谈论一个非常简单的“防滑”环境。但是如果我们有一个“湿滑”的环境呢？</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="02da" class="ne ma it nq b gy nu nv l nw nx">slippedy_env = gym.make(‘FrozenLake-v0’, is_slippery=True)</span><span id="4562" class="ne ma it nq b gy ny nv l nw nx">class OneHotWrapper(gym.ObservationWrapper):<br/>      def __init__(self, env):<br/>          super(OneHotWrapper, self).__init__(env)<br/>          self.observation_space = gym.spaces.Box(0.0, 1.0,<br/>                (env.observation_space.n, ), dtype=np.float32)<br/> <br/>      def observation(self, observation):<br/>          r = np.copy(self.observation_space.low)<br/>          r[observation] = 1.0<br/>          return r</span><span id="89f8" class="ne ma it nq b gy ny nv l nw nx">env = OneHotWrapper(slippedy_env)</span></pre><p id="c315" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">再次 TensorBoard 是一个大的帮助。在下图中，我们看到了算法在第一次迭代中的行为。它不能够脱去奖赏的价值:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/789d16f9dbb72abc72c44ce03cf89fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CC1ph-4sC3XSRpuZFJPWYA.png"/></div></div></figure><p id="0025" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">但是，如果我们再等待 5000 次迭代，我们会看到它会有所改善，但从那里开始，它会停滞不前，不再能够超过某个阈值:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/a797490ffbaabfade93b4ca74c61835a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uXJPEVYN313oLoGjLiiF1w.png"/></div></div></figure><p id="74e9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">尽管我们已经等待了两个多小时，但它仍然没有改善，没有超过 60%的阈值:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/a7f429df35e4281a74c61ce98a951f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_6hy_7gZM7B9dt-fdb0beA.png"/></div></div></figure><h1 id="b9c9" class="lz ma it bd mb mc md me mf mg mh mi mj ki mk kj ml kl mm km mn ko mo kp mp mq bi translated">5.摘要</h1><p id="5282" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">在这两篇关于交叉熵方法的文章中，读者熟悉了这种方法。我们选择这种方法是因为它是一个很好的热身，因为它简单但非常强大，尽管它有局限性，并且合并了强化学习和深度学习。</p><p id="bf3e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将它应用于冰冻湖环境。我们已经看到，对于简单的“防滑”环境，with 可以找到一个很好的神经网络。但是如果我们考虑一个“光滑”的环境，交叉熵方法就无法找到(训练神经网络的)解决方案。在本系列的后面，您将熟悉解决这些限制的其他方法。</p><p id="d2a2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在开始本系列新部分的下一篇文章中，我们将转向对 RL 方法的更系统的研究，并讨论基于值的方法家族。</p><p id="8916" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nd" rel="noopener" target="_blank" href="/the-bellman-equation-59258a0d3fa7">下期</a>见！。</p><p id="39cf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mz"/><a class="ae nd" href="https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="mz">这篇文章的全部代码可以在 GitHub</em></a><em class="mz"/><a class="ae nd" href="https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="mz">上找到，使用这个链接</em> </a> <em class="mz">可以作为一个 Colab 谷歌笔记本运行。</em></p><blockquote class="mw mx my"><p id="8ef0" class="ld le mz lf b lg lh kd li lj lk kg ll na ln lo lp nb lr ls lt nc lv lw lx ly im bi translated">鸣谢:这篇文章中的代码是从 Maxim Lapan 的代码中得到启发的，他写了一本关于这个主题的优秀的实用书籍。</p></blockquote></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><h1 id="5061" class="lz ma it bd mb mc pc me mf mg pd mi mj ki pe kj ml kl pf km mn ko pg kp mp mq bi translated">深度强化学习讲解系列</h1><p id="c09a" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated"><strong class="lf jd">由</strong> <a class="ae nd" href="https://www.upc.edu/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> UPC 巴塞罗那理工大学</strong> </a> <strong class="lf jd">和</strong> <a class="ae nd" href="https://www.bsc.es/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">巴塞罗那超级计算中心</strong> </a></p><p id="4cf4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一个轻松的介绍性<a class="ae nd" href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener ugc nofollow" target="_blank">系列</a>以一种实用的方式逐渐向读者介绍这项令人兴奋的技术，它是人工智能领域最新突破性进展的真正推动者。</p><div class="ph pi gp gr pj pk"><a href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener  ugc nofollow" target="_blank"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd jd gy z fp pp fr fs pq fu fw jc bi translated">深度强化学习解释-乔迪托雷斯。人工智能</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">本系列的内容</h3></div></div><div class="ps l"><div class="pt l pu pv pw ps px lb pk"/></div></div></a></div><h1 id="89c0" class="lz ma it bd mb mc md me mf mg mh mi mj ki mk kj ml kl mm km mn ko mo kp mp mq bi translated">关于这个系列</h1><p id="87a4" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">我在五月开始写这个系列，在巴塞罗那的<strong class="lf jd">封锁期。</strong>老实说，由于封锁，在业余时间写这些帖子帮助了我<a class="ae nd" href="https://twitter.com/hashtag/StayAtHome?src=hashtag_click" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> #StayAtHome </strong> </a>。感谢您当年阅读这份刊物；它证明了我所做的努力。</p><p id="e6c5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">免责声明</strong> —这些帖子是在巴塞罗纳封锁期间写的，目的是分散个人注意力和传播科学知识，以防对某人有所帮助，但不是为了成为 DRL 地区的学术参考文献。如果读者需要更严谨的文档，本系列的最后一篇文章提供了大量的学术资源和书籍供读者参考。作者意识到这一系列的帖子可能包含一些错误，如果目的是一个学术文件，则需要对英文文本进行修订以改进它。但是，尽管作者想提高内容的数量和质量，他的职业承诺并没有留给他这样做的自由时间。然而，作者同意提炼所有那些读者可以尽快报告的错误。</p></div></div>    
</body>
</html>