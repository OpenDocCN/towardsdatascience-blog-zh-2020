<html>
<head>
<title>Implementing Neural Machine Translation with Attention mechanism using Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Tensorflow 实现带注意机制的神经机器翻译</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-neural-machine-translation-with-attention-using-tensorflow-fc9c6f26155f?source=collection_archive---------5-----------------------#2020-02-17">https://towardsdatascience.com/implementing-neural-machine-translation-with-attention-using-tensorflow-fc9c6f26155f?source=collection_archive---------5-----------------------#2020-02-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6598" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Bahdanau 注意力的神经机器翻译(NMT)的 Tensorflow 实现的逐步解释。</h2></div><p id="2889" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，您将学习如何使用 Bahdanau 的注意力机制实现序列到序列(seq2seq)神经机器翻译(NMT)。我们将使用门控递归单元(GRU)在 Tensorflow 2.0 中实现代码。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/7f6fbce620e485e121ad24c5bf6c0b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6EBbmkaylzBvz7vgEJqpDQ.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">照片由<a class="ae lu" href="https://unsplash.com/@aaronburden?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Aaron Burden </a>在<a class="ae lu" href="https://unsplash.com/s/photos/diiferent-language?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="86a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">先决条件</strong></p><p id="4742" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a">使用注意机制的序列对序列模型</a></p><p id="9951" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/intuitive-explanation-of-neural-machine-translation-129789e3c59f">神经机器翻译的直观解释</a></p><p id="107a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">神经机器翻译(NMT)是使用深度神经网络将源语言(如英语)的单词序列转换为目标语言(如印地语或西班牙语)的单词序列的任务。</strong></p><p id="1a12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用由编码器和解码器组成的序列到序列(seq2seq)模型来实现 NMT。<strong class="kk iu">编码器将源序列的完整信息编码成单个实值向量，也称为上下文向量，传递给解码器以产生输出序列，该输出序列是类似于印地语或西班牙语的目标语言。</strong></p><p id="6ede" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上下文向量负责将整个输入序列总结成单个向量，这是低效的，所以我们使用注意机制。</p><p id="e22e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">注意机制的基本思想是避免试图学习每个句子的单一向量表示；相反，它基于关注权重关注输入序列的特定输入向量。</strong></p><p id="cb30" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">出于实施目的，我们将使用英语作为源语言，西班牙语作为目标语言。代码将使用 TensorFlow 2.0 实现，数据可以从<a class="ae lu" href="http://www.manythings.org/anki/." rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><p id="2263" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">实现具有注意机制的 NMT 的步骤</strong></p><ul class=""><li id="cb24" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">加载数据并通过删除空格、特殊字符等对其进行预处理。</li><li id="9613" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">创建数据集</li><li id="b29e" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">创建编码器、注意力层和解码器</li><li id="e592" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">创建优化器和损失函数</li><li id="b866" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">训练模型</li><li id="577f" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">做出推论</li></ul><h2 id="310d" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">导入所需的库</h2><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="9625" class="mj mk it nd b gy nh ni l nj nk">import pandas as pd<br/>import numpy as np<br/>import string<br/>from string import digits<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import tensorflow as tf<br/>import matplotlib.ticker as ticker<br/>from sklearn.model_selection import train_test_split<br/>import re<br/>import os<br/>import io<br/>import time</span></pre><h2 id="ad3b" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">从文件中读取数据</h2><p id="12f5" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">阅读可以从<a class="ae lu" href="http://www.manythings.org/anki/" rel="noopener ugc nofollow" target="_blank">这里</a>下载的英语-西班牙语翻译文件。</p><p id="cf75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我已将文件存储在“spa.txt”中</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="05de" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">data_path = "spa.txt"</strong></span><span id="6ab8" class="mj mk it nd b gy nq ni l nj nk">#Read the data<br/><strong class="nd iu">lines_raw= pd.read_table(data_path,names=['source', 'target', 'comments'])<br/>lines_raw.sample(5)</strong></span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/02d5750889f2ad4a18a585311c5b84dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*E1_FYWVVEOSjPKAGjYfC1g.png"/></div></figure><h2 id="fc2b" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">对源语句和目标语句进行清洗和预处理。</h2><p id="5052" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">我们应用以下文本清理</p><ul class=""><li id="a739" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">将文本转换为小写</li><li id="6b4c" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">删除引号</li><li id="de5b" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">清除源句子和目标句子中的数字。如果源语言或目标语言对数字使用不同的符号，那么删除这些符号</li><li id="e10c" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">删除空格</li><li id="e004" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">在单词和标点符号之间加一个空格，比如“？”</li><li id="4af6" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">在句首添加“start_”标记，在句尾添加“_end”标记</li></ul><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="4c1c" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">def preprocess_sentence(sentence):</strong><br/>    <br/>    num_digits= str.maketrans('','', digits)<br/>    <br/>    <strong class="nd iu">sentence= sentence.lower()<br/>    sentence= re.sub(" +", " ", sentence)<br/>    sentence= re.sub("'", '', sentence)<br/>    sentence= sentence.translate(num_digits)<br/>    sentence= re.sub(r"([?.!,¿])", r" \1 ", sentence)<br/>    sentence = sentence.rstrip().strip()<br/>    sentence=  'start_ ' + sentence + ' _end'</strong><br/>    <br/>    <strong class="nd iu">return sentence</strong></span></pre><p id="8497" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们选取英语中的一个句子并对其进行预处理</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="b181" class="mj mk it nd b gy nh ni l nj nk">print(preprocess_sentence(“Can you do it in thirty minutes?”))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e02d807ae56bbbf464e0efa8ad5dc56f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*RFLqH6SbhANJVAHRz1X2eQ.png"/></div></figure><p id="b545" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">预处理源句子和目标句子，使单词对的格式为:[英语，西班牙语]</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="2f82" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">def create_dataset(path, num_examples):</strong><br/>  <br/>  <strong class="nd iu">lines = io.open(path, encoding='UTF-8').read().strip().split('\n')</strong><br/>  <br/> <strong class="nd iu"> word_pairs = [[preprocess_sentence(w) for w in l.split('\t')]  for l in lines[:num_examples]]</strong><br/>  <br/>  <strong class="nd iu">return zip(*word_pairs)</strong></span><span id="9f29" class="mj mk it nd b gy nq ni l nj nk"><strong class="nd iu">sample_size=60000<br/>source, target = create_dataset(data_path, sample_size)</strong></span></pre><h2 id="dcdf" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">标记源句子和目标句子</h2><p id="3c65" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">我们需要对文本语料库进行矢量化，将文本转换成整数序列。</p><p id="5577" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先创建分词器，然后在源句子上应用分词器</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="9e46" class="mj mk it nd b gy nh ni l nj nk"># create a tokenizer for source sentence<br/><strong class="nd iu">source_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')</strong></span><span id="48cb" class="mj mk it nd b gy nq ni l nj nk"># Fit the source sentences to the source tokenizer<br/><strong class="nd iu">source_sentence_tokenizer.fit_on_texts(source)</strong></span></pre><p id="917e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在将源句子中的每个单词转换成一个整数序列，用相应的整数值替换这个单词。</p><p id="0146" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">只有分词器知道的单词才会被考虑</strong></p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="f97f" class="mj mk it nd b gy nh ni l nj nk">#Transforms each text in texts to a sequence of integers.<br/><strong class="nd iu">source_tensor = source_sentence_tokenizer.texts_to_sequences(source)</strong></span></pre><p id="756a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要创建长度相同的序列，所以我们用“0”来填充长度较短的序列</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="65d5" class="mj mk it nd b gy nh ni l nj nk">#Sequences that are shorter than num_timesteps, padded with 0 at the end.<br/><strong class="nd iu">source_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_tensor,padding='post' )</strong></span></pre><p id="07eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以类似的方式标记目标句子</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="8a28" class="mj mk it nd b gy nh ni l nj nk"># create the target sentence tokenizer<strong class="nd iu"><br/>target_sentence_tokenizer= # Fit tf.keras.preprocessing.text.Tokenizer(filters='')</strong></span><span id="920f" class="mj mk it nd b gy nq ni l nj nk"># Fit the tokenizer on target sentences<strong class="nd iu"><br/>target_sentence_tokenizer.fit_on_texts(target)</strong></span><span id="7607" class="mj mk it nd b gy nq ni l nj nk">#conver target text to sequnec of integers<strong class="nd iu"><br/>target_tensor = target_sentence_tokenizer.texts_to_sequences(target)</strong></span><span id="3c9a" class="mj mk it nd b gy nq ni l nj nk"># Post pad the shorter sequences with 0<strong class="nd iu"><br/>target_tensor= tf.keras.preprocessing.sequence.pad_sequences(target_tensor,padding='post' )</strong></span></pre><h2 id="cd5e" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">创建训练和测试数据集</h2><p id="3aa9" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">将数据集拆分为测试和训练。80%的数据用于训练，20%用于测试模型</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="aff7" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor= train_test_split(source_tensor, target_tensor,test_size=0.2)</strong></span></pre><p id="092f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当数据集很大时，我们希望在内存中创建数据集以提高效率。我们将使用<strong class="kk iu"><em class="nt">TF . data . dataset . from _ tensor _ slices()</em></strong>方法以对象的形式获取数组的切片。</p><p id="b4b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据集以 64 个为一批创建。</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="5916" class="mj mk it nd b gy nh ni l nj nk">#setting the BATCH SIZE<br/><strong class="nd iu">BATCH_SIZE = 64</strong></span><span id="7551" class="mj mk it nd b gy nq ni l nj nk">#Create data in memeory <strong class="nd iu">dataset=tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BATCH_SIZE)</strong></span><span id="a9cf" class="mj mk it nd b gy nq ni l nj nk"># shuffles the data in the batch<br/><strong class="nd iu">dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)</strong></span></pre><p id="a0c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们遍历数据集中的所有元素。返回的迭代器实现了 Python 迭代器协议，因此<strong class="kk iu">只能在急切模式下使用</strong></p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="1ad9" class="mj mk it nd b gy nh ni l nj nk">#Creates an Iterator for enumerating the elements of this dataset.<br/>#Extract the next element from the dataset<br/><strong class="nd iu">source_batch, target_batch =next(iter(dataset))<br/>print(source_batch.shape)</strong></span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/418e0826b241a9e70537070bc6b76159.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/format:webp/1*k4ZNFZvWE0GAr6HipQ60cw.png"/></div></figure><p id="a3d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每批源数据的大小为<strong class="kk iu"> <em class="nt"> (BATCH_SIZE，max_source_length)，</em> </strong>，目标数据的批量为<strong class="kk iu"> <em class="nt"> (BATCH_SIZE，max_target_length)。</em> </strong>在我们的例子中，最大源长度是 11，最大目标长度是 16</p><h1 id="ec4a" class="nv mk it bd ml nw nx ny mo nz oa ob mr jz oc ka mu kc od kd mx kf oe kg na of bi translated">在 Bahdanau 的关注下，使用门控循环单元(GRU)创建序列对序列模型</h1><p id="0f98" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">有注意的 seq2seq 模型和无注意的 seq2seq 模型的区别</p><ul class=""><li id="ce07" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">编码器和解码器的所有隐藏状态(向前和向后)用于生成上下文向量，不像 eq2seq 没有注意，它使用最后的编码器隐藏状态。</li><li id="e1d7" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">注意机制利用由前馈网络参数化的比对分数来比对输入和输出序列。它有助于注意源序列中最相关的信息。</li><li id="c568" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">Seq2Seq 注意模型基于与源位置相关联的上下文向量来预测目标单词，并且与 seq2seq 不同，先前生成的目标单词在没有注意的情况下将所有源序列编码到单个上下文向量中</li></ul><p id="df6f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">为模型设置一些参数</strong></p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="5839" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">BUFFER_SIZE = len(source_train_tensor)<br/>steps_per_epoch= len(source_train_tensor)//BATCH_SIZE<br/>embedding_dim=256<br/>units=1024<br/>source_vocab_size= len(source_sentence_tokenizer.word_index)+1<br/>target_vocab_size= len(target_sentence_tokenizer.word_index)+1</strong></span></pre><h2 id="4484" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">创建编码器</h2><p id="c3f0" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">编码器将输入作为源令牌，将它们传递给嵌入层以获得矢量的密集表示，然后传递给 GRU。</p><p id="89e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">为 GRU </strong>设置返回序列和返回状态为真。默认情况下，return _ sequneces 设置为 False。<strong class="kk iu">当 return_sequences 设置为真时，则返回编码器</strong>中所有单元的整个输出序列。当 return_sequences 设置为 False 时，我们只返回最后一个编码器单元的隐藏状态。</p><p id="3f40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> seq2seq 在没有注意的情况下会将编码器的 return_sequences 设置为 False。Seq2seq 将把编码器的 return_sequences 设置为 True。</strong></p><p id="c048" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了<strong class="kk iu">返回 GRU 的内部状态，我们将 retrun_state 设置为 True </strong></p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="d93c" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">class Encoder(tf.keras.Model):<br/>    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):<br/>        super(Encoder, self).__init__()<br/>        self.batch_size= batch_size<br/>        self.encoder_units=encoder_units<br/>        self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_dim)<br/>        self.gru= tf.keras.layers.GRU(encoder_units, <br/>                                      return_sequences=True,<br/>                                      return_state=True,                                      recurrent_initializer='glorot_uniform'<br/>                                     )</strong><br/>    <br/>    <strong class="nd iu">def call(self, x, hidden):</strong><br/>        #pass the input x to the embedding layer<br/>       <strong class="nd iu"> x= self.embedding(x)</strong><br/>        # pass the embedding and the hidden state to GRU<br/>        <strong class="nd iu">output, state = self.gru(x, initial_state=hidden)<br/>        return output, state</strong><br/>    <br/>    <strong class="nd iu">def initialize_hidden_state(self):<br/>        return tf.zeros((self.batch_size, self.encoder_units))</strong></span></pre><p id="6655" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">测试编码器类并打印编码器输出和隐藏状态的尺寸</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="5ee2" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">encoder = Encoder(source_vocab_size, embedding_dim, units, BATCH_SIZE)</strong></span><span id="27bc" class="mj mk it nd b gy nq ni l nj nk"><strong class="nd iu">sample_hidden = encoder.initialize_hidden_state()</strong></span><span id="ccab" class="mj mk it nd b gy nq ni l nj nk"><strong class="nd iu">sample_output, sample_hidden= encoder(source_batch, sample_hidden)</strong></span><span id="f9e7" class="mj mk it nd b gy nq ni l nj nk">print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))<br/>print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi og"><img src="../Images/5e7d0a4495658a4c6158cff0ba562aba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qicBIrlajJS3RPPoHdSZNQ.png"/></div></div></figure><h2 id="44bf" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">创建 Bahdanau 注意力图层</h2><p id="a00d" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">注意力层包括</p><ul class=""><li id="d88d" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">比对分数</li><li id="e343" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">注意力权重</li><li id="bf1c" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">上下文向量</li></ul><p id="ed3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在注意力层实现这些简化的等式</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a3ca314d6dd9ef32e09c1864c6a3eca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*Dfc4YZunqNQ_bGizc63jXA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">巴赫达瑙注意方程式</p></figure><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="346b" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">class BahdanauAttention(tf.keras.layers.Layer):<br/>    def __init__(self, units):<br/>        super( BahdanauAttention, self).__init__()<br/>        self.W1= tf.keras.layers.Dense(units)  # encoder output<br/>        self.W2= tf.keras.layers.Dense(units)  # Decoder hidden<br/>        self.V= tf.keras.layers.Dense(1)</strong><br/>    <br/>    <strong class="nd iu">def call(self, query, values):</strong><br/>        #calculate the Attention score<br/>        <br/>        <strong class="nd iu">score= self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))</strong><br/>        <br/>        # attention_weights shape == (batch_size, max_length, 1)<br/>       <strong class="nd iu"> attention_weights= tf.nn.softmax(score, axis=1)</strong><br/>        <br/>         #context_vector <br/>        <strong class="nd iu">context_vector= attention_weights * values</strong><br/>       <br/>        #Computes the sum of elements across dimensions of a tensor<br/>       <strong class="nd iu"> context_vector = tf.reduce_sum(context_vector, axis=1)<br/>        return context_vector, attention_weights</strong></span></pre><p id="4931" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用十个单位测试 Bahdanau 注意层</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="16b1" class="mj mk it nd b gy nh ni l nj nk">a<strong class="nd iu">ttention_layer= BahdanauAttention(10)</strong><br/><strong class="nd iu">attention_result, attention_weights = attention_layer(sample_hidden, sample_output)</strong></span><span id="29a6" class="mj mk it nd b gy nq ni l nj nk">print("Attention result shape: (batch size, units) {}".format(attention_result.shape))<br/>print("Attention weights shape: (batch_size, sequence_length, 1) {}".format(attention_weights.shape))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oi"><img src="../Images/b98ce941c166b1ce239b3d811107d2a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXRXUFK9eSv26z_BCXVdXA.png"/></div></div></figure><h2 id="c719" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">创建解码器</h2><p id="f866" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">解码器具有嵌入层、GRU 层和全连接层。</p><p id="597e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">来预测解码器使用的目标字</p><ul class=""><li id="341e" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">上下文向量:注意力权重和编码器输出的总和</li><li id="684e" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">前一时间步的解码器输出和</li><li id="1a84" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">前一个解码器的隐藏状态</li></ul><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="3b36" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">class Decoder(tf.keras.Model):<br/>    def __init__(self, vocab_size, embedding_dim, decoder_units, batch_sz):<br/>        super (Decoder,self).__init__()<br/>        self.batch_sz= batch_sz<br/>        self.decoder_units = decoder_units<br/>        self.embedding = tf.keras.layers.Embedding(vocab_size, <br/>                                                   embedding_dim)<br/>        self.gru= tf.keras.layers.GRU(decoder_units, <br/>                                      return_sequences= True,<br/>                                      return_state=True,<br/>                          recurrent_initializer='glorot_uniform')</strong><br/>        # Fully connected layer<br/>        <strong class="nd iu">self.fc= tf.keras.layers.Dense(vocab_size)</strong><br/>        <br/>        # attention<br/>        <strong class="nd iu">self.attention = BahdanauAttention(self.decoder_units)</strong><br/>    <br/>   <strong class="nd iu"> def call(self, x, hidden, encoder_output):</strong><br/>        <br/>        <strong class="nd iu">context_vector, attention_weights = self.attention(hidden,      <br/>                                                    encoder_output)</strong><br/>        <br/>        # pass output sequnece thru the input layers<br/>        <strong class="nd iu">x= self.embedding(x)</strong><br/>        <br/>        # concatenate context vector and embedding for output sequence<br/>        <strong class="nd iu">x= tf.concat([tf.expand_dims( context_vector, 1), x], <br/>                                      axis=-1)</strong><br/>        <br/>        # passing the concatenated vector to the GRU<br/>       <strong class="nd iu"> output, state = self.gru(x)</strong><br/>        <br/>        # output shape == (batch_size * 1, hidden_size)<br/>       <strong class="nd iu"> output= tf.reshape(output, (-1, output.shape[2]))</strong><br/>        <br/>        # pass the output thru Fc layers<br/>               <strong class="nd iu">x= self.fc(output)<br/>        return x, state, attention_weights</strong></span></pre><p id="3e12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">测试解码器</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="6425" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">decoder= Decoder(target_vocab_size, embedding_dim, units, BATCH_SIZE)<br/>sample_decoder_output, _, _= decoder(tf.random.uniform((BATCH_SIZE,1)), sample_hidden, sample_output)</strong><br/>print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/905a1f0a527577ef9cdaa8f471dd28e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*NF1v7RMPCWnsgmpGILrxHg.png"/></div></figure><h2 id="b71b" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">定义优化器</h2><p id="979b" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">我们在这里使用 Adam 优化器；你也可以试试 Rmsprop</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="3f6f" class="mj mk it nd b gy nh ni l nj nk">#Define the optimizer and the loss function<br/>optimizer = tf.keras.optimizers.Adam()</span></pre><h2 id="1334" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">定义损失函数</h2><p id="6fc9" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">使用<strong class="kk iu">SparseCategoricalCrossentropy</strong>计算实际和预测输出之间的损失。</p><p id="e37a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果输出是一次性编码的向量，那么使用 categorical _ crossentropy。对包含整数的 word2index 向量使用<strong class="kk iu">SparseCategoricalCrossentropy</strong>loss。</p><p id="f2ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">稀疏分类交叉熵在计算和内存上是高效的，因为它使用单个整数，而不是整个向量[0 0 1]</strong></p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="2fd8" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">loss_object = tf.keras.losses.SparseCategoricalCrossentropy(<br/>    from_logits=True, reduction='none')</strong></span><span id="8889" class="mj mk it nd b gy nq ni l nj nk"><strong class="nd iu">def loss_function(real, pred):<br/>  mask = tf.math.logical_not(tf.math.equal(real, 0))<br/>  loss_ = loss_object(real, pred)</strong></span><span id="4318" class="mj mk it nd b gy nq ni l nj nk"><strong class="nd iu">mask = tf.cast(mask, dtype=loss_.dtype)<br/>  loss_ *= mask</strong></span><span id="4728" class="mj mk it nd b gy nq ni l nj nk"><strong class="nd iu">return tf.reduce_mean(loss_)</strong></span></pre><h2 id="ee4e" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">训练数据集</h2><p id="b938" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">使用编码器-解码器模型训练数据集</p><ol class=""><li id="183a" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ok mb mc md bi translated">将编码后的源句子通过编码器，并返回编码器输出序列和隐藏状态</li><li id="534e" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ok mb mc md bi translated">编码器输出、编码器隐藏状态和解码器输入被传递给解码器。在时间步长=0 时，解码器将“start_”作为输入。</li><li id="1438" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ok mb mc md bi translated">解码器返回预测字和解码器隐藏状态</li><li id="91d2" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ok mb mc md bi translated">解码器隐藏状态被传递回模型，并且预测的字被用于计算损失</li><li id="5c02" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ok mb mc md bi translated">为了训练，我们使用教师强制，在每个时间步将实际单词传递给解码器。</li><li id="9f66" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ok mb mc md bi translated"><strong class="kk iu">在推断过程中，我们将前一时间步的预测单词作为输入传递给解码器</strong></li><li id="bf2a" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ok mb mc md bi translated">计算梯度下降，将其应用于优化器并反向传播</li></ol><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/979039ff28905ff0d8eb63086d59719d.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*KUZI_craAgT3w5VaYXZEPw.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">注意机制</p></figure><p id="ff12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Tensorflow 跟踪每个 tf.Variable 上每个计算的每个梯度。<strong class="kk iu">为了训练，我们使用梯度带</strong>，因为我们需要控制需要梯度信息的代码区域。对于具有注意机制的 seq2seq，我们仅计算解码器输出的梯度。</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="fe0b" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">def train_step(inp, targ, enc_hidden):<br/>    loss = 0<br/>    with tf.GradientTape() as tape:</strong><br/>        #create encoder<br/>       <strong class="nd iu"> enc_output, enc_hidden = encoder(inp, enc_hidden)<br/>        dec_hidden = enc_hidden</strong><br/>        #first input to decode is start_<br/>        <strong class="nd iu">dec_input = tf.expand_dims(<br/>            [target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)</strong><br/>        # Teacher forcing - feeding the target as the next input<br/>        <strong class="nd iu">for t in range(1, targ.shape[1]):</strong><br/>          # passing enc_output to the decoder<br/>          <strong class="nd iu">predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)</strong><br/>          # calculate loss based on predictions  <br/>          <strong class="nd iu">loss += tf.keras.losses.sparse_categorical_crossentropy(targ[:, t], predictions)</strong><br/>          # using teacher forcing<br/>          <strong class="nd iu">dec_input = tf.expand_dims(targ[:, t], 1)<br/>    batch_loss = (loss / int(targ.shape[1]))<br/>    variables = encoder.trainable_variables + decoder.trainable_variables<br/>    gradients = tape.gradient(loss, variables)<br/>    optimizer.apply_gradients(zip(gradients, variables))<br/>    return batch_loss</strong></span></pre><p id="0eba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用多个历元集中训练编码器-解码器模型</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="b427" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">EPOCHS=20<br/>for epoch in range(EPOCHS):<br/>  start = time.time()</strong></span><span id="eef1" class="mj mk it nd b gy nq ni l nj nk"><strong class="nd iu">enc_hidden = encoder.initialize_hidden_state()<br/>  total_loss = 0</strong><br/>  # train the model using data in bataches <br/>  f<strong class="nd iu">or (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):<br/>    batch_loss = train_step(inp, targ, enc_hidden)<br/>    total_loss += batch_loss</strong></span><span id="990a" class="mj mk it nd b gy nq ni l nj nk"><strong class="nd iu">if batch % 100 == 0:<br/>      print('Epoch {} Batch {} Loss {}'.format(epoch + 1,<br/>                                                   batch,                                                   <br/>                                         batch_loss.numpy()))<br/>  print('Epoch {} Loss {}'.format(epoch + 1,<br/>                                      total_loss / steps_per_epoch))<br/>  print('Time taken for 1 epoch {} sec\n'.format(time.time() - start))</strong></span></pre><h2 id="7273" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">对测试数据进行推断</h2><p id="78c3" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated"><strong class="kk iu">进行推理类似于训练，只是我们不知道教师强制中使用的实际单词，所以我们将来自前一时间步的预测单词作为输入传递给解码器。</strong></p><p id="3463" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们计算每个时间步的注意力权重，因为它有助于关注用于进行预测的源序列中最相关的信息。</p><p id="af1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">当我们达到最大目标句子长度时，或者当我们遇到“stop_”标签时，我们停止预测单词。</strong></p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="476b" class="mj mk it nd b gy nh ni l nj nk">#Calculating the max length of the source and target sentences<br/><strong class="nd iu">max_target_length= max(len(t) for t in  target_tensor)<br/>max_source_length= max(len(t) for t in source_tensor)</strong></span></pre><p id="b189" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">做出这样的推断</p><ul class=""><li id="2008" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">传源句，</li><li id="cb06" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">预处理句子以转换成小写，删除空格，特殊字符，在单词和标点符号之间加一个空格，等等。</li><li id="dc58" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">对句子进行标记以创建 word2index 词典</li><li id="9c20" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">Post 用 0 填充源序列，使其长度与最大源句子的长度相同</li><li id="e012" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">创建输入张量</li><li id="e16c" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">创建编码器并传递输入向量和隐藏状态。初始隐藏状态被设置为零</li><li id="3334" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">解码器的第一个输入将是“start_”标签。解码器的初始隐藏状态是编码器隐藏状态</li><li id="73c7" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">创建解码器，向其传递解码器输入、解码器隐藏状态和编码器输出</li><li id="23bc" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">存储注意力权重，并使用解码器输入、隐藏和上下文向量，找到具有最大概率的单词的整数。</li><li id="3de6" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">将整数转换为单词，并不断追加预测的单词以形成目标句子，直到我们遇到“end_”标签或达到最大目标序列长度</li></ul><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="a7fe" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">def evaluate(sentence):</strong><br/>    <strong class="nd iu">attention_plot= np.zeros((max_target_length, max_source_length))</strong><br/>    #preprocess the sentnece<br/>    <strong class="nd iu">sentence = preprocess_sentence(sentence)</strong><br/>    <br/>    #convert the sentence to index based on word2index dictionary<br/>    <strong class="nd iu">inputs= [source_sentence_tokenizer.word_index[i] for i in sentence.split(' ')]</strong><br/>    <br/>    # pad the sequence <br/>    <strong class="nd iu">inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_source_length, padding='post')</strong><br/>    <br/>    #conver to tensors<br/>    <strong class="nd iu">inputs = tf.convert_to_tensor(inputs)</strong><br/>    <br/>    <strong class="nd iu">result= ''</strong><br/>    <br/>    # creating encoder<br/>    <strong class="nd iu">hidden = [tf.zeros((1, units))]</strong><br/>    <strong class="nd iu">encoder_output, encoder_hidden= encoder(inputs, hidden)</strong><br/>    <br/>    # creating decoder<br/>    <strong class="nd iu">decoder_hidden = encoder_hidden</strong><br/>    <strong class="nd iu">decoder_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)</strong><br/>    <br/>    <strong class="nd iu">for t in range(max_target_length):</strong><br/>        <strong class="nd iu">predictions, decoder_hidden, attention_weights= decoder(decoder_input, decoder_hidden, encoder_output)</strong><br/>        <br/>        # storing attention weight for plotting it<br/>       <strong class="nd iu"> attention_weights = tf.reshape(attention_weights, (-1,))<br/>        attention_plot[t] = attention_weights.numpy()<br/>        <br/>        prediction_id= tf.argmax(predictions[0]).numpy()</strong><br/>        <strong class="nd iu">result += target_sentence_tokenizer.index_word[prediction_id] + ' '<br/>        <br/>        if target_sentence_tokenizer.index_word[prediction_id] == '_end':<br/>            return result,sentence, attention_plot</strong><br/>        <br/>        # predicted id is fed back to as input to the decoder<br/>       <strong class="nd iu"> decoder_input = tf.expand_dims([prediction_id], 0)</strong><br/>        <br/>    <strong class="nd iu">return result,sentence, attention_plot</strong></span></pre><p id="8df7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">函数来绘制源单词和目标单词之间的注意力权重。该图将帮助我们理解哪个源词在预测目标词时被给予了更多的关注</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="c4a1" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">def plot_attention(attention, sentence, predicted_sentence):<br/>    fig = plt.figure(figsize=(10,10))<br/>    ax= fig.add_subplot(1,1,1)<br/>    ax.matshow(attention, cmap='Greens')<br/>    fontdict={'fontsize':10}<br/>    <br/>    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)<br/>    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)</strong></span><span id="b8c7" class="mj mk it nd b gy nq ni l nj nk"><strong class="nd iu">ax.xaxis.set_major_locator(ticker.MultipleLocator(1))<br/>    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))</strong></span><span id="d808" class="mj mk it nd b gy nq ni l nj nk"><strong class="nd iu">plt.show()</strong></span></pre><h2 id="45c2" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">将源句翻译成目标句</h2><p id="92e6" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">为了将源句子翻译成目标语言，我们调用<strong class="kk iu"> <em class="nt"> evaluate </em> </strong>函数来创建编码器、解码器和关注层</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="b471" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">def translate(sentence):<br/>    result, sentence, attention_plot = evaluate(sentence)<br/>    <br/>    print('Input : %s' % (sentence))<br/>    print('predicted sentence :{}'.format(result))<br/>    <br/>    attention_plot= attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]<br/>    plot_attention(attention_plot, sentence.split(' '), result.split(' '))</strong></span></pre><p id="02c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后的预测</p><pre class="lf lg lh li gt nc nd ne nf aw ng bi"><span id="9585" class="mj mk it nd b gy nh ni l nj nk"><strong class="nd iu">translate(u'I am going to work.')</strong></span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi om"><img src="../Images/c9f78f7117bdef47fdb54d976f08fb22.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*2RSoBXddcvB017qe1RUExQ.png"/></div></figure><p id="0bf0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">翻译句子的注意情节</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi on"><img src="../Images/3878cb29a51b4f5de8bdab3d0eadc3b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*yHEhhcUIzzXa7eSGjAKNrg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">注意力图</p></figure><p id="24ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在翻译过程中，我们发现“去”比“去”更能预测“去”，同样，“工作”比“工作”更能预测“去”</p><p id="525f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可从<a class="ae lu" href="https://github.com/arshren/NMT-with-Attention/blob/master/NMT%20with%20Attention%20trained%20with%2020%20epochs.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>获得的代码</p><h2 id="6ada" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">参考</h2><p id="1b5d" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">巴丹瑙注意了-【https://arxiv.org/pdf/1409.0473.pdf T2】</p><p id="ebe1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lu" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/tutorials/text/NMT _ with _ attention</a></p><p id="089a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lu" href="https://www.tensorflow.org/guide/keras/rnn" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/keras/rnn</a></p></div></div>    
</body>
</html>