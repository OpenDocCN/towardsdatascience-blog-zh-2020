<html>
<head>
<title>Automating Doom with Deep Q-Learning: An Implementation in Tensorflow.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度 Q 学习的自动化毁灭:Tensorflow 中的一个实现。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c?source=collection_archive---------37-----------------------#2020-06-02">https://towardsdatascience.com/automating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c?source=collection_archive---------37-----------------------#2020-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ca37" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习的基础</h2></div><h1 id="8423" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi la"><img src="../Images/67cec3bd79756c819c1aae57d7cb063a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/0*ZsVLanVVz8hICO85"/></div></figure><p id="a1f6" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">在线学习方法是一个动态的强化学习算法家族，它是过去十年来人工智能领域许多成就的背后。在线学习方法属于<a class="ae mn" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff" rel="noopener">基于样本的学习</a>类强化学习方法，允许简单地通过重复观察来确定状态值，消除了对转换动态的需要。与它们的<a class="ae mn" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed" rel="noopener">离线对应方式</a>，<strong class="lk iu">不同，在线学习方法允许在环境事件期间对状态和动作的值进行增量更新，允许观察到持续的、增量的性能改进。</strong></p><p id="680c" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了时间差异学习(TD ),我们还讨论了 Q-learning 的<a class="ae mn" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830" rel="noopener">理论</a>和<a class="ae mn" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">实际实现</a>,这是 TD 的一种发展，旨在允许增量估计和状态-动作值的改进。Q-learning 因成为模拟游戏环境的强化学习方法的支柱而闻名，如在 OpenAI 的健身房中观察到的那些。因为我们已经在<a class="ae mn" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">过去的文章</a>中涉及了 Q-learning 的理论方面，所以这里不再重复。</p><p id="8d72" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在线学习方法(如 TD)的快速情景内响应能力使其适用于高度动态的环境，在这种环境中，状态和动作的值通过多组估计值不断更新。也许最值得注意的是，<strong class="lk iu"> TD 是 Q-learning 的基础，</strong>这是一种更先进的算法，用于训练代理处理游戏环境，如在 OpenAI Atari 健身房中观察到的环境，正如我们之前的实现中的<a class="ae mn" rel="noopener" target="_blank" href="/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc">一些</a>所涵盖的。</p><p id="6c2f" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文中，我们将探索如何通过使用开源的 OpenAI gym 包装库<a class="ae mn" href="https://github.com/shakenes/vizdoomgym" rel="noopener ugc nofollow" target="_blank"> Vizdoomgym </a>，将 Q-learning 应用于训练代理玩经典 FPS 游戏<strong class="lk iu"> Doom </strong>。我们将指导您设置您的第一个代理，并为未来的工作奠定基础。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="mo mp l"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">永恒经典的演变。我们将在这里关注 1993 年的版本。</p></figure><h1 id="7606" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">超越 TD: SARSA &amp; Q-learning </strong></h1><p id="b254" class="pw-post-body-paragraph li lj it lk b ll mu ju ln lo mv jx lq lr mw lt lu lv mx lx ly lz my mb mc md im bi translated">回想一下，在时间差异学习中，我们观察到一个主体在一个环境中通过一系列状态(S)、动作(A)和(奖励)循环地行为。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a2f2e70cb4a7849698566b6b3a502ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*Ug3u2nIX_GVTYRFYm6f2jg.png"/></div></figure><p id="cc2f" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在 TD 期间，我们可以在到达下一个状态时更新前一个状态的值。我们可以进一步扩展我们的模型的范围，以包括状态-动作值，在一种称为<strong class="lk iu"> SARSA 的方法中，一种用于估计动作值</strong>的基于策略的 TD 控制算法。在 SARSA 期间，我们在给定的策略下连续地估计跨增量时间步长的动作值，同时使用这种知识来修改策略，使其趋向于动作值贪婪的替代方案。</p><p id="8a15" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们比较状态-动作和状态-值 TD 更新方程:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a84fe83ad173a3f5fa54b8e84a2bdd5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*PyhofFmoKZn0bu1Ud2sLuw.png"/></div></figure><p id="42bd" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk iu"> Q-learning 不同于 SARSA，它在更新</strong>期间强制选择具有当前最高动作值的动作，这与使用<a class="ae mn" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82" rel="noopener">贝尔曼优化方程观察到的情况类似。</a>我们可以检查贝尔曼和贝尔曼优化方程旁边的 SARSA 和 Q-learning，如下所示:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/e4d2627f6e373537d7a664f33f43a459.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*qGtUpZKJ8FhXx-UD6q3ulg.png"/></div></figure><p id="5bee" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可能想知道如何确保我们的状态-动作空间的完整探索，给定需要不断地为具有最高现有动作值的状态选择动作。从理论上讲，我们可能只是因为一开始就没有评估而避免了最佳行动。<strong class="lk iu">为了鼓励探索，我们可以使用一个衰减的 e-greedy 策略，</strong>本质上迫使代理以衰减的概率选择一个明显的次优行为，以便了解更多关于其价值的信息。通过使用衰减值，我们可以在评估完所有状态后限制探索，之后我们将为每个状态永久选择最佳操作。</p><p id="346d" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有了我们的理论，让我们开始我们的实现。</p><h1 id="f79a" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">履行</h1><p id="dd0d" class="pw-post-body-paragraph li lj it lk b ll mu ju ln lo mv jx lq lr mw lt lu lv mx lx ly lz my mb mc md im bi translated">我们的 Google 协作实现是利用 Tensorflow Core 用 Python 编写的，可以在<a class="ae mn" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> GradientCrescent Github 上找到。</a>关注我们出版物的读者会发现代码与我们之前在<a class="ae mn" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">雅达利</a>T4 环境中的实现相似。由于这种方法的实现非常复杂，让我们用<strong class="lk iu">总结一下所需动作的顺序</strong>:</p><p id="01a0" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">1.<strong class="lk iu">我们定义我们的深度 Q 学习神经网络</strong>。这是一个 CNN，它拍摄游戏中的屏幕图像，并输出 Ms-Pacman gamespace 中每个动作的概率，或 Q 值。为了获得概率张量，我们在最后一层不包括任何激活函数。</p><p id="7885" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">2.由于 Q-learning 要求我们了解当前和下一个状态，我们需要从数据生成开始。我们将表示初始状态<em class="nb"> s </em>的游戏空间的预处理输入图像输入到网络中，并获取动作的初始概率分布，或 Q 值。在训练之前，这些值将是随机的和次优的。</p><p id="e614" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">3.利用我们的概率张量，我们然后<strong class="lk iu">使用 argmax()函数选择具有当前最高概率的动作</strong>，并使用它来构建ε贪婪策略。</p><p id="aede" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">4.使用我们的策略，我们将选择动作<em class="nb"> a </em>，并评估我们在健身房环境中的决定<strong class="lk iu">接收关于新状态<em class="nb">s’</em>、奖励<em class="nb"> r </em> </strong>以及该集是否已结束的信息。</p><p id="feff" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">5.我们以列表形式<s>将该信息组合存储在一个缓冲区中，并重复步骤 2-4 预设次数，以建立一个足够大的缓冲区数据集。</s></p><p id="aefa" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">6.一旦步骤 5 完成，我们转到<strong class="lk iu">生成我们的目标<em class="nb">y</em>-值<em class="nb">R’</em>和<em class="nb">A’</em></strong>，这是损失计算所需的。虽然前者只是从<em class="nb"> R </em>中减去，但是我们通过将<em class="nb">S’</em>输入到我们的网络中来获得 A’。</p><p id="48d0" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">7.有了所有的组件，我们就可以<strong class="lk iu">计算训练网络的损耗</strong>。</p><p id="4ba8" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">8.一旦训练结束，我们将评估我们的代理在新一集游戏中的表现，并记录表现。</p><p id="adff" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们开始吧。随着 Tensorflow 2 在实验室环境中的使用，我们已经使用新的<em class="nb"> compat </em>包将我们的代码转换为符合 TF2 标准。注意，这段代码不是 TF2 本地的。</p><p id="b71d" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们通过导入所有必需的包，包括 OpenAI 和 Vizdoomgym 环境以及 Tensorflow 核心:</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="f09c" class="nh kj it nd b gy ni nj l nk nl">import gym<br/>import vizdoomgym<br/>!pip install tensorflow==1.15<br/>import numpy as np<br/>import tensorflow as tf<br/>from tensorflow.contrib.layers import flatten, conv2d, fully_connected<br/>from collections import deque, Counter<br/>import random<br/>from datetime import datetime</span></pre><p id="97a2" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们定义一个预处理函数，对健身房环境中的观察结果进行标准化和调整大小，并将它们转换成一维张量。</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="4a3b" class="nh kj it nd b gy ni nj l nk nl">from skimage.color import rgb2gray<br/>from skimage import transform</span><span id="39c4" class="nh kj it nd b gy nm nj l nk nl">#prepro (240, 320, 3) uint8 frame into 30x40 1D float vector <br/>color = np.array([240, 320, 74]).mean()<br/>def preprocess_observation(obs):<br/> <br/> <br/> img =obs/255.0<br/> img[img==color] = 0</span><span id="6c97" class="nh kj it nd b gy nm nj l nk nl"> img_gray = rgb2gray(img)<br/> preprocessed_frame = transform.resize(img_gray, [60,80])<br/> <br/> return preprocessed_frame</span></pre><p id="74ce" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，让我们初始化健身房环境。我们将使用 Vizdoomgym 的健康收集场景，其中的目标是收集尽可能多的健康令牌，以便在通过一个具有破坏性酸性地板的方形房间时保持活着。</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="7a92" class="nh kj it nd b gy ni nj l nk nl">env = gym.make(‘VizdoomHealthGathering-v0’)<br/>n_outputs = env.action_space.n<br/>print(n_outputs)</span><span id="129a" class="nh kj it nd b gy nm nj l nk nl">observation = env.reset()</span><span id="85ca" class="nh kj it nd b gy nm nj l nk nl">import tensorflow as tf<br/>import matplotlib.pyplot as plt</span><span id="470b" class="nh kj it nd b gy nm nj l nk nl">for i in range(22):<br/> <br/> if i &gt; 20:<br/> print(observation.shape)<br/> plt.imshow(observation)<br/> plt.show()</span><span id="45a6" class="nh kj it nd b gy nm nj l nk nl">observation, _, _, _ = env.step(1)</span></pre><p id="c73a" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以查看游戏画面，也可以查看游戏空间中的 3 个可用动作，即左转、右转或前进。当然，我们的代理人无法获得这些信息。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/bc4a69b287567f3da52d8ab692f12a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*7bMAwN-RuqfitLAcwnOFOg.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">原始观察输入</p></figure><p id="bec4" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以借此机会比较我们的原始和预处理输入图像:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi no"><img src="../Images/6b53f53eea5b8fa93fdd05ba26b2cf1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*sM-o-A-UT_ohcP08dSPsww.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">预处理图像输入</p></figure><p id="9f82" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们将<strong class="lk iu">输入帧堆叠</strong>和<strong class="lk iu">帧合成</strong>引入我们的预处理流水线，这是 Deepmind 在 2015 年推出的两项技术。这些方法分别为我们的输入提供时间和运动参考。</p><p id="5a86" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们通过获取两个输入帧来应用帧合成，并返回这两个帧的元素式最大总和<em class="nb"> maxframe </em>。然后，这些合成的帧被存储在一个队列或“堆栈”中，当引入新的条目时，它会自动删除旧的条目。</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="721a" class="nh kj it nd b gy ni nj l nk nl">stack_size = 4 # We stack 4 composite frames in total</span><span id="02c8" class="nh kj it nd b gy nm nj l nk nl"><br/>stacked_frames = deque([np.zeros((60,80), dtype=np.int) for i in range(stack_size)], maxlen=4)</span><span id="56c2" class="nh kj it nd b gy nm nj l nk nl">def stack_frames(stacked_frames, state, is_new_episode):<br/> # Preprocess frame<br/> frame = preprocess_observation(state)<br/> <br/> if is_new_episode:<br/>   # Clear our stacked_frames<br/>   stacked_frames = deque([np.zeros((60,80), dtype=np.int) for i in  range(stack_size)], maxlen=4)<br/> <br/>   # Because we’re in a new episode, copy the same frame 4x, apply  elementwise maxima<br/>   maxframe = np.maximum(frame,frame)<br/>   stacked_frames.append(maxframe)<br/>   stacked_frames.append(maxframe)<br/>   stacked_frames.append(maxframe)<br/>   stacked_frames.append(maxframe)<br/> <br/> <br/> <br/>   # Stack the frames<br/>   stacked_state = np.stack(stacked_frames, axis=2)<br/> <br/> else:<br/>   #Since deque append adds t right, we can fetch rightmost element<br/>   maxframe=np.maximum(stacked_frames[-1],frame)<br/>   # Append frame to deque, automatically removes the oldest frame<br/>   stacked_frames.append(maxframe)</span><span id="2b65" class="nh kj it nd b gy nm nj l nk nl">   # Build the stacked state (first dimension specifies different frames)<br/>   stacked_state = np.stack(stacked_frames, axis=2) <br/>   <br/> return stacked_state, stacked_frames</span></pre><p id="a740" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，让我们定义我们的模型，一个深度 Q 网络。这基本上是一个三层卷积网络，它采用预处理的输入观察值，将生成的展平输出馈送到一个全连接层，生成将游戏空间中的每个动作作为输出的概率。请注意，这里没有激活层，因为激活层的存在会导致二进制输出分布。</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="dfda" class="nh kj it nd b gy ni nj l nk nl">tf.compat.v1.reset_default_graph()<br/></span><span id="363c" class="nh kj it nd b gy nm nj l nk nl">def q_network(X, name_scope):<br/> <br/> # Initialize layers<br/> initializer =  tf.compat.v1.keras.initializers.VarianceScaling(scale=2.0)</span><span id="e920" class="nh kj it nd b gy nm nj l nk nl"> with tf.compat.v1.variable_scope(name_scope) as scope:</span><span id="a2b3" class="nh kj it nd b gy nm nj l nk nl">   # initialize the convolutional layers<br/>   layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4,  padding=’SAME’, weights_initializer=initializer) <br/>   tf.compat.v1.summary.histogram(‘layer_1’,layer_1)<br/> <br/>   layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4),  stride=2, padding=’SAME’, weights_initializer=initializer)<br/>   tf.compat.v1.summary.histogram(‘layer_2’,layer_2)<br/> <br/>   layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding=’SAME’, weights_initializer=initializer)<br/>   tf.compat.v1.summary.histogram(‘layer_3’,layer_3)<br/> <br/>   # Flatten the result of layer_3 before feeding to the fully connected layer<br/>   flat = flatten(layer_3)<br/>   # Insert fully connected layer<br/>   fc = fully_connected(flat, num_outputs=128,  weights_initializer=initializer)<br/>   tf.compat.v1.summary.histogram(‘fc’,fc)<br/>   #Add final output layer<br/>   output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)<br/>   tf.compat.v1.summary.histogram(‘output’,output)</span><span id="da93" class="nh kj it nd b gy nm nj l nk nl">   # Vars will store the parameters of the network such as weights<br/>   vars = {v.name[len(scope.name):]: v for v in   tf.compat.v1.get_collection(key=tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)} <br/>   #Return both variables and outputs together<br/>   return vars, output</span></pre><p id="9529" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们也借此机会为我们的模型和训练过程定义超参数。注意，由于我们的堆叠帧，X_shape 现在是<em class="nb">(无，60，80，4) </em>。</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="6b33" class="nh kj it nd b gy ni nj l nk nl">num_episodes = 1000<br/>batch_size = 48</span><span id="5681" class="nh kj it nd b gy nm nj l nk nl">input_shape = (None, 60, 80, 1)</span><span id="5287" class="nh kj it nd b gy nm nj l nk nl">learning_rate = 0.002<br/>#Modified for composite stacked frames<br/>X_shape = (None, 60, 80, 4)<br/>discount_factor = 0.99</span><span id="9073" class="nh kj it nd b gy nm nj l nk nl">global_step = 0<br/>copy_steps = 100<br/>steps_train = 4<br/>start_steps = 2000</span></pre><p id="a144" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">回想一下，Q-learning 要求我们选择具有最高行动价值的行动。为了确保我们仍然访问每一个可能的状态-行为组合，我们将让我们的代理遵循一个衰减的ε-贪婪策略，探索率为 5%。</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="c167" class="nh kj it nd b gy ni nj l nk nl">epsilon = 0.5<br/>eps_min = 0.05<br/>eps_max = 1.0<br/>eps_decay_steps = 500000</span><span id="d02c" class="nh kj it nd b gy nm nj l nk nl">def epsilon_greedy(action, step):<br/> p = np.random.random(1).squeeze() #1D entries returned using squeeze<br/> epsilon = max(eps_min, eps_max — (eps_max-eps_min) * step/eps_decay_steps) #Decaying policy with more steps<br/> if p&lt; epsilon:<br/>   return np.random.randint(n_outputs)<br/> else:<br/>   return action</span></pre><p id="bcf9" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">回想上面的等式，Q-learning 的更新函数要求如下:</p><ul class=""><li id="3d52" class="np nq it lk b ll lm lo lp lr nr lv ns lz nt md nu nv nw nx bi translated">当前状态<em class="nb"> s </em></li><li id="77a8" class="np nq it lk b ll ny lo nz lr oa lv ob lz oc md nu nv nw nx bi translated">当前动作<em class="nb">一个</em></li><li id="29ce" class="np nq it lk b ll ny lo nz lr oa lv ob lz oc md nu nv nw nx bi translated">当前动作后的奖励<em class="nb"> r </em></li><li id="4fee" class="np nq it lk b ll ny lo nz lr oa lv ob lz oc md nu nv nw nx bi translated">下一个状态<em class="nb">s’</em></li><li id="e246" class="np nq it lk b ll ny lo nz lr oa lv ob lz oc md nu nv nw nx bi translated">下一个动作<em class="nb">a’</em></li></ul><p id="2f7a" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了以有意义的数量提供这些参数，我们需要按照一组参数评估我们当前的策略，并将所有变量存储在一个缓冲区中，我们将在训练期间从该缓冲区中提取迷你批次中的数据。让我们继续创建我们的缓冲区和一个简单的采样函数:</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="2f09" class="nh kj it nd b gy ni nj l nk nl">buffer_len = 20000<br/>exp_buffer = deque(maxlen=buffer_len)</span><span id="17c2" class="nh kj it nd b gy nm nj l nk nl">def sample_memories(batch_size):<br/> perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]<br/> mem = np.array(exp_buffer)[perm_batch]<br/> return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]</span></pre><p id="0d62" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，让我们将原始网络的权重参数复制到目标网络中。这种双网络方法允许我们在使用现有策略的训练过程中生成数据，同时仍然为下一个策略迭代优化我们的参数，减少损失振荡。</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="bae0" class="nh kj it nd b gy ni nj l nk nl"># we build our Q network, which takes the input X and generates Q values for all the actions in the state<br/>mainQ, mainQ_outputs = q_network(X, ‘mainQ’)</span><span id="7448" class="nh kj it nd b gy nm nj l nk nl"># similarly we build our target Q network, for policy evaluation<br/>targetQ, targetQ_outputs = q_network(X, ‘targetQ’)</span><span id="441f" class="nh kj it nd b gy nm nj l nk nl">copy_op = [tf.compat.v1.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]<br/>copy_target_to_main = tf.group(*copy_op)</span></pre><p id="3db6" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们还将定义我们的损失。这就是我们的目标动作(具有最高动作值)和我们的预测动作的平方差。我们将使用 ADAM 优化器来最大限度地减少我们在训练中的损失。</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="f674" class="nh kj it nd b gy ni nj l nk nl"># define a placeholder for our output i.e action<br/>y = tf.compat.v1.placeholder(tf.float32, shape=(None,1))</span><span id="c4cf" class="nh kj it nd b gy nm nj l nk nl"># now we calculate the loss which is the difference between actual value and predicted value<br/>loss = tf.reduce_mean(input_tensor=tf.square(y — Q_action))</span><span id="d8b9" class="nh kj it nd b gy nm nj l nk nl"># we use adam optimizer for minimizing the loss<br/>optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)<br/>training_op = optimizer.minimize(loss)</span><span id="7da9" class="nh kj it nd b gy nm nj l nk nl">init = tf.compat.v1.global_variables_initializer()<br/>loss_summary = tf.compat.v1.summary.scalar(‘LOSS’, loss)<br/>merge_summary = tf.compat.v1.summary.merge_all()<br/>file_writer = tf.compat.v1.summary.FileWriter(logdir, tf.compat.v1.get_default_graph())</span></pre><p id="ab65" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">定义好所有代码后，让我们运行我们的网络并检查培训过程。我们已经在最初的总结中定义了大部分，但是让我们为后代回忆一下。</p><ul class=""><li id="8a05" class="np nq it lk b ll lm lo lp lr nr lv ns lz nt md nu nv nw nx bi translated">对于每个时期，在使用ε-贪婪策略选择下一个动作之前，我们将输入图像堆栈输入到我们的网络中，以生成可用动作的概率分布</li><li id="fdd0" class="np nq it lk b ll ny lo nz lr oa lv ob lz oc md nu nv nw nx bi translated">然后，我们将它输入到网络中，获取下一个状态和相应奖励的信息，并将其存储到我们的缓冲区中。我们更新我们的堆栈，并通过一些预定义的步骤重复这一过程。</li><li id="3966" class="np nq it lk b ll ny lo nz lr oa lv ob lz oc md nu nv nw nx bi translated">在我们的缓冲区足够大之后，我们将下一个状态输入到我们的网络中，以便获得下一个动作。我们还通过贴现当前的奖励来计算下一个奖励</li><li id="c165" class="np nq it lk b ll ny lo nz lr oa lv ob lz oc md nu nv nw nx bi translated">我们通过 Q 学习更新函数生成我们的目标 y 值，并训练我们的网络。</li><li id="874a" class="np nq it lk b ll ny lo nz lr oa lv ob lz oc md nu nv nw nx bi translated">通过最小化训练损失，我们更新网络权重参数，以便为下一个策略输出改进的状态-动作值。</li></ul><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="0e90" class="nh kj it nd b gy ni nj l nk nl">with tf.compat.v1.Session() as sess:<br/> init.run()<br/> # for each episode<br/> history = []<br/> for i in range(num_episodes):<br/>   done = False<br/>   obs = env.reset()<br/>   epoch = 0<br/>   episodic_reward = 0<br/>   actions_counter = Counter()<br/>   episodic_loss = []<br/>   # First step, preprocess + initialize stack<br/>   obs,stacked_frames= stack_frames(stacked_frames,obs,True)</span><span id="4c99" class="nh kj it nd b gy nm nj l nk nl">   # while the state is not the terminal state<br/>   while not done:<br/>    #Data generation using the untrained network<br/>    # feed the game screen and get the Q values for each action<br/>    actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})<br/>    # get the action<br/>    action = np.argmax(actions, axis=-1)<br/>    actions_counter[str(action)] += 1<br/>    # select the action using epsilon greedy policy<br/> <br/>    action = epsilon_greedy(action, global_step)<br/>    # now perform the action and move to the next state, next_obs, receive reward<br/>    next_obs, reward, done, _ = env.step(action)<br/>    #Updated stacked frames with new episode<br/>    next_obs, stacked_frames = stack_frames(stacked_frames, next_obs, False)<br/>    # Store this transition as an experience in the replay buffer! <br/>    exp_buffer.append([obs, action, next_obs, reward, done])</span><span id="aaff" class="nh kj it nd b gy nm nj l nk nl">    # After certain steps, we train our Q network with samples from the experience replay buffer<br/>    if global_step % steps_train == 0 and global_step &gt; start_steps:<br/> <br/>      o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)<br/>      # states<br/>      o_obs = [x for x in o_obs]<br/>      # next states<br/>      o_next_obs = [x for x in o_next_obs]<br/>      # next actions<br/>      next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})<br/>      # discounted reward: these are our Y-values<br/>      y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done)</span><span id="96ec" class="nh kj it nd b gy nm nj l nk nl">     # merge all summaries and write to the file<br/>     mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})<br/>     file_writer.add_summary(mrg_summary, global_step)<br/>     # To calculate the loss, we run the previously defined functions mentioned while feeding inputs<br/>     train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})<br/>     episodic_loss.append(train_loss)</span><span id="c796" class="nh kj it nd b gy nm nj l nk nl"> # after some interval we copy our main Q network weights to target Q network<br/>   if (global_step+1) % copy_steps == 0 and global_step &gt; start_steps:<br/>     copy_target_to_main.run()</span><span id="66bb" class="nh kj it nd b gy nm nj l nk nl">   obs = next_obs<br/>   epoch += 1<br/>   global_step += 1<br/>   episodic_reward += reward<br/>   next_obs=np.zeros(obs.shape)<br/>   exp_buffer.append([obs, action, next_obs, reward, done])<br/>   obs= env.reset()<br/>   obs,stacked_frames= stack_frames(stacked_frames,obs,True)<br/>   history.append(episodic_reward)<br/>   print(‘Epochs per episode:’, epoch, ‘Episode Reward:’, episodic_reward,”Episode number:”, len(history))</span></pre><p id="6900" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦训练完成，我们就可以根据增量情节绘制奖励分布图。前 1000 集如下所示:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi od"><img src="../Images/dcf35e1155c5dabfba1e5e5e911178e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*ZpInK4OaG4iNn6tVos8U3w.png"/></div></figure><p id="bee0" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这一结果是可以接受的，改善的开始是可见的，考虑到最初的 Vizdoom 论文表明，要观察到更显著的改善，需要成千上万的情节。</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="c8f8" class="nh kj it nd b gy ni nj l nk nl">img_array=[]<br/>with tf.compat.v1.Session() as sess:<br/> init.run()<br/> observation, stacked_frames = stack_frames(stacked_frames, observation, True)<br/> <br/> while True:</span><span id="87f3" class="nh kj it nd b gy nm nj l nk nl"># feed the game screen and get the Q values for each action<br/> actions = mainQ_outputs.eval(feed_dict={X:[observation], in_training_mode:False})</span><span id="6448" class="nh kj it nd b gy nm nj l nk nl"># get the action<br/> action = np.argmax(actions, axis=-1)<br/> actions_counter[str(action)] += 1</span><span id="2deb" class="nh kj it nd b gy nm nj l nk nl"># select the action using epsilon greedy policy<br/> action = epsilon_greedy(action, global_step)<br/> environment.render()<br/> new_observation, stacked_frames = stack_frames(stacked_frames, new_observation, False)<br/> <br/> observation = new_observation <br/> # now perform the action and move to the next state, next_obs, receive reward<br/> new_observation, reward, done, _ = environment.step(action)<br/> <br/> img_array.append(new_observation)<br/> if done: <br/> #observation = env.reset()<br/> break<br/> <br/> environment.close()</span></pre><p id="e0ca" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们可以将我们的帧列表提供给 scikit-video 库，以生成视频序列输出供检查:</p><pre class="lb lc ld le gt nc nd ne nf aw ng bi"><span id="caac" class="nh kj it nd b gy ni nj l nk nl">from random import choice<br/>import cv2 <br/>from google.colab.patches import cv2_imshow</span><span id="57c7" class="nh kj it nd b gy nm nj l nk nl">import numpy as np<br/>import skvideo.io</span><span id="2d09" class="nh kj it nd b gy nm nj l nk nl">out_video = np.empty([len(img_array), 240, 320, 3], dtype = np.uint8)<br/>out_video = out_video.astype(np.uint8)</span><span id="777a" class="nh kj it nd b gy nm nj l nk nl">for i in range(len(img_array)):<br/> frame = img_array[i]<br/> out_video[i] = frame<br/># Writes the the output image sequences in a video file<br/>skvideo.io.vwrite(“/content/doom.mp4”, out_video)</span></pre><p id="c0bd" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们来看看我们的代理是如何工作的！</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="oe mp l"/></div></figure><p id="e289" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意代理在发现健康包时如何暂停，然后再继续向其移动。只是为了好玩，我们还为<em class="nb">的基本</em>场景训练了一个代理，目标是尽快击中怪物。而我们则取得了<em class="nb"> ca 的最好成绩。1.3 </em>秒，我们将在下面展示更早的一集。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="oe mp l"/></div></figure><p id="fbf8" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就结束了 Q-learning 的实现。在我们的下一篇文章中，我们将继续用更先进的 Q 学习方法处理更复杂的厄运场景，并将我们的代码切换到 Pytorch，这是人工智能研究中最流行的语言之一。</p><p id="a2e4" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们希望你喜欢这篇文章，并希望你查看 GradientCrescent 上的许多其他文章，涵盖人工智能的应用和理论方面。为了保持对<a class="ae mn" href="https://medium.com/@adrianitsaxu" rel="noopener"> GradientCrescent </a>的最新更新，请考虑关注该出版物并关注我们的<a class="ae mn" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> Github </a>资源库。</p><p id="f2fb" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk iu">参考文献</strong></p><p id="ecb7" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">萨顿等人。强化学习</p><p id="d5d7" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">怀特等人。阿尔伯塔大学强化学习基础</p><p id="2c9a" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">席尔瓦等人。阿尔，强化学习，UCL</p><p id="933d" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">马卡洛夫等人。al，使用 VizDoom 第一人称射击游戏进行深度强化学习，HSE</p><p id="a1b7" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">肯普卡等人。al，ViZDoom:一个基于 Doom 的人工智能研究平台，用于视觉强化学习，PUT</p><p id="60a6" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Ravichandiran 等人。al，用 Python 实践强化学习</p></div></div>    
</body>
</html>