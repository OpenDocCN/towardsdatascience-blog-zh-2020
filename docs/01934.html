<html>
<head>
<title>Machine Learning with PySpark and Amazon EMR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 PySpark 和亚马逊 EMR 进行机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-with-pyspark-and-amazon-emr-3149dbc847ae?source=collection_archive---------22-----------------------#2020-02-23">https://towardsdatascience.com/machine-learning-with-pyspark-and-amazon-emr-3149dbc847ae?source=collection_archive---------22-----------------------#2020-02-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/5b9784824a14a3818367e5989d25ccfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JSagOtKqmNztIncABeHK4w.png"/></div></div></figure><div class=""/><p id="ddf0" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">想象一下，你有客户如何与你的在线服务互动的详细数据。数据包括客户访问过的页面、他们在每个页面上花费的时间以及客户人口统计数据。一切都在那里。</p><p id="322a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在你的老板想利用这些数据。你的公司最近失去了相当多的客户，他希望你提供一个关于剩余客户的预测，这些客户很可能会流失。</p><p id="a7f5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">熊猫，Scikit-learn，Matplotlib 没什么搞不定的吧？</p><p id="750d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">不对。</p><p id="3fcc" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在您查看了数据集之后，您会注意到一个重要的细节。该数据有 1 亿行。没有一台机器能处理这么多数据。那你是做什么的？</p><p id="b295" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">推荐的方法是使用多台机器。这就是 PySpark 和 EMR 非常有用的地方。</p><p id="fd25" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在本文中，我将指导您如何分析一个非常大的数据集(大数据),以及如何针对这些数据运行一些计算量很大的 ML 算法。最终目标是将数据转化为有用的见解，例如，提供关于客户流失的预测。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="b26b" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">分析</h1><h2 id="eea6" class="me lh je bd li mf mg dn lm mh mi dp lq km mj mk lu kq ml mm ly ku mn mo mc mp bi translated">1.建立</h2><p id="32f9" class="pw-post-body-paragraph kb kc je kd b ke mq kg kh ki mr kk kl km ms ko kp kq mt ks kt ku mu kw kx ky im bi translated">本文中执行的分析依赖于 PySpark 和 AWS EMR 技术。您可能需要遵循和复制分析的所有技术信息都可以在本<a class="ae mv" rel="noopener" target="_blank" href="/big-data-in-10-minutes-bead3c012ba4">文本</a>中找到。该文本是如何设置 AWS EMR(创建您的集群)、启用 PySpark 和启动 Jupyter 笔记本的分步指南。</p><h2 id="c85a" class="me lh je bd li mf mg dn lm mh mi dp lq km mj mk lu kq ml mm ly ku mn mo mc mp bi translated">2.数据</h2><p id="e987" class="pw-post-body-paragraph kb kc je kd b ke mq kg kh ki mr kk kl km ms ko kp kq mt ks kt ku mu kw kx ky im bi translated">用于分析的数据是用户日志的集合。这些日志来源于用户与一家名为 Sparkify 的虚拟在线音乐流媒体公司的互动。数据集有 12 GB，超过 2000 万行，来自<a class="ae mv" href="https://www.udacity.com/" rel="noopener ugc nofollow" target="_blank"> Udacity </a>，在亚马逊 S3 服务器上公开提供:</p><p id="ecad" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">" s3n://uda city-dsnd/spark ify/spark ify _ event _ data . JSON "</p><figure class="mx my mz na gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mw"><img src="../Images/c158d017c172ab7a1a22da61944fa50c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VyEcUpRRYTnVnE2DI2BfSQ.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">数据集预览</p></figure><p id="71d8" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">数据集中大多数列的名称都是不言自明的。<em class="nf">注册</em>是客户加入服务的时间。ts 是客户进入特定网页时的时间戳。<em class="nf">注册</em>和<em class="nf"> ts </em>都以 Unix 时间给出(自 1970 年以来的秒数)。l <em class="nf"> ength </em>是用户在特定页面上花费的秒数。每个会话是用户活跃的特定时间段。<em class="nf"> itemInSession </em>是客户在会话期间的页面/网络互动数量。列<em class="nf">级别</em>显示用户是否为服务付费。</p><p id="b01b" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">数据集中最重要的列可能是第<em class="nf">页</em>。它保存了客户访问过的所有 Sparkify 页面。</p><figure class="mx my mz na gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/c2acc9fcfd202e773fc2876de364947c.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*3hsE7zdR7m6_zMoLSsIAsw.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">页面总访问量</p></figure><p id="0bc1" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">通过统计单个页面的访问量，我们可以注意到页面<em class="nf"> NextSong </em>是最常被访问的(这是有道理的，因为 Sparkify 是一个在线音乐流媒体服务)。然而，更重要的是了解到达<em class="nf">取消确认</em>页面的客户。这些都是客户，任务是建立一个可以识别他们的预测模型。</p><p id="905f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在解释了项目的目标和数据集的基本特征之后，现在开始分析的一切都准备好了。</p><h2 id="9cf3" class="me lh je bd li mf mg dn lm mh mi dp lq km mj mk lu kq ml mm ly ku mn mo mc mp bi translated">3.预处理</h2><p id="fb53" class="pw-post-body-paragraph kb kc je kd b ke mq kg kh ki mr kk kl km ms ko kp kq mt ks kt ku mu kw kx ky im bi translated">上述数据需要一些预处理。在我的 g <a class="ae mv" href="https://github.com/maleckicoa/Sparkify-Project/blob/master/Sparkify_Big.ipynb" rel="noopener ugc nofollow" target="_blank"> ithub 库</a>中的 Sparkify_Big.ipynb 文件中详细记录了通常的第一步，如包安装、包导入、数据清理、数据标记和重命名。我将跳过这里的预处理，而是专注于:</p><ul class=""><li id="bd2c" class="nh ni je kd b ke kf ki kj km nj kq nk ku nl ky nm nn no np bi translated">如何将给定的庞大数据集转化为对预测模型有用的输入(<strong class="kd jf">特征工程)</strong></li><li id="db08" class="nh ni je kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">如何对数据集应用分类算法(<strong class="kd jf">机器学习)</strong></li></ul><h2 id="88a7" class="me lh je bd li mf mg dn lm mh mi dp lq km mj mk lu kq ml mm ly ku mn mo mc mp bi translated">4.特征工程</h2><p id="47fd" class="pw-post-body-paragraph kb kc je kd b ke mq kg kh ki mr kk kl km ms ko kp kq mt ks kt ku mu kw kx ky im bi translated">为了获得好的预测结果，适当的特征工程通常比 ML 模型的后续选择更重要。理解数据集并强调最有判别潜力的特征(在分类的情况下)非常重要。</p><p id="7a43" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最终，我们的模型应该预测潜在的流失客户，因此日志数据应该聚合到 UserId 级别。我将原始数据集(经过预处理后)转换成 13 个特征，这些特征按每个用户 Id 聚合(大部分是平均的),并将作为模型的输入。下面列出了这些功能:</p><figure class="mx my mz na gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/bd2843abe4412eb357008ca2d6b97e5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FwfOX8P5fb2VDL51fbyu9w.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">功能列表</p></figure><p id="32db" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果您对我如何将原始数据从日志级别提升到 UserId 级别感兴趣，这里有一些好消息:PySpark 支持 SQL 语法。在运行模型之前，我广泛地使用它来准备数据。SQL 语法的替代方法是本机 PySpark 语法。PySpark 语法也很直观，但它允许更多的自由。最好能很好地掌握这两种语法。</p><p id="78ac" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">预处理数据然后聚合数据的完整代码在前面提到的<a class="ae mv" href="https://github.com/maleckicoa/Sparkify-Project" rel="noopener ugc nofollow" target="_blank"> github 库</a>中。在这里，我只是想证明，如果您对 SQL 有基本的了解，PySpark 中的数据操作确实是轻而易举的事情。</p><figure class="mx my mz na gt iv"><div class="bz fp l di"><div class="nw nx l"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">PySpark 中使用 SQL 的特征工程</p></figure><p id="2f71" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">经过这种操作后，数据现在是适当的格式，并从大约 2000 多万行减少到大约 22K 行(用户 id 的数量)。</p><figure class="mx my mz na gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/9b9451cb0fbb00d3b437d3cf79b122f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TyYrQeOqCUyhzbwos6BaSA.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">聚合和特征工程后的数据</p></figure><p id="d983" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="nf">标签</em>是因变量，目的是通过使用提供的 13 个特征来预测它。</p><p id="bee2" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我的假设是，在客户流失发生前不久，流失客户和非流失客户的行为应该是不同的。对于每个用户，我丢弃了自上次交互以来超过 14 天的所有数据。我想在客户流失前抓住他们的行为。</p><p id="d08b" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我的期望是流失客户经历更多的错误，发起更多的“反对”,并且通常与非流失客户不同。以下是 13 个特征的平均值，分别针对有不良反应和无不良反应的客户(0 表示无不良反应，1 表示有不良反应):</p><figure class="mx my mz na gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/ac306e3d71df038cdfd9bdc1db34777f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VRaNsyyo1EezqIY51XPvsw.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">流失和非流失客户的平均特征值</p></figure><p id="d2ce" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我将上表中的数据按列<strong class="kd jf"/>标准化，为蝴蝶图准备数据。</p><figure class="mx my mz na gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/c34c50131e048fe3d5f2877c2852d8b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j7AW-gIbrMJl8Jwv6kfoRg.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">按列标准化的平均特征值</p></figure><p id="1aef" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下图显示了两个用户类别之间存在的差异。对于流失客户来说，错误、降级和否决的数量更高，这是有道理的。然而，起初，我发现大多数其他功能对客户流失有更高的价值是不寻常的。一个看似合理的解释是，流失客户确实是更活跃的用户，他们离开了 Sparkify，用更好的音乐替代了它。换句话说，也许顾客在离开之前对 Sparkify 并不完全满意。或许 Sparkifys 的竞争更胜一筹。</p><figure class="mx my mz na gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/b089e44e85c2f99fa761967e3593648a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_prNEgxNyFeMKQHiFQ3_eQ.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">用户特征的蝶形图</p></figure><p id="9c1b" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该图证明了这两个用户群之间确实存在明显的差异，因此分类模型可能会成功预测流失的客户。</p><h2 id="c481" class="me lh je bd li mf mg dn lm mh mi dp lq km mj mk lu kq ml mm ly ku mn mo mc mp bi translated">5.建模</h2><p id="f00b" class="pw-post-body-paragraph kb kc je kd b ke mq kg kh ki mr kk kl km ms ko kp kq mt ks kt ku mu kw kx ky im bi translated">我们准备试用一些精选的型号。PySpark 有一个非常好的<a class="ae mv" href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html" rel="noopener ugc nofollow" target="_blank">机器学习库</a>，它支持一系列的<a class="ae mv" href="https://spark.apache.org/docs/2.2.0/ml-classification-regression.html" rel="noopener ugc nofollow" target="_blank"> ML 算法</a>。</p><p id="4095" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于客户流失分析，以下软件包就足够了:</p><figure class="mx my mz na gt iv"><div class="bz fp l di"><div class="nw nx l"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">导入一些 PySpark ML 包</p></figure><p id="f5f4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在第一行中，我们导入 VectorAssembler。这是 PySpark 特有的步骤，因为 Pyspark 要求将所有特征值作为一个列表添加到一个输入列中，输入到模型中。</p><p id="6609" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我们导入我们想要尝试的模型，以及它们的评估者。还引入了流水线方法，这有助于训练过程更加连贯。最后导入 CrossValidator、ParameterGridBuilder 等模型训练优化的工具。</p><p id="2d5a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们可以开始构建 ML 基础设施。</p><figure class="mx my mz na gt iv"><div class="bz fp l di"><div class="nw nx l"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">PySpark 中的 ML 设置步骤</p></figure><p id="b8f3" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">此阶段通常的设置步骤是:</p><ul class=""><li id="673c" class="nh ni je kd b ke kf ki kj km nj kq nk ku nl ky nm nn no np bi translated">将数据分为训练和测试。</li><li id="3c15" class="nh ni je kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">创建保存所有特征值的列(<em class="nf"> inputFeatures) </em>。</li><li id="44e4" class="nh ni je kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">特征值然后被归一化并传递到输出列(<em class="nf">特征)</em></li><li id="717e" class="nh ni je kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">模特被称为</li><li id="78c8" class="nh ni je kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">这些步骤都被连接成一个管道</li></ul><p id="186e" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们已经准备好训练模型，让我们从简单的<strong class="kd jf">逻辑回归</strong>开始</p><figure class="mx my mz na gt iv"><div class="bz fp l di"><div class="nw nx l"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">逻辑回归的模型训练</p></figure><p id="df90" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在 ML 流程的这一部分，我们正在运行模型训练并优化参数。</p><p id="1613" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">步骤:</p><ul class=""><li id="f6f6" class="nh ni je kd b ke kf ki kj km nj kq nk ku nl ky nm nn no np bi translated">创建参数网格(可选，但推荐)</li><li id="eb07" class="nh ni je kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">将评估指标传递给分类评估员</li><li id="6644" class="nh ni je kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">定义交叉验证器中的折叠数(可选，但推荐)</li></ul><p id="a245" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ParamGridBuilder 对象可以保存特定 ML 算法支持的所有参数值。在上面的逻辑回归的例子中，我决定设置正则化参数和最大迭代次数的值。您可以设置许多其他值和参数，但请注意，对于给定的每个参数值，模型训练都将再次运行(这可能代价很高)。</p><p id="4525" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为手头的任务选择的度量是 F1。</p><p id="b82f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在交叉验证步骤中，我加入了前面的两个步骤，并设置了 K 倍交叉验证参数的值。K-fold 参数定义了训练将被重复多少次，每次使用不同的测试数据子集。</p><p id="d257" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">模型被训练和拟合。最终结果是，我们的 F1 指标产生了<strong class="kd jf"> 0.69 </strong></p><p id="5ed8" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于第一次尝试来说还不错。</p><p id="f595" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们使用更高级的模型，比如<strong class="kd jf">随机森林分类器</strong></p><figure class="mx my mz na gt iv"><div class="bz fp l di"><div class="nw nx l"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">随机森林的 F1 度量</p></figure><p id="e4ef" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">随机森林产生更好的结果 F1= <strong class="kd jf"> 0.8 </strong></p><p id="52a8" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这次我跳过了参数网格的输入，因为(即使在 AWS EMR 上)训练过程相当慢。加快速度的一个方法是增加集群的大小。你可以随意自己做，你可以在这里找到要调整的参数。</p><p id="8632" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我尝试的最后一个模型是<strong class="kd jf">梯度提升树分类器</strong></p><figure class="mx my mz na gt iv"><div class="bz fp l di"><div class="nw nx l"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">GBT 分类器的 F1 度量</p></figure><p id="66f1" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">GBT 分类器的表现略好于随机森林，F1= <strong class="kd jf"> 0.81 </strong></p><p id="69d5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">看起来我们有一个赢家！</p><h2 id="1275" class="me lh je bd li mf mg dn lm mh mi dp lq km mj mk lu kq ml mm ly ku mn mo mc mp bi translated">6.形象化</h2><p id="5427" class="pw-post-body-paragraph kb kc je kd b ke mq kg kh ki mr kk kl km ms ko kp kq mt ks kt ku mu kw kx ky im bi translated">让我们可视化我们获胜模型的 ROC 曲线，只是为了证明 AWS EMR 上的 Jupyter 笔记本可以支持可视化，正如预期的那样。</p><p id="fc34" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将使用 Matplotlib 进行可视化，但是我们需要首先在 AWS EMR 上安装这个包。这里需要注意的一点是，软件包安装不能与通常的<em class="nf"> pip 安装</em>一起工作。相反，软件包的安装如下所示:</p><figure class="mx my mz na gt iv"><div class="bz fp l di"><div class="nw nx l"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">PySpark 上的软件包安装</p></figure><p id="fa2d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后要考虑的是，AWS EMR 上 Jupyter 中的 plot 命令需要括在<strong class="kd jf"><em class="nf">PLT . clf()</em></strong><em class="nf"/>和<strong class="kd jf"> <em class="nf"> %matplot plt </em> </strong>魔法函数之间。</p><figure class="mx my mz na gt iv"><div class="bz fp l di"><div class="nw nx l"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">注意 plt.clf()和%matplot plt 神奇函数</p></figure><p id="5f18" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是我们的 ROC 曲线</p><figure class="mx my mz na gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/2d1a47cd432f585ee904642fe420559f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lVMRUxRa5pp5-k3KZFX-Wg.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">GBT 分类器的 ROC 曲线</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="ecb0" class="me lh je bd li mf mg dn lm mh mi dp lq km mj mk lu kq ml mm ly ku mn mo mc mp bi translated">结论</h2><p id="9c41" class="pw-post-body-paragraph kb kc je kd b ke mq kg kh ki mr kk kl km ms ko kp kq mt ks kt ku mu kw kx ky im bi translated">我们的分析已经结束了！</p><p id="854e" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们设法利用 PySpark 和 AWS EMR 的强大功能，通过庞大的数据集，对其进行压缩并提取有用的特征。最后，我们训练了一个模型，预测客户流失与体面的表现！</p><p id="ed20" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但是没有一个 ML 过程是完全结束的，并且总是有改进的空间。我们的分析也可以在几个方面得到加强:</p><ul class=""><li id="13ca" class="nh ni je kd b ke kf ki kj km nj kq nk ku nl ky nm nn no np bi translated">一些共享强相关性的特征应该被排除在模型之外</li><li id="2b76" class="nh ni je kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">新的和更复杂的特征可以被设计并添加到模型中</li><li id="a516" class="nh ni je kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">尝试其他 ML 算法</li><li id="4681" class="nh ni je kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">在参数网格中包含更多参数值</li></ul><p id="bc6a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我希望这篇文章能让你更熟悉在处理大数据时可以采用的分析/ML 方法。PySpark 和 AWS EMR 是强大的技术，使大数据处理对更广泛的公众可用且负担得起。下次这个庞大的数据集出现时，您将做好准备！</p><p id="dc2f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="nf">本文有意省略了一些数据处理部分，以便更加关注围绕模型训练的 PySpark 步骤。如果您希望详细查看客户流失分析，您可以在此</em>  <em class="nf">找到代码</em> <a class="ae mv" href="https://github.com/maleckicoa/Sparkify-Project" rel="noopener ugc nofollow" target="_blank"> <em class="nf">。</em></a></p></div></div>    
</body>
</html>