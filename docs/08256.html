<html>
<head>
<title>What are Loss Functions?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是损失函数？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-loss-function-1e2605aeb904?source=collection_archive---------4-----------------------#2020-06-17">https://towardsdatascience.com/what-is-loss-function-1e2605aeb904?source=collection_archive---------4-----------------------#2020-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/dbd269e421b5607d0308d5a29d27f0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACbDS2jrLif4hrTcUNA4cg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来源:米里亚姆·埃斯帕奇</p></figure><div class=""/><div class=""><h2 id="bf07" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">在关于激活功能的帖子之后，我们将潜入第二部分，将<strong class="ak"> <em class="kv">【损失】</em> </strong>或<strong class="ak"> <em class="kv">目标</em> </strong> <em class="kv"> </em> <strong class="ak"> <em class="kv">功能</em> </strong>用于神经网络</h2></div><p id="7494" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">包含所有代码的笔记本可以在这里找到:<a class="ae jd" href="https://github.com/Christophe-pere/Loss_function" rel="noopener ugc nofollow" target="_blank"> GitHub </a>你可以找到代码来生成不同类型的数据集和神经网络来测试损失函数。</p><p id="ce5d" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了理解什么是<em class="ls">损失函数，</em>这里引用一下学习过程:</p><blockquote class="lt lu lv"><p id="4ab7" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">一种衡量算法是否做得很好的方法-这是确定算法的当前输出与其预期输出之间的距离所必需的。该测量被用作反馈信号来调整算法的工作方式。这个调整步骤就是我们所说的<em class="jg">学习</em>。<br/>Fran ois Chollet，Python深度学习(2017)，Manning，第1章，第6页</p></blockquote><p id="bc17" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky jh">损失函数</strong>是<strong class="ky jh">计算算法当前输出和期望输出</strong>之间距离的函数。这是一种评估算法如何对数据建模的方法。它可以分为两类。一个用于<strong class="ky jh">分类</strong>(离散值，0，1，2…)，另一个用于<strong class="ky jh">回归</strong>(连续值)。</p><h2 id="7b58" class="lz ma jg bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">训练一个神经网络常用的损失函数有哪些？</h2><h1 id="b7ec" class="ms ma jg bd mb mt mu mv me mw mx my mh km mz kn mk kp na kq mn ks nb kt mq nc bi translated">内容</h1><ul class=""><li id="8f12" class="nd ne jg ky b kz nf lc ng lf nh lj ni ln nj lr nk nl nm nn bi translated">交叉熵</li><li id="f839" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">原木损失</li><li id="45c9" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">指数损失</li><li id="e0fd" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">铰链损耗</li><li id="534a" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">Kullback Leibler发散损失</li><li id="48b4" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">均方误差(MSE — L2)</li><li id="f5de" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">平均绝对误差(美-L1)</li><li id="68e5" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">胡伯损失</li></ul><p id="274e" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些软件包是必需的:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="625a" class="lz ma jg ny b gy oc od l oe of">%matplotlib inline<br/>import keras.backend as K<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span></pre><h1 id="c76b" class="ms ma jg bd mb mt mu mv me mw mx my mh km mz kn mk kp na kq mn ks nb kt mq nc bi translated"><strong class="ak">分类</strong></h1><h2 id="cc1f" class="lz ma jg bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated"><strong class="ak">交叉熵</strong></h2><p id="a918" class="pw-post-body-paragraph kw kx jg ky b kz nf kh lb lc ng kk le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">该函数来自信息论，其目标是<strong class="ky jh">测量信息分布比特数的两个平均值之差</strong>。<strong class="ky jh">交叉熵</strong>作为<strong class="ky jh">对数损失函数</strong>(不相同，但它们测量相同的东西)计算两个概率分布函数之间的差异。</p><blockquote class="lt lu lv"><p id="87b6" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated"><strong class="ky jh">熵</strong>是传输从概率分布中随机选择的事件所需的比特数。偏斜分布具有低熵，而事件概率相等的分布具有较大的熵。</p><p id="962f" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">在信息论中，我们喜欢描述一个事件的<em class="jg">惊喜</em>。低概率事件更令人惊讶，因此有更多的信息。而事件同样可能发生的概率分布更令人惊讶，熵也更大。</p><p id="aecc" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated"><strong class="ky jh"> -偏态概率分布</strong> ( <em class="jg">不足为奇</em>):低熵。</p><p id="e03f" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated"><strong class="ky jh"> -均衡概率分布</strong> ( <em class="jg">惊人</em>):高熵。</p></blockquote><p id="1146" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">交叉熵是机器学习中最常用的一类损失函数，因为它导致更好的泛化模型和更快的训练。</p><p id="f168" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">交叉熵可用于二元和多类分类问题(许多类具有一个标签，不同于许多类具有称为多标签分类的多标签)。</p><p id="fca8" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky jh">交叉熵的类型</strong>:</p><ul class=""><li id="b0b5" class="nd ne jg ky b kz la lc ld lf oj lj ok ln ol lr nk nl nm nn bi translated">二元交叉熵:用于二元分类问题</li><li id="f9e5" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">分类交叉熵:二进制和多类问题，标签需要被编码为分类的，一键编码表示(对于3类:[0，1，0]，[1，0，0]…)</li><li id="6a1c" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">稀疏交叉熵:二元和多类问题(标签是整数——0或1或… n，取决于标签的数量)</li></ul><p id="3182" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这类损失函数的取值范围:</p><ul class=""><li id="d91a" class="nd ne jg ky b kz la lc ld lf oj lj ok ln ol lr nk nl nm nn bi translated"><strong class="ky jh"> 0.00 </strong>:完美概率</li><li id="2c1f" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky jh"> &lt; 0.02 </strong>:大概率</li><li id="7356" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky jh"> &lt; 0.05 </strong>:良好</li><li id="1674" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky jh"> &lt; 0.20 </strong>:太好了</li><li id="3a88" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky jh"> &gt; 0.30 </strong>:不大</li><li id="ce64" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">1.00 :地狱</li><li id="6315" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky jh"> &gt; 2.00 </strong>有些东西不工作</li></ul><h2 id="14dd" class="lz ma jg bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated"><strong class="ak">原木损耗</strong></h2><p id="c1ee" class="pw-post-body-paragraph kw kx jg ky b kz nf kh lb lc ng kk le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">对数损失是高达1 / log(2)因子的二进制交叉熵。这个损失函数是凸的，并且对于负值线性增长(对异常值不太敏感)。使用对数损失的常用算法是<strong class="ky jh"> <em class="ls">逻辑回归</em> </strong>。</p><blockquote class="lt lu lv"><p id="15ee" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">二元分类问题的负对数似然性通常简称为“对数损失”,作为逻辑回归的损失函数。</p><p id="965f" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">-对数损失=负对数似然，符合伯努利概率分布</p><p id="a668" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">对于分类问题，“<em class="jg">对数损失</em>”、“<em class="jg">交叉熵</em>”和“<em class="jg">负对数似然</em>”可以互换使用。</p><p id="e831" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">更一般地，术语“<em class="jg">交叉熵</em>”和“<em class="jg">负对数似然</em>”在分类模型的损失函数的上下文中可互换使用。</p></blockquote><h2 id="aab6" class="lz ma jg bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">指数损失</h2><p id="7e20" class="pw-post-body-paragraph kw kx jg ky b kz nf kh lb lc ng kk le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">指数损失被设计在Adaboost算法的开始，该算法贪婪地优化它。数学形式为:<br/>exp _ loss = 1/m * sum(exp(-y * f(x)))</p><p id="9faa" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以这样编码:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="56b3" class="lz ma jg ny b gy oc od l oe of">def exponential_loss(y_pred, y_true):<br/>    return np.mean(np.exp(- y_pred * y_true))</span></pre><p id="6c7a" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果如下所示:</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/1c3337785e75d3aed329d61654ec82ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wxku2jfvO6lo9JNgp2UHlg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">指数损失与错误分类(如果y&lt;0，则为1，否则为0)</p></figure><h2 id="d071" class="lz ma jg bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">铰链损耗</h2><p id="9774" class="pw-post-body-paragraph kw kx jg ky b kz nf kh lb lc ng kk le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">在分类任务中，铰链损失函数被用来修正SVM算法的超平面。目标是在没有被正确预测或离超平面太近的点上进行不同的惩罚。</p><p id="db32" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它的数学公式是Hinge = max(0，1-y*f(x))和相应的代码:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="2a0f" class="lz ma jg ny b gy oc od l oe of">def Hinge(y_pred, y_true):<br/>    return np.max([0., 1. - y_pred * y_true])</span></pre><p id="9172" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个方程的结果是凸的，但不可微，需要一个次梯度算法来优化。</p><p id="491a" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果如下所示:</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/96c463af9f1845312127e643bc224c38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*phZL0v6-AvisZYsgC5Qxzg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">铰链损失vs误分类(1 if y &lt;0 else 0)</p></figure><h2 id="48ca" class="lz ma jg bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">Kullback Leibler Divergence Loss</h2><p id="6819" class="pw-post-body-paragraph kw kx jg ky b kz nf kh lb lc ng kk le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">The KL divergence is the score of two different probability distribution functions. The KL difference between a PDF of q(x) and a PDF of p(x) is noted KL(Q||P) where || means <em class="ls">散度</em>(不是对称KL(P||Q)！= KL(Q||P))。</p><p id="70cf" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">KL(Q | | P)=-sum(Q(x)* log(P(x)/Q(x))或sum(q(x)*log(q(x)/p(x))</p><p id="f9d0" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">求和是针对离散情况的，积分是针对连续情况的。这意味着，对于相同的数据，如果q(x)的PDF大，而p(x)的PDF小，则散度增加。在机器学习中，你可以将这表示为预测和基本事实之间的差异。</p><p id="7d20" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下代码显示了如何将KL散度用于预测和地面实况:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="3018" class="lz ma jg ny b gy oc od l oe of">def kl_divergence(y_true, y_pred):<br/>    return y_true * np.log(y_true / y_pred)</span></pre><p id="1312" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里可以显示一个简单的可视化:</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/6a5efc9185f6d76255f901cce649cc97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4PvmefkzzrK5ZA218m31Q.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来源:维基百科</p></figure><blockquote class="lt lu lv"><p id="17f1" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">因此，KL散度通常被称为“<em class="jg">相对熵</em></p><p id="5637" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated"><strong class="ky jh"> -交叉熵</strong>:用Q而不是p来表示一个事件的平均总位数</p><p id="5839" class="kw kx ls ky b kz la kh lb lc ld kk le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated"><strong class="ky jh"> -相对熵</strong> ( <em class="jg"> KL散度</em>):从Q而不是p来表示事件的平均额外比特数。</p></blockquote><h1 id="0f11" class="ms ma jg bd mb mt mu mv me mw mx my mh km mz kn mk kp na kq mn ks nb kt mq nc bi translated"><strong class="ak">回归</strong></h1><h2 id="49ee" class="lz ma jg bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">均方误差损失(也称为L2正则化)</h2><p id="567c" class="pw-post-body-paragraph kw kx jg ky b kz nf kh lb lc ng kk le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">它是当前输出<em class="ls"> y_pred </em>和预期输出<em class="ls"> y_true </em>的平方差除以输出数。<strong class="ky jh">MSE函数对异常值非常敏感</strong>，因为差值是给予异常值更多重要性的平方。如果我们必须预测所有目标的一个值，预测值应该是平均值。</p><p id="d55b" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是这样表述的:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="542d" class="lz ma jg ny b gy oc od l oe of">def mean_square_error(y_true, y_pred):<br/>    return K.mean(K.square(y_true-y_pred), axis=-1)</span></pre><p id="fe82" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以将MSE函数的行为可视化，将一系列值(此处为-10000到10000)与一个常数值(此处为100)进行比较:</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/d9f9bd0a6b6ac3210574c579c6c738f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KXPuHcCAcQhJn3u_e4Www.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">均方误差损失的行为</p></figure><p id="3969" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该行为是对梯度下降算法特别有用的二次曲线。越接近最小值，梯度越小。如果异常值对问题很重要，则MSE非常有用。如果异常值是有噪声的、错误的数据或错误的测量值，则应使用MAE损失函数。</p><h2 id="862e" class="lz ma jg bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">平均绝对误差损失(也称为L1正则化)</h2><p id="5cbd" class="pw-post-body-paragraph kw kx jg ky b kz nf kh lb lc ng kk le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">在前一个损失函数的差值处，平方被一个绝对值代替。这种差异对具有“V”形的损失函数的行为有很大的影响。每个点的梯度都是相同的，即使值接近最小值(可能会产生跳跃)。它需要动态修改学习速率，以减少接近最小值的步长。MAE函数对异常值更稳健，因为它基于与MSE的平方相比的绝对值。这就像一个中位数，离群值不能真正影响她的行为。</p><p id="a3f3" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以像这样轻松地实现它:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="cc4e" class="lz ma jg ny b gy oc od l oe of">def mean_square_error(y_true, y_pred):<br/>    return K.mean(K.abs(y_true-y_pred), axis=-1)</span></pre><p id="d068" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以将一系列值(此处为-10000到10000)与一个常量值(此处为100)进行比较，从而直观地了解MAE函数的行为:</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/1d8b518fb976475280bab935ad3051af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_G-_BWSB9TGCdEEWg4jI3g.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">平均绝对误差损失的行为</p></figure><h2 id="dff9" class="lz ma jg bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">均方对数误差</h2><h2 id="17ff" class="lz ma jg bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">胡伯损失</h2><p id="2d60" class="pw-post-body-paragraph kw kx jg ky b kz nf kh lb lc ng kk le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">Huber损失是MAE和MSE(L1-L2)的组合，但它取决于一个影响损失函数形状的附加参数delta。这个参数需要通过算法进行微调。<strong class="ky jh">当值很大(远离最小值)时，函数具有MAE的行为，接近最小值时，函数的行为类似于MSE </strong>。所以<strong class="ky jh"> <em class="ls"> delta </em> </strong>参数就是你对离群值的敏感度。胡伯损失的数学形式是:</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/621266975fcf4ebba4b94165d38d4e30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*M0BeXxKh1khQ_XF9-E1qfw.png"/></div></figure><p id="bf43" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以用两种方式实现这个函数，这里我给出一个函数，其中所有的块都是显式的。</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="8822" class="lz ma jg ny b gy oc od l oe of"><em class="ls"># custom huber loss function </em><br/>def huber_loss_error(y_true, y_pred, delta=0.1):<br/>    res = []<br/>    for i in zip(y_true, y_pred):<br/>        if abs(i[0]-i[1])&lt;=delta:<br/>            res.append(0.5*((i[0]-i[1])**2))<br/>        else:<br/>            res.append(delta*((abs(i[0]-i[1]) )-0.5*(delta**2)))<br/> <em class="ls"># can also be write as:<br/> # np.where(np.abs(y_true-y_pred) &lt; delta, 0.5*(y_true-y_pred)**2 , delta*(np.abs(y_true-y_pred)-0.5*delta))</em><br/>    return res # np.sum(res)</span></pre><p id="550b" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以将一系列值(此处为-10到10)与一个常数值(此处为0)进行比较，从而直观地了解Huber损失函数的行为:</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/4d62b4c4b24abcc73315e7bc57d28278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*26Ul_zA6TwsFSupEczxzgg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">胡伯损失行为</p></figure><p id="f8cc" class="pw-post-body-paragraph kw kx jg ky b kz la kh lb lc ld kk le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Huber损耗允许对较大的数字有较大的梯度，但当值变小时梯度减小。但是，这个函数需要微调增量，但计算量很大。为了避免这种情况，您可以使用Log-Cosh损失(本文中没有解释，但您可以在下一个图中看到它们之间的差异)。</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/f7ab7a422ade8d7e8e55ed05f64d4be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywHVRJCq0fbZIP25Onl4kw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Huber损失函数(δ= 1)和Log-Cosh损失函数的比较</p></figure><h2 id="5c5d" class="lz ma jg bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">参考资料:</h2><ul class=""><li id="ba95" class="nd ne jg ky b kz nf lc ng lf nh lj ni ln nj lr nk nl nm nn bi translated"><a class="ae jd" href="https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/</a></li><li id="ec48" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://keras.io/api/losses/regression_losses/#meansquaredlogarithmicerror-class" rel="noopener ugc nofollow" target="_blank">https://keras . io/API/losses/regression _ losses/# meansqualdlogarithmicerror-class</a></li><li id="220b" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+J" rel="noopener ugc nofollow" target="_blank">许静怡</a>等，<a class="ae jd" href="https://arxiv.org/abs/1711.11157" rel="noopener ugc nofollow" target="_blank">一种利用符号知识进行深度学习的语义损失函数(2017) </a>，arxiv</li><li id="e054" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/kull back % E2 % 80% 93 lei bler _ divergence</a></li><li id="aa42" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/kl-divergence-python-example-b87069e4b810">https://towards data science . com/KL-divergence-python-example-b 87069 e4b 810</a></li><li id="624f" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://machinelearningmastery.com/divergence-between-probability-distributions/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/divergence-between-probability-distributions/</a></li><li id="5b13" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://dibyaghosh.com/blog/probability/kldivergence.html" rel="noopener ugc nofollow" target="_blank">https://dibyaghosh.com/blog/probability/kldivergence.html</a></li><li id="6394" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://algorithmia.com/blog/introduction-to-loss-functions#:~:text=There%20are%20several%20different%20common,of%20loss%20functions%20is%20discussed." rel="noopener ugc nofollow" target="_blank">https://algorithm ia . com/blog/introduction-to-loss-functions #:~:text =这里讨论了% 20几种% 20不同% 20常见% 20损失% 20函数% 20。</a></li><li id="c5ed" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="http://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">伊恩·古德菲勒、约舒阿·本吉奥、亚伦·库维尔，《深度学习》(2016)，麻省理工学院出版社</a></li><li id="3bc2" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://www.manning.com/books/deep-learning-with-python" rel="noopener ugc nofollow" target="_blank">Fran ois Chollet，用Python进行深度学习(2017)，Manning </a></li><li id="ce36" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://github.com/fchollet/deep-learning-with-python-notebooks" rel="noopener ugc nofollow" target="_blank">Fran ois Chollet，</a>该书的配套Jupyter笔记本<a class="ae jd" href="https://github.com/fchollet/deep-learning-with-python-notebooks" rel="noopener ugc nofollow" target="_blank">用Python进行深度学习(2017)，github </a></li><li id="62ef" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#kullback-leibler" rel="noopener ugc nofollow" target="_blank">https://ml-cheat sheet . readthedocs . io/en/latest/loss _ functions . html</a></li><li id="4df9" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/common-loss-functions-in-machine-learning-46af0ffc4d23">https://towards data science . com/common-loss-functions-in-machine-learning-46 af 0 ffc 4d 23</a></li><li id="0348" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://medium.com/@phuctrt/loss-functions-why-what-where-or-when-189815343d3f" rel="noopener">https://medium . com/@ phu ctrt/loss-functions-why-what-where-when-189815343 d3f</a></li><li id="f84a" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://algorithmia.com/blog/introduction-to-loss-functions" rel="noopener ugc nofollow" target="_blank">https://algorithmia.com/blog/introduction-to-loss-functions</a></li><li id="8a8a" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/loss-and-loss-functions-for-training-deep-learning-neural-networks/</a></li><li id="c7b7" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Loss_function#:~:text=In%20mathematical%20optimization%20and%20decision,cost%22%20associated%20with%20the%20event." rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Loss _ function #:~:text =在% 20数学% 20优化% 20和% 20决策，成本% 22% 20关联% 20与% 20事件。</a></li><li id="2f46" class="nd ne jg ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">墨菲，凯文，<em class="ls">《机器学习:概率视角》</em> (2012)，麻省理工学院</li></ul></div></div>    
</body>
</html>