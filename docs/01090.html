<html>
<head>
<title>Deep Learning’s mathematics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learnings-mathematics-f52b3c4d2576?source=collection_archive---------8-----------------------#2020-01-31">https://towardsdatascience.com/deep-learnings-mathematics-f52b3c4d2576?source=collection_archive---------8-----------------------#2020-01-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/19351a90bc258e5d92d96d3daecea8b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LhHcwUIwxjq8BduJ8BvgNw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">罗马法师在<a class="ae jg" href="https://unsplash.com/s/photos/mathematics?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="9e12" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">探索深度学习成功背后的数学和方程式</h2></div><p id="c581" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">深度学习是基于人工神经网络的机器学习科学的一个分支。它有几个衍生物，如多层感知器-MLP，卷积神经网络-CNN-和递归神经网络-RNN-可应用于许多领域，包括计算机视觉，自然语言处理，机器翻译…</p><p id="d46d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">深度学习的兴起有三个主要原因:</p><ul class=""><li id="6dc1" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la jk">本能特征工程</strong>:虽然大多数机器学习算法需要人类的专业知识来进行特征工程和提取，但深度学习会自动处理变量及其权重的选择</li><li id="13cf" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">巨大的数据集:持续不断的数据收集产生了大型数据库，这使得更深层次的神经网络成为可能</li><li id="ee3c" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><strong class="la jk">硬件发展</strong>:用于图形处理单元的新GPU允许更快的代数计算，这是DL的核心基础</li></ul><p id="de8d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇博客中，我们将主要关注多层感知器-MLP-我们将详细介绍深度学习成功背后的数学背景，并探索用于提高其性能的优化算法。</p><p id="f1b6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">摘要如下:</p><ol class=""><li id="a2cb" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt mi ma mb mc bi translated">定义</li><li id="02ac" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt mi ma mb mc bi translated">学习算法</li><li id="7fbf" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt mi ma mb mc bi translated">参数初始化</li><li id="2f79" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt mi ma mb mc bi translated">正向-反向传播</li><li id="0c2e" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt mi ma mb mc bi translated">激活功能</li><li id="a627" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt mi ma mb mc bi translated">最优化算法</li></ol><p id="ac54" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意:因为Medium不支持LaTeX，所以数学表达式是作为图像插入的。因此，为了更好的阅读体验，我建议你关闭黑暗模式。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="7870" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">1-定义</h1><h2 id="265f" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">神经元</h2><blockquote class="nu nv nw"><p id="e769" class="ky kz nx la b lb lc kk ld le lf kn lg ny li lj lk nz lm ln lo oa lq lr ls lt im bi translated">它是一组连接实体的数学运算</p></blockquote><p id="bc06" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们考虑这样一个问题，我们根据房子的大小来估计房子的价格，它可以被图解如下:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/bc5c75295ec67b3b534318aedf3e541a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AgPX07eNaA_26GcdP1zePg.png"/></div></div></figure><blockquote class="nu nv nw"><p id="162f" class="ky kz nx la b lb lc kk ld le lf kn lg ny li lj lk nz lm ln lo oa lq lr ls lt im bi translated">一般来说，神经网络更好地被称为MLP，意为“多层感知器”，是一种直接形式的神经网络，被组织成若干层，其中信息仅从输入层流向输出层。</p><p id="467f" class="ky kz nx la b lb lc kk ld le lf kn lg ny li lj lk nz lm ln lo oa lq lr ls lt im bi translated">每一层都由一定数量的神经元组成，我们区分:</p><p id="487a" class="ky kz nx la b lb lc kk ld le lf kn lg ny li lj lk nz lm ln lo oa lq lr ls lt im bi translated">-输入层</p><p id="4a3c" class="ky kz nx la b lb lc kk ld le lf kn lg ny li lj lk nz lm ln lo oa lq lr ls lt im bi translated">-隐藏层</p><p id="df8e" class="ky kz nx la b lb lc kk ld le lf kn lg ny li lj lk nz lm ln lo oa lq lr ls lt im bi translated">-输出层</p></blockquote><p id="7c46" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图表示一个神经网络，其输入端有5个神经元，第一个隐藏层有3个，第二个隐藏层有3个，输出端有2个。</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi og"><img src="../Images/0835784366c0b24d119aba53fd568c10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_5P5-6IM938jHAMdrppETA.png"/></div></div></figure><p id="87b1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">隐含层中的一些变量可以根据输入特征来解释:在房屋定价的情况下，在假设第一个隐含层的第一个神经元更关注变量<em class="nx">x1</em>et<em class="nx">x2</em>的情况下，可以解释为例如房屋的家庭规模的量化。</p><h2 id="4d68" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">作为监督任务的DL</h2><p id="d53b" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">在大多数DL问题中，我们倾向于使用一组变量X来预测输出y，在这种情况下，我们假设对于数据库的每一行<em class="nx"> X_i </em>我们都有相应的预测<em class="nx"> y_i </em>，因此有标记的数据。</p><p id="2a4d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">应用</strong>:房地产、语音识别、图像分类……</p><p id="0a3b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用的数据可以是:</p><ul class=""><li id="5d2c" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">结构化:明确定义了特性的数据库</li><li id="9298" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">非结构化:音频、图像、文本……</li></ul><h2 id="0df8" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">通用逼近定理</h2><p id="4b9d" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">现实生活中的深度学习是给定函数<em class="nx"> f </em>的近似。由于以下定理，这种近似是可能的和精确的:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/f32330cf8a67a6571cd817172939cfdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rAyOtPcTCrwee9E3OvdZaA.png"/></div></div></figure><p id="fdcd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(*)在有限维中，如果一个集合是闭且有界的，则称它是紧的。更多详情，请访问此<a class="ae jg" href="https://en.wikipedia.org/wiki/Compact_space" rel="noopener ugc nofollow" target="_blank">链接</a>。<br/>这种算法的主要优点是深度学习允许解决任何可以用数学表达的问题</p><h2 id="1ab3" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">数据预处理</h2><p id="e532" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">一般来说，在任何机器学习项目中，我们将数据分为3组:</p><blockquote class="nu nv nw"><p id="8488" class="ky kz nx la b lb lc kk ld le lf kn lg ny li lj lk nz lm ln lo oa lq lr ls lt im bi translated"><strong class="la jk"> -训练集</strong>:用于训练算法和构造批次</p><p id="d354" class="ky kz nx la b lb lc kk ld le lf kn lg ny li lj lk nz lm ln lo oa lq lr ls lt im bi translated"><strong class="la jk"> - Dev set </strong>:用于微调算法，评估偏差和方差</p><p id="7319" class="ky kz nx la b lb lc kk ld le lf kn lg ny li lj lk nz lm ln lo oa lq lr ls lt im bi translated"><strong class="la jk"> -测试集</strong>:用于概括最终算法的误差/精度</p></blockquote><p id="a75d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下表根据数据集<em class="nx"> m </em>的大小总结了三个数据集的重新划分:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/ee431cfc9b97f0856bd2663ced51354c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bm3pTZeq9YiSPz6gzfRZlA.png"/></div></div></figure><p id="2e26" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">标准的深度学习算法需要一个大的数据集，其中样本数量大约为行。现在数据已经准备好了，我们将在下一节看到训练算法。通常，在分割数据之前，我们还会对输入进行归一化处理，这一步将在本文后面详细介绍。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="917c" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">2.学习算法</h1><p id="4b98" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">神经网络中的学习是计算与整个网络中各种回归相关的参数权重的步骤。换句话说，我们的目标是从输入开始，找到给出真实值的最佳预测/近似的最佳参数。<br/>为此，我们定义了一个目标函数，称为<code class="fe oo op oq or b">loss function</code>，记为<code class="fe oo op oq or b">J</code>，它量化了整个训练集的实际值和预测值之间的距离。我们通过以下两个主要步骤来最小化J:</p><blockquote class="nu nv nw"><p id="fb23" class="ky kz nx la b lb lc kk ld le lf kn lg ny li lj lk nz lm ln lo oa lq lr ls lt im bi translated"><code class="fe oo op oq or b"><strong class="la jk">- Forward Propagation</strong></code>:我们通过网络完整地或成批地传播数据，并且我们计算该批上的损失函数，该损失函数只不过是在不同行的预测输出处提交的误差的总和。</p><p id="8659" class="ky kz nx la b lb lc kk ld le lf kn lg ny li lj lk nz lm ln lo oa lq lr ls lt im bi translated"><code class="fe oo op oq or b"><strong class="la jk">- Backpropagation</strong></code>:包括计算关于不同参数的成本函数的梯度，然后应用下降算法来更新它们。</p></blockquote><p id="87fd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们多次重复同样的过程，称为<code class="fe oo op oq or b">epoch number</code>。定义架构后，学习算法编写如下:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/6e54303966f164d7c475f65a19c4ae85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ScizItZtlgGLfL5tIGwf-Q.png"/></div></div></figure><p id="bc14" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(∫)成本函数<em class="nx"> L </em>计算单个点上实际值和预测值之间的距离。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="ff15" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">3.参数初始化</h1><p id="b01a" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">定义神经网络结构后的第一步是参数初始化。这相当于将初始噪声注入模型的权重。</p><ul class=""><li id="28f8" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la jk">零初始化:</strong>我们可以考虑用0来初始化参数，即:W=0，b=0</li></ul><p id="7f1a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用正向传播方程，我们注意到所有隐藏单元将是对称的，这不利于学习阶段。</p><ul class=""><li id="1c5d" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la jk">随机初始化:</strong>这是一种常用的替代方法，包括在参数中注入随机噪声。如果噪声太大，一些激活函数可能会饱和，这可能会影响梯度的计算。</li></ul><p id="8664" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">两种最著名的初始化方法是:</p><ul class=""><li id="0094" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><code class="fe oo op oq or b">Xavier</code>的:它包括用从服从正态分布的中心变量中随机抽样的值填充参数；</li></ul><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/ddf41ff544f4d7932065db0824ac5031.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3hZRAhhwBufChcdv3AKmvA.png"/></div></div></figure><ul class=""><li id="b0d5" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><code class="fe oo op oq or b">Glorot</code>的:相同的方法有不同的方差:</li></ul><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/1036716df9d9d7f77072239fc170ca9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Raq-95unTFOofEC4_rtEnQ.png"/></div></div></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="267a" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">4.正向和反向传播</h1><p id="f68c" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">在深入研究深度学习背后的代数之前，我们将首先设置注释，该注释将用于解释正向和反向传播的方程。</p><h2 id="2f68" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated"><strong class="ak">神经网络的表示法</strong></h2><p id="363b" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">神经网络是一系列的<code class="fe oo op oq or b">regressions</code>后跟一个<code class="fe oo op oq or b">activation function</code>。它们都定义了我们所说的正向传播。并且是每层的<strong class="la jk">学习参数</strong>。反向传播也是一系列从输出到输入的代数运算。</p><h2 id="a34f" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">正向传播</h2><ul class=""><li id="5388" class="lu lv jj la b lb oh le oi lh ov ll ow lp ox lt lz ma mb mc bi translated"><strong class="la jk">通过网络代数</strong></li></ul><p id="601a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们考虑具有如下<code class="fe oo op oq or b">L layers</code>的神经网络:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/abb3778f5eb5570e830c23388ed33fb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-gNu03HlMMlr6hlgXA6L0g.png"/></div></div></figure><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/42982bc2117fc056c79280729aa276ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-gQVEPeTOXrgB6I3P248Q.png"/></div></div></figure><h2 id="5d94" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">通过训练集的代数</h2><p id="b41f" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">让我们考虑通过神经网络预测单行数据帧的输出。</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/fedf29fe1826dee7e81f3b7b8032d70e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SSvqKlsLDAdcHUWBxq9A2Q.png"/></div></div></figure><p id="40a1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当处理一个<em class="nx"> m </em>行的数据集时，对每一行分别重复这些操作是非常昂贵的。<br/>我们在每一层都有[ <em class="nx">我</em> ]:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/5a453290466318407658fbeb4f3c5c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yvsqXoGG5Le642k_yK7lJQ.png"/></div></div></figure><p id="5d28" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">参数<em class="nx"> b_i </em>使用广播通过列重复自身。下图对此进行了总结:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/e634886fd1c4a39e8d35ed40892baaac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zvjlVm8WVn_tlyAEVvJfjw.png"/></div></div></figure><h2 id="3a8e" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">反向传播</h2><p id="50d6" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">反向传播是学习的第二步，它包括将预测(正向)阶段犯下的错误注入网络，并更新其参数以在下一次迭代中表现更好。<br/>因此，函数<em class="nx"> J </em>的优化，通常通过下降法。</p><p id="6084" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">计算图形</strong></p><p id="1cc4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大多数下降方法需要计算表示为∇ <em class="nx"> J </em> ( <em class="nx"> θ </em>)的损失函数的梯度。</p><p id="1e50" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在神经网络中，使用将函数<em class="nx"> J </em>分解成几个中间变量的计算图进行运算。</p><p id="3271" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们来考虑下面这个函数:<em class="nx"> f </em> ( <em class="nx"> x </em>，<em class="nx"> y </em>，<em class="nx">z</em>)=(<em class="nx">x</em>+<em class="nx">y</em>)。<em class="nx"> z </em></p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/593847dd58fe8d944db03265e7679d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zpgxVNNz6Mgs1gQ7AWUScw.png"/></div></div></figure><p id="c6dd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们使用两个通道进行计算:</p><ul class=""><li id="feb2" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la jk">正向传播</strong>:计算从输入到输出的<em class="nx"> f </em>的值:<em class="nx">f</em>(2，5，4)=-12</li><li id="1ca4" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><strong class="la jk">反向传播</strong>:递归应用链规则计算从输出到输入的梯度:</li></ul><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/8e3ebd089a199b59b6faf2ccd6c3d72b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*deTEKLDllVxua1imqtaeXw.png"/></div></div></figure><p id="bc5c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">导数可以在下面的<em class="nx">计算图</em>中恢复:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/067a24e71866e452f9cad5f591f821a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XmTMLneqoDBDZtwgzpTV5g.png"/></div></div></figure><h2 id="7ec7" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">方程式</h2><p id="c44f" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">数学上，我们计算成本函数<em class="nx"> J </em>，w.r.t架构参数<em class="nx"> W </em>和<em class="nx"> b </em>的梯度。</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/5bd7d4eb460c03e1376ed0750314049d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRrgfiiMdxHn0LY4gdd3XQ.png"/></div></div></figure><p id="332c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中(⋆)是逐元素乘法。<br/>我们递归地将这些等式应用于<em class="nx"> i </em> = <em class="nx"> L </em>，<em class="nx">L</em>1，…，1</p><h2 id="2380" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">梯度检查</h2><p id="cd12" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">当执行反向传播时，增加了额外的检查以确保代数计算是正确的。</p><p id="eebb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">算法:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/4af9dc522aaec8120b10fa8a3cce136e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5IOZBHSSeD7cf7HdR9RCVg.png"/></div></div></figure><p id="2ca5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它应该接近<em class="nx"> ϵ </em>的值，当量值比<em class="nx"> ϵ.高一千<em class="nx">倍</em>时，怀疑有错误</em></p><p id="b76a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以在下面的方框中总结前向和后向传播:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/f36656cbd6a7809bcf684d7416d492ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hv7BlLtJsv8BxTImcSGAnw.png"/></div></div></figure><h2 id="784a" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">参数与超参数</h2><p id="6d40" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated"><em class="nx">-参数</em>，表示为<em class="nx"> θ </em>，是我们通过迭代学习的元素，我们对其应用反向传播和更新:<em class="nx"> W </em>和<em class="nx"> b. </em></p><p id="e0f5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nx">-超参数</em>是我们在算法中定义的所有其他变量，可以<em class="nx">调整</em>以改进神经网络:</p><ul class=""><li id="ef1b" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">学习率<em class="nx"> α </em></li><li id="4d18" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">迭代次数</li><li id="5a59" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">激活功能的选择</li><li id="d7d1" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">层数<em class="nx"> L </em></li><li id="4048" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">每层中的单元数量</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="454c" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">5.激活功能</h1><p id="783f" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">激活函数是一种选择在神经网络中传播的数据的传递函数。潜在的解释是，只有当网络中的神经元被充分激发时，才允许它传播学习数据(如果它处于学习阶段)。</p><p id="4957" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是最常见的函数列表:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/f1b0760c752c7cd87ed627e0b341ede9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SOpOpjZkEmSI1dk_1F8VEA.png"/></div></div></figure><p id="035c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">备注</strong>:如果激活函数都是线性的，那么神经网络就相当于一个简单的线性回归</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="ef3a" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">6.最优化算法</h1><h2 id="1f87" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated"><strong class="ak">风险</strong></h2><p id="e3b0" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">让我们考虑一个用<em class="nx"> f </em>表示的神经网络。优化的真正目标被定义为所有语料库的预期损失:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/89cf8f06a32228adde5b80248f0ac63d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qm-ZF2X0EuO7TQlaCmNKw.png"/></div></div></figure><p id="ccff" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<em class="nx"> X </em>是来自一个连续的可观测空间的一个元素，对应于一个目标<em class="nx"> Y </em>和<em class="nx"> p </em> ( <em class="nx"> X </em>，<em class="nx"> Y </em>)是观测到该对的边际概率(<em class="nx"> X </em>，<em class="nx"> Y </em>)。</p><p id="0cd7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">经验风险</strong></p><p id="471a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于我们不能拥有所有的语料库，因此我们忽略了分布，我们将风险的估计限制在能很好地代表整个语料库的某个数据集上，并考虑所有情况都是等概率的。<br/>在这种情况下:我们将m设为代表性语料库的大小，我们得到:∫=∑和<em class="nx"> p </em> ( <em class="nx"> X </em>，<em class="nx"> Y </em> )=1/m。因此，我们迭代优化损失函数，定义如下:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pi"><img src="../Images/1580445ab86c333f942a36a676b5d418.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mhAVcUX9M-rqlOoIgKS9Gw.png"/></div></div></figure><p id="8a68" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另外我们可以断言:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/1b82f71cc8a8838d7040e7175e513fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgOVI-szgH1KiWLYIzze_Q.png"/></div></div></figure><p id="f3e3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">存在许多技术和算法，主要基于梯度下降来执行优化。在下面的章节中，我们将介绍最著名的几个。值得注意的是，这些算法可能会陷入局部极小值，没有什么能保证达到全局最优。</p><h2 id="44fd" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">标准化输入</h2><p id="e383" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">在优化损失函数之前，我们需要对输入进行归一化，以加快学习速度。在这种情况下，<em class="nx"> J </em> ( <em class="nx"> θ </em>)变得更紧密和更对称，这有助于梯度下降更快地找到最小值，从而减少迭代次数。<br/> <strong class="la jk">标准数据</strong>是常用的方法，包括减去变量的平均值并除以它们的标准差。考虑到这一点，下图说明了对右侧标准数据等高线上的输入进行归一化的效果:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/cbd5decb3a748458495f1ef96f87d97c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O3EK4XzGdNjUEBnGKtSbLQ.png"/></div></div></figure><p id="af65" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设X是我们数据库中的一个变量，我们设置:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/0cefb781fd5187018b51f1b3e90a31f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ozWnxEsK8YIR1Yqg3DNKuw.png"/></div></div></figure><h2 id="dacf" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">梯度下降</h2><p id="ea46" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">通常，我们倾向于构造一个<code class="fe oo op oq or b">convex</code>和<code class="fe oo op oq or b">differentiable</code>函数<em class="nx"> J </em>，其中任何局部极小值都是全局极小值。从数学上讲，寻找凸函数的全局最小值等价于求解方程∇ <em class="nx"> J </em> ( <em class="nx"> θ </em> )=0，我们把它的解记为<em class="nx"> θ </em> ⋆。<br/>大多数使用的算法是这样的:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pj"><img src="../Images/d2781d40b72469ed5a321d9c251afa59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JqFYIioBTjLxThuO2k-cvA.png"/></div></div></figure><p id="4713" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">小批量梯度下降</strong></p><p id="563c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这项技术包括将训练集分成几批:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/ad466816ff2e95e84221d19bcb65476d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yGUeKwz0l2MMt_2s_nn0bA.png"/></div></div></figure><p id="ebd0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">小批量的选择:</p><ul class=""><li id="af5d" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">少量行2000行</li><li id="6589" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">典型大小:2的幂，有利于记忆</li><li id="4d90" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">小批量应该适合CPU/GPU内存</li></ul><p id="fa73" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">备注:在批处理中只有一条数据线的情况下，该算法称为随机梯度下降</p><h2 id="205e" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">动量梯度下降</h2><p id="81ae" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">包括动量概念的梯度下降的变体，算法如下:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/0eb56901a61e4728af130a525f1425bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OORIn6-iHZlE6eFg-2_H_g.png"/></div></div></figure><p id="0836" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(<em class="nx"> α </em>，<em class="nx"> β </em>)为超参数。<br/>由于<em class="nx"> dθ </em>是在小批量上计算的，因此产生的梯度∇ <em class="nx"> J </em>噪声很大，动量中包含的指数加权平均值可以更好地估计导数。</p><h2 id="77a6" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">RMSprop</h2><p id="9b02" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">均方根prop非常类似于带动量的梯度下降，唯一的区别是它包括二阶动量而不是一阶动量，加上参数更新的微小变化:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/3baa03a9ee1c6eb40110e79633595cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ucb4wTK5hQYH9i7G7ohutw.png"/></div></div></figure><p id="1b4a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(<em class="nx"> α </em>，<em class="nx"> β </em>)是超参数，而<em class="nx"> ϵ </em>确保数值稳定性(≈108)</p><h2 id="47e8" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h2><p id="276e" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">Adam是一种自适应学习率优化算法，专门用于训练深度神经网络。Adam可以看作是RMSprop和带动量的梯度下降的组合。<br/>它使用平方梯度将学习率设置为RMSprop，并通过使用梯度的移动平均值而不是梯度本身来利用动量，因为梯度随动量下降。<br/>主要思想是通过加速向正确方向下降来避免优化过程中的振荡。<br/>Adam优化器的算法如下:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/69288368bb333f8270ca631ea85fab94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x6e43_ZXyuybVUtOpLzo_w.png"/></div></div></figure><h2 id="a0fa" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">学习率衰减</h2><p id="7438" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">学习率衰减的主要目的是随着时间/迭代缓慢降低学习率。它在这样一个事实中找到了合理性，即我们在学习开始时可以迈出大步，但当接近全局最小值时，我们会放慢速度，从而降低学习速度。<br/>学习率存在许多衰减规律，下面是一些最常见的:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/927e9a5d06be0b87347252bfe18e2f1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B78FgIVz-qyKMarn1fLPvQ.png"/></div></div></figure><h2 id="6e36" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">正规化</h2><p id="26de" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated"><strong class="la jk">方差/偏差</strong></p><p id="9dfa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练神经网络时，它可能会遇到以下问题:</p><ul class=""><li id="672d" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la jk">偏高</strong>:或者欠拟合，网络在数据中找不到路径，这种情况下<em class="nx"> J_train </em>非常高与<em class="nx"> J_dev </em>相同。从数学上讲，进行交叉验证时；在所有考虑的褶皱上，<em class="nx"> J </em>的平均值较高。</li><li id="2b68" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><strong class="la jk">高方差</strong>或过拟合，模型完全符合训练数据，但无法对未见过的数据进行概括，在这种情况下，<em class="nx"> J_train </em>非常低，而<em class="nx"> J_dev </em>相对较高。从数学上讲，进行交叉验证时；在所有考虑的褶皱上<em class="nx"> J </em>的方差很高。</li></ul><p id="8d6d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们考虑一下飞镖游戏，击中红色目标是最好的情况。拥有<strong class="la jk">低偏差</strong>(第一行)意味着<strong class="la jk">平均而言</strong>我们接近目标。在<strong class="la jk">低方差的情况下，</strong>击中全部集中在目标周围(击中分布的方差低)。当方差较高时，在低偏差的假设下，命中分散开，但仍在红色圆圈周围。<br/>反之亦然，我们可以用低/高方差来定义高偏差。</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/7f9a4921ccd9b48cf09dcd81765c13a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0l6qfO1ZflhrKb2WFpAHw.png"/></div></div></figure><p id="068c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从数学上讲，设<em class="nx"> f </em>为真回归函数:y =<em class="nx">f</em>(<em class="nx">x</em>)+<em class="nx">ϵ</em>其中:<em class="nx"> ϵ~N(0,σ ) </em> <br/>我们用MSE拟合一个假设<em class="nx">h</em>(<em class="nx">x</em>)=<em class="nx">wx</em>+<em class="nx">b</em>并认为x_0是一个新的数据点，【t</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/d8d66ffaee528f1723d26d70772dcf80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFWCXD7rDIM9AnORB6su9g.png"/></div></div></figure><p id="d991" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过使用<em class="nx"> AIC </em>标准或交叉验证，必须在方差和偏差之间找到一个平衡点，以找到模型的最佳复杂性。<br/>以下是解决偏差/差异问题的简单方案:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/1f6eea5a7d212a69eefb180d4e48ce7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BhmP2g95WzsbYIBqJLiruA.png"/></div></div></figure><p id="a832" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> L1 — L2正规化</strong></p><p id="7d37" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正则化是一种防止过度拟合的优化技术。<br/>它包括在目标函数中添加一项，以最小化如下:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pm"><img src="../Images/80ae21229bc725f6baa6ae526783e7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HNBLGdvyZuoiIAj9dPhQPg.png"/></div></div></figure><p id="5214" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nx"> λ </em>是正则化的超参数。</p><p id="7828" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">反向传播和正则化</strong> <br/>反向传播期间参数的更新取决于梯度∇ <em class="nx"> J </em>，其中增加了新的正则化项。在L2正规化，它变成如下:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/d2da8e340ea97c8dbed4a6d40046ad00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B8AqwPoOsFqw6BT4L0nngQ.png"/></div></div></figure><p id="2755" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">考虑到<em class="nx"> λ </em> &gt; &gt; 1，最小化成本函数导致参数的弱值，因为项(<em class="nx">λ/2m)</em>∨<em class="nx">θ</em>∨)简化了网络并使其更加一致，因此较少暴露于过拟合。</p><p id="b9b5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">退学正规化</strong></p><p id="afd6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">粗略地说，主要思想是采样一个均匀的随机变量，<code class="fe oo op oq or b">for each layer for each node</code>，并且有<em class="nx"> p </em>的机会保留该节点，并且有1<em class="nx">p</em>的机会移除该节点，这减小了网络。辍学的主要直觉是基于这样的想法，网络不应该依赖于一个特定的特征，而是应该分散权重！<br/>从数学上讲，当dropout关闭时，考虑到第<em class="nx">I</em>层的第<em class="nx">j</em>节点，我们有以下等式:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/2288c46b71e21708e9bbffa3f71ad5bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RVqLB-GqXPqbHb15_6TH6w.png"/></div></div></figure><p id="0778" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当dropout打开时，方程式如下:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pn"><img src="../Images/ff4a6e11ed0634b5cebe95575cb24981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFrwSxwmsE5tzNVCIrRRhg.png"/></div></div></figure><h2 id="75f6" class="ni mr jj bd ms nj nk dn mw nl nm dp na lh nn no nc ll np nq ne lp nr ns ng nt bi translated">提前停止</h2><p id="e56e" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">该技术非常简单，包括当<em class="nx"> J_train </em>和<em class="nx"> J_dev </em>开始分离时，停止该区域周围的迭代:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/1c4ed44e5df7e14a8a2f2b1bb834ae2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gxGWCWAifT5q6mEBLkVWyA.png"/></div></div></figure><p id="3596" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">梯度问题</strong></p><p id="795e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">梯度的计算有两个主要问题:梯度消失和梯度爆炸。<br/>为了说明这两种情况，让我们考虑一个神经网络，其中所有激活函数<em class="nx"> ψ </em> [ <em class="nx"> i </em> ]都是线性的，并且:</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/b533d57854db965baea5684d6f1b4af2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NZyzx9043uhfvXmyc3XdQw.png"/></div></div></figure><p id="8a0d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们注意到，1,5^((T7)将作为深度l的函数按指数规律爆炸。如果我们使用0.5而不是1.5，那么0,5^(l-1(T7)也将按指数规律消失。<br/>渐变也会出现同样的问题。</p><h1 id="74cc" class="mq mr jj bd ms mt pp mv mw mx pq mz na kp pr kq nc ks ps kt ne kv pt kw ng nh bi translated">结论</h1><p id="d648" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">作为一名数据科学家，了解神经网络背景下的数学转变非常重要。这允许更好的理解和更快的调试。</p><p id="6e3e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不要犹豫，检查我以前的文章处理:</p><ul class=""><li id="6460" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><a class="ae jg" href="https://medium.com/p/convolutional-neural-networks-mathematics-1beb3e6447c0" rel="noopener">卷积神经网络的数学</a></li><li id="5318" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae jg" href="https://medium.com/p/object-detection-face-recognition-algorithms-146fec385205" rel="noopener">物体检测&amp;人脸识别算法</a></li><li id="bbaa" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/recurrent-neural-networks-b7719b362c65">递归神经网络</a></li></ul><p id="2b4a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">机器学习快乐！</p><h1 id="3cf9" class="mq mr jj bd ms mt pp mv mw mx pq mz na kp pr kq nc ks ps kt ne kv pt kw ng nh bi translated">参考</h1><ul class=""><li id="e055" class="lu lv jj la b lb oh le oi lh ov ll ow lp ox lt lz ma mb mc bi translated"><a class="ae jg" href="https://fr.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习专业化</a>，Coursera，吴恩达</li><li id="902e" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae jg" href="http://www.iecl.univ-lorraine.fr/~Antoine.Henrot/" rel="noopener ugc nofollow" target="_blank">优化课程</a>，Mines Nancy，Antoine Henrot</li><li id="3938" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae jg" href="http://deeploria.gforge.inria.fr/cours/cours1.html#/machine-learning-introduction" rel="noopener ugc nofollow" target="_blank">机器学习</a>，洛里亚，克里斯托夫·塞里萨拉</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="4ad7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nx">原载于2020年1月31日</em><a class="ae jg" href="https://www.ismailmebsout.com/deep-learning/" rel="noopener ugc nofollow" target="_blank"><em class="nx">https://www.ismailmebsout.com</em></a><em class="nx">。</em></p></div></div>    
</body>
</html>