<html>
<head>
<title>How to embed a Spark ML Model as a Kafka Real-Time Streaming Application for Production Deployment</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将 Spark ML 模型作为 Kafka 实时流应用程序嵌入到生产部署中</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-embed-a-spark-ml-model-as-a-kafka-real-time-streaming-application-for-production-deployment-933aecb79f3f?source=collection_archive---------11-----------------------#2020-02-26">https://towardsdatascience.com/how-to-embed-a-spark-ml-model-as-a-kafka-real-time-streaming-application-for-production-deployment-933aecb79f3f?source=collection_archive---------11-----------------------#2020-02-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/a86bc98828e052a86cf0565b25904499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3GqAZFVuSd5x28sWON4SHQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">[OC]</p></figure><h1 id="cf1d" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">用于生产评分的 2 种模型部署</h1><p id="8ed1" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">今天的数据科学家可以依靠丰富的高质量开源工具、框架和库来训练模型。训练模型从未如此简单，只需几行 Python 代码即可完成。</p><p id="51db" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">然而，模型的生产部署仍然是数据科学中最棘手的问题，也是许多数据科学项目难以适应业务并产生影响的原因。Gartner 2019 等行业调查发现，多达 80%的数据科学项目失败。数据科学的成功受到一些通常称为“最后一英里问题”的困扰。</p><p id="8a77" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">你可以在我之前关于模型开发最佳实践如何缩小数据科学和生产环境之间差距的博客文章中读到:</p><div class="mg mh gp gr mi mj"><a rel="noopener follow" target="_blank" href="/complete-data-science-project-template-with-mlflow-for-non-dummies-d082165559eb"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd iu gy z fp mo fr fs mp fu fw is bi translated">用 Mlflow 为非傻瓜完成数据科学项目模板。</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">数据科学项目最佳实践，适用于在本地或云中工作的每个人，从刚起步的忍者到大…</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx jz mj"/></div></div></a></div><p id="5e5f" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">模型开发的最佳工具不一定是大规模生产中模型评分或模型推断的正确选择。为生产中的模型评分和模型推断选择最佳框架，并理解这对模型开发本身的约束和要求是很重要的。虽然最佳开发实践使模型更接近生产环境，但数据科学家仍然必须将模型与最合适的模型服务和评分框架相结合，以实现成功的生产部署。</p><p id="9681" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">用于模型服务和评分的框架和平台的选择越来越多，与所有最新的创新和发布保持同步会让人感到势不可挡。幸运的是，所有的解决方案都分为两大类。了解哪种类型最适合您的业务问题将大大简化选择最佳可用选项的过程。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi my"><img src="../Images/9a5d64b249b7c457d10a874d30b0c863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4U7mG_Ta5eqV0wbeTvT-yQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">[OC]</p></figure><p id="02ac" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">模型部署的两个高级类型是(1)嵌入式模型或(2)模型推理服务器。</p><p id="e7da" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">在嵌入式模型的情况下，模型作为数据处理基础结构的一部分被托管和管理。当模型性能至关重要时，这种部署类型是理想的，因为嵌入式模型速度更快，延迟最低。如果您希望在设备或 edge 上支持离线推理，而不依赖于将数据发送到通常位于云中的独立模型推理服务器，那么也需要嵌入您的模型。嵌入式模型还继承了数据处理框架的优势和能力，例如，将模型嵌入到 Kafka 中可以保证只有一次语义而没有副作用。嵌入模型的目标基础设施的正确选择是</p><ul class=""><li id="40e3" class="nd ne it lf b lg mb lk mc lo nf ls ng lw nh ma ni nj nk nl bi translated">Kafka: <br/>使用无状态模型和类似于精确一次语义的保证对事件数据流进行实时评分。</li><li id="5a2a" class="nd ne it lf b lg nm lk nn lo no ls np lw nq ma ni nj nk nl bi translated">Spark: <br/>针对大量现有数据对模型进行分布式批量评分<br/>或对具有宽松实时要求的模型进行小批量评分，但先进的 ELT 步骤和功能工程受益于 SparkSQL</li><li id="4688" class="nd ne it lf b lg nm lk nn lo no ls np lw nq ma ni nj nk nl bi translated">数据库内:<br/>有状态模型对不断变化的数据记录进行评分，比如需要高性能数据库的用户配置文件。选项有 RedisAI、ksqlDB 和 SQL Server。</li></ul><p id="33fa" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">在专用的模型推理服务器上部署模型也有它的优势:首先，它是最灵活的部署，对模型开发的约束最少。其次，通过 API 和 gRPC 调用与现有的遗留基础设施集成推理服务器和建模微服务也更简单，允许额外的中间层或集成层。但最重要的是，大多数专用的模型推理服务器都带有现成的功能，用于管理和监控生产中的模型。生产中的典型数据科学模型是动态的，具有复杂的生命周期。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/41c25615f55777234265753e7fc99e60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YIiSnQ_KvPlbDRTe"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">[OC]</p></figure><p id="a61b" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">一个成功的数据科学团队将开发许多不同的现任和挑战者模型，并要求在不断变化的环境中通过持续的基准测试、监控和生产移交来重新培训这些模型。专用的模型推理服务器可以更轻松地实现复杂数据科学模型生命周期的端到端自动化。例如，专用推理服务器和模型微服务的选项有:</p><ul class=""><li id="a401" class="nd ne it lf b lg mb lk mc lo nf ls ng lw nh ma ni nj nk nl bi translated">张量流服务</li><li id="8aa5" class="nd ne it lf b lg nm lk nn lo no ls np lw nq ma ni nj nk nl bi translated">ONNX 运行时</li><li id="af29" class="nd ne it lf b lg nm lk nn lo no ls np lw nq ma ni nj nk nl bi translated">模型微服务，例如通过 AWS Sagemaker</li><li id="d5a1" class="nd ne it lf b lg nm lk nn lo no ls np lw nq ma ni nj nk nl bi translated"><strong class="lf iu">会合建筑</strong></li></ul><h1 id="d5b7" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">卡夫卡的嵌入式模型</h1><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/3a732f6b05f23da55439d4047d512edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*ml-UWFkZmCweBpQNuNyaDg.png"/></div></figure><p id="50d1" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">建立模型似乎是大多数数据科学项目中最容易的部分。然而，由于企业生产系统的需求，将模型部署到生产中是一个挑战。模型的部署必须解决伸缩和负载平衡问题，以满足不同需求的 SLA，并需要与上游数据系统和下游模型消费者集成。虽然这是一项极其复杂的工程任务</p><ul class=""><li id="b7e3" class="nd ne it lf b lg mb lk mc lo nf ls ng lw nh ma ni nj nk nl bi translated">为了管理分布式模型，</li><li id="7b19" class="nd ne it lf b lg nm lk nn lo no ls np lw nq ma ni nj nk nl bi translated">对数据进行实时评分</li><li id="4459" class="nd ne it lf b lg nm lk nn lo no ls np lw nq ma ni nj nk nl bi translated">保证每个请求只获得一次分数</li><li id="5849" class="nd ne it lf b lg nm lk nn lo no ls np lw nq ma ni nj nk nl bi translated">包括容错和恢复</li></ul><p id="6692" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">这些要求在企业部署中很常见。</p><p id="4da3" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">通过将我们的模型嵌入到 Kafka Streams 应用程序中，我们只需继承 Kafka 的企业特性，就可以满足所有这些需求。</p><p id="c094" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">Kafka 是分布式流媒体应用程序的代理，Kafka 可以</p><ol class=""><li id="fe88" class="nd ne it lf b lg mb lk mc lo nf ls ng lw nh ma nt nj nk nl bi translated">将数据作为消息在应用程序之间传递，作为一个<strong class="lf iu"> KStream </strong>和</li><li id="0626" class="nd ne it lf b lg nm lk nn lo no ls np lw nq ma nt nj nk nl bi translated">将分布式应用程序的状态作为一个<strong class="lf iu">表</strong>进行全局管理。</li></ol><p id="c79b" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">在商业数据科学家的日常工作中，构建和训练模型的要求与部署用于评分的模型非常不同。模型通常使用 Spark 对大量历史数据进行批量训练，然后部署在生产中进行实时评分。</p><p id="1c23" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">以下示例显示了如何将在 Spark 中构建和训练的数据科学模型作为 Kafka 流应用程序嵌入到实时评分中。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nu"><img src="../Images/02f42731c158c9a521f47a8618cf7d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1YjJS8_YdqT5_1EVFhbd2g.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">[OC]</p></figure><h1 id="72f3" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">创建模型</h1><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1b463918bf5d3c88e5190e030d51ad8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/0*YPVbWmF_6ZQvW-zY"/></div></figure><p id="078e" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">为了将 Spark 管道部署为 Kafka 流应用程序，我们使用<a class="ae nw" href="https://mleap-docs.combust.ml/" rel="noopener ugc nofollow" target="_blank"> Mleap 项目</a>来序列化我们的 Spark 管道，而不需要任何 Spark 上下文。</p><p id="f0b9" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">我们将 mleap 包与 pip 或 conda 一起安装在我们的 Jupyter 笔记本中使用，或者将 Python 脚本作为我们的 Spark 驱动程序脚本:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="13f2" class="oc kg it ny b gy od oe l of og">pip install mleap</span></pre><p id="e1e5" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">下面的代码将 Mleap 添加到我们的 Spark 会话中:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="7e53" class="oc kg it ny b gy od oe l of og">from pyspark.sql import SparkSession</span><span id="edb1" class="oc kg it ny b gy oh oe l of og">spark = (<br/>    SparkSession.builder<br/>    .config('spark.jars.packages', 'ml.combust.mleap:mleap-spark-base_2.11:0.14.0,ml.combust.mleap:mleap-spark_2.11:0.14.0')<br/>    .getOrCreate()<br/>)</span></pre><p id="838a" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">然后像平常一样使用 Spark，并使用 PySpark ML 包训练一个模型:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="3762" class="oc kg it ny b gy od oe l of og">from pyspark.ml import Pipeline<br/>from pyspark.ml.classification import RandomForestClassifier<br/>from pyspark.ml.feature import IndexToString, StringIndexer, VectorAssembler</span><span id="f54b" class="oc kg it ny b gy oh oe l of og">iris.show(1)<br/># +---------------+--------------+---------------+--------------+---------------+<br/># |sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|          class|<br/># +---------------+--------------+---------------+--------------+---------------+<br/># |            7.0|           3.2|            4.7|           1.4|Iris-versicolor|<br/># +---------------+--------------+---------------+--------------+---------------+</span><span id="bc8f" class="oc kg it ny b gy oh oe l of og">labelIndexer = StringIndexer(inputCol="class", outputCol="indexedLabel").fit(iris)<br/>assembler = VectorAssembler(<br/>    inputCols=["sepal_length_cm", "sepal_width_cm", "petal_length_cm", "petal_width_cm"],<br/>    outputCol="features"<br/>)<br/>labelConverter = IndexToString(<br/>    inputCol="prediction", outputCol="predictedLabel",<br/>    labels=labelIndexer.labels<br/>)<br/>rf = RandomForestClassifier(labelCol="indexedLabel", featuresCol="features", numTrees=10)<br/>pipeline = Pipeline(stages=[labelIndexer, assembler, rf, labelConverter])</span><span id="740c" class="oc kg it ny b gy oh oe l of og">model = pipeline.fit(iris)</span><span id="db6a" class="oc kg it ny b gy oh oe l of og">model.transform(iris.select('sepal_length_cm', 'sepal_width_cm', 'petal_length_cm', 'petal_width_cm')).show(1)</span><span id="d1fd" class="oc kg it ny b gy oh oe l of og"># +---------------+--------------+---------------+--------------+-----------------+--------------+-------------+----------+--------------+<br/># |sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|         features| rawPrediction|  probability|prediction|predictedLabel|<br/># +---------------+--------------+---------------+--------------+-----------------+--------------+-------------+----------+--------------+<br/># |            5.1|           3.5|            1.4|           0.2|[5.1,3.5,1.4,0.2]|[10.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|   Iris-setosa|</span></pre><p id="7b7b" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">在真实的项目中，我们至少会使用测试数据集来验证我们的模型，并使用交叉验证来调整参数。在模型构建和训练的最后，我们有一个拟合的 Spark 管道模型，在上面的代码中称为 model，我们可以使用它来转换数据，以在 Spark 中批量获得预测。</p><p id="cc89" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">为了将我们的模型从 Spark 中释放出来，并将其移动到 Kafka 流应用程序中进行实时评分，我们使用以下简单代码将其序列化为 Mleap 包:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="8e1f" class="oc kg it ny b gy od oe l of og">import mleap.pyspark<br/>from mleap.pyspark.spark_support import SimpleSparkSerializer</span><span id="b0aa" class="oc kg it ny b gy oh oe l of og">model.serializeToBundle(<br/>    "file:/home/jovyan/notebooks/model", <br/>    model.transform(iris)<br/>)</span></pre><p id="6450" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">它在名为 model:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/91ca34f9c7717f2cd6f608878797de0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/0*-Szwg0_m9auZ1Ark"/></div></figure><h1 id="6e30" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">创建 Kafka 流应用程序</h1><p id="b3ff" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">通过将我们的模型序列化为 Mleap 包，我们可以创建 Kafka 流应用程序来将我们的模型嵌入到 Kafka 中。我们必须为此创建一个简单的 Scala 应用程序，最重要的是将带有 Mleap 包的模型文件夹移动到 Scala 项目的资源文件夹中。</p><p id="23c7" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">您可以在我的 Git 存储库中找到完整的代码和项目设置:</p><div class="mg mh gp gr mi mj"><a href="https://gitlab.com/jan-teichmann/kafka-mleap-iris-classifier" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd iu gy z fp mo fr fs mp fu fw is bi translated">Jan Teichmann / kafka-mleap-iris 分类器</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">借助 Mleap 嵌入 Spark ML 管道的 Kafka Streams 应用程序</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">gitlab.com</p></div></div><div class="ms l"><div class="oj l mu mv mw ms mx jz mj"/></div></div></a></div><p id="b322" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">下面的 Scala 代码将我们的 Mleap 捆绑包分解成一个 Mleap 管道模型，并创建一个名为 score()的简单函数，接受 iris 输入数据进行评分。你可以看到 Mleap 非常接近 Spark API。这段代码与 Kafka 无关，只是如何用 Scala 从一个包中加载任何 Mleap 模型。你可以在这里找到关于 Mleap 运行时<a class="ae nw" href="https://mleap-docs.combust.ml/mleap-runtime/" rel="noopener ugc nofollow" target="_blank">的扩展文档。</a></p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="61d9" class="oc kg it ny b gy od oe l of og">object IrisModel {</span><span id="9ff7" class="oc kg it ny b gy oh oe l of og">val schema: StructType = StructType(<br/>        StructField("sepal_length_cm", ScalarType.Double),<br/>        StructField("sepal_width_cm", ScalarType.Double),<br/>        StructField("petal_length_cm", ScalarType.Double),<br/>        StructField("petal_width_cm", ScalarType.Double),<br/>        StructField("class", ScalarType.String)<br/>    ).get</span><span id="f37c" class="oc kg it ny b gy oh oe l of og">val modelpath = getClass.getResource("/model").getPath</span><span id="967c" class="oc kg it ny b gy oh oe l of og">val model = (<br/>        for(bundle &lt;- managed(BundleFile(s"jar:$modelpath"))) yield {<br/>            bundle.loadMleapBundle().get<br/>        }<br/>    ).tried.get.root</span><span id="0221" class="oc kg it ny b gy oh oe l of og">def score(<br/>        sepal_length_cm: Double, sepal_width_cm: Double,<br/>        petal_length_cm: Double, petal_width_cm: Double<br/>    ): String = {</span><span id="004b" class="oc kg it ny b gy oh oe l of og">model.transform(<br/>            DefaultLeapFrame(<br/>                schema, <br/>                Seq(Row(sepal_length_cm, sepal_width_cm, petal_length_cm, petal_width_cm, "Iris-setosa"))<br/>            )<br/>        ).get.select("predictedLabel").get.dataset.map(_.getString(0)).head<br/>    <br/>    }</span><span id="7d90" class="oc kg it ny b gy oh oe l of og">}</span></pre><p id="e624" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">我们可以使用如下简单的单元测试来测试我们的模型是否如预期的那样工作:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="ac17" class="oc kg it ny b gy od oe l of og">class IrisSpec extends FlatSpec with Matchers {<br/>  "An Iris Classifier" should "return the class Iris-setosa" in {<br/>    val prediction = IrisModel.score(5.1, 3.5, 1.4, 0.2)<br/>    prediction should be ("Iris-setosa")<br/>  }<br/>  it should "return the class Iris-versicolor" in {<br/>    val prediction = IrisModel.score(6.2, 2.2, 4.5, 1.5)<br/>    prediction should be ("Iris-versicolor")<br/>  }<br/>  it should "return the class Iris-virginica" in {<br/>    val prediction = IrisModel.score(6.1, 2.6, 5.6, 1.4)<br/>    prediction should be ("Iris-virginica")<br/>  }<br/>}</span></pre><p id="1fde" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">下一步，我们创建 Kafka 流应用程序并嵌入我们的模型:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="170c" class="oc kg it ny b gy od oe l of og">object IrisStreamClassifier extends App {</span><span id="f9a7" class="oc kg it ny b gy oh oe l of og">import org.apache.kafka.streams.scala.Serdes._<br/>    import org.apache.kafka.streams.scala.ImplicitConversions._</span><span id="2683" class="oc kg it ny b gy oh oe l of og">val config: Properties = {<br/>        val p = new Properties()<br/>        p.put(StreamsConfig.APPLICATION_ID_CONFIG, "iris-classifier")<br/>        val bootstrapServers = if (args.length &gt; 0) args(0) else "kafka:9092"<br/>        p.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)<br/>        p<br/>    }</span><span id="0e17" class="oc kg it ny b gy oh oe l of og">def irisStreamClassifier(<br/>        inputTopic: String, outputTopic: String<br/>    ): Topology = {</span><span id="99bf" class="oc kg it ny b gy oh oe l of og">val builder: StreamsBuilder = new StreamsBuilder()<br/>        val irisInput = builder.stream[String, String](inputTopic)<br/>        val irisScore: KStream[String, String] = irisInput.map(<br/>            (_, value) =&gt; {<br/>                val iris_values = value.split(",").map(_.toDouble)<br/>                (null, Seq(value, IrisModel.score(iris_values(0), iris_values(1), iris_values(2), iris_values(3))).mkString(","))<br/>            }<br/>        )<br/>        irisScore.to(outputTopic)<br/>        builder.build()<br/>    }</span><span id="3c0b" class="oc kg it ny b gy oh oe l of og">val streams: KafkaStreams = new KafkaStreams(<br/>        irisStreamClassifier(<br/>            "iris-classifier-input",<br/>            "iris-classifier-output"<br/>        ), config<br/>    )<br/>    streams.start()</span><span id="b00e" class="oc kg it ny b gy oh oe l of og">sys.ShutdownHookThread {<br/>        streams.close(Duration.ofSeconds(10))<br/>    }</span><span id="adca" class="oc kg it ny b gy oh oe l of og">}</span></pre><p id="acf0" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">在 irisStreamClassifier 函数中，我们创建了流拓扑，它接收作为 csv 字符串的 iris 数据流，并通过将分类器应用于每个消息来映射该流。我们将流处理拓扑封装到一个新的 Kafka 流中，以将该拓扑转变为我们使用 streams.start()运行的应用程序</p><p id="fc91" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">我们可以使用 Kafka 的 TopologyTestDriver 对我们的流应用程序进行单元测试:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="9db0" class="oc kg it ny b gy od oe l of og">class IrisClassifierSpec extends FlatSpec with Matchers with BeforeAndAfterAll {<br/>    val config: Properties = {<br/>        val p = new Properties()<br/>        p.put(StreamsConfig.APPLICATION_ID_CONFIG, "integration-test")<br/>        p.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "dummy config")<br/>        p<br/>    }</span><span id="2da1" class="oc kg it ny b gy oh oe l of og">val driver = new TopologyTestDriver(<br/>        IrisStreamClassifier.irisStreamClassifier("input-topic", "output-topic"), config)</span><span id="c56a" class="oc kg it ny b gy oh oe l of og">val recordFactory = new ConsumerRecordFactory("input-topic", new StringSerializer(), new StringSerializer())</span><span id="ba38" class="oc kg it ny b gy oh oe l of og">override def afterAll() {<br/>        driver.close()<br/>    }<br/>    <br/>    "Iris Stream Classifier" should "return the class Iris-setosa" in {<br/>        driver.pipeInput(recordFactory.create("5.1,3.5,1.4,0.2"))<br/>        val record: ProducerRecord[String, String] = driver.readOutput("output-topic", new StringDeserializer(), new StringDeserializer())<br/>        record.value() should be("5.1,3.5,1.4,0.2,Iris-setosa")<br/>    }<br/>    it should "return the class Iris-versicolor" in {<br/>        driver.pipeInput(recordFactory.create("6.2,2.2,4.5,1.5"))<br/>        val record: ProducerRecord[String, String] = driver.readOutput("output-topic", new StringDeserializer(), new StringDeserializer())<br/>        record.value() should be("6.2,2.2,4.5,1.5,Iris-versicolor")   <br/>    }<br/>    it should "return the class Iris-virginica" in {<br/>        driver.pipeInput(recordFactory.create("6.1,2.6,5.6,1.4"))<br/>        val record: ProducerRecord[String, String] = driver.readOutput("output-topic", new StringDeserializer(), new StringDeserializer())<br/>        record.value() should be("6.1,2.6,5.6,1.4,Iris-virginica")   <br/>    }<br/>}</span></pre><p id="959b" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">为了构建我们的应用程序，我们将使用 SBT 并如下定义我们的 build.sbt 项目:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="9e8f" class="oc kg it ny b gy od oe l of og">name := "iris-classifier-streams"<br/>version := "1.0.0"<br/>scalaVersion := "2.12.10"<br/>exportJars := true</span><span id="c6d5" class="oc kg it ny b gy oh oe l of og">lazy val iris = (project in file("."))<br/>    .settings(<br/>        libraryDependencies ++= Seq(<br/>            "org.scalatest" %% "scalatest" % "3.1.0" % Test,<br/>            "org.apache.kafka" % "kafka-streams-test-utils" % "2.4.0" % Test,<br/>            "org.apache.kafka" %% "kafka-streams-scala" % "2.4.0",<br/>            "ml.combust.bundle" %% "bundle-ml" % "0.14.0",<br/>            "ml.combust.mleap" %% "mleap-runtime" % "0.14.0",<br/>            "com.jsuereth" %% "scala-arm" % "2.0",</span><span id="3fc3" class="oc kg it ny b gy oh oe l of og">),<br/>        assemblyMergeStrategy in assembly := {<br/>            case PathList("org", "apache", "spark", "unused", "UnusedStubClass.class") =&gt; MergeStrategy.first<br/>            case "module-info.class" =&gt; MergeStrategy.discard<br/>            case x =&gt;<br/>                val oldStrategy = (assemblyMergeStrategy in assembly).value<br/>                oldStrategy(x)<br/>        }<br/>    )</span></pre><p id="7c64" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">这包括汇编插件的合并策略，以处理一些依赖关系抛出的重复数据删除错误。我们使用一个名为 scala-sbt 的 Docker 映像，它提供了 scala 和 sbt 来编译和打包我们的应用程序，作为一个汇编超级 Jar:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="f967" class="oc kg it ny b gy od oe l of og">ocker build \<br/>    --build-arg BASE_IMAGE_TAG="8u242-jdk-stretch" \<br/>    --build-arg SBT_VERSION="1.3.5" \<br/>    --build-arg SCALA_VERSION="2.12.10" \<br/>    -t scala-sbt github.com/hseeberger/scala-sbt.git#:debian</span><span id="9855" class="oc kg it ny b gy oh oe l of og">docker run -it --rm -w /home/sbtuser \<br/>   --mount type=bind,source="$(PWD)/src",target=/home/sbtuser/src \<br/>   --mount type=bind,source="$(PWD)/build.sbt",target=/home/sbtuser/build.sbt \<br/>   --mount type=bind,source="$(PWD)/project",target=/home/sbtuser/project \<br/>   --mount type=bind,source="$(PWD)/target",target=/home/sbtuser/target \<br/>   scala-sbt sbt assembly</span></pre><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/8213f57aeea0fb8c1f5146dd52ca60b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EP2r-JwgHZsZfq6Y"/></div></div></figure><h2 id="7df1" class="oc kg it bd kh ol om dn kl on oo dp kp lo op oq kt ls or os kx lw ot ou lb ov bi translated">在本地运行 Kafka 流应用程序</h2><p id="3477" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">要将我们的新 Iris 分类器作为 Kafka 流应用程序运行，我们可以使用以下 docker-compose.yml 在我们的笔记本电脑上运行 Kafka broker:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="bfa5" class="oc kg it ny b gy od oe l of og">version: '2'</span><span id="246a" class="oc kg it ny b gy oh oe l of og">services:<br/>  zookeeper:<br/>    image: 'bitnami/zookeeper:3'<br/>    ports:<br/>      - '2181:2181'<br/>    environment:<br/>      - ALLOW_ANONYMOUS_LOGIN=yes<br/>  <br/>  kafka:<br/>    image: 'bitnami/kafka:2'<br/>    ports:<br/>      - '9092:9092'<br/>      - '29092:29092'<br/>    environment:<br/>      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181<br/>      ALLOW_PLAINTEXT_LISTENER: "yes"<br/>      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT<br/>      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,PLAINTEXT_HOST://:29092<br/>      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092<br/>      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"<br/>    depends_on:<br/>      - zookeeper</span></pre><p id="45dd" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">docker-compose up 将开始我们的单一卡夫卡经纪人</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="5969" class="oc kg it ny b gy od oe l of og">docker-compose up kafka</span></pre><p id="5662" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">然后为我们的应用程序创建 Kafka 主题:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="c595" class="oc kg it ny b gy od oe l of og">docker-compose exec kafka kafka-topics.sh --create --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 --topic iris-classifier-input<br/>docker-compose exec kafka kafka-topics.sh --create --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 --topic iris-classifier-output</span></pre><p id="3570" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">然后，我们使用以下命令启动我们的流应用程序:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="21b3" class="oc kg it ny b gy od oe l of og">java -classpath target/scala-2.12/iris-classifier-streams-assembly-1.0.0.jar  kafka.iris.IrisStreamClassifier localhost:29092</span></pre><p id="43bf" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">我们启动一个控制台监听器，等待关于应用程序输出主题的消息:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="0697" class="oc kg it ny b gy od oe l of og">docker-compose exec kafka kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic iris-classifier-output --from-beginning --max-messages 1</span></pre><p id="d191" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">最后，我们用下面的命令向模型的输入主题发送一条消息:</p><pre class="mz na nb nc gt nx ny nz oa aw ob bi"><span id="1751" class="oc kg it ny b gy od oe l of og">docker-compose exec kafka bash -c "echo 5.1,3.5,1.4,0.2 | kafka-console-producer.sh --broker-list kafka:9092 --topic iris-classifier-input &amp;&amp; echo"</span></pre><p id="fa8b" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">由于我们的努力，我们获得了模型输出主题上的消息，该消息由控制台消费者实时流式传输并打印到我们的终端:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ow"><img src="../Images/61b4357f5f77aadbbac4ae5b82cf506f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yX_30eDYwGU4b5_3"/></div></div></figure><p id="ffcc" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">在我的 GitLab 上找到这个例子的所有代码</p><div class="mg mh gp gr mi mj"><a href="https://gitlab.com/jan-teichmann/kafka-mleap-iris-classifier" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd iu gy z fp mo fr fs mp fu fw is bi translated">Jan Teichmann / kafka-mleap-iris 分类器</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">借助 Mleap 嵌入 Spark ML 管道的 Kafka Streams 应用程序</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">gitlab.com</p></div></div><div class="ms l"><div class="oj l mu mv mw ms mx jz mj"/></div></div></a></div><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ox"><img src="../Images/b775f6ee39b5d2b35fa03776e8d59462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9sw3zr2-ILCfYnjCKz364Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">[OC]</p></figure><p id="3d78" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">如果你有兴趣了解更多关于在 Kafka 上构建实时数据产品的信息，可以看看我写的关于 Trainline.com 实时数据产品团队的博客</p><div class="mg mh gp gr mi mj"><a href="https://engineering.thetrainline.com/meet-kronos-trainlines-real-time-data-product-platform-and-team-f4b3dbe663ec" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd iu gy z fp mo fr fs mp fu fw is bi translated">认识克罗诺斯:Trainline 的实时数据产品平台和团队</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">让我们的数据按时运行</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">engineering.thetrainline.com</p></div></div><div class="ms l"><div class="oy l mu mv mw ms mx jz mj"/></div></div></a></div></div><div class="ab cl oz pa hx pb" role="separator"><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe"/></div><div class="im in io ip iq"><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pg"><img src="../Images/b40fa03f9762d1ec3c427365a4c45786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7HvEyV_ETBrk5E7jVL971A.png"/></div></div></figure><p id="20f0" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">Jan 是公司数据转型方面的成功思想领袖和顾问，拥有将数据科学大规模应用于商业生产的记录。他最近被 dataIQ 评为英国 100 位最具影响力的数据和分析从业者之一。</p><p id="4db2" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu">在 LinkedIn 上连接:</strong><a class="ae nw" href="https://www.linkedin.com/in/janteichmann/" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu">【https://www.linkedin.com/in/janteichmann/】</strong>T5】</a></p><p id="d479" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu">阅读其他文章:</strong><a class="ae nw" href="https://medium.com/@jan.teichmann" rel="noopener"><strong class="lf iu">https://medium.com/@jan.teichmann</strong></a></p></div></div>    
</body>
</html>