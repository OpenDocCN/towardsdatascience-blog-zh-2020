<html>
<head>
<title>Music Genre Classification: Transformers vs Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">音乐流派分类:变形金刚 vs 递归神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/music-genre-classification-transformers-vs-recurrent-neural-networks-631751a71c58?source=collection_archive---------32-----------------------#2020-06-14">https://towardsdatascience.com/music-genre-classification-transformers-vs-recurrent-neural-networks-631751a71c58?source=collection_archive---------32-----------------------#2020-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="05cf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在音频分类任务中比较两种架构。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2bf6564ee8ef798e9cd2b307f33054eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TR95R6Tq8Q7kyaDUmB_1cg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马库斯·斯皮斯克在 Unsplash 上拍摄的照片</p></figure><p id="3361" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇文章的目的是通过比较两种流行的序列建模架构来实现一个音乐流派分类模型:递归神经网络和变压器。</p><p id="7234" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">rnn 在各种 1D 序列处理任务中很受欢迎，它们在每个时间步重复使用相同的权重，并通过保持内部状态和使用门控机制(LSTM、格鲁什…)将信息从一个时间步传递到下一个时间步。由于它们使用递归，这些模型可能遭受消失/爆炸梯度，这可能使训练和学习长期模式更加困难。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lr"><img src="../Images/820761d03311a8d27c99f1a7e32ab54c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3gB5yUL9lqQBuEY7qFIH2A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae ls" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">来源:https://en.wikipedia.org/wiki/Recurrent_neural_network</a>BY<a class="ae ls" href="https://commons.wikimedia.org/wiki/User:Ixnay" rel="noopener ugc nofollow" target="_blank">fdeloche</a>下<a class="ae ls" href="https://creativecommons.org/licenses/by-sa/4.0" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a></p></figure><p id="0693" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">变压器是一种相对较新的架构，它可以处理序列，而无需使用任何递归或卷积[<a class="ae ls" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1706.03762.pdf</a>。变压器层大多是逐点前馈操作和自我关注。这些类型的网络在自然语言处理方面取得了一些巨大的成功，特别是在对大量未标记数据进行预训练时[<a class="ae ls" href="https://arxiv.org/pdf/1810.04805" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.04805</a>]。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/8f92cd57742bdc4c7a66e2748b7239b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*SW0xA1VEJZd3XSqc3NvxNw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">变形层—作者提供的图像</p></figure><h1 id="e03d" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">数据集</h1><p id="e6ec" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">我们将使用免费的音乐档案数据集<a class="ae ls" href="https://github.com/mdeff/fma/" rel="noopener ugc nofollow" target="_blank">https://github.com/mdeff/fma/</a>，更具体地说是包含 106，574 首 30 秒曲目、161 个不平衡流派的大型版本，总共有 93 Gb 的音乐数据。每首曲目都标有一组最能描述它的流派。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="f525" class="mw lv iq ms b gy mx my l mz na">"20": [<br/>        "Experimental Pop",<br/>        "Singer-Songwriter"<br/>    ],<br/>    "26": [<br/>        "Experimental Pop",<br/>        "Singer-Songwriter"<br/>    ],<br/>    "30": [<br/>        "Experimental Pop",<br/>        "Singer-Songwriter"<br/>    ],<br/>    "46": [<br/>        "Experimental Pop",<br/>        "Singer-Songwriter"<br/>    ],<br/>    "48": [<br/>        "Experimental Pop",<br/>        "Singer-Songwriter"<br/>    ],<br/>    "134": [<br/>        "Hip-Hop"<br/>    ]</span></pre><p id="cd1c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们在这个项目中的目标是预测这些标签。因为一首歌可以被附加到多个标签，所以它可以被公式化为具有 163 个目标的多标签分类问题，每个类别一个目标。</p><p id="f322" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一些类别非常频繁，例如电子音乐，占数据的 22%，但其他一些类别出现的次数很少，如萨尔萨，占数据集的 0.01%。这在训练和评估中造成了极端的不平衡，导致我们使用精确召回曲线下的微平均面积作为我们的度量。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="656d" class="mw lv iq ms b gy mx my l mz na">|     | Genre                    |   Frequency |    Fraction |<br/>|----:|:-------------------------|------------:|------------:|<br/>|   0 | Experimental             |       24912 | 0.233753    |<br/>|   1 | Electronic               |       23866 | 0.223938    |<br/>|   2 | Avant-Garde              |        8693 | 0.0815677   |<br/>|   3 | Rock                     |        8038 | 0.0754218   |<br/>|   4 | Noise                    |        7268 | 0.0681967   |<br/>|   5 | Ambient                  |        7206 | 0.067615    |<br/>|   6 | Experimental Pop         |        7144 | 0.0670332   |<br/>|   7 | Folk                     |        7105 | 0.0666673   |<br/>|   8 | Pop                      |        6362 | 0.0596956   |<br/>|   9 | Electroacoustic          |        6110 | 0.0573311   |<br/>|  10 | Instrumental             |        6055 | 0.056815    |<br/>|  11 | Lo-Fi                    |        6041 | 0.0566836   |<br/>|  12 | Hip-Hop                  |        5922 | 0.055567    |<br/>|  13 | Ambient Electronic       |        5723 | 0.0536998   |<br/>.<br/>.<br/>.<br/>| 147 | North African            |          40 | 0.000375326 |<br/>| 148 | Sound Effects            |          36 | 0.000337793 |<br/>| 149 | Tango                    |          30 | 0.000281495 |<br/>| 150 | Fado                     |          26 | 0.000243962 |<br/>| 151 | Talk Radio               |          26 | 0.000243962 |<br/>| 152 | Symphony                 |          25 | 0.000234579 |<br/>| 153 | Pacific                  |          23 | 0.000215812 |<br/>| 154 | Musical Theater          |          18 | 0.000168897 |<br/>| 155 | South Indian Traditional |          17 | 0.000159514 |<br/>| 156 | Salsa                    |          12 | 0.000112598 |<br/>| 157 | Banter                   |           9 | 8.44484e-05 |<br/>| 158 | Western Swing            |           4 | 3.75326e-05 |<br/>| 159 | N. Indian Traditional    |           4 | 3.75326e-05 |<br/>| 160 | Deep Funk                |           1 | 9.38315e-06 |</span></pre><h1 id="8f81" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">音频处理</h1><p id="6652" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">我们使用 Mel-Spectrograms 作为网络的输入，因为它是音频输入的密集表示，并且它更适合 transformer 架构，因为它将原始音频波转换为向量序列。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/084033e99aad3b74eae61138d9e3f1b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*EJCF1HTrNGnLCa9Pb-fTIA.png"/></div></figure><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="42ef" class="mw lv iq ms b gy mx my l mz na"><strong class="ms ir">def </strong>pre_process_audio_mel_t(audio, sample_rate=16000):<br/>    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate,<br/>                                              n_mels=n_mels)<br/>    mel_db = (librosa.power_to_db(mel_spec, ref=np.max) + 40) / 40</span><span id="bc5c" class="mw lv iq ms b gy nc my l mz na"><strong class="ms ir">return </strong>mel_db.T</span></pre><p id="1121" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">时间轴上的每个 128 维向量被认为是输入序列的一个元素。</p><p id="aaca" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">加载音频文件并将其子采样到 16kHz，然后计算 Mel 频谱图可能需要大量时间，因此我们使用 NumPy.save 预先计算并将其作为. npy 文件保存在磁盘上。</p><h1 id="7f63" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">模型</h1><p id="47f4" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">我选择超参数，使 rnn 和变压器都有类似数量的可训练参数。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="3619" class="mw lv iq ms b gy mx my l mz na">Model: "Transformer"<br/>_________________________________________________________________<br/><strong class="ms ir">Layer </strong>(<strong class="ms ir">type</strong>)                 <strong class="ms ir">Output Shape              Param</strong><em class="nd"> #   <br/></em>=================================================================<br/><strong class="ms ir">input_1 </strong>(<strong class="ms ir">InputLayer</strong>)         [(<strong class="ms ir">None</strong>, <strong class="ms ir">None</strong>, <strong class="ms ir">128</strong>)]       <strong class="ms ir">0         <br/></strong>_________________________________________________________________<br/><strong class="ms ir">encoder </strong>(<strong class="ms ir">Encoder</strong>)            (<strong class="ms ir">None</strong>, <strong class="ms ir">None</strong>, <strong class="ms ir">128</strong>)         <strong class="ms ir">529920    <br/></strong>_________________________________________________________________<br/><strong class="ms ir">dropout_9 </strong>(<strong class="ms ir">Dropout</strong>)          (<strong class="ms ir">None</strong>, <strong class="ms ir">None</strong>, <strong class="ms ir">128</strong>)         <strong class="ms ir">0         <br/></strong>_________________________________________________________________<br/><strong class="ms ir">global_average_pooling1d </strong>(<strong class="ms ir">Gl </strong>(<strong class="ms ir">None</strong>, <strong class="ms ir">128</strong>)               <strong class="ms ir">0         <br/></strong>_________________________________________________________________<br/><strong class="ms ir">dense_24 </strong>(<strong class="ms ir">Dense</strong>)             (<strong class="ms ir">None</strong>, <strong class="ms ir">652</strong>)               <strong class="ms ir">84108     <br/></strong>_________________________________________________________________<br/><strong class="ms ir">dense_25 </strong>(<strong class="ms ir">Dense</strong>)             (<strong class="ms ir">None</strong>, <strong class="ms ir">163</strong>)               <strong class="ms ir">106439    <br/></strong>=================================================================<br/><strong class="ms ir">Total </strong>params: 720,467<br/><strong class="ms ir">Trainable </strong>params: 720,467<br/><strong class="ms ir">Non-trainable </strong>params: 0<br/>_________________________________________________________________</span><span id="0f97" class="mw lv iq ms b gy nc my l mz na">Model: "RNN"<br/>_________________________________________________________________<br/><strong class="ms ir">Layer </strong>(<strong class="ms ir">type</strong>)                 <strong class="ms ir">Output Shape              Param</strong><em class="nd"> #   <br/></em>=================================================================<br/><strong class="ms ir">input_1 </strong>(<strong class="ms ir">InputLayer</strong>)         [(<strong class="ms ir">None</strong>, <strong class="ms ir">None</strong>, <strong class="ms ir">128</strong>)]       <strong class="ms ir">0         <br/></strong>_________________________________________________________________<br/><strong class="ms ir">bidirectional </strong>(<strong class="ms ir">Bidirectional </strong>(<strong class="ms ir">None</strong>, <strong class="ms ir">None</strong>, <strong class="ms ir">256</strong>)         <strong class="ms ir">198144    <br/></strong>_________________________________________________________________<br/><strong class="ms ir">bidirectional_1 </strong>(<strong class="ms ir">Bidirection </strong>(<strong class="ms ir">None</strong>, <strong class="ms ir">None</strong>, <strong class="ms ir">256</strong>)         <strong class="ms ir">296448    <br/></strong>_________________________________________________________________<br/><strong class="ms ir">dropout </strong>(<strong class="ms ir">Dropout</strong>)            (<strong class="ms ir">None</strong>, <strong class="ms ir">None</strong>, <strong class="ms ir">256</strong>)         <strong class="ms ir">0         <br/></strong>_________________________________________________________________<br/><strong class="ms ir">global_average_pooling1d </strong>(<strong class="ms ir">Gl </strong>(<strong class="ms ir">None</strong>, <strong class="ms ir">256</strong>)               <strong class="ms ir">0         <br/></strong>_________________________________________________________________<br/><strong class="ms ir">dense </strong>(<strong class="ms ir">Dense</strong>)                (<strong class="ms ir">None</strong>, <strong class="ms ir">652</strong>)               <strong class="ms ir">167564    <br/></strong>_________________________________________________________________<br/><strong class="ms ir">dense_1 </strong>(<strong class="ms ir">Dense</strong>)              (<strong class="ms ir">None</strong>, <strong class="ms ir">163</strong>)               <strong class="ms ir">106439    <br/></strong>=================================================================<br/><strong class="ms ir">Total </strong>params: 768,595<br/><strong class="ms ir">Trainable </strong>params: 768,595<br/><strong class="ms ir">Non-trainable </strong>params: 0<br/>_________________________________________________________________</span></pre><p id="85a2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这两种型号之间的唯一区别是编码器部分是变压器或双向 GRU。这两种型号具有 700k 可训练参数。</p><h1 id="0c24" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">估价</h1><p id="0bf6" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">我们将使用精确回忆曲线下的面积来评估每一种体裁，然后对各个班级进行微观平均。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/046317da8ec524bce8b09f6af1245534.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*NsDZgurOTHQTwMkLCynXeQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">变形金刚的嘻哈 PR 曲线</p></figure><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="1036" class="mw lv iq ms b gy mx my l mz na"><strong class="ms ir">RNN vs Transformer AUC PR =&gt;</strong></span><span id="329a" class="mw lv iq ms b gy nc my l mz na"><strong class="ms ir">transformer micro-average     </strong>:  <strong class="ms ir">0.20<br/>rnn micro-average             </strong>:  <strong class="ms ir">0.18</strong></span></pre><p id="0dc7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以看到 transformer 比 GRU 工作得好一点。我们可以通过做一些测试时间增加和平均输入序列的多个作物的预测来提高性能。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="2d81" class="mw lv iq ms b gy mx my l mz na"><strong class="ms ir">Test-Time Augmentation =&gt;</strong></span><span id="8b4a" class="mw lv iq ms b gy nc my l mz na"><strong class="ms ir">transformer micro-average     </strong>:  <strong class="ms ir">0.22<br/>rnn micro-average             </strong>:  <strong class="ms ir">0.19</strong></span></pre><p id="be97" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果总体上看起来有点弱，这可能是由于大量的课程使任务变得更加困难，或者可能是由于课程的不平衡。一个可能的改进是放弃多标签方法，转而采用排序方法，因为它对类别不平衡和大量类别不太敏感。</p><h2 id="c9f6" class="mw lv iq bd lw ne nf dn ma ng nh dp me le ni nj mg li nk nl mi lm nm nn mk no bi translated">预测示例:</h2><p id="d0aa" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">五大预测:</p><p id="5f9b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">贾扎的《午睡》</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="7b7f" class="mw lv iq ms b gy mx my l mz na">('Folk', 0.7591149806976318)<br/>('<strong class="ms ir">Pop</strong>', <strong class="ms ir">0.7336021065711975</strong>)<br/>('<strong class="ms ir">Indie-Rock</strong>', <strong class="ms ir">0.6384000778198242</strong>)<br/>('Instrumental', 0.5678483843803406)<br/>('Singer-Songwriter', 0.558732271194458)</span></pre><p id="4e66" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae ls" href="https://freemusicarchive.org/music/Jahzzar/Travellers_Guide/Siesta" rel="noopener ugc nofollow" target="_blank">扬·卡茨的《聪明人》</a></p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="39fa" class="mw lv iq ms b gy mx my l mz na">('Electronic', 0.8624182939529419)<br/>('Experimental', 0.6041183471679688)<br/>('<strong class="ms ir">Hip-Hop</strong>', <strong class="ms ir">0.369397908449173</strong>)<br/>('Glitch', 0.31879115104675293)<br/>('Techno', 0.30013027787208557)</span></pre><h1 id="8ccd" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">结论</h1><p id="52f0" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">在这篇文章中，我们比较了两种流行的序列建模结构 rnn 和变压器。我们看到 transformers 在性能上略胜 GRUs，这表明 Transformers 甚至可以在自然语言处理之外进行测试。</p><h1 id="07ee" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">参考</h1><p id="9666" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">TF2 变形金刚:<a class="ae ls" href="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb" rel="noopener ugc nofollow" target="_blank">T5】https://github . com/tensor flow/docs/blob/master/site/en/tutorials/text/transformer . ipynbT7】</a></p><p id="8507" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">代号:<a class="ae ls" href="https://github.com/CVxTz/music_genre_classification" rel="noopener ugc nofollow" target="_blank">https://github.com/CVxTz/music_genre_classification</a></p></div></div>    
</body>
</html>