<html>
<head>
<title>Converting PDF and Gutenberg Document Formats into Text: Natural Language Processing in Production</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将 PDF 和 Gutenberg 文档格式转换为文本:生产中的自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-processing-in-production-converting-pdf-and-gutenberg-document-formats-into-text-9e7cd3046b33?source=collection_archive---------20-----------------------#2020-08-22">https://towardsdatascience.com/natural-language-processing-in-production-converting-pdf-and-gutenberg-document-formats-into-text-9e7cd3046b33?source=collection_archive---------20-----------------------#2020-08-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5852" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在<strong class="ak"> </strong>生产级<strong class="ak">自然语言处理(NLP </strong>)中最关键的是将流行的文档格式快速预处理成文本。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fae8865e4c34fca18635ef253457f99a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TY4cMAE6o_x7KR3AW2Z07Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">企业中有大量不同的文档。资料来源:联合国人类住区规划署</p></figure><p id="1af0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">据估计，世界上 70%–85%的数据是文本(非结构化数据)。大多数英语和欧盟商务数据格式为字节文本、MS Word 或<strong class="la iu"> Adobe </strong> PDF。[1]</p><p id="633c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">组织 web 显示一个<strong class="la iu">dobe</strong>T9】Postscript 文档格式文档(<strong class="la iu"> PDF </strong>)。[2]</p><p id="9e80" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇博客中，我详述了以下内容:</p><ol class=""><li id="8113" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">从 web 文件名和本地文件名创建文件路径；</li><li id="8ea3" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">将字节编码的古腾堡项目文件转换成文本语料库；</li><li id="2e06" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">将 PDF 文档转换成文本语料库；</li><li id="d91b" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">将连续文本分割成单词文本语料库。</li></ol></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="cedd" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">将流行的文档格式转换为文本</h1><h2 id="4e40" class="nh mq it bd mr ni nj dn mv nk nl dp mz lh nm nn nb ll no np nd lp nq nr nf ns bi translated">1.从 web 文件名或本地文件名创建本地文件路径</h2><p id="dd63" class="pw-post-body-paragraph ky kz it la b lb nt ju ld le nu jx lg lh nv lj lk ll nw ln lo lp nx lr ls lt im bi translated">以下函数将采用本地文件名或远程文件 URL 并返回一个文件路径对象。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="48bd" class="nh mq it nz b gy od oe l of og">#in file_to_text.py<br/>--------------------------------------------<br/>from io import StringIO, BytesIO<br/>import urllib<br/><br/>def file_or_url(pathfilename:str) -&gt; Any:<br/>    <em class="oh">"""<br/>    Reurn filepath given local file or URL.<br/>    Args:<br/>        pathfilename:<br/><br/>    Returns:<br/>        filepath odject istance<br/><br/>    """<br/>    </em>try:<br/>        fp = open(pathfilename, mode="rb")  # file(path, 'rb')<br/>    except:<br/>        pass<br/>    else:<br/>        url_text = urllib.request.urlopen(pathfilename).read()<br/>        fp = BytesIO(url_text)<br/>    return fp</span></pre><h2 id="d962" class="nh mq it bd mr ni nj dn mv nk nl dp mz lh nm nn nb ll no np nd lp nq nr nf ns bi translated">2.将 Unicode 字节编码文件转换成一个 o Python Unicode 字符串</h2><p id="66a7" class="pw-post-body-paragraph ky kz it la b lb nt ju ld le nu jx lg lh nv lj lk ll nw ln lo lp nx lr ls lt im bi translated">您将经常遇到 8 位 Unicode 格式的文本 blob 下载(在浪漫的语言中)。您需要将 8 位 Unicode 转换为 Python Unicode 字符串。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="3ccc" class="nh mq it nz b gy od oe l of og">#in file_to_text.py<br/>--------------------------------------------<br/>def unicode_8_to_text(text: str) -&gt; str:<br/>    return text.decode("utf-8", "replace")</span><span id="da44" class="nh mq it nz b gy oi oe l of og">import urllib<br/>from file_to_text import unicode_8_to_text</span><span id="ad90" class="nh mq it nz b gy oi oe l of og">text_l = 250</span><span id="854b" class="nh mq it nz b gy oi oe l of og">text_url = r'<a class="ae oj" href="http://www.gutenberg.org/files/74/74-0.txt'" rel="noopener ugc nofollow" target="_blank">http://www.gutenberg.org/files/74/74-0.txt'</a> <br/>gutenberg_text =  urllib.request.urlopen(text_url).read()<br/>%time gutenberg_text = unicode_8_to_text(gutenberg_text)<br/>print('{}: size: {:g} \n {} \n'.format(0, len(gutenberg_text) ,gutenberg_text[:text_l]))</span></pre><p id="3d75" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出= &gt;</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="8bfc" class="nh mq it nz b gy od oe l of og">CPU times: user 502 µs, sys: 0 ns, total: 502 µs<br/>Wall time: 510 µs<br/>0: size: 421927 <br/> ﻿<br/>The Project Gutenberg EBook of The Adventures of Tom Sawyer, Complete by<br/>Mark Twain (Samuel Clemens)<br/><br/>This eBook is for the use of anyone anywhere at no cost and with almost<br/>no restrictions whatsoever. You may copy it, give it away or re-use<br/>it under the terms of the Project Gutenberg License included with this<br/>eBook or online at www.guten</span></pre><p id="80b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结果是<code class="fe ok ol om nz b">text.decode('utf-8') </code>可以在大约 1/1000 秒内格式化成一个包含一百万个字符的<strong class="la iu"> Python </strong>字符串。这一速度远远超过了我们的生产率要求。</p><h2 id="0640" class="nh mq it bd mr ni nj dn mv nk nl dp mz lh nm nn nb ll no np nd lp nq nr nf ns bi translated">3.将 PDF 文档转换为文本语料库。</h2><p id="bfd7" class="pw-post-body-paragraph ky kz it la b lb nt ju ld le nu jx lg lh nv lj lk ll nw ln lo lp nx lr ls lt im bi translated"><strong class="la iu"> <em class="oh">“将 PDF 文档转换成文本语料库</em> </strong>”是我为<strong class="la iu"> NLP </strong>文本预处理做的最麻烦也是最常见的任务之一。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="64ac" class="nh mq it nz b gy od oe l of og">#in file_to_text.py<br/>--------------------------------------------<br/>def PDF_to_text(pathfilename: str) -&gt; str:<br/>    <em class="oh">"""<br/>    Chane PDF format to text.<br/>    Args:<br/>        pathfilename:<br/><br/>    Returns:<br/><br/>    """<br/><br/>    </em>fp = file_or_url(pathfilename)<br/>    rsrcmgr = PDFResourceManager()<br/>    retstr = StringIO()<br/>    laparams = LAParams()<br/>    device = TextConverter(rsrcmgr, retstr, laparams=laparams)<br/>    interpreter = PDFPageInterpreter(rsrcmgr, device)<br/>    password = ""<br/>    maxpages = 0<br/>    caching = True<br/>    pagenos = set()<br/><br/>    for page in PDFPage.get_pages(<br/>        fp,<br/>        pagenos,<br/>        maxpages=maxpages,<br/>        password=password,<br/>        caching=caching,<br/>        check_extractable=True,<br/>    ):<br/>        interpreter.process_page(page)<br/><br/>    text = retstr.getvalue()<br/><br/>    fp.close()<br/>    device.close()<br/>    retstr.close()<br/><br/>    return text<br/>-------------------------------------------------------</span><span id="6e53" class="nh mq it nz b gy oi oe l of og">arvix_list =['<a class="ae oj" href="https://arxiv.org/pdf/2008.05828v1.pdf'" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2008.05828v1.pdf'</a><br/>             , '<a class="ae oj" href="https://arxiv.org/pdf/2008.05981v1.pdf'" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2008.05981v1.pdf'</a><br/>             , '<a class="ae oj" href="https://arxiv.org/pdf/2008.06043v1.pdf'" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2008.06043v1.pdf'</a><br/>             , 'tmp/inf_finite_NN.pdf' ]<br/>for n, f in enumerate(arvix_list):<br/>    %time pdf_text = PDF_to_text(f).replace('\n', ' ')<br/>    print('{}: size: {:g} \n {} \n'.format(n, len(pdf_text) ,pdf_text[:text_l])))</span></pre><p id="da5f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出= &gt;</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="0ed7" class="nh mq it nz b gy od oe l of og">CPU times: user 1.89 s, sys: 8.88 ms, total: 1.9 s<br/>Wall time: 2.53 s<br/>0: size: 42522 <br/> On the Importance of Local Information in Transformer Based Models  Madhura Pande, Aakriti Budhraja, Preksha Nema  Pratyush Kumar, Mitesh M. Khapra  Department of Computer Science and Engineering  Robert Bosch Centre for Data Science and AI (RBC-DSAI)  Indian Institute of Technology Madras, Chennai, India  {mpande,abudhra,preksha,pratyush,miteshk}@ <br/><br/>CPU times: user 1.65 s, sys: 8.04 ms, total: 1.66 s<br/>Wall time: 2.33 s<br/>1: size: 30586 <br/> ANAND,WANG,LOOG,VANGEMERT:BLACKMAGICINDEEPLEARNING1BlackMagicinDeepLearning:HowHumanSkillImpactsNetworkTrainingKanavAnand1anandkanav92@gmail.comZiqiWang1z.wang-8@tudelft.nlMarcoLoog12M.Loog@tudelft.nlJanvanGemert1j.c.vangemert@tudelft.nl1DelftUniversityofTechnology,Delft,TheNetherlands2UniversityofCopenhagenCopenhagen,DenmarkAbstractHowdoesauser’sp <br/><br/>CPU times: user 4.82 s, sys: 46.3 ms, total: 4.87 s<br/>Wall time: 6.53 s<br/>2: size: 57204 <br/> 0 2 0 2     g u A 3 1         ]  G L . s c [      1 v 3 4 0 6 0  .  8 0 0 2 : v i X r a  Ofﬂine Meta-Reinforcement Learning with  Advantage Weighting  Eric Mitchell1, Rafael Rafailov1, Xue Bin Peng2, Sergey Levine2, Chelsea Finn1  1 Stanford University, 2 UC Berkeley  em7@stanford.edu  Abstract  Massive datasets have proven critical to successfully <br/><br/>CPU times: user 12.2 s, sys: 36.1 ms, total: 12.3 s<br/>Wall time: 12.3 s<br/>3: size: 89633 <br/> 0 2 0 2    l u J    1 3      ]  G L . s c [      1 v 1 0 8 5 1  .  7 0 0 2 : v i X r a  Finite Versus Inﬁnite Neural Networks:  an Empirical Study  Jaehoon Lee  Samuel S. Schoenholz∗  Jeffrey Pennington∗  Ben Adlam†∗  Lechao Xiao∗  Roman Novak∗  Jascha Sohl-Dickstein  {jaehlee, schsam, jpennin, adlam, xlc, romann, jaschasd}@google.com  Google Brain</span></pre><p id="3083" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个硬件配置上，“将 PDF 文件转换成 Python 字符串需要 150 秒。对于 Web 交互式生产应用程序来说不够快。</p><p id="1d09" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可能希望在后台设置格式。</p><h2 id="8b5d" class="nh mq it bd mr ni nj dn mv nk nl dp mz lh nm nn nb ll no np nd lp nq nr nf ns bi translated">4.将连续文本分割成单词文本语料库</h2><p id="fa20" class="pw-post-body-paragraph ky kz it la b lb nt ju ld le nu jx lg lh nv lj lk ll nw ln lo lp nx lr ls lt im bi translated">当我们阅读<a class="ae oj" href="https://arxiv.org/pdf/2008.05981v1.pdf'" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2008.05981v1.pdf'</a>时，它返回的是没有分隔字符的连续文本。使用来自<strong class="la iu"> wordsegment，</strong>的包，我们把连续的字符串分成单词。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="8b81" class="nh mq it nz b gy od oe l of og">from wordsegment import load,  clean, segment<br/>%time words = segment(pdf_text)<br/>print('size: {:g} \n'.format(len(words)))<br/>' '.join(words)[:text_l*4]</span></pre><p id="2779" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出= &gt;</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="b425" class="nh mq it nz b gy od oe l of og">CPU times: user 1min 43s, sys: 1.31 s, total: 1min 44s<br/>Wall time: 1min 44s<br/>size: 5005</span><span id="1ee1" class="nh mq it nz b gy oi oe l of og">'an and wang loog van gemert blackmagic in deep learning 1 blackmagic in deep learning how human skill impacts network training kanavanand1anandkanav92g mailcom ziqiwang1zwang8tudelftnl marco loog12mloogtudelftnl jan van gemert 1jcvangemerttudelftnl1 delft university of technology delft the netherlands 2 university of copenhagen copenhagen denmark abstract how does a users prior experience with deep learning impact accuracy we present an initial study based on 31 participants with different levels of experience their task is to perform hyper parameter optimization for a given deep learning architecture the results show a strong positive correlation between the participants experience and then al performance they additionally indicate that an experienced participant nds better solutions using fewer resources on average the data suggests furthermore that participants with no prior experience follow random strategies in their pursuit of optimal hyperparameters our study investigates the subjective human factor in comparisons of state of the art results and scientic reproducibility in deep learning 1 introduction the popularity of deep learning in various elds such as image recognition 919speech1130 bioinformatics 2124questionanswering3 etc stems from the seemingly favorable tradeoff between the recognition accuracy and their optimization burden lecunetal20 attribute their success t'</span></pre><p id="9971" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你会注意到<strong class="la iu">单词切分</strong>完成了相当精确的单词切分。有一些错误，或者我们不想要的单词，<strong class="la iu"> NLP </strong>文本预处理清除掉。</p><p id="5089" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">阿帕奇语段速度很慢。对于少于 1000 字的小文档，它几乎不能满足生产的需要。我们能找到更快的分割方法吗？</p><h2 id="7b4b" class="nh mq it bd mr ni nj dn mv nk nl dp mz lh nm nn nb ll no np nd lp nq nr nf ns bi translated">4b。将连续文本分割成单词文本语料库</h2><p id="4f3b" class="pw-post-body-paragraph ky kz it la b lb nt ju ld le nu jx lg lh nv lj lk ll nw ln lo lp nx lr ls lt im bi translated">似乎有一种更快的方法来“将连续文本分割成单词文本的语料库”</p><p id="8401" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如以下博客中所讨论的:</p><div class="on oo gp gr op oq"><a rel="noopener follow" target="_blank" href="/symspell-vs-bk-tree-100x-faster-fuzzy-string-search-spell-checking-c4f10d80a078"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">符号拼写与 BK 树:模糊字符串搜索和拼写检查快 100 倍</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">传统智慧和教科书说 BK 树特别适合拼写纠正和模糊字符串搜索…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe ks oq"/></div></div></a></div><p id="66ba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> SymSpell </strong>快 100-1000 倍。哇！</p><p id="e50a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="oh">注:编辑:2020 年 8 月 24 日沃尔夫·加贝指出</em>值得称赞</p><blockquote class="pf pg ph"><p id="7b67" class="ky kz oh la b lb lc ju ld le lf jx lg pi li lj lk pj lm ln lo pk lq lr ls lt im bi translated">SymSpell 博客文章中给出的基准测试结果(快 100-1000 倍)仅指拼写纠正，而非分词。在那篇文章中，SymSpell 与其他拼写纠正算法进行了比较，而不是与分词算法进行比较。2020 年 8 月 23 日</p></blockquote><p id="ece8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">和</p><blockquote class="pf pg ph"><p id="24dc" class="ky kz oh la b lb lc ju ld le lf jx lg pi li lj lk pj lm ln lo pk lq lr ls lt im bi translated">此外，从 Python 调用 C#库还有一种更简单的方法:<a class="ae oj" href="https://stackoverflow.com/questions/7367976/calling-a-c-sharp-library-from-python" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/7367976/calling-a-C-sharp-library-from-Python</a>—Wolfe Garbe 8/23/2020</p></blockquote><p id="be36" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注:编辑日期:2020 年 8 月 24 日。我打算试试 Garbe 的 C##实现。如果我没有得到相同的结果(很可能如果我得到了)，我将尝试 cython port，看看我是否能作为管道元素适合 spacy。我会让你知道我的结果。</p><p id="b5b6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是在<strong class="la iu"> C# </strong>中实现。因为我不会掉进无限的老鼠洞:</p><ul class=""><li id="09be" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt pl ma mb mc bi translated">把我所有的<strong class="la iu"> NLP </strong>转换成<strong class="la iu"> C# </strong>。不可行的选择。</li><li id="0a53" class="lu lv it la b lb md le me lh mf ll mg lp mh lt pl ma mb mc bi translated">从<strong class="la iu"> Python </strong>调用<strong class="la iu"> C# </strong>。我和 Python 团队的两位工程师经理谈过。他们拥有<strong class="la iu"> Python-C# </strong>功能，但是它涉及到:</li></ul><p id="94e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意:</p><ol class=""><li id="946f" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">翻译成<strong class="la iu">VB</strong>-香草；</li><li id="1dc0" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">人工干预和翻译必须通过再现性测试；</li><li id="6fb9" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">从<strong class="la iu"> VB </strong> -vanilla 翻译成<strong class="la iu">C</strong>；</li><li id="5b2e" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">手动干预和翻译必须通过重复性测试。</li></ol><p id="7d56" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相反，我们使用一个到 Python 的端口。下面是一个版本:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="072d" class="nh mq it nz b gy od oe l of og">def segment_into_words(input_term):<br/>    # maximum edit distance per dictionary precalculation<br/>    max_edit_distance_dictionary = 0<br/>    prefix_length = 7<br/>    # create object<br/>    sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)<br/>    # load dictionary<br/>    dictionary_path = pkg_resources.resource_filename(<br/>        "symspellpy", "frequency_dictionary_en_82_765.txt")<br/>    bigram_path = pkg_resources.resource_filename(<br/>        "symspellpy", "frequency_bigramdictionary_en_243_342.txt")<br/>    # term_index is the column of the term and count_index is the<br/>    # column of the term frequency<br/>    if not sym_spell.load_dictionary(dictionary_path, term_index=0,<br/>                                     count_index=1):<br/>        print("Dictionary file not found")<br/>        return<br/>    if not sym_spell.load_bigram_dictionary(dictionary_path, term_index=0,<br/>                                            count_index=2):<br/>        print("Bigram dictionary file not found")<br/>        return</span><span id="28f4" class="nh mq it nz b gy oi oe l of og">result = sym_spell.word_segmentation(input_term)<br/>    return result.corrected_string</span><span id="74b3" class="nh mq it nz b gy oi oe l of og">%time long_s = segment_into_words(pdf_text)<br/>print('size: {:g} {}'.format(len(long_s),long_s[:text_l*4]))</span></pre><p id="1aa1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出= &gt;</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="a6a2" class="nh mq it nz b gy od oe l of og">CPU times: user 20.4 s, sys: 59.9 ms, total: 20.4 s<br/>Wall time: 20.4 s<br/>size: 36585 ANAND,WANG,LOOG,VANGEMER T:BLACKMAGICINDEEPLEARNING1B lack MagicinDeepL earning :HowHu man S kill Imp acts Net work T raining Ka nav An and 1 an and kana v92@g mail . com ZiqiWang1z. wang -8@tu delft .nlM arc oLoog12M.Loog@tu delft .nlJ an van Gemert1j.c. vang emert@tu delft .nl1D elf tUniversityofTechn ology ,D elf t,TheN ether lands 2UniversityofC open hagen C open hagen ,Den mark Abs tract How does a user ’s prior experience with deep learning impact accuracy ?We present an initial study based on 31 participants with different levels of experience .T heir task is to perform hyper parameter optimization for a given deep learning architecture .T here -s ult s show a strong positive correlation between the participant ’s experience and the ﬁn al performance .T hey additionally indicate that an experienced participant ﬁnds better sol u-t ions using fewer resources on average .T he data suggests furthermore that participants with no prior experience follow random strategies in their pursuit of optimal hyper pa-ra meters .Our study investigates the subjective human factor in comparisons of state of the art results and sci entiﬁc reproducibility in deep learning .1Intro duct ion T he popularity of deep learning in various ﬁ eld s such as image recognition [9,19], speech [11,30], bio informatics [21,24], question answering [3] etc . stems from the seemingly fav or able trade - off b</span></pre><p id="bcd5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<strong class="la iu"> Python </strong>中实现的 SymSpellpy 大约快了 5 倍。我们没有看到 100-1000 倍的速度。</p><p id="9579" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我猜<strong class="la iu"> SymSpell-C# </strong>是在比较用<strong class="la iu"> Python </strong>实现的不同分割算法。</p><p id="731b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也许我们看到的加速是由于<strong class="la iu"> C# </strong>，一种编译过的静态类型语言。由于<strong class="la iu"> C# </strong>和<strong class="la iu"> C </strong>的计算速度差不多，我们应该期待<strong class="la iu"> C# </strong>比<strong class="la iu"> Python </strong>实现快 100-1000 倍。</p><blockquote class="pf pg ph"><p id="5748" class="ky kz oh la b lb lc ju ld le lf jx lg pi li lj lk pj lm ln lo pk lq lr ls lt im bi translated">注意:有一个<strong class="la iu"> spacy </strong>流水线实现<strong class="la iu"> spacy_symspell，</strong>直接调用<strong class="la iu"> SymSpellpy。</strong>我建议你<strong class="la iu">不要</strong>使用 spacy_symspell。Spacy 首先生成令牌作为流水线的第一步，这是不可变的。<strong class="la iu"> spacy_symspell </strong>从<strong class="la iu"> </strong>分割连续文本生成新文本。它不能在空间<strong class="la iu">中生成新的令牌，因为空间</strong>已经生成了令牌<strong class="la iu">。。一个</strong> <strong class="la iu">空间</strong>管道工作于一个令牌序列，而不是一个文本流<strong class="la iu">。人们将不得不衍生出一个改变了的版本。何必呢？</strong>代替<strong class="la iu">，</strong>将连续文本分割成单词文本语料库。然后更正单词中嵌入的空格和带连字符的单词。做任何其他你想做的原始清洁。然后将原始文本输入到<strong class="la iu">空间</strong>。</p></blockquote><p id="7c92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我显示<strong class="la iu">空间 _ 符号拼写。</strong>再一次我的建议是<strong class="la iu"> <em class="oh">不要</em> </strong>使用它。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="25af" class="nh mq it nz b gy od oe l of og">import spacy<br/>from spacy_symspell import SpellingCorrector</span><span id="7e3a" class="nh mq it nz b gy oi oe l of og"><br/>def segment_into_words(input_term):<br/>nlp = spacy.load(“en_core_web_lg”, disable=[“tagger”, “parser”])<br/>corrector = SpellingCorrector()<br/>nlp.add_pipe(corrector)</span></pre></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="2b85" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">结论</h1><p id="f997" class="pw-post-body-paragraph ky kz it la b lb nt ju ld le nu jx lg lh nv lj lk ll nw ln lo lp nx lr ls lt im bi translated">在以后的博客中，我会详细介绍许多常见和不常见的<strong class="la iu">快速文本预处理方法。</strong>此外，我将展示从移动<strong class="la iu"> SymSpellpy </strong>到<strong class="la iu"> cython 的预期加速。</strong></p><p id="3d0a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在“将 X 格式转换成文本语料库”的世界中，您需要支持更多的格式和 API。</p><p id="4e73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我详细介绍了两种更常见的文档格式，<strong class="la iu"> PDF </strong>和<strong class="la iu">古腾堡</strong> <strong class="la iu">项目</strong>格式。另外，我给出了两个<strong class="la iu"> NLP </strong>实用函数<code class="fe ok ol om nz b">segment_into_words</code>和<code class="fe ok ol om nz b">file_or_url.</code></p><p id="f957" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你学到了一些东西，并且可以使用这个博客中的一些代码。</p><p id="84e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你有一些格式转换或者更好的软件包，请告诉我。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="900f" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">参考</h1><p id="b6d3" class="pw-post-body-paragraph ky kz it la b lb nt ju ld le nu jx lg lh nv lj lk ll nw ln lo lp nx lr ls lt im bi translated">[1] <a class="ae oj" href="https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/#4eb3f0f960ba" rel="noopener ugc nofollow" target="_blank">我们每天会产生多少数据？</a></p><p id="7c87" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]<a class="ae oj" href="https://en.wikipedia.org/wiki/PDF" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/PDF</a></p><p id="b0d9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">【3】<a class="ae oj" href="https://arxiv.org/pdf/1202.2518.pdf" rel="noopener ugc nofollow" target="_blank">将 DNA 序列分割成‘单词’</a></p></div></div>    
</body>
</html>