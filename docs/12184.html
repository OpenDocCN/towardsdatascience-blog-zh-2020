<html>
<head>
<title>RepNet: Counting Repeating Actions in a Video</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RepNet:计算视频中的重复动作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/repnet-counting-repeating-actions-in-a-video-bec1bdcec149?source=collection_archive---------26-----------------------#2020-08-22">https://towardsdatascience.com/repnet-counting-repeating-actions-in-a-video-bec1bdcec149?source=collection_archive---------26-----------------------#2020-08-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d204" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用于计数重复动作和估计视频中周期的最新模型的方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a18a396a017549f93f346f9039cadd9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jd5ePIdTERW5ADfZ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯·贾维斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4d72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然我们大多数人都可以在锻炼或测量脉搏时计数，但如果能有一些东西为我们计数，甚至提供关于重复动作的有价值的信息，那就太好了。特别是对于周期较长的动作，如行星周期，或者周期太短的动作，如制造带。</p><p id="01b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谷歌研究和 DeepMind 团队最近在 2020 年 CPVR 发表的一篇名为<strong class="lb iu">计算时间:野外不可知视频重复计数</strong>的论文解决了这个有趣的问题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="8f9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们采用一种非常“简单”的方法来识别和计数重复的动作，并最终预测动作的周期性。但作者发现的主要障碍是管理足够大的数据集来做到这一点。因此，本文[1]的主要贡献在于:</p><ul class=""><li id="3240" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">释放<strong class="lb iu">计数</strong>:一个新的视频重复计数数据集，它比先前最大的数据集大 90 倍</li><li id="642a" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">发布<strong class="lb iu"> RepNet </strong>:一种用于计数和测量“野外”视频中重复周期的神经网络架构[1]，使其优于之前的最先进方法</li><li id="2b1b" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">使用合成的、无标签的剪辑并生成可用于训练的增强视频</li></ul></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="55a1" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">Countix 数据集</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/6c1e54b115cef84f096c84868a17d694.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*o_BWSymr84PSx0C0eXhSYg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Countix 中的样本来自[1]</p></figure><p id="b459" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，作者提出了一个新的、巨大的带有重复动作的带注释视频数据集——因为现有的重复数据集太小了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/e2a40ab8f4c561136dbfb5c002ac2e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*4ZROqbcJCZlU2pHgC1eVQA.png"/></div></figure><p id="4fc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请看上图，它比较了新数据集:<strong class="lb iu"> Countix </strong>和现有的“基准测试”(只有 100 个样本！).Countix 有从野外拍摄的视频(如 YouTube ),团队通过众包收集注释。标注器“分割包含具有明确计数的有效重复的视频部分”[1]。并且它们也继续添加计数值。一个漂亮的过程最终产生了一个比之前的数据集大 90 倍的带注释的数据集。</p><blockquote class="nm nn no"><p id="926f" class="kz la np lb b lc ld ju le lf lg jx lh nq lj lk ll nr ln lo lp ns lr ls lt lu im bi translated">你可以在这里下载 Countix</p><p id="3c25" class="kz la np lb b lc ld ju le lf lg jx lh nq lj lk ll nr ln lo lp ns lr ls lt lu im bi translated">在这里看一看<a class="ae ky" href="https://deepmind.com/research/open-source/kinetics" rel="noopener ugc nofollow" target="_blank">动力学</a></p></blockquote><h2 id="3292" class="nt mt it bd mu nu nv dn my nw nx dp nc li ny nz ne lm oa ob ng lq oc od ni oe bi translated">又一个数据集</h2><p id="5dce" class="pw-post-body-paragraph kz la it lb b lc of ju le lf og jx lh li oh lk ll lm oi lo lp lq oj ls lt lu im bi translated">除了 Countix(其剪辑来自<a class="ae ky" href="https://deepmind.com/research/open-source/kinetics" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">、动力学</strong> </a>)之外，作者还提出了一种很酷的方式，即<strong class="lb iu">从类似真实重复视频的普通视频中生成合成视频</strong>。</p><p id="045e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，他们所做的是:</p><ul class=""><li id="a393" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">从运动训练集中的真实视频中截取短片，并重复该视频片段一定次数。<strong class="lb iu">这固定了一个计数、周期，并产生一个重复的视频。</strong></li><li id="3d8f" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">为了保持“<strong class="lb iu">真实度</strong>，他们<strong class="lb iu">在重复片段前后预挂起并附加非重复片段</strong>。例如，一个女孩可以走进视频帧，进行一次旋转(就像在<strong class="lb iu">结果可视化</strong>部分的图像 1 的帧 3 中)，然后走开。因此，这种方法将导致重复单次旋转的片段多次——保持开始和结束时的行走片段完好无损。</li><li id="2db8" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">请看下图中的第二帧。这种奇怪的变换是将<strong class="lb iu">摄像机运动增强</strong>应用于视频的结果。正如我们所知，在图像中，我们可以通过旋转、倾斜、通道转换二维图像来轻松执行<strong class="lb iu">图像增强</strong>。类似地，作者以<strong class="lb iu">时间</strong>方式对这些视频应用旋转、平移和缩放变换。时间时尚本质上意味着应用于每一帧的变换不是随机的，并且当播放视频时，它在变换的顺序和流程方面仍然有意义。这个<strong class="lb iu">确保了我们在第二帧中看到的一个流畅的、看似动画的视频</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/c65fda61d2783261e5af752fee0d1b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*lUe7YFu16n0d0q-FuMzbKA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">摄像机运动增强来自[2]</p></figure><h1 id="4df3" class="ms mt it bd mu mv ol mx my mz om nb nc jz on ka ne kc oo kd ng kf op kg ni nj bi translated">让我们训练吧</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/ba7a96dc8fcbc09926f9089208b15097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vSQmJ-rpL_2SVdpv"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">布莱恩·苏曼在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="6fea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型架构由以下部分组成:</p><ul class=""><li id="18b1" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated"><strong class="lb iu">编码器</strong>:包括一个特征提取器(基于 ResNet-50)，一个 3D 卷积层(检测重复运动的不同部分)，以及使用全局 2D 最大池的维数减少</li><li id="9d9f" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><strong class="lb iu">时间自相似性矩阵:</strong>计算两个帧之间的成对相似性(或者，更准确地说，两个帧的编码嵌入)</li><li id="d220" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><strong class="lb iu">周期预测器:</strong>使用相似矩阵来预测每帧的周期长度和周期性。该预测器由 32 个 3D conv 层、多头注意力转换器层和作为分类器的 2 个 FC 层组成</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/a0495ae34c50b47eea5424389611843f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04iiiv4yrqbIMia6ymxRLA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型架构来自[1]</p></figure><blockquote class="nm nn no"><p id="932f" class="kz la np lb b lc ld ju le lf lg jx lh nq lj lk ll nr ln lo lp ns lr ls lt lu im bi translated">这款内容丰富的<a class="ae ky" href="https://colab.research.google.com/github/google-research/google-research/blob/master/repnet/repnet_colab.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Colab 笔记本</strong> </a>带你经历在<strong class="lb iu">本地视频</strong>和你的<strong class="lb iu">现场网络摄像头</strong>上执行模型和测试的每一步</p></blockquote></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="b76d" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">可视化结果</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/88c9b8ebee5cea2df4ea95c9af9e7279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*jeaBj2BmXg2CZ9i_gTR7Og.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。重复计数的应用[2]</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/81f0a40c82a1fe4e072cc18be9aac2a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/1*gJysluEULp7Jr7cWEEiOYQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">速度和周期性来自[2]。这表明该模型甚至能够捕捉不同频率的重复运动；这可以告诉主体是减速还是加速。</p></figure><h1 id="dd20" class="ms mt it bd mu mv ol mx my mz om nb nc jz on ka ne kc oo kd ng kf op kg ni nj bi translated">接下来呢？</h1><p id="5d44" class="pw-post-body-paragraph kz la it lb b lc of ju le lf og jx lh li oh lk ll lm oi lo lp lq oj ls lt lu im bi translated">你能想出这种方法需要解决的事件或角度吗？这在现实世界中哪里可以工作？哪里没有？</p><p id="6ec6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文提供了 RepNet 的以下实际应用:</p><ul class=""><li id="7958" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">运动追踪:计数、速度等。</li><li id="7e14" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">依靠心电图视频</li><li id="598e" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">天文和地球物理变化，如一天的长度或分析天体事件的卫星图像</li></ul></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="0ef0" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">参考资料:</h1><p id="65e6" class="pw-post-body-paragraph kz la it lb b lc of ju le lf og jx lh li oh lk ll lm oi lo lp lq oj ls lt lu im bi translated">[1]Dwibedi，Debidatta 等，“计算时间:野外的类不可知视频重复计数。”2020 年，<em class="np">IEEE/CVF 计算机视觉和模式识别会议论文集</em>。</p><blockquote class="nm nn no"><p id="a1fc" class="kz la np lb b lc ld ju le lf lg jx lh nq lj lk ll nr ln lo lp ns lr ls lt lu im bi translated"><a class="ae ky" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dwibedi_Counting_Out_Time_Class_Agnostic_Video_Repetition_Counting_in_the_CVPR_2020_paper.pdf" rel="noopener ugc nofollow" target="_blank">链接到 PDF </a></p></blockquote><p id="fe80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]RepNet[1]的项目网页:<a class="ae ky" href="https://sites.google.com/view/repnet" rel="noopener ugc nofollow" target="_blank">https://sites.google.com/view/repnet</a></p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><blockquote class="nm nn no"><p id="7c80" class="kz la np lb b lc ld ju le lf lg jx lh nq lj lk ll nr ln lo lp ns lr ls lt lu im bi translated">感谢您从头到尾的阅读！您可以通过 LinkedIn<a class="ae ky" href="https://www.linkedin.com/in/param-raval/" rel="noopener ugc nofollow" target="_blank">联系我，获取任何信息、想法或建议。</a></p></blockquote></div></div>    
</body>
</html>