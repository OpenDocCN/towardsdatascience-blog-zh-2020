# 评估推荐引擎

> 原文：<https://towardsdatascience.com/evaluating-recommendation-engines-74daebbe46db?source=collection_archive---------43----------------------->

## 解释来自混淆矩阵的度量

我最近被要求为一个数字活动过滤邮件列表。邮件列表有 40k+强，如果邮件发送到整个列表，那么退订的机会将会很高，因为电子邮件被发送给那些可能对产品不感兴趣的人。

此外，打开率和点击率也不是一个好的销售指标，因为电子邮件会被“错误的”人阅读和点击。举个例子，如果我收到一封关于最新法拉利的电子邮件，我很可能会打开邮件，阅读它，甚至点击“立即购买”链接，但只是为了查看价格，而不是实际购买。在这种情况下，打开率和点击率会因为我的动作而增加，但这是一个嘈杂的指标。因此，过滤掉邮件列表，只向目标少数群体发送广告的重要性就显现出来了。

## 方法学

为了推荐活动的最终电子邮件 id 列表，我获得了每个客户对公司销售的所有产品的购买历史记录，将其划分为特定的产品类别，并用阈值将数据二进制化，即创建一个客户与产品的矩阵，1 表示客户之前购买了超过一定金额的产品，否则为 0。这种方法背后的直觉是，有类似需求的客户很可能已经购买了类似的产品。基于这些数据构建了不同的模型，并为每个客户提供了 5、10、15 和 20 个建议。然后根据给定产品推荐给谁来编辑邮件列表。

## 推荐引擎

数据集以 80:20 的比例分为训练和测试；以下模型使用 80%的数据进行训练，并使用剩余的 20%进行测试。

1.  随机模型:一组随机建议，目的是测试其他机器学习算法的性能。
2.  基于项目的协同过滤(IBCF):根据购买过类似产品的人推荐产品
3.  基于用户的协同过滤(UBCF):根据客户的相似性推荐产品。即计算客户之间的“距离”,并根据类似客户购买的产品为给定客户提供推荐。
4.  关联规则使用市场篮分析来寻找最可能的产品组合。在此基础上，根据每位顾客已经购买的商品向其推荐产品。
5.  流行模型推荐是基于一个项目在数据集中的“流行”程度。购买产品的客户越多，向没有购买该产品的客户推荐该产品的可能性就越大。

## 混淆矩阵

如果客户购买了模型推荐的产品，则它被标记为真阳性。如果客户没有购买模型推荐的产品，那么它将被标记为误报，以此类推。生成了混淆矩阵，并计算了各种度量来评估模型。

![](img/91a406d2586070f59abe4507f9719a66.png)

**回忆** (TPR = TP / AP)以测量模型从整个列表中正确推荐的电子邮件 id 的比例，即模型正确识别的实际阳性类别的比例。

**精度** (PPV = TP / PP)检查推荐的精度，即。推荐邮件列表中正确的电子邮件 id 的比例，即在所有将收到该电子邮件的人中，有多少人会真正发现它有用。

**掉出** (FPR = FP / AN)来衡量模型错误推荐的邮件 id 比例。随着这一指标的增加，取消订阅的机会也将增加，因为电子邮件将被发送到大量不应接收该电子邮件的电子邮件 id。

**特异性** (TNR = TN / AN)衡量模型从邮件列表中正确忽略的电子邮件 id 的比例，从而降低退订风险。它还表明有多少“噪音”已经从推荐中去除，这将对如何解释电子邮件的点击率和打开率产生影响。

**阴性预测值** (NPV = TN / PN)用于衡量被正确排除的电子邮件 id 的比例，但这里的分母是预测的阴性类别(与特异性不同，在特异性中，分母是实际的阴性)。换句话说，这是模型归类为负面的电子邮件 id 的比例。

**遗漏率** (FNR = FN / AP)或模型遗漏推荐的邮件 id 比例。这些是应该包括在内的电子邮件 id，但模型错误地排除在最终的建议列表之外，即模型错过了多大的机会。

**错误发现** (FDR = FP / PP)或模型错误推荐的比例。这类似于 Fall Out -指标越大，退订的几率越高，因为电子邮件是发给那些可能对产品不感兴趣的人的。

**假遗漏率** (FOR = FN / PN)衡量所有被遗漏但本应包含在邮件列表中的人的比例。这和漏失率差不多；指标越大，模型错过的机会就越多。然而，这里的分母是预测的负类，不像漏失率，分母是实际的负类。

**准确率** (TP + TN / n)或者模型正确做出的推荐和遗漏的比例。这是最常用的指标，但是在某些情况下，优化上面提到的其他指标可能会更好。

**平衡准确度** (TPR + TNR / 2)或者召回率和特异性的平均值，或者正确包含和正确省略的平均比例。度量标准的选择通常取决于练习的目的。在大多数情况下，需要考虑不止一个指标，通常不超过 2 或 3 个。

## 模型评估

模型的性能如下所示。我在红绿标度上使用了条件格式来表示每个度量中哪些是性能更好的模型。(为了保护公司数据，所有数字都统一减少了，即实际数字都高于下面显示的数字)

![](img/7a8ef402b771144c63ad1b78da6d3a87.png)

以下是一些观察结果:

1.  很明显，所有模型的性能都优于随机推荐模型，并且在构建这些模型上花费的时间是合理的。
2.  虽然流行的模型有最多的绿色单元，但它可能是也可能不是最适合当前目标的。答案将取决于所提建议的质量。稍后将详细介绍。
3.  基于项目的协同过滤仅在落点和特异性测量中表现良好，因此其在准确性测量上的性能也有所提高。因此，除非必须发送电子邮件营销活动*，并且没有行动号召，否则这种模式可能不是最合适的。在这种情况下，该模型将确保退订率得到控制。如果每个产品的更多属性被考虑到模型中，基于项目的 CF 可能会有所改进。

模型的选择归结为基于用户的 CF、关联规则和流行。为了选择模型，我们查看了每个模型提出的建议，如下表所示。

![](img/4251eba8cb855ef93c7441fb68895a73.png)

1.  受欢迎的型号已经向过去没有购买过该产品的人推荐了该产品。这可能不是推荐产品的最合适的方法。
2.  Associative Rules 已经向之前购买并退回该产品的人推荐了该产品。这些客户被基于用户的 CF 和流行模型正确地忽略了。因此，除非活动的目标是向上销售产品，否则关联规则可以放在一边。

这给我们留下了基于用户的协同过滤，它给出了大约一万个客户的邮件列表。

## 活动评估

邮件将于本月晚些时候发出，并由测试和控制小组进行评估。将考虑的主要指标是电子邮件的打开和点击，以及模型推荐的客户与收到电子邮件但模型不推荐的客户相比的退订率。

如果该模式成立，那么这种方法也将用于其他数字活动，我们希望看到对网站参与度(包括网站访问量和 PDF 下载量)和销售额的积极影响。

如果您有任何意见或反馈，或其他评估推荐引擎的方法，请使用下面的评论部分。

![](img/bbc9bd75f416f7b28e7812f290b2e5d1.png)

约翰·戈迪内斯在 [Unsplash](https://unsplash.com/s/photos/customer?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片