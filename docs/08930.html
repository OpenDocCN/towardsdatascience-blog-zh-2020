<html>
<head>
<title>NGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NGBoost</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ngboost-aca51711c8f5?source=collection_archive---------23-----------------------#2020-06-27">https://towardsdatascience.com/ngboost-aca51711c8f5?source=collection_archive---------23-----------------------#2020-06-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b3c4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="http://towardsdatascience.com/tagged/the-gradient-boosters" rel="noopener" target="_blank">梯度助推器</a></h2><div class=""/><div class=""><h2 id="9753" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">梯度增强中的自然梯度</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/4380e0cff94b6fd0c21a8fb337caba30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSS-6OZLzIzchvvDK6v1gg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@paulgilmore_?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">保罗·吉尔摩</a>在<a class="ae lh" href="https://unsplash.com/s/photos/mountains?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="c189" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在表格数据领域，梯度助推器的统治几乎已经完成。在大多数真实世界以及竞赛中，几乎没有一个解决方案不具有来自梯度推进算法之一的至少一个模型。但是，随着机器学习社区的成熟，机器学习应用程序开始得到更多的使用，对不确定性输出的需求变得很重要。对于分类，梯度增强的输出形式已经可以让您了解模型预测的可信度。但是对于回归问题，情况并非如此。模型吐出一个数字，告诉我们这是它的预测。如何从点预测中获得不确定性估计？这个问题不仅仅是梯度推进算法的问题。但几乎适用于所有主要的 ML 算法。这是新成员 NGBoost 试图解决的问题。</p><blockquote class="me mf mg"><p id="af59" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">如果你还没有读过本系列的前几部分，我强烈建议你去读一读，至少是第一部分，在那里我谈到了<a class="ae lh" href="https://deep-and-shallow.com/2020/02/02/the-gradient-boosters-i-the-math-heavy-primer-to-gradient-boosting-algorithm/" rel="noopener ugc nofollow" target="_blank">梯度增强算法</a>，因为我认为你已经知道了什么是梯度增强。我也强烈建议阅读 VI(A) <em class="it">，这样你会对什么是</em><a class="ae lh" href="https://deep-and-shallow.com/2020/04/01/the-gradient-boosters-via-natural-gradient/" rel="noopener ugc nofollow" target="_blank"><em class="it"/></a><em class="it">有更好的理解。</em></p></blockquote><h1 id="6ce8" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">自然梯度增强</h1><p id="1f80" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">NGBoost 的关键创新是在 boosting 算法中使用自然渐变而不是常规渐变。通过采用这种概率途径，它在协变量的条件下，模拟了结果空间上的全概率分布。</p><p id="0cd6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该文件将他们的方法模块化为三个部分</p><ol class=""><li id="765a" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated">基础学习者</li><li id="c71c" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">参数分布</li><li id="705a" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">评分规则</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/a710beba5b16447c3b12acae0a77faa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Cpcttnm9guWYqmRR.png"/></div></div></figure><h1 id="ac2d" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">基础学习者</h1><p id="ccf0" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在任何增强技术中，都有一些基础学习者被组合在一起以得到一个完整的模型。NGBoost 不做任何假设，并声明基础学习者可以是任何简单的模型。该实现支持决策树和岭回归作为开箱即用的基础学习器。但是您可以轻松地用任何其他 sci-kit 学习风格模型来替换它们。</p><h1 id="bac3" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">参数分布</h1><p id="921f" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在这里，我们不是训练一个模型来预测作为点估计的结果，相反，我们预测的是一个完整的概率分布。每个分布都由几个参数来参数化。对于 eg，正态分布由其平均值和标准偏差来参数化。你不需要其他任何东西来定义一个正态分布。因此，如果我们训练模型来预测这些参数，而不是点估计，我们将有一个完整的概率分布作为预测。</p><h1 id="8a0f" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">评分规则</h1><p id="c483" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">任何机器学习系统都有一个学习目标，通常情况下，它的任务是最小化一些损失。在点预测中，用损失函数将预测与数据进行比较。评分规则类似于概率回归世界。评分规则将估计的概率分布与观察到的数据进行比较。</p><p id="3cde" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">适当的评分规则<em class="mh"> S </em>将预测的概率分布<em class="mh"> P </em>和一个观察值 y <em class="mh">(结果)</em>作为输入，并给预测分配分数<em class="mh"> S(P，y) </em>，使得结果的真实分布获得预期中的最佳分数。</p><p id="ea03" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最常用的适当评分规则是对数分数<em class="mh"> L </em>，当最小化时，我们得到 MLE</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/9d3259ec98ff3ed12f1961be77704a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/0*vjbtaGTTZHZebsgu"/></div></figure><p id="60df" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是我们在很多地方看到的对数可能性。评分规则由θ参数化，因为这是我们作为机器学习模型的一部分所预测的。</p><p id="9495" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一个例子是 CRPS(连续排名概率得分)。对数分数或对数似然将均方误差推广到概率空间，而 CRPS 对平均绝对误差做了同样的事情。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/898c401a8fc8452c72d0f84f4a810c03.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/0*Y6A0RGdZ3rIj1GV7"/></div></figure><h1 id="aaa1" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">广义自然梯度</h1><p id="89e0" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在这个系列的最后一部分，我们看到了什么是自然梯度。在那次讨论中，我们谈到了 KL 散度，因为传统上，自然梯度是根据 MLE 评分规则定义的。但本文提出了这一概念的推广，并提供了将这一概念推广到 CRPS 评分规则的方法。他们将 KL 散度推广为一般散度，并为 CRPS 评分规则提供了推导。</p><h1 id="44ee" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">把所有的放在一起</h1><p id="8aa1" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">现在我们已经看到了主要组件，让我们看看所有这些组件是如何协同工作的。NGBoost 是一种用于概率预测的监督学习方法，它使用 boosting 来估计条件概率分布<em class="mh"> P(y|x) </em>的参数。如前所述，我们需要提前选择三个模块化组件:</p><ol class=""><li id="ac6c" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated">基础学习者(<em class="mh"> f </em></li><li id="9b2d" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">参数概率分布(<em class="mh"> P 由θ </em>参数化)</li><li id="6da7" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">合适的评分规则(<em class="mh"> S </em></li></ol><p id="0973" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对新输入<em class="mh"> x </em>的预测<em class="mh"> y|x </em>以条件分布 p 的形式进行，其参数θ通过对<em class="mh"> M 个</em>基本学习器输出和初始θ₀.的相加组合来获得让我们用<em class="mh"> f </em> ⁽ᵐ⁾[2].来表示 m 个基本学习者学习到的所有参数的组合函数并且对于所选择的概率分布中的每个参数，将有一组单独的基础学习者。例如，在正态分布中，μ有<em class="mh"> f </em> ⁽ᵐ⁾，log σ有<em class="mh"> f </em> ⁽ᵐ⁾。预测的输出也用一个特定阶段的比例因子(ρᵐ)和一个通用的学习速率η进行缩放:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/5cf552900117502c7f3cbe074e3fc7a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/0*0LPkHBtA83ATuGeF"/></div></figure><p id="676c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里需要注意的一点是，即使你的概率分布有<em class="mh"> n </em>个参数，ρ仍然是一个标量。</p><h1 id="abb1" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">算法</h1><p id="025c" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">让我们看看论文[2]中解释的算法。</p><p id="4a10" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们考虑一个数据集</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/b28d50bfc45162176fdae14c1aed531e.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/0*s-ri616qnQDJnU8C"/></div></figure><p id="4456" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">助推迭代<em class="mh"> M </em>，学习率η，带参数θ的概率分布，合适的评分规则<em class="mh"> S </em>，基础学习器<em class="mh"> f </em></p><ol class=""><li id="672f" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated">初始化θ₀到边缘。这只是估计分布的参数，不考虑任何协变量；类似于初始化为均值。数学上，我们解这个方程:</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/3416afe95180f054141cad325511ee47.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/0*tU0gKP-Oz3beesFO"/></div></figure><ol class=""><li id="f068" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated">对于 M 中的每次迭代:</li><li id="5b51" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">针对数据集中的所有<em class="mh"> n </em>个示例，计算评分规则 s 相对于前一阶段θᵢᵐ⁻的参数的自然梯度 gᵢᵐ。</li><li id="ee9e" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">用于迭代<em class="mh"> f </em> ⁽ᵐ⁾的一组基本学习器适合于预测自然梯度 gᵢᵐ.的相应分量这个输出可以被认为是自然梯度在基础学习者类别的范围上的投影，因为我们正在训练基础学习者预测当前阶段的自然梯度。</li><li id="4551" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">该投影梯度然后通过比例因子ρᵐ.进行缩放这是因为自然梯度依赖于局部近似(正如我们在前面的帖子中看到的)，这些局部近似在远离当前参数位置的地方不会保持良好。<br/>在实践中，我们使用线搜索来获得最佳的比例因子，使整体评分规则最小化。在实现中，他们发现在线搜索中将缩放因子减半效果很好。</li><li id="30a2" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">一旦估计了缩放参数，在通过学习速率进一步缩放之后，我们通过将负缩放投影梯度添加到前一级的输出来更新参数。</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/6c009b5d6106cd3bd0dade83eab3c89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/0*urhq1unTnuBZU7mm"/></div></figure><h1 id="3fc6" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">履行</h1><p id="e3a0" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">该算法在<a class="ae lh" href="https://github.com/stanfordmlgroup/ngboost" rel="noopener ugc nofollow" target="_blank">https://github.com/stanfordmlgroup/ngboost</a>具有现成可用的 Sci-kit 学习风格实现。让我们来看看调整模型的关键参数。</p><h1 id="562a" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">超参数</h1><ul class=""><li id="a7cd" class="ni nj it lk b ll nd lo ne lr od lv oe lz of md og no np nq bi translated">Dist:该参数设置输出的分布。目前，该库支持用于回归的<em class="mh">正态、对数正态和指数</em>分布，用于分类的<em class="mh">k _ 分类和伯努利</em>。<em class="mh">默认:正常</em></li><li id="e759" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md og no np nq bi translated">评分:指定评分规则。目前，选项在<em class="mh"> LogScore 或 CRPScore </em>之间。<em class="mh">默认值:LogScore </em></li><li id="e4bb" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md og no np nq bi translated">基础:这指定了基础学习者。这可以是任何 Sci-kit 学习估计器。<em class="mh">默认为三层决策树</em></li><li id="ee53" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md og no np nq bi translated">n_estimators:提升迭代的次数。<em class="mh">默认值:500 </em></li><li id="4eb9" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md og no np nq bi translated"><em class="mh"> learning_rate </em>:学习率。<em class="mh">默认值:0.01 </em></li><li id="c001" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md og no np nq bi translated"><em class="mh"> minibatch_frac </em>:在每次提升迭代中使用的行的百分比子样本。这与其说是性能调优，不如说是性能黑客。当数据集很大时，这个参数可以大大加快速度。</li></ul><h1 id="31f0" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">解释</h1><p id="514e" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">尽管在使用来自机器学习模型的重要性之前需要相当多的谨慎，NGBoost 也提供了特性重要性。对于它估计的每个参数，它都有单独的一组重要性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/482415d5301b54c7e08bbd88f8c0acc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/0*7i8p9rqtihJ3hR17"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:NGBoost 文档[3]</p></figure><p id="e64a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但最精彩的部分不仅仅是这个，还有那辆<a class="ae lh" href="https://deep-and-shallow.com/2019/11/24/interpretability-cracking-open-the-black-box-part-iii/" rel="noopener ugc nofollow" target="_blank"> SHAP </a>，也是现成的型号。您只需要使用 TreeExplainer 来获取值。(要了解更多关于 SHAP 和其他可解释技术的信息，请查看我的另一个博客系列——<a class="ae lh" href="https://deep-and-shallow.com/2020/06/27/the-gradient-boosters-vib-ngboost/Cracking%20open%20the%20black%20box" rel="noopener ugc nofollow" target="_blank">可解释性:打开黑盒</a>)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/aac4ce2aa60c361feed7776abf4b3bb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/0*LogCP_qAOpWGDjiR"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:NGBoost 文档[3]</p></figure><h1 id="0e2f" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">实验</h1><p id="8339" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">本文还研究了该算法与其他流行算法相比的性能。有两种不同类型评估——概率评估和点估计</p><h1 id="0e25" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">概率回归</h1><p id="9b46" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在来自 UCI 机器学习知识库的各种数据集上，NGBoost 与其他主要的概率回归算法进行了比较，如<a class="ae lh" href="http://proceedings.mlr.press/v48/gal16.pdf" rel="noopener ugc nofollow" target="_blank">蒙特卡洛丢失</a>、<a class="ae lh" href="https://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf" rel="noopener ugc nofollow" target="_blank">深度集成</a>、<a class="ae lh" href="http://papers.neurips.cc/paper/6949-concrete-dropout.pdf" rel="noopener ugc nofollow" target="_blank">混凝土丢失</a>、高斯过程、<a class="ae lh" href="https://www.jstatsoft.org/article/view/v023i07" rel="noopener ugc nofollow" target="_blank">位置、规模和形状的广义加法模型(GAMLSS) </a>、<a class="ae lh" href="https://arxiv.org/pdf/1804.02921.pdf" rel="noopener ugc nofollow" target="_blank">分布式森林</a>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/8fb30529dadf3e815b3a23cd185cf8ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w_xL8KqMEpxqDLh3"/></div></div></figure><h1 id="dd85" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">点估计</h1><p id="c3ba" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">他们还针对其他回归算法(如弹性网络、随机森林(Sci-kit Learn)、梯度推进(Sci-kit Learn))评估了点估计用例上的算法。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/6de667a9c9c87f723a7ba6d09f050b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GLgB5mbdVG_7Eczt"/></div></div></figure><h1 id="60c1" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">结论</h1><p id="89d9" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">NGBoost 的性能与现有算法一样好，甚至更好，但它还有一个额外的优势，就是为我们提供了一个概率预测。并且公式和实现足够灵活和模块化，使其易于使用。</p><p id="1b73" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是这里的一个缺点是算法的性能。时间复杂度随着我们必须估计的每个额外参数而线性增加。所有效率方面的改进/改变都已经成为 LightGBM 或 XGBoost 等流行的梯度提升包的一部分，但在当前的实现中并不存在。也许它会很快被移植过来，因为我看到回购正在积极发展，并将其视为他们的目标行动项目之一。但是在这之前，这是相当慢的，尤其是对于大数据。一种解决方法是使用<em class="mh"> minibatch_frac </em>参数来加快自然梯度的计算。</p><p id="7df7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们已经看到了所有主要的梯度助推器，让我们挑选一个样本数据集，看看它们在本系列的下一部分如何表现。</p><p id="a8ae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="http://towardsdatascience.com/tagged/the-gradient-boosters" rel="noopener" target="_blank"> <em class="mh">中的其他文章</em> </a></p><ul class=""><li id="b8bc" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md og no np nq bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/the-good-old-gradient-boosting-f4614b0e62b0">老好的渐变增强</a></li><li id="0aeb" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md og no np nq bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/regularized-greedy-forest-a17cd0c85f06">正规化的贪婪森林</a></li><li id="e77e" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md og no np nq bi translated">XGBoost </li><li id="f046" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md og no np nq bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/lightgbm-800340f21415"> LightGBM </a></li><li id="4927" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md og no np nq bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/catboost-d1f1366aca34"> CatBoost </a></li><li id="cc6f" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md og no np nq bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/natural-gradient-ce454b3dcdfa">自然坡度</a></li><li id="67e8" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md og no np nq bi translated">NGBoost(你在这里)</li></ul><h1 id="4629" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">参考</h1><ol class=""><li id="7560" class="ni nj it lk b ll nd lo ne lr od lv oe lz of md nn no np nq bi translated">甘利顺一。自然梯度在学习中很有效。神经计算，第 10 卷，第 2 期，第 251–276 页。</li><li id="b95b" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">段，托尼。NGBoost: <a class="ae lh" href="https://arxiv.org/pdf/1910.03225.pdf" rel="noopener ugc nofollow" target="_blank">用于概率预测的自然梯度推进</a>，arXiv:1910.03225 v4【cs .2020 年 6 月 9 日</li><li id="29fa" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">NGBoost 文档，【https://stanfordmlgroup.github.io/ngboost T4】</li></ol></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="b020" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mh">原载于 2020 年 6 月 27 日 http://deep-and-shallow.com</em><em class="mh"/><a class="ae lh" href="https://deep-and-shallow.com/2020/06/27/the-gradient-boosters-vib-ngboost/" rel="noopener ugc nofollow" target="_blank"><em class="mh">。</em></a></p></div></div>    
</body>
</html>