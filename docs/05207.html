<html>
<head>
<title>Real-time eye tracking using OpenCV and Dlib</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用OpenCV和Dlib的实时眼睛跟踪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/real-time-eye-tracking-using-opencv-and-dlib-b504ca724ac6?source=collection_archive---------3-----------------------#2020-05-04">https://towardsdatascience.com/real-time-eye-tracking-using-opencv-and-dlib-b504ca724ac6?source=collection_archive---------3-----------------------#2020-05-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9a43" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本教程中，学习通过python中的网络摄像头创建一个实时凝视探测器。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/18a0fe82085bf2e5565a47a6856f98e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*TdAPsNWwLeLH1MenAKMG0Q.gif"/></div></figure><p id="db4b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">第一步是下载所需的包。通过pip安装:</p><pre class="kj kk kl km gt lm ln lo lp aw lq bi"><span id="60f4" class="lr ls it ln b gy lt lu l lv lw">pip install opencv-python<br/>pip install dlib</span></pre><p id="9a1d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">或者，如果您使用Anaconda，那么使用conda:</p><pre class="kj kk kl km gt lm ln lo lp aw lq bi"><span id="7136" class="lr ls it ln b gy lt lu l lv lw">conda install -c conda-forge opencv<br/>conda install -c menpo dlib</span></pre><p id="5927" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">除此之外，我们需要一个面部关键点检测器，可以实时检测眼睛。为此，我们将使用dlib库中预先训练好的网络，该网络可以检测出本<a class="ae lx" href="https://www.semanticscholar.org/paper/One-millisecond-face-alignment-with-an-ensemble-of-Kazemi-Sullivan/d78b6a5b0dcaa81b1faea5fb0000045a62513567" rel="noopener ugc nofollow" target="_blank">论文</a>中提出的“68个关键点”。所需的预训练模型可以从<a class="ae lx" href="https://github.com/davisking/dlib-models/blob/master/shape_predictor_68_face_landmarks.dat.bz2" rel="noopener ugc nofollow" target="_blank">这里</a>下载。使用Dlib是因为它可以实时给出预测，不像CNN模型，它对我来说非常重要，因为我正在为在线监督制作AI。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ly"><img src="../Images/ad4821fb32c35023937b17c493f7d5c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SgfJ7xl7QKZm037P.jpg"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">Dlib面部关键点。图片取自<a class="ae lx" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.pyimagesearch.com%2F2017%2F04%2F03%2Ffacial-landmarks-dlib-opencv-python%2F&amp;psig=AOvVaw3WbGbCrqFX4vkvRUP3m3TW&amp;ust=1588628143544000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCKiiwpTTmOkCFQAAAAAdAAAAABAT" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></figure></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="3dcf" class="lr ls it bd mo mp mq dn mr ms mt dp mu kz mv mw mx ld my mz na lh nb nc nd ne bi translated">使用Dlib的眼睛检测</h2><p id="f3b5" class="pw-post-body-paragraph kq kr it ks b kt nf ju kv kw ng jx ky kz nh lb lc ld ni lf lg lh nj lj lk ll im bi translated">首先要做的是找到眼睛，然后我们才能继续进行图像处理，并找到我们需要找到一张脸的眼睛。面部关键点检测器将dlib模块的一个<em class="nk">矩形对象作为输入，它只是一张脸的坐标。为了找到人脸，我们可以使用dlib内置的正面人脸检测器。您可以使用任何分类器来完成这项任务。如果你想要高精度和速度对你来说不是问题，那么我会建议你使用CNN，因为它会提供更好的精度，特别是对于非正面人脸和部分遮挡的人脸，如下面链接的文章所示。</em></p><div class="nl nm gp gr nn no"><a rel="noopener follow" target="_blank" href="/robust-facial-landmarks-for-occluded-angled-faces-925e465cbf2e"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">用于遮挡倾斜面部的鲁棒面部标志</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">Dlib提供了一个很好的面部标志检测器，但是当面部处于陡峭的角度时，它不能很好地工作。学习如何…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">towardsdatascience.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc ko no"/></div></div></a></div><pre class="kj kk kl km gt lm ln lo lp aw lq bi"><span id="b574" class="lr ls it ln b gy lt lu l lv lw">import cv2<br/>import dlib</span><span id="f8f3" class="lr ls it ln b gy od lu l lv lw">img = cv2.imread('image.png')<br/>gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # convert to grayscale detector = dlib.get_frontal_face_detector()<br/>rects = detector(gray, 1) # rects contains all the faces detected</span></pre><p id="5656" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这时我们有了矩形的人脸对象，我们可以把它传递给关键点检测器。</p><pre class="kj kk kl km gt lm ln lo lp aw lq bi"><span id="9c13" class="lr ls it ln b gy lt lu l lv lw">def shape_to_np(shape, dtype="int"):<br/>    coords = np.zeros((68, 2), dtype=dtype)<br/>    for i in range(0, 68):<br/>        coords[i] = (shape.part(i).x, shape.part(i).y)<br/>    return coords</span><span id="e839" class="lr ls it ln b gy od lu l lv lw">predictor = dlib.shape_predictor('shape_68.dat')<br/>for (i, rect) in enumerate(rects):<br/>    shape = predictor(gray, rect)<br/>    shape = shape_to_np(shape)<br/>    for (x, y) in shape:<br/>        cv2.circle(img, (x, y), 2, (0, 0, 255), -1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oe"><img src="../Images/983b2008088fa8bbb413c022c8d068e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*Qv8xQ-i_7dEWVNJWqyRIpA.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">获得输出。</p></figure><p id="cbcb" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">关于dlib的更详细的描述可以在这篇很棒的文章<a class="ae lx" href="https://www.pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/" rel="noopener ugc nofollow" target="_blank">中找到，这篇文章是我在做这个项目时参考的。</a></p><h2 id="3e08" class="lr ls it bd mo mp mq dn mr ms mt dp mu kz mv mw mx ld my mz na lh nb nc nd ne bi translated">利用OpenCV寻找眼球中心</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi of"><img src="../Images/5f6e47c06a10ea4c30a1c842ce278649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qEOt-aMqHm6g2xnQ"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">照片由<a class="ae lx" href="https://unsplash.com/@visualsbyroyalz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Anastase Maragos </a>在<a class="ae lx" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="f36b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们将通过网络摄像头获得现场直播。要打开网络摄像头，读取帧并显示它，您可以使用以下代码:</p><pre class="kj kk kl km gt lm ln lo lp aw lq bi"><span id="6ac1" class="lr ls it ln b gy lt lu l lv lw">import cv2</span><span id="e246" class="lr ls it ln b gy od lu l lv lw">cap = cv2.VideoCapture(0)<br/>while(True)<br/>    ret, img = cap.read()<br/>    cv2.imshow("Output", img)<br/>    if cv2.waitKey(1) &amp; 0xFF == ord('q'): # escape when q is pressed<br/>        break</span></pre><p id="666c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">所以，现在我们如何从网络摄像头读取帧，是时候对他们进行工作，以达到我们的目标。我们创建一个新的黑色遮罩，使用与我们的摄像头框架相同尺寸的NumPy。存储来自关键点阵列<em class="nk">形状</em>的左右眼点的(x，y)坐标，并使用<code class="fe og oh oi ln b">cv2.fillConvexPoly</code>将其绘制在蒙版上。它接受一个图像，点作为一个NumPy数组，数据类型= <code class="fe og oh oi ln b">np.int32</code>和颜色作为参数，并返回一个图像，这些点之间的区域用该颜色填充。</p><pre class="kj kk kl km gt lm ln lo lp aw lq bi"><span id="4262" class="lr ls it ln b gy lt lu l lv lw">def eye_on_mask(mask, side):<br/>    points = [shape[i] for i in side]<br/>    points = np.array(points, dtype=np.int32)<br/>    mask = cv2.fillConvexPoly(mask, points, 255)<br/>    return mask</span><span id="fb45" class="lr ls it ln b gy od lu l lv lw">left = [36, 37, 38, 39, 40, 41] # keypoint indices for left eye<br/>right = [42, 43, 44, 45, 46, 47] # keypoint indices for right eye<br/>mask = np.zeros(img.shape[:2], dtype=np.uint8)<br/>mask = eye_on_mask(mask, left)<br/>mask = eye_on_mask(mask, right)</span></pre><p id="bdac" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这样做之后，我们有了一个黑色的蒙版，眼睛区域是白色的。这个白色区域用一个形态学操作<code class="fe og oh oi ln b">cv2.dilate</code>扩大了一点。使用<code class="fe og oh oi ln b">cv2.bitwise_and</code>和我们的蒙版作为我们图像上的蒙版，我们可以分割出眼睛。将所有的(0，0，0)像素转换为(255，255，255)，这样只有眼球是唯一剩下的黑暗部分。将结果转换为灰度，使图像为阈值处理做好准备。</p><pre class="kj kk kl km gt lm ln lo lp aw lq bi"><span id="febf" class="lr ls it ln b gy lt lu l lv lw">kernel = np.ones((9, 9), np.uint8)<br/>mask = cv2.dilate(mask, kernel, 5)<br/>eyes = cv2.bitwise_and(img, img, mask=mask)<br/>mask = (eyes == [0, 0, 0]).all(axis=2)<br/>eyes[mask] = [255, 255, 255]<br/>eyes_gray = cv2.cvtColor(eyes, cv2.COLOR_BGR2GRAY)</span></pre><p id="c666" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">阈值处理用于创建二元掩模。因此，我们的任务是找到一个最佳阈值，根据这个阈值，我们可以从眼睛的其余部分中分割出眼球，然后我们需要找到它的中心。但是阈值对于不同的照明条件是不同的，所以我们可以制作一个可调节的跟踪条来控制阈值。平心而论，我从<a class="oj ok ep" href="https://medium.com/u/683e0406453b?source=post_page-----b504ca724ac6--------------------------------" rel="noopener" target="_blank"> Stepan Filonov </a>那里得到了这个想法，他也试图在这篇<a class="ae lx" href="https://medium.com/@stepanfilonov/tracking-your-eyes-with-python-3952e66194a6" rel="noopener">文章</a>中解决这个凝视检测的问题，并使用了Haar cascade和Blob检测。阈值处理步骤，即腐蚀，膨胀和中值模糊也是从他那里得到的，但他的最终结果并不令人信服，所以我做了这个解决方案。</p><pre class="kj kk kl km gt lm ln lo lp aw lq bi"><span id="c198" class="lr ls it ln b gy lt lu l lv lw">def nothing(x):<br/>    pass<br/>cv2.namedWindow('image')<br/>cv2.createTrackbar('threshold', 'image', 0, 255, nothing)<br/>threshold = cv2.getTrackbarPos('threshold', 'image')<br/>_, thresh = cv2.threshold(eyes_gray, threshold, 255, cv2.THRESH_BINARY)<br/>thresh = cv2.erode(thresh, None, iterations=2)<br/>thresh = cv2.dilate(thresh, None, iterations=4)<br/>thresh = cv2.medianBlur(thresh, 3)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/f54d78bfea7129482d8cd5506c3bafd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*cb_pRQ81A3NLvpRq_G2lTg.gif"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">用跟踪条反转后显示阈值</p></figure><p id="cc57" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们已经到了项目的最后一步。眼球被分割出来，我们可以利用<code class="fe og oh oi ln b">cv2.findContours</code>来找到它们。现在我们的背景是白色的，眼球是黑色的。但是在OpenCV的<code class="fe og oh oi ln b">cv2.findContours()</code>方法中，要查找的对象应该是白色的，背景是黑色的。所以我们需要使用<code class="fe og oh oi ln b">cv2.bitwise_not</code>来反转我们的<em class="nk">阈值</em>。现在我们可以找到轮廓。理论上，我们可以说，我们现在需要做的是找到两个最大的轮廓，这些应该是我们的眼球。然而，这为假阳性留下了一点空间，可以通过找到眼睛之间的中点并将图像除以该点来解决。然后我们在那些划分中找到最大的轮廓，应该是我们的眼球。关键点40和43(在Python中是39和42，因为index从零开始)用于寻找中点。用<code class="fe og oh oi ln b">cv2.contourArea</code>排序，找到中点两边最大的等高线。我们可以利用<code class="fe og oh oi ln b">cv2.moments</code>找到眼球的中心。</p><pre class="kj kk kl km gt lm ln lo lp aw lq bi"><span id="a923" class="lr ls it ln b gy lt lu l lv lw">def contouring(thresh, mid, img, right=False):<br/>    cnts, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)<br/>    cnt = max(cnts, key = cv2.contourArea) # finding contour with #maximum area<br/>    M = cv2.moments(cnt)<br/>    cx = int(M['m10']/M['m00'])<br/>    cy = int(M['m01']/M['m00'])<br/>    if right:<br/>        cx += mid # Adding value of mid to x coordinate of centre of #right eye to adjust for dividing into two parts<br/>    cv2.circle(img, (cx, cy), 4, (0, 0, 255), 2)# drawing over #eyeball with red</span><span id="3757" class="lr ls it ln b gy od lu l lv lw">mid = (shape[39][0] + shape[42][0]) // 2<br/>contouring(thresh[:, 0:mid], mid, img)<br/>contouring(thresh[:, mid:], mid, img, True)</span></pre><p id="2858" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在运行时，我们的代码会抛出几个类似于<code class="fe og oh oi ln b">max() arg is an empty sequence</code>或<code class="fe og oh oi ln b">division by zero</code>的错误，分别在没有找到轮廓或<code class="fe og oh oi ln b">M['m00']</code>为零时抛出。要解决这个问题，请将轮廓绘制函数包含在try块中。如果只在没有检测到眼睛时才会出现错误，我们不需要做任何事情。新的轮廓绘制函数将如下所示:</p><pre class="kj kk kl km gt lm ln lo lp aw lq bi"><span id="ebed" class="lr ls it ln b gy lt lu l lv lw">def contouring(thresh, mid, img, right=False):<br/>    cnts, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)<br/>    try:<br/>        cnt = max(cnts, key = cv2.contourArea)<br/>        M = cv2.moments(cnt)<br/>        cx = int(M['m10']/M['m00'])<br/>        cy = int(M['m01']/M['m00'])<br/>        if right:<br/>            cx += mid<br/>        cv2.circle(img, (cx, cy), 4, (0, 0, 255), 2)<br/>    except:<br/>        pass</span></pre><p id="9737" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">一切都结束了。只需显示img和thresh并相应地设置阈值跟踪栏，就可以享受了。完整代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="3f52" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">使用人工智能进行在线监督的完整代码可以在我的Github上找到。</p></div></div>    
</body>
</html>