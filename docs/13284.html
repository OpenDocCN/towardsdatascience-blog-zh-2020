<html>
<head>
<title>Decoding: State Of The Art Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解码:最先进的对象检测技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decoding-state-of-the-art-object-detection-99f79d97b75d?source=collection_archive---------12-----------------------#2020-09-12">https://towardsdatascience.com/decoding-state-of-the-art-object-detection-99f79d97b75d?source=collection_archive---------12-----------------------#2020-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7555" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这篇文章中，我正在解码当前艺术对象检测的主要构建模块，并看看它如何与一些商业云视觉 API 进行比较，如谷歌云视觉、微软计算机视觉 API。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dbfb3f5b9dd53812e17ad7fe9d0ad7ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eS2pxPdTOM22WF-TIZAkkQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://arxiv.org/pdf/1911.09070v7.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet 纸</a></p></figure><p id="227c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">E</span>efficient det 是一个神经网络架构，它实现了<strong class="lb iu">S</strong>tate-<strong class="lb iu">O</strong>f-T he-<strong class="lb iu">A</strong>rt(<strong class="lb iu">SOTA</strong>)结果(<strong class="lb iu"> ~55.1 平均精度</strong>)的对象检测(<a class="ae ky" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank">微软 COCO </a>数据集)任务，其复杂度远低于以前的检测器[]EfficientDet 的关键构件是</p><ol class=""><li id="d8ef" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated"><strong class="lb iu">复合缩放</strong></li><li id="4946" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><strong class="lb iu">双向特征金字塔网络</strong></li></ol><h1 id="86d8" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">复合缩放</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/102adc2a860f1f1206ed9ed7173abecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UG9PKySSQw_9rrXkzE2kcw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">常规缩放变量[宽度(b)、深度、分辨率(d)]、复合(e)。图片来自<a class="ae ky" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank">高效网论文</a></p></figure><p id="327c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> S </span>在宽度(b)、深度(c)或分辨率(d)上放大 ConvNets 被广泛用于实现更好的精度，但是在二维或三维上放大时需要繁琐的手动调整，并且仍然会导致次优的精度和效率。复合缩放(e)使用单个复合系数ϕ，根据以下等式以原则方式缩放网络宽度、深度和分辨率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/44898127059f17d07a984fff5aec4473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PS7M6VkgCYdrsOvJtfDe6Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">复合缩放公式，图片来自<a class="ae ky" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNet </a>论文。</p></figure><p id="05a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定基线网络和目标 flops，复合缩放被公式化为优化问题，如以上等式所示，找出用于缩放基础网络的缩放系数，并且分两步完成:</p><p id="ba03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nm">“1。固定ϕ = 1，假设多 2 倍的可用资源，根据上述等式对⍺、β、𝛾进行网格搜索。</em></p><p id="83e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nm"> 2。固定⍺、β、𝛾为常数，放大基线网络用不同的ϕ"</em><strong class="lb iu"><em class="nm"/></strong><a class="ae ky" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="nm">3</em></strong></a><strong class="lb iu"><em class="nm"/></strong></p><p id="6945" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">左上等式的参数ℱᵢ(输入层)、Lᵢ(层数)、Hᵢ(高度)、Wᵢ(宽度)、Cᵢ(通道数)是基线网络的预定义参数，如下面的有效网络详情表所示，w、d、r 是用于缩放网络宽度、深度和分辨率的系数。上面右边等式的参数ϕ控制模型缩放的资源，而⍺、β、𝛾分别控制深度、宽度或分辨率缩放。由于复合缩放不会改变基线网络中的图层运算符，因此拥有强大的基线网络也至关重要。"<em class="nm">基线网络是通过利用多目标神经架构搜索来选择的，该多目标神经架构搜索优化了精确度和 FLOPs "</em><a class="ae ky" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank"><em class="nm">3</em></a><em class="nm"/>。</p><h1 id="570c" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">双向特征金字塔网络</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/8315f0ed887def9c91749c63f65caee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FwIoVEB6RF4s3JG0MuJHMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">融合多尺度特征的特征网络设计，图片来自<a class="ae ky" href="https://arxiv.org/pdf/1911.09070v7.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet </a>论文</p></figure><p id="4fe2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多尺度特征融合聚合不同分辨率的特征和广泛使用的实践来学习尺度不变特征。特征金字塔网络[ <strong class="lb iu"> FPN </strong> ] (a)融合自上而下从第 3 级到第 7 级的特征并受限于一个方向的信息流，路径聚合网络[ <strong class="lb iu"> PANet </strong> ] (b)在 FPN 的顶部添加自下而上的路径，NAS-FPN (c)使用神经架构搜索来查找跨规模的特征网络拓扑，需要 1000 多个小时的搜索。还发现使用 NAS-FPN 的网络不规则且难以解释。[ <strong class="lb iu"> 2 </strong></p><p id="52db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BiFPN (d)将双向跨尺度连接与加权特征融合相结合。下面的等式和框图描述了级别 6 的融合特征。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/28b4c3c6a91ecf0a46575e2a87356cfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o1Gn1gr0nnRh8DCudi_7hg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://arxiv.org/pdf/1911.09070v7.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet </a>论文的等式</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/cc3a54b59470dc6d54d3da01a82f3f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_FSjmLTAqpqjJx-OpNyA4A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">六级 BiFPN 特征融合</p></figure><p id="7130" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">P7_in，P6_in (x，y，w)分别是 6/7 级的输入特征，P5_out 是 5 级的输出特征，P6_out (x，y，w)是 6 级的输出融合特征。使用最近邻插值对 P7_in 特征进行上采样，使用最大池对 P5_out 特征进行下采样，用于第 6 级的特征融合。加权添加层实现<strong class="lb iu">快速归一化融合</strong>，通过对加权添加层中的每个权重应用<strong class="lb iu"> Relu </strong>来确保正权重。</p><h1 id="3525" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">高效检测架构</h1><p id="8d6f" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">EfficientDet 架构使用 4 个主要网络:EfficientNet 主干网、BiFPN、边界框预测和类预测网络。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/08c67c17bac784a6b91ab01d53382556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bcr2pmLnho8ZI8oBLSJ7Yw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">完整的 EfficientDet 架构，图片来自<a class="ae ky" href="https://arxiv.org/pdf/1911.09070.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet </a>论文</p></figure><p id="6f7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">输入图像分辨率:</strong>由于 BiFPN 使用 3-7 级的特征，输入分辨率必须能被 pow(2，7) = 128 整除，并且输入图像根据以下等式进行缩放</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/150780c354f03b6b4dbe4cd73c29888f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pbcx_wLBRrc1pTvZkGqCgw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输入比例方程</p></figure><p id="9f97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> BiFPN 网络:</strong>由于深度需要四舍五入到最小整数，深度线性缩放。对于宽度(通道数)呈指数增长的 BiFPN，执行宽度网格搜索，并选择最佳值 1.35</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/b2693e5bbc8b61b9c183ee5b176da1d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tnjuOG9HH30v_XEDhfcCWg.png"/></div></div></figure><p id="7732" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">主干网络:</strong> EfficientDet 使用没有任何改变的 EfficientNet 网络，以重新使用预先训练的 imagenet 模型的权重。下表显示了高效网络 B0 和 B7 网络的详细信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/56177db5285590a662f8c719fb39e97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3vfjm4-Z9IEjneJxuLPVA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">高效的网络细节</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/6ad8ccacdca787e96d261d7395d643dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*QY8szaIwpEztWJp1O-BU1A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MBConv 框图</p></figure><p id="2ddc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EfficientNet 使用移动反向瓶颈 MBConv 作为构建模块，并依赖于深度方向可分离卷积和残差连接的思想，从而实现更快的训练和更好的准确性。如上图所示，这些 MBConv 模块根据主干网络中的复合缩放系数ϕ ( <strong class="lb iu"> num_repeat </strong>)进行缩放。</p><p id="97d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">盒/类预测网络:</strong>盒和类预测网络的宽度根据 BiFPN 网络的宽度进行缩放，深度根据下面的等式进行线性缩放。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/1d841a79ae32983ac3a76959859bee64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pz9KH9Vq-BPGBh8IEiZd2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://arxiv.org/pdf/1911.09070v7.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet </a>论文的等式</p></figure><p id="b9fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下表显示了根据复合系数ϕ.的 EfficientDet 检测器的比例配置</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/541c45022dfdc66e3305f3342155b8ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Mm69o1M_ds02HJQzjgJ3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/pdf/1911.09070v7.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet </a>网络的缩放配置，图片来自<a class="ae ky" href="https://arxiv.org/pdf/1911.09070v7.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet </a>纸张</p></figure><h1 id="42d0" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">使用预先训练的 COCO 模型进行推理:</h1><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="ac93" class="og mt it oc b gy oh oi l oj ok"><strong class="oc iu">#Make sure python3 pip3 is installed and updated</strong></span><span id="f9d3" class="og mt it oc b gy ol oi l oj ok">sudo apt-get update<br/>sudo apt install python3-pip <br/>sudo -H pip3 install -U pip (to upgrade the pip to latest version)</span><span id="1dfb" class="og mt it oc b gy ol oi l oj ok"><strong class="oc iu">#Clone EfficientDet git repo<br/></strong>git clone <a class="ae ky" href="https://github.com/google/automl.git" rel="noopener ugc nofollow" target="_blank">https://github.com/google/automl.git</a></span><span id="cbd2" class="og mt it oc b gy ol oi l oj ok">cd ~/automl/efficientdet</span><span id="a958" class="og mt it oc b gy ol oi l oj ok"><strong class="oc iu">#Install all the EfficientDet requirements</strong><br/>pip3 install -r requirements.txt</span><span id="da28" class="og mt it oc b gy ol oi l oj ok"><strong class="oc iu">#Download the pretrained weights. Bold d0 represent the model version and can be in the range d0-d7.<br/></strong>wget <a class="ae ky" href="https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d7.tar.gz" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-<strong class="oc iu">d0</strong>.tar.gz</a></span><span id="d1fd" class="og mt it oc b gy ol oi l oj ok">tar zxf efficientdet-d0.tar.gz</span><span id="27ba" class="og mt it oc b gy ol oi l oj ok">mkdir -p ./savedmodels/efficient-d0</span><span id="b9f7" class="og mt it oc b gy ol oi l oj ok"><strong class="oc iu"># Export saved model.<br/></strong>python3.6 model_inspect.py --runmode=<strong class="oc iu">saved_model</strong> --model_name=efficientdet-d0 --ckpt_path=./efficientdet-d0 --hparams="image_size=1920x1280" --saved_model_dir=./savedmodels/efficientdet-d0</span><span id="d532" class="og mt it oc b gy ol oi l oj ok"><strong class="oc iu">#Make output dir and do inferencing with the saved model<br/></strong>mkdir -p outdir<br/>python3.6 model_inspect.py --runmode=<strong class="oc iu">saved_model_infer</strong> --model_name=efficientdet-d0 --saved_model_dir=./savedmodels/efficientdet-d0 --input_image=<strong class="oc iu">path_to_input_image</strong> --output_image_dir=./output --min_score_thresh=0.6</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/a86feaf3c2eb7b60e2efb2462db50061.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1T2j6wL4z6Ge-Iln5-z2mg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输入图像(左)，EfficientDet-D0 输出(右)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/78080de2c2cd5e358a6ef2c253a0adaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dinxlNLTjUZt35AE2mEB1Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">谷歌视觉 API 的输出，<a class="ae ky" href="https://cloud.google.com/vision" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/vision</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/19537f0b1c89756bd36302138c047e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bLjSyWleEARfvSWF-MuBgQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Azure Vision API 的输出，<a class="ae ky" href="https://azure.microsoft.com/en-in/services/cognitive-services/computer-vision/#features" rel="noopener ugc nofollow" target="_blank">https://Azure . Microsoft . com/en-in/services/cognitive-services/computer-Vision/# features</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/17a1008c99486f6f3566455739128f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVVq4l9GoLRMPmb6TDo_mQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输入图像(左)，EfficientDet-D0 输出(右)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/299e4f693c938eb0acf5e900915d3240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R5NOf3JUTM-hIRJIrbhUEw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">谷歌视觉 API 的输出，<a class="ae ky" href="https://cloud.google.com/vision" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/vision</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/7fdbbc7375194969901208606308f5c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XraZsGXRGTi9W1dQWDkt_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Azure Vision API 的输出，<a class="ae ky" href="https://azure.microsoft.com/en-in/services/cognitive-services/computer-vision/#features" rel="noopener ugc nofollow" target="_blank">https://Azure . Microsoft . com/en-in/services/cognitive-services/computer-Vision/# features</a></p></figure><p id="099d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> E </span>就云视觉 API 而言，在测试图像上，fficientDet 输出看起来更好<strong class="lb iu"/>。<strong class="lb iu">需要进行详细的对比分析，以确定生产云视觉 API 的效率和质量。</strong>采用新型 BiFPN 和复合缩放的 EfficientDet 无疑将成为未来目标检测相关研究的新基础，并将使目标检测模型在实际应用中更加有用。</p><p id="c945" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您阅读这篇文章，我希望这对您有所帮助。如果你有，请在你最喜欢的社交媒体上分享，这样其他人也可以找到它。此外，如果有不清楚或不正确的地方，请在评论区告诉我们。</p><h1 id="4755" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">参考资料:</h1><ol class=""><li id="6746" class="me mf it lb b lc nq lf nr li os lm ot lq ou lu mj mk ml mm bi translated"><a class="ae ky" href="https://ai.googleblog.com/2020/04/efficientdet-towards-scalable-and.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2020/04/efficient det-forward-scalable-and . html</a></li><li id="a16b" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1911.09070v7.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1911.09070v7.pdf</a></li><li id="30b3" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><a class="ae ky" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1905.11946</a></li><li id="693e" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1801.04381.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1801.04381.pdf</a></li><li id="b455" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><a class="ae ky" href="https://github.com/google/automl/tree/master/efficientdet" rel="noopener ugc nofollow" target="_blank">https://github.com/google/automl/tree/master/efficientdet</a></li></ol></div></div>    
</body>
</html>