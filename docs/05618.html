<html>
<head>
<title>How to Rank Text Content by Semantic Similarity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何根据语义相似度对文本内容进行排序</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-rank-text-content-by-semantic-similarity-4d2419a84c32?source=collection_archive---------1-----------------------#2020-05-11">https://towardsdatascience.com/how-to-rank-text-content-by-semantic-similarity-4d2419a84c32?source=collection_archive---------1-----------------------#2020-05-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="433e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 NLP、TF-idf 和 GloVe 查找 Python 中的相关内容并进行排名</h2></div><p id="f2a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">搜索是查找内容的标准工具——无论是在线还是设备上的——但是如果你想更深入地根据单词的<em class="le">含义</em>来查找内容呢？为自然语言处理开发的工具会有所帮助。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/a46c6143506243d12defeaa60511047f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xfXoVO5IfvGaoqT6aqu_qQ.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">帕特里克·托马索在<a class="ae lv" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="f7b1" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">内容</h2><p id="5c36" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">在本文中，我们将介绍两种计算文本相似度的方法:</p><ol class=""><li id="5752" class="mu mv it kk b kl km ko kp kr mw kv mx kz my ld mz na nb nc bi translated"><strong class="kk iu">术语频率-逆文档频率(TF-idf) </strong> : <strong class="kk iu"> </strong>这查看在两个文本中出现的单词，并根据它们出现的频率对它们进行评分。如果你希望两个文本中出现相同的单词，这是一个有用的工具，但是有些单词比其他的更重要。</li><li id="af1d" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld mz na nb nc bi translated"><strong class="kk iu">语义相似度</strong>:根据单词的相似程度来评分，即使它们并不完全匹配。它借用了自然语言处理(NLP)的技术，比如单词嵌入。如果文本之间的单词重叠是有限的，这是很有用的，例如，如果您需要将'<em class="le">水果和蔬菜</em>'与'<em class="le">西红柿</em>'联系起来。</li></ol><p id="2e0a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于第一部分，我们将单独使用<code class="fe ni nj nk nl b">scikit-learn</code>中的 TF-idf 实现，因为它非常简单，只需要几行代码。对于语义相似性，我们将使用来自<code class="fe ni nj nk nl b">gensim</code>的一些函数(包括它的 TF-idf 实现)和来自<a class="ae lv" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套算法</a>的预训练单词向量。此外，我们还需要一些来自<code class="fe ni nj nk nl b">nltk</code>的工具。这些软件包可以使用 pip 安装:</p><pre class="lg lh li lj gt nm nl nn no aw np bi"><span id="63bf" class="lw lx it nl b gy nq nr l ns nt">pip install scikit-learn~=0.22<br/>pip install gensim~=3.8<br/>pip install nltk~=3.4</span></pre><p id="62f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者，您可以从 GitHub 存储库中获取示例代码，并从需求文件中安装:</p><pre class="lg lh li lj gt nm nl nn no aw np bi"><span id="b80b" class="lw lx it nl b gy nq nr l ns nt">git clone <a class="ae lv" href="https://github.com/4OH4/doc-similarity" rel="noopener ugc nofollow" target="_blank">https://github.com/4OH4/doc-similarity</a></span><span id="f92c" class="lw lx it nl b gy nu nr l ns nt">cd doc-similarity</span><span id="2219" class="lw lx it nl b gy nu nr l ns nt">pip install -r requirements.txt</span></pre><div class="nv nw gp gr nx ny"><a href="https://github.com/4OH4/doc-similarity" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">4oh 4/doc-相似性</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">使用 Python - 4OH4/doc-similarity 中的语义相似度对文档进行排序</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">github.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om lp ny"/></div></div></a></div><h2 id="e58a" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated"><strong class="ak">快速入门</strong></h2><ol class=""><li id="315c" class="mu mv it kk b kl mp ko mq kr on kv oo kz op ld mz na nb nc bi translated">运行<code class="fe ni nj nk nl b">examples.ipynb,</code>中的演示代码</li><li id="d5e7" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld mz na nb nc bi translated">使用<code class="fe ni nj nk nl b">tfidf.rank_documents(search_terms: str, documents: list)</code>功能根据重叠内容对文档进行评分，</li><li id="fb03" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld mz na nb nc bi translated">使用<code class="fe ni nj nk nl b">docsim.DocSim()</code>类对使用<code class="fe ni nj nk nl b">doc2vec</code>和<code class="fe ni nj nk nl b">GloVe</code>单词嵌入模型的文档进行相似性评分。</li></ol></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><h2 id="3f2a" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">术语</h2><p id="d9b3" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated"><strong class="kk iu"> <em class="le">文档</em> </strong>:字符串形式的一段文本。这可能只是几句话，也可能是一整部小说。</p><p id="bd3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">文集</em> </strong>:文件的集合。</p><p id="1f9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">术语</em> </strong>:文档中的一个词。</p></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><h1 id="341f" class="ox lx it bd ly oy oz pa mb pb pc pd me jz pe ka mh kc pf kd mk kf pg kg mn ph bi translated">1.TF-idf</h1><p id="62af" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated"><em class="le">术语频率-逆文档频率</em>(或 TF-idf)是一种基于文档共享单词的重要性对文档相似性进行评分的既定技术。非常高层次的总结:</p><ul class=""><li id="5e6d" class="mu mv it kk b kl km ko kp kr mw kv mx kz my ld pi na nb nc bi translated">如果某个术语(单词)在文档中频繁出现，则该术语在该文档中可能很重要。</li><li id="7b68" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld pi na nb nc bi translated">但是，如果一个术语在许多文档中频繁出现，则该术语通常可能不太重要。</li></ul><p id="3ecb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TF-idf 分数根据一个单词出现的频率来估计这两种启发式方法之间的权衡。这里有更详细的<a class="ae lv" href="http://www.tfidf.com/" rel="noopener ugc nofollow" target="_blank">总结</a>。</p><p id="e038" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以用几行代码从<code class="fe ni nj nk nl b">scikit-learn</code>中创建并装配一个<a class="ae lv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> TF-idf 矢量器模型</a>:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pj pk l"/></div></figure><p id="032e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们创建模型并使用文本语料库进行“拟合”。<code class="fe ni nj nk nl b">TfidfVectorizer</code>使用其默认的记号赋予器处理预处理——这将字符串转换成单个单词“记号”的列表。它产生包含频率项的文档向量的稀疏矩阵。</p><p id="2534" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们用文档的第一个向量(包含搜索项)的点积(线性核)来确定相似性。我们必须忽略第一个相似性结果(<code class="fe ni nj nk nl b">[1:]</code>)，因为这是比较搜索词本身。</p><p id="0951" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这对于一个简单的例子来说很有效，但是在现实世界中可能会因为一些原因而失败。</p><p id="b5ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，在匹配过程中使用诸如['and '，' on '，' the '，' are']这样的词没有多大意义，因为这些<em class="le">停用词</em>不包含上下文信息。在确定相似性之前，这些单词应该被剔除。</p><p id="45e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其次，我们希望“水果”和“水果”被认为是相关的，尽管上面的模型只能找到精确的匹配。解决这个问题的一个方法是将每个单词简化为最简单的<a class="ae lv" href="https://dictionary.cambridge.org/dictionary/english/lemma" rel="noopener ugc nofollow" target="_blank"> <em class="le">引理</em> </a> — <em class="le"> </em>为此我们需要一个<em class="le">引理器</em>。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pj pk l"/></div></figure><p id="c9bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，第二个文档中的“tomatos”被 lemmatizer (tokenizer)简化为“tomato ”,然后与搜索词中的相同单词进行匹配。</p><p id="a54c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从概念上讲，每个文档都是高维空间中的一个点，其中维度的数量等于文本语料库中唯一词的数量。这个空间通常是非常空的，因为文档包含各种各样的单词，所以点(文档)之间的距离很大。在这种情况下，点之间的角度作为相似性的度量比距离相关的度量(如欧几里德距离)更有意义。</p><p id="ff8a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里的相似度被称为<em class="le">余弦相似度</em>。<code class="fe ni nj nk nl b">TfidfVectorizer</code>的输出(默认情况下)是 L2 归一化的，因此两个向量的点积是向量所表示的点之间角度的余弦。</p><h2 id="f3ea" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">摘要:TF-idf</h2><ul class=""><li id="74b0" class="mu mv it kk b kl mp ko mq kr on kv oo kz op ld pi na nb nc bi translated">当文档很大和/或有很多重叠时，它速度很快，效果很好。</li><li id="da7a" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld pi na nb nc bi translated">它寻找精确的匹配，所以至少你应该使用一个 lemmatizer 来处理复数。</li><li id="d683" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld pi na nb nc bi translated">当比较短的文档和有限的术语种类时——比如搜索查询——有一个风险，就是在没有精确的单词匹配时，你会错过语义关系。</li></ul><h1 id="46b5" class="ox lx it bd ly oy pl pa mb pb pm pd me jz pn ka mh kc po kd mk kf pp kg mn ph bi translated">2.语义相似度</h1><p id="1041" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">一种更高级的方法是根据单词的相似程度来比较文档。例如，“苹果”和“橙子”可能比“苹果”和“木星”更相似。大规模判断单词相似性是困难的——一种广泛使用的方法是分析大量文本，将经常一起出现的单词排序为更相似。</p><p id="51c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是单词嵌入模型<a class="ae lv" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>的基础:它将单词映射到数字向量——多维空间中的点，以便经常一起出现的单词在空间上彼此靠近。这是一种无监督学习算法，由斯坦福大学于 2014 年开发。</p><p id="9954" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在前面的例子中，我们使用了来自<code class="fe ni nj nk nl b">nltk</code>包的<code class="fe ni nj nk nl b">WordNetLemmatizer</code>对我们的数据进行词干化和标记化(转换成单个单词串的<code class="fe ni nj nk nl b">list</code>)。这里我们在<code class="fe ni nj nk nl b">gensim</code>中做了类似的预处理，也删除了任何可能存在的 HTML 标签，比如我们从网上抓取了数据:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pj pk l"/></div></figure><p id="b0c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们创建一个相似性矩阵，其中包含每对单词之间的相似性，使用术语频率进行加权:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pj pk l"/></div></figure><p id="ae4e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们计算查询和每个文档之间的软余弦相似度。与常规余弦相似度不同(对于没有重叠项的向量，它将返回零)，<em class="le">软余弦相似度</em>也考虑单词相似度。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pj pk l"/></div></figure><p id="3656" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代码库中的笔记本中有完整的示例。还有一个自包含的<code class="fe ni nj nk nl b">DocSim</code>类，可以作为模块导入，用于运行语义相似性查询，而无需额外的代码:</p><pre class="lg lh li lj gt nm nl nn no aw np bi"><span id="6ddc" class="lw lx it nl b gy nq nr l ns nt">from docsim import DocSim</span><span id="9040" class="lw lx it nl b gy nu nr l ns nt">docsim = DocSim(verbose=True)</span><span id="81f7" class="lw lx it nl b gy nu nr l ns nt">similarities = docsim.similarity_query(query_string, documents)</span></pre><p id="fb3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GloVe word 嵌入模型可能非常大——在我的机器上，从磁盘加载大约需要 40 秒。然而，一旦它被加载，随后的操作会更快。该类的多线程版本在后台加载模型，以避免长时间锁定主线程。它以类似的方式使用，尽管如果模型仍在加载时会引发一个异常，所以应该首先检查<code class="fe ni nj nk nl b">model_ready</code>属性的状态。</p><pre class="lg lh li lj gt nm nl nn no aw np bi"><span id="e55d" class="lw lx it nl b gy nq nr l ns nt">from docsim import DocSim_threaded</span><span id="b7c3" class="lw lx it nl b gy nu nr l ns nt">docsim = DocSim_threaded(verbose=True)</span><span id="7412" class="lw lx it nl b gy nu nr l ns nt">similarities = docsim.similarity_query(query_string, documents)</span></pre><h2 id="5926" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">概述:使用 GloVe 的语义相似性</h2><ul class=""><li id="3fa1" class="mu mv it kk b kl mp ko mq kr on kv oo kz op ld pi na nb nc bi translated">它更加灵活，因为它不依赖于寻找精确的匹配。</li><li id="ccd2" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld pi na nb nc bi translated">涉及的计算要多得多，所以可能会慢一些，而且单词嵌入模型可能会相当大，需要一段时间来准备第一次使用。这很容易扩展，但是运行单个查询很慢。</li><li id="857a" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld pi na nb nc bi translated">大多数单词与其他单词有某种程度的相似性，因此几乎所有文档都会与其他文档有某种非零的相似性。<strong class="kk iu">语义相似性有利于按顺序排列内容，而不是对文档是否与特定主题相关做出具体判断。</strong></li></ul></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><p id="5391" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经研究了两种比较文本内容相似性的方法，例如可能用于搜索查询或内容推荐系统的方法。第一个(TF-idf)基于共有词的出现频率对文档关系进行评分。当文档很大和/或有很多重叠的术语时，这种方法速度很快，效果很好。第二种技术寻找表达相似概念的共有词，但不要求完全匹配:例如，它将“水果和蔬菜”与“番茄”联系起来。这种方法速度较慢，有时会产生不太清晰的结果，但对于较短的搜索查询或单词重叠度较低的文档很有用。</p><p id="290a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在现实世界的应用程序中检查了这两种方法:我需要知道某个<code class="fe ni nj nk nl b">&lt;collection of documents&gt;</code>是否与一小部分<code class="fe ni nj nk nl b">&lt;search terms&gt;</code>相关。最初，我认为我需要使用语义相似性匹配——搜索词来自不受控制的用户输入，因此可能存在一些非常微妙的关系。文档排序是一个有用的功能，但总体目标是应用一个决策阈值，以便生成一个二进制的是/否结果。</p><p id="4600" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于那个特定的应用，我发现单独使用 TF-idf 就足够了。文档集合通常足够大，以至于在确实应该找到匹配的情况下，与搜索词有足够的重叠。以色列国防军工作队对正面和负面案例进行了更明确的区分，尽管它确实遗漏了一些也是相关的文件。</p><blockquote class="pq"><p id="1ab4" class="pr ps it bd pt pu pv pw px py pz ld dk translated">值得采取数据驱动的方法，看看什么最适合你的应用程序。</p></blockquote><p id="3f18" class="pw-post-body-paragraph ki kj it kk b kl qa ju kn ko qb jx kq kr qc kt ku kv qd kx ky kz qe lb lc ld im bi translated">你有什么想法？你在工作中使用过这些工具吗，或者你喜欢不同的方法？在下面留下你的评论吧！</p><blockquote class="qf qg qh"><p id="cf3c" class="ki kj le kk b kl km ju kn ko kp jx kq qi ks kt ku qj kw kx ky qk la lb lc ld im bi translated"><a class="ae lv" href="https://twitter.com/rupertthomas" rel="noopener ugc nofollow" target="_blank"> <em class="it">鲁珀特·托马斯</em> </a> <em class="it">是一名技术顾问，专门研究机器学习、机器视觉和数据驱动产品。</em><a class="ae lv" href="https://twitter.com/rupertthomas" rel="noopener ugc nofollow" target="_blank"><em class="it">@ Rupert Thomas</em>T9】</a></p></blockquote></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><h1 id="f001" class="ox lx it bd ly oy oz pa mb pb pc pd me jz pe ka mh kc pf kd mk kf pg kg mn ph bi translated">参考</h1><p id="fbd4" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated"><a class="ae lv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> Sci-kit 学习:tfidf 矢量器</a></p><p id="e562" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lv" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套:单词表示的全局向量</a></p><p id="55a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lv" href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb" rel="noopener ugc nofollow" target="_blank"> Gensim:软余弦教程</a></p></div></div>    
</body>
</html>