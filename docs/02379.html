<html>
<head>
<title>Machine Learning: Word Embedding and Predicting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:单词嵌入和预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-word-embedding-and-predicting-e603254e4d7b?source=collection_archive---------20-----------------------#2020-03-06">https://towardsdatascience.com/machine-learning-word-embedding-and-predicting-e603254e4d7b?source=collection_archive---------20-----------------------#2020-03-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8f8f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过开发简单的单词预测器来理解单词嵌入</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5d771fa6b8a8d6e91946ace6e2c51d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*viypLwSkda7wg166q0PLMA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由来自<a class="ae kv" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2574751" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae kv" href="https://pixabay.com/users/quinntheislander-1139623/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2574751" rel="noopener ugc nofollow" target="_blank"> Quinn Kampschroer </a>提供</p></figure><p id="5496" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">单词嵌入是一种将稀疏表示向量转换成密集的更小向量的流行技术。这大大增加了计算时间，并节省了资源。在本文中，让我们看看如何开发一个预测引擎，并在工作流中利用单词嵌入的知识。</p><h1 id="f115" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">一点背景</h1><h2 id="5639" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">错别字更正</h2><p id="8716" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">这是将单个单词调整为最匹配的正确单词的地方。在大多数情况下，这种方法试图解决拼写问题。比如你输入<strong class="ky ir"><em class="nb">【helo】</em></strong>，很有可能你的手机会变成<strong class="ky ir"> <em class="nb">【你好】</em> </strong>。</p><h2 id="97cd" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">单词预测</h2><p id="0249" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">这是一种新的方法，可以纠正你的语法。单词被替换以使句子更自然。例如，如果你说<strong class="ky ir"> <em class="nb">“你很漂亮”</em> </strong> <em class="nb"> </em>我们可以有一个合理的逻辑，这个词可能会变成<strong class="ky ir"> <em class="nb">【人】</em> </strong>。事实上，这应当完成为<strong class="ky ir"> <em class="nb">【你是一个美丽的人】</em> </strong>。在本文中，我们将关注这种使用上下文信息的自动更正。这比修复错别字要复杂一些。我希望你们都是手机这个功能的受害者/使用者。让我们自己造一个吧！</p><h1 id="8811" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">查找训练数据集</h1><p id="4959" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">有一次，我偶然发现了这个网址，它指引我找到了名为《变形记》的电子书。这是一个 ASCII 编码(UTF 8)的文本文档，可以免费下载。从今以后，我将引用这个源作为我的训练数据集。重要的事情先来！让我们快速地进行预处理和标记化，这样我们就可以开始了。我将使用以下 python 库进行预处理(以防您还没有它们)。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="6a75" class="mk lt iq nd b gy nh ni l nj nk">numpy<br/>tensorflow<br/>keras (comes as the API for tensorflow backend)</span></pre><h1 id="898f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">初步了解</h1><p id="6628" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">我们要执行的第一步是对训练集中的单词进行标记化。这是因为我们应该将整个语料库中的所有单词放在一个有限的空间中，以便进行下游处理。通常，神经网络在一个值范围内表现更好。因此，我们应该将所有这些数据嵌入到密集的向量中。我们的单词符号更像是对空间的压缩。让我们看看它是如何工作的。</p><h2 id="1169" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">嵌入技术</h2><ol class=""><li id="8de8" class="nl nm iq ky b kz mw lc mx lf nn lj no ln np lr nq nr ns nt bi translated"><strong class="ky ir"> TF-IDF 矢量化</strong></li></ol><p id="5f0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是通过考虑单词在文档中的频率及其在语料库中的出现频率来嵌入单词的非常常见的方法。向量的大小将等于所考虑的唯一单词的数量。通常使用稀疏矩阵来实现。让我们看看下面的示例代码。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="e305" class="mk lt iq nd b gy nh ni l nj nk">from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer<br/>import pandas as pd</span><span id="b5c8" class="mk lt iq nd b gy nu ni l nj nk">texts = ["Data Science is a popular field.", <br/>         "Data Science is useful in predicting financial performance of economies", <br/>         "Performance is important in data science"]</span><span id="f450" class="mk lt iq nd b gy nu ni l nj nk">vectorizer = CountVectorizer()<br/>counts = vectorizer.fit_transform(texts)<br/>tfidf = TfidfVectorizer()<br/>features = tfidf.fit_transform(texts)</span><span id="cf97" class="mk lt iq nd b gy nu ni l nj nk">pd.DataFrame(counts.todense(),columns=vectorizer.get_feature_names())<br/>pd.DataFrame(features.todense(),columns=tfidf.get_feature_names())</span></pre><p id="69a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将在终端中打印以下输出(在这里阅读更多关于理论<a class="ae kv" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"/>)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/d0f90434558dde2b16b6eed34acd6880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*lpAK-Z8FNgBo3N279gYgtg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每个文档中的字数</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/16398c4c056fd9ca4508fd26ecd0e589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NwdBTsAULbgiUK2kO4miiA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">TF-IDF 单词向量</p></figure><p id="7600" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过这个向量的外观，你会看到它们是高维的，因此需要大量的计算资源。</p><h2 id="b401" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">单词嵌入</h2><p id="a272" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">这就是我们使用嵌入层的地方，它将稀疏表示压缩到一个目标维度中，形成一个更小的表示。让我们看一个例子。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="6d25" class="mk lt iq nd b gy nh ni l nj nk">from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Embedding</span><span id="837c" class="mk lt iq nd b gy nu ni l nj nk">docs = ["Data Science is a popular field.", <br/>         "Data Science is useful in predicting financial performance of economies", <br/>         "Performance is important in data science"]</span><span id="7336" class="mk lt iq nd b gy nu ni l nj nk">tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(docs)<br/>vocab_size = len(t.word_index) + 1</span><span id="fc2a" class="mk lt iq nd b gy nu ni l nj nk">model = Sequential()<br/>model.add(Embedding(vocab_size, 3, input_length=1))<br/>model.compile('rmsprop', 'mse')</span><span id="b530" class="mk lt iq nd b gy nu ni l nj nk">input_array = tokenizer.texts_to_sequences(["Data"])<br/>output_array = model.predict(input_array)</span><span id="4b63" class="mk lt iq nd b gy nu ni l nj nk">output_array</span></pre><p id="aed0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来是输出；</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="cd8e" class="mk lt iq nd b gy nh ni l nj nk">array([[[0.02056189, 0.04136733, 0.03566055]]], dtype=float32)</span></pre><p id="2d4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以看到我们看到一个形状为<strong class="ky ir"> <em class="nb"> N，1，3 </em> </strong>的张量，这是因为我们的目标维度是 3，我们有一个单词。多个单词会增加张量的第二维度。这些将是你学习模型的超参数。现在，你将拥有包含三维单词的句子，与之前的词汇大小形成对比。</p><h1 id="eb36" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">实施我们的模型</h1><p id="c3cb" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">既然我们已经掌握了初步步骤的专业知识，让我们来构建模型。</p><h2 id="f8a9" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">基本进口</h2><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="160a" class="mk lt iq nd b gy nh ni l nj nk">from numpy as np<br/>import tensorflow as tf<br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.utils import to_categorical</span></pre><h2 id="e486" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">加载训练数据</h2><p id="6d19" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">我们将使用前面提到的书<strong class="ky ir"> <em class="nb">【蜕变】</em> </strong>作为语料库来加载数据和分词。我们将考虑 3 个连续的单词。模型的输入是前两个单词，输出将是每组 3 个单词的最后一个单词。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="5e7b" class="mk lt iq nd b gy nh ni l nj nk">data = ""<br/>with open("./metamorphosis.txt") as f:<br/>    data = f.read().strip()</span><span id="cfa5" class="mk lt iq nd b gy nu ni l nj nk">tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts([data])</span><span id="29bd" class="mk lt iq nd b gy nu ni l nj nk"># determine the vocabulary size<br/>vocab_size = len(tokenizer.word_index) + 1<br/>print('Vocabulary Size: %d' % vocab_size)</span><span id="9f37" class="mk lt iq nd b gy nu ni l nj nk"># create line-based sequences<br/>sequences = list()</span><span id="6b15" class="mk lt iq nd b gy nu ni l nj nk">for encoded in tokenizer.texts_to_sequences(data.split("\n")):<br/>    if len(encoded) &gt; 0:<br/>        for i in range(0, len(encoded) - 2):<br/>            sequences.append(encoded[i:i+3])<br/>    <br/>print('Total Sequences: %d' % len(sequences))<br/>sequences = np.array(sequences)</span><span id="86d3" class="mk lt iq nd b gy nu ni l nj nk">X, y = sequences[:,:-1], to_categorical(sequences[:,-1], num_classes=vocab_size)</span></pre><h2 id="e1f7" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">构建模型</h2><p id="fe7c" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">让我们为我们的培训建立一个简单的模型。在本文中，我将开发一个非常简单的模型，它将考虑前面的两个单词来预测或纠正下一个单词。我的输出将以分类二进制格式来预测下一个单词。我将使用我喜欢使用的<strong class="ky ir"> <em class="nb"> Keras 功能 API </em> </strong>。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="2454" class="mk lt iq nd b gy nh ni l nj nk"># define model<br/>i = tf.keras.layers.Input(shape=(X.shape[1]))</span><span id="8a88" class="mk lt iq nd b gy nu ni l nj nk">e = tf.keras.layers.Embedding(vocab_size, 10, input_length=max_length)(i)</span><span id="7622" class="mk lt iq nd b gy nu ni l nj nk">l = tf.keras.layers.LSTM(10)(e)</span><span id="ce24" class="mk lt iq nd b gy nu ni l nj nk">d = tf.keras.layers.Dense(vocab_size, activation='softmax')(l)</span><span id="2aa0" class="mk lt iq nd b gy nu ni l nj nk">model = tf.keras.Model(inputs=i, outputs=[d])</span><span id="0f56" class="mk lt iq nd b gy nu ni l nj nk">print(model.summary())</span><span id="920d" class="mk lt iq nd b gy nu ni l nj nk"># compile network<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/><br/>tf.keras.utils.plot_model(model)</span></pre><p id="19d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，我们使用<strong class="ky ir"> <em class="nb">分类交叉熵</em> </strong>，因为我们正在训练预测二进制分类值。一位将指示特定索引中的一个项目。</p><p id="4846" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们运行代码时，我们将能够看到下面的模型。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="194d" class="mk lt iq nd b gy nh ni l nj nk">Model: "model"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_1 (InputLayer)         [(None, 2)]               0         <br/>_________________________________________________________________<br/>embedding_4 (Embedding)      (None, 2, 10)             2700      <br/>_________________________________________________________________<br/>lstm (LSTM)                  (None, 10)                840       <br/>_________________________________________________________________<br/>dense (Dense)                (None, 270)               2970      <br/>=================================================================<br/>Total params: 6,510<br/>Trainable params: 6,510<br/>Non-trainable params: 0</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/4996723c14032f9173fee828babfd1f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*R3N405k2gncAMIhRQw6XXQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">可视化网络</p></figure><p id="7d14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用下面几行代码启动培训程序。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="0424" class="mk lt iq nd b gy nh ni l nj nk">model.fit(X, y, epochs=1000, verbose=2)</span></pre><h1 id="d2bc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">预言</h1><p id="fd82" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">现在我们已经训练了模型，我们可以开始预测下一个单词并进行纠正。让我们看看如何根据训练好的模型进行预测。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="75ea" class="mk lt iq nd b gy nh ni l nj nk">text = " there's still <strong class="nd ir">so</strong>"<br/>text = " ".join(text.split(" ")[:3])</span><span id="4244" class="mk lt iq nd b gy nu ni l nj nk">encoded = tokenizer.texts_to_sequences([text])[0]<br/>encoded = array([encoded])</span><span id="e882" class="mk lt iq nd b gy nu ni l nj nk">next = model.predict(encoded, verbose=0)</span><span id="09f7" class="mk lt iq nd b gy nu ni l nj nk">for x in next:<br/>    next_word_token = np.argmax(x)</span><span id="03e5" class="mk lt iq nd b gy nu ni l nj nk">    # map predicted word index to word<br/>    for word, index in tokenizer.word_index.items():<br/>        if index == next_word_token:<br/>            print(word + " ")</span></pre><p id="1e38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里用粗体表示，我把<strong class="ky ir"> <em class="nb">写成了</em> </strong>，这是打了一半的最后一个单词(用于模拟用户输入)。请注意，我认为前两个词是预测。我们的预测者会预测单词<strong class="ky ir"> <em class="nb">【某】</em> </strong>。所以完整的句子会是<strong class="ky ir"> <em class="nb">【还有一些】</em> </strong>。我们使用<strong class="ky ir"> <em class="nb"> argmax </em> </strong>来挑选预测的类别，并在 tokenizer 字典中查找单词。</p><p id="c97f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nb">让工作流程更智能</em> </strong></p><p id="14eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们天真地选择了最有可能的项目。然而，如果我们有了<strong class="ky ir"> <em class="nb">【还有一些 fo】</em></strong>，下一个单词很容易就是<strong class="ky ir"><em class="nb">【food】</em></strong>。对此，最简单的解决方法是挑选最可能的项目集，而不选择<strong class="ky ir"> <em class="nb"> argmax </em> </strong>。然后查找单词并指定最接近的匹配。为此，您可以使用余弦距离。</p><p id="0313" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nb">提示</em> </strong></p><p id="1e1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了清晰起见，尝试绘制您的<strong class="ky ir"> <em class="nb"> Keras </em> </strong>模型。<br/>找到一个好的语料库进行训练(我选择的是天真的)<br/>在分类编码不适合的地方，使用二进制编码和<strong class="ky ir"> <em class="nb">二进制交叉熵</em> </strong>。</p><p id="439b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望你喜欢阅读这篇文章！你好。</p></div></div>    
</body>
</html>