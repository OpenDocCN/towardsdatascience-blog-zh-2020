<html>
<head>
<title>Finding the right model complexity using Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用正则化找到正确的模型复杂度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/finding-the-right-model-complexity-using-regularization-af13606cb393?source=collection_archive---------45-----------------------#2020-05-14">https://towardsdatascience.com/finding-the-right-model-complexity-using-regularization-af13606cb393?source=collection_archive---------45-----------------------#2020-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0adb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解偏差-方差的权衡，以及如何使用收缩正则化技术实现它们之间的平衡。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/abea03b7834ebd87f7fb6f81493c81be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i2XO8n1Gezm5tkmPzgXuPA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://pixabay.com/photos/scale-kitchen-shelves-measure-1209837/" rel="noopener ugc nofollow" target="_blank">Pixabay.com</a></p></figure><p id="f8e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在正态线性回归模型中，我们试图通过将所有预测因子拟合到一个线性方程来预测一个响应变量，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/c8ca00f0c8c20868b140a6120af08b84.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*AZ8I56_iNss3J8Yn4Xlx1g.png"/></div></figure><p id="2a81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的等式中，我们尝试使用两个预测因子 X1 和 X2 来预测 Y，其中β1 和β2 是有助于估计预测因子对响应变量 Y 的影响的系数。这里ε是不可约误差，不在我们的控制范围内。</p><p id="ec7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们用单个预测器/特征对一组训练数据点拟合上述方程时，我们得到一个模型，该模型可以如下所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/306ed241c89cbca23c880c88cf078b69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dsVgatUGODjJN7dsdnuKtg.png"/></div></div></figure><p id="8454" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，上面的等式试图在数据点上拟合一条线，该线近似空间中的每个数据点。就训练数据而言，上述线并不理想，因为该线并不完全适合每个点。在被拟合的点和线之间有一个距离，我们称之为<strong class="lb iu">残差</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/2678c9437ef84b3726fe4ccd79df3e06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*toe7gyNkTLwMDRpvIFlUjA.png"/></div></div></figure><p id="78d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上图中，残差显示为实际数据点和拟合的线性方程线上的点之间的垂直距离。因此，我们可以用下面的等式来表示残差</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/6a538ab4415281a2f33d96cdbde17875.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*IYmO8faMtVN8vKKZ2cm8Hg.png"/></div></figure><p id="633a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的等式中，我们取实际数据点和直线上拟合点之间的差值。每当我们试图拟合一个线性模型时，我们都试图使所有点的残差之和最小化。这个总和被称为<strong class="lb iu">残差平方和</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/a0d837856ad87d583c5fe99d5f3fd6cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*meQQT7lonZ4ZmaRlcgBngQ.png"/></div></figure><p id="08c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们计算距离的平方，这样正距离值和负距离值的处理方式是一样的。下图显示了一个最小化所有数据点残差的模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/6eb0fe75d46bfb56f76f5e6617bee272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8tM19JqrxCDEEb80GwroTw.png"/></div></div></figure><p id="5ef2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述模型适合曲线上的所有点。上述模型的 RSS 等于 0，并且该模型具有<strong class="lb iu">高复杂度。</strong>具有高复杂性的模型试图捕捉数据点中的每一个变化。这样的模型据说有<strong class="lb iu">高方差。</strong></p><p id="eb50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练数据上具有高方差的模型往往在测试数据上表现不佳，因为当模型试图捕捉训练数据中的每个变化时，它还考虑离群值、随机数据点和高杠杆点，这些点很少并且可能不存在于测试数据中。在这种情况下，我们说模型是<strong class="lb iu">过拟合</strong>列车数据<strong class="lb iu">。</strong></p><p id="b8d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，为了提高模型在测试数据上的性能，我们降低了模型的复杂性并减少了它在训练数据集上的方差。当我们减少模型的方差时，我们在模型中引入了误差。该误差被称为<strong class="lb iu">偏差</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/25613509760f23e28723b4e2f396a36d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4l_CpMKDgpCFqJ2pYJzByA.png"/></div></div></figure><p id="bae7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面一行可以看出，模型的复杂度降低了。上面的模型没有试图以增加偏差为代价来捕捉每一个数据点。这样一个复杂度稍低的模型在测试数据上会比我们上面看到的模型表现得更好。</p><p id="7b21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，减少方差会增加我们模型中的偏差，反之亦然。这被称为<strong class="lb iu">偏差-方差权衡。</strong>偏差方差权衡可在下图中总结。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ma"><img src="../Images/2068b619dcb15509a31c385b0fb93b6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wh12AdHdmtV1dDki3lP5kA.png"/></div></div></figure><p id="47d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面我们可以看到，随着模型复杂度的增加，模型的方差增加，方差减小。随着复杂度的增加，测试误差也开始减小，但是直到某个点，之后它开始增加。我们必须在拟合模型时找到这个最佳点。</p><p id="23bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们必须在拟合模型时找到合适的方差，并且必须确保方差不会太高。</p><h1 id="5adb" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">减少方差的正则化技术</h1><p id="152a" class="pw-post-body-paragraph kz la it lb b lc mt ju le lf mu jx lh li mv lk ll lm mw lo lp lq mx ls lt lu im bi translated">正则化技术本质上用于减少模型中的方差并避免过度拟合的问题。</p><p id="dc3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">减少模型中方差的一种方法是将系数估计值向零收缩。我们估计系数βi，用于估计预测值/特征 Xi 对响应变量变量 y 的影响。因此，将βi 的值向零收缩将低估该特征对响应变量的影响，并将使模型不那么复杂。我们有两种使用这种思想的正则化技术:</p><ol class=""><li id="e214" class="my mz it lb b lc ld lf lg li na lm nb lq nc lu nd ne nf ng bi translated">山脊正则化(L2)</li><li id="6c65" class="my mz it lb b lc nh lf ni li nj lm nk lq nl lu nd ne nf ng bi translated">套索正规化(L1)</li></ol><h2 id="43cf" class="nm mc it bd md nn no dn mh np nq dp ml li nr ns mn lm nt nu mp lq nv nw mr nx bi translated"><strong class="ak">山脊正规化</strong></h2><p id="7d50" class="pw-post-body-paragraph kz la it lb b lc mt ju le lf mu jx lh li mv lk ll lm mw lo lp lq mx ls lt lu im bi translated">由于我们希望降低系数估计βi 的值，从而降低特征对响应变量的影响，因此我们需要一个模型来惩罚系数估计的高值<strong class="lb iu">。</strong></p><p id="8ff1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在回归模型中，残差平方和(RSS)被最小化。这种方法在岭回归中得到了扩展，除了最小化 RSS 之外，还最小化了系数估计的平方。</p><p id="074b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在岭回归中最小化以下内容</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8da180d9eeda7006783acfd240d7c819.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*rOoPsTltCKL0gKCGDCZuaQ.png"/></div></figure><p id="6303" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这不过是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/5599f1a386a68161383bbb3ccb048b8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*23x08OQNk5LQmLL1hjJoYw.png"/></div></figure><p id="7b3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在岭回归中，我们不仅最小化 RSS，而且最小化所有特征的系数估计的平方。这导致系数估计值向零收缩。系数值向零收缩的程度由调谐参数λ控制。如果λ的值很大，模型将更多地惩罚系数估计的大值，并且将更多地降低模型复杂性。因此，调整参数用于控制模型复杂性。</p><p id="784a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我们只是缩小了特征的系数值，也就是说，我们减小了β1、β2、…，βp 其中 p 是特征的数量。我们没有缩小截距β0 的值，它是所有特征都为 0 时响应变量的平均值(X1=X2=…。Xp=0)</p><h2 id="ea40" class="nm mc it bd md nn no dn mh np nq dp ml li nr ns mn lm nt nu mp lq nv nw mr nx bi translated"><strong class="ak">套索正规化</strong></h2><p id="8821" class="pw-post-body-paragraph kz la it lb b lc mt ju le lf mu jx lh li mv lk ll lm mw lo lp lq mx ls lt lu im bi translated">脊正则化成功地将系数的值缩小到零，但从未将它们减小到等于零。这似乎是具有大量预测值的模型的一个问题。使用岭正则化和大量预测因子的模型是不可解释的。例如，如果我们有<strong class="lb iu"> p </strong>个预测值，其中只有 3 个预测值是有用的，那么岭正则化将创建一个包含所有<strong class="lb iu"> p </strong>个预测值的模型。</p><p id="7233" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">套索正则化是一个轻微的修改岭，克服了这个问题。在套索正规化，我们试图尽量减少以下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/b9111c340aa2854d46c4f99305a9597d.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*P8P0_ZE3OgRu9k69nCij7w.png"/></div></figure><p id="d168" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">唯一的区别是，不是最小化系数估计的平方，而是最小化模数。在 lasso 正则化中，如果调谐参数λ的值足够大，一些系数估计将恰好等于零。因此，lasso 也有助于只找到那些对模型有用的预测因子。</p><h2 id="9357" class="nm mc it bd md nn no dn mh np nq dp ml li nr ns mn lm nt nu mp lq nv nw mr nx bi translated"><strong class="ak">选择调谐参数</strong> λ</h2><p id="ff9b" class="pw-post-body-paragraph kz la it lb b lc mt ju le lf mu jx lh li mv lk ll lm mw lo lp lq mx ls lt lu im bi translated">脊和套索正则化都利用调整参数来控制模型的复杂性。为了确定λ的最佳值，我们使用了交叉验证。我们制作λ值的网格，并对每个值的数据进行交叉验证，并检查哪个值的误差最小。然后，我们用从交叉验证中获得的λ值来拟合模型。</p><h2 id="4969" class="nm mc it bd md nn no dn mh np nq dp ml li nr ns mn lm nt nu mp lq nv nw mr nx bi translated"><strong class="ak">分类问题的正则化</strong></h2><p id="7c22" class="pw-post-body-paragraph kz la it lb b lc mt ju le lf mu jx lh li mv lk ll lm mw lo lp lq mx ls lt lu im bi translated">我们现在知道，在回归模型中，我们最小化 RSS，并且当我们在这些模型中应用正则化技术时，我们最小化 RSS 之外的系数估计。</p><p id="27f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，由于 RSS 在回归中被最小化，我们有许多用于分类模型的损失函数。例如，一个这样的损失函数是交叉熵</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/aab1b4be7894ade571bd07248a1a5644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*aSlR0IAEs5q-MlZosGuIoA.png"/></div></figure><p id="a2b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> y </strong>为实际值，<strong class="lb iu">y′</strong>为模型预测值。除了损失函数之外，其余的思想保持不变:将系数估计值向零收缩，并降低模型的复杂性。代替 RSS 的是分类的损失函数。</p><h2 id="17ac" class="nm mc it bd md nn no dn mh np nq dp ml li nr ns mn lm nt nu mp lq nv nw mr nx bi translated"><strong class="ak">使用哪种技术？套索还是山脊？</strong></h2><p id="1626" class="pw-post-body-paragraph kz la it lb b lc mt ju le lf mu jx lh li mv lk ll lm mw lo lp lq mx ls lt lu im bi translated">至于用哪种技术的问题，全靠数据。如果响应变量只能由一些预测值来衡量，lasso 将优于 ridge，因为它使系数估计值等于零。如果响应变量需要许多预测值，ridge 将优于 lasso。我们可以通过使用交叉验证来检查哪种技术更合适。</p><p id="7dd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">抛开模型的准确性不谈，lasso 总是具有固有特征选择的优势，因为它使系数估计等于零，因此人们可以解释哪些预测因子对于模型是不必要的。</p></div></div>    
</body>
</html>