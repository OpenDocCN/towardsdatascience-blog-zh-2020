<html>
<head>
<title>Bayesian Thinking for Linear Regression @ Kaggle Days Meetup</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的贝叶斯思维</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06?source=collection_archive---------38-----------------------#2020-05-24">https://towardsdatascience.com/bayesian-thinking-estimating-posterior-distribution-for-linear-regression-data-ketchup-2f50a597eb06?source=collection_archive---------38-----------------------#2020-05-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4b1b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">贝叶斯与吉布斯采样和MCMC模拟</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/09b019eff1f9c7a639414e0f4e591faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_JiZZjoa_MSe09ISRaQhw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">深度神经模型中的不确定性，</strong> <a class="ae kw" href="https://www.nature.com/articles/s41598-017-17876-z/figures/5" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kv">链接</strong> </a></p></figure><h2 id="f606" class="kx ky iq bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">简介:</h2><p id="9c48" class="pw-post-body-paragraph lt lu iq lv b lw lx jr ly lz ma ju mb lg mc md me lk mf mg mh lo mi mj mk ml ij bi translated">这项研究的主要动机之一是，随着越来越复杂的模型的出现，人们越来越关注深度模型的可解释性。更多的是模型的复杂性，很难对输出进行解释，在贝叶斯思维和学习领域正在进行大量的研究。</p><p id="eafb" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">但是在理解并能够欣赏深度神经模型中的贝叶斯之前，我们应该精通并熟练线性模型中的贝叶斯思维，例如贝叶斯线性回归。但是很少有很好的在线材料能够以一种组合的方式给出清晰的动机和对贝叶斯线性回归的理解。</p><p id="2cd1" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">这是我写这篇博客的主要动机之一，在这里，我将尝试从贝叶斯分析的角度来理解如何进行线性回归。</p><h2 id="f643" class="kx ky iq bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated"><strong class="ak">最大似然估计&amp;贝叶斯</strong></h2><p id="28ca" class="pw-post-body-paragraph lt lu iq lv b lw lx jr ly lz ma ju mb lg mc md me lk mf mg mh lo mi mj mk ml ij bi translated">在开始之前，让我们明白，我们非常清楚线性回归模型的最大似然估计。我在之前的<a class="ae kw" href="https://www.youtube.com/watch?v=6rWvmwucgZM&amp;t=1850s" rel="noopener ugc nofollow" target="_blank">演讲</a>中讨论过这个问题，请参考链接。</p><p id="1360" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">简单介绍一下线性回归中最大似然估计的概念。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/8401d7dfbb75ee86671d1c1d49657eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T9kndoqrTzk9Eev70-l1fA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">线性回归最大似然法—二元图，</strong> <a class="ae kw" href="https://medium.com/quick-code/maximum-likelihood-estimation-for-regression-65f9c99f815d" rel="noopener"> <strong class="bd kv">链接</strong> </a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/de7ea0ab1f5df38e373d92bf2269c7d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vZhu9ObA2R2kICk2lwtj6A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">可能为回归，</strong> <a class="ae kw" href="https://www.youtube.com/watch?v=ulZW99jsMXY&amp;t=655s" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kv">链接</strong> </a></p></figure><blockquote class="mu mv mw"><p id="5a6c" class="lt lu mr lv b lw mm jr ly lz mn ju mb mx mo md me my mp mg mh mz mq mj mk ml ij bi translated">由此要明白的最重要的一点是<strong class="lv ir"> <em class="iq"> MLE </em> </strong>通过最大化<strong class="lv ir"> <em class="iq">似然P(D|θ) </em> </strong>给你一个参数的点估计。</p><p id="2536" class="lt lu mr lv b lw mm jr ly lz mn ju mb mx mo md me my mp mg mh mz mq mj mk ml ij bi translated">偶，<strong class="lv ir"> <em class="iq">映射</em> </strong>即<a class="ae kw" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">最大后验概率</a>估计最大后验概率<strong class="lv ir"> <em class="iq"> P(θ|D)，</em> </strong>即也给出点估计。</p></blockquote><p id="0f07" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">因此，这些方法不能给你足够的关于系数分布的信息，而在贝叶斯方法中，我们估计参数的后验分布。因此，在这种情况下，输出不是单个值，而是概率密度/质量函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/523f52ca45660f240f5f8f958de2892c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rm5N186ftTD_8B_e5We7CA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">后验概率，贝叶斯定理</strong></p></figure><p id="601d" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">只是为了理解背后的直觉。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/a6bf793ae7f770b8548ef2c454857a5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDi46CLhrhdJq24AKVSQRg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">先验，&amp;后验，</strong><a class="ae kw" href="https://www.researchgate.net/figure/Likelihood-prior-and-posterior-probability-distribution-for-a-parameter-x_fig9_258158633" rel="noopener ugc nofollow" target="_blank">T5】链接 </a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/8aa355b27cedd1ad48e781b31eceeaf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hyfmEgr8Q_ECY9uUMZnrRw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">理解后验意义，</strong> <a class="ae kw" href="https://www.youtube.com/watch?v=yvWlpwnT1nw" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kv">链接</strong> </a></p></figure><h2 id="dc4e" class="kx ky iq bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">马尔可夫链蒙特卡罗模拟</h2><p id="9a49" class="pw-post-body-paragraph lt lu iq lv b lw lx jr ly lz ma ju mb lg mc md me lk mf mg mh lo mi mj mk ml ij bi translated">只差一步了！！！</p><p id="4105" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">在深入研究贝叶斯回归之前，我们需要了解另一件事，即<em class="mr">马尔可夫链蒙特卡罗模拟</em>以及为什么需要它？</p><p id="259a" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">MCMC方法用于通过概率空间中的随机采样来近似感兴趣参数的后验分布。但是为什么近似分布而不计算精确分布可能是一个你一定很感兴趣的问题。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/7bfd64b4e2696cf673f264cafd2cc1d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NTgt_BNJwjagoebWRR-VEw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">计算后验分布的复杂度，</strong> <a class="ae kw" href="https://www.youtube.com/watch?v=8FbqSVFzmoY&amp;t=390s" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kv">链接</strong> </a></p></figure><p id="d056" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">由于分母的原因，这几乎无法计算，也很难处理，因此我们使用MCMC来近似后验分布。</p><p id="4020" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">蒙特卡罗方法有助于我们生成符合给定分布的随机变量。例如:- <strong class="lv ir"> θ ~ N(平均值，sigma**2)，</strong>将有助于从正态分布生成多个<em class="mr"> θ </em>，有许多方法可以做到这一点。</p><p id="eed8" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">马尔可夫链是一个数字序列，其中每个数字都依赖于序列中的前一个数字。马尔可夫链蒙特卡罗有助于从分布中生成随机变量，其中每一个<em class="mr"> θ </em>的值都是从一个平均值等于前一个值的已知分布中提取的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/cc5ffcff415130f7cf8208da5505af29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*068gyZT7I5f-PNN-N_vxXA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马尔可夫链蒙特卡罗与高斯建议分布，<a class="ae kw" href="https://www.youtube.com/watch?v=OTO1DygELpY&amp;t=198s" rel="noopener ugc nofollow" target="_blank">链接</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/5f0033d15a7ac6ef1a36fba2ef5c3b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6T7oQ3HFBBjeBAOnZy-0Cw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">通过MCMC模拟生成密度，</strong> <a class="ae kw" href="https://www.youtube.com/watch?v=OTO1DygELpY&amp;t=198s" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kv">链接</strong> </a></p></figure><p id="c65a" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">正如您所理解的，这基本上是一个随机行走，可能会生成许多不必要的和不相关的值。因此，我们需要对值进行接受/拒绝采样，以生成感兴趣的分布。</p><p id="aa29" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">在这种情况下，Metropolis-Hastings算法用于通过拒绝和接受采样来改进值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/31cf07e31f3b00c9b1540b12868b2f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhPJ9VwP8DiMJFTEZ69tSg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">接受和拒绝样本的Metropolis-Hastings算法</strong></p></figure><p id="7a85" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">我们也可以使用<strong class="lv ir">吉布的</strong>采样，其目标是通过获得后验条件分布<strong class="lv ir"> <em class="mr"> P(θ1|θ2，y，x) </em> </strong>和<strong class="lv ir"> <em class="mr"> P(θ2|θ1，y，x)来找到后验分布P(θ1，θ2|y，x)。</em> </strong>所以，我们生成</p><p id="83e0" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated"><strong class="lv ir"> <em class="mr"> θ1 ~P(θ1|θ2，y，x)</em></strong>替换第二个方程中生成的θ1的值，生成<strong class="lv ir"> <em class="mr"> θ2 ~ P(θ2|θ1，y，x) </em> </strong>我们继续这个过程，进行几次迭代，得到后验。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/0ff34f8711f8015d03d2e9106a115222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OMm7s4aTgZT-uMnhi5zrrA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">估计后验MCMC，</strong> <a class="ae kw" href="https://www.youtube.com/watch?v=7QX-yVboLhk" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kv">链接</strong> </a></p></figure><p id="42e2" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">现在，让我们用一个例子来说明这一点。</p><p id="c0ee" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">在下面的例子中，我将首先用吉布斯抽样来说明贝叶斯线性回归方法。这里，我假设了参数的某些分布。</p><h2 id="476f" class="kx ky iq bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated"><strong class="ak"> <em class="ni">用吉布斯采样实现贝叶斯线性回归</em> </strong>:</h2><p id="6fc3" class="pw-post-body-paragraph lt lu iq lv b lw lx jr ly lz ma ju mb lg mc md me lk mf mg mh lo mi mj mk ml ij bi translated">在这一节中，我将展示一个使用吉布斯抽样进行贝叶斯线性回归的例子。这一部分摘自Kieran R Campbell关于贝叶斯线性回归的博客<a class="ae kw" href="https://kieranrcampbell.github.io/blog/2016/05/15/gibbs-sampling-bayesian-linear-regression.html" rel="noopener ugc nofollow" target="_blank"> <em class="mr">。</em></a></p><p id="7896" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">因此，首先，让我们了解数据对于我们的情况是如何的。设D为以下实验的数据集，D定义为:</p><p id="2ad3" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated"><strong class="lv ir"> <em class="mr"> D = ((x1，y1)，(x2，y2)，(x3，y3) …… (xn，yn)) </em> </strong>为数据，其中<em class="mr"> x1，x2 </em> …。属于单变量特征空间。</p><p id="7133" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated"><strong class="lv ir"> <em class="mr"> Y ~ N( b*x + c，1/t) </em> </strong>，其中Y为正态分布的随机变量，均值<em class="mr"> b*x + c </em>，方差<em class="mr"> 1/t </em>，其中<em class="mr"> t </em>代表精度。</p><p id="9981" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">在这种情况下，我们将考虑具有两个参数的单变量特征空间(<em class="mr"> X </em>),即<em class="mr">斜率(c) </em>和<em class="mr">截距(b) </em>。因此，在本实验中，我们将学习上述参数<em class="mr"> c &amp; b </em>和精度<em class="mr"> t. </em>的后验分布</p><p id="9f9a" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">让我们写下上述参数的假设先验分布</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/5bc0b1fd79ee96172e963132b9da55d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uT4t0O1LU6daaiIyZAzPpQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">参数的先验分布</strong></p></figure><p id="7518" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">因此，让我们首先写下正态分布的密度函数和log_pdf，以得到一个概括的形式，这将在以后多次使用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/0db191d4ba3235e16cba0f4212634fc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O2YmjsJ1HOTC9euEjjCp8Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">对数正态分布的pdf展开</strong></p></figure><p id="db57" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">现在，我们将很容易写出我们的情况的可能性，它也遵循均值<strong class="lv ir"> <em class="mr"> bx+c </em> </strong>和精度<strong class="lv ir"> <em class="mr"> t </em> </strong>的正态分布。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/019dc725d7e52112175336e192919f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ekit5uHSAi88GXavBptqtw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">我们场景中的可能性估计</strong></p></figure><p id="ca58" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">取其对数给出下面的表达式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/a8767cd04843c18ba66e41e129130015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LhPelWqKZcHOsCmIdxgQWQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">我们场景的对数可能性</strong></p></figure><p id="b2d6" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">接下来是一个复杂的小部分，即导出所有三个参数<em class="mr"> b，c，t. </em>的条件后验分布</p><p id="f3b2" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated"><strong class="lv ir"> <em class="mr">条件后验分布为截距_c : </em> </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/f70ce2d93aa324f0433968579a93666b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B3gRDm1321bnFuR--UnPCA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">c _ part 1的条件后验分布</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/98e8af8ac1e8238cb83e230bfdf2147c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORrXqgL5wH1VB8F9wJ4T1w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">c _ part 2的条件后验分布</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/cee8dad21ae335a4435a8d5642016a7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5cJIhgGeh0v7Xqzy9gDiYA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">c _ part 3的条件后验分布</strong></p></figure><p id="c93b" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">因此，我们可以看到，这是找出截距c的条件后验分布的过程。</p><p id="9cef" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">上述等式的代码片段</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="b04e" class="kx ky iq nq b gy nu nv l nw nx"><strong class="nq ir">def get_intercept_c</strong>(y, x, b, t, c0, tc):<br/>    n = len(y)<br/>    assert len(x) == n<br/>    precision = c0 + t * n<br/>    mean = tc * c0 + t * np.sum(y - b * x)<br/>    mean = mean/precision<br/>    <strong class="nq ir">return</strong> np.random.normal(mean, 1 / np.sqrt(precision)</span></pre><p id="e4de" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">同样，我们可以找到同样的<strong class="lv ir"> <em class="mr">斜率b. </em> </strong></p><p id="55de" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated"><strong class="lv ir"> <em class="mr">条件后验分布为斜率_b : </em> </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/4a9a6e19129d1a914481ace915bbb139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mAJ-mBACgI_eaQY0JOM1-w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">b _ part 1的条件后验分布</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/8cc847b896c0bab94c220f2249ddf6a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NU9bMQMSJzFG2kjs3pAeAQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">b _ part 2的条件后验分布</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/b91c278f2b7a4762562bb9af23e391c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w81k8rQ9t-OrBvSqJGKV1Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">b _ part 3的条件后验分布</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/b3bd15ba712bb90eaa78f9174e95442d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6SqgKfL_FiESuavlXZXFtg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">b _ part 4的条件后验分布</strong></p></figure><p id="3de9" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">斜坡更新的代码片段如下</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="cd65" class="kx ky iq nq b gy nu nv l nw nx"><strong class="nq ir">def get_slope_b</strong>(y, x, c, t, b0, tb):<br/>    n = len(y)<br/>    assert len(x) == n<br/>    precision = tb + t * np.sum(x * x)<br/>    mean = tb * b0 + t * np.sum( (y - c) * x)<br/>    mean = mean/precision<br/>    <strong class="nq ir">return</strong> np.random.normal(mean, 1 / np.sqrt(precision))</span></pre><p id="9315" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated"><strong class="lv ir"> <em class="mr">条件后验分布为精度_t : </em> </strong></p><p id="8bcb" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">条件后验分布给出b，c将不会像上面两个一样，因为先验遵循伽马分布。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/47b5a7846e265f37eca125c19f6abda3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ri4xST2KMayUgQMEojjoOw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">t _ part 1的条件后验分布</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/9b6fccdb7d5b11e17273dc481e73f178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r7jQOokvfCR0vVH8TLz4Mg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">t _ part 2的条件后验分布</strong></p></figure><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="3849" class="kx ky iq nq b gy nu nv l nw nx"><strong class="nq ir">def get_precision_t</strong>(y, x, b, c, alpha, beta):<br/>    n = len(y)<br/>    alpha_new = alpha + n / 2<br/>    resid = y - c - b * x<br/>    beta_new = beta + np.sum(resid * resid) / 2<br/>    <strong class="nq ir">return</strong> np.random.gamma(alpha_new, 1 / beta_new)</span></pre><p id="465a" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">现在，由于我们可以获得参数的封闭分布形式，现在我们可以使用<strong class="lv ir"> MCMC模拟</strong>从后验分布中生成。</p><p id="4695" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">因此，首先，为了运行实验，我生成了一个具有已知斜率和截距系数的样本数据，我们可以通过贝叶斯线性回归对其进行验证。我们假设实验a=6，b = 2</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="b8ad" class="kx ky iq nq b gy nu nv l nw nx"># observed data<br/>n = 1000<br/>_a = 6<br/>_b = 2<br/>x = np.linspace(0, 1, n)<br/>y = _a*x + _b + np.random.randn(n)</span><span id="4910" class="kx ky iq nq b gy od nv l nw nx">synth_plot = plt.plot(x, y, "o")<br/>plt.xlabel("x")<br/>plt.ylabel("y")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/adebfc0ea2b93ee0307b3b3d66ebec3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_ej_5AB2mefa0k7vnSFFw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">生成数据运行贝叶斯线性回归</strong></p></figure><p id="085e" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">现在，在基于上面所示的等式和片段更新参数之后，我们运行MCMC仿真来获得参数的真实后验分布。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="04f7" class="kx ky iq nq b gy nu nv l nw nx"><strong class="nq ir">def gibbs</strong>(y, x, iters, init, hypers):<br/>    assert len(y) == len(x)<br/>    c = init["c"]<br/>    b = init["b"]<br/>    t = init["t"]<br/>    <br/>    trace = np.zeros((iters, 3)) ## trace to store values of b, c, t<br/>    <br/>    for it in tqdm(range(iters)):<br/>        c = get_intercept_c(y, x, b, t, hypers["c0"], hypers["tc"])<br/>        b = get_slope_b(y, x, c, t, hypers["b0"], hypers["tb"])<br/>        t = get_precision_t(y, x, b, c, hypers["alpha"], hypers["beta"])<br/>        trace[it,:] = np.array((c, b, t))<br/>        <br/>    trace = pd.DataFrame(trace)<br/>    trace.columns = ['intercept', 'slope', 'precision']<br/>        <br/>    <strong class="nq ir">return</strong> trace</span><span id="45f3" class="kx ky iq nq b gy od nv l nw nx">iters = 15000<br/>trace = gibbs(y, x, iters, init, hypers)</span></pre><p id="d5bf" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">在运行实验15000次迭代后，我们看到了验证我们假设的轨迹图。在MCMC仿真中有一个<strong class="lv ir"><em class="mr"/></strong>老化期的概念，我们忽略最初的几次迭代，因为它不会从真实的后验随机样本中复制样本。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/dc5f96a1cac1de27ebff47998cff9434.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UCVHaA0E27hx_IFioKAoBA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">MCMC仿真的轨迹图</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/b5f78a1bd96c6c940f3b9aaae9e70338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*adQFWkF1dargUqQd6TLZ3g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">【MCMC模拟参数的采样后验分布</p></figure><p id="5ef1" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">如你所见，样本后验分布复制了我们的假设，我们有更多关于系数和参数的信息。</p><p id="178e" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated">但是并不总是可能有条件后验的封闭分布形式，因此我们必须使用上面简要讨论的<strong class="lv ir">Metropolis-Hastings</strong>T10】算法选择接受和拒绝抽样的建议分布。</p><p id="fca8" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated"><strong class="lv ir">注意</strong>:贝叶斯思维和推理有很多优点，我将在我即将发布的博客和材料中提供关于<strong class="lv ir">变分推理和贝叶斯思维</strong>的后续材料。我将深入研究贝叶斯深度学习及其优势。所以，坚持阅读，分享知识。</p><p id="0fb1" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated"><strong class="lv ir"> P </strong>。这个演讲是Kaggle Days Meetup德里-NCR会议的一部分，请关注这些材料和会议。你也会看到我简要解释这个话题的视频。</p><p id="916d" class="pw-post-body-paragraph lt lu iq lv b lw mm jr ly lz mn ju mb lg mo md me lk mp mg mh lo mq mj mk ml ij bi translated"><strong class="lv ir">参考文献</strong>:</p><ol class=""><li id="cd47" class="oh oi iq lv b lw mm lz mn lg oj lk ok lo ol ml om on oo op bi translated"><a class="ae kw" href="https://kieranrcampbell.github.io/blog/2016/05/15/gibbs-sampling-bayesian-linear-regression.html" rel="noopener ugc nofollow" target="_blank">https://kieranrcampbell . github . io/blog/2016/05/15/Gibbs-sampling-Bayesian-linear-regression . html</a></li><li id="78d6" class="oh oi iq lv b lw oq lz or lg os lk ot lo ou ml om on oo op bi translated"><a class="ae kw" href="https://www.youtube.com/watch?v=7QX-yVboLhk" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=7QX-yVboLhk</a></li><li id="367c" class="oh oi iq lv b lw oq lz or lg os lk ot lo ou ml om on oo op bi translated"><a class="ae kw" href="https://medium.com/quick-code/maximum-likelihood-estimation-for-regression-65f9c99f815d" rel="noopener">https://medium . com/quick-code/maximum-likelihood-estimation-for-regression-65 F9 c 99 f 815d</a></li><li id="5216" class="oh oi iq lv b lw oq lz or lg os lk ot lo ou ml om on oo op bi translated"><a class="ae kw" href="https://www.youtube.com/watch?v=ulZW99jsMXY&amp;t=655s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=ulZW99jsMXY&amp;t = 655s</a></li><li id="08ef" class="oh oi iq lv b lw oq lz or lg os lk ot lo ou ml om on oo op bi translated"><a class="ae kw" href="https://www.youtube.com/watch?v=OTO1DygELpY&amp;t=198s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=OTO1DygELpY&amp;t = 198s</a></li></ol><div class="ov ow gp gr ox oy"><a href="https://www.linkedin.com/in/souradip-chakraborty/" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd ir gy z fp pd fr fs pe fu fw ip bi translated">Souradip Chakraborty -数据科学家-沃尔玛印度实验室| LinkedIn</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">我是一名有抱负的统计学家和机器学习科学家。我探索机器学习、深度学习和…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">www.linkedin.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm kp oy"/></div></div></a></div><div class="ov ow gp gr ox oy"><a href="https://developers.google.com/community/experts/directory/profile/profile-souradip_chakraborty" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd ir gy z fp pd fr fs pe fu fw ip bi translated">专家|谷歌开发者</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">机器学习我是Souradip Chakraborty，目前在印度沃尔玛实验室担任数据科学家(研究)</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">developers.google.com</p></div></div><div class="ph l"><div class="pn l pj pk pl ph pm kp oy"/></div></div></a></div></div></div>    
</body>
</html>