<html>
<head>
<title>To Serve Man</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为人类服务</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/to-serve-man-60246a82d953?source=collection_archive---------14-----------------------#2020-02-22">https://towardsdatascience.com/to-serve-man-60246a82d953?source=collection_archive---------14-----------------------#2020-02-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="97c5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用 Seldon Core 在 Kubernetes 上部署模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3a97c52e99e911bbee17261d34a7944d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rXJ4Xtcc1dfAdWXZjE9K2Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:Pixabay</p></figure><h1 id="76c4" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="0470" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">讽刺的是，有史以来关于机器学习的最好的论文之一与实际的机器学习几乎没有关系！</p><p id="36e6" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在论文<a class="ae mo" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=11&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjBoYzg-8DnAhWIl-AKHfQ2AOEQFjAKegQIAxAB&amp;url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5656-hidden-technical-debt-in-machine-learning-systems&amp;usg=AOvVaw3pwxPSkCQaevZTdxkTOe0G" rel="noopener ugc nofollow" target="_blank">机器学习系统中隐藏的技术债务</a>中，一群来自谷歌的机器学习研究人员敏锐地指出，“任何现实世界的 ML 系统中只有一小部分是由 ML 代码组成的”，并且通常“[模型]所需的周围基础设施是巨大而复杂的。”</p><p id="0d0f" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">换句话说，能够将一个模型从 Jupyter 笔记本中的单个单元转换为一个实际的生产软件并不容易。</p><h1 id="7a61" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">服务基础设施</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/a38dacfc1329c0ff0de9b7606e28ffa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pTn8k4eIA2c50XWIHulFNg.jpeg"/></div></div></figure><p id="89d2" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">正如本文的图表所示，任何生产 ML 系统的一个较大部分是它的服务基础设施，它通常是利益相关者与 ML 代码实际交互的地方。</p><p id="ed85" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">对于许多企业来说，该基础架构是某种容器编排引擎，如 Kubernetes。</p><p id="b56f" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">但是在深入研究如何使用像 Seldon Core 这样的框架为 Kubernetes 下的模型提供服务的技术细节之前，<em class="mq">为什么首先要经历所有这些麻烦呢？</em></p><h1 id="06ed" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">作为一等公民的模范</h1><p id="b402" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">当一个复杂的系统在其主要的功能领域中定义和执行真正的关注点分离时，维护和管理它就容易得多。</p><p id="9902" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">简而言之，您希望所有的机器学习代码都是自包含的，并且不依赖于它所在的更广泛的应用程序的运行时。</p><p id="4f19" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">这就是为什么，例如，将模型作为自包含的 Docker 图像，或将它们隐藏在定制的烧瓶或闪亮的应用程序中，在生产中通常会受到反对——它将机器学习位隐藏在许多层之下，使其更难管理。</p><p id="35c1" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">理想情况下，您希望能够利用与 CPU、存储等其他计算资源相同的部署和管理习惯。但是你的模特。</p><p id="b432" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">这样，数据科学家和/或机器学习工程师可以专注于模型设计和性能，而您的 MLop 人员则可以专注于模型部署和相关基础设施。</p><p id="b41f" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在 Kubernetes 的世界中，这意味着模型应该被定义为真正的 Kubernetes 对象(<a class="ae mo" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" rel="noopener ugc nofollow" target="_blank"> YAML </a> / <a class="ae mo" href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" rel="noopener ugc nofollow" target="_blank"> CRDs </a>)，并使用 kubectl 进行管理。</p><h1 id="a17a" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">进入谢顿核心</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/02a697ed26d46c4ad8df727f5dd040d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7piKtwfDPgClo1NFBxTEnA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">资料来源:谢顿核心文件</p></figure><p id="af0c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">Seldon Core 是一个开源项目，为您的 Kubernetes 集群提供通用模型服务框架。它解决了三个主要挑战:</p><ul class=""><li id="7ab1" class="ms mt iq lp b lq mj lt mk lw mu ma mv me mw mi mx my mz na bi translated">支持大量不同语言的机器学习框架。</li><li id="9023" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">将模型作为 REST 或 gRPC web 服务自动公开。</li><li id="d218" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">支持模型部署的全生命周期管理，包括健康、遥测、安全和扩展。</li></ul><p id="f1df" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在本文中，我们将在 Seldon Core 下部署一个简单的 REST 风格的模型服务，它接收一个图像，然后不加修改地返回它。</p><p id="e637" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在这个过程中，您将看到注入 ML 代码来执行推理以创建一个成熟的模型微服务是多么容易。</p><p id="bddf" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">到本文结束时，您应该对如何在 Kubernetes 上的 Seldon Core 下部署模型有了很好的理解。</p><h1 id="19df" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">推理图</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/91e74f9add9b112ea68ffbf90a49e73d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sEjkpcJhihD7sKPcGXyQ_w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">资料来源:谢顿核心文件</p></figure><p id="2807" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">谢顿核心的预测 API 让你将你的模型定义为一个<strong class="lp ir">推理图</strong>。</p><p id="dc20" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">请求流过图的每个节点，最终到达一个<strong class="lp ir">模型</strong>叶节点，该节点在您最喜欢的 ML 框架(例如 Tensorflow、scikit-learn)中运行某种预测功能，并返回结果。</p><p id="c10e" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><a class="ae mo" href="https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/internal-api.html" rel="noopener ugc nofollow" target="_blank"> Seldon Core 支持许多不同种类的节点</a>，允许您创建更复杂的推理图工作流程，如上图所示。</p><p id="30e8" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">最简单的推理图只有一个根模型<strong class="lp ir"> </strong>节点，这是我们的示例服务将要实现的。</p><h1 id="bbcc" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">先决条件</h1><p id="c111" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在阅读下面的教程时，会做出以下假设:</p><ul class=""><li id="7cf4" class="ms mt iq lp b lq mj lt mk lw mu ma mv me mw mi mx my mz na bi translated">您可以访问已经安装了 Seldon Core 的 Kubernetes 集群(≥1.12)，或者拥有管理权限来安装该集群。</li></ul><p id="4a9a" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">谢顿核心很容易安装:<a class="ae mo" href="https://docs.seldon.io/projects/seldon-core/en/latest/workflow/install.html" rel="noopener ugc nofollow" target="_blank">只需遵循几个简单的指令来部署它</a>。</p><ul class=""><li id="27e4" class="ms mt iq lp b lq mj lt mk lw mu ma mv me mw mi mx my mz na bi translated">您已经安装了 Docker，并且可以轻松地编写 Docker 文件以及构建映像并将其推送到本地集群注册表。</li></ul><h1 id="f564" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">选择一种语言，任何语言…</h1><p id="c42d" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">目前，<a class="ae mo" href="https://docs.seldon.io/projects/seldon-core/en/latest/wrappers/README.html" rel="noopener ugc nofollow" target="_blank"> Seldon Core 支持多种语言</a>包括 Python、R、Java、NodeJS，现在还有 Go (alpha)。</p><p id="e2ab" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">但是<em class="mq">如何</em>？</p><p id="2792" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">任何 k8s 集群中的基本执行单元是一个 Pod，它总是由一个或多个 Docker 容器支持。</p><p id="1d25" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">Seldon Core 让您首先在他们的预测 API 下实现您的模型代码，然后将其包装在 Docker 容器中。</p><p id="5974" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">您创建的图像既可以通过<strong class="lp ir"> Dockerfile </strong>直接构建，也可以使用<a class="ae mo" href="https://github.com/openshift/source-to-image" rel="noopener ugc nofollow" target="_blank"> OpenShift 的 s2i 工具</a>以 Seldon Core 的包装图像为基础构建。</p><p id="770a" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">对于我们简单的基于 Python 的服务示例，我们将使用几个简单的 Dockerfile 指令直接创建它。</p><h1 id="8fe2" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">听候吩咐</h1><p id="5962" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">好了，让我们编写一个简单的 Python 服务，我们可以使用它的预测 API 部署在 Seldon Core 下。</p><p id="762b" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">我们的服务只是将图像作为张量读入，然后返回。</p><p id="5e70" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">创建一个名为<strong class="lp ir"> MyModel.py </strong>的文件，如下<strong class="lp ir"> : </strong></p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="3bad" class="nm kw iq ni b gy nn no l np nq">#!/usr/bin/env python3</span><span id="89b4" class="nm kw iq ni b gy nr no l np nq">import io<br/>import logging<br/>import numpy as np</span><span id="1a3f" class="nm kw iq ni b gy nr no l np nq">from PIL import Image</span><span id="67bf" class="nm kw iq ni b gy nr no l np nq">logger = logging.getLogger('__mymodel__')</span><span id="6a47" class="nm kw iq ni b gy nr no l np nq"><strong class="ni ir">class MyModel(object):</strong></span><span id="2cb0" class="nm kw iq ni b gy nr no l np nq">  def __init__(self):</span><span id="3e88" class="nm kw iq ni b gy nr no l np nq">    logger.info("initializing...")<br/>    logger.info("load model here...")<br/>    self._model = None<br/>    logger.info("model has been loaded and initialized...")</span><span id="9f67" class="nm kw iq ni b gy nr no l np nq">  <strong class="ni ir">def predict(self, X, features_names):<br/>    </strong>""" Seldon Core Prediction API """<br/> <br/>    logger.info("predict called...")</span><span id="1704" class="nm kw iq ni b gy nr no l np nq">    # Use Pillow to convert to an RGB image then reverse channels.<br/>    logger.info('converting tensor to image')<br/>    <strong class="ni ir">img = Image.open(io.BytesIO(X)).convert('RGB')<br/>    img = np.array(img)<br/>    img = img[:,:,::-1]</strong></span><span id="d100" class="nm kw iq ni b gy nr no l np nq">    logger.info("image size = {}".format(img.shape))</span><span id="6fb8" class="nm kw iq ni b gy nr no l np nq">    if self._model:<br/>      logger.info("perform inference here...")</span><span id="d97d" class="nm kw iq ni b gy nr no l np nq">    # This will serialize the image into a JSON tensor<br/>    logger.info("returning prediction...")</span><span id="a2aa" class="nm kw iq ni b gy nr no l np nq">    # Return the original image sent in RGB<br/>    <strong class="ni ir">return img[:,:,::-1]</strong></span></pre><p id="b858" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们来分解一下:</p><ul class=""><li id="3fc4" class="ms mt iq lp b lq mj lt mk lw mu ma mv me mw mi mx my mz na bi translated">我们的模型服务类是一个简单的 Python 对象，它实现了一个<strong class="lp ir"> predict() </strong>方法。请注意，在您的实际模型代码中不需要 Seldon 核心 API。您所需要做的就是用 predict()方法创建一个类对象，这样就完成了！</li><li id="0409" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">predict()方法将一个<strong class="lp ir">张量 X </strong>和一列<strong class="lp ir">特征名</strong>作为输入。这些类型由谢顿核心 protobuf 规范定义，在此处找到<a class="ae mo" href="https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/prediction.html#proto-buffer-and-grpc-definition" rel="noopener ugc nofollow" target="_blank"/>。在我们的示例服务中，我们将只使用张量对象，因为我们的模型吸收图像。</li><li id="3b9a" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">我们需要将 Seldon Core 传入的 object X 这样的字节数组转换成 RGB 图像。</li><li id="8b51" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">然后我们反转图像的通道来创建我们最终的 BGR 图像进行处理(适合一些 OpenCV 处理)。</li><li id="bde6" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">注意，我们可以在类实例化时注入加载模型，然后在 predict()函数中非常容易地运行推理。</li></ul><h1 id="c5db" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">包起来！</h1><p id="a703" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">既然我们已经整理好了模型类，让我们使用这段代码来构建一个使用 Seldon 核心包装 API 的 Docker 容器。</p><p id="dfb4" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">创建一个<strong class="lp ir"> Dockerfile </strong>，看起来像:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="df1b" class="nm kw iq ni b gy nn no l np nq">FROM python:3.7-buster</span><span id="213e" class="nm kw iq ni b gy nr no l np nq">RUN apt-get update &amp;&amp; DEBIAN_FRONTEND=noninteractive &amp;&amp; apt-get install -y \<br/>   curl \<br/>   python3-setuptools &amp;&amp; \<br/>apt-get clean &amp;&amp; apt-get autoremove -y &amp;&amp; rm -rf /var/lib/apt/lists/*</span><span id="b715" class="nm kw iq ni b gy nr no l np nq">COPY requirements.txt .<br/>RUN curl -s <a class="ae mo" href="https://bootstrap.pypa.io/get-pip.py" rel="noopener ugc nofollow" target="_blank">https://bootstrap.pypa.io/get-pip.py</a> -o get-pip.py &amp;&amp; python3 get-pip.py &amp;&amp; rm -f get-pip.py</span><span id="8c4d" class="nm kw iq ni b gy nr no l np nq">RUN pip3 install --no-cache numpy Pillow <strong class="ni ir">seldon-core</strong></span><span id="1ffc" class="nm kw iq ni b gy nr no l np nq"><strong class="ni ir"># Seldon Core specific</strong><br/><strong class="ni ir">COPY . /microservice<br/>WORKDIR /microservice</strong></span><span id="aa18" class="nm kw iq ni b gy nr no l np nq"><strong class="ni ir">ENV MODEL_NAME MyModel<br/>ENV API_TYPE REST<br/>ENV SERVICE_TYPE MODEL<br/>ENV PERSISTENCE 0</strong></span><span id="0ef3" class="nm kw iq ni b gy nr no l np nq"><strong class="ni ir">CMD exec seldon-core-microservice $MODEL_NAME $API_TYPE --service-type $SERVICE_TYPE --persistence $PERSISTENCE</strong></span><span id="e227" class="nm kw iq ni b gy nr no l np nq"><strong class="ni ir">EXPOSE 5000</strong></span></pre><p id="7f4c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们来分解一下:</p><ul class=""><li id="309a" class="ms mt iq lp b lq mj lt mk lw mu ma mv me mw mi mx my mz na bi translated">docker 文件的大部分是相当标准的内容，因为我们创建了一个小的 Python 3.7 buster 映像，有一些运行时需求。</li><li id="2a05" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">注意，我们安装了<strong class="lp ir">谢顿核心</strong> Python 模块，作为模型运行时的一部分。</li><li id="f769" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">我们将应用程序安装在容器的<strong class="lp ir">/微服务</strong>目录中。</li><li id="0e5d" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">您的模型类对象实际上是由<strong class="lp ir">seldon-core-microservice</strong>应用程序加载的，并在内部将其公开为一个<a class="ae mo" href="https://flask.palletsprojects.com" rel="noopener ugc nofollow" target="_blank"> Flask </a> webapp(我们将在实际部署模型时意识到这一点)。</li><li id="1599" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">Seldon Core 同时支持 REST 和 gRPC web 服务 API。这里我们指定 REST，因为我们将使用一个简单的 http POST 操作来测试我们的服务。</li></ul><p id="d06d" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">构建映像并将其推送到集群的存储库:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="8209" class="nm kw iq ni b gy nn no l np nq">$ docker build --no-cache -t my-model:0.1 .<br/>Sending build context to Docker daemon  6.144kB<br/>Step 1/14 : FROM python:3.7-buster<br/>---&gt; 879165535a54<br/>...<br/>Step 14/14 : EXPOSE 5000<br/> ---&gt; Running in 8e9f588abe89<br/> ---&gt; 83b0a4682783<br/>Successfully built 83b0a4682783</span><span id="275b" class="nm kw iq ni b gy nr no l np nq">$ docker tag my-model:latest &lt;your repo&gt;/my-model:0.1<br/>$ docker push &lt;your repo&gt;/my-model:0.1</span></pre><h1 id="ea2b" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">为当地服务</h1><p id="2498" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">现在我们已经构建了一个映像，让我们通过运行我们的<strong class="lp ir"> seldon-core-microservice </strong>作为一个独立的 docker 容器来测试它:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="276a" class="nm kw iq ni b gy nn no l np nq">$ docker run -d --rm --name my-model -p 5000:5000 my-model:0.1<br/>5aa997b0b093612f88499e13260cf96ee6d9931749f2dfa23ee3d61d303c2cc5<br/>$ docker logs -f my-model<br/>2020-02-21 01:49:38,651 - seldon_core.microservice:main:190 - INFO:  Starting microservice.py:main<br/>...<br/><strong class="ni ir">2020-02-21 01:49:38,659 - __mymodel__:__init__:15 - INFO:  initializing...<br/>2020-02-21 01:49:38,659 - __mymodel__:__init__:16 - INFO:  load model here...<br/>2020-02-21 01:49:38,659 - __mymodel__:__init__:18 - INFO:  model has been loaded and initialized...</strong><br/>2020-02-21 01:49:38,659 - seldon_core.microservice:main:325 - INFO:  REST microservice running on <strong class="ni ir">port 5000</strong><br/>2020-02-21 01:49:38,659 - seldon_core.microservice:main:369 - INFO:  Starting servers<br/> * Serving Flask app "seldon_core.wrapper" (lazy loading)<br/> * Environment: production<br/>   WARNING: This is a development server. Do not use it in a production deployment.<br/>   Use a production WSGI server instead.<br/> * Debug mode: off<br/>2020-02-21 01:49:38,673 - werkzeug:_log:113 - INFO:   * Running on <a class="ae mo" href="http://0.0.0.0:5000/" rel="noopener ugc nofollow" target="_blank">http://0.0.0.0:5000/</a> (Press CTRL+C to quit)</span></pre><p id="9bbe" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">请注意，您上面编写的 MyModel 类已被加载，并作为 Flask 应用程序在端口 5000 上提供。模型的预测终点在<strong class="lp ir">'/预测'</strong>或<strong class="lp ir">'/API/v 0.1/预测'</strong>。</p><h1 id="a199" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">当地商店</h1><p id="7c35" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们通过编写一个简单的客户端脚本来测试我们的 MyModel 服务，该脚本将向我们的模型的预测端点发送一个图像文件。我们将使用相同的脚本来测试我们在 Kubernetes 上的部署，只需更改 URL。</p><p id="b96c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">这是我们的客户，my-model-client.py :</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="a4fe" class="nm kw iq ni b gy nn no l np nq">#!/usr/bin/env python3</span><span id="83ea" class="nm kw iq ni b gy nr no l np nq">import base64<br/>import json<br/>import logging<br/>import os<br/>import numpy as np<br/>import requests<br/>import sys</span><span id="8965" class="nm kw iq ni b gy nr no l np nq">from PIL import Image</span><span id="4787" class="nm kw iq ni b gy nr no l np nq">logger = logging.getLogger('__mymodelclient__')<br/>logger.setLevel(logging.INFO)<br/>logger.addHandler(logging.StreamHandler())</span><span id="4045" class="nm kw iq ni b gy nr no l np nq">if __name__ == '__main__':<br/>    url = sys.argv[1]<br/>    path = sys.argv[2]</span><span id="eec4" class="nm kw iq ni b gy nr no l np nq">    <strong class="ni ir"># base64 encode image for HTTP POST<br/>    data = {}<br/>    with open(path, 'rb') as f:<br/>        data['binData'] = base64.b64encode(f.read()).decode('utf-8')</strong></span><span id="b8f0" class="nm kw iq ni b gy nr no l np nq">    logger.info("sending image {} to {}".format(path, url))<br/>  <strong class="ni ir">  response = requests.post(url, json = data, timeout = None)</strong><br/>    <br/>    logger.info("caught response {}".format(response))<br/>    status_code = response.status_code<br/>    js = response.json()</span><span id="e98b" class="nm kw iq ni b gy nr no l np nq">    if response.status_code == requests.codes['ok']:<br/>        logger.info('converting tensor to image')<br/>        data = js.get('data')<br/>        tensor = data.get('tensor')<br/>        shape = tensor.get('shape')<br/>        values = tensor.get('values')<br/>        logger.info("output image shape = {}".format(shape))</span><span id="67ab" class="nm kw iq ni b gy nr no l np nq">        # Convert Seldon tensor to image<br/>       <strong class="ni ir"> img_bytes = np.asarray(values)<br/>        img = img_bytes.reshape(shape)<br/>        Image.fromarray(img.astype(np.uint8)).save('result.jpg')</strong><br/>        logger.info('wrote result image to result.jpg')<br/>    elif response.status_code == requests.codes['service_unavailable']:<br/>        logger.error('Model service is not available.')<br/>    elif response.status_code == requests.codes['internal_server_error']:<br/>        logger.error('Internal model error.')</span></pre><p id="beca" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们来分解一下:</p><ul class=""><li id="50f4" class="ms mt iq lp b lq mj lt mk lw mu ma mv me mw mi mx my mz na bi translated">我们传递想要张贴图像的 URL 和图像本身的路径。</li><li id="7403" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">我们需要对图像进行 base64 编码，为发布到本地作为容器运行的 Seldon Core 微服务做准备。</li><li id="a80c" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">我们发送一个包含 JSON 字符串的 POST，该字符串包含<strong class="lp ir"> binData </strong>键和 base64 编码图像作为其值。</li><li id="16f8" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">如果 POST 成功(HTTP STATUS OK 200 ),我们从 JSON 响应中读取<strong class="lp ir">数据</strong>键，并提取<strong class="lp ir">张量</strong>,这实际上是我们的结果图像。</li><li id="b281" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">张量有一个<strong class="lp ir">形状</strong>和<strong class="lp ir">值</strong>键——值键是图像本身作为像素强度的数组。</li><li id="a28d" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">我们使用<strong class="lp ir"> Pillow </strong>将张量值写成一个名为“result.jpg”的 JPEG 文件。</li></ul><p id="7d19" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">现在让我们使用这个脚本来测试我们的服务:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="f394" class="nm kw iq ni b gy nn no l np nq">$ file test_image.jpg<br/>test_image.jpg: JPEG image data, JFIF standard 1.00, resolution (DPI), density 0x0, segment length 16, comment: "LEAD Technologies Inc. V1.01", baseline, precision 8, <strong class="ni ir">1280x960, components 3</strong></span><span id="4111" class="nm kw iq ni b gy nr no l np nq">$ python3 my-model-client.py <a class="ae mo" href="http://localhost:5000/api/v0.1/predictions" rel="noopener ugc nofollow" target="_blank">http://localhost:5000/api/v0.1/predictions</a> test_image.jpg<br/>sending image test_image.jpg to <a class="ae mo" href="http://localhost:5000/api/v0.1/predictions" rel="noopener ugc nofollow" target="_blank">http://localhost:5000/api/v0.1/predictions</a><br/>caught response &lt;Response [200]&gt;<br/>converting tensor to image<br/>result image shape = [960, 1280, 3]<br/>wrote result image to result.jpg</span><span id="4bd1" class="nm kw iq ni b gy nr no l np nq">$ file result.jpg<br/>result.jpg: JPEG image data, JFIF standard 1.01, aspect ratio, density 1x1, segment length 16, baseline, precision 8, <strong class="ni ir">1280x960, components 3</strong></span></pre><p id="8c5b" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们看看处理图像的 docker 容器的日志消息:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="7540" class="nm kw iq ni b gy nn no l np nq">$ docker logs my-model<br/>2020-02-21 15:32:33,363 - __mymodel__:predict:22 - <strong class="ni ir">INFO:  predict called...</strong><br/>2020-02-21 15:32:33,391 - __mymodel__:predict:27 - <strong class="ni ir">INFO:  image size = (960, 1280, 3)</strong><br/>2020-02-21 15:32:33,391 - __mymodel__:predict:33 - <strong class="ni ir">INFO:  returning prediction...</strong><br/>2020-02-21 15:32:33,977 - seldon_core.user_model:client_class_names:166 - INFO:  class_names is not implemented<br/>2020-02-21 15:32:33,977 - seldon_core.user_model:client_custom_tags:134 - INFO:  custom_tags is not implemented<br/>2020-02-21 15:32:33,977 - seldon_core.user_model:client_custom_tags:134 - INFO:  custom_tags is not implemented<br/>2020-02-21 15:32:33,977 - seldon_core.user_model:client_custom_metrics:307 - INFO:  custom_metrics is not implemented<br/>2020-02-21 15:32:34,271 - werkzeug:_log:113 - INFO:  172.17.0.1 - - [21/Feb/2020 15:32:34] <strong class="ni ir">"POST /api/v0.1/predictions HTTP/1.1" 200 -</strong></span></pre><h1 id="4fcd" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">部署它！</h1><p id="18e5" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">像所有 Kubernetes 的东西一样，Seldon Core 通过一个<strong class="lp ir">自定义资源定义(CRD) </strong>文件定义了自己的部署对象，称为<strong class="lp ir"> SeldonDeployment </strong>。</p><p id="9183" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们通过一个<strong class="lp ir"> seldon-deploy.yaml </strong>文件来定义我们的模型的 seld deploy:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="0f7f" class="nm kw iq ni b gy nn no l np nq">apiVersion: machinelearning.seldon.io/v1alpha2<br/>kind: SeldonDeployment<br/>metadata:<br/>  name: my-model<br/>spec:<br/>  name: my-deployment<br/>  <strong class="ni ir">predictors:</strong><br/>  - componentSpecs:<br/>    - spec:<br/>        containers:<br/>        - name: my-model-graph<br/>          image: &lt;your cluster's registry&gt;/my-model:0.1<br/>    <strong class="ni ir">graph:</strong><br/>      <strong class="ni ir">children: []</strong><br/>      <strong class="ni ir">endpoint:<br/>        type: REST<br/>      name: classifier</strong><br/>      <strong class="ni ir">type: MODEL<br/>    annotations:<br/>      predictor_version: "0.1"<br/>      seldon.io/svc-name: my-model-svc   </strong> <br/>    <strong class="ni ir">name: my-graph</strong><br/> <strong class="ni ir">   replicas: 1</strong></span></pre><p id="a894" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们来分解一下:</p><ul class=""><li id="ef30" class="ms mt iq lp b lq mj lt mk lw mu ma mv me mw mi mx my mz na bi translated">一个很少部署由一个或多个<strong class="lp ir">预测器</strong>组成，这些预测器定义了这个部署中包含哪些模型。注意:您可能想要为金丝雀或多臂强盗类型场景定义多个预测器。</li><li id="f15c" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">每个预测器都由一个<strong class="lp ir"> Pod spec </strong>组成，它定义了我们上面构建的模型代码的 Docker 映像。</li><li id="6c49" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">由于 SeldonDeployment 是一种部署类型，每个预测器都由一个或多个<strong class="lp ir">复制集</strong>支持，复制集定义了应该创建多少个 pod 来支持您的模型(推理图)。这是允许您扩展部署以满足计算需求的方法之一。</li><li id="0f51" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">我们还设置了一个自定义的服务名，因为一个很少部署会将我们的微服务自动公开为 Kubernetes <strong class="lp ir"> Service </strong>对象。</li></ul><p id="e238" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们创建 SeldonDeployment，并查看部署以及创建的背后的对象:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="ec77" class="nm kw iq ni b gy nn no l np nq">$ kubectl get seldondeployments<br/>NAME       AGE<br/>my-model   6s</span><span id="80e6" class="nm kw iq ni b gy nr no l np nq">$ kubectl create -f seldon-deploy.yaml<br/>seldondeployment.machinelearning.seldon.io/my-model created</span><span id="e7aa" class="nm kw iq ni b gy nr no l np nq">$ kubectl get all<br/>NAME                                                  READY   STATUS    RESTARTS   AGE<br/><strong class="ni ir">pod/my-deployment-my-graph-20302ae-5cfc6c47f4-m78ll</strong>   2/2     Running   0          80s</span><span id="5086" class="nm kw iq ni b gy nr no l np nq">NAME                                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE<br/><strong class="ni ir">service/my-model-svc</strong>                              ClusterIP   172.26.126.119   &lt;none&gt;        8000/TCP,5001/TCP   55s<br/>service/seldon-9d8927429acc983eba0168e21059f589   ClusterIP   172.28.210.215   &lt;none&gt;        9000/TCP            80s</span><span id="0606" class="nm kw iq ni b gy nr no l np nq">NAME                                             READY   UP-TO-DATE   AVAILABLE   AGE<br/><strong class="ni ir">deployment.apps/my-deployment-my-graph-20302ae</strong>   1/1     1            1           80s</span><span id="04c1" class="nm kw iq ni b gy nr no l np nq">NAME                                                        DESIRED   CURRENT   READY   AGE<br/>replicaset.apps/my-deployment-my-graph-20302ae-5cfc6c47f4   1         1         1       80s</span></pre><p id="847f" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">注意一些事情:</p><ul class=""><li id="4794" class="ms mt iq lp b lq mj lt mk lw mu ma mv me mw mi mx my mz na bi translated">创建的运行您的模型代码的 Pod 实际上有两个容器——您的模型代码以及在部署时注入的“sidecar”<strong class="lp ir">seldon-container-engine</strong>。</li><li id="5459" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated"><strong class="lp ir">谢顿容器引擎</strong>将把所有请求和响应整理成谢顿核心信息格式，并监控你的模型容器的健康和遥测。</li><li id="9bb8" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">ClusterIP 服务在端口 8000 上公开，以便我们与微服务进行通信。</li></ul><p id="01ba" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">现在我们的模型微服务已经启动并运行，让我们使用上面的同一个客户端来测试它。我们需要设置一个公共可访问的<strong class="lp ir">入口</strong>或者使用<strong class="lp ir">端口转发</strong>来创建到我们的服务对象的直接连接。</p><p id="ee85" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在我们的简单示例中，我们只使用一个简单的端口转发:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="056d" class="nm kw iq ni b gy nn no l np nq">$ kubectl port-forward svc/my-model-svc 8000:8000<br/>Forwarding from 127.0.0.1:8000 -&gt; 8000<br/>Forwarding from [::1]:8000 -&gt; 8000<br/>Handling connection for 8000</span><span id="9e39" class="nm kw iq ni b gy nr no l np nq">$ python3 my-model-client.py <a class="ae mo" href="http://localhost:8000/api/v0.1/predictions" rel="noopener ugc nofollow" target="_blank">http://localhost:8000/api/v0.1/predictions</a> test_image.jpg<br/>sending image test_image.jpg to <a class="ae mo" href="http://localhost:8000/api/v0.1/predictions" rel="noopener ugc nofollow" target="_blank">http://localhost:8000/api/v0.1/predictions</a><br/>caught response &lt;Response [200]&gt;<br/>converting tensor to image<br/>result image shape = [960, 1280, 3]<br/>wrote result image to result.jpg</span></pre><p id="aac6" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">成功！</p><p id="a7f4" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">我们刚刚发送了一个图像进行推断，并从运行在我们集群上的微服务得到了一个响应。</p><p id="ccd2" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">你现在可以打开你的原始图像和我们的微服务返回的结果图像，看起来应该是一样的。</p><p id="0306" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">为了验证我们甚至可以在 Pod 内部转储日志:</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="15d1" class="nm kw iq ni b gy nn no l np nq">$ kubectl logs deployment.apps/my-deployment-my-graph-20302ae -c my-model-graph<br/>...<br/>2020-02-22 01:44:49,163 - __mymodel__:predict:22 - INFO:  predict called...<br/>2020-02-22 01:44:49,223 - __mymodel__:predict:27 - INFO:  image size = (960, 1280, 3)<br/>2020-02-22 01:44:49,223 - __mymodel__:predict:33 - INFO:  returning prediction...<br/>2020-02-22 01:44:49,948 - seldon_core.user_model:client_class_names:166 - INFO:  class_names is not implemented<br/>2020-02-22 01:44:49,949 - seldon_core.user_model:client_custom_tags:134 - INFO:  custom_tags is not implemented<br/>2020-02-22 01:44:49,949 - seldon_core.user_model:client_custom_tags:134 - INFO:  custom_tags is not implemented<br/>2020-02-22 01:44:49,949 - seldon_core.user_model:client_custom_metrics:307 - INFO:  custom_metrics is not implemented<br/>2020-02-22 01:44:50,355 - werkzeug:_log:113 - INFO:  127.0.0.1 - - [22/Feb/2020 01:44:50] "POST /predict HTTP/1.1" 200 -</span></pre><h1 id="f79d" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">打扫</h1><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="d85d" class="nm kw iq ni b gy nn no l np nq">$ kubectl delete -f seldon-deploy.yaml<br/>seldondeployment.machinelearning.seldon.io "my-model" deleted</span></pre><p id="98c9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">我们很少部署和相关的物体现在都被终止了。</p><h1 id="877e" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结论</h1><p id="1a25" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我希望通过这篇简短的教程，你能更深入地了解如何使用 Seldon Core 将你的模型部署为 Kubernetes 下的微服务。</p><p id="8a89" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">一般来说，谢顿核心开发工作流程如下:</p><ul class=""><li id="c7d3" class="ms mt iq lp b lq mj lt mk lw mu ma mv me mw mi mx my mz na bi translated">按照 Seldon 核心预测 API，将您的推理图编码为一组简单的 Python 对象。</li><li id="0978" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">将模型代码和运行时包装成 Docker 容器。</li><li id="8633" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">首先在本地测试您的容器，以验证您的模型代码是否按预期工作。</li><li id="82ef" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">将您的模型部署为一个微服务，作为一个取消部署对象。</li><li id="81c0" class="ms mt iq lp b lq nb lt nc lw nd ma ne me nf mi mx my mz na bi translated">通过创建入口或使用端口转发将请求推送给它，来测试您的 SeldonDeployment 微服务。</li></ul><p id="b580" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">Seldon Core 支持许多我在本文中没有提到的高级特性，因此我鼓励您仔细阅读该项目的<a class="ae mo" href="https://docs.seldon.io/projects/seldon-core/en/latest/" rel="noopener ugc nofollow" target="_blank">大量文档</a>，以更好地理解其整体设计和丰富的特性集。</p><p id="c079" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">但是，希望这篇文章能让您体会到为什么像 Seldon Core 这样的专用服务基础设施值得您花费时间和精力。</p></div></div>    
</body>
</html>