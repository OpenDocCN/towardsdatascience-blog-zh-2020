# 我对欧盟泄露的人工智能白皮书的评论

> 原文：<https://towardsdatascience.com/my-comments-on-eus-leaked-white-paper-on-ai-99750eb1d80b?source=collection_archive---------28----------------------->

![](img/eb280aeb3f430466db1705709a47f619.png)

人工智能白皮书的“[结构-欧洲方法](https://www.euractiv.com/wp-content/uploads/sites/2/2020/01/AI-white-paper-EURACTIV.pdf)”概述了欧盟人工智能的监管层，并提出了未来监管的几种选择。它在欧盟层面和更大范围内为人工智能制定了现有的政策框架；概述了为什么需要在整个欧洲推广人工智能；提议通过边缘计算更好地访问数据；并深入挖掘未来欧洲人工智能综合立法框架的关键要素；以治理的五个选项结束。

在通读了这篇论文的全部内容，并在日常工作中花了大量时间思考人工智能法规和道德的含义之后，我的评论集中在一些有趣的(令人困扰的？)部分，不深入每一节。我将重点讨论第 5 和第 6 部分，这是本白皮书的核心部分，因为其余大部分都是背景材料。在第 5B 节(欧盟人工智能立法框架)中，该文件讨论了当前立法框架的弱点:

1.  基本权利范围的限制:《基本权利宪章》不适用于仅涉及私营部门的情况，并且排除了除获得就业、社会保护、教育、公共服务(住房)以外的权利。人工智能系统的含义远不止这些基本原理。
2.  产品范围的限制:迄今为止，欧盟产品安全立法仅适用于将产品投放市场，不适用于基于人工智能的服务。随着新一波公司吹捧“人工智能即服务”，这将成为一个问题。
3.  供应链中不同经济运营商之间责任划分的不确定性:根据目前的欧盟立法，人工智能软件现有产品(如自动驾驶汽车)不在覆盖范围内，只有产品才在覆盖范围内，即如果宝马的自动驾驶汽车使用中国的算法，这些汽车出现故障，只有宝马而不是算法开发商需要承担责任。
4.  改变产品的性质:例如，如果你的身体机能发生变化，欧盟立法没有考虑动态调整电流的起搏器。它是为那些一旦投放市场就不会发生重大变化(软件升级或不断学习)的产品而设计的。
5.  新风险的出现:目前的立法没有充分涵盖网络安全风险、因连接丢失而导致的故障、产品使用过程中的机器学习等。
6.  与执行相关的困难:目前的法律没有涵盖提供自动决策的“黑箱”系统，在这种系统中因果关系难以证明，因此责任归属也很困难。

第 5E 节(可能的义务类型)概述了未来监管人工智能系统的可能的事前和事后要求。我的评论只集中在两个看起来有点问题和/或不完整的事前要求上。

作为维护开发人员(我在这里假设是指创建人工智能系统的组织，而不仅仅是编码人员)的问责制和透明度要求的一部分，提出的要求是“披露人工智能系统的设计参数、用于训练的数据集的元数据、进行的审计等。”——虽然这是一个很好的建议，但欧盟需要认真思考它将如何保护知识产权。公司围绕算法保密开展业务，就像制药公司围绕药物配方保密开展业务一样。也就是说，类似的知识产权原则可以应用于人工智能算法，就像应用于药物制造一样。

他们的另一个事前要求是对人工监督的要求，或者人工智能系统对自动决策的可能审查。虽然我原则上同意这一点，但人工智能系统将在我们生活中越来越复杂的领域(例如，人口健康)中实施，没有一个人，甚至一群人，可能能够提供无偏见的监督，除非他们知道人工智能系统为什么会做出这样的决定。然而，由于黑盒算法，知道这一点也不总是可能的，尽管有越来越多的工具使黑盒更容易解释。

欧盟提议的 5 个可能的监管选项都很弱:

1.  自愿贴标签——这给人一种负责任行为的错觉，同时让可能的肇事者逍遥法外，或者制造一个打地鼠问题；
2.  公共管理和面部识别的部门要求-首先，这不仅仅是面部识别(任何类型的生物识别、步态或运动分析、指纹或视网膜扫描、行为模式识别，所有这些都会带来类似的问题)，其次-这仅涵盖公共部门-我更关心的是私营部门对人工智能系统的使用。最后，它建议在欧盟制定规则的 3-5 年内禁止面部识别等技术——这肯定是一个笑话:世界将在 3-5 年内进步光年(以人工智能为单位),此后，欧盟参与者不仅将在这些类型的技术上远远落后于世界，他们还将完全错过通过更快/现在参与开放和富有成效的讨论来改善监管的机会；
3.  针对高风险应用的基于风险的强制性要求——这是最接近潜在解决方案的要求，但是，这为高风险行业和高风险要求留下了相当大的回旋余地(在这里可能会出现偏差)。也就是说，这似乎是 5 个选项中最明智的方法；
4.  选项 4(安全和责任)和选项 5(治理)适用于上述所有三个选项，但不在此列表中——编写白皮书的团队在到达这一点时一定已经筋疲力尽了！

文件的结论是“委员会认为”将备选方案 3 与备选方案 4 和 5 结合起来是最好的。那太好了；然而，重要的是在每个现有的监管机构内建立一个部门，以雇用一批人工智能专家，他们能够判断人工智能系统的设计、开发和部署是否符合道德，如果不符合道德，他们可以明智地提供建议或执行决定。

从历史上看，欧盟在保护人权、尊严和尊重等基本原则的开创性法规方面一直非常进步。这通常需要激进的思想领导、大量的资源和对现状的改变。我认为现在是再次进行这种改革的时候了。监管人工智能系统并为其建立正确的治理需要的不是渐进式而是破坏性创新，我宁愿这种创新来自欧盟，而不是谷歌。