<html>
<head>
<title>NLP: Word Embedding Techniques Demystified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP:单词嵌入技术去神秘化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-embedding-techniques-51b7e6ec9f92?source=collection_archive---------9-----------------------#2020-03-02">https://towardsdatascience.com/nlp-embedding-techniques-51b7e6ec9f92?source=collection_archive---------9-----------------------#2020-03-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f181" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">词袋vs TF-IDF vs word 2 vec vs doc 2 vec vs doc 2 vec</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9960bb1a0676a3e165ed1516b589403c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aDdSNSMrrFUHijNx"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔纳斯·雅各布森在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="536e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单词嵌入是一种单词表示技术，允许具有相似含义的单词被机器学习算法理解。从技术上讲，它是使用神经网络、概率模型或对单词共现矩阵进行降维，将单词映射到实数向量。</p><p id="0e7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以使用各种语言模型来学习单词嵌入。比如一只<strong class="lb iu"> ' <em class="lv">狗'</em> </strong>会用向量<strong class="lb iu"><em class="lv">【0.75，0.22，0.66，0.97】</em></strong>来表示。如果字典中的所有单词都以这种方式编码，就有可能相互比较单词的矢量，例如通过测量矢量之间的<em class="lv">余弦距离或</em>欧几里德距离。单词的良好表示将使得有可能发现单词<strong class="lb iu"> ' <em class="lv">宠物'</em> </strong>比单词<strong class="lb iu"> ' <em class="lv">足球'</em> </strong>或<strong class="lb iu">'引擎<em class="lv"> ' </em> </strong>更接近单词<strong class="lb iu"> ' <em class="lv">狗'</em> </strong>。所以这些表象让我们希望，在嵌入的向量空间里，我们会有方程<strong class="lb iu"> <em class="lv">国王-男人+女人=王后</em> </strong>甚至方程<strong class="lb iu"> <em class="lv">伦敦-英格兰+意大利=罗马</em> </strong>。</p><p id="6813" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单词嵌入在减轻维数灾难方面也非常有用，维数灾难是人工智能中经常出现的问题。如果没有单词嵌入，表示单词的唯一标识符会生成分散的数据，即大量稀疏表示中的孤立点。另一方面，使用单词嵌入，空间在维度方面变得更加有限，语义信息量更加丰富。有了这些数字特征，计算机更容易执行不同的数学运算，如矩阵分解、点积等。这是强制使用浅层和深层学习技术。有许多技术可供我们使用来实现这种转变。在本文中，我们将涉及:B <em class="lv"> ag-Of-Words、TF-IDF、Word2Vec、Doc2vec和doc 2 vec。</em></p><h1 id="18d0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">1.词汇袋</h1><p id="5ea9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">词袋(<em class="lv"> a.k.a. </em> BOW)是一种流行的生成文档表示的基本方法。文本被表示为包含大量单词的包。语法和词序被忽略，而频率保持不变。词袋生成的特征是一个向量，其中<strong class="lb iu"> <em class="lv"> n </em> </strong>是输入文档词汇中的字数。</p><p id="3026" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，有两个文档:</p><ul class=""><li id="83ca" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">doc_1: <strong class="lb iu"> <em class="lv">“我喜欢意大利菜和突尼斯菜。”</em>T9】</strong></li><li id="251e" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">doc_2: <strong class="lb iu"> <em class="lv">“我喜欢这里的食物。”</em>T13】</strong></li></ul><p id="7ff0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两份文件的词汇是:</p><p id="f33a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">【“我”、“爱”、“意大利”、“美食”、“和”、“突尼斯”、“这里”】</em> </strong></p><p id="af94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个词汇将产生长度为<strong class="lb iu"> 8 </strong> ( <em class="lv">即</em>词汇基数)的特征向量。给定这样的词汇表，这两个文档的词袋特征表示为:</p><ul class=""><li id="ad6e" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">BOW(doc _ 1):<strong class="lb iu"><em class="lv">【1，1，1，2，1，1，0，0】</em></strong></li><li id="077d" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">BOW(doc _ 2):<strong class="lb iu"><em class="lv">【1，1，0，1，0，0，1，1】</em></strong></li></ul><p id="a2ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用这种技术，我们为数据集中的每个文档创建单词包表示。如果我们的数据集包含大量独特的单词，其中一些并不经常使用，这是通常的情况。因此，在这些词中，我们选择了N个最频繁出现的词，并创建了一个维度为<strong class="lb iu"> <em class="lv"> Nx1 </em> </strong>的特征向量。这些特征向量然后被用于任何机器学习任务。</p><h1 id="b3be" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">2.术语频率-逆文档频率</h1><p id="fa8d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">术语频率-逆文档频率(<em class="lv"> a.k.a. </em> TF-IDF)是基于单词表示文档的另一种方式。利用TF-IDF，通过TF-IDF重要性而不仅仅是频率来给单词赋予权重。TF-IDF提供了一种统计度量，用于评估单词相对于集合或语料库中的文档的重要性。每个数据集都有许多常用词，它们在文档中多次出现，但并不提供任何重要信息。权重与该单词在文档中出现的次数成比例增加。它还根据该词在语料库中的出现频率而变化。原始公式的变体经常在搜索引擎中使用，以基于用户的搜索标准来评估文档的相关性。</p><p id="5341" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据定义，TF-IDF嵌入由两项组成:第一项计算归一化词频(TF)，<em class="lv">也称为</em>一个单词在文档中出现的次数，除以该文档中的单词总数；第二项是逆文档频率(IDF ),它计算语料库中文档数量的对数除以特定术语出现的文档数量。</p><p id="fd11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如下图所示，<a class="ae ky" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> scikit-learn </em> </a>库在子模块<code class="fe nh ni nj nk b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">sklearn.feature_extraction.text</strong></a></code>中包含了从文本文档构建特征向量的收集工具，在其中我们会发现<code class="fe nh ni nj nk b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">CountVectorizer</strong></a><strong class="lb iu">.</strong></code> <code class="fe nh ni nj nk b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">CountVectorizer</strong></a></code>可以轻松地将一组原始文档转换成TF-IDF数字特征矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/12211e4ea80c1b46a51919cb9ddf0835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BCYKfLsSjtvqKB4rVBQq4g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" rel="noopener ugc nofollow" target="_blank">使用Sklearn的tfidf矢量器</a></p></figure><h1 id="7b11" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">3.Word2Vec</h1><p id="d32a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">表示单词的最有效的技术之一是Word2Vec。Word2vec是一个计算效率高的预测模型，用于从原始文本中学习单词嵌入。它在多维向量空间中绘制单词，其中相似的单词倾向于彼此接近。一个单词的周围单词提供了该单词的上下文。</p><p id="c070" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从一个高层次的视角来看我们要去哪里。Word2Vec在深度学习中使用了一个众所周知的技巧。在这个技巧中，我们训练了一个具有单个隐藏层的简单神经网络来执行一个<em class="lv">假任务</em>，而不需要我们训练它的任务的结果。相反，主要目标包括学习输入数据的表示，其实际上是从学习的隐藏层的权重收集的<strong class="lb iu"> <em class="lv">【字向量】</em> </strong>。这种方法可能会让我们想起我们训练自动编码器进行降维的方式。</p><p id="cb51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Word2Vec可以依赖两种模型架构中的任何一种来生成输入单词的分布式表示:<strong class="lb iu"> <em class="lv">连续单词包(CBoW) </em> </strong>或<strong class="lb iu">连续跳格</strong>，如下图所示。向量表示基于数据集中单词的共现来提取语义关系。</p><p id="0cc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用二元分类来训练CBoW和skip-gram模型，以区分真实的目标单词和相同上下文中的其他单词。模型预测单词的准确性取决于模型在整个数据集中的相同上下文中看到这些单词的次数。在训练过程中，通过更多的单词和上下文共现来改变隐藏表示，这允许模型具有更多的未来成功预测，从而导致单词和上下文在向量空间中的更好表示。Skip gram比CBOW慢得多，但是对于不常用的单词执行得更准确。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/2002a317b0004425766e808ad9e6ab70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6_DSo9VZx5xKgq54VNAMTA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="http://@misc{mikolov2013efficient,     title={Efficient Estimation of Word Representations in Vector Space},     author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},     year={2013},     eprint={1301.3781},     archivePrefix={arXiv},     primaryClass={cs.CL} }" rel="noopener ugc nofollow" target="_blank"> Word2Vec训练模型架构</a></p></figure><p id="97f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们更深入地了解CBOW体系结构和连续跳转程序之间的区别。</p><ul class=""><li id="08d0" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">第一个选项中，<strong class="lb iu"><em class="lv">CBOW</em></strong>；该模型从周围上下文单词的窗口中预测当前单词。上下文单词的顺序不影响预测(单词袋假设)。例如，如果你给训练好的网络输入单词<em class="lv">、【NBA】、【游戏】、</em>和<em class="lv">、【网球】、</em>，那么像<em class="lv">、</em>、<em class="lv">、【网球】、</em>这样的单词的输出概率要比像<em class="lv">、</em>、<em class="lv">、【选举】、</em>这样的不相关单词的输出概率高得多。</li><li id="3244" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">而在使用<strong class="lb iu"> <em class="lv">连续跳格架构的第二选项中；</em> </strong>该模型使用当前单词来预测周围窗口中的上下文单词。跳格结构比更远的上下文单词更重视附近的上下文单词。输出概率将与在我们的输入单词附近找到每个词汇单词的可能性有关。例如，如果你给训练好的网络输入单词<em class="lv">"欧洲"</em>，那么像<em class="lv">"比利时"</em>和<em class="lv">"大陆"</em>这样的单词的输出概率要比像<em class="lv">"水果"</em>和<em class="lv">"猫"</em>这样的不相关单词的输出概率高得多。</li></ul><p id="2f29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这两个模型中，当我们说<strong class="lb iu"> <em class="lv">【环绕】</em> </strong>时，实际上有一个<strong class="lb iu"> <em class="lv">【窗口大小】</em> </strong>参数给算法。上下文窗口的大小限制了给定单词前后有多少单词将被包括作为给定单词的上下文单词。例如，<em class="lv"> 3 </em>的窗口大小将包括句子中每个观察到的单词左边的<em class="lv"> 3 </em>单词和右边的<em class="lv"> 3 </em>单词作为上下文。增加窗口大小会增加训练时间，因为需要训练更多的单词上下文对。它还可以捕获与当前单词不相关的上下文单词。减少上下文单词可以捕获单词和停用单词之间的关系，这不是优选的。</p><h1 id="72d8" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">4.Doc2Vec</h1><p id="6711" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Doc2Vec是另一种广泛使用的技术，它创建文档的嵌入，而不考虑其长度。Word2Vec为语料库中的每个<strong class="lb iu">单词</strong>计算特征向量，Doc2Vec为语料库中的每个<strong class="lb iu">文档</strong>计算特征向量。Doc2vec模型基于Word2Vec，只是在输入中增加了另一个向量<em class="lv">(段落ID) </em>。与Word2Vec类似，Doc2Vec模型可以依赖于两种模型架构中的一种:段落向量的分布式存储器版本<strong class="lb iu"> (PV-DM) </strong>和段落向量的分布式单词包版本<strong class="lb iu"> (PV-DBOW)。</strong></p><p id="e90a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下图中，我们展示了PV-DM的模型架构:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/479b68e0a559f41d51dab6a7562f335a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldPT3xU1TFFeWm0rEGPkmw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://shuzhanfan.github.io/2018/08/understanding-word2vec-and-doc2vec/" rel="noopener ugc nofollow" target="_blank"> Doc2Vec模型架构</a></p></figure><p id="f49f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图是基于CBOW模型的，但是我们没有使用邻近的单词来预测单词，而是添加了另一个特征向量，它是文档特有的。因此，当训练单词向量<em class="lv"> W </em>时，文档向量<em class="lv"> D </em>也被训练，并且在训练结束时，它保存文档的数字表示。</p><p id="e14c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入由单词向量和文档Id向量组成。单词vector是一个一维的hot vector<code class="fe nh ni nj nk b">1xV</code>。文档Id向量的维数为<code class="fe nh ni nj nk b">1xC</code>，其中C是文档总数。隐藏层的权重矩阵W的维数为<code class="fe nh ni nj nk b">NxV</code>。隐藏层的权重矩阵D的维数为<code class="fe nh ni nj nk b">CxN</code>。</p><h1 id="5e7f" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">5.Doc2vecC</h1><p id="000b" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Doc2vec通过捕获文档的语义来包含全局上下文，从而解决了doc 2 vec的问题。架构与Word2vec非常相似。在Doc2VecC中，使用文档中单词嵌入的平均值来表示全局上下文。但是，与Word2vec不同，在每次迭代中，单词都是从文档中随机抽取的(文档损坏)。然后，对这些单词的向量进行平均，以获得文档向量。该文档向量然后被用于使用局部上下文单词以及全局上下文来预测当前单词。全球环境是通过无偏见的辍学腐败产生的。此损坏的文档表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/7da74706175ece3cfe525a5420848cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ySBAJ2h45Umzd67VCVmgyA.png"/></div></div></figure><p id="27e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，给定局部上下文<em class="lv"> c_t </em>和全局上下文<em class="lv"> x_i </em>，观察到单词<em class="lv"> w_t </em>的概率被给出为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/0c60912477f4d82698f572a38ddea5f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V8NllcPNHtKUZYVBPisiMA.png"/></div></div></figure><p id="f10c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们使用神经网络学习了<em class="lv"> U </em>，每个文档就可以简单地表示为嵌入在该文档中的单词的平均值。因此，文档向量I被给定为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/11ccdc018e3a28a5dad94239dd23b9f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*rFGjq_MPowQQNdfxOjyydQ.png"/></div></figure><p id="5970" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型的输出共享数据集的全局和局部语义。平均还帮助我们获得看不见的文档的向量，因此解决了Doc2vec提出的问题。因为只有一小部分单词用于训练，所以与Doc2vec相比，训练时间要少得多。在任何文档中，单词都从其相邻单词中获得上下文。因此，为了提供更多的上下文，我们将公开可用的数据集附加到我们的数据集，这些数据集是用于商业、评论的文本数据以及用于个人、教育和学术目的的用户数据的子集。</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><h2 id="17fb" class="ny lx it bd ly nz oa dn mc ob oc dp mg li od oe mi lm of og mk lq oh oi mm oj bi translated">了解你的作者</h2><p id="16cd" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">拉贝赫·阿亚里是首席数据科学家，致力于信用风险建模和欺诈分析的应用人工智能问题，并在机器学习方面进行原创研究。我的专业领域包括使用深度神经网络的数据分析、机器学习、数据可视化、特征工程、线性/非线性回归、分类、离散优化、运筹学、进化算法和线性编程。随时给我留言<a class="ae ky" href="http://rabeh.ayari@polymtl.ca" rel="noopener ugc nofollow" target="_blank">这里</a>！</p></div></div>    
</body>
</html>