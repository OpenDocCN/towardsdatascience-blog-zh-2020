<html>
<head>
<title>GSoC 2020 with CERN-HSF | Dark Matter and Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GSoC 2020 与 CERN-HSF |暗物质和深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gsoc-2020-with-cern-hsf-dark-matter-and-deep-learning-eb611850bb79?source=collection_archive---------44-----------------------#2020-09-02">https://towardsdatascience.com/gsoc-2020-with-cern-hsf-dark-matter-and-deep-learning-eb611850bb79?source=collection_archive---------44-----------------------#2020-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="9ca1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个博客是我在 HSF 欧洲核子研究中心的谷歌代码之夏(GSoC) 2020 项目的一个非常简短的总结。今年是谷歌代码之夏 16 周年，共有 6，626 名学生提交了 8，902 份提案，其中 1，198 名学生获得了与 199 个组织合作的机会。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/3b805633347d0018a502219693a1e51b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rZ8cRGBG-wCbFGWt1vFpeQ.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">图片来源:<a class="ae ll" href="https://developers.google.com/open-source/gsoc/resources/marketing" rel="noopener ugc nofollow" target="_blank">谷歌代码之夏</a></p></figure><h1 id="8c6a" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">DeepLense 项目</h1><h2 id="835d" class="mk ln it bd lo ml mm dn ls mn mo dp lw kb mp mq ma kf mr ms me kj mt mu mi mv bi translated">项目描述</h2><p id="1239" class="pw-post-body-paragraph jq jr it js b jt mw jv jw jx mx jz ka kb my kd ke kf mz kh ki kj na kl km kn im bi translated">DeepLense 是一个深度学习管道，用于利用强引力透镜进行粒子暗物质搜索，是伞式组织<a class="ae ll" href="https://hepsoftwarefoundation.org/activities/gsoc.html" rel="noopener ugc nofollow" target="_blank"> CERN-HSF </a>的一部分。具体来说，我的项目是题为“<a class="ae ll" href="https://arxiv.org/abs/1909.07346" rel="noopener ugc nofollow" target="_blank">深度学习暗物质亚结构的形态学</a>”的论文中发表的工作的扩展，在该论文中，我的导师探索了使用最先进的监督深度学习模型，如 ResNet，对强透镜图像进行多类分类。</p><p id="4d37" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">自 1936 年爱因斯坦的计算中讨论引力透镜并于 1979 年发现以来，引力透镜一直是许多宇宙学实验和研究的基石，一个特别感兴趣的领域是通过强透镜图像中的亚结构研究暗物质。虽然统计和监督机器学习算法已经实现了这项任务，但无监督深度学习算法的潜力仍有待探索，并可能被证明在 LSST 数据的分析中至关重要。这个 GSoC 2020 项目的主要目标是设计一个基于 python 的框架，用于实现无监督的深度学习架构，以研究强透镜图像。</p><p id="f23a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更多细节参见论文“<a class="ae ll" href="https://arxiv.org/abs/2008.12731" rel="noopener ugc nofollow" target="_blank">无监督解码暗物质子结构</a>”。</p><h2 id="143d" class="mk ln it bd lo ml mm dn ls mn mo dp lw kb mp mq ma kf mr ms me kj mt mu mi mv bi translated">仓库</h2><p id="ea34" class="pw-post-body-paragraph jq jr it js b jt mw jv jw jx mx jz ka kb my kd ke kf mz kh ki kj na kl km kn im bi translated">我已经将我的工作编译成了两个开源库。第一个名为<a class="ae ll" href="https://github.com/DeepLense-Unsupervised/PyLensing" rel="noopener ugc nofollow" target="_blank"> PyLensing </a>，这是一个基于 PyAutoLens 模拟生成透镜图像的工具，第二个名为<a class="ae ll" href="https://github.com/DeepLense-Unsupervised/unsupervised-lensing" rel="noopener ugc nofollow" target="_blank"> Unsupervised Lensing </a>，这是一个基于 PyTorch 的工具，用于强透镜宇宙学中的无监督深度学习应用。</p><h2 id="572f" class="mk ln it bd lo ml mm dn ls mn mo dp lw kb mp mq ma kf mr ms me kj mt mu mi mv bi translated">关于我</h2><p id="2bcc" class="pw-post-body-paragraph jq jr it js b jt mw jv jw jx mx jz ka kb my kd ke kf mz kh ki kj na kl km kn im bi translated">我是 K·普拉纳斯·雷迪，荣誉理学硕士。物理学和理学士(荣誉)。)印度 Pilani Birla 理工学院(BITS)海德拉巴校区电气和电子工程专业。</p><h2 id="5b92" class="mk ln it bd lo ml mm dn ls mn mo dp lw kb mp mq ma kf mr ms me kj mt mu mi mv bi translated">为什么是 DeepLense？</h2><p id="314d" class="pw-post-body-paragraph jq jr it js b jt mw jv jw jx mx jz ka kb my kd ke kf mz kh ki kj na kl km kn im bi translated">作为一名物理系学生，我熟悉 CERN 的运作，并对该组织的许多相关项目有着基本的了解，我在宇宙学领域的深度学习应用方面做了大量工作。这一经历激励我为 DeepLense 项目做贡献。</p><h2 id="6b1d" class="mk ln it bd lo ml mm dn ls mn mo dp lw kb mp mq ma kf mr ms me kj mt mu mi mv bi translated"><strong class="ak">数据</strong></h2><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nb"><img src="../Images/1b91328dd3e757ca8341aee9c3d8d65e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PiTwpdt51UtT7MTFu2Gr9Q.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">不同子结构的模拟样品透镜图像。无(左)、漩涡(中)和球形(右)。|作者图片</p></figure><p id="b9a2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的数据集由三类组成，没有子结构的强透镜图像、涡旋子结构和球形子结构。考虑到具有子结构的样本是异常值，我们将在一组没有子结构的强透镜图像上训练我们的无监督模型，以解决异常检测的任务。</p><p id="be28" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们已经生成了两组透镜图像，模型 A 和模型 b。我们已经使用 python 包<a class="ae ll" href="https://github.com/Jammy2211/PyAutoLens" rel="noopener ugc nofollow" target="_blank"> PyAutoLens </a>进行模拟。这两个模型之间的区别在于，模型 A 的所有模拟图像都保持在固定的红移，而模型 B 允许透镜星系和透镜星系的红移在一个数值范围内浮动。两种模型的另一个区别是信噪比。模型 A 的图像 SNR ≈ 20，而模型 B 的构造使得模拟产生的图像 SNR 在 10 到 30 之间变化。关于模拟的更多细节可以在<a class="ae ll" href="https://arxiv.org/abs/2008.12731" rel="noopener ugc nofollow" target="_blank">的论文</a>中找到。</p><h1 id="6d02" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">无监督模型</h1><p id="2407" class="pw-post-body-paragraph jq jr it js b jt mw jv jw jx mx jz ka kb my kd ke kf mz kh ki kj na kl km kn im bi translated">我在异常检测的背景下研究并实现了各种无监督模型。在本节中，我将讨论四个模型，即深度卷积自动编码器(DCAE)、卷积变分自动编码器(VAE)、对抗自动编码器(AAE)和受限玻尔兹曼机器(RBM)，以及使用我的 PyTorch 工具<a class="ae ll" href="https://github.com/DeepLense-Unsupervised/unsupervised-lensing" rel="noopener ugc nofollow" target="_blank">无监督透镜</a>实现这些模型的代码。</p><h2 id="fc57" class="mk ln it bd lo ml mm dn ls mn mo dp lw kb mp mq ma kf mr ms me kj mt mu mi mv bi translated">深度卷积自动编码器(DCAE)</h2><p id="813d" class="pw-post-body-paragraph jq jr it js b jt mw jv jw jx mx jz ka kb my kd ke kf mz kh ki kj na kl km kn im bi translated">自动编码器是一种学习自身表示的神经网络，由编码器网络和解码器网络组成。编码器学习将输入样本映射到其维度低于输入样本维度的潜在向量，解码器网络学习从潜在维度重构输入。因此，自动编码器可以定性地理解为寻找给定类的最佳压缩表示的算法。</p><p id="533d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们首先考虑深度卷积自动编码器，它主要用于图像的特征提取和重建。在训练期间，我们利用均方误差(MSE)，</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nc"><img src="../Images/9cb1caaf85ba20cb76ee5df05d9cd167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8MkGCAXH3mmTg52stOyCQ@2x.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">MSE 损失|作者图片</p></figure><p id="3744" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为我们的重建损失，其中θ和θ’是真实的和重建的样本。</p><p id="458c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用<a class="ae ll" href="https://github.com/DeepLense-Unsupervised/unsupervised-lensing" rel="noopener ugc nofollow" target="_blank"> PyTorch 工具</a>实现:</p><pre class="kw kx ky kz gt nd ne nf ng aw nh bi"><span id="a846" class="mk ln it ne b gy ni nj l nk nl"><strong class="ne iu">from</strong> <strong class="ne iu">unsupervised_lensing.models</strong> <strong class="ne iu">import</strong> Convolutional_AE<br/><strong class="ne iu">from</strong> <strong class="ne iu">unsupervised_lensing.models.DCAE_Nets</strong> <strong class="ne iu">import</strong> *<br/><strong class="ne iu">from</strong> <strong class="ne iu">unsupervised_lensing.utils</strong> <strong class="ne iu">import</strong> loss_plotter <strong class="ne iu">as</strong> plt<br/><strong class="ne iu">from</strong> <strong class="ne iu">unsupervised_lensing.utils.EMD_Lensing</strong> <strong class="ne iu">import</strong> EMD</span><span id="c66f" class="mk ln it ne b gy nm nj l nk nl"><em class="nn"># Model Training</em><br/>out = Convolutional_AE.train(data_path='./Data/no_sub_train.npy', <br/>                             epochs=100,<br/>                             learning_rate=2e-3,<br/>                             optimizer='Adam',<br/>                             checkpoint_path='./Weights',         <br/>                             pretrain=<strong class="ne iu">True</strong>,                       <br/>                             pretrain_mode='transfer',            <br/>                             pretrain_model='A')                  <br/><br/><em class="nn"># Plot the training loss</em><br/>plt.plot_loss(out)</span><span id="bc3c" class="mk ln it ne b gy nm nj l nk nl"><em class="nn"># Model Validation</em><br/>recon_loss = Convolutional_AE.evaluate(data_path='./Data/no_sub_test.npy', <br/>                                       checkpoint_path='./Weights',        <br/>                                       out_path='./Results')               <br/><br/><em class="nn"># Plot the reconstruction loss</em><br/>plt.plot_dist(recon_loss)<br/><br/><em class="nn"># Calculate Wasserstein distance</em><br/>print(EMD(data_path='./Data/no_sub_test.npy', recon_path='./Results/Recon_samples.npy'))</span></pre><h2 id="a439" class="mk ln it bd lo ml mm dn ls mn mo dp lw kb mp mq ma kf mr ms me kj mt mu mi mv bi translated">卷积变分自动编码器(VAE)</h2><p id="75e7" class="pw-post-body-paragraph jq jr it js b jt mw jv jw jx mx jz ka kb my kd ke kf mz kh ki kj na kl km kn im bi translated">我们还考虑了一个变分自动编码器，它以 Kullback-Liebler (KL)散度的形式对潜在维度的表示引入了一个附加约束，</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nc"><img src="../Images/894516c7b9bf1d2afc9b86d02b60aefb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*098YHKdp5D_sJNk94u7DPg@2x.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">Kullback-Liebler (KL)散度|作者图片</p></figure><p id="b3e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中 P(x)是目标分布，Q(x)是算法学习的分布。r.h.s .上的第一项是 P 和 Q 之间的交叉熵，第二项是 P 的熵。因此，KL 散度对分布 Q 离 P 有多远的信息进行编码。在变分自动编码器的情况下，KL 散度用作正则化，以在潜在空间上施加先验。出于我们的目的，P 被选择为在潜在空间 z 上采用高斯先验的形式，而 Q 对应于由编码器表示的近似后验 q(z|x)。模型的总损失是重建(MSE)损失和 KL 散度之和。</p><p id="fa31" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用<a class="ae ll" href="https://github.com/DeepLense-Unsupervised/unsupervised-lensing" rel="noopener ugc nofollow" target="_blank"> PyTorch 工具</a>实现:</p><pre class="kw kx ky kz gt nd ne nf ng aw nh bi"><span id="e079" class="mk ln it ne b gy ni nj l nk nl">from unsupervised_lensing.models import Variational_AE<br/>from unsupervised_lensing.models.VAE_Nets import *<br/>from unsupervised_lensing.utils import loss_plotter as plt<br/>from unsupervised_lensing.utils.EMD_Lensing import EMD</span><span id="ca68" class="mk ln it ne b gy nm nj l nk nl"># Model Training<br/>out = Variational_AE.train(data_path='./Data/no_sub_train.npy', <br/>                           epochs=100,<br/>                           learning_rate=2e-3,<br/>                           optimizer='Adam',<br/>                           checkpoint_path='./Weights',         <br/>                           pretrain=True,                      <br/>                           pretrain_mode='transfer',            <br/>                           pretrain_model='A')</span><span id="0073" class="mk ln it ne b gy nm nj l nk nl"># Plot the training loss<br/>plt.plot_loss(out)</span><span id="22ab" class="mk ln it ne b gy nm nj l nk nl"># Model Validation<br/>recon_loss = Variational_AE.evaluate(data_path='./Data/no_sub_test.npy', <br/>                                     checkpoint_path='./Weights',        <br/>                                     out_path='./Results')</span><span id="99a0" class="mk ln it ne b gy nm nj l nk nl"># Plot the reconstruction loss<br/>plt.plot_dist(recon_loss)</span><span id="c61c" class="mk ln it ne b gy nm nj l nk nl"># Calculate Wasserstein distance<br/>print(EMD(data_path='./Data/no_sub_test.npy', recon_path='./Results/Recon_samples.npy'))</span></pre><h2 id="1ad9" class="mk ln it bd lo ml mm dn ls mn mo dp lw kb mp mq ma kf mr ms me kj mt mu mi mv bi translated">对抗性自动编码器(AAE)</h2><p id="900d" class="pw-post-body-paragraph jq jr it js b jt mw jv jw jx mx jz ka kb my kd ke kf mz kh ki kj na kl km kn im bi translated">最后，我们考虑一个对抗的自动编码器，它用对抗学习代替变分自动编码器的 KL 散度。我们训练鉴别器网络 D，以在由自动编码器 G 生成的样本和从对应于我们的训练数据的先验分布 P(z)获取的样本之间进行分类。该模型的总损耗是重建(MSE)损耗和鉴别器网络损耗之和，</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nc"><img src="../Images/ce5165453dee5acd0d29090fbbf46d1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bm5pOpVGv1fOddwlxXxbFA@2x.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">鉴别器丢失|作者图片</p></figure><p id="37e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们另外向自动编码器添加以下形式的正则化项，</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nc"><img src="../Images/331cf22f90f3c786b88343a4394e14cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kEbyHQljYac-mPSHdYxL-g@2x.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">作者图片</p></figure><p id="2871" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随着自动编码器在重建输入方面变得熟练，鉴别器的能力下降。然后，鉴别器网络通过提高其区分真实数据和生成数据的性能来进行迭代。</p><p id="9157" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用<a class="ae ll" href="https://github.com/DeepLense-Unsupervised/unsupervised-lensing" rel="noopener ugc nofollow" target="_blank"> PyTorch 工具</a>实现:</p><pre class="kw kx ky kz gt nd ne nf ng aw nh bi"><span id="46bf" class="mk ln it ne b gy ni nj l nk nl">from unsupervised_lensing.models import Adversarial_AE<br/>from unsupervised_lensing.models.AAE_Nets import *<br/>from unsupervised_lensing.utils import loss_plotter as plt<br/>from unsupervised_lensing.utils.EMD_Lensing import EMD</span><span id="f481" class="mk ln it ne b gy nm nj l nk nl"># Model Training<br/>out = Adversarial_AE.train(data_path='./Data/no_sub_train.npy', <br/>                           epochs=100,<br/>                           learning_rate=2e-3,<br/>                           optimizer='Adam',<br/>                           checkpoint_path='./Weights',         <br/>                           pretrain=True,                       <br/>                           pretrain_mode='transfer',            <br/>                           pretrain_model='A')</span><span id="d05b" class="mk ln it ne b gy nm nj l nk nl"># Plot the training loss<br/>plt.plot_loss(out)</span><span id="c876" class="mk ln it ne b gy nm nj l nk nl"># Model Validation<br/>recon_loss = Adversarial_AE.evaluate(data_path='./Data/no_sub_test.npy', <br/>                                     checkpoint_path='./Weights',        <br/>                                     out_path='./Results')</span><span id="e3d2" class="mk ln it ne b gy nm nj l nk nl"># Plot the reconstruction loss<br/>plt.plot_dist(recon_loss)</span><span id="5d85" class="mk ln it ne b gy nm nj l nk nl"># Calculate Wasserstein distance<br/>print(EMD(data_path='./Data/no_sub_test.npy', recon_path='./Results/Recon_samples.npy'))</span></pre><h2 id="f5da" class="mk ln it bd lo ml mm dn ls mn mo dp lw kb mp mq ma kf mr ms me kj mt mu mi mv bi translated">受限玻尔兹曼机(RBM)</h2><p id="6d38" class="pw-post-body-paragraph jq jr it js b jt mw jv jw jx mx jz ka kb my kd ke kf mz kh ki kj na kl km kn im bi translated">为了与我们的三个自动编码器模型进行比较，我们还训练了一个受限的波尔兹曼机器(RBM)，这是一个生成式人工神经网络算法，实现为一个二分图，学习输入的概率分布。RBMs 由两层组成，一个隐藏层和一个可见层，其中训练是在一个称为对比发散的过程中完成的。</p><p id="0c70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所有模型的详细架构可在<a class="ae ll" href="https://arxiv.org/abs/2008.12731" rel="noopener ugc nofollow" target="_blank">论文</a>的附录 B 中找到。</p><p id="6236" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用<a class="ae ll" href="https://github.com/DeepLense-Unsupervised/unsupervised-lensing" rel="noopener ugc nofollow" target="_blank"> PyTorch 工具</a>实现:</p><pre class="kw kx ky kz gt nd ne nf ng aw nh bi"><span id="64a2" class="mk ln it ne b gy ni nj l nk nl">from unsupervised_lensing.models import RBM_Model<br/>from unsupervised_lensing.models.RBM_Nets import *<br/>from unsupervised_lensing.utils import loss_plotter as plt<br/>from unsupervised_lensing.utils.EMD_Lensing import EMD</span><span id="583c" class="mk ln it ne b gy nm nj l nk nl"># Model Training<br/>out = RBM_Model.train(data_path='./Data/no_sub_train.npy', <br/>                      epochs=100,<br/>                      learning_rate=2e-3,<br/>                      optimizer='Adam',<br/>                      checkpoint_path='./Weights',         <br/>                      pretrain=True,                       <br/>                      pretrain_mode='transfer',            <br/>                      pretrain_model='A')</span><span id="061c" class="mk ln it ne b gy nm nj l nk nl"># Plot the training loss<br/>plt.plot_loss(out)</span><span id="4c69" class="mk ln it ne b gy nm nj l nk nl"># Model Validation<br/>recon_loss = RBM_Model.evaluate(data_path='./Data/no_sub_test.npy', <br/>                                checkpoint_path='./Weights',        <br/>                                out_path='./Results')</span><span id="6eb3" class="mk ln it ne b gy nm nj l nk nl"># Plot the reconstruction loss<br/>plt.plot_dist(recon_loss)</span><span id="3642" class="mk ln it ne b gy nm nj l nk nl"># Calculate Wasserstein distance<br/>print(EMD(data_path='./Data/no_sub_test.npy', recon_path='./Results/Recon_samples.npy'))</span></pre><h2 id="4ca1" class="mk ln it bd lo ml mm dn ls mn mo dp lw kb mp mq ma kf mr ms me kj mt mu mi mv bi translated">结果</h2><p id="d5a6" class="pw-post-body-paragraph jq jr it js b jt mw jv jw jx mx jz ka kb my kd ke kf mz kh ki kj na kl km kn im bi translated">我使用了 25，000 个没有子结构的样本和每类 2，500 个验证样本来训练和评估无监督模型。这些模型是使用 PyTorch 包实现的，并在单个 NVIDIA Tesla K80 GPU 上运行 500 个时代。我们利用 ROC 曲线下的面积(AUC)作为我们所有模型的分类器性能的度量。对于无监督模型，ROC 值是针对重建损失的设定阈值计算的。此外，我们还使用 Wasserstein 距离值来比较重建的保真度。一组更详细的结果可以在<a class="ae ll" href="https://arxiv.org/abs/2008.12731" rel="noopener ugc nofollow" target="_blank">的论文</a>中找到。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi no"><img src="../Images/ce4e7d29eb8139893e4e6be53a88dc40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gX7-wMNZ1H7U5U2aVRGdgg.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">无监督算法的 ROC-AUC 曲线。左边的图对应于模型 A，右边的图对应于模型 b。|图片由作者提供</p></figure><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi np"><img src="../Images/39c9cbd65435712bdf4081bf8da9df3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M2yXcltQdIvPaFrCMZxPvw.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">本分析中使用的体系结构的性能。ResNet 的 AUC 值是针对有和没有亚结构的图像分类计算的，因此它不是宏观平均 AUC。W₁是没有子结构的图像的平均第一瓦瑟斯坦距离。|作者图片</p></figure><h1 id="20e3" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">未来的工作和最后的想法</h1><p id="64d5" class="pw-post-body-paragraph jq jr it js b jt mw jv jw jx mx jz ka kb my kd ke kf mz kh ki kj na kl km kn im bi translated">尽管我们的无监督模型得到了一些非常有希望的结果，但是与 ResNet 模型的监督结果相比，它们的性能仍有进一步改进的空间。我目前正在探索基于图的模型的应用，因为它们在与稀疏数据集相关的任务中取得了成功，如稀疏 3D 点云和稀疏探测器数据。另一个未来的任务是使用迁移学习，通过从我们已经在模拟上训练过的模型开始，在真实数据上训练我们的架构。</p><p id="1fd7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我要感谢我的导师迈克尔·图米、谢尔盖·格莱泽、斯蒂芬·亚力山大和伊曼纽·乌赛，以及整个欧洲粒子物理研究所-HSF 社区对我的支持。我在 GSoC 项目上度过了一个美好的夏天。我还要感谢阿里·哈里里、汉娜·帕鲁尔和赖克·冯·克拉尔进行了有益的讨论。</p><p id="75b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">给以后想参加 GSoC 的同学们，不要把 GSoC 看成一个需要“破解”的竞赛或者考试。GSoC 是关于开源开发和成为优秀开发者社区的一部分。找到你热爱的项目，了解组织的需求。最重要的是，在社区论坛上保持活跃，定期与你的项目导师互动。</p><p id="a0fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢谷歌给我这么一个神奇的机会。</p><blockquote class="nq nr ns"><p id="d9dc" class="jq jr nn js b jt ju jv jw jx jy jz ka nt kc kd ke nu kg kh ki nv kk kl km kn im bi translated"><strong class="js iu">更新:</strong>deep lens 项目现在是<a class="ae ll" href="https://ml4sci.org/" rel="noopener ugc nofollow" target="_blank"> ML4SCI </a>保护伞组织的一部分。</p></blockquote><h1 id="ce7e" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">重要链接</h1><div class="nw nx gp gr ny nz"><a href="https://arxiv.org/abs/2008.12731" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd iu gy z fp oe fr fs of fu fw is bi translated">无监督解码暗物质亚结构</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">暗物质的身份仍然是当今物理学中最紧迫的问题之一。虽然许多有前途的黑暗…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://hepsoftwarefoundation.org/gsoc/2020/proposal_DEEPLENSE.html" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd iu gy z fp oe fr fs of fu fw is bi translated">用强引力透镜模拟暗物质</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">强引力透镜是对暗物质亚结构的一个有希望的探索，以更好地理解它的结构。</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">hepsoftwarefoundation.org</p></div></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://github.com/DeepLense-Unsupervised/unsupervised-lensing" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd iu gy z fp oe fr fs of fu fw is bi translated">深透镜-无监督/无监督-透镜</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">基于 PyTorch 的工具，用于强透镜宇宙学中的无监督深度学习应用</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">github.com</p></div></div><div class="oi l"><div class="oj l ok ol om oi on lf nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://github.com/DeepLense-Unsupervised/PyLensing" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd iu gy z fp oe fr fs of fu fw is bi translated">DeepLense 无人监督/PyLensing</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">一个基于 PyAutoLens 模拟生成透镜图像的工具</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">github.com</p></div></div><div class="oi l"><div class="oo l ok ol om oi on lf nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://www.linkedin.com/in/pranath-reddy/" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd iu gy z fp oe fr fs of fu fw is bi translated">印度特伦甘纳邦皮兰尼-海德拉巴普拉纳特·雷迪-比拉科技学院</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">查看 Pranath Reddy 在全球最大的职业社区 LinkedIn 上的个人资料。Pranath 有 2 份工作列在…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">www.linkedin.com</p></div></div><div class="oi l"><div class="op l ok ol om oi on lf nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://github.com/pranath-reddy" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd iu gy z fp oe fr fs of fu fw is bi translated">普拉纳斯-雷迪-概述</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">github.com</p></div></div><div class="oi l"><div class="oq l ok ol om oi on lf nz"/></div></div></a></div></div></div>    
</body>
</html>