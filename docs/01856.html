<html>
<head>
<title>Deep learning image enhancement insights on loss function engineering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习图像增强对损失函数工程的见解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-image-enhancement-insights-on-loss-function-engineering-f57ccbb585d7?source=collection_archive---------3-----------------------#2020-02-21">https://towardsdatascience.com/deep-learning-image-enhancement-insights-on-loss-function-engineering-f57ccbb585d7?source=collection_archive---------3-----------------------#2020-02-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7128" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对超分辨率、色彩和风格转换的技术和损失函数工程的见解。</h2></div><p id="4283" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些是我从使用深度学习进行各种图像处理技术的实验中获得的关于损失函数工程和深度神经网络架构的一些见解，这些技术包括超分辨率、着色(将颜色修补到黑白图像中)和风格转换。</p><p id="508d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章扩展了我的文章<a class="ae lb" rel="noopener" target="_blank" href="/loss-functions-based-on-feature-activation-and-style-loss-2f0b72fd32a9">中描述的损失函数基于特征激活和风格损失的损失函数</a>，我在我的研究中使用了基于深度学习的图像增强，用于<a class="ae lb" rel="noopener" target="_blank" href="/loss-functions-based-on-feature-activation-and-style-loss-2f0b72fd32a9">超分辨率</a>和黑白图像的<a class="ae lb" rel="noopener" target="_blank" href="/u-net-deep-learning-colourisation-of-greyscale-images-ee6c1c61aabe">着色</a>。事实证明，这些文章比我预想的要受欢迎得多，尤其是在过去的几个月里，我的文章在谷歌搜索“超级分辨率”的结果中名列前茅。</p><p id="abea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我在过去一年的研究中发现了一些有趣的发现和见解。有效损失函数在训练有效模型中是重要的，在某些情况下，它们可能比模型的架构更重要。</p><p id="0e44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理论上，损失函数也可以申请专利。随着深度学习的成长和兴起，未来看到损失函数工程师这样的工作成为角色也就不足为奇了。</p><p id="42d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文的后半部分，有一些使用这些损失函数技术生成图像的例子，用于超分辨率、着色和风格转换。</p><h1 id="da28" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">用于训练图像增强的深度学习模型的损失函数中的常见评估度量</h1><p id="ca02" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">对于图像增强，损失函数旨在评估模型的预测/生成输出离目标/地面真实图像有多远，以训练模型来最小化该损失。</p><p id="aa5f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">传统上，在学术和研究论文中，通常使用基于均方误差(MSE)、均方根误差(RMSE)或峰值信噪比(PSNR)的像素损失来对此进行评估。</p><p id="ef16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像素损失本质上是目标图像的像素距离预测/生成图像的像素有多远的度量。</p><h2 id="6e88" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">峰值信噪比(PSNR)</h2><p id="bc9b" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">来自维基百科:PSNR最容易通过均方差(MSE)来定义。给定无噪声的m×n单色图像I及其有噪声的近似K，MSE定义如下。</p><h2 id="3a30" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">均方差(MSE)/ L2损耗</h2><p id="2bd2" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">这里的数学让这比看起来更复杂。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/8f45404c0b676375ab777bc0f8dd79ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*0gbpwoMlFzM2MrHGR9EjZw.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">均方误差</p></figure><p id="89ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是在RGB图像中的三个通道之间进行比较。MSE用于比较目标图像的像素距离预测/生成图像的像素有多远。取每个像素差值的平均值，然后平方。</p><p id="7026" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于L2损失，目标是最小平方偏差，以最小化地面真实和预测/生成图像之间的平方差之和。</p><p id="98ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果取均方误差的平方根，这允许考虑所生成的图像和待评估的地面真实图像之间的距离。</p><h2 id="18a3" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">峰值信噪比定义(PSNR)</h2><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/167ac3540dffa747e1f24a4a6ab56aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*KDct9dNtMQzBS-gm8zgZ3g.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">峰值信噪比</p></figure><p id="74e6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">峰值信噪比定义(PSNR)最常用于对不同编解码器和图像压缩造成的质量损失进行质量评估，其中信号是原始图像，噪声是压缩图像产生的误差。</p><p id="612a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">PSNR非常常用于评估图像增强技术，例如超分辨率，其中信号是原始/地面真实图像，噪声是模型无法恢复的误差。</p><p id="7ec7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管PSNR是基于对数的指标，但它是基于MSE的。</p><h2 id="fcd8" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">MSE不是图像增强质量的良好指示。</h2><p id="81a6" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">为什么均方误差(MSE)不是图像增强质量的良好指标。</p><p id="f905" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用MSE或基于MSE的度量可能导致训练找到基于深度学习的模糊滤波器，因为这可能具有最低的损失和最容易收敛到最小化损失的解决方案。</p><p id="6bbc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最小化MSE的损失函数鼓励寻找通常过度平滑的似乎合理的解决方案的像素平均值，并且尽管最小化了损失，但是从吸引人类观察者的角度来看，所生成的图像将具有较差的感知质量。</p><p id="c2fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑具有盐和胡椒噪声的图像将导致比许多其他可能生成的图像更低的损失，这些图像从人类感知来看将更接近地面真实。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/299977408f4cbca38d7fd16d1b399d80.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*pwFbHBd9gcxhNxrW.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">椒盐噪声。来源:维基百科</p></figure><h1 id="ad7d" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">平均绝对误差/ L1损耗</h1><p id="cad9" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">对于L1损失，目标是最小绝对偏差(LAD ),以最小化地面真实和预测/生成图像之间的绝对差异之和。</p><p id="e35d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">MAE减少了平均误差，而MSE没有。相反，MSE非常容易受到异常值的影响。</p><p id="8974" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于图像增强，MAE将可能产生从人类观察者的角度看起来质量更高的图像。</p><h1 id="293f" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">PSNR基于MAE而非MSE</h1><p id="671d" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">如果使用MSE的上述PSNR的类似等式被改变为使用MAE，则在我的实验中，这产生了比PSNR更吸引人的增强图像，尽管不如以下度量有效。</p><h1 id="07c8" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结构相似指数(SSIM)</h1><p id="fb7d" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">结构相似性指数(SSIM)是一个感知指标。SSIM是基于图像中可见的结构。使用SSIM进行图像增强评估是因为对于一些研究人员来说，PSNR不再被认为是图像质量退化的可靠指标。这是一个感知指标，用于量化由处理导致的图像质量下降。</p><p id="8f52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我的实验再次使用SSIM作为损失函数的度量，结果得到比PSNR更吸引人的增强图像</p><h1 id="af64" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">特征损失(感知损失)</h1><p id="8cae" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">如果使用差异固定的预训练模型，则通过在模型内放置回调，可以在感兴趣的层在地面真实图像和预测图像之间比较来自该模型的激活。感兴趣的图层是那些紧接在提取信息的平均或最大池图层之前的图层。</p><p id="ac85" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些有趣的层是模型对特征的感知。</p><p id="f2d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以比较来自这些感兴趣层的激活的平均绝对误差(MAE ),然后这可以是构成损失函数一部分的度量。</p><p id="e521" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果生成的图像可以具有更清晰的特征，则从人类观察者对图像质量的观点来看，其余的像素通常不太重要。</p><p id="f833" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这在我的文章<a class="ae lb" rel="noopener" target="_blank" href="/loss-functions-based-on-feature-activation-and-style-loss-2f0b72fd32a9">基于特征激活和风格损失的损失函数</a>中有更详细的描述</p><p id="9695" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于超分辨率，我相信这是感知图像质量最重要的指标。然而，这种类型的损失度量在许多研究论文中并不流行，因为它不容易与其他研究进行比较。</p><h1 id="b0c3" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">克矩阵损失</h1><p id="2d77" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">很少使用的是Gram矩阵损失，它本质上是一种风格损失的评估形式。</p><p id="aa2d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">向量集的格拉姆矩阵是这些向量所有可能的内积的矩阵。</p><p id="649a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以获取来自每个通道的激活，将每个激活展平成一维向量，然后获取这些向量彼此的点积以形成格拉姆矩阵。点积表示每个通道组合的相关程度。如果一个通道指示纹理，而另一个通道指示明亮的颜色，则高点积将指示具有纹理的单元也倾向于具有明亮的颜色。</p><p id="a9cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这已被有效地用于评估风格损失。如果两幅图像具有相同的风格，那么它们将具有相似的gram矩阵。激活的展平从比较中移除了它们的空间信息。这是克矩阵损失对风格转移如此有效的原因之一。</p><p id="16c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果损失函数使用不同的固定预训练模型，来自该预训练模型的激活可以在地面真实图像和感兴趣层的预测图像之间进行比较。同样，感兴趣的图层是那些位于平均或最大汇集图层之前的图层，在这些图层中提取信息。不是查看特征/感知损失，而是可以比较激活的gram矩阵的平均绝对误差(MAE ),然后构成作为损失函数一部分的度量。</p><p id="156a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，与前面提到的特征损失一样，这种损失度量在许多研究论文中并不流行，因为它不容易与其他研究进行比较。</p><p id="adea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果与损失函数中的其他损失指标一起使用，则需要一个适当的乘数，以将其作为一个类似或适当的尺度。</p><p id="af1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">进一步在下面，这种损失度量在着色中的有效性被显示为非常令人印象深刻。</p><h1 id="c9c6" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">重量标准化与批次标准化</h1><p id="b6ca" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">批量归一化是用于训练深度神经网络的常见实践，包括用于图像生成的深度神经网络，包括生成对抗网络(GANs)。</p><p id="0f08" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在论文<a class="ae lb" href="https://arxiv.org/abs/1704.03971" rel="noopener ugc nofollow" target="_blank"> <em class="mz">关于生成对抗网络</em> </a> <em class="mz">【司涛祥，郝丽】</em>中批量和权重归一化的影响中，发现批量归一化会对训练模型的质量和训练过程的稳定性产生负面影响。最近的技术，重量标准化，被发现可以改善重建，训练速度，特别是GANs的稳定性。</p><p id="565b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我的实验中，我发现在训练超分辨率和着色的两个模型的情况下，权重归一化是有效的，不限于使用GANs进行训练。</p><h1 id="e0f4" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">自我关注</h1><p id="f217" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">在论文<a class="ae lb" href="https://arxiv.org/abs/1805.08318" rel="noopener ugc nofollow" target="_blank"> <em class="mz">自我注意生成对抗网络</em> </a> <em class="mz">(张寒、伊恩·古德菲勒、迪米特里斯·梅塔克萨斯、奥登纳)</em>中，自我注意的思想是基于自然语言处理中用于图像生成的注意机制来实现的</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi na"><img src="../Images/2901536549e9bd8c83d0f88e6a7e5120.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*FkGS1BZkXokjGd6cetaSeg.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">自我关注模块。来源:https://arxiv.org/pdf/1805.08318.pdf<a class="ae lb" href="https://arxiv.org/pdf/1805.08318.pdf" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="400e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自我注意允许使用来自所有特征位置的线索来生成细节。它有助于确保图像的远处部分彼此一致，而传统上这是卷积神经网络(CNN)的一个缺点。</p><p id="ce4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我的没有GANs的深度学习超分辨率和着色的实验中，我发现自我关注提高了模型基于损失标准和从人类评估角度生成图像的性能。这在着色时更加明显，图像不同部分的相似特征具有更好的一致性。</p><h1 id="c0d6" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">超分辨率</h1><h2 id="b6e5" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">光谱归一化</h2><p id="3242" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">在论文<a class="ae lb" href="https://arxiv.org/abs/1802.05957" rel="noopener ugc nofollow" target="_blank"><em class="mz"/></a><em class="mz">(Takeru Miyato，Toshiki Kataoka，Masanori Koyama，Yuichi Yoshida) </em>中，提出了一种称为谱归一化的新的权重归一化技术来稳定鉴别器的训练。克里斯蒂安·科斯格罗维的一篇文章<a class="ae lb" href="https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html" rel="noopener ugc nofollow" target="_blank">解释了什么是光谱归一化，通过限制权重的李普希兹常数来控制梯度。</a></p><p id="5b82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在论文<a class="ae lb" href="https://arxiv.org/abs/1805.08318" rel="noopener ugc nofollow" target="_blank"><em class="mz"/></a><em class="mz">(张寒，伊恩·古德菲勒，迪米特里斯·梅塔克萨斯，奥登纳)</em>中，鉴别器和生成器都使用了谱归一化。</p><p id="0449" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我最近对基于深度学习的超分辨率进行的实验(没有GANs)中，我发现光谱归一化比重量归一化和批量归一化更有效地提高了模型生成图像的性能——基于损失标准和从人类评估的角度。</p><p id="1b4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我对黑白图像进行基于深度学习的着色(颜色修复)的实验中(同样没有GANs)，我发现光谱归一化<strong class="kh ir"> <em class="mz">不如</em> </strong>批量归一化有效。</p><h2 id="d36d" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">2倍超分辨率:基于ResNet 34的编码器和基于U-Net架构的解码器的改进型号</h2><p id="20ff" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">该模型的目标是生成更大的高保真图像，其高度和宽度是输入图像的两倍，像素是输入图像的四倍。对于一个训练有素的模特来说，这是一项艰巨的任务。</p><p id="5c9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">生成这些超分辨率图像的模型在U-Net架构中具有基于ResNet34的编码器和解码器。在我过去的实验中，这个改进的模型被训练用于使用损失函数的超分辨率，该损失函数组合了SSIM(结构相似性指数)、来自固定预训练的VGG16模型的特征/感知损失和来自固定预训练的VGG16模型的gram矩阵(风格)损失，以及小部分MAE(平均绝对误差)损失。</p><p id="1264" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这也在模型的训练中使用了光谱归一化和自我注意。</p><p id="f1da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下面的例子中，低分辨率输入图像在左边，生成的预测在中间，地面真实/目标图像在右边。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nb"><img src="../Images/b17d4551f55d6d5f4af21debaf3e7abb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jo-rwVkUfti3iMRq"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">2倍的超分辨率源于一个改进的超分辨率模型，该模型采用U-Net架构和基于ResNet34的编码器/解码器。低分辨率图像(左)，生成的预测(中)，地面实况(右)</p></figure><p id="02bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你仔细观察第一排冲浪板上的岩石和手臂，这些特征清晰而鲜明。如果你仔细观察屋顶、窗户和悬挂在上面的纺织品，同样的情况也可以在最下面一排的图片中看到——房屋。图像底部中心的栅栏线清晰可见。</p><h1 id="2039" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">着色/将颜色修补成黑白图像。</h1><p id="c927" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">这里的着色实验使用了比我之前的实验小得多的U-Net架构，基于基于ResNet18的编码器和解码器，而不是使用基于ResNet34的编码器和解码器。</p><h2 id="d265" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">改进的基于ResNet18的编码器和解码器U-Net。</h2><p id="8a21" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">与我过去的实验相比，这个改进的模型被训练用于使用损失函数的图像着色，该损失函数结合了SSIM(结构相似性指数)、来自固定的预训练VGG16模型的少量特征/感知损失和来自固定的预训练VGG16模型的gram矩阵(风格)损失，以及非常小比例的MAE(平均绝对误差)损失。</p><p id="236e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个模型的训练使用了体重标准化和自我关注。此外，以16位浮点精度进行训练有助于进一步减少损失。与此结合的组合或混合损失函数允许模型最小化不同因素的损失，从而产生有吸引力的结果。</p><p id="60ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下面的例子中，低分辨率输入图像在左边，生成的预测在中间，地面真实/目标图像在右边。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi ng"><img src="../Images/cb974eece2fd00bfd0704c317a9fe5ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*416I2gUG8a4nyZs9"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">色彩化产生于具有U-Net架构的模型，该U-Net架构具有利用混合损失函数训练的基于ResNet18的编码器/解码器。低分辨率图像(左)，生成的预测(中)，地面实况(右)</p></figure><p id="1dcc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在最后一个例子中，生成的熊的图像比地面的真实情况更具视觉吸引力。</p><h2 id="3a31" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">Gram (style)损失函数用于训练具有基于ResNet18的编码器和解码器U-Net架构的模型</h2><p id="bff3" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">这一模型的结果令人惊讶地有效。用于训练该模型的损失函数不使用MSE或MAE像素损失。损失函数也没有使用PSNR、SSIM或特征损失。损失函数仅将感兴趣特征的gram矩阵与ImageNet预先训练的固定模型进行比较。</p><p id="5f50" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果，虽然不如一个模型训练有素的其他损失，在我看来，是相当了不起的，修复了纯粹来自训练的颜色，以尽量减少风格的损失。</p><p id="97f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下面的例子中，低分辨率输入图像在左边，生成的预测在中间，地面真实/目标图像在右边。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nh"><img src="../Images/f148c83cf574744286b365698e9e6480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f7nagrq-VypjXr6Y"/></div></div></figure><p id="ac23" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我看来，一部分生成的图像看起来比真实的地面更具视觉吸引力。</p><h1 id="8561" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">4倍超分辨率实验</h1><p id="23ca" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">该模型的目标是生成更大的高保真图像，其高度和宽度是输入图像的四倍，像素是输入图像的16倍。对于一个模特来说，这是一个很难完成的任务。</p><p id="cff3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练使用了与本文前面描述的2x超分辨率实验相似的损失函数度量。</p><p id="4452" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下图中，低分辨率输入图像位于左侧，生成的预测位于中间，地面真实/目标图像位于右侧。考虑到输入图像的低质量，该模型正在执行一项令人印象深刻的任务来提高图像的质量。</p><p id="300d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的图像中，灌木、波浪和岩石明显更清晰，图像中的两个人也是如此。在下图中，建筑线条和屋顶的质量再次明显提高。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nb"><img src="../Images/1e5c97428493d125c8669bd1e9dca0b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xbud6au02_jiyZFH"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">4倍超分辨率源于采用U-Net架构和基于ResNet34的编码器/解码器的超分辨率模型。低分辨率图像(左)，生成的预测(中)，地面实况(右)</p></figure><h1 id="71eb" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">基于U-Net的风格迁移实验</h1><p id="572b" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">我最近做了一些实验，试图评估上述技术是否可以应用于风格转换。这些实验给出了一些有趣的结果，使用了U-Net架构、特征损失和风格损失度量。</p><p id="8665" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">风格转移是一个有趣的、不同的并且难以解决的问题，因为与其他图像增强训练不同，在训练期间没有单一的基础事实/目标可以用来与生成的模型进行比较。</p><p id="c673" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些实验训练了基于ResNet34编码器和解码器的具有U-Net架构的模型。网络的编码器部分基于在ImageNet上预先训练的模型。</p><p id="d490" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">损失函数使用特征损失和gram矩阵损失的度量的组合，比较原始和生成/风格化图像的固定预训练VGG-16模型内感兴趣层的激活。</p><p id="4372" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我一直在试验的U-Net架构的身份/跳过连接可以允许学习一种不寻常的算法，该算法使用图像的一组高度样式化的部分，而图像的大部分保持不样式化。这是模型发现在损失函数度量中最有效地最小化误差。与特征/感知损失相比，需要对gram矩阵/风格损失进行仔细加权，以产生吸引人的结果。</p><h2 id="d287" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">风格转移实验——好结果</h2><p id="30cc" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">这些风格转移实验的例子试图转移文森特·梵高的<a class="ae lb" href="https://www.vangoghmuseum.nl/en/collection/s0196V1962" rel="noopener ugc nofollow" target="_blank">疯人院花园</a>的风格，这是我最喜欢的画作之一，我今年早些时候在阿姆斯特丹有幸看到过。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/7bf100216c6037082758314c7f7b8f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*cpYiZn10RBWfLBPp8pcfUA.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">文森特·梵高的<a class="ae lb" href="https://www.vangoghmuseum.nl/en/collection/s0196V1962" rel="noopener ugc nofollow" target="_blank">疯人院花园</a></p></figure><p id="e560" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在花了相当多的时间调整损失函数并对其进行微调后，经过训练的模型学会了生成有趣的图像，并将梵高画作的风格转移到这些图像上。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nj"><img src="../Images/1954d2dfb976e842aaf41d18e969dcbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HqUXJR1VxSk35Fdgj1IeuQ.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">基于文森特·梵高的<a class="ae lb" href="https://www.vangoghmuseum.nl/en/collection/s0196V1962" rel="noopener ugc nofollow" target="_blank">疯人院花园</a>的风格转变的一个较好的例子。原始图像在左侧，样式化/生成的图像在右侧</p></figure><p id="0c38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来自同一个训练过的模型的另一个非常有趣的例子是这个生成的狼的图像，模型决定给它发光的眼睛，我认为这可能总结了原画中的情绪，它的不祥的人潜伏在树上。发光的眼睛是模型特征检测的结果。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nk"><img src="../Images/501d158166dc174e1f85d610bcd9dc9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MeQnr_VrzFc3BvQWDZ78Ew.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">另一个更好的风格转变的例子基于文森特·梵高的<a class="ae lb" href="https://www.vangoghmuseum.nl/en/collection/s0196V1962" rel="noopener ugc nofollow" target="_blank">疯人院花园</a>——一只眼睛发光的狼。原始图像在左侧，样式化/生成的图像在右侧</p></figure><h2 id="278e" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">风格转移实验—其他结果</h2><p id="dbc0" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">这些例子试图传达文森特·梵高的《星夜》的风格。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nl"><img src="../Images/5f1d34deef1beddaf82b546b120369f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACIP_FThCGTMbFahy4B7xg.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">文森特·梵高的《星夜》</p></figure><p id="3f27" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来自训练模型的风格化生成的图像具有绘画的风格和图像，尽管结果似乎过度使用了某些风格。如果对损失函数进行调整，我敢肯定会发现不同的更有吸引力的结果。我怀疑这种风格的图像是U-Net的跳过/身份连接的结果。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nm"><img src="../Images/7d819b603e2cd55972d2e15e79e1826e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q_1VrK5vZBNUaRg7pHHeKQ.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">基于文森特·梵高的<a class="ae lb" href="https://www.vangoghmuseum.nl/en/collection/s0196V1962" rel="noopener ugc nofollow" target="_blank">疯人院花园</a>的风格转换示例。原始图像在左侧，样式化/生成的图像在右侧</p></figure><p id="87e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，从训练模型生成的风格化图像既有绘画风格也有图像。风格化的效果非常有趣，如果对损失函数进行更多的调整和重新训练，我相信会产生更吸引人的结果。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nn"><img src="../Images/097e0251a9be2d38e2245ddad8de5ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dhSQYudHwBI5NlAAV6udaw.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">基于文森特·梵高<a class="ae lb" href="https://www.vangoghmuseum.nl/en/collection/s0196V1962" rel="noopener ugc nofollow" target="_blank">疯人院花园</a>的风格转移示例。原始图像在左侧，样式化/生成的图像在右侧</p></figure><h2 id="bf63" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">风格转移实验——最糟糕的结果</h2><p id="918b" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">这些实验部分是由于损失函数的风格损失部分权重太大。我也相信这个模型被训练了太长时间和/或学习率太高。</p><p id="a372" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你仔细观察，你仍然可以在图像中看到蝴蝶的轮廓，尽管它和它的特征已经大部分丢失了。左侧和顶部有丰富的样式信息。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi no"><img src="../Images/49f5e93850876fd696850b593889a81c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uRnr8lt-MCGG_ZNQFIE5-A.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">基于文森特·梵高的<a class="ae lb" href="https://www.vangoghmuseum.nl/en/collection/s0196V1962" rel="noopener ugc nofollow" target="_blank">疯人院花园</a>的风格转移的失败例子。原始图像在左侧，样式化/生成的图像在右侧</p></figure><p id="8ce3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你非常仔细地看，你仍然可以看到图像中一些特征的轮廓，尽管几乎看不到。同样，左侧和顶部生成了丰富的样式信息。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi no"><img src="../Images/6fef6554f3bfff81d078405ba79883bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BtGHdarP7eFxhtBR-vYfAQ.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">基于文森特·梵高的<a class="ae lb" href="https://www.vangoghmuseum.nl/en/collection/s0196V1962" rel="noopener ugc nofollow" target="_blank">疯人院花园</a>的风格转移的失败例子。原始图像在左侧，样式化/生成的图像在右侧</p></figure><p id="2783" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些例子说明了训练一个有效的模型有多困难，尤其是当没有单一目标或基础事实来评估每个训练图像的模型时。</p><h1 id="686c" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结论</h1><p id="ac04" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">与深度学习图像增强研究中传统使用的度量相比，存在可以产生视觉上更吸引人的生成图像的损失函数。作为损失函数度量的一部分，使用和比较来自单独的固定预训练模型的激活非常有效。</p><p id="0e1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望有更多的时间来重温这篇文章，以提供更好的图像生成结果，更多的例子和这里概述的技术和结果的扩展解释。此外，当时间允许时，我会将这些模型中最好的放入生产环境中使用。</p><p id="cc26" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我也希望能找到一个我能与之合作的出版物或学术机构，来撰写和发表一篇详细介绍这些实验和结果的论文。</p><p id="e5fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我要感谢杰瑞米·霍华德和杰森·安蒂奇的灵感，他们试图训练基于深度学习的模型来执行这些超级分辨率和着色任务。</p></div></div>    
</body>
</html>