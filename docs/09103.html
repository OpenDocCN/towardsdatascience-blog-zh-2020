<html>
<head>
<title>LSTM Text Classification Using Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 Pytorch 的 LSTM 文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0?source=collection_archive---------0-----------------------#2020-06-30">https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0?source=collection_archive---------0-----------------------#2020-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fa6f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一步一步的指导你如何在 Pytorch 中建立一个双向 LSTM！</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/31f8bb2a25aa4a0d603bd675bfe406ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e28BCwajzG_9pmQYU9YhZQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">克里斯托夫·高尔在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="709b" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">介绍</h1><p id="0edf" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">欢迎来到本教程！本教程将在几分钟内教你如何为文本分类构建一个双向<a class="ae lf" href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory" rel="noopener ugc nofollow" target="_blank"><strong class="ma iu">【LSTM】</strong></a>。如果你还没有看过我之前关于<a class="ae lf" rel="noopener" target="_blank" href="/bert-text-classification-using-pytorch-723dfb8b6b5b"> <strong class="ma iu"> BERT 文本分类</strong> </a>的文章，这篇教程包含了和那篇类似的代码，但是包含了一些支持 LSTM 的修改。本文还解释了我是如何对两篇文章中使用的数据集进行预处理的，这是来自 Kaggle 的<a class="ae lf" href="https://www.kaggle.com/nopdev/real-and-fake-news-dataset" rel="noopener ugc nofollow" target="_blank"> <strong class="ma iu">真假新闻数据集</strong> </a>。</p><p id="475b" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">首先，什么是 LSTM，我们为什么要使用它？LSTM 代表<strong class="ma iu">长短期记忆网络</strong>，属于一个更大的神经网络类别，称为<strong class="ma iu">递归神经网络(RNN) </strong>。与传统的 RNN 相比，它的主要优势在于，它能够通过复杂的架构更好地处理长期依赖性，该架构包括三个不同的门:输入门、输出门和遗忘门。这三个门共同决定在任意时间内 LSTM 细胞中哪些信息该记住，哪些该忘记。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mz"><img src="../Images/ea3052e376f5bcba2ff5c1af3342acdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eEIAtVm41hnA7Sb9O3z1xg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">LSTM 细胞</p></figure><p id="8a07" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">现在，我们对 LSTM 有了更多的了解，让我们集中在如何实现它的文本分类。本教程分为以下步骤:</p><ol class=""><li id="35bd" class="na nb it ma b mb mu me mv mh nc ml nd mp ne mt nf ng nh ni bi translated">预处理数据集</li><li id="2198" class="na nb it ma b mb nj me nk mh nl ml nm mp nn mt nf ng nh ni bi translated">导入库</li><li id="a085" class="na nb it ma b mb nj me nk mh nl ml nm mp nn mt nf ng nh ni bi translated">加载数据集</li><li id="6de3" class="na nb it ma b mb nj me nk mh nl ml nm mp nn mt nf ng nh ni bi translated">建立模型</li><li id="d1e6" class="na nb it ma b mb nj me nk mh nl ml nm mp nn mt nf ng nh ni bi translated">培养</li><li id="77f2" class="na nb it ma b mb nj me nk mh nl ml nm mp nn mt nf ng nh ni bi translated">估价</li></ol><p id="4619" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">在我们开始学习本教程之前，您可以在这里访问本文中的代码:</p><ul class=""><li id="9035" class="na nb it ma b mb mu me mv mh nc ml nd mp ne mt no ng nh ni bi translated"><a class="ae lf" href="https://colab.research.google.com/drive/1gX8ERqDMQGTO1fKJwELFO11f66xuKVyP?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="ma iu">假新闻数据集的预处理</strong> </a></li><li id="a53e" class="na nb it ma b mb nj me nk mh nl ml nm mp nn mt no ng nh ni bi translated"><a class="ae lf" href="https://colab.research.google.com/drive/1cpn6pk2J4liha9jgDLNWhEWeWJb2cdch?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="ma iu"> LSTM 文本分类 Google Colab </strong> </a></li></ul><h1 id="f62c" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">步骤 1:预处理数据集</h1><p id="a34a" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">原始数据集如下所示:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi np"><img src="../Images/ab2ef5b81d417823293c6d54cbda9651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y137EUR9-rNDxVJfZ07UOA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">数据集概述</p></figure><p id="4928" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">数据集包含任意的索引、标题、文本和相应的标签。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="f2d9" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对于预处理，我们导入 Pandas 和 Sklearn，并为路径、训练验证和测试比率定义一些变量，以及用于将每个句子切割到第一个<code class="fe ns nt nu nv b">first_n_words</code>单词的<code class="fe ns nt nu nv b">trim_string</code>函数。修剪数据集中的样本不是必需的，但是它能够更快地训练较重的模型，并且通常足以预测结果。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="a1bf" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">接下来，我们将<em class="nw">真实的</em>转换为 0，将<em class="nw">虚假的</em>转换为 1，连接<em class="nw">标题</em>和<em class="nw">文本</em>以形成新列<em class="nw">标题文本</em>(我们使用标题和文本来决定结果)，删除带有空文本的行，将每个样本修剪为<code class="fe ns nt nu nv b">first_n_words</code>，并根据<code class="fe ns nt nu nv b">train_test_ratio</code>和<code class="fe ns nt nu nv b">train_valid_ratio</code>分割数据集。我们将生成的数据帧保存到<em class="nw">中。csv </em>文件，获取<em class="nw"> train.csv </em>、<em class="nw"> valid.csv </em>和<em class="nw"> test.csv </em>。</p><h1 id="65b2" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">步骤 2:导入库</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="4fe0" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们导入 Pytorch 用于模型构建，torchText 用于加载数据，matplotlib 用于绘图，sklearn 用于评估。</p><h1 id="4010" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">步骤 3:加载数据集</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="a86d" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">首先，我们使用 torchText 为数据集中的<em class="nw">标签</em>创建一个标签字段，并为<em class="nw">标题</em>、<em class="nw">文本</em>和<em class="nw">标题文本</em>创建一个文本字段。然后，我们通过将 TabularDataset 指向包含<em class="nw"> train.csv </em>、<em class="nw"> valid.csv </em>和<em class="nw"> test.csv </em>数据集文件的路径来构建 tabular dataset。我们创建加载数据的 train、valid 和 test 迭代器，最后，使用 train 迭代器构建词汇表(只计算最小频率为 3 的标记)。</p><h1 id="e414" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">步骤 4:构建模型</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="d3b2" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们构造了继承自<em class="nw"> nn 的 LSTM 类。模块</em>。在 LSTM 内部，我们构建了一个嵌入层，然后是一个双 LSTM 层，最后是一个完全连接的线性层。在<em class="nw"> forward </em>函数中，我们通过嵌入层传递文本 id 以获得嵌入，通过容纳变长序列的 LSTM 传递，从两个方向学习，通过完全连接的线性层传递，最后<em class="nw"> sigmoid </em>以获得序列属于假的概率(为 1)。</p><h1 id="5e67" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">第五步:培训</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="ba6e" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">在训练之前，我们为检查点和指标构建保存和加载函数。对于检查点，保存模型参数和优化器；对于度量标准，将保存列车损失、有效损失和全局步骤，以便稍后可以轻松地重新构建图表。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="73e5" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们用 10 个历元训练 LSTM，每当超参数设置实现最佳(最低)验证损失时，保存检查点和指标。以下是培训期间的输出:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nx"><img src="../Images/b976e05394aac1e0b879e5235b4d2f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pT71oFF_5LGl8neJmy7g6w.png"/></div></div></figure><p id="e649" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">整个训练过程在 Google Colab 上很快。训练了不到两分钟！</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="cc86" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">一旦我们完成了训练，我们可以加载之前保存的指标，并输出一个图表，显示整个时间内的训练损失和验证损失。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ca"><img src="../Images/7f219dbc47fbdd8c74b0018db4451f52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*apjCPsFfm3nK0G2cd149OA.png"/></div></div></figure><h1 id="a836" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">第六步:评估</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="ddef" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">最后，为了进行评估，我们选择之前保存的最佳模型，并根据我们的测试数据集对其进行评估。我们使用默认阈值 0.5 来决定何时将样本归类为假样本。如果模型输出大于 0.5，我们把那个新闻归类为假的；否则，真实的。我们输出分类报告，指出每个类别的精确度、召回率和 F1 值，以及总体精确度。我们还输出混淆矩阵。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ny"><img src="../Images/15ecc167a3e447d7de4ed55fba353bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qdvRlrUgEW95DfhIiTo5cQ.png"/></div></div></figure><p id="1254" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们可以看到，使用一层双 LSTM，我们可以在假新闻检测任务上实现 77.53%的准确率。</p><h1 id="de0f" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">结论</h1><p id="5378" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">本教程一步一步地解释了如何使用 Pytorch 实现你自己的文本分类 LSTM 模型。我们发现，双 LSTM 在假新闻检测方面达到了可接受的准确率，但仍有改进的空间。如果你想要更有竞争力的表现，可以看看我之前关于<a class="ae lf" rel="noopener" target="_blank" href="/bert-text-classification-using-pytorch-723dfb8b6b5b"> <strong class="ma iu"> BERT 文本分类</strong> </a>的文章！</p><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/bert-text-classification-using-pytorch-723dfb8b6b5b"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">使用 Pytorch 的 BERT 文本分类</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">文本分类是自然语言处理中的一项常见任务。我们应用 BERT，一个流行的变压器模型，对假新闻检测使用…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq kz oc"/></div></div></a></div><p id="398a" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">如果你想了解更多关于现代自然语言处理和深度学习的知识，请关注我的最新文章:)</p><h1 id="fd03" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">参考</h1><p id="0c02" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">[1] S. Hochreiter，J. Schmidhuber，<a class="ae lf" href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory" rel="noopener ugc nofollow" target="_blank">长短期记忆</a> (1997)，神经计算</p></div></div>    
</body>
</html>