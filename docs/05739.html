<html>
<head>
<title>Surprise And The Tale Of Entropy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">惊奇和熵的故事</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/surprise-and-the-tale-of-entropy-c127da84937a?source=collection_archive---------57-----------------------#2020-05-12">https://towardsdatascience.com/surprise-and-the-tale-of-entropy-c127da84937a?source=collection_archive---------57-----------------------#2020-05-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/b3ecdfba40e69cc7f49abfb29e223abb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YCFPWoqr3yRDS44PDEtgGg.png"/></div></div></figure><p id="e48d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">今天，我从一个有趣的问题开始我的小文章。假设有一天，外面正下着雨，你走出家门，发现你的露天庭院完全干涸了。你的第一反应会是什么？</p><p id="7ce2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">惊喜！</p><p id="dd51" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为什么？</p><p id="90ac" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因为，当外面下雨时，你已经知道你的院子会是湿的。如果你已经看到了这一点，那么你就不会“收到”任何新的信息，因此就不会有惊讶的成分。但是既然你发现它没有湿，你就“接收”了一些信息。这让人感到意外。</p><p id="08b3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，我们可以试着在这种情况下更正式一点。假设你已经知道庭院有 0.1%的可能是干的，0.9%的可能是湿的。<em class="kz">(你注意到价值观的选择了吗？我选择它们的方式是，如果你把它们加起来，值就是 1.0。你能解释一下吗？)</em>所以，在这个设置中，我们有两个变量。</p><ol class=""><li id="8806" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">x(庭院干燥的可能性)</li><li id="10ba" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">y(庭院潮湿的可能性)</li></ol><p id="d130" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，让我问这个问题(即使你没有完全理解它的意思，请耐心听我说)变量 X 的“熵”是什么？</p><p id="a5e5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在我们做数学之前，让我告诉你熵是变量不确定性的度量。越不确定，熵越高。那么基于这种直觉，你认为变量的熵会是多少？在这里停下来想一想。</p></div><div class="ab cl lo lp hx lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="im in io ip iq"><p id="4cc2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">好的。现在，让我们做数学。(如果对 Claude Shannon 的原论文感兴趣，可以查看一下<a class="ae lv" href="http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>我们说一个变量的总熵等于——</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/df3b78c2af6d47cbd8f2edfcc600dc6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*E8sQfLyvp-IVBiC_Xdc3Tg.png"/></div></figure><p id="653f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中 p(x)代表事件的概率。</p><p id="d211" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面来一点 Python:)</p><p id="2e7a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们知道 X 的概率是 0.1 所以，</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="8629" class="mg mh it mc b gy mi mj l mk ml">In [1]: import math</span><span id="cc74" class="mg mh it mc b gy mm mj l mk ml">In [2]: (0.1) * round(math.log2(0.1), 2)<br/>Out[2]: -0.332</span></pre><p id="4f13" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">根据同样的定律，Y 的熵是—</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="d582" class="mg mh it mc b gy mi mj l mk ml">In [3]: (0.9) * round(math.log2(0.9), 2)<br/>Out[3]: -0.135</span></pre><p id="a5f6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">正如所料，X 的熵大于 Y 的熵(如果我们暂时忽略负号的话)。所以，我们可以从这个例子中看到，一个事件发生的可能性越小，这个事件的熵就越大。</p><p id="0a65" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果我们愿意，我们也可以从这个角度来考虑——我们对一个事件的信息越少，也就是说，它越令人惊讶，这个事件的熵就越高。</p><p id="c9b6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，我们要问另一个问题。系统的总熵是多少？其实很简单。我们是这样衡量的—</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/b0075f60ed29ca51a7cdd9cf2e1f8f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*HSucYAtRxsvZsO3C8bxb3g.png"/></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">系统的总熵</p></figure><p id="9ee9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以，基本上我们要把 X 和 Y 的值加起来，然后取反。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="afe7" class="mg mh it mc b gy mi mj l mk ml">In [4]: - (-0.332 + (-0.135))<br/>Out[4]: 0.467</span></pre><p id="279f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们用称为“比特”的单位来表示这个量。所以我们可以说这个系统有 0.467 比特的熵。因此，你可以从这个系统接收那么多的信息。</p></div><div class="ab cl lo lp hx lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="im in io ip iq"><p id="f0d9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了完成这篇文章，我们要测量一次公平抛硬币的熵。它有多少比特的熵？</p><p id="878e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在公平抛硬币的情况下，我们可以像下面这样列出正面和反面的概率</p><ol class=""><li id="14fc" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">P(H) = 0.5</li><li id="6105" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">P(T) = 0.5</li></ol><p id="9de4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以，应用上面的方法，我们可以计算熵。它将是 1 比特。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="ac22" class="mg mh it mc b gy mi mj l mk ml">In [5]: - ( (0.5 * math.log2(0.5)) + (0.5 * math.log2(0.5)))<br/>Out[5]: 1.0</span></pre><p id="1551" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这并不奇怪！</p></div><div class="ab cl lo lp hx lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="im in io ip iq"><p id="41aa" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">希望你喜欢这个关于熵和信息论的小介绍。这个小等式支撑着我们现在的整个通信系统。有个想法是好事。</p><p id="2a1a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你喜欢，请按拍手图标多次。这会鼓励我写更多。此外，你可以在这里关注我的未来文章，撰写关于信息论、复杂性理论、机器学习、算法、离散数学等方面的文章。</p></div><div class="ab cl lo lp hx lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="im in io ip iq"><p id="ade2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有关更多信息(！)和进一步的阅读—</p><ol class=""><li id="f56d" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated"><a class="ae lv" href="https://arxiv.org/pdf/1802.05968.pdf" rel="noopener ugc nofollow" target="_blank">信息论:教程介绍</a></li><li id="7c4a" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated"><a class="ae lv" href="http://tuvalu.santafe.edu/~simon/it.pdf" rel="noopener ugc nofollow" target="_blank">智能人的信息论</a></li></ol></div></div>    
</body>
</html>