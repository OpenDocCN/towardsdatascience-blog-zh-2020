<html>
<head>
<title>Hands-On PCA Data Preprocessing Series. Part I: Scaling Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">动手 PCA 数据预处理系列。第一部分:缩放变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pca-a-practical-journey-preprocessing-encoding-and-inspiring-applications-64371cb134a?source=collection_archive---------21-----------------------#2020-05-29">https://towardsdatascience.com/pca-a-practical-journey-preprocessing-encoding-and-inspiring-applications-64371cb134a?source=collection_archive---------21-----------------------#2020-05-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a8ef" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">尝试使用 PCA 却卡在处理阶段？看看这个。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e96276f3db14c5398e5a69b240fbb0d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*STjmCaXZ8Ua2Gqii03EgYA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="kv kw ep" href="https://medium.com/u/9377014bd7ca?source=post_page-----64371cb134a--------------------------------" rel="noopener" target="_blank">王思韵</a>。树枝上毛茸茸的小鸟【水彩】(2020)。我们用低聚艺术来表示 PCA 的感觉，就是降维。但是 PCA 能做的不止这些。</p></figure><h1 id="7946" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">系列介绍</h1><p id="410d" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">在本系列中，我们将探索缩放数据和 PCA 的结合。我们希望看到，每当我们遇到新的数据集时，我们如何才能更好地为机器学习任务准备数据。旅程由三部分组成。</p><ul class=""><li id="4ba2" class="ml mm iq lr b ls mn lv mo ly mp mc mq mg mr mk ms mt mu mv bi translated">第一部分:定标器和 PCA</li><li id="c2e2" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ms mt mu mv bi translated">第二部分:认识离群值</li><li id="1b5c" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ms mt mu mv bi translated">第三部分:分类数据编码</li></ul><h1 id="c9db" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">我们将在这篇文章中做什么</h1><ol class=""><li id="13d5" class="ml mm iq lr b ls lt lv lw ly nb mc nc mg nd mk ne mt mu mv bi translated">简要回顾定标器和 PCA 的背景</li><li id="fbf6" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ne mt mu mv bi translated">介绍要处理的数据集和任务</li><li id="cfa2" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ne mt mu mv bi translated">对数据集执行缩放变换</li><li id="26e1" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ne mt mu mv bi translated">对缩放变换后的数据集进行主成分分析并评估性能</li></ol><h1 id="bb0d" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">你将学到什么</h1><ul class=""><li id="cd30" class="ml mm iq lr b ls lt lv lw ly nb mc nc mg nd mk ms mt mu mv bi translated">理解定标器的重要性及其与 PCA 的密切关系</li><li id="deca" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ms mt mu mv bi translated">明智选择定标器</li><li id="ca87" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ms mt mu mv bi translated">使相关和漂亮的可视化:)</li></ul><blockquote class="nf ng nh"><p id="8c1c" class="lp lq ni lr b ls mn jr lu lv mo ju lx nj nk ma mb nl nm me mf nn no mi mj mk ij bi translated">在你开始阅读之前，我们明确建议你先玩一下笔记本(在 Colab 和 Github 上都有。请在这篇文章的末尾找到链接。)</p></blockquote></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="0d70" class="kx ky iq bd kz la nw lc ld le nx lg lh jw ny jx lj jz nz ka ll kc oa kd ln lo bi translated">定标器和 PCA</h1><p id="4152" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">众所周知，PCA 对数据集的规模很敏感，因此将每个要素调整到合理的规模对 PCA 至关重要。在这一部分，我们将了解不同的定标器如何影响 PCA 结果。</p><blockquote class="nf ng nh"><p id="d321" class="lp lq ni lr b ls mn jr lu lv mo ju lx nj nk ma mb nl nm me mf nn no mi mj mk ij bi translated">这篇文章的灵感来自 Scikit-Learn 文档中的这个<a class="ae ob" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py" rel="noopener ugc nofollow" target="_blank">教程</a>。您将看到我们使用相同的<a class="ae ob" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html" rel="noopener ugc nofollow" target="_blank">葡萄酒数据集</a>测试不同的定标器，并将不同定标的数据传递到 PCA 步骤。这款小玩具展示了 scaler + PCA 组合的强大功能，也许你想把它放在自己的工具箱里。</p></blockquote><h1 id="f2d5" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">主成分分析简介</h1><p id="9ff3" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">在深入了解更多细节之前，为了让我们的旅程更加完整，我们想先简单介绍一下 PCA。已经理解了后面的数学知识的读者可以直接跳到下一部分。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/216f23657dacd6ce2e207d8f12c550d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GAe1Ufa_HLjbHQ0rK-1daw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd od">术语警告</strong></p></figure><blockquote class="nf ng nh"><p id="2101" class="lp lq ni lr b ls mn jr lu lv mo ju lx nj nk ma mb nl nm me mf nn no mi mj mk ij bi translated">PCA 是“主成分分析”的缩写，其主要功能之一是降低数据集的维度(列)。并且是通过线性代数的<a class="ae ob" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">奇异值分解</a> (SVD)来完成的。直观上，数据是以矩阵的形式出现的，矩阵的列是高维空间的轴。用这样的轴表示的数据点有时对于机器学习模型来说很难探索。SVD 所做的是找到数据集的更智能的表示(通过原始轴的线性组合)。新的抽象轴表示的数据更加模型友好。</p><p id="347a" class="lp lq ni lr b ls mn jr lu lv mo ju lx nj nk ma mb nl nm me mf nn no mi mj mk ij bi translated">再多说几句:新的一组轴是相互正交的(甚至更好，正交)。这个属性对于基于树的模型来说尤其可爱。新的一组轴按照它们表示数据集信息或可变性的能力排序</p></blockquote><h1 id="25bd" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">数据缩放器</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/4d4006996d0bc7ef9dd5310c71c16a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-PSHoQcJNCq2d6bJsrDOTw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd od">总体思路</strong></p></figure><blockquote class="nf ng nh"><p id="0d68" class="lp lq ni lr b ls mn jr lu lv mo ju lx nj nk ma mb nl nm me mf nn no mi mj mk ij bi translated"><strong class="lr ir">我们为什么需要定标器？</strong>想象一个房价数据集，包含房间数量、面积和单个公寓的价格等特征。这三列的可变性有很大的差异:房间数的最大差异很难超过 10 个房间，即数量级为 1；公寓面积的最大差异可以是 100 平方英尺的数量级；价格的差异很容易达到 10K 订单。</p><p id="81ee" class="lp lq ni lr b ls mn jr lu lv mo ju lx nj nk ma mb nl nm me mf nn no mi mj mk ij bi translated"><strong class="lr ir">scaler 和 PCA 有什么关系？</strong>如果我们要对这样一个列的可变性按顺序不同的数据集应用 PCA，那些相对“平滑”的特征的效果将被最激烈的特征(本例中的价格)完全淹没。为了解决这个问题，我们对数据进行了缩放。</p></blockquote><p id="0031" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">SciKit-Learn 中的一行代码提供了许多不同的定标器，如最常用的标准定标器和最小-最大定标器，以及其他非线性定标器。在这篇博文中，我们感兴趣的是在应用 PCA 之前测试所有这些可用的定标器，并看看它们如何与 PCA 一起工作。如果你有兴趣，这里有一个详细的<a class="ae ob" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py" rel="noopener ugc nofollow" target="_blank">演示</a>在这篇文章中测试的所有定标器。</p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="3b0b" class="kx ky iq bd kz la nw lc ld le nx lg lh jw ny jx lj jz nz ka ll kc oa kd ln lo bi translated">开始测试吧！</h1><p id="8e89" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">让我们开始吧。首先，我们需要一些准备代码。我们将如下使用葡萄酒数据。</p><p id="a7e5" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated"><a class="ae ob" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lr ir">葡萄酒数据集</strong> </a>是一个图例数据集。这些数据是对意大利同一地区三个不同种植者种植的葡萄酒进行化学分析的结果。对于三种类型(产地)的葡萄酒中发现的不同成分，有十三种不同的测量方法。数据集仅包含数字要素。<strong class="lr ir">我们的目标是使用 13 种不同的测量方法，找出目标标签(原点)。</strong>首先，我们来快速看一下数据。</p><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="fee6" class="oj ky iq of b gy ok ol l om on"># note: the code in this post is mainly for illustration purpose. More details, please refer to the original notebook. link at the end of the blog.</span><span id="86b2" class="oj ky iq of b gy oo ol l om on">df_wine.head()</span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="op oq l"/></div></figure><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="76f3" class="oj ky iq of b gy ok ol l om on">df_wine.describe()</span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="b464" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">我们诚挚地邀请您注意特征可变性顺序的差异:“非类黄酮 _ 酚类”的标准偏差为 0.4，而“脯氨酸”的标准偏差为 746。我们将在后面看到，如果不对数据集进行任何缩放，PCA 的结果将会因那些具有巨大差异的特征而产生很大偏差。</p><h2 id="1896" class="oj ky iq bd kz or os dn ld ot ou dp lh ly ov ow lj mc ox oy ll mg oz pa ln pb bi translated">关于任务</h2><p id="b806" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">数据集的目标列包含三个标签，因此它可以是一个多类分类任务。在这里，我们通过屏蔽不同数量的真实标签来模拟半监督和无监督的学习情况。我们做这个额外步骤的原因很快就会清楚了。让我们首先将数据可视化，以便获得一些见解。</p><h2 id="ae37" class="oj ky iq bd kz or os dn ld ot ou dp lh ly ov ow lj mc ox oy ll mg oz pa ln pb bi translated">探索性数据分析</h2><p id="ff6e" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">数据非常干净。所有的特征都是数字变量。所以我们没有任何繁琐的数据清理工作要做。使用 seaborn 的<a class="ae ob" href="https://seaborn.pydata.org/generated/seaborn.pairplot.html" rel="noopener ugc nofollow" target="_blank"> pairplot </a>和 box plot 可以很好地揭示数据集。</p><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="8564" class="oj ky iq of b gy ok ol l om on">fig, axes = plt.subplots(nrows=1, ncols=len(df_features.columns), figsize=(len(df_features.columns)*2,4))<br/>for index, col in enumerate(df_features.columns):<br/>    ax = axes[index]<br/>    sns.boxplot(y=df_wine.loc[:,col], x=df_wine.loc[:,'target_original'], palette=customer_palette, ax=ax)<br/>    ax.set_ylabel(ax.get_ylabel(), fontsize='x-large')<br/>    ax.set_xlabel("")</span><span id="8f5e" class="oj ky iq of b gy oo ol l om on">plt.tight_layout(pad=0.5)<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/0a484d7a04d3fad577f4062faf3af80c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pQH2yHor82h0u0HYl_JhCA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(提示:在单独的窗口中打开以获得更好的视图)</p></figure><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="f448" class="oj ky iq of b gy ok ol l om on">g = sns.pairplot(data=df_wine, <br/>                 vars=df_features.columns, <br/>                 hue='target_original',            <br/>                 corner=True, palette=customer_palette,   <br/>                 hue_order=target_order)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/de7af56fb74f37d41c45051787cfacf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X7NeskJE5WL0jTDdDcCFDg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(提示:在单独的窗口中打开以获得更好的视图)</p></figure><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="e235" class="oj ky iq of b gy ok ol l om on"># target_large_missinng<br/>g = sns.pairplot(data=df_wine, <br/>                 vars=df_features.columns, <br/>                 hue='target_large_missinng', <br/>                 corner=True, <br/>                 palette=customer_palette, <br/>                 hue_order=target_order)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/50d2304f9b47e89501388067c1cf0e44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d6OexmSG5G10IRi8S1OQ0A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(提示:在单独的窗口中打开以获得更好的视图)</p></figure><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="8a6f" class="oj ky iq of b gy ok ol l om on"># target_all_missinng<br/>g = sns.pairplot(data=df_wine, <br/>                 vars=df_features.columns, <br/>                 hue='target_all_missinng', <br/>                 corner=True, <br/>                 palette=customer_palette, <br/>                 hue_order=target_order)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/b9d31068f0f99c966f93bc4bce42bdbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KrKjcGErUFgyNLO_AYFifA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(提示:在单独的窗口中打开以获得更好的视图)</p></figure><p id="4275" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">您可能想知道为什么用不同的颜色将相同的数据可视化三次。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/bb1b9ba6274a06115d711d041e1a552c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1wzyN30V9btaK55WLmNKAQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd od">思考时间</strong></p></figure><blockquote class="nf ng nh"><p id="44ae" class="lp lq ni lr b ls mn jr lu lv mo ju lx nj nk ma mb nl nm me mf nn no mi mj mk ij bi translated">我们要求你在三种情况下区分三种不同的目标类别。</p></blockquote><ul class=""><li id="18ec" class="ml mm iq lr b ls mn lv mo ly mp mc mq mg mr mk ms mt mu mv bi translated">场景 1:使用颜色辅助(第一对图)</li><li id="ed13" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ms mt mu mv bi translated">场景 2:使用弱颜色辅助(第二对图)</li><li id="737b" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ms mt mu mv bi translated">场景三:没有颜色辅助(最后一对图)。</li></ul><p id="c33c" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">这里的要点是，在没有目标标签或只有有限数量的标签的情况下，人眼区分不同的目标类别变得非常困难(注意:没有目标标签的场景类似于无监督学习的情况；标签数量有限的情况类似于半监督学习情况)。</p><p id="9c40" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">虽然人类可能看不到特征之间更高维的交互，但机器学习模型可能会发现一些模式。然而，为什么不尝试以一种更智能的方式来表示数据呢？例如，在 PCA 之后加上一个适当的定标器。</p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="e83f" class="kx ky iq bd kz la nw lc ld le nx lg lh jw ny jx lj jz nz ka ll kc oa kd ln lo bi translated">比较不同的缩放器</h1><h2 id="f913" class="oj ky iq bd kz or os dn ld ot ou dp lh ly ov ow lj mc ox oy ll mg oz pa ln pb bi translated">第一眼</h2><p id="7948" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">在这里，我们将使用 SciKit-Learn 的预处理子类中所有八种不同的缩放器来转换数据集。我们没有太注意调整定标器的超参数。我们绘制变换数据集的 PCA 的前两个分量的散点图，始终保持逐渐减少颜色辅助的精神。</p><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="8265" class="oj ky iq of b gy ok ol l om on"># customer visualization function<br/># show scatter of 1st and 2nd PCA component<br/>pca_scatter_plot(X_pca_dict, y_train.iloc[:,1:])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/7b33f6c6657bffea7add1aa5a6e42cd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FfkWfG0la4lILy29pzfbwA.png"/></div></div></figure><p id="5077" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">不要迷失在大量的散点图中。让我们仔细看看那些有助色的。我们的结论是，有了这个数据集，无论我们选择哪种定标器，都最好对数据进行定标。但是有一个例外:标准化器。规格化器以一种奇怪的方式缩放数据:按行而不是按列。我们不确定这种定标器何时有用，但我们很乐意与您讨论何时使用它。</p><p id="b709" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">如果您想知道为什么缩放的数据集比未缩放的数据更容易区分每个类。我们很高兴你问了。原因是缩放后的变量可以同等比较。相反，机器学习模型所看到的未缩放特征的重要性受其缩放比例的影响很大。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/bb1b9ba6274a06115d711d041e1a552c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1wzyN30V9btaK55WLmNKAQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd od">思考时间</strong></p></figure><blockquote class="nf ng nh"><p id="b960" class="lp lq ni lr b ls mn jr lu lv mo ju lx nj nk ma mb nl nm me mf nn no mi mj mk ij bi translated">这里有一个小练习，供有 PCA 数学知识的读者参考。不同类别的集群并不总是以相同的顺序出现，比如从左到右是红色、蓝色和绿色；它可以是绿色、蓝色和红色。为什么会出现这样的排列现象？</p></blockquote><p id="438d" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">让我们继续深入研究 PCA 对不同规模数据集的作用。以下是 PCA 后每个特征的箱线图，不同的定标器按行区分。</p><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="0194" class="oj ky iq of b gy ok ol l om on"># customer visualization function show PCA components<br/>pca_box_plot(X_pca_dict, df_target.loc[:,'target_original'])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ph"><img src="../Images/73dadc7452619bc6a0bec46e478f9ea3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NgOwzVEh7nK3daxS-V39oA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(提示:在单独的窗口中打开以获得更好的视图)</p></figure><p id="68cb" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">同样，在没有任何转换的情况下，几乎所有有趣的信息都集中在第一个组件上，给空间的其余部分留下很少的变化。我们认为第一个成分中最重要的成分是原始数据中的“脯氨酸”。实际上，这些框的行为几乎与使用上述原始特征空间为“脯氨酸”绘制的框的行为相同。</p><p id="e0d4" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">如果我们看看其他缩放器(除了规格化器)，我们可以看到当从第一个分量(即，最有信息的分量)到最后一个分量(即，最没有信息的分量)时，盒子反弹越来越少的趋势。更有趣的是，几乎所有的定标器都会对第三个分量产生截止效应。在这个阶段，我们可能有足够的信心，前两个组件是有代表性的。</p><p id="fbb1" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">我们可以更深入地研究一下协方差矩阵。因为 PCA 直接作用于协方差矩阵并输出主成分。这是故事的开头。</p><p id="83a3" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">让我们通过 Seaborn 的热图函数来可视化协方差矩阵。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/ef877b5864165abb6fa0f581dfb6e3b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H4MtIg7Hvld6SWA4lT9OpQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd od">警告注释</strong></p></figure><blockquote class="nf ng nh"><p id="a4b9" class="lp lq ni lr b ls mn jr lu lv mo ju lx nj nk ma mb nl nm me mf nn no mi mj mk ij bi translated">一个<!-- -->重要提示:我们通常用热图来表示皮尔逊相关系数矩阵(CC 矩阵)，但这两个矩阵有一个关键区别:一个协方差矩阵中的值不一定在(-1，1)之间，对角线上的值也不一定都是 1。这就是为什么我们使用下方的三角形热图来绘制对角线值。</p></blockquote><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="d816" class="oj ky iq of b gy ok ol l om on">trans_heat_plot_abs(X_trans_dict, y_axis_labels=df_features.columns)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pi"><img src="../Images/cc422c072eddbf84c94894211f1dcdb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*7AzDFRUzV2tYkhHtRpG4Dw.png"/></div></div></figure><p id="aa03" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">此时，与散点图和/或盒状图相比，热图可能不是很直观，因为标签信息没有进入热图。此外，一些颜色条的精细渐变使得一些热图彼此非常相似。</p><p id="e2e8" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">然而，我们确实从这些热图中得到了一个线索，即矩阵如何适应 PCA。<strong class="lr ir">经验法则是，热图越丰富多彩，PCA 结果越好。通常，PCA 不喜欢看起来简单的热图，并且会输出不太有趣的主成分。</strong>在我们的例子中，由于“脯氨酸”是最突出的，PCA 将会关注它。这和我们之前的分析是一致的。另一个简单的方法是归一化转换数据。根据热图，我们可能会猜测五氯苯甲醚中的一种成分主要来自“镁”；因此，标准化器+ PCA 给出了沉闷的结果。</p><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="e284" class="oj ky iq of b gy ok ol l om on"># plot the explained_variance cumsum using previously calculated PCA instances.<br/>pca_plot_cumsum(instance_pca_dict)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/65b3c93132ef8417e16a27ea5e8a8981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJ3_6sh1uFE46sF4goqzdg.png"/></div></div></figure><h2 id="25a2" class="oj ky iq bd kz or os dn ld ot ou dp lh ly ov ow lj mc ox oy ll mg oz pa ln pb bi translated">聚类算法如何喜欢我们处理过的矩阵？</h2><p id="9c12" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">通过观察由 PCA 发现的特征空间的新方面的前两个维度，我们或多或少对预处理结果感到满意。然而，能够看到更高维度相互作用的聚类算法会像我们一样偏爱相同的变换矩阵吗？让我们使用机器学习聚类模型对数据进行聚类。</p><p id="c1c3" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">我们在标记数据集上执行聚类任务的一个优势是，我们知道正确的类数。接下来，我们将看到 k 均值与真实标签的吻合程度。</p><blockquote class="nf ng nh"><p id="6212" class="lp lq ni lr b ls mn jr lu lv mo ju lx nj nk ma mb nl nm me mf nn no mi mj mk ij bi translated">这里我们采用了博客的第一个度量标准:<a class="ae ob" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lr ir"> V-measure score </strong> </a>。我们不会详细讨论这个指数；你只需要知道:<br/>V-measure 得分有界在[0，1]范围内:接近 0 的值是不好的，1.0 代表完全匹配。</p></blockquote><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="31b0" class="oj ky iq of b gy ok ol l om on">df_scores</span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="e138" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">我们比较了三个场景:两个原始特征的聚类，两个信息量最大的主成分分析，以及整个特征集。我们选择了两个特征或组件来保持与之前散点图的一致性。虽然更多的列可能会更彻底地揭示数据集的信息，但我们认为至少从一开始，两个维度就足够了。</p><p id="977a" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">让我们仔细看看上表。</p><ul class=""><li id="2f7a" class="ml mm iq lr b ls mn lv mo ly mp mc mq mg mr mk ms mt mu mv bi translated"><strong class="lr ir">首先是</strong>，让我们关注最后一列，它包含使用整个数据集的 V-measure 得分。无论我们选择哪种缩放器，缩放数据总是更好。(除了“规格化器”)</li><li id="95ae" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ms mt mu mv bi translated"><strong class="lr ir">其次</strong>，对于特征空间的原始方面，使用整个数据集总是更好。选择“似乎”能更好地说明聚类的特征子集似乎不是一个好主意。(只看第一列和第三列的对比)。</li><li id="bd41" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ms mt mu mv bi translated">另一方面，对于 PCA 变换的数据，情况就不同了。有时，只使用几个组件比使用所有组件会产生更合理的集群。(看一下第二列和第三列的对比)。即使在某些测试中，前两个组成部分的选择并没有超过总体，但两个选择之间的 V-measure 得分非常接近。(因为这里我们比较的是 2 列矩阵和 13 列矩阵的计算来源。)</li><li id="f269" class="ml mm iq lr b ls mw lv mx ly my mc mz mg na mk ms mt mu mv bi translated"><strong class="lr ir">最后是</strong>，查看第二列，我们发现最适合这项任务的定标器是两个非线性定标器和标准定标器。因此，在直接应用最常用的标准定标器之前，您可能希望通过简单的测试来尝试更多的定标器。</li></ul></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><p id="e260" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">我们想就 PCA 显示的少即是多现象多说几句。人们经常争论说，丢弃原始数据集中的信息是不明智的；这样做有时非常危险。然而，主成分分析揭示了数据矩阵中最重要的模式，同时过滤掉了一些细节。这些细节可能只是噪音。因此，采用前几个 PCA 组成部分不仅有助于减少条件性，还可以使信息更加顺畅，使事情更加清晰。</p><pre class="kg kh ki kj gt oe of og oh aw oi bi"><span id="9969" class="oj ky iq of b gy ok ol l om on">kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE)</span><span id="fe21" class="oj ky iq of b gy oo ol l om on">pca_cluster_contour_plot(X_pca_dict,y_train, kmeans)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/68ee3b2dd5dd13e8591b4670c32b0b1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*YlRkGK7dqgfSx6U1d31RCw.png"/></div></figure><h1 id="b9bb" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">摘要</h1><p id="77aa" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">在本帖中，我们通过展示散点图，比较了 PCA 作用于不同尺度数据的结果。从视觉上我们可以看到，这种组合确实比没有预处理的数据集的模式更清晰。在应用 PCA 之前，外卖将始终检查数据集中每个要素的方差，如果方差之间存在较大差距，则使用适当的缩放器缩放数据。根据你的任务，“适当”的定义可以有所不同。</p><p id="0637" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated">下一次，我们将通过引入合成异常值和分类特征来扩展这种定标器+ PCA 组合的限制。</p><p id="eb7c" class="pw-post-body-paragraph lp lq iq lr b ls mn jr lu lv mo ju lx ly nk ma mb mc nm me mf mg no mi mj mk ij bi translated"><strong class="lr ir">我希望你喜欢这篇文章。请随时留下评论。</strong></p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><blockquote class="nf ng nh"><p id="f447" class="lp lq ni lr b ls mn jr lu lv mo ju lx nj nk ma mb nl nm me mf nn no mi mj mk ij bi translated">声明:该系列由莫克菲和王思韵<a class="kv kw ep" href="https://medium.com/u/9377014bd7ca?source=post_page-----64371cb134a--------------------------------" rel="noopener" target="_blank">王思韵</a>合作。第一次发布:2020 年 6 月 8 日。</p></blockquote><h1 id="99ef" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">笔记本链接:</h1><div class="pl pm gp gr pn po"><a href="https://drive.google.com/file/d/1UQ-f7e1ImdLX7mJuxY-pY1RWkXDwn-cc/view?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd ir gy z fp pt fr fs pu fu fw ip bi translated">blog-PCA-part I _ kefei _ 0608 . ipynb</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">合作笔记本</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">drive.google.com</p></div></div></div></a></div><div class="pl pm gp gr pn po"><a href="https://github.com/kefeimo/DataScienceBlog/blob/master/1.%20PCA/blog-PCA-part%20I_kefei_0608.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd ir gy z fp pt fr fs pu fu fw ip bi translated">kefeimo/数据科学博客</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">github.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc kp po"/></div></div></a></div></div></div>    
</body>
</html>