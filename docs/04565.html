<html>
<head>
<title>Top Machine Learning Algorithms for Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于聚类的顶级机器学习算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/top-machine-learning-algorithms-for-clustering-a09c6771805?source=collection_archive---------14-----------------------#2020-04-23">https://towardsdatascience.com/top-machine-learning-algorithms-for-clustering-a09c6771805?source=collection_archive---------14-----------------------#2020-04-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d9a1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何找到数据的底层结构</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c679005198b3abf9d7aee749d802a145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k0qweIc5cvXwTtYI7L2ARw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@kymellis?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">金莎·艾利斯</a>在<a class="ae ky" href="https://unsplash.com/s/photos/view?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="60e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">聚类是一种对一组数据点进行分组的方法，将相似的数据点分组在一起。因此，聚类算法寻找数据点之间的相似或相异之处。聚类是一种无监督的学习方法，因此没有与数据点相关联的标签。聚类算法试图找到数据的底层结构。</p><p id="8eda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有不同的方法和算法来执行聚类任务，这些任务可以分为三个子类别:</p><ul class=""><li id="50b2" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">基于分区的聚类</strong></li><li id="9792" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">层次聚类</strong></li><li id="c270" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">基于密度的聚类</strong></li></ul><p id="fdbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这些方法都旨在将数据分组到簇中。他们使用的方法略有不同。我将用每种方法的通用算法详细解释每种方法。在开始这个话题之前，我想指出聚类和分类之间的区别。</p><h1 id="5996" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">聚类vs分类</strong></h1><ul class=""><li id="2fb5" class="lv lw it lb b lc nb lf nc li nd lm ne lq nf lu ma mb mc md bi translated">分类任务中的观察值(或数据点)有标签。根据一些测量值对每个观察值进行分类。分类算法试图对观测值的测量值(特征)和它们的指定类别之间的关系进行建模。然后，模型预测新观察的类别。</li><li id="62a9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">聚类中的观察值(或数据点)没有标签。我们希望该模型能够在数据集中找到结构，以便可以将相似的观察结果分组到聚类中。我们基本上要求模型标记观察值。</li></ul></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="a55c" class="mj mk it bd ml mm nn mo mp mq no ms mt jz np ka mv kc nq kd mx kf nr kg mz na bi translated"><strong class="ak">基于分区的聚类</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/0cbb53e2938656a83a591b3a0ba55144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nQefDjs5SvqeVoxV3X-7Bg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@shashanksahay?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Shashank Sahay </a>在<a class="ae ky" href="https://unsplash.com/s/photos/view?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="9c00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于分区的聚类技术试图根据应用于数据点的距离度量来创建数据分区。这种方法最常见的算法是<strong class="lb iu"> k均值聚类</strong>。</p><p id="4a72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">k-均值聚类旨在将数据划分为k个聚类，使得同一聚类中的数据点相似，而不同聚类中的数据点相距较远。</p><blockquote class="nt nu nv"><p id="4a99" class="kz la nw lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated">两点的相似性由它们之间的距离决定。</p></blockquote><p id="2cc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测量距离的方法有很多。<a class="ae ky" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>(p = 2的闵可夫斯基距离)是最常用的距离度量之一。</p><p id="6a48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-means聚类试图最小化一个类内的距离，最大化不同类之间的距离。K-means算法不能确定聚类数。我们需要在创建KMeans对象时定义它，这可能是一项具有挑战性的任务。</p><p id="ebb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-means是一个迭代过程。它建立在<a class="ae ky" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">期望最大化</a>算法的基础上。确定集群数量后，它通过执行以下步骤来工作:</p><ol class=""><li id="647c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu oa mb mc md bi translated">为每个簇随机选择质心(簇的中心)。</li><li id="3348" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oa mb mc md bi translated">计算所有数据点到质心的距离。</li><li id="18d4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oa mb mc md bi translated">将数据点分配给最近的聚类。</li><li id="e6d3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oa mb mc md bi translated">通过取聚类中所有数据点的平均值，找到每个聚类的新质心。</li><li id="8158" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oa mb mc md bi translated">重复步骤2、3和4，直到所有点收敛并且聚类中心停止移动。</li></ol><blockquote class="nt nu nv"><p id="7e7f" class="kz la nw lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated"><strong class="lb iu">注意</strong>:初始质心是随机选择的，这可能会导致最终的聚类有些不同。为了解决这个问题，scikit learn提供了<strong class="lb iu"> n_init </strong>参数。k-means算法以不同的初始质心运行“n_init”次，并且最终结果将根据n_init次连续运行来确定。</p></blockquote><p id="0346" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点:</strong></p><ul class=""><li id="d1c1" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">容易理解</li><li id="e0ec" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">相对较快</li><li id="e78a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">可扩展用于大型数据集</li><li id="1a15" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">能够以一种聪明的方式选择初始质心的位置，从而加速收敛</li><li id="f82e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">保证收敛</li></ul><p id="0352" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ul class=""><li id="bd90" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">集群的数量必须预先确定。K-means算法无法猜测数据中存在多少个聚类。确定集群的数量可能是一项具有挑战性的任务。</li><li id="7026" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">只能画线性边界。如果数据中存在非线性结构来分隔组，k-means将不是一个好的选择。</li><li id="6943" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">随着样本数量的增加而变慢，因为在每一步，k-means算法都会访问所有数据点并计算距离。另一种方法是使用数据点的子集来更新质心的位置(即sk learn . cluster . minibatchkmeans)</li><li id="c09b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">对异常值敏感</li></ul></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="7cfc" class="mj mk it bd ml mm nn mo mp mq no ms mt jz np ka mv kc nq kd mx kf nr kg mz na bi translated"><strong class="ak">层次聚类</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/c616511fa990c3a77dcc7b7accce9741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XMTSOc-iQVx_BHlP6J66hA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">塞缪尔·查伦在<a class="ae ky" href="https://unsplash.com/s/photos/hidden?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="37ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法通过构建集群的层次结构来工作。分层聚类意味着通过迭代分组或分离数据点来创建聚类树。有两种类型的分层聚类:</p><ul class=""><li id="14c9" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">凝聚聚类</li><li id="6a6f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">分裂聚类</li></ul><blockquote class="nt nu nv"><p id="fabe" class="kz la nw lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated">分层聚类的优点之一是我们不必指定聚类的数量(但是我们可以)。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/a0204593e2b7e88fdf6a39a11fec369f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3-G10imn-BuupK4P.png"/></div></div></figure><p id="d03f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">聚集聚类</strong>是一种自下而上的方法。每个数据点首先被假定为一个单独的聚类。然后迭代地组合相似的聚类。让我们看一个例子来解释清楚这个概念。</p><p id="87c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有一个由9个样本组成的数据集。我选择与这些样本相关的数字来演示相似性的概念。在每次迭代(或级别)中，最接近的数字(即样本)被组合在一起。如下图所示，我们从9个集群开始。最接近的在第一级被组合，然后我们有7个集群。与蓝线相交的黑线的数量代表簇的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/16c179ef864e56a6a1be6a2a4445b6a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/0*HKJd3t7RfswVxMXN.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">树突图</p></figure><p id="07c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图称为<strong class="lb iu">树状图</strong>，这是一个表示基于树的方法的图表。在层次聚类中，树状图用于可视化聚类之间的关系。</p><p id="fea6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着我们往上走，随着更多样本的合并，聚类的数量会减少。在级别6之后，所有样本被合并到一个大的聚类下。</p><p id="85b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">层次聚类的优点之一是我们不必预先指定聚类的数量。但是，将所有数据点合并到一个聚类中是不明智的。我们应该在某个时候停止组合集群。Scikit-learn为此提供了两个选项:</p><ul class=""><li id="5fb1" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">达到一定数量的簇后停止(<strong class="lb iu"> n_clusters </strong>)</li><li id="c4de" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">设置联动的阈值(<strong class="lb iu">距离_阈值</strong>)。如果两个聚类之间的距离超过阈值，这些聚类将不会被合并。</li></ul><p id="eb5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">分裂聚类</strong>在现实生活中并不常用，所以我将简单地提一下。一个简单而清晰的解释是，分裂性聚集与聚集性聚集相反。我们从一个包含所有数据点的巨大集群开始。然后数据点被分成不同的簇。这是一种自上而下的方法。</p><p id="f75e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果基础数据具有某种层次结构，分层聚类是有用的，并且会给出更好的结果。</p><p id="ffe1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">分层聚类的一些常见用例:</p><ul class=""><li id="ec27" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">遗传或其他生物数据可以用来创建一个树状图，以代表突变或进化水平。<a class="ae ky" href="https://en.wikipedia.org/wiki/Phylogenetic_tree" rel="noopener ugc nofollow" target="_blank">种系发生树</a>用于展示基于相似性和差异性的进化关系。</li><li id="67d1" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">层次聚类也用于对文本文档进行分组。</li><li id="5874" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">层次聚类的另一个常见用例是<strong class="lb iu">社交网络分析。</strong></li><li id="4a56" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">层次聚类也用于<strong class="lb iu">离群点检测。</strong></li></ul><p id="bfb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点</strong></p><ul class=""><li id="2136" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">不需要预先指定集群的数量。必须为k-means算法指定聚类数。</li><li id="ba74" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">借助于树状图，它很容易实现和解释。</li><li id="e4ba" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">总是生成相同的聚类。k-均值聚类可能会产生不同的聚类，这取决于质心(聚类的中心)是如何初始化的。</li></ul><p id="3e25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点</strong></p><ul class=""><li id="0484" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">与k-means相比，这是一种较慢的算法。分层聚类需要很长时间来运行，尤其是对于大型数据集。</li></ul></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="47e0" class="mj mk it bd ml mm nn mo mp mq no ms mt jz np ka mv kc nq kd mx kf nr kg mz na bi translated"><strong class="ak">基于密度的聚类</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/5f349a4d22b2d0ce4d9b937ebba23e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ettuEuI_B1e_QHfdVXY3uw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@greystoke?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">哈坎·奥尔德林</a>在<a class="ae ky" href="https://unsplash.com/s/photos/view?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="9ce1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于分区和层次聚类技术对于正常形状的聚类非常有效。然而，当涉及到任意形状的聚类或检测异常值时，基于密度的技术更有效。</p><p id="42e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请考虑以下数字:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/30e7dc8bfba081e6af16ba11acae51c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*MzC5qxC4sGComE_J.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/33bbf83953f393f35e2dfcb5f1c96565.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/0*eLUWEL9pzxp5k-g9.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/5ad79cd88d7cc151cd4ef888db53f4a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/0*ABIBSo4BtfG-iSrA.png"/></div></figure><p id="5080" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些图中的数据点以任意形状分组或者包含异常值。基于密度的聚类算法在发现高密度区域和离群点方面非常有效。对于某些任务来说，检测异常值是非常重要的，例如异常检测。这种方法最常用的算法之一是DBSCAN。</p><p id="4afd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> DBSCAN </strong>代表<strong class="lb iu">d</strong>en sity-<strong class="lb iu">b</strong>as<strong class="lb iu">s</strong>partial<strong class="lb iu">c</strong>illumination of<strong class="lb iu">a</strong>a<strong class="lb iu">n</strong>oise。它能够找到任意形状的聚类和带有噪声的聚类(即异常值)。</p><p id="4a49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DBSCAN背后的主要思想是，如果一个点靠近来自该簇的许多点，则该点属于该簇。</p><p id="8075" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DBSCAN有两个关键参数:</p><ul class=""><li id="6f9f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu"> eps </strong>:指定邻域的距离。如果两点之间的距离小于或等于eps，则认为这两点是相邻的。</li><li id="9541" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu"> minPts: </strong>定义一个聚类的最小数据点数。</li></ul><p id="c5ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于这两个参数，点被分类为核心点、边界点或异常点:</p><ul class=""><li id="4166" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">核心点:</strong>如果一个点在其半径为eps的周围区域内至少有minPts个数的点(包括该点本身)，则该点为核心点。</li><li id="c631" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">边界点:</strong>如果一个点可以从一个核心点到达，并且其周围区域内的点数少于minPts，那么这个点就是边界点。</li><li id="0a7f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">离群点:</strong>如果一个点不是核心点并且从任何核心点都不可达，那么这个点就是离群点。</li></ul><p id="6386" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些观点可以用形象化来更好地解释。下图摘自维基百科:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/f87134ee3d43d12edba07a6ed561a534.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/0*0dQS6PN8q3TvIMO9.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/DBSCAN" rel="noopener ugc nofollow" target="_blank">图来源</a></p></figure><p id="93e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，minPts是4。红色点是核心点，因为在其半径为eps的周围区域内至少有<strong class="lb iu">4个点。该区域在图中用圆圈表示。黄色点是边界点，因为它们可以从核心点到达，并且其邻域内的点少于4个。可到达意味着在核心点的周围区域。点B和C在其邻域内(即以eps为半径的周围区域)有两个点(包括点本身)。最后，N是一个异常值，因为它不是一个核心点，不能从核心点到达。</strong></p><p id="f363" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经学习了参数和不同类型点的定义。现在我们可以谈谈算法是如何工作的。这其实很简单:</p><ul class=""><li id="225e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">确定minPts和eps。</li><li id="b8f9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">随机选择一个起始点，使用半径eps确定其邻域。如果邻域中至少有minPts个数的点，则将该点标记为核心点，并开始形成聚类。如果不是，则该点被标记为噪声。一旦聚类形成开始(假设聚类A)，初始点邻域内的所有点都成为聚类A的一部分。如果这些新点也是核心点，则它们邻域内的点也被添加到聚类A中。</li></ul><blockquote class="nt nu nv"><p id="c57b" class="kz la nw lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated">注意:被标记为噪声的点可以被重新访问，并且是聚类的一部分。</p></blockquote><ul class=""><li id="d505" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">下一步是在前面步骤中没有访问过的点中随机选择另一个点。然后同样的程序适用。</li><li id="81b8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">当所有点都被访问时，该过程结束。</li></ul><blockquote class="nt nu nv"><p id="3507" class="kz la nw lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated">使用k-means算法中的距离测量方法来确定点之间的距离。最常用的方法是欧氏距离。</p></blockquote><p id="90cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过应用这些步骤，DBSCAN算法能够找到高密度区域并将它们从低密度区域中分离出来。</p><p id="caea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个集群包括相邻的核心点(即彼此可到达的)以及这些核心点的所有边界点。形成集群的必要条件是至少有一个核心点。尽管可能性很小，但我们可能有一个只有一个核心点及其边界点的集群。</p><p id="adc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点:</strong></p><ul class=""><li id="ee0c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">不需要预先指定簇的数量。</li><li id="2e58" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">适用于任意形状的集群。</li><li id="7f53" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">DBSCAN对异常值是鲁棒的，并且能够检测异常值。</li></ul><p id="dc89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ul class=""><li id="de05" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">在某些情况下，确定适当的邻域距离(eps)并不容易，这需要领域知识。</li><li id="4b88" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">如果集群在集群内密度方面差异很大，那么DBSCAN就不太适合定义集群。聚类的特征由eps-minPts参数的组合来定义。因为我们将一个eps-minPts组合传递给该算法，所以它不能很好地推广到具有很大不同密度的聚类。</li></ul></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="347c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读。如果您有任何反馈，请告诉我。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="092c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><ul class=""><li id="ccde" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">https://en.wikipedia.org/wiki/Phylogenetic_tree<a class="ae ky" href="https://en.wikipedia.org/wiki/Phylogenetic_tree" rel="noopener ugc nofollow" target="_blank"/></li><li id="d508" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">【https://en.wikipedia.org/wiki/DBSCAN T4】</li></ul></div></div>    
</body>
</html>