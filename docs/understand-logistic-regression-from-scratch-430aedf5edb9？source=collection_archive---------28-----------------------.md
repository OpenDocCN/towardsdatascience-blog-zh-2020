# ä»é›¶å¼€å§‹ç†è§£é€»è¾‘å›å½’â€” Kaggle ç¬”è®°æœ¬

> åŸæ–‡ï¼š<https://towardsdatascience.com/understand-logistic-regression-from-scratch-430aedf5edb9?source=collection_archive---------28----------------------->

## é€šè¿‡è‡ªå·±å®ç°æ¥å­¦ä¹ ç®—æ³•ã€‚

![](img/b878d7dc19be08af5fc211781fca42f6.png)

ç”±ä½œè€…åˆ›å»º

## ç›®å½•

1.ç›®æ ‡

2.åŠ è½½æ•°æ®

3.ä»æ–‡æœ¬ä¸­æå–ç‰¹å¾

4.å®æ–½é€»è¾‘å›å½’

*   4.1 æ¦‚è¿°
*   4.2 ä¹™çŠ¶ç»“è‚ 
*   4.3 æˆæœ¬å‡½æ•°
*   4.4 æ¢¯åº¦ä¸‹é™
*   4.5 æ­£è§„åŒ–

5.ç«è½¦æ¨¡å‹

6.æµ‹è¯•æˆ‘ä»¬çš„é€»è¾‘å›å½’

7.ç”¨ Scikit å­¦ä¹ é€»è¾‘å›å½’æµ‹è¯•

è®©æˆ‘ä»¬ç”¨ Python å¯¼å…¥æ‰€æœ‰å¿…è¦çš„æ¨¡å—ã€‚

```
# regular expression operations
import re    
# string operation 
import string  
# shuffle the list
from random import shuffle

# linear algebra
import numpy as np 
# data processing
import pandas as pd 

# NLP library
import nltk
# download twitter dataset
from nltk.corpus import twitter_samples                          

# module for stop words that come with NLTK
from nltk.corpus import stopwords          
# module for stemming
from nltk.stem import PorterStemmer        
# module for tokenizing strings
from nltk.tokenize import TweetTokenizer   

# scikit model selection
from sklearn.model_selection import train_test_split

# smart progressor meter
from tqdm import tqdm
```

## 1.ç›®æ ‡

è¿™ä¸ªå†…æ ¸çš„ç›®æ ‡æ˜¯ä½¿ç”¨ twitter æ•°æ®é›†ä»é›¶å¼€å§‹å®ç°ç”¨äºæƒ…æ„Ÿåˆ†æçš„é€»è¾‘å›å½’ã€‚æˆ‘ä»¬å°†ä¸»è¦å…³æ³¨é€»è¾‘å›å½’çš„æ„å»ºæ¨¡å—ã€‚è¿™ä¸ªå†…æ ¸å¯ä»¥æä¾›å¯¹ ***å†…éƒ¨å¦‚ä½•è¿›è¡Œé€»è¾‘å›å½’*** çš„æ·±å…¥ç†è§£ã€‚ä½¿ç”¨ [JupytertoMedium](https://pypi.org/project/jupyter-to-medium/) python åº“å°†ç¬”è®°æœ¬è½¬æ¢æˆä¸­å‹æ–‡ç« ã€‚Kaggle ç¬”è®°æœ¬å¯ä»[è¿™é‡Œ](https://www.kaggle.com/narendrageek/understand-the-logistic-regression-from-scratch)è·å¾—ã€‚

ç»™å®šä¸€æ¡æ¨æ–‡ï¼Œå¦‚æœå®ƒæœ‰**æ­£é¢æƒ…ç»ªï¼Œå®ƒå°†è¢«åˆ†ç±»ğŸ‘æˆ–è€…æ¶ˆææƒ…ç»ªğŸ‘**ã€‚è¿™å¯¹åˆå­¦è€…å’Œå…¶ä»–äººéƒ½å¾ˆæœ‰ç”¨ã€‚

## 2.åŠ è½½æ•°æ®

```
# Download the twitter sample data from NLTK repository
nltk.download('twitter_samples')
```

*   `twitter_samples`åŒ…å« 5000 æ¡æ­£é¢æ¨æ–‡å’Œ 5000 æ¡è´Ÿé¢æ¨æ–‡ã€‚æ€»å…±æœ‰ 10ï¼Œ000 æ¡æ¨æ–‡ã€‚
*   æˆ‘ä»¬æ¯ä¸ªç­éƒ½æœ‰ç›¸åŒæ•°é‡çš„æ•°æ®æ ·æœ¬ã€‚
*   è¿™æ˜¯ä¸€ä¸ªå¹³è¡¡çš„æ•°æ®é›†ã€‚

```
# read the positive and negative tweets
pos_tweets = twitter_samples.strings('positive_tweets.json')
neg_tweets = twitter_samples.strings('negative_tweets.json')
print(f"positive sentiment ğŸ‘ total samples {len(pos_tweets)} \nnegative sentiment ğŸ‘ total samples {len(neg_tweets)}")positive sentiment ğŸ‘ total samples 5000 
negative sentiment ğŸ‘ total samples 5000# Let's have a look at the data
no_of_tweets = 3
print(f"Let's take a look at first {no_of_tweets} sample tweets:\n")
print("Example of Positive tweets:")
print('\n'.join(pos_tweets[:no_of_tweets]))
print("\nExample of Negative tweets:")
print('\n'.join(neg_tweets[:no_of_tweets]))Let's take a look at first 3 sample tweets:
```

**è¾“å‡º:**

```
Example of Positive tweets:
#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)
@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!
@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!

Example of Negative tweets:
hopeless for tmr :(
Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(
@Hegelbon That heart sliding into the waste basket. :(
```

*   æ¨æ–‡å¯èƒ½åŒ…å« URLã€æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ã€‚

## é¢„å¤„ç†æ–‡æœ¬

é¢„å¤„ç†æ˜¯æµæ°´çº¿ä¸­çš„é‡è¦æ­¥éª¤ä¹‹ä¸€ã€‚å®ƒåŒ…æ‹¬åœ¨å»ºç«‹æœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹å‰æ¸…ç†å’Œåˆ é™¤ä¸å¿…è¦çš„æ•°æ®ã€‚

é¢„å¤„ç†æ­¥éª¤:

1.  å¯¹å­—ç¬¦ä¸²è¿›è¡Œæ ‡è®°
2.  å°† tweet è½¬æ¢æˆå°å†™ï¼Œå¹¶å°† tweet æ‹†åˆ†æˆä»¤ç‰Œ(å•è¯)
3.  åˆ é™¤åœç”¨å­—è¯å’Œæ ‡ç‚¹ç¬¦å·
4.  åˆ é™¤ twitter å¹³å°ä¸Šçš„å¸¸ç”¨è¯ï¼Œå¦‚æ ‡ç­¾ã€è½¬å‘æ ‡è®°ã€è¶…é“¾æ¥ã€æ•°å­—å’Œç”µå­é‚®ä»¶åœ°å€
5.  å µå¡ç‰©

*   è¿™æ˜¯æŠŠä¸€ä¸ªå•è¯è½¬æ¢æˆå®ƒæœ€æ™®é€šå½¢å¼çš„è¿‡ç¨‹ã€‚å®ƒæœ‰åŠ©äºå‡å°‘æˆ‘ä»¬çš„è¯æ±‡é‡ã€‚ä¾‹å¦‚ï¼Œengage è¿™ä¸ªè¯æœ‰ä¸åŒçš„è¯å¹²ï¼Œ
*   **è®¢å©š**
*   è®¢å©šçš„
*   è®¢å©š

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å®ç°è¿™ä¸€ç‚¹ã€‚

```
# helper class for doing preprocessing
class Twitter_Preprocess():

    def __init__(self):
        # instantiate tokenizer class
        self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                                       reduce_len=True)
        # get the english stopwords 
        self.stopwords_en = stopwords.words('english') 
        # get the english punctuation
        self.punctuation_en = string.punctuation
        # Instantiate stemmer object
        self.stemmer = PorterStemmer() 

    def __remove_unwanted_characters__(self, tweet):

        # remove retweet style text "RT"
        tweet = re.sub(r'^RT[\s]+', '', tweet)

        # remove hyperlinks
        tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)

        # remove hashtags
        tweet = re.sub(r'#', '', tweet)

        #remove email address
        tweet = re.sub('\S+@\S+', '', tweet)

        # remove numbers
        tweet = re.sub(r'\d+', '', tweet)

        ## return removed text
        return tweet

    def __tokenize_tweet__(self, tweet):        
        # tokenize tweets
        return self.tokenizer.tokenize(tweet)

    def __remove_stopwords__(self, tweet_tokens):
        # remove stopwords
        tweets_clean = []

        for word in tweet_tokens:
            if (word not in self.stopwords_en and  # remove stopwords
                word not in self.punctuation_en):  # remove punctuation
                tweets_clean.append(word)
        return tweets_clean

    def __text_stemming__(self,tweet_tokens):
        # store the stemmed word
        tweets_stem = [] 

        for word in tweet_tokens:
            # stemming word
            stem_word = self.stemmer.stem(word)  
            tweets_stem.append(stem_word)
        return tweets_stem

    def preprocess(self, tweets):
        tweets_processed = []
        for _, tweet in tqdm(enumerate(tweets)):        
            # apply removing unwated characters and remove style of retweet, URL
            tweet = self.__remove_unwanted_characters__(tweet)            
            # apply nltk tokenizer
/            tweet_tokens = self.__tokenize_tweet__(tweet)            
            # apply stop words removal
            tweet_clean = self.__remove_stopwords__(tweet_tokens)
            # apply stemmer 
            tweet_stems = self.__text_stemming__(tweet_clean)
            tweets_processed.extend([tweet_stems])
        return tweets_processed# initilize the text preprocessor class object
twitter_text_processor = Twitter_Preprocess()

# process the positive and negative tweets
processed_pos_tweets = twitter_text_processor.preprocess(pos_tweets)
processed_neg_tweets = twitter_text_processor.preprocess(neg_tweets)5000it [00:02, 2276.81it/s]
5000it [00:02, 2409.93it/s]
```

è®©æˆ‘ä»¬çœ‹çœ‹é¢„å¤„ç† tweets åå¾—åˆ°äº†ä»€ä¹ˆè¾“å‡ºã€‚æˆ‘ä»¬èƒ½å¤ŸæˆåŠŸå¤„ç†æ¨æ–‡ï¼Œè¿™å¾ˆå¥½ã€‚

```
pos_tweets[:no_of_tweets], processed_pos_tweets[:no_of_tweets](['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',
  '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',
  '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!'],
 [['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)'],
  ['hey',
   'jame',
   'odd',
   ':/',
   'pleas',
   'call',
   'contact',
   'centr',
   'abl',
   'assist',
   ':)',
   'mani',
   'thank'],
  ['listen', 'last', 'night', ':)', 'bleed', 'amaz', 'track', 'scotland']])
```

## 3.ä»æ–‡æœ¬ä¸­æå–ç‰¹å¾

*   ç»™å®šæ–‡æœ¬ï¼Œä»¥è¿™æ ·ä¸€ç§æ–¹å¼è¡¨ç¤º`features (numeric values)`æ˜¯éå¸¸é‡è¦çš„ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚

## 3.1 åˆ›å»ºä¸€ä¸ªå•è¯åŒ…(BOW)è¡¨ç¤ºæ³•

BOW ä»£è¡¨å•è¯åŠå…¶åœ¨æ¯ä¸ªç±»ä¸­çš„å‡ºç°é¢‘ç‡ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª`dict`æ¥å­˜å‚¨æ¯ä¸ªå•è¯çš„`positive`å’Œ`negative`ç±»çš„é¢‘ç‡ã€‚è®©æˆ‘ä»¬æŒ‡å‡ºä¸€æ¡`positive`æ¨æ–‡æ˜¯`1`ï¼Œè€Œ`negative`æ¨æ–‡æ˜¯`0`ã€‚`dict`é”®æ˜¯ä¸€ä¸ªåŒ…å«`(word, y)`å¯¹çš„å…ƒç»„ã€‚`word`æ˜¯å¤„ç†è¿‡çš„å­—ï¼Œ`y`è¡¨ç¤ºç±»çš„æ ‡ç­¾ã€‚dict å€¼ä»£è¡¨ç±»`y`çš„`frequency of the word`ã€‚

ç¤ºä¾‹:#å•è¯ bad åœ¨ 0(è´Ÿ)ç±»ä¸­å‡ºç° 45 æ¬¡{(â€œbadâ€ï¼Œ0) : 32}

```
# word bad occurs 45 time in the 0 (negative) class 
{("bad", 0) : 45}# BOW frequency represent the (word, y) and frequency of y class
def build_bow_dict(tweets, labels):
    freq = {}
    ## create zip of tweets and labels
    for tweet, label in list(zip(tweets, labels)):
        for word in tweet:
            freq[(word, label)] = freq.get((word, label), 0) + 1

    return freq# create labels of the tweets
# 1 for positive labels and 0 for negative labels
labels = [1 for i in range(len(processed_pos_tweets))]
labels.extend([0 for i in range(len(processed_neg_tweets))])

# combine the positive and negative tweets
twitter_processed_corpus = processed_pos_tweets + processed_neg_tweets

# build Bog of words frequency 
bow_word_frequency = build_bow_dict(twitter_processed_corpus, labels)
```

ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰å„ç§æ–¹æ³•æ¥è¡¨ç¤º twitter è¯­æ–™åº“çš„ç‰¹å¾ã€‚ä¸€äº›åŸºæœ¬è€Œå¼ºå¤§çš„æŠ€æœ¯æ˜¯ï¼Œ

*   è®¡æ•°çŸ¢é‡å™¨
*   TF-IDF åŠŸèƒ½

## 1.è®¡æ•°çŸ¢é‡å™¨

è®¡æ•°çŸ¢é‡å™¨æŒ‡ç¤ºç¨€ç–çŸ©é˜µï¼Œå¹¶ä¸”è¯¥å€¼å¯ä»¥æ˜¯å•è¯çš„é¢‘ç‡**ã€‚åœ¨æˆ‘ä»¬çš„è¯­æ–™åº“ä¸­ï¼Œæ¯ä¸€åˆ—éƒ½æ˜¯å”¯ä¸€çš„æ ‡è®°ã€‚**

> ç¨€ç–çŸ©é˜µçš„ç»´æ•°å°†æ˜¯`*no of unique tokens in the corpus * no of sample tweets*`ã€‚

ç¤ºä¾‹:`corpus = [ 'This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?', ]`å¹¶ä¸” CountVectorizer è¡¨ç¤ºä¸º

`[[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]]`

## 2.TF-IDF(æœ¯è¯­é¢‘ç‡-é€†æ–‡æ¡£é¢‘ç‡)

TF-IDF ç»Ÿè®¡åº¦é‡ï¼Œç”¨äºè¯„ä¼°å•è¯ä¸æ–‡æ¡£é›†åˆä¸­çš„æ–‡æ¡£çš„ç›¸å…³ç¨‹åº¦ã€‚TF-IDF çš„è®¡ç®—å¦‚ä¸‹:

![](img/68e4884c6d2e08c0027b49b284d0f7db.png)

TF-IDF æ–¹ç¨‹

**è¯é¢‘:**è¯é¢‘ ***tf(tï¼Œd)*** ï¼Œæœ€ç®€å•çš„é€‰æ‹©å°±æ˜¯ä½¿ç”¨ä¸€ä¸ªè¯(è¯)åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°é¢‘ç‡ã€‚**é€†æ–‡æ¡£é¢‘ç‡:** ***idf(tï¼ŒD)*** è¡¡é‡å•è¯æä¾›å¤šå°‘ä¿¡æ¯ï¼Œå³å®ƒåœ¨æ‰€æœ‰æ–‡æ¡£ä¸­æ˜¯å¸¸è§è¿˜æ˜¯ç½•è§ã€‚å®ƒæ˜¯åŒ…å«è¯¥å•è¯çš„æ–‡æ¡£çš„é€†åˆ†æ•°çš„**å¯¹æ•°æ ‡åº¦**ã€‚å®šä¹‰è§[ç»´åŸº](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)ã€‚

## 3.2.ä¸ºæˆ‘ä»¬çš„æ¨¡å‹æå–ç®€å•ç‰¹å¾

*   ç»™å®šä¸€ä¸ªæ¨æ–‡åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†æå–ä¸¤ä¸ªç‰¹å¾ã€‚
*   ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯ä¸€æ¡æ¨æ–‡ä¸­æ­£é¢è¯çš„æ•°é‡ã€‚
*   ç¬¬äºŒä¸ªç‰¹å¾æ˜¯æ¨æ–‡ä¸­è´Ÿé¢è¯çš„æ•°é‡ã€‚

è¿™çœ‹ä¼¼ç®€å•ï¼Œä¸æ˜¯å—ï¼Ÿä¹Ÿè®¸æ˜¯çš„ã€‚æˆ‘ä»¬æ²¡æœ‰å‘ç¨€ç–çŸ©é˜µè¡¨ç¤ºæˆ‘ä»¬çš„ç‰¹å¾ã€‚å°†ä½¿ç”¨æœ€ç®€å•çš„ç‰¹å¾è¿›è¡Œåˆ†æã€‚

```
# extract feature for tweet
def extract_features(processed_tweet, bow_word_frequency):
    # feature array
    features = np.zeros((1,3))
    # bias term added in the 0th index
    features[0,0] = 1

    # iterate processed_tweet
    for word in processed_tweet:
        # get the positive frequency of the word
        features[0,1] = bow_word_frequency.get((word, 1), 0)
        # get the negative frequency of the word
        features[0,2] = bow_word_frequency.get((word, 0), 0)

    return features
```

æ‰“ä¹±è¯­æ–™åº“ï¼Œå°†è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¼€ã€‚

```
# shuffle the positive and negative tweets
shuffle(processed_pos_tweets)
shuffle(processed_neg_tweets)

# create positive and negative labels
positive_tweet_label = [1 for i in processed_pos_tweets]
negative_tweet_label = [0 for i in processed_neg_tweets]

# create dataframe
tweet_df = pd.DataFrame(list(zip(twitter_processed_corpus, positive_tweet_label+negative_tweet_label)), columns=["processed_tweet", "label"])
```

## 3.3 è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²

è®©æˆ‘ä»¬ä¿ç•™ 80%çš„æ•°æ®ç”¨äºè®­ç»ƒï¼Œ20%çš„æ•°æ®æ ·æœ¬ç”¨äºæµ‹è¯•ã€‚

```
# train and test split
train_X_tweet, test_X_tweet, train_Y, test_Y = train_test_split(tweet_df["processed_tweet"], tweet_df["label"], test_size = 0.20, stratify=tweet_df["label"])
print(f"train_X_tweet {train_X_tweet.shape}, test_X_tweet {test_X_tweet.shape}, train_Y {train_Y.shape}, test_Y {test_Y.shape}")train_X_tweet (8000,), test_X_tweet (2000,), train_Y (8000,), test_Y (2000,)# train X feature dimension
train_X = np.zeros((len(train_X_tweet), 3))

for index, tweet in enumerate(train_X_tweet):
    train_X[index, :] = extract_features(tweet, bow_word_frequency)

# test X feature dimension
test_X = np.zeros((len(test_X_tweet), 3))

for index, tweet in enumerate(test_X_tweet):
    test_X[index, :] = extract_features(tweet, bow_word_frequency)

print(f"train_X {train_X.shape}, test_X {test_X.shape}")train_X (8000, 3), test_X (2000, 3)
```

**è¾“å‡º:**

```
train_X[0:5]array([[1.000e+00, 6.300e+02, 0.000e+00],
       [1.000e+00, 6.930e+02, 0.000e+00],
       [1.000e+00, 1.000e+00, 4.570e+03],
       [1.000e+00, 1.000e+00, 4.570e+03],
       [1.000e+00, 3.561e+03, 2.000e+00]])
```

çœ‹ä¸€çœ‹æ ·æœ¬è®­ç»ƒç‰¹å¾ã€‚

*   ç¬¬ 0 ä¸ªç´¢å¼•æ˜¯æ·»åŠ çš„åå·®é¡¹ã€‚
*   ç¬¬ä¸€ä¸ªæŒ‡æ ‡ä»£è¡¨æ­£è¯é¢‘
*   ç¬¬äºŒä¸ªæŒ‡æ•°ä»£è¡¨è´Ÿè¯é¢‘

## 4.å®æ–½é€»è¾‘å›å½’

## 4.1 æ¦‚è¿°

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹é€»è¾‘å›å½’æ˜¯å¦‚ä½•å·¥ä½œå’Œå®ç°çš„ã€‚

å¾ˆå¤šæ—¶å€™ï¼Œå½“ä½ å¬åˆ°é€»è¾‘å›å½’æ—¶ï¼Œä½ å¯èƒ½ä¼šæƒ³ï¼Œè¿™æ˜¯ä¸€ä¸ªå›å½’é—®é¢˜ã€‚ä¸ï¼Œä¸æ˜¯ï¼Œ **Logistic å›å½’**æ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œæ˜¯ä¸€ä¸ªéçº¿æ€§æ¨¡å‹ã€‚

![](img/c002a89ec693b84600d4afa6d86ea2fb.png)

ç”±ä½œè€…åˆ›å»º

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¤§å¤šæ•°æœ€å¤§ä¼¼ç„¶ç®—æ³•æœ‰ 4 ä¸ªé˜¶æ®µï¼Œ

ç¬¬ä¸€æ­¥ã€‚åˆå§‹åŒ–æƒé‡

*   éšæœºæƒé‡å·²åˆå§‹åŒ–

ç¬¬äºŒæ­¥ã€‚åº”ç”¨åŠŸèƒ½

*   è®¡ç®—ä¹™çŠ¶ç»“è‚ 

ç¬¬ä¸‰æ­¥ã€‚è®¡ç®—æˆæœ¬(ç®—æ³•çš„ç›®æ ‡)

*   è®¡ç®—äºŒå…ƒåˆ†ç±»çš„å¯¹æ•°æŸå¤±

ç¬¬å››æ­¥ã€‚æ¢¯åº¦ä¸‹é™

*   è¿­ä»£æ›´æ–°æƒé‡ï¼Œç›´åˆ°æ‰¾åˆ°æœ€å°æˆæœ¬

é€»è¾‘å›å½’é‡‡ç”¨çº¿æ€§å›å½’ï¼Œå¹¶å°† **sigmoid** åº”ç”¨äºçº¿æ€§å›å½’çš„è¾“å‡ºã€‚å› æ­¤ï¼Œå®ƒäº§ç”Ÿäº†æ¯ä¸€ç±»çš„æ¦‚ç‡ï¼Œå…¶æ€»å’Œä¸º 1ã€‚

**å›å½’:**ä¸€å…ƒçº¿æ€§å›å½’æ–¹ç¨‹å¦‚ä¸‹:

![](img/3c68572ec470c6a18f6bc68eb88b4aa4.png)

å•å˜é‡çº¿æ€§å›å½’å…¬å¼

*   æ³¨æ„ï¼Œ**Î¸**å€¼æ˜¯ ***æƒé‡***
*   **x_0ï¼Œx_1ï¼Œx_2ï¼Œâ€¦ x_N** æ˜¯è¾“å…¥ç‰¹å¾

ä½ å¯èƒ½ä¼šæƒ³åˆ°è¿™ä¸ªæ–¹ç¨‹æœ‰å¤šå¤æ‚ã€‚æˆ‘ä»¬éœ€è¦å°†åœ¨`ith`ä½ç½®çš„æ¯ä¸ªç‰¹å¾çš„æ‰€æœ‰æƒé‡ç›¸ä¹˜ï¼Œç„¶åæ±‚å’Œã€‚

> å¥½åœ¨**çº¿æ€§ä»£æ•°**å¸¦æ¥äº†è¿™ä¸ªæ˜“æ“ä½œçš„æ–¹ç¨‹ã€‚æ²¡é”™ï¼Œå°±æ˜¯çŸ©é˜µ`*dot*`äº§å“ã€‚æ‚¨å¯ä»¥åº”ç”¨ç‰¹å¾å’Œæƒé‡çš„ç‚¹ç§¯æ¥æ‰¾åˆ° **z** ã€‚

## 4.2 ä¹™çŠ¶ç»“è‚ 

*   sigmoid å‡½æ•°å®šä¹‰ä¸º:

![](img/9f593c09ac0f3d74c4032153436bf378.png)

[ä¹™çŠ¶ç»“è‚ åŠŸèƒ½](https://en.wikipedia.org/wiki/Sigmoid_function)

å®ƒå°†è¾“å…¥â€œzâ€æ˜ å°„åˆ°ä¸€ä¸ªä»‹äº 0 å’Œ 1 ä¹‹é—´çš„å€¼ï¼Œå› æ­¤å®ƒå¯ä»¥è¢«è§†ä¸ºä¸€ä¸ª**æ¦‚ç‡**ã€‚

```
def sigmoid(z): 

    # calculate the sigmoid of z
    h = 1 / (1+ np.exp(-z))

    return h
```

## 4.3 æˆæœ¬å‡½æ•°

é€»è¾‘å›å½’ä¸­ä½¿ç”¨çš„æˆæœ¬å‡½æ•°æ˜¯:

![](img/ba3e27988e6d279e37bc4f82cd5bdaea.png)

è¿™å°±æ˜¯äºŒè¿›åˆ¶åˆ†ç±»çš„**æµ‹äº•æŸå¤±ã€‚**åœ¨é€»è¾‘å›å½’ä¸­è®¡ç®—æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„å¯¹æ•°æŸå¤±çš„å¹³å‡å€¼ï¼Œå¯¹æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„ç­‰å¼ ***3*** ä¿®æ”¹å¦‚ä¸‹:

![](img/9d019eb4e9d17942b1ef9fa78acf4fd9.png)

*   ***m*** æ˜¯è®­ç»ƒæ ·æœ¬çš„æ•°é‡
*   *æ˜¯ ***ä¸*** è®­ç»ƒå®ä¾‹çš„å®é™…æ ‡ç­¾ã€‚*
*   ****ã€h(z(\theta)^{(i)}ã€‘***ä¸º ***ä¸*** è®­ç»ƒæ ·æœ¬çš„æ¨¡å‹é¢„æµ‹ã€‚*

*å•ä¸ªè®­ç»ƒç¤ºä¾‹çš„æŸå¤±å‡½æ•°æ˜¯ï¼Œ*

*![](img/7185f6f9710d6f7fd3afeb667b4c2b2b.png)*

*æŸå¤±å‡½æ•°*

*   *æ‰€æœ‰çš„ ***h*** çš„å€¼éƒ½åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œæ‰€ä»¥æ—¥å¿—ä¼šæ˜¯è´Ÿæ•°ã€‚è¿™å°±æ˜¯å°†ç³»æ•°-1 åº”ç”¨äºä¸¤ä¸ªæŸå¤±é¡¹ä¹‹å’Œçš„åŸå› ã€‚*
*   *å½“æ¨¡å‹é¢„æµ‹ 1ï¼Œ(*h*(*z*(*Î¸*))= 1)ä¸”æ ‡ç­¾ ***y*** ä¹Ÿä¸º 1 æ—¶ï¼Œè¯¥è®­ç»ƒç¤ºä¾‹çš„æŸå¤±ä¸º 0ã€‚*
*   *åŒæ ·ï¼Œå½“æ¨¡å‹é¢„æµ‹ä¸º 0ï¼Œ(*h*(*z*(*Î¸*))= 0ï¼Œè€Œå®é™…æ ‡ç­¾ä¹Ÿä¸º 0 æ—¶ï¼Œè¯¥è®­ç»ƒç¤ºä¾‹çš„æŸå¤±ä¸º 0ã€‚*
*   *ä½†å½“æ¨¡å‹é¢„æµ‹æ¥è¿‘ 1(*h*(*z*(*Î¸*))= 0.9999)ä¸”æ ‡å·ä¸º 0 æ—¶ï¼Œå¯¹æ•°æŸå¤±çš„ç¬¬äºŒé¡¹å˜æˆä¸€ä¸ªå¾ˆå¤§çš„è´Ÿæ•°ï¼Œå†ä¹˜ä»¥-1 çš„æ€»å› å­ï¼Œè½¬æ¢æˆæ­£çš„æŸå¤±å€¼ã€‚1Ã—(1 0)Ã—*log*(1 0.9999)â‰ˆ9.2 æ¨¡å‹é¢„æµ‹è¶Šæ¥è¿‘ 1ï¼ŒæŸè€—è¶Šå¤§ã€‚*

## *4.4 æ¢¯åº¦ä¸‹é™*

*æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§ç”¨äº**è¿­ä»£æ›´æ–°æƒé‡*Î¸*ä»¥æœ€å°åŒ–ç›®æ ‡å‡½æ•°(æˆæœ¬)çš„ç®—æ³•ã€‚æˆ‘ä»¬éœ€è¦è¿­ä»£åœ°æ›´æ–°æƒé‡ï¼Œå› ä¸ºï¼Œ***

> *åœ¨åˆå§‹éšæœºæƒé‡ä¸‹ï¼Œæ¨¡å‹ä¸ä¼šå­¦åˆ°å¤ªå¤šä¸œè¥¿ã€‚ä¸ºäº†æ”¹è¿›é¢„æµ‹ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡å¤šæ¬¡è¿­ä»£ä»æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´éšæœºæƒé‡ã€‚*

*å¯¹äºæƒé‡ä¹‹ä¸€***Î¸_ J***çš„æˆæœ¬å‡½æ•° ***J*** çš„æ¢¯åº¦æ˜¯:*

*![](img/8e7b378da360008deb2a9c6bbf42323c.png)*

*æ¢¯åº¦å‡½æ•°*

## *4.5 æ­£è§„åŒ–*

*æ­£åˆ™åŒ–æ˜¯ä¸€ç§é€šè¿‡æƒ©ç½šæˆæœ¬å‡½æ•°æ¥è§£å†³æœºå™¨å­¦ä¹ ç®—æ³•ä¸­è¿‡æ‹Ÿåˆé—®é¢˜çš„æŠ€æœ¯ã€‚åœ¨æˆæœ¬å‡½æ•°ä¸­ä¼šæœ‰ä¸€ä¸ªé™„åŠ çš„æƒ©ç½šé¡¹ã€‚æœ‰ä¸¤ç§ç±»å‹çš„æ­£åˆ™åŒ–æŠ€æœ¯:*

*   *æ‹‰ç´¢(L1 èŒƒæ•°)æ­£åˆ™åŒ–*
*   *å²­(L2 èŒƒæ•°)æ­£åˆ™åŒ–*

***æ‹‰ç´¢å›å½’(L1)**L1-èŒƒæ•°æŸå¤±å‡½æ•°ä¹Ÿè¢«ç§°ä¸ºæœ€å°ç»å¯¹è¯¯å·®(LAE)ã€‚$Î»*âˆ‘ |w| $æ˜¯ä¸€ä¸ªæ­£åˆ™é¡¹ã€‚å®ƒæ˜¯$Î»$æ­£åˆ™åŒ–é¡¹ä¸æƒçš„ç»å¯¹å’Œçš„ä¹˜ç§¯ã€‚è¾ƒå°çš„å€¼è¡¨ç¤ºè¾ƒå¼ºçš„æ­£åˆ™åŒ–ã€‚*

***å²­å›å½’(L2)**L2-èŒƒæ•°æŸå¤±å‡½æ•°ä¹Ÿç§°ä¸ºæœ€å°äºŒä¹˜è¯¯å·®(LSE)ã€‚$Î»*âˆ‘ (w) $æ˜¯ä¸€ä¸ªæ­£åˆ™é¡¹ã€‚å®ƒæ˜¯$Î»$æ­£åˆ™åŒ–é¡¹ä¸æƒçš„å¹³æ–¹å’Œçš„ä¹˜ç§¯ã€‚è¾ƒå°çš„å€¼è¡¨ç¤ºè¾ƒå¼ºçš„æ­£åˆ™åŒ–ã€‚*

*ä½ ä¼šæ³¨æ„åˆ°ï¼Œè¿™æœ‰å¾ˆå¤§çš„ä¸åŒã€‚æ˜¯çš„ï¼Œå®ƒåšå¾—å¾ˆå¥½ã€‚ä¸»è¦çš„åŒºåˆ«åœ¨äºä½ åœ¨æˆæœ¬å‡½æ•°ä¸­åŠ å…¥äº†ä»€ä¹ˆç±»å‹çš„æ­£åˆ™é¡¹æ¥æœ€å°åŒ–è¯¯å·®ã€‚*

> *L2(å²­)ç¼©å°æ‰€æœ‰ç³»æ•°ç›¸åŒçš„æ¯”ä¾‹ï¼Œä½†å®ƒä¸æ¶ˆé™¤ä»»ä½•ç‰¹å¾ï¼Œè€Œ L1(æ‹‰ç´¢)å¯ä»¥ç¼©å°ä¸€äº›ç³»æ•°ä¸ºé›¶ï¼Œä¹Ÿæ‰§è¡Œç‰¹å¾é€‰æ‹©ã€‚*

*åœ¨ä¸‹é¢çš„ä»£ç ä¸­å°†æ·»åŠ  L2 æ­£åˆ™åŒ–*

```
*# implementation of gradient descent algorithm def gradientDescent(x, y, theta, alpha, num_iters, c): # get the number of samples in the training
    m = x.shape[0]

    for i in range(0, num_iters):

        # find linear regression equation value, X and theta
        z = np.dot(x, theta)

        # get the sigmoid of z
        h = sigmoid(z)

        # calculate the cost function, log loss
        #J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1-h)))

        # let's add L2 regularization
        # c is L2 regularizer term
        J = (-1/m) * ((np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1-h))) + (c * np.sum(theta)))

        # update the weights theta
        theta = theta - (alpha / m) * np.dot((x.T), (h - y))

    J = float(J)
    return J, theta*
```

## *5.ç«è½¦æ¨¡å‹*

*è®©æˆ‘ä»¬è®­ç»ƒæ¢¯åº¦ä¸‹é™å‡½æ•°æ¥ä¼˜åŒ–éšæœºåˆå§‹åŒ–çš„æƒé‡ã€‚åœ¨ç¬¬ 4 èŠ‚ä¸­å·²ç»ç»™å‡ºäº†ç®€è¦çš„è§£é‡Šã€‚*

```
*# set the seed in numpy
np.random.seed(1)
# Apply gradient descent of logistic regression
# 0.1 as added L2 regularization term
J, theta = gradientDescent(train_X, np.array(train_Y).reshape(-1,1), np.zeros((3, 1)), 1e-7, 1000, 0.1)
print(f"The cost after training is {J:.8f}.")
print(f"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}")The cost after training is 0.22154867.
The resulting vector of weights is [2.18e-06, 0.00270863, -0.00177371]*
```

## *6.æµ‹è¯•æˆ‘ä»¬çš„é€»è¾‘å›å½’*

*æ˜¯æ—¶å€™åœ¨æ¨¡å‹ä¹‹å‰æ²¡æœ‰è§è¿‡çš„æµ‹è¯•æ•°æ®ä¸Šæµ‹è¯•æˆ‘ä»¬çš„é€»è¾‘å›å½’å‡½æ•°äº†ã€‚*

*é¢„æµ‹ä¸€æ¡æ¨æ–‡æ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ã€‚*

*   *å°† sigmoid åº”ç”¨äº logits ä»¥è·å¾—é¢„æµ‹å€¼(ä»‹äº 0 å’Œ 1 ä¹‹é—´çš„å€¼)ã€‚*

*![](img/07d5dbb24323cf817ef18d3673875aef.png)*

*æ–°æ¨æ–‡é¢„æµ‹*

```
*# predict for the features from learned theata values
def predict_tweet(x, theta):

    # make the prediction for x with learned theta values
    y_pred = sigmoid(np.dot(x, theta))

    return y_pred# predict for the test sample with the learned weights for logistics regression
predicted_probs = predict_tweet(test_X, theta)
# assign the probability threshold to class
predicted_labels = np.where(predicted_probs > 0.5, 1, 0)
# calculate the accuracy
print(f"Own implementation of logistic regression accuracy is {len(predicted_labels[predicted_labels == np.array(test_Y).reshape(-1,1)]) / len(test_Y)*100:.2f}")Own implementation of logistic regression accuracy is 93.45*
```

*åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•è‡ªå·±å®ç°é€»è¾‘å›å½’ã€‚å¾—åˆ°äº† **94.45 çš„ç²¾åº¦ã€‚**è®©æˆ‘ä»¬çœ‹çœ‹æ¥è‡ªæµè¡Œçš„æœºå™¨å­¦ä¹ (ML) Python åº“çš„ç»“æœã€‚*

## *7.ç”¨ Scikit å­¦ä¹ é€»è¾‘å›å½’æµ‹è¯•*

*è¿™é‡Œï¼Œæˆ‘ä»¬å°†è®­ç»ƒå†…ç½® Python åº“ä¸­çš„é€»è¾‘å›å½’æ¥æ£€æŸ¥ç»“æœã€‚*

```
*# scikit learn logiticsregression and accuracy score metric
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
clf = LogisticRegression(random_state=42, penalty='l2')
clf.fit(train_X, np.array(train_Y).reshape(-1,1))
y_pred = clf.predict(test_X)/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  return f(**kwargs)print(f"Scikit learn logistic regression accuracy is {accuracy_score(test_Y , y_pred)*100:.2f}")Scikit learn logistic regression accuracy is 94.45*
```

*å¤ªå¥½äº†ï¼ï¼ï¼ã€‚ç»“æœéå¸¸æ¥è¿‘ã€‚*

*æœ€åï¼Œæˆ‘ä»¬è‡ªå·±å®ç°äº†é€»è¾‘å›å½’ï¼Œå¹¶å°è¯•ä½¿ç”¨å†…ç½®çš„ Scikit learn é€»è¾‘å›å½’æ¥è·å¾—ç±»ä¼¼çš„å‡†ç¡®æ€§ã€‚ä½†æ˜¯ï¼Œè¿™ç§ç‰¹å¾æå–çš„æ–¹æ³•éå¸¸ç®€å•å’Œç›´è§‚ã€‚*

****æˆ‘æ˜¯è¾¹åšè¾¹å­¦ã€‚*** *æ¬¢è¿åœ¨è¯„è®ºä¸­ç•™ä¸‹æ‚¨çš„æƒ³æ³•æˆ–ä»»ä½•å»ºè®®ã€‚éå¸¸æ„Ÿè°¢ä½ çš„åé¦ˆï¼Œå®ƒèƒ½å¢å¼ºæˆ‘çš„ä¿¡å¿ƒã€‚**

*ğŸ™æ„Ÿè°¢é˜…è¯»ï¼ä½ å¯ä»¥é€šè¿‡ LinkedIn è”ç³»æˆ‘ã€‚*