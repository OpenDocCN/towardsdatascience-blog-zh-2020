<html>
<head>
<title>Build a Simple Neural Network using Numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Numpy 构建一个简单的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-a-simple-neural-network-using-numpy-2add9aad6fc8?source=collection_archive---------19-----------------------#2020-05-15">https://towardsdatascience.com/build-a-simple-neural-network-using-numpy-2add9aad6fc8?source=collection_archive---------19-----------------------#2020-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9c25" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 NumPy 构建单个神经元进行图像分类。</h2></div><p id="29a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将讨论如何使用 NumPy 制作一个简单的神经网络。</p><ol class=""><li id="eaa7" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu">导入库</strong></li></ol><p id="2f41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们将导入所有我们需要的包。我们将需要<strong class="kk iu"> <em class="ln"> numpy </em> </strong>，<strong class="kk iu"> <em class="ln"> h5py </em> </strong>(用于加载存储在 H5 文件中的数据集)，以及<strong class="kk iu"> <em class="ln"> matplotlib </em> </strong>(用于绘图)。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="1b3b" class="lx ly it lt b gy lz ma l mb mc"><strong class="lt iu">import</strong> numpy as np<br/><strong class="lt iu">import</strong> matplotlib.pyplot as plt<br/><strong class="lt iu">import</strong> h5py</span></pre><p id="fc07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 2。数据准备</strong></p><p id="cdbd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据以(" . h5 ")格式提供，包含标记为 cat 或非 cat 的训练和测试图像集。该数据集可从<a class="ae md" href="https://rpaudel42.github.io/datasets/NN.zip" rel="noopener ugc nofollow" target="_blank"> github repo </a>下载。使用以下函数加载数据集:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="3773" class="lx ly it lt b gy lz ma l mb mc"><strong class="lt iu">def</strong> <strong class="lt iu">load_dataset</strong>():<br/>    train_dataset = h5py.File('datasets/train_catvnoncat.h5', "r")<br/>    train_x = np.array(train_dataset["train_set_x"][:]) <br/>    train_y = np.array(train_dataset["train_set_y"][:])</span><span id="23d7" class="lx ly it lt b gy me ma l mb mc">test_dataset = h5py.File('datasets/test_catvnoncat.h5', "r")<br/>    test_x = np.array(test_dataset["test_set_x"][:]) <br/>    test_y = np.array(test_dataset["test_set_y"][:])</span><span id="281b" class="lx ly it lt b gy me ma l mb mc">classes = np.array(test_dataset["list_classes"][:]) <br/>    <br/>    train_y = train_y.reshape((1, train_y.shape[0]))<br/>    test_y = test_y.reshape((1, test_y.shape[0]))<br/>    <br/>    return train_x, train_y, test_x, test_y, classes</span></pre><p id="39c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过观察它们的形状来分析数据。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="51d9" class="lx ly it lt b gy lz ma l mb mc">train_x, train_y, test_x, test_y, classes = load_dataset()</span><span id="4210" class="lx ly it lt b gy me ma l mb mc">print ("Train X shape: " + str(train_x.shape))<br/>print ("Train Y shape: " + str(train_y.shape))</span><span id="b6e5" class="lx ly it lt b gy me ma l mb mc">print ("Test X shape: " + str(test_x.shape))<br/>print ("Test Y shape: " + str(test_y.shape))</span></pre><figure class="lo lp lq lr gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mf"><img src="../Images/03b8b05b1d88a832409a4194c420d1df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oS7fG5w2KTDBxACQVytC3Q.png"/></div></div></figure><p id="7a26" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们有 209 个训练图像，其中每个图像是正方形(高度= 64px)和(宽度= 64px)且具有 3 个通道(RGB)。类似地，我们有 50 个相同维度的测试图像。</p><p id="1e55" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们想象一下这个图像。您可以更改索引来查看不同的图像。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="2ca8" class="lx ly it lt b gy lz ma l mb mc"><em class="ln"># change index for another image</em><br/>index = 2<br/>plt.imshow(train_x[index])</span></pre><figure class="lo lp lq lr gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mf"><img src="../Images/9a5e9bf2484704a6df9e994a4042e75d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gQh4NsUY2YBQDDEvjXapA.png"/></div></div></figure><p id="5570" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">数据预处理:</strong>图像数据常见的数据预处理包括:</p><ol class=""><li id="7448" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">算出数据的维度和形状(m_train，m_test，num_px，…)</li><li id="2047" class="le lf it kk b kl mn ko mo kr mp kv mq kz mr ld lj lk ll lm bi translated">重塑数据集，使每个示例现在都是一个大小为(高度*宽度*通道，1)的向量</li><li id="42a1" class="le lf it kk b kl mn ko mo kr mp kv mq kz mr ld lj lk ll lm bi translated">将数据“标准化”</li></ol><p id="6503" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们需要使图像变平。这可以通过在形状的 numpy 数组(高度÷宽度÷通道，1)中重塑形状(高度、宽度、通道)的图像来完成。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="8efc" class="lx ly it lt b gy lz ma l mb mc">train_x = train_x.reshape(train_x.shape[0], -1).T<br/>test_x = test_x.reshape(test_x.shape[0], -1).T</span><span id="5904" class="lx ly it lt b gy me ma l mb mc">print ("Train X shape: " + str(train_x.shape))<br/>print ("Train Y shape: " + str(train_y.shape))</span><span id="ae75" class="lx ly it lt b gy me ma l mb mc">print ("Test X shape: " + str(test_x.shape))<br/>print ("Test Y shape: " + str(test_y.shape))</span></pre><figure class="lo lp lq lr gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ms"><img src="../Images/4bea498d623dbce5181088052aa61f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AaL3VP_3_fbxXW9Kg9uVFA.png"/></div></div></figure><p id="e831" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">标准化数据:</strong>机器学习中常见的预处理步骤是对数据集进行居中和标准化。对于给定的图片数据集，可以通过将数据集的每一行除以 255(像素通道的最大值)来完成。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="a8a5" class="lx ly it lt b gy lz ma l mb mc">train_x = train_x/255.<br/>test_x = test_x/255.</span></pre><p id="8d24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们将建立一个简单的神经网络模型，它可以正确地将图片分类为猫或非猫。</p><h2 id="47a2" class="lx ly it bd mt mu mv dn mw mx my dp mz kr na nb nc kv nd ne nf kz ng nh ni nj bi translated">3.神经网络模型</h2><p id="10a4" class="pw-post-body-paragraph ki kj it kk b kl nk ju kn ko nl jx kq kr nm kt ku kv nn kx ky kz no lb lc ld im bi translated">我们将构建一个神经网络，如下图所示。</p><figure class="lo lp lq lr gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi np"><img src="../Images/45405cf2dab2efa59db225c39788e997.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jzKGLRTLPElYP3DTMgDlzw.png"/></div></div></figure><p id="7a17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">关键步骤</strong>:构建神经网络的主要步骤有:</p><ol class=""><li id="49c4" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">定义模型结构(如输入特征数量、输出数量等)。)</li><li id="420f" class="le lf it kk b kl mn ko mo kr mp kv mq kz mr ld lj lk ll lm bi translated">初始化模型的参数(权重和偏差)</li><li id="b910" class="le lf it kk b kl mn ko mo kr mp kv mq kz mr ld lj lk ll lm bi translated">循环:</li></ol><ul class=""><li id="dbe9" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld nq lk ll lm bi translated">计算电流损耗(正向传播)</li><li id="5f60" class="le lf it kk b kl mn ko mo kr mp kv mq kz mr ld nq lk ll lm bi translated">计算电流梯度(反向传播)</li><li id="07cd" class="le lf it kk b kl mn ko mo kr mp kv mq kz mr ld nq lk ll lm bi translated">更新参数(梯度下降)</li></ul><p id="ccd4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3.1 激活功能</strong></p><p id="a1c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">sigmoid 激活函数由下式给出</p><figure class="lo lp lq lr gt mg gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c3b5fe32ff9ab2bcf5d3cf7f90c7c9ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*sF8pcVKFObMog1vOiDsVVg.png"/></div></figure><p id="e5a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以使用<em class="ln"> np.exp()计算 sigmoid 激活函数。</em></p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="5ac6" class="lx ly it lt b gy lz ma l mb mc">def <strong class="lt iu">sigmoid</strong>(z):<br/>    return 1/(1+np.exp(-z))</span></pre><p id="966b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3.2 初始化参数</strong></p><p id="679b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要初始化参数𝑤(权重)和𝑏(偏差)。在下面的例子中，𝑤使用<em class="ln"> np.random.randn() </em>初始化为一个随机数向量，而𝑏初始化为零。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="2dee" class="lx ly it lt b gy lz ma l mb mc">def <strong class="lt iu">initialize_parameters</strong>(dim):<br/>    w = np.random.randn(dim, 1)*0.01<br/>    b = 0<br/>    return w, b</span></pre><p id="2056" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3.3 正向和反向传播</strong></p><p id="7f9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦参数被初始化，我们就可以执行“向前”和“向后”传播步骤来学习参数。</p><ul class=""><li id="9779" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld nq lk ll lm bi translated">给定一组输入特征(X)。</li><li id="8e53" class="le lf it kk b kl mn ko mo kr mp kv mq kz mr ld nq lk ll lm bi translated">我们将如下计算激活函数。</li></ul><figure class="lo lp lq lr gt mg gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0134f86a9a8a41c0bccfafd1ab282bb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*Bf5JXOKQtpBgadTcw_LJRw.png"/></div></figure><ul class=""><li id="e470" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld nq lk ll lm bi translated">我们将按如下方式计算成本。</li></ul><figure class="lo lp lq lr gt mg gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/bbbb961647110ba6c5429462e47300e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*4P8h6_6HGaOjK-bh4k9hFA.png"/></div></figure><ul class=""><li id="e887" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld nq lk ll lm bi translated">最后，我们将如下计算梯度(反向传播)。</li></ul><figure class="lo lp lq lr gt mg gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/e6b2ef824a5a2464a9c5dbeca28ac495.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*jqxgWMA6j5ABzddUpw5gDQ.png"/></div></figure><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="d417" class="lx ly it lt b gy lz ma l mb mc">def <strong class="lt iu">propagate</strong>(w, b, X, Y):<br/>    m = X.shape[1]<br/>    <br/>    #calculate activation function<br/>    A = <strong class="lt iu">sigmoid</strong>(np.dot(w.T, X)+b)</span><span id="571c" class="lx ly it lt b gy me ma l mb mc">    #find the cost<br/>    cost = (-1/m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  <br/>    #find gradient (back propagation)<br/>    dw = (1/m) * np.dot(X, (A-Y).T)<br/>    db = (1/m) * np.sum(A-Y)</span><span id="a3ae" class="lx ly it lt b gy me ma l mb mc">    cost = np.squeeze(cost)<br/>    grads = {"dw": dw,<br/>             "db": db} <br/>    return grads, cost</span></pre><p id="196a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3.4 优化</strong></p><p id="12d0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在初始化参数、计算成本函数和计算梯度之后，我们现在可以使用梯度下降来更新参数。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="19ec" class="lx ly it lt b gy lz ma l mb mc">def <strong class="lt iu">gradient_descent</strong>(w, b, X, Y, iterations, learning_rate):<br/>    costs = []<br/>    for i in range(iterations):<br/>        grads, cost = propagate(w, b, X, Y)<br/>        <br/>        #update parameters<br/>        w = w - learning_rate * grads["dw"]<br/>        b = b - learning_rate * grads["db"]<br/>        costs.append(cost)<br/>        if i % 500 == 0:<br/>            print ("Cost after iteration %i: %f" %(i, cost))<br/>    <br/>    params = {"w": w,<br/>              "b": b}    <br/>    return params, costs</span></pre><p id="b450" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3.5 预测</strong></p><p id="ee94" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用学习到的参数<em class="ln"> w </em>和<em class="ln"> b，</em>我们可以预测训练或测试示例的标签。为了预测，我们首先需要计算如下给出的激活函数。</p><figure class="lo lp lq lr gt mg gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/5594cf25a8a64962768252f347860047.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*Z9IUuHJO7L117awisWGuFw.png"/></div></figure><p id="b346" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后将输出(预测)转换成 0(if<strong class="kk iu"><em class="ln">A</em></strong>&lt;= 0.5)或 1(if<strong class="kk iu"><em class="ln">A</em></strong>&gt;0.5)存储在<em class="ln"> y_pred </em>中。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="10ff" class="lx ly it lt b gy lz ma l mb mc">def <strong class="lt iu">predict</strong>(w, b, X):    <br/>    # number of example<br/>    m = X.shape[1]<br/>    y_pred = np.zeros((1,m))<br/>    w = w.reshape(X.shape[0], 1)<br/>    <br/>    A = sigmoid(np.dot(w.T, X)+b)<br/>    <br/>    for i in range(A.shape[1]):<br/>        y_pred[0,i] = 1 if A[0,i] &gt;0.5 else 0 <br/>        pass<br/>    return y_pred</span></pre><p id="b9f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.6 最终模型</p><p id="bb1b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以按正确的顺序把所有的构件放在一起，制作一个神经网络模型。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="14c8" class="lx ly it lt b gy lz ma l mb mc">def <strong class="lt iu">model</strong>(train_x, train_y, test_x, test_y, iterations, learning_rate):<br/>    w, b = <strong class="lt iu">initialize_parameters</strong>(train_x.shape[0])<br/>    parameters, costs = <strong class="lt iu">gradient_descent</strong>(w, b, train_x, train_y, iterations, learning_rate)<br/>    <br/>    w = parameters["w"]<br/>    b = parameters["b"]<br/>    <br/>    # predict <br/>    train_pred_y = <strong class="lt iu">predict</strong>(w, b, train_x)<br/>    test_pred_y = <strong class="lt iu">predict</strong>(w, b, test_x)</span><span id="e24a" class="lx ly it lt b gy me ma l mb mc">    print("Train Acc: {} %".format(100 - np.mean(np.abs(train_pred_y - train_y)) * 100))<br/>    print("Test Acc: {} %".format(100 - np.mean(np.abs(test_pred_y - test_y)) * 100))<br/>    <br/>    return costs</span></pre><p id="75e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用下面的代码，使用上面构建的模型对图像数据集进行训练和预测。我们将使用 0.005 的<em class="ln"> learning_rate </em>，并为 2000 次<em class="ln">迭代</em>训练模型。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="cde1" class="lx ly it lt b gy lz ma l mb mc">costs = <strong class="lt iu">model</strong>(train_x, train_y, test_x, test_y, iterations = 2000, learning_rate = 0.005)</span></pre><figure class="lo lp lq lr gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nw"><img src="../Images/f1c79ea3ef241242c81733fcefebeea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EQ-RzGWka_Q9y9EfqnjnRA.png"/></div></div></figure><p id="9e48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练准确率在 99%左右，这意味着我们的模型正在工作，并且以高概率拟合训练数据。测试准确率在 70%左右。给定简单的模型和小的数据集，我们可以认为它是一个好的模型。</p><p id="ca7b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们可以画出成本图，看看模型是如何学习参数的。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="5d46" class="lx ly it lt b gy lz ma l mb mc">plt.plot(costs)<br/>plt.ylabel('cost')<br/>plt.xlabel('iterations')<br/>plt.title("Learning rate =" + str(d["learning_rate"]))<br/>plt.show()</span></pre><figure class="lo lp lq lr gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nx"><img src="../Images/41db1217eca39dda58ab0c22b110f70e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*frS0JUUFA6wPFdJHA0REUA.png"/></div></div></figure><p id="7c53" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到每次迭代的成本都在下降，这表明参数正在被学习。</p><p id="5325" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae md" rel="noopener" target="_blank" href="/building-a-neural-network-with-a-single-hidden-layer-using-numpy-923be1180dbf">下篇</a>中，我们将讨论如何制作一个带隐藏层的神经。</p><blockquote class="ny nz oa"><p id="8725" class="ki kj ln kk b kl km ju kn ko kp jx kq ob ks kt ku oc kw kx ky od la lb lc ld im bi translated">在这里成为媒体会员<a class="ae md" href="https://medium.com/@rmesfrmpkr/membership" rel="noopener">并支持独立写作，每月 5 美元，可以完全访问媒体上的每个故事。</a></p></blockquote></div></div>    
</body>
</html>