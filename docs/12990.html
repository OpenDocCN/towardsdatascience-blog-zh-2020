<html>
<head>
<title>BERT for Text Classification with NO model training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于无模型训练的文本分类的 BERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classification-with-no-model-training-935fe0e42180?source=collection_archive---------1-----------------------#2020-09-07">https://towardsdatascience.com/text-classification-with-no-model-training-935fe0e42180?source=collection_archive---------1-----------------------#2020-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bf7a59d41b19f964ce71b34753c515bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgqxDMP5qD1HE_uM33zZrg.png"/></div></div></figure><div class=""/><div class=""><h2 id="ce48" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">如果没有带标签的训练集，请使用 BERT、单词嵌入和向量相似度</h2></div></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><h2 id="55da" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">摘要</h2><p id="99f0" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">您是否因为没有带标签的数据集而难以对文本数据进行分类？在本文中，我将使用 BERT 和 Python 解释如何执行一种基于相似性的“无监督”文本分类。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/f94b673ab86710650fa386a4a26df041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*t5Cte8JzO1x4btdQ7VS0iA.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="dc06" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated"><a class="ae nd" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"><strong class="ly jf">【NLP(自然语言处理)</strong> </a>是人工智能领域，研究计算机与人类语言之间的交互，特别是如何给计算机编程以处理和分析大量自然语言数据。NLP 通常用于文本数据的分类。<strong class="ly jf">文本分类</strong>就是根据文本数据的内容给文本数据分配类别的问题。为了执行分类用例，您需要一个用于机器学习模型训练的标记数据集。如果你没有，会发生什么？</p><p id="3daa" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">这种情况在现实世界中发生的次数比你想象的要多。如今，人工智能被大肆宣传，以至于企业甚至在没有数据的情况下也想使用它。特别是，大多数非技术人员并没有完全理解“目标变量”的概念，以及它在监督机器学习中是如何使用的。那么，当你有文本数据但没有标签时，如何构建一个分类器呢？在本教程中，我将解释一种应用 W2V 和 BERT 通过词向量相似度对文本进行分类的策略。</p><p id="f38a" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">我将展示一些有用的 Python 代码，这些代码可以很容易地应用于其他类似的情况(只需复制、粘贴、运行)，并通过注释遍历每一行代码，以便您可以复制这个示例(下面是完整代码的链接)。</p><div class="is it gp gr iu ne"><a href="https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jf gy z fp nj fr fs nk fu fw jd bi translated">mdipietro 09/data science _ 人工智能 _ 实用工具</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">github.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ja ne"/></div></div></a></div><p id="27b4" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">我将使用“<strong class="ly jf">新闻类别数据集</strong>，其中为您提供了从<em class="nt">赫芬顿邮报</em>获得的 2012 年至 2018 年的新闻标题，并要求您将它们分类到正确的类别，因此这是一个多类别分类问题(下面的链接)。</p><div class="is it gp gr iu ne"><a href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jf gy z fp nj fr fs nk fu fw jd bi translated">新闻类别数据集</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">根据标题和简短描述识别新闻的类型</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.kaggle.com</p></div></div><div class="nn l"><div class="nu l np nq nr nn ns ja ne"/></div></div></a></div><p id="841e" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">特别是，我将经历:</p><ul class=""><li id="4920" class="nv nw je ly b lz my mc mz lj nx ln ny lr nz mo oa ob oc od bi translated">设置:导入包，读取数据。</li><li id="39dd" class="nv nw je ly b lz oe mc of lj og ln oh lr oi mo oa ob oc od bi translated">预处理:清理文本数据。</li><li id="4c08" class="nv nw je ly b lz oe mc of lj og ln oh lr oi mo oa ob oc od bi translated">创建目标集群:使用 Word2Vec 和<em class="nt"> gensim </em>构建目标变量。</li><li id="afd8" class="nv nw je ly b lz oe mc of lj og ln oh lr oi mo oa ob oc od bi translated">特征工程:用<em class="nt">变形金刚</em>和 BERT <em class="nt">嵌入单词。</em></li><li id="206f" class="nv nw je ly b lz oe mc of lj og ln oh lr oi mo oa ob oc od bi translated">模型设计和测试:通过余弦相似性将观察值分配给集群，并评估性能。</li><li id="7e5f" class="nv nw je ly b lz oe mc of lj og ln oh lr oi mo oa ob oc od bi translated">可解释性:理解模型如何产生结果。</li></ul></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><h2 id="f13d" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">设置</h2><p id="db60" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">首先，我需要导入以下包:</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="ee9b" class="la lb je ok b gy oo op l oq or"><strong class="ok jf">## for data<br/></strong>import <strong class="ok jf">json<br/></strong>import <strong class="ok jf">pandas </strong>as pd<br/>import <strong class="ok jf">numpy </strong>as np<br/>from <strong class="ok jf">sklearn </strong>import metrics, manifold</span><span id="2b6a" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## for processing<br/></strong>import <strong class="ok jf">re</strong><br/>import <strong class="ok jf">nltk</strong></span><span id="a808" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## for plotting</strong><br/>import <strong class="ok jf">matplotlib</strong>.pyplot as plt<br/>import <strong class="ok jf">seaborn </strong>as sns</span><span id="06cb" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## for w2v</strong><br/>import <strong class="ok jf">gensim<br/></strong>import gensim.downloader as gensim_api</span><span id="b616" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## for bert</strong><br/>import <strong class="ok jf">transformers</strong></span></pre><p id="f203" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">数据集包含在一个 json 文件中，所以我将首先用<em class="nt"> json </em>把它读入一个字典列表，然后把它转换成一个<em class="nt"> pandas </em> Dataframe。</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="f23a" class="la lb je ok b gy oo op l oq or">lst_dics = []<br/>with <strong class="ok jf">open</strong>('data.json', mode='r', errors='ignore') as json_file:<br/>    for dic in json_file:<br/>        lst_dics.append( json<strong class="ok jf">.loads</strong>(dic) )</span><span id="40ed" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## print the first one</strong><br/>lst_dics[0]</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/bb93abf7957730b12bffc659571c6d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cfm0OUB6TwdBlzBCoHhAdg.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="b530" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">原始数据集包含超过 30 个类别，但是出于本教程的目的，我将使用 3 个类别的子集:娱乐、政治和技术。</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="4295" class="la lb je ok b gy oo op l oq or"><strong class="ok jf">## create dtf</strong><br/>dtf = pd.DataFrame(lst_dics)</span><span id="b4e2" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## filter categories</strong><br/>dtf = dtf[ dtf["category"].isin(['<strong class="ok jf">ENTERTAINMENT</strong>','<strong class="ok jf">POLITICS</strong>','<strong class="ok jf">TECH</strong>'])        ][["category","headline"]]</span><span id="6503" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## rename columns</strong><br/>dtf = dtf.rename(columns={"category":"<strong class="ok jf">y</strong>", "headline":"<strong class="ok jf">text</strong>"})</span><span id="ede3" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## print 5 random rows</strong><br/>dtf.sample(5)</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/c331c42eb10537ab4cfb69673ae00f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQV-xL4fo6xLBMP7od3hsg.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="4f6e" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">如您所见，数据集还包括一个目标变量。我不会将它用于建模，只是用于性能评估。</p><p id="fff2" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">所以我们有一些原始的文本数据，我们的任务是把它分成我们一无所知的 3 类(娱乐、政治、科技)。这是我计划要做的:</p><ul class=""><li id="1dac" class="nv nw je ly b lz my mc mz lj nx ln ny lr nz mo oa ob oc od bi translated">清理数据并将其嵌入向量空间，</li><li id="bdb2" class="nv nw je ly b lz oe mc of lj og ln oh lr oi mo oa ob oc od bi translated">为每个类别创建一个主题聚类并将其嵌入向量空间，</li><li id="ce66" class="nv nw je ly b lz oe mc of lj og ln oh lr oi mo oa ob oc od bi translated">计算每个文本向量和主题聚类之间的相似度，然后将其分配给最接近的聚类。</li></ul><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/8a7a4969d816d3b7a8ae5a2f85294557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7kOa3bhoGATgawvkA3Vvaw.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="f7af" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">这就是为什么我称之为“一种无监督的文本分类”。这是一个非常基本的想法，但是执行起来会很棘手。</p><p id="e4df" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">现在都准备好了，让我们开始吧。</p><h2 id="6259" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">预处理</h2><p id="11c5" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">绝对的第一步是对数据进行预处理:清理文本、删除停用词和应用词汇化。我将编写一个函数，并将其应用于整个数据集。</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="a4ea" class="la lb je ok b gy oo op l oq or"><strong class="ok jf">'''<br/>Preprocess a string.<br/>:parameter<br/>    :param text: string - name of column containing text<br/>    :param lst_stopwords: list - list of stopwords to remove<br/>    :param flg_stemm: bool - whether stemming is to be applied<br/>    :param flg_lemm: bool - whether lemmitisation is to be applied<br/>:return<br/>    cleaned text<br/>'''</strong><br/>def <strong class="ok jf">utils_preprocess_text</strong>(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):<br/>    <strong class="ok jf">## clean (convert to lowercase and remove punctuations and   <br/>    characters and then strip)</strong><br/>    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())<br/>            <br/>    <strong class="ok jf">## Tokenize (convert from string to list)</strong><br/>    lst_text = text.split()<strong class="ok jf">    </strong></span><span id="96b0" class="la lb je ok b gy os op l oq or"><strong class="ok jf">    ## remove Stopwords</strong><br/>    if lst_stopwords is not None:<br/>        lst_text = [word for word in lst_text if word not in <br/>                    lst_stopwords]<br/>                <br/>    <strong class="ok jf">## Stemming (remove -ing, -ly, ...)</strong><br/>    if flg_stemm == True:<br/>        ps = nltk.stem.porter.PorterStemmer()<br/>        lst_text = [ps.stem(word) for word in lst_text]<br/>                <br/>    <strong class="ok jf">## Lemmatisation (convert the word into root word)</strong><br/>    if flg_lemm == True:<br/>        lem = nltk.stem.wordnet.WordNetLemmatizer()<br/>        lst_text = [lem.lemmatize(word) for word in lst_text]<br/>            <br/>    <strong class="ok jf">## back to string from list</strong><br/>    text = " ".join(lst_text)<br/>    return text</span></pre><p id="e5a9" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">该函数从语料库中删除一组给定的单词。我可以用<em class="nt"> nltk </em>为英语词汇创建一个通用停用词列表(我们可以通过添加或删除单词来编辑这个列表)。</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="d15b" class="la lb je ok b gy oo op l oq or">lst_stopwords = <strong class="ok jf">nltk</strong>.corpus.stopwords.words("<strong class="ok jf">english</strong>")<br/>lst_stopwords</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/0256ecf866f72fba87f57927ab64ddfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZSIQRuoL7QrhpIxAa0R-jA.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="b625" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">现在，我将对整个数据集应用该函数，并将结果存储在一个名为“<em class="nt"> text_clean </em>的新列中，我将把它用作语料库。</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="490b" class="la lb je ok b gy oo op l oq or">dtf["<strong class="ok jf">text_clean</strong>"] = dtf["text"].apply(lambda x: <br/>          <strong class="ok jf">utils_preprocess_text</strong>(x, flg_stemm=False, <strong class="ok jf">flg_lemm=True</strong>, <br/>          <strong class="ok jf">lst_stopwords=lst_stopwords</strong>))</span><span id="1f30" class="la lb je ok b gy os op l oq or">dtf.head()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/e3aa1bdb08a36810789726250d592321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gC4l4PI4dv9YALTD4-O3iw.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="04bf" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">我们有了预处理的语料库，因此下一步是构建目标变量。基本上，我们在这里:</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/bd39f45e88440464b32913a23517e0de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ikFCUL_s1N42AbwBYyysOQ.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><h2 id="9218" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">创建目标集群</h2><p id="2de0" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">本节的目标是创建一些可以代表每个类别的上下文的关键字。通过进行一些文本分析，你可以很容易地发现，出现频率最高的 3 个词是“<em class="nt">电影</em>”、“<em class="nt">王牌</em>”和“<em class="nt">苹果</em>”(关于详细的文本分析教程，你可以查看<a class="ae nd" rel="noopener" target="_blank" href="/text-analysis-feature-engineering-with-nlp-502d6ea9225d">这篇文章</a>)。我建议从这些关键词开始。</p><p id="efad" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">让我们以政治类别为例:单词“<em class="nt"> trump </em>”可以有不同的含义，因此我们需要添加关键字来避免多义性问题(例如，“<em class="nt"> donald </em>”、“<em class="nt"> republican </em>”、“<em class="nt"> white house </em>”、“<em class="nt"> obama </em>”)。这项任务可以手动执行，或者您可以使用预先训练的 NLP 模型的帮助。您可以从<a class="ae nd" href="https://github.com/RaRe-Technologies/gensim-data" rel="noopener ugc nofollow" target="_blank"><em class="nt">genism-data</em></a><em class="nt"/>中加载一个预训练的单词嵌入模型，如下所示:</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="655b" class="la lb je ok b gy oo op l oq or">nlp = gensim_api.load("<strong class="ok jf">glove-wiki-gigaword-300</strong>")</span></pre><p id="671d" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">gensim 包有一个非常方便的功能，可以将任何给定单词的最相似的单词返回到词汇表中。</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="45df" class="la lb je ok b gy oo op l oq or">nlp.<strong class="ok jf">most_similar</strong>(["<strong class="ok jf">obama</strong>"], topn=3)</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/7c7bd92f9c81a200c71a25923ec56a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*teSWxsxsszCcipQ_gJJl3w.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="b64a" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">我将使用它为每个类别创建一个关键字字典:</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="b247" class="la lb je ok b gy oo op l oq or"><strong class="ok jf">## Function to apply</strong><br/>def <strong class="ok jf">get_similar_words</strong>(lst_words, top, nlp):<br/>    lst_out = lst_words<br/>    for tupla in nlp.most_similar(lst_words, topn=top):<br/>        lst_out.append(tupla[0])<br/>    return list(set(lst_out))<br/></span><span id="afbe" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## Create Dictionary {category:[keywords]}<br/></strong>dic_clusters = {}</span><span id="6c18" class="la lb je ok b gy os op l oq or">dic_clusters["<strong class="ok jf">ENTERTAINMENT</strong>"] = get_similar_words([<strong class="ok jf">'celebrity','cinema','movie','music'</strong>], <br/>                  top=30, nlp=nlp)</span><span id="1e98" class="la lb je ok b gy os op l oq or">dic_clusters[<strong class="ok jf">"POLITICS"</strong>] = get_similar_words([<strong class="ok jf">'gop','clinton','president','obama','republican'</strong>]<br/>                  , top=30, nlp=nlp)</span><span id="8ff9" class="la lb je ok b gy os op l oq or">dic_clusters["<strong class="ok jf">TECH</strong>"] = get_similar_words([<strong class="ok jf">'amazon','android','app','apple','facebook',<br/>                   'google','tech'</strong>], <br/>                   top=30, nlp=nlp)<br/></span><span id="bdde" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## print some</strong><br/>for k,v in dic_clusters.items():<br/>    print(k, ": ", v[0:5], "...", len(v))</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/5208d371e0202dc1ea06023b27e2cb8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XpL87qBhr1Q1aIcTXXIIzg.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="b912" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">让我们通过应用降维算法(即<a class="ae nd" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank"> TSNE </a>)来尝试在 2D 空间中可视化那些关键词。我们希望确保集群之间能够很好地分离。</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="6bed" class="la lb je ok b gy oo op l oq or"><strong class="ok jf">## word embedding<br/></strong>tot_words = [word for v in <strong class="ok jf">dic_clusters</strong>.values() for word in v]<br/>X = nlp[tot_words]<br/>        </span><span id="b678" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## pca</strong><br/>pca = manifold.<strong class="ok jf">TSNE</strong>(perplexity=40, n_components=2, init='pca')<br/>X = pca.fit_transform(X)</span><span id="7381" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## create dtf</strong><br/>dtf = pd.DataFrame()<br/>for k,v in <strong class="ok jf">dic_clusters</strong>.items():<br/>    size = len(dtf) + len(v)<br/>    dtf_group = pd.DataFrame(X[len(dtf):size], columns=["x","y"], <br/>                             index=v)<br/>    dtf_group["cluster"] = k<br/>    dtf = dtf.append(dtf_group)<br/>        </span><span id="36c2" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## plot</strong><br/>fig, ax = plt.subplots()<br/>sns.<strong class="ok jf">scatterplot</strong>(data=dtf, x="x", y="y", hue="cluster", ax=ax)</span><span id="f51c" class="la lb je ok b gy os op l oq or">ax.legend().texts[0].set_text(None)<br/>ax.set(xlabel=None, ylabel=None, xticks=[], xticklabels=[], <br/>       yticks=[], yticklabels=[])</span><span id="f5a3" class="la lb je ok b gy os op l oq or">for i in range(len(dtf)):<br/>    ax.annotate(dtf.index[i], <br/>               xy=(dtf["x"].iloc[i],dtf["y"].iloc[i]), <br/>               xytext=(5,2), textcoords='offset points', <br/>               ha='right', va='bottom')</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/bfaca6b1348a9924f8d1e707439e337d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a7-ci3xSj2aNXnfy9dEu6A.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="434c" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">酷，他们看起来已经足够孤立了。娱乐集群比政治集群更接近科技集群，这是有道理的，因为像“<em class="nt">苹果</em>”和“<em class="nt"> youtube </em>”这样的词可以同时出现在科技和娱乐新闻中。</p><h2 id="1f29" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">特征工程</h2><p id="afde" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">是时候将我们预处理的语料库和我们创建的目标聚类嵌入到同一个向量空间中了。基本上，我们是这样做的:</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/be98d23800c95e2897b91a88dc10c7f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ut247YNIbRqkLPK7lM6-Ng.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="a279" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">是的，我在用<a class="ae nd" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf"> BERT </strong> </a>做这个。的确，您可以利用任何单词嵌入模型(即 Word2Vec、Glove 等)，甚至是我们已经加载的定义关键字的模型，所以为什么要费心使用如此沉重和复杂的语言模型呢？这是因为 BERT 没有应用固定的嵌入，而是查看整个句子，然后给每个单词分配一个嵌入。因此，BERT 分配给一个单词的向量是整个句子的函数，因此一个单词可以基于上下文具有不同的向量。</p><p id="9da7" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">我将使用包<em class="nt"> transformers </em>加载原始预训练版本的 BERT，并给出一个动态嵌入的示例:</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="7d20" class="la lb je ok b gy oo op l oq or">tokenizer = transformers.<strong class="ok jf">BertTokenizer</strong>.from_pretrained('<strong class="ok jf">bert-base-<br/>            uncased'</strong>, do_lower_case=True)</span><span id="81fb" class="la lb je ok b gy os op l oq or">nlp = transformers.<strong class="ok jf">TFBertModel</strong>.from_pretrained(<strong class="ok jf">'bert-base-uncased'</strong>)</span></pre><p id="527a" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">让我们使用该模型将字符串"<em class="nt"> river bank </em>"转换成向量，并打印分配给单词"<em class="nt"> bank </em>"的向量:</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="02a2" class="la lb je ok b gy oo op l oq or">txt = <strong class="ok jf">"river bank"</strong></span><span id="564f" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## tokenize</strong><br/>idx = tokenizer.encode(txt)<br/>print("tokens:", tokenizer.convert_ids_to_tokens(idx))<br/>print("ids   :", tokenizer.encode(txt))</span><span id="2857" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## word embedding</strong><br/>idx = np.array(idx)[None,:]<br/>embedding = nlp(idx)<br/>print("shape:", embedding[0][0].shape)</span><span id="431c" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## vector of the second input word</strong><br/>embedding[0][0][2]</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/e7cf0221859bf1584f5848bd69426d28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0J0E8ts5RhkRv_2taEAUVg.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="85b1" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">如果您对字符串“<em class="nt">金融银行</em>”做同样的处理，您会发现分配给单词“<em class="nt">银行</em>的向量因上下文而异。请注意，BERT 记号赋予器在句子的开头和结尾插入特殊记号，其向量空间的维数为 768(为了更好地理解 BERT 如何处理文本，您可以查看<a class="ae nd" rel="noopener" target="_blank" href="/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794">这篇文章</a>)。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/35c538412e537c1f5190d4a7cfe6df08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xK4rVX1Ovlr_3HIG837nsQ.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="cedf" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">说了这么多，计划是用 BERT Word Embedding 用一个数组(shape: number of tokens x 768)表示每篇文本，然后把每篇文章汇总成一个均值向量。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/3e242f0f37c1bfd473f7c28960d83b26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_TKFRFPv2_km9fXRwlOMnw.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="401d" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">因此，最终的特征矩阵将是一个形状为:文档数(或均值向量)x 768 的数组。</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="8e81" class="la lb je ok b gy oo op l oq or"><strong class="ok jf">## function to apply<br/></strong>def <strong class="ok jf">utils_bert_embedding</strong>(txt, tokenizer, nlp):<br/>    idx = tokenizer.encode(txt)<br/>    idx = np.array(idx)[None,:]  <br/>    embedding = nlp(idx)<br/>    X = np.array(embedding[0][0][1:-1])<br/>    return X</span><span id="510d" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## create list of news vector</strong><br/>lst_mean_vecs = [<strong class="ok jf">utils_bert_embedding</strong>(txt, tokenizer, nlp)<strong class="ok jf">.mean(0)</strong> <br/>                 for txt in dtf["<strong class="ok jf">text_clean</strong>"]]</span><span id="f54b" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## create the feature matrix (n news x 768)</strong><br/>X = np.array(lst_mean_vecs)</span></pre><p id="5932" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">我们可以对目标集群中的关键字做同样的事情。事实上，每个标签都由一个单词列表来标识，帮助 BERT 理解集群中的上下文。因此，我将创建一个字典标签:聚类均值向量。</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="0d28" class="la lb je ok b gy oo op l oq or">dic_y = {k:<strong class="ok jf">utils_bert_embedding</strong>(v, tokenizer, nlp)<strong class="ok jf">.mean(0)</strong> for k,v<br/>         in dic_clusters.items()}</span></pre><p id="8b81" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">我们开始时只有一些文本数据和 3 个字符串(<em class="nt">“娱乐”、“政治”、“技术”</em>)，现在我们有了一个特征矩阵和一个目标变量… ish。</p><h2 id="5fd8" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">模型设计和测试</h2><p id="fef3" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">最后，是时候建立一个模型，根据与每个目标聚类的相似性对新闻进行分类了。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/9b2527fd377c64fb6aaafa95bc37a29e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QLf4HWQogYtmypFGboDxYQ.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="d23e" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">我将使用<a class="ae nd" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">余弦相似度</strong> </a>，这是一种基于两个非零向量之间的角度余弦的相似性度量，它等于归一化为长度都为 1 的相同向量的内积。您可以轻松地使用<a class="ae nd" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html" rel="noopener ugc nofollow" target="_blank"> <em class="nt"> scikit 的余弦相似性实现-learn </em> </a> <em class="nt">，</em>它采用 2 个数组(或向量)并返回一个分数数组(或单个分数)。在这种情况下，输出将是一个具有形状的矩阵:新闻数量 x 标签数量(3，娱乐/政治/技术)。换句话说，每行将代表一篇文章，并包含每个目标聚类的一个相似性得分。</p><p id="73ad" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">为了运行通常的评估指标(准确性、AUC、精确度、召回率等)，我们必须重新调整每一行的分数，使它们的总和为 1，并决定文章的类别。我将选择得分最高的一个，但设置一些最低阈值并忽略得分非常低的预测可能是明智的。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/c4d42a0cf2a7fa9cfd342a01bc77f84e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RuDmaE47lt20i0ZedgUYXA.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="9d7f" class="la lb je ok b gy oo op l oq or"><strong class="ok jf">#--- Model Algorithm ---#</strong></span><span id="1a34" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## compute cosine similarities</strong><br/>similarities = np.array(<br/>            [metrics.pairwise.<strong class="ok jf">cosine_similarity</strong>(X, y).T.tolist()[0] <br/>             for y in dic_y.values()]<br/>            ).T</span><span id="5c5b" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## adjust and rescale</strong><br/>labels = list(dic_y.keys())<br/>for i in range(len(similarities)):</span><span id="3343" class="la lb je ok b gy os op l oq or"><strong class="ok jf">    ### assign randomly if there is no similarity</strong><br/>    if sum(similarities[i]) == 0:<br/>       similarities[i] = [0]*len(labels)<br/>       similarities[i][np.random.choice(range(len(labels)))] = 1</span><span id="3139" class="la lb je ok b gy os op l oq or"><strong class="ok jf">    ### rescale so they sum = 1</strong><br/>    similarities[i] = similarities[i] / sum(similarities[i])</span><span id="6b66" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## classify the label with highest similarity score<br/></strong>predicted_prob = similarities<br/>predicted = [labels[np.argmax(pred)] for pred in predicted_prob]</span></pre><p id="777a" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">就像在经典的监督用例中一样，我们有一个具有预测概率的对象(这里它们是调整后的相似性分数),另一个具有预测标签。让我们检查一下我们做得如何:</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="f33e" class="la lb je ok b gy oo op l oq or">y_test = dtf[<strong class="ok jf">"y"</strong>].values<br/>classes = np.unique(y_test)<br/>y_test_array = pd.get_dummies(y_test, drop_first=False).values</span><span id="b9a7" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## Accuracy, Precision, Recall</strong><br/>accuracy = metrics.accuracy_score(y_test, predicted)<br/>auc = metrics.roc_auc_score(y_test, predicted_prob, <br/>                            multi_class="ovr")<br/>print("Accuracy:",  round(accuracy,2))<br/>print("Auc:", round(auc,2))<br/>print("Detail:")<br/>print(metrics.classification_report(y_test, predicted))<br/>    </span><span id="f2e6" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## Plot confusion matrix</strong><br/>cm = metrics.confusion_matrix(y_test, predicted)<br/>fig, ax = plt.subplots()<br/>sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, <br/>            cbar=False)<br/>ax.set(xlabel="Pred", ylabel="True", xticklabels=classes, <br/>       yticklabels=classes, title="Confusion matrix")<br/>plt.yticks(rotation=0)<br/>fig, ax = plt.subplots(nrows=1, ncols=2)</span><span id="f6ed" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## Plot roc</strong><br/>for i in range(len(classes)):<br/>    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  <br/>                           predicted_prob[:,i])<br/>    ax[0].plot(fpr, tpr, lw=3, <br/>              label='{0} (area={1:0.2f})'.format(classes[i], <br/>                              metrics.auc(fpr, tpr))<br/>               )<br/>ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')<br/>ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], <br/>          xlabel='False Positive Rate', <br/>          ylabel="True Positive Rate (Recall)", <br/>          title="Receiver operating characteristic")<br/>ax[0].legend(loc="lower right")<br/>ax[0].grid(True)<br/>    </span><span id="16eb" class="la lb je ok b gy os op l oq or"><strong class="ok jf">## Plot precision-recall curve<br/></strong>for i in range(len(classes)):<br/>    precision, recall, thresholds = metrics.precision_recall_curve(<br/>                 y_test_array[:,i], predicted_prob[:,i])<br/>    ax[1].plot(recall, precision, lw=3, <br/>               label='{0} (area={1:0.2f})'.format(classes[i], <br/>                                  metrics.auc(recall, precision))<br/>              )<br/>ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', <br/>          ylabel="Precision", title="Precision-Recall curve")<br/>ax[1].legend(loc="best")<br/>ax[1].grid(True)<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pi"><img src="../Images/57cb3c6d73895c5a2b0fa48f6d1f89c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4K6dROUaPlvR9RlxqYQ9xg.png"/></div></div></figure><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pj"><img src="../Images/7d46d5866808ab0572f9c6e30cb8ceaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CdwpPMxrHfsDK3tEbpLUng.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="97d7" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">好吧，我第一个说这不是我见过的最好的准确度。另一方面，考虑到我们没有训练任何模型，我们甚至虚构了目标变量，这一点也不差。主要问题是分类为娱乐的 4k 以上的政治观察，但这些性能可以通过微调这两个类别的关键字来轻松改善。</p><h2 id="cf26" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">可解释性</h2><p id="021d" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">让我们试着理解是什么让我们的算法用一个类别而不是其他类别对新闻进行分类。让我们从语料库中随机观察一下:</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="d224" class="la lb je ok b gy oo op l oq or">i = 7</span><span id="6a8a" class="la lb je ok b gy os op l oq or"><strong class="ok jf">txt_instance </strong>= dtf[<strong class="ok jf">"text_clean"</strong>].iloc[i]</span><span id="b5bf" class="la lb je ok b gy os op l oq or">print("True:", y_test[i], "--&gt; Pred:", predicted[i], "| <br/>      Similarity:", round(np.max(predicted_prob[i]),2))<br/>print(txt_instance)</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/a74963cd1b7f2b9f5d176dc5218f7007.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kABcQZDS53O_KIlJVG9qSA.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="91b7" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">这是一个正确分类的政治观察。大概，“<em class="nt">共和党</em>”和“<em class="nt">克林顿</em>”这两个词给了伯特正确的暗示。我将在 2D 空间中可视化文章的平均向量，并绘制与目标聚类的最高相似度。</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="e032" class="la lb je ok b gy oo op l oq or"><strong class="ok jf">## create embedding Matrix<br/></strong>y = np.concatenate([embedding_bert(v, tokenizer, nlp) for v in <br/>                    dic_clusters.values()])<br/>X = embedding_bert(txt_instance, tokenizer,<br/>                   nlp).mean(0).reshape(1,-1)<br/>M = np.concatenate([y,X])</span><span id="5e91" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## pca</strong><br/>pca = manifold.<strong class="ok jf">TSNE</strong>(perplexity=40, n_components=2, init='pca')<br/>M = pca.fit_transform(M)<br/>y, X = M[:len(y)], M[len(y):]</span><span id="d54f" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## create dtf clusters</strong><br/>dtf = pd.DataFrame()<br/>for k,v in dic_clusters.items():<br/>    size = len(dtf) + len(v)<br/>    dtf_group = pd.DataFrame(y[len(dtf):size], columns=["x","y"], <br/>                             index=v)<br/>    dtf_group["cluster"] = k<br/>    dtf = dtf.append(dtf_group)</span><span id="294d" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## plot clusters</strong><br/>fig, ax = plt.subplots()<br/>sns.<strong class="ok jf">scatterplot</strong>(data=dtf, x="x", y="y", hue="cluster", ax=ax)<br/>ax.legend().texts[0].set_text(None)<br/>ax.set(xlabel=None, ylabel=None, xticks=[], xticklabels=[], <br/>       yticks=[], yticklabels=[])<br/>for i in range(len(dtf)):<br/>    ax.annotate(dtf.index[i], <br/>               xy=(dtf["x"].iloc[i],dtf["y"].iloc[i]), <br/>               xytext=(5,2), textcoords='offset points', <br/>               ha='right', va='bottom')</span><span id="5d5d" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## add txt_instance<br/></strong>ax.scatter(x=X[0][0], y=X[0][1], c="red", linewidth=10)<br/>           ax.annotate("x", xy=(X[0][0],X[0][1]), <br/>           ha='center', va='center', fontsize=25)</span><span id="5285" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## calculate similarity<br/></strong>sim_matrix = metrics.pairwise.<strong class="ok jf">cosine_similarity</strong>(X, y)</span><span id="a3ce" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## add top similarity</strong><br/>for row in range(sim_matrix.shape[0]):</span><span id="7952" class="la lb je ok b gy os op l oq or">    <strong class="ok jf">### sorted {keyword:score}</strong><br/>    dic_sim = {n:sim_matrix[row][n] for n in <br/>               range(sim_matrix.shape[1])}<br/>    dic_sim = {k:v for k,v in sorted(dic_sim.items(), <br/>                key=lambda item:item[1], reverse=True)}</span><span id="e332" class="la lb je ok b gy os op l oq or">    <strong class="ok jf">### plot lines</strong><br/>    for k in dict(list(dic_sim.items())[0:5]).keys():<br/>        p1 = [X[row][0], X[row][1]]<br/>        p2 = [y[k][0], y[k][1]]<br/>        ax.plot([p1[0],p2[0]], [p1[1],p2[1]], c="red", alpha=0.5)</span><span id="187c" class="la lb je ok b gy os op l oq or">plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/15f719e0c5bc98205bf7958226f78fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C1z8IbU5QruzhVk8qbsyaA.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="f625" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">让我们放大一下感兴趣的集群:</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/f94b673ab86710650fa386a4a26df041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*t5Cte8JzO1x4btdQ7VS0iA.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="5b9d" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">总的来说，我们可以说均值向量非常类似于政治聚类。让我们将文章分解成令牌，看看哪些令牌“激活”了正确的集群。</p><pre class="mq mr ms mt gt oj ok ol om aw on bi"><span id="5ac9" class="la lb je ok b gy oo op l oq or"><strong class="ok jf">## create embedding Matrix<br/></strong>y = np.concatenate([embedding_bert(v, tokenizer, nlp) for v in <br/>                    dic_clusters.values()])<br/>X = embedding_bert(txt_instance, tokenizer,<br/>                   nlp).mean(0).reshape(1,-1)<br/>M = np.concatenate([y,X])</span><span id="ef8a" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## pca</strong><br/>pca = manifold.<strong class="ok jf">TSNE</strong>(perplexity=40, n_components=2, init='pca')<br/>M = pca.fit_transform(M)<br/>y, X = M[:len(y)], M[len(y):]</span><span id="f9cb" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## create dtf clusters</strong><br/>dtf = pd.DataFrame()<br/>for k,v in dic_clusters.items():<br/>    size = len(dtf) + len(v)<br/>    dtf_group = pd.DataFrame(y[len(dtf):size], columns=["x","y"], <br/>                             index=v)<br/>    dtf_group["cluster"] = k<br/>    dtf = dtf.append(dtf_group)</span><span id="8c49" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## add txt_instance<br/></strong>tokens = tokenizer.convert_ids_to_tokens(<br/>               tokenizer.encode(txt_instance))[1:-1]<br/>dtf = pd.DataFrame(X, columns=["x","y"], index=tokens)<br/>dtf = dtf[~dtf.index.str.contains("#")]<br/>dtf = dtf[dtf.index.str.len() &gt; 1]<br/>X = dtf.values<br/>ax.scatter(x=dtf["x"], y=dtf["y"], c="red")<br/>for i in range(len(dtf)):<br/>     ax.annotate(dtf.index[i], <br/>                 xy=(dtf["x"].iloc[i],dtf["y"].iloc[i]), <br/>                 xytext=(5,2), textcoords='offset points', <br/>                 ha='right', va='bottom')</span><span id="3a56" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## calculate similarity<br/></strong>sim_matrix = metrics.pairwise.<strong class="ok jf">cosine_similarity</strong>(X, y)</span><span id="66cc" class="la lb je ok b gy os op l oq or"><strong class="ok jf"><br/>## add top similarity</strong><br/>for row in range(sim_matrix.shape[0]):</span><span id="5c10" class="la lb je ok b gy os op l oq or"><strong class="ok jf">    ### sorted {keyword:score}</strong><br/>    dic_sim = {n:sim_matrix[row][n] for n in <br/>               range(sim_matrix.shape[1])}<br/>    dic_sim = {k:v for k,v in sorted(dic_sim.items(), <br/>                key=lambda item:item[1], reverse=True)}</span><span id="be98" class="la lb je ok b gy os op l oq or"><strong class="ok jf">    ### plot lines</strong><br/>    for k in dict(list(dic_sim.items())[0:5]).keys():<br/>        p1 = [X[row][0], X[row][1]]<br/>        p2 = [y[k][0], y[k][1]]<br/>        ax.plot([p1[0],p2[0]], [p1[1],p2[1]], c="red", alpha=0.5)</span><span id="c013" class="la lb je ok b gy os op l oq or">plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pm"><img src="../Images/046e2828cd122e7d39ac5589ec5e7024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UXLC-LVcnbim9d0GhM0vNA.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="3ed2" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">正如我们所想，文本中有一些词明显与政治相关，但其他一些词更类似于娱乐的一般上下文。</p><div class="mq mr ms mt gt ab cb"><figure class="pn iv po pp pq pr ps paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/d2af8ce7492b1f31ec3820089cf7df9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*qsfnfslfVCB7IOYze9isYA.png"/></div></figure><figure class="pn iv pt pp pq pr ps paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/e3926aa4ef7091b7d9a7c86abfc4931b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*l5p_vQx5uF9t4D4BqY-LvQ.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk pu di pv pw translated">作者图片</p></figure></div><h2 id="50d9" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">结论</h2><p id="9228" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">这篇文章是一个教程，演示了当一个带标签的训练集不可用时如何执行文本分类。</p><p id="1456" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">我使用预先训练的单词嵌入模型来构建一组关键字，以将目标变量置于上下文中。然后我用预先训练好的 BERT 语言模型把那些词和语料库转换到同一个向量空间。最后，我计算文本和关键词之间的余弦相似度，以确定每篇文章的上下文，并使用该信息来标记新闻。</p><p id="f8cf" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">这种策略不是最有效的，但它肯定是有效的，因为它能让你迅速获得好的结果。此外，一旦获得标记数据集，该算法可以用作监督模型的基线。</p><p id="c81c" class="pw-post-body-paragraph lw lx je ly b lz my kf mb mc mz ki me lj na mg mh ln nb mj mk lr nc mm mn mo im bi translated">我希望你喜欢它！如有问题和反馈，或者只是分享您感兴趣的项目，请随时联系我。</p><blockquote class="px"><p id="3199" class="py pz je bd qa qb qc qd qe qf qg mo dk translated">👉<a class="ae nd" href="https://linktr.ee/maurodp" rel="noopener ugc nofollow" target="_blank">我们来连线</a>👈</p></blockquote></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><blockquote class="qh qi qj"><p id="3822" class="lw lx nt ly b lz my kf mb mc mz ki me qk na mg mh ql nb mj mk qm nc mm mn mo im bi translated">本文是使用 Python 的<strong class="ly jf">NLP</strong>系列的一部分，参见:</p></blockquote><div class="is it gp gr iu ne"><a rel="noopener follow" target="_blank" href="/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jf gy z fp nj fr fs nk fu fw jd bi translated">使用 NLP 的文本摘要:TextRank vs Seq2Seq vs BART</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">使用 Python、Gensim、Tensorflow、Transformers 进行自然语言处理</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="qn l np nq nr nn ns ja ne"/></div></div></a></div><div class="is it gp gr iu ne"><a rel="noopener follow" target="_blank" href="/text-analysis-feature-engineering-with-nlp-502d6ea9225d"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jf gy z fp nj fr fs nk fu fw jd bi translated">使用自然语言处理的文本分析和特征工程</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">语言检测，文本清理，长度，情感，命名实体识别，N-grams 频率，词向量，主题…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="qo l np nq nr nn ns ja ne"/></div></div></a></div><div class="is it gp gr iu ne"><a rel="noopener follow" target="_blank" href="/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jf gy z fp nj fr fs nk fu fw jd bi translated">基于自然语言处理的文本分类:Tf-Idf vs Word2Vec vs BERT</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">预处理、模型设计、评估、词袋的可解释性、词嵌入、语言模型</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="qp l np nq nr nn ns ja ne"/></div></div></a></div><div class="is it gp gr iu ne"><a rel="noopener follow" target="_blank" href="/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jf gy z fp nj fr fs nk fu fw jd bi translated">带 NLP 的 AI 聊天机器人:语音识别+变形金刚</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">用 Python 构建一个会说话的聊天机器人，与你的人工智能进行对话</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="qq l np nq nr nn ns ja ne"/></div></div></a></div></div></div>    
</body>
</html>