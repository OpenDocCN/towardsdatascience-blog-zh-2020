<html>
<head>
<title>Improving sentence embeddings with BERT and Representation Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用BERT和表征学习改进句子嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improving-sentence-embeddings-with-bert-and-representation-learning-dfba6b444f6b?source=collection_archive---------11-----------------------#2020-02-29">https://towardsdatascience.com/improving-sentence-embeddings-with-bert-and-representation-learning-dfba6b444f6b?source=collection_archive---------11-----------------------#2020-02-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="efcb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个实验中，我们对BERT模型进行了微调，以提高它对短文本进行编码的能力。这为下游NLP任务产生了更有用的句子嵌入。</p><p id="aeb5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然一个普通的BERT <a class="ae ko" href="https://github.com/hanxiao/bert-as-service" rel="noopener ugc nofollow" target="_blank">可以用来编码句子</a>，但是用它生成的嵌入并不健壮。正如我们在下面看到的，被模型认为相似的样本通常在词汇上比语义上更相关。输入样本中的小扰动会导致预测相似性的大变化。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/0983be97ccd5553f2c77f1a0b2f796c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*InNXnIVqitLlJRwS-dFwHA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">平均池BERT-base模型编码的句子对之间的相似性</p></figure><p id="283c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了改进，我们使用斯坦福自然语言推理数据集，该数据集包含手动标记有<em class="lf">蕴涵</em>、<em class="lf">矛盾</em>和<em class="lf">中性</em>标签<em class="lf">的句子对。对于这些句子，我们将学习这样一种表述，即蕴涵对之间的相似性大于矛盾对之间的相似性。</em></p><p id="22ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了评估学习嵌入的质量，我们在STS和SICK-R数据集上测量Spearman等级相关性。</p><p id="5c0e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个实验的计划是:</p><ol class=""><li id="95e2" class="lg lh it js b jt ju jx jy kb li kf lj kj lk kn ll lm ln lo bi translated">准备SNLI和MNLI数据集</li><li id="8301" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated">实现数据生成器</li><li id="7cb9" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated">定义损失</li><li id="d381" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated">构建模型</li><li id="f0b4" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated">准备评估管道</li><li id="6fc4" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated">训练模型</li></ol><h1 id="b098" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">这本指南里有什么？</h1><p id="7ffe" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">本指南包含在标记数据上构建和训练句子编码器的代码。</p><h1 id="8d4d" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">需要什么？</h1><p id="8604" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">对于一个熟悉的读者来说，完成这个指南和训练句子编码器需要大约90分钟。代码用tensorflow==1.15测试。</p><h1 id="03f9" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">好吧，给我看看代码。</h1><p id="232e" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">这个实验的代码可以在<a class="ae ko" href="https://colab.research.google.com/drive/1Ns--41DmUN-xwd-rZhoZ1DWIAVojRz6G" rel="noopener ugc nofollow" target="_blank">这里</a>获得。这一次，来自<a class="ae ko" rel="noopener" target="_blank" href="/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2">先前实验</a>的大部分代码被重用。我建议先去看看。</p><p id="1315" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">独立版本可以在<a class="ae ko" href="https://github.com/gaphex/bert_experimental" rel="noopener ugc nofollow" target="_blank">库</a>中找到。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="a868" class="lu lv it bd lw lx ne lz ma mb nf md me mf ng mh mi mj nh ml mm mn ni mp mq mr bi translated">步骤1:设置</h1><p id="c343" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">我们从下载<a class="ae ko" href="https://nlp.stanford.edu/projects/snli/" rel="noopener ugc nofollow" target="_blank"> SNLI </a>、<a class="ae ko" href="https://www.nyu.edu/projects/bowman/multinli/" rel="noopener ugc nofollow" target="_blank"> MNLI </a>、<a class="ae ko" href="https://github.com/brmson/dataset-sts" rel="noopener ugc nofollow" target="_blank"> STS和SICK </a>数据集以及预训练的英语BERT模型开始。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="8570" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为SNLI <em class="lf"> jsonl </em>格式定义一个loader函数。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="bc8c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了更方便地处理数据集，我们稍微重新安排了一下。对于每个唯一的锚，我们创建一个ID和一个包含<em class="lf">锚</em>、<em class="lf">蕴涵</em>和<em class="lf">矛盾</em>样本的条目。每类缺少至少一个样本的锚被过滤掉。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="d9ab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">单个条目如下所示</p><pre class="kq kr ks kt gt nl nm nn no aw np bi"><span id="8e2a" class="nq lv it nm b gy nr ns l nt nu">{<br/>  'anchor': ["No, don't answer."],  <br/>  'contradiction': ['Please respond.'],  <br/>  'entailment': ["Don't respond. ", "Don't say a word. "]<br/>}</span></pre><p id="0b2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，加载SNLI和MNLI数据集。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="571e" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">步骤2:数据生成器</h1><p id="c9b8" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">为了训练模型，我们将对三元组进行采样，由锚、正样本和负样本组成。为了处理复杂的批处理生成逻辑，我们使用以下代码:</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="4b13" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">高层逻辑包含在<strong class="js iu"> <em class="lf"> generate_batch </em> </strong>方法中。</p><ol class=""><li id="746d" class="lg lh it js b jt ju jx jy kb li kf lj kj lk kn ll lm ln lo bi translated">批量锚点id是从所有可用id中随机选择的。</li><li id="3ca0" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated"><em class="lf">锚</em>样本从其id的<em class="lf">锚</em>样本中检索。</li><li id="80e5" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated"><em class="lf">阳性</em>样本从其id的<em class="lf">蕴涵</em>样本中检索。</li><li id="446c" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated"><em class="lf">阴性</em>样本是从<em class="lf">矛盾</em>样本中检索出它们的id。<br/>这些可以被认为是<strong class="js iu">硬否定样本，</strong>因为它们通常在语义上与它们的锚相似。为了减少过度拟合，我们将它们与从其他随机ID检索的<strong class="js iu">随机阴性样本</strong>混合。</li></ol><h1 id="c3ee" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">第三步:损失函数</h1><p id="1c28" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">我们可以将学习句子相似性度量的问题框架化为排序问题。假设我们有一个由k<strong class="js iu">k</strong>转述的句子对<strong class="js iu"> x </strong>和<strong class="js iu"> y </strong>组成的语料库，并且想要学习一个函数来估计<strong class="js iu"> y </strong>是否是<strong class="js iu"> x </strong>的转述<strong class="js iu">。</strong>对于某些<strong class="js iu"> x </strong>我们有单个阳性样本<strong class="js iu"> y </strong>和<strong class="js iu"> k-1 </strong>阴性样本<strong class="js iu"> y_k </strong>。这个概率分布可以写成:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nv"><img src="../Images/55f28081c2ae631d4935d910dd920b26.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*UUTwddUjUMaVP-JigZeGrQ.png"/></div></div></figure><p id="2a5b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用评分函数估计<strong class="js iu"> P(x，y) </strong>的联合概率，S:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/67d3dad452fbc57e46708a6da0552e49.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*vAcY9NlUl1VDeUU9LaTVsQ.png"/></div></figure><p id="7444" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在训练期间，对数据集中的所有<strong class="js iu"> k-1 </strong>阴性样本求和是不可行的。相反，我们通过从我们的语料库中为每一批抽取<strong class="js iu"> K </strong>个响应并使用它们作为负样本来近似<strong class="js iu"> P(x) </strong>。我们得到:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a7a54c9d569c60a4b08656b55391489d.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*NyBcvTAFxZEZUpC-YRYOYA.png"/></div></figure><p id="cc36" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将最小化数据的负对数概率。所以，对于一批K个三胞胎的损失我们可以写下:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl ny"><img src="../Images/bd037c42c4bc48490f50149162b1a5ff.png" data-original-src="https://miro.medium.com/v2/format:webp/1*zEFUaUuE5QBHvzHEcPnT4A.png"/></div></figure><p id="d504" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="lf">注</em> </strong>:以上表达式称为Softmax-Loss。</p><p id="b7bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本实验中，内积用作相似性函数<strong class="js iu"> S </strong>。计算最后括号中表达式的代码如下</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="ba49" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">第四步:模型</h1><p id="476a" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">首先，我们导入之前实验中的微调代码，并构建BERT模块。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="dcb2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该模型对于锚、阳性和阴性样本有三个输入。具有平均池操作的BERT层被用作共享文本编码器。文本预处理由编码器层处理。对编码的句子计算Softmax损失。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="f6d5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了方便起见，创建了3个模型:<em class="lf"> enc_model </em>用于编码句子，<em class="lf"> sim_model </em>用于计算句子对之间的相似度，<em class="lf"> trn_model </em>用于训练。所有型号都使用共享重量。</p><h1 id="7e91" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">步骤5:评估渠道</h1><p id="d1c0" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">自然语言编码器通常通过嵌入标记的句子对，测量它们之间的某种相似性，然后计算相似性与人类判断的相关性来评估。</p><p id="d098" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们使用STS 2012–2016和SICK 2014数据集来评估我们的模型。对于测试集中的所有句子对，我们计算余弦相似度。我们报告了带有人工标注标签的Pearson等级相关性。</p><p id="306d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面的回调处理评估过程，并在每次达到新的最佳结果时将提供的<em class="lf">保存模型</em>保存到<em class="lf">保存路径</em>。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="6184" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">第六步:培训</h1><p id="565e" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">我们训练10个时期的模型，每个时期有256个批次。每批由256个三胞胎组成。我们在每个时期的开始执行评估。</p><pre class="kq kr ks kt gt nl nm nn no aw np bi"><span id="d25b" class="nq lv it nm b gy nr ns l nt nu">trn_model.fit_generator(tr_gen._generator, validation_data=ts_gen._generator, steps_per_epoch=256, validation_steps=32, epochs=5, callbacks=callbacks)</span><span id="b1f0" class="nq lv it nm b gy nz ns l nt nu">*** New best: STS_spearman_r = 0.5426<br/>*** New best: STS_pearson_r = 0.5481<br/>*** New best: SICK_spearman_r = 0.5799<br/>*** New best: SICK_pearson_r = 0.6069<br/>Epoch 1/10<br/>255/256 [============================&gt;.] - ETA: 1s - loss: 0.6858<br/>256/256 [==============================] - 535s 2s/step - loss: 0.6844 - val_loss: 0.4366<br/>*** New best: STS_spearman_r = 0.7186<br/>*** New best: STS_pearson_r = 0.7367<br/>*** New best: SICK_spearman_r = 0.7258<br/>*** New best: SICK_pearson_r = 0.8098<br/>Epoch 2/10<br/>255/256 [============================&gt;.] - ETA: 1s - loss: 0.3950<br/>256/256 [==============================] - 524s 2s/step - loss: 0.3950 - val_loss: 0.3700<br/>*** New best: STS_spearman_r = 0.7337<br/>*** New best: STS_pearson_r = 0.7495<br/>*** New best: SICK_spearman_r = 0.7444<br/>*** New best: SICK_pearson_r = 0.8216<br/>...<br/>Epoch 9/10<br/>255/256 [============================&gt;.] - ETA: 1s - loss: 0.2481<br/>256/256 [==============================] - 524s 2s/step - loss: 0.2481 - val_loss: 0.2631<br/>*** New best: STS_spearman_r = 0.7536<br/>*** New best: STS_pearson_r = 0.7638<br/>*** New best: SICK_spearman_r = 0.7623<br/>*** New best: SICK_pearson_r = 0.8316<br/>Epoch 10/10<br/>255/256 [============================&gt;.] - ETA: 1s - loss: 0.2381<br/>256/256 [==============================] - 525s 2s/step - loss: 0.2383 - val_loss: 0.2492<br/>*** New best: STS_spearman_r = 0.7547<br/>*** New best: STS_pearson_r = 0.7648<br/>*** New best: SICK_spearman_r = 0.7628<br/>*** New best: SICK_pearson_r = 0.8325</span></pre><p id="4cbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为参考，我们可以查看来自<a class="ae ko" href="https://arxiv.org/pdf/1908.10084.pdf" rel="noopener ugc nofollow" target="_blank"> Sentence-BERT </a>论文的评估结果，在该论文中，作者对STS和SICK任务上的几个预训练句子嵌入系统进行了评估。</p><p id="fc32" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们使用普通平均池BERT模型的结果与公布的指标一致，在SICK-R. <br/>上获得<strong class="js iu"> 57.99 </strong> Spearman等级相关分数，在10个时期后，最佳Colab模型获得<strong class="js iu"> 76.94 </strong>，与通用句子编码器的最佳结果<strong class="js iu"> 76.69 </strong>相当。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oa"><img src="../Images/e1cbe19711a34faf50f61c55831ed22e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pYUu5IpaWQEAZA4FZzE9bw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">Spearman对句子表述的余弦相似性和各种文本相似性任务的黄金标签之间的相关性进行排序。(摘自句子-伯特:使用暹罗伯特网络的句子嵌入)</p></figure><p id="8986" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于在一个批次中的所有样本之间共享负面示例，因此使用较大的batch_size进行微调往往会将指标提高到某个程度。如果你的GPU能够处理的话，解冻更多的编码器层也会有所帮助。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="b3d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦训练完成，我们就可以通过编码测试三元组并并排检查预测的相似性来比较基础模型和训练模型。一些例子:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ob"><img src="../Images/9d93b15beba158dc253102dfdf2afacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HqMhQPzpdb2J3QogGeqciQ.png"/></div></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oc"><img src="../Images/57f4e9230d703e68df96b21782d9c8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PcpSPh-OulEKbJvO91jXrA.png"/></div></div></figure><h1 id="fb69" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">结论</h1><p id="e7a9" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">上面我们提出了一种使用标记句子对来改进句子嵌入的方法。</p><p id="cd1e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过显式地训练该模型来根据它们的语义关系对句子对进行编码，我们能够学习更有效和更健壮的句子表示。</p><p id="9930" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">自动和人工评估都证明了相对于基线句子表示模型的实质性改进。</p><h2 id="ad15" class="nq lv it bd lw od oe dn ma of og dp me kb oh oi mi kf oj ok mm kj ol om mq on bi translated">本系列中的其他指南</h2><ol class=""><li id="8682" class="lg lh it js b jt ms jx mt kb oo kf op kj oq kn ll lm ln lo bi translated"><a class="ae ko" rel="noopener" target="_blank" href="/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379">用云TPU从头开始预训练BERT】</a></li><li id="f87d" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated"><a class="ae ko" rel="noopener" target="_blank" href="/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a">用BERT和Tensorflow构建搜索引擎</a></li><li id="bee0" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated"><a class="ae ko" rel="noopener" target="_blank" href="/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2">用Keras和tf微调BERT。模块</a></li><li id="4a84" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated"><a class="ae ko" rel="noopener" target="_blank" href="/improving-sentence-embeddings-with-bert-and-representation-learning-dfba6b444f6b">用BERT和表征学习改进句子嵌入</a> <br/>【你在这里】</li></ol></div></div>    
</body>
</html>