<html>
<head>
<title>Training Neural Networks for Leela Zero With PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PyTorch训练Leela Zero神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-neural-networks-for-leela-zero-using-pytorch-and-pytorch-lightning-bbf588683065?source=collection_archive---------32-----------------------#2020-03-23">https://towardsdatascience.com/training-neural-networks-for-leela-zero-using-pytorch-and-pytorch-lightning-bbf588683065?source=collection_archive---------32-----------------------#2020-03-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9fed" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用PyTorch、PyTorch Lightning和Hydra为Leela Zero实现了一个简单的培训管道</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/10597e8da373d3addd2c9fe7f5c67341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dEx2KRXtJKWl0A3zjTYn7w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">闪电和九头蛇(？)— <a class="ae kv" href="https://unsplash.com/photos/M4biGF0pN5s" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="b86a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最近，我一直在寻找加快我的研究和管理我的实验的方法，特别是围绕编写训练管道和管理实验配置，我发现了这两个新项目，分别名为<a class="ae kv" href="https://github.com/PyTorchLightning/pytorch-lightning" rel="noopener ugc nofollow" target="_blank"> PyTorch Lightning </a>和<a class="ae kv" href="https://hydra.cc/" rel="noopener ugc nofollow" target="_blank"> Hydra </a>。PyTorch Lightning帮助您快速编写培训管道，而Hydra帮助您以干净的方式管理配置。</p><p id="200c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了在更真实的环境中练习使用它们，我决定为围棋引擎<a class="ae kv" href="https://github.com/leela-zero/leela-zero" rel="noopener ugc nofollow" target="_blank"> Leela Zero </a>编写一个训练管道。我选择这样做是因为这是一个范围很广的项目，具有有趣的技术挑战，涉及使用多个GPU在大数据集上训练巨大的网络。此外，我以前很喜欢为国际象棋实现AlphaGo的小版本<a class="ae kv" href="https://medium.com/@peterkeunwoo/beating-my-brother-in-chess-cb17739ffe2" rel="noopener">，所以我认为这将是一个有趣的附带项目。</a></p><p id="4cda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个博客中，我将解释这个项目的主要细节，这样你就可以很容易地理解我做了什么。你可以在这里阅读我的代码:<a class="ae kv" href="https://github.com/yukw777/leela-zero-pytorch" rel="noopener ugc nofollow" target="_blank">https://github.com/yukw777/leela-zero-pytorch</a></p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="3982" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">莉拉·零</h1><p id="0f29" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">第一步是弄清楚Leela Zero神经网络的内部工作方式。我大量参考了Leela Zero的文档及其<a class="ae kv" href="https://github.com/leela-zero/leela-zero/tree/next/training/tf" rel="noopener ugc nofollow" target="_blank"> Tensorflow培训管道</a>。</p><h2 id="4a85" class="mw ma iq bd mb mx my dn mf mz na dp mj lf nb nc ml lj nd ne mn ln nf ng mp nh bi translated">神经网络体系结构</h2><p id="ef60" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">Leela Zero的神经网络由一个ResNet“塔”组成，有两个“头”，策略头和值头，如<a class="ae kv" href="https://deepmind.com/blog/article/alphago-zero-starting-scratch" rel="noopener ugc nofollow" target="_blank"> AlphaGo Zero论文</a>中所述。所有卷积滤波器都是3x3，除了在策略和值头开头的滤波器是1x1，如本文中所示。游戏和棋盘特征被编码为形状张量[批量大小、棋盘宽度、棋盘高度、特征数量]，并首先通过ResNet塔传送。然后，该塔提取抽象特征，并通过每个头来计算下一步行动的策略概率分布和游戏的价值，以预测游戏的赢家。</p><p id="33e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以在下面的代码片段中找到网络的实现细节。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://github.com/yukw777/leela-zero-pytorch/blob/master/leela_zero_pytorch/network.py#L90"><div class="gh gi ni"><img src="../Images/bc4265e97e10ebdb06943a0634976ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PVcC74rleidpzwV_swD0fQ.png"/></div></a><p class="kr ks gj gh gi kt ku bd b be z dk translated">Leela零神经网络在PyTorch中的实现</p></figure><h2 id="61e9" class="mw ma iq bd mb mx my dn mf mz na dp mj lf nb nc ml lj nd ne mn ln nf ng mp nh bi translated">权重格式</h2><p id="b576" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">Leela Zero使用一个简单的文本文件来保存和加载网络权重。文本文件中的每一行都有一系列数字，代表网络每一层的权重。先有残塔，后有政策头，再有价值头。</p><p id="c16f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">卷积层有2个权重行:</p><ol class=""><li id="77d3" class="nj nk iq ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr bi translated">形状卷积权重[输出、输入、过滤器大小、过滤器大小]</li><li id="b7e8" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated">渠道偏差</li></ol><p id="54ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Batchnorm层有2个权重行:</p><ol class=""><li id="7916" class="nj nk iq ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr bi translated">批处理方式</li><li id="5f4d" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated">批次方差</li></ol><p id="900e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">内部产品(完全连接)层有2个重量行:</p><ol class=""><li id="8816" class="nj nk iq ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr bi translated">形状的层权重[输出，输入]</li><li id="f819" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated">输出偏差</li></ol><p id="0b6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我编写了单元测试来确保我的权重文件是正确的。我使用的另一个简单的健全性检查是计算层数，并将其与加载我的权重文件后Leela Zero所说的进行比较。层数的公式为:</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="6c71" class="mw ma iq ny b gy oc od l oe of">n_layers = 1 (version number) +<br/>           2 (input convolution) + <br/>           2 (input batch norm) + <br/>           n_res (number of residual blocks) * <br/>           8 (first conv + first batch norm + <br/>              second conv + second batch norm) + <br/>           2 (policy head convolution) + <br/>           2 (policy head batch norm) + <br/>           2 (policy head linear) + <br/>           2 (value head convolution) + <br/>           2 (value head batch norm) + <br/>           2 (value head first linear) + <br/>           2 (value head second linear)</span></pre><p id="c956" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，这似乎很简单，但是有一个奇怪的实现细节需要注意。Leela Zero实际上使用卷积层的偏差来表示下一批范数层的可学习参数(<code class="fe og oh oi ny b">gamma</code>和<code class="fe og oh oi ny b">beta</code>)。这样做是为了在添加批次标准图层时，权重文件的格式(只有一行用于图层权重，另一行用于偏差)不必更改。</p><p id="bda1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">目前，Leela Zero仅使用批次定额的<code class="fe og oh oi ny b">beta</code>项，并将<code class="fe og oh oi ny b">gamma</code>设置为1。那么，实际上如何使用卷积偏差来产生与应用batch norm中的可学习参数相同的结果呢？让我们先来看看批量定额的等式:</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="a585" class="mw ma iq ny b gy oc od l oe of">y = gamma * (x — mean)/sqrt(var — eps) + beta</span></pre><p id="2da4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于莉拉零点将<code class="fe og oh oi ny b">gamma</code>设为1，等式变成:</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="bfb6" class="mw ma iq ny b gy oc od l oe of">y = (x — mean)/sqrt(var — eps) + beta</span></pre><p id="83d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，假设<code class="fe og oh oi ny b">x_conv</code>是没有偏差的卷积层的输出。然后，我们要给<code class="fe og oh oi ny b">x_conv</code>增加一些偏差，这样当你在没有<code class="fe og oh oi ny b">beta</code>的情况下通过批范数运行它时，结果和在上面提到的只有<code class="fe og oh oi ny b">beta</code>的情况下通过批范数方程运行<code class="fe og oh oi ny b">x_conv</code>是一样的。以方程式的形式:</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="49c9" class="mw ma iq ny b gy oc od l oe of">(x_conv + bias — mean)/sqrt(var — eps) = <br/>(x_conv — mean)/sqrt(var — eps) + beta </span><span id="a733" class="mw ma iq ny b gy oj od l oe of">x_conv + bias — mean = <br/>x_conv — mean + beta * sqrt(var — eps) </span><span id="e010" class="mw ma iq ny b gy oj od l oe of">bias = beta * sqrt(var — eps)</span></pre><p id="e3a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，如果我们在权重文件中将卷积偏差设置为<code class="fe og oh oi ny b">beta * sqrt(var — eps)</code>，我们将获得所需的输出，这就是LeelaZero所做的。</p><p id="4000" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么，我们实际上如何实现这一点呢？在Tensorflow中，您可以通过调用<code class="fe og oh oi ny b">tf.layers.batch_normalization(scale=False)</code>来告诉batch norm层只忽略<code class="fe og oh oi ny b">gamma</code>项，并完成它。不幸的是，在PyTorch中你不能设置批处理规范化层只忽略<code class="fe og oh oi ny b">gamma</code>；将<code class="fe og oh oi ny b">affine</code>参数设置为<code class="fe og oh oi ny b">False</code> : <code class="fe og oh oi ny b">BatchNorm2d(out_channels, affine=False)</code>只能忽略<code class="fe og oh oi ny b">gamma</code>和<code class="fe og oh oi ny b">beta</code>。所以，我设置批处理规范化来忽略这两者，然后简单地在后面添加一个张量，它代表<code class="fe og oh oi ny b">beta</code>。然后，我使用公式<code class="fe og oh oi ny b">bias = beta * sqrt(var — eps)</code>计算权重文件的卷积偏差。</p><h1 id="57b2" class="lz ma iq bd mb mc ok me mf mg ol mi mj jw om jx ml jz on ka mn kc oo kd mp mq bi translated">培训渠道</h1><p id="49e8" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">在弄清楚Leela Zeros的神经网络的细节后，是时候解决训练管道了。正如我提到的，我想练习使用两个工具——py torch Lightning和Hydra——来加速编写训练管道和干净地管理实验配置。让我们深入了解我如何使用它们的细节。</p><h2 id="163c" class="mw ma iq bd mb mx my dn mf mz na dp mj lf nb nc ml lj nd ne mn ln nf ng mp nh bi translated">PyTorch闪电</h2><p id="b97d" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">编写培训管道是迄今为止我最不喜欢的研究部分:它涉及大量重复的样板代码，并且很难调试。正因为如此，PyTorch闪电对我来说就像一股清新的空气。它是一个轻量级的库，在PyTorch之上没有很多辅助的抽象，在编写训练管道时负责大部分样板代码。它允许您关注培训管道中更有趣的部分，如模型架构，并使您的研究代码更加模块化和可调试。此外，它支持开箱即用的多GPU和TPU培训！</p><p id="f9cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了将PyTorch Lightning用于我的训练管道，我必须做的最多的编码工作就是编写一个类，我称之为<code class="fe og oh oi ny b">NetworkLightningModule</code>，它继承了<code class="fe og oh oi ny b">LightningModule</code>来指定我的训练管道的细节，并将其传递给<code class="fe og oh oi ny b">Trainer</code>。你可以遵循PyTorch Lightning官方文档，了解如何编写自己的<code class="fe og oh oi ny b">LightningModule</code>的细节。</p><h2 id="2e5e" class="mw ma iq bd mb mx my dn mf mz na dp mj lf nb nc ml lj nd ne mn ln nf ng mp nh bi translated">水螅</h2><p id="b6fe" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我一直在寻找好的解决方案的另一部分研究是实验管理。当你进行研究时，不可避免地要运行无数的实验变量来测试你的假设，以一种可扩展的方式跟踪它们是极其重要的。到目前为止，我一直依靠配置文件来管理我的实验变体，但是使用平面配置文件很快变得难以管理。模板是解决这个问题的一种方法。然而，我发现模板最终也会变得混乱，因为当您覆盖多层值文件来呈现您的配置文件时，很难跟踪哪个值来自哪个值文件。</p><p id="87ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，Hydra是一个基于组合的配置管理系统。您可以组合多个较小的配置文件来构成最终配置，而不是使用单独的模板和值文件来呈现最终配置。它不如基于模板的配置管理系统灵活，但是我发现基于组合的系统在灵活性和可维护性之间取得了很好的平衡。Hydra就是这样一个系统，它是专门为研究脚本定制的。它的调用有点笨拙，因为它要求您将它用作脚本的主入口点函数的装饰器，但我实际上认为这种设计选择使它易于与您的训练脚本集成。此外，它允许您通过命令行手动覆盖配置，这在运行您的实验的不同变体时非常有用。我使用Hydra来管理不同规模的网络架构和培训管道配置。</p><h1 id="7791" class="lz ma iq bd mb mc ok me mf mg ol mi mj jw om jx ml jz on ka mn kc oo kd mp mq bi translated">估价</h1><p id="fcd8" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">为了评估我训练过的网络，我用<a class="ae kv" href="https://github.com/mattheww/gomill" rel="noopener ugc nofollow" target="_blank"> GoMill </a>来运行围棋比赛。这是一个在GTP引擎之间运行比赛的库，Leela Zero就是其中之一。你可以在这里找到我用的<a class="ae kv" href="https://github.com/yukw777/leela-zero-pytorch/blob/master/eval/bg-vs-sm.ctl" rel="noopener ugc nofollow" target="_blank">的锦标赛配置</a>。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="19df" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结论</h1><p id="16f6" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">通过使用PyTorch-Lightning和Hydra，我能够大大加快编写训练管道的速度，并有效地管理实验配置。我希望这个项目和博客帖子也能帮助你的研究。你可以在这里查看代码:【https://github.com/yukw777/leela-zero-pytorch<a class="ae kv" href="https://github.com/yukw777/leela-zero-pytorch" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>