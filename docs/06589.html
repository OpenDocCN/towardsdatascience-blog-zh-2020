<html>
<head>
<title>Day 145 of #NLP365: NLP Papers Summary — SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第 145 天的#NLP365: NLP 论文摘要 SUPERT:迈向多文档摘要的无监督评估度量的新前沿</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-145-of-nlp365-nlp-papers-summary-supert-towards-new-frontiers-in-unsupervised-evaluation-188295f82ce5?source=collection_archive---------61-----------------------#2020-05-24">https://towardsdatascience.com/day-145-of-nlp365-nlp-papers-summary-supert-towards-new-frontiers-in-unsupervised-evaluation-188295f82ce5?source=collection_archive---------61-----------------------#2020-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="96a5" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="59c2" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">NLP 论文摘要是我总结 NLP 研究论文要点的系列文章</h2></div><p id="5f4f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我在 2020 年每天记录我的 NLP 学习旅程的地方。在这里，你可以随意查看我在过去的 280 天里学到了什么。在这篇文章的最后，你可以找到以前按自然语言处理领域分类的论文摘要，你也可以订阅# NLP 365 @<a class="ae jc" href="http://eepurl.com/gW7bBP" rel="noopener ugc nofollow" target="_blank">http://eepurl.com/gW7bBP</a>:)</p><p id="e0df" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的 NLP 论文是<strong class="lf jp"> <em class="lz"> SUPERT:迈向多文档摘要的无监督评估度量的新前沿</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="29b1" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="8d20" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">提出了 SUPERT，一种通过测量摘要和伪引用摘要之间的语义相似性来评估多文档摘要的无监督评估度量。伪引用摘要是通过使用上下文嵌入和软标记对齐从源文档中选择显著句子来生成的。SUPERT 能够实现与人类评估 18–39%的更好的相关性。我们使用 SUPERT 和一个强化学习摘要器，与 SOTA 无监督摘要器相比，它产生了很好的性能。这展示了 SUPERT 的有效性，也意味着我们可以从无限数量的文档中创建许多参考摘要，以增加数据集的大小。</p><h1 id="c7ad" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">数据集和评估指标</h1><p id="2226" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们使用了两个多文档汇总数据集:TAC'08 和 TAC'09。两个 TAC 数据集包含大约 45+个主题，每个主题有 10 篇新闻文章、4 个参考摘要和 55+个机器生成的摘要。我们的评估标准是三个不同的相关系数:皮尔森的，斯皮尔曼的和肯德尔的。</p><h2 id="1f63" class="nj mi jf bd mj nk nl dn mn nm nn dp mr lm no np mt lq nq nr mv lu ns nt mx jl bi translated">模型比较</h2><ol class=""><li id="5db9" class="nu nv jf lf b lg mz lj na lm nw lq nx lu ny ly nz oa ob oc bi translated"><em class="lz"> TFIDF </em></li><li id="ba68" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated"><em class="lz"> JS 发散</em>。测量源和摘要中单词分布之间的 JS 差异</li><li id="62b4" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated"><em class="lz">死神</em></li><li id="6092" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated"><em class="lz">余弦-埃尔莫</em>。语境化词语嵌入</li><li id="0943" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated"><em class="lz">波姆 19 </em></li><li id="376a" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated"><em class="lz"> ROUGE-1 和 ROUGE-2 以及 MoverScore </em>。性能测量上限</li></ol><h1 id="c0b8" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">使用伪引用和 bERT (SUPERT)的汇总评估</h1><p id="ed36" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">SUPERT 测量多文档摘要的相关性，它测量来自源文档的摘要中包含多少重要信息。我们用两个步骤来衡量相关性:</p><ol class=""><li id="cd53" class="nu nv jf lf b lg lh lj lk lm oi lq oj lu ok ly nz oa ob oc bi translated">从源文档中找出突出的句子</li><li id="35be" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated">测量伪引用(步骤 1)和生成的摘要之间的语义重叠</li></ol><figure class="om on oo op gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/6f52fb7f13fcec8890c9ec12297244b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/0*Hly_Qvwomp7iHaUt.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">SUPERT 的工作流程[1]</p></figure><p id="20a3" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下面的结果表展示了所有基准方法在显著低于性能上限时的表现。令人惊讶的是，基于嵌入的方法比基于词典的方法表现更差。这告诉我们，现有的单文档评估度量在评估多文档摘要时是无效的。</p><figure class="om on oo op gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/c7865b45bce97ce8cd793fcd27966099.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/0*6Ke7uV0Za2KRo7PG.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">不同评估指标和人员评级之间的汇总级相关性[1]</p></figure><h2 id="fb3c" class="nj mi jf bd mj nk nl dn mn nm nn dp mr lm no np mt lq nq nr mv lu ns nt mx jl bi translated">用情境化嵌入度量相似性</h2><p id="196e" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们通过探索不同的文本编码器，如 BERT、ROBERTa、ALBERT 和 s BERT，扩展了余弦 ELMo。结果显示如下。如图所示，SBERT 作为具有余弦相似性的文本编码器产生了最高相关性的生成摘要。然而，与基于词典的方法相比，这仍然表现不佳。我们探索的另一个扩展是使用单词移动距离(WMDs)来度量两个文档之间的语义相似性，而不是使用余弦相似性。先前的工作已经证明 WMDs 产生了更强的性能，并且我们下面的结果支持了带 SBERT 的 as WMD(M _ SBERT)明显优于它的余弦相似性对应物和所有基于词典的方法。这让我们想到了计算文档间语义相似度的终极方法，那就是使用 SBERT 和 WMD。</p><figure class="om on oo op gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/81490236aae730cb8fe3e48e81f7e593.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/0*BCyGEp_jqQll3yZP.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">上下文嵌入度量的性能[1]</p></figure><h2 id="2d3c" class="nj mi jf bd mj nk nl dn mn nm nn dp mr lm no np mt lq nq nr mv lu ns nt mx jl bi translated">构建伪引用</h2><p id="4f68" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">前面表格中的结果显示了无监督评估和基于参考的评估之间的巨大性能差异。这表明我们仍然需要参考文献摘要，因此我们探索了建立伪参考文献的不同方法。</p><p id="bb8e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，我们探索了两种建立基线结果的简单策略:选择 N 个随机句子或前 N 个句子。结果显示如下。结果显示了随机选择的句子的较差性能，我们应该选择前 10-15 个句子作为伪引用，因为它优于基于词汇的方法和我们的 M_SBERT 方法。这也说明了新闻文章中的立场偏差。</p><figure class="om on oo op gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/c9526352dae23fc68a7887125ab9c75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/0*QVZo2oYIrEGmV3rE.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">通过随机选择句子建立伪引用[1]</p></figure><p id="f40e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其次，我们探索了两种基于图的方法来构建伪引用:位置不可知图和位置感知图。对于位置不可知的图，我们使用 SBERT (SLR)扩展了 LexRank 来度量余弦相似性。我们还探索了相似传播聚类算法，该算法对句子进行聚类，并选择每个聚类的中心来建立伪引用。这种聚类算法不需要我们预设聚类数。对于单反和 SC，我们有两种变体:个体图和全局图。个体图为每个源文档构建一个图，并选择前 K 个句子。全局图使用来自同一主题的所有源文档的所有句子构建一个图，并选择前 M 个句子。</p><p id="82bc" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于位置感知图，我们使用 SBERT (SPS)扩展了 PacSum 来度量句子相似性，并且类似地，考虑了个体和全局图版本。PacSum 选择语义中心的句子，这意味着它与后面的句子具有高平均相似度，而与前面的句子具有低平均相似度。此外，我们还提出了 Top + Clique (TC ),它选择前 N 个句子和语义中心句来构建伪引用。TC 是这样工作的:</p><ol class=""><li id="5ab9" class="nu nv jf lf b lg lh lj lk lm oi lq oj lu ok ly nz oa ob oc bi translated">将每个文档中的前 N 个句子标记为显著</li><li id="1282" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated">构建一个连接高度相似的非前 N 名句子的图表</li><li id="a02b" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated">从图中识别集团，并从每个集团中选择语义中心句作为潜在的显著句</li><li id="94d1" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated">对于每个潜在的显著句子，将其与前 N 个句子进行比较，如果它与前 N 个句子不高度相似，则将其标记为显著</li></ol><p id="91e7" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下表展示了位置不可知图和位置感知图的结果。所有方法(除了 SC_G)都优于上表 1 中的基准模型。我们的位置不可知图表现不如位置感知图。此外，我们的位置感知图表现不如表 3 中选择前 N 个句子的简单句子提取方法。这向我们表明，新闻中的位置偏向是非常强烈的，它仍然是选择正面信息的最有效的方法。</p><figure class="om on oo op gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/0253964568129c566a127f9a6196794b.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/0*H-nqs0ftuB5V6M9o.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">使用位置不可知和位置感知构建伪引用[1]</p></figure><h2 id="cd93" class="nj mi jf bd mj nk nl dn mn nm nn dp mr lm no np mt lq nq nr mv lu ns nt mx jl bi translated">引导强化学习</h2><p id="dd6c" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们使用我们的新的无监督评估度量来指导基于 RL 的多文档摘要器神经时间差异(NTD)的训练。我们考虑了三个无监督的奖励函数:JS，REAPER 和 SUPERT (SP)。SUPERT 从每个源文档中选择前 10-15 个句子作为伪引用，并使用 SBERT 来测量摘要和伪引用之间的语义相似性。结果如下所示，使用 SUPERT 的 NTD 产生了最强的结果。</p><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="ou ov di ow bf ox"><div class="gh gi ot"><img src="../Images/d064491a8761175b530d0e7a11fd219c.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/0*vehrDe-uBy3kVQVx.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">NTD 2008 年和 2009 年 TAC 的 ROUGE 结果[1]</p></figure><p id="bf51" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">来源:</p><p id="eb81" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[1]高，杨，赵，魏和埃格，s，2020 .SUPERT:迈向多文档摘要的无监督评估度量的新前沿。arXiv 预印本 arXiv:2005.03724 。</p><p id="ce42" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于 2020 年 5 月 24 日 https://ryanong.co.uk</em><a class="ae jc" href="https://ryanong.co.uk/2020/05/24/day-145-nlp-papers-summary-supert-towards-new-frontiers-in-unsupervised-evaluation-metrics-for-multi-document-summarization/" rel="noopener ugc nofollow" target="_blank"><em class="lz"/></a><em class="lz">。</em></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="ba9a" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="03db" class="nu nv jf lf b lg mz lj na lm nw lq nx lu ny ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="3d1f" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sense-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="56c4" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitive-analysis-f 24 a2 EC 1 ca 32</a></li><li id="0867" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="063a" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towards data science . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li><li id="8016" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towardsdatascience . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li><li id="f61c" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5">https://towards data science . com/day-112-of-NLP 365-NLP-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b 7 a5 e 245 b5</a></li><li id="e659" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-123-of-nlp365-nlp-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f998d1131">https://towards data science . com/day-123-of-NLP 365-NLP-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f 998d 1131</a></li></ul><h1 id="d95b" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">总结</h1><ul class=""><li id="f4c9" class="nu nv jf lf b lg mz lj na lm nw lq nx lu ny ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="b124" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1 b 27</a></li><li id="fc18" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-113-of-nlp365-nlp-papers-summary-on-extractive-and-abstractive-neural-document-87168b7e90bc">https://towards data science . com/day-113-of-NLP 365-NLP-papers-summary-on-extractive-and-abstract-neural-document-87168 b 7 e 90 BC</a></li><li id="7ef7" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-116-of-nlp365-nlp-papers-summary-data-driven-summarization-of-scientific-articles-3fba016c733b">https://towards data science . com/day-116-of-NLP 365-NLP-papers-summary-data-driven-summary-of-scientific-articles-3 FBA 016 c 733 b</a></li><li id="71bf" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-117-of-nlp365-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge-61ae6cdf32f">https://towards data science . com/day-117-of-NLP 365-NLP-papers-summary-abstract-text-summary-a-low-resource-challenge-61a E6 CDF 32 f</a></li><li id="22aa" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-118-of-nlp365-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-aea118a5eb3f">https://towards data science . com/day-118-of-NLP 365-NLP-papers-summary-extractive-summary-of-long-documents-by-combining-AEA 118 a5 eb3f</a></li><li id="db69" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-120-of-nlp365-nlp-papers-summary-a-simple-theoretical-model-of-importance-for-summarization-843ddbbcb9b">https://towards data science . com/day-120-of-NLP 365-NLP-papers-summary-a-simple-theory-model-of-importance-for-summary-843 ddbcb 9b</a></li><li id="2a20" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-121-of-nlp365-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization-cd55e577f6de">https://towards data science . com/day-121-of-NLP 365-NLP-papers-summary-concept-pointer-network-for-abstract-summary-cd55e 577 F6 de</a></li><li id="3eed" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-124-nlp-papers-summary-tldr-extreme-summarization-of-scientific-documents-106cd915f9a3">https://towards data science . com/day-124-NLP-papers-summary-tldr-extreme-summary-of-scientific-documents-106 CD 915 F9 a 3</a></li><li id="595b" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-143-of-nlp365-nlp-papers-summary-unsupervised-pseudo-labeling-for-extractive-summarization-3b94920e04c6">https://towards data science . com/day-143-of-NLP 365-NLP-papers-summary-unsupervised-pseudo-labeling-for-extract-summary-3b 94920 e04c 6</a></li><li id="588a" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-144-of-nlp365-nlp-papers-summary-attend-to-medical-ontologies-content-selection-for-ff7cded5d95b">https://towards data science . com/day-144-of-NLP 365-NLP-papers-summary-attend-to-medical-ontology-content-selection-for-ff 7 cded 5d 95 b</a></li></ul><h1 id="1164" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">其他人</h1><ul class=""><li id="1e54" class="nu nv jf lf b lg mz lj na lm nw lq nx lu ny ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li><li id="e8e0" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-111-of-nlp365-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection-bff7f5f20ce5">https://towards data science . com/day-111-of-NLP 365-NLP-papers-summary-the-risk-of-race-of-bias-in-hate-speech-detection-BFF 7 F5 f 20 ce 5</a></li><li id="b982" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-115-of-nlp365-nlp-papers-summary-scibert-a-pretrained-language-model-for-scientific-text-185785598e33">https://towards data science . com/day-115-of-NLP 365-NLP-papers-summary-scibert-a-pre trained-language-model-for-scientific-text-185785598 e33</a></li><li id="d042" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-119-nlp-papers-summary-an-argument-annotated-corpus-of-scientific-publications-d7b9e2ea1097">https://towards data science . com/day-119-NLP-papers-summary-an-argument-annoted-corpus-of-scientific-publications-d 7 b 9 e 2e ea 1097</a></li><li id="7cb7" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-122-of-nlp365-nlp-papers-summary-applying-bert-to-document-retrieval-with-birch-766eaeac17ab">https://towards data science . com/day-122-of-NLP 365-NLP-papers-summary-applying-Bert-to-document-retrieval-with-birch-766 EAC 17 ab</a></li><li id="7658" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-125-of-nlp365-nlp-papers-summary-a2n-attending-to-neighbors-for-knowledge-graph-inference-87305c3aebe2">https://towards data science . com/day-125-of-NLP 365-NLP-papers-summary-a2n-attending-to-neighbors-for-knowledge-graph-inference-87305 C3 aebe 2</a></li><li id="7a41" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-126-of-nlp365-nlp-papers-summary-neural-news-recommendation-with-topic-aware-news-4eb9604330bb">https://towards data science . com/day-126-of-NLP 365-NLP-papers-summary-neural-news-recommendation-with-topic-aware-news-4eb 9604330 bb</a></li><li id="78ee" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-140-of-nlp365-nlp-papers-summary-multimodal-machine-learning-for-automated-icd-coding-b32e02997ea2">https://towards data science . com/day-140-of-NLP 365-NLP-papers-summary-multimodal-machine-learning-for-automated-ICD-coding-b32e 02997 ea 2</a></li><li id="7f53" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-141-of-nlp365-nlp-papers-summary-textattack-a-framework-for-adversarial-attacks-in-aac2a282d72c">https://towards data science . com/day-141-of-NLP 365-NLP-papers-summary-text attack-a-framework-for-adversarial-attack-in-aac2a 282d 72 c</a></li><li id="a042" class="nu nv jf lf b lg od lj oe lm of lq og lu oh ly oy oa ob oc bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-142-of-nlp365-nlp-papers-summary-measuring-emotions-in-the-covid-19-real-world-worry-d565098a0937">https://towards data science . com/day-142-of-NLP 365-NLP-papers-summary-measuring-emotions-in-the-the-新冠肺炎-现实世界-忧虑-d565098a0937 </a></li></ul></div></div>    
</body>
</html>