<html>
<head>
<title>Learn AI Today 02: Introduction to Classification Problems using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">今天学习人工智能 02:使用 PyTorch 的分类问题介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learn-ai-today-02-introduction-to-classification-problems-using-pytorch-b710918cba63?source=collection_archive---------69-----------------------#2020-07-27">https://towardsdatascience.com/learn-ai-today-02-introduction-to-classification-problems-using-pytorch-b710918cba63?source=collection_archive---------69-----------------------#2020-07-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1c67" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="http://towardsdatascience.com/tagged/learn-ai-today" rel="noopener" target="_blank">今天学 AI</a></h2><div class=""/><div class=""><h2 id="a378" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">用神经网络对花卉进行分类，可视化决策边界和理解过度拟合。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/753a82c6dfb5d3295dd0c058b3195ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kezj7XoSUzMQ7fwjuxFA7Q.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">改编自<a class="ae le" href="https://unsplash.com/@kellysikkema?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">凯莉·西克玛</a>在<a class="ae le" href="https://unsplash.com/s/photos/organize?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片。</p></figure><p id="e249" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是我创作的<a class="ae le" href="http://learn-ai-today.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">今日 AI</strong></a><strong class="lh ja"/>系列的第二个故事！这些故事，或者至少是前几个，是基于我在研究/学习<strong class="lh ja"> PyTorch </strong>和<strong class="lh ja">深度学习</strong>时创作的一系列<strong class="lh ja"> Jupyter 笔记本</strong>。我希望你和我一样觉得它们很有用！</p><p id="2701" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你还没有，一定要检查以前的故事！</p><div class="mb mc gp gr md me"><a rel="noopener follow" target="_blank" href="/learn-ai-today-01-getting-started-with-pytorch-2e3ba25a518"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ja gy z fp mj fr fs mk fu fw iz bi translated">今日学习人工智能:Pytorch 入门</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">定义和训练 Pytorch 模型并动态可视化结果</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms ky me"/></div></div></a></div><h1 id="f7c0" class="mt mu iq bd mv mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">你将从这个故事中学到什么:</h1><ul class=""><li id="512f" class="nl nm iq lh b li nn ll no lo np ls nq lw nr ma ns nt nu nv bi translated">验证的重要性</li><li id="20f1" class="nl nm iq lh b li nw ll nx lo ny ls nz lw oa ma ns nt nu nv bi translated">如何为分类问题训练模型</li><li id="4527" class="nl nm iq lh b li nw ll nx lo ny ls nz lw oa ma ns nt nu nv bi translated">动态可视化决策边界</li><li id="2764" class="nl nm iq lh b li nw ll nx lo ny ls nz lw oa ma ns nt nu nv bi translated">如何避免过度拟合</li></ul></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h1 id="9d0f" class="mt mu iq bd mv mw oi my mz na oj nc nd kf ok kg nf ki ol kj nh kl om km nj nk bi translated">1.鸢尾花数据集</h1><p id="85d2" class="pw-post-body-paragraph lf lg iq lh b li nn ka lk ll no kd ln lo on lq lr ls oo lu lv lw op ly lz ma ij bi translated">让我们从介绍数据集开始。我将使用非常著名的<a class="ae le" href="https://en.wikipedia.org/wiki/Iris_flower_data_set" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">鸢尾花数据集</strong> </a>，它包含以下 3 种花的 4 种不同测量值(萼片长度、萼片宽度、花瓣长度、花瓣宽度)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/a4951d1a24cc7406d02082eac5a61bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lPxifAduIyVhNEBCLZvCYg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片改编自<a class="ae le" href="https://en.wikipedia.org/wiki/Iris_flower_data_set" rel="noopener ugc nofollow" target="_blank">维基百科</a>。</p></figure><p id="7d11" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">目标是使用每朵花的 4 个测量值准确识别物种。</strong>注意，现在使用直接从图像中学习的模型(卷积神经网络)相对容易，但我将把这个话题留到下一课。如下面的代码所示，<strong class="lh ja">鸢尾花数据集</strong>可以很容易地从<strong class="lh ja"> sklearn 数据集</strong>中下载。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="5da7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了快速显示数据，我们绘制每对要素的散点图和每个要素的直方图。为了实现这个表示，我使用了<strong class="lh ja">pandas . plotting . scatter _ matrix</strong>函数(和往常一样，你可以在最后找到完整代码的链接)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ot"><img src="../Images/669c2d1506260fb16b04042cc84dcfea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdCxIq99he6bjJ9k6aEduw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">鸢尾花数据的可视化(蓝点——鸢尾；蓝绿色的点——杂色鸢尾；黄点——海滨鸢尾)。图片由作者提供。</p></figure><p id="b92e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如你在上面的散点图中看到的，对于大多数样品来说，区分物种应该很容易。例如，在大多数地块中，刚毛鸢尾点与其他两个物种非常接近。对于这样一个简单的例子，你可以通过画几条线来创建一个基于规则的算法。然而，为了简单起见，<strong class="lh ja">也是一个用神经网络介绍<strong class="lh ja">分类的好例子</strong>！</strong></p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h1 id="0d57" class="mt mu iq bd mv mw oi my mz na oj nc nd kf ok kg nf ki ol kj nh kl om km nj nk bi translated">2.验证集</h1><p id="67fc" class="pw-post-body-paragraph lf lg iq lh b li nn ka lk ll no kd ln lo on lq lr ls oo lu lv lw op ly lz ma ij bi translated">在直接进入模型和训练<strong class="lh ja">之前，创建一个验证集</strong>非常重要。为了避免一次引入太多概念，我在第一课中跳过了这一步。</p><p id="14e6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">验证集</strong>的想法很简单。不是用你所有的数据训练一个模型，而是<strong class="lh ja">把一小部分数据</strong>(通常是 20% — 30%)储存起来，你将使用这些数据来评估训练好的模型<strong class="lh ja">是否能很好地推广到看不见的数据</strong>。这对于确保您的模型可以安全地投入生产以准确评估新数据非常重要。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="263c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在上面的代码中，我使用了<code class="fe ou ov ow ox b">train_test_split</code>函数来随机分割数据，我选择了一个<code class="fe ou ov ow ox b">test_size=0.5</code>。设置<code class="fe ou ov ow ox b">random_state</code>总是一个好主意，以确保当您重新运行代码时，将使用相同的分割。</p><p id="1717" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请注意，这是一个常见且良好的实践，两个不是 2 个而是<strong class="lh ja"> 3 个数据分割:训练、验证和测试</strong>。<strong class="lh ja"> </strong>在这种情况下，当你尝试几个模型、想法和超参数(如学习率)时，你使用验证集来检查进度，当你对结果满意时，才在最后使用测试集。这就是在<strong class="lh ja">卡格尔比赛</strong>中发生的事情，那里通常有一个<strong class="lh ja">隐藏测试装置</strong>。</p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h1 id="6e74" class="mt mu iq bd mv mw oi my mz na oj nc nd kf ok kg nf ki ol kj nh kl om km nj nk bi translated">3.为分类训练模型</h1><p id="8cfd" class="pw-post-body-paragraph lf lg iq lh b li nn ka lk ll no kd ln lo on lq lr ls oo lu lv lw op ly lz ma ij bi translated">我将在这个例子中使用的模型与我在<a class="ae le" rel="noopener" target="_blank" href="/learn-ai-today-01-getting-started-with-pytorch-2e3ba25a518">上一课</a>中用于回归问题的模型完全相同！</p><p id="edb7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">那么有什么区别呢？</strong>不同之处在于<strong class="lh ja">损失函数</strong>。对于<strong class="lh ja">多类分类</strong>问题，通常的选择是<strong class="lh ja">交叉熵损失</strong> (nn。PyTorch 中的 CrossEntropyLoss)。<strong class="lh ja"> </strong>对于<strong class="lh ja">二元分类</strong>的问题，你通常会用<strong class="lh ja">二元交叉熵</strong> <strong class="lh ja">损失</strong> (nn。BCEWithLogitsLoss)。因此，用于定义模型、标准和优化器的代码与我在上一课中用于回归的代码非常相似！</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="e7a8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">要考虑的另一个区别是最后一个激活函数。对于<strong class="lh ja">回归</strong>问题，模型的输出是一个数字，可以是任何实数值。对于<strong class="lh ja">二进制分类</strong>，您需要使用一个<strong class="lh ja"> Sigmoid 激活函数</strong>将输出映射到 0–1 范围。对于<strong class="lh ja">多级分类</strong>，您需要<strong class="lh ja"> Softmax 激活功能</strong>(除非您想要允许多项选择，在这种情况下使用 Sigmoid 激活)。Softmax 输出可以解释为分配给每个类的概率。</p><p id="4809" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在不要太担心激活函数。我只是在这里提一下，让你意识到他们的存在。目前，值得一提的是。CrossEntropyLoss 包括为您激活的 Softmax 和<strong class="lh ja"> nn。BCEWithLogitsLoss </strong>为您提供乙状结肠。这样你就不需要在模型的末尾添加任何激活函数。PyTorch 会帮你搞定的！</p><p id="0e7d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在训练模型之前，我还修改了上一课的<code class="fe ou ov ow ox b">fit</code>函数(你可以在 Kaggle 笔记本上查看，最后有完整的代码),以考虑到<code class="fe ou ov ow ox b">train</code>和<code class="fe ou ov ow ox b">test/validation</code>数据。(当只处理两个数据集时，术语<strong class="lh ja">验证</strong>和<strong class="lh ja">测试</strong>经常互换使用。)</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="f065" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">绘制 1000 次训练中的训练和测试损失图，你可以看到<strong class="lh ja">有些奇怪的事情正在发生</strong>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f43fd06a4a162b0f85d7896e9af9eeac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*eh69wZq1Ar255ed6Iz370A.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">训练和测试损失。图片由作者提供。</p></figure><p id="cd8d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然<strong class="lh ja">列车损失</strong>随着时间的推移不断改善，但<strong class="lh ja">测试损失</strong>最初稳步改善，但随后<strong class="lh ja">开始增加</strong>。也许最重要的是，<strong class="lh ja">你可以通过绘制模型在 1000 个时期的精确度</strong>来看到同样的效果。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/d3635b95a4e0e1ef3cb54744326ed9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*5Nf2aVh8ffyeYVqJLumguQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">训练和测试准确性。图片由作者提供。</p></figure><p id="00c1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你现在看到的是我们所说的<strong class="lh ja">过拟合</strong>——在<strong class="lh ja"> </strong>的某个点，模型开始“记忆”训练数据，而<strong class="lh ja">的泛化性能</strong>(用测试集评估)<strong class="lh ja">下降</strong>。这就是为什么你需要一个验证/测试集。在这种情况下，你可以看到，如果你在 200 个时期停止训练，结果可能会更好。这种方法称为<strong class="lh ja">提前停止</strong>，是一种减少<strong class="lh ja">过拟合</strong>的简单方法。然而，还有其他的方法，比如使用<strong class="lh ja">重量衰减</strong>，我马上会谈到。让我们首先做一些有趣的可视化来更好地理解正在发生的事情。</p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h1 id="faca" class="mt mu iq bd mv mw oi my mz na oj nc nd kf ok kg nf ki ol kj nh kl om km nj nk bi translated">4.可视化决策边界并减少过度拟合</h1><p id="3066" class="pw-post-body-paragraph lf lg iq lh b li nn ka lk ll no kd ln lo on lq lr ls oo lu lv lw op ly lz ma ij bi translated">为了可视化训练边界并更好地理解过度拟合，我仅使用 2 个(而不是 4 个)特征来重新训练模型，以便容易地在 2D 图形中绘制结果。<strong class="lh ja">在下面的动画中，您可以看到在 1000 个训练时期中，训练(左)和测试(右)的决策界限不断演变。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/3e12778b54961fb7a98f10c0a9a7fefa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*AoJrWnIF0TRczLJE5mqutA.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在 1000 个训练时期可视化模型决策边界。作者的动画。</p></figure><p id="e0a3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">看看<strong class="lh ja">最初模型如何快速学习划分区域的 2 条直线边界</strong>。然后，<strong class="lh ja">红黄边界开始向上弯曲</strong>，更好地适应列车组。然而，观察测试集，这导致了性能的<strong class="lh ja">轻微下降，因为更多的黄色点移动到中间区域，而蓝绿色点移动到顶部区域。而<strong class="lh ja">这正是过度拟合的样子</strong>。对于更复杂的问题，你可以在<strong class="lh ja">多维度</strong>中想象这一点——当你有很多维度时，更容易过度拟合。</strong></p><p id="879e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你很难想象任何超过 3 维的东西，只要遵循 Geoffrey Hinton 的建议:<strong class="lh ja">“要处理 14 维空间中的超平面，想象一个 3 维空间，并大声对自己说‘14’。大家都这么干。”</strong></p><p id="d603" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了减少这个例子中的过度拟合，可以使用两个简单的“技巧”:</p><ul class=""><li id="ed2a" class="nl nm iq lh b li lj ll lm lo pa ls pb lw pc ma ns nt nu nv bi translated"><strong class="lh ja">提前停止:</strong>用更少的时期训练模型</li><li id="c874" class="nl nm iq lh b li nw ll nx lo ny ls nz lw oa ma ns nt nu nv bi translated"><strong class="lh ja">权重衰减:</strong>通过在每次迭代中少量减少模型权重，使其变小</li></ul><h2 id="b476" class="pd mu iq bd mv pe pf dn mz pg ph dp nd lo pi pj nf ls pk pl nh lw pm pn nj iw bi translated">4.1 提前停止</h2><p id="e629" class="pw-post-body-paragraph lf lg iq lh b li nn ka lk ll no kd ln lo on lq lr ls oo lu lv lw op ly lz ma ij bi translated"><strong class="lh ja">提前停止</strong>的想法非常简单，正如我之前提到的，如果模型在大约 200 个历元时具有<strong class="lh ja">最佳验证精度，那么如果你只训练 200 个历元，你将得到一个概括得更好的模型——根据验证精度。问题是，您可能会在视觉上过度适应验证集——特别是当您根据验证分数调整许多超参数时。<strong class="lh ja">这就是为什么在完成所有实验后，获得一组额外的数据来评估模型是非常重要的。</strong></strong></p><h2 id="2273" class="pd mu iq bd mv pe pf dn mz pg ph dp nd lo pi pj nf ls pk pl nh lw pm pn nj iw bi translated">4.2 重量衰减</h2><p id="7ede" class="pw-post-body-paragraph lf lg iq lh b li nn ka lk ll no kd ln lo on lq lr ls oo lu lv lw op ly lz ma ij bi translated"><strong class="lh ja">体重衰减</strong>的思路也很简单。当拟合一个<strong class="lh ja">神经网络时，</strong>一般来说，不存在最优解，而是存在多个可能的相似解。<strong class="lh ja">权重衰减</strong>，通过迫使权重保持较小，将迫使优化过程达到更简单的解决方案。</p><p id="aea0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们给我们的模型添加一个<code class="fe ou ov ow ox b">weight_decay=0.01</code>，并像以前一样在训练 1000 个时期后可视化结果。在 PyTorch 中，您只需要将这个参数作为<code class="fe ou ov ow ox b">optimizer = optim.Adam(model.parameters, lr=0.001, weight_decay=0.01)</code>添加到优化器中。生成的动画如下。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/e84c9c1c3d20ca73d988566f7ca0b735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*xE-F6xvvP300eK98fdvLbw.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在权重衰减为 0.01 的 1000 个训练时期期间可视化模型决策边界。作者的动画。</p></figure><p id="30d4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如您所见，现在红黄边界不会像以前那样向上弯曲，因为这需要大幅增加权重，而且精度不会有太大变化。</p><p id="84d0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">用具有重量衰减的 4 个输入特征来训练模型，得到了下面的训练和测试损失图。看，现在<strong class="lh ja">测试损失没有像以前一样开始增加！</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/99eb0d0290f4bf3efc4b5af02ca09ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*skKhIx3rlA6_6vsG207_sg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">用重量衰减训练的模型的训练和测试损失。图片由作者提供。</p></figure><p id="3208" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">同样重要的是要提到<strong class="lh ja">当你开始一个新项目时，过度适应是你应该瞄准的目标</strong>。从一个可以过度拟合数据的模型开始，这样你就知道你的模型有足够的“灵活性”来学习数据中的模式。<strong class="lh ja">然后增加正则化，像权重衰减，避免过拟合！</strong></p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h1 id="1d76" class="mt mu iq bd mv mw oi my mz na oj nc nd kf ok kg nf ki ol kj nh kl om km nj nk bi translated">家庭作业</h1><p id="92d6" class="pw-post-body-paragraph lf lg iq lh b li nn ka lk ll no kd ln lo on lq lr ls oo lu lv lw op ly lz ma ij bi translated">我可以给你看一千个例子，但如果你能自己做一两个实验，你会学到更多！这些实验的完整代码可以在<a class="ae le" href="https://www.kaggle.com/mnpinto/learn-ai-today-02-classification-with-pytorch" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">这本笔记本</strong> </a>上找到。</p><ul class=""><li id="20f2" class="nl nm iq lh b li lj ll lm lo pa ls pb lw pc ma ns nt nu nv bi translated">和上一课一样，试着玩学习率、时期数、重量衰减和模型大小。</li><li id="1009" class="nl nm iq lh b li nw ll nx lo ny ls nz lw oa ma ns nt nu nv bi translated">做些实验，看看结果是否如你所愿，如果不是，看看视觉效果，试着理解为什么。</li></ul><p id="153b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">和往常一样，如果你通过实验创作出了有趣的动画笔记本，那就在 GitHub、Kaggle 上分享吧，或者写一个中型故事！</p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h1 id="5000" class="mt mu iq bd mv mw oi my mz na oj nc nd kf ok kg nf ki ol kj nh kl om km nj nk bi translated">结束语</h1><p id="0711" class="pw-post-body-paragraph lf lg iq lh b li nn ka lk ll no kd ln lo on lq lr ls oo lu lv lw op ly lz ma ij bi translated"><strong class="lh ja"> Learn AI Today </strong>系列第二个故事到此结束！</p><ul class=""><li id="ec4a" class="nl nm iq lh b li lj ll lm lo pa ls pb lw pc ma ns nt nu nv bi translated">请考虑将<a class="ae le" href="https://docs.google.com/forms/d/e/1FAIpQLSc0IBzdCn7osIjvGno1GjBakI-DfXHE8gDLZ--jNzWsXtRW0g/viewform" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">加入我的邮件列表，点击此链接</strong> </a> <strong class="lh ja"> </strong>获取更新，这样你就不会错过下面的任何故事或重要更新了！</li><li id="0d20" class="nl nm iq lh b li nw ll nx lo ny ls nz lw oa ma ns nt nu nv bi translated">我还会在 learn-ai-today.com<a class="ae le" href="http://learn-ai-today.com/" rel="noopener ugc nofollow" target="_blank"><strong class="lh ja"/></a>列出新的故事，这是我为这次学习之旅创建的页面！</li><li id="5afd" class="nl nm iq lh b li nw ll nx lo ny ls nz lw oa ma ns nt nu nv bi translated">如果你以前错过了，<a class="ae le" href="https://www.kaggle.com/mnpinto/learn-ai-today-02-classification-with-pytorch" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">这是 Kaggle 笔记本的链接，上面有这个故事的代码</strong>！</a></li></ul><p id="708c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">欢迎在评论中给我一些反馈。你觉得什么最有用，或者什么可以解释得更好？让我知道！</p><p id="0041" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你可以在下面的故事中了解更多关于我的旅程！</p><div class="mb mc gp gr md me"><a rel="noopener follow" target="_blank" href="/my-3-year-journey-from-zero-python-to-deep-learning-competition-master-6605c188eec7"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ja gy z fp mj fr fs mk fu fw iz bi translated">我的 3 年历程:从零 Python 到深度学习竞赛高手</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">自从 2017 年开始学习 Python 以来，我一直遵循的道路是成为一名独自参加 Kaggle 比赛的大师…</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="po l mp mq mr mn ms ky me"/></div></div></a></div><div class="mb mc gp gr md me"><a rel="noopener follow" target="_blank" href="/my-2-year-journey-on-kaggle-how-i-became-a-competition-master-ef0f0955c35d"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ja gy z fp mj fr fs mk fu fw iz bi translated">我在 Kaggle 上的两年旅程:我如何成为竞赛大师</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">描述我的旅程和策略，我遵循成为一个竞赛大师与个人金牌</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="pp l mp mq mr mn ms ky me"/></div></div></a></div><p id="0db3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="pq">感谢阅读！祝您愉快！</em></p></div></div>    
</body>
</html>