<html>
<head>
<title>Linear Regression Algorithm from Scratch in Python: Step by Step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的线性回归算法:一步一步</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/basic-linear-regression-algorithm-in-python-for-beginners-c519a808b5f8?source=collection_archive---------6-----------------------#2020-07-04">https://towardsdatascience.com/basic-linear-regression-algorithm-in-python-for-beginners-c519a808b5f8?source=collection_archive---------6-----------------------#2020-07-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/7761c4479bfdbcb9b72fbd7c8c288a9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ugEg9Sj-gbBePFtt"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">杰里米·毕晓普在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="e8b4" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">学习线性回归的概念，并使用 python 从头开始开发一个完整的线性回归算法</h2></div><p id="3282" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最基本的机器学习算法必须是单变量线性回归算法。如今，有如此多的先进的机器学习算法、库和技术可用，以至于线性回归似乎并不重要。但是学习基础知识总是一个好主意。这样你会非常清楚地掌握这些概念。在本文中，我将逐步解释线性回归算法。</p><h2 id="9b59" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">想法和公式</h2><p id="d97d" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">线性回归使用预测的非常基本的想法。公式如下:</p><p id="aae4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Y = C + BX</p><p id="0d1e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在学校都学过这个公式。提醒一下，这是一条直线的方程。这里，Y 是因变量，B 是斜率，C 是截距。通常，对于线性回归，它被写成:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/6326b1003287c31f8c4a4cd6e2b92a98.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/0*w9i5SAJ5QPCb_weK.png"/></div></figure><p id="caf4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，“h”是假设或预测的因变量，X 是输入特征，θ0 和θ1 是系数。θ值是随机初始化的。然后使用梯度下降，我们将更新θ值以最小化成本函数。下面是成本函数和梯度下降的解释。</p><h2 id="3359" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">成本函数和梯度下降</h2><p id="df24" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">成本函数决定了预测与原始因变量的距离。这是它的公式</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/5b6eb74ba781a95b3561c2e72f467137.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/0*KMyttb_CKsvZGml4.png"/></div></figure><p id="aea2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">任何机器学习算法的思想都是最小化代价函数，使假设接近原始因变量。为此，我们需要优化θ值。如果我们分别基于θ0 和θ1 对代价函数取偏导数，就会得到梯度下降。为了更新θ值，我们需要从相应的θ值中减去梯度下降:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/ea8c9ed5c48723346e3b48603a20280d.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*5_3TeXfjy-2uTXEe1dms9A.png"/></div></figure><p id="c627" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">偏导数之后，上面的公式将变成:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/625ecd196ca144b89d11257689b0dfd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/0*9QUPcWEDb_5ZkACX.png"/></div></figure><p id="91f4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，m 是训练数据的数量，α是学习率。我说的是一元线性回归。这就是为什么我只有两个θ值。如果有很多变量，那么每个变量都会有θ值。</p><h2 id="8021" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">工作示例</h2><p id="c82e" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我要用的数据集来自吴恩达在 Coursera 上的机器学习课程。下面是用 Python 一步步实现线性回归的过程。</p><ol class=""><li id="18c6" class="na nb jj la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated">导入包和数据集。</li></ol><pre class="mt mu mv mw gt nj nk nl nm aw nn bi"><span id="0b99" class="lu lv jj nk b gy no np l nq nr">import numpy as np<br/>import pandas as pd<br/>df = pd.read_csv('ex1data1.txt', header = None)<br/>df.head()</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/8b4e825f58e17b18f941a613f2c3d8f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/0*c4GtlKtGwRAode0S.png"/></div></figure><p id="dc03" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在该数据集中，第 0 列是输入要素，第 1 列是输出变量或因变量。我们将使用上面的直线公式使用第 0 列来预测第 1 列。</p><p id="0363" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.根据第 0 列绘制第 1 列。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/3f9083d6ad6f295a087b270ae6e461eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/0*ikIOsagMe3ezt6Wc.png"/></div></figure><p id="bdd5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输入变量和输出变量之间的关系是线性的。当关系是线性时，线性回归效果最好。</p><p id="d40b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.初始化θ值。我将θ值初始化为零。但是任何其他值也应该起作用。</p><pre class="mt mu mv mw gt nj nk nl nm aw nn bi"><span id="0f3c" class="lu lv jj nk b gy no np l nq nr">theta = [0,0]</span></pre><p id="67f8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.根据之前讨论的公式定义假设和成本函数。</p><pre class="mt mu mv mw gt nj nk nl nm aw nn bi"><span id="56b1" class="lu lv jj nk b gy no np l nq nr">def hypothesis(theta, X):<br/>    return theta[0] + theta[1]*X</span><span id="3fc4" class="lu lv jj nk b gy nu np l nq nr">def cost_calc(theta, X, y):<br/>    return (1/2*m) * np.sum((hypothesis(theta, X) - y)**2)</span></pre><p id="7222" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">5.根据数据帧的长度计算训练数据的数量。然后定义梯度下降的函数。在这个函数中，我们将更新θ值，直到成本函数最小。它可能需要任意次迭代。在每次迭代中，它将更新 theta 值，并且利用每次更新的 theta 值，我们将计算成本以跟踪成本。</p><pre class="mt mu mv mw gt nj nk nl nm aw nn bi"><span id="eb9e" class="lu lv jj nk b gy no np l nq nr">m = len(df)<br/>def gradient_descent(theta, X, y, epoch, alpha):<br/>    cost = []<br/>    i = 0<br/>    while i &lt; epoch:<br/>        hx = hypothesis(theta, X)<br/>        theta[0] -= alpha*(sum(hx-y)/m)<br/>        theta[1] -= (alpha * np.sum((hx - y) * X))/m<br/>        cost.append(cost_calc(theta, X, y))<br/>        i += 1<br/>    return theta, cost</span></pre><p id="21ce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">6.最后，定义预测函数。它将从梯度下降函数中获得更新的θ，并预测假设或预测的输出变量。</p><pre class="mt mu mv mw gt nj nk nl nm aw nn bi"><span id="03ff" class="lu lv jj nk b gy no np l nq nr">def predict(theta, X, y, epoch, alpha):<br/>    theta, cost = gradient_descent(theta, X, y, epoch, alpha)<br/>    return hypothesis(theta, X), cost, theta</span></pre><p id="b689" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">7.使用预测函数，查找假设、成本和更新的θ值。我选择学习率为 0.01，我将运行这个算法 2000 个时期或迭代。</p><pre class="mt mu mv mw gt nj nk nl nm aw nn bi"><span id="8fac" class="lu lv jj nk b gy no np l nq nr">y_predict, cost, theta = predict(theta, df[0], df[1], 2000, 0.01)</span></pre><p id="e07d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最终的θ值为-3.79 和 1.18。</p><p id="a8f8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">8.在同一个图表中绘制原始 y 和假设或预测 y。</p><pre class="mt mu mv mw gt nj nk nl nm aw nn bi"><span id="438b" class="lu lv jj nk b gy no np l nq nr">%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>plt.figure()<br/>plt.scatter(df[0], df[1], label = 'Original y')<br/>plt.scatter(df[0], y_predict, label = 'predicted y')<br/>plt.legend(loc = "upper left")<br/>plt.xlabel("input feature")<br/>plt.ylabel("Original and Predicted Output")<br/>plt.show()</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/e2619c878ec058e73a376bad8d39689e.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/0*j8uysSOb9It4zAu7.png"/></div></figure><p id="c609" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据公式，假设图是一条直线，并且该直线穿过最佳位置。</p><p id="658d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">9.记住，我们在每次迭代中跟踪成本函数。让我们画出成本函数。</p><pre class="mt mu mv mw gt nj nk nl nm aw nn bi"><span id="4fae" class="lu lv jj nk b gy no np l nq nr">plt.figure()<br/>plt.scatter(range(0, len(cost)), cost)<br/>plt.show()</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e86e56dc69e1a925ec3888b7390684cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*5GlxI0c1ccMRlbGt.png"/></div></figure><p id="7688" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我之前提到的，我们的目的是优化θ值，以最小化成本。正如你从这张图表中看到的，成本在开始时急剧下降，然后变得稳定。这意味着θ值如我们预期的那样得到了正确的优化。</p><p id="29d7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望这有所帮助。下面是本文中使用的数据集的链接:</p><div class="is it gp gr iu nx"><a href="https://github.com/rashida048/Machine-Learning-With-Python/blob/master/ex1data1.txt" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd jk gy z fp oc fr fs od fu fw ji bi translated">rashida 048/用 Python 进行机器学习</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">通过在 GitHub 上创建一个帐户，为 rashida 048/用 Python 进行机器学习开发做出贡献。</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">github.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol ja nx"/></div></div></a></div><p id="9589" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是其他一些机器学习算法的解决方案:</p><p id="2537" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/multivariate-linear-regression-in-python-step-by-step-128c2b127171">Python 中的多元线性回归逐步</a></p><p id="154a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/logistic-regression-with-python-using-optimization-function-91bd2aee79b">使用优化功能的 Python 逻辑回归</a></p></div></div>    
</body>
</html>