<html>
<head>
<title>POS Tagging Using RNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 RNN 的词性标注</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pos-tagging-using-rnn-7f08a522f849?source=collection_archive---------1-----------------------#2020-09-03">https://towardsdatascience.com/pos-tagging-using-rnn-7f08a522f849?source=collection_archive---------1-----------------------#2020-09-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1397" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用 rnn 在英语语料库中使用词性(POS)标签来标记单词</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/799a788787795b65112af261b948770f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dVQQmZFZ4m-t6OQF"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安格尔·坎普在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7cf1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">做<strong class="lb iu">词性标注</strong>的经典方式是使用<strong class="lb iu">隐马尔可夫模型的某种变体。</strong>在这里，我们将看到如何使用递归神经网络来实现这一点。最初的 RNN 建筑也有一些变化。它有一个新颖的 RNN 架构——<strong class="lb iu">双向 RNN </strong>,能够以“逆序”读取序列，并已被证明可以显著提高性能。</p><p id="c6af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后是 RNN 的两个重要的前沿变体，它们使得在真实数据集上训练大型网络成为可能。尽管 rnn 能够解决各种序列问题，但是由于在 rnn 的训练期间出现的爆炸和消失梯度的问题，它们的架构本身是它们最大的敌人。这个问题由两个流行的<strong class="lb iu">门控 RNN 架构</strong>—<strong class="lb iu">长、短时记忆(LSTM) </strong>和<strong class="lb iu">门控递归单元(GRU)解决。</strong>我们将在这里研究所有这些与词性标注相关的模型。</p><h1 id="77a7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">词性标注—概述</h1><p id="33e6" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">将单词分类到它们的<strong class="lb iu">词性</strong>并相应地标注它们的过程被称为<strong class="lb iu">词性标注</strong>，或者简称为<strong class="lb iu">词性标注</strong>。NLTK 库有许多包含单词及其词性标签的语料库。我将使用来自 NLTK 的带词性标记的语料库，即<strong class="lb iu"> treebank </strong>、<strong class="lb iu"> conll2000、</strong>和<strong class="lb iu"> brown </strong>来演示关键概念。为了直接进入代码，Kaggle 上发布了一个附带的<a class="ae ky" href="https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><div class="ms mt gp gr mu mv"><a href="https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">使用 RNN 的词性标注</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">使用 Kaggle 笔记本探索和运行机器学习代码|使用来自单词嵌入的数据</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">www.kaggle.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ks mv"/></div></div></a></div><p id="8e79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下表提供了一些主要标签的信息:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/11d4c8670bfb939bea657164e6777166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*IFSNZ_fYt6goNXjdevjIgQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nl">位置标记</strong></p></figure><h1 id="8745" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">文章布局</h1><ol class=""><li id="1923" class="nm nn it lb b lc mn lf mo li no lm np lq nq lu nr ns nt nu bi translated">预处理数据</li><li id="328a" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">单词嵌入</li><li id="78e1" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">香草 RNN</li><li id="d229" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">LSTM</li><li id="825b" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">苏军总参谋部情报总局</li><li id="1fcd" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">双向 LSTM</li><li id="9848" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">模型评估</li></ol><h1 id="d1f0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">导入数据集</h1><p id="1237" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让我们从导入必要的库和加载数据集开始。这是每一个数据分析过程中必不可少的一步(完整的代码可以在<a class="ae ky" href="https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn" rel="noopener ugc nofollow" target="_blank">这里</a>查看)。我们将首先使用三个著名的文本语料库加载数据，并对它们进行合并。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="5008" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># Importing and Loading the data into data frame<br/># load POS tagged corpora from NLTK</em></strong><br/>treebank_corpus = treebank.tagged_sents(tagset='universal')<br/>brown_corpus = brown.tagged_sents(tagset='universal')<br/>conll_corpus = conll2000.tagged_sents(tagset='universal')<br/><br/><strong class="ob iu"><em class="ok"># Merging the dataframes to create a master df<br/></em></strong>tagged_sentences = treebank_corpus + brown_corpus + conll_corpus</span></pre><h1 id="84a7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak"> 1。预处理数据</strong></h1><p id="2db5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">作为预处理的一部分，我们将执行各种步骤，例如将数据分成单词和标签，对 X 和 Y 进行矢量化，以及填充序列。</p><p id="107b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们先来看数据。对于下面的每个单词，都有一个与之相关的标签。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="6cb3" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># let's look at the data</em></strong><br/>tagged_sentences[7]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/e4b3dc569690ac0fceb7b731d42280ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iO7wJSFP_3w3xIdk2o3q2w.png"/></div></div></figure><h2 id="766f" class="of lw it bd lx om on dn mb oo op dp mf li oq or mh lm os ot mj lq ou ov ml ow bi translated">将数据分为单词(X)和标签(Y)</h2><p id="8cbe" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">由于这是一个<strong class="lb iu">多对多</strong>问题，每个数据点将是语料库的不同句子。每个数据点在<strong class="lb iu">输入序列</strong>中都有多个字。这就是我们所说的<strong class="lb iu"> X </strong>。每个单词在<strong class="lb iu">输出序列</strong>中都有相应的标签。这就是我们所说的<strong class="lb iu"> Y </strong>。样本数据集:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/500f135428f523118d33f0f0d1a34259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*5B3nQBl3lu1VBOKoXdRK0A.png"/></div></figure><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="cc51" class="of lw it ob b gy og oh l oi oj">X = [] <strong class="ob iu"><em class="ok"># store input sequence</em></strong><br/>Y = [] <strong class="ob iu"><em class="ok"># store output sequence</em></strong></span><span id="0f18" class="of lw it ob b gy oy oh l oi oj">for sentence in tagged_sentences:<br/> X_sentence = []<br/> Y_sentence = []<br/> for entity in sentence: <br/> X_sentence.append(entity[0])<strong class="ob iu"><em class="ok"> # entity[0] contains the word</em></strong><br/> Y_sentence.append(entity[1]) <strong class="ob iu"><em class="ok"># entity[1] contains corresponding tag</em></strong><br/> <br/> X.append(X_sentence)<br/> Y.append(Y_sentence)</span><span id="e65b" class="of lw it ob b gy oy oh l oi oj">num_words = len(set([word.lower() for sentence in X for word in sentence]))<br/>num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))</span><span id="9dbd" class="of lw it ob b gy oy oh l oi oj">print("Total number of tagged sentences: {}".format(len(X)))<br/>print("Vocabulary size: {}".format(num_words))<br/>print("Total number of tags: {}".format(num_tags))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/a536d53e8561941168dfd72e60dc5e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFoSzNTfjJd0wOtJs4Ygog.png"/></div></div></figure><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="7b22" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># let’s look at first data point<br/># this is one data point that will be fed to the RNN</em></strong><br/>print(‘sample X: ‘, X[0], ‘\n’)<br/>print(‘sample Y: ‘, Y[0], ‘\n’)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/c8bae8c7766324377a1eeffc8b706da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*70pwrtr56Ome7AXE1mNjIQ.png"/></div></div></figure><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="b72b" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># In this many-to-many problem, the length of each input and output sequence must be the same.<br/># Since each word is tagged, it’s important to make sure that the length of input sequence equals the output sequence</em></strong></span><span id="4754" class="of lw it ob b gy oy oh l oi oj">print(“Length of first input sequence : {}”.format(len(X[0])))<br/>print(“Length of first output sequence : {}”.format(len(Y[0])))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/d3abff1cb78fb8ec6166254dc3e96feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*okaKk_eJi0S1g2_ztmm8gQ.png"/></div></div></figure><p id="02cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要解决的下一件事是，我们如何将这些输入提供给 RNN。如果我们必须把单词作为神经网络的输入，那么我们必须把它们转换成数字。我们需要创建一个单词嵌入或一个热点矢量，即每个单词的数字形式的矢量。首先，我们将首先对输入和输出进行编码，这将为输入数据的整个语料库中的每个单词提供一个盲唯一 id。另一方面，我们有 Y 矩阵(标签/输出数据)。我们这里有 12 个 pos 标签，将它们中的每一个视为一个类，每个 POS 标签都被转换为长度为 12 的独热编码。我们将使用 Keras 库中的 Tokenizer()函数将文本序列编码为整数序列。</p><h2 id="d27a" class="of lw it bd lx om on dn mb oo op dp mf li oq or mh lm os ot mj lq ou ov ml ow bi translated">向量化 X 和 Y</h2><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="ac09" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># encode X<br/></em></strong>word_tokenizer = Tokenizer()              <strong class="ob iu"><em class="ok"># instantiate tokeniser</em></strong><br/>word_tokenizer.fit_on_texts(X)            <strong class="ob iu"><em class="ok"># fit tokeniser on data</em></strong></span><span id="a2c3" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># use the tokeniser to encode input sequence</em></strong><br/>X_encoded = word_tokenizer.texts_to_sequences(X)  </span><span id="d9d0" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># encode Y<br/></em></strong>tag_tokenizer = Tokenizer()<br/>tag_tokenizer.fit_on_texts(Y)<br/>Y_encoded = tag_tokenizer.texts_to_sequences(Y)</span><span id="7c41" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># look at first encoded data point<br/></em></strong>print("** Raw data point **", "\n", "-"*100, "\n")<br/>print('X: ', X[0], '\n')<br/>print('Y: ', Y[0], '\n')<br/>print()<br/>print("** Encoded data point **", "\n", "-"*100, "\n")<br/>print('X: ', X_encoded[0], '\n')<br/>print('Y: ', Y_encoded[0], '\n')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/d3a3bce25fb066f8963e8f768f029cd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vt7yR8x0ZCLKQFZyjdBSA.png"/></div></div></figure><p id="6355" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">确保每个输入和输出序列的长度相同。</p><h2 id="dc9e" class="of lw it bd lx om on dn mb oo op dp mf li oq or mh lm os ot mj lq ou ov ml ow bi translated">填充序列</h2><p id="111c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">语料库中的句子长度不同。在我们输入 RNN 模型之前，我们需要确定句子的长度。我们不能动态分配处理语料库中每个句子所需的内存，因为它们的长度不同。因此，数据编码后的下一步是<strong class="lb iu">定义序列长度</strong>。我们需要要么填充短句，要么把长句截成固定长度。然而，这个固定长度是一个<strong class="lb iu">超参数</strong>。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="db52" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># Pad each sequence to MAX_SEQ_LENGTH using KERAS’ pad_sequences() function. <br/># Sentences longer than MAX_SEQ_LENGTH are truncated.<br/># Sentences shorter than MAX_SEQ_LENGTH are padded with zeroes.</em></strong></span><span id="84f6" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># Truncation and padding can either be ‘pre’ or ‘post’. <br/># For padding we are using ‘pre’ padding type, that is, add zeroes on the left side.<br/># For truncation, we are using ‘post’, that is, truncate a sentence from right side.</em></strong></span><span id="76eb" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># sequences greater than 100 in length will be truncated<br/></em></strong>MAX_SEQ_LENGTH = 100</span><span id="9622" class="of lw it ob b gy oy oh l oi oj">X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=”pre”, truncating=”post”)<br/>Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=”pre”, truncating=”post”)</span><span id="0550" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># print the first sequence</em></strong><br/>print(X_padded[0], "\n"*3)<br/>print(Y_padded[0])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/cdd366d91d3bca0ce3861c2fbef37b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kMd8iuO0gbgiaWA3IcMHkw.png"/></div></div></figure><h1 id="3f92" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">2.单词嵌入</h1><p id="3fa8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">你知道一种更好的表示文本的方式是<strong class="lb iu">单词嵌入</strong>。目前，每个单词和每个标签都被编码为一个整数。我们将使用一种更复杂的技术来表示输入单词(X ),这就是所谓的单词嵌入。</p><p id="795a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，为了表示 Y 中的每个标签，我们将简单地使用独热编码方案，因为数据集中只有 12 个标签，并且 LSTM 在学习这些标签的自己的表示时没有问题。</p><p id="b8f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要使用单词嵌入，您可以使用以下任一模型:</p><ol class=""><li id="208d" class="nm nn it lb b lc ld lf lg li pe lm pf lq pg lu nr ns nt nu bi translated"><a class="ae ky" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> word2vec 型号</strong> </a></li><li id="55d2" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">手套型号</strong> </a></li></ol><p id="a73e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用 word2vec 模型没有特别的原因。这两者在表示单词方面都非常高效。你可以两个都试试，看看哪个效果更好。</p><p id="69bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">一个单词嵌入的维数是:(词汇 _ 大小，嵌入 _ 维数)</strong></p><h2 id="82cc" class="of lw it bd lx om on dn mb oo op dp mf li oq or mh lm os ot mj lq ou ov ml ow bi translated">对输入序列使用单词嵌入(X)</h2><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="f25f" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># word2vec<br/></em></strong>path = ‘../input/wordembeddings/GoogleNews-vectors-negative300.bin’</span><span id="4e22" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># load word2vec using the following function present in the gensim library</em></strong><br/>word2vec = KeyedVectors.load_word2vec_format(path, binary=True)</span><span id="d92e" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># assign word vectors from word2vec model<br/># each word in word2vec model is represented using a 300 dimensional vector</em></strong></span><span id="d760" class="of lw it ob b gy oy oh l oi oj">EMBEDDING_SIZE  = 300  <br/>VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1</span><span id="38c5" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"># create an empty embedding matix</strong><br/>embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))</span><span id="c06d" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"># create a word to index dictionary mapping</strong><br/>word2id = word_tokenizer.word_index</span><span id="17e0" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"># copy vectors from word2vec model to the words present in corpus</strong><br/>for word, index in word2id.items():<br/>    try:<br/>        embedding_weights[index, :] = word2vec[word]<br/>    except KeyError:<br/>        pass</span></pre><h2 id="09e9" class="of lw it bd lx om on dn mb oo op dp mf li oq or mh lm os ot mj lq ou ov ml ow bi translated">对输出序列使用一键编码(Y)</h2><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="5827" class="of lw it ob b gy og oh l oi oj"># use Keras’ to_categorical function to one-hot encode Y<br/>Y = to_categorical(Y)</span></pre><p id="b4dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有的数据预处理现在都完成了。现在让我们通过<strong class="lb iu">将数据分割为训练、验证和测试集</strong>来跳转到建模部分。</p><p id="f9c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在使用 RNN 之前，我们必须确保数据的维度是 RNN 所期望的。一般来说，RNN 人期望以下形状</p><p id="536c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">X 的形状:(#样本，#时间步长，#特征)</strong></p><p id="a39c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">Y 的形状:(#样本，#时间步长，#特征)</strong></p><p id="9806" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，根据建筑的类型，你可以用不同的形状来制作 RNN。由于我们正在研究的问题具有多对多的架构，输入和输出都包括时间步长的数量，而时间步长就是序列长度。但是注意张量 X 没有第三维度，也就是特征数。这是因为在将数据输入 RNN 之前，我们将使用单词嵌入，因此没有必要明确提到第三个维度。这是因为当您在 Keras 中使用 Embedding()图层时，训练数据将自动转换为(<strong class="lb iu"> #samples、#timesteps、#features </strong>)，其中<strong class="lb iu"> #features </strong>将是嵌入维度(请注意，嵌入图层始终是 RNN 的第一个图层)。使用嵌入层时，我们只需要将数据整形为我们已经完成的(#samples，#timesteps)。但是，请注意，如果不使用 Keras 中的 Embedding()层，您需要将其调整为(#samples、#timesteps、#features)。</p><h1 id="1947" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">3.香草 RNN</h1><p id="154c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">接下来，让我们建立 RNN 模型。我们将使用单词嵌入来表示单词。现在，在训练模型的同时，你也可以<strong class="lb iu">训练单词嵌入</strong>和网络权重。这些通常被称为<strong class="lb iu">嵌入权重</strong>。训练时，嵌入权重将被视为网络的正常权重，在每次迭代中更新。</p><p id="423f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在接下来的几节中，我们将尝试以下三种 RNN 模型:</p><ul class=""><li id="42f4" class="nm nn it lb b lc ld lf lg li pe lm pf lq pg lu ph ns nt nu bi translated">具有任意初始化的<strong class="lb iu">的 RNN，不可训练的嵌入</strong>:在这个模型中，我们将任意初始化嵌入权重。此外，我们将<strong class="lb iu">冻结嵌入</strong>，也就是说，我们将不允许网络训练它们。</li><li id="0823" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu ph ns nt nu bi translated">RNN 与<strong class="lb iu">任意初始化，可训练的嵌入</strong>:在这个模型中，我们将允许网络训练嵌入。</li><li id="c976" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu ph ns nt nu bi translated">具有<strong class="lb iu">可训练 word2vec 嵌入的 RNN:</strong>在这个实验中，我们将使用 word2vec 单词嵌入<em class="ok">和</em>也允许网络进一步训练它们。</li></ul><h2 id="a534" class="of lw it bd lx om on dn mb oo op dp mf li oq or mh lm os ot mj lq ou ov ml ow bi translated">未初始化的固定嵌入</h2><p id="4bef" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让我们从第一个实验开始:一个普通的 RNN，带有任意初始化的、不可训练的嵌入。对于这个 RNN，我们不会使用预先训练的单词嵌入。我们将使用随机初始化的嵌入。此外，我们不会更新嵌入权重。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="7c35" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># create architecture<br/></em></strong>rnn_model = Sequential()</span><span id="5849" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># create embedding layer — usually the first layer in text problems<br/># vocabulary size — number of unique words in data<br/></em></strong>rnn_model.add(Embedding(input_dim = VOCABULARY_SIZE, <br/><strong class="ob iu"><em class="ok"># length of vector with which each word is represented</em></strong><br/> output_dim = EMBEDDING_SIZE, <br/><strong class="ob iu"><em class="ok"># length of input sequence</em></strong><br/> input_length = MAX_SEQ_LENGTH, <br/><strong class="ob iu"><em class="ok"># False — don’t update the embeddings</em></strong><br/> trainable = False <br/>))</span><span id="db4d" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># add an RNN layer which contains 64 RNN cells<br/># True — return whole sequence; False — return single output of the end of the sequence<br/></em></strong>rnn_model.add(SimpleRNN(64, <br/> return_sequences=True<br/>))</span><span id="a026" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># add time distributed (output at each sequence) layer</em></strong><br/>rnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation=’softmax’)))</span><span id="87d7" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok">#compile model</em></strong><br/>rnn_model.compile(loss      =  'categorical_crossentropy',<br/>                  optimizer =  'adam',<br/>                  metrics   =  ['acc'])</span><span id="c183" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># check summary of the model</em></strong><br/>rnn_model.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/c03d4e6bfddb2e40bcb5936cc776da2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oTjqEo0e1IdJRr_VH2JKIg.png"/></div></div></figure><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="eaa3" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok">#fit model</em></strong><br/>rnn_training = rnn_model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_validation, Y_validation))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/8c0bf5529971ceca14141bd5d6b0b8e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRlvevKTszxFzcHUsyh83A.png"/></div></div></figure><p id="1fba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以在这里看到，在十个纪元之后，它给出了大约 95%的相当不错的<strong class="lb iu">准确度。</strong>此外，我们在下图中看到了一条健康的增长曲线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/841f7fb3c181b7acdc2248b8822cbff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*UuWW-j8pbuf7ZvoRz0T3qg.png"/></div></figure><h2 id="eea8" class="of lw it bd lx om on dn mb oo op dp mf li oq or mh lm os ot mj lq ou ov ml ow bi translated">未初始化的可训练嵌入</h2><p id="4275" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">接下来，尝试第二种模型——带有任意初始化的<strong class="lb iu">的 RNN，可训练的嵌入</strong>。这里，我们将允许嵌入与网络一起训练。我所做的只是将参数<strong class="lb iu">trainiable 改为 true，即 trainable = True。</strong>其余一切同上。在检查模型摘要时，我们可以看到所有的参数都是可训练的。即可训练参数等于总参数。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="5a34" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># check summary of the model</em></strong><br/>rnn_model.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/961c59bdaa603264334ea0bf68b62ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RyGL3uWYCJbmVwXxG8G4jw.png"/></div></div></figure><p id="cde9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在拟合模型时，精确度显著提高。通过允许嵌入权重训练，它已经上升到大约 98.95%。因此，嵌入对网络的性能有着重要的影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/73dfb647d8cdf455296ea0306244fb75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*728TjrUgRQRzDs4JBWNxrw.png"/></div></div></figure><p id="70e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在将尝试 word2vec 嵌入，看看它是否能改进我们的模型。</p><h2 id="a46d" class="of lw it bd lx om on dn mb oo op dp mf li oq or mh lm os ot mj lq ou ov ml ow bi translated">使用预先训练的嵌入权重</h2><p id="b0d3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在让我们尝试第三个实验——使用<strong class="lb iu">可训练 word2vec 嵌入的 RNN。</strong>回想一下，我们已经在一个名为‘embedding _ weights’的矩阵中加载了 word2vec 嵌入。使用 word2vec 嵌入就像在模型架构中包含这个矩阵一样简单。</p><p id="59e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网络架构与上面相同，但是我们将使用来自 word2vec 的预训练嵌入权重(<strong class="lb iu">weights =【embedding _ weights】</strong>)，而不是从任意嵌入矩阵开始。在这种情况下，精确度进一步提高到了<strong class="lb iu">大约 99.04%。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/675a7a0e50a8041156ca3bb66ccbb015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u95oGmGrqfxb3-efOpzRsg.png"/></div></div></figure><p id="5c95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，结果略有改善。那是因为这个模型已经表现得很好了。在没有如此好的模型性能的情况下，通过使用预先训练的嵌入，您将看到更多的改进。预训练的嵌入在许多应用中提供了真正的提升。</p><h1 id="26b4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">4.LSTM</h1><p id="bec3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了解决渐变消失的问题，人们已经做了很多尝试来调整传统的 RNNs，使得当序列变长时，渐变不会消失。这些尝试中最受欢迎和最成功的是<strong class="lb iu">长短期记忆网络</strong>，或<strong class="lb iu"> LSTM </strong>。LSTMs 被证明是如此有效，以至于它们几乎取代了普通的 rnn。</p><p id="ac24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，RNN 和 LSTM 之间的一个根本区别是，LSTM 有一个<strong class="lb iu">外显记忆</strong> <strong class="lb iu">单元</strong>，它存储与学习某些任务相关的信息。在标准 RNN 中，网络记忆过去信息的唯一方式是随着时间的推移更新隐藏状态，但它没有显式的内存来存储信息。</p><p id="32dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，在 LSTMs 中，即使序列变得很长，存储单元也能保留信息片段。</p><p id="4587" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将建立一个 LSTM 模型，而不是 RNN。我们只需要用 LSTM 层替换 RNN 层。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="1802" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># create architecture<br/></em></strong>lstm_model = Sequential()<br/><strong class="ob iu"><em class="ok"># vocabulary size — number of unique words in data<br/># length of vector with which each word is represented<br/></em></strong>lstm_model.add(Embedding(input_dim = VOCABULARY_SIZE, <br/> output_dim = EMBEDDING_SIZE, <br/><strong class="ob iu"><em class="ok"># length of input sequence<br/></em></strong>input_length = MAX_SEQ_LENGTH, <br/><strong class="ob iu"><em class="ok"># word embedding matrix<br/></em></strong>weights = [embedding_weights],<br/><strong class="ob iu"><em class="ok"># True — update embeddings_weight matrix<br/></em></strong>trainable = True <br/>))</span><span id="5768" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># add an LSTM layer which contains 64 LSTM cells<br/># True — return whole sequence; False — return single output of the end of the sequence</em></strong><br/>lstm_model.add(<strong class="ob iu">LSTM</strong>(64, return_sequences=True))<br/>lstm_model.add(TimeDistributed(Dense(NUM_CLASSES, activation=’softmax’)))</span><span id="53f7" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok">#compile model</em></strong><br/>rnn_model.compile(loss      =  'categorical_crossentropy',<br/>                  optimizer =  'adam',<br/>                  metrics   =  ['acc'])</span><span id="c403" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># check summary of the model</em></strong><br/>rnn_model.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/368ca68f9947c6712220b119fb54fc3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZHUBTbBOOoMx8t_eqEh4FA.png"/></div></div></figure><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="4a96" class="of lw it ob b gy og oh l oi oj">lstm_training = lstm_model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_validation, Y_validation))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/d886093e98ecbceb09b73000cf7d374c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ShdvezFHbGsgGCgynfTE0g.png"/></div></div></figure><p id="1400" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LSTM 模式也提供了一些边际改进。然而，如果我们在其他任务中使用 LSTM 模型，如语言翻译、图像字幕、时间序列预测等。那么您可能会看到性能的显著提升。</p><h1 id="98b2" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">5.苏军总参谋部情报总局</h1><p id="0d7f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">考虑到计算开销和过度拟合的问题，研究人员试图提出 LSTM 单元的替代结构。这些替代方案中最受欢迎的是<strong class="lb iu">门控循环单元(GRU)。作为一个比 LSTM 更简单的车型，GRU 总是更容易训练。LSTMs 和 GRUs 在实践中几乎完全取代了标准的 RNNs，因为它们比普通的 RNNs 更有效，训练速度更快(尽管参数数量更多)。</strong></p><p id="6231" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们建立一个 GRU 模型。然后，我们还将比较 RNN、LSTM 和 GRU 模型的性能。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="8e20" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># create architecture<br/></em></strong>lstm_model = Sequential()<br/><strong class="ob iu"><em class="ok"># vocabulary size — number of unique words in data<br/># length of vector with which each word is represented<br/></em></strong>lstm_model.add(Embedding(input_dim = VOCABULARY_SIZE, <br/> output_dim = EMBEDDING_SIZE, <br/><strong class="ob iu"><em class="ok"># length of input sequence<br/></em></strong>input_length = MAX_SEQ_LENGTH, <br/><strong class="ob iu"><em class="ok"># word embedding matrix<br/></em></strong>weights = [embedding_weights],<br/><strong class="ob iu"><em class="ok"># True — update embeddings_weight matrix<br/></em></strong>trainable = True <br/>))</span><span id="6eea" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># add an LSTM layer which contains 64 LSTM cells<br/># True — return whole sequence; False — return single output of the end of the sequence</em></strong><br/>lstm_model.add(<strong class="ob iu">GRU</strong>(64, return_sequences=True))<br/>lstm_model.add(TimeDistributed(Dense(NUM_CLASSES, activation=’softmax’)))</span><span id="b360" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok">#compile model</em></strong><br/>rnn_model.compile(loss      =  'categorical_crossentropy',<br/>                  optimizer =  'adam',<br/>                  metrics   =  ['acc'])</span><span id="1297" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># check summary of the model</em></strong><br/>rnn_model.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/59ab6e0c537cc6f2694ffc2fb39e87ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sobqvW_DuBSjkKTjRNe_Uw.png"/></div></div></figure><p id="c3db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与 LSTM 相比，GRU 的参数有所减少。因此，我们在计算效率方面得到了显著的提高，而模型的性能几乎没有任何下降。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="9e90" class="of lw it ob b gy og oh l oi oj">gru_training = gru_model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_validation, Y_validation))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/a6fe3945b0f35c1ca5d533758e2cc52b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AEzK0F-CG3MEd179aLW_Vg.png"/></div></div></figure><p id="7fcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型的精度与 LSTM 相同。但是我们看到 LSTM 花费的时间比 GRU 和 RNN 要长。这是意料之中的，因为 LSTM 和 GRU 的参数分别是正常 RNN 的 4 倍和 3 倍。</p><h1 id="69db" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">6.双向 LSTM</h1><p id="6089" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">例如，当你想<strong class="lb iu">给一段文字(比如一篇客户评论)分配一个情感分数</strong>时，网络可以在给它们分配分数之前看到整个评论文字。另一方面，在给定先前几个键入的单词的情况下，在诸如<strong class="lb iu">预测下一个单词</strong>的任务中，在预测下一个单词时，网络不能访问未来时间步中的单词。</p><p id="d1bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这<strong class="lb iu">两类任务</strong>分别称为离线<strong class="lb iu">和在线</strong>序列处理。</p><p id="22ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，你可以用一个巧妙的技巧来处理<strong class="lb iu">离线任务</strong>——既然网络在做出预测之前可以访问整个序列，为什么不使用这个任务来让网络在训练时“查看序列中的未来元素”，希望这将使网络学习得更好？</p><p id="9e2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是所谓的<strong class="lb iu">双向 RNNs </strong>所利用的想法。</p><p id="3f1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用双向 RNNs，几乎可以肯定你会得到更好的结果。然而，由于网络参数的数量增加，双向 rnn 花费几乎两倍的时间来训练。因此，您需要在训练时间和性能之间进行权衡。使用双向 RNN 的决定取决于您拥有的计算资源和您想要的性能。</p><p id="6932" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，让我们再构建一个模型——一个<strong class="lb iu">双向 LSTM </strong>,并与之前的模型在准确性和训练时间方面进行比较。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="9959" class="of lw it ob b gy og oh l oi oj"><strong class="ob iu"><em class="ok"># create architecture</em></strong></span><span id="6492" class="of lw it ob b gy oy oh l oi oj">bidirect_model = Sequential()<br/>bidirect_model.add(Embedding(input_dim = VOCABULARY_SIZE,<br/> output_dim = EMBEDDING_SIZE,<br/> input_length = MAX_SEQ_LENGTH,<br/> weights = [embedding_weights],<br/> trainable = True<br/>))<br/>bidirect_model.add(<strong class="ob iu">Bidirectional</strong>(<strong class="ob iu">LSTM</strong>(64, return_sequences=True)))<br/>bidirect_model.add(TimeDistributed(Dense(NUM_CLASSES, activation=’softmax’)))</span><span id="c080" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok">#compile model<br/></em></strong>bidirect_model.compile(loss='categorical_crossentropy',<br/>              optimizer='adam',<br/>              metrics=['acc'])</span><span id="5294" class="of lw it ob b gy oy oh l oi oj"><strong class="ob iu"><em class="ok"># check summary of model</em></strong><br/>bidirect_model.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/6eb8288aeb9ce3c3d5b38103619517a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EnBMR4Oq-mCxVvFbPYD0-Q.png"/></div></div></figure><p id="6c90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以看到参数的数量增加了。它确实大大提高了参数的数量。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="5d83" class="of lw it ob b gy og oh l oi oj">bidirect_training = bidirect_model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_validation, Y_validation))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pt"><img src="../Images/2116747b0cd3d3cc342375bf1337c874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CNRgcSwT0VMxt0D5y0cZ-w.png"/></div></div></figure><p id="6eec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">双向 LSTM 确实大大提高了精确度(考虑到精确度已经达到顶点)。这显示了双向 LSTMs 的威力。然而，这种精度的提高是有代价的。所花的时间几乎是普通 LSTM 网络的两倍。</p><h1 id="114c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">7.模型评估</h1><p id="4dc4" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">下面是我们尝试的四种模式的快速总结。当我们从一种模式转向另一种模式时，我们可以看到一种趋势。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="a183" class="of lw it ob b gy og oh l oi oj">loss, accuracy = rnn_model.evaluate(X_test, Y_test, verbose = 1)<br/>print(“Loss: {0},\nAccuracy: {1}”.format(loss, accuracy))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/8b4b434ee16472bc0c18d517c9cf545b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3t7fBwGvkVj-9wCkJMjULA.png"/></div></div></figure><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="9f5e" class="of lw it ob b gy og oh l oi oj">loss, accuracy = lstm_model.evaluate(X_test, Y_test, verbose = 1)<br/>print(“Loss: {0},\nAccuracy: {1}”.format(loss, accuracy))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pv"><img src="../Images/11f35998a5bea6a62e9915785e2a19d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mputhPKtLjXkb_wtMnZ8mg.png"/></div></div></figure><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="e086" class="of lw it ob b gy og oh l oi oj">loss, accuracy = gru_model.evaluate(X_test, Y_test, verbose = 1)<br/>print(“Loss: {0},\nAccuracy: {1}”.format(loss, accuracy))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/107e253537e9d7df6cbea15abd6a5f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i5yeUfRp9XGDB1fPUGyOqg.png"/></div></div></figure><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="4bbd" class="of lw it ob b gy og oh l oi oj">loss, accuracy = bidirect_model.evaluate(X_test, Y_test, verbose = 1)<br/>print("Loss: {0},\nAccuracy: {1}".format(loss, accuracy))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/912c4af9e87604667b3090f6d7f4ac64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G4LP6OIQdNjpD9EFVvlCtQ.png"/></div></div></figure><p id="b990" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有任何问题、建议或批评，可以通过 LinkedIn 或评论区联系我。</p></div></div>    
</body>
</html>