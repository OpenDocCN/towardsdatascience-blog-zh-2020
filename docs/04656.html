<html>
<head>
<title>Why Sigmoid: A Probabilistic Perspective</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么是 Sigmoid:概率观点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-sigmoid-a-probabilistic-perspective-42751d82686?source=collection_archive---------7-----------------------#2020-04-25">https://towardsdatascience.com/why-sigmoid-a-probabilistic-perspective-42751d82686?source=collection_archive---------7-----------------------#2020-04-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="3a33" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/video-tutorial" rel="noopener" target="_blank">视频教程</a></h2><div class=""/><div class=""><h2 id="2863" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated"><em class="kr">你知道为什么 sigmoid 函数的输出可以解释为概率吗？</em></h2></div><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ks"><img src="../Images/f4a4e87b544a3d38cc91cb65674d853d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Th7oBTqTheHutmzXQfI9w.jpeg"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">飞越太浩湖，2018 年春天，我和 DJI 马维克航空公司拍摄的照片</p></figure><p id="a828" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di">如果你以前上过任何机器学习课程，你一定在某个时候遇到过逻辑回归。有一个 sigmoid 函数将线性预测值与最终预测值联系起来。</span></p><blockquote class="mn"><p id="1b4c" class="mo mp it bd mq mr ms mt mu mv mw md dk translated">根据您所学的课程，这个 sigmoid 函数通常是凭空产生的，并作为将数字线映射到所需范围[0，1]的函数引入。有无限多的函数可以做这种映射，为什么是这个呢？</p></blockquote><p id="6cc6" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">需要注意的一个关键点是，sigmoid 的输出被解释为一个<em class="nc">概率</em>。很明显，0 和 1 之间的任何数字都不能被解释为概率。解释必须来自模型公式和随之而来的一系列假设。</p><p id="d93b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nc">如果不想看全文，可以在这里看视频版:</em></p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="915a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">“为什么是乙状结肠”这个问题曾经困扰了我很久。网上很多回答都没有切中要害。我发现的这类答案最常提到的关键词是“logit”和“log odds ”,只是简单地将 sigmoid 转换为它的逆函数，这不仅没有解释为什么我们选择 log odds 作为线性预测器的目标，也没有说明这种选择的含义。一些更好的提到了“广义线性模型”，但它们与介绍性课程有着相同的弱点，介绍性课程提到了概念，但没有真正回答“为什么”的内在联系。<strong class="lk jd">真正的答案应该可以帮助你从零开始设计这个算法，而不需要事先了解它</strong>。当你面对一个只有基本概率和统计知识的二元分类问题时，你应该能够想到“好吧，解决这个问题的最符合逻辑的方法之一就是遵循这个精确的模型设计”。</p><p id="9d07" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章中，我将尽最大努力以一种易于阅读的方式安排逻辑流程，这样就清楚了 sigmoid 是带有一些重要假设的概率二进制分类的自然设计选择。就像讲故事一样，逻辑不一定是好的和线性的，一些点可能看起来是平行的，但它们都有助于逻辑模型的设计动机。所以如果你关心这个话题，请坐下来耐心听我说一会儿。这将是一篇很长的帖子，信息量相当于机器学习书籍中的一整章。</p><p id="b7eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一些关键词和主题在前面:</p><ul class=""><li id="72de" class="nf ng it lk b ll lm lo lp lr nh lv ni lz nj md nk nl nm nn bi translated">线性回归的概率解释，最大似然估计</li><li id="e5d0" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">高斯鉴别分析</li><li id="ba76" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">逻辑回归的潜在变量公式</li><li id="57c7" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">从替代方案中获得洞察力:概率单位模型</li><li id="1a70" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">指数族、广义线性模型和标准连接函数</li></ul><h1 id="bc15" class="nt nu it bd nv nw nx ny nz oa ob oc od ki oe kj of kl og km oh ko oi kp oj ok bi translated">1.线性回归的概率解释，最大似然估计</h1><p id="34b6" class="pw-post-body-paragraph li lj it lk b ll ol kd ln lo om kg lq lr on lt lu lv oo lx ly lz op mb mc md im bi translated">这里提到线性回归的原因是为了看看我们如何将它视为数据的概率模型，以及我们是否可以将类似的想法应用于分类。</p><p id="0f96" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们假设我们的目标变量 y 和输入 x 通过(上标 I 是数据点的索引)相关</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/1ceaef54464a42230dd1815f69fc1002.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/0*lPgkXJ2mVe3efF_a"/></div></figure><p id="6331" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中，ε是一个误差项，表示未建模效应或随机噪声。我们假设噪声来自不同的源并且不相关，所以基于中心极限定理它应该是高斯的。我们可以写出分布并将误差表示为目标值和线性预测值之间的差值，</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi or"><img src="../Images/a7bf7ae5726053c3c63419ff5a85c6ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/0*sLIofqR_A3ZJaDAh"/></div></figure><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi os"><img src="../Images/2a77f0c52b3a5d27f326e69d7655347c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/0*xOjes1q7wv6CMuV2"/></div></figure><p id="4c13" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们称之为<em class="nc">给定 x 的 y 的分布，以θ </em>为参数。我们不以θ为条件，因为它不是随机变量，而是要学习的参数。接下来，我们将可能性定义为</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/1c0c8b74ded3e18d2cab595a22213ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/0*fF1j8spKpTw-ItBQ"/></div></figure><p id="79d7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">可能性是θ的函数。当被视为 y 和 X 的函数，且θ固定时，它只是概率密度函数。但当被视为θ的函数时，这意味着通过改变θ，我们可以将分布“拟合”到观察到的数据。寻找最佳拟合的过程称为最大似然估计(MLE)。换句话说，MLE 是在假设分布类型(在这种情况下是高斯分布)和参数(在这种情况下是θ，请注意，我们只关心平均值，而不关心方差/协方差矩阵)的情况下，试图找到使观察数据的概率最大化的分布。因为我们假设了独立的观察值，所以我们进一步以下面的形式将它写成单个数据点的乘积。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/06349ac71228173b56150bef7dbbd8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/0*ctTxBpq5qM4t7aiI"/></div></figure><p id="41c3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于对数变换是单调的，我们使用下面的对数似然来优化 MLE。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/5f73d0f4d026f522c2ef59c0f44a3e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/0*-GBpModqrVHShobo"/></div></figure><p id="9f9b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了找到描述生成数据的真实基础模型的最佳高斯，换句话说，就是最佳θ，我们需要找到给出最大对数似然的峰值。最大化上面的表达式等价于最小化下面的项，</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/d434048356908434b4f342f45a4eaf16.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/0*PA2efPEMqtmd2CVh"/></div></figure><p id="cb30" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">现在我们看到了神奇之处:这正是最小二乘法！</strong></p><p id="27a4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">简而言之，<em class="nc">为什么线性回归使用最小二乘法拟合数据？</em></p><blockquote class="mn"><p id="4089" class="mo mp it bd mq mr ms mt mu mv mw md dk translated"><em class="kr">因为它试图以线性预测器加高斯噪声项的形式找到最佳模型，使从中提取数据的概率最大化</em>。</p></blockquote><p id="9ffd" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">线性回归的概率公式不仅为我们后来的逻辑回归公式提供了一个鼓舞人心的例子，而且它还展示了模型设计的合理解释。我们将带有高斯噪声的线性预测器映射到目标变量。对于二元分类，如果我们可以做一些类似的事情，那就太好了，即，将带有<em class="nc">某物</em>的线性预测器映射到属于两个类别之一的概率(后验 p(y=1|x))，并使用 MLE 来证明模型设计，即它最大化了从我们的参数化分布中提取观察数据的概率。我将在第 3 节展示如何做到这一点，但接下来，让我们看一个激励人心的例子。</p><h1 id="6b0b" class="nt nu it bd nv nw nx ny nz oa ob oc od ki oe kj of kl og km oh ko oi kp oj ok bi translated">2.一个令人振奋的例子:高斯判别分析</h1><p id="851d" class="pw-post-body-paragraph li lj it lk b ll ol kd ln lo om kg lq lr on lt lu lv oo lx ly lz op mb mc md im bi translated">让我们考虑对 1D 数据的二进制分类任务，其中我们已经知道两个类的基本生成分布:具有相同方差 1 和不同均值 3 和 5 的高斯分布。两个高斯都有 50k 个数据点，即相同的先验，p(C0) = 0.5 = p(C1)。(Ck 代表 y 的类)</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/3573757f36f3cb49cae7f0614efc31cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/0*Vmhn48lWhWeeA3Kj"/></div></figure><p id="3ac8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于我们在数据中只有一维，我们能做的最好的事情就是在某个地方画一个垂直的边界，尽可能的把两个类分开。视觉上很明显，边界应该在 4 左右。使用一种<em class="nc">生成</em>方法，其中我们知道类条件 p(X|Ck)，这是两个高斯函数，以及先验 p(Ck)，我们可以<strong class="lk jd">使用贝叶斯规则得到后验</strong></p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/ad550cb6222bcd247d250691182e0c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/0*OdlGaNaAbv3YB9xm"/></div></figure><p id="cbe5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果绘制如下</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/b8ac885c69b8b8392fabb96275d3c30e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/0*TEGFaIhJrXKZBYn2"/></div></figure><p id="b46b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以清楚地看到后验中的边界，即我们算法的最终概率预测。红色区域被分类为 0 级，蓝色区域为 1 级。<em class="nc">这种方法是一种生成模型，称为高斯判别分析(GDA) </em>。它模拟连续的特征。你可能听说过它的兄弟离散特征:<em class="nc">朴素贝叶斯分类器</em>。</p><p id="a224" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在看看 S 形的后验边界周围，它描述了两个类之间的不确定性的过渡。</p><blockquote class="mn"><p id="fa5f" class="mo mp it bd mq mr ms mt mu mv mw md dk translated"><strong class="ak">如果我们能够<em class="kr">在事先不知道类条件的情况下直接建模这个 S 形状，岂不是很酷</em>？</strong></p></blockquote><p id="0e79" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">但是怎么做呢？让我们解决一些数学问题。</p><p id="6ea8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，红色和蓝色曲线是对称的，它们的总和总是 1，因为它们在贝叶斯定理中是归一化的。让我们看看红色的。它就是 p(C0|X ),它是 X 的函数。我们通过将顶部和底部分成以下形式，对前面的等式进行了一点修改，</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/22e320f247e8c3d3e193e8e3a5b5acb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/0*VldxWe2KfY035bSj"/></div></figure><p id="9711" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于右下角的项，我们可以取消先验，因为它们是相等的，并插入高斯条件句。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/d66efccf925f97cee2e7dff96900beb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/0*lCYfvHelOX7LjQ75"/></div></figure><p id="996a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好吧，这很好！我们在 exp()里面有一个 x 的线性函数，如果我们设 z = -2x + 8，写出它的后验，就变成了，</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/024f2af6dbcb1ead0b29fd284f8c9326.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/0*D9i-urWfQ_WxP43R"/></div></figure><p id="7c86" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">这就是逻辑 s 形函数！</strong>如果你问为什么 z 有负号，那是因为为了方便起见，我们希望 p 和 z 在相同方向上是单调的，这意味着增加 z 将增加 p。这种情况的逆过程被称为<em class="nc">对数几率</em>或<em class="nc"> logit </em>，这是我们可以使用线性函数建模的部分。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/83c88209f5b78f62ddef98b86dd233ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/0*5MxXOWvHH1Tv1KOm"/></div></figure><p id="5fec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">回头看看上面的逻辑流程，<strong class="lk jd">到底发生了什么，使得有一个 sigmoid 形式的后验概率和 x 对 z 的线性函数</strong>成为可能？这将为我们提供一些见解，以决定<em class="nc">何时</em>我们可以这样对分类建模。</p><p id="421e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">对于 sigmoid 形式，您可以看到它自然地来自两个类别的 Bayes 规则，即目标变量的 Bernoulli 分布</strong>。它不要求类条件句是高斯型的！可能有一族分布具有相似的指数形式，符合我们上面的推导！只要输出 y 是二进制的，输入 X 在它们的类条件分布中就可以有一定的灵活性。</p><p id="ac4d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，z 的线性形式。在本例中，我们有两个高斯函数，方差<em class="nc">相同，先验为</em>。这些事实让我们在推导中，抵消了先验和 X 的二次项。这个要求看起来挺严格的。事实上，如果我们改变高斯函数的形状，决策边界就不再是一条直线。考虑下面 2D 的例子。如果两个高斯函数具有相同的协方差矩阵，则判决边界是线性的；在第二张图中，它们具有不同的协方差矩阵，判定边界是抛物线。</p><div class="kt ku kv kw gt ab cb"><figure class="pd kx pe pf pg ph pi paragraph-image"><img src="../Images/adc1ba4882cce28f26c2d8b58958d12c.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*DfIHlD9J5V81fdLgGwjdKA.png"/></figure><figure class="pd kx pe pf pg ph pi paragraph-image"><img src="../Images/83d9a199628c85cb1a842e963c4c6a6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*hEbBYAXJurTLKV1SbiZEcQ.png"/><p class="le lf gj gh gi lg lh bd b be z dk pj di pk pl translated">来源:K. Murphy，机器学习:概率观点</p></figure></div><p id="f8f4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">这告诉我们，如果我们使用 sigmoid 函数和线性边界直接对后验模型进行建模(<em class="nc">判别</em>方法),这也被称为<em class="nc">逻辑回归</em>,与 GDA 的<em class="nc">生成</em>方法相比，它有一些优点和缺点。</strong></p><ul class=""><li id="b521" class="nf ng it lk b ll lm lo lp lr nh lv ni lz nj md nk nl nm nn bi translated">GDA 具有比逻辑回归强得多的假设，但是<strong class="lk jd">当高斯假设为真时，它比逻辑回归</strong>需要更少的训练数据来实现类似的性能。</li><li id="19c4" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">但是<strong class="lk jd">如果类条件句的假设不正确，逻辑回归做得更好</strong>，因为它不需要对特征的分布建模。</li></ul><p id="e5e4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在凯文·墨菲的<em class="nc">机器学习:概率观点</em>的第 8.6.1 节中，对 GDA 和逻辑回归进行了广泛的比较。我在这里讨论 GDA 只是为了说明这一点</p><blockquote class="mn"><p id="196e" class="mo mp it bd mq mr ms mt mu mv mw md dk translated">当我们试图用一些假设对一个伯努利目标变量建模时，sigmoid 函数会自然出现。</p></blockquote><h1 id="c273" class="nt nu it bd nv nw nx ny nz oa ob oc od ki pm kj of kl pn km oh ko po kp oj ok bi translated">3.逻辑回归的潜在变量公式</h1><p id="80cd" class="pw-post-body-paragraph li lj it lk b ll ol kd ln lo om kg lq lr on lt lu lv oo lx ly lz op mb mc md im bi translated">现在回到第一点。我们通过定义带有高斯噪声项的线性预测因子来设计线性回归。我们能在二进制分类的情况下做一些类似的事情吗？是的，我们可以！让我们这样来看，</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/5e51ff27e3f1bccf6f1c31dc2a9a0d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/0*nHrqcp1lmdM2fDIM"/></div></figure><p id="0fa8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">线性预测值加上此处的误差评估为我们所谓的<em class="nc">潜在变量</em>，因为它是不可观测的，并且是根据观测变量 x 计算的。二元结果由潜在变量是否超过阈值(本例中为 0)决定。(注意，为了方便后面的累积分布解释，判定阈值被设置为 0，而不是通常的 0.5。从数学上讲，这里的值是 0 还是 0.5 并不重要，因为线性预测器可以更新偏差项进行补偿。)</p><blockquote class="mn"><p id="1273" class="mo mp it bd mq mr ms mt mu mv mw md dk translated">如果我们假设误差项有一个<a class="ae pq" href="https://en.wikipedia.org/wiki/Logistic_distribution" rel="noopener ugc nofollow" target="_blank"> <em class="kr">逻辑分布</em> </a>，<em class="kr">，其累积分布是逻辑 sigmoid 函数</em>(如下并排所示)，那么我们就得到逻辑回归模型！</p></blockquote><div class="pr ps pt pu pv ab cb"><figure class="pd kx pw pf pg ph pi paragraph-image"><img src="../Images/a0c7600536474eb2dbebfe48315144be.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*2rhQbptAlqKmTEWd6eT1AA.png"/></figure><figure class="pd kx px pf pg ph pi paragraph-image"><img src="../Images/0215a15b746ba9bbe236e22caf57ab05.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*nv8VwYq08b4eHtHOliglLQ.png"/><p class="le lf gj gh gi lg lh bd b be z dk py di pz pl translated">来源:<a class="ae pq" href="https://en.wikipedia.org/wiki/Logistic_distribution" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure></div><p id="9d01" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将潜在随机变量表示为 Y*，线性预测值表示为 z，累积分布表示为 F，则观察结果 y = 1 的概率为，</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/ae7d42a31df9a00b7dfe3c7adea41b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/0*eoHGEL76dk7RdBaz"/></div></figure><p id="250e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们让 F 成为 sigmoid 函数，所以它在 0 左右对称，</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/94feff59dd5b1dd92f81f152917398c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/0*qAc23E8fmN2fUpzn"/></div></figure><p id="fada" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所以我们可以写作，</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/df9c30bed6e1fcea6d3fe4365236ecb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/0*rsx1Lzb_DzGeSCwJ"/></div></figure><p id="1a3b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">现在我们达到了目标，伯努利结果的概率表示为线性预测值的 sigmoid！</strong></p><p id="67d4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面给了我们线性预测器 z 和预测 p 之间的关系，函数 F，或者机器学习上下文中的<em class="nc">激活函数</em>就是逻辑 sigmoid。激活函数的逆函数称为<em class="nc">链接函数</em>，它将预测映射回 z。它是逻辑回归中的<em class="nc"> logit </em>。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi qd"><img src="../Images/992b9e4efb433e82d388264880df38fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/0*TAqXGAvGbUv6rtWk"/></div></div></figure><p id="c620" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">概括地说，推导基本上是说<strong class="lk jd">如果我们假设误差项具有逻辑分布，我们的伯努利结果的概率是线性预测值的 sigmoid。</strong></p><p id="79c1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你仔细观察推导，<strong class="lk jd">这个公式并不需要一个逻辑分布来工作</strong>。<strong class="lk jd">只需要在 0 附近对称分布。什么是合理的替代方案？一个高斯！</strong></p><blockquote class="mn"><p id="fc5b" class="mo mp it bd mq mr ms mt mu mv mw md dk translated">如果我们假设误差是高斯型的呢？</p></blockquote><p id="a03d" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">它实际上给了我们另一个与逻辑回归相似的模型，也能完成这项工作。它被称为<strong class="lk jd">概率单位回归</strong>。</p><h1 id="5844" class="nt nu it bd nv nw nx ny nz oa ob oc od ki oe kj of kl og km oh ko oi kp oj ok bi translated">4.另一种选择:概率单位模型</h1><p id="4b4c" class="pw-post-body-paragraph li lj it lk b ll ol kd ln lo om kg lq lr on lt lu lv oo lx ly lz op mb mc md im bi translated">与设计用于解决相同任务的替代模型进行比较是深入了解我们的主题:逻辑回归及其假设的一个很好的方法。</p><p id="9fc8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如前一节所述，二元分类的概率单位模型可以用相同的潜在变量公式表示，但存在高斯误差。你可能想知道为什么它不像逻辑回归那样被广泛使用，因为假设高斯误差似乎更自然。一个原因是高斯分布不具有封闭形式的 CDF，并且其导数在训练期间更难计算。逻辑分布的形状与高斯分布非常相似，但其 CDF(也称为逻辑 sigmoid)具有封闭形式且易于计算的导数。</p><p id="e262" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们来看看推导过程</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/1def476c200f1ef6281bb7dfa9bdc350.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/0*wjwfSHpJxtLn2RmN"/></div></figure><p id="515a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">φ是高斯的 CDF。注意，我们除以σ得到一个标准正态变量，并利用对称性得到最后的结果。这说明我们不能分别识别θ和σ，因为 p 只取决于它们的比值。这意味着潜在变量的规模没有确定。因此，我们设置σ = 1，并以潜在变量的标准差为单位解释θ。</p><p id="1380" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上述推导与逻辑回归的唯一区别在于<strong class="lk jd">激活函数被设置为高斯 CDF，而不是逻辑 sigmoid </strong>，即逻辑分布的 CDF。高斯 CDF 的逆函数被称为<em class="nc">概率单位</em>，它在这里被用作<em class="nc">链接函数</em>。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/0bcda8074ca5d0404949cb2b6a04cca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/0*nUpnJtFcHex_TLOz"/></div></figure><p id="aa01" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">概率单位回归作为一种惯例更多地用于生物和社会科学。它通常产生与逻辑回归相似的结果，并且更难计算。如果你不是这方面的专业统计学家，逻辑回归是首选模型。</p><p id="d8db" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">还有另一个叫做<em class="nc">互补双对数</em>的链接函数，可以用于伯努利响应，我不会在这里详细介绍，但如果你有兴趣，可以阅读一下。</p><h1 id="5d45" class="nt nu it bd nv nw nx ny nz oa ob oc od ki oe kj of kl og km oh ko oi kp oj ok bi translated">5.指数族、广义线性模型和标准连接函数</h1><p id="25e4" class="pw-post-body-paragraph li lj it lk b ll ol kd ln lo om kg lq lr on lt lu lv oo lx ly lz op mb mc md im bi translated">到目前为止，我们已经看到了线性回归、逻辑回归和概率回归。它们的主要区别之一是<em class="nc">链接功能</em>。如果我们把它抽象出来，并做一些额外的假设，我们可以定义一个更广泛的模型类别，称为广义线性模型。</p><p id="2bdf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">GLM 对 p(y|x)的期望值进行建模，即μ= E[y | x；θ].<strong class="lk jd">对于线性回归，μ只是线性预测器，换句话说，它的连接函数就是恒等函数。</strong>但对于其他情况，p(y|x)可以是指数形式或其他形式，如果我们仍然想以某种方式使用线性预测器，我们必须将其转换以匹配输出。</p><p id="b119" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了跳到 GLM，我们首先利用一个很好的数学形式，将一些最广泛使用的分布组合在一起，这样我们就可以研究它们的共享属性。我们可以查看如下所示的共享表单，而不是查看每个带有各自参数的发行版，</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi qg"><img src="../Images/7dde5e2782964ae380e6f43e66d2f155.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/0*Vi4VYVhzH80JnWvT"/></div></div></figure><p id="3db2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">可以调整成这种形式的分布称为<strong class="lk jd">指数族</strong>(注意，它与<em class="nc">指数分布</em>不同)。这里，y 是我们试图预测的目标响应变量。统计学家为这些术语起了一些别出心裁的名字。<strong class="lk jd">但我在这里关注的是η项，也称为<em class="nc">自然参数</em>。</strong>出于我们的目的，我们可以假设 T(y)(称为<em class="nc">充分统计量</em>)只是 y。因此，自然参数η只是将 exp()中的结果 y 映射到左边的概率。让我们用一个具体的例子来说明我的意思。</p><p id="d458" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">对于均值为μ的伯努利目标变量</strong>，我们可以写为</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/4c3bc2016cbc2e379ef10bc3d61cd793.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/0*JHzXN6ErolwA2Z6s"/></div></figure><p id="2061" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">自然参数η原来是 logit！</strong></p><blockquote class="mn"><p id="b8aa" class="mo mp it bd mq mr ms mt mu mv mw md dk translated">由于这种指数族公式，logit 也被称为伯努利分布的<strong class="ak">规范链接函数</strong>。</p></blockquote><p id="12e3" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">正如我们之前看到的，概率单位也是一个链接函数，但它不是<em class="nc">规范的</em>，因为它不属于这里的指数族设置。</p><p id="966c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们有能力跨越到 GLM。利用指数族及其自然参数，我们可以根据结果 y 的分布为我们的线性预测器定义一个<strong class="lk jd">规范链接函数</strong>。<strong class="lk jd">在伯努利结果的情况下，这种方法为我们提供了 logit 链接和逻辑回归。</strong></p><p id="df08" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">指数族给了我们很多好的性质。表明<strong class="lk jd">它们的对数似然总是凹的(等价地，负对数似然总是凸的)，并且它们的基于梯度的优化共享相同的形式，因此我们总是可以使用一些迭代算法来找到最佳拟合</strong>。</p><p id="0fbc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了伯努利分布，其他一些著名的指数分布包括高斯分布、泊松分布、伽玛分布、指数分布、贝塔分布和狄利克雷分布。</p><p id="7154" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要为机器学习任务选择 GLM，请考虑目标变量 y 的类型。例如，</p><ul class=""><li id="2164" class="nf ng it lk b ll lm lo lp lr nh lv ni lz nj md nk nl nm nn bi translated">如果 y 是一个实数值，使用高斯(最小二乘回归)</li><li id="368f" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">如果是二元的，用伯努利(逻辑回归)</li><li id="a2a2" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">如果是计数，使用泊松(泊松回归)</li></ul><p id="3243" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">等等。</p><h1 id="4c6b" class="nt nu it bd nv nw nx ny nz oa ob oc od ki oe kj of kl og km oh ko oi kp oj ok bi translated">摘要</h1><p id="5ac5" class="pw-post-body-paragraph li lj it lk b ll ol kd ln lo om kg lq lr on lt lu lv oo lx ly lz op mb mc md im bi translated">在介绍性的课程和书籍中，解决方案经常在没有充分理由的情况下强加给读者。从许多不同的资源中寻找线索并理解它们并不容易。希望这篇文章能为有疑问的人提供一个比较全面和直观的答案。学习的目标不仅仅是知道<em class="nc">如何</em>，还要知道<em class="nc">为什么</em>，这样我们才能在实际应用中概括我们的学习。</p><p id="e977" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个话题引出了广义线性模型这个更广泛的话题。glm 是一类强大的模型，没有像深度学习一样受到关注。在许多情况下，正确应用 GLMs 可以完成工作，同时使您的生活更加轻松。与深度学习技术相比，GLM 的优势在于数学的简单性和经过充分研究的可解释性。对基础理论的深刻理解也可以帮助机器学习研究人员和实践者开发新的方法。如果你有兴趣进一步探讨这个话题，我推荐 Philippe Rigollet 的麻省理工学院 18.650 应用统计学讲座和我的参考资料。坚持学习！</p></div><div class="ab cl qi qj hx qk" role="separator"><span class="ql bw bk qm qn qo"/><span class="ql bw bk qm qn qo"/><span class="ql bw bk qm qn"/></div><div class="im in io ip iq"><p id="4b6f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nc">在</em> <a class="ae pq" href="https://medium.com/@loganyang" rel="noopener"> <em class="nc">中</em> </a> <em class="nc">上看我的其他帖子，或者在</em> <a class="ae pq" href="https://twitter.com/logancyang" rel="noopener ugc nofollow" target="_blank"> <em class="nc">上关注我的【推特】</em> </a> <em class="nc">。</em></p><h1 id="b788" class="nt nu it bd nv nw nx ny nz oa ob oc od ki oe kj of kl og km oh ko oi kp oj ok bi translated">参考</h1><ul class=""><li id="2ce9" class="nf ng it lk b ll ol lo om lr qp lv qq lz qr md nk nl nm nn bi translated">主教。模式识别和机器学习。斯普林格，2006 年。第四章。</li><li id="b82b" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">K. P .墨菲。机器学习:概率观点。马萨诸塞州剑桥。[美国]:麻省理工学院出版社，2013 年。</li><li id="6a32" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">吴恩达的机器学习讲座和笔记，第一至第三部分。</li><li id="1a38" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">罗德里格斯(2007 年)。广义线性模型讲义。网址:<a class="ae pq" href="http://data.princeton.edu/wws509/notes/" rel="noopener ugc nofollow" target="_blank"><em class="nc">http://data.princeton.edu/wws509/notes/</em></a></li><li id="0b9e" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated"><em class="nc">麻省理工学院 18.650 统计学应用讲座菲利普·里戈莱特</em><a class="ae pq" href="https://www.youtube.com/watch?v=X-ix97pw0xY" rel="noopener ugc nofollow" target="_blank"><em class="nc">https://www.youtube.com/watch?v=X-ix97pw0xY</em></a></li></ul></div></div>    
</body>
</html>