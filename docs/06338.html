<html>
<head>
<title>A complete guide to transfer learning from English to other Languages using Sentence Embeddings BERT Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用句子嵌入BERT模型将学习从英语转移到其他语言的完整指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-complete-guide-to-transfer-learning-from-english-to-other-languages-using-sentence-embeddings-8c427f8804a9?source=collection_archive---------13-----------------------#2020-05-21">https://towardsdatascience.com/a-complete-guide-to-transfer-learning-from-english-to-other-languages-using-sentence-embeddings-8c427f8804a9?source=collection_archive---------13-----------------------#2020-05-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/c6095a7a1981f993e2a12425d5656b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*saanK3QVGnNzStY_asR2qA.png"/></div></div></figure><div class=""/><div class=""><h2 id="6c1c" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">这个故事旨在向机器学习实践者提供关于句子嵌入BERT模型的细节。</h2></div><h1 id="37ce" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">用连体伯特网络嵌入句子的思想📋</h1><p id="c117" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi mh translated"><span class="l mi mj mk bm ml mm mn mo mp di"> W </span> <strong class="ln jf"> ord Vector </strong>使用<strong class="ln jf">神经向量表示法</strong>在自然语言处理(NLP)的所有子领域中已经变得无处不在，这显然是很多人熟悉的领域[1]。在这些技术中，<strong class="ln jf">句子嵌入</strong>思想具有各种应用潜力，它试图将一个句子或短文本段落编码成一个固定长度的向量(密集向量空间)，然后使用该向量来评估它们的余弦相似度如何反映人类对语义相关度的判断[2]。</p><p id="c19c" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated"><strong class="ln jf">句子嵌入</strong>在自然语言处理中有着广泛的应用，例如信息检索、聚类、自动文章评分和语义文本相似性。到目前为止，比较流行的生成定长句子嵌入的方法论有:<strong class="ln jf">连续词包</strong> (CBOW) <strong class="ln jf">序列到序列模型</strong> (seq2seq)和<strong class="ln jf">来自变形金刚的双向编码器表示</strong> (BERT)。</p><ul class=""><li id="b5e1" class="mv mw je ln b lo mq lr mr lu mx ly my mc mz mg na nb nc nd bi translated"><strong class="ln jf">CBOW</strong>【3】是一种极其幼稚的方法，其中一个简单的求和(或平均)的单词嵌入由<em class="ne"> word2vec算法</em>编码来生成句子嵌入。老实说，这是一种在句子中捕捉信息的糟糕方式，因为它完全不考虑单词的顺序。</li></ul><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/ec6661ace574eb0eb2898fb11dbd8112.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l_-bavh3OIQCaJdXoDY7Wg.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated"><strong class="bd no"> <em class="np">同样CBOW嵌入</em> </strong> <em class="np">，尽管它们的</em> <strong class="bd no"> <em class="np">含义与</em> </strong>相反(<em class="np">图片由作者</em>)</p></figure><ul class=""><li id="67cf" class="mv mw je ln b lo mq lr mr lu mx ly my mc mz mg na nb nc nd bi translated"><strong class="ln jf"> Seq2seq模型</strong> [4]如RNNs、LSTMs、GRUs等..另一方面，获取一个项目序列(单词、字母、时间序列等)，并通过逐个处理单词嵌入来输出另一个项目序列，同时保持存储上下文信息的隐藏状态。由<em class="ne">编码器</em>产生的嵌入以<em class="ne">隐藏状态向量</em>的形式捕获输入序列的<em class="ne">上下文</em>，并将其发送到<em class="ne">解码器</em>，解码器然后产生执行一些其他任务的输出序列。这种方法能够根据句子中的单词顺序进行区分，因此可能比<strong class="ln jf"> CBOW </strong>提供更丰富的嵌入。</li></ul><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/6f36fc0d2e98602de6695db833cb6eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XyKZWWVBMSgZAc0G7vFHmg.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">Seq2seq注意(<a class="ae nr" href="https://arxiv.org/abs/1807.03006" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><ul class=""><li id="ed7d" class="mv mw je ln b lo mq lr mr lu mx ly my mc mz mg na nb nc nd bi translated"><strong class="ln jf"> BERT </strong> <strong class="ln jf">模型</strong>【5】在各种句子分类、句子对回归以及语义文本相似性任务上实现了最先进的性能。<strong class="ln jf"> BERT </strong>使用交叉编码器网络，将两个句子作为变压器网络的输入，然后预测目标值。然而，在一个由大量句子组成的语料库中，寻找相似的句子对需要大量的推理计算和时间(即使使用GPU)。成对句子推理在计算上是相当昂贵的，并且与<code class="fe ns nt nu nv b">O(<em class="ne">n.(n-1</em>)/2)</code>成比例。例如，对于1000个句子，我们需要<code class="fe ns nt nu nv b">1000.(1000 – 1)/2 = 499500</code>推理计算。具体来说，如果我们想要使用与<strong class="ln jf"> BERT </strong>的成对比较技术在<em class="ne"> Quora </em>上的4000万个问题中过滤出最相似的问题，查询将需要50多个小时才能完成。下图解释了如何使用BERT进行句子对分类。</li></ul><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/e16b1b56ed212ba3d9976bed2bdf3df0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SjryN9Yz-S675M_I.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">用BERT进行句子对分类(<a class="ae nr" href="https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="b151" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">为了克服这个问题，我们可以借鉴计算机视觉研究人员的想法，使用<strong class="ln jf">连体</strong>和<strong class="ln jf">三元组网络结构</strong>来导出固定大小的句子嵌入向量，然后使用类似余弦相似度或曼哈顿/欧几里德距离的相似性度量来计算语义相似的句子[6]。这个解决方案是由来自泛在知识处理实验室(UKP-TUDA)Nils Reimers和Iryna Gurevych提出的，它被称为<strong class="ln jf">句子-BERT </strong> (SBERT)。通过使用优化的索引结构，模型求解上述Quora 示例所需的运行时间可以从50小时减少到几毫秒！！！</p><p id="8faf" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">SBERT的体系结构简单到足以说明。首先，原始句子对通过BERT / RoBERTa来嵌入固定大小的句子。然后在池层中，<em class="ne">平均聚合，</em>被证明与<em class="ne">最大值</em>或<em class="ne"> CLS聚合相比具有最佳性能，</em>被用于生成<em class="ne"> u </em>和<em class="ne"> v </em></p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/017de749ea50342f6a5e1f231aa718a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hl0A1_ovV_T41BO9FogBEw.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated"><strong class="bd no"> SBERT </strong>具有分类目标函数的架构(<em class="np">图片由作者</em>提供)</p></figure><h1 id="6a05" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated"><strong class="ak">分类目标函数</strong></h1><p id="d0ce" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">然后我们<em class="ne">连接</em>嵌入如下:(u，v，‖u-v‖)，乘以可训练的权重矩阵W∈ℝ ᴺ ˣ ᴷ，其中n是句子嵌入维数，k是标签的数量。我们优化交叉熵损失。</p><p id="d431" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated"><strong class="ln jf"> Softmax(Wt(u，v，| u v |)</strong></p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/aa6267a5237491ae6f9097cceeb856af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l3XwRQFwLwnPFQQMh9XUOw.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated"><strong class="bd no"> SBERT </strong>带回归目标函数的架构<strong class="bd no"> <em class="np"> </em> </strong> ( <em class="np">图片作者</em>)</p></figure><h1 id="e631" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">回归目标函数</h1><p id="f43c" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">在回归任务的情况下，我们计算句子嵌入和各个句子对的余弦相似性。我们使用均方误差损失作为目标函数。</p><p id="6e3b" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated"><strong class="ln jf"> MSE(Wt(u，v，余弦- sim(u，v)) </strong></p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/2da2d666fb466706cbd57c69276f7707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p5Y7fv76V9oY5jjs4pnpkg.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated"><strong class="bd no"> SBERT </strong>具有三个目标函数的架构(<em class="np">图片由作者</em>提供)</p></figure><h1 id="d08f" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">三元目标函数</h1><p id="8bbb" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">给定一个锚定句A，一个肯定句P，一个否定句N，数学上。我们最小化P距离度量和N距离度量的损失函数。保证金ɛ确保p至少是ɛ更接近a比n</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="bbd5" class="kt ku je bd kv kw og ky kz la oh lc ld kk oi kl lf kn oj ko lh kq ok kr lj lk bi translated">迁移学习句子嵌入📊</h1><p id="b2b5" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">到目前为止，我们可以看到SBERT可以用于信息检索，聚类，自动短文评分，以及语义文本相似性，具有不可思议的时间和高准确性。然而，SBERT的局限性在于它目前只支持英语，而对其他语言则保持空白。为了解决这个问题，我们可以使用类似于<strong class="ln jf">连体</strong>和<strong class="ln jf">三联体网络结构</strong>的模型架构来扩展SBERT到新语言【7】。</p><p id="dc94" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">这个想法很简单，首先我们用SBERT产生英语句子中的句子嵌入，我们称之为<strong class="ln jf">教师模型</strong>。然后我们为我们想要的语言创建新的模型，我们称之为<strong class="ln jf">学生模型，</strong>，这个模型试图模仿<strong class="ln jf">教师模型</strong>。换句话说，原始英语句子将在<strong class="ln jf">学生模型</strong>中训练，以便得到与<strong class="ln jf">教师模型中的向量相同的向量。</strong></p><p id="12a3" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">如下例所示,“Hello World”和“Hallo Welt”都是通过<strong class="ln jf">学生模型</strong>输入的，该模型试图生成两个与<strong class="ln jf">教师模型中的向量相似的向量。</strong>经过培训后，<strong class="ln jf">学生模型</strong>应具备用英语和所需语言对句子进行编码的能力。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/b00e7b2c8ea5efa0e22b34fe7df887ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AhyLwLD_TxkhvYVY.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">英语到德语的迁移学习示例结构(<a class="ae nr" href="https://arxiv.org/pdf/2004.09813.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="5cd5" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">让我们从零开始，用一个例子把SBERT英语转换成日语。</p><p id="976f" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">首先，我们需要安装SBERT和MeCab包(将日语句子解析为含义词的重要包)。</p><pre class="ng nh ni nj gt om nv on oo aw op bi"><span id="0103" class="oq ku je nv b gy or os l ot ou">!pip install -U sentence-transformers<br/>!pip install mecab-python3</span></pre><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/dc5a115a58a783258373bade8b6d9e83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xySvTCVkrZBDJf5PGSNz-A.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">MeCab解析日语句子后的示例</p></figure><p id="9bee" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">然后，一些人类智能需要为翻译数据集以及英语和日语的语义文本相似度数据集准备几对句子。在预处理日语句子后，我们将得到如下所示的数据</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/450a32492ca8eb55a0b61276c6cd6b2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cPU_VmB3Vabs_0Jp2L8FGA.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">EN-JA的翻译数据集</p></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/70724dcaaed3e6767305ef7457130334.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rm8cQnzyJKcy16xi6y9L3A.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">面向EN-JA的语义文本相似度数据集</p></figure><p id="cf52" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">我使用XLM-罗伯塔创建单词嵌入作为<strong class="ln jf">学生模型</strong>(当然，如果你愿意，你可以尝试其他BERT预训练模型，即<strong class="ln jf"> mBERT </strong>)，来自SentenceTransformer的“bert-base-nli-stsb-mean-tokens”作为<strong class="ln jf">教师模型</strong>和<em class="ne">均值聚合</em>作为池层。其他参数是max_seq_length = 128和train_batch_size = 64(如果超出了内存限制，可以将batch_size减少到32或16)。</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="ebef" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">在创建了<strong class="ln jf">教师模型</strong>和<strong class="ln jf">学生模型</strong>之后，我们可以开始加载训练、开发、测试数据集和训练模型。训练集和测试集是翻译数据集，而dev集是遵循迁移学习SBERT体系结构的语义文本相似度数据集。在本例中，我将在20个时期内训练模型，学习率= 2e-5，ε= 1e-6，您可以自由尝试另一个超参数，以获得您的语言的最佳结果。我还为下游应用程序保存了模型，如果您只想玩玩这个，可以通过设置save_bet_model = False来关闭它。</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="111c" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">最后，让我们享受结果。我们将在英语和日语语料库中使用相同的句子含义来评估<strong class="ln jf">学生模型</strong>。</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="oy oz l"/></div></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/ebed1bc8221e27e2d68a7c04fe44c259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PfGriqD7oxgXxyc1ksPS1Q.jpeg"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">结果很好，我们在语料库中得到了相似的句子</p></figure><p id="831e" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">让我们在日语语料库中检查<strong class="ln jf">学生模型</strong>的能力</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="oy oz l"/></div></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/0d67c766b833218d66468cdd7a1b64ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jj7mjQ4qbQ8dI79pvVZ2Lg.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">厉害！！我们的模型也可以得到日语和英语中相似的句子</p></figure></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="2c9e" class="kt ku je bd kv kw og ky kz la oh lc ld kk oi kl lf kn oj ko lh kq ok kr lj lk bi translated">最后的想法📕</h1><p id="9924" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">我们可以看到，在从英语SBERT中提取知识之后，现在我们的模型有能力从NLP的任何下游任务中嵌入新的语言句子。你可以试着用你自己的语言训练SBERT，然后让我知道结果。此外，<strong class="ln jf">学生模型</strong>能力<strong class="ln jf"> </strong>不仅限于2种语言，我们可以扩展语言的数量。准备新的语言数据集，一切准备就绪。你可以使用之前解释过的代码(现在<strong class="ln jf">学生模型</strong>将变成<strong class="ln jf">教师模型</strong>来教新<strong class="ln jf">学生</strong>😅) .然而，作为一个经验法则，准确性会随着能力的降低而降低。</p><p id="2781" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">如果你想进一步讨论，可以随时联系我。下面是我的<a class="ae nr" href="https://www.linkedin.com/in/vumichien/" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="3b8b" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">尽情享受吧！！！👦🏻</p><h1 id="fc33" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">附录</h1><p id="b861" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">如果设置save_bet_model = True，就可以很容易地用这段代码预加载模型</p><pre class="ng nh ni nj gt om nv on oo aw op bi"><span id="7972" class="oq ku je nv b gy or os l ot ou">from sentence_transformers import SentenceTransformer</span><span id="c0f4" class="oq ku je nv b gy pc os l ot ou">model = SentenceTransformer('output/model-2020-05-21/')</span></pre><h1 id="dec1" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">参考</h1><p id="2c02" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">[1] Jeffrey Pennington，Richard Socher和Christopher Manning，GloVe:单词表示的全局向量。《2014年自然语言处理经验方法会议论文集》(EMNLP 2014)，第1532–1543页，2014年。</p><p id="e58e" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">[2] Quoc Le和Tomas Mikolov，句子和文档的分布式表示。ICML 2014年会议录。PMLR，2014年。</p><p id="8a0b" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">[3]托马斯·米科洛夫、程凯、格雷戈·科拉多和杰弗里·迪恩，向量空间中词表示的有效估计，2013年。</p><p id="e527" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">[4] Ilya Sutskever，Oriol Vinyals和Quoc V. Le，用神经网络进行序列到序列学习，2014年。</p><p id="ff5a" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">[5] Jacob Devlin，Ming-Wei Chang，Kenton Lee和Kristina Toutanova，BERT:用于语言理解的深度双向转换器的预训练，2019。</p><p id="b4e4" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">[6] Nils Reimers和Iryna Gurevych，句子-BERT:使用暹罗BERT-网络的句子嵌入，2019年。</p><p id="a428" class="pw-post-body-paragraph ll lm je ln b lo mq kf lq lr mr ki lt lu ms lw lx ly mt ma mb mc mu me mf mg im bi translated">[7] Nils Reimers和Iryna Gurevych，使用知识蒸馏使单语句子嵌入多语言，2020年。</p></div></div>    
</body>
</html>