<html>
<head>
<title>Creating Deep Neural Networks from Scratch, an Introduction to Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始创建深度神经网络，强化学习导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-95bcb493a0c9?source=collection_archive---------37-----------------------#2020-04-19">https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-95bcb493a0c9?source=collection_archive---------37-----------------------#2020-04-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="760c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第三部分:反思和改进</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/88ba897d1d4611d1d25179a188031b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GcKeDIHnKxKv0Qwm"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@bantersnaps?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> bantersnaps </a>拍摄的照片</p></figure><blockquote class="kw kx ky"><p id="42fa" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这是一系列文章中的第三篇也是最后一篇，旨在全面介绍OpenAI gym上的<a class="ae kv" href="https://gym.openai.com/envs/CartPole-v1/" rel="noopener ugc nofollow" target="_blank">侧翻问题</a>的解决方案，该解决方案是在没有使用Pytorch或Tensorflow等标准机器学习框架的情况下从零开始构建的。完整的代码可以在<a class="ae kv" href="https://github.com/abhavk/dqn_from_scratch" rel="noopener ugc nofollow" target="_blank">这里</a>找到。<br/> <a class="ae kv" rel="noopener" target="_blank" href="/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2">第一部</a>奠定了基础。在其中，我们讨论了神经网络体系结构，并实现了前向传播来计算代理动作的值。</p><p id="23cd" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db">第二部分</a>深入研究了强化学习理论的细节，形式化了Q值和DQN的概念。我们还在第二部分中实现了反向传播。</p><p id="ed7f" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">第三部分将包含一些不同配置的代理性能的可视化和反映。这最后一部分还将完成实现并添加增强功能，如Adam优化器。在这里，我们不太关注超参数选择背后的严格性，而是更多地探索可以为模型改进而调整的配置。</p></blockquote><p id="b6f8" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">在上一节的最后，我们完成了Cartpole代理的实现。时间来看看结果和代理的表现随着时间的推移！</p><h1 id="6d06" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">跟踪代理的改进</h1><p id="b05e" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">让我们进行一次完整的训练，从随机初始化开始跟随代理，直到它学会平衡极点的艺术。这次训练用了141集来实现它的目标(连续100集的平均分数为195)。</p><p id="ae4e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">首先，这是分数的图表—</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/719a102fe182dff907fbf4b8d4e08f5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*CsitS0DO3IwsCnAZst2NHg.png"/></div></figure><p id="eed7" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">现在让我们看看代理在培训过程中的3个不同阶段的表现。代理的前5次运行非常糟糕，</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最初几轮</p></figure><p id="ee4e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">在培训的中途，我们可以看到代理取得了进步，尽管仍有改进的空间。这是第75和76集，</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练中途</p></figure><p id="647c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">最后，在训练接近尾声时，代理人能够几乎完美地平衡杆子。这是第138轮，</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最终受训代理人</p></figure><p id="a357" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我们可以看到代理到最后还是挺不错的！</p><h1 id="9b39" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">目标网络</h1><p id="bb30" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">在<a class="ae kv" rel="noopener" target="_blank" href="/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db">第二部分</a>关于<strong class="lc ir">累积奖励和行动值</strong>的部分，我们讨论了如何使用DQN的完整实施的简化版本，通过使用相同的权重来计算预测的行动值和目标值。相反，我们需要在计算目标动作值(<em class="lb">实验_值</em>在RLAgent中)时有一个固定的权重网络。<em class="lb">经验_回放</em>方法)。让我们开始实施吧。首先，我们在NNLayer <em class="lb">中添加了<em class="lb"> stored_weights </em>参数的初始化。初始化</em>函数，</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="4251" class="ne ma iq na b gy nf ng l nh ni">def __init__(self, input_size, output_size, activation=None, lr = 0.001):<br/>        self.input_size = input_size<br/>        self.output_size = output_size<br/>        self.weights = np.random.uniform(low=-0.5, high=0.5, size=(input_size, output_size))<br/>        <strong class="na ir">self.stored_weights = np.copy(self.weights)</strong><br/>        self.activation_function = activation<br/>        self.lr = lr</span></pre><p id="b66e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">记住参数<em class="lb">remember _ for _ back prop</em>= False中传入的<em class="lb"> experimental_values </em>的计算(通过<em class="lb"> next_action_values </em>计算)。这个参数实际上可以被重新使用来告诉网络使用存储的权重而不是当前的网络权重。编辑NNLayer。<em class="lb">前进</em>功能:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="82ce" class="ne ma iq na b gy nf ng l nh ni"># Compute the forward pass for this layer<br/>    def forward(self, inputs, remember_for_backprop=True):<br/>        # inputs has shape batch_size x layer_input_size <br/>        input_with_bias = np.append(inputs,1)<br/>        unactivated = None<br/>        <strong class="na ir">if remember_for_backprop:<br/>            unactivated = np.dot(input_with_bias, self.weights)<br/>        else: <br/>            unactivated = np.dot(input_with_bias, self.stored_weights)</strong><br/>        # store variables for backward pass<br/>        output = unactivated<br/>        ...</span></pre><p id="a970" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">最后，在每次体验重放后，我们将把<em class="lb">存储的权重</em>更新为新的网络权重。将粗体行添加到<em class="lb"> experience_replay </em>方法的最后一位:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="6483" class="ne ma iq na b gy nf ng l nh ni">...<br/>       for layer in self.layers:<br/><strong class="na ir">           layer.update_stored_weights()<br/></strong>           layer.lr = layer.lr if layer.lr &lt; 0.0001 else layer.lr*0.99</span></pre><p id="f668" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">最后，添加NNLayer。<em class="lb">更新_存储_权重</em>方法:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="5baa" class="ne ma iq na b gy nf ng l nh ni">def update_stored_weights(self):<br/>   self.<strong class="na ir">stored_weights</strong> = np.copy(self.<strong class="na ir">weights</strong>)</span></pre><p id="649a" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">很好，这个相对简单的修正意味着我们的目标网络计算不依赖于我们当前的权重。</p><h1 id="8953" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">解决方案的平均发作次数</h1><p id="bc87" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">很好，现在我们已经完成了一次典型的跑步，是时候看看代理在多次不同的训练中学习得有多快了。为了做到这一点，我们从头开始初始化一个新的代理，运行多次，看看需要多少集才能达到平均奖励阈值。</p><p id="d4d1" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">这是50次运行的数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/5c2058613ccf6582e3b3652cdc0021bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l0gIisQeO-Z21-J37ykEng.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每次运行的解决方案集</p></figure><p id="e007" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">除了两次运行长时间陷入局部极小值并花费了超过2000个时间步骤来解决之外，几乎所有其他运行都花费了不到200集来收敛。在50次运行中要解决的平均事件数是240.84。这包括两次异常运行。</p><h1 id="63bd" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">不同的批量</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/f6003ce9f7d45f4c538f3bab7a756cd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*fpPe1J6B6NhuG4vsNavRRg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">超过20次运行的平均发作到解决方案</p></figure><p id="1ab0" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">该图显示了改变批次大小如何影响平均发作次数(每个批次大小超过20次)。我测试了4个不同的值——5、10、20和40。就要解决的平均情节数而言，表现最好的是批量大小为20，平均有大约173个情节要解决。然而，考虑到我们对批量大小为10的算法进行了一半的更新，我们仍然能够平均只解决304集。这比double低15%左右。在批量大小为40的情况下，尽管大多数时候该算法收敛得非常快(超过50%的解决方案处于最低可能的100集标记)，但是该算法在某些集内非常不稳定，并且直到远远超过3000集时才收敛。</p><p id="0cb0" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">接下来，我们将使用批量大小<strong class="lc ir"> </strong> 10进行其余的增强。</p><h1 id="2683" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">Adam优化器</h1><p id="2987" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">到目前为止，在计算了梯度之后，我们的NNLayer。<em class="lb"> update_weights </em>函数使用随时间不断降低的学习率更新层权重，直到达到最小阈值。</p><p id="c5e9" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">对于权重矩阵中的每个参数，我们当前的权重更新具有相同的学习率。我们现在将使用<a class="ae kv" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">亚当优化技术</a>，看看是否能改善结果。Adam的工作方式是跟踪网络中每个参数的个人学习率，使用关于该参数的梯度的一阶和二阶矩的估计值。这通常会导致更快的收敛。</p><p id="8df4" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">参考<a class="ae kv" rel="noopener" target="_blank" href="/adam-latest-trends-in-deep-learning-optimization-6be9a291375c">这篇文章</a>来了解下面代码的细节。如果您想直接了解超参数配置，可以跳过本节关于Adam实现的其余部分。</p><p id="a354" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我们开始吧。我们将更改NNLayer中的<em class="lb"> update_weights </em>方法，如下所示:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="2eda" class="ne ma iq na b gy nf ng l nh ni">def <strong class="na ir">update_weights</strong>(self, gradient):        <br/>        m_temp = np.copy(self.m)<br/>        v_temp = np.copy(self.v) <br/>        <br/>        m_temp = self.<strong class="na ir">beta_1</strong>*m_temp + (1-self.<strong class="na ir">beta_1</strong>)*gradient<br/>        v_temp = self.<strong class="na ir">beta_2</strong>*v_temp + (1-self.<strong class="na ir">beta_2</strong>)*(gradient*gradient)<br/>        m_vec_hat = m_temp/(1-np.power(self.<strong class="na ir">beta_1</strong>, self.<strong class="na ir">time</strong>+0.1))<br/>        v_vec_hat = v_temp/(1-np.power(self.<strong class="na ir">beta_2</strong>, self.<strong class="na ir">time</strong>+0.1))<br/>        self.<strong class="na ir">weights </strong>= self.<strong class="na ir">weights </strong>- np.divide(self.<strong class="na ir">lr</strong>*m_vec_hat, np.sqrt(v_vec_hat)+self.<strong class="na ir">adam_epsilon</strong>)<br/>        <br/>        self.m = np.copy(m_temp)<br/>        self.v = np.copy(v_temp)</span></pre><p id="46e2" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">beta_1、beta_2和adam_epsilon参数是在adam优化器的实现中使用的常数。它们几乎从未改变。矩阵m和v以及时间参数是在训练过程中更新的变量。它们都在层的init方法中初始化:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="d5e1" class="ne ma iq na b gy nf ng l nh ni">def <strong class="na ir">__init__</strong>(self, input_size, output_size, activation=None, lr = 0.001):<br/>        ...<br/>        self.<strong class="na ir">lr</strong> = lr<br/>        self.<strong class="na ir">m</strong> = np.zeros((input_size, output_size))<br/>        self.<strong class="na ir">v </strong>= np.zeros((input_size, output_size))<br/>        self.<strong class="na ir">beta_1 </strong>= 0.9<br/>        self.<strong class="na ir">beta_2 </strong>= 0.999<br/>        self.<strong class="na ir">time </strong>= 1<br/>        self.<strong class="na ir">adam_epsilon </strong>= 0.00000001</span></pre><p id="cee9" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我们还用Adam的<em class="lb">时间</em>参数的增加来代替层学习速率的减少。Adam使用<em class="lb">时间</em>自动降低学习率。像这样更新experience_replay方法的最后3行:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="8732" class="ne ma iq na b gy nf ng l nh ni">...<br/>   for layer in self.layers:<br/>      <strong class="na ir">layer.update_time()<br/> </strong>     layer.update_stored_weights()</span></pre><p id="6399" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">update_time()实现只是每次将时间参数增加1。</p><p id="218e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">将我们的实现与代码开头链接的文章进行比较，以验证它确实是准确的！</p><p id="b555" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">太好了，现在是实现的时候了，看看它是否真的表现得更好！以下是50次运行(批量为10次)后解决方案的集数图表:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/ea92323f351f2f0ca5a1aa477bd5ead0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*CXPTPg2-TSZmhSYFYH-Y6w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用Adam optimizer解决情节</p></figure><p id="40f2" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">虽然这仍然有一些不稳定性，但与我们的旧优化器相比，它的性能提高了大约17%(261对304)。</p><p id="504d" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">虽然这不是结论性的，试验的数量也很少，但它表明Adam在某些情况下是一种有效的技术。对Adam性能的全面分析，以及何时使用这种优化技术与其他优化技术的对比，可以在<a class="ae kv" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank">原始论文</a>中找到。</p><h1 id="72c7" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated"><strong class="ak">隐藏层尺寸</strong></h1><p id="bc8a" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">隐藏层的大小也有所不同。以下是4种不同隐藏层大小(12、24、48和96)的平均集数。在<a class="ae kv" rel="noopener" target="_blank" href="/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2">第一部分</a>中的神经网络图描述的每个实验中有两个隐藏层，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/7ab6914770fd866505b11db4e5135024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bybL146BQfsXKWhzSBJ7JA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">为隐藏层解决不同大小的剧集</p></figure><p id="5fa1" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">该图呈下降趋势，图层大小为96时表现最佳。同样，少量的运行并不能提供确凿的证据，但它表明更多的参数通常会提高代理的性能。当然，代价是训练更大的网络所需的时间和内存通常更大。</p><h1 id="14ac" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">隐藏层数</h1><p id="958e" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">到目前为止，我们所有的实验都有两个隐藏层。相反，尝试使用1层和3层，在20次运行中得到以下结果，每次运行有96个隐藏单元—</p><ol class=""><li id="c760" class="nm nn iq lc b ld le lg lh lw no lx np ly nq lv nr ns nt nu bi translated">1层收敛的平均步骤— 198.6</li><li id="24e4" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">收敛到两层的平均步数— 163.75</li><li id="5f29" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">3层的平均收敛步骤— (&gt;1000)集。在这里，网络需要很长时间才能收敛。此外，更深层次的神经网络会遇到其他问题，比如需要小心处理的<a class="ae kv" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失梯度问题</a>。</li></ol><p id="6ec9" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">不幸的是，硬件的限制使我无法对更深层次的神经网络进行更彻底的分析。</p><h1 id="ed29" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">摘要</h1><p id="f18e" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">在这一部分中，我们总结并完成了算法的实现，以训练我们的cartpole代理。包括目标网络实现和Adam的更新代码可在<a class="ae kv" href="https://github.com/abhavk/dqn_from_scratch" rel="noopener ugc nofollow" target="_blank">这里</a>找到。由于普遍性低，我故意没有对这个特殊问题的各种超参数配置进行全面分析，但这里有一篇<a class="ae kv" href="https://arxiv.org/pdf/1709.06560.pdf" rel="noopener ugc nofollow" target="_blank">的信息论文</a>，它非常详细地研究了改变dqn配置的影响。</p><p id="1304" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">综上所述，这部分我们做了以下工作。</p><ol class=""><li id="4aab" class="nm nn iq lc b ld le lg lh lw no lx np ly nq lv nr ns nt nu bi translated">从随机初始化到接近完美平衡的最终状态，分析并可视化了代理改进的样本运行。</li><li id="cdb6" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">添加了目标网络，完成了实施。</li><li id="956e" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">增加了一个Adam优化器来代替原来的一揽子学习率。</li><li id="43dc" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">探索了一些不同的超参数配置，如批量大小、隐藏层大小和隐藏层数。</li></ol><h1 id="72c0" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">进一步阅读</h1><p id="5750" class="pw-post-body-paragraph kz la iq lc b ld mr jr lf lg ms ju li lw mt ll lm lx mu lp lq ly mv lt lu lv ij bi translated">如果您已经完成了这一步，那么您现在已经完成了横竿问题的实现！您可能希望:</p><ol class=""><li id="948e" class="nm nn iq lc b ld le lg lh lw no lx np ly nq lv nr ns nt nu bi translated">进一步调整这个程序，找出最佳的超参数配置。你能在50次运行中把平均集数降到110以下吗？</li><li id="4528" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">继续讨论在<a class="ae kv" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank">露天健身房</a>环境中的其他问题。<a class="ae kv" href="https://gym.openai.com/envs/MountainCarContinuous-v0/" rel="noopener ugc nofollow" target="_blank"> MountainCar </a>是很好的下一步！</li><li id="ebf9" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated">看看强化学习和人工通用智能的前沿。OpenAI在他们的博客上记录了所有的进展。</li></ol><p id="5b13" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">感谢您的阅读！！！</p><h1 id="745f" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">参考</h1><ol class=""><li id="bb76" class="nm nn iq lc b ld mr lg ms lw oa lx ob ly oc lv nr ns nt nu bi translated"><a class="ae kv" href="https://arxiv.org/abs/1709.06560" rel="noopener ugc nofollow" target="_blank">重要的深度强化学习</a>【彼得·亨德森等】</li><li id="71f1" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/adam-latest-trends-in-deep-learning-optimization-6be9a291375c">Adam——深度强化学习的最新趋势</a>。</li><li id="065d" class="nm nn iq lc b ld nv lg nw lw nx lx ny ly nz lv nr ns nt nu bi translated"><a class="ae kv" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">Adam优化技术简介</a>。</li></ol></div></div>    
</body>
</html>