<html>
<head>
<title>The relationship between Perplexity and Entropy in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的困惑与熵的关系</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc?source=collection_archive---------12-----------------------#2020-06-07">https://towardsdatascience.com/the-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc?source=collection_archive---------12-----------------------#2020-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f54b2b034ccf3d157cb10b6f47b03973.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NyJ-NV18ZvLJlvCsA1h9VQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">艾萨克·奎萨达在<a class="ae jg" href="https://unsplash.com/t/wallpapers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="66c5" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">使用信息论理解NLP度量</h2></div><p id="5382" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在评估语言模型时，困惑是一个常用的度量标准。例如，scikit-learn实现的<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html?highlight=perplexity" rel="noopener ugc nofollow" target="_blank">潜在狄利克雷分配</a>(一种主题建模算法)包含了作为内置度量的困惑。</p><p id="b7fe" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我将定义困惑，然后讨论熵，两者之间的关系，以及它如何在自然语言处理应用中自然出现。</p><h1 id="77de" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">语境</h1><p id="3f97" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">在许多自然语言任务中，一个非常普遍的情况是，你有一门语言<em class="mr"> L </em>并且想要为这门语言建立一个模型<em class="mr"> M </em>。“语言”可以是一种特定的体裁/语料库，如“英语维基百科”、“尼日利亚推特”或“莎士比亚”，或者(至少在概念上)只是一种通用的，如“法语”</p><p id="bbb6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">具体来说，我们所说的语言是指生成文本的过程。为了清楚起见，我们将考虑这样一种情况，我们正在对句子建模，文本由以句尾“单词”结尾的序列单词组成但是你可以用“token”代替“word”，用“document”代替“sentence”来概括任何上下文。</p><p id="6035" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">什么是“过程”？出于我们的目的，我们可以把一个过程想象成概率分布的集合。给定由一个句子中的一系列先前单词组成的历史<em class="mr"> h </em>，语言L是下一个单词是<em class="mr"> w: </em>的概率</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ms"><img src="../Images/b6cdaf313a2a68cfd6fd60b69cf522c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3dikCAxgmnBhJwaSg2WCw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">语言是概率分布的集合</p></figure><p id="350e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，我愿意打赌，如果L是“英国人”:</p><ol class=""><li id="6154" class="mx my jj la b lb lc le lf lh mz ll na lp nb lt nc nd ne nf bi translated">l(狗|敏捷的棕色狐狸跳过懒惰的棕色)≈ 1</li><li id="eb3a" class="mx my jj la b lb ng le nh lh ni ll nj lp nk lt nc nd ne nf bi translated">L(ipsum | Lorem) ≈ 1</li><li id="1c25" class="mx my jj la b lb ng le nh lh ni ll nj lp nk lt nc nd ne nf bi translated">l(翅膀|水牛水牛水牛水牛水牛)≈ 0</li></ol><p id="fa8d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">类似地，给定一个完整的句子<em class="mr"> s </em>，我们可以评估L( <em class="mr"> s </em>)这个句子出现的概率。如果我们包括一个特殊的句子开头“单词”wₒ，并让第n个“单词”是句尾“单词”，我们得到</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/4a0486eb3e98eef5721f858603cde9db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hPr-p8AVVltOF3_IeMQfIw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">语言<em class="nm"> L </em>给出一个句子<em class="nm"> s </em>的概率</p></figure><p id="3e03" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，在产品中省略第一个术语是很常见的，或者有时使用更长的起始上下文。</p><p id="9962" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">获得(比如说)美式英语口语的完美复制品是非常容易的。只要在街上拦住任何一个以英语为母语的人。当然，我们通常对教计算机模型感兴趣(因此，机器学习)。因此，我们将让<em class="mr"> M </em>成为我们在计算机上成功构建的任何语言模型。</p><p id="c143" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种设置具有语言<em class="mr"> L </em>和模型<em class="mr"> M </em>非常通用，在各种自然语言任务中发挥作用:语音到文本、自动更正、自动完成、机器翻译——不胜枚举。自动完成是最明显的例子:给定某人到目前为止键入的单词，通过选择最有可能完成的单词来猜测他们接下来可能键入什么。</p><h1 id="b60a" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">困惑</h1><p id="7a1d" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">给定一个语言模型M，我们可以使用一个保持dev(验证)集来计算一个句子的复杂度。句子上的困惑<em class="mr"> s </em>定义为:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/a1382369a5dba8cbfde1edc271a82fd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tc2HbUQ-4fYNaOlIfRu01w.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">语言模型M的困惑</p></figure><p id="0ab1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你会从第二行注意到，这是乘积分母中各项的<em class="mr">几何平均值</em>的倒数。由于每个单词的概率(取决于历史)都被计算过一次，我们可以将此解释为每个单词的<em class="mr">度量。这意味着，在其他条件相同的情况下，困惑不受句子长度的影响。</em></p><p id="ec18" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一般来说，我们希望我们的概率高，这意味着困惑低。如果所有的概率都是1，那么困惑度将是1，模型将完美地预测文本。相反，对于较差的语言模型，困惑度会更高。</p><p id="0aeb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">很难为困惑提供一个基准，因为像大多数自然语言任务一样，这个标准高度依赖于词汇量。给定一个语料库，较小的词汇量意味着其他单词将全部被替换为一个<oov>(词汇外)标记，从而立即提高在其上训练的任何语言模型的表面质量</oov></p><p id="0d82" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是一些基准测试:</p><ol class=""><li id="2db4" class="mx my jj la b lb lc le lf lh mz ll na lp nb lt nc nd ne nf bi translated"><strong class="la jk">最先进的。</strong>对于<a class="ae jg" href="https://arxiv.org/pdf/1609.07843.pdf" rel="noopener ugc nofollow" target="_blank"> WikiText-103 </a>，一个大约28，000篇高质量维基百科文章和大量(0.4% OOV率)词汇的精选，一个语言模型的<a class="ae jg" href="https://arxiv.org/abs/1909.08053" rel="noopener ugc nofollow" target="_blank">最先进的</a>困惑度(截至本文撰写时)是<strong class="la jk"> 10.8 </strong>。</li><li id="cb70" class="mx my jj la b lb ng le nh lh ni ll nj lp nk lt nc nd ne nf bi translated">最坏的情况。在任何数据集上，基线模型只是以相等的概率随机猜测词汇表中的一个单词。在这种情况下，困惑只是词汇表的大小:WikiText-103为<strong class="la jk"> 267，735 </strong>，但是WikiText-2要小得多(<strong class="la jk"> 33，278 </strong>)。一般来说，30，000对于一个语言模型的词汇量来说是相当合理的。</li><li id="052e" class="mx my jj la b lb ng le nh lh ni ll nj lp nk lt nc nd ne nf bi translated">最好的情况。我在上面说过“最好的”可能的困惑是1。但是如果那是真的，那么在一种语言中就只能有一个可能的句子，这是很无聊的。最近一篇探索文本生成的论文使用了OpenAI的<a class="ae jg" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">GPT-2</a>(WikiText-103上有困惑的22.1大版本)。在他们选择的数据集上(WebText，GPT-2就是在上面训练的)，他们发现了<strong class="la jk"> 12.4 </strong>的困惑。但是，重要的是，他们发现，虽然他们的模型能够以低得多的复杂度生成文本(1.5！)，生成的文本不是重复就是不连贯。离人类的困惑越近越好！</li></ol><p id="64f1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这最后一点非常重要。语言本身确定了一个困惑的下限。我们将在下面看到这一点。但是这指出了NLP中度量的一个普遍特征:一个容易评估的度量，比如困惑，不一定是模型真实性能的最佳预测者。困惑有利于开发(验证)，但不一定有利于评估。评价的黄金标准仍然是人的评价。</p><h1 id="6520" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">熵</h1><p id="f82f" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">熵在物理学中是一个模糊的概念，但在信息论中却非常简单。假设你有一个过程(就像一门生成单词的语言)。在这个过程的每一步，发生的事情(事件)都有可能会发生。<strong class="la jk">惊奇</strong>的量是–log(<em class="mr">p</em>)，其中对数取任意你想要的底数(相当于换单位)。低概率事件具有高度的惊奇性。肯定会发生的事件(<em class="mr"> p </em> =1)没有意外。不可能的事件(<em class="mr"> p </em> =0)有无限的惊奇。</p><p id="5050" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">熵是由<em class="mr"> i </em>索引的所有可能事件中意外事件的期望值；</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/7556a20fe811c3de5c157ed00b9aeb10.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*GWe3yLwQQihu0x8_jRePkw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">概率分布的熵</p></figure><p id="c51d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，熵是当某件事发生时的平均惊奇程度。</p><p id="f5f3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据克劳德·香农的<a class="ae jg" href="https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem" rel="noopener ugc nofollow" target="_blank">信源编码定理</a>，以2为基数的熵也是存储所发生信息的最佳位数。例如，如果我告诉你，一条280个字符的全长推文每个字符的熵为1位，这意味着，根据数学定律，无论Twitter做什么，他们的数据库中总要有280位(35字节)的存储空间。(当然，在实践中，他们必须拥有更多)。</p><p id="887b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的语言模型的上下文中，我们必须做一个调整。假设我们对长度为<em class="mr"> n </em>的句子<em class="mr"> s </em>(事件序列)感兴趣，我们将每个单词(事件)的熵率定义为:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e345eb49c40d8f8dce595e457aa965d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*5fIqNwfE0ac8D0Va5RS7Iw.png"/></div></figure><p id="3a7b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中总和是所有句子的长度<em class="mr"> n </em>，L(s)是句子的概率。最后一个技术点:我们要定义语言L(或者语言模型M)的熵，而不考虑句子长度<em class="mr"> n </em>。所以最后我们定义了</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3f8a69c70a7f515ee654b68eb6061735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*IudqhcpGCMC2eBsWrXdn3g.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">语言(模型)熵的最终定义</p></figure><h2 id="cbe7" class="nr lv jj bd lw ns nt dn ma nu nv dp me lh nw nx mg ll ny nz mi lp oa ob mk oc bi translated">香农-麦克米兰-布雷曼定理</h2><p id="698d" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">在不痛不痒的假设下，熵会进一步简化。本质的观点是，如果我们取一个足够长的文本串，无论如何每个句子出现的概率都是成比例的。所以没有必要总结可能的句子。我们得到:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/129c6af6b40d941bb6070c0a22acec13.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*1-nFIylNPQJPWu1ljh1S9w.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用香农-麦克米兰-布雷曼定理简化熵</p></figure><p id="106d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这告诉我们，我们可以只取一个大的(n是大的)文本，而不是试图从不同的文本中采样。</p><h2 id="771c" class="nr lv jj bd lw ns nt dn ma nu nv dp me lh nw nx mg ll ny nz mi lp oa ob mk oc bi translated">交叉熵</h2><p id="257f" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">假设我们错误地认为我们的语言模型M是正确的。然后我们在没有意识到的情况下观察实际语言L生成的文本。交叉熵H(L，M)就是我们测量的熵</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f4a3b7d9d069461639180d7988bfbaaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*9eit79wZjp0iQFlCSluzxQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">我们的语言模型M的交叉熵</p></figure><p id="0bee" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中第二行再次应用了香农-麦克米兰-布雷曼定理。</p><p id="c1f3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">至关重要的是，这告诉我们，我们可以通过测量随机句子样本(第一行)或足够大的文本块(第二行)的log M(s)来估计交叉熵H(L，M)。</p><h2 id="9806" class="nr lv jj bd lw ns nt dn ma nu nv dp me lh nw nx mg ll ny nz mi lp oa ob mk oc bi translated">交叉熵受语言真实熵的限制</h2><p id="0a59" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">交叉熵有一个很好的性质，即<strong class="la jk"> H(L) </strong> ≤ <strong class="la jk"> H(L，M) </strong>。在证明中省略极限和归一化1/n:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/42e6d083b8f162c99ccf6a39be743202.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nnPHvMFwp0LdTwdIJOowNw.png"/></div></div></figure><p id="2317" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在第三行，第一项只是交叉熵(记住极限和1/n项是隐式的)。第二项是Kullback-Leibler背离(或KL背离)。根据<a class="ae jg" href="https://en.wikipedia.org/wiki/Gibbs%27_inequality" rel="noopener ugc nofollow" target="_blank">吉布斯不等式</a>，KL散度是非负的，并且只有当模型L和M相同时才为0。KL-divergence有点像距离度量(告诉你L和M有多不同)。⁴ ⃞</p><h1 id="600d" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">困惑与熵的关系</h1><p id="4ec9" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">概括一下:</p><ol class=""><li id="b43a" class="mx my jj la b lb lc le lf lh mz ll na lp nb lt nc nd ne nf bi translated">根据Shannon-McMillan-Breiman定理，在L生成的足够长(<em class="mr"> n </em>大)的开发/验证/测试数据集上评估M的熵近似于交叉熵H(L，M)。我们只是通过取一个足够大的评估集来近似极限。</li><li id="cfc1" class="mx my jj la b lb ng le nh lh ni ll nj lp nk lt nc nd ne nf bi translated">此外，这种交叉熵受到生成数据的语言的真实熵的限制</li></ol><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f74f1a7178142077093634ca05b9b53e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*OytXr9mF03RPWQ3xajSicg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">足够大数据集交叉熵的困惑定义和简化</p></figure><p id="9d49" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在剩下要做的就是展示两者之间的关系。假设我们以e为底取对数:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/2a82c429d016e10cf49bc8c45090eac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*lnXZJM_WIsibGcp8buc1dQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">困惑与熵的关系</p></figure><p id="f203" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们以2为底取对数，用2为底，以此类推。</p><p id="7e76" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，总结一下:</p><ol class=""><li id="fc34" class="mx my jj la b lb lc le lf lh mz ll na lp nb lt nc nd ne nf bi translated">我们为生成数据的真实语言l建立一个语言模型M。</li><li id="fd02" class="mx my jj la b lb ng le nh lh ni ll nj lp nk lt nc nd ne nf bi translated">我们评估M(相对于L)的困惑度，或者等价地，交叉熵。</li><li id="d804" class="mx my jj la b lb ng le nh lh ni ll nj lp nk lt nc nd ne nf bi translated">M的困惑被实际语言L的困惑所限制(同样，交叉熵)。</li></ol><p id="f309" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">困惑度量了我们模型中“随机性”的数量。如果困惑度是3(每个单词)，那么这意味着模型有1/3的机会猜出文本中的下一个单词。因此，它有时被称为<em class="mr">平均分支因子</em>。</p><h1 id="ca4c" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">结论</h1><p id="1de8" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我想给你们留下一个有趣的记录。书面英语文本的真实熵是多少，这是一个悬而未决的问题(抛开其他格式，如“推特”或“口语”以及其他语言，如“俄语”。)</p><p id="41be" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过不等式H(L) ≤ H(L，M)，一种获得困惑度或熵的上界的方法是创建一个语言模型，我们在上面看到了一些困惑。</p><p id="972d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这种情况下，我们通常对每个角色的熵<em class="mr"/>(同样每个角色的困惑)感兴趣。当使用以2为底的对数测量时，这就变成了每字符位数(BPC)。</p><p id="a6f3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">克劳德·香农(在计算机出现之前)估计，书面英语的熵在每个字符0.6到1.3比特之间。上面提到的OpenAI的GPT-2在(另一个)维基百科数据集上实现了每个字符1比特。</p><p id="8b18" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请记住，在书面英语中，每个单词大约有5个字符，这相当于大约5位，或2⁵=32.的困惑请注意，这远远高于作为最新性能指标评测讨论的复杂性！怎么回事？记住不要在词汇或数据集之间比较困惑:单词长度可能不一样。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="3897" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想了解更多关于信息论的内容，请看我之前的文章<a class="ae jg" rel="noopener" target="_blank" href="/understanding-logistic-regression-coefficients-7a719ebebd35">了解逻辑回归系数</a>。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="79fd" class="lu lv jj bd lw lx op lz ma mb oq md me kp or kq mg ks os kt mi kv ot kw mk ml bi translated">参考</h1><p id="b76e" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">除了上面联系/提到的那些</p><ul class=""><li id="9f40" class="mx my jj la b lb lc le lf lh mz ll na lp nb lt ou nd ne nf bi translated"><a class="ae jg" href="https://web.stanford.edu/~jurafsky/slp3/" rel="noopener ugc nofollow" target="_blank">语音和语言处理</a>(茹拉夫斯基和马丁)</li><li id="deb8" class="mx my jj la b lb ng le nh lh ni ll nj lp nk lt ou nd ne nf bi translated">再举一个例子，<a class="ae jg" href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/" rel="noopener ugc nofollow" target="_blank">理解语言模型的度量</a> (Chip Huyen，The Gradient)</li></ul></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="26ec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[1]通常使用<a class="ae jg" href="https://en.wikipedia.org/wiki/Beam_search" rel="noopener ugc nofollow" target="_blank">光束搜索</a>进行估计。</p><p id="6fb2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]或任何给定上下文的一个可能的延续。因此，每个可能的起始单词只有一个可能的句子，或者，在论文的上下文中，给定前40个单词，一篇文章只有一个可能的结尾。这将违反格莱斯的准则，一套关于语言的通用规则。特别是，如果文章的其余部分是由开头决定的，为什么还要写呢？一般来说，自然语言避免低复杂度(熵)的话语，因为它们是不必要的。</p><p id="61bd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3]假设该过程是平稳的和遍历的。事实上，这些假设不适用于自然语言。如果这困扰着你，你可以把这个定理看作是一个非常合理的近似。</p><p id="2d58" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4]它不是距离度量，因为它不是对称的D(p||q)！= D(q||p)。然而，在<a class="ae jg" href="https://en.wikipedia.org/wiki/Statistical_manifold" rel="noopener ugc nofollow" target="_blank">统计流形</a>上解释，其围绕D(p||p)的二阶泰勒展开给出了<a class="ae jg" href="https://en.wikipedia.org/wiki/Fisher_information_metric" rel="noopener ugc nofollow" target="_blank">费希尔信息度量</a>，这是唯一的(根据陈佐夫定理，直到一个标量常数)适用于统计流形的黎曼度量。参见<a class="ae jg" href="https://bookstore.ams.org/mmono-191/" rel="noopener ugc nofollow" target="_blank">信息几何方法</a>了解有限维情况下的更多参考。</p></div></div>    
</body>
</html>