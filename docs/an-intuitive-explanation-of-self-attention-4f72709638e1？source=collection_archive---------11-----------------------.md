# 自我注意的直观解释

> 原文：<https://towardsdatascience.com/an-intuitive-explanation-of-self-attention-4f72709638e1?source=collection_archive---------11----------------------->

## 多头自我注意块的逐步解释

![](img/7496a8cae6325ebd9fd7431b853a2f92.png)

来源:[注意力是你所需要的全部](https://arxiv.org/abs/1706.03762)

在这篇文章中，我将解释你需要知道的关于自我关注的一切。

变压器神经网络包含什么使它们比常规递归神经网络更强大、性能更好？

答:他们使用多头自我注意块给单词嵌入输入更多的上下文信息。但是，自我关注模块对单词嵌入究竟做了什么，使得这些变形金刚模型变得如此强大？

这是本文的重点。这篇文章的主要目的是描述自我关注块的每个部分背后的直觉和其中涉及的数学运算。

本文的目的不是解释变压器神经网络的整体结构。它也没有描述自我注意和常规注意之间的区别。

# 什么是自我关注，为什么我们需要它？

我们知道单词嵌入是代表单词语义的向量。意思相似的词可能有相似的嵌入。然而，在一个句子中，单词的单个含义并不代表它们在句子中的含义。例如，如果我有一个短语，Bank of a river，Bank 和 river 的嵌入分别表示完全不同的东西，但它们在句子中有很强的相关性。没有自我关注的单词嵌入不具备这种上下文信息的意义，因此给定上面的短语，语言模型预测 river 的机会很低。为了解决这个问题，在论文[中提出了自我关注模块](https://arxiv.org/abs/1706.03762)，作为原始变压器架构的一部分。

自我注意模块通过将句子中的每个单词与句子中的每个其他单词(包括其自身)进行比较，并重新加权每个单词的单词嵌入以包括上下文相关性来工作。它接受没有上下文的 *n* 个单词嵌入，并返回有上下文信息的 *n* 个单词嵌入。例如，在短语 Bank of the river 中，Bank 将与 Bank、of、the 和 river 进行比较，当 Bank 与这四个单词进行比较时，其单词 embedding 将被重新加权，以相应地包括单词与其在句子中的含义的相关性。

# 但是它到底是如何工作的呢？

自我关注模块由三个步骤/部分组成:

1.  点积相似性来查找比对分数
2.  对分数进行归一化以获得权重
3.  使用权重对原始嵌入进行重新加权

# 1.点积相似度

当输入的单词嵌入被传递到自我注意模块并找出他们应该注意谁时，他们需要一个函数来找出他们与句子中的其他单词有多相似。这就是点积相似性发挥作用的地方。

对于下面的解释，我将使用下面的例子:

```
Words: 
v1 v2 v3 v4 
```

其中 v1、v2、v3 和 v4 是单词在句子中的单词嵌入。

在我们的例子中，我们将自我关注 v3，并将其与 v1、v2、v3 和 v4 进行比较。我们还必须将每个单词与其自身进行比较(例如 v3 到 v3)的原因是为了使模型能够了解它应该注意单词的语义的哪些部分。一个词可能不得不关注它自己，这似乎是不寻常的。然而，有时一个单词在其嵌入中可以有一个以上的含义(如 Bank 的例子)，除非我们也将它自己包括在点积相似性中，否则模型将无法了解它应该注意单词含义的哪些部分。简而言之，单词通常有不止一个含义，为了让模型识别出它应该关注单词的哪个含义，我们必须通过将单词包括在点积相似度中来将单词与它们自身进行比较。在我们的示例中，v3 与所有其他单词及其自身进行比较，如下所示:

```
(v1*v3) = S31
(v2*v3) = S32
(v3*v3) = S33
(v4*v3) = S34
```

我们将取 v1 和 v3、v2 和 v3、v3 和 v3 以及 v4 和 v3 的点积来确定每对嵌入的比对分数。比对分数 S31、S32、S33 和 S34 将告诉我们 v3 和四个被比较单词中的每一个的语义有多相似。对齐分数越高，单词的语义就越相似，v3 就必须更加注意该对中的另一个单词。注意，每个比对分数是单个数字，而不是数字的向量或矩阵。

我只对一个词做过这个过程:v3。实际上，这个过程将使用向量化和线性代数对所有四个单词同时进行。

## 为什么点积相似会起作用？

就拿国王和王后的嵌字来说吧。

```
King = [0.99, 0.01, 0.02]Queen = [0.97, 0.03, 0.02]
```

这一对的比对分数将是:

```
(0.99 * 0.97) + (0.01 * 0.03) + (0.02 * 0.02) = 0.961
```

现在以单词“国王和狗”的嵌入为例

```
King = [0.99, 0.01, 0.02]Dog = [0.01, 0.02, 0.02]
```

这一对的比对分数是

```
(0.99 * 0.01) + (0.01 * 0.02) + (0.02 * 0.02) = 0.0105 
```

请注意，这些单词嵌入是虚构的，真实的单词嵌入 dog、king 和 queen 的大小可能要大得多。还要注意第一个索引，其中 king 和 queen 的数字较高，表示特定单词的版税。

如你所见，国王和王后的比对分数比国王和狗大得多。这是因为国王和狗的语义不同，所以它们在句子中对彼此并不重要。指数 1 代表这个词的王权，因此国王和王后在这方面是相似的。

简而言之，如果两个单词的语义不同，则对齐分数不会很高，单词的对应性会很低，并且每个单词对彼此的关注度会很低，因此每个单词的原始单词嵌入不会改变太多。如果两个单词的语义在任何方面都相似，那么对齐分数将会很高，这些单词将具有很高的对应性，并且这两个单词将会高度关注彼此，因此每个单词的原始单词嵌入将会经历显著的变化。

# 2.标准化比对分数

通过激活函数归一化任何神经网络的输出值是一个常见的过程。我们将使用 SoftMax 激活函数来归一化对齐分数，以获得我们将应用于原始单词嵌入的权重。SoftMax 函数将使每个比对分数成为概率分布，因此它们加起来都是 1。在我们的例子中，

```
S31 -> SoftMax -> W31
S32 -> SoftMax -> W32
S33 -> SoftMax -> W33
S34 -> SoftMax -> W34
```

将对所有四个对齐分数进行软最大化，以获得必须应用于原始单词嵌入的最终权重，从而创建最终的上下文化嵌入。

请注意，当我在上面的段落中说“权重”时，我不是指模型将学习的参数。

# 3.最终重新称重过程

现在进行最后的重新称重过程。为了确定单词(v3)应该给予其他单词的关注总量，我们将权重乘以它们各自的原始单词嵌入，然后将所有这些值加在一起，以获得被比较单词(v3)的最终单词嵌入；在这种情况下，

```
W31 * v1 = Y31
W32 * v2 = Y32
W33 * v3 = Y33
W34 * v4 = Y34Y31 + Y32 + Y33 + Y34 = Y3
```

Y3 是 v3 的最终重新加权的字嵌入向量。请记住，Y31、Y32、Y33 和 Y34 都是独立的向量，因此我们必须将它们相加，以获得一个最终向量。

请记住，上述过程也将针对句子中的每一个其他单词进行。

直觉上，上述所有过程都是有意义的，除了一件事:因为没有要学习的权重，所以对齐分数和自我关注权重本质上是预先确定的，模型将无法学习两个单词之间的任何更深层次的联系。这就是为什么我们将在自我关注模块中引入权重。但是在哪里？为了让模型学习单词之间最深层的联系，我们将在三个位置引入权重:输入单词嵌入、点积相似性比较和重新加权单词嵌入的最后一步。此外，在这些位置引入单词的另一个好处是相乘的向量的形状/维度不会改变。请记住，如果我们有一个 1 x k 形状的向量，我们把它乘以一个 k x k 形状的矩阵，这将产生 1 x k 形状的向量。因此，即使我们在描述的位置引入权重，维度或形状也不会有任何改变。

这就是为什么我们必须在具有原始单词嵌入向量 v1、v2、v3 和 v4 的地方引入权重，因为这些向量的形状都是相同的，并且即使我们引入 k×k 的权重矩阵，这些向量的形状仍然是 1×k。我们使用原始嵌入向量的三个位置是输入单词嵌入、当我们将每个单词与其他单词进行比较时的点积相似性，以及在最终的重新加权过程中当我们将归一化权重乘以原始单词嵌入时的点积相似性。对于权重，我们的计算如下所示:

```
(v1 * Mk) * (v3 * Mq) = S31
(v2 * Mk) * (v3 * Mq) = S32
(v3 * Mk) * (v3 * Mq) = S33
(v4 * Mk) * (v3 * Mq) = S34S31 -> SoftMax -> W31
S32 -> SoftMax -> W32
S33 -> SoftMax -> W33
S34 -> SoftMax -> W34(v1 * Mv) * W31 = Y31
(v2 * Mv) * W32 = Y32
(v3 * Mv) * W33 = Y33
(v4 * Mv) * W34 = Y34Y31 + Y32 + Y33 + Y34 = Y3
```

如您所见，无论我们在哪里使用原始的单词嵌入 v1、v2、v3 和 v4，我们都将这些向量与相应的权重矩阵相乘。Mk、Mq 和 Mv 只是模型将学习的键、查询和值矩阵/权重。记得在上面的计算中，我只做了一个词的自我关注操作，v3。实际上，通过矢量化和一些线性代数，这将同时发生在所有单词上。

请注意，在上图中，当我将键、查询和值矩阵与它们各自的嵌入向量相乘时，我做的是矩阵乘法，而不是点积。

# 多头注意力

虽然一个自我注意块足以让一个词获得语境相关性似乎是合理的，但事实并非如此。往往一个词会不得不关注多个其他词，一个自我关注块对多个词来说可能不够关注。这在输入文本非常大的例子中尤其明显(例如，对于文本摘要任务)。因此，一对具有上下文相关性的单词有时不会得到足够的关注，从而使它们各自的嵌入发生可观察到的变化。

为了解决这个问题，我们将使用多头注意力块。多头注意力块扩展了模型关注输入文本中不同位置的能力。

多头关注块本质上与常规的自我关注块是相同的东西，但是多头关注块将包含并行操作的多个自我关注块，而不是仅仅一个关注块。这些自我关注块将不共享任何权重；他们唯一共享的是相同的输入单词嵌入。多头注意块中自我注意块的数量是模型的超参数。假设我们选择有 *n* 个自我关注块。这样做的结果是，在每个自我关注模块完成所有单独的计算后，我们将为每个单词提供 *n* 个嵌入。为了修复这个问题，多头注意力块将这些嵌入串联起来，最后通过一个密集层传递。请记住，我们希望输入的形状和数量等于输出的形状和数量。因为我们连接了嵌入，所以我们保持输入的数量等于输出的数量。因为我们通过密集层传递连接的输出，所以我们能够控制它们的形状，确保它与输入的形状相同。

# 摘要

自我注意块将句子中单词的单词嵌入作为输入，并返回相同数量的单词嵌入，但带有上下文。它通过一系列键、查询和值权重矩阵来实现这一点。多头关注块由多个并行操作且不共享权重的自我关注块组成。在每个自关注块返回新的上下文化单词嵌入之后，多头关注块将这些新的嵌入连接在一起，并通过密集层传递它们，以便控制输出的形状。这允许单词嵌入得到足够的关注，这将在其嵌入中产生可观察到的变化。

我希望您觉得这些内容简单易懂。如果你认为我需要进一步阐述或澄清，请在下面留言。

## 参考

你所需要的就是关注(arxiv.org)

[Rasa](https://www.youtube.com/watch?v=yGTUuEx3GkA)

## 相关文章

[插图:自我关注](/illustrated-self-attention-2d627e33b20a)(towardsdatascience.com)