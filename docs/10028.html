<html>
<head>
<title>PGM 1: Introduction to Probabilistic Graphical Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PGM 1:概率图形模型介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-probabilistic-graphical-models-7d2c0b4bef19?source=collection_archive---------20-----------------------#2020-07-15">https://towardsdatascience.com/introduction-to-probabilistic-graphical-models-7d2c0b4bef19?source=collection_archive---------20-----------------------#2020-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d791" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">概率图形模型世界的快速简单入门</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/69a7290290d8bde3f0703194c5b1bff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*OTlMxXiFbu0ETMS7U5cx2A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">由作者创建，用于说明贝叶斯网络中的节点和边</p></figure><blockquote class="ku kv kw"><p id="99b0" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">概率图形模型(PGM)提供了一种图形表示来理解一组随机变量(RVs)之间的<strong class="la iu">复杂</strong>关系。</p></blockquote><p id="3e87" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">RVs 表示节点，它们之间的统计相关性称为边。上面显示了一个概率图形模型的例子。</p><p id="d4ae" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">我在上面强调了“复杂”这个词；<strong class="la iu">让我们了解一下房车之间的复杂程度:</strong></p><p id="e98d" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">假设我们有 n 辆房车:X1，X2…Xn。让我们举一个非常简单的例子，假设每个 RV 只能取 2 个可能的值，即我们有一组二进制 RV。</p><p id="fd66" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">现在，学习概率图形模型的关键目标是学习由 P(X1，X2，..Xn)为一组随机变量。我们注意到，随着 2^n 状态的增加，n 个二元 RVs 分布的复杂性增长到指数级。</p><h2 id="e6da" class="lx ly it bd lz ma mb dn mc md me dp mf lu mg mh mi lv mj mk ml lw mm mn mo mp bi translated"><strong class="ak">建立直觉的例子:</strong></h2><h2 id="b405" class="lx ly it bd lz ma mb dn mc md me dp mf lu mg mh mi lv mj mk ml lw mm mn mo mp bi translated">RVs 及其关联借助一个示例进行解释:</h2><p id="a186" class="pw-post-body-paragraph kx ky it la b lb mq ju ld le mr jx lg lu ms lj lk lv mt ln lo lw mu lr ls lt im bi translated">如果患者带着一些症状(例如发烧)去看医生，医生为调查症状和可能的致病原因而询问的问题可以被描述为 RVs。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mv"><img src="../Images/eb5ea85e579918417f6d348d5a2bbce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PlygKYKC8crs0MS7ZiaOSg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">医学诊断是 PGM 的应用之一；照片由<a class="ae na" href="https://unsplash.com/@nci?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">国家癌症研究所</a>在<a class="ae na" href="https://unsplash.com/s/photos/medical-diagnosis?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4626" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">这些问题可能采取如下形式:</p><ul class=""><li id="9b84" class="nb nc it la b lb lc le lf lu nd lv ne lw nf lt ng nh ni nj bi translated">发烧多少天了？</li><li id="f029" class="nb nc it la b lb nk le nl lu nm lv nn lw no lt ng nh ni nj bi translated">食欲不振？</li><li id="50a8" class="nb nc it la b lb nk le nl lu nm lv nn lw no lt ng nh ni nj bi translated">发烧级别——高还是低？</li><li id="271e" class="nb nc it la b lb nk le nl lu nm lv nn lw no lt ng nh ni nj bi translated">头疼还是头晕？</li><li id="277c" class="nb nc it la b lb nk le nl lu nm lv nn lw no lt ng nh ni nj bi translated">病史？</li></ul><p id="2d8f" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated"><strong class="la iu">补充说明</strong>:上面描述了两种变量——连续变量和离散变量。我们稍后将对此进行更多讨论，但在这一点上需要注意的是，连续变量和离散变量的共同存在会导致混合/混合网络(通常难以建模)。</p><p id="51d1" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">因此，基于这些属性(又名 RVs)之间的关联，医生可能会建议进一步的行动方案，要么是一些药物，要么是一些测试，以某种概率<strong class="la iu">得出结论</strong>。</p><p id="0a9b" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">这里出现了一个需要更多强调和讨论的重要方面——概率。虽然我们大概知道没有机器学习模型或专家(在我们的情况下，是医生)可以在任何时候都 100%准确地预测输出，但让我们明白其中的一些原因，如下所述:</p><ul class=""><li id="add5" class="nb nc it la b lb lc le lf lu nd lv ne lw nf lt ng nh ni nj bi translated"><strong class="la iu">对世界的一知半解:</strong>无论医生对可能的原因调查得多好，总有一定程度的不确定性。这主要是因为我们并不完全了解世界上所有的国家。</li><li id="21b1" class="nb nc it la b lb nk le nl lu nm lv nn lw no lt ng nh ni nj bi translated"><strong class="la iu">噪音:</strong>测试结果可能会有噪音观察的成分，并清楚地表明它们可以被信任的程度。</li><li id="8120" class="nb nc it la b lb nk le nl lu nm lv nn lw no lt ng nh ni nj bi translated"><strong class="la iu">建模限制:</strong>即使有一些可能的疾病以发热为初步症状，但在建立模型时，几乎不可能考虑所有这些因素。</li></ul><h2 id="e38f" class="lx ly it bd lz ma mb dn mc md me dp mf lu mg mh mi lv mj mk ml lw mm mn mo mp bi translated"><strong class="ak">我们为什么需要 PGM，它在机器学习世界中处于什么位置？</strong></h2><ul class=""><li id="cdd6" class="nb nc it la b lb mq le mr lu np lv nq lw nr lt ng nh ni nj bi translated">PGM 提供了一种结构，在这种结构中，我们可以利用<strong class="la iu"> <em class="kz">独立性属性</em> </strong>以更紧凑的方式表示高维数据。</li><li id="3075" class="nb nc it la b lb nk le nl lu nm lv nn lw no lt ng nh ni nj bi translated">它是对某些<strong class="la iu"><em class="kz"/></strong>未观测变量，给出<strong class="la iu"> <em class="kz">观测变量</em> </strong>的证据，得出<strong class="la iu"> <em class="kz">推论</em> </strong>的有力工具。</li></ul><p id="b60b" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">总之，PGM 以联合分布的形式提供了一种基于图形的表示，对 RVs 之间的潜在关联进行了紧凑建模</p><p id="8188" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">我在上面强调了一些术语(对你来说可能是新的)，所以在我们继续之前，让我简单扼要地解释一下，以便更容易保持联系:</p><p id="1684" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated"><strong class="la iu">独立性属性:</strong></p><p id="c1c1" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">让我们从概率的世界来重温一下独立事件的概念。a 和 B 是独立的事件，如果:</p><p id="572d" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">P(A)= P(A | B)；这意味着知道事件 B 的发生并不能告诉我们任何关于事件 a 发生的事情。</p><p id="a336" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">让我们把这个概念推广到随机变量。例如，X 和 Y 是独立的随机变量，如果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/7aeb9d773f4bcaf97b62c9bdd861fffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*oK1MXn7tpYuujPJdCavn8w.png"/></div></figure><p id="2488" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">很好，现在我们理解了独立性，但是在理解一个复杂的结构时，我们如何从中受益呢？</p><ul class=""><li id="0055" class="nb nc it la b lb lc le lf lu nd lv ne lw nf lt ng nh ni nj bi translated">它使我们更容易从系统的其余部分中独立地理解特定属性的特征。话虽如此，但在复杂网络中很少会遇到独立的情况。</li><li id="55d5" class="nb nc it la b lb nk le nl lu nm lv nn lw no lt ng nh ni nj bi translated">因此出现了条件独立的概念，即两个 RV 在观察到第三个 RV 时变得独立。</li><li id="1c72" class="nb nc it la b lb nk le nl lu nm lv nn lw no lt ng nh ni nj bi translated">现在，可以合理地假设条件独立的概念在如何计算和表示网络结构方面带来了显著的节约。</li></ul><p id="62c2" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated"><strong class="la iu">推论:</strong></p><p id="ab1a" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">由于概率图形建模的结果，我们已经了解了属性/RVs 的联合概率分布，最困难和唯一的任务之一是计算一个或多个变量的边际密度和条件密度。</p><p id="0694" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">简而言之，推断是在给定观察变量的状态的情况下，对一组变量(即未观察变量)的条件密度的计算。</p><p id="df88" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated"><strong class="la iu">图形模型的类型:</strong></p><p id="78e4" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">1) <strong class="la iu">贝叶斯网络:</strong>通过有向无环图来表示结构。<strong class="la iu"> <em class="kz"> </em> </strong>这类网络中变量之间概率影响的流动有明确的方向。箭头的存在意味着两个变量之间的因果关系，这由条件概率分布(CPDs)表示。</p><p id="fb02" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated"><strong class="la iu">示例:</strong>开头所示的图形网络代表贝叶斯网络</p><p id="8991" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated"><strong class="la iu"> 2) </strong> <strong class="la iu">马尔可夫网络:</strong>马尔可夫网络中的底层图是无向的。由于这个网络中的边没有任何方向，因此缺少因果关系。因此，我们只能根据<strong class="la iu">相似度</strong>来表示哪个组合更有可能出现，这不一定在 0 和 1 之间，不像贝叶斯网络。</p><p id="cd80" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated"><strong class="la iu">示例:</strong>与示例(开头所示)类似的马尔可夫网络如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7fb4f156dd0b6eba5585dba38c735d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*MG4JUBy1nx_BdnPqxV3e9g.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">马尔可夫网络</p></figure><h2 id="f2a9" class="lx ly it bd lz ma mb dn mc md me dp mf lu mg mh mi lv mj mk ml lw mm mn mo mp bi translated">应用:</h2><p id="4ee7" class="pw-post-body-paragraph kx ky it la b lb mq ju ld le mr jx lg lu ms lj lk lv mt ln lo lw mu lr ls lt im bi translated">PGM 广泛应用于信息提取、语音识别、医疗诊断、计算机视觉等领域。</p><p id="459b" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">还有一个应用，我将使用“pgmpy”演示 PGM 的实现。但在此之前，我打算写一系列的帖子，以清晰的方式解释先决条件的概念。</p><p id="a661" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated">所以，敬请期待，快乐阅读。</p><p id="67a8" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lu li lj lk lv lm ln lo lw lq lr ls lt im bi translated"><strong class="la iu">参考文献:</strong></p><ul class=""><li id="7321" class="nb nc it la b lb lc le lf lu nd lv ne lw nf lt ng nh ni nj bi translated">https://www . coursera . org/specializations/probability-graphical-models</li><li id="fb8f" class="nb nc it la b lb nk le nl lu nm lv nn lw no lt ng nh ni nj bi translated"><a class="ae na" href="https://en.wikipedia.org/wiki/Graphical_model" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Graphical_model</a></li><li id="0550" class="nb nc it la b lb nk le nl lu nm lv nn lw no lt ng nh ni nj bi translated"><a class="ae na" href="http://ai.stanford.edu/~paskin/gm-short-course/lec2.pdf" rel="noopener ugc nofollow" target="_blank">http://ai.stanford.edu/~paskin/gm-short-course/lec2.pdf</a></li></ul></div></div>    
</body>
</html>