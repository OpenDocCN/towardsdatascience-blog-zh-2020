<html>
<head>
<title>Introduction to Squeeze-Excitation Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">压缩激励网络简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-squeeze-excitation-networks-f22ce3a43348?source=collection_archive---------39-----------------------#2020-09-15">https://towardsdatascience.com/introduction-to-squeeze-excitation-networks-f22ce3a43348?source=collection_archive---------39-----------------------#2020-09-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4734" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于注意力的机制来提高你的深度 CNN</h2></div><p id="7406" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">挤压和激励网络(<a class="ae lb" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank"> SENet </a>)是 2017 年<a class="ae lb" href="http://image-net.org/challenges/LSVRC/2017/" rel="noopener ugc nofollow" target="_blank"> Imagenet 分类挑战赛</a>的获胜者，比 2016 年的获胜者相对提高了约 25%。SENets 引入了一个关键的架构单元——挤压和激励模块(SE 模块),这对性能的提升至关重要。SE 模块还可以轻松添加到其他架构中，额外开销很低。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/6bdc598c5090c8d29286c2554f2c5fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xOaHk6yFJpbgchVg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">照片由<a class="ae lb" href="https://unsplash.com/@ricardoviana?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">里卡多·维亚纳</a>在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="bd1a" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">SE 模块介绍</h1><p id="1baf" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">典型地，CNN 通过从空间维度提取信息并将它们存储在信道维度中来工作。这就是为什么当我们在 CNN 中深入时，特征地图的空间维度会缩小，而频道会增加。当考虑一个特定 CNN 层的输出特征图时，所有通道被同等加权。</p><p id="93c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们知道，CNN 中的早期层负责捕捉基本特征，如边缘、拐角和线条，而后期层则捕捉更高级别的特征，如面部和文本。因此，这是有道理的</p><p id="0d85" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SE 块的主要思想是:<strong class="kh ir">基于每个通道的重要程度(挤压)，为特征图的每个通道分配不同的权重(激励)。</strong></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mw"><img src="../Images/2eac181528a5504e81db5ccd7c7e34d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QK1TVTasgdRYpVC31CuPyA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">SE 块可视化。作者创建的图像。</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="bdab" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">技术解释</h1><p id="0a91" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">SE 模块可以分为 3 个主要部分——挤压、计算和激励。在这里，我将更详细地介绍它们。</p><ol class=""><li id="162d" class="mx my iq kh b ki kj kl km ko mz ks na kw nb la nc nd ne nf bi translated"><strong class="kh ir">【挤压】</strong>操作</li></ol><p id="ec9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 CNN 图层的输出要素地图上执行全局平均池。这实质上是在空间维度(H×W)上取所有激活的平均值，给出每个通道一个激活。这样做的结果是一个形状矢量(1 x 1 x C)。</p><p id="74f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.<strong class="kh ir">计算</strong></p><p id="1ff8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来自前一个操作的向量通过两个连续的完全连接的层。这用于完全捕获从空间地图聚集的通道相关。ReLU 激活在第一 FC 层之后执行，而 sigmoid 激活在第二 FC 层之后使用。在该论文中，还有一个缩减率，使得第一 FC 层的中间输出具有较小的尺寸。这一步的最终输出也有一个形状(1 x 1 x C)。</p><p id="0fe5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.<strong class="kh ir">【励磁】</strong>运行</p><p id="7de9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，计算步骤的输出被用作每声道权重调制向量。它只是简单地与大小为(H x W x C)的原始输入特征图相乘。这根据它们的“重要性”缩放每个通道的空间图。</p><p id="a337" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SE 模块可以很容易地与许多现有的 CNN 集成。在这篇论文中，ResNets、VGG 和 Inception 等架构的精度显著提高，但额外的计算成本很低。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="dec8" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">代码示例</h1><p id="6198" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">这里有一个示例代码片段，您可以尝试一下。写于<a class="ae lb" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>。如您所见，添加挤压和激励模块的功能非常简单！在大约 10 行代码中，我们有一个来自<a class="ae lb" href="https://github.com/moskomule/senet.pytorch" rel="noopener ugc nofollow" target="_blank"> moskomule </a>的模块化实现，可以很容易地实现到大多数深度 CNN。</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="07a0" class="nl ma iq nh b gy nm nn l no np">class SELayer(nn.Module):<br/>    def __init__(self, channel, reduction=16):<br/>        super(SELayer, self).__init__()<br/>        self.avg_pool = nn.AdaptiveAvgPool2d(1)<br/>        self.fc = nn.Sequential(<br/>            nn.Linear(channel, channel // reduction, bias=False),<br/>            nn.ReLU(inplace=True),<br/>            nn.Linear(channel // reduction, channel, bias=False),<br/>            nn.Sigmoid()<br/>        )</span><span id="54ad" class="nl ma iq nh b gy nq nn l no np">def forward(self, x):<br/>        b, c, _, _ = x.size()<br/>        y = self.avg_pool(x).view(b, c)<br/>        y = self.fc(y).view(b, c, 1, 1)<br/>        return x * y.expand_as(x)</span></pre><h1 id="3c5a" class="lz ma iq bd mb mc nr me mf mg ns mi mj jw nt jx ml jz nu ka mn kc nv kd mp mq bi translated">结论</h1><p id="559e" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">SE 模块的主要吸引力在于其简单性。仅从图中，我们可以理解挤压-激发过程中涉及的功能和步骤。此外，它们可以添加到模型中，而不会增加太多的计算成本，所以每个人都应该尝试将其集成到他们的深度学习架构中！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="ca52" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">参考</h1><div class="nw nx gp gr ny nz"><a href="https://arxiv.org/abs/1709.01507" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">压缩和激励网络</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">卷积神经网络(CNN)的核心构件是卷积算子，它使网络能够…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://pytorch.org/" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">PyTorch</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">开源深度学习平台，提供从研究原型到生产部署的无缝路径。</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">pytorch.org</p></div></div><div class="oi l"><div class="oj l ok ol om oi on lm nz"/></div></div></a></div></div></div>    
</body>
</html>