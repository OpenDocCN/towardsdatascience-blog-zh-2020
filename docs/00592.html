<html>
<head>
<title>Regularization Method: Noise for improving Deep Learning models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化方法:用于改进深度学习模型的噪声</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/noise-its-not-always-annoying-1bd5f0f240f?source=collection_archive---------16-----------------------#2020-01-17">https://towardsdatascience.com/noise-its-not-always-annoying-1bd5f0f240f?source=collection_archive---------16-----------------------#2020-01-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/77c4d2bbd864fd44bf9246c9284041fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jZiMYBZMQs6tPYvMPh3D9w.png"/></div></div></figure><div class=""/><div class=""><h2 id="de4a" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">将随机噪声引入训练过程</h2></div><p id="80e6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当你开始学习神经网络时，你学到的第一个概念是<a class="ae lp" rel="noopener" target="_blank" href="/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690"> <strong class="kv jf">过拟合</strong>和<strong class="kv jf">欠拟合</strong> </a>的含义。有时，训练一个完美概括数据的模型是一项挑战，尤其是当您有一个小数据集时，因为:</p><ul class=""><li id="ff2c" class="lq lr je kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">当您使用小数据集训练神经网络时，网络通常会记住训练数据集，而不是学习我们数据的一般特征。因此，该模型在训练集上表现良好，在新数据(例如:测试数据集)上表现不佳</li><li id="36e9" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">一个小的数据集不能很好地描述我们的问题，因此，它可能会导致一个难以学习的问题。</li></ul><p id="fc4d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">获取更多的数据是一项非常昂贵和艰巨的任务。然而，有时你可以应用一些技术(正则化方法)来获得更好的模型性能。</p><p id="6a8c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在本文中，我们将重点关注使用<strong class="kv jf">噪声作为神经网络</strong>中的正则化方法。这种技术不仅减少了过度拟合，而且还可以更快地优化我们的模型，提高整体性能。</p><p id="40f3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你可以在我的<a class="ae lp" href="https://github.com/alejandrods/Noise-Regularization-Method-Neural-Network" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中找到完整的代码！:)</p><h1 id="3f1a" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">目标</h1><p id="83d7" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">这一条的目标如下:</p><ul class=""><li id="11ec" class="lq lr je kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">使用 sklearn 生成合成数据</li><li id="71a0" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">正则化方法</li><li id="c5ca" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">训练一个基本的神经网络作为基线(MLP)</li><li id="6bea" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">使用噪波作为正则化方法-输入图层</li><li id="9aa6" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">使用噪波作为正则化方法-隐藏层</li><li id="964c" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">使用噪声作为正则化方法-输入和隐藏层</li><li id="bd9a" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">网格搜索以查找模型最佳性能的值</li></ul><h1 id="d1f2" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">正则化方法</h1><p id="19fb" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">训练一个机器学习模型是一个挑战，它将在以前看不见的输入上表现良好，而不仅仅是那些我们的模型被训练的输入。这个特性被称为<a class="ae lp" href="https://books.google.es/books?id=Np9SDQAAQBAJ&amp;pg=PA107&amp;lpg=PA107&amp;dq=The+central+challenge+in+machine+learning+is+that+our+algorithm+must+perform+well+on+...&amp;source=bl&amp;ots=kROllLy-_Z&amp;sig=ACfU3U1FzdT_Vg1GkcsBupzbmt8YHWQvhw&amp;hl=es&amp;sa=X&amp;ved=2ahUKEwiph6T22YfnAhW9DWMBHVkNDkgQ6AEwAHoECAwQAQ#v=onepage&amp;q=The%20central%20challenge%20in%20machine%20learning%20is%20that%20our%20algorithm%20must%20perform%20well%20on%20...&amp;f=false" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf">泛化</strong> </a> <strong class="kv jf">，</strong>对未观测到的输入表现良好。有一些像<strong class="kv jf">训练测试分割</strong>或<strong class="kv jf">交叉验证</strong>的方法来衡量我们的模型有多一般化。</p><p id="331f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们可以将模型的性能分为三种情况:</p><ul class=""><li id="2e6c" class="lq lr je kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">该模型在训练数据集和新数据上表现不佳— <strong class="kv jf">欠拟合模型。</strong></li><li id="0bf9" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">该模型在训练数据集上表现良好，而在看不见的数据上表现不佳— <strong class="kv jf">过拟合模型。</strong></li><li id="c37c" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">该模型学习我们的训练数据集，并在看不见的数据上表现良好，它能够进行归纳— <strong class="kv jf">良好拟合模型</strong></li></ul><p id="b3cf" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我们的问题中更有可能面临<strong class="kv jf">过拟合模型</strong>因此，在训练期间监控性能以检测其是否过拟合是很重要的。通常在训练过程中绘制<strong class="kv jf">准确度的演变和</strong>损失，以检测常见模式。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/164133ec824d9f7b2cc27243db7ee5f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9v1iVtgseS_dwFHlgZq3og.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">合身、合身和合身</p></figure><p id="2a8a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><a class="ae lp" rel="noopener" target="_blank" href="/regularization-in-machine-learning-76441ddcf99a"> <strong class="kv jf">正则化</strong> </a>是修改我们的学习算法，减少它的泛化误差而不是训练误差。神经网络中最常见的正则化方法有:</p><ul class=""><li id="4f31" class="lq lr je kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated"><a class="ae lp" href="https://medium.com/konvergen/understanding-dropout-ddb60c9f98aa" rel="noopener"> <strong class="kv jf">丢弃</strong> </a>:以概率<em class="nk"> p. </em>在每次迭代中关闭某个神经元</li><li id="b163" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated"><a class="ae lp" href="https://medium.com/swlh/early-stopping-in-polynomial-regression-d1183bd363a7" rel="noopener"> <strong class="kv jf">提前停止</strong> </a> <strong class="kv jf"> : </strong>提供在模型开始溢出之前可以运行多少次迭代的指导。</li><li id="1433" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated"><a class="ae lp" href="https://arxiv.org/abs/1207.0580" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf">权重约束</strong> </a> <strong class="kv jf"> : </strong>将权重缩放到预定义的阈值。</li><li id="437f" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated"><a class="ae lp" href="https://pdfs.semanticscholar.org/d79b/a428e1cf1b8aa5d320a93166315bb30b4765.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf">噪声</strong> </a> <strong class="kv jf"> : </strong>将随机噪声引入训练过程。</li></ul><p id="a509" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这些方法在神经网络中很流行，其中大多数已被证明可以减少过拟合。然而，噪声对深度学习模型的影响从未被系统地研究过，准确性提高的潜在原因也没有。<a class="ae lp" href="https://pdfs.semanticscholar.org/d79b/a428e1cf1b8aa5d320a93166315bb30b4765.pdf" rel="noopener ugc nofollow" target="_blank">上述观察的一个假设是放松一致性将随机噪声引入训练过程</a>。这隐含地减轻了模型的过度拟合，并且更好地概括了模型以分类测试数据。</p><h1 id="7614" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">使用 Sklearn 生成数据</h1><p id="3697" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">我们想了解在过度拟合的神经网络中使用<strong class="kv jf">噪声作为正则化方法</strong>的效果，我们决定使用二元分类问题来解释这一点。因此，我们将应用 sklearn 生成一个二进制数据集，具体来说就是生成 2 个二维同心圆的<code class="fe nl nm nn no b">make_circles</code>。这些参数是:</p><ul class=""><li id="29d8" class="lq lr je kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated"><code class="fe nl nm nn no b">n_samples=100</code>(生成的总点数)</li><li id="57a5" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated"><code class="fe nl nm nn no b">noise=0.09</code>(加到数据上的高斯噪声的标准偏差)</li><li id="ee6e" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated"><code class="fe nl nm nn no b">param_random=24</code>(通过多个函数调用传递一个 int 以获得可再现的输出)</li></ul><figure class="nc nd ne nf gt iv"><div class="bz fp l di"><div class="np nq l"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">生成并绘制数据我们数据</p></figure><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/94f7b9a0382f2c0588f0874fa5db9e55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EPR8k0l72maOBBxSnSq2hA.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">培训用数据</p></figure><p id="c6e1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们需要评估我们的网络的性能，看看我们是否有过度拟合，因此，我们需要分割我们的数据，以生成另一个数据集<code class="fe nl nm nn no b">x_test</code>进行测试。我们将数据分为<code class="fe nl nm nn no b">train_set</code> (30%)和<code class="fe nl nm nn no b">test_set</code> (70%)。</p><p id="f5ae" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">因为我们需要强制过度拟合，所以我们为我们的训练集选择了较小的大小(30%)，因为我们希望创建一个不会概括我们的数据并且在测试数据集上具有较高错误率的神经网络。</p><figure class="nc nd ne nf gt iv"><div class="bz fp l di"><div class="np nq l"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">分割和绘制我们的训练和测试数据</p></figure><p id="32b6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们可以绘制出<code class="fe nl nm nn no b">X_train</code>和<code class="fe nl nm nn no b">X_test</code>的分布图:</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/559c6b8f2e667f0f5182c5ffcbb9799e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7LIyKOFYAI677D5XM51XwA.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">训练和测试集的分布</p></figure><p id="9912" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们选择了这种类型的数据，称为<strong class="kv jf">圆形数据，</strong>，因为这些类是<strong class="kv jf">不可线性分离的</strong>(我们不能使用直线分割我们的数据)。为此，我们需要一个神经网络来解决这个非线性问题。</p><p id="9e59" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">因为我们需要在我们的模型中找到过度拟合，以作为一种正则化方法来研究噪声的影响，所以我们只生成了 100 个样本。这是一个训练神经网络的小样本，它使我们能够过拟合训练。</p><h1 id="003e" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">第一步:基本神经网络——MLP</h1><p id="505e" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">为了研究噪音如何影响我们的训练，我们训练了一个基本的神经网络作为基线。<strong class="kv jf">我们定义了一个多层感知器(MLP) </strong>来解决我们的二元分类问题。</p><p id="7f81" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">第一层是使用<code class="fe nl nm nn no b">400 nodes</code>和<code class="fe nl nm nn no b">relu</code>激活函数的隐藏层。在输出层，我们使用了一个<code class="fe nl nm nn no b">sigmoid</code>,因为我们想要预测 0 或 1 的类值。我们使用<code class="fe nl nm nn no b">binary_crossentropy</code>作为损失(适用于二元分类),使用<code class="fe nl nm nn no b">adam</code>作为优化器。</p><p id="d640" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们为<code class="fe nl nm nn no b">5000 epochs</code>训练神经网络，并使用<code class="fe nl nm nn no b">X_test</code>和<code class="fe nl nm nn no b">y_test</code>作为验证数据。</p><figure class="nc nd ne nf gt iv"><div class="bz fp l di"><div class="np nq l"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">基本神经网络作为基线</p></figure><p id="8f2a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们绘制了一个图表来表示训练集和测试集的准确性和损失。可以观察到，我们的神经网络具有过拟合，因为该图具有过拟合模型的预期形状，<code class="fe nl nm nn no b">test accuracy</code>增加到一点，然后再次开始降低。同时，<code class="fe nl nm nn no b">loss</code>是发散性的。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/e283f115c824cff872dd72cf05ca83d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AtmD-BSesCMh3-7IRUN_7Q.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">基本神经网络训练精度和损失的演变</p></figure><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/b0db3d229ff88d909a207cb82cb2bcfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1tE05un5QppYiGNDo7NIDQ.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">结果基本神经网络</p></figure><p id="377e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们可以观察到，测试机组的<strong class="kv jf">精度</strong>约为<code class="fe nl nm nn no b">train_acc=1</code>，测试机组的<strong class="kv jf">精度约为<code class="fe nl nm nn no b">test_acc=0.5857</code>。它在训练集上表现出比在测试数据集中更好的性能；<strong class="kv jf">这可能是过度合身的迹象。</strong></strong></p><p id="8e42" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，我们将使用 Keras 的<code class="fe nl nm nn no b">Gaussian Noise Layer</code>添加噪声，并比较结果。该层应用加性的零中心高斯噪声，这对于减轻过拟合是有用的。<strong class="kv jf">高斯噪声(GS)是实值输入破坏过程的自然选择。</strong></p><blockquote class="nu"><p id="5150" class="nv nw je bd nx ny nz oa ob oc od lo dk translated"><em class="oe">这个正则化层只在训练时有效。</em></p></blockquote><h1 id="aec3" class="me mf je bd mg mh mi mj mk ml mm mn mo kk of kl mq kn og ko ms kq oh kr mu mv bi translated">但是什么是高斯噪声呢？</h1><p id="e9be" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated"><a class="ae lp" href="https://en.wikipedia.org/wiki/Gaussian_noise" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf">高斯噪声</strong> </a>是概率密度函数(PDF)等于正态分布的统计噪声。它也被称为高斯分布。高斯随机变量𝑧的概率密度函数𝑝由下式给出:</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/0f683d83ea517ce67c65c19668020d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*rFSLx5GUtnomc0XFcBIGKQ.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">概率密度函数</p></figure><p id="62a4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">其中𝑧代表灰度级，𝜇代表平均值，𝜎代表标准差。综上所述，噪声可能呈现的值是高斯分布的。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/f65cc6d48a9168fe54e08b98aec78438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HKue500ETixf7eZUH_iCEw.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">2 个概率密度函数—高斯噪声</p></figure><p id="f29c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了理解高斯噪声的含义，假设我们有一幅图像，并绘制了两个概率密度函数。如果我们观察红色 PDF: <strong class="kv jf">噪声的平均值将是-2 </strong>。因此，平均而言，图像的所有像素将减去 2。但是，如果我们观察橙色的 PDF，<strong class="kv jf">平均值是 3 </strong>。所以平均来说，所有像素加 3。例如，如果我们拍摄这张图像，并应用高斯噪声:</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/eeb0be603c6d7887d353576280ebd6dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0XmVlwWdRPjqLVgn-2QY3g.jpeg"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">左:原始图像—右:带有高斯噪声的图像</p></figure><p id="7129" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们可以检查每个图像的直方图，以评估应用高斯噪声的效果:</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/6b9d70a74142b835a85f4a6005acf394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VJ9IPebZTR_70Pmds_0-mQ.jpeg"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">直方图—左侧:原始图像。右图:带有高斯噪声的图像</p></figure><p id="0fb3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">虽然我们已经用图像解释了高斯噪声，但是在 Keras 中应用<strong class="kv jf">高斯噪声作为正则化方法的方法应用了相同的理论。</strong></p><p id="962e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">添加噪声会增加我们的训练数据集的大小。当我们训练神经网络时，随机噪声被添加到每个训练样本中，这是数据扩充的一种形式。此外，当我们使用噪声时，我们增加了数据的随机性，并且模型不太能够从训练样本中学习，因为它们在每次迭代中都是变化的。<strong class="kv jf">因此，神经网络学习更多的一般特征，并具有更低的泛化误差。</strong></p><p id="293a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当我们应用噪声时，我们在训练样本的附近创建新的样本，因此，输入数据的分布被平滑。这使得神经网络更容易从我们的数据中学习。</p><h1 id="ac2a" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">神经网络中的输入层噪声</h1><p id="5ee0" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">我们将添加一个<strong class="kv jf">高斯噪声层作为输入层</strong>，我们将分析这是否有助于提高泛化性能。当我们添加噪声时，我们会创建更多的样本，并使数据分布更加平滑。</p><figure class="nc nd ne nf gt iv"><div class="bz fp l di"><div class="np nq l"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">具有输入层噪声的神经网络</p></figure><p id="b74a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在线图中可以看出，由于我们在训练中引入的具有噪声的点与训练数据集的点冲突，噪声导致模型的准确性和损失跳跃。我们使用<code class="fe nl nm nn no b">std=0.1</code>作为输入噪声，这可能有点高。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/0d4b6d94250995bfee24cef8806880ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dETiU_z9mLApa5BiMxzcZg.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">精度的演变和训练损失——输入层噪声</p></figure><p id="71c2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">通过在输入层使用噪声作为正则化方法，我们减少了模型中的过拟合，此外，我们还改进了<code class="fe nl nm nn no b">test_accuracy=0.642</code>。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/9a299ecf74418697761e3f96b81d9616.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xsRApZdE_RNem0el-SARQw.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">结果神经网络-输入层噪声</p></figure><h1 id="0279" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">神经网络中的隐含层噪声</h1><p id="e996" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">现在，我们将尝试<strong class="kv jf">用高斯噪声</strong>创建一个隐藏层。这必须在应用激活功能之前完成。我们将使用 0.1 的标准偏差，也是任意选择的。</p><figure class="nc nd ne nf gt iv"><div class="bz fp l di"><div class="np nq l"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">具有隐含层噪声的神经网络</p></figure><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/4e70a82013a84cf30fe9789107bd7ce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8m3oqeh9hxYcUAuV3Vi3qw.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">训练精度的演变和损失——隐含层噪声</p></figure><p id="f116" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这种情况下，可以看到<code class="fe nl nm nn no b">train_accuracy</code>保持不变，尽管我们设法增加了<code class="fe nl nm nn no b">test_accuracy=0.671</code>。似乎将<strong class="kv jf">噪声</strong>添加到我们的模型中允许改进神经网络的训练，并且对于减轻过拟合是有用的。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/eab6195bb346138d0583197e9670a943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X7td1_0ary2UzexaAocqLw.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">结果神经网络—隐藏层噪声</p></figure><h1 id="6d15" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">神经网络中的输入+隐含层噪声</h1><p id="10c9" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">我们结合了前面的两个例子，通过同时添加一个<code class="fe nl nm nn no b">input layer noise</code>和一个<code class="fe nl nm nn no b">hidden layer noise</code>来研究我们的模型的性能。我们将使用 0.1 的标准偏差，也是任意选择的。</p><figure class="nc nd ne nf gt iv"><div class="bz fp l di"><div class="np nq l"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">输入+隐含层噪声的神经网络</p></figure><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/be853ac713cd03f802a239314acf9296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KD2oAnt0G63-uEDtKnauMg.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">训练——输入+隐含层噪声——精度和损失的演变</p></figure><p id="ff1e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">再次可以看到，随着噪声的使用，如<code class="fe nl nm nn no b">input_layer</code>和<code class="fe nl nm nn no b">hidden_layer</code>，同时，我们减少了模型中的过拟合。此外，它增加了<code class="fe nl nm nn no b">test_accuracy=0.6857</code>，似乎<strong class="kv jf">高斯噪声</strong>作为一种正则化方法允许模型更好地概括我们的数据。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/7d07cbc7bf562b68f549a163c250ec1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gmHARlTJVEqwM6GzgVUQLg.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">结果神经网络—输入+隐藏层噪声</p></figure><blockquote class="nu"><p id="e3d8" class="nv nw je bd nx ny op oq or os ot lo dk translated">应用噪声，我们在训练样本附近创建新的样本，从而平滑输入数据的分布。</p></blockquote><h1 id="e839" class="me mf je bd mg mh mi mj mk ml mm mn mo kk of kl mq kn og ko ms kq oh kr mu mv bi translated">网格搜索-噪波图层</h1><p id="97cd" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">我们将开发一个<strong class="kv jf">网格搜索</strong>来找出噪声的确切数量和隐藏层中的节点，从而使我们能够获得性能最佳的模型。</p><p id="aebc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们将使用带有隐藏层噪声的<strong class="kv jf">神经网络</strong>作为网格搜索的例子。我们需要用模型创建一个函数来搜索噪声的最佳值。</p><figure class="nc nd ne nf gt iv"><div class="bz fp l di"><div class="np nq l"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">网格搜索-隐藏图层噪波</p></figure><p id="53c4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们已经创建了一个名为<code class="fe nl nm nn no b">grid_values</code>的字典，其中包含了模型中每个参数的值的范围。最后，我们将模型<code class="fe nl nm nn no b">create_model()</code>插入到名为<code class="fe nl nm nn no b">KerasClassifier</code>的包装器中，该包装器实现了 Scikit-Learn 分类器接口。</p><figure class="nc nd ne nf gt iv"><div class="bz fp l di"><div class="np nq l"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">计算网格搜索</p></figure><pre class="nc nd ne nf gt ou no ov ow aw ox bi"><span id="9598" class="oy mf je no b gy oz pa l pb pc"><strong class="no jf">Best: 0.833333 using {'nodes': 300, 'noise_amount': 0.001} 0.766667 (0.133333) with: {'nodes': 50, 'noise_amount': 0.001} 0.766667 (0.133333) with: {'nodes': 50, 'noise_amount': 0.01} 0.766667 (0.133333) with: {'nodes': 50, 'noise_amount': 0.1} 0.766667 (0.081650) with: {'nodes': 50, 'noise_amount': 0.2} 0.700000 (0.066667) with: {'nodes': 50, 'noise_amount': 0.7} 0.633333 (0.124722) with: {'nodes': 50, 'noise_amount': 1} 0.800000 (0.163299) with: {'nodes': 100, 'noise_amount': 0.001} 0.800000 (0.163299) with: {'nodes': 100, 'noise_amount': 0.01} 0.766667 (0.133333) with: {'nodes': 100, 'noise_amount': 0.1} 0.800000 (0.124722) with: {'nodes': 100, 'noise_amount': 0.2} 0.766667 (0.169967) with: {'nodes': 100, 'noise_amount': 0.7} 0.666667 (0.105409) with: {'nodes': 100, 'noise_amount': 1} 0.833333 (0.149071) with: {'nodes': 300, 'noise_amount': 0.001} 0.800000 (0.163299) with: {'nodes': 300, 'noise_amount': 0.01} 0.766667 (0.133333) with: {'nodes': 300, 'noise_amount': 0.1} 0.833333 (0.105409) with: {'nodes': 300, 'noise_amount': 0.2} 0.666667 (0.105409) with: {'nodes': 300, 'noise_amount': 0.7} 0.633333 (0.124722) with: {'nodes': 300, 'noise_amount': 1} 0.800000 (0.163299) with: {'nodes': 500, 'noise_amount': 0.001} 0.800000 (0.163299) with: {'nodes': 500, 'noise_amount': 0.01} 0.800000 (0.124722) with: {'nodes': 500, 'noise_amount': 0.1} 0.833333 (0.105409) with: {'nodes': 500, 'noise_amount': 0.2} 0.600000 (0.133333) with: {'nodes': 500, 'noise_amount': 0.7} 0.600000 (0.133333) with: {'nodes': 500, 'noise_amount': 1}</strong></span></pre><p id="3ec9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们可以看到，使用在隐藏层中具有<code class="fe nl nm nn no b">300 neurons</code>和具有大约 83%精确度的<code class="fe nl nm nn no b">noise_amount=0.001</code>的网络获得了最好的结果。</p><h1 id="083b" class="me mf je bd mg mh mi mj mk ml mm mn mo kk mp kl mq kn mr ko ms kq mt kr mu mv bi translated">未来的实验</h1><p id="38fc" class="pw-post-body-paragraph kt ku je kv b kw mw kf ky kz mx ki lb lc my le lf lg mz li lj lk na lm ln lo im bi translated">我们可以用下面的想法来改进这个添加噪声作为正则化方法的实验:</p><ul class=""><li id="e57a" class="lq lr je kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">用噪波添加更多的层来研究他的效果。</li><li id="e84a" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">用更深层次的神经网络重复同样的实验。</li><li id="4b59" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">在没有过度拟合的情况下，在模型中研究噪声作为正则化方法的影响。</li><li id="4b7e" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">尝试在激活和权重中添加噪声。</li></ul><p id="4a08" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">欢迎建议和评论。关注我，感谢你的阅读！:)</p></div></div>    
</body>
</html>