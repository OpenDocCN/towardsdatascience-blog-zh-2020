<html>
<head>
<title>Monte Carlo Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">蒙特卡罗方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e?source=collection_archive---------13-----------------------#2020-07-21">https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e?source=collection_archive---------13-----------------------#2020-07-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="15d7" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/deep-r-l-explained" rel="noopener" target="_blank">深度强化学习讲解— 13 </a></h2><div class=""/><div class=""><h2 id="1f8e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">探索-解释困境</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fa0f6596bcb8f7d913d0517d5a751d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDhcpkgdzkisARJU93i5xw.png"/></div></div></figure><p id="8351" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在<em class="lz"/><a class="ae ma" rel="noopener" target="_blank" href="/value-iteration-for-q-function-ac9e508d85bd"><em class="lz">深度强化学习讲解</em></a><em class="lz"/>系列的这篇新帖中，我们将介绍另一种经典的强化学习方法来估计一个策略π的值。最直接的方法是运行几集，收集数百条轨迹，然后计算每个州的平均值。这种估计价值函数的方法叫做<strong class="lf jd">蒙特卡罗预测</strong> (MC)。</p><p id="d62a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在本帖中，我们还将介绍如何评估最优策略和<strong class="lf jd">勘探-开采困境</strong>。</p><blockquote class="mb mc md"><p id="b7f6" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><a class="ae ma" href="https://medium.com/aprendizaje-por-refuerzo/5-evaluaci%C3%B3n-de-pol%C3%ADticas-con-monte-carlo-a6d70d1db7d4" rel="noopener">本出版物的西班牙语版本</a></p></blockquote><div class="mh mi gp gr mj mk"><a href="https://medium.com/aprendizaje-por-refuerzo/5-evaluaci%C3%B3n-de-pol%C3%ADticas-con-monte-carlo-a6d70d1db7d4" rel="noopener follow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">5.蒙特卡洛政治评估</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">请访问第 5 页的自由介绍</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">medium.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="a5b9" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">蒙特卡洛与动态规划</h1><p id="f89b" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">在本系列的第 1 部分中，我们介绍了一种 MDP 的解决方案，称为动态编程，由理查德·贝尔曼首创。记住，贝尔曼方程允许我们递归地定义值函数，并且可以用值迭代算法求解。总而言之，动态编程为强化学习提供了基础，但我们需要在每次迭代中循环遍历所有状态(它们的大小可以呈指数级增长，状态空间可以非常大，也可以无限大)。动态编程还需要环境的模型，特别是知道状态转移概率<em class="lz">p(s′，r|s，a) </em>。</p><p id="649b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">相比之下，蒙特卡罗方法都是从经验中学习。任何期望值都可以通过样本均值来近似——换句话说，我们需要做的就是播放一堆剧集，收集回报，然后取平均值。蒙特卡罗方法实际上是基本算法的一组替代方案。这些仅适用于偶发任务，当代理遇到终止状态时，交互停止。也就是说，我们假设体验被分成几集，并且不管选择什么动作，所有的集最终都会终止。</p><blockquote class="mb mc md"><p id="0b17" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">重要的是要注意，蒙特卡罗方法只给我们遇到的状态和动作一个值，如果我们从未遇到一个状态，它的值是未知的。</p></blockquote><h1 id="40a2" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">蒙特卡罗方法</h1><blockquote class="mb mc md"><p id="789e" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">这篇文章将提供一个用于强化学习的蒙特卡罗的实用方法。关于这些方法的更正式的解释，我邀请读者阅读理查德·萨顿和安德鲁·巴尔托的教科书<a class="ae ma" href="http://www.incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">强化学习:简介</strong> </a>的第五章。</p></blockquote><p id="5221" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回想一下，<strong class="lf jd">最优策略</strong><strong class="lf jd">【π∫</strong>规定了，对于每个环境状态<strong class="lf jd"><em class="lz"/></strong>，代理应该如何选择一个行动<strong class="lf jd"><em class="lz"/></strong>来实现其最大化报酬<strong class="lf jd"><em class="lz"/></strong>的目标。我们还了解到，代理可以通过首先估计<strong class="lf jd">最优行动值函数</strong><em class="lz">q</em>∑来构建对最优策略的搜索；那么一旦<em class="lz">q</em>∫已知，就很快得到<em class="lz">π</em>∫。</p><p id="b5be" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">代理开始采取一个基本策略，像等概率随机策略<strong class="lf jd">，</strong>一个随机策略，代理从每个状态中随机选择一组可用的动作，每个动作以等概率被选择。代理使用当前策略π将与环境交互以收集一些情节，然后合并结果以得出更好的策略。</p><p id="eb5a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">方法是用一个我们称之为<strong class="lf jd"> Q 表</strong>的表来估算动作值函数。蒙特卡罗方法中的这个核心表为每个状态提供了一行，为每个动作提供了一列。对应于状态<em class="lz"> s </em>和动作<em class="lz"> a </em>的条目表示为<em class="lz"> Q </em> ( <em class="lz"> s </em>，<em class="lz"> a </em>)。</p><p id="00d2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们称之为预测问题:<strong class="lf jd"> </strong> <em class="lz">给定一个策略，代理如何估计该策略的价值函数？</em>。我们将预测问题的蒙特卡罗(MC)方法称为<strong class="lf jd"> MC 预测方法</strong>。</p><blockquote class="mb mc md"><p id="450e" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">我们将把我们的解释集中于动作值函数 Q，但是 MC 也可以用于估计状态值函数 v。</p></blockquote><p id="5f62" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在用于 MC 预测的算法中，我们通过收集策略的许多片段开始。然后，我们注意到 Q 表中的每个条目对应于一个特定的状态和动作。为了填充 Q-table 的一个条目，我们使用当代理处于那个状态并选择动作时跟随的返回。</p><p id="8250" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将一集中状态的每次出现定义为对该状态-动作对的<strong class="lf jd">访问</strong>。在一集里，一个状态-动作对可能被访问不止一次。这导致我们有两个版本的 MC 预测算法:</p><ul class=""><li id="6534" class="nw nx it lf b lg lh lj lk lm ny lq nz lu oa ly ob oc od oe bi translated"><strong class="lf jd">每次访问 MC 预测</strong>:在所有事件中，对每个状态-动作对的所有访问后的回报进行平均。</li><li id="b3fb" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated"><strong class="lf jd">初访 MC 预测</strong>:每一集，我们只考虑初访状态-动作对。</li></ul><p id="4ef9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首次访问方法和每次访问都被认为是保证收敛到真正的行动价值函数。</p><p id="1016" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在本帖中，我们将对 OpenAI 健身房的工作示例进行首次访问:<a class="ae ma" href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py" rel="noopener ugc nofollow" target="_blank">21 点环境</a>。但是实际上，在这个例子中，首次访问和每次访问 MC 返回相同的结果。注意，同一个状态不会在一个情节中重复出现，所以在首次访问和每次访问 MC 方法之间没有区别。首次就诊 MC 预测的伪代码如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/9c78dccbfc11012c97d6521e883ee5de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bwwT1Jk-fz-n18m9iw9qRw.png"/></div></div></figure><p id="0c9d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在这个伪代码中，变量<em class="lz"> num </em> _ <em class="lz">剧集</em>表示代理收集的剧集数，有三个相关的表格:</p><ul class=""><li id="59cf" class="nw nx it lf b lg lh lj lk lm ny lq nz lu oa ly ob oc od oe bi translated"><em class="lz"> Q </em>:一个 Q 表，每个状态一行，每个动作一列。</li><li id="a8d1" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated"><em class="lz"> N </em>:记录我们对每个状态-动作对的第一次访问次数的表格。</li><li id="71e6" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated"><em class="lz"> returns </em> _ <em class="lz"> sum:一个</em>表，记录第一次访问每个状态-动作对后获得的奖励的总和。</li></ul><p id="a62d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在每一集之后，<em class="lz"> N </em>和<em class="lz">返回</em> _ <em class="lz"> sum </em>表被更新以存储该集包含的信息。在收集了所有剧集之后，完成对<em class="lz">Q(</em><em class="lz"/>Q 表)的最终估计。</p><h1 id="693f" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">案例研究:21 点</h1><p id="854c" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">为了更好地理解蒙特卡罗在实践中是如何工作的，让我们使用 OpenAI 的健身房环境<code class="fe ol om on oo b">Blackjack-v0</code>对 21 点游戏执行一步一步的编码。</p><h2 id="cfe4" class="op na it bd nb oq or dn nf os ot dp nj lm ou ov nl lq ow ox nn lu oy oz np iz bi translated">二十一点规则</h2><p id="27ea" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">首先，让我们定义游戏的规则和条件:</p><ul class=""><li id="f554" class="nw nx it lf b lg lh lj lk lm ny lq nz lu oa ly ob oc od oe bi translated">流行的赌场纸牌游戏 21 点的目标是获得数值总和尽可能大而不超过 21 的纸牌。超过 21 导致破产，而双方都有 21 导致平局。</li><li id="9039" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated">我们考虑一个简化的版本，其中每个玩家独立地与庄家竞争。这意味着我们将只和庄家对打，没有其他玩家参与。</li><li id="50ce" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated">数字卡的价值是按面值计算的。牌 J、K 和 Q 的值是 10。ace 的值可以是 1 或 11，取决于玩家的选择。</li><li id="1fd2" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated">双方，玩家和庄家，都有两张牌。玩家的两张牌面朝上，而庄家的一张牌面朝上。在玩家看到他们的牌和庄家的第一张牌后，玩家可以选择击打或站立，直到他对自己的总和感到满意，之后他将站立。</li><li id="961f" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated">然后，发牌者展示他们的第二张牌——如果总数小于 17，他们将继续抽牌，直到达到 17，之后他们将站着。</li></ul><h2 id="0bb6" class="op na it bd nb oq or dn nf os ot dp nj lm ou ov nl lq ow ox nn lu oy oz np iz bi translated">开放式健身房:21 点环境</h2><p id="ad37" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">我们将使用 OpenAI 的健身房环境<code class="fe ol om on oo b"><a class="ae ma" href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py" rel="noopener ugc nofollow" target="_blank">Blackjack-v0</a></code>。每个状态是一个三元组:</p><ul class=""><li id="5c27" class="nw nx it lf b lg lh lj lk lm ny lq nz lu oa ly ob oc od oe bi translated">玩家目前的总和∈{0，1，…，31}</li><li id="6943" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated">庄家的面朝上的牌∈{1，…，10}</li><li id="98f0" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated">无论玩家是否有可用的 a。</li></ul><p id="0c23" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">代理有两个潜在的操作:</p><ul class=""><li id="0332" class="nw nx it lf b lg lh lj lk lm ny lq nz lu oa ly ob oc od oe bi translated"><em class="lz">棍</em>(动作<code class="fe ol om on oo b">0</code>):不再取牌(也称“立”或“留”)。</li><li id="0489" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated"><em class="lz">击</em>(动作<code class="fe ol om on oo b">1</code>):从庄家那里再拿一张牌。</li></ul><p id="9bc8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">21 点的每一局都是一集。赢、输和抽牌的奖励分别为+1、1 和 0。一个游戏内所有奖励都是 0，我们不打折(gamma = 1)；因此，这些最终的回报也是回报。</p><blockquote class="mb mc md"><p id="4784" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">这篇文章的全部代码可以在 GitHub 上找到，而<a class="ae ma" href="https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_13_Monte_Carlo.ipynb" rel="noopener ugc nofollow" target="_blank">可以通过这个链接作为一个谷歌笔记本来运行。</a></p></blockquote><p id="3d46" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们从导入必要的包开始:</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="3cef" class="op na it oo b gy pe pf l pg ph">import sys<br/>import gym<br/>import numpy as np<br/>from collections import defaultdict</span><span id="3f1b" class="op na it oo b gy pi pf l pg ph">env = gym.make('Blackjack-v0')<br/>print(env.observation_space)<br/>print(env.action_space)</span></pre><p id="eb20" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以看到，在环境中有 704 种不同的状态，对应于 32 乘以 11 乘以 2，并且有两种可能的动作对应于选择坚持或击打:</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="2f39" class="op na it oo b gy pe pf l pg ph">Tuple(Discrete(32), Discrete(11), Discrete(2)) <br/>Discrete(2)</span></pre><p id="1614" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了在每集的每个时间步骤中查看示例游戏，代理与环境进行交互，我们可以运行以下代码(多次):</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="31cd" class="op na it oo b gy pe pf l pg ph">state = env.reset()<br/>while True:<br/>       print(state)<br/>       action = env.action_space.sample()<br/>       state, reward, done, info = env.step(action)<br/>       if done:<br/>          if reward &gt; 0: <br/>             print('Reward: ', reward, '(Player won)\n')<br/>          else: <br/>             print('Reward: ', reward, '(Player lost)\n')<br/>          break</span></pre><p id="0047" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">示例游戏的一个例子是:</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="3aad" class="op na it oo b gy pe pf l pg ph">(21, 4, True) <br/>(13, 4, False) <br/>(14, 4, False) <br/>Reward:  -1.0</span></pre><p id="c504" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">经纪人输掉比赛的地方。我建议读者运行这段代码几次，看看不同的剧本。</p><h1 id="42cf" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">一种简单的 MC 预测方法实现</h1><p id="afb1" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">考虑这样一个策略，玩家只根据她/他当前的分数来决定一个动作。例如，如果我们拥有的卡的总数是 18 或更少，我们认为如果我们要求一辆新的汽车可能是没问题的。我们有 75%的可能性做到这一点。如果卡的总数大于 18，我们认为接受一张新卡太危险了，我们不会以 75%的概率这样做。</p><p id="f3ca" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">总之，如果总和大于 18，我们以 75%的概率选择动作<em class="lz">棒</em>;并且，如果总和等于或小于 18，我们选择动作<em class="lz">以 75%的概率命中</em>。</p><p id="3c20" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请注意，在第一种策略中，我们忽略了状态中的一些信息，例如，庄家的面朝上的牌或我们是否有可用的 a。这是为了简化示例中一段重点代码的解释。以下函数<code class="fe ol om on oo b">generate_episode</code>使用该策略对情节进行采样:</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="eeef" class="op na it oo b gy pe pf l pg ph">def generate_episode(env):<br/>    episode = []<br/>    state = env.reset() <br/>    while True:<br/>        probs = [0.75, 0.25] if state[0] &gt; 18 else [0.25, 0.75]<br/>        action = np.random.choice(np.arange(2), p=probs)<br/>        next_state, reward, done, info = env.step(action)<br/>        episode.append((state, action, reward))<br/>        state = next_state<br/>        if done:<br/>           break<br/>        return episode</span></pre><p id="cadc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这段代码返回一个<code class="fe ol om on oo b">episode</code>(使用策略<strong class="lf jd"> π </strong>决定)作为元组的(状态、动作、回报)元组列表。<code class="fe ol om on oo b">episode[i][0]</code>、<code class="fe ol om on oo b">episode[i][1]</code>、<code class="fe ol om on oo b">episode[i][2]</code>分别对应时步𝑖的状态、时步<em class="lz"> i </em>的动作、时步<em class="lz"> 𝑖+1 </em>的奖励。</p><p id="9021" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了玩(10 个游戏)，我们可以如下执行前面的功能:</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="ef80" class="op na it oo b gy pe pf l pg ph">for i in range(10):<br/>    print(generate_episode(env))</span></pre><p id="9429" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">输出将是(在我的运行中):</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="b746" class="op na it oo b gy pe pf l pg ph">[((19, 3, True), 1, 0.0), ((19, 3, False), 0, 1.0)] <br/>[((14, 5, False), 1, -1.0)] <br/>[((15, 10, False), 0, -1.0)] <br/>[((15, 10, False), 1, 0.0), ((17, 10, False), 1, -1.0)] <br/>[((13, 2, False), 1, -1.0)] <br/>[((13, 4, False), 1, 0.0), ((14, 4, False), 0, -1.0)] <br/>[((11, 10, False), 1, 0.0), ((21, 10, False), 0, 0.0)] <br/>[((15, 10, False), 1, -1.0)] <br/>[((9, 9, False), 1, 0.0), ((16, 9, False), 0, 1.0)] <br/>[((13, 7, False), 1, 0.0), ((18, 7, False), 1, 0.0), ((21, 7, False), 1, -1.0)]</span></pre><p id="ae0d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">奖励只有在游戏结束时才会收到，如果我们赢了就是<code class="fe ol om on oo b">1.0</code>，如果我们输了就是<code class="fe ol om on oo b">-1.0</code>。我们看到有时我们赢了，有时我们输了。</p><p id="26df" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们开始根据前面的伪代码编写一个代码。首先，我们需要初始化字典<code class="fe ol om on oo b">N</code>、<code class="fe ol om on oo b">return_sum</code>和<code class="fe ol om on oo b">Q</code>:</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="e4dd" class="op na it oo b gy pe pf l pg ph">N = defaultdict(lambda: np.zeros(env.action_space.n))<br/>returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))<br/>Q = defaultdict(lambda: np.zeros(env.action_space.n))</span></pre><p id="fd42" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">接下来，该算法在使用所提供的使用该策略的函数<code class="fe ol om on oo b">generate_episode</code>生成的剧集上循环。每一集都将是状态、动作和奖励元组的列表。然后，我们使用 zip 命令将状态、动作和奖励分成不同的量:</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="34b3" class="op na it oo b gy pe pf l pg ph">episode = generate_episode(env)<br/>states, actions, rewards = zip(*episode)</span></pre><p id="4f65" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们回到 de 伪代码，看看我们在时间步长上循环，查看每个时间步长对应的状态动作对。如果这是我们第一次访问该对，我们将表<code class="fe ol om on oo b">N</code>的相应位置增加 1，并将此时步的返回添加到表<code class="fe ol om on oo b">return_sum</code>的相应条目中。</p><p id="baad" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请记住，在这个 21 点的例子中，第一次访问和每次访问 MC 预测是等效的，因此我们将对每个时间步长进行更新。然后，一旦我们有了<code class="fe ol om on oo b">return_sum</code>和<code class="fe ol om on oo b">N</code>的相应更新值，我们就可以用它们来更新我们的<code class="fe ol om on oo b">Q</code>估算表。这部分的代码如下:</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="c979" class="op na it oo b gy pe pf l pg ph">discounts = np.array([gamma**i for i in range(len(rewards)+1)])</span><span id="1558" class="op na it oo b gy pi pf l pg ph">for i, state in enumerate(states):<br/>    returns_sum[state][actions[i]] += <br/>               sum(rewards[i:]*discounts[:-(1+i)])<br/>    N[state][actions[i]] += 1.0<br/>    Q[state][actions[i]] = returns_sum[state][actions[i]] <br/>               / N[state][actions[i]]</span></pre><p id="1c66" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们的 MC 预测方法的完整代码如下:</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="cda1" class="op na it oo b gy pe pf l pg ph">def mc_prediction(env, num_episodes, generate_episode, gamma=1.0):<br/>   <br/>   returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))<br/>   N = defaultdict(lambda: np.zeros(env.action_space.n))<br/>   Q = defaultdict(lambda: np.zeros(env.action_space.n))</span><span id="880a" class="op na it oo b gy pi pf l pg ph">for episode in range(1, num_episodes+1):<br/>      episode = generate_episode(env)<br/>      states, actions, rewards = zip(*episode)</span><span id="50ec" class="op na it oo b gy pi pf l pg ph">discounts = np.array([gamma**i for i in <br/>                           range(len(rewards)+1)])</span><span id="954b" class="op na it oo b gy pi pf l pg ph">for i, state in enumerate(states):<br/>             returns_sum[state][actions[i]] += <br/>                        sum(rewards[i:]*discounts[:-(1+i)])<br/>             N[state][actions[i]] += 1.0<br/>             Q[state][actions[i]] = returns_sum[state][actions[i]] <br/>                      / N[state][actions[i]]<br/>   return Q</span></pre><p id="a943" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该算法将 OpenAI 健身房环境的实例、生成的剧集数量以及折扣率(默认值<code class="fe ol om on oo b">1</code>)作为参数。该算法返回作为输出的 Q 表(动作值函数的估计)，一个字典(一维数组)。</p><p id="c3ab" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">绘制相应的状态值函数，看看哪些状态值更大，会很有意思。我们可以使用这个简单的代码从 Q-table 中完成，它通过根据我们如何定义问题来加权每个动作的值来获得状态的值:</p><pre class="ks kt ku kv gt pa oo pb pc aw pd bi"><span id="d851" class="op na it oo b gy pe pf l pg ph">num_episodes=1000000</span><span id="1e46" class="op na it oo b gy pi pf l pg ph">Q = mc_prediction(env, num_episodes, generate_episode)</span><span id="f4ef" class="op na it oo b gy pi pf l pg ph">State_Value_table={}<br/>for state, actions in Q.items():<br/>           State_Value_table[state]= <br/>                 (state[0]&gt;18)*(np.dot([0.75, 0.25],actions)) +<br/>                 (state[0]&lt;=18)*(np.dot([0.75, 0.25],actions))</span></pre><p id="346d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以用从 Udacity 借来的代码<a class="ae ma" href="https://github.com/udacity/deep-reinforcement-learning/blob/master/monte-carlo/plot_utils.py" rel="noopener ugc nofollow" target="_blank">来绘制这个图。有两个图对应于我们是否有可用的 a 或者我们没有可用的 a。但是在这两种情况下，我们看到最高的状态值对应于当玩家和是 20 或 21 的时候，这看起来很明显，因为在这种情况下我们最有可能赢得游戏。</a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/0a972a885fd55eccffbe2002048c6b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jJHRNUzWcilPehhCSN7tGA.png"/></div></div><p class="pk pl gj gh gi pm pn bd b be z dk translated">(来源:作者)</p></figure><h1 id="686d" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">探索与开发</h1><p id="6548" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">到目前为止，我们已经了解了一个代理如何采用一个像等概率随机策略这样的策略，使用它来与环境交互，然后使用该经验来填充相应的 Q 表，该 Q 表成为该策略的动作值函数的估计。所以，现在的问题是，我们如何利用这一点来寻找最优策略？</p><h2 id="0032" class="op na it bd nb oq or dn nf os ot dp nj lm ou ov nl lq ow ox nn lu oy oz np iz bi translated">贪婪的政策</h2><p id="491f" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">为了得到一个更好的政策，不一定是最优的，我们只需要为每个状态选择最大化 Q 表的行动。让我们称这个新政策为<strong class="lf jd">【π】</strong>。当我们拿一个 Q 表，并使用最大化每一行 <strong class="lf jd">的动作<strong class="lf jd">来提出策略</strong>时，我们说我们正在构建关于 Q 表的<strong class="lf jd">贪婪</strong>的策略。</strong></p><p id="1d40" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">通常将所选动作称为<strong class="lf jd">贪婪动作</strong>。在有限 MDP 的情况下，作用值函数估计用 Q 表表示。然后，为了得到贪婪动作，对于表中的每一行，我们只需要选择对应于最大化该行的列的动作。</p><h2 id="0ca3" class="op na it bd nb oq or dn nf os ot dp nj lm ou ov nl lq ow ox nn lu oy oz np iz bi translated">ε-贪婪政策</h2><p id="7b23" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">然而，不是总是构造一个贪婪策略(总是选择贪婪动作),而是构造一个所谓的<strong class="lf jd">ε贪婪策略</strong>,它最有可能选择贪婪动作，但也有很小但非零的概率选择其他动作。在这种情况下，使用一定在零和一之间的某个小正数ε<em class="lz">ϵ</em>。这是有动机的，正如我们将在后面更详细地解释的那样，由于代理必须找到一种方法来平衡基于他们当前知识的最佳行为的驱动和获取知识以获得更好的未来行为的需要。</p><h2 id="8de0" class="op na it bd nb oq or dn nf os ot dp nj lm ou ov nl lq ow ox nn lu oy oz np iz bi translated">蒙特卡洛控制</h2><p id="9487" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">我们已经了解了代理如何采用策略<em class="lz"> π </em>，使用它与环境进行多次交互，然后使用结果通过 Q 表来估计动作值函数。一旦 q 表非常接近动作值函数，代理就可以构造策略<em class="lz"> π </em>，即<em class="lz">ϵ</em>——相对于 q 表是贪婪的，这将产生比原始策略<em class="lz"> π </em>更好的策略。然后我们可以改进这个策略，把它变成ϵ的贪婪策略。</p><p id="d6e4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因此，如果代理人在这两个步骤之间反复交替，直到我们得到越来越好的策略，希望我们收敛到最优策略，我们最终会得到最优策略<em class="lz">π</em>∫:</p><ul class=""><li id="9b97" class="nw nx it lf b lg lh lj lk lm ny lq nz lu oa ly ob oc od oe bi translated"><strong class="lf jd">步骤 1 </strong>:使用策略<em class="lz"> π </em>构建 Q 表，以及</li><li id="84d2" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated"><strong class="lf jd">第二步</strong>:将策略修改为<em class="lz">ϵ</em>——贪婪关于 q 表(标注为<strong class="lf jd"><em class="lz"/></strong>)。</li></ul><p id="bd81" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">只要我们运行足够长的时间，这个被提议的算法是如此接近给我们最优策略。</p><p id="42c7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们把这种算法称为<strong class="lf jd">蒙特卡罗控制方法</strong>，用来估计最优策略。通常将<strong class="lf jd">步骤 1 </strong>称为<strong class="lf jd">策略评估</strong>，因为它用于确定策略的动作<strong class="lf jd">值</strong>函数。同样，由于<strong class="lf jd">步骤 2 </strong>用于<strong class="lf jd">改进</strong>策略，我们也将其称为<strong class="lf jd">策略改进</strong>步骤。示意性地，我们可以将其表示为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi po"><img src="../Images/be58859e4f4711fa6e9c2f39b4a22780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e7IS7WkHiFDqHD8Vfm0YyQ.png"/></div></div></figure><p id="49a9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因此，使用这个新术语，我们可以总结出，我们的<strong class="lf jd">蒙特卡罗控制方法</strong>在<strong class="lf jd">策略评估</strong>和<strong class="lf jd">策略改进</strong>步骤之间交替进行，以找到最优策略π∫:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pp"><img src="../Images/7560d9c1e4770c6697f95c51c402642d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S7lcRp4ptxQ4q73d7RdIug.png"/></div></div></figure><p id="a3d3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其中箭头中的“E”表示完整的政策评估,“I”表示完整的政策改进。在下一篇文章中，我们将展示蒙特卡罗控制算法的实现。</p><p id="da5f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然而，一些问题出现了，在构造<em class="lz"> ϵ </em>贪婪策略时，如何设置<em class="lz"> ϵ </em>的值？在下一节中，我们将看到如何实现。</p><h1 id="b6d0" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">勘探开发困境</h1><p id="5a18" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">回想一下，我们的代理最初不知道环境的动态，由于目标是最大化回报，代理必须通过交互来了解环境。然后，在每一个时间步，当代理选择一个动作时，它根据过去对环境的经验作出决定。而且，本能可能是选择基于过去经验的<em class="lz"/><em class="lz"/>的行动，将获得最大回报。正如我们在上一节中所讨论的，这种对动作值函数估计贪婪的策略很容易导致收敛到次优策略。</p><h2 id="a7be" class="op na it bd nb oq or dn nf os ot dp nj lm ou ov nl lq ow ox nn lu oy oz np iz bi translated">平衡开发与勘探</h2><p id="9fee" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">这可能会发生，因为在早期，代理人的知识非常有限，并拒绝考虑就未来回报而言比已知行为更好的非贪婪行为。这意味着一个成功的代理人不可能在每个时间点都贪婪地行动；相反，为了发现最优策略，它必须继续改进所有状态-动作对的估计回报。但同时保持一定程度的贪婪行动，以尽快维持回报最大化的目标。这激发了我们之前提出的ϵ贪婪政策的想法。</p><p id="baac" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将平衡这两个竞争需求的需求称为<strong class="lf jd">探索-开发困境</strong>，其中代理必须找到一种方法来平衡基于其当前知识的最佳行为驱动(<strong class="lf jd">开发</strong>)和获取知识以获得更好判断的需求(<strong class="lf jd">探索</strong>)。</p><h2 id="efc6" class="op na it bd nb oq or dn nf os ot dp nj lm ou ov nl lq ow ox nn lu oy oz np iz bi translated">设置ε的值</h2><p id="98bf" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">这种困境的一个潜在解决方案是通过在构建<em class="lz"> ϵ </em>贪婪策略时逐渐修改<em class="lz"> ϵ </em>的值来实现的。对于代理人来说，通过选择<strong class="lf jd">探索</strong>而不是<strong class="lf jd">开发</strong>尝试<strong class="lf jd"> </strong>各种策略来最大化回报，开始与环境的互动是有意义的。考虑到这一点，最好的开始策略是等概率随机策略，因为它同样有可能探索每个状态的所有可能行为。设置<em class="lz"> ϵ </em> =1 产生一个<em class="lz">ϵ</em>-贪婪策略，它等价于等概率随机策略。</p><p id="4dd4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在随后的时间步骤中，促进<strong class="lf jd">开发</strong>而不是<strong class="lf jd">勘探</strong>是有意义的，在这种情况下，政策相对于行动价值函数估计逐渐变得更加贪婪。设置<em class="lz"> ϵ </em> =0 产生贪婪策略。已经表明，最初倾向于通过开发进行勘探，并逐渐倾向于开发而不是勘探是一种最佳策略。</p><p id="50b1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了保证 MC 控制收敛到最优策略<em class="lz">π</em>∑，我们需要保证满足两个条件:每个状态-动作对被访问无限多次，并且该策略收敛到关于动作-值函数估计<em class="lz"> Q 的贪婪策略</em>我们将这些条件称为<strong class="lf jd">无限探索极限中的贪婪</strong>以确保代理在所有时间步继续探索，并且代理逐渐探索更多和探索更少。</p><p id="a4e5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">满足这些条件的一种方法是修改<em class="lz"> ϵ </em>的值，当指定<em class="lz"> ϵ </em>贪婪策略时，使其逐渐衰减。然而，在设定<em class="lz"> ϵ.的衰减率时必须非常小心</em>确定最佳衰变不是小事，需要一点炼金术，也就是经验。我们将在下一篇文章中看到实现的例子。</p><h1 id="93ef" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">下一步是什么？</h1><p id="1828" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">到目前为止，在我们当前的蒙特卡罗控制算法中，我们收集了大量的片段来构建 Q 表。然后，在 Q 表中的值收敛之后，我们使用该表来提出改进的策略。然而，蒙特卡罗预测方法<strong class="lf jd">可以在逐集的基础上逐步实施</strong>。<a class="ae ma" rel="noopener" target="_blank" href="/mc-control-methods-50c018271553">在下一篇文章</a>中，我们将介绍如何按照这个想法构建更好的 MC 控制算法。下期帖子再见！</p></div><div class="ab cl pq pr hx ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="im in io ip iq"><h1 id="7fb6" class="mz na it bd nb nc px ne nf ng py ni nj ki pz kj nl kl qa km nn ko qb kp np nq bi translated">深度强化学习讲解系列</h1><p id="c09a" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated"><strong class="lf jd">由</strong> <a class="ae ma" href="https://www.upc.edu/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> UPC 巴塞罗那理工大学</strong> </a> <strong class="lf jd">和</strong> <a class="ae ma" href="https://www.bsc.es/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">巴塞罗那超级计算中心</strong> </a></p><p id="4cf4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一个轻松的介绍性<a class="ae ma" href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener ugc nofollow" target="_blank">系列</a>以一种实用的方式逐渐向读者介绍这项令人兴奋的技术，它是人工智能领域最新突破性进展的真正推动者。</p><div class="mh mi gp gr mj mk"><a href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">深度强化学习解释-乔迪托雷斯。人工智能</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">本系列的内容</h3></div></div><div class="mt l"><div class="qc l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="89c0" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">关于这个系列</h1><p id="87a4" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">我在五月开始写这个系列，在巴塞罗那的<strong class="lf jd">封锁期。</strong>老实说，由于封锁，在业余时间写这些帖子帮助了我<a class="ae ma" href="https://twitter.com/hashtag/StayAtHome?src=hashtag_click" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> #StayAtHome </strong> </a>。感谢您当年阅读这份刊物；它证明了我所做的努力。</p><p id="74d2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">免责声明</strong> —这些帖子是在巴塞罗纳封锁期间写的，目的是分散个人注意力和传播科学知识，以防对某人有所帮助，但不是为了成为 DRL 地区的学术参考文献。如果读者需要更严谨的文档，本系列的最后一篇文章提供了大量的学术资源和书籍供读者参考。作者意识到这一系列的帖子可能包含一些错误，如果目的是一个学术文件，则需要对英文文本进行修订以改进它。但是，尽管作者想提高内容的数量和质量，他的职业承诺并没有留给他这样做的自由时间。然而，作者同意提炼所有那些读者可以尽快报告的错误。</p></div></div>    
</body>
</html>