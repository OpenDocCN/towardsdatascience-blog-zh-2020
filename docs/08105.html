<html>
<head>
<title>Can you remove 99% of a neural network without losing accuracy?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你能移除99%的神经网络而不损失准确性吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/can-you-remove-99-of-a-neural-network-without-losing-accuracy-915b1fab873b?source=collection_archive---------14-----------------------#2020-06-15">https://towardsdatascience.com/can-you-remove-99-of-a-neural-network-without-losing-accuracy-915b1fab873b?source=collection_archive---------14-----------------------#2020-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/88f5f1d3ffca9989eb39b235cc1e53e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HF9JNCf_mPNKl2_59eDphg.png"/></div></div></figure><div class=""/><div class=""><h2 id="df6d" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">权重剪枝简介</h2></div><p id="5c23" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">即使是最常见的神经网络架构也有许多参数。ResNet50是一种常用的基线模型，拥有约2500万个。这意味着在训练期间，我们在2500万维的参数空间中执行搜索。</p><p id="6d3b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了客观地看待这个数字，让我们来看看这个空间中的一个立方体。一个n维立方体有2ⁿ顶点，所以在2500万维中，我们谈论的是2个⁵⁰⁰⁰⁰⁰⁰点。在搜索网格中，这只是一个元素。相比之下，可观测宇宙中的原子数量估计在10⁸左右。可以肯定地说，这个问题的严重性是我们人类无法理解的。</p><p id="fd0a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">因此，减少参数的数量有几个好处。稀疏网络不仅更小，而且训练和使用更快。在硬件有限的地方，比如嵌入式设备或智能手机，速度和大小可以决定一个模式的成败。此外，更复杂的模型更容易过度拟合。因此，限制搜索空间也是一种正则化。</p><p id="6425" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然而，这并不是一项简单的任务，因为减少模型的容量也会导致准确性的损失。因此，复杂性和性能之间存在微妙的平衡。在本帖中，我们将深入探讨挑战和潜在的解决方案。</p><h1 id="4946" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">权重修剪</h1><p id="2756" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">首先，最简单的问题是在训练后，减少模型参数<em class="lp">。这对训练本身没有帮助，但会减少推理的计算需求。</em></p><p id="66d9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">消除权重的过程被称为<em class="lp">修剪</em>。(从现在起，我将使用可互换的权重和参数。)它的起源可以追溯到Yann LeCun、John S. Denker和Sara A. Solla的著名论文<a class="ae mn" href="http://papers.nips.cc/paper/250-optimal-brain-damage.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lp">最优脑损伤</em> </a>。<em class="lp">(如果知道更早的参考，欢迎留言评论！)</em></p><p id="786c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">他们提出了以下迭代修剪方法。</p><ol class=""><li id="7df2" class="mo mp je kv b kw kx kz la lc mq lg mr lk ms lo mt mu mv mw bi translated">训练模型。</li><li id="41e0" class="mo mp je kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">估计每个权重的<em class="lp">显著性</em>，这由扰动权重时损失函数的变化来定义。变化越小，重量对训练的影响越小。</li><li id="8869" class="mo mp je kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">移除显著性最低的权重。(也就是说，将它们的值设置为零，并在余下的过程中保持不变。)</li><li id="967c" class="mo mp je kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">转到步骤1。并重新训练修剪后的模型。</li></ol><p id="f301" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">继续训练与修剪重量是必要的。作者观察到，如果没有它，当大部分重量被移除时，目标函数(也称为损失)会显著增加。</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/ea263e2de930e27a186b69ec851d1011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6OVOlD_8aGL43g0Jne8W1A.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">来源:<a class="ae mn" href="http://papers.nips.cc/paper/250-optimal-brain-damage.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nl">最佳大脑损伤</em> </a>作者:扬·勒昆、约翰·s·登克和萨拉·a·索拉</p></figure><p id="985f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当修剪后的网络被重新训练时，这种方法出现了一个特殊的挑战。事实证明，由于其能力下降，再培训更加困难。这个问题的解决方案是后来才出现的，还有所谓的彩票假说，把这个问题放到了一个完全不同的角度。</p><h1 id="19dd" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">彩票假说</h1><p id="d4a2" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">赢得彩票头奖的机会很小。例如，如果你在玩强力球，你每张彩票有292，201，338分之一的胜算。如果你购买了<em class="lp"> n </em>张票，你的机会有多大？所以，如果赢的概率是</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/edbcf098ea611e9e5f1c440c3fd0e624.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*_91FGT1VH1KNHD-iVAIzlg@2x.png"/></div></figure><p id="d62e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">那么<em class="lp">不赢</em>的概率是</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/542ce008f205ff2cc1df240a69efdff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*S8kxzA-94lASH2oYRoTTKA@2x.png"/></div></figure><p id="48c1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当我们购买<em class="lp"> n </em>张票时，<em class="lp">张票都没中</em> s的概率是</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/3a7640e351daa2ccb19359862cac2ac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*ccNmUmdbhHDLGz0Zq9lzmQ@2x.png"/></div></figure><p id="5831" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">由此得出结论，他们中至少有一个人赢的概率是</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/501f79b56a8dd3d9fcbc007a8fd0c81b.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*bqxwLy-GwhIPdQHnLjWAiA@2x.png"/></div></figure><p id="2200" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果<em class="lp"> n </em>很大，这个可以任意接近1。</p><div class="is it gp gr iu nq"><a rel="noopener follow" target="_blank" href="/the-mathematical-foundations-of-probability-beb8d8426651"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd jf gy z fp nv fr fs nw fu fw jd bi translated">概率的数学基础</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">测度论导论</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe ja nq"/></div></div></a></div><p id="922f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这和神经网络有什么关系？在训练之前，模型的权重被随机初始化<em class="lp"/>。会不会碰巧有一个随机初始化的网络的子网<em class="lp">“中了初始化彩票”</em>？在他们的工作中，乔纳森·弗兰克尔和迈克尔·卡宾陈述了<a class="ae mn" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf">彩票假说</strong> </a>:</p><blockquote class="of og oh"><p id="b019" class="kt ku lp kv b kw kx kf ky kz la ki lb oi ld le lf oj lh li lj ok ll lm ln lo im bi translated">一个随机初始化的密集神经网络包含一个子网络，该子网络被初始化为当被隔离训练时，它可以在最多相同次数的迭代训练后匹配原始网络的测试精度。</p></blockquote><p id="dbe7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">最重要的问题是用最多相同数量的训练步骤实现与整个网络相同的精度。正如我们之前看到的，这是修剪的最大挑战:由于建模能力下降，训练较小的网络可能更加困难。</p><p id="8936" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们如何找到这样的初始化彩票中奖者？他们真的存在吗？作者已经找到了一种在某些架构中一致识别这种子网的方法。他们的方法如下。</p><ol class=""><li id="56a3" class="mo mp je kv b kw kx kz la lc mq lg mr lk ms lo mt mu mv mw bi translated">随机初始化一个神经网络。</li><li id="4ccb" class="mo mp je kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">为<em class="lp"> n </em>个训练步骤训练网络。</li><li id="6ea8" class="mo mp je kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">移除最低重量的<em class="lp"> k% </em>。</li><li id="c838" class="mo mp je kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">将剩余权重重置为随机初始化期间的值。</li><li id="1981" class="mo mp je kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">转到步骤2。并且迭代训练和修剪。</li></ol><p id="604b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">与以前的方法相比，这里有两个关键步骤。首先，简单地根据它们的大小去除权重。第二，<strong class="kv jf">修剪后的网络的权重不重新初始化，而是重置到第一次初始化后的状态</strong>。这已经被证明是必要的:当超过80%的权重被移除时，被修剪的网络的随机重新初始化导致显著更差的结果。</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/f8e963ce9bf8c0f69d437f7f11b73ce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dLhyeKCt3SVZl2sdfuTkJg.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">乔纳森·弗兰克尔和迈克尔·卡宾修剪方法的性能。来源:<a class="ae mn" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank">彩票假说:寻找稀疏的、可训练的神经网络</a>，作者乔纳森·弗兰克尔和迈克尔·卡宾</p></figure><p id="4935" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然而，这种方法有很大的局限性。首先，它不能很好地处理大规模的问题和架构。在最初的论文中，作者指出，对于更复杂的数据集(如ImageNet)和更深层次的架构(如ResNet)，该方法无法识别初始化抽奖的赢家。</p><p id="5ad7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">一般来说，实现良好的稀疏性-准确性权衡是一个困难的问题。这是一个非常活跃的研究领域，并且技术水平在不断提高。</p><p id="90dd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">尽管这种方法是一个巨大的改进，但它并没有解决一个重要的问题:修剪后的子网仍然需要重新训练。令人惊讶的是，这是通过颠倒整个问题来解决的。</p><h1 id="a1cf" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">重量不可知的神经网络</h1><p id="a40e" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">到目前为止，我们已经从一个大的神经网络开始，反复地修剪它来压缩它。然而，有一种与逻辑相反的替代方法:我们可以从最小的架构开始，然后逐步增加。</p><p id="8830" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这是亚当·盖尔和大卫·哈在他们最近的论文<a class="ae mn" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank">重量不可知神经网络</a>中的出发点。除了获得最小的网络，他们还有一个更雄心勃勃的目标:<em class="lp">最终的架构应该在随机权重下表现良好</em>。如果你还记得的话，权重剪枝方法最大的缺点是产生的子网很难训练。这样，就不需要培训了。</p><p id="c91c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">本质上，他们的方法是在可能的架构空间中的进化搜索。与以前的方法相比，他们的搜索更喜欢简单和重量不可知的属性。</p><ol class=""><li id="e554" class="mo mp je kv b kw kx kz la lc mq lg mr lk ms lo mt mu mv mw bi translated">创建一组最小的神经网络架构，这些架构<strong class="kv jf">可以嵌入到单个父架构</strong>中。此时没有权重值，只有连接。基本上，每个网络都相当于对父架构的特定修剪。</li><li id="9810" class="mo mp je kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">用多组共享权重测试网络<strong class="kv jf">。共享权重是必不可少的，因为这迫使搜索更倾向于权重不可知的架构。</strong></li><li id="a8b3" class="mo mp je kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">根据网络的性能和复杂性对网络进行排序。因为权重在网络之间共享，所以性能也反映了权重不可知的能力。</li><li id="ef60" class="mo mp je kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">通过选择排名靠前的网络并随机修改它们来创建新网络。</li><li id="844d" class="mo mp je kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">获取获得的网络，并转到步骤2。</li></ol><p id="5dd3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">作者在下图中总结了这一过程。</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/456478081eb23594295b5b1c481e9da0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*Wljp8aBwZA-qprCiYQYndw.png"/></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">来源:<a class="ae mn" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank">亚当·盖尔和大卫·哈的重量不可知神经网络</a></p></figure><p id="7d32" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这样，他们就能够找到在随机权重下表现良好的架构。这是一个相当令人印象深刻的结果。</p><p id="2bfa" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">该方法适用于多种机器学习任务，而不仅仅是手写数字识别。他们能够为几个连续的控制问题设计最小的重量不可知的架构，例如杆平衡和赛车。</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/990dbd40e98591b05baea353d7681722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*4cLYM_3OgPcnmJAeEODx8Q.png"/></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">来源:<a class="ae mn" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank"> <em class="nl">权重不可知神经网络</em> </a>作者:亚当·盖尔和大卫·哈</p></figure><p id="b85a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">到目前为止，这种方法让我们最接近于找到不需要培训的最小架构。然而，还有许多事情要做。对于更复杂的任务，如ImageNet分类，找到精确的权重不可知网络仍然是未解决的问题。</p><h2 id="ac9f" class="oo lr je bd ls op oq dn lw or os dp ma lc ot ou mc lg ov ow me lk ox oy mg oz bi translated">令人兴奋的新发展</h2><p id="f056" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">就在我写这篇文章的时候，一篇新的论文出现了，并声称实现了理论上的最大压缩——无需查看数据！在田畑秀则·田中等人的论文<a class="ae mn" href="https://arxiv.org/abs/2006.05467" rel="noopener ugc nofollow" target="_blank"> <em class="lp">中，通过迭代保存突触流</em> </a>在没有任何数据的情况下修剪神经网络，他们的算法SynFlow能够处理那些以前的方法无法处理的复杂任务。我非常期待了解更多这方面的知识，并深入研究这篇论文！</p><h1 id="0623" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">实践中的修剪</h1><p id="a5e5" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">到目前为止，我们只讨论了修剪的理论方面。最后，我想收集一些你可以马上开始使用的工具。</p><p id="2d4a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果你是一个<a class="ae mn" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>用户，有几个内置的模型优化工具，包括剪枝。</p><div class="is it gp gr iu nq"><a href="https://medium.com/tensorflow/introducing-the-model-optimization-toolkit-for-tensorflow-254aca1ba0a3" rel="noopener follow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd jf gy z fp nv fr fs nw fu fw jd bi translated">TensorFlow模型优化工具包简介</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">我们很高兴在TensorFlow中推出一个新的优化工具包:一套技术，开发者，无论是…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">medium.com</p></div></div><div class="nz l"><div class="pa l ob oc od nz oe ja nq"/></div></div></a></div><div class="is it gp gr iu nq"><a href="https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-pruning-api-42cac9157a6a" rel="noopener follow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd jf gy z fp nv fr fs nw fu fw jd bi translated">TensorFlow模型优化工具包—修剪API</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">自从我们引入了模型优化工具包——一套开发人员，无论是新手还是高级开发人员…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">medium.com</p></div></div><div class="nz l"><div class="pb l ob oc od nz oe ja nq"/></div></div></a></div><p id="2d95" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">类似地，<a class="ae mn" href="https://pytorch.org" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>也有类似的功能。你可以在这里找到他们关于权重修剪的<a class="ae mn" href="https://pytorch.org/tutorials/intermediate/pruning_tutorial.html" rel="noopener ugc nofollow" target="_blank">教程</a>，但是我也写了一个关于这个和其他可用工具如模型量化的快速总结。</p><div class="is it gp gr iu nq"><a rel="noopener follow" target="_blank" href="/5-advanced-pytorch-tools-to-level-up-your-workflow-d0bcf0603ad5"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd jf gy z fp nv fr fs nw fu fw jd bi translated">5个高级PyTorch工具提升您的工作流程</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">从开发到生产</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="pc l ob oc od nz oe ja nq"/></div></div></a></div><h1 id="58cd" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">摘要</h1><p id="e442" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">机器学习不会停留在训练精确的模型上。在部署到生产环境时，速度和大小非常重要。为了使复杂的体系结构适应生产约束，可以通过削减某些权重来减小模型的大小。(还有很多其他的方法，我们没有讲过。)</p><p id="6a96" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这不是一项简单的任务，因为必须考虑精度损失。此外，较小的架构需要重新培训，由于建模能力下降，这可能会更加困难。这里，我们回顾了两种权重修剪方法和一种互补的架构搜索方法，它们逐步解决了这些挑战。</p><p id="93dc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然而，这只是冰山一角。权重剪枝是一个活跃的研究领域，特别是对于更复杂的任务，如大规模图像识别。(想想ImageNet大小。)</p><p id="20e3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现代深度学习框架(如TensorFlow和PyTorch)中提供了修剪方法，因此您可以在工作中立即开始尝试这些方法，甚至可以尝试推动艺术的发展。因此，如果您发现这个主题很有趣，请将这些令人惊叹的方法付诸实践:)</p><h1 id="b5d7" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">参考</h1><p id="3738" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">[1] <a class="ae mn" href="http://papers.nips.cc/paper/250-optimal-brain-damage.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lp">最佳大脑损伤</em> </a>作者扬·勒昆、约翰·s·登克和萨拉·a·索拉</p><p id="36fa" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">[2] <a class="ae mn" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank"> <em class="lp">彩票假说:寻找稀疏的、可训练的神经网络</em> </a>作者乔纳森·弗兰克尔和迈克尔·卡宾</p><p id="fc2b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">[3] <a class="ae mn" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank"> <em class="lp">权重不可知的神经网络</em> </a>由Adam Gaier和David Ha</p><p id="7276" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">[4] <a class="ae mn" href="https://arxiv.org/abs/2006.05467" rel="noopener ugc nofollow" target="_blank"> <em class="lp">通过迭代保存突触流来修剪无任何数据的神经网络</em></a><em class="lp"/>田畑秀则·田中、丹尼尔·库宁、丹尼尔·l·k·亚明斯和苏亚甘古利</p></div><div class="ab cl pd pe hx pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="im in io ip iq"><p id="407c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><a class="ae mn" href="https://www.tivadardanka.com/blog" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf"> <em class="lp">如果你喜欢把机器学习概念拆开，理解是什么让它们运转，我们有很多共同点。看看我的博客，我经常在那里发表这样的技术文章！</em> </strong> </a></p></div></div>    
</body>
</html>