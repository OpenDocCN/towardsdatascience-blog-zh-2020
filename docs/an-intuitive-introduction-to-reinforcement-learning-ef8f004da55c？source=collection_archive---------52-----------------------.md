# 强化学习的直观介绍

> 原文：<https://towardsdatascience.com/an-intuitive-introduction-to-reinforcement-learning-ef8f004da55c?source=collection_archive---------52----------------------->

## 欢迎来到人工智能的未来

![](img/f1203602b5f24637ad001c6aef439327.png)

在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上由 [Franck V.](https://unsplash.com/@franckinjapan?utm_source=medium&utm_medium=referral) 拍摄的照片

> 强化学习是最接近人类学习方式的学习类型。

强化学习与监督和非监督学习技术相反，是一种面向目标的学习技术。它基于在一个环境中操作，在该环境中，一个假设的人(代理人)被期望从一组可能的决策中做出一个决策(行动),并通过反复学习选择一个导致期望目标的决策(基本上是试错法)来最大化通过做出该决策获得的利润(回报)。当我们继续阅读这篇文章时，我会更详细地解释这一点。

在这篇文章中，我将讨论强化学习(RL)的基础知识(尽可能用例子)。

# 被监督？无人监管？强化学习！

重要的事情先来！在开始谈论 RL 之前，我们先来看看它与监督和非监督学习技术到底有什么不同。

让我们考虑一个正在学骑自行车的孩子的例子。我们将会看到，如果孩子在有人监督、无人监督或强化学习的情况下学习，这个问题将如何解决:

1.  **监督学习:**现在，如果孩子开始计算他需要在踏板上施加的力，或者他需要与地面保持平衡的角度；他开始在每次骑自行车时优化这些计算，以完善他的骑行技术，那么可以说他是在监督下学习。
2.  无监督学习:然而，如果孩子开始观察成千上万骑自行车的人，并基于这些知识，如果他开始弄清楚骑自行车到底要做什么，那么可以说他是以无监督的方式学习的。
3.  **强化学习:**最后，如果给他几个选项，比如踩踏板、向左或向右转动手柄、踩刹车等等。以及在这些选项中尝试任何他想要成功骑自行车的自由，他首先会做错并失败(可能会摔下来)；但最终，在几次失败的尝试后，他会想出如何去做，并最终成功。这个案例是强化学习的一个例子。

好了，现在你知道为什么说它是最接近人类学习的方式了吧！现在，随着我们继续深入，你可以预期话题会变得有点正式。

# 勘探开发

让我们继续这个孩子的例子，他知道他骑自行车时可以做的一系列动作。所以，考虑这样一个场景，他最终发现持续踩下踏板可以驱动自行车。然而，他没有意识到骑行之后，他必须在某个点停下来(即在正确的时间踩刹车是骑自行车的一个组成部分)。但是，他很高兴现在他知道如何骑自行车，并且不关心未来的事情。我们姑且把他的快乐称之为'**奖励**'，意思是他因为踩踏板的动作而得到奖励。由于他得到了奖励，他纯粹是“**利用**”当前的行动，即踩踏板，不知道也许最终他可能会在某个地方撞车，这将使他远离实现他的最终目标；正确地骑自行车。

现在，他可以从一系列可用的动作中探索其他选项，而不仅仅是踩踏板。最终，他将能够随时停下自行车。以类似的方式，他将学习如何转弯，这样，他将是一个好骑手。

但是，任何东西太多都是不好的！我们看到，过度开发会导致失败。同理，过度探索也是不好的。例如，如果他只是在每一个情况下随机改变他的动作，他就不会骑自行车了，不是吗？所以基本上，这是一种权衡，它被称为**勘探开发困境**，是解决 RL 问题时要考虑的主要参数之一。

**注意**孩子会根据他对环境的当前状态**【w r t】来决定他在给定情况下的行动，即他在骑车时的当前运动/位置以及从先前尝试中获得的奖励(这种决策机制就是 RL 的全部内容)。**

# **RL 问题的构建模块**

1.  ****策略:****策略定义了 RL 代理的行为。在我们的例子中，策略是孩子思考在可用的动作中选择什么动作的方式(孩子是代理)。****
2.  ******奖励:**这些定义了一个问题的目标。每走一步，环境都会给代理发送一个奖励。在我们的例子中，骑自行车的乐趣，或者从自行车上摔下来的痛苦，就是奖励(第二种情况可以称为惩罚)。****
3.  ******价值函数:**奖励是环境对主体的即时反应。然而，我们感兴趣的是长期回报的最大化。这是使用值函数计算的。从形式上来说，一个状态的价值是一个代理在未来可以期望积累的总回报，从那个状态开始([萨顿&巴尔托](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf))。如果孩子仔细考虑了未来可能发生的事情，如果他选择了一个特定的动作，比如说几百米，那么这可以被称为*值。*****
4.  ****模型:环境的模型是计划的工具。它模拟实际环境，因此可用于推断环境的行为方式。例如，给定一个状态和一个动作，模型可能会预测下一个状态和下一个奖励([萨顿&巴尔托](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf))。当然，RL 机制可以分为基于模型的方法和无模型的方法。****

# ****结论****

****我们对 RL 问题是什么样子以及如何解决它有一种直觉。此外，我们将 RL 与监督学习和非监督学习区分开来。然而，强化学习比本文中的概述要复杂得多；但这足以明确基本概念。****

# ****参考****

****萨顿和巴尔托:[https://web . Stanford . edu/class/psych 209/Readings/suttonbartoiprlbook 2 nded . pdf](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)****

****NPTEL RL 课程:[https://www.youtube.com/watch?v=YaPSPu7K9S0&list = plyqspqzte 6m _ fwzhfayf 4 lskz _ ijmyjd 9&index = 5](https://www.youtube.com/watch?v=YaPSPu7K9S0&list=PLyqSpQzTE6M_FwzHFAyf4LSkz_IjMyjD9&index=5)****