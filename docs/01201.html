<html>
<head>
<title>Building Classification Models with Sklearn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Sklearn构建分类模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-classification-models-with-sklearn-6a8fd107f0c1?source=collection_archive---------9-----------------------#2020-02-03">https://towardsdatascience.com/building-classification-models-with-sklearn-6a8fd107f0c1?source=collection_archive---------9-----------------------#2020-02-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4a4f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Sklearn分类简介</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d07e59be661ddd81afbff31fdca738dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K_GNV_536-yPzoHnsvhu5Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">莎伦·麦卡琴在<a class="ae ky" href="https://www.pexels.com/photo/art-materials-art-supplies-blocks-blur-1148496/" rel="noopener ugc nofollow" target="_blank">的照片</a></p></figure><p id="c726" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-learn是一个针对python的开源机器学习库。它提供了各种回归、分类和聚类算法。在我的上一篇文章<a class="ae ky" rel="noopener" target="_blank" href="/a-brief-tour-of-scikit-learn-sklearn-6e829a9db2fd">Sklearn</a>中，我讨论了几种使用机器学习包进行回归的方法。在这篇文章中，我们将回顾一些建立分类模型的基本方法。该软件包的文档内容丰富，是每位数据科学家的绝佳资源。你可以在这里找到文档<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="c0f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用<em class="lv">电信客户流失</em>数据集。这里可以找到<a class="ae ky" href="https://www.kaggle.com/blastchar/telco-customer-churn" rel="noopener ugc nofollow" target="_blank"/></p><p id="0eea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们导入数据并打印前五行:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="82b9" class="mb mc it lx b gy md me l mf mg">import pandas as pd <br/>df = pd.read_csv("Customer_Churn.csv")<br/>print(df.head())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mh"><img src="../Images/592e40b585ccbbcdaf1c9153e04285f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_TIDg_6EWcflqAryVSoNDw.png"/></div></div></figure><p id="b6b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用所有的分类和数字数据来预测客户流失。首先，我们需要将分类列转换为神经网络可以处理的数值。例如，对于性别，我们有:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="337a" class="mb mc it lx b gy md me l mf mg">df.gender = pd.Categorical(df.gender)<br/>df['gender_code'] = df.gender.cat.codes</span></pre><p id="6646" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们定义输入和输出数组:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="bf27" class="mb mc it lx b gy md me l mf mg">import numpy as np</span><span id="b2b8" class="mb mc it lx b gy mi me l mf mg">features = ['gender_code', 'SeniorCitizen_code', 'PhoneService_code', 'MultipleLines_code', <br/>                 'InternetService_code', 'Partner_code', 'Dependents_code', 'PaymentMethod_code', <br/>                 'PaymentMethod_code', 'PaperlessBilling_code','Contract_code', 'StreamingMovies_code',<br/>                 'StreamingTV_code', 'TechSupport_code', 'DeviceProtection_code', 'OnlineBackup_code',<br/>                 'OnlineSecurity_code', 'Dependents_code', 'Partner_code','tenure', 'MonthlyCharges']</span><span id="0995" class="mb mc it lx b gy mi me l mf mg">X = np.array(df[features])<br/>y = np.array(df['Churn_code'])</span></pre><p id="b1b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将分割数据用于训练和测试:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="d340" class="mb mc it lx b gy md me l mf mg">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)</span></pre><p id="71f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们所有必要的变量都定义好了。让我们建立一些模型！</p><p id="dc24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">逻辑回归</strong></p><p id="a7ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">先说逻辑回归。逻辑回归使用逻辑函数来预测二元因变量。</p><p id="15dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们导入LogisticRegression包，如下所示:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="c15d" class="mb mc it lx b gy md me l mf mg">from sklearn.linear_model import LogisticRegression</span></pre><p id="e23c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们定义一个逻辑回归对象，拟合我们的模型，并评估性能:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="67ff" class="mb mc it lx b gy md me l mf mg">reg_log = LogisticRegression()<br/>reg_log.fit(X_train, y_train)<br/>y_pred = reg_log.predict(X_test)</span></pre><p id="3ea7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用度量分类报告来可视化预测:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="db23" class="mb mc it lx b gy md me l mf mg">from sklearn import metrics<br/>print(metrics.classification_report(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/f51408f96d0c156de156af5e2c1b601e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FLHlAZIkdeooaQzCBJZ40w.png"/></div></div></figure><p id="c7e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以查看“roc_auc_score”和“f1_score”“roc_auc_score”是接收操作特征曲线下的面积。这是衡量二元分类模型区分类别的能力的一种方法。“roc_auc_score”为0.5意味着模型无法区分类别。接近1.0的值对应于类之间的强分离。“f1_score”是精确度和召回率的调和平均值。与“roc_auc_score”类似，完美的“f1_score”等于1.0:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="a5fe" class="mb mc it lx b gy md me l mf mg">print("roc_auc_score: ", roc_auc_score(y_test, y_pred))<br/>print("f1 score: ", f1_score(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/429affbf9f025d9b5f8b2ab1b0ff5e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*G2YmxtU_DuXoR3Crtnca0A.png"/></div></figure><p id="8eb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">随机森林</strong></p><p id="9e95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们来看看随机森林。随机森林是一种基于树的方法，它集成了多个单独的决策树。</p><p id="215a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们导入RandomForestClassifier包，如下所示:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="6385" class="mb mc it lx b gy md me l mf mg">from sklearn.ensemble import RandomForestClassifier</span></pre><p id="f482" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们定义一个随机森林分类对象，符合我们的模型，并评估性能:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="9c83" class="mb mc it lx b gy md me l mf mg">reg_rf = RandomForestClassifier()<br/>reg_rf.fit(X_train, y_train)<br/>y_pred = reg_rf.predict(X_test)</span></pre><p id="0301" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一下指标分类报告:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="dea4" class="mb mc it lx b gy md me l mf mg">print(metrics.classification_report(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ml"><img src="../Images/035126833051309ebf8d7e92f4abd345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJiDg7AzD33XWdex6NT7BA.png"/></div></div></figure><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="a0ef" class="mb mc it lx b gy md me l mf mg">print("roc_auc_score: ", roc_auc_score(y_test, y_pred))<br/>print("f1 score: ", f1_score(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/12dfa260e7736b7efa9087be27b52503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*zDtbY4ZdJtrxhQpMWEJ5XA.png"/></div></figure><p id="0379" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到随机森林的性能比逻辑回归差。我们还可以打印特征重要性。这使我们能够了解哪些变量对温度预测最为重要:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="015a" class="mb mc it lx b gy md me l mf mg">feature_df = pd.DataFrame({'Importance':reg_rf.feature_importances_, 'Features': features })<br/>print(feature_df)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/111eb4241e2678321dc6a9ec3a794409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*6-3d0zYrJuLLggRie07g-Q.png"/></div></figure><p id="a56e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我想指出的是，通过不传递任何参数，比如max_depth和n_estimators，我选择了默认的随机森林值(n_estimators = 10和max_depth = 10)。我们可以通过优化随机森林中的参数来进一步提高性能。这可以手动完成，也可以使用网格搜索技术自动完成。我将把参数优化的问题留给另一篇文章。</p><p id="b8a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">支持向量机</strong></p><p id="4472" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我要讨论的下一个方法叫做支持向量回归。这是支持向量机(SVM)的扩展。支持向量机在高维特征空间中构造一组超平面，可用于回归和分类问题。</p><p id="dd56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们导入SVC包，如下所示:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="bb6f" class="mb mc it lx b gy md me l mf mg">from sklearn.svm import SVC</span></pre><p id="eadf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们定义一个支持向量分类对象，适合我们的模型，并评估性能:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="ed55" class="mb mc it lx b gy md me l mf mg">reg_svc = SVC()<br/>reg_svc.fit(X_train, y_train)<br/>y_pred = reg_svc.predict(X_test)</span></pre><p id="279a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用度量分类报告来可视化预测:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="8c83" class="mb mc it lx b gy md me l mf mg">print(metrics.classification_report(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/a18374987e3770e2e50e4a0e3bd8407c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pz0vmFcNbVoxUtoE5OOATw.png"/></div></div></figure><p id="0245" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以查看roc_auc_score和f1_scores:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="a6fb" class="mb mc it lx b gy md me l mf mg">print("roc_auc_score: ", roc_auc_score(y_test, y_pred))<br/>print("f1 score: ", f1_score(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/65a03a25db1c94a7e86b2c9ae21658dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*TjJG-o84i6Zn8ZgZkC8DHw.png"/></div></figure><p id="8d80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到支持向量分类性能比逻辑回归略差，比随机森林略好。与随机森林类似，SVC采用可用于优化性能的参数。这些包括正则化参数(默认C = 1.0)、核(默认核= 'rbf ')和核系数(默认伽马= 'scale ')</p><p id="a901" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"><em class="lv">K</em>-最近邻居</strong></p><p id="9e00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我要讨论的最后一种方法是<em class="lv"> k- </em>最近邻法进行分类。k-最近邻使用欧几里德距离计算，其中预测是k个最近邻的平均值。</p><p id="f2d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们导入KNeighborsClassifier包，如下所示:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="483e" class="mb mc it lx b gy md me l mf mg">from sklearn.neighbors import KNeighborsClassifier</span></pre><p id="48ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们定义一个<em class="lv"> k- </em>最近邻分类对象，拟合我们的模型，并评估性能:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="a880" class="mb mc it lx b gy md me l mf mg">reg_knn = KNeighborsClassifier()<br/>reg_knn.fit(X_train, y_train)<br/>y_pred = reg_knn.predict(X_test)</span></pre><p id="2a34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一下指标分类报告:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="9812" class="mb mc it lx b gy md me l mf mg">print(metrics.classification_report(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/3cd608801d17f21acad41dac53e4a630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4scB3Q_Qu-fY6cbK11i9Zg.png"/></div></div></figure><p id="d6c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以查看roc_auc_score和f1_scores:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="bc04" class="mb mc it lx b gy md me l mf mg">print("roc_auc_score: ", roc_auc_score(y_test, y_pred))<br/>print("f1 score: ", f1_score(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/2ff2b7ff19d780d7bbd5f3c85ca37bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*IhxKbzCoaP6G0hON2lw0KQ.png"/></div></figure><p id="8b6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">K</em>-最近邻算法也采用超参数，特别是n_neighbors，可以选择它来最小化误差。</p><p id="2dcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结论</strong></p><p id="8e1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将在这里停下来，但是您可以随意选择模型特性，看看是否可以改进其中一些模型的性能。</p><p id="a114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">概括地说，我简要介绍了使用python机器学习库进行分类。我讲述了如何定义模型对象、使模型适合数据，以及使用逻辑回归、随机森林、支持向量机和最近邻模型来预测输出。</p><p id="b97d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章是有益的。这篇文章的代码可以在GitHub 上找到。感谢阅读，机器学习快乐！</p></div></div>    
</body>
</html>