<html>
<head>
<title>A Basic Introduction to TensorFlow Lite</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow Lite的基本介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-basic-introduction-to-tensorflow-lite-59e480c57292?source=collection_archive---------7-----------------------#2020-06-14">https://towardsdatascience.com/a-basic-introduction-to-tensorflow-lite-59e480c57292?source=collection_archive---------7-----------------------#2020-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6de5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">介绍TensorFlow Lite转换器、量化优化和在边缘运行Tensorflow Lite模型的解释器</h2></div><p id="919c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">在本文中，我们将了解在边缘部署深度学习模型所需的功能，什么是TensorFlow Lite，以及如何使用TensorFlow Lite的不同组件在边缘进行推断。</em></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/6056f4ab0db54ee0e93ed4a7d0379d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*XvzxS6arCPF4FwnRgV4gog.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">Priscilla Du Preez在Unsplash上拍摄的照片</p></figure><p id="f5b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">你正在尝试将你的深度学习模型部署在一个他们没有良好网络连接但仍然需要深度学习模型来提供出色性能的区域。</em> </strong></p><blockquote class="lr"><p id="ad73" class="ls lt it bd lu lv lw lx ly lz ma ld dk translated">TensorFlow Lite可用于这种情况</p></blockquote><h2 id="3f84" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">深度学习模型在边缘进行推理的特征</h2><ol class=""><li id="c971" class="mu mv it kk b kl mw ko mx kr my kv mz kz na ld nb nc nd ne bi translated"><strong class="kk iu">轻量级:</strong>边缘设备在存储和计算能力方面资源有限。深度学习模型是资源密集型的，因此我们在边缘设备上部署的模型应该是轻量级的，具有较小的二进制大小。</li><li id="452a" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated"><strong class="kk iu">低延迟:</strong>边缘的深度学习模型应该做出更快的推断，而不管网络连接性如何。由于推断是在边缘设备上进行的，因此将消除从设备到服务器的往返行程，从而使推断更快。</li><li id="19ef" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated"><strong class="kk iu">安全:</strong>模型部署在边缘设备上，推断在设备上进行，没有数据离开设备或在网络上共享，因此不存在数据隐私问题。</li><li id="6068" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated"><strong class="kk iu">最佳功耗:</strong>网络需要大量功率，边缘设备可能没有连接到网络，因此功耗需求较低。</li><li id="1296" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated"><strong class="kk iu">预训练:</strong>模型可以在内部或云端进行训练，用于不同的深度学习任务，如图像分类、对象检测、语音识别等。并且可以很容易地部署以在边缘做出推断。</li></ol><p id="3176" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Tensorflow Lite提供了在边缘进行推理所需的所有功能。</p><p id="ecd2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">但是TensorFlow Lite是什么？</em>T19】</strong></p><p id="2018" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> TensorFlow Lite是一个开源、产品就绪、跨平台的深度学习框架，可以将TensorFlow中预先训练的模型转换为一种特殊的格式，可以针对速度或存储进行优化</strong>。</p><p id="0e4f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">特殊格式模型可以<strong class="kk iu">部署在边缘设备上，如使用Android或iOS或Linux的移动设备，基于嵌入式设备，如Raspberry Pi或微控制器，以在边缘进行推理</strong>。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/8666c026197dbbabacf46068c43256ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*YyUienrsApR3H2aFZy_Dsw.png"/></div></figure><p id="6fb9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="le">tensor flow Lite(TF Lite)是如何工作的？</em> </strong></p><h2 id="35b1" class="mb mc it bd md me nl dn mg mh nm dp mj kr nn ml mm kv no mo mp kz np mr ms mt bi translated">选择并训练一个模型</h2><p id="866a" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr nq kt ku kv nr kx ky kz ns lb lc ld im bi translated">假设您想要执行图像分类任务。第一件事是决定任务的模型。你的选择是</p><ul class=""><li id="de63" class="mu mv it kk b kl km ko kp kr nt kv nu kz nv ld nw nc nd ne bi translated">创建自定义模型</li><li id="cd5b" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nw nc nd ne bi translated">使用预先训练好的模型，如InceptionNet、MobileNet、NASNetLarge等。</li><li id="7bb8" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nw nc nd ne bi translated">在预先训练的模型上应用迁移学习</li></ul><h2 id="620e" class="mb mc it bd md me nl dn mg mh nm dp mj kr nn ml mm kv no mo mp kz np mr ms mt bi translated">使用转换器转换模型</h2><p id="24b7" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr nq kt ku kv nr kx ky kz ns lb lc ld im bi translated">模型定型后，您将把模型转换为Tensorflow Lite版本。<strong class="kk iu"> TF lite模型是一种特殊格式的模型，在准确性方面很有效，并且是一种占用空间较少的轻量型模型，这些特性使TF lite模型非常适合在移动和嵌入式设备上工作。</strong></p><p id="6547" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> TensorFlow Lite转换流程</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/09a7f832bbdc769159b6df12afe49714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l3VtMk5W6SLV4Bs-X6ly_Q.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">来源:https://www.tensorflow.org/lite/convert/index<a class="ae oc" href="https://www.tensorflow.org/lite/convert/index" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="e77c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">在从Tensorflow模型到Tensorflow Lite模型的转换过程中，文件的大小会减小</strong>。我们可以选择进一步减小文件大小，同时权衡模型的执行速度。</p><p id="23f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> Tensorflow Lite转换器将Tensorflow模型转换为Tensorflow Lite平面缓冲文件(<em class="le">)。tflite </em>)。</strong></p><p id="cda9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Tensorflow Lite平面缓冲文件部署到客户端，在我们的例子中，客户端可以是运行在iOS或Android上的移动设备或嵌入式设备。</p><p id="f971" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">如何将TensorFlow模型转换为TFlite模型？</em>T15】</strong></p><p id="5855" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练完模型后，您现在需要保存模型。</p><p id="2076" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">保存的模型在单个文件中序列化模型的架构、权重和偏差以及训练配置。保存的模型可以很容易地用于共享或部署模型。</strong></p><p id="92b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">转换器支持使用保存的模型</p><ul class=""><li id="a6ea" class="mu mv it kk b kl km ko kp kr nt kv nu kz nv ld nw nc nd ne bi translated"><strong class="kk iu"> tf.keras.Model: </strong>使用keras创建并编译模型，然后使用TFLite转换模型。</li></ul><pre class="lg lh li lj gt od oe of og aw oh bi"><span id="9624" class="mb mc it oe b gy oi oj l ok ol">#Save the keras model after compiling<br/><strong class="oe iu">model.save('model_keras.h5')<br/>model_keras= tf.keras.models.load_model('model_keras.h5')</strong></span><span id="e754" class="mb mc it oe b gy om oj l ok ol"># Converting a tf.Keras model to a TensorFlow Lite model.<br/><strong class="oe iu">converter = tf.lite.TFLiteConverter.from_keras_model(model_keras)<br/>tflite_model = converter.convert()</strong></span></pre><ul class=""><li id="0d1d" class="mu mv it kk b kl km ko kp kr nt kv nu kz nv ld nw nc nd ne bi translated">SavedModel :一个SavedModel包含一个完整的TensorFlow程序，包括权重和计算。</li></ul><pre class="lg lh li lj gt od oe of og aw oh bi"><span id="4302" class="mb mc it oe b gy oi oj l ok ol">#save your model in the SavedModel format<br/><strong class="oe iu">export_dir = 'saved_model/1'<br/>tf.saved_model.save(model, export_dir)</strong></span><span id="6893" class="mb mc it oe b gy om oj l ok ol"># Converting a SavedModel to a TensorFlow Lite model.<br/><strong class="oe iu">converter = lite.TFLiteConverter.from_saved_model(export_dir)<br/>tflite_model = converter.convert()</strong></span></pre><p id="b7de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le"> export_dir </em> </strong>遵循一个惯例，其中最后一个路径组件是模型的版本号。</p><p id="6e85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> SavedModel </strong>是保存在<strong class="kk iu"> <em class="le"> export_dir，</em> </strong>上的元图，使用<strong class="kk iu"> lite转换为TFLite模型。TFLiteConverter </strong>。</p><ul class=""><li id="eea6" class="mu mv it kk b kl km ko kp kr nt kv nu kz nv ld nw nc nd ne bi translated"><strong class="kk iu">具体功能:</strong> TF 2.0默认开启急切执行，影响性能和可部署性。为了克服性能问题，我们可以使用<strong class="kk iu"> tf.function来创建图形</strong>。图形包含模型结构，以及模型的所有计算操作、变量和权重。</li></ul><p id="5a8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将模型导出为具体函数，然后将具体函数转换为TF Lite模型</p><pre class="lg lh li lj gt od oe of og aw oh bi"><span id="d5c7" class="mb mc it oe b gy oi oj l ok ol"><strong class="oe iu"># export model as concrete function</strong><br/>func = tf.function(model).get_concrete_function(<br/>    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))</span><span id="c620" class="mb mc it oe b gy om oj l ok ol">#Returns a serialized graphdef representation of the concrte function<br/><strong class="oe iu">func.graph.as_graph_def()</strong></span><span id="af13" class="mb mc it oe b gy om oj l ok ol"># converting the concrete function to Tf Lite <br/><strong class="oe iu">converter =  tf.lite.TFLiteConverter.from_concrete_functions([func])<br/>tflite_model = converter.convert()</strong></span></pre><h2 id="abdd" class="mb mc it bd md me nl dn mg mh nm dp mj kr nn ml mm kv no mo mp kz np mr ms mt bi translated">优化模型</h2><p id="c4e2" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr nq kt ku kv nr kx ky kz ns lb lc ld im bi translated"><strong class="kk iu"> <em class="le">为什么要优化模型？</em>T41】</strong></p><p id="2ac3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">边缘模型需要重量轻</p><ul class=""><li id="d699" class="mu mv it kk b kl km ko kp kr nt kv nu kz nv ld nw nc nd ne bi translated">在边缘设备上占用较少的空间。</li><li id="d4da" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nw nc nd ne bi translated">在带宽较低的网络上下载速度更快</li><li id="ad98" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nw nc nd ne bi translated">为模型占用更少的内存，以便更快地做出推断</li></ul><p id="86fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">处于边缘的模型也应该具有运行推理的低延迟。轻量级和低延迟模型可以通过减少预测所需的计算量来实现。</p><p id="303c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">优化减少了模型的大小或改善了延迟。在模型的大小和模型的准确性之间有一个权衡。</p><p id="f15a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="le">tensor flow Lite中优化是如何实现的？</em>T3】</strong></p><p id="98af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Tensorflow Lite通过以下方式实现优化</p><ul class=""><li id="3526" class="mu mv it kk b kl km ko kp kr nt kv nu kz nv ld nw nc nd ne bi translated">量化</li><li id="0113" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nw nc nd ne bi translated">权重修剪</li></ul><p id="14b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">量子化</strong></p><p id="500e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们保存TensorFlow模型时，它存储为包含计算操作、激活函数、权重和偏差的图形。激活函数、权重和偏差是32位浮点。</p><blockquote class="lr"><p id="7e8a" class="ls lt it bd lu lv lw lx ly lz ma ld dk translated">量化降低了用于表示张量流模型的不同参数的数字的精度，这使得模型重量轻。</p></blockquote><p id="5602" class="pw-post-body-paragraph ki kj it kk b kl on ju kn ko oo jx kq kr op kt ku kv oq kx ky kz or lb lc ld im bi translated"><strong class="kk iu">量化可以应用于权重和激活。</strong></p><ul class=""><li id="cd12" class="mu mv it kk b kl km ko kp kr nt kv nu kz nv ld nw nc nd ne bi translated"><strong class="kk iu">具有32位浮点的权重可以转换为16位浮点或8位浮点或整数，并且</strong>将减小模型的大小。</li><li id="468b" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nw nc nd ne bi translated"><strong class="kk iu">权重和激活都可以通过转换为整数来量化，</strong>这将提供低延迟、更小的尺寸和降低的功耗。</li></ul><p id="edcc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">权重修剪</strong></p><p id="c0a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们修剪植物以去除植物的非生产性部分，使其更结果实和更健康一样，我们也可以修剪模型的权重。</p><blockquote class="lr"><p id="af81" class="ls lt it bd lu lv lw lx ly lz ma ld dk translated">权重修剪会修剪模型中对模型性能影响很小的参数。</p></blockquote><p id="3b5a" class="pw-post-body-paragraph ki kj it kk b kl on ju kn ko oo jx kq kr op kt ku kv oq kx ky kz or lb lc ld im bi translated"><strong class="kk iu">权重剪枝实现模型稀疏，稀疏模型压缩效率更高。</strong>修剪后的模型将具有相同的大小和运行时延迟，但具有更好的压缩性能，可在边缘实现更快的下载时间。</p><h2 id="65d4" class="mb mc it bd md me nl dn mg mh nm dp mj kr nn ml mm kv no mo mp kz np mr ms mt bi translated">部署TF Lite模型并进行推理</h2><p id="95c7" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr nq kt ku kv nr kx ky kz ns lb lc ld im bi translated">TF lite模型可以部署在Android和iOS等移动设备上，以及Raspberry和微控制器等边缘设备上。</p><p id="d3a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要从边缘设备进行推断，您需要</p><ul class=""><li id="6286" class="mu mv it kk b kl km ko kp kr nt kv nu kz nv ld nw nc nd ne bi translated">初始化解释器，并用模型加载解释器</li><li id="42b9" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nw nc nd ne bi translated">分配张量，得到输入和输出张量</li><li id="8abb" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nw nc nd ne bi translated">通过读入张量对图像进行预处理</li><li id="bf62" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nw nc nd ne bi translated">通过调用解释器对输入张量进行推理。</li><li id="b462" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nw nc nd ne bi translated">通过映射推理的结果来获得图像的结果</li></ul><pre class="lg lh li lj gt od oe of og aw oh bi"><span id="b8f1" class="mb mc it oe b gy oi oj l ok ol"># Load TFLite model and allocate tensors.<br/><strong class="oe iu">interpreter = tf.lite.Interpreter(model_content=tflite_model)<br/>interpreter.allocate_tensors()</strong></span><span id="b83d" class="mb mc it oe b gy om oj l ok ol">#get input and output tensors<br/><strong class="oe iu">input_details = interpreter.get_input_details()<br/>output_details = interpreter.get_output_details()</strong></span><span id="5596" class="mb mc it oe b gy om oj l ok ol"># Read the image and decode to a tensor<br/><strong class="oe iu">img = cv2.imread(image_path)<br/>img = cv2.resize(img,(WIDTH,HEIGHT))</strong></span><span id="724b" class="mb mc it oe b gy om oj l ok ol">#Preprocess the image to required size and cast<br/><strong class="oe iu">input_shape = input_details[0]['shape']<br/>input_tensor= np.array(np.expand_dims(img,0), dtype=np.float32)</strong></span><span id="003d" class="mb mc it oe b gy om oj l ok ol">#set the tensor to point to the input data to be inferred<br/><strong class="oe iu">input_index = interpreter.get_input_details()[0]["index"]<br/>interpreter.set_tensor(input_index, input_tensor)</strong></span><span id="06d8" class="mb mc it oe b gy om oj l ok ol">#Run the inference<br/><strong class="oe iu">interpreter.invoke()<br/>output_details = interpreter.get_output_details()[0]</strong></span></pre><p id="bef9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">还有其他方法可以改善延迟吗？</em>T19】</strong></p><p id="415a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Tensorflow lite使用委托来提高TF Lite模型在边缘的性能。<strong class="kk iu"> TF lite delegate是一种将部分图形执行交给另一个硬件加速器的方式，比如GPU或DSP </strong>(数字信号处理器)。</p><p id="6d67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TF lite使用几个硬件加速器来提高速度、准确性和优化功耗，这是在边缘运行推理的重要特性。</p><p id="7c83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结论:</strong> TF lite模型是轻量级模型，可以部署在移动设备、Raspberry Pi和微控制器等边缘设备上进行低延迟推理。当与硬件加速器一起使用时，TF lite delegate可用于进一步提高速度、准确性和功耗</p><h2 id="11bb" class="mb mc it bd md me nl dn mg mh nm dp mj kr nn ml mm kv no mo mp kz np mr ms mt bi translated">参考资料:</h2><div class="os ot gp gr ou ov"><a href="https://www.tensorflow.org/lite/guide" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">TensorFlow Lite指南</h2><div class="pc l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">www.tensorflow.org</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi ll ov"/></div></div></a></div><p id="c71f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">https://www.tensorflow.org/model_optimization/guide/pruning<a class="ae oc" href="https://www.tensorflow.org/model_optimization/guide/pruning" rel="noopener ugc nofollow" target="_blank"/></p><div class="os ot gp gr ou ov"><a href="https://www.tensorflow.org/model_optimization/guide/quantization/post_training" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">训练后量化|张量流模型优化</h2><div class="pc l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">www.tensorflow.org</p></div></div><div class="pd l"><div class="pj l pf pg ph pd pi ll ov"/></div></div></a></div><p id="0b35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae oc" href="https://www.tensorflow.org/lite/performance/model_optimization" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/lite/performance/model _ optimization</a></p></div></div>    
</body>
</html>