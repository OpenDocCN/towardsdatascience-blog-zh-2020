# 不透明的人工智能模型可能具有欺骗性。

> 原文：<https://towardsdatascience.com/opaque-ai-models-can-be-deceptive-17165cb91d61?source=collection_archive---------40----------------------->

## 我们使用的模型可能会产生昂贵的、意想不到的影响。

![](img/0e0ffd132c0f8ca093fd7b22b21671ff.png)

[Icons8 团队](https://unsplash.com/@icons8?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

就其本身而言，技术就像能源一样，是纯粹的潜力。我们如何部署它，以及出于何种目的，对它的影响至关重要。人工智能(AI)也不例外。

网飞最近的纪录片《社会困境》深入探讨了从控制我们的注意力和改变我们的行为中获利的商业利益是如何利用技术，尤其是人工智能来控制我们的。

虽然误用的可能性不是人工智能的独特特征，但人工智能模型构成更严重威胁的一些独特原因。作为这个行业的专业人士和它的“用户”,我观察到驱动人工智能的模型的不透明是我们正在冒的一个关键风险。

## 不透明的人工智能模型

去年在多伦多大学的深度学习暑期学校，我了解到大多数人工智能模型使用机器学习、深度学习或强化学习方法。机器学习方法基于高级统计建模，用于做出更好的预测。深度学习和强化学习使用神经网络。神经网络是模拟我们大脑中决策结构的多层算法。虽然不像生物神经网络那样复杂，但这些网络提供了相当真实和详细的过程模型。

高级统计方法和神经网络的问题在于，决策越复杂，它们就越不能清楚地说明模型是如何得出特定结果的。几乎不可能弄清楚最初输入模型的几个输入中的哪一个或两个驱动了这些结果。这是因为大多数此类分析都在观察新的模式并创造新的见解。

科学研究，尤其是在商业和经济研究中，直到这一点始于对世界运行方式的直观理解，然后使用数据来证实或质疑它。人工智能建模基于不同的理念。我们使用大数据集来训练算法，然后期望这样训练的模型查看其他大数据集，以获得关于世界运行方式的新见解。这种见解是有价值的，因为它们通过揭示我们用肉眼或纯逻辑思维无法辨别的模式和关系，扩展了我们对我们所居住的世界的知识。不幸的是，这并不等同于对世界有了更好的理解。这些模型和它们产生的洞见并不一定能加深我们对这些模式存在的原因和驱动因素的理解。这些模型是不透明的。

## 意想不到的后果

在这些技术处理的任务的复杂性和方法的可解释性之间有一个不可否认的权衡。应用程序越复杂，解释模型如何决定就越困难。当然，研究人员可以控制各种特征(变量)的权重，以及组合这些特征来表示交互的方式。然而，一旦模型被调整到位，它就变得神奇了，并不是所有的都可以被剖析回最重要的特性，以什么方式和为什么。

2007 年金融危机之后，我有机会与几家不同的储备银行合作。让我们对金融风险模型的不透明感到恐惧的是，基于此类模型的结果而采取的政策措施会产生“意想不到的后果”。这些模型和它们的建议过于复杂，难以理解。

类似地，预测基于黑盒型人工智能(或更早的金融工程模型)设计的行动或政策的所有后果是不可能的。

模型的不透明是导致金融危机的复杂性的根源，却没有人注意到。正如用于构建金融产品的复杂金融工程模型模糊了这些产品的实际风险一样，黑盒人工智能可能隐藏着可能会解开整个系统的关键相互联系。一个微小但关键的故障螺母可以拖垮一个庞大但相互关联的系统——这就是复杂性经济学，它同样适用于技术支持的决策系统，也适用于风险相互关联的银行系统。

## 走向可解释和更安全的人工智能

在过去的几年里，有一股巨大的推动力促使人工智能变得可以解释，消除模型偏见，并为机器学习和人工智能的道德使用建立道德和原则。

可解释的人工智能已经成为谷歌、特斯拉、微软等大型科技公司议程中不可或缺的一部分，以及许多其他将人工智能用于主流业务的公司。关注它是完全有商业意义的。不仅很难说服高管根据黑盒软件的建议采取行动，而且几乎不可能说服政策监管机构相信这种人工智能的安全性。因此，可解释的人工智能已经成为一种必然。

大学和智库也在缓慢但稳步地研究让人工智能模型更安全的科学方法。OpenAI 和人类未来研究所(Future of Humanity Institute)等智库正在推动研究和调整哪些方法可以使人工智能模型本质上更安全，即使那些无法完全解释的方法也是如此。多伦多的 Vector Institute 和英国的 Alan Turing Institute 等其他机构正在围绕人工智能模型的安全使用开展政策对话。例如，牛津大学人类未来研究所的研究员瑞安·凯里(Ryan Carey)利用他的医学博士背景，将经济学理论应用于代理人的激励，以帮助创造更安全的人工智能。

## 人工智能模型必须帮助我们进一步理解。

随着我们取得进展，我们必须记住，在人工智能的最初几十年里，我们做出和不做出的选择将对人工智能和由人工智能驱动的世界的功能产生乘数效应。

复杂模型的诱惑是巨大的，然而，人工智能产生任何有意义影响的唯一方式是拥抱简单。我们必须坚持发展对人工智能能做什么的*直觉*理解。我们需要建立模型，这些模型不仅能通过消化大量数据集得出答案，还能帮助我们加深对世界运行机制的理解。