<html>
<head>
<title>A short tutorial on Naive Bayes Classification with implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯分类及其实现的简短教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-short-tutorial-on-naive-bayes-classification-with-implementation-2f69183d8ce1?source=collection_archive---------17-----------------------#2020-08-22">https://towardsdatascience.com/a-short-tutorial-on-naive-bayes-classification-with-implementation-2f69183d8ce1?source=collection_archive---------17-----------------------#2020-08-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f075" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">朴素贝叶斯从计数开始，然后转向概率</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4ba15e58a33c60172a3305f3066f1d9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oQ2DTIFtw-xCd7OW"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:Unsplash</p></figure><p id="1baa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">朴素贝叶斯分类是数据挖掘或机器学习中最简单和最流行的算法之一(被 CRC Press Reference [1]列为十大流行算法)。朴素贝叶斯分类的基本思想非常简单。</p><p id="5139" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(如果你觉得视频格式更适合你，可以跳<a class="ae lu" href="https://www.youtube.com/watch?v=g71vdMhs57Q&amp;list=PLTS7rWcD0Do2ZoO4Sad3jRxnVFyxHd6_S&amp;index=2" rel="noopener ugc nofollow" target="_blank">这里</a>你也可以去<a class="ae lu" href="https://www.kaggle.com/saptarsi/naive-bayes-sg/" rel="noopener ugc nofollow" target="_blank">笔记本</a>。)</p><h1 id="fac7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">基本直觉:</h1><p id="33bd" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">比方说，我们有两类书。一类是体育，一类是机器学习。我统计了“匹配”(属性 1)和“算法”(属性 2)这两个词的出现频率。让我们假设，我从这两个类别各有 6 本书，这 6 本书的字数如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/473a79d5feda3898cd6cfe5bcbd5e87f.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*MwIofJjN-5-busRT99iasw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:书籍中的字数</p></figure><p id="8739" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们清楚地看到，“算法”这个词更多地出现在机器学习的书籍中，“比赛”这个词更多地出现在体育运动中。有了这些知识，假设我有一本书，它的类别是未知的。我知道属性 1 的值是 2，属性 2 的值是 10，我们可以说这本书属于体育类。</p><blockquote class="mt mu mv"><p id="843a" class="ky kz mw la b lb lc ju ld le lf jx lg mx li lj lk my lm ln lo mz lq lr ls lt im bi translated">基本上，我们想要找出哪一个类别更有可能，给定属性 1 和属性 2 的值。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/706bc7400cee335d4545f27cb3a180b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*VGIar8REaiKEfIvs6Z4-Ew.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:根据数量查找图书的类别</p></figure><h1 id="f832" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">从计数到概率:</strong></h1><p id="972d" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">这种基于计数的方法适用于少量类别和少量单词。使用<strong class="la iu">条件概率可以更好地遵循同样的直觉。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/371032b8bb9606b81f40bd8418a96baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*jBmbn3nDG8t9nkN17j1ZWw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:条件概率(图片来源:作者)</p></figure><p id="5b1c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有一个例子可以更好地理解条件概率</p><p id="6716" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们假设</p><p id="3623" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事件 A:面值为奇数|事件 B:面值小于 4</p><p id="3164" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">P(A) = 3/6(有利情况 1，3，5 总情况 1，2，3，4，5，6)类似地 P(B)也是 3/6(有利情况 1，2，3 总情况 1，2，3，4，5，6)。条件概率的一个例子是，给定一个小于 4 的奇数(A)的概率是多少(B)。为了找到这一点，我们首先找到事件 A 和 B 的交集，然后除以情况 B 中的案例数。更正式地说，这由以下等式给出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/027f95bef8c05cd3b55b7fdcebf50414.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*jcD0GmaMSiz__bCKqxUj5Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4:条件概率(图片来源:作者)</p></figure><p id="6f62" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">P(A|B)是条件概率，读作给定 B 的概率。这个等式构成了中心原则。现在让我们再回到我们的图书类别问题，我们想更正式地找到图书的类别。</p><h1 id="1419" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">条件概率到朴素贝叶斯分类器</h1><p id="4a88" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">让我们使用下面的符号 Book=ML 是事件 A，book=Sports 是事件 B,“属性 1 = 2，属性 2 = 10”是事件 C。事件 C 是一个联合事件，我们一会儿就会谈到这一点。</p><p id="e5ec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此问题变成这样，我们计算 P(A|C)和 P(B|C)。假设第一个值为 0.01，第二个值为 0.05。那么我们的结论将是这本书属于第二类。这是一个贝叶斯分类器，<strong class="la iu">朴素贝叶斯假设属性是独立的。因此:</strong></p><p id="247a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">P(属性 1 = 2，属性 2 = 10) = P(属性 1 = 2) * P(属性= 10)。我们把这些条件分别称为 x1 和 x2。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/4693550125a1950c794b18f91801d183.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*EeuhLPz7ilvcy4ThepWXVg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5:用条件概率寻找类(图片来源:作者)</p></figure><p id="8504" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，使用可能性和先验，我们计算后验概率。然后，我们假设属性是独立的，因此可能性扩展为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/191d6a07186b52db8a9427c96eec733d.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*KWn7SniFpRUVuYQNKTmS9g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6:扩展条件概率</p></figure><p id="fda4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的等式显示了两个属性，但是，可以扩展到更多。因此，对于我们的特定场景，等式变为如下。它仅在 Book='ML '中显示，在 Book ='Sports '中也会类似地显示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/180fcb5b1c4ef1f3f20c1c1e7d003008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DSM44ViRdCJteO1vvvqHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 7:书籍的朴素贝叶斯方程示例(图片来源:</p></figure><h1 id="89e0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">实施:</strong></h1><p id="84b8" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">让我们使用著名的朴素贝叶斯流感数据集，并导入它，你可以改变路径。你可以从<a class="ae lu" href="https://www.kaggle.com/saptarsi/naiveflu" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据。</p><p id="d7e4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">导入数据:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/816889cffc06f9975f9b28c0ceac609d.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*eEQu8CRtH2SKYJQ3fUNF3A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 8:流感数据集</p></figure><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="c05f" class="nm lw it ni b gy nn no l np nq">nbflu=pd.read_csv('/kaggle/input/naivebayes.csv')</span></pre><p id="ee92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">数据编码:</strong></p><p id="0418" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将列存储在不同的变量中，并对它们进行相同的编码</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="e10a" class="nm lw it ni b gy nn no l np nq"><strong class="ni iu"># Collecting the Variables</strong><br/>x1= nbflu.iloc[:,0]<br/>x2= nbflu.iloc[:,1]<br/>x3= nbflu.iloc[:,2]<br/>x4= nbflu.iloc[:,3]<br/>y=nbflu.iloc[:,4]</span><span id="f802" class="nm lw it ni b gy nr no l np nq">#<strong class="ni iu"> Encoding the categorical variables</strong><br/>le = preprocessing.LabelEncoder()<br/>x1= le.fit_transform(x1)<br/>x2= le.fit_transform(x2)<br/>x3= le.fit_transform(x3)<br/>x4= le.fit_transform(x4)<br/>y=le.fit_transform(y)</span><span id="2ddd" class="nm lw it ni b gy nr no l np nq"><strong class="ni iu"># Getting the Encoded in Data Frame</strong><br/>X = pd.DataFrame(list(zip(x1,x2,x3,x4)))</span></pre><p id="98dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">模型拟合:</strong></p><p id="83bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这一步中，我们将首先训练模型，然后为患者进行预测</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="cdbf" class="nm lw it ni b gy nn no l np nq">model = CategoricalNB()<br/><br/><em class="mw"># Train the model using the training sets</em><br/>model.fit(X,y)<br/><br/><em class="mw">#Predict Output</em><br/><em class="mw">#['Y','N','Mild','Y']</em><br/>predicted = model.predict([[1,0,0,1]]) <br/>print("Predicted Value:",model.predict([[1,0,0,1]]))<br/>print(model.predict_proba([[1,0,0,1]]))</span></pre><p id="40e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">输出:</strong></p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="374f" class="nm lw it ni b gy nn no l np nq">Predicted Value: [1]<br/>[[0.30509228 0.69490772]]</span></pre><p id="c0b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出表明不流感的概率是 0.31，流感的概率是 0.69，因此结论是流感。</p><p id="4f8d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">结论</strong>:</p><p id="5360" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">朴素贝叶斯作为基线分类器工作得非常好，它速度快，可以处理较少数量的训练样本，可以处理有噪声的数据。挑战之一是它假设属性是独立的。</p><p id="44d2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">参考:</strong></p><p id="43f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[1]吴 X，库马尔 V，编者。数据挖掘的十大算法。CRC 出版社；2009 年 4 月 9 日。</p><p id="b6e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]<a class="ae lu" rel="noopener" target="_blank" href="/all-about-naive-bayes-8e13cef044cf">https://towards data science . com/all-about-naive-Bayes-8e 13 cef 044 cf</a></p></div></div>    
</body>
</html>