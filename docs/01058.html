<html>
<head>
<title>The Book for Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习的书</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-book-for-deep-reinforcement-learning-e6a288402447?source=collection_archive---------14-----------------------#2020-01-30">https://towardsdatascience.com/the-book-for-deep-reinforcement-learning-e6a288402447?source=collection_archive---------14-----------------------#2020-01-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="88fb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从零到AlphaGo零的指南</h2></div><p id="1b8a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对AlphaGo、OpenAI Five、AlphaStar的成绩印象深刻？准备好打开引擎盖，构建自己的<a class="ae le" href="https://www.datahubbs.com/reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">强化学习模型</a>但不知道从哪里开始？没有比马克西姆·拉潘的<a class="ae le" href="https://www.datahubbs.com/63ym" rel="noopener ugc nofollow" target="_blank"> <em class="lf">深度强化学习实践</em> </a> <em class="lf"> </em>更好的书了(现在是第二版)。</p><p id="058c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在过去的二十年里，强化学习领域已经有了一本规范的教科书(<a class="ae le" href="https://www.datahubbs.com/79kp" rel="noopener ugc nofollow" target="_blank">，现在也有了第二版</a>)，但是几乎没有用编码示例来指导你的实践。为数不多的向你展示如何实现这些算法的书籍只不过是该领域流行博客文章的翻版。拉潘的书改变了这一点，成为第一本我可以完全推荐给学习这个领域的人的书。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/e6021d075476c470c7c1abd1d5fbbb33.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*77aAcs9-IJih5u4sHJB5eA.jpeg"/></div></figure><p id="ba48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我看来，Lapan的书是快速开始深度强化学习的最佳指南。它是使用<a class="ae le" href="https://wp.me/p8dQbd-7G" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>框架编写的——所以TensorFlow爱好者可能会失望——但这是这本书的魅力所在，也是它如此易于初学者理解的原因。PyTorch更容易阅读和消化，因为它的代码更简洁，让读者更专注于算法的逻辑，而不是代码示例的具体细节，代码示例本身写得很清楚，并依赖OpenAI Gym框架来简化复制。</p><p id="441f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这本书不像Packt Publishing发行的其他一些“实践”书籍，这些书籍更多地收集了要遵循的食谱，但对你被要求实现的机器学习算法几乎没有背景或解释。Lapan包括了足够的理论和对正在发生的事情的解释，使读者对深度强化学习世界感到舒适，而不会让它感觉像一本充满推导、证明等内容的沉重教科书。因此，这本书取得了健康的平衡，给读者足够的理论知识来理解算法，同时也提供了例子，使人们能够把算法付诸实践，从而赢得了“动手”的绰号。</p><p id="b7b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些例子大多来自OpenAI Gym的环境设置，车杆和pong问题比比皆是，然而，更多的现实生活中的问题也被纳入书中。在介绍了deep-Q networks (DQN的)之后，Lapan从OpenAI健身房环境转移到股票交易示例，他带领读者构建了自己的股票交易环境。如果一个人希望在预先建立的健身房环境的狭窄范围之外应用强化学习，这是一个非常宝贵的练习，因为他必须与状态表示、奖励定义、潜在行动以及手头的数据进行斗争，以做出这些决定。从健身房环境的用户那里移除这些设计决策对于学习算法和它们的机制是很好的，但是它隐藏了强化学习本身的复杂性。其他章节致力于将RL用于聊天机器人和网络导航(后一个例子依赖于OpenAI Universe，但它提出了一些关于RL其他潜在用例的有趣问题)。</p><p id="cbeb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你认为这本书只是为了让初学者快速掌握DQN和A2C算法的基础，那你就错了。它也为更高级的读者提供了许多内容，涵盖了来自<a class="ae le" href="https://github.com/openai/baselines" rel="noopener ugc nofollow" target="_blank"> OpenAI Baselines </a>版本的一些最新算法，如邻近策略优化(PPO)、信任区域策略优化(TRPO)和演员评论家Kroenecker因子信任区域(ACKTR)。这些依赖于<a class="ae le" href="https://github.com/openai/roboschool" rel="noopener ugc nofollow" target="_blank">机器人学校</a>的环境，玩起来很有趣。最后，这本书以使用进化算法对cart pole和半猎豹环境进行<a class="ae le" href="https://www.artificiallyintelligent.tech/episode-64-evolutionary-ai-with-risto-miikkulainen/" rel="noopener ugc nofollow" target="_blank">优化的章节结束，并在AlphaGo Zero章节之前查看了基于模型的方法。</a></p><p id="b6aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总之，这本书涵盖了深度强化学习领域的大量内容，但它非常好地从MDP转向了该领域的一些最新发展。我对这本书唯一的不满是作者的PyTorch代理网(PTAN)的使用。这个库由多个助手函数组成，用于与整本书使用的OpenAI环境进行接口，以完成从预处理到生成剧集和播放策略的所有工作。它很容易使用，但是为了理解关于处理训练数据、传递奖励、折扣等等的一些具体细节，我发现它混淆了这些细节。如果你想了解剧集数据在做什么，以及你是如何处理它的，你必须阅读代码本身，让它非常清楚。然而，最终这只是一个小问题，这个库确实有助于保持代码示例比其他情况下更短、更简洁。</p><p id="be93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果现在还不清楚，我全心全意地推荐<a class="ae le" href="https://www.datahubbs.com/63ym" rel="noopener ugc nofollow" target="_blank"> <em class="lf">深度强化学习实践</em> </a>以获得对深度强化学习领域的简单介绍。它充满了充分解释的例子，有足够的理论基础，读者能够快速通过这本书实现算法的每一步。</p></div></div>    
</body>
</html>