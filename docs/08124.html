<html>
<head>
<title>Value Iteration for V-function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">V 函数的值迭代</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/value-iteration-for-v-function-d7bcccc1ec24?source=collection_archive---------33-----------------------#2020-06-15">https://towardsdatascience.com/value-iteration-for-v-function-d7bcccc1ec24?source=collection_archive---------33-----------------------#2020-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2766" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/deep-r-l-explained" rel="noopener" target="_blank">深度强化学习讲解—10 </a></h2><div class=""/><div class=""><h2 id="1f8e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">v 函数在冰湖环境中的应用</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fa0f6596bcb8f7d913d0517d5a751d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDhcpkgdzkisARJU93i5xw.png"/></div></div></figure><p id="3932" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在<a class="ae lz" rel="noopener" target="_blank" href="/the-value-iteration-algorithm-4714f113f7c5">之前的帖子</a>中，我们提出了<strong class="lf jd">值迭代</strong>方法来计算基于价值的代理所需的 V 值和 Q 值。在本文中，我们将介绍如何通过求解冰湖环境来实现计算状态值的值迭代法。</p><blockquote class="ma mb mc"><p id="3fa5" class="ld le md lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><a class="ae lz" href="https://medium.com/aprendizaje-por-refuerzo/4-programaci%C3%B3n-din%C3%A1mica-924c5abf3bfc" rel="noopener">本出版物的西班牙语版本</a></p></blockquote><div class="mh mi gp gr mj mk"><a href="https://medium.com/aprendizaje-por-refuerzo/4-programaci%C3%B3n-din%C3%A1mica-924c5abf3bfc" rel="noopener follow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">4.数字电视节目</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">请访问第 4 页的自由介绍</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">medium.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="ef19" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">V 函数的值迭代在实践中的应用</h1><blockquote class="ma mb mc"><p id="8c35" class="ld le md lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">这篇文章的<a class="ae lz" href="https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_10_VI_Algorithm_for_V.ipynb" rel="noopener ugc nofollow" target="_blank">完整代码可以在 GitHub </a>上找到，并且<a class="ae lz" href="https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_10_VI_Algorithm_for_V.ipynb" rel="noopener ugc nofollow" target="_blank">可以使用这个链接</a>作为一个 Colab google 笔记本运行。</p></blockquote><p id="58ab" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">接下来，我们将详细介绍组成我们在上一篇文章中介绍的值迭代方法的代码。所以让我们来看看代码。开始时，我们导入使用的库并定义主要的常量:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="6a25" class="nw na it ns b gy nx ny l nz oa">import gym<br/>import collections<br/>from torch.utils.tensorboard import SummaryWriter</span><span id="6405" class="nw na it ns b gy ob ny l nz oa">ENV_NAME="FrozenLake-v0"</span><span id="5e08" class="nw na it ns b gy ob ny l nz oa">GAMMA = 0.9<br/>TEST_EPISODES = 20<br/>N =100<br/>REWARD_GOAL = 0.8</span></pre><h2 id="cb50" class="nw na it bd nb oc od dn nf oe of dp nj lm og oh nl lq oi oj nn lu ok ol np iz bi translated">代理的数据结构</h2><p id="386c" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">保存代理信息的主要数据结构有:</p><ul class=""><li id="a010" class="or os it lf b lg lh lj lk lm ot lq ou lu ov ly ow ox oy oz bi translated"><code class="fe pa pb pc ns b">rewards</code>:组合键为“源状态”+“动作”+“目标状态”的字典。价值是从眼前的奖励中获得的。</li><li id="5aeb" class="or os it lf b lg pd lj pe lm pf lq pg lu ph ly ow ox oy oz bi translated"><code class="fe pa pb pc ns b">transits</code>:一个表<strong class="lf jd"> </strong>作为<strong class="lf jd"> </strong>字典，保存所经历的转变的计数器。关键是复合“state”+“action”，值是另一个字典，它将目标状态映射到我们看到它的次数。</li><li id="228b" class="or os it lf b lg pd lj pe lm pf lq pg lu ph ly ow ox oy oz bi translated"><code class="fe pa pb pc ns b">values</code>:将一个状态映射到这个状态的计算值(V 值)的字典。</li><li id="504a" class="or os it lf b lg pd lj pe lm pf lq pg lu ph ly ow ox oy oz bi translated"><code class="fe pa pb pc ns b">state</code>:代理的当前状态。</li></ul><p id="76d8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">主数据结构是在代理的类构造函数中创建的。</p><h2 id="d5ef" class="nw na it bd nb oc od dn nf oe of dp nj lm og oh nl lq oi oj nn lu ok ol np iz bi translated">训练算法</h2><p id="31d9" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">我们训练算法的整体逻辑很简单。在未达到预期奖励目标之前，我们将执行以下步骤:</p><ol class=""><li id="273b" class="or os it lf b lg lh lj lk lm ot lq ou lu ov ly pi ox oy oz bi translated">从环境中随机选择 N 步来填充<code class="fe pa pb pc ns b">reward</code>和<code class="fe pa pb pc ns b">transits</code>表格。</li><li id="acd0" class="or os it lf b lg pd lj pe lm pf lq pg lu ph ly pi ox oy oz bi translated">在这 N 个步骤之后，它对所有状态执行一个值迭代步骤，更新<code class="fe pa pb pc ns b">value</code>表。</li><li id="fd62" class="or os it lf b lg pd lj pe lm pf lq pg lu ph ly pi ox oy oz bi translated">然后，我们播放几集完整的剧集，使用更新后的值表来检查改进。</li><li id="0faf" class="or os it lf b lg pd lj pe lm pf lq pg lu ph ly pi ox oy oz bi translated">如果这些测试的平均回报高于期望的界限，那么我们就停止训练。</li></ol><p id="7c42" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在我们深入了解该代码的更多细节之前，先回顾一下代理的代码会有所帮助。</p><h2 id="09e0" class="nw na it bd nb oc od dn nf oe of dp nj lm og oh nl lq oi oj nn lu ok ol np iz bi translated">代理类</h2><p id="32b1" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">在<code class="fe pa pb pc ns b">Agent</code>类构造函数中，我们创建了一个将用于数据样本的环境，获得了我们的第一个观察值，并定义了奖励、转换和值的表:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="e3dc" class="nw na it ns b gy nx ny l nz oa">class Agent: <br/>      def __init__(self):<br/>          self.env = gym.make(ENV_NAME) <br/>          self.state = self.env.reset()<br/>          self.rewards = collections.defaultdict(float)<br/>          self.transits = collections.defaultdict(<br/>                        collections.Counter)<br/>          self.values = collections.defaultdict(float)</span></pre><h2 id="bb7a" class="nw na it bd nb oc od dn nf oe of dp nj lm og oh nl lq oi oj nn lu ok ol np iz bi translated">玩随机步骤</h2><p id="cccc" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">请记住，在上一篇文章中，我们提出了转换和奖励的估计将通过代理与环境的交互历史来获得。</p><p id="d751" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是通过方法<code class="fe pa pb pc ns b">play_n_random_steps</code>完成的，它从环境中随机播放<code class="fe pa pb pc ns b">N</code>步骤，用随机体验填充<code class="fe pa pb pc ns b">reward</code>和<code class="fe pa pb pc ns b">transits</code>表格。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="21f9" class="nw na it ns b gy nx ny l nz oa">def play_n_random_steps(self, count):<br/>    for _ in range(count):<br/>        action = self.env.action_space.sample()<br/>        new_state, reward, is_done, _ = self.env.step(action)<br/>        self.rewards[(self.state, action, new_state)] = reward<br/>        self.transits[(self.state, action)][new_state] += 1<br/>        if is_done:<br/>           self.state = self.env.reset()<br/>        else:<br/>           self.state = new_state</span></pre><p id="f97c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">注意，我们不需要等到一集结束才开始学习；我们只是执行<code class="fe pa pb pc ns b">N</code>步骤并记住它们的结果。这是之前一个帖子展示的值迭代和交叉熵方法的区别之一，需要全集学习。</p><h2 id="807a" class="nw na it bd nb oc od dn nf oe of dp nj lm og oh nl lq oi oj nn lu ok ol np iz bi translated">行动的价值</h2><p id="1a70" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">下一个方法使用代理的<code class="fe pa pb pc ns b">transits</code>、<code class="fe pa pb pc ns b">reward</code>和<code class="fe pa pb pc ns b">value</code>表计算 Q 函数，即来自一个状态的动作值。我们将把它用于两个目的:从状态中选择要执行的最佳动作，并根据值迭代算法计算状态的新值。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="c870" class="nw na it ns b gy nx ny l nz oa">def calc_action_value(self, state, action):<br/>    target_counts = self.transits[(state, action)]<br/>    total = sum(target_counts.values())<br/>    <br/>    action_value = 0.0<br/>    for tgt_state, count in target_counts.items():<br/>        reward = self.rewards[(state, action, tgt_state)]<br/>        val = reward + GAMMA * self.values[tgt_state]<br/>        action_value += (count / total) * val<br/>    return action_value</span></pre><p id="7c87" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，从<code class="fe pa pb pc ns b">transits</code>表中，我们提取方法作为参数接收的给定状态和动作的转换计数器。我们对所有的计数器求和，以获得我们从状态执行动作的总次数。然后，我们迭代我们的动作已经到达的每个目标状态，并使用贝尔曼方程文章中的公式计算它对总动作值的贡献:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/e3d65da14d86ccbec6eb059b9253ba5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0de7o7wBdpgI0px8DKQWA.png"/></div></div></figure><p id="2c4e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该值等于即时奖励加上目标状态的贴现值，并将该总和乘以该转换的概率(单个计数器除以之前计算的总值)。我们将每次迭代的结果添加到一个变量<code class="fe pa pb pc ns b">action_value</code>，这个变量将被返回。</p><h2 id="7af8" class="nw na it bd nb oc od dn nf oe of dp nj lm og oh nl lq oi oj nn lu ok ol np iz bi translated">选择最佳操作</h2><p id="0114" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">为了从给定的状态中选择最佳的动作，我们有方法<code class="fe pa pb pc ns b">select_action</code>，它使用前面的<code class="fe pa pb pc ns b">calc_action_value</code>方法来做出决定:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="895c" class="nw na it ns b gy nx ny l nz oa">def select_action(self, state):<br/>    best_action, best_value = None, None<br/>    for action in range(self.env.action_space.n):<br/>        action_value = self.calc_action_value(state, action)<br/>        if best_value is None or best_value &lt; action_value:<br/>           best_value = action_value<br/>           best_action = action<br/>    return best_action</span></pre><p id="d592" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该方法的代码迭代环境中所有可能的动作，计算每个动作的值，并返回具有最大 Q 值的动作。</p><h2 id="d4b6" class="nw na it bd nb oc od dn nf oe of dp nj lm og oh nl lq oi oj nn lu ok ol np iz bi translated">值迭代函数</h2><p id="36ee" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">这里我们有一个主要功能，正如我们在上一篇文章中所描述的，值迭代方法所做的只是在环境中的所有状态上循环:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="0ccd" class="nw na it ns b gy nx ny l nz oa">def value_iteration(self):<br/>    for state in range(self.env.observation_space.n):<br/>        state_values = [<br/>              self.calc_action_value(state, action)<br/>              for action in range(self.env.action_space.n)<br/>        ]<br/>   self.values[state] = max(state_values)</span></pre><p id="dda3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于每个状态，我们用该状态可用的动作的最大值来更新它的值。</p><h2 id="658c" class="nw na it bd nb oc od dn nf oe of dp nj lm og oh nl lq oi oj nn lu ok ol np iz bi translated">训练循环和监控代码</h2><p id="b2b6" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">在介绍了代理的类及其方法之后，我们回来描述主循环。首先，我们创建将用于测试的环境，创建代理类的实例，TensorBoard 的摘要编写器，以及我们将使用的一些变量:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="a9a8" class="nw na it ns b gy nx ny l nz oa">test_env = gym.make(ENV)<br/>agent = Agent()<br/>writer = SummaryWriter()</span><span id="6b16" class="nw na it ns b gy ob ny l nz oa">iter_no = 0<br/>best_reward = 0.0</span></pre><p id="0d1b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">记住值迭代法形式上需要无限次迭代才能精确收敛以获得最优值函数。实际上，在上一篇文章中，我们展示了一旦值函数在训练循环的迭代中仅发生少量变化，我们就可以停止。</p><p id="f7f6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在这个例子中，为了保持代码简单和迭代次数少，我们决定在达到某个奖励阈值时停止。但是其余的代码都是一样的。</p><p id="0cb3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们代码的整体逻辑是一个简单的循环，它将迭代直到代理达到预期的性能(如果这些测试集的平均回报高于<code class="fe pa pb pc ns b">REWARD_GOAL</code>界限，那么我们停止训练):</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="6f41" class="nw na it ns b gy nx ny l nz oa">while best_reward &lt; REWARD_GOAL:<br/>        agent.play_n_random_steps(N)<br/>        agent.value_iteration()<br/> <br/>        ...</span></pre><p id="40f1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">循环体由前面介绍的步骤组成:</p><p id="48fa" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">步骤 1:播放调用方法<code class="fe pa pb pc ns b">plays_n_random_steps.</code>的 N 个随机步骤</p><p id="b025" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第 2 步:调用<code class="fe pa pb pc ns b">value_iteration.</code>方法，对所有状态进行值迭代扫描</p><p id="5fc4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第三步:然后我们播放几集完整的剧集，使用更新后的值表来检查改进。为此，代码使用<code class="fe pa pb pc ns b">agent.elect_action()</code>来寻找在新的<code class="fe pa pb pc ns b">test_env</code>环境中采取的最佳行动(我们不想弄乱用于收集随机数据的主环境的当前状态)，以检查代理的改进:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="ce3a" class="nw na it ns b gy nx ny l nz oa">iter_no += 1<br/>reward_avg = 0.0<br/>for _ in range(TEST_EPISODES):<br/>    total_reward = 0.0<br/>    state = test_env.reset()<br/>    while True:<br/>        action = Select_action(state)<br/>        new_state, new_reward, is_done, _ = test_env.step(action)<br/>        total_reward += new_reward<br/>        if is_done: break<br/>        state = new_state<br/>    reward_test += total_reward<br/>reward_test /= TEST_EPISODES</span></pre><p id="aca9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">此外，代码将数据写入 TensorBoard，以便跟踪最佳平均奖励:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="6e77" class="nw na it ns b gy nx ny l nz oa">writer.add_scalar("reward", reward_test, iter_no)<br/>if reward_test &gt; best_reward:<br/>    print("Best reward updated %.2f at iteration %d " % <br/>         (reward_test ,iter_no) )<br/>    best_reward = reward_test</span></pre><p id="125c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">仅此而已！</p><h2 id="2311" class="nw na it bd nb oc od dn nf oe of dp nj lm og oh nl lq oi oj nn lu ok ol np iz bi translated">运行程序</h2><p id="c228" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">好，让我们运行我们的程序:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="8123" class="nw na it ns b gy nx ny l nz oa">Best reward updated 0.40 in iteration 13 <br/>Best reward updated 0.65 in iteration 20 <br/>Best reward updated 0.80 in iteration 23 <br/>Best reward updated 0.85 in iteration 28 <br/>Best reward updated 0.90 in iteration 76</span></pre><h1 id="073a" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">测试客户端</h1><p id="49c4" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">现在，如果我们尝试使用与交叉熵相同的客户端测试代码，我们可以看到我们构建的代理可以从一个不稳定的环境中学习:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="49f7" class="nw na it ns b gy nx ny l nz oa">new_test_env = gym.make(‘FrozenLake-v0’, is_slippery=True)<br/>state = new_test_env.reset()<br/>new_test_env.render()<br/>is_done = False<br/>iter_no = 0<br/>while not is_done:<br/>    action = Select_action(state)<br/>    new_state, reward, is_done, _ = new_test_env.step(action)<br/>    new_test_env.render()<br/>    state = new_state<br/>    iter_no +=1<br/>print(“reward = “, reward)<br/>print(“iterations =”, iter_no)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi gj"><img src="../Images/f15de63d0225fb556217569dd0c59cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pLzJrx26Dv7jiNQy9MuJSg.png"/></div></div></figure><h1 id="ab52" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi">. . .</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/bc8139987e146de57716dae82e4c7f66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ru7oeHkCrV418Gakrwtig.png"/></div></div></figure><h1 id="6309" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">结论</h1><p id="0067" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">请注意，提出的算法是随机的，对于不同的执行，它采用不同的迭代次数来获得解决方案。然而，它可以从光滑的环境中学习，而不是前面提到的交叉熵。我们可以使用 Tensorboard 绘制它们，它会显示如下图所示的图形:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/68185aa579ccaae11b0bfae6e311a798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4wjJoqSI3xxV0ZWUZWkXSw.png"/></div></div></figure><p id="0edb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以注意到，在所有情况下，最多需要几秒钟就可以找到一个在 80%的运行中解决环境问题的好策略。如果你还记得交叉熵方法，对于一个湿滑的环境，花了许多小时才达到只有 60%的成功率。</p><p id="0669" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">而且，我们可以把这个算法应用到更大版本的 FrozenLake 上，这个版本的名字是 FrozenLake8x8-v0。更大版本的 FrozenLake 可能需要更多的迭代来求解，根据 TensorBoard 图表，大多数时间它会等待第一个成功的情节(它需要至少有一个成功的情节来开始从有用值表学习)，然后它会很快达到收敛。下图比较了在 FrozenLake-4x4 和 8x8 版本上训练期间的奖励动态:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/93919722bffcfcece9d0fc568564c1d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3Q6FLzP7Va7BL0ecBDEVg.png"/></div></div></figure><p id="6583" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">除了在<code class="fe pa pb pc ns b">ENV_NAME</code>(“frozen lake-v 0”vs“frozen lake 8 x8-v 0”)中改变环境外，读者还可以使用不同的超参数值进行测试，如<code class="fe pa pb pc ns b">GAMMA, TEST_EPISODE, REWARD_GOAL </code>或<code class="fe pa pb pc ns b">N</code>。你为什么不试试呢？</p><h1 id="f28a" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">下一步是什么？</h1><p id="37a5" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">在下一篇文章中，我们将展示学习动作值的值迭代方法代码，而不是像我们在这里所做的那样学习状态值。下一个帖子见<a class="ae lz" rel="noopener" target="_blank" href="/value-iteration-for-q-function-ac9e508d85bd">！</a></p><blockquote class="ma mb mc"><p id="f574" class="ld le md lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">这篇文章的<a class="ae lz" href="https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_10_VI_Algorithm_for_V.ipynb" rel="noopener ugc nofollow" target="_blank">全部代码可以在 GitHub </a>上找到，并且<a class="ae lz" href="https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_10_VI_Algorithm_for_V.ipynb" rel="noopener ugc nofollow" target="_blank">可以通过这个链接</a>作为 Colab google 笔记本运行。</p><p id="7e0f" class="ld le md lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><strong class="lf jd">鸣谢:这篇文章中的代码灵感来自于 Maxim Lapan 的代码，他写了一本关于这个主题的优秀实用书籍</strong>  <strong class="lf jd">。</strong></p></blockquote></div><div class="ab cl pn po hx pp" role="separator"><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps"/></div><div class="im in io ip iq"><h1 id="b656" class="mz na it bd nb nc pu ne nf ng pv ni nj ki pw kj nl kl px km nn ko py kp np nq bi translated">深度强化学习讲解系列</h1><p id="c09a" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated"><strong class="lf jd">由</strong> <a class="ae lz" href="https://www.upc.edu/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> UPC 巴塞罗那理工大学</strong> </a> <strong class="lf jd">和</strong> <a class="ae lz" href="https://www.bsc.es/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">巴塞罗那超级计算中心</strong> </a></p><p id="4cf4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一个轻松的介绍性<a class="ae lz" href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener ugc nofollow" target="_blank">系列</a>逐渐并以实用的方法向读者介绍这一令人兴奋的技术，这是人工智能领域最新突破性进展的真正推动者。</p><div class="mh mi gp gr mj mk"><a href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">深度强化学习解释-乔迪托雷斯。人工智能</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">本系列的内容</h3></div></div><div class="mt l"><div class="pz l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="89c0" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">关于这个系列</h1><p id="87a4" class="pw-post-body-paragraph ld le it lf b lg om kd li lj on kg ll lm oo lo lp lq op ls lt lu oq lw lx ly im bi translated">我是在五月份开始写这个系列的，那是在巴塞罗纳的<strong class="lf jd">封锁期。</strong>老实说，由于封锁，在业余时间写这些帖子帮助了我<a class="ae lz" href="https://twitter.com/hashtag/StayAtHome?src=hashtag_click" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> #StayAtHome </strong> </a>。感谢您当年阅读这份刊物；这证明了我所做的努力。</p><p id="f836" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">免责声明</strong> —这些帖子是在巴塞罗纳封锁期间写的，目的是分散个人注意力和传播科学知识，以防对某人有所帮助，但无意成为 DRL 地区的学术参考文献。如果读者需要更严谨的文档，本系列的最后一篇文章提供了大量的学术资源和书籍供读者参考。作者意识到这一系列的帖子可能包含一些错误，如果目的是一个学术文件，则需要对英文文本进行修订以改进它。但是，尽管作者想提高内容的数量和质量，他的职业承诺并没有留给他这样做的自由时间。然而，作者同意提炼所有那些读者可以尽快报告的错误。</p></div></div>    
</body>
</html>