<html>
<head>
<title>Understanding the Backbone of Video Classification: The I3D Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解视频分类的支柱:I3D架构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-the-backbone-of-video-classification-the-i3d-architecture-d4011391692?source=collection_archive---------33-----------------------#2020-06-07">https://towardsdatascience.com/understanding-the-backbone-of-video-classification-the-i3d-architecture-d4011391692?source=collection_archive---------33-----------------------#2020-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="720e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">单个图像中的信息和视频中的信息之间的显著差异之一是时间元素。这导致了深度学习模型架构的改进，以结合3D处理，从而额外处理时间信息。本文通过I3D模式总结了从图像到视频的建筑变革。</p><h1 id="6fe6" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">I3D</h1><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/5ecc706c5b76aa03c1b9429531c0b514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*MpFAlMthP4bgsVaasAITbg.png"/></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">图一。基于动力学数据集的双流I3D训练过程。图片由作者提供，改编自Carreira和Zisserman (2017) [1]。</p></figure><p id="f22b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">I3D模型是由DeepMind和牛津大学的研究人员在一篇名为“<em class="ko"> Quo Vadis，动作识别？新模型和动力学数据集</em>【1】。本文比较了以前解决视频中动作检测问题的方法，同时还介绍了一种新的体系结构，这是本文的重点。他们的方法从2D架构开始，<em class="ko">膨胀</em>所有的过滤器和池内核。通过膨胀他们，他们增加了一个额外的维度要考虑，在我们的情况下是时间。虽然2D模型中的过滤器是方形的<em class="ko"> N x N </em>，但是通过对它们充气，过滤器就变成了立方体的<em class="ko">N</em>x<em class="ko">N</em>x<em class="ko">N</em></p><p id="5ba2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇论文中，研究人员在已经训练好的模型上从2D滤波器中引导3D滤波器。换句话说，在他们的3D实现中，他们使用在非常大的数据集(如ImageNet)上训练的2D模型的参数。为了直观地思考这个问题，他们重复给定图像<em class="ko"> T </em>次。他们实际做的是沿着时间维度重复2D滤波器的权重<em class="ko"> N </em>次，然后通过除以<em class="ko"> N </em>来重新缩放它们。为确保正确完成，2D案例的平均和最大池层应与ImageNet的3D案例相同。</p><p id="e4cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一个要考虑的修改是池和卷积层的接收域。为了刷新，卷积神经网络中的<strong class="js iu">感受域</strong>是图像的一部分，一次只能被一个过滤器看到，随着我们堆叠更多的层，它会增加。2D卷积和池集中在图像的高度和宽度上，因此是对称的(例如，7×7的核是对称的，而7×3的核是不对称的)。然而，当包括时间维度时，找到最佳感受野是重要的，这取决于帧速率和图像维度。根据研究人员在[1]中的说法，如果感受野在时间上相对于空间增长过快，它可能会合并来自不同对象的边缘，破坏早期特征检测。如果感受野生长太慢，它可能无法捕捉场景动态。总之，由于额外的时间维度，I3D的核是不对称的。</p><h2 id="5db4" class="lz kq it bd kr ma mb dn kv mc md dp kz kb me mf ld kf mg mh lh kj mi mj ll mk bi translated">建筑</h2><p id="8f59" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">考虑到这些，作者用下面的图表形象化了他们的架构:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mq"><img src="../Images/ef1f5a4af22e02ed4605c22f1e4d7d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*20UNfL2azlY3Cz3M0qtJOw.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">图二。图片由作者提供，改编自Carreira和Zisserman (2017) [1]。</p></figure><p id="c7da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从图中可以看出，网络的起点使用不对称滤波器进行最大汇集，在空间维度上汇集的同时保持时间。直到后来在网络中，他们才运行包含时间维度的卷积和池。初始模块通常在2D网络中使用，不在本文讨论范围之内。然而，总的来说，它是最佳局部稀疏结构的近似。它还处理不同比例的空间(在这种情况下还有时间)信息，然后汇总结果。这个模块的目的是让网络变得“更宽”而不是“更深”。1x1x1卷积用于减少更大的3x3x3卷积之前的输入通道数，也使其计算成本低于替代方案。</p><h1 id="8add" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">贡献</h1><p id="0aa9" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">虽然该架构的正式介绍是本文的主要贡献，但主要贡献是从动力学数据集到其他视频任务的迁移学习。<a class="ae mv" href="https://deepmind.com/research/open-source/kinetics" rel="noopener ugc nofollow" target="_blank">动力学人体动作数据集</a>包含人体动作的注释视频。这就是在各种与动作相关的深度学习任务中使用预训练的I3D网络作为特征提取网络的原因。<strong class="js iu">功能通常从“混合5c”模块</strong>中提取出来，并传递到新的架构或I3D的微调版本中。</p><h1 id="a2fc" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">结论</h1><p id="aca6" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">I3D是视频处理中最常用的特征提取方法之一。虽然也实现了像S3D模型[2]这样的其他方法，但是它们是在I3D架构的基础上构建的，对使用的模块做了一些修改。如果你想对视频或视频中的动作进行分类，I3D是个好地方。如果你想为你的视频相关实验从一个预先训练好的模型中获得特征，I3D也是值得推荐的。我希望你喜欢这个总结！</p><h1 id="212e" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">参考</h1><p id="0043" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">[1]卡雷拉，j .，&amp;齐塞尔曼，A. (2017)。Quo vadis，动作识别？一个新的模型和动力学数据集。在<em class="ko">IEEE计算机视觉和模式识别会议论文集</em>(第6299–6308页)。</p><p id="d61e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2]谢，s，孙，c，黄，j，涂，z .，&amp;墨菲，K. (2018)。重新思考时空特征学习:视频分类中的速度-精度权衡。在<em class="ko">欧洲计算机视觉会议(ECCV) </em>(第305–321页)。</p><p id="57f2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">*本文中的图像与[1] </em>中的原始图像相似</p></div></div>    
</body>
</html>