<html>
<head>
<title>How to build KNN from scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 Python 从头开始构建 KNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-knn-from-scratch-in-python-5e22b8920bd2?source=collection_archive---------2-----------------------#2020-09-05">https://towardsdatascience.com/how-to-build-knn-from-scratch-in-python-5e22b8920bd2?source=collection_archive---------2-----------------------#2020-09-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="46ad" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">…嗯，至少没有 sklearn 的 KNeighborsClassifier。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9f21ae0247c48676edfe59a9353a1218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XMHv3zTlFkiYafnA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@brenoassis?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Breno Assis </a>拍照</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="0566" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">k-最近邻</h1><p id="9f5d" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">k-最近邻(KNN)是一种受监督的机器学习算法，可用于回归或分类任务。KNN 是非参数的，这意味着该算法不对数据的基本分布做出假设。这与线性回归等技术形成对比，线性回归是参数化的，需要我们找到一个描述因变量和自变量之间关系的函数。</p><p id="32a0" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">KNN 的优点是非常直观易懂。当用于分类时，查询点(或测试点)基于最接近该查询点的<strong class="ma iu"> k </strong>标记的训练点被分类。</p><p id="f18a" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">有关简化的示例，请参见下图。左侧面板显示了一个包含 16 个数据点的二维图，其中 8 个数据点标为绿色，8 个数据点标为紫色。现在，右边的面板显示了当<strong class="ma iu"> k </strong> =3 时，我们如何使用 KNN 对一个新点(黑色十字)进行分类。我们找到三个最接近的点，并计算出在这三个点内每种颜色有多少张“选票”。在这种情况下，三个点中的两个是紫色的，因此，黑色十字将被标记为紫色。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/e04c7bf6d3840e1c7fdd08f5a4c7b11c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9mN0mO61lmoj0-95i-vV7A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">当 k=3 时使用 KNN 的二维分类</p></figure><p id="947a" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><strong class="ma iu">计算距离</strong></p><p id="057e" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">通过使用闵可夫斯基距离方程的几个版本之一来确定点之间的距离。闵可夫斯基距离的通用公式可表示如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/e0596a864630370c812c73fd6bc19065.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*sJWZHgzGa9jpKZwBrSr9Og.png"/></div></figure><p id="3d7a" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">其中<strong class="ma iu"> X </strong>和<strong class="ma iu"> Y </strong>为数据点，<strong class="ma iu"> n </strong>为维数，<strong class="ma iu"> p </strong>为闵可夫斯基幂参数。当<strong class="ma iu"> p </strong> =1 时，距离已知为曼哈顿(或出租车)距离，当<strong class="ma iu"> p </strong> =2 时，距离已知为欧几里德距离。在二维空间中，两点之间的曼哈顿距离和欧几里德距离很容易可视化(见下图)，但是在更高阶的<strong class="ma iu"> p </strong>处，闵可夫斯基距离变得更加抽象。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/634ec50597d4ced86f91daff3b639ac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5q3CkkN3Ns2PwJ28jSuo4w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二维中的曼哈顿距离和欧几里德距离</p></figure><h1 id="3cb4" class="lg lh it bd li lj nc ll lm ln nd lp lq jz ne ka ls kc nf kd lu kf ng kg lw lx bi translated">蟒蛇皮 KNN</h1><p id="1afc" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">为了用 Python 实现我自己版本的 KNN 分类器，我首先想导入几个公共库来帮忙。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="fc8f" class="nj lh it bd li nk nl dn lm nm nn dp lq mh no np ls ml nq nr lu mp ns nt lw nu bi translated">加载数据</h2><p id="f351" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">为了测试 KNN 分类器，我将使用 sklearn.datasets 中的鸢尾数据集。该数据集包含 150 种鸢尾植物的测量值(萼片长度、萼片宽度、花瓣长度、花瓣宽度)，平均分为三个物种(0 = setosa，1 = versicolor，2 = virginica)。下面，我加载数据并将其存储在一个数据帧中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/aabbbffdbf1f3956e38c875b8a159a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*5OPDu02OrKk5W29wqDsvkg.png"/></div></figure><p id="7083" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我还会将数据分为要素(X)和目标变量(y)，后者是每种植物的物种标签。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="a75e" class="nj lh it bd li nk nl dn lm nm nn dp lq mh no np ls ml nq nr lu mp ns nt lw nu bi translated">建立 KNN 框架</h2><p id="7bef" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">创建一个有效的 KNN 分类器可以分为几个步骤。虽然《KNN》包含了更多的细微差别，但我还是列出了最基本的任务清单:</p><ol class=""><li id="cf36" class="nw nx it ma b mb mu me mv mh ny ml nz mp oa mt ob oc od oe bi translated">定义一个函数来计算两点之间的距离</li><li id="82a2" class="nw nx it ma b mb of me og mh oh ml oi mp oj mt ob oc od oe bi translated">使用距离函数获得测试点和所有已知数据点之间的距离</li><li id="b838" class="nw nx it ma b mb of me og mh oh ml oi mp oj mt ob oc od oe bi translated">对距离测量值进行排序，以找到离测试点最近的点(即，找到最近的邻居)</li><li id="6932" class="nw nx it ma b mb of me og mh oh ml oi mp oj mt ob oc od oe bi translated">使用那些最接近点的多数类标签来预测测试点的标签</li><li id="a512" class="nw nx it ma b mb of me og mh oh ml oi mp oj mt ob oc od oe bi translated">重复步骤 1 到 4，直到所有测试数据点都被分类</li></ol><h2 id="924e" class="nj lh it bd li nk nl dn lm nm nn dp lq mh no np ls ml nq nr lu mp ns nt lw nu bi translated">1.定义一个函数来计算两点之间的距离</h2><p id="9e51" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">首先，我定义了一个名为<strong class="ma iu"> minkowski_distance </strong>的函数，它接受两个数据点(<strong class="ma iu"> a </strong> &amp; <strong class="ma iu"> b </strong>)和一个 minkowski 幂参数<strong class="ma iu"> p，</strong>的输入，并返回两点之间的距离。请注意，这个函数计算距离的方式与我之前提到的 Minkowski 公式完全一样。通过使<strong class="ma iu"> p </strong>成为一个可调参数，我可以决定是要计算曼哈顿距离(p=1)、欧几里德距离(p=2)还是闵可夫斯基距离的更高阶。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="b04a" class="nj lh it ol b gy op oq l or os">0.6999999999999993</span></pre><h2 id="b1c9" class="nj lh it bd li nk nl dn lm nm nn dp lq mh no np ls ml nq nr lu mp ns nt lw nu bi translated">2.使用距离函数获得测试点和所有已知数据点之间的距离</h2><p id="ac2e" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">对于第 2 步，我简单地重复 X 中所有标记点的<strong class="ma iu"> minkowski_distance </strong>计算，并将它们存储在数据帧中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/cb7e3d1f890401577f9dc1707508b6c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:126/format:webp/1*VyEux8cz2Z4m47TWra-dgg.png"/></div></figure><h2 id="a13b" class="nj lh it bd li nk nl dn lm nm nn dp lq mh no np ls ml nq nr lu mp ns nt lw nu bi translated"><strong class="ak"> 3。对距离测量值进行排序，以找到最接近测试点的点</strong></h2><p id="1b40" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在第三步，我使用熊猫<strong class="ma iu">。sort_values() </strong>方法按距离排序，只返回前 5 个结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/76dc1b73b7415a2d4f19a6fbaf59a15c.png" data-original-src="https://miro.medium.com/v2/resize:fit:126/format:webp/1*YFFomr0ww6sX05OvyHStNA.png"/></div></figure><h2 id="e30f" class="nj lh it bd li nk nl dn lm nm nn dp lq mh no np ls ml nq nr lu mp ns nt lw nu bi translated">4.使用那些最接近点的多数类标签来预测测试点的标签</h2><p id="065f" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">对于这一步，我使用<strong class="ma iu">集合。计数器</strong>跟踪与最近邻点重合的标签。然后我使用<strong class="ma iu">。most_common() </strong>方法返回最常出现的标签。注意:如果“最常见”标签的标题在两个或多个标签之间有关联，那么返回的将是<strong class="ma iu"> Counter() </strong>对象首先遇到的那个标签。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="b4e9" class="nj lh it ol b gy op oq l or os">1</span></pre><h2 id="f3bd" class="nj lh it bd li nk nl dn lm nm nn dp lq mh no np ls ml nq nr lu mp ns nt lw nu bi translated">5.重复步骤 1 到 4，直到所有测试数据点都被分类</h2><p id="163e" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在这一步中，我将已经编写的代码投入使用，并编写一个使用 KNN 对数据进行分类的函数。首先，我对数据执行<strong class="ma iu">train _ test _ split</strong>(75%训练，25%测试)，然后使用<strong class="ma iu"> StandardScaler() </strong>缩放数据。由于 KNN 是基于距离的，因此在将要素输入算法之前，确保对其进行适当的缩放非常重要。</p><p id="716a" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">此外，为了避免数据泄露，在执行完<strong class="ma iu"> train_test_split </strong>之后，缩放特征<em class="ou">是一个好的做法。首先，只缩放来自训练集的数据<em class="ou"/>(<strong class="ma iu">scaler . fit _ transform(X _ train)</strong>)，然后使用该信息缩放测试集(<strong class="ma iu">scaler . transform(X _ test)</strong>)。这样，我可以确保不使用训练数据之外的信息来创建模型。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="cfdd" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">接下来，我定义一个名为<strong class="ma iu"> knn_predict </strong>的函数，它接受所有的训练和测试数据、<strong class="ma iu"> k </strong>和<strong class="ma iu"> p </strong>，并返回我的 knn 分类器对测试集做出的预测(<strong class="ma iu"> y_hat_test </strong>)。这个函数实际上并没有包含任何新的东西——它只是简单地应用了我在上面已经研究过的东西。该函数应该返回一个只包含 0、1 和 2 的标签预测列表。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="95a3" class="nj lh it ol b gy op oq l or os">[0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 2, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0]</span></pre><p id="762d" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">他们在那里！这些是这个自制的 KNN 分类器在测试集上做出的预测。让我们看看效果如何:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="b625" class="nj lh it ol b gy op oq l or os">0.9736842105263158</span></pre><p id="69ea" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">看起来分类器在测试集上达到了 97%的准确率。一点也不差！但是我怎么知道它实际上是否工作正常呢？让我们检查 sklearn 的<strong class="ma iu"> KNeighborsClassifier </strong>在相同数据上的结果:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><pre class="kj kk kl km gt ok ol om on aw oo bi"><span id="c44e" class="nj lh it ol b gy op oq l or os">Sklearn KNN Accuracy: 0.9736842105263158</span></pre><p id="78d9" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">不错！sklearn 的 KNN 分类器的实现给了我们完全相同的准确度分数。</p><h2 id="f041" class="nj lh it bd li nk nl dn lm nm nn dp lq mh no np ls ml nq nr lu mp ns nt lw nu bi translated">探索不同 k 值的影响</h2><p id="8bfb" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">我的 KNN 分类器在选择值为<strong class="ma iu"> k </strong> = 5 的情况下表现相当好。KNN 没有像决策树或随机森林等其他算法那样多的可调参数，但<strong class="ma iu"> k </strong>恰好是其中之一。让我们看看当我改变 k 时，分类精度是如何变化的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/118399af64674567fba060aa70917716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*Dl1huSnIS9g9J3WkumxS0Q.png"/></div></figure><p id="3dac" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">在这种情况下，使用几乎任何小于 20 的<strong class="ma iu"> k </strong>值都会在测试集上产生很高的(&gt; 95%)分类精度。然而，当<strong class="ma iu"> k </strong>变得大于大约 60 时，精确度确实开始下降。这是有意义的，因为数据集只有 150 个观察值——当<strong class="ma iu"> k </strong>那么高时，分类器可能会考虑离测试点太远的标记训练数据点。</p><h2 id="8e04" class="nj lh it bd li nk nl dn lm nm nn dp lq mh no np ls ml nq nr lu mp ns nt lw nu bi translated">每个邻居都有投票权——或者他们有吗？</h2><p id="5a8f" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在编写我自己的 KNN 分类器时，我选择忽略一个明显的超参数调整机会:每个最近的点在分类一个点时的权重。在 sklearn 的<strong class="ma iu"> KNeighborsClassifier </strong>中，这是<strong class="ma iu">权重</strong>参数，可以设置为<strong class="ma iu">‘均匀’</strong>、<strong class="ma iu">‘距离’</strong>，或者其他自定义函数。</p><p id="dc2a" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">当设置为<strong class="ma iu">‘uniform’</strong>时，k 个最近邻点中的每一个在标记新点时获得相同的投票。当设置为<strong class="ma iu">‘distance’</strong>时，距离新点最近的邻居的权重大于距离较远的邻居。当然，在某些情况下，通过<strong class="ma iu">“距离”</strong>进行加权会产生更好的结果，而找出答案的唯一方法就是通过超参数调整。</p><h2 id="979b" class="nj lh it bd li nk nl dn lm nm nn dp lq mh no np ls ml nq nr lu mp ns nt lw nu bi translated">最后的想法</h2><p id="a5dc" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">毫无疑问，sklearn 的实现无疑比我在这里拼凑的更加高效和用户友好。然而，我发现从头开始研究 KNN 是一个很有价值的练习，它只是巩固了我对算法的理解。我希望它对你也一样！</p></div></div>    
</body>
</html>