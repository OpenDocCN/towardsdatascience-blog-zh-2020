# A/B 测试-第 1 部分

> 原文：<https://towardsdatascience.com/a-b-testing-c9cb90d2b6aa?source=collection_archive---------34----------------------->

![](img/2c459c784be794b37dab4a5f319fb977.png)

A/B 测试是一个很难理解和解释的概念。在这篇文章中，我将带你了解 A/B 测试背后的理论。

我们将关注以下主题:

*   什么是假设检验？
*   选择正确的指标
*   设计测试
*   分析测试的结果

# 什么是 A/B 测试

A/B 测试是一种用于测试在线平台上新功能推出或现有功能变化的性能的方法。这种方法通常被称为假设检验，并被用于许多不同的领域。例如，在医学领域，研究人员进行临床试验，并使用假设检验来衡量新药的有效性。假设检验基本上是用来检验研究人员在他们的新发明背后的假设。

然而在科技领域，假设检验的用例是不同的。比方说，一家科技公司的设计团队对网站上的注册按钮进行了新的设计。我们应该继续改变设计吗，因为我们*认为*它会对我们的用户更有吸引力？

不要！我们需要测试这个新设计，看看用户对这一变化的反应。为了测试这一点，我们得到了我们人群的一个子集(我所说的人群是指我们平台的所有用户),并将他们分成两个不同的组，名为**控制组**和**测试组**。控制组中的用户将看到注册按钮的当前设计，测试组中的用户将看到新的设计。

每当我们对一个特性进行修改时，我们应该运行一个测试吗？

不要！而一个数据科学家可能会说:*请不要！*

## 但是，为什么呢？

根据测试背后的假设，在某些情况下，A/B 测试可能不会告诉您需要什么。

衡量长期效果:假设一家房屋租赁公司想要对其推荐计划进行一些更改。事实是，用户不会经常寻找公寓，所以需要几个月甚至几年的时间用户才会回到网站，因此，在这种情况下运行 A/B 测试对你没有帮助。

衡量新体验:想象你正在完全改变你的网站首页的用户界面。测试组中的老用户可能会喜欢新的 UI，并想尝试所有的东西。这被称为*新奇效应。我们可能会错误地认为测试组表现得更好。在推出新的用户体验时，我们应该始终考虑新鲜感的影响，并记住，在用户适应这种新体验以及你看到他们的真实行为之前，需要一些时间(几天、几周，取决于业务)。*

或者，您可以使用其他技术，如焦点小组或调查来获得一些定性数据，并在此基础上做出一些假设。

# 测试前分析

陈述你的假设:在运行测试甚至设计测试之前，你需要陈述你的假设。你有一个想法，你*认为*为用户提供了更好的体验，并为你的公司创造了更多的收入。识别测试中的无效假设和替代假设。在注册按钮的设计变更的例子中，你假设设计的变更会鼓励更多的用户注册。

为你的测试选择和描述度量标准:一般来说，我们需要两种不同类型的度量标准。

A.不变度量

在我们进行更改时，这些指标应该保持不变。这些度量标准的完美用途是进行健全性检查和识别实验设置中的问题。在我们的示例中，两组中查看注册按钮的用户数量应该保持不变，因为我们没有做出任何影响用户查看注册按钮的更改。

B.评估指标

这些是我们用来衡量我们所做的改变的影响的指标。评估指标分为不同的类别:

*   总和与计数
*   平均值和中位数
*   概率和比率

在工业中，概率和比率经常被用来检查测试的性能。这两者是有区别的！再次考虑我们的注册按钮:

*   点击率(CTP)= #点击注册按钮的用户数/#查看注册按钮的用户数
*   点击率(CTR)= #点击事件/#查看事件

基于上面的定义，CTP 告诉我们有百分之多少的用户会点击这个按钮。它告诉我们有多少百分比的用户进入了我们的用户漏斗的下一步。在计算 CTP 时，你不关心用户点击或查看按钮的次数。只要他们至少有一次浏览/点击，他们就会被考虑计算这个指标。恰恰相反，CTR 经常被用来发现用户点击一个按钮的频率，这使得它们非常适合用来比较两个按钮。通常，点击率的变化会告诉我们一个技术问题，比如当用户使用特定设备时无法点击按钮。

在指标选择的过程中，考虑以下三个重要的事情:

1.  考虑您的指标的敏感性和健壮性。您需要选择一个对您的变化足够敏感的度量标准，但不要太敏感。基本上，如果没有什么有趣的事情发生，度量标准不应该改变。测量您的度量标准的敏感度或健壮性的一种方法是使用运行 A/B 测试，并查看度量标准在两组之间如何变化。考虑用户在你网站的某一部分花费时间的平均值和中间值。用户在平台上花费的时间通常遵循具有长尾的对数正态分布(显示在平台上花费大量时间的用户)。根据您在产品中所做的更改，您会影响在平台上花费大量时间或中等时间的用户。这个变量(花费的时间)的平均值对在平台上花费大量时间的用户非常敏感，但它的中值实际上更稳健。
2.  为你的指标设定一个实际的重要水平。从接受测试的商业角度来看，这是您期望看到的变化量。
3.  检查您的指标的可变性。换句话说，你应该检查**多久**和**一次你的指标变化多少**。鉴于其可变性，您应该检查实际显著性水平是否可行。

# 设计测试

在选择了正确的评估和不变的度量标准之后，我们需要设计我们的测试。这是游戏最关键的部分。糟糕的实验设置会导致误导性分析和错误决策！

C 选择分流单位:分流单位是我们*根据其组成*对照组和测试组的标识符。转移的单元可以是用户 id、cookies 或事件。这里需要注意用户体验！如果您将事件随机分配给控制组和测试组，那么用户在您的网站上停留期间会多次看到控制和测试版本。你不希望你的用户有这种体验吧？您可以使用 user-id，它通常与用户的登录事件相关。但是请记住，一个用户可以拥有与多个帐户相关的多个登录。这意味着如果我改变我的帐户，我可能会在不同的组中结束，并经历不同的体验。

使用用户 id 作为转移单位的另一个缺点是，当变化从测试组泄漏到控制组时，会影响控制组的行为。想象一下，一个约会应用程序允许测试组的用户在他们的个人资料中写一篇关于自己的短文，假设他们的体验会更好(他们会收到更多的消息等等)。同时，我们令人惊奇的控制用户会看到测试用户的个人资料，这也会影响他们的行为。因为他们突然发现了更多有趣的用户！这就是所谓的网络效应。

为了防止网络效应，我们可以按社区运行测试(一个社区中的用户)。但是困难的部分是在你的产品中定义一个社区。社区可以是彼此交互最多的用户，也可以是处于相同地理位置的用户。

使用 cookies 是另一种选择。Cookies 将被分配给特定的浏览器或设备！因此，如果用户更换他们的设备，他们可能会经历不同的体验。

明智地选择你的测试分流单位！检查*您的用户*在使用您的产品时拥有多个帐户或更换设备的可能性。

C 选择合适的人群:选择你想要锁定的用户子集。这可能基于他们的平台、国家或语言。当脸书推出反应表情符号时，他们必须包括表达情感在文化上并非罕见的国家。想想看，如果测试组中看到新反应特性的用户普遍缺乏表达能力，会发生什么！

S 确定实验的规模:你需要找出测试组和对照组中需要多少用户，才能得到有统计意义的结果。确定测试规模时要考虑的事情有:

*   评估指标及其可变性
*   实际意义水平
*   分流单位

实验的时机:你需要知道什么时候是进行实验的最佳时机。这取决于您的业务、您的实验所需的样本量以及评估指标。

测试的置信水平和功效:95%的置信水平和 80%的功效是工业上最常用的。

# 分析测试的结果

现在我们已经进行了实验，我们有两个组(测试组和对照组)。请记住，我们没有将测试暴露给整个人群，我们从人群中抽取样本。

我们需要回答的问题是，我们观察到的这两组之间的差异是偶然发生的可能性有多大？

检查不变指标:如前所述，这些指标用于健全性检查。**如果测试组和对照组之间的这些指标值存在显著差异，则不要继续分析。试着找出测试的问题。问题出在实验装置上吗？也许一个事件没有被正确捕获，也许你为某种语言传递的过滤器没有工作！在这些情况下，工程师和分析师是最好的帮手。如果您的不变度量的值是预期的，那么进入下一步。**

分析结果:终于，我们到了激动人心的部分！还记得我们之前描述的注册按钮测试吗？假设我们决定将点击率作为我们的评估指标。

我们所拥有的数据通常是查看过该按钮的用户数量和点击过该按钮的用户数量。因此，我们可以计算出测试组和对照组中有人点击的*估计概率*。

然后我们计算这两个概率的差(称之为 d)。基于零假设，这两个概率应该没有区别(d 为零)，但是替代假设说有区别(d 不为零)。

计算差额还不够！你需要围绕它建立一个置信区间。你需要做的是检查观察到的 d 在置信区间中的位置。置信区间告诉你有 95%可能性的 d 的**真值位于置信区间内的某处。如果你的置信区间不包含零，那么你就有充分的证据证明测试组和对照组的 CTP 之间存在有意义的差异。**

在对结果过于兴奋之前，请记住您为测试设置的实际显著性水平，并查看它相对于您的置信区间和 d 的观察值的位置。如果结果是显著的，并且我们观察到与对照组相比，测试中增加了 1%,但实际显著性水平是 2%,那么您需要非常小心地接受测试！在接受一个没有给出最低实际意义水平的测试之前，需要考虑工程师构建一个特性并在全球公开它背后的努力。

这是对 A/B 测试背后理论的快速总结和演练。下一个帖子会有更详细的计算， [A/B 测试部分](https://medium.com/@gharibkimia/a-b-testing-part-2-42b94e1fb1dc) 2。

# 资源

*   [Udacity 的 A/B 测试课程](https://www.udacity.com/course/ab-testing--ud257)
*   [A/B 测试的陷阱](https://tech.okcupid.com/the-pitfalls-of-a-b-testing-in-social-networks/)