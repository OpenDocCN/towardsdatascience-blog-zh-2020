# 人工智能统计学(下)

> 原文：<https://towardsdatascience.com/statistics-for-ai-part-2-43d81986c87c?source=collection_archive---------51----------------------->

## 构成复杂算法基础的向量和矩阵概念

今天我打算讨论以下主题:线性独立性、特殊矩阵和矩阵分解。

# 线性独立性

如果这些向量中没有一个可以写成其他向量的线性组合，那么这个向量集就是线性无关的。例如，V1=(1，0)和 V2=(0，1)。在这里，V2 不能用 V1 来写。然而，V3 (3，4)是线性相关的，因为 V3 可以表示为 3V1+4V2。

数学上，s={V1，V2，…。，Vn}线性无关当且仅当线性组合α1V1+α2V2+…..+αnVn=0 意味着所有αi=0。

# 矩阵运算

矩阵可以将一个向量转换成另一个向量。例如，V 是 Nx1 向量，w 也是 Nx1 向量。

![](img/2434bd14be40ef4b69c155110b6de8df.png)

矩阵乘法通过保持形状，图像由作者

# 矩阵的迹

矩阵的迹由其对角元素的和给出。对于矩阵 A，其迹将是行和列具有相同值的所有元素的总和。

![](img/5dd5007bf69047b2b99dce46e12aa5b0.png)

矩阵的轨迹，作者的图像

# 一些属性

1.  Tr(A+B) = Tr(A)+Tr(B)
2.  Tr(AB) = Tr(BA)
3.  Tr(A) = Tr(A.T) (A.T 表示矩阵 A 的转置)

# 矩阵的行列式

NxN 矩阵的拉普拉斯展开由以下公式给出:

![](img/8a037e699a63da49e90a9b98ef10d203.png)

矩阵的行列式，作者图片

行列式实际上表示由列向量形成的体积。对于 2x2 向量，它表示面积。

![](img/748c4bf0ddf215d3f1caa234da4a2646.png)

解释空间中的 2x2 向量，图片由作者提供

# 矩阵的可逆性

只有当 det(A)不为 0 时，矩阵 A 的逆矩阵才有可能。请注意，这自动意味着的列必须是线性独立的。考虑下面的矩阵。

![](img/9aa413685c16ef497c918709cde7d831.png)

矩阵 A，作者图片

请注意，V1、V2……，Vn 是向量，如果任何向量，比如说 Vn，可以写成其余向量的线性相关向量，比如 Vn=α1V1+α2V2+…..+αn-1Vn-1 然后，我们可以做一个简单的列操作，即最后一列=最后一列- (α1V1+α2V2+…..+αn-1Vn-1)，这将产生充满零的列。这会使矩阵的行列式为 0。对于一个 2x2 矩阵，我们将有两个向量 V1 和 V2。如果 V1 和 V2 是线性相关的，像 V1=2V2，那么由这两个向量形成的面积将为零。一个聪明的说法是，这两个向量相互平行。

# 特殊矩阵和向量

1.  对角矩阵:只有对角元素不为零，其余所有元素为零。如果 I 不等于 j，则 D(i，j) = 0。
2.  对称矩阵:如果一个矩阵及其转置矩阵相等，则称该矩阵对称。
3.  单位向量:具有单位长度的向量。向量的 2-范数是 1。
4.  正交向量:如果(X.T)Y = 0，则两个向量 X 和 Y 是正交的
5.  正交矩阵:如果一个矩阵的转置等于它的逆矩阵，那么我们可以说这个矩阵是正交的。此外，所有列都是正交的。正交矩阵可用于旋转保持体积的向量。
6.  标准正交矩阵:如果一个矩阵的逆矩阵等于它的单位行列式转置矩阵，则称这个矩阵是标准正交的。

![](img/cd3c4431e6cd5cbe398ab6cb27e32e8d.png)

正交矩阵，作者图片

![](img/75ac31f34bd4ce34f945bc5a009ba394.png)

正交矩阵，作者图片

# 特征分解

特征分解对于正方形对称矩阵非常有用。让我们看看这个术语的物理意义。

每一个实矩阵都可以认为是旋转和拉伸的组合。

![](img/e2567e2cf58b07a139fafb8f2bbda7a0.png)

向量乘法，作者图片

![](img/b6866b518247577fcc8dcf2f10a73fd5.png)

对向量 v 进行运算，生成向量 w，由作者生成图像

这里，A 可以被认为是一个算子，它拉伸并旋转一个向量 v 以获得一个新的向量 w。矩阵的特征向量是那些只在矩阵的作用下拉伸的特殊向量。特征值是特征向量拉伸的因子。在下面的等式中，当与特征向量 a 一起操作时，向量 v 被拉伸了λ的值

![](img/4da18dfa758e8d8e79512a817c021f5c.png)

向量 v 的特征值λ，图片作者

比如说，A 有 n 个线性无关的特征向量{V1，V2，…..，Vn}。将所有向量串联成一列，我们得到一个特征向量矩阵 V，其中 V=[V1，V2，…..，Vn】。如果我们将相应的特征值连接成一个对角矩阵，即λ**=**diag(λ1，λ2，…，λn)，我们得到 A 的特征分解(因式分解)如下:

![](img/3d3fd1b6bc1cfe5b8466640e518eb756.png)

作者图像的特征分解

实对称矩阵有实特征向量和实特征值。

![](img/b69f5ee708bc426812ea06781fbf9e2b.png)

实对称矩阵，图像 y 作者

# 二次型与正定矩阵

二次型可以解释为“加权”长度。

![](img/7cc89dd6fa781ec1ffce2f8aa3744368.png)

二次型，作者图片

![](img/763109da0cc0d2a32992a201ea72fd2a.png)

二次型，作者图片

正定(PD)矩阵的所有特征值都大于零。半正定(PSD)矩阵具有大于等于零的特征值。PD 矩阵具有对于所有 X，(X.T)AX 大于 0 的性质。例如，如果 A=I 或单位矩阵，则(X.T)I(X)=(X.T)(X)大于 0。PSD 矩阵具有对于所有 X，(X.T)AX 大于等于 0 的性质。类似地，负定(ND)矩阵的所有特征值都小于零。半负定(PD)矩阵的所有特征值都小于等于零。

# 奇异值分解

如果 A 是一个 MxN 矩阵，那么

![](img/586adc50fb1a52d27d6f50c092f29db6.png)

奇异值分解，作者图片

1.  u 是一个 MxM 矩阵并且是正交的
2.  v 是一个 NxN 矩阵并且是正交的
3.  d 是一个 MxN 矩阵和对角线
4.  U 的元素是 A(A.T)的特征向量，称为左奇异向量
5.  (A.T)A 的特征向量，称为右奇异向量
6.  D 的非零元素是平方根(λ((A.T)(A)))，这意味着(A.T)(A)的特征值的平方根，称为奇异值

# 结束

谢谢，请继续关注更多关于人工智能的博客。