<html>
<head>
<title>Compute the Incomputable | How SVI and ELBO work</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算无法计算的| SVI和艾尔波是如何工作的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/compute-the-incomputable-how-svi-and-elbo-work-505ce0868fdd?source=collection_archive---------50-----------------------#2020-06-22">https://towardsdatascience.com/compute-the-incomputable-how-svi-and-elbo-work-505ce0868fdd?source=collection_archive---------50-----------------------#2020-06-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="04ba" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">贝叶斯建模与真实世界数据一起工作的一个原因。随机海洋中的近似灯塔。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/604ab8fe9405cd4dff937512dd9e5433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FC7yMsyoeZl5Zh-uVR5ZA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="kv">照片由威廉·布特在U</em><a class="ae kw" href="https://unsplash.com/photos/7cdFZmLlWOM" rel="noopener ugc nofollow" target="_blank"><em class="kv">n spash</em></a>上拍摄</p></figure><p id="73d7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">当你想对你的数据获得更多的洞察力时，你依赖于编程框架，它允许你与概率进行交互。开始时你所拥有的只是数据点的集合。它只是对数据来源的底层分布的一瞥。然而，您最终不仅仅想要简单的数据点。您想要的是精细的、健谈的密度分布，您可以用它来执行测试。为此，您使用概率框架，如<a class="ae kw" href="https://www.tensorflow.org/probability" rel="noopener ugc nofollow" target="_blank">张量流概率</a>、<a class="ae kw" href="https://pyro.ai/" rel="noopener ugc nofollow" target="_blank"> Pyro </a>或<a class="ae kw" href="https://mc-stan.org/" rel="noopener ugc nofollow" target="_blank"> STAN </a>来计算概率的后验概率。<br/>正如我们将看到的，这种计算并不总是可行的，我们依靠马尔可夫链蒙特卡罗(MCMC)方法或随机变分推断(SVI)来解决这些问题。特别是对于大型数据集，甚至是每天的中型数据集，我们必须执行神奇的采样和推理来计算值和拟合模型。如果这些方法不存在，我们将被我们想出来的一个整洁的模型所困，但是没有办法知道它实际上是否有意义。<br/>我们使用Pyro构建了一个预测模型，该模型使用的正是——SVI:</p><div class="lt lu gp gr lv lw"><a rel="noopener follow" target="_blank" href="/pyro-top-down-forecasting-application-case-4781eb2c8485"><div class="lx ab fo"><div class="ly ab lz cl cj ma"><h2 class="bd ir gy z fp mb fr fs mc fu fw ip bi translated">Pyro自上而下预测|应用案例</h2><div class="md l"><h3 class="bd b gy z fp mb fr fs mc fu fw dk translated">将一段时间内的点连接起来，并满怀信心地进行预测(-区间)。</h3></div><div class="me l"><p class="bd b dl z fp mb fr fs mc fu fw dk translated">towardsdatascience.com</p></div></div><div class="mf l"><div class="mg l mh mi mj mf mk kp lw"/></div></div></a></div><p id="73e2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这个故事中，我们将从Pyro的角度来看计算。<br/>简而言之，Pyro并没有强行计算后验概率，而是做出了一个我们可以改进的估计。但是为什么我们不能计算整个事情呢？很高兴你问了。</p><h1 id="bde9" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">从无穷大到常数</h1><p id="4b5a" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">假设我们有一个数据集<em class="ni"> D </em>，我们想要拟合一个模型(θ)来准确描述我们的数据。为此，我们需要一组变量。这些变量对应于随机事件，我们称之为潜在随机变量，例如<em class="ni"> z. </em> <br/>现在来看看我们的模型对后验概率分布<em class="ni"> P </em>的依赖程度。这是一个很好的起点，现在我如何计算这个P？为了让这个工作，我们必须做这样的推论:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/58b4f2d02524e47e115077c5cd485a0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*ZnZmCMQpxUT744q48rmc3g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(1) —给定我们数据中的观察值x，计算潜在随机变量z的后验概率。</p></figure><p id="fc76" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">精通数学的读者已经看到上面等式的右边部分通常是不可计算的。我们面对未知事物上的全能积分的恐惧。现在把z作为连续变量，它可能是无限的。那相当大。<br/>我们能做的是用一个<strong class="kz ir">变分分布</strong>；姑且称之为<em class="ni"> Q </em>。在一个理想的世界中，这个近似的Q应该尽可能的接近我们正在计算的P。那我们怎么去呢？<br/>我们首先需要的是P和Q 相似程度的<strong class="kz ir">度量。输入Kullback Leibler散度——你可以在数理统计年鉴中找到Kullback和Leibler的传奇论文[1]。<br/>我们用来比较分布的工具，Kullback Leibler散度被<br/>定义为:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/bcb3999da06328bdfb8446305e60aaa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*tRQMRNgEbXq_6N88E9t9tw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(2) —定义库尔贝克-莱布勒散度</p></figure><p id="e591" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这个度量告诉我们分布<em class="ni"> P </em>和<em class="ni"> Q </em>有多相似。如果它们相似，KL就低，如果它们非常不同，差异就大。</p><p id="f947" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在，这个度量要求我们知道分布P和q，但是我们还不知道P。毕竟，这是我们正在计算的，到目前为止，我们的Q是一个野生猜测。<br/>经过一些重新表述，我们最终得到了这个(参见[5]的分步解决方案):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/8deec3533f89d28c156bf1600107ae82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*KxMCoXiInXcwzb_zGzDRrQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(3) —从KL导出ELBO，作为与常数log(P)的和</p></figure><p id="1870" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">第一部分(-E[log P(z，x)-log Q(z|x)])就是所谓的<strong class="kz ir">证据下界</strong>或<strong class="kz ir"> ELBO </strong>。第二部分(log(P))是一个常数。这对我们以后的计算更有利。我们现在要做的就是最大化ELBO，使P和q之间的偏差最小。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="e8e5" class="nq mm iq nm b gy nr ns l nt nu">Minimizing KL divergence corresponds to maximizing ELBO and vice versa.</span></pre><h1 id="d339" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">用深度学习学习Q</h1><p id="60d5" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">既然我们已经有了基本的理论，那么我们该如何处理我们的问题呢？我们可以通过使用某种形式的神经网络来估计Q。我们已经有了TensorFlow或者PyTorch来处理Pyro。与我们已经建立的模型相比，我们现在可以建立一些东西来描述我们的变分分布。一旦我们有了这个分布估计，我们就可以使用<a class="ae kw" href="https://leon.bottou.org/papers/bottou-mlss-2004" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>来拟合我们的模型。所以让我们开始吧。</p><p id="c436" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们使用我们的数据作为模型和指南的输入。模型给了我们P，而指南建议网络输出一个Q。如果这是高斯分布，那么网络会给出平均值μ和标准差σ。如果我们用一个非常简单的形式来描绘这个过程，随着时间的推移，它看起来就像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1c6a0c0bb1c8c08cae10e922074e2ff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*cwA3hRsRVbPaa-AcP7_uRw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1 — <em class="kv">计算n个时间步上的后验概率时，用向导拟合模型的过程。</em></p></figure><h1 id="e719" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">跟我说说Pyro</h1><p id="ee14" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">让我们举一个简单的例子，给定潜在随机变量<em class="ni"> z </em>，我们拟合一个模型函数<em class="ni"> f </em>。该变量来自β分布(仅作为一个例子)。我们观察到的事件是伯努利随机发生的。这意味着事件要么发生，要么不发生。最后，事件是独立同分布的，我们写为<em class="ni">烟火板</em>符号。所以我们的模型是:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="70ff" class="nq mm iq nm b gy nr ns l nt nu">def model(self):<br/>        # sample `z` from the beta prior<br/>        f = pyro.sample("z", dist.Beta(10, 10))<br/>        # plate notion for conditionally independent events given f<br/>        with pyro.plate("data"):<br/>            # observe all datapoints using the bernoulli distribution<br/>            pyro.sample("obs", dist.Bernoulli(f), obs=self.data)</span></pre><p id="27aa" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">到目前为止，这是标准的烟火，你不必总是指定一个向导。Pyro可以处理，但是会有一个向导在那里。通过指定向导，你可以<em class="ni">完全控制你的建模过程。在这个例子中，向导的潜在z来自于此:</em></p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="7965" class="nq mm iq nm b gy nr ns l nt nu">def guide(self):<br/>        # register the two variational parameters with pyro<br/>        alpha_q = pyro.param("alpha_q", torch.tensor(15),<br/>                             constraint=constraints.positive)<br/>        beta_q = pyro.param("beta_q", torch.tensor(15),<br/>                            constraint=constraints.positive)<br/>        pyro.sample("latent_z", NonreparameterizedBeta(alpha_q, beta_q))</span></pre><p id="9db8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">最后，我们通过使用SVI对象来运行我们的推理。我们提供先前指定的模型和指定的指南。我们还依赖标准PyTorch <a class="ae kw" href="https://pytorch.org/docs/stable/optim.html" rel="noopener ugc nofollow" target="_blank"> Adam optimizer </a>进行随机梯度下降。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="5d71" class="nq mm iq nm b gy nr ns l nt nu">def inference(self):<br/>        # clear everything that might still be around<br/>        pyro.clear_param_store()<br/>        # setup the optimizer and the inference algorithm<br/>        optimizer = optim.Adam({"lr": .0005, "betas": (0.93, 0.999)})<br/>        svi = SVI(self.model, self.guide, optimizer, loss=TraceGraph_ELBO())</span></pre><p id="9e2a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">你都准备好了。现在我们来看一些输出。</p><p id="92df" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">上面的例子只是伪代码，为了获得正确的输出，我使用了一个简单的贝叶斯回归任务，并计算了模型函数的后验概率(具体细节请参见我的<a class="ae kw" href="https://github.com/RMichae1/PyroStudies/blob/master/PyTorch_BayesRegr.ipynb" rel="noopener ugc nofollow" target="_blank">github</a>)。<br/>我们使用Pyro SVI和Markov Chain Monte Carlo程序以及NUTS采样器。</p><h1 id="3aad" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">丑陋的真相|比较SVI和MCMC</h1><p id="2a2d" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">到目前为止，一切听起来都很有希望——好得令人难以置信。对于生活中的每一件事，你应该经常检查你的结果。下面的例子(图2-3)显示了前面提到的<a class="ae kw" href="https://github.com/RMichae1/PyroStudies/blob/master/PyTorch_BayesRegr.ipynb" rel="noopener ugc nofollow" target="_blank">回归任务</a>的拟合后验分布，并同时使用SVI和MCMC。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/a21c2dc275b156b68d5ee3d1171663d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2iNBqIda09poyEb024841g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="kv">图2——使用SVI和MCMC对具有两个变量的函数f进行贝叶斯回归。</em></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/e54325b05b5f0dcad1a01d983a9b0d33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vVk05onK5UGN2rNE4LZ2fg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="kv">图3——模型拟合后，贝叶斯回归任务的密度分布与后验分布重叠的等高线图。两个模型变量bF2的相互作用和相互作用项bF12。</em></p></figure><p id="06e3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">先说好的一点:我们已经为SVI和麦克米伦安排了足够的后验概率。不太理想的是，它们明显不同。对马尔可夫方法和随机推理如何以及为什么不同的深入理解是各种研究的主题，深入的理解将是另一篇文章的主题。在这一点上，我们唯一能得出的结论是<strong class="kz ir">推理程序对我们最终的模型</strong>有明显的影响。</p><h1 id="b3e7" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">整洁——SVI作品。关键点。</h1><ol class=""><li id="6579" class="ny nz iq kz b la nd ld ne lg oa lk ob lo oc ls od oe of og bi translated">我们正视了一个看似不可行的问题，并研究了潜在过程的理论，以找出解决它的方法。</li><li id="eab4" class="ny nz iq kz b la oh ld oi lg oj lk ok lo ol ls od oe of og bi translated">你可以使用深度学习框架，如TF和PyTorch，通过近似另一个模型-分布来建立你的概率模型。</li><li id="5ef4" class="ny nz iq kz b la oh ld oi lg oj lk ok lo ol ls od oe of og bi translated">这一切的目标是最小化我们的两个近似后验之间的分歧(或最大化下界)。</li><li id="b9f6" class="ny nz iq kz b la oh ld oi lg oj lk ok lo ol ls od oe of og bi translated">我们应该总是看着我们的输出，问自己我们所看到的是否有意义。取样程序显然会影响我们的后验结果——所以我们应该小心。</li></ol><p id="1d9c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">总而言之，SVI允许我们采用任何数据集，并用它进行贝叶斯建模。如果你问我，这是一个非常好的功能。</p></div><div class="ab cl om on hu oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="ij ik il im in"><h1 id="a42a" class="ml mm iq bd mn mo ot mq mr ms ou mu mv jw ov jx mx jz ow ka mz kc ox kd nb nc bi translated">参考</h1><ol class=""><li id="b1ed" class="ny nz iq kz b la nd ld ne lg oa lk ob lo oc ls od oe of og bi translated">南Kullback和r . lei bler<em class="ni">关于信息和充分性</em>。1951年在<a class="ae kw" href="https://projecteuclid.org/euclid.aoms/1177729694" rel="noopener ugc nofollow" target="_blank">数理统计年鉴</a>。</li><li id="0a5f" class="ny nz iq kz b la oh ld oi lg oj lk ok lo ol ls od oe of og bi translated">D.温盖特和t .韦伯。<em class="ni">概率规划中的自动变分推理</em>。2013年<a class="ae kw" href="https://arxiv.org/abs/1301.1299" rel="noopener ugc nofollow" target="_blank">月arxiv </a>日。</li><li id="bbf8" class="ny nz iq kz b la oh ld oi lg oj lk ok lo ol ls od oe of og bi translated">Pyro文档— <a class="ae kw" href="https://pyro.ai/examples/svi_part_i.html" rel="noopener ugc nofollow" target="_blank"> SVI教程一</a></li><li id="7ef9" class="ny nz iq kz b la oh ld oi lg oj lk ok lo ol ls od oe of og bi translated">烟火文档— <a class="ae kw" href="https://pyro.ai/examples/svi_part_ii.html" rel="noopener ugc nofollow" target="_blank"> SVI教程二</a></li><li id="8383" class="ny nz iq kz b la oh ld oi lg oj lk ok lo ol ls od oe of og bi translated">烟火文档— <a class="ae kw" href="https://pyro.ai/examples/svi_part_iii.html" rel="noopener ugc nofollow" target="_blank"> SVI教程三</a></li><li id="7507" class="ny nz iq kz b la oh ld oi lg oj lk ok lo ol ls od oe of og bi translated">礼萨·巴巴纳扎德。<em class="ni">随机变分推理。</em> <a class="ae kw" href="https://www.cs.ubc.ca/labs/lci/mlrg/slides/SVI.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ni"> UBC </em> </a></li><li id="fc83" class="ny nz iq kz b la oh ld oi lg oj lk ok lo ol ls od oe of og bi translated">D.马德拉斯。<a class="ae kw" href="http://www.cs.toronto.edu/~madras/presentations/svi-tutorial.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ni">教程随机变分推理</em> </a> <em class="ni">。多伦多大学。2017.</em></li></ol><h2 id="e4fd" class="nq mm iq bd mn oy oz dn mr pa pb dp mv lg pc pd mx lk pe pf mz lo pg ph nb pi bi translated">完整的烟火例子</h2><p id="3a5d" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">上面提到的例子只是作为一个关于如何建立一个Pyro类的粗略概述。功能代码可在Pyro文档中找到(参见[ <a class="ae kw" href="https://pyro.ai/examples/svi_part_iii.html" rel="noopener ugc nofollow" target="_blank"> 5 </a> ])。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="069e" class="nq mm iq nm b gy nr ns l nt nu">class BetaExample:<br/>    def __init__(self, data):<br/>        # ... specify your model needs here ...<br/>        # the dataset needs to be a torch vector<br/>        self.data = data</span><span id="bf5e" class="nq mm iq nm b gy pj ns l nt nu">    def model(self):<br/>        # sample `z` from the beta prior<br/>        f = pyro.sample("z", dist.Beta(10, 10))<br/>        # plate notion for conditionally independent events given f<br/>        with pyro.plate("data"):<br/>            # observe all datapoints using the bernoulli likelihood<br/>            pyro.sample("obs", dist.Bernoulli(f), obs=self.data)</span><span id="99be" class="nq mm iq nm b gy pj ns l nt nu">    def guide(self):<br/>        # register the two variational parameters with pyro<br/>        alpha_q = pyro.param("alpha_q", torch.tensor(15),<br/>                             constraint=constraints.positive)<br/>        beta_q = pyro.param("beta_q", torch.tensor(15),<br/>                            constraint=constraints.positive)<br/>        pyro.sample("latent_z", NonreparameterizedBeta(alpha_q, beta_q))</span><span id="3c49" class="nq mm iq nm b gy pj ns l nt nu">    def inference(self):<br/>        # clear the param store<br/>        pyro.clear_param_store()<br/>        # setup the optimizer and the inference algorithm<br/>        optimizer = optim.Adam({"lr": .0005, "betas": (0.93, 0.999)})<br/>        svi = SVI(self.model, self.guide, optimizer, loss=TraceGra</span></pre></div></div>    
</body>
</html>