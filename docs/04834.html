<html>
<head>
<title>Statistical Learning (IV): Support Vector Machine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">统计学习(四):支持向量机</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/statistical-learning-iiii-support-vector-machine-632df8da0b41?source=collection_archive---------51-----------------------#2020-04-27">https://towardsdatascience.com/statistical-learning-iiii-support-vector-machine-632df8da0b41?source=collection_archive---------51-----------------------#2020-04-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e80d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">遍历最大间隔分类器、支持向量分类器和支持向量机。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1ced5606cb03f91fba6f0c64d919a2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c9ifpaP0rhndBi1UJE7lxQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://www.shutterstock.com/g/portfolio" rel="noopener ugc nofollow" target="_blank">Kit8.net</a>从快门股票</p></figure><p id="736d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> S </span>支持向量机是一种常用于文本分类的模型。然而，SVM也是一种适用于线性问题的机器学习算法。了解了SVM的概念，如超平面和核，您将更好地了解如何将该模型应用于回归和分类问题！</p><p id="d347" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，您将了解到:</p><p id="e090" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(1)用最大间隔分类器处理SVM上的线性问题</p><p id="0252" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(2)非线性问题SVM核的引入</p><p id="4bd9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(3)SVM在Python文本分类中的应用</p><h1 id="bcd5" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">最大间隔分类器</h1><p id="680c" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">当数据点分布在多于2维的空间中时，应用超平面将它们分成两半。从下面的图中，它显示了两类观察结果，蓝点表示一类，紫色点表示另一类。对于左边的一个，它显示有3个超平面将数据点分成2类。对于右边的一个，数据点被分成两半，yi=1属于蓝色类，yi=-1属于紫色类。因此，数据点被分为两类，遵循以下等式:</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="636d" class="nd mc iq mz b gy ne nf l ng nh">B0+B1Xi1+B2Xi2+B3Xi3+.....+BpXip &gt; 0 if yi = 1<br/>B0+B1Xi1+B2Xi2+B3Xi3+.....+BpXip &lt; 0 if yi = -1</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/bf080a7390e836e6608ac8136cdc2f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ifDBjy9dTuSikiclFuKQ1Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">摘自《统计学习导论》第340页</p></figure><h2 id="16d2" class="nd mc iq bd md nj nk dn mh nl nm dp ml lf nn no mn lj np nq mp ln nr ns mr nt bi translated">如何决定使用哪个超平面？</h2><p id="473b" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">最大间隔分类器基于数据点和超平面内的间隔总和的值来决定最佳超平面。分类器通过从训练观察中选取最远的来选择超平面。最大间隔分类器的等式定义如下:</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="3c0f" class="nd mc iq mz b gy ne nf l ng nh"><strong class="mz ir">yi(β0 + β1xi1 + β2xi2 +...+ βpxip)≥ M ∀i=1,...,n</strong></span></pre><p id="85b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">m是应该最大化的超平面的边缘。<br/> yi，…，yn∈{-1，1}属于标签<br/> x1，…，xn ∈ Rp属于训练观测值</p><p id="7e4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分类器将通过选择B0，B1，B2，…来最大化M。，Bp来优化问题。</p><p id="7c8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，就非分离问题而言，不存在用于计算值M &gt;0的超平面。为了概括这种困境，使用支持向量分类器来计算不可分离情况的软裕度。</p><h1 id="cecc" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">支持向量分类器</h1><p id="2592" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">支持向量分类器，也称为软边缘分类器，允许一些观察值出现在超平面或边缘的对面。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/b89fbc82544cbb656a149502f820b49b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QYz8of-O-1OvXqEk2vmbrw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">摘自《统计学习导论》一书的方程式，第346页</p></figure><p id="0827" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> M </strong>是页边距的宽度；<strong class="ky ir"> C </strong>为非负调谐参数；<br/> <strong class="ky ir"> ε1，。。。，εn </strong>允许观测在不正确的一侧。如果<strong class="ky ir"> εi &gt; 0 </strong>，第I次观察出现在错误的一侧。c是εi值的总和，它决定了裕度能够承受的严重程度范围。当C变得更高时，裕度变得更宽，它容忍更多的数据违规，这导致高偏差和低方差。另一方面，当C减小时，裕度缩小，错误侧出现的观测值更少，从而导致低偏倚和高方差。因此，C决定了支持向量分类器的偏差-方差权衡。从下面的图可以看出，C从图1收缩到图4，允许在超平面的窄边上出现更少的支持向量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/493790fca8e753a3df449e87a06eb8d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tGLxRglVdK3BNeG-1j-B_w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">摘自《统计学习导论》一书，第348页</p></figure><p id="8601" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意:如果边界是线性的以分离观察值，支持向量分类器仅支持2类分类。</p><blockquote class="nw nx ny"><p id="b083" class="kw kx nz ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated">注意，支持向量分类器仅用于分离2类问题，而支持向量机(SVM)能够处理具有多于2类分类的更复杂的非线性问题。</p></blockquote><h1 id="4ae7" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">支持向量机</h1><p id="875e" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">支持向量机使用在两个以上类别之间具有非线性边界的核来扩大特征空间。在使用核方法的基础上，我们将通过支持向量分类器的数学概念来处理线性和非线性问题。</p><p id="a2d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据下面的等式，它表示线性支持向量分类器，因为SVM需要观察值的内积。有a_i⋯ a_n训练观察。为了优化F(x)函数，我们需要进行新点x和第I个观察点的内积。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/ac7f9556a8b4e4aabd1ce1b326f23edc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*kOvgiJAwIc89999lI4zYow.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">线性支持向量分类器方程</p></figure><h2 id="8725" class="nd mc iq bd md nj nk dn mh nl nm dp ml lf nn no mn lj np nq mp ln nr ns mr nt bi translated">基于内核的SVM</h2><ul class=""><li id="46da" class="oe of iq ky b kz mt lc mu lf og lj oh ln oi lr oj ok ol om bi translated"><strong class="ky ir">基于线性和多项式的内核</strong></li></ul><p id="dadf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">内核用于<x x_i="">内积的推广。根据等式1，k(x_i，x _ I’)被称为核函数，用于量化两个观察值的相似性。为了解决多项式问题，来自等式2的核函数适合于适应多项式次数d &gt;1的高维空间。多项式核函数能够为分布在非线性模式上的数据点绘制灵活的决策边界。</x></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/7c6753aea6c899dde1b556b981e5c730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tnn8_bPSCwPkbd_EdYC4BA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">核函数方程</p></figure><ul class=""><li id="54f3" class="oe of iq ky b kz la lc ld lf oo lj op ln oq lr oj ok ol om bi translated"><strong class="ky ir">径向内核</strong></li></ul><p id="3e07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">径向核</strong>是另一个流行的<strong class="ky ir">非线性</strong>函数。根据下面的等式3，<strong class="ky ir"> γ </strong>是一个正的常数<strong class="ky ir">。给定一个新的测试观察x_ij，径向核函数用训练观察x_i'j计算<strong class="ky ir">欧几里德距离</strong>。当测试数据点远离训练数据点时，对两个更远的数据点的平方距离的负常数取指数会导致小的值。γ越大，支持向量分类器的输出性能越好。因此，k(x_i，x _ I’)的输出对支持向量分类器f(x)函数几乎没有影响，如下面的等式4所定义的。换句话说，只有训练观测值x_i'j附近的测试观测值x_ij会对测试数据的预测类别产生本质上的影响。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/275787686276c6cac5181040b3d6f5e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*37wYYvEfIQBlhCqAkztbzQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">核函数方程和基于核的SVM</p></figure><h2 id="93b3" class="nd mc iq bd md nj nk dn mh nl nm dp ml lf nn no mn lj np nq mp ln nr ns mr nt bi translated">应用内核的优势</h2><p id="9a07" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">当应用核时，它减少了计算，同时它只关注位于K(x_i，x _ I’)上的核函数的边缘的支持向量。另一方面，扩大的特征空间方法需要大的计算能力，因为会有无穷多个数据点落入更高维度的空间中。</p><h1 id="ee5b" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">Python中的支持向量机</h1><p id="ccf6" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">我们将使用python Sklearn包遍历示例数据集上的支持向量分类器。该数据集包括来自IMDB的5万部电影评论，用于自然语言处理或文本分析。这是用来预测消息是正面的还是负面的。Kaggle网站链接:<a class="ae kv" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/data" rel="noopener ugc nofollow" target="_blank"> IMDB数据集</a>。SVM很适合文本分类，所以这将是我们的主要预测文本分类模型。在通过支持向量分类器深入研究预测方法之前，让我们先了解一些基本方法，以便为预测准备好训练和测试数据。</p><h2 id="e3a6" class="nd mc iq bd md nj nk dn mh nl nm dp ml lf nn no mn lj np nq mp ln nr ns mr nt bi translated">文本数据集的预处理:</h2><ul class=""><li id="9867" class="oe of iq ky b kz mt lc mu lf og lj oh ln oi lr oj ok ol om bi translated">剥离html标签</li><li id="a681" class="oe of iq ky b kz os lc ot lf ou lj ov ln ow lr oj ok ol om bi translated">删除标点符号</li><li id="f058" class="oe of iq ky b kz os lc ot lf ou lj ov ln ow lr oj ok ol om bi translated">删除停用词</li><li id="4565" class="oe of iq ky b kz os lc ot lf ou lj ov ln ow lr oj ok ol om bi translated">词干:把单词变成词根形式</li><li id="cb08" class="oe of iq ky b kz os lc ot lf ou lj ov ln ow lr oj ok ol om bi translated">标记化</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">文本预处理的代码段</p></figure><h2 id="0f71" class="nd mc iq bd md nj nk dn mh nl nm dp ml lf nn no mn lj np nq mp ln nr ns mr nt bi translated">单词袋模型:</h2><p id="38fb" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">输入文本特征需要转换成机器学习模型的数字特征。以下示例通过<strong class="ky ir"> CountVectorizer </strong>函数展示了词袋模型方法。它计算单词在整个文档中出现的次数(计数),并输出编码向量。输出向量通常是稀疏的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CountVectorizer的代码片段</p></figure><h2 id="86ba" class="nd mc iq bd md nj nk dn mh nl nm dp ml lf nn no mn lj np nq mp ln nr ns mr nt bi translated">SVM模型</h2><p id="d504" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">以下示例显示了将嵌入文本要素输入到SVM模型中。用径向核参数设定模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">支持向量机的代码片段</p></figure><h2 id="32a4" class="nd mc iq bd md nj nk dn mh nl nm dp ml lf nn no mn lj np nq mp ln nr ns mr nt bi translated">sklearn中SVM的参数调整</h2><ul class=""><li id="21a9" class="oe of iq ky b kz mt lc mu lf og lj oh ln oi lr oj ok ol om bi translated">内核:rbf(默认)、线性、多边形、sigmoid <br/> RBF:径向基函数内核</li><li id="2cd9" class="oe of iq ky b kz os lc ot lf ou lj ov ln ow lr oj ok ol om bi translated">C:正则化参数<br/> C控制误分类观测值的容差。较低的C值允许以较低精度为代价的较大裕量，并且对性能具有较低的方差。相反，<strong class="ky ir">较高的C值</strong>绘制较小的边界，对大多数数据点的分类性能较好，在决策函数上表现出较高的方差。</li><li id="3f2a" class="oe of iq ky b kz os lc ot lf ou lj ov ln ow lr oj ok ol om bi translated">伽马:它影响观察对模型性能的影响程度。较大的gamma值会扩大半径，从而考虑更多的观察结果，而较低的gamma值会绘制较小的半径。注意到较大的伽玛会导致过度拟合问题</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SVM网格搜索的代码片段</p></figure><h1 id="6930" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">结论</h1><ul class=""><li id="4f7f" class="oe of iq ky b kz mt lc mu lf og lj oh ln oi lr oj ok ol om bi translated">支持向量分类器使用超平面通过最大化边缘值来区分来自两类的数据点。</li><li id="6809" class="oe of iq ky b kz os lc ot lf ou lj ov ln ow lr oj ok ol om bi translated">核被应用于观测值的非线性分离，这减少了出现在标签类中的不正确观测值的出现。与线性核和多项式核相比，径向核对单个观测值的变化不敏感。</li><li id="b335" class="oe of iq ky b kz os lc ot lf ou lj ov ln ow lr oj ok ol om bi translated">为了使文本分类模型具有更好的性能，需要一种彻底的文本处理方法来获得干净的数据集。模型的参数调整可以提高模型性能。</li></ul><h2 id="9007" class="nd mc iq bd md nj nk dn mh nl nm dp ml lf nn no mn lj np nq mp ln nr ns mr nt bi translated">参考:</h2><p id="103b" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">[1]:加雷斯·詹姆斯、丹妮拉·威滕、特雷弗·哈斯蒂和罗伯特·蒂布拉尼。统计学习导论。第七版。斯普林格。</p><h2 id="8a44" class="nd mc iq bd md nj nk dn mh nl nm dp ml lf nn no mn lj np nq mp ln nr ns mr nt bi translated">请继续关注更多关于机器学习概念的初学者友好文章！☺️</h2></div></div>    
</body>
</html>