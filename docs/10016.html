<html>
<head>
<title>Gradient Descent, the Learning Rate, and the importance of Feature Scaling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降、学习率和特征缩放的重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-the-learning-rate-and-the-importance-of-feature-scaling-6c0b416596e1?source=collection_archive---------8-----------------------#2020-07-15">https://towardsdatascience.com/gradient-descent-the-learning-rate-and-the-importance-of-feature-scaling-6c0b416596e1?source=collection_archive---------8-----------------------#2020-07-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/37f85b71751ad90bb459889b38edf389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zb2_HcSV5saDfApj"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">史蒂夫·阿灵顿在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><blockquote class="kd ke kf"><p id="8a9f" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">这篇帖子的内容是书中一章的部分复制:<strong class="kj ir"/><a class="ae kc" href="https://leanpub.com/pytorch/" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">用 PyTorch 进行深度学习循序渐进:初学者指南</strong> </a> <strong class="kj ir">。</strong></p></blockquote><h1 id="b985" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">介绍</h1><p id="24c4" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated"><strong class="kj ir">梯度下降</strong>、<strong class="kj ir">学习率</strong>和<strong class="kj ir">特征缩放</strong>有什么共同点？让我想想…</p><p id="8059" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">每次我们训练深度学习模型或任何神经网络时，我们都在使用<strong class="kj ir">梯度下降</strong>(带反向传播)。我们用它来通过<em class="ki">更新模型的参数/权重</em>来<em class="ki">最小化损失</em>。</p><p id="747e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">参数更新取决于两个值:梯度和<strong class="kj ir">学习率</strong>。学习率让你可以控制更新的大小。一个<strong class="kj ir">更大的学习率</strong>意味着<strong class="kj ir">更大的更新</strong>，并且有希望是一个<strong class="kj ir">学习更快的模型</strong>。</p><p id="19b9" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">但是有一个圈套，一如既往…如果学习率<strong class="kj ir">太大</strong>，模型<strong class="kj ir">什么都学不到</strong>。这让我们想到了两个基本问题:</p><ul class=""><li id="4d66" class="ml mm iq kj b kk kl ko kp mf mn mh mo mj mp le mq mr ms mt bi translated"><strong class="kj ir">多大才算“太大”？</strong></li><li id="4b0f" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><strong class="kj ir">有什么办法可以用更大的学习率吗？</strong></li></ul><p id="44ab" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">不幸的是，第一个问题没有明确的答案。它总是取决于许多因素。</p><p id="9d3e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">但是<strong class="kj ir">有一个</strong>第二个<strong class="kj ir">第一个</strong>的答案:<strong class="kj ir">特征缩放</strong>！这是怎么回事？这就是我写这篇文章的原因:向你详细展示梯度下降、学习率和特征缩放之间的联系。</p><p id="014e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在本帖中，我们将:</p><ul class=""><li id="a965" class="ml mm iq kj b kk kl ko kp mf mn mh mo mj mp le mq mr ms mt bi translated">定义一个<strong class="kj ir">模型</strong>并生成一个<strong class="kj ir">合成数据集</strong></li><li id="80c1" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated">随机地<strong class="kj ir">初始化</strong>参数</li><li id="b4d3" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated">探索<strong class="kj ir">损失面</strong>和<strong class="kj ir">可视化梯度</strong></li><li id="8231" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated">了解使用不同<strong class="kj ir">学习率</strong>的<strong class="kj ir">效果</strong></li><li id="6261" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated">了解<strong class="kj ir">特征缩放</strong>的<strong class="kj ir">效果</strong></li></ul><h1 id="1419" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">模型</h1><p id="729a" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">模型必须是<strong class="kj ir">简单的</strong>和<strong class="kj ir">熟悉的</strong>，这样你就可以专注于梯度下降的<strong class="kj ir">内部运作</strong>。因此，我将坚持使用一个尽可能简单的模型:一个具有单一特征的<strong class="kj ir">线性回归<em class="ki"> x </em> </strong>！</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/986b0a47574a8c9746f9e375cee2d190.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*dMRM0e0zKTyskJpb6P7N_w.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">简单线性回归</p></figure><p id="a223" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在这个模型中，我们使用一个<strong class="kj ir">特征</strong> ( <strong class="kj ir"> <em class="ki"> x </em> </strong>)来尝试预测一个<strong class="kj ir">标签</strong> ( <strong class="kj ir"> <em class="ki"> y </em> </strong>)的值。我们的模型中有三个元素:</p><ul class=""><li id="847b" class="ml mm iq kj b kk kl ko kp mf mn mh mo mj mp le mq mr ms mt bi translated"><strong class="kj ir">参数<em class="ki"> b </em></strong></li><li id="1abd" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><strong class="kj ir">参数<em class="ki"> w </em></strong></li><li id="dc01" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated">还有那个<strong class="kj ir">上学期</strong>(为什么<em class="ki">总是</em>必须是希腊字母？)，<em class="ki">ε</em>，这是为了说明固有的<strong class="kj ir">噪声</strong>，即我们无法消除的<strong class="kj ir">误差</strong></li></ul><h1 id="915d" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">数据生成</h1><p id="aedf" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">我们已经知道我们的模型了。为了给它生成<strong class="kj ir">合成数据</strong>，我们需要为它的<strong class="kj ir">参数</strong>取值。我选了<strong class="kj ir"><em class="ki">b</em>= 1</strong><strong class="kj ir">w = 2</strong>。</p><p id="db2a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">先来生成我们的<strong class="kj ir">特征</strong> ( <strong class="kj ir"> <em class="ki"> x </em> </strong>):我们使用 Numpy 的<a class="ae kc" href="https://numpy.org/doc/1.18/reference/random/generated/numpy.random.RandomState.rand.html#numpy.random.RandomState.rand" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir"> rand </strong> </a>方法，在 0 到 1 之间随机生成 100 ( <em class="ki"> N </em>)个点。</p><p id="54ec" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">然后，我们将我们的<strong class="kj ir">特征</strong> ( <em class="ki"> x </em>)和我们的<strong class="kj ir">参数<em class="ki"> b </em>和<em class="ki"> w </em> </strong>代入我们的<strong class="kj ir">方程</strong>来计算我们的<strong class="kj ir">标签</strong> ( <strong class="kj ir"> <em class="ki"> y </em> </strong>)。但是我们需要添加一些<a class="ae kc" href="https://en.wikipedia.org/wiki/Gaussian_noise" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">高斯噪声</strong></a>(<strong class="kj ir"><em class="ki">ε</em></strong>)为好；否则，我们的合成数据集将是一条完美的直线。</p><p id="42d3" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们可以使用<em class="ki"> Numpy </em>的<a class="ae kc" href="https://numpy.org/doc/1.18/reference/random/generated/numpy.random.RandomState.randn.html#numpy.random.RandomState.randn" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir"> randn </strong> </a>方法生成噪声，该方法从正态分布(均值为 0，方差为 1)中抽取样本，然后将其乘以<strong class="kj ir">因子</strong>以调整噪声的<strong class="kj ir">水平</strong>。由于我不想添加这么多噪声，所以我选择了 0.1 作为我的因子。</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="ne nf l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">合成数据集</p></figure><h2 id="44bd" class="ng lg iq bd lh nh ni dn ll nj nk dp lp mf nl nm lt mh nn no lx mj np nq mb nr bi translated">训练-验证-测试分割</h2><p id="41ce" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">解释<strong class="kj ir">训练- <br/>验证-测试分割</strong>背后的原因已经超出了本文的范围，但是我想提出两点:</p><ol class=""><li id="c72a" class="ml mm iq kj b kk kl ko kp mf mn mh mo mj mp le ns mr ms mt bi translated">分割应该<strong class="kj ir">总是</strong>是你做的第一件事<strong class="kj ir">——没有预处理，没有转换；<strong class="kj ir">在分割</strong>之前什么也没有发生——这就是为什么我们在合成数据生成</strong>之后立即进行分割<strong class="kj ir"/></li><li id="688f" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le ns mr ms mt bi translated">在这篇文章中，我们将只使用<strong class="kj ir">训练集</strong>——所以我没有费心创建一个<strong class="kj ir">测试集</strong>，但是我还是对<strong class="kj ir">高亮点#1 </strong> :-)进行了分割</li></ol><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="ne nf l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">训练-验证分割</p></figure><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/c204c574a78ab05f7521ab87332ff7d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WwtPZuYN372vaQcsjgIMPQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">综合数据</p></figure><h1 id="b3ac" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">随机初始化</h1><p id="3199" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">在我们的例子中，我们已经<strong class="kj ir">知道</strong>参数的<strong class="kj ir">真实值</strong>，但是这在现实生活中显然永远不会发生:如果我们<em class="ki">知道</em>真实值，为什么还要费心训练一个模型来找到它们呢？！</p><p id="858e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">好的，鉴于<strong class="kj ir">我们永远不会知道</strong>参数的<strong class="kj ir">真值</strong>，我们需要为它们设置<strong class="kj ir">初始值</strong>。我们如何选择他们？事实证明。一个<strong class="kj ir">随机猜测</strong>和其他的一样好。</p><p id="446a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">所以，我们可以<strong class="kj ir">随机初始化</strong> <strong class="kj ir">参数/权重</strong>(我们只有两个，<strong class="kj ir"> <em class="ki"> b </em> </strong>和<strong class="kj ir"> <em class="ki"> w </em> </strong>)。</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="ne nf l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">随机起点</p></figure><p id="774c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们随机初始化的参数是:<strong class="kj ir"> <em class="ki"> b </em> = 0.49 </strong>和<strong class="kj ir"> <em class="ki"> w </em> = -0.13 </strong>。这些参数有用吗？</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/a237eba6e15ad4f694e9dfc15022b423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Kw2NAqSMDFQkikv5kik4g.png"/></div></div></figure><p id="e56f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">显然不是…但是，他们到底有多坏？这就是<strong class="kj ir">损失</strong>的原因。我们的目标将是<strong class="kj ir">最小化</strong>它。</p><h1 id="a054" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">损失面</h1><p id="cae4" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">在为我们的参数选择了一个随机起点后，我们用它们来<em class="ki">预测</em>，<em class="ki">计算相应的误差</em>，<em class="ki">将这些误差</em>汇总成一个<strong class="kj ir">损失</strong>。由于这是一个线性回归，我们使用<strong class="kj ir">均方差(MSE) </strong>作为我们的损失。下面的代码执行这些步骤:</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="ne nf l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">进行预测并计算损失</p></figure><p id="275c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们刚刚计算了对应于我们的<strong class="kj ir">随机初始化参数</strong> ( <em class="ki"> b </em> = 0.49 和<em class="ki"> w </em> = -0.13)的<strong class="kj ir">损失</strong> (2.74)。现在，如果我们对<strong class="kj ir"> <em class="ki"> b </em> </strong>和<strong class="kj ir"> <em class="ki"> w </em> </strong>的所有可能值做同样的处理会怎么样？嗯，不是<em class="ki">所有</em>可能的值，而是<em class="ki">给定范围内的所有等间距值的组合</em>？</p><p id="dd5f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们可以在<strong class="kj ir"> -2 </strong>和<strong class="kj ir"> 4 </strong>之间改变<strong class="kj ir"> <em class="ki"> b </em> </strong>，同时在<strong class="kj ir"> -1 </strong>和<strong class="kj ir"> 5 </strong>之间改变<strong class="kj ir"><em class="ki"/></strong>，例如，每个范围包含 101 个均匀间隔的点。如果我们在这些范围内计算对应于<strong class="kj ir">的参数<em class="ki"> b </em>和<em class="ki"> w </em>的每个不同组合的<strong class="kj ir">损耗</strong>，结果将是损耗</strong>的<strong class="kj ir">网格，形状(101，101)的矩阵。</strong></p><p id="e968" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">这些损耗就是我们的<strong class="kj ir">损耗面</strong>，它可以在 3D 图中可视化，其中纵轴(<em class="ki"> z </em>)代表损耗值。如果我们<strong class="kj ir">连接</strong>产生<strong class="kj ir">相同损失值</strong>的<em class="ki"> b </em>和<em class="ki"> w </em>的组合，我们将得到一个<strong class="kj ir">椭圆</strong>。然后，我们可以在原来的<em class="ki"> b x w </em>平面上画出这个椭圆(蓝色，损耗值为 3)。简而言之，这就是<strong class="kj ir">等高线图</strong>的作用。从现在开始，我们将总是使用等高线图，而不是相应的 3D 版本。</p><p id="5a96" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">下图显示了建议的<strong class="kj ir">参数范围</strong>的损失面，使用我们的训练集来计算<em class="ki"> b </em>和<em class="ki"> w </em>的每个组合的损失。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/277f418e55533a40eca3bbefd3e198d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k_LdFIxWKrs4ehlPkZ0YIA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">损失面</p></figure><p id="764b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在图的中心，参数(<em class="ki"> b，w </em>)的值接近(1，2)，损耗处于其最小值<strong class="kj ir"/>。这就是我们试图用梯度下降达到的点。</p><p id="04a8" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在底部，稍微偏左的地方，是<strong class="kj ir">随机开始</strong>点，对应我们随机初始化的参数(<em class="ki"> b </em> = 0.49、<em class="ki"> w </em> = -0.13)。</p><p id="3621" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">这是处理简单问题的一个好处，比如用单一特征进行线性<br/>回归:我们只有<strong class="kj ir">两个参数</strong>，因此<strong class="kj ir">我们可以计算并可视化损失面</strong>。</p><h2 id="47b4" class="ng lg iq bd lh nh ni dn ll nj nk dp lp mf nl nm lt mh nn no lx mj np nq mb nr bi translated">横截面</h2><p id="39ca" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">另一件好事是，我们可以在损失表面切割一个<strong class="kj ir">横截面</strong>来检查如果其他参数保持不变时<strong class="kj ir">损失</strong>看起来是什么样的。</p><p id="72ca" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">让我们从使<strong class="kj ir"> <em class="ki"> b </em> =0.52 </strong>(来自我们的均匀间隔范围的值，最接近我们的初始随机值<em class="ki"> b </em>，0.4967)开始——我们在我们的损失表面(左图)上垂直地(红色虚线)切割一个横截面<em class="ki">，我们得到右边的结果图:</em></p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/9435def66ed5ed963e3e87563f9b2f77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Byxg21fMMCEoQo5R-0eaUw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">垂直横截面—参数<strong class="bd nu"> b </strong>固定</p></figure><p id="7561" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">这个横截面告诉我们什么？它告诉我们，<strong class="kj ir">如果我们保持<em class="ki"> b </em>不变</strong>(在 0.52)，从参数<em class="ki"> w </em> 的<strong class="kj ir">角度看，如果<strong class="kj ir"> <em class="ki"> w </em>增加</strong>(达到 2 和 3 之间的某个值)，则<strong class="kj ir">损失</strong>可以最小化。</strong></p><p id="bcff" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">当然，<strong class="kj ir">不同的<em class="ki">b</em>T44】值会产生<strong class="kj ir">不同的<em class="ki">w</em>T48】截面损失曲线。这些曲线将取决于损失面的<em class="ki">形状(稍后将在“<strong class="kj ir">学习率</strong>”部分详细介绍)。</em></strong></strong></p><p id="7de9" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">好的，到目前为止，一切顺利…那么<em class="ki">其他</em>截面呢？现在让我们水平切割它，使<strong class="kj ir"> <em class="ki"> w </em> = -0.16 </strong>(来自我们的均匀间隔范围的值，最接近我们的初始随机值 b，-0.1382)。结果图在右边:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/d5151e8e9df8fa04c7f5b03c8b0bd4a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YH1_gAIEiCqdIbZS0DCNgQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">水平横截面—参数<strong class="bd nu"> w </strong>固定</p></figure><p id="df6a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">现在，<strong class="kj ir">如果我们保持<em class="ki"> w </em>恒定</strong> (at -0.16)，从参数<em class="ki"> b </em> 的<strong class="kj ir">角度看，如果<em class="ki"> b </em>增加</strong>(达到接近 2 的某个值)，则<strong class="kj ir">损耗</strong>可以最小化<strong class="kj ir">。</strong></p><blockquote class="kd ke kf"><p id="a134" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">总的来说，该横截面的目的是获得<strong class="kj ir">改变单个参数</strong>对损失的影响，同时保持<strong class="kj ir">其他一切不变</strong>。简单地说，这是一个<strong class="kj ir">渐变</strong> :-)</p></blockquote><h1 id="bf04" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">可视化渐变</h1><p id="5ede" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">从上一节我们已经知道，为了使<em class="ki">损失</em>最小化，<em class="ki"> b </em>和<em class="ki"> w </em>都需要增加<strong class="kj ir"/>。所以，保持使用渐变的精神，让我们<strong class="kj ir">增加每个参数一个<em class="ki">小</em> <em class="ki">位</em> </strong>(总是保持另一个固定！).顺便说一下，在这个例子中，一个<em class="ki">小点</em>等于 0.12(为了方便起见，所以它产生了一个更好的图)。</p><p id="3d1f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">这些增加对损失有什么影响？让我们来看看:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/45e315cb0a492065b3baf99a807e046d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*31-_X2NT7bm9mDV2aJm0Tw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">几何计算(近似)梯度</p></figure><p id="5dfd" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在左图中，<strong class="kj ir">将<em class="ki"> w </em>增加 0.12 </strong>会导致<strong class="kj ir">损失减少 0.21 </strong>。几何计算和粗略近似的梯度由两个值之间的比率给出:<strong class="kj ir"> -1.79 </strong>。该结果与梯度的<em class="ki">实际</em>值(-1.83)相比如何？对于粗略的近似来说，这实际上还不错……还能更好吗？当然，<strong class="kj ir">如果我们使<em class="ki"> w </em>的增量越来越小</strong>(比如 0.01，而不是 0.12)，我们将得到越来越好的<strong class="kj ir">近似值……在极限中，随着<strong class="kj ir">增量接近零</strong>，我们将达到</strong>的<strong class="kj ir">精确值。嗯，这就是导数的定义！</strong></p><p id="70f6" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">同样的推理也适用于右边的图:<strong class="kj ir">将<em class="ki"> b </em>增加相同的 0.12 </strong>会使<strong class="kj ir">损失减少 0.35 </strong>。更大的损耗减少、更大的比率、更大的梯度——以及更大的误差，因为几何近似值(-2.90)离实际值(-3.04)更远。</p><h1 id="1069" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">更新参数</h1><p id="8194" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">最后，我们<strong class="kj ir">使用渐变来更新</strong>参数。因为我们试图<strong class="kj ir">最小化我们的损失</strong>，我们<strong class="kj ir">反转梯度的符号</strong>用于更新。</p><p id="466d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">还有另一个(超)参数需要考虑:学习率<strong class="kj ir"/>，用<em class="ki">希腊字母</em> <strong class="kj ir"> <em class="ki"> eta </em> </strong>(看起来像字母<strong class="kj ir"> n </strong>)表示，这是我们需要应用于梯度的<strong class="kj ir">乘法因子</strong>，用于参数更新。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/26e3ab8099e5aec79e3e940c8b90a55b.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*jBH__RN7Feu8UHcE_wUHAA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">更新参数<strong class="bd nu"> b </strong>和<strong class="bd nu"> w </strong></p></figure><p id="2938" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们也可以稍微不同地解释这一点:<strong class="kj ir">每个参数</strong>将使其<br/>值<strong class="kj ir">被一个常数值<em class="ki"> eta </em> </strong>(学习速率)更新，但是这个常数将通过该参数对最小化损失(其梯度)的贡献大小来<strong class="kj ir">加权。</strong></p><p id="b657" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">老实说，我相信这种思考参数更新的方式更有意义:首先，你决定一个<em class="ki">学习率</em>，它指定了你的<strong class="kj ir">步长</strong>，而<em class="ki">梯度</em>告诉你每走一步对每个参数的<strong class="kj ir">相对影响</strong>(对损失)。然后你采取一个给定的<strong class="kj ir">步数</strong>这是<strong class="kj ir">比例</strong>到那个<strong class="kj ir">相对影响:更多的影响，更多的步数</strong>。</p><blockquote class="kd ke kf"><p id="88a6" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">"如何<strong class="kj ir">选择</strong>一个学习率？"</p><p id="1f92" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">不幸的是，这是一个独立的话题，超出了本文的范围。</p></blockquote><h2 id="ec11" class="ng lg iq bd lh nh ni dn ll nj nk dp lp mf nl nm lt mh nn no lx mj np nq mb nr bi translated">学习率</h2><p id="073e" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated"><strong class="kj ir">学习率</strong>是最重要的超参数——关于如何选择学习率、如何在训练期间修改学习率以及错误的学习率如何完全破坏模型训练，有大量的<br/>资料。</p><p id="4e38" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">也许你已经看到了下面这张著名的图表(来自斯坦福大学的 CS231n 课程)，它显示了在训练过程中<strong class="kj ir">太大</strong>或<strong class="kj ir">太小</strong>的学习率如何影响<strong class="kj ir">损失</strong>。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/6d21e68b95f9e7ae7dca1a3b4a5a7e8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/0*xVt4A1THpzJNRpVL.jpeg"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><a class="ae kc" href="https://cs231n.github.io/neural-networks-3/#loss-function" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="d0c8" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">大部分人都会在某个时间点看到(或者已经看到)。这基本上是常识，但我认为需要<strong class="kj ir">彻底解释和直观演示</strong>才能让<em class="ki">真正理解。所以，我们开始吧！</em></p><p id="3e34" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我给你讲个小故事(这里试图打个比方，请多包涵！):想象你正在从山里徒步旅行回来，你想尽快回家。在你前进的道路上，你可以选择<em class="ki">继续前进</em>或者<em class="ki">右转</em>。</p><p id="b639" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">前方的道路几乎是平坦的，而通往你右边的<em class="ki">的道路有些陡峭。<br/> <strong class="kj ir">陡度</strong>就是<strong class="kj ir">坡度</strong>。如果你以这样或那样的方式迈出一步，就会导致不同的结果(如果你向右迈一步，而不是向前，你会下降得更多)。</em></p><p id="d248" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">但是，事情是这样的:你知道通往你右边<em class="ki">的路</em>会让你更快地回到<strong class="kj ir">的家</strong>，所以你不会只走一步，而是在那个方向<strong class="kj ir">走多步</strong>:<strong class="kj ir">路越陡，你走的步就越多</strong>！记住，“<em class="ki">多点冲击，多点脚步</em>”！你就是无法抗拒走那么多步的冲动；你的行为似乎完全由风景决定。我知道这个类比越来越奇怪了…</p><p id="90c7" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">但是，你还有<strong class="kj ir">一个选择</strong>:你<strong class="kj ir">可以调整你的步长</strong>。你可以选择任何大小的步伐，从小步到大步。那就是你的<strong class="kj ir">学习率</strong>。</p><p id="4be4" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">好了，让我们看看这个小故事把我们带到了什么地方……简而言之，这就是你的行动方式:</p><p id="585a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><strong class="kj ir">更新位置=先前位置+步长*步数</strong></p><p id="06ec" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">现在，将它与我们对参数所做的比较:</p><p id="6fdb" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><strong class="kj ir">更新值=先前值-学习率*梯度</strong></p><p id="5212" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">你明白了，对吧？我希望如此，因为这个类比现在完全站不住脚了……在这一点上，在向一个方向移动之后(比如说，我们谈到的<em class="ki">右转</em>，你必须停下来向另一个方向移动(仅仅一小步，因为路径几乎是<em class="ki">平的</em>，记得吗？).诸如此类……嗯，我想没有人从这样一条正交的之字形路径徒步旅行回来过…</p><p id="c927" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">无论如何，让我们进一步探索你唯一的选择:你的步幅，我的意思是学习速度。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/0ae943fdfbaea51f1c2a6d2598f4170e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/0*_k8RGsFTIqjA6uVa.jpg"/></div></figure><h2 id="e732" class="ng lg iq bd lh nh ni dn ll nj nk dp lp mf nl nm lt mh nn no lx mj np nq mb nr bi translated">小学习率</h2><p id="785f" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">从<em class="ki">婴儿步</em>开始是有道理的吧？这意味着使用<strong class="kj ir">小学习率</strong>。小学率<strong class="kj ir"> safe(r) </strong>，不出所料。如果你在徒步旅行回家的路上迈着小步，你更有可能安全到达目的地——但是这需要很长时间。这同样适用于训练模型:小的学习率可能会让你达到(某个)最低点，<strong class="kj ir">最终</strong>。不幸的是，时间就是金钱，尤其是当你在云中支付 GPU 时间的时候……所以，有一个<em class="ki">激励</em>去尝试<strong class="kj ir">更大的学习率</strong>。</p><p id="6703" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">这个推理如何应用于我们的模型？通过计算我们的(几何)梯度，我们知道我们需要采取<strong class="kj ir">给定的步数</strong> : <strong class="kj ir"> 1.79 </strong>(参数<em class="ki"> w </em>)和<strong class="kj ir"> 2.90 </strong>(参数<em class="ki"> b </em>)。让我们将<strong class="kj ir">步长设置为 0.2 </strong>(小一点)。意思是我们<strong class="kj ir">为<em class="ki">w</em>T15】移动 0.36，为<em class="ki">b</em>T19】移动<strong class="kj ir"> 0.58。</strong></strong></p><blockquote class="kd ke kf"><p id="5ded" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">重要提示:在现实生活中，0.2 的学习率通常被认为是很大的——但是在我们非常简单的线性回归例子中，它仍然是很小的。</p></blockquote><p id="ba11" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">这场运动将我们引向何方？正如你在下面的图中所看到的(如原始点右侧的新点所示)，在这两种情况下，移动使我们更接近最小值——在右侧更是如此，因为曲线更加陡峭。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/2229a38aeca82d94a2a52d3ab44cc143.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pJSQoifuCicU3JAa2PEc5g.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">使用一个小的学习率</p></figure><h2 id="a396" class="ng lg iq bd lh nh ni dn ll nj nk dp lp mf nl nm lt mh nn no lx mj np nq mb nr bi translated">大学习率</h2><p id="35e3" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">如果我们使用一个大的学习率，比如说步长为 0.8 的学习率，会发生什么呢？正如我们在下面的图中看到的，我们开始真正地<strong class="kj ir">遇到麻烦</strong> …</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/8af23a9215efe2c4dbe5d332b404fa95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8JhSI_W5ppXKee2PS4qsdg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">使用大的学习率</p></figure><p id="b82d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">即使左边的图一切正常，右边的图给我们展示了一个完全不同的画面:<strong class="kj ir">我们最终在曲线的另一边</strong>。那是<strong class="kj ir"> <em class="ki">而不是</em> </strong>好…你会来回<strong class="kj ir"/>，交替地击打曲线的两边。</p><blockquote class="kd ke kf"><p id="a7e7" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">“嗯，即便如此，我可能<strong class="kj ir">还是</strong>达到最低，为什么会这么差？”</p></blockquote><p id="6697" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在我们这个简单的例子中，是的，你最终会达到最小值，因为<strong class="kj ir">曲线是漂亮和圆的</strong>。</p><p id="40c1" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">但是，在实际问题中，“曲线”有一些真正<strong class="kj ir">奇怪的形状</strong>，允许<br/> <strong class="kj ir">奇怪的结果</strong>，例如来回<strong class="kj ir">而从未接近最小值</strong>。</p><p id="e48b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们打个比方，你<strong class="kj ir">移动得太快</strong>以至于你<strong class="kj ir">摔倒</strong>撞到<strong class="kj ir">山谷另一边</strong>，然后像一个<strong class="kj ir"> <em class="ki">乒乓</em> </strong>一样一直往下。很难相信，我知道，但你肯定不想那样…</p><h2 id="1a2d" class="ng lg iq bd lh nh ni dn ll nj nk dp lp mf nl nm lt mh nn no lx mj np nq mb nr bi translated">非常大的学习率</h2><p id="e5d1" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">等等，可能会比那更糟…让我们用一个真正大的学习率，比如说，1.1 的步长！</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/3713027946a0f503fb244468511d567e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fw7YoluP3HNLKLsJP1zBPw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">使用一个非常大的学习率</p></figure><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/d23a9ac49e06b1013ebfb6977d77d47d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/0*o10kUx7dUhPMJsXD.jpg"/></div></figure><p id="30e4" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">好了，那个<strong class="kj ir"> <em class="ki">是</em> </strong>坏了……在右边的剧情中，我们不仅再次在的<em class="ki">另一边结束了，而且我们居然<strong class="kj ir">爬上了</strong>。这意味着<strong class="kj ir">我们的损失增加了</strong>，而不是减少了！这怎么可能呢？<em class="ki">你下山的速度如此之快，以至于最后你不得不爬回来？！不幸的是，这个类比再也帮不了我们了。我们需要以不同的方式考虑这个特殊的案例…</em></em></p><p id="dbf9" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">首先，请注意左边图中的一切都很好。巨大的学习率<em class="ki"/><strong class="kj ir">并没有引起任何问题</strong>，因为左边的曲线<strong class="kj ir">没有右边的曲线</strong>那么陡。换句话说，左边的曲线<strong class="kj ir">比右边的曲线能够获得更大的学习率</strong>。</p><p id="5b73" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们能从中学到什么？</p><blockquote class="kd ke kf"><p id="1536" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj ir">太大</strong>，对于一个<strong class="kj ir">学习率</strong>来说，是一个相对的概念:它取决于<strong class="kj ir">曲线有多陡</strong>，或者换句话说，它取决于<strong class="kj ir">坡度有多大</strong>。</p><p id="9188" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">我们有许多曲线，<strong class="kj ir">许多梯度</strong>:每个参数一个。但是我们只有<strong class="kj ir">一个单学率</strong>可以选择(不好意思，就是这么回事！).</p><p id="6a9e" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">这意味着学习率的<strong class="kj ir">大小受到最陡曲线</strong>的限制。所有其他曲线必须跟随，这意味着，鉴于它们的形状，它们将使用次优的学习率。</p><p id="797e" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">合理的结论是:如果所有的<strong class="kj ir">曲线都同样陡峭</strong>，那么<strong class="kj ir">学习率</strong>对它们来说都更接近最优<strong class="kj ir">最好</strong>！</p></blockquote><h2 id="931e" class="ng lg iq bd lh nh ni dn ll nj nk dp lp mf nl nm lt mh nn no lx mj np nq mb nr bi translated">“坏”特征</h2><p id="ef66" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">我们如何实现同样的<em class="ki"/><em class="ki">陡峭的</em>曲线？我来了。首先，让我们看一个<em class="ki">稍微</em>修改过的例子，我称之为“坏”数据集:</p><ul class=""><li id="2a81" class="ml mm iq kj b kk kl ko kp mf mn mh mo mj mp le mq mr ms mt bi translated">我<strong class="kj ir">把我们的特征(<em class="ki"> x </em>)乘以 10 </strong>，所以现在在范围【0，10】内，改名为<strong class="kj ir"> <em class="ki"> bad_x </em> </strong></li><li id="a877" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated">但是由于我<strong class="kj ir">不希望标签(<em class="ki"> y </em>)改变</strong>，所以我<strong class="kj ir">将原来的<em class="ki"> true_w </em>参数除以 10 </strong>并将其重命名为<strong class="kj ir"><em class="ki">bad _ w</em></strong>——这样，bad_w  * <em class="ki"> bad_x </em> 和</li></ul><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="ne nf l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">生成“坏”数据集</p></figure><p id="1e06" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">然后，我对两个数据集<em class="ki">原始的</em>和<em class="ki">坏的</em>执行了与之前相同的分割，并并排绘制了训练集，如下所示:</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="ne nf l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">“坏”数据集的训练验证分割</p></figure><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/3666729186c255d1550a7825bcb442dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PLO5epjv2cX4JK9cMEybIQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">相同的数据，不同比例的特征<strong class="bd nu"> x </strong></p></figure><p id="547f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">两个图之间唯一的<strong class="kj ir">差异是特征的<strong class="kj ir">比例<em class="ki"> x </em> </strong>。它的范围是[0，1]，现在是[0，10]。标签 y 没变，我也没碰<strong class="kj ir"> <em class="ki"> true_b </em> </strong>。</strong></p><p id="b527" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">这个简单的<strong class="kj ir">缩放</strong>对我们的梯度下降有任何有意义的影响吗？如果没有，我就不会问了，对吧？让我们计算一个新的<strong class="kj ir">损失面</strong>，并与之前的损失面进行比较:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/154ee51b4baad8795a4dec8f376649b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7mSmzEtfybnyWDTklgVU3g.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">损失表面—缩放特征之前和之后<strong class="bd nu"> x </strong></p></figure><p id="5e64" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">再看上图的<strong class="kj ir">等高线值</strong>:原来<em class="ki">深蓝色</em>线是<strong class="kj ir"> 3.0 </strong>，现在是<strong class="kj ir"> 50.0 </strong>！对于相同范围的参数值，<strong class="kj ir">的损失值要大得多</strong>。</p><p id="ff62" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">让我们看看将特征<em class="ki"> x </em>乘以 10 前后的<em class="ki">横截面</em>:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/b786543b633296e1d7cd836577fb8c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5v20omxnKY0lZmqoOw0aOQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">比较横截面:之前和之后</p></figure><p id="8620" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">这里发生了什么？<strong class="kj ir">红色曲线</strong>变得更加<strong class="kj ir">陡峭</strong>(更大的坡度)，因此我们必须使用<strong class="kj ir">较小的学习率</strong>才能安全地沿着它下降。</p><blockquote class="kd ke kf"><p id="4d3b" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">更重要的是，红色和黑色曲线<strong class="kj ir">之间的<strong class="kj ir">差异</strong>在<strong class="kj ir">陡度</strong>上增加</strong>。</p><p id="f015" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">这正是<strong class="kj ir">我们需要避免的</strong>！</p><p id="85d8" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">你记得为什么吗？</p><p id="a2be" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">因为<strong class="kj ir">学习率的大小受到最陡曲线</strong>的限制！</p></blockquote><p id="d2d7" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">怎么才能修好？嗯，我们<em class="ki">把</em>放大了 10 倍 …也许我们可以让它变得更好，如果我们<strong class="kj ir">以不同的方式放大</strong>。</p><h2 id="585f" class="ng lg iq bd lh nh ni dn ll nj nk dp lp mf nl nm lt mh nn no lx mj np nq mb nr bi translated">缩放/标准化/规范化</h2><p id="be0a" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">怎么不同？有个<em class="ki">漂亮的</em>东西叫做<a class="ae kc" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">标准定标器</strong> </a>，它把一个<strong class="kj ir">特征</strong>转换成最后的<strong class="kj ir">零均值</strong>和<strong class="kj ir">单位标准差</strong>。</p><p id="332d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">它是如何实现的？首先，它使用训练集(<strong class="kj ir"> <em class="ki"> N </em> </strong>点)计算给定<strong class="kj ir">特征的<em class="ki">均值</em>和<em class="ki">标准差</em>:</strong></p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/5a9acf5a7adae63cfbfafd8f397e2a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*8zH9nDALOmKJUtyZz9rvxg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">平均值和标准偏差，在标准定标器中计算</p></figure><p id="6bab" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">然后，它使用这两个值来<strong class="kj ir">缩放</strong>特征:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/dc8f1f7b84b6f09ee87a8c642731652f.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*wupjrhTHsvejCfXy2BGs6A.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">标准化</p></figure><p id="1522" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">如果我们要重新计算缩放要素的平均值和标准差，我们将分别得到 0 和 1。这个预处理步骤通常被称为<em class="ki">标准化</em>，尽管从技术上来说，它应该总是被称为<em class="ki">标准化</em>。</p></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><blockquote class="kd ke kf"><p id="d478" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj ir">零均值和单位标准差</strong></p><p id="f8e5" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">让我们从<strong class="kj ir">单位标准差</strong>开始，即缩放特征值，使其<strong class="kj ir">标准差</strong>等于<strong class="kj ir">一个</strong>。这是<strong class="kj ir">最重要的预处理步骤</strong>之一，不仅是为了提高<strong class="kj ir">梯度下降</strong>的性能，也是为了其他技术，如<strong class="kj ir">主成分分析(PCA) </strong>。<strong class="kj ir">的目标</strong>是将<strong class="kj ir">所有的数字特征</strong>放在一个<strong class="kj ir">相似的刻度</strong>中，因此结果不受每个特征的原始<strong class="kj ir">范围</strong>的影响。</p><p id="26bd" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">想到一个模特身上的两个共同特征:<strong class="kj ir">年龄</strong>和<strong class="kj ir">薪资</strong>。虽然年龄通常在 0 到 110 之间变化，但薪水可以从几百(比如说 500)到几千(比如说 9000)。如果我们计算相应的标准<br/>偏差，我们可能会分别得到 25 和 2000 这样的值。因此，我们需要<br/>到<strong class="kj ir">标准化</strong>两个特性，使它们处于<strong class="kj ir">的平等地位</strong>。</p><p id="469a" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">然后<strong class="kj ir">零点表示</strong>，即<strong class="kj ir">在<strong class="kj ir">零点</strong>处对中</strong>特征。<strong class="kj ir">更深层次的神经网络</strong>可能会遭受一种叫做<strong class="kj ir">消失梯度</strong>的非常严重的情况。由于梯度用于更新参数，越来越小(即消失)的梯度意味着越来越小的更新，直到停滞点:网络简单地停止学习。帮助网络对抗这种情况的一种方法是<strong class="kj ir">将它的输入</strong>即特征集中在零。关于渐变消失问题的更多细节，请查看我的<a class="ae kc" rel="noopener" target="_blank" href="/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404">“超参数在起作用！第二部分—权重初始值设定项。</a></p></blockquote></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><blockquote class="kd ke kf"><p id="289c" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj ir">重要</strong>:类似<strong class="kj ir">标准定标器</strong> <strong class="kj ir">的预处理步骤必须在</strong>列车验证测试分割后进行；否则，您将<strong class="kj ir">从验证和/或测试集向您的模型泄露</strong>信息！</p><p id="5b90" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在使用<strong class="kj ir">训练集仅</strong>来适应<strong class="kj ir">标准缩放器</strong>之后，您应该使用其<strong class="kj ir">转换</strong>方法来将预处理步骤应用于<strong class="kj ir">所有数据集</strong>:训练、验证和测试。</p></blockquote><p id="93fa" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">下面的代码将很好地说明这一点:</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="ne nf l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">标准化特征<strong class="ak"> x </strong></p></figure><p id="3c89" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">请注意，我们不是<strong class="kj ir">重新生成数据，而是使用<strong class="kj ir">原始特征<em class="ki"> x </em> </strong>作为<strong class="kj ir">标准缩放器</strong>的输入，并将其转换为<strong class="kj ir">缩放的<em class="ki"> x </em> </strong>。标签(<em class="ki"> y </em>)保持不变。</strong></p><p id="c585" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">让我们把它们三个、<em class="ki">原</em>、<em class="ki">坏</em>和<em class="ki">缩小</em>并列起来，来说明区别:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/7b387f7fdb90b66efad1b9d0de7ee747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EszcmFua2fp03c5Kfj7OAA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">相同的数据，特征<strong class="bd nu"> x </strong>的三种不同比例</p></figure><p id="1f65" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">同样，图与图之间唯一的<strong class="kj ir">差异是特征的<strong class="kj ir">比例<em class="ki"> x </em> </strong>。它的范围本来是[0，1]，后来我们把它做成[0，10]，现在<strong class="kj ir">标准缩放器</strong>把它做成[-1.5，1.5]。</strong></p><p id="b7ba" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">好了，检查<strong class="kj ir">损失面</strong>的时间到了:为了说明差异，我将它们三个并排绘制:<em class="ki">原</em>、<em class="ki">坏</em>和<em class="ki">缩放</em>……看起来是这样的:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/7d0cc86bd12e51c58837270f0ac01a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bU0DM8_svTYZ9UzuvZwDdA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">特征<strong class="bd nu"> x </strong>不同比例的损失面</p></figure><p id="3a46" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><strong class="kj ir">漂亮的</strong>，不是吗？教科书上对<strong class="kj ir">碗的定义</strong> :-)</p><p id="1106" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在实践中，这是人们可能希望的<strong class="kj ir">最佳表面</strong>:截面<strong class="kj ir">将会是<strong class="kj ir">同样陡峭的</strong>，并且其中一个的<strong class="kj ir">好的学习率</strong>对另一个也是好的。</strong></p><p id="1df4" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">当然，在现实世界中，你永远也不会得到那样一个漂亮的碗。但是我们的结论仍然成立:</p><ol class=""><li id="bf5b" class="ml mm iq kj b kk kl ko kp mf mn mh mo mj mp le ns mr ms mt bi translated">总是标准化(缩放)你的特征。</li><li id="2515" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le ns mr ms mt bi translated"><strong class="kj ir">永远不要忘记第一条:-) </strong></li></ol><h1 id="501f" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">最后的想法</h1><p id="fa7f" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">我希望这篇文章能帮助你发展<strong class="kj ir">直觉</strong>关于<strong class="kj ir">特征缩放如何影响学习率</strong>的选择，以及这两个元素如何在 <strong class="kj ir">梯度下降</strong>的<strong class="kj ir">性能中发挥基本作用。</strong></p><p id="d896" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><em class="ki">如果你有什么想法、意见或问题，欢迎在下方留言评论或联系我</em> <a class="ae kc" href="https://twitter.com/dvgodoy" rel="noopener ugc nofollow" target="_blank"> <em class="ki">推特</em> </a> <em class="ki">。</em></p></div></div>    
</body>
</html>