<html>
<head>
<title>From Streaming Data to COVID-19 Twitter Analysis: Using Spark and AWS Kinesis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从流数据到新冠肺炎推特分析:使用 Spark 和 AWS Kinesis</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/playing-with-data-gracefully-1-a-near-real-time-data-pipeline-using-spark-structured-streaming-409dc1b4aa3a?source=collection_archive---------17-----------------------#2020-03-30">https://towardsdatascience.com/playing-with-data-gracefully-1-a-near-real-time-data-pipeline-using-spark-structured-streaming-409dc1b4aa3a?source=collection_archive---------17-----------------------#2020-03-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f88d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">摆弄社交媒体数据、大数据平台和代码的指南。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d2222021f61f5e81432f99bde22c0662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VUwwJj97ya02jUyhDgVU8w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片引自<a class="ae kv" href="http://www.siasat.com" rel="noopener ugc nofollow" target="_blank">www.siasat.com</a></p></figure><p id="219d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着新冠肺炎的蔓延和确诊病例的飙升，世界正在关闭。曾经熙熙攘攘的地方已经变成了鬼城。相比之下，社交媒体比平时更吵。好消息、坏消息和假消息在推特、脸书和 Instagram 上流传。恐惧和安慰，错误的信息和澄清，可以很容易地在所有激烈的讨论中找到。社交媒体可能从未像现在这样发挥过如此重要的作用，让人们体验外部世界和自己。</p><p id="f8b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于数据科学界来说，是时候做出反应并跟踪新冠肺炎的全球影响了。在这篇文章中，我将展示一个研究社交媒体的简单方法——分析 Twitter。</p><h1 id="f419" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">概述</h1><p id="1ea7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">本文将讲述如何利用流行的大数据平台<em class="mp"> Spark </em>和<em class="mp"> AWS </em>构建数据管道，实现 Twitter 上的<strong class="ky ir">情绪</strong>和<strong class="ky ir">时间序列</strong> <strong class="ky ir">分析</strong>。管道将把通过标签和关键词过滤的推文转换成流数据格式，这种格式可以在 Spark 和 Pandas 中操作。以接近实时处理的方式处理流数据<a class="ae kv" href="https://blog.syncsort.com/2015/11/big-data/the-difference-between-real-time-near-real-time-and-batch-processing-in-big-data/" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/7a37b1da0a1ec47e05666cd03ca1d827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sccUyqc0VR_JBhwNGkfNbg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据管道的架构</p></figure><p id="a37e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上图所示，该流程将经历以下步骤:</p><ul class=""><li id="e1d6" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><strong class="ky ir">步骤 1 </strong>:使用<em class="mp"> Tweepy </em>的 python 程序将在<em class="mp"> EC2 </em>实例上运行，获取用标签和关键字过滤的 tweets。数据将被发送到 Kinesis 数据流中。</li><li id="d7f2" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">第二步</strong>:<em class="mp">kine sis 数据流</em>以可管理、可扩展的方式消化数据。监控指标并检查数据是否进入 Kinesis。</li><li id="6e32" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">步骤 3 </strong>:依靠<em class="mp">Data bricks Community Edition</em>提供的笔记本，应用程序将实现<em class="mp"> Spark 结构化流</em>和 Kinesis 数据流之间的连接。该应用程序使用<em class="mp"> Spark SQL </em>和 T <em class="mp"> extBlob </em>构建了一个情感分析器。</li><li id="4960" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">步骤 4 </strong>:转换成<em class="mp"> Pandas Dataframes </em>，使用<em class="mp"> Matplotlib </em>用时间戳索引数据，并用时间序列可视化。</li></ul><p id="e9b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://github.com/joking-clock/twitter-capture-python/blob/master/twitter_kinesis_data.py" rel="noopener ugc nofollow" target="_blank">推文捕捉</a>和<a class="ae kv" href="https://github.com/joking-clock/twitter-analysis-bigdata/blob/master/spark_SStream_Pandas.ipynb" rel="noopener ugc nofollow" target="_blank">分析</a>的源代码已经上传到 GitHub。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="dd85" class="ls lt iq bd lu lv nm lx ly lz nn mb mc jw no jx me jz np ka mg kc nq kd mi mj bi translated">第 1 部分:捕获推文</h1><p id="1314" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">要分析数据，首先需要有数据。<a class="ae kv" href="http://docs.tweepy.org/en/latest/getting_started.html" rel="noopener ugc nofollow" target="_blank"> Tweepy </a>是一个强大的 python 库，用于捕捉实时推文。想象一下，捕获 tweets 的程序可能需要几个小时，这会消耗 CPU 和内存。在这种情况下，在远程机器 EC2 上运行它是个好主意。</p><p id="e843" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面附上了 tweet 捕获代码的一个片段。<a class="ae kv" href="http://docs.tweepy.org/en/latest/streaming_how_to.html" rel="noopener ugc nofollow" target="_blank">这里用的是 Tweepy 流</a>。完整的代码上传到<a class="ae kv" href="https://github.com/joking-clock/twitter-capture-python/blob/master/twitter_kinesis_data.py" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="9acd" class="nw lt iq ns b gy nx ny l nz oa">stream_name = ''  # fill the name of Kinesis data stream you created</span><span id="8bd2" class="nw lt iq ns b gy ob ny l nz oa">if __name__ == '__main__':<br/>    # create kinesis client connection<br/>    kinesis_client = boto3.client('kinesis', <br/>       region_name='',  # enter the region<br/>       aws_access_key_id='',  # fill your AWS access key id<br/>       aws_secret_access_key='')  # fill you aws secret access key<br/>    # create instance of the tweepy tweet stream listener<br/>    listener = TweetStreamListener()<br/>    # set twitter keys/tokens<br/>    auth = OAuthHandler(consumer_key, consumer_secret)<br/>    auth.set_access_token(access_token, access_token_secret)<br/>    # create instance of the tweepy stream<br/>    stream = Stream(auth, listener)</span></pre><p id="017e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">记得用您的 AWS 和 twitter 凭证填写缺失的字段。</p><h1 id="2741" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">第 2 部分:使用 Kinesis 数据流处理流数据</h1><p id="7b9e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果我们开始运行代码，数据就会像水一样源源不断地被提取出来。它需要通过软管或管道，或者需要储存。AWS Kinesis 数据流(KDS)就是这样一个软管，提供弹性缩放、实时度量和数据分析等功能。</p><p id="8c1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">创建 KDS 非常简单。进入 AWS Kinesis 服务页面，选择创建数据流，填写数据流名称。至于碎片的数量，因为这只是一个演示，1 个碎片应该没问题。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/c5a969b818edbd05765031391e48de52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nEM5FlPoFqNjEiu-nh4nYQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">创建 Kinesis 数据流的页面</p></figure><p id="ec9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦创建了 KDS，我们就可以运行第 1 部分中的 tweet 捕获代码。在这个演示中，捕获的推文应该包含标签“#新冠肺炎”和关键字“加拿大”。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/791ebeeded1b482593576c70d0300660.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAKFqwpIewhLlFjIHBh4lQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">运行 tweet 捕获代码</p></figure><p id="c6d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">等待几分钟后，从 Kinesis 数据流的 monitoring 选项卡中，我们可以看到 tweet 流的指标。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/acc4b23ec8c538132ef4cf5e35cf375f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q4BN2gzoCEyr7YKnDb-djQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">反映在 Kinesis 数据流上的指标</p></figure><h1 id="17ca" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">第 3 部分:使用 Spark 结构化流接收数据</h1><p id="ca11" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Spark 是一个流行的集群计算框架，用来处理大数据问题。自 2014 年诞生以来，Spark 一直在不断发展，经历了很多变化。某些变化是关键的，可能会令人困惑，所以我会解释一下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/a41633ed3974c413e7222cac0c9d7df4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*3vHF4PJl6yutfII9pmipYg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">引自 javatpoint.com</p></figure><p id="fd89" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">【Spark】弹性分布式数据集(RDD) </strong> </a> <em class="mp"> </em>是<em class="mp"> </em>跨集群节点划分的、可以并行操作的元素集合。rdd 是 Spark 中主要的逻辑数据单元。基于 RDD，<a class="ae kv" href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> Spark Streaming </strong> </a>构建为可扩展的容错流式处理系统，本机支持批处理和流式工作负载。</p><p id="420f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从 Spark 2.x 开始引入了变化。<a class="ae kv" href="https://spark.apache.org/docs/latest/sql-getting-started.html#creating-dataframes" rel="noopener ugc nofollow" target="_blank"> <em class="mp">数据帧</em> </a>作为 RDD 之上的抽象发布，随后是<em class="mp">数据集</em>。在 Spark 1.x 中，RDD 是主要的 API，但是从 Spark 2.x 开始，鼓励使用 DataFrame API。为了支持数据帧和数据集，<a class="ae kv" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> Spark 结构化流</strong> </a>应运而生，提供了比<em class="mp"> Spark 流 API </em>更高级的接口。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e12243a30931f77a78c426e7381f977b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2CbME5idjflA3l83Bjxbag.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">位于<a class="ae kv" href="https://www.slideshare.net/AnkitBeohar/spark-rdd-vs-data-frame-vs-dataset" rel="noopener ugc nofollow" target="_blank">链接的幻灯片</a></p></figure><p id="821d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://spark.apache.org/docs/latest/sql-getting-started.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> Spark SQL </strong> </a>允许用户操作数据帧。从名字我们就能猜到 Spark SQL 提供了<em class="mp"> SQL </em>语言支持，挺像<a class="ae kv" href="https://en.wikipedia.org/wiki/Apache_Hive" rel="noopener ugc nofollow" target="_blank"> Hive </a>的。</p><p id="a55f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经完成了理论，是时候动手了。Spark 结构化流的编程范例如下:</p><ul class=""><li id="ca2e" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">导入必要的类并创建一个本地 SparkSession</li><li id="2fac" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">创建火花流数据帧</li><li id="bdd8" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">在数据帧上进行操作</li></ul><p id="ed22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更多内容请参考<a class="ae kv" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#quick-example" rel="noopener ugc nofollow" target="_blank">如何使用 Spark 结构化流进行编程</a>。</p><p id="afe3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://databricks.com/product/faq/community-edition" rel="noopener ugc nofollow" target="_blank"> Databrick 社区版</a>是一站式大数据平台。用户可以像在<em class="mp"> Jupyter </em>笔记本中一样编写 Python 代码，但是能够与 Spark 集群流畅地交互。此外，Databricks 社区版还提供了有用的功能，如<a class="ae kv" href="https://docs.databricks.com/notebooks/notebooks-use.html#mix-languages" rel="noopener ugc nofollow" target="_blank"> <em class="mp">混合语言</em> </a>和<a class="ae kv" href="https://docs.databricks.com/notebooks/visualizations/index.html#visualizations" rel="noopener ugc nofollow" target="_blank"> <em class="mp">可视化</em> </a>。</p><p id="d0c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们需要创建一个 spark 会话:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="7568" class="nw lt iq ns b gy nx ny l nz oa">spark = SparkSession.builder\<br/>                    .master("local")\<br/>                    .appName("Structured Streaming")\<br/>                    .getOrCreate()</span></pre><p id="69d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后从 Kinesis 数据流中创建火花流数据帧:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="5f87" class="nw lt iq ns b gy nx ny l nz oa">pythonSchema = StructType() \<br/>          .add("id", StringType(), True) \<br/>          .add("tweet", StringType(), True) \<br/>          .add("ts", StringType(), True)</span><span id="d8ad" class="nw lt iq ns b gy ob ny l nz oa">awsAccessKeyId = "" # update the access key<br/>awsSecretKey = ""   # update the secret key<br/>kinesisStreamName = ""  # update the kinesis stream name<br/>kinesisRegion = ""</span><span id="e217" class="nw lt iq ns b gy ob ny l nz oa">kinesisDF = spark \<br/>  .readStream \<br/>  .format("kinesis") \<br/>  .option("streamName", kinesisStreamName)\<br/>  .option("region", kinesisRegion) \<br/>  .option("initialPosition", "LATEST") \<br/>  .option("format", "json") \<br/>  .option("awsAccessKey", awsAccessKeyId)\<br/>  .option("awsSecretKey", awsSecretKey) \<br/>  .option("inferSchema", "true") \<br/>  .load()</span></pre><p id="a058" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过在 EC2 和 Kinesis 数据流上运行 tweet 捕获程序，我们可以看到来自 Kinesis 数据流的数据是如何进入 Spark 流数据帧的:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="2d08" class="nw lt iq ns b gy nx ny l nz oa">df = kinesisDF \<br/>  .writeStream \<br/>  .format("memory") \<br/>  .outputMode("append") \<br/>  .queryName("tweets")  \<br/>  .start()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/2f8613afff34c31a2a4307c65405cb34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KATnU4jDNFcF6O4apq9XiA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">流式数据正在进入 Spark</p></figure><p id="1d42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mp"> df </em>是一个<em class="mp">流查询</em>来处理主动流查询。<em class="mp"> format("memory") </em>表示输出将作为内存表存储在内存中。<strong class="ky ir">输出模式</strong>指定写入输出接收器的内容，这里我们选择<em class="mp">追加</em>模式，这意味着只向结果表添加新行。</p><p id="7b6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦开始流式传输，就可以通过控制面板监控传入的数据。通过检查<em class="mp"> df </em>的状态，我们知道数据是可用的，并且我们可以开始探索数据。</p><p id="b34a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对查询应用 SQL 命令:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="9246" class="nw lt iq ns b gy nx ny l nz oa">tweets = spark.sql("select cast(data as string) from tweets")</span></pre><p id="3b96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看查询的标题数据:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/8b2adc9e18bca569b9dc2ac35ac80f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*flsmBbhV0rQocuRdowDkDA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">浏览数据帧中的前 5 行数据</p></figure><h1 id="44c7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">第 4 部分:使用 Spark 数据框架进行情感分析</h1><p id="7e5e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了分析数据，它需要更加结构化。所有返回 tweets 的 Twitter APIs 都提供了使用 JavaScript Object Notation (JSON)编码的数据。让我们简单看一下一个例子:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="0a06" class="nw lt iq ns b gy nx ny l nz oa">{<br/>    "created_at": "Thu May 10 15:24:15 +0000 2018",<br/>    "id_str": "850006245121695744",<br/>    "text": "Here is the Tweet message.",<br/>    "user": {<br/>    ...<br/>    },<br/>    "place": {},<br/>    "entities": {},<br/>    "extended_entities": {<br/>     ...<br/>    }<br/>}</span></pre><p id="e7a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当<a class="ae kv" href="https://github.com/joking-clock/twitter-capture-python/blob/master/twitter_kinesis_data.py" rel="noopener ugc nofollow" target="_blank"> tweet 捕获代码</a>运行时，我们已经指定了所需的字段:“id”表示为 tweet id，“text”表示为 tweet 内容，“ts”表示为时间戳。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="cabf" class="nw lt iq ns b gy nx ny l nz oa"># tweet capture code running in EC2</span><span id="24d8" class="nw lt iq ns b gy ob ny l nz oa">def on_data(self, data):<br/>        # decode json<br/>        tweet = json.loads(data)<br/>        # print(tweet)<br/>        if "text" in tweet.keys():<br/>            payload = {'id': str(tweet['id']),<br/>                                  'tweet': str(tweet['text'].encode('utf8', 'replace')),<br/>                                  'ts': str(tweet['created_at']),<br/>            },<br/>            print(payload)<br/>            try:<br/>                put_response = kinesis_client.put_record(<br/>                                StreamName=stream_name,<br/>                                Data=json.dumps(payload),<br/>                                PartitionKey=str(tweet['user']['screen_name']))<br/>                ...</span></pre><p id="d96c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以我们收集的数据已经组织好了。使用<a class="ae kv" href="https://docs.databricks.com/spark/latest/spark-sql/udf-python.html" rel="noopener ugc nofollow" target="_blank"> <em class="mp"> UDF </em> </a>，我们可以将这些字段添加到 Spark 数据帧中。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="3357" class="nw lt iq ns b gy nx ny l nz oa">def parse_tweet(text):<br/>    data = json.loads(text)<br/>    id = data[0]['id']<br/>    ts = data[0]['ts']<br/>    tweet = data[0]['tweet'] <br/>    return (id, ts, tweet)<br/>    <br/># Define your function<br/>getID = UserDefinedFunction(lambda x: parse_tweet(x)[0], StringType())<br/>getTs = UserDefinedFunction(lambda x: parse_tweet(x)[1], StringType())<br/>getTweet = UserDefinedFunction(lambda x: parse_tweet(x)[2], StringType())</span><span id="82aa" class="nw lt iq ns b gy ob ny l nz oa"># Apply the UDF using withColumn<br/>tweets = (tweets.withColumn('id', getID(col("data")))<br/>               .withColumn('ts', getTs(col("data")))<br/>               .withColumn('tweet', getTweet(col("data")))<br/>         )</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/b04f54b3f4128760109fb1e9a63843ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9OZGVnbDJHO9AGldoJ06lQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">添加字段后浏览数据框架</p></figure><p id="b472" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们对它进行情感分析。情感分析是自然语言处理(<strong class="ky ir"> NLP </strong>)的一部分，对文本中的情感进行解释和分类。有许多学术和工业作品涉及这一领域，大多数 NLP 图书馆都有能力做到这一点。仅出于演示目的，我们使用<a class="ae kv" href="https://textblob.readthedocs.io/en/dev/quickstart.html" rel="noopener ugc nofollow" target="_blank"> <em class="mp"> TextBlob </em> </a>来执行这项工作。</p><p id="a384" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">情感分析的基本任务是将给定文本的<a class="ae kv" href="https://www.analyticsvidhya.com/blog/2018/02/natural-languageNatural%20Language%20Processing%20for%20Beginners:%20Using%20TextBlob-processing-for-beginners-using-textblob/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"/></a>极性分类，分为负面、中性和正面。极性数是一个位于[-1，1]范围内的浮点数。1 表示肯定的陈述，而-1 表示否定的陈述。因此，我们可以将数据分类如下:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="45f0" class="nw lt iq ns b gy nx ny l nz oa">import textblob</span><span id="0fa9" class="nw lt iq ns b gy ob ny l nz oa">def get_sentiment(text):<br/>    from textblob import TextBlob<br/>    tweet = TextBlob(text)<br/>    if tweet.sentiment.polarity &lt; 0:<br/>      sentiment = "negative"<br/>    elif tweet.sentiment.polarity == 0:<br/>        sentiment = "neutral"<br/>    else:<br/>        sentiment = "positive"<br/>    return sentiment</span></pre><p id="9c38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们有了情感信息的领域，我们可以利用它对推文进行高层次的洞察。在这里，我们希望看到这些捕获的推文的情绪分类，猜测哪些类型的情绪被揭示。利用 Databricks Community Edition 平台上的<a class="ae kv" href="https://docs.databricks.com/notebooks/notebooks-use.html#mix-languages" rel="noopener ugc nofollow" target="_blank"> <em class="mp">混合语言</em> </a>和<a class="ae kv" href="https://docs.databricks.com/notebooks/visualizations/index.html#visualizations" rel="noopener ugc nofollow" target="_blank"> <em class="mp">可视化</em> </a>的特性，实现了关于情感与推文数量关系的情感分析。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="4e61" class="nw lt iq ns b gy nx ny l nz oa">%sql<br/>select sentiment, count(*) as cnt from tweets_parsed group by sentiment</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/0f593b4cb3b7b49d25079f9560ee89ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*57EU3Yscqbca3r_UkQwQ9w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用条形图表示的情感分析</p></figure><h1 id="feee" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">第 5 部分:使用 Numpy，Pandas，Matplotlib 进行时间序列分析</h1><p id="c481" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">以上分析基于<a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=streamingquery#pyspark.sql.DataFrame" rel="noopener ugc nofollow" target="_blank"><em class="mp">PySpark data frame</em></a><em class="mp">s</em>。但是 Numpy/Pandas/Matplotlib 的组合更专业的对数据做操作。因此，我们需要将 PySpark 数据帧转换成帕纳斯数据帧。幸运的是，这并不难。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="be97" class="nw lt iq ns b gy nx ny l nz oa">tweets_pdf = tweets.toPandas()</span></pre><p id="deab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以使用 Pandas 的丰富功能来探索数据:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/5857ee67a51757d7938940c37c92111b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7evdZy5HWF-xenyLrRMD3w.png"/></div></div></figure><p id="59fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要进行时间序列分析，最佳实践是用时间单位索引数据。这里我们需要将字段<em class="mp"> ts </em>转换成<em class="mp"> datatime </em>类型，然后用它来索引数据帧。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="1d0b" class="nw lt iq ns b gy nx ny l nz oa">pd.to_datetime(tweets_pdf['ts'])<br/>idx = pd.DatetimeIndex(pd.to_datetime(tweets_pdf['ts']))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/4e2f57f101158d5304e0707e8758c469.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*nUOHyTdo6u3kAeqXjKVABA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">检查 ts 字段是否被正确解析为时间戳索引</p></figure><p id="ce54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码是对数据进行操作，然后使用 Matplotlib 将其可视化成时间序列折线图。该图表描述了包含标签“#新冠肺炎”和关键词“加拿大”的推文数量如何随时间变化。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="6d5c" class="nw lt iq ns b gy nx ny l nz oa"># Plotting the series<br/>%matplotlib inline</span><span id="1704" class="nw lt iq ns b gy ob ny l nz oa">fig, ax = plt.subplots()<br/>ax.grid(True)</span><span id="2356" class="nw lt iq ns b gy ob ny l nz oa">ax.set_title("Tweet Numbers")<br/>interval = mdates.MinuteLocator(interval=10)<br/>date_formatter = mdates.DateFormatter('%H:%M')</span><span id="ae6e" class="nw lt iq ns b gy ob ny l nz oa">datemin = datetime(2020, 3, 28, 16, 00) <br/>datemax = datetime(2020, 3, 28, 17, 10)</span><span id="843b" class="nw lt iq ns b gy ob ny l nz oa">ax.xaxis.set_major_locator(interval) <br/>ax.xaxis.set_major_formatter(date_formatter) <br/>ax.set_xlim(datemin, datemax)<br/>max_freq = per_minute.max()<br/>min_freq = per_minute.min()<br/>ax.set_ylim(min_freq-100, max_freq+100) <br/>ax.plot(per_minute.index, per_minute)</span><span id="da72" class="nw lt iq ns b gy ob ny l nz oa">display(fig)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/aad8d6aee848c53ec18c7bdb24875d29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*sHO6RKjVCp6J6SuwAnfh2Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">x 轴:推文发布时间，y 轴:推文数量</p></figure><h1 id="3070" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="f084" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在这项工作中，我们经历了从建立数据管道到进行有意义的分析的过程。但这只是挖掘社交媒体的一小步。在未来，我将继续探索并找出其他方法来洞察大数据。</p><p id="424a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mp">编者按:</em> </strong> <em class="mp"> </em> <a class="ae kv" href="http://towardsdatascience.com/" rel="noopener" target="_blank"> <em class="mp">走向数据科学</em> </a> <em class="mp">是一份以数据科学和机器学习研究为主的中型刊物。我们不是健康专家或流行病学家，本文的观点不应被解释为专业建议。想了解更多关于疫情冠状病毒的信息，可以点击</em> <a class="ae kv" href="https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports" rel="noopener ugc nofollow" target="_blank"> <em class="mp">这里</em> </a> <em class="mp">。</em></p></div></div>    
</body>
</html>