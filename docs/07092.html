<html>
<head>
<title>Node embeddings: Node2vec with Neo4j</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">节点嵌入:使用Neo4j的Node2vec</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/node-embeddings-node2vec-with-neo4j-5152d3472d8e?source=collection_archive---------23-----------------------#2020-05-31">https://towardsdatascience.com/node-embeddings-node2vec-with-neo4j-5152d3472d8e?source=collection_archive---------23-----------------------#2020-05-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1ff4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何使用Neo4j图形数据科学训练您的自定义node2vec算法</h2></div><p id="cc1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我上一篇关于<a class="ae lb" rel="noopener" target="_blank" href="/nlp-and-graphs-go-hand-in-hand-with-neo4j-and-apoc-e57f59f46845">将图形与NLP </a>技术相结合的博文是迄今为止最成功的。这促使我写更多关于这个话题的东西。在我的研究过程中，我偶然发现了node2vec算法，并注意到用<a class="ae lb" href="https://neo4j.com/" rel="noopener ugc nofollow" target="_blank"> Neo4j </a>和<a class="ae lb" href="https://github.com/neo4j/graph-data-science" rel="noopener ugc nofollow" target="_blank">图形数据科学库</a>来实现它是多么容易。我想这让我别无选择，只能戴上我的<a class="ae lb" href="https://imgur.com/a/eR1t5Sk" rel="noopener ugc nofollow" target="_blank"> Neo4j数据科学眼镜</a>并展示实现它是多么容易。</p><h1 id="87db" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">图形导入</h1><p id="cd66" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">今天我们将使用Kaggle上的<a class="ae lb" href="https://www.kaggle.com/hawkash/spoonacular-food-dataset" rel="noopener ugc nofollow" target="_blank">匙羹鱼食物数据集</a>。它包含营养信息以及1600多种菜肴中使用的配料。不幸的是，它没有包含酸奶面包的配方。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/4aaa0fdf6e5a68e53f8cc88845e6488c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xiI0On7i1zf36tcV"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图表模式</p></figure><p id="34e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图形模式中有三种类型的节点。一道菜由一种或多种食材组成，我们将其表示为一道菜与其食材之间的联系。食谱分为不同的类别或类型，如午餐、早餐等等。我们使用<code class="fe mp mq mr ms b">apoc.schema.assert</code>过程来定义图表模式。它允许我们在一个查询中描述多个索引和唯一约束。</p><pre class="ma mb mc md gt mt ms mu mv aw mw bi"><span id="50c6" class="mx ld iq ms b gy my mz l na nb">CALL apoc.schema.assert(<br/>  // define indexes<br/>  null,<br/>  // define unique constraints<br/> {Ingredient:['name'], Dish:['id'], DishType:['name']})</span></pre><p id="eeff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在执行导入查询之前，我们必须下载数据集并将其复制到Neo4j导入文件夹中。在导入查询中，我们做了一点点预处理，将配料的名称小写，并用空格替换破折号(<code class="fe mp mq mr ms b">-</code>)。</p><pre class="ma mb mc md gt mt ms mu mv aw mw bi"><span id="1547" class="mx ld iq ms b gy my mz l na nb">LOAD CSV WITH HEADERS FROM "file:///newfood.csv" as row<br/>CREATE (d:Dish{id:row.id})<br/>SET d += apoc.map.clean(row, ['id','dishTypes','ingredients'],[])<br/>FOREACH (i in split(row.ingredients,',') | <br/>    MERGE (in:Ingredient{name:toLower(replace(i,'-',' '))}) <br/>    MERGE (in)&lt;-[:HAS_INGREDIENT]-(d))<br/>FOREACH (dt in split(row.dishTypes,',') | <br/>    MERGE (dts:DishType{name:dt}) <br/>    MERGE (dts)&lt;-[:DISH_TYPE]-(d))</span></pre><h1 id="c302" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">基本图形探索</h1><p id="b7e9" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">首先，我们将进行一些图形探索。我们来看看哪些食材用的最多。</p><pre class="ma mb mc md gt mt ms mu mv aw mw bi"><span id="a205" class="mx ld iq ms b gy my mz l na nb">MATCH (n:Ingredient) <br/>RETURN n.name as ingredient, <br/>       size((n)&lt;--()) as mentions<br/>ORDER BY mentions DESC <br/>LIMIT 10</span></pre><p id="4048" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="6e9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">橄榄油是迄今为止最受欢迎的，因为它在超过一半的食谱中使用。慢慢接着是大蒜、盐和黄油。我不知道黄油如此受欢迎。我也发现凤尾鱼被如此广泛地使用是相当令人惊讶的。或者也许数据集只是偏向于含有凤尾鱼的菜肴。</p><p id="30cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以在这个数据集的基础上构建一个应用程序，根据我们想要烹饪的配料来搜索食谱。我从<a class="ae lb" href="https://twitter.com/markhneedham" rel="noopener ugc nofollow" target="_blank">马克·尼达姆</a>和<a class="ae lb" href="https://twitter.com/ElLazal" rel="noopener ugc nofollow" target="_blank"> Lju Lazarevic </a>写的<a class="ae lb" href="https://dzone.com/articles/whats-cooking-part-2-what-can-i-make-with-these-in" rel="noopener ugc nofollow" target="_blank">什么是烹饪系列</a>中借用了这个密码查询。比方说，你今天想吃西葫芦和羊乳酪，但不知道该吃哪种食谱。幸运的是，我们的应用程序可以通过下面的cypher查询帮助我们解决这个问题。</p><pre class="ma mb mc md gt mt ms mu mv aw mw bi"><span id="2bc3" class="mx ld iq ms b gy my mz l na nb">WITH ["feta cheese", "zucchini"] as ingredients<br/>MATCH (d:Dish)<br/>WHERE <br/>  all(i in ingredients WHERE exists(<br/>  (d)-[:HAS_INGREDIENT]-&gt;(:Ingredient {name: i})))<br/>RETURN d.title AS dish<br/>ORDER BY size(ingredients)<br/>LIMIT 10</span></pre><p id="1ff5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="a3f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以配沙拉或鱼。我想我会跳过去node2vec吃甜点。</p><h1 id="ede9" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">Node2vec算法</h1><p id="9fb3" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">node2vec算法相对较新。它是在2016年由Jure Leskovac和Aditya Grover在一篇文章<a class="ae lb" href="https://arxiv.org/abs/1607.00653" rel="noopener ugc nofollow" target="_blank"> node2vec:可扩展的网络特征学习</a>中提出的。要了解它是如何工作的，首先要了解word2vec算法。Word2vec算法是由谷歌的托马斯·米科洛夫领导的研究团队在2013年提出的。使用神经网络来学习单词嵌入是一种流行的技术。它将一个句子列表作为输入，并为文本语料库中出现的每个单词生成一个向量或嵌入。词义相近的词在嵌入空间上要更近一些。比如苹果和梨应该比苹果和汽车更相似。word2vec有两种训练算法。第一种方法被称为连续单词袋(CBOW)，它使用单词的上下文来预测目标术语。上下文被定义为出现在文本中目标单词附近的单词。第二种方法叫做跳格法。它不是试图从上下文中预测目标单词，而是试图预测给定术语的上下文。如果您想了解更多关于word2vec算法的知识，互联网上有大量关于它的好文献。</p><p id="dd51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可能会问我们是如何从word2vec到node2vec的。这其实很简单。我们不使用句子列表作为输入，而是使用随机漫步列表。这是唯一的区别。</p><h1 id="5461" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">Neo4j图形数据科学库</h1><p id="a70e" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated"><a class="ae lb" href="https://github.com/neo4j/graph-data-science" rel="noopener ugc nofollow" target="_blank"> Neo4j图形数据科学</a>库支持<a class="ae lb" href="https://neo4j.com/docs/graph-data-science/current/alpha-algorithms/random-walk/" rel="noopener ugc nofollow" target="_blank">随机游走算法</a>，这使得我们实现node2vec算法变得非常容易。如果你需要快速复习一下GDS图书馆是如何运作的，你可以看看我之前的博客文章。我们将从投影内存中的图形开始。我们将所有三个节点标签和项目关系描述为无向的。</p><pre class="ma mb mc md gt mt ms mu mv aw mw bi"><span id="1eca" class="mx ld iq ms b gy my mz l na nb">CALL gds.graph.create('all',<br/>    ['Dish', 'Ingredient'],<br/>    {undirected:{type:'*', orientation:'UNDIRECTED'}})</span></pre><p id="b43f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们准备训练我们的第一个node2vec模型。该流程将包括三个部分:</p><ol class=""><li id="dc77" class="ne nf iq kh b ki kj kl km ko ng ks nh kw ni la nj nk nl nm bi translated">从图中的每个节点开始执行随机行走算法</li><li id="29b6" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">将随机漫步输入word2vec算法</li><li id="eac2" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">通过查看最相似的邻居来检查结果</li></ol><p id="fde9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随机行走算法有一个可选的<code class="fe mp mq mr ms b">start</code>参数，可以用来定义行走的起始节点。我们还可以用<code class="fe mp mq mr ms b">steps </code>设置指定行走的长度，用<code class="fe mp mq mr ms b">walks</code>参数指定行走的次数。请注意，每次执行随机漫步时，我们都期望得到不同的结果。</p><p id="01c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用<a class="ae lb" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank"> gensim库</a>中的Word2vec算法实现。它还有几个我们可以定义的超参数。最值得注意的是:</p><ul class=""><li id="7782" class="ne nf iq kh b ki kj kl km ko ng ks nh kw ni la ns nk nl nm bi translated"><strong class="kh ir">大小</strong>:嵌入向量的维数</li><li id="9aae" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la ns nk nl nm bi translated"><strong class="kh ir">窗口</strong>:当前字和预测字之间的最大距离</li><li id="edcd" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la ns nk nl nm bi translated"><strong class="kh ir"> min_count </strong>:训练模型时要考虑的最小字数；出现次数少于此计数的单词将被忽略。</li><li id="e1d7" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la ns nk nl nm bi translated"><strong class="kh ir"> sg </strong>:训练算法:跳格1；否则默认CBOW</li></ul><p id="00e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">查看<a class="ae lb" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank">官方文档</a>了解更多关于word2vec超参数的信息</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="7beb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果</p><pre class="ma mb mc md gt mt ms mu mv aw mw bi"><span id="c321" class="mx ld iq ms b gy my mz l na nb">[('anchovy fillet', 0.6236759424209595),<br/> ('pork shoulder roast', 0.6039043068885803),<br/> ('penne', 0.5999650955200195),<br/> ('cherry', 0.5930663347244263),<br/> ('cooked quinoa', 0.5898399353027344),<br/> ('turkey', 0.5864514112472534),<br/> ('asiago cheese', 0.5858502388000488),<br/> ('pasillas', 0.5852196216583252),<br/> ('fresh marjoram', 0.5819133520126343),<br/> ('prunes', 0.5735701322555542)]</span></pre><p id="edf2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我知道有这么简单，我早就写node2vec算法了。另一方面，结果有点可疑。我不知道这个数据集和凤尾鱼有什么关系。看起来食谱大多是由真正喜欢它们的人写的。你可能会得到完全不同的结果。</p><p id="dd19" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae lb" href="https://arxiv.org/abs/1607.00653" rel="noopener ugc nofollow" target="_blank">最初的node2vec论文</a>中，作者定义了两个控制随机漫步执行的参数。第一个是返回参数。</p><blockquote class="nt nu nv"><p id="c68c" class="kf kg nw kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">返回参数</em> </strong> <em class="iq">，p .参数p控制在行走中立即重访一个节点的可能性。将其设置为较高的值(&gt; max(q，1))确保我们不太可能在接下来的两步中对已经访问过的节点进行采样(除非遍历中的下一个节点没有其他邻居)。该策略鼓励适度探索，并避免采样中的2跳冗余。另一方面，如果p较低(&lt; min(q，1))，它将导致遍历返回一步(图2)，这将使遍历“局部”靠近起始节点u。</em></p></blockquote><p id="0529" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二个参数称为输入输出参数。</p><blockquote class="nt nu nv"><p id="ffce" class="kf kg nw kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq"> In-out参数</em> </strong> <em class="iq">，q .参数q允许搜索区分“向内”和“向外”节点。回到图2，如果q &gt; 1，则随机行走偏向于靠近节点t的节点。这种行走相对于行走中的开始节点获得基础图的局部视图，并且在我们的样本包括小局部内的节点的意义上近似BFS行为。相反，如果q &lt; 1，walk更倾向于访问离节点t更远的节点。这种行为反映了鼓励向外探索的DFS。然而，这里的一个本质区别是我们在随机游走框架内实现了类似DFS的探索。因此，被采样的节点与给定的源节点u的距离不是严格递增的，但是反过来，我们受益于易处理的预处理和随机行走的优越采样效率。注意，通过将πv，x设置为行走t中前一节点的函数，随机行走是二阶马尔可夫的。</em></p></blockquote><p id="0084" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总之，return参数指示随机漫步多长时间回溯一步或两步。进出参数控制随机行走是更侧重于局部探索(类似于BFS)还是更倾向于向外探索(类似于DFS)。尽管随机游走算法仍处于alpha层，但它支持这两个node2vec参数。让我们在实践中尝试一下。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="6633" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果</p><pre class="ma mb mc md gt mt ms mu mv aw mw bi"><span id="3215" class="mx ld iq ms b gy my mz l na nb">[('leg of lamb', 0.7168825268745422),<br/> ('anise seeds', 0.6833588480949402),<br/> ('basic bruschetta', 0.6759517192840576),<br/> ('dried chilli flakes', 0.6719993352890015),<br/> ('tuna in olive oil', 0.6697120666503906),<br/> ('spaghetti pasta', 0.669307291507721),<br/> ('prime rib roast', 0.661544919013977),<br/> ('baby artichokes', 0.6588324308395386),<br/> ('rice vermicelli', 0.6581511497497559),<br/> ('whole wheat fusilli', 0.6571477651596069)]</span></pre><p id="5687" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看结果让我很饿。羊腿和橄榄油要不要算类似食材，真的不好说。如果我们观察图表，在八个包含羊腿的食谱中，有七个也使用橄榄油。按这种逻辑，两者颇为相似。</p><p id="d8d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下一个例子中，我们将展示如何运行node2vec算法并将结果嵌入存储回Neo4j。我们将返回内部的Neo4j节点id，而不是返回菜肴和配料的标题。这将有助于我们有效地将结果链接回Neo4j。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="f8b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些嵌入现在在word2vec模型的词汇表中可用。我们将使用<code class="fe mp mq mr ms b">UNWIND</code> cypher语句将它们一次性存储到Neo4j中。如果可能的话，尽量避免每行提交一个事务，因为这不是很高效。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="ccf9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Word2vec模型使用余弦相似度来查找最相似的单词。图形数据科学库还支持<a class="ae lb" href="https://neo4j.com/docs/graph-data-science/current/alpha-algorithms/cosine/" rel="noopener ugc nofollow" target="_blank">余弦相似度算法</a>，可用于推断相似度算法。与所有相似性算法一样，我们必须微调<code class="fe mp mq mr ms b">similarityCutoff</code>和<code class="fe mp mq mr ms b">topK</code>参数以获得最佳结果。它们直接影响推断的相似性图的稀疏程度。</p><pre class="ma mb mc md gt mt ms mu mv aw mw bi"><span id="1a6c" class="mx ld iq ms b gy my mz l na nb">MATCH (node) <br/>WITH id(node) as id, node.embedding as weights <br/>WITH {item:id, weights: weights} as dishData <br/>WITH collect(dishData) as data <br/>CALL gds.alpha.similarity.cosine.write({<br/>    nodeProjection: '*', <br/>    relationshipProjection: '*', <br/>    similarityCutoff:0.5, <br/>    topK:5, <br/>    data: data,<br/>    writeRelationshipType:'COSINE_SIMILARITY'}) <br/>YIELD nodes, similarityPairs <br/>RETURN nodes, similarityPairs</span></pre><p id="a512" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="3179" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了完成这个分析，我们将使用<a class="ae lb" href="https://neo4j.com/docs/graph-data-science/current/algorithms/label-propagation/" rel="noopener ugc nofollow" target="_blank">标签传播算法</a>检查推断网络的社区结构。由于我们只对社区结构的大致轮廓感兴趣，我们可以使用算法的<code class="fe mp mq mr ms b">stats</code>模式为我们提供一些基本的社区结构统计数据。</p><pre class="ma mb mc md gt mt ms mu mv aw mw bi"><span id="7d40" class="mx ld iq ms b gy my mz l na nb">CALL gds.labelPropagation.stats({<br/>    nodeProjection:'*', <br/>    relationshipProjection:'COSINE_SIMILARITY', <br/>    maxIterations:20}) <br/>YIELD communityCount, communityDistribution<br/>RETURN communityCount,<br/>       apoc.math.round(communityDistribution.p50,2) as p50,<br/>       apoc.math.round(communityDistribution.p75,2) as p75,<br/>       apoc.math.round(communityDistribution.p90,2) as p90,<br/>       apoc.math.round(communityDistribution.p90,2) as p95,<br/>       apoc.math.round(communityDistribution.mean,2) as mean,<br/>       apoc.math.round(communityDistribution.max,2) as max</span></pre><p id="abb7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="cd8f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">标签传播算法在相似性网络中找到118个组。其中大多数成员不到40人。有一些大型社区，最大的有393名成员。</p><h1 id="529f" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结论</h1><p id="160b" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">node2vec算法是一种学习图中节点的低维表示的有用方法，可以在机器学习管道的下游使用。在这篇博文中，我意识到改变随机游走算法参数以及word2vec超参数可以产生非常不同的结果。尝试一下，看看什么最适合你。</p><p id="f721" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和往常一样，代码可以在<a class="ae lb" href="https://github.com/tomasonjo/blogs/blob/master/custom_node2vec/node2vec%20algorithm.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。</p></div></div>    
</body>
</html>