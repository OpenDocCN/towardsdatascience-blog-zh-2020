# 差分隐私的温和介绍

> 原文：<https://towardsdatascience.com/a-gentle-introduction-to-differential-privacy-330434437cdf?source=collection_archive---------31----------------------->

![](img/25a17539ab613f164f522808527bf4ca.png)

## 什么是差分隐私？为什么我们会想要它？如何才能实现？

想象一下，一位社会学家来找你，希望你回答关于各种话题的是/否问题，以了解你所在城镇的人口统计数据。你被问到的大多数问题都很平常，但是有几个问题你不愿意透露你的答案。“不要担心隐私”，研究人员向你保证，“在我公布数据之前，我会删除任何会让你的答案追溯到你的信息。这将是完全匿名的。”

作为一个有隐私意识的人，你知道严格来说这不是真的:例如，通过比较网飞电影评级和 IMDB 评级，Netflix 奖数据集成功地去匿名化，尽管去除了姓名和地址等识别特征[1]。也许你已经*公开*通过各种社交媒体发布了足够多的世俗问题的答案，以至于有人可以追踪到你的答案。毕竟学过海洋学的热爱动漫和奶酪的自由市场资本家能有几个？

你决定用一点随机性来修饰你的答案，以此来保护自己。对于每个问题，你抛硬币。如果是正面，你如实回答；如果是反面，你再掷一次来决定你将给出哪个答案。

![](img/a576f61d0f7c0cd03afd4f890d6f41ae.png)

恶作剧问题。答案是肯定的。

现在，*即使*有人计算出你是社会学家数据集中的记录 X，他们也无法知道与你相关的答案是否是真的。你也让别人更难将你的匿名回答与任何现有信息联系起来。通过以这种方式给你的数据添加噪声，你已经不知不觉地确保了你的数据在某种程度上是*本地差分隐私*。

## 定义隐私

差分隐私所追求的隐私定义是某些东西(数据集、模型、统计数据等)的结果。)不要在数据中删除或添加任何个人时更改。也就是说，不可能发现任何一个人的贡献，因为无论他们的数据是否存在，你正在寻找的数据都是相同的。

有些人可能会觉得这样的要求超出了应该被认为是“隐私”的范围:也许可以接受的是，数据集中至少一半的个人是安全的，而不是每个人。这没有什么无可争议的错误——隐私是一个道德和政策问题，而不是宇宙中的绝对常数——但差别隐私是商业、政府和研究中隐私的黄金标准。

差分隐私的数学概念(我们不会在这里表达——毕竟这是一个温和的介绍)不是一个单一的算法——你不能简单地在某些东西上“运行差分隐私”。相反，这是一个界限，如果满足，意味着您的数据在某种程度上受到保护。在实践中，事情不会触及隐私的最终定义(当某人被移除时，什么都不会改变)，但它可能会使了解个人数据变得更加困难。

使某些东西有差别地私人化“简单地”是向来自非常特定的分布的数据添加噪声的情况。困难在于确定你是否添加了正确的噪声级别，并以正确的方式应用它。对于复杂的数据集和函数来说，这可能会变得非常棘手。

## 只有几个数字

有两个数字决定保护级别，*ε*(ε)和*δ*(δ)*。*私有*机制*(你对你的数据集/模型/统计数据做的事情，使之成为有差别的私有)被表述为(ε，*δ*)-有差别的私有。选择这些数字是为了确定隐私和结果准确性之间的折衷，这对于特定情况是可接受的。

ε与泄露的私人信息的级别有关。如果你想让你的数据得到更好的保护(一个更小的ε)，你必须给你的机制增加更多的噪声。ε 越大，意味着隐私保护越少，但也意味着结果更有效，因为它们更接近真实值。

*δ* 本质上是发生灾难性错误以及某人的隐私被泄露的可能性。对于一个由 *N* 个个体组成的数据集，你希望 *δ* 至多为 1/N，这样预期的灾难数量将小于 1。

保护隐私所需的噪声量不仅取决于这两个值，还取决于数据和对数据的查询。例如，考虑一个人的年龄列表，其值为*【10】*， *20，30* 。如果你想找出平均值(20)，去掉 10 岁或 30 岁的人会使平均值改变 5。但是，如果您想知道最大值(30)，删除 30 岁将使您的查询改变 10。对于ε和 *δ* 的给定值，您必须为最大值查询添加比平均值查询更多的噪声，以说明个体贡献的相对影响。如果年龄改为[*11，12，13*，您将不得不添加更少的噪声，因为与使用[*10，20，30*时相比，查询变化更少。

混淆单个数据确实会扭曲批量数据的结果，但随着数据集的增长，单个噪声会被抵消，从总体计算的统计数据会更接近无噪声值。

## 这能有多难

在我们的第一个例子中，随机更改您的答案是局部差异隐私，因为它直接应用于数据，然后才进行任何处理。另一种公式是全局差分隐私，其中噪声被添加到作为整体对数据集进行的查询中——在上面的均值/最大值示例中，如果我们将噪声添加到人口统计中，我们将实现全局差分隐私。一般来说，对于相同的保护级别，全局差异隐私比本地差异隐私产生更准确的答案。缺点是，您必须能够信任数据收集器来正确应用全局差异隐私，并在此之前保护您的数据。建立信任是一项比差分隐私困难得多的任务，这就是为什么本地差分隐私是一个流行的解决方案。

虽然差别隐私不是一个新的概念，但它只是在过去几年才开始成为政府和企业共同关注的问题。如果你使用苹果或谷歌的产品，你发送给他们的数据可能会被保密。除了大型科技公司中的几个例子，差分隐私领域是一个很少有人涉足的领域。虽然，这正在慢慢改变:2020 年的美国人口普查将是第一个差分隐私[3]，差分隐私的开源工具正在积极开发[4]。

如前所述，差分隐私的最大困难是确保你认为是差分隐私的东西。使用不完美实现的噪声分布会使差分隐私机制变得不安全[5]，对于那些没有 Google 式资源来保证隐私的人来说，即插即用隐私的基础设施尚不存在。

当然，即使存在可证明正确的差分隐私，隐私和匿名问题仍然存在。在社会学家问卷的例子中，如果有人知道你给出的随机答案，没有什么能阻止他们假装这是事实；如果人们选择忽视，一些不同的隐私不会保护你免受负面影响。如果你在 100 份其他问卷中被问到同样的问题，你嘈杂的答案会开始向真实值靠拢，因为噪音会自己抵消掉。在某些情况下，即使仅仅知道某人在数据集中，也可能暴露私人信息。

由于这些原因，理解差异隐私并不能“解决”隐私是很重要的。隐私是一个需要立法的政治意愿、实施和促进隐私的广泛文化动力以及建设隐私基础设施的技术专长的问题。在我们的数据真正隐私化之前还有一段路要走，但是如果没有差分隐私，我们将走不了多远。

## 资源

[1][https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf](https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf)

[2][https://machine learning . apple . com/2017/12/06/learning-with privacy-at-scale . html](https://machinelearning.apple.com/2017/12/06/learning-with-privacy-at-scale.html)

[3][https://www . census . gov/about/policies/privacy/statistical _ safeguard/disclosure-avoidance-2020-census . html](https://www.census.gov/about/policies/privacy/statistical_safeguards/disclosure-avoidance-2020-census.html)

[https://www.openmined.org/](https://www.openmined.org/)

[http://citeseerx.ist.psu.edu/viewdoc/download?[5]doi = 10 . 1 . 1 . 366 . 5957&rep = re P1&type = pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.366.5957&rep=rep1&type=pdf)