# 监督学习——什么，什么时候，为什么，好与坏(第一部分)

> 原文：<https://towardsdatascience.com/supervised-learning-the-what-when-why-good-and-bad-part-1-f90e6fe2a606?source=collection_archive---------61----------------------->

## 深入回归

# 深度学习并不总是大数据问题的答案

通常在工作场所，业务利益相关者和经理会将机器学习和大数据与深度学习联系起来。他们通常认为所有数据问题的最佳解决方案是深度学习、人工智能或神经网络，或者这些技术的某种组合(插入你最喜欢的流行语)。作为一个有统计学背景的人，我发现这种观点非常令人沮丧，因为选择的机器学习算法类型应该基于五个关键因素:

## 选择机器学习算法时的 5 个关键考虑因素

1.  数据问题的类型——即监督与非监督
2.  数据集内的变量类型，即分类或数值
3.  验证统计模型的基本假设
4.  由此产生的模型精度
5.  精确度与召回率以及灵敏度与特异性之间的平衡(*用于分类*

在接下来的几个部分和几篇博文中，我将介绍不同类型的监督学习模型，并将上述考虑应用于每一个模型。

# 监督学习

很多时候，数据问题需要监督学习的应用。这是当你确切地知道你想要预测什么— *目标或因变量*，并且有一组*自变量*或*预测变量*时，你想要更好地理解它们对目标变量的影响。然后，模型的选择基于将数据中的基本模式映射到依赖于数据分布和变量类型的函数。

名称“*监督学习*”用于描述这些类型的模型，因为模型在训练集上学习基础模式。迭代/循环的次数决定了模型有机会从过去学习的次数。在随后的每一轮中，模型都会根据在之前的运行中所学到的知识，尝试提高模型的准确性(由用户选择的准确性度量)。模型在达到最大运行次数后或不再能提高模型精度时停止运行(在某些模型中由*提前停止*指定)。

有两种类型的监督学习算法，如下图所示，其中结果变量的类型决定了您是选择回归还是分类。

![](img/47d3e79ab6a8a6e6455d04f47a6d75c0.png)

图 1:监督学习模型的类型

我们先来钻研一下回归。

# 回归

人们通常认为一切都是线性回归问题，但事实并非如此。R *回归*算法有一套非常严格的假设，必须满足这些假设，才能使最终结果在统计上*可靠*(**注:我没有写统计意义上的*)。

## 为什么要回归？

回归分析是一种预测算法，常用于时间序列分析和预测。这是因为它模拟了预测变量和因变量之间的潜在关系，并确定了预测变量的值的变化如何解释结果变量的变化。

有几种类型的回归算法，但我将集中在下面的几个。

![](img/e35dad527334f419ae56676f1e9ae754.png)

图 2:回归的类型

在我进一步探索上述回归技术之前。让我们了解一下与回归技术相关的假设。

## 回归假设

1.  **回归线的残差呈正态分布—** 为确保回归模型的结果有效，残差*(观察值和预测值之间的差值)应遵循正态分布(平均值为零，标准偏差恒定)。残差也称为误差项，它们的分布可以在正态概率图上观察到(Q-Q)。如果大多数残差位于正态对角线上，那么我们可以假设它们是正态的。*

*![](img/cb5f4debbe6cea2b793c178dd4de7ed3.png)*

*图 3:用于检查正态性的密度图*

***2。数据集中的变量来自正态分布总体** —数据中的所有变量也应该来自正态分布总体。通常，在统计学中，如果样本量足够大，则可以根据中心极限定理假设正态性。在一些教科书中，它也低至 n = 30。*

***3。残差是独立的—** 检查该假设以避免残差中的*自相关*。当数据集中存在自相关时，可能会导致低估标准误差，进而使预测值看起来具有统计学意义，即使它们并不具有统计学意义。*

*   **杜宾-沃森测试*可用于检查自相关性。零假设是残差不是线性自相关的。*德宾-沃森的 d* 可以取 2 到 4 之间的任何值，但是 1.5 到 2.5 之间的 *d* 的经验法则值用于指示残差中缺乏自相关。*
*   **注意:德宾-沃森测试只能证明直接相邻者之间的线性自相关(一阶效应)。**

*![](img/65c5b36c1364c09ef2ea080910c14058.png)*

*图 4:残差的随机分布(同方差)*

*![](img/2a7a5694dd67fca9165ae271029828ef.png)*

*图 5:残差的扇形展开(异方差)*

***4。数据显示同方差—** 当残差沿最佳拟合线均匀分布而不是显示一种模式时，数据中出现同方差(残差具有恒定方差)。当存在异方差时，残差的形状可能呈现扇形(锥形)甚至线性趋势。*

*   *通过将残差(y 轴)与预测值(x 轴)绘制到散点图上，并寻找点的随机分散而不是任何聚类或模式，可以评估同方差性。异方差的统计检验是*戈德菲尔德-匡特检验*。它包括将数据集分成两组，并检查两组之间的残差方差是否相似。*

*![](img/f7c27016899a2c09250c9b0e77b555fd.png)*

*图 6:所示的正线性关系*

***5。关系是线性的** —进行线性回归(一元或多元)时，自变量和因变量之间的关系必须是线性的。从视觉上看，这可以通过在每个自变量和因变量对之间创建一个散点图来确定，并检查是否有一条可以通过这些点绘制的直线。*

*![](img/1d214991574f1b1a39bdab6e35c478f6.png)*

*图 7:异常值由实心圆表示*

*5.**没有显著的/有影响的异常值** —异常值通常是数据集中偏离平均值+/- 3 个标准偏差或超出箱线图的点。由于异常值是极值，它们会对最佳拟合线的斜率产生显著影响。*

*   *异常值可以在箱线图(位于须状物外部的点)或直方图的偏斜度中直观地识别出来。*
*   *为了确定异常值是否有*影响，*有两个统计指标可以使用: *1)库克距离和 2)马氏距离**
*   **Cook's Distance* 检查当数据集中的每个观察值被移除时，回归系数如何变化，从而检查它们的影响。数据集中的每个值都被赋予一个库克距离值。厨师的距离越大，观察的影响越大。用于确定某个值是否为异常值的一个典型试探法是检查库克的 D 值是否大于 *4/n* (其中 *n* 是数据集的大小)。*
*   **Mahanalobis 距离*测量每个观察值距离大多数数据点(或平均值)有多远(多少标准差)，通常用于多元数据集。该距离可以使用具有 *n* 个自由度的*卡方分布*来近似计算。计算数据集中每个观察值的 *p 值*。检查 p 值是否小于显著性水平(即 0.01)，以确定它们是否极端。*

*6.**自变量之间没有多重共线性***—*当自变量彼此相关时，多重共线性*出现。如果是这种情况，通常您希望回归模型中只包含变量对中的一个变量。这个假设可以用三种方法来检验。*

***不适用于简单线性回归**

*   ****方差膨胀因子(VIF)* :** 如果变量的 VIF 值大于 10，则变量是多重共线的。*

*![](img/635097facd6ec762b5e6e0c2bbf6ba39.png)*

*图 7:波士顿住房数据集的相关图*

*   ****【相关系数(r)*** :创建所有变量对之间的相关矩阵。如果相关系数大于或等于-/+ 0.7，则变量高度相关。*
*   ****容差*** *—* 该指标衡量每个自变量如何影响所有其他自变量，由等式 *T = 1 — R 平方*定义。通过对剩余的独立变量进行回归，可以计算每个独立变量的 r 平方(决定系数)。通常使用 0.1 的容差系数，当 *T < 0.1* 时，这表明数据集中存在多重共线性。*

## *回归假设概述*

*让我们通过对波士顿住房数据运行回归诊断来测试我们已经学到的一些概念。*

*![](img/35cff2d1dae9e8413d95b057d27d7012.png)*

*图 8:残差的诊断图*

1.  ***残差与拟合值—** 该图用于检查线性度。假设这条线是水平的，没有任何明显的图案，我们可以假设它是线性关系。*
2.  ***正常 Q-Q** 。该图用于检查残差的正态性。由于大部分残差都在对角线上，所以满足这个假设。*
3.  ***标度位置**(或分布位置)——该图用于检查残差的方差是否均匀(同质方差)。我们正在寻找一条水平线，在这条线的上下有平均分布的残差。在这种情况下，线有一些曲率，这意味着我们有一些异方差。*
4.  ***残差 vs 杠杆**。此图用于确定有影响的观察结果或那些有杠杆作用的观察结果。图中标记的点似乎是有影响的。通过去除这些，我们也可以解决异方差问题，因为相同的观察值在前面的图中被标记。*

*现在让我们看看常用于评估模型性能的统计数据。*

# *评估模型性能*

*有几个指标可以用来衡量回归模型的准确性。我已经描述了我在构建模型时常用的方法。*

*   ****R 平方*** :决定系数衡量自变量的变化可以解释多少结果变量的变化。更高的 *R 平方*值被视为反映了更高的模型精度。然而，当模型中有多个预测器时， *R 平方*不可信。这是因为每个预测值都会影响 R 平方值。在这些情况下，使用*调整的 R 平方*更可靠。在决定是否信任 R 平方值时，需要该领域的专业知识。例如，在化工厂等受控环境中， *R 平方*可能相当高(接近 90%);然而，当模拟人类行为时，高的 *R 平方*值可以代表*过拟合*。在这些情况下，最好使用以下任何指标来验证您的模型。*
*   ****RMSE(均方根误差):*** 计算为预测值与观测值之间方差的平方根(*残差*)。该值越低，模型拟合得越好。 *RMSE* 在某些情况下是更好的度量，因为它惩罚大的误差(残差)。*
*   ****MAE(平均绝对误差):*** 这个度量比 *RMSE* 简单，只是对残差的绝对值求和。它受异常值的影响较小。每个残差对等式中的总误差有成比例的贡献。同样，值越低越好。*

*我们终于可以深入了解各种类型的回归算法。*

# *最小二乘回归*

*线性回归，也称为最小二乘回归，使用独立变量来逼近结果变量的最佳拟合线。它是一个试图最小化平方差之和的加权方程，由以下方程表示，其中 *y* 是预测的因变量，每个 *β* 是对应于每个自变量或 *x 的系数，β₀* 是 y 截距， *e* 是误差项。*

*![](img/17889b8e9316df3298180f64a586fdd5.png)*

*最小二乘回归方程的数学表示*

*在*简单线性回归*中，只有一个系数和一个自变量。为多元线性回归中包含的每个自变量( *x)* 添加一个系数项( *β)* 。*

## *解释线性回归方程*

*让我们从 y 轴截距或常数开始， *β₀.*是所有自变量( *X)* 均为零时 *y* 的预测值。它通常也是预测的 *y.* 的平均值*

*对于连续预测变量的*系数*，其解释如下。保持所有其他变量不变， *X* ₁每增加 1 个单位， *y* 将增加(如果 *β* ₁ *为正)*或减少(如果 *β* ₁ *为负)*平均增加 *β* ₁ *个单位*。*

*对于线性回归，分类自变量编码为 0 或 1(虚拟变量)。因此，当解释它们的系数时，即 *y 的预测值的差异，*一个单位的差异表示从一个类别切换到另一个类别，同时保持所有其他变量不变。*

# *多项式回归*

*当自变量和因变量之间的关系本质上是非线性的并且包括曲率时，多项式回归可以被认为是一种选择。*

*![](img/91bf1acafc7606afcf4ad1e875564d38.png)*

*图 6:多项式回归示例*

*在左边的图中，我们可以看到多项式回归(到 2 的数量级)比直线拟合略好。*

*每个独立变量的多项式回归方程如下所示。对于变量被提升到的每一阶，在等式中为变量添加一个附加项。*

*继续将方程提升到更高次的多项式以获得最佳拟合可能是诱人的；然而，这会导致**过度配合**。*过度拟合*发生在试图解释尽可能多的点时，包括数据集(即训练集)中的随机误差，使得难以推广到其他数据集(即测试集)。*

*![](img/56414146e326258d8cd734f87c0affb7.png)*

*多项式回归方程的数学表示*

# *逐步回归*

*逐步回归就像它的名字一样，涉及在线性模型中添加(从模型中没有独立变量开始的正向选择)或移除(从完整模型或模型中的所有独立变量开始的反向选择)独立变量，以基于模型评估度量找到因变量的最佳预测子集。*

**前向选择*在模型评估度量不再有改进时停止(即 R 平方停止增加)，而 b *后向选择*在最无影响的(即那些对 R 平方有贡献的)独立变量被移除后停止。*

# *R 的回归*

## *线性回归*

*坚持波士顿住房数据集，我对所有预测变量对中值房价进行了线性回归(“lm”)。请注意，在运行模型之前，我没有“处理”多重共线性、正态性和异常值问题。*

```
*> linearMod <- lm(medv ~ ., data=Boston) 
> summary(linearMod)Call:
lm(formula = medv ~ ., data = Boston)Residuals:
    Min      1Q  Median      3Q     Max 
-15.595  -2.730  -0.518   1.777  26.199Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** 
zn           4.642e-02  1.373e-02   3.382 0.000778 ***
indus        2.056e-02  6.150e-02   0.334 0.738288    
chas         2.687e+00  8.616e-01   3.118 0.001925 ** 
nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
rm           3.810e+00  4.179e-01   9.116  < 2e-16 ***
age          6.922e-04  1.321e-02   0.052 0.958229    
dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
black        9.312e-03  2.686e-03   3.467 0.000573 ***
lstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 4.745 on 492 degrees of freedom
Multiple R-squared:  0.7406, Adjusted R-squared:  0.7338 
F-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16*
```

*上面我们可以看到*调整后的 R 平方值为 73.38%* ，也就是说房价中位数的这个变化量可以用模型中的预测变量来解释。这是一个非常好的 R 平方值，并且 *p 值< 0.05* 表示统计显著性。*

*接下来，我将确定哪些变量对这个 R 平方统计有贡献。我可以看到，除了*印度河*和*年龄*都是非显著贡献者(p 值> 0.05)。*

*让我们看看如果我运行逐步回归会发生什么。*

## *逐步回归*

***向后***

*让我们首先尝试解释对模型的调用。*

*   *我的因变量(medv)已经针对所有自变量进行了回归。)*
*   *我使用了逆向回归*
*   *该模型将被调整以尝试由 1 到 13 个变量组成的模型。*

```
*# Set seed for reproducibility
> set.seed(123)
> # Set up repeated k-fold cross-validation (10 fold)
> train.control <- trainControl(method = "cv", number = 10)
> # Train the model
> step.model <- train(medv ~ ., data=Boston,
+                     method = "leapBackward", 
+                     tuneGrid = data.frame(nvmax = 1:13),
+                     trControl = train.control
+ )
> step.model$results
   nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD
1      1 6.190699 0.5542196 4.512277 0.6480792 0.03585104 0.5143497
2      2 5.537331 0.6393649 3.975099 0.6194193 0.08470812 0.4184383
3      3 5.216943 0.6757786 3.675974 0.7844579 0.09606216 0.3282287
4      4 5.144946 0.6843838 3.621869 0.6838151 0.08435306 0.3033078
5      5 4.998163 0.7010740 3.543459 0.7416779 0.08756225 0.2999431
6      6 5.048162 0.6953426 3.520865 0.7245789 0.08673190 0.2717374
7      7 5.049469 0.6952020 3.500608 0.6987592 0.08276566 0.2597508
8      8 5.008368 0.7004709 3.497253 0.6740135 0.08021108 0.2756674
9      9 5.012961 0.7018661 3.490065 0.6963818 0.08564270 0.2683468
10    10 4.947244 0.7098667 3.447318 0.6896883 0.08705085 0.2222943
**11    11 4.809318 0.7249622 3.359429 0.6861143 0.08341394 0.2409937**
12    12 4.825967 0.7231964 3.365846 0.6808653 0.08309689 0.2442360
13    13 4.829796 0.7227732 3.368084 0.6796633 0.08336179 0.2455718
> step.model$bestTune
   nvmax
11    11> summary(step.model$finalModel)
Subset selection object
13 Variables  (and intercept)
        Forced in Forced out
crim        FALSE      FALSE
zn          FALSE      FALSE
indus       FALSE      FALSE
chas        FALSE      FALSE
nox         FALSE      FALSE
rm          FALSE      FALSE
age         FALSE      FALSE
dis         FALSE      FALSE
rad         FALSE      FALSE
tax         FALSE      FALSE
ptratio     FALSE      FALSE
black       FALSE      FALSE
lstat       FALSE      FALSE
1 subsets of each size up to 11
Selection Algorithm: backward
          crim zn  indus chas nox rm  age dis rad tax ptratio black lstat
1  ( 1 )  " "  " " " "   " "  " " " " " " " " " " " " " "     " "   "*"  
2  ( 1 )  " "  " " " "   " "  " " "*" " " " " " " " " " "     " "   "*"  
3  ( 1 )  " "  " " " "   " "  " " "*" " " " " " " " " "*"     " "   "*"  
4  ( 1 )  " "  " " " "   " "  " " "*" " " "*" " " " " "*"     " "   "*"  
5  ( 1 )  " "  " " " "   " "  "*" "*" " " "*" " " " " "*"     " "   "*"  
6  ( 1 )  " "  " " " "   " "  "*" "*" " " "*" " " " " "*"     "*"   "*"  
7  ( 1 )  " "  " " " "   " "  "*" "*" " " "*" "*" " " "*"     "*"   "*"  
8  ( 1 )  "*"  " " " "   " "  "*" "*" " " "*" "*" " " "*"     "*"   "*"  
9  ( 1 )  "*"  " " " "   " "  "*" "*" " " "*" "*" "*" "*"     "*"   "*"  
10  ( 1 ) "*"  "*" " "   " "  "*" "*" " " "*" "*" "*" "*"     "*"   "*"  
11  ( 1 ) "*"  "*" " "   "*"  "*" "*" " " "*" "*" "*" "*"     "*"   "*"*
```

*在上面的例子中，我用 10 倍交叉验证和 13 个最大变量进行了逐步回归。我没有全部使用它们，因为我们知道在之前运行全线性模型时，有两个并不重要。我们可以看到，最佳模型由 11 个变量组成(最低的 RMSE、最低的 MAE 和最高的 R 平方)。*

*星号(*)表示变量包含在相应的模型中。最好的 11 变量模型包括除了印度河流域和年龄以外的所有变量，就像完全线性模型一样。*

*向前逐步回归给出了与完全线性模型和向后回归相似的结果。*

```
*> set.seed(123)
> train.control <- trainControl(method = "cv", number = 10)
> step.model <- train(medv ~ ., data=Boston,
+                     method = "leapForward", 
+                     tuneGrid = data.frame(nvmax = 1:13),
+                     trControl = train.control
+ )
> step.model$results
   nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD
1      1 6.190699 0.5542196 4.512277 0.6480792 0.03585104 0.5143497
2      2 5.537331 0.6393649 3.975099 0.6194193 0.08470812 0.4184383
3      3 5.216943 0.6757786 3.675974 0.7844579 0.09606216 0.3282287
4      4 5.243481 0.6736566 3.658219 0.7510340 0.08224804 0.3304744
5      5 5.098698 0.6904107 3.568604 0.7354806 0.08392928 0.2726078
6      6 5.045755 0.6953140 3.512300 0.7223225 0.08606320 0.2787373
7      7 4.969144 0.7036333 3.448631 0.7542104 0.08842114 0.2944921
8      8 4.985899 0.7027153 3.456932 0.7099500 0.08432911 0.2895184
9      9 4.985690 0.7030156 3.466382 0.6879376 0.08295944 0.2782166
10    10 4.912877 0.7137519 3.421565 0.6865803 0.07813425 0.2716994
11    11 4.809318 0.7249622 3.359429 0.6861143 0.08341394 0.2409937
12    12 4.825967 0.7231964 3.365846 0.6808653 0.08309689 0.2442360
13    13 4.829796 0.7227732 3.368084 0.6796633 0.08336179 0.2455718
> step.model$bestTune
   nvmax
11    11*
```

# *正规化*

*正则化是一种常用于解决回归中可能出现的过度拟合和多重共线性问题的技术。这是通过在目标函数中增加一个惩罚项来实现的，目标函数的目的是减少标准误差。*

## *里脊回归*

*在岭回归中，增加了一个惩罚项(收缩参数)λ，以减少 *β* 系数项中的较大变化。 *L2 正则化*用于骑行回归，目标是通过试图使系数更接近零来最小化系数平方的 s **um。***

*岭回归适用于包含大量变量的小型数据集，具有小到中等的效果。*

## *套索回归*

*Lasso(最小绝对收缩和选择算子)回归使用 *L1 正则化*，其中λ被添加到**系数的绝对平方和。***

*Lasso 回归适用于变量较少但对因变量有中到大影响的数据集。*

## *弹性网络回归*

*ElasticNet 结合了套索和岭回归技术，其中应用了 L1 和 L2 正则化。*

## *选择正则化参数*

*在决定使用哪个正则化参数时，我们可以基于两种方法:1)选择λ，其中信息标准最小化(如 AIC 或 BIC)，侧重于模型拟合；2)运行交叉验证模型，并选择λ值，使交叉验证残差最小化，侧重于预测精度。*

# *回归的利与弊*

***优点***

*   *易于理解并向利益相关者展示*
*   *可用于解释——即每个预测因素对结果变量的相对影响*

***缺点***

*   *将所有独立变量标准化(定标和居中)以避免多重共线性非常重要*
*   *需要检查严格的模型假设*

*这就是我关于回归的全部内容。我的下一篇博文将是关于分类算法的。*