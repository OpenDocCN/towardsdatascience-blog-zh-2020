<html>
<head>
<title>Text Classifier with Multiple Outputs and Multiple Losses in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras中多输出多损失的文本分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classifier-with-multiple-outputs-and-multiple-losses-in-keras-4b7a527eb858?source=collection_archive---------15-----------------------#2020-05-10">https://towardsdatascience.com/text-classifier-with-multiple-outputs-and-multiple-losses-in-keras-4b7a527eb858?source=collection_archive---------15-----------------------#2020-05-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4bd7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Keras构建多标签分类器似乎不是一项困难的任务，但当你处理一个高度<strong class="ak">不平衡的数据集</strong>时，它有30多个不同的标签和多个丢失，这可能会变得非常棘手。</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="5fee" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在本帖中，我们将详细介绍多标签分类器的定义、多重损失、文本预处理，并逐步解释如何在Keras中构建多输出RNN-LSTM。</p><p id="a228" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们将要处理的数据集由自然灾害信息组成，这些信息被分为36个不同的类别。数据集由<a class="ae ll" href="https://appen.com/" rel="noopener ugc nofollow" target="_blank">图八</a>提供。输入消息示例:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="f648" class="lv lw it lr b gy lx ly l lz ma">['Weather update - a cold front from Cuba that could pass over Haiti',<br/> 'Is the Hurricane over or is it not over',<br/> 'Looking for someone but no name',<br/> 'UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately.',<br/> 'says: west side of Haiti, rest of the country today and tonight']</span></pre><h1 id="25fb" class="mb lw it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">什么是多标签分类问题？</h1><p id="8f1c" class="pw-post-body-paragraph kp kq it kr b ks ms ju ku kv mt jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">在解释它是什么之前，让我们先来看一个更常见的分类类型的定义:Multiclass。在一个多类中，类是互斥的，也就是说，你一次只能分类一个类。例如，如果您有几个类:{Car，Person，Motorcycle}，那么您的模型必须输出:Car或Person或Motorcycle。对于这类问题，使用Softmax函数进行分类:</p><figure class="lm ln lo lp gt my gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/03f99135ffe012bce801d6b016ad7b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*nhTFTmcnEaRM99c3UlfqQQ.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">神经网络中的Softmax分类函数</p></figure><p id="1867" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">对于多标签分类，一个数据样本可以属于多个类别。从上面的例子中，您的模型可以对同一个样本进行分类:Car和Person(假设每个样本是一个可能包含这3个类的图像)。</p><p id="2214" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在所研究的数据集中，有36个不同的类，其中35个有一个<strong class="kr iu">二进制输出</strong> : 0或1；并且其中1个具有3个可能的类别<strong class="kr iu">(多类别情况):</strong> 0、1或2。</p><h1 id="4f10" class="mb lw it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">多重损失</h1><p id="6046" class="pw-post-body-paragraph kp kq it kr b ks ms ju ku kv mt jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">在同一个模型中使用多个损失函数意味着您正在执行不同的任务，并在这些任务之间共享模型的一部分。有时，你可能会想，也许为每种不同类型的输出建立不同的模型更好，但在某些情况下，共享你的神经网络的某些层有助于模型更好地概括。</p><p id="b86e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">Keras如何处理多次亏损？</p><p id="096e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">从<a class="ae ll" href="https://keras.io/models/model/" rel="noopener ugc nofollow" target="_blank"> Keras文档</a>、<em class="nf">“…模型将最小化的损失值将是所有单个损失的加权和，由</em> <code class="fe ng nh ni lr b"><em class="nf">loss_weights</em></code> <em class="nf">系数加权。</em>”。因此，最终损失是每个损失的加权和，传递给<code class="fe ng nh ni lr b"><em class="nf">loss</em></code>参数。</p><p id="6c60" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在所研究的案例中，将使用两种不同的损失:</p><ul class=""><li id="0af3" class="nj nk it kr b ks kt kv kw ky nl lc nm lg nn lk no np nq nr bi translated">对于<strong class="kr iu">二进制类，</strong>使用的度量将是具有相应<code class="fe ng nh ni lr b">binary_crossentropy</code>损失的<code class="fe ng nh ni lr b">binary_accuracy</code>。由于每个输出只有两个可能的类别(0或1)，因此<code class="fe ng nh ni lr b">sigmoid</code>功能将被用作激活功能。</li><li id="cb1b" class="nj nk it kr b ks ns kv nt ky nu lc nv lg nw lk no np nq nr bi translated">对于<strong class="kr iu">多类</strong>输出<strong class="kr iu">，</strong>，使用的度量将是具有相应<code class="fe ng nh ni lr b">sparse_categorical_crossentropy</code>损耗的<code class="fe ng nh ni lr b">sparse_categorical_accuracy</code>。对于该输出，有3个可能的类别:0、1和2，这样将使用<code class="fe ng nh ni lr b">softmax</code>激活功能。与传统的<code class="fe ng nh ni lr b">categorical_crossentropy</code>损耗不同，第一种损耗不需要对输出<strong class="kr iu"> Y </strong>进行一键编码。因此，不用将输出转换为:[1，0，0]，[0，1，0]和[0，0，1]，我们可以将其保留为整数:[0]，[1]和[2]。需要强调的是，这两种损失属于同一个等式:</li></ul><figure class="lm ln lo lp gt my gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/98bb4cb83f667fcd3a44fada0dd271d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*ZdTlTgks_NBes76-PUh6wQ.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">类别交叉熵和稀疏类别交叉熵的损失函数</p></figure><h1 id="3ce2" class="mb lw it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">文本预处理</h1><p id="3e50" class="pw-post-body-paragraph kp kq it kr b ks ms ju ku kv mt jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">就像任何其他NLP问题一样，在将文本输入数据应用到模型中之前，我们必须对其进行预处理。在这个数据集中，标点符号，网址链接和“@”被删除。尽管“@”提到给消息增加了一些信息，但它没有给分类模型增加价值。标签(' #') 可能包含有用的信息，因为它们通常与事件相关。因此，它们被保留下来，只删除了“#”字符。</p><p id="c766" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><em class="nf">停用词</em>(一种语言中最常见的词)移除和词条化也应用于数据集。</p><figure class="lm ln lo lp gt my"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="7f70" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">文本样本被格式化为张量，可以使用Keras实用程序将其输入神经网络:</p><figure class="lm ln lo lp gt my"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="ac60" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">上述过程主要包括3个步骤:</p><ul class=""><li id="30e5" class="nj nk it kr b ks kt kv kw ky nl lc nm lg nn lk no np nq nr bi translated">首先，将记号赋予器实例(T7)适配到语料库，基于词频创建词汇索引。每个单词都映射到一个索引，所以每个单词都有一个唯一的整数值，整数越小，单词出现的频率越高。要保留的单词大小由<code class="fe ng nh ni lr b">num_words</code>参数定义，即词汇量。只会保留最常用的词。在我们的数据集中，单词映射如下:</li></ul><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="44c8" class="lv lw it lr b gy lx ly l lz ma">print(tokenizer.word_index)</span><span id="ce85" class="lv lw it lr b gy oa ly l lz ma">{'water': 1, 'people': 2, 'food': 3, 'need': 4, 'help': 5, 'please': 6, 'earthquake': 7, 'would': 8, 'area': 9, 'like': 10, 'said': 11, 'country': 12,...}</span></pre><ul class=""><li id="5120" class="nj nk it kr b ks kt kv kw ky nl lc nm lg nn lk no np nq nr bi translated">然后使用<code class="fe ng nh ni lr b">tokenizer.texts_to_sequences</code>方法将输入的句子映射成整数。从我们的例子来看:</li></ul><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="7135" class="lv lw it lr b gy lx ly l lz ma">'weather update cold front cuba could pa haiti'</span></pre><p id="1842" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">映射到:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="e381" class="lv lw it lr b gy lx ly l lz ma">[138, 1480, 335, 863, 2709, 80, 411, 18]</span></pre><ul class=""><li id="a863" class="nj nk it kr b ks kt kv kw ky nl lc nm lg nn lk no np nq nr bi translated">最后，为了创建嵌入，我们所有的句子需要长度相同。因此，我们使用<code class="fe ng nh ni lr b">pad_sequences</code>给每个句子填充零。</li></ul><h1 id="cf07" class="mb lw it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">多输出多损耗RNN</h1><p id="8f58" class="pw-post-body-paragraph kp kq it kr b ks ms ju ku kv mt jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">为了构建这个模型，我们将使用<a class="ae ll" href="https://keras.io/getting-started/functional-api-guide/" rel="noopener ugc nofollow" target="_blank"> Keras functional API </a>，而不是<a class="ae ll" href="https://keras.io/models/sequential/" rel="noopener ugc nofollow" target="_blank"> Sequential API </a>，因为前者允许我们构建更复杂的模型，例如多输出和输入问题。</p><p id="c72b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了总结我们到目前为止的成果:</p><ul class=""><li id="75ac" class="nj nk it kr b ks kt kv kw ky nl lc nm lg nn lk no np nq nr bi translated">每个输入样本是MAXLEN (50)大小的整数向量</li><li id="b0ba" class="nj nk it kr b ks ns kv nt ky nu lc nv lg nw lk no np nq nr bi translated">每个样本将被分为36个不同的类别，其中35个类别具有<strong class="kr iu">二进制输出</strong> : 0或1；并且其中1个具有<strong class="kr iu"> 3个可能的类别(多类别情况):</strong> 0、1或2。</li></ul><h2 id="1691" class="lv lw it bd mc ob oc dn mg od oe dp mk ky of og mm lc oh oi mo lg oj ok mq ol bi translated">模型架构</h2><p id="f453" class="pw-post-body-paragraph kp kq it kr b ks ms ju ku kv mt jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">在本节中，我们将使用Keras嵌入层来训练我们自己的嵌入。</p><figure class="lm ln lo lp gt my"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="beee" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">嵌入层将以下内容作为输入:</p><ul class=""><li id="d712" class="nj nk it kr b ks kt kv kw ky nl lc nm lg nn lk no np nq nr bi translated"><strong class="kr iu"> input_dim </strong>:我们选择的词汇量</li><li id="082d" class="nj nk it kr b ks ns kv nt ky nu lc nv lg nw lk no np nq nr bi translated"><strong class="kr iu"> output_dim </strong>:嵌入的大小。在我们的例子中，它被设置为50d。</li><li id="ab92" class="nj nk it kr b ks ns kv nt ky nu lc nv lg nw lk no np nq nr bi translated"><strong class="kr iu">输入长度</strong>:输入序列的长度。在我们的例子中:MAXLEN</li></ul><p id="33c1" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了加快训练时间，在LSTM之前增加了卷积层。CNN更有可能从句子中提取局部和深层特征。你可以在这篇文章中读到更多关于CNN和RNN组合的内容。</p><h2 id="4906" class="lv lw it bd mc ob oc dn mg od oe dp mk ky of og mm lc oh oi mo lg oj ok mq ol bi translated">输出层</h2><p id="f318" class="pw-post-body-paragraph kp kq it kr b ks ms ju ku kv mt jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">大多数输出类都是二进制的，但其中一个是多类输出。如“多重损耗”部分所述，所用损耗为:<code class="fe ng nh ni lr b">binary_crossentropy</code>和<code class="fe ng nh ni lr b">sparse_categorical_crossentropy</code>。</p><p id="7e36" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">由于数据集高度不平衡，添加了<code class="fe ng nh ni lr b">class_weight</code>参数以减少不平衡分布。将为每个输出创建一个<strong class="kr iu">密集层</strong>。输出将存储在一个数组中，而每个输出的度量和损失将存储在相应的字典中。</p><figure class="lm ln lo lp gt my"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="d6f2" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">上面的代码遍历每个输出二进制列并创建一个密集层，将相应的度量和损失保存到一个字典中。</p><p id="8f3e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">下面的代码对单个多类输出列应用相同的过程。</p><figure class="lm ln lo lp gt my"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="6ef5" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">对于每个输出，我们以字典格式定义每个类的权重:</p><figure class="lm ln lo lp gt my"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="612e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">然后，我们可以实例化一个模型类并训练我们的模型:</p><figure class="lm ln lo lp gt my"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="73b6" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">该模型遵循以下格式:</p><figure class="lm ln lo lp gt my gh gi paragraph-image"><div role="button" tabindex="0" class="on oo di op bf oq"><div class="gh gi om"><img src="../Images/91b0fcdce26d629a41c8c7a86d258588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SKBxTMtZxADXTEq1yKtLDQ.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">RNN的图形表示</p></figure><p id="97b1" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">显然，对于多标签输出，Keras有一个<a class="ae ll" href="https://github.com/keras-team/keras/issues/8011" rel="noopener ugc nofollow" target="_blank">未解决的问题</a>与<code class="fe ng nh ni lr b">class_weights</code>和<code class="fe ng nh ni lr b">binary_crossentropy</code>。上面提出的解决方案(为每个输出添加一个密集图层)是一个有效的解决方案。</p><h1 id="af36" class="mb lw it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">结论</h1><p id="621b" class="pw-post-body-paragraph kp kq it kr b ks ms ju ku kv mt jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">在这篇文章中，我们使用Keras functional API构建了一个RNN文本分类器，具有多个输出和丢失。我们浏览了关于多重损失的解释，以及多标签和多类别分类问题之间的区别。</p><p id="43d8" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">你可以在<a class="ae ll" href="https://github.com/DanielDaCosta/RNN-Keras" rel="noopener ugc nofollow" target="_blank">这里</a>查看完整的代码和额外的分析！在这篇文章中，我们训练了我们自己的嵌入，在GitHub Repo中，你可以检查使用<a class="ae ll" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">预训练手套向量</a>重新训练的相同模型。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="73d4" class="mb lw it bd mc md or mf mg mh os mj mk jz ot ka mm kc ou kd mo kf ov kg mq mr bi translated">参考</h1><div class="ow ox gp gr oy oz"><a href="https://medium.com/towards-artificial-intelligence/keras-for-multi-label-text-classification-86d194311d0e" rel="noopener follow" target="_blank"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd iu gy z fp pe fr fs pf fu fw is bi translated">多标签文本分类的Keras</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">使用Keras进行多标签文本分类的CNN和LSTMs体系结构</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">medium.com</p></div></div><div class="pi l"><div class="pj l pk pl pm pi pn mz oz"/></div></div></a></div><div class="ow ox gp gr oy oz"><a href="https://stats.stackexchange.com/questions/323961/how-to-define-multiple-losses-in-machine-learning" rel="noopener  ugc nofollow" target="_blank"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd iu gy z fp pe fr fs pf fu fw is bi translated">机器学习中如何定义多重损失？</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">begingroup$使用两个损失意味着您对优化这两个损失都感兴趣。它可能来自于你是…</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">stats.stackexchange.com</p></div></div><div class="pi l"><div class="po l pk pl pm pi pn mz oz"/></div></div></a></div><div class="ow ox gp gr oy oz"><a href="https://stackoverflow.com/questions/49404309/how-does-keras-handle-multiple-losses" rel="noopener  ugc nofollow" target="_blank"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd iu gy z fp pe fr fs pf fu fw is bi translated">keras如何处理多次亏损？</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">来自模型文档:loss: String(目标函数的名称)或目标函数。参见损失。如果模型有…</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">stackoverflow.com</p></div></div><div class="pi l"><div class="pp l pk pl pm pi pn mz oz"/></div></div></a></div><div class="ow ox gp gr oy oz"><a href="https://cwiki.apache.org/confluence/display/MXNET/Multi-hot+Sparse+Categorical+Cross-entropy" rel="noopener  ugc nofollow" target="_blank"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd iu gy z fp pe fr fs pf fu fw is bi translated">多热点稀疏分类交叉熵</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">在进行多类分类时，会大量使用类别交叉熵损失。它比较预测的标签…</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">cwiki.apache.org</p></div></div></div></a></div><div class="ow ox gp gr oy oz"><a href="https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer" rel="noopener  ugc nofollow" target="_blank"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd iu gy z fp pe fr fs pf fu fw is bi translated">Keras嵌入层的详细说明</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">使用Kaggle笔记本探索和运行机器学习代码|使用来自多个数据源的数据</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">www.kaggle.com</p></div></div><div class="pi l"><div class="pq l pk pl pm pi pn mz oz"/></div></div></a></div></div></div>    
</body>
</html>