<html>
<head>
<title>(Reinforce)Policy Gradient with TensorFlow2.x</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 TensorFlow2.x(加强)策略梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforce-policy-gradient-with-tensorflow2-x-be1dea695f24?source=collection_archive---------14-----------------------#2020-07-23">https://towardsdatascience.com/reinforce-policy-gradient-with-tensorflow2-x-be1dea695f24?source=collection_archive---------14-----------------------#2020-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6ef3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用 TensorFlow 2.x 实施增强(PG)</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/2b6089b9bc3b12f2bc7670557c9936ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*0K_rAJ7gQ-tZue25ApNy0A.gif"/></div></figure><p id="9c64" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在本文中，我们将尝试理解称为加强的策略梯度算法背后的概念。然后，我们将查看 TensorFlow 2.x 中算法的代码。让我们首先了解什么是策略梯度，然后我们将了解一种特定的策略梯度方法，也称为增强。</p><h2 id="d77c" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kw ls lt lu la lv lw lx le ly lz ma mb bi translated">什么/为什么是政策梯度？</h2><p id="7e9b" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">到目前为止，我们一直在学习状态-动作对的 Q 值。策略梯度算法不是先学习 Q 值再找一个策略，而是直接学习一个策略。但是为什么呢？让我们找出答案</p><ol class=""><li id="9bd4" class="mh mi iq kp b kq kr kt ku kw mj la mk le ml li mm mn mo mp bi translated">学习策略导致更好的收敛，同时遵循梯度。而基于价值的方法在行动选择上可能会有很大的变化，即使在价值评估上有很小的变化。</li><li id="29b5" class="mh mi iq kp b kq mq kt mr kw ms la mt le mu li mm mn mo mp bi translated">这些算法对于像自动飞行无人机或自动驾驶汽车这样的大量动作非常有用。</li><li id="6a78" class="mh mi iq kp b kq mq kt mr kw ms la mt le mu li mm mn mo mp bi translated">可以学习随机政策。</li></ol></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="45e4" class="nc lk iq bd ll nd ne nf lo ng nh ni lr jw nj jx lu jz nk ka lx kc nl kd ma nm bi translated">算法和实现</h1><p id="42b7" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">加强是一种蒙特卡罗策略梯度方法，它在每一集之后进行更新。我们的神经网络将当前状态作为输入，输出所有动作的概率。我们可以通过调整代理网络的权重来优化我们的策略，以便在某个状态下选择更好的行动。通过对我们的目标函数执行梯度上升来调整这些权重。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/0da8448049e044d579747dd7065be3b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5nFL3yqr91kzl5EqgBIzcg.png"/></div></div></figure><p id="82d5" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">让我们看看代码。</p><p id="d910" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">神经网络:</p><p id="adf0" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们的模型实现如下。<strong class="kp ir">请</strong> <strong class="kp ir">注意</strong>最后一层包含<strong class="kp ir"> softmax </strong> <strong class="kp ir">激活</strong>，输出每个动作的概率。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="592b" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">动作选择:</p><ol class=""><li id="93c0" class="mh mi iq kp b kq kr kt ku kw mj la mk le ml li mm mn mo mp bi translated">通过输入状态数组获得预测概率。</li><li id="aaf2" class="mh mi iq kp b kq mq kt mr kw ms la mt le mu li mm mn mo mp bi translated">我们利用 TensorFlow 概率库将概率转化为分布。</li><li id="bc9b" class="mh mi iq kp b kq mq kt mr kw ms la mt le mu li mm mn mo mp bi translated">然后我们从分布中抽取行动样本。</li><li id="12db" class="mh mi iq kp b kq mq kt mr kw ms la mt le mu li mm mn mo mp bi translated">然后我们作为一个整体回到行动。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="354e" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了更好地理解，让我们先来看看主要的训练。</p><p id="afa8" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">主训练循环:</p><ol class=""><li id="4403" class="mh mi iq kp b kq kr kt ku kw mj la mk le ml li mm mn mo mp bi translated">我们维护了三个记录状态、奖励和行动的列表。</li><li id="00b6" class="mh mi iq kp b kq mq kt mr kw ms la mt le mu li mm mn mo mp bi translated">每集之后都要训练模型。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="03b0" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">学习功能:</p><ol class=""><li id="83c3" class="mh mi iq kp b kq kr kt ku kw mj la mk le ml li mm mn mo mp bi translated">该函数将状态、动作和奖励列表作为参数。</li><li id="4391" class="mh mi iq kp b kq mq kt mr kw ms la mt le mu li mm mn mo mp bi translated">我们将从奖励列表的最后一个元素开始循环。</li><li id="6ca8" class="mh mi iq kp b kq mq kt mr kw ms la mt le mu li mm mn mo mp bi translated">然后，我们将计算每个州的预期累积奖励，就像我们在表格蒙特卡罗方法中所做的那样。</li><li id="3ea1" class="mh mi iq kp b kq mq kt mr kw ms la mt le mu li mm mn mo mp bi translated">然后，我们计算损失的梯度和应用优化。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="faa3" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">损失函数:</p><ol class=""><li id="b310" class="mh mi iq kp b kq kr kt ku kw mj la mk le ml li mm mn mo mp bi translated">损失是所选行动的对数概率乘以该州的折现回报的负数。</li><li id="7694" class="mh mi iq kp b kq mq kt mr kw ms la mt le mu li mm mn mo mp bi translated">请注意，这里的负号意味着我们正在执行梯度上升。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="f1b0" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">代码就这么多了，你可以在这里找到完整代码<a class="ae nu" href="https://github.com/abhisheksuran/Atari_DQN/blob/master/Reinforce_(PG).ipynb" rel="noopener ugc nofollow" target="_blank">。让我们讨论一下政策梯度的一些弊端。</a></p><h2 id="8ce4" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kw ls lt lu la lv lw lx le ly lz ma mb bi translated">缺点:</h2><ol class=""><li id="b588" class="mh mi iq kp b kq mc kt md kw nv la nw le nx li mm mn mo mp bi translated">很多时候，政策梯度收敛于局部极大值。</li><li id="31cd" class="mh mi iq kp b kq mq kt mr kw ms la mt le mu li mm mn mo mp bi translated">政策梯度法需要很长时间的训练。</li></ol><p id="c695" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">所以，本文到此结束。谢谢你的阅读，希望你喜欢并且能够理解我想要解释的东西。希望你阅读我即将发表的文章。哈里奥姆…🙏</p><h1 id="6660" class="nc lk iq bd ll nd ny nf lo ng nz ni lr jw oa jx lu jz ob ka lx kc oc kd ma nm bi translated">参考资料:</h1><div class="od oe gp gr of og"><a href="https://www.coursera.org/specializations/reinforcement-learning" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd ir gy z fp ol fr fs om fu fw ip bi translated">强化学习</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">由阿尔伯塔大学提供。强化学习专业化包括 4 门课程探索权力…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">www.coursera.org</p></div></div><div class="op l"><div class="oq l or os ot op ou kl og"/></div></div></a></div><div class="od oe gp gr of og"><a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd ir gy z fp ol fr fs om fu fw ip bi translated">强化学习，第二版</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">显着扩大和更新的广泛使用的文本强化学习的新版本，最…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">mitpress.mit.edu</p></div></div><div class="op l"><div class="ov l or os ot op ou kl og"/></div></div></a></div><div class="od oe gp gr of og"><a href="https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd ir gy z fp ol fr fs om fu fw ip bi translated">关于政策梯度的介绍</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">托马斯·西蒙尼尼对政策梯度的介绍</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">www.freecodecamp.org</p></div></div><div class="op l"><div class="ow l or os ot op ou kl og"/></div></div></a></div><div class="od oe gp gr of og"><a href="https://adventuresinmachinelearning.com" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd ir gy z fp ol fr fs om fu fw ip bi translated">机器学习的冒险——学习和探索机器学习</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">由 admin |深度学习、策略梯度、强化学习、TensorFlow 2.0 在最近的一系列帖子中，我…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">adventuresinmachinelearning.com</p></div></div><div class="op l"><div class="ox l or os ot op ou kl og"/></div></div></a></div></div></div>    
</body>
</html>