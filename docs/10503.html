<html>
<head>
<title>Teaching a computer how to play Snake with Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Q-Learning 教电脑玩蛇</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/teaching-a-computer-how-to-play-snake-with-q-learning-93d0a316ddc0?source=collection_archive---------23-----------------------#2020-07-23">https://towardsdatascience.com/teaching-a-computer-how-to-play-snake-with-q-learning-93d0a316ddc0?source=collection_archive---------23-----------------------#2020-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="acd0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用一个蛇的游戏探索强化学习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f1020f20cde42395a44f5b8f481ab28e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IfvtvorZyb3jmjC6"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kv" href="https://unsplash.com/@franckinjapan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Franck V. </a>拍摄的照片</p></figure><p id="ee1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我最近看了《T4》AlphaGo——电影，这是一部关于 DeepMind 的 alpha go 的纪录片。AlphaGo 是一个玩围棋的人工智能，纪录片详细描述了它与 Lee Sedol 比赛前的故事。当 IBM 的“深蓝”在 1997 年击败国际象棋大师加里·卡斯帕罗夫时，全世界的围棋选手都认为这在围棋上是不可能的。20 年后，谷歌的人工智能击败了世界上有史以来最好的围棋选手之一。据信，它采用的一些策略非常有创意，围棋专家正在研究这些策略。我发现这一切都很迷人。</p><p id="1e1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然我无法构建这样的人工智能，但我想探索强化学习。我决定了贪吃蛇这个游戏(一个简单多了的游戏！)而且没用多长时间就得到一些相当不错的结果。我的代码通过<a class="ae kv" href="https://github.com/jl4r1991/SnakeQlearning" rel="noopener ugc nofollow" target="_blank"> Github </a>完整共享。</p><h1 id="d643" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是 Q-Learning？</h1><p id="3226" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">质量学习，或 Q 学习，类似于训练一只狗。我们第一次把我的狗带回家时，它还是一只小狗。她不知道任何技巧。她不知道不要咬我们的鞋子。最重要的是，她没受过如厕训练。但是她喜欢零食。这给了我们一个激励她的方法。每次她按命令坐下或摇摇她的爪子，我们就给她一份礼物。如果她咬了我们的鞋子…嗯，真的没什么，她只是没有得到奖励。然而，随着时间的推移，当她需要出去上厕所时，她甚至学会了踩我们的脚。</p><p id="43f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Q-learning 是一种强化学习方法，它通过奖励良好行为和惩罚不良行为来教会学习代理如何执行任务。以蛇为例，靠近食物是有益的。离开屏幕是不好的。在游戏中的每一点，代理人都会选择预期报酬最高的行动。</p><h1 id="dc4f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">那看起来像什么？</h1><p id="c0a6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">一开始，蛇不知道如何吃食物，也不太“有目的”。它也倾向于以与现在相反的方向前进，然后立刻撞到尾巴而死去。但是代理学习如何玩游戏并不需要很长时间。不到 30 局就玩的相当不错了。</p><p id="80f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">培训:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/4e16c8c16f19bd92703d20dec146cccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*4wdaTXJKhZ9JIFQb.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片/GIF 作者</p></figure><p id="4d6a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">100 场比赛后:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/7db9038fa1f004b1a228455fb1a399dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*JjWellxnkvdYOJ2b.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片/GIF 作者</p></figure><p id="2892" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看我们是如何到达那里的。</p><h1 id="355d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">游戏引擎</h1><p id="e1d2" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在这篇文章中，我们将更多地关注学习代理而不是游戏。也就是说，我们仍然需要一个游戏引擎。我发现 Edureka 的这个教程以 Snake 为例，很好地介绍了 Pygame。我对代码做了一些小的修改，使它能够与学习代理交互，它已经准备好了。谢谢 Edureka！</p><h1 id="5780" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">游戏状态</h1><p id="78bd" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们需要一种方法来定义当前的游戏状态。我们想用一种方式来定义它，我们可以代表游戏的所有可能的状态，或者至少是游戏的关键部分。我将我的状态定义为以下各项的组合:</p><ul class=""><li id="1339" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">食物的水平方向——蛇现在是在食物的左边还是右边？</li><li id="afc5" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">垂直方向——蛇现在是在食物的上面还是下面？</li><li id="df5a" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">相邻(左、右、上、下)方格中有墙或尾巴吗？</li></ul><p id="e345" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这让我们对博弈做了一点概括，博弈的状态空间相当小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/95b8331b4143be001e9960a1e874665b.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*ez6SUlXF85XdXZgy4NHefA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9608" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为蛇的左边有直接的危险，而食物在蛇的上方和右边，所以所有 3 个场景都由相同的状态表示。</p><h1 id="987d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">贝尔曼方程</h1><p id="a35c" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们使用贝尔曼方程来计算一个行动的总期望回报，并将信息存储在一个<em class="nf"> Q 表</em>中。</p><p id="05fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在时间<em class="nf"> t </em>，我们有以下状态和动作:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/4e626d3080427a692bc6392ce435cf3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*xtQwMa8q5dSeLNxA60GsHA.png"/></div></figure><p id="29d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种状态下，我们对该行为的回报可以表示为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/69ccc2c6ad96e5204b8622e862ebff65.png" data-original-src="https://miro.medium.com/v2/resize:fit:228/0*fo2b8vxRKZ5f4G4R"/></div></figure><p id="63e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们采取一个行动时，我们转换到下一个状态，在那里我们可以选择另一个行动来获得另一个奖励。除了眼前的奖励，我们还想向前看，看看未来是否会有更大的奖励。这对于像国际象棋这样的游戏来说尤其重要，因为代理人必须提前考虑好几个步骤。然后我们可以说在时间<em class="nf"> t </em>的总期望回报是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e28ec3030eefc592e1b979ae586c6c2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/0*NqRFzgZTGlrdm7od"/></div></figure><p id="3525" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们引入了不耐烦的概念——现在的奖励比以后的奖励好。也就是说，我们降低了未来的回报。折扣系数是一个介于 0 和 1 之间的数字。这个因素越大，我们给予未来奖励的权重就越大。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0ac65942483355ffb344dbe24f99b86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/0*6q_bLSj3y40UFKzI"/></div></figure><p id="a5c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">和简化，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/c4ee4390f872310048f7b9abeb0b3e14.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/0*_RgSmwHJCeXn-FqC"/></div></figure><p id="adb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在代理人玩第一个游戏之前，它不知道任何行动的预期回报是什么。所以我们从全零的 Q 表开始。当代理进行游戏时，它会观察每个动作<em class="nf"> a </em>的奖励值，从而了解哪个<em class="nf"> a </em>最适合每个状态<em class="nf"> s </em>。</p><h1 id="4bc5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">选择操作</h1><p id="dfab" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在每个给定的状态下，代理查看 Q 表，并采取具有最高预期回报的行动。这里的要点是，蛇应该如何行动没有预先定义的逻辑。完全是从数据中学习来的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="8985" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是这里还有其他的东西——一种叫做<strong class="ky ir"><em class="nf">ε贪婪</em> </strong>的技术。大多数时候，它遵循 Q 表。但如果有一条尚未探索的道路通向更高的回报呢？这是开发(基于我们目前所知的最佳选择)和探索(寻找潜在的更好选择)之间的权衡。在培训阶段，我们告诉代理偶尔随机选择一个动作。在这种情况下，10%的时间。我们称之为 10%ε。</p><h1 id="e39e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">更新 Q 表</h1><p id="c22e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">每次移动后，我们将使用贝尔曼方程更新我们的 Q 表。如果蛇靠近了食物，或者吃了食物，它会得到一个肯定的奖励。如果蛇撞到了自己的尾巴或墙壁，或者已经远离食物，它将获得负奖励。请注意历史列表的反向列表，它允许很容易地计算贝尔曼方程。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="1265" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">研究结果</h1><p id="c6b1" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们还想知道分数是否随着时间的推移而增加，或者是否已经稳定下来。因此，我们在每场比赛后输出分数，并绘制滚动的 30 个分数。我们在 100 场比赛后禁用 epsilon，看看它的表现如何。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/4468fb881c67fd9188c355ca0117a6a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*n3UE9vmB3qMMHemSGRcVMQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6329" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到分数不断增加，直到大约 150 场比赛时才趋于平稳。下一节将详细介绍这一点。</p><h1 id="b773" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">警告</h1><p id="a958" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在我的 Q-Learning 实现中，我的代理永远不会玩一个完美的游戏。除了躲避眼前的危险，它总是向食物靠近。但是我们知道这并不总是有效的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/9674f536a49de19404e3814093ceff4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*4She0bNqFk4Act-i6LL5-A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="8114" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，蛇应该向下移动，绕过它的尾巴。相反，它会向上移动并被卡住。我需要改变状态空间，以一种能捕捉到它并从中学习的方式。</p><h1 id="1092" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结束语</h1><p id="ee87" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我很惊讶 Q-Learning 的实现是如此简单，如此有效。但是正如我们在上面看到的，它显然有它的缺点。有更复杂的强化学习方法，可以更好地处理更复杂的游戏。不久的某一天，我希望能够建造一个会下棋的人工智能。</p><p id="38bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完整的代码可以在我的 Github 上的<a class="ae kv" href="https://github.com/jl4r1991/SnakeQlearning" rel="noopener ugc nofollow" target="_blank">这里</a>找到。如果你对我如何改进我的代码或学习代理有任何想法，请分享！</p></div></div>    
</body>
</html>