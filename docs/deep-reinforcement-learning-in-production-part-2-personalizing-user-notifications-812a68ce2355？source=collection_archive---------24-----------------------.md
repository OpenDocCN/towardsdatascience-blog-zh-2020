# 生产中的深度强化学习第 2 部分:个性化用户通知

> 原文：<https://towardsdatascience.com/deep-reinforcement-learning-in-production-part-2-personalizing-user-notifications-812a68ce2355?source=collection_archive---------24----------------------->

Zynga ML 工程团队的迈赫迪·本·阿耶德(T1)和 T2·帕特里克·哈琳娜(T3)。

这个[系列](https://medium.com/@benayed.mehdouch/7e1e63471e2)讨论了 Zynga 如何在产品中使用深度强化学习来个性化我们游戏中的用户体验。它基于我们在 [Spark 峰会](https://databricks.com/session_na20/productionizing-deep-reinforcement-learning-with-spark-and-mlflow)和[多伦多 ML 峰会](https://www.youtube.com/watch?v=q4b-HHG5dG4)上的陈述。

在这篇文章中，我们将讨论如何使用 RL 来个性化通知，并提高 Words with Friends Instant 的点击率。它作为一个例子来说明构建 RL 应用程序需要什么，以及为什么 RL 如此强大。

![](img/c455e297ffd4d3e6b247b2a635a26698.png)

亚历克·鲍德温欣喜若狂地收到了来自 Words With Friends 的个性化推送通知(由 Zynga 公司提供)

我们使用强化学习的旅程始于 2018 年。我们知道它在理论上符合个性化的需求，但我们需要测试它在现实世界中是否可行。在本文中，我们将讨论一个 Zynga 应用程序来说明 RL 如何对个性化产生真正的影响。为了保护商业秘密，一些细节被模糊处理。剧透警告:RL 特工成功了！从那以后，我们继续为深度 RL 应用开发开源库， [RL Bakery](https://github.com/zynga/rl-bakery) ，并推出了其他几个生产应用。

# 推送通知计时

大多数移动应用程序每天都向他们的用户群发送消息。例如，Zynga 的热门文字游戏 Words With Friends 会发送消息提醒用户，他们的朋友正在等待搬家。决定何时发送该信息是一个挑战。我们有全球用户群，每个人都有自己的时间表。发送通知的时间是敬业度的关键驱动因素。

*   如果在合适的时间发送通知，用户可能会回来参与游戏。
*   如果通知是在一个糟糕的时间发送的，用户可能会忽略它，不再回到游戏中来。你甚至会错过你的妈妈正等着你采取下一步行动！

![](img/7eae2f5e62b07f879dc54311d5f97ca3.png)

与朋友的对话 2(由 Zynga 公司提供)

# 以前的方法

在我们开始之前，游戏已经实现了一个基于工作段的解决方案。玩家群根据用户所在的国家分为 3 个时区。每个用户在他们所在时区的晚上都会收到一条消息，因为那是最受欢迎的游戏时间。这种方法作为快速的第一次尝试效果很好，但是他们准备对其进行优化。

![](img/d7b580ce7f0483b0e3029b0908487a5b.png)

我们如何为全球用户选择最佳的通知时间？(由[凯尔·格伦](https://unsplash.com/@kylejglenn?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/map?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄)

你能想到这个解决方案有什么缺点吗？

首先是粗略的时区近似值。当然，世界上不止有三个时区。不仅如此，像美国和加拿大这样的国家有多个时区，很难将玩家的国家映射到一个特定的时区。我们甚至可能没有与所有用户相关联的正确国家。

除了数据问题之外，缺乏个性化是基于细分的个性化方案的常见问题。即使我们确定了某人的时区，每个人都有自己的时间表。如果是周一，在凌晨 3 点给某人发信息可能会更好，因为那是他们夜班的“午休时间”。细分策略没有使用足够的玩家背景来执行细粒度的用户体验个性化。

这种方法的前提有一个更关键的问题，说明了为什么 RL 对于个性化如此强大。最终，我们的目标是提高用户参与度。如果我们有一个完美的模型告诉我们这个用户将在晚上 7 点玩，那么在那个时间发送消息有意义吗？他们已经开始参与游戏了！我们应该试着在一小时前发送消息吗？12 小时前？如果我们连续三次尝试在晚上 7 点给用户发消息，而他们没有回应，我们是否应该尝试不同的时间？这就是 RL 的用武之地。它回答了这个问题:我对这个用户采取什么行动来增加参与度。

# 强化学习设置

回顾我们系列的第 1 部分，我们将个性化问题公式化为为给定状态选择一个动作，以最大化长期回报。对于通知定时问题，我们使用以下状态、动作和奖励:

*   **状态:**用户与之前推送通知的历史交互。时间窗口被限定为 14 天，并且数据点被结构化为时间序列。
*   **操作:**发送下一个通知的时间。所以有 24 个动作可用。
*   **奖励:**当用户与推送通知交互时，提供一个积极的奖励。否则不提供奖励。

我们建立了一个机器学习管道，为所有符合推送通知条件的玩家收集、训练和执行批量推荐。

(关于基础设施的更多细节将在本系列的第 3 部分中分享。)

# 强化学习培训设置

构建强化学习应用实际上意味着什么？本质上，我们是在训练一个代理来选择动作。有很多训练智能体的深度学习算法:DQN、PPO、TD3、SAC 等。每个月都有新的算法(有新的 3 个字母缩写)发表在 ML 论文上。

我们的专业不是研究和实现这些算法。我们使用了来自开源 [TF-Agents](https://github.com/tensorflow/agents) 库的现成实现。我们选择使用 DQN 算法进行概念验证。这是一种更简单的算法，但它实际上被用于 AlphaGo 项目，以击败世界上最好的围棋选手。

选择 RL 算法后，我们选择了一组超参数。其中许多都涉及任何深度学习算法核心的深度学习模型。您需要选择最能代表您的功能和策略(代理的决策逻辑)的网络架构。)您还需要选择深度学习超参数，如学习率、优化算法(如 ADAM、SGD)等。

还有一些 RL 特定参数。对 DQN 来说，探索是以一种ε贪婪机制进行的。这意味着在概率为ε的情况下，会选择一个随机行动。其余时间，将使用具有最佳预测奖励的动作。通常以较高的ε开始，以鼓励探索，然后随着时间的推移降低ε，因为代理专注于成功的策略。

使用 RL 选择超参数值很困难。与监督学习不同，您不能简单地提供一组静态的标记数据，并确定哪些超参数会产生最佳结果。RL 只从环境的反馈中学习。从一个时间步长到未来时间的复杂交互无法从静态数据集中捕捉到。然而，在这种情况下，我们有一个现有的策略(简单的分割方法，将世界分成 3 个时区。)我们通过训练代理模仿现有方法来测试不同的超参数。我们将在本系列的第 4 部分对此进行更详细的讨论。

# 强化学习训练循环

每天，我们都会运行一个批处理培训工作流来更新我们的代理，并生成发送下一个推送通知的时间。该工作流收集以下培训数据:

*   两天前用户的状态
*   2 天前的操作(发送通知的时间)
*   用户的下一个状态(1 天前)
*   1 天前的用户参与度值(奖励)

对每个用户来说，这被组合成一个轨迹

> **(状态、动作、下一状态、奖励)**

这组轨迹用于更新现有的 RL 代理。更新意味着在 Tensorflow 上运行深度学习工作流:代表智能体的神经网络的值被更新，以更好地反映状态、行动和长期奖励之间的关系。

RL 代理的一个好处是，他们暗中跟踪“长期回报”。请注意，我们只告诉代理第二天的即时奖励。我们不需要等一个星期再告诉 it 关于 7 天保留期的信息。RL 算法基于即时奖励和下一个状态，建立它们自己的长期奖励的内部估计。

这是一个离线、批量 RL 问题的例子。它是离线的，因为我们从离线数据(而不是与环境的即时交互)中训练代理。)之所以是批量，是因为我们在批量输入数据。这种设置不同于论文和研究中讨论的大多数 RL 问题。在大多数学术问题中，代理人在模拟环境中接受在线训练，并立即做出反应。然而，离线问题在现实世界中更为典型，因为奖励往往从行动发生时就开始延迟。

# 结果

经过几天的数据收集和训练，**代理将点击率提高了 10%左右(相对而言)**。这第一次成功展示了使用一个相当简单的解决方案强化学习的力量。它通过算法测试不同的策略，以个性化每个用户的价值，并学习如何优化关键指标，如长期保留和参与。

代理能够回答这个问题:我应该在什么时候给每个用户发消息，以便**提高参与度**。它不知疲倦地工作，探索新的策略，寻找新的模式。如果游戏或用户模式改变，它也能适应。在这种情况下，强化学习框架为我们提供了一种通过最小的调整和无手动过程来优化关键指标的方法。这使得它成为个性化应用程序的完美方法。

这第一次的成功让我们将这个解决方案部署到 Friends Instant 的所有玩家群中。它现在每天为几百万玩家投入生产。

# 下一步是什么

培训 RL 代理并将其部署到生产环境中会带来一些挑战。它需要设置定制的机器学习基础设施，能够在需要时维护和提供建议。本系列的下一篇文章将讨论这些挑战。