<html>
<head>
<title>How Neural Nets, Computer Vision and Autonomous Vehicles are Related</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络、计算机视觉和自动驾驶汽车之间的关系</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stereo-vision-neural-nets-and-demand-in-autonomous-vehicles-6a9b6e6de41c?source=collection_archive---------41-----------------------#2020-05-03">https://towardsdatascience.com/stereo-vision-neural-nets-and-demand-in-autonomous-vehicles-6a9b6e6de41c?source=collection_archive---------41-----------------------#2020-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="48da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi ko translated">自动驾驶汽车(AV)正在大力发展，许多公司在这方面投资了数百万美元。2017年，福特投资Argo AI生产自动驾驶技术。Argo AI随后与卡耐基梅隆大学合作，在T2投资了1500万美元用于AV研究。</p><p id="f1c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其他公司也纷纷效仿，包括<a class="ae kx" href="https://emerj.com/ai-adoption-timelines/self-driving-car-timeline-themselves-top-11-automakers/" rel="noopener ugc nofollow" target="_blank">韩国汽车制造商现代和日本巨头丰田</a>分别投入<a class="ae kx" href="http://www.koreaherald.com/view.php?ud=20160317000551" rel="noopener ugc nofollow" target="_blank">17亿</a>和<a class="ae kx" href="https://www.reuters.com/article/uber-softbank-group-selfdriving/uber-lands-1-billion-from-softbank-toyota-for-self-driving-unit-idUSL1N22100A" rel="noopener ugc nofollow" target="_blank">6 . 67亿</a>。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi ky"><img src="../Images/605fcee7d1666b46812aff4a9d6343b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*XLlG4FXQT5bNT1qkJstGXg.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">来源:Self，<a class="ae kx" href="https://www.youtube.com/watch?v=dbNN1OgYGTE" rel="noopener ugc nofollow" target="_blank">特斯拉网络卡车大会</a> via Carscoops |有些公司确实比其他公司更挣扎</p></figure><p id="5a40" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">管理咨询公司麦肯锡公司(Mckinsey &amp; Company)估计，2015年至2030年将是以研发为重点的AVs 商业化应用的第一个时代。据预测，从2030年及以后，消费者将更频繁地购买AVs。目前，随着买家的疲惫，公司正努力与目标市场建立联系，需求增加可能还需要一段时间。</p><p id="5d9a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">研究兴趣和资金随时可用，因此现在是试验各种AV技术的最佳时机，如视觉系统。当前的视觉系统通过用景深数据辅助其运动估计算法来充当车辆的眼睛。最大的两个竞争对手是<strong class="js iu">激光雷达</strong>(光探测和测距)和<strong class="js iu">立体视觉</strong>。</p></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><h1 id="4bf7" class="lr ls it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">什么是<strong class="ak">激光雷达</strong>为什么要用？</h1><p id="5a71" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">激光雷达是一种激光扫描仪，它产生点云，提供周围环境的3D重建，允许深度和海拔估计。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/c914d6eec4a9c1ee674e6b49a5063a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYD-1XLtt9VNgTndV_FnPA.png"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">来源:<a class="ae kx" href="https://commons.wikimedia.org/wiki/File:Ouster_OS1-64_lidar_point_cloud_of_intersection_of_Folsom_and_Dore_St,_San_Francisco.png" rel="noopener ugc nofollow" target="_blank">丹尼尔l卢via维基媒体</a>；<strong class="bd mz">旧金山十字路口的点云</strong></p></figure><p id="4be7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="na">激光雷达的工作原理是将一束光射到一面旋转的镜子上</em>，光线被反射到多个方向。一旦<em class="na">射线返回</em>，它被反射回<em class="na">扫描仪</em>，扫描仪<em class="na">测量物体</em>有多远。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/2e9c8a3dd757236d0c227894299eb214.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*JIhEvu2uAedy4u62VewzgA.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">来源:<a class="ae kx" href="http://wavelab.uwaterloo.ca/sharedata/ME597/ME597_Lecture_Slides/ME597-4-Measurement.pdf" rel="noopener ugc nofollow" target="_blank"> Steven Waslander via ME597在滑铁卢大学</a>；<strong class="bd mz">图</strong>用于<strong class="bd mz">激光雷达</strong>的旋转镜和扫描仪</p></figure><p id="8a19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于光的波粒二象性，光线可以表示为正弦函数。发出的光线和返回的光线会产生相移。给定相移和已知射线波长，计算距离；这个过程重复得非常快，并产生一个详细的地图。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/4ae94adeb4127374c9ddf0e5a32f1587.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*1qduQbBnWJAIRufU1Jk-ww.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">来源:<a class="ae kx" href="http://wavelab.uwaterloo.ca/sharedata/ME597/ME597_Lecture_Slides/ME597-4-Measurement.pdf" rel="noopener ugc nofollow" target="_blank"> Steven Waslander via ME597在滑铁卢大学</a>；<strong class="bd mz">激光雷达扫描仪</strong>应用的<strong class="bd mz">相移&amp;距离</strong>推导演示</p></figure><p id="2729" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于扫描仪和反射镜所需的精度，激光雷达系统可能会很贵，根据所需的自主性和应用水平，市场价格从1 000美元到75 000美元不等。</p><h1 id="9f54" class="lr ls it bd lt lu nd lw lx ly ne ma mb mc nf me mf mg ng mi mj mk nh mm mn mo bi translated">立体视觉呢？</h1><p id="5da8" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">立体视觉是一种有趣的替代方式，它使用多相机设置来估计深度，令人惊讶的是，它是基于人类如何做到这一点的！</p><h1 id="1cbd" class="lr ls it bd lt lu nd lw lx ly ne ma mb mc nf me mf mg ng mi mj mk nh mm mn mo bi translated">实验时间到了</h1><p id="5e0f" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">现在试试这个:闭上你的右眼，把你的手指放在你的面前，集中注意力。现在闭上另一只眼睛，重复这个动作。</p><blockquote class="ni"><p id="1699" class="nj nk it bd nl nm nn no np nq nr kn dk translated">如果你很难做到这一点，不要担心，有一个<a class="ae kx" href="https://www.wikihow.com/Estimate-Distances-(by-Using-Your-Thumb-and-Eyes)" rel="noopener ugc nofollow" target="_blank"> Wikihow可用</a>！</p></blockquote><p id="6671" class="pw-post-body-paragraph jq jr it js b jt ns jv jw jx nt jz ka kb nu kd ke kf nv kh ki kj nw kl km kn im bi translated">好了，现在你做完了，你的手指动了一下，对吗？</p><p id="d8ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">别担心，你的眼睛工作正常，这是它们的几何学的结果。你所经历的现象被称为<strong class="js iu">视差</strong>，当从两条不同的视线看到一幅图像时，这种现象就会发生，因此，图像看起来会移动。在这种情况下，两条视线就是你的眼睛。</p></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><h1 id="80ae" class="lr ls it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">将几何学用于视差的动机</h1><p id="4e0f" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">用双眼正面盯着一个物体会产生深度，因为我们自己的视觉似乎是会聚的。这类似于一张透视照片，所有平行线相交于某个遥远的地平线点。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/f10849cba7845f671946e954f71926bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L9oBWH-TMeYVLnrvmBU8wQ.jpeg"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">来源:<a class="ae kx" href="https://commons.wikimedia.org/wiki/File:Cortes_del_Condado_de_Wabash,_Wabash,_Indiana,_Estados_Unidos,_2012-11-12,_DD_01.jpg" rel="noopener ugc nofollow" target="_blank">Diego Delso via Wikimedia</a>|<strong class="bd mz">透视信息丢失</strong>，相机需要额外处理才能重现</p></figure><p id="1c81" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，相机拍摄的图像不能像我们的眼睛那样创造深度。</p><p id="8e44" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图像将世界的3D视图转换成2D投影。更正式地说，这导致一条<em class="na">线转变成一个点</em>。在<strong class="js iu">欧几里得空间</strong>，即包含在2D平面中的几何空间中，<a class="ae kx" href="http://www.songho.ca/math/homogeneous/homogeneous.html" rel="noopener ugc nofollow" target="_blank">平行线不能像在透视图像中那样与</a>相交(透视图像属于<strong class="js iu">射影空间</strong>)。这由左边所示的线<em class="na">或</em>表示，尽管它对应于不同距离处的独特点，但相机无法分辨。从摄像机的角度来看，它看到的是一个点<em class="na"> x </em>而不是线<em class="na"> OX </em>。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6c3de3504e5da54988270e745ddadb25.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*02dSoU81CdihSFodbpLtqA.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">来源:<a class="ae kx" href="https://docs.opencv.org/master/da/de9/tutorial_py_epipolar_geometry.html" rel="noopener ugc nofollow" target="_blank"> OpenCV文档</a> |立体设置涉及<strong class="bd mz">极线几何</strong>到<strong class="bd mz">关联</strong>两个<strong class="bd mz">相机投影；</strong>注意<strong class="bd mz">尾线如何穿过<strong class="bd mz">可能解决方案</strong>处的</strong>l’</p></figure><p id="6350" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">相机<em class="na">不能像我们的大脑一样相互通信</em>，以帮助我们的眼睛产生图像。这导致在不同角度拍摄的同一图像的两个平面投影。所有深度信息都已丢失。然而，通过对两个投影进行三角测量，应用一些几何图形，并使用视差，相机可以相互关联以解析地求解距离。</p><p id="5eeb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面显示的双摄像头设置是深度估计的基础，它被称为立体视觉。</p><p id="7665" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有关坐标转换和相机的更多信息，请阅读这篇关于相机校准的<a class="ae kx" href="https://www.learnopencv.com/camera-calibration-using-opencv/" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><h1 id="f122" class="lr ls it bd lt lu nd lw lx ly ne ma mb mc nf me mf mg ng mi mj mk nh mm mn mo bi translated">数学估算深度</h1><p id="497d" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">推导和量化深度的视差效果是数学密集型的。如果你只对主公式感兴趣，请跳到这一小节的末尾。</p><p id="e978" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="na">有关推导，请参考上一小节中的图表。</em></p><p id="de1d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设两个摄像机都位于中心<em class="na"> O </em>和<em class="na">O’</em>处，它们在<em class="na"> X </em>处会聚。</p><p id="c6c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">线<em class="na"> OX </em>在左投影平面上显示为一个点<em class="na"> x </em>。然而，当从右投影平面观察时，有一组对应的点，<em class="na">x’</em>，它们在称为<strong class="js iu">极线</strong>，<em class="na">l’</em>的平面中形成一条线。这表示深度并证明点<em class="na"> x </em>实际上是一条线<em class="na">OX；现在的任务是确定x T21在哪里。</em></p><p id="d5ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">沿着<em class="na"> OX </em>可以画出称为<strong class="js iu">延长线</strong>的特殊线条，在<em class="na">O’</em>处汇合。通过检查每条核线可以找到点<em class="na"> x </em>，直到找到一条与<em class="na"> x </em>相交的核线，这个解就是<strong class="js iu">核线约束。</strong>多个<em class="na"> X </em>的指示，每一个都与一个特定的外延线相匹配，暗示该点的可能解决方案<em class="na"> x. </em></p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/87085514faaa35b66ed48e0ac2d47756.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*xjsUXFEQ46EDMiR0-qVJgQ.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">来源:<a class="ae kx" href="https://docs.opencv.org/master/dd/d53/tutorial_py_depthmap.html" rel="noopener ugc nofollow" target="_blank"> OpenCV文档</a> | <strong class="bd mz">通过外极线O'X解决</strong>的<strong class="bd mz">外极线约束</strong>；注意在<strong class="bd mz"> <em class="nz"> xOf、x’o’f和OO’x</em></strong>处形成的三个<strong class="bd mz">相似三角形</strong></p></figure><p id="a709" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，这个问题可以从自上而下的角度重新审视。</p><p id="1c5d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">给定对极线约束的特定解，<em class="na"> x </em>，其在相对平面中的对应点，<em class="na">x’</em>，视线<em class="na"> OX </em>和相对相机的对极线，<em class="na">O’x，</em>深度可以被求解。</p><p id="da68" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">点<em class="na"> x和x’</em>在不同的深度。</p><p id="bca3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">两条线，<em class="na"> OX </em>和<em class="na">O’x</em>处于不同的角度，但是可以通过等价的三角形来关联，这就是为什么差(<em class="na">x-x’</em>)等于参数:<em class="na"> B，f，Z(下面等式中的</em>)。(<em class="na">x—x’</em>)的差值就是<strong class="js iu">视差</strong> : <em class="na">视差误差的数值度量。</em></p><p id="aa9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更正式地，视差指的是两个相似区域的坐标距离(从任一图像投影来看)。<em class="na">视差与</em> <strong class="js iu"> <em class="na">深度</em> </strong> <em class="na">、Z、</em>成反比关系，通过求解等价三角关系得到证明。</p><p id="72c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于更近的物体，视差更明显，这表明当同一点出现在两个投影的不同位置时，视差很大。由于反比关系，它必须对应于一个短的深度。</p><p id="b357" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">用前面一节中展示的实验来验证上述假设。你的观察应该与定义的数学关系相匹配。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/fa4c82b053a9368300e78f216d81a200.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*p4TmG8UKoOxfS931lXp8vQ.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">来源:<a class="ae kx" href="https://docs.opencv.org/master/dd/d53/tutorial_py_depthmap.html" rel="noopener ugc nofollow" target="_blank"> OpenCV Docs </a> | <strong class="bd mz">视差计算</strong>用于隔离，<strong class="bd mz">求解深度:Z </strong></p></figure><p id="f95c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="na"> x </em>的值通过极线约束求解，并且<em class="na">x’</em>通过检查对面摄像机的图像投影得知。给定相机、<em class="na"> B </em>和<strong class="js iu">焦距</strong>、<em class="na"> f </em>之间的一些<strong class="js iu">距离，然后可以计算出<em class="na"> Z </em>。</strong></p><p id="8664" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将此应用于场景的所有像素产生一个<strong class="js iu">视差图</strong>，它通过颜色指示深度。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/241713d28ff818165b94674636a7c575.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*fouBVviwygNTJ8HguAvGxQ.jpeg"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">来源:<a class="ae kx" href="https://docs.opencv.org/master/dd/d53/tutorial_py_depthmap.html" rel="noopener ugc nofollow" target="_blank"> OpenCV文档</a> |浅灰色物体更近，深灰色物体更远<strong class="bd mz">视差图</strong></p></figure><p id="9687" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有关立体视觉和3D重建的更多信息，请访问<a class="ae kx" href="https://docs.opencv.org/master/dd/d53/tutorial_py_depthmap.html" rel="noopener ugc nofollow" target="_blank"> OpenCV的文档</a>和<a class="ae kx" href="https://www.youtube.com/watch?v=rE-hVtytT-I&amp;list=PLuh62Q4Sv7BUJlKlt84HFqSWfW36MDd5a" rel="noopener ugc nofollow" target="_blank">Rich Radke博士的ECE6969课程</a>。</p></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><h1 id="b8a8" class="lr ls it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">立体视觉的缺陷与改进</h1><p id="f5c4" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">虽然立体视觉不贵，但它的主要问题是处理时间。理想情况下，在AVs中，大多数系统将接近实时工作。然而<em class="na">由于解决极线约束需要很长时间，立体视觉系统本身无法足够快地工作</em>。为了减少计算时间，正在研究使用<em class="na">机器学习</em>来改进立体声系统并<em class="na">加速</em>它们。</p><h1 id="87a7" class="lr ls it bd lt lu nd lw lx ly ne ma mb mc nf me mf mg ng mi mj mk nh mm mn mo bi translated">通过机器学习的立体视觉</h1><p id="c8fe" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">在深入神经网络的细节之前，理解什么是图像是关键:一个<strong class="js iu">图像</strong>是一个<em class="na"> 2D数字数组，每个数组元素是一个像素值。</em></p><p id="dd49" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于这种结构，<strong class="js iu">卷积神经网络</strong>(CNN)是理想的，因为它们在图像上应用滑动过滤器，将图像下采样为代表其可能值的单个向量。向量被传递到一个完全连接的网络，该网络为每个值分配概率，并输出最可能的匹配作为预测。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi oc"><img src="../Images/fff293357945e7f1d58d1fca7db08cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3BRLw4lsANPEfGgimG3YVQ.png"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">来源:<a class="ae kx" href="https://commons.wikimedia.org/wiki/Category:Convolutional_neural_networks#/media/File:Typical_cnn.png" rel="noopener ugc nofollow" target="_blank"> Aphex34 via Wikimedia </a> |典型的<strong class="bd mz">神经网络架构:</strong>对图像进行子采样，馈入全连通层并输出预测</p></figure><p id="53f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了了解更多关于CNN和神经网络的知识，<a class="ae kx" href="https://www.youtube.com/watch?v=py5byOOHZM8" rel="noopener ugc nofollow" target="_blank">迈克·庞德博士</a>提供了简短的解释，正式的概念可以在<a class="ae kx" href="https://www.youtube.com/watch?v=py5byOOHZM8" rel="noopener ugc nofollow" target="_blank">斯坦福大学的视频讲座</a>中找到。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi od"><img src="../Images/b07ea491a88c7357f28ca62c9ce44973.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*EsVyDzKoCHKmXE1Flk9cYg.png"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated"><a class="ae kx" href="https://www.cs.toronto.edu/~urtasun/publications/luo_etal_cvpr16.pdf" rel="noopener ugc nofollow" target="_blank">罗，G. Swing，Urtasun Via多伦多大学计算机科学系</a>暹罗CNN架构学习分发</p></figure><p id="7da4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">至于立体系统，图像被分割成左右两个投影，一次分析一个小区域。两个投影都被馈送到一个<a class="ae kx" href="https://www.cs.toronto.edu/~urtasun/publications/luo_etal_cvpr16.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">连体CNN </strong> </a>，其同时为左右投影中的<em class="na">对应区域上的</em>视差<em class="na">创建一组<em class="na">可能的</em>概率分布。然后，该组可能性被传递到一个<em class="na">全连接层</em>，该层<em class="na">输出</em>在<em class="na">原始</em>图像的单个片上<em class="na">最可能的视差分布</em>。</em></p><p id="eec4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这意味着<em class="na">网络以被称为<em class="na">概率分布</em>的<em class="na">连续函数</em>的形式输出<em class="na">差异</em>的可能值的<em class="na">范围</em>。</em></p><p id="3fae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">概率分布</strong>代表结果发生的<em class="na">可能性，其中<em class="na">组结果</em>显示在<em class="na"> x轴</em>上。随机变量包含这些结果作为可能的值。每个结果都有相应的可能性显示在y轴上。</em></p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/c0a9ed736829397f7535f9938aa5935f.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*dzN3jcFkeZoaxKffTl5eCg.jpeg"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">来源:<a class="ae kx" href="https://commons.wikimedia.org/wiki/File:Gaussian_distribution_2.jpg" rel="noopener ugc nofollow" target="_blank"> Atakbi1 via Wikimedia </a> |一种叫做高斯分布的概率分布，最大值在中心(平均值)。以上CNN产生独特的概率分布</p></figure><p id="2f12" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">按照上面的CNN，x轴将填充视差值，y轴填充这些差异是正确的可能性。CNN学习并预测分布的<em class="na">形状。</em>通过找到分布的最大值点，使用单独的算法来找到最可能的视差。</p><p id="47a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">暹罗CNN设计允许对应于不同图像区域的<em class="na">差异被关联</em>，这可能表明那些区域是同一表面的一部分。</p><h1 id="67f2" class="lr ls it bd lt lu nd lw lx ly ne ma mb mc nf me mf mg ng mi mj mk nh mm mn mo bi translated">暹罗CNN架构的扩展</h1><p id="7c29" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">先前的网络比较了左右投影并优化了匹配成本。这意味着在两个投影中搜索指向同一部分的补片。一旦找到，每个补片将被用于计算视差，给出该补片相对于平面的位置。由于已经找到了精确的点，这些网络围绕着解决极线约束的过程工作。</p><p id="fef9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面讨论的连体CNN是对过去网络的改进。通过改变优化参数来学习可能差异的分布，消除了计算差异的<em class="na">需要。</em>取而代之的是，选择最有可能的视差，这使得连体CNN架构比其前身运行得更快。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/e0fd76598ca9097640e61df228a42df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l2l3PTu55HrUnM9Lc3Z10A.png"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated"><a class="ae kx" href="https://commons.wikimedia.org/wiki/File:EmiMa-099-semantic-segmentation.png" rel="noopener ugc nofollow" target="_blank">Martin thoma via Wikimedia</a>|扩展深度估计，共同解决语义分割等其他视觉任务</p></figure><p id="4ff8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在过去，网络已经实现了组合解决方案，其中统一解决了对象识别和深度估计。因此，<em class="na">找到相关的差异可以扩展连体CNN </em>设计，以允许<em class="na">其他视觉任务</em>。</p></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><h1 id="694f" class="lr ls it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">自主视觉的未来</h1><p id="bd2e" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">行业领导者<em class="na"> Waymo使用激光雷达系统</em>，特斯拉创始人埃隆·马斯克<a class="ae kx" href="https://www.youtube.com/watch?v=HM23sjhtk4Q" rel="noopener ugc nofollow" target="_blank">发表评论</a>，</p><blockquote class="ni"><p id="644d" class="nj nk it bd nl nm nn no np nq nr kn dk translated">"..任何依赖激光雷达的人都注定要失败..”</p></blockquote><p id="74fc" class="pw-post-body-paragraph jq jr it js b jt ns jv jw jx nt jz ka kb nu kd ke kf nv kh ki kj nw kl km kn im bi translated">双方显然存在激烈的竞争和相互矛盾的观点。康奈尔大学的研究人员支持立体视觉，尽管公众担心它在无人驾驶汽车中的功效和安全性。</p><p id="3e3c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">特斯拉不仅仅使用立体视觉；它的车辆实现了立体摄像机、雷达和地图绘制，以提供具有竞争力的精确度。尽管需要更多的技术，立体声系统的运行成本仍然低于激光雷达。</p><p id="4bfc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然安全至关重要，但企业必须盈利才能继续运营。对消费者来说,<em class="na">最大的障碍之一是市场价值</em>,它直接<em class="na">将</em>与用于汽车设计的技术<em class="na">联系起来。正如开头提到的，AVs还处于起步阶段，随着更多的研究和开发，很多东西都会改变。只有时间能证明哪种视觉系统更胜一筹。</em></p></div></div>    
</body>
</html>