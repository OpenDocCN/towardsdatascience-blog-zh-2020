<html>
<head>
<title>Multiclass Text Classification using LSTM in Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pytorch 中使用 LSTM 的多类文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df?source=collection_archive---------3-----------------------#2020-04-07">https://towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df?source=collection_archive---------3-----------------------#2020-04-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9a6e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于客户评论预测项目评级</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/003655bdcd187fd9fde541f00f4b52cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MutFuG2Za6Wz1v89Oo_gww.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="8394" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人类的语言充满了歧义，很多时候，同一个短语可以根据上下文有多种解释，甚至会让人感到困惑。这些挑战使得自然语言处理成为一个有趣但难以解决的问题。然而，在过去的几年中，我们已经看到了 NLP 的许多进步，探索所使用的各种技术是非常令人着迷的。本文旨在使用 Pytorch 涵盖深度学习中的一种技术:长短期记忆(LSTM)模型。</p><p id="e82d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里有一个笔记本的链接，其中包含了我在本文中使用的所有代码:<a class="ae lr" href="https://jovian.ml/aakanksha-ns/lstm-multiclass-text-classification" rel="noopener ugc nofollow" target="_blank">https://jovian . ml/aakanksha-ns/lstm-multi class-text-classification</a></p><p id="de9d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您是 NLP 新手，或者需要深入了解预处理和单词嵌入，可以阅读下面的文章:</p><div class="ls lt gp gr lu lv"><a rel="noopener follow" target="_blank" href="/getting-started-with-natural-language-processing-nlp-2c482420cc05"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">自然语言处理(NLP)入门</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">使用简单的 Python 库</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">towardsdatascience.com</p></div></div><div class="me l"><div class="mf l mg mh mi me mj kp lv"/></div></div></a></div><h1 id="5b98" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">rnn 和 LSTMs 简介:</h1><p id="409f" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">使语言模型不同于传统神经网络的是它们对上下文的依赖性。传统的前馈网络假设输入彼此独立。对于 NLP，我们需要一种机制，能够使用来自先前输入的顺序信息来确定当前输出。递归神经网络(RNNs)通过拥有循环来解决这个问题，允许信息在网络中持续存在。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/ff0aa1003d0aaa8dabf198a3f8fd90b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TjGIdoBXI7KAAt8A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">展开的递归神经网络(图片由作者提供)</p></figure><p id="37fc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，传统的 rnn 有爆炸和消失梯度的问题，并且不擅长处理长序列，因为它们遭受短期记忆。</p><p id="1008" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">长短期记忆网络(LSTM)是一种特殊的 RNN，能够学习长期依赖关系。它们通过维持一种称为“细胞状态”的内部记忆状态来实现这一点，并通过称为“门”的调节器来控制每个 LSTM 单元内部的信息流。这里有一个很好的解释 LSTMs 细节的来源:</p><div class="ls lt gp gr lu lv"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">了解 LSTM 网络</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">2015 年 8 月 27 日发布人类不是每秒钟都从零开始思考。当你读这篇文章时，你…</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">colah.github.io</p></div></div></div></a></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/fc1a5b1c3d682b1cd28196ccae60d03a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pq30znWeMlVXUxxX.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">LSTM 细胞的结构。(<a class="ae lr" href="https://www.researchgate.net/publication/329362532_Designing_neural_network_based_decoders_for_surface_codes" rel="noopener ugc nofollow" target="_blank">来源</a>:瓦萨莫普洛斯、萨瓦斯&amp;贝特尔斯、科恩&amp;阿尔穆德韦、卡门。(2018).设计基于神经网络的表面码解码器。)</p></figure><h1 id="1b55" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">Pytorch 的基本 LSTM</h1><p id="742a" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">在我们进入主要问题之前，让我们看看 Pytorch 中使用随机输入的 LSTM 的基本结构。在进入复杂的输入之前，这是一个有用的步骤，因为它帮助我们学习如何更好地调试模型，检查维度是否增加，并确保我们的模型按预期工作。</p><p id="0a3c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">尽管我们将处理文本，但由于我们的模型只能处理数字，我们将输入转换成数字序列，其中每个数字代表一个特定的单词(下一节将详细介绍)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="5dfc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们首先将输入(3×8)通过一个<a class="ae lr" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank">嵌入</a>层，因为单词嵌入在捕捉上下文方面更好，并且在空间上比一键向量表示更有效。</p><p id="ee73" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在 Pytorch 中，我们可以使用<code class="fe nl nm nn no b">nn.Embedding</code>模块来创建这一层，它将词汇大小和期望的单词向量长度作为输入。您可以选择提供填充索引，以指示嵌入矩阵中填充元素的索引。</p><p id="555b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下面的例子中，我们的词汇表由 100 个单词组成，所以我们对嵌入层的输入只能是从 0 到 100，它返回给我们一个 100x7 的嵌入矩阵，第 0 个索引代表我们的填充元素。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="a054" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将嵌入层的输出传递到 LSTM 层(使用<code class="fe nl nm nn no b">nn.LSTM</code>创建)，它将单词向量长度、隐藏状态向量长度和层数作为输入。此外，如果我们的输入形状中的第一个元素具有批量大小，我们可以指定<code class="fe nl nm nn no b">batch_first = True</code></p><p id="1413" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">LSTM 层输出三样东西:</p><ul class=""><li id="884c" class="np nq iq kx b ky kz lb lc le nr li ns lm nt lq nu nv nw nx bi translated">序列中所有隐藏状态的合并输出</li><li id="06d2" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq nu nv nw nx bi translated">最后一个 LSTM 单元的隐藏状态—最终输出</li><li id="1be1" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq nu nv nw nx bi translated">细胞状态</li></ul><p id="89e2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以验证在通过所有层之后，我们的输出具有预期的尺寸:</p><p id="42fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">3x8 -&gt;嵌入-&gt; 3x8x7 -&gt; LSTM(隐藏大小=3)-&gt; 3x3</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="d035" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">多类文本分类—根据评论预测评级</h1><p id="ecfa" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">现在让我们来看看 LSTMs 的一个应用。</p><p id="5d08" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">问题陈述:</strong>给定一个商品的评论，预测评分(取 1 到 5 的整数值，1 为最差，5 为最好)</p><p id="6194" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">数据集</strong>:我使用了以下来自 Kaggle 的数据集:</p><div class="ls lt gp gr lu lv"><a href="https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">女性电子商务服装评论</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">23，000 条客户评论和评级</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">www.kaggle.com</p></div></div><div class="me l"><div class="od l mg mh mi me mj kp lv"/></div></div></a></div><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="de09" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">公制的</h1><p id="aa00" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">对于大多数分类问题，我们通常以准确性作为衡量标准，然而，评级是有序的。如果实际值为 5，但模型预测值为 4，则不认为它与预测值 1 一样差。因此，我们选择<strong class="kx ir">RMSE</strong>——均方根误差作为我们的北极星度量标准，而不是精确。此外，评级预测是一个相当困难的问题，即使对人类来说也是如此，所以预测误差不超过 1 分就已经很不错了。</p><h1 id="b25b" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">预处理</h1><p id="4e65" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">如前所述，我们需要将文本转换成数字形式，作为输入提供给模型。在删除标点符号、特殊字符和小写文本后，我使用了<code class="fe nl nm nn no b">spacy</code>进行标记化:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="0a05" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们统计每个单词在语料库中出现的次数，并去掉不经常出现的单词:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="f004" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们丢了 6000 字左右！这是意料之中的，因为我们的语料库很小，不到 25k 条评论，出现重复单词的机会很小。</p><p id="7c58" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，我们创建一个词汇索引映射，并使用该映射对我们的评论文本进行编码。我选择任何评论的最大长度为 70 个字，因为评论的平均长度约为 60 个字。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="33d4" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">Pytorch 数据集</h1><p id="2c9a" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">数据集非常简单，因为我们已经将编码存储在输入数据帧中。我们还输出每种情况下输入序列的长度，因为我们可以有接受可变长度序列的 LSTMs。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="d12f" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">Pytorch 训练循环</h1><p id="3474" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">训练循环相当标准。我用过亚当优化器和交叉熵损失。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="8315" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">LSTM 模型</h1><p id="cba8" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">我在模型中使用了三种变体:</p><ol class=""><li id="3e62" class="np nq iq kx b ky kz lb lc le nr li ns lm nt lq oe nv nw nx bi translated"><strong class="kx ir">固定输入大小的 LSTM:</strong></li></ol><p id="27ba" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这与我们之前看到的基本 LSTM 的结构非常相似，只是增加了一个漏失层来防止过度拟合。由于我们有一个分类问题，我们有一个最终的线性层有 5 个输出。这种实现实际上在分类 LSTMs 中工作得最好，精确度约为 64%,均方根误差仅为 0.817</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="02e0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 2。可变输入尺寸的 LSTM:</strong></p><p id="3ed3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以稍微修改一下我们的模型，让它接受可变长度的输入。这最终增加了训练时间，因为<code class="fe nl nm nn no b">pack_padded_sequence</code>函数调用返回一批填充的可变长度序列。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="f696" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 3。具有固定输入大小和固定预训练手套词向量的 LSTM:</strong></p><p id="ed5f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">代替训练我们自己的单词嵌入，我们可以使用预先训练的手套单词向量，其已经在大规模语料库上被训练并且可能具有更好的上下文捕获。然而对于我们的问题来说，这似乎帮助不大。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="52f3" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">使用回归而不是分类来预测评级</h1><p id="2679" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">由于评级是有顺序的，并且在许多情况下，3.6 的预测可能比四舍五入到 4 更好，因此将此作为回归问题进行探讨是有帮助的。不足为奇的是，这种方法给了我们最低的误差，仅仅是<code class="fe nl nm nn no b">0.799</code>，因为我们不再只有整数预测了。</p><p id="d81b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的模型的唯一变化是，最终层没有 5 个输出，而是只有一个。训练循环也有点变化，我们使用<code class="fe nl nm nn no b">MSE</code>损失，我们不再需要使用<code class="fe nl nm nn no b">argmax</code>来获得最终预测。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="3388" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">结论:</h1><p id="0a8f" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">理论上 LSTM 似乎参与其中，但是它的 Pytorch 实现非常简单。此外，在研究任何问题时，选择正确的指标非常重要，在我们的案例中，如果我们追求准确性，模型似乎做得非常糟糕，但 RMSE 显示它相差不到 1 个评分点，这与人类的表现相当！</p><p id="43a3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">参考文献</strong>:</p><ul class=""><li id="87fe" class="np nq iq kx b ky kz lb lc le nr li ns lm nt lq nu nv nw nx bi translated"><a class="ae lr" href="https://www.usfca.edu/data-institute/certificates/deep-learning-part-one" rel="noopener ugc nofollow" target="_blank">https://www . usfca . edu/data-institute/certificates/deep-learning-part-one</a></li><li id="6bce" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq nu nv nw nx bi translated">【https://colah.github.io/posts/2015-08-Understanding-LSTMs/ T4】</li><li id="28e7" class="np nq iq kx b ky ny lb nz le oa li ob lm oc lq nu nv nw nx bi translated"><a class="ae lr" href="http://web.stanford.edu/class/cs224n/" rel="noopener ugc nofollow" target="_blank">http://web.stanford.edu/class/cs224n/</a></li></ul></div></div>    
</body>
</html>