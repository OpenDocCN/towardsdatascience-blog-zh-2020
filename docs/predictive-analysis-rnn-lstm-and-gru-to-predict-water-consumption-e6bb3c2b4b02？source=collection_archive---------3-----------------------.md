# é¢„æµ‹åˆ†æž:ç”¨ TensorFlow ä¸­çš„ LSTMã€GRU å’Œæ¯”å°”æ–¯ç‰¹å§†è¿›è¡Œå›žå½’åˆ†æž

> åŽŸæ–‡ï¼š<https://towardsdatascience.com/predictive-analysis-rnn-lstm-and-gru-to-predict-water-consumption-e6bb3c2b4b02?source=collection_archive---------3----------------------->

![](img/d4246c039b5c2109517fed74fd420b8c.png)

åœ¨ [Unsplash](https://unsplash.com/) ä¸Šçš„[this is å·¥ç¨‹](https://unsplash.com/@thisisengineering)æ‹æ‘„

## å…³äºŽå¼€å‘ LSTMã€GRU å’Œæ¯”å°”æ–¯ç‰¹å§†æ¨¡åž‹è¿›è¡Œç”¨æ°´é‡å¤šæ­¥é¢„æµ‹çš„åˆ†æ­¥æŒ‡å—

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å¼€å‘äº†ä¸‰ä¸ªè¿žç»­æ¨¡åž‹ï¼›LSTMã€GRU å’ŒåŒå‘ LSTMï¼Œé¢„æµ‹æ°”å€™å˜åŒ–å½±å“ä¸‹çš„ç”¨æ°´é‡ã€‚ç„¶åŽï¼Œæˆ‘ç”¨æœ€å¯é çš„ä¸€ä¸ªå¯¹æœªæ¥ 10 å¹´çš„åŸŽå¸‚ç”¨æ°´é‡è¿›è¡Œå¤šæ­¥é¢„æµ‹ã€‚

é¦–å…ˆï¼Œè®©æˆ‘æé†’ä½ ä¸€ä¸‹åŸºæœ¬é¢ã€‚ç„¶åŽï¼Œæˆ‘å°†å¸¦æ‚¨å®Œæˆä¸€ä¸ªå®Œæ•´çš„ Python æ•°æ®ç§‘å­¦é¡¹ç›®ã€‚

**ðŸ‘©â€ðŸ’»** [**ä¸Šçš„ Python ä»£ç  GitHub**](https://github.com/NioushaR/LSTM-GRU-BiLSTM-in-TensorFlow-for-predictive-analytics)

# é€’å½’ç¥žç»ç½‘ç»œ

é€’å½’ç¥žç»ç½‘ç»œ(RNN)æ˜¯ä¸€ç§è®¾è®¡ç”¨äºŽä½¿ç”¨æ—¶åºæ•°æ®çš„ç¥žç»ç½‘ç»œã€‚åœ¨ RNNs ä¸­ï¼Œè¾“å‡ºå¯ä»¥ä½œä¸ºè¾“å…¥åé¦ˆåˆ°ç½‘ç»œä¸­ï¼Œåˆ›å»ºä¸€ä¸ªå¾ªçŽ¯ç»“æž„ã€‚

## æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

é€šè¿‡åå‘ä¼ æ’­æ¥è®­ç»ƒ rnnã€‚åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼ŒRNNs ä¼šé‡åˆ°æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚æ¢¯åº¦æ˜¯ç”¨äºŽæ›´æ–°ç¥žç»ç½‘ç»œæƒé‡çš„å€¼ã€‚æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ˜¯å½“æ¢¯åº¦éšç€æ—¶é—´å‘åŽä¼ æ’­è€Œæ”¶ç¼©æ—¶ã€‚å› æ­¤ï¼Œæ¢¯åº¦è¾ƒå°çš„å±‚ä¸ä¼šå­¦ä¹ ï¼Œå®ƒä»¬ä¼šå¯¼è‡´ç½‘ç»œå…·æœ‰çŸ­æœŸè®°å¿†ã€‚

**ðŸ’¡â“** æ¸å˜æ¶ˆå¤±é—®é¢˜çš„è§£å†³æ–¹æ¡ˆæ˜¯ä»€ä¹ˆ

# é•¿çŸ­æœŸè®°å¿†

é•¿çŸ­æœŸè®°å¿†(LSTM)æ˜¯ä¸€ç§ä¸“é—¨çš„ RNNï¼Œä»¥å‡è½»æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚LSTMs å¯ä»¥ä½¿ç”¨ä¸€ç§ç§°ä¸º gates çš„æœºåˆ¶æ¥å­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»ã€‚è¿™äº›é—¨å¯ä»¥äº†è§£åºåˆ—ä¸­å“ªäº›ä¿¡æ¯æ˜¯é‡è¦çš„ï¼Œåº”è¯¥ä¿ç•™æˆ–ä¸¢å¼ƒã€‚LSTMs æœ‰ä¸‰ä¸ªé—¨ï¼›è¾“å…¥ï¼Œå¿˜è®°ï¼Œè¾“å‡ºã€‚

![](img/aaf7785d34594586d9329f48781345fb.png)

LSTM ç»†èƒžçš„ç»“æž„

![](img/d942fb3786c704f1b2abd725b7c64e50.png)

# åŒå‘ LSTMs

åŒå‘ lstm(BiSTM)çš„æ€æƒ³æ˜¯åœ¨ LSTM æ¨¡åž‹ä¸­èšåˆç‰¹å®šæ—¶é—´æ­¥é•¿çš„è¿‡åŽ»å’Œæœªæ¥çš„è¾“å…¥ä¿¡æ¯ã€‚åœ¨ BiLSTM ä¸­ï¼Œåœ¨ä»»ä½•æ—¶é—´ç‚¹ï¼Œæ‚¨éƒ½å¯ä»¥ä¿å­˜è¿‡åŽ»å’Œæœªæ¥çš„ä¿¡æ¯ã€‚

# é—¨æŽ§å¾ªçŽ¯å•å…ƒ

é—¨æŽ§é€’å½’å•å…ƒ(GRU)æ˜¯æ–°ä¸€ä»£çš„ç¥žç»ç½‘ç»œï¼Œéžå¸¸ç±»ä¼¼äºŽ LSTMã€‚GRU æ‘†è„±äº†ç»†èƒžçŠ¶æ€ï¼Œä½¿ç”¨éšè—çŠ¶æ€æ¥ä¼ é€’ä¿¡æ¯ã€‚GRU å’Œ LSTM çš„å¦ä¸€ä¸ªåŒºåˆ«æ˜¯ GRU åªæœ‰ä¸¤ä¸ªå¤§é—¨ï¼›å¤ä½å’Œæ›´æ–°é—¨ã€‚

![](img/10f207acf691c6e0fd139e7a13d1db87.png)

GRU ç»†èƒžçš„ç»“æž„

![](img/8b1c74f5420387dfae86af0753b737b9.png)

# â˜ºæ—¶é—´è®©æˆ‘ä»¬çš„æ‰‹ dirtyâ—

## èµ„æ–™ç»„

åŠ æ‹¿å¤§é­åŒ—å…‹çœçš„ Brossard å¸‚è¢«é€‰ä¸ºç ”ç©¶åœ°ç‚¹ã€‚è¿™åº§åŸŽå¸‚æ˜¯è’™ç‰¹åˆ©å°”å¤§éƒ½å¸‚åŒºçš„ä¸€éƒ¨åˆ†ã€‚æœ¬é¡¹ç›®**æ—¥ç”¨æ°´é‡**æ•°æ®å–è‡ª 2011 å¹´ 9 æœˆ 1 æ—¥è‡³ 2015 å¹´ 9 æœˆ 30 æ—¥ã€‚æ”¶é›†åŒæœŸ**æœ€ä½Žæ°”æ¸©**ã€**æœ€é«˜æ°”æ¸©**å’Œ**æ€»é™æ°´é‡**ã€‚è¿™äº›æ°”å€™å˜é‡çš„æµ‹é‡æ•°æ®æ¥è‡ª[åŠ æ‹¿å¤§çŽ¯å¢ƒéƒ¨](https://www.concordia.ca/news/stories/2019/01/07/historical-canadian-climate-data-is-now-only-a-few-clicks-away.html)ã€‚

## å¯¼å…¥åº“

![](img/86adab9c7807e3bd7f6322aca0549fec.png)

## è®¾ç½®éšæœºç§å­

è®¾ç½®éšæœºç§å­ä»¥åœ¨æ¯æ¬¡è¿è¡Œä»£ç åŽèŽ·å¾—ç›¸åŒçš„ç»“æžœã€‚

```
**# Set random seed for reproducibility**
tf.random.set_seed(1234)
```

# æ­¥éª¤ 1:è¯»å–å’Œæµè§ˆæ•°æ®

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘æ­£åœ¨å¤„ç†å¤šå˜é‡æ—¶é—´åºåˆ—æ•°æ®ã€‚å½“æˆ‘ä»Ž CSV æ–‡ä»¶å¯¼å…¥æ•°æ®æ—¶ï¼Œæˆ‘é€šè¿‡ **parse_dates = ['Date']** ç¡®ä¿ **Date** åˆ—å…·æœ‰æ­£ç¡®çš„ *DateTime* æ ¼å¼ã€‚æ­¤å¤–ï¼Œå½“æˆ‘å¤„ç†æ—¥æœŸå’Œæ—¶é—´æ—¶ï¼Œå¦‚æžœæˆ‘å°†**æ—¥æœŸ**åˆ—è®¾ç½®ä¸º dataframe ç´¢å¼•ï¼Œå°±ä¼šå˜å¾—å®¹æ˜“å¾—å¤šã€‚

```
**# Read file**
file = 'Data.csv'
raw_data = pd.read_csv(file, parse_dates = ['Date'],
                       index_col = 'Date')
df = raw_data.copy()
```

![](img/0f8f5a51309d131369f6ffb95431908e.png)

*   Max_T:æœ€é«˜æ¸©åº¦(â„ƒ)
*   Min_T:æœ€ä½Žæ¸©åº¦(â„ƒ)
*   T_P:æ€»é™æ°´é‡(æ¯«ç±³)
*   UWC:åŸŽå¸‚ç”¨æ°´é‡(ç«‹æ–¹ç±³/äºº.å¤©)

```
**# Define a function to draw time_series plot**
def timeseries (x_axis, y_axis, x_label, y_label):
    plt.figure(figsize = (10, 6))
    plt.plot(x_axis, y_axis, color ='black')
    plt.xlabel(x_label, {'fontsize': 12}) 
    plt.ylabel(y_label, {'fontsize': 12})timeseries(df.index, df['WC (m3/capita.day)'], 'Time (day)','Daily Water consumption ($m^3$/capita.day)')
```

![](img/605873cd11ee6112485b60c44bb7e482.png)

2011 å¹´ 9 æœˆ 1 æ—¥è‡³ 2015 å¹´ 9 æœˆ 30 æ—¥çš„æ—¥ç”¨æ°´é‡æ—¶é—´åºåˆ—

# ç¬¬äºŒæ­¥:æ•°æ®é¢„å¤„ç†

æ•°æ®é¢„å¤„ç†æ˜¯æœ€è€—æ—¶çš„æ­¥éª¤ï¼ŒåŒ…æ‹¬:

*   å¤„ç†ç¼ºå¤±å€¼
*   æ›¿æ¢å¼‚å¸¸å€¼
*   å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®
*   æ‹†åˆ†ç›®æ ‡å˜é‡å’Œå› å˜é‡
*   æ•°æ®è½¬æ¢
*   åˆ›å»º 3D è¾“å…¥æ•°æ®é›†

## 2.1 å¤„ç†ç¼ºå¤±å€¼

å¯¹äºŽæ—¶é—´åºåˆ—æ•°æ®ï¼Œä½¿ç”¨çº¿æ€§æ’å€¼æ›¿æ¢ç¼ºå¤±å€¼æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚

```
**# Check missing values**
df.isnull().sum()
```

![](img/2c6dcbd867519661fdf14774d487782f.png)

```
**# Replace missing values by interpolation**
def replace_missing (attribute):
    return attribute.interpolate(inplace=True)replace_missing(df['Max_T'])
replace_missing(df['Min_T'])
replace_missing(df['T_P'])
replace_missing(df['UWC'])
```

## 2.2 æ›¿æ¢å¼‚å¸¸å€¼

æˆ‘ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ¥æ£€æµ‹å¼‚å¸¸å€¼ã€‚ç»Ÿè®¡æ–¹æ³•å‡è®¾æ•°æ®ç‚¹å‘ˆæ­£æ€åˆ†å¸ƒã€‚å› æ­¤ï¼Œä½Žæ¦‚çŽ‡åŒºåŸŸä¸­çš„å€¼è¢«è§†ä¸ºå¼‚å¸¸å€¼ã€‚æˆ‘åœ¨ç»Ÿè®¡æ–¹æ³•ä¸­åº”ç”¨äº†æœ€å¤§ä¼¼ç„¶çš„æ¦‚å¿µï¼Œè¿™æ„å‘³ç€è¶…å‡ºÎ¼ 2ÏƒèŒƒå›´çš„å€¼è¢«æ ‡è®°ä¸ºå¼‚å¸¸å€¼ã€‚æ³¨æ„ï¼Œåœ¨æ­£æ€åˆ†å¸ƒçš„å‡è®¾ä¸‹ï¼ŒÎ¼ 2ÏƒåŒ…å« 95%çš„æ•°æ®ã€‚

```
**# Outlier detection**
up_b = df['UWC'].mean() + 2*df['UWC'].std()
low_b = df['UWC'].mean() - 2*df['UWC'].std()**# Replace outlier by interpolation for base consumption**
df.loc[df['UWC'] > up_b, 'UWC'] = np.nan
df.loc[df['UWC'] < low_b, 'UWC'] = np.nan
df['UWC'].interpolate(inplace=True)
```

## 2.3 å°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘å°†å‰ 80%çš„æ•°æ®è®¾ç½®ä¸ºè®­ç»ƒæ•°æ®ï¼Œå‰©ä¸‹çš„ 20%ä¸ºæµ‹è¯•æ•°æ®ã€‚æˆ‘ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒæ¨¡åž‹ï¼Œå¹¶ç”¨æµ‹è¯•æ•°æ®éªŒè¯å…¶æ€§èƒ½ã€‚

```
**# Split train data and test data**
train_size = int(len(df)*0.8)
train_dataset, test_dataset = df.iloc[:train_size],
df.iloc[train_size:]**# Plot train and test data**
plt.figure(figsize = (10, 6))
plt.plot(train_dataset.UWC)
plt.plot(test_dataset.UWC)
plt.xlabel('Time (day)')
plt.ylabel('Daily water consumption ($m^3$/capita.day)')
plt.legend(['Train set', 'Test set'], loc='upper right')print('Dimension of train data: ',train_dataset.shape)
print('Dimension of test data: ', test_dataset.shape)
```

![](img/5c7c85e937c8c7c8d292bf0de594e61d.png)![](img/f0db92de7e645aee915dbbd6bde926e7.png)

è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„é¢„å¤„ç†æ—¥ç”¨æ°´é‡æ—¶é—´åºåˆ—

## 2.4 æ‹†åˆ†ç›®æ ‡å˜é‡å’Œå› å˜é‡

UWC æ˜¯ç›®æ ‡å˜é‡(è¾“å‡º)ï¼Œæ˜¯å› å˜é‡(è¾“å…¥)çš„å‡½æ•°ï¼›Max_Tï¼ŒMin_T å’Œ T_Pã€‚

```
**# Split train data to X and y**
X_train = train_dataset.drop('UWC', axis = 1)
y_train = train_dataset.loc[:,['UWC']]**# Split test data to X and y**
X_test = test_dataset.drop('UWC', axis = 1)
y_test = test_dataset.loc[:,['UWC']]
```

## 2.5 æ•°æ®è½¬æ¢

ä¸€ä¸ªå¾ˆå¥½çš„ç»éªŒæ³•åˆ™æ˜¯ï¼Œè§„èŒƒåŒ–çš„æ•°æ®ä¼šåœ¨ç¥žç»ç½‘ç»œä¸­äº§ç”Ÿæ›´å¥½çš„æ€§èƒ½ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä½¿ç”¨æ¥è‡ª [scikit-learn](https://scikit-learn.org/stable/) çš„ [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) ã€‚

æˆ‘ä¸ºè¾“å…¥å’Œè¾“å‡ºå®šä¹‰äº†ä¸åŒçš„æ ‡é‡ï¼Œå› ä¸ºå®ƒä»¬æœ‰ä¸åŒçš„å½¢çŠ¶ã€‚è¿™å¯¹äºŽä½¿ç”¨**é€†å˜æ¢**åŠŸèƒ½å°¤å…¶é‡è¦ã€‚

*   X_train.shape: (1192ï¼Œ3)
*   y_train.shape: (1192ï¼Œ1)
*   X_test.shape: (299ï¼Œ3)
*   y_test.shape: (299ï¼Œ1)

åŠ¡å¿…ç¡®ä¿è¾“å‡ºçš„æ¯”ä¾‹åœ¨ 0â€“1 èŒƒå›´å†…ï¼Œä»¥åŒ¹é… LSTMã€GRU å’Œæ¯”å°”æ–¯ç‰¹å§†è¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°(tanh)çš„æ¯”ä¾‹ã€‚æ­¤å¤–ï¼Œè¾“å…¥å˜é‡æœ€å¥½æ˜¯å°å€¼ï¼Œå¯èƒ½åœ¨ 0-1 çš„èŒƒå›´å†…ã€‚

**ðŸ’¡****stepsâ“æœ‰å“ªäº›æ•°æ®è½¬æ¢**

*   ä½¿ç”¨å¯ç”¨çš„è®­ç»ƒæ•°æ®æ‹Ÿåˆå®šæ ‡å™¨(MinMaxScaler)(è¿™æ„å‘³ç€ä½¿ç”¨è®­ç»ƒæ•°æ®ä¼°è®¡æœ€å°å’Œæœ€å¤§å¯è§‚æµ‹å€¼ã€‚)
*   å°†ç¼©æ”¾å™¨åº”ç”¨äºŽè®­ç»ƒæ•°æ®
*   å°†å®šæ ‡å™¨åº”ç”¨äºŽæµ‹è¯•æ•°æ®

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨è®­ç»ƒæ•°æ®ä¸Šå®‰è£…çš„ç¼©æ”¾å™¨æ¥ç¼©æ”¾ä¸å¯è§çš„æ•°æ®ã€‚

```
**# Different scaler for input and output**
scaler_x = MinMaxScaler(feature_range = (0,1))
scaler_y = MinMaxScaler(feature_range = (0,1))**# Fit the scaler using available training data**
input_scaler = scaler_x.fit(X_train)
output_scaler = scaler_y.fit(y_train)**# Apply the scaler to training data**
train_y_norm = output_scaler.transform(y_train)
train_x_norm = input_scaler.transform(X_train)**# Apply the scaler to test data**
test_y_norm = output_scaler.transform(y_test)
test_x_norm = input_scaler.transform(X_test)
```

## 2.6 åˆ›å»º 3D è¾“å…¥æ•°æ®é›†

LSTMã€GRU å’Œæ¯”å°”æ–¯ç‰¹å§†é‡‡ç”¨ 3D è¾“å…¥(æ ·æœ¬æ•°ã€æ—¶é—´æ­¥æ•°ã€ç‰¹å¾æ•°)ã€‚å› æ­¤ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªåŠ©æ‰‹å‡½æ•°ï¼Œ *create_dataset* ï¼Œæ¥é‡å¡‘è¾“å…¥ã€‚

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘å®šä¹‰ time_steps = 30ã€‚è¿™æ„å‘³ç€æ¨¡åž‹åŸºäºŽæœ€è¿‘ 30 å¤©çš„æ•°æ®è¿›è¡Œé¢„æµ‹(åœ¨ for å¾ªçŽ¯çš„ç¬¬ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œè¾“å…¥æºå¸¦å‰ 30 å¤©ï¼Œè¾“å‡ºæ˜¯ç¬¬ 30 å¤©çš„ UWC)ã€‚

```
**# Create a 3D input**
def create_dataset (X, y, time_steps = 1):
    Xs, ys = [], []
    for i in range(len(X)-time_steps):
        v = X[i:i+time_steps, :]
        Xs.append(v)
        ys.append(y[i+time_steps])
    return np.array(Xs), np.array(ys)TIME_STEPS = 30X_test, y_test = create_dataset(test_x_norm, test_y_norm,   
                                TIME_STEPS)
X_train, y_train = create_dataset(train_x_norm, train_y_norm, 
                                  TIME_STEPS)
print('X_train.shape: ', X_test.shape)
print('y_train.shape: ', y_train.shape)
print('X_test.shape: ', X_test.shape)
print('y_test.shape: ', y_train.shape)
```

![](img/52cd204156b257c807699d2d8b8629e3.png)

# æ­¥éª¤ 3:åˆ›å»ºæ¯”å°”æ–¯ç‰¹å§†ã€LSTM å’Œ GRU æ¨¡åž‹

# 3.1 tensor flow ä¸­çš„æ¯”å°”æ–¯ç‰¹å§†ã€LSTM å’Œ GRU æ¨¡åž‹

ç¬¬ä¸€ä¸ªå‡½æ•°ï¼Œ *create_model_bilstm* ï¼Œåˆ›å»ºä¸€ä¸ª BDLSM å¹¶èŽ·å–éšè—å±‚ä¸­çš„å•å…ƒ(ç¥žç»å…ƒ)æ•°é‡ã€‚ç¬¬äºŒä¸ªå‡½æ•° *create_model* èŽ·å¾—ä¸¤ä¸ªè¾“å…¥ï¼›éšè—å±‚ä¸­çš„å•å…ƒæ•°é‡å’Œæ¨¡åž‹åç§°(LSTM æˆ– GRU)ã€‚

ä¸ºäº†ç®€å•èµ·è§ï¼Œæ¯”å°”æ–¯ç‰¹å§†ã€LSTM å’Œ GRU åœ¨è¾“å…¥å±‚æœ‰ 64 ä¸ªç¥žç»å…ƒï¼Œä¸€ä¸ªéšè—å±‚åŒ…æ‹¬ 64 ä¸ªç¥žç»å…ƒï¼Œåœ¨è¾“å‡ºå±‚æœ‰ 1 ä¸ªç¥žç»å…ƒã€‚

ä¸ºäº†ä½¿ LSTM å’Œ GRU æ¨¡åž‹å¯¹å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œä½¿ç”¨äº†**ä¸‹é™**å‡½æ•°ã€‚**æŽ‰çº¿(0.2)** éšæœºæŽ‰çº¿ 20%çš„å•ä½ã€‚

```
**# Create BiLSTM model**
def create_model_bilstm(units):
    model = Sequential()
    model.add(Bidirectional(LSTM(units = units,                             
              return_sequences=True),
              input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Bidirectional(LSTM(units = units)))
    model.add(Dense(1))
    **#Compile model**
    model.compile(loss='mse', optimizer='adam')
    return model**# Create LSTM or GRU model**
def create_model(units, m):
    model = Sequential()
    model.add(m (units = units, return_sequences = True,
                input_shape = [X_train.shape[1], X_train.shape[2]]))
    model.add(Dropout(0.2))
    model.add(m (units = units))
    model.add(Dropout(0.2))
    model.add(Dense(units = 1))
    **#Compile model**
    model.compile(loss='mse', optimizer='adam')
    return model**# BiLSTM**
model_bilstm = create_model_bilstm(64)**# GRU and LSTM**
model_gru = create_model(64, GRU)
model_lstm = create_model(64, LSTM)
```

# 3.2 æ‹Ÿåˆæ¨¡åž‹

æˆ‘ç”¨ 100 ä¸ª**æ—¶æœŸ**å’Œ**æ‰¹é‡** = 32 çš„è®­ç»ƒæ•°æ®è®­ç»ƒæ¨¡åž‹ã€‚æˆ‘è®©æ¨¡åž‹ä½¿ç”¨ 20%çš„è®­ç»ƒæ•°æ®ä½œä¸ºéªŒè¯æ•°æ®ã€‚æˆ‘è®¾ç½® **shuffle = False** æ˜¯å› ä¸ºå®ƒæä¾›äº†æ›´å¥½çš„æ€§èƒ½ã€‚

ä¸ºäº†é¿å…è¿‡åº¦æ‹Ÿåˆï¼Œæˆ‘è®¾ç½®äº†ä¸€ä¸ª*æå‰åœæ­¢*ï¼Œå½“*éªŒè¯æŸå¤±*åœ¨ 10 ä¸ªå‘¨æœŸåŽæ²¡æœ‰æ”¹å–„æ—¶(è€å¿ƒ= 10)åœæ­¢è®­ç»ƒã€‚

```
**# Fit BiLSTM, LSTM and GRU**
def fit_model(model):
    early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss',
                                               patience = 10)
    history = model.fit(X_train, y_train, epochs = 100,  
                        validation_split = 0.2, batch_size = 32, 
                        shuffle = False, callbacks = [early_stop])
    return historyhistory_bilstm = fit_model(model_bilstm)
history_lstm = fit_model(model_lstm)
history_gru = fit_model(model_gru)
```

## ç»˜åˆ¶åˆ—è½¦æŸå¤±å’ŒéªŒè¯æŸå¤±

åœ¨æ­¤å›¾ä¸­ï¼Œæˆ‘å°†æŸ¥çœ‹æ¯ä¸ªæ¨¡åž‹ä¸­çš„æ—¶æœŸæ•°ï¼Œå¹¶è¯„ä¼°æ¨¡åž‹åœ¨é¢„æµ‹ä¸­çš„æ€§èƒ½ã€‚

```
**# Plot train loss and validation loss**
def plot_loss (history):
    plt.figure(figsize = (10, 6))
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylabel('Loss')
    plt.xlabel('epoch')
    plt.legend(['Train loss', 'Validation loss'], loc='upper right')plot_loss (history_bilstm)
plot_loss (history_lstm)
plot_loss (history_gru)
```

![](img/da3ef85ca3f594f559a423965fd561a5.png)

BiLSTM çš„åˆ—è½¦æŸå¤±ä¸ŽéªŒè¯æŸå¤±

![](img/f959490a60a75a23ff51d54a7de00e75.png)

LSTM çš„åˆ—è½¦æŸå¤±ä¸ŽéªŒè¯æŸå¤±

![](img/85acbb4fb7404f7552ed8ead576fa385.png)

GRU çš„åˆ—è½¦æŸå¤±ä¸ŽéªŒè¯æŸå¤±

# 3.3 é€†å˜æ¢ç›®æ ‡å˜é‡

å»ºç«‹æ¨¡åž‹åŽï¼Œæˆ‘å¿…é¡»ä½¿ç”¨**scaler _ y . inverse _ transform**å°†ç›®æ ‡å˜é‡è½¬æ¢å›žåŽŸå§‹æ•°æ®ç©ºé—´ï¼Œç”¨äºŽè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚

```
y_test = scaler_y.inverse_transform(y_test)
y_train = scaler_y.inverse_transform(y_train)
```

# 3.4 ä½¿ç”¨æ¯”å°”æ–¯ç‰¹å§†ã€LSTM å’Œ GRU è¿›è¡Œé¢„æµ‹

åœ¨è¿™é‡Œï¼Œæˆ‘ç”¨æ¯”å°”æ–¯ç‰¹å§†ã€LSTM å’Œ GRU æ¨¡åž‹é¢„æµ‹ UWCã€‚ç„¶åŽï¼Œæˆ‘ç»˜åˆ¶äº†ä¸‰ä¸ªæ¨¡åž‹çš„çœŸå®žæœªæ¥(æµ‹è¯•æ•°æ®)ä¸Žé¢„æµ‹ã€‚

```
**# Make prediction**
def prediction(model):
    prediction = model.predict(X_test)
    prediction = scaler_y.inverse_transform(prediction)
    return predictionprediction_bilstm = prediction(model_bilstm)
prediction_lstm = prediction(model_lstm)
prediction_gru = prediction(model_gru)**# Plot true future vs prediction**
def plot_future(prediction, y_test):
    plt.figure(figsize=(10, 6))
    range_future = len(prediction)
    plt.plot(np.arange(range_future), np.array(y_test), 
             label='True Future')     
    plt.plot(np.arange(range_future),np.array(prediction),
            label='Prediction')
    plt.legend(loc='upper left')
    plt.xlabel('Time (day)')
    plt.ylabel('Daily water consumption ($m^3$/capita.day)')plot_future(prediction_bilstm, y_test)
plot_future(prediction_lstm, y_test)
plot_future(prediction_gru, y_test)
```

![](img/3420ce9b51eb5c05259f4487da176e75.png)

BiLSTM æ¨¡åž‹çš„çœŸå®žæœªæ¥ä¸Žæ—¥ç”¨æ°´é‡é¢„æµ‹

![](img/cb4f54f4b7c36cbf487cf6d86a54b7d6.png)

LSTM æ¨¡åž‹çš„çœŸå®žæœªæ¥ä¸Žæ—¥ç”¨æ°´é‡é¢„æµ‹

![](img/06d4303ea502575e3752a5a61e1246d0.png)

GRU æ¨¡åž‹çš„çœŸå®žæœªæ¥ä¸Žæ—¥ç”¨æ°´é‡é¢„æµ‹

# 3.5 è®¡ç®— RMSE å’Œæ¢…

è®©æˆ‘ç”¨ä¸¤ä¸ªæ‹Ÿåˆä¼˜åº¦æ¥è¯„ä¼°æ¨¡åž‹çš„æ€§èƒ½ã€‚

```
**# Define a function to calculate MAE and RMSE**
def evaluate_prediction(predictions, actual, model_name):
    errors = predictions - actual
    mse = np.square(errors).mean()
    rmse = np.sqrt(mse)
    mae = np.abs(errors).mean()print(model_name + ':')
    print('Mean Absolute Error: {:.4f}'.format(mae))
    print('Root Mean Square Error: {:.4f}'.format(rmse))
    print('')evaluate_prediction(prediction_bilstm, y_test, 'Bidirectional LSTM')
evaluate_prediction(prediction_lstm, y_test, 'LSTM')
evaluate_prediction(prediction_gru, y_test, 'GRU')
```

![](img/ad0757160024f3897e8a44f488ea57e1.png)

ä¸‰ä¸ªæ¨¡åž‹çš„æ‹Ÿåˆä¼˜åº¦è¡¨æ˜Žå®ƒä»¬å…·æœ‰éžå¸¸ç›¸ä¼¼çš„æ€§èƒ½ã€‚å³ä¾¿å¦‚æ­¤ï¼Œä¸Ž LSTM å’Œ GRU ç›¸æ¯”ï¼Œ **BiLSTM** æ¨¡åž‹å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä½¿ç”¨ BiLSTM æ¨¡åž‹å¯¹ UWC æœªæ¥ 10 å¹´è¿›è¡Œå¤šæ­¥é¢„æµ‹ã€‚

âš ï¸ **æ³¨æ„:**è¿™ä¸ªé¡¹ç›®çš„ç»“æžœå¹¶ä¸æ„å‘³ç€ BiLSTM æ¯” LSTM å’Œ GRU æœ‰æ›´å¥½çš„ç»“æžœã€‚è¿™åªæ˜¯è¯´æ˜Žå¦‚ä½•æ¯”è¾ƒè¿™äº›æ¨¡åž‹ï¼Œä»¥å¾—å‡ºæœ€å¯é çš„é¢„æµ‹ã€‚

# æ­¥éª¤ 4:10 å¹´ç”¨æ°´é‡çš„å¤šæ­¥é¢„æµ‹

æˆ‘åœ¨ç ”ç©¶åœ°ç‚¹å¯¼å…¥æ°”å€™æ•°æ®é¢„æµ‹ï¼Œå¹¶å¯¹ 2015 å¹´ 1 æœˆ 1 æ—¥è‡³ 2025 å¹´ 1 æœˆ 1 æ—¥æœŸé—´çš„æ•°æ®è¿›è¡Œè¿‡æ»¤ã€‚

![](img/fe22db8af5b618c23fefdcd594f31070.png)

æˆ‘åˆ›å»ºäº†ä¸€ä¸ªåŠ©æ‰‹å‡½æ•°ï¼Œ *plot_history_future* ï¼Œæ¥ç»˜åˆ¶åŽ†å²å’Œæœªæ¥çš„ UWCã€‚ç„¶åŽï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªå‡½æ•°ï¼Œ *forecastï¼Œ*æ¥é‡å¡‘çœ‹ä¸è§çš„è¾“å…¥ï¼Œå¹¶ä½¿ç”¨ LSTM æ¨¡åž‹è¿›è¡Œé¢„æµ‹ã€‚

```
**# Plot histoy and future data**
def plot_history_future(y_train, prediction):
    plt.figure(figsize=(10, 6)) range_history = len(y_train)
    range_future = list(range(range_history, range_history +
                   len(prediction))) plt.plot(np.arange(range_history), np.array(y_train), 
             label='History')
    plt.plot(range_future, np.array(prediction),label='Prediction')
    plt.legend(loc='upper right')
    plt.xlabel('Time (day)')
    plt.ylabel('Daily water consumption ($m^3$/capita.day)')**# Multi-step forecasting** 
def forecast(X_input, time_steps):
    **# Scale the unseen input with the scaler fitted on the train set**
    X = input_scaler.transform(X_input)
    **# Reshape unseen data to a 3D input**
    Xs = []
    for i in range(len(X) - time_steps):
        v = X[i:i+time_steps, :]
        Xs.append(v) X_transformed = np.array(Xs)**# Make prediction for unseen data using LSTM model**
    prediction = model_bilstm.predict(X_transformed)
    prediction_actual = scaler_y.inverse_transform(prediction)
    return prediction_actualprediction = forecast(X_new, TIME_STEPS)
plot_history_future(y_train, prediction)
```

![](img/a2aabfb65b4b7b671225b82281d07eac.png)

åŸºäºŽ BiLSTM æ¨¡åž‹çš„æ—¥ç”¨æ°´é‡åŽ†å²ä¸Žé¢„æµ‹

# ç»“è®º

æ„Ÿè°¢æ‚¨é˜…è¯»è¿™ç¯‡æ–‡ç« ã€‚æˆ‘çŸ¥é“è¿™æ˜¯ä¸€ä¸ªç›¸å½“é•¿çš„æ•™ç¨‹ðŸ˜æˆ‘å¸Œæœ›å®ƒèƒ½å¸®åŠ©ä½ åœ¨ Tensorflow ä¸­ä¸ºä¸€ä¸ªæ•°æ®ç§‘å­¦é¡¹ç›®å¼€å‘ LSTMã€GRU å’Œæ¯”å°”æ–¯ç‰¹å§†æ¨¡åž‹ðŸ˜Š

éžå¸¸æ„Ÿè°¢æ‚¨çš„åé¦ˆã€‚ä½ å¯ä»¥åœ¨ LinkedIn ä¸Šæ‰¾åˆ°æˆ‘ã€‚