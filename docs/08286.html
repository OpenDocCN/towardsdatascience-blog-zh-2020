<html>
<head>
<title>Bias Variance decomposition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">偏差方差分解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bias-variance-decomposition-d0e22d1506b1?source=collection_archive---------34-----------------------#2020-06-17">https://towardsdatascience.com/bias-variance-decomposition-d0e22d1506b1?source=collection_archive---------34-----------------------#2020-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="861f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用mlxtend库演示偏差和方差权衡的实际实现</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/cf77ba4a05dcdb222d49726840cdf3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U0paSCxEo0WLzI-vNlfJrQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">迈克·冯在<a class="ae kv" href="https://unsplash.com/photos/RvM_tS_C7sE" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1a95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们经常想知道如何从一堆机器学习方法中选择一种方法，为给定的数据集提供最好的结果。</p><blockquote class="ls lt lu"><p id="eb07" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">“为特定问题陈述选择具有适当复杂性的最佳模型的过程称为模型选择”</p></blockquote><p id="d1ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这给我们带来了统计学习方法的一个非常重要的属性，称为偏差方差权衡，它强调模型在训练数据集中学习关联的程度。</p><p id="26ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将讨论什么是偏差和方差，以及如何减少它们。最后，我们将实现几个实际的例子来看看这些概念是如何应用于模型构建的。</p><p id="c5e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么，我们开始吧。</p><p id="c839" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于偏离实际真相，预测中有3种类型的误差:</p><ol class=""><li id="e8f2" class="lz ma iq ky b kz la lc ld lf mb lj mc ln md lr me mf mg mh bi translated"><strong class="ky ir">不可约误差</strong></li><li id="80a5" class="lz ma iq ky b kz mi lc mj lf mk lj ml ln mm lr me mf mg mh bi translated"><strong class="ky ir">偏置</strong></li><li id="578c" class="lz ma iq ky b kz mi lc mj lf mk lj ml ln mm lr me mf mg mh bi translated"><strong class="ky ir">差异</strong></li></ol><p id="a409" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不可约误差是对数据中固有噪声的度量。可能总会有一些预测因素对目标变量有一些小的影响，并且不是我们模型的一部分。</p><p id="f379" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，无论我们选择多好的模型，它都不能完美地逼近实际函数，从而留下无法减少的误差。</p><p id="b075" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，让我们关注我们在构建模型时可以控制的内容，即偏差和方差以及<a class="ae kv" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" rel="noopener ugc nofollow" target="_blank">如何控制</a>。</p><p id="e55b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为此，我们将使用' mlxtend '库(由Sebastian Raschka开发)来计算<a class="ae kv" href="http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/" rel="noopener ugc nofollow" target="_blank"> 'bias_variance_decomp' </a>。</p><h2 id="877b" class="mn mo iq bd mp mq mr dn ms mt mu dp mv lf mw mx my lj mz na nb ln nc nd ne nf bi translated"><strong class="ak">偏差:</strong></h2><p id="0766" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">偏差是指当模型学习的近似函数对于非常复杂的问题来说微不足道时的误差，从而忽略了预测器和目标之间的结构关系。</p><p id="467c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">高偏差导致拟合不足和较高的训练误差。可以通过向模型增加更多信号来减少，即，如果我们可以添加更好地描述与目标变量关联的特征。</p><p id="b24f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看下面的例子:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="d9db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们假设好像所有的特性都不能立刻可用，并按照其贡献的顺序(来自df_feat)一次引入一个特性。从下面的图表可以明显看出，以较高的方差为代价，偏差显著下降</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/8867da7af5ce29b0eb9be4dc2a7068dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*O9TnREqgBJS5yE6BW_6ruw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">添加功能以降低偏差</p></figure><p id="c8ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是如果我们在模型中已经有了全面的特征，也就是说，没有从商业知识中引入额外的特征，但是偏差仍然很高，该怎么办呢？在这种情况下，我们需要改进建模空间，即使用能够更好地理解数据中结构细微差别的算法。</p><h2 id="688f" class="mn mo iq bd mp mq mr dn ms mt mu dp mv lf mw mx my lj mz na nb ln nc nd ne nf bi translated"><strong class="ak">差异:</strong></h2><p id="0d5a" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">方差是模型学习的近似函数在不同训练集之间差异很大的程度。高方差导致过度拟合</p><p id="0a51" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当模型的复杂程度增加时，它开始通过学习微小的波动来拟合数据，甚至是随机噪声的程度。</p><p id="f959" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于模型在训练数据中学习到一些偶然存在的模式，而这些模式并不是数据的真实属性，因此在测试数据中可能不存在类似的模式。这导致较低的训练误差，但是非常高的测试集误差</p><p id="987e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正则化方法通常用于控制方差。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="dd24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上述示例中，我们将lasso模型(正则化参数设置为0.05)中的方差与线性回归中的方差进行了比较，观察到方差减少了7.5%</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="43f0" class="mn mo iq np b gy nt nu l nv nw">variance Reduction: -7.53%<br/>At the expense of introducing bias: 5.87%</span></pre><p id="8d2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Decision_tree_pruning#:~:text=Pruning%20is%20a%20technique%20in,by%20the%20reduction%20of%20overfitting." rel="noopener ugc nofollow" target="_blank">剪枝</a>常用于正则化决策树。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="7c31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们观察到通过修剪树，方差减少了62 %</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="4458" class="mn mo iq np b gy nt nu l nv nw">variance Reduction: -62.01%<br/>At the expense of introducing bias: 118.32%</span></pre><p id="d762" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看当我们在<em class="lv"> K最近邻</em>算法中增加邻居数量时，偏差和方差会发生什么变化</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="85c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于越来越多的邻居决定输出，这导致了具有更低方差和更高偏差的更简单的模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/646505434e62ddde992a531bd2821538.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*KqBOmJtm_dBxllIt6SHsQQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">高k值导致KNN中的高偏差和低方差</p></figure><h2 id="9e60" class="mn mo iq bd mp mq mr dn ms mt mu dp mv lf mw mx my lj mz na nb ln nc nd ne nf bi translated"><strong class="ak">权衡:</strong></h2><p id="e2d6" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">我们看到，为了减少一个误差，我们最终增加了另一个误差，这是不可取的。那么，我们如何减少偏差和方差。是否可以实现？</p><p id="8223" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">答案是肯定的！</p><p id="4558" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两个误差项不以线性方式变化；因此，预测误差取决于两者的相对变化率。</p><p id="161f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">集成和交叉验证是克服偏差方差困境的常用方法。</p><p id="fb0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来看看，与单一决策树相比，随机森林如何在不增加太多偏差的情况下减少82%的方差</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="5724" class="mn mo iq np b gy nt nu l nv nw">variance Reduction: -82.29%<br/>At the expense of introducing bias: 10.73%</span></pre><p id="0e6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Jupyter笔记本可以在<a class="ae kv" href="https://github.com/vidhi-am/bias-variance-trade-off/blob/master/Bias%20Variance_decomp.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div></div>    
</body>
</html>