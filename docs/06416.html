<html>
<head>
<title>ICLR 2020: NLP Highlights</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ICLR 2020: NLP亮点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/iclr-2020-nlp-highlights-511deb99b967?source=collection_archive---------25-----------------------#2020-05-22">https://towardsdatascience.com/iclr-2020-nlp-highlights-511deb99b967?source=collection_archive---------25-----------------------#2020-05-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6ecb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">ICLR 2020概述，重点关注自然语言处理(NLP)，尤其是变形金刚和BERT变体。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3f02cdbc6db7f93255f470c244dc8de7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3GNqe9FgVl9r8cP108FlgA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="cb7e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章是我关于ICLR 2020的一些亮点。因为我目前的研究是自然语言处理(NLP)，所以这篇文章将重点关注这一领域。但是，计算机视觉、量子启发算法、通用深度学习方法等。也会提到。</p><p id="12b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这也是我在Medium上的第一篇帖子，所以我也很乐意联系并听取社区的反馈！</p><h1 id="e19e" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">1.关于ICLR 2020</h1><p id="924c" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">从主网站<a class="ae mr" href="https://iclr.cc/" rel="noopener ugc nofollow" target="_blank">https://iclr.cc/</a>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/2c49dc0caeb383522584d06c995cac48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*HW2ju8hJO1fj5Mt5gasIOQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">https://iclr.cc/ ICLR标志(来源:<a class="ae mr" href="https://iclr.cc/" rel="noopener ugc nofollow" target="_blank"/>)</p></figure><p id="e4a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">学习表征国际会议(ICLR) </strong>是专业人士的顶级聚会，致力于促进人工智能的表征学习分支，但通常被称为深度学习</p><p id="a5fe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">ICLR是顶级的国际人工智能会议之一，与NIPS和ICML齐名。其中一个区别是，ICLR的主要关注点是深度学习。</p><p id="aefd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">参与者可以来自许多背景，“从 <strong class="la iu">学术和工业研究人员，到企业家和工程师，到研究生和博士后。”</strong></p><p id="e7eb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">审查过程是双盲和开放式审查，即作者和审查者在审查过程中互不认识，所有审查和作者的回应都可以公开查看。</p><p id="4e4d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae mr" href="https://iclr.cc/virtual_2020/index.html" rel="noopener ugc nofollow" target="_blank">所有资料均可在ICLR虚拟页面上访问</a></p><p id="9ff1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae mr" href="https://paperswithcode.com/conference/iclr-2020-1" rel="noopener ugc nofollow" target="_blank">带有ICLR 2020代码的文件</a></p><h1 id="1ae7" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">2.技术趋势</h1><p id="86d3" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我的主要兴趣是自然语言处理(NLP)，所以我将把重点放在这个领域。</p><p id="2c73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">答:热门话题:</strong></p><ul class=""><li id="8f62" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated"><strong class="la iu">变形金刚和预先训练好的语言模型仍然是非常热门的话题。</strong>自从“注意力是你所需要的全部”(<a class="ae mr" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Vaswani等人，2017 </a>)和“BERT:用于语言理解的深度双向变压器的预训练”(<a class="ae mr" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> Devlin等人，2018 </a>)在各种NLP任务上取得了最先进的(SOTA)结果后，研究人员继续深入研究变压器的架构和预训练的语言模型。</li><li id="557d" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">论文可以放在<strong class="la iu">理论证明方</strong> ( <a class="ae mr" href="https://arxiv.org/abs/2002.06622" rel="noopener ugc nofollow" target="_blank">石等，2020</a>；<a class="ae mr" href="https://arxiv.org/abs/1908.04211" rel="noopener ugc nofollow" target="_blank">布鲁纳等人，2020</a>；<a class="ae mr" href="https://arxiv.org/abs/1912.10077" rel="noopener ugc nofollow" target="_blank">云等，2019</a>；<a class="ae mr" href="https://arxiv.org/abs/1911.03584" rel="noopener ugc nofollow" target="_blank"> Cordonnier等人，2019 </a>。</li><li id="486b" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">有论文关注<strong class="la iu">提高变形金刚和预训练语言模型的任务绩效</strong>(<a class="ae mr" href="https://arxiv.org/abs/1908.04577" rel="noopener ugc nofollow" target="_blank">王等2019</a>；<a class="ae mr" href="https://arxiv.org/abs/1909.11299" rel="noopener ugc nofollow" target="_blank">李等人，2019 </a>。</li><li id="5333" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">其他一些论文关注的是<strong class="la iu">缩小模型的</strong>或者<strong class="la iu">减少训练的</strong>时间(<a class="ae mr" href="https://arxiv.org/abs/2004.11886" rel="noopener ugc nofollow" target="_blank">吴等，2020</a>；<a class="ae mr" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank">兰等，2019</a>；<a class="ae mr" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">基塔耶夫等人，2020</a>；<a class="ae mr" href="https://arxiv.org/abs/2003.10555" rel="noopener ugc nofollow" target="_blank">克拉克等人，2020</a>；<a class="ae mr" href="https://arxiv.org/abs/1911.05507" rel="noopener ugc nofollow" target="_blank"> Rae等人，2019</a>；<a class="ae mr" href="https://arxiv.org/abs/1909.11556" rel="noopener ugc nofollow" target="_blank">范等，2019</a>；<a class="ae mr" href="https://arxiv.org/abs/1904.00962" rel="noopener ugc nofollow" target="_blank">尤等，2019 </a>)。</li><li id="af9e" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">传统的<strong class="la iu"> LSTM，RNN </strong>也得到了更深入的发掘(<a class="ae mr" href="https://arxiv.org/abs/1909.01792" rel="noopener ugc nofollow" target="_blank"> Melis et al .，2019</a>；<a class="ae mr" href="https://arxiv.org/abs/1905.13715" rel="noopener ugc nofollow" target="_blank">奥尔罕等人，2019</a>；<a class="ae mr" href="https://openreview.net/forum?id=rkgg6xBYDH" rel="noopener ugc nofollow" target="_blank">涂等著2019 </a></li></ul><p id="d5e4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">NLP的许多其他主题我在这里没有涉及，所以请到网站来阅读和发现这些主题。</p><p id="813f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> b .新出现的议题:</strong></p><p id="9588" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我发现会议中出现了几个话题:</p><ul class=""><li id="69fe" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated"><strong class="la iu">多语言/跨语言任务的语言架构和模型</strong> ( <a class="ae mr" href="https://arxiv.org/abs/1912.07840" rel="noopener ugc nofollow" target="_blank"> Karthikeyan等人，2019</a>；<a class="ae mr" href="https://openreview.net/forum?id=HyeYTgrFPB" rel="noopener ugc nofollow" target="_blank">Berend 2020</a>；<a class="ae mr" href="https://arxiv.org/abs/2002.03518" rel="noopener ugc nofollow" target="_blank">曹等人，2020</a>；<a class="ae mr" href="https://arxiv.org/abs/1910.04708" rel="noopener ugc nofollow" target="_blank">王等，2019 </a></li><li id="dc18" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><strong class="la iu">用于计算机视觉和NLP的量子启发算法</strong>(<a class="ae mr" href="https://arxiv.org/abs/1911.04975" rel="noopener ugc nofollow" target="_blank">Panahi等人，2019</a>；<a class="ae mr" href="https://arxiv.org/abs/1911.01117" rel="noopener ugc nofollow" target="_blank"> Kerenidis等人，2019 </a></li><li id="71f4" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><strong class="la iu">多模态模型</strong> ( <a class="ae mr" href="https://arxiv.org/abs/1908.08530" rel="noopener ugc nofollow" target="_blank">苏等，2019 </a>)</li><li id="3b66" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><strong class="la iu">强化学习与NLP </strong>跨学科(<a class="ae mr" href="https://arxiv.org/abs/1906.02768" rel="noopener ugc nofollow" target="_blank">于等，2019</a>；<a class="ae mr" href="https://arxiv.org/abs/1909.00668" rel="noopener ugc nofollow" target="_blank">克里夫特等人，2019 </a></li><li id="668b" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated"><strong class="la iu">模型压缩/修剪</strong> ( <a class="ae mr" href="https://arxiv.org/abs/1912.00120" rel="noopener ugc nofollow" target="_blank">张等，2019 </a>)</li></ul><h1 id="b3ee" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">3.思想</h1><p id="18f7" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">今年是ICLR第一次以虚拟方式举行，从我的角度来看，组织者做了出色的工作:接受论文的搜索引擎，论文相似性的可视化，作者和参与者的聊天论坛，缩放室等。</p><p id="85df" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我有机会直接询问许多作者，也参与了NLP和Huggingface(一个流行的NLP框架)研究人员的公共论坛。我也有机会参加公司的赞助商聊天。</p><p id="8a08" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Yann LeCun和Yoshua Bengio关于深度学习方向的最后一次演讲也非常有见地，例如如何将NLP中的自我监督学习成功应用于计算机视觉，他们如何看待量子算法，神经科学思想也将是重要的。他们如何走到今天的故事也很鼓舞人心。</p><p id="828f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在<a class="ae mr" href="https://venturebeat.com/2020/05/02/yann-lecun-and-yoshua-bengio-self-supervised-learning-is-the-key-to-human-level-intelligence/" rel="noopener ugc nofollow" target="_blank"> VentureBeat </a>上读到其中一篇总结。</p><h1 id="a52a" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">4.详情:精选论文集锦:</h1><p id="e662" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated"><a class="ae mr" href="https://openreview.net/forum?id=HJeT3yrtDr" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">【多语伯特跨语言能力实证研究】</strong></a></p><p id="0e96" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">多语言模型的一个流行假设是，由于语言之间的词汇重叠，它们学习得很好。在本文中，作者发现“语言之间的词汇重叠在跨语言成功中起着微不足道的作用，而网络的深度是其中不可或缺的一部分。”(Karthikeyan等人，2020年)</p><p id="4239" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae mr" href="https://openreview.net/forum?id=r1xMH1BtvB" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> ELECTRA:预训练文本编码器作为鉴别器而不是生成器</strong> </a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/bbad2f00872a6730be64569e4104573e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b6E9zpDovRX9jH8JqvpYQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae mr" href="https://openreview.net/forum?id=r1xMH1BtvB" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ni"> ELECTRA:预训练文本编码器作为鉴别器而不是生成器</strong> </a></p></figure><p id="fa37" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇论文为计算资源少的实体预先训练他们自己的语言模型打开了大门。特别是，作者“在一个GPU上训练了一个模型4天，在GLUE自然语言理解基准测试中，它的表现超过了GPT(使用30倍以上的计算进行训练)。”(克拉克等人，2020年)</p><ul class=""><li id="415c" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated">代码可在<a class="ae mr" href="https://github.com/google-research/electra" rel="noopener ugc nofollow" target="_blank"> Github </a>上获得。</li><li id="cf8e" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">ELECTRA型号可用于<a class="ae mr" href="https://huggingface.co/transformers/model_doc/electra.html" rel="noopener ugc nofollow" target="_blank">支撑面</a>。</li><li id="98b2" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">在<a class="ae mr" href="https://github.com/nguyenvulebinh/vietnamese-electra" rel="noopener ugc nofollow" target="_blank"> Github </a>上有一个针对越南人的培训前电子小程序项目</li></ul><p id="6263" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae mr" href="https://openreview.net/forum?id=HJlnC1rKPB" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">关于自我注意与卷积层的关系</strong> </a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/af532e98c7a19a9b07c42889eb7b2233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ceruWGP_24p57SRcC6Wj4Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae mr" href="https://openreview.net/forum?id=HJlnC1rKPB" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ni">关于自我注意与卷积层的关系</strong> </a></p></figure><p id="e812" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在NLP任务中成功地用基于注意力的模型取代了RNN之后，研究人员现在正在探索他们是否可以在卷积层和计算机视觉任务中做同样的事情。在这篇论文中，作者“证明了具有足够数量的头的多头自我关注层至少与任何卷积层一样具有表达能力。我们的数值实验表明，自我关注层与CNN层一样关注像素网格模式，这证实了我们的分析。(Cordornnier等人，2019年)</p><ul class=""><li id="72a4" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated">该代码可在<a class="ae mr" href="https://github.com/epfml/attention-cnn" rel="noopener ugc nofollow" target="_blank"> Github </a>上获得</li></ul><p id="032c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae mr" href="https://openreview.net/forum?id=BJgQ4lSFPH" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> StructBERT:将语言结构融入深度语言理解的前期训练</strong> </a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/18f3ed6e8f50f025fbbb713edd80a329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FUMbRgBicc4KfdKJriNq4A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae mr" href="https://openreview.net/forum?id=BJgQ4lSFPH" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ni"> StructBERT:将语言结构融入深度语言理解的前期训练</strong> </a></p></figure><p id="92be" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，作者添加了两个额外的预训练任务，这些任务“在单词和句子级别利用语言结构”，并在GLUE benchmark中取得了SOTA结果。(王等，2019)</p><p id="9fa3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae mr" href="https://openreview.net/forum?id=ByxRM0Ntvr" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">变压器是序列间函数的通用近似器吗？</strong> </a></p><p id="dd1b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文更多的是在理论方面。在这篇论文中，作者“证明了变换器模型可以普遍地逼近紧域上任意连续的序列间函数。”他们还证明了“固定宽度的自我注意层可以计算输入序列的上下文映射，在变压器的通用近似属性中起着关键作用。”(云等，2019)</p><h1 id="b9fb" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">关于我</h1><p id="ecbd" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我是一名AI工程师和数据工程师，专注于研究最先进的AI解决方案和构建机器学习系统。你可以在LinkedIn上找到我。</p><h1 id="3603" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">参考</h1><p id="94c9" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">瓦斯瓦尼，a。新泽西州沙泽尔；新泽西州帕马尔；Uszkoreit，j；琼斯湖；戈麦斯。凯撒大帝。；和Polosukhin，I. 2017。你需要的只是关注。神经信息处理系统进展，5998–6008。</p><p id="e19d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">德夫林，j。张明伟；李，k。以及图塔诺瓦，K. 2018。Bert:用于语言理解的深度双向转换器的预训练。arXiv预印本arXiv:1810.04805。</p><p id="c364" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">变压器是序列到序列函数的通用近似器吗？."<em class="nl"> arXiv预印本arXiv:1912.10077 </em> (2019)。</p><p id="f246" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">《论变形金刚中的可识别性》(2020).</p><p id="31bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">石，，等，“变压器的鲁棒性验证”arXiv预印本arXiv:2002.06622  (2020)。</p><p id="fd3b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">科多尼尔、让·巴普蒂斯特、安德里亚斯·洛卡斯和马丁·贾吉。"自我注意和卷积层的关系."<em class="nl"> arXiv预印本arXiv:1911.03584 </em> (2019)。</p><p id="816e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">把语言结构融入深层语言理解的前训练。<em class="nl"> arXiv预印本arXiv:1908.04577 </em> (2019)。</p><p id="0539" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Lee，Cheolhyoung，Kyunghyun Cho和Wanmo Kang。"混合:微调大规模预训练语言模型的有效正则化."<em class="nl"> arXiv预印本arXiv:1909.11299 </em> (2019)。</p><p id="a2fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">吴，，等。〈长短距注意的简装变压器〉。arXiv预印本:2004.11886  (2020)。</p><p id="2b7a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">兰，等译，《阿尔伯特:一个用于自我监督学习语言表征的简易伯特》<em class="nl"> arXiv预印本arXiv:1909.11942 </em> (2019)。</p><p id="4e44" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尤，杨，等《面向深度学习的大批量优化:76分钟训练bert》国际学习代表会议。2019.</p><p id="c2c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基塔耶夫、尼基塔、祖卡斯·凯泽和安塞姆·列夫斯卡娅。"改革家:高效的变压器."<em class="nl"> arXiv预印本arXiv:2001.04451 </em> (2020)。</p><p id="61f5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">伊莱克特:预先训练文本编码器作为鉴别器而不是生成器<em class="nl"> arXiv预印本arXiv:2003.10555 </em> (2020)。</p><p id="aafc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">《用于长程序列模拟的压缩变压器》<em class="nl"> arXiv预印本arXiv:1911.05507 </em> (2019)。</p><p id="4541" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">范、安吉拉、爱德华·格雷夫和阿曼德·朱林。"通过结构化压差按需降低变压器深度."<em class="nl"> arXiv预印本arXiv:1909.11556 </em> (2019)。</p><p id="5f70" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Lee，Cheolhyoung，Kyunghyun Cho和Wanmo Kang。"混合:微调大规模预训练语言模型的有效正则化."<em class="nl"> arXiv预印本arXiv:1909.11299 </em> (2019)。</p><p id="547a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">梅利斯、加博尔、托马什·科奇斯克和菲尔·布伦松。"莫格里弗·伊斯特姆"<em class="nl"> arXiv预印本arXiv:1909.01792 </em> (2019)。</p><p id="a963" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Orhan、A. Emin和Xaq Pitkow。“用顺序非正常动力学改进循环神经网络的记忆。”<em class="nl"> arXiv预印本arXiv:1905.13715 </em> (2019)。</p><p id="f3ec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">涂、、何凤翔、陶大成。"理解递归神经网络中的泛化."<em class="nl">国际学习代表会议</em>。2019</p><p id="e264" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">多语种伯特的跨语言能力:实证研究。<em class="nl">国际学习代表会议</em>。2020.</p><p id="a328" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">加博尔·贝伦德。"大规模多语言稀疏单词表示."(2020年):Azonosító-2582。</p><p id="cdae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">曹、史蒂文、尼基塔·基塔耶夫和丹·克莱因。"上下文单词表示的多语言对齐."arXiv预印本:2002.03518  (2020)。</p><p id="4f17" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">王，子瑞，等。“跨语言对齐与联合训练:一个比较研究和一个简单的统一框架。”<em class="nl"> arXiv预印本arXiv:1910.04708 </em> (2019)。</p><p id="9157" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">帕纳西、阿里阿克巴、塞兰·萨伊迪和汤姆·阿罗兹。" word2ket:受量子纠缠启发的节省空间的单词嵌入."<em class="nl"> arXiv预印本arXiv:1911.04975 </em> (2019)。</p><p id="f8c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">克伦尼迪斯，约尔达尼，乔纳斯·兰德曼和阿努帕姆·普拉卡什。"深度卷积神经网络的量子算法."<em class="nl"> arXiv预印本arXiv:1911.01117 </em> (2019)。</p><p id="6f8d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">苏，，等。〈视觉语言表征的预训练〉。<em class="nl"> arXiv预印本arXiv:1908.08530 </em> (2019)。</p><p id="72f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">陈、于、吴和穆罕默德扎基。"基于强化学习的自然问句生成图序列模型."<em class="nl"> arXiv预印本arXiv:1908.04942 </em> (2019)。</p><p id="aaef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">《逻辑和$ 2 $-单纯变压器》<em class="nl"> arXiv预印本arXiv:1909.00668 </em> (2019)。</p><p id="ef2f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">张，马修·顺时，和布拉德利·斯塔迪。"通过雅可比谱评估对递归神经网络进行一次性剪枝."<em class="nl"> arXiv预印本arXiv:1912.00120 </em> (2019)。</p><p id="f8a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">余，郝楠，等。〈用奖励和多种语言玩彩票:RL和NLP中的彩票〉<em class="nl"> arXiv预印本arXiv:1906.02768 </em> (2019)。</p></div></div>    
</body>
</html>