<html>
<head>
<title>Machine Learning with Python: Classification (complete tutorial)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python进行机器学习:分类(完整教程)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec?source=collection_archive---------0-----------------------#2020-05-11">https://towardsdatascience.com/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec?source=collection_archive---------0-----------------------#2020-05-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bf7a59d41b19f964ce71b34753c515bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgqxDMP5qD1HE_uM33zZrg.png"/></div></div></figure><div class=""/><div class=""><h2 id="4761" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">数据分析和可视化、特征工程和选择、模型设计和测试、评估和解释</h2></div></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><h2 id="8ce8" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">摘要</h2><p id="876c" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">在本文中，我将使用数据科学和Python解释分类用例的主要步骤，从数据分析到理解模型输出。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mp"><img src="../Images/778b5ac9c09c7591b4d2a2cc3fd1efd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gS5PdcX1sk1yQgYQnRSGcg.png"/></div></div></figure><p id="4d6a" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">由于本教程对于初学者来说是一个很好的起点，我将使用著名的Kaggle竞赛中的“<strong class="ly jf"> Titanic dataset </strong>”，其中为您提供了乘客数据，任务是建立一个预测模型来回答这个问题:“什么样的人更有可能幸存？”(下面链接)。</p><div class="is it gp gr iu mz"><a href="https://www.kaggle.com/c/titanic/overview" rel="noopener  ugc nofollow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">泰坦尼克号:机器从灾难中学习</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">从这里开始！预测泰坦尼克号上的生存并熟悉ML基础知识</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">www.kaggle.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ja mz"/></div></div></a></div><p id="5bf2" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我将展示一些有用的Python代码，这些代码可以很容易地用于其他类似的情况(只需复制、粘贴、运行)，并通过注释遍历每一行代码，这样您就可以很容易地复制这个示例(下面是完整代码的链接)。</p><div class="is it gp gr iu mz"><a href="https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/machine_learning/example_classification.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">mdipietro 09/data science _人工智能_实用工具</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">permalink dissolve GitHub是4000多万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">github.com</p></div></div><div class="ni l"><div class="no l nk nl nm ni nn ja mz"/></div></div></a></div><p id="fd93" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">特别是，我将经历:</p><ul class=""><li id="6a75" class="np nq je ly b lz mu mc mv lj nr ln ns lr nt mo nu nv nw nx bi translated">环境设置:导入库并读取数据</li><li id="bd95" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">数据分析:理解变量的意义和预测能力</li><li id="d836" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">特征工程:从原始数据中提取特征</li><li id="39c1" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">预处理:数据划分、处理缺失值、编码分类变量、缩放</li><li id="5527" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">特征选择:只保留最相关的变量</li><li id="bc1a" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">模型设计:训练、调整超参数、验证、测试</li><li id="7e0b" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">绩效评估:阅读指标</li><li id="b83c" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">可解释性:理解模型如何产生结果</li></ul></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><h2 id="4dc8" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">设置</h2><p id="6aa4" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">首先，我需要导入以下库。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="2b7a" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## for data</strong><br/>import <strong class="oe jf">pandas </strong>as pd<br/>import <strong class="oe jf">numpy </strong>as np</span><span id="042c" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## for plotting</strong><br/>import <strong class="oe jf">matplotlib</strong>.pyplot as plt<br/>import <strong class="oe jf">seaborn </strong>as sns</span><span id="55ef" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## for statistical tests</strong><br/>import <strong class="oe jf">scipy</strong><br/>import <strong class="oe jf">statsmodels</strong>.formula.api as smf<br/>import statsmodels.api as sm</span><span id="2214" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## for machine learning</strong><br/>from <strong class="oe jf">sklearn </strong>import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition</span><span id="c8fd" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## for explainer</strong><br/>from <strong class="oe jf">lime </strong>import lime_tabular</span></pre><p id="f5f9" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">然后我会把数据读入熊猫数据框。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="846c" class="la lb je oe b gy oi oj l ok ol">dtf = pd.read_csv('data_titanic.csv')<br/>dtf.head()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/2246b29c51699cd2d93e6b8094363655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ur-xUqrNt4qoHV8NnLUeXQ.png"/></div></div></figure><p id="f5df" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">有关列的详细信息可以在所提供的数据集链接中找到。</p><p id="60d4" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">请注意，表格的每一行代表一个特定的乘客(或观察)。如果您正在处理一个不同的数据集，它没有这样的结构，其中每一行代表一个观察，那么您需要汇总数据并转换它。</p><p id="c7c3" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">现在一切都设置好了，我将从分析数据开始，然后选择特征，建立机器学习模型并进行预测。</p><p id="ddea" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我们开始吧，好吗？</p><h2 id="76de" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">数据分析</h2><p id="b497" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">在统计学中，<a class="ae oo" href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" rel="noopener ugc nofollow" target="_blank">探索性数据分析</a>是对数据集的主要特征进行总结的过程，以了解数据在正式建模或假设检验任务之外还能告诉我们什么。</p><p id="acd6" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我总是从获得整个数据集的概述开始，特别是我想知道有多少<strong class="ly jf">分类</strong>和<strong class="ly jf">数值</strong>变量，以及<strong class="ly jf">缺失数据</strong>的比例。识别变量的类型有时会很棘手，因为类别可以用数字表示(Su <em class="op"> rvived c </em>列由1和0组成)。为此，我将编写一个简单的函数来完成这项工作:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="f51c" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">'''<br/>Recognize whether a column is numerical or categorical.<br/>:parameter<br/>    :param dtf: dataframe - input data<br/>    :param col: str - name of the column to analyze<br/>    :param max_cat: num - max number of unique values to recognize a column as categorical<br/>:return<br/>    "cat" if the column is categorical or "num" otherwise<br/>'''</strong><br/>def <strong class="oe jf">utils_recognize_type</strong>(dtf, col, max_cat=20):<br/>    if (dtf[col].dtype == "O") | (dtf[col].nunique() &lt; max_cat):<br/>        return <strong class="oe jf">"cat"</strong><br/>    else:<br/>        return <strong class="oe jf">"num"</strong></span></pre><p id="ad1a" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">这个功能非常有用，可以用在很多场合。为了举例说明，我将绘制dataframe的<a class="ae oo" href="http://Heat map" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">热图</strong> </a> <strong class="ly jf"> </strong>，以可视化列类型和缺失的数据。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="0200" class="la lb je oe b gy oi oj l ok ol">dic_cols = {col:<strong class="oe jf">utils_recognize_type</strong>(dtf, col, max_cat=20) for col in dtf.columns}</span><span id="3a0b" class="la lb je oe b gy om oj l ok ol">heatmap = dtf.isnull()<br/>for k,v in dic_cols.items():<br/> if v == "num":<br/>   heatmap[k] = heatmap[k].apply(lambda x: 0.5 if x is False else 1)<br/> else:<br/>   heatmap[k] = heatmap[k].apply(lambda x: 0 if x is False else 1)</span><span id="b6fd" class="la lb je oe b gy om oj l ok ol">sns.<strong class="oe jf">heatmap</strong>(heatmap, cbar=False).set_title('Dataset Overview')<br/>plt.show()</span><span id="12a0" class="la lb je oe b gy om oj l ok ol">print("\033[1;37;40m Categerocial ", "\033[1;30;41m Numeric ", "\033[1;30;47m NaN ")</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/67b365ca697f7a83f958d1d74e0e5ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YNeaA2mB5kuXkn80Yb8vpQ.png"/></div></div></figure><p id="5801" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">有885行和12列:</p><ul class=""><li id="f0a0" class="np nq je ly b lz mu mc mv lj nr ln ns lr nt mo nu nv nw nx bi translated">表中的每一行都代表一个由<em class="op"> PassengerId </em>标识的特定乘客(或观察)，所以我将它设置为index(或SQL爱好者的表的<a class="ae oo" href="https://en.wikipedia.org/wiki/Primary_key" rel="noopener ugc nofollow" target="_blank">主键</a>)。</li><li id="1eb1" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated"><em class="op">幸存的</em>是我们想要理解和预测的现象(或目标变量)，所以我将该列重命名为“<em class="op">Y”</em>。它包含两个类别:如果乘客幸存，则为1，否则为0，因此这个用例是一个二元分类问题。</li><li id="7a54" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated"><em class="op">年龄</em>和<em class="op">费用</em>是数字变量，而其他是分类变量。</li><li id="0e93" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">只有<em class="op">年龄</em>和<em class="op">舱室</em>包含缺失数据。</li></ul><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="ecac" class="la lb je oe b gy oi oj l ok ol">dtf = dtf.set_index("<strong class="oe jf">PassengerId</strong>")</span><span id="5934" class="la lb je oe b gy om oj l ok ol">dtf = dtf.rename(columns={"<strong class="oe jf">Survived</strong>":"<strong class="oe jf">Y</strong>"})</span></pre><p id="dcb5" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我相信可视化是数据分析的最好工具，但是你需要知道什么样的图更适合不同类型的变量。因此，我将提供代码来为不同的示例绘制适当的可视化。</p><p id="e8cd" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">首先，让我们看看单变量分布(只有一个变量的概率分布)。一个<a class="ae oo" href="https://en.wikipedia.org/wiki/Bar_chart" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">条形图</strong> </a> <strong class="ly jf"> </strong>适用于理解单个<strong class="ly jf">分类</strong>变量的标签频率。例如，让我们绘制目标变量:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="9e3d" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">y = "Y"</strong></span><span id="830a" class="la lb je oe b gy om oj l ok ol">ax = dtf[y].value_counts().sort_values().plot(kind="barh")<br/>totals= []<br/>for i in ax.patches:<br/>    totals.append(i.get_width())<br/>total = sum(totals)<br/>for i in ax.patches:<br/>     ax.text(i.get_width()+.3, i.get_y()+.20, <br/>     str(round((i.get_width()/total)*100, 2))+'%', <br/>     fontsize=10, color='black')<br/>ax.grid(axis="x")<br/>plt.suptitle(y, fontsize=20)<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e4f55046be10adab58009cf7154e8e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*-KhpzEZfVUFdFfBnf54xIw.png"/></div></figure><p id="c717" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">多达300名乘客幸存，大约550人没有，换句话说，存活率(或人口平均值)是38%。</p><p id="012c" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">此外，一个<a class="ae oo" href="https://en.wikipedia.org/wiki/Histogram" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">直方图</strong> </a>完美地给出了单个<strong class="ly jf">数值</strong>数据的基本分布密度的粗略感觉。我推荐使用一个<a class="ae oo" href="https://en.wikipedia.org/wiki/Box_plot" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">箱线图</strong> </a> <strong class="ly jf"> </strong>来图形化地描绘数据组的四分位数。让我们以<em class="op">年龄</em>变量为例:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="9685" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">x = "Age"</strong></span><span id="0563" class="la lb je oe b gy om oj l ok ol">fig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)<br/>fig.suptitle(x, fontsize=20)</span><span id="916b" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### distribution</strong><br/>ax[0].title.set_text('distribution')<br/>variable = dtf[x].fillna(dtf[x].mean())<br/>breaks = np.quantile(variable, q=np.linspace(0, 1, 11))<br/>variable = variable[ (variable &gt; breaks[0]) &amp; (variable &lt; <br/>                    breaks[10]) ]<br/>sns.distplot(variable, hist=True, kde=True, kde_kws={"shade": True}, ax=ax[0])<br/>des = dtf[x].describe()<br/>ax[0].axvline(des["25%"], ls='--')<br/>ax[0].axvline(des["mean"], ls='--')<br/>ax[0].axvline(des["75%"], ls='--')<br/>ax[0].grid(True)<br/>des = round(des, 2).apply(lambda x: str(x))<br/>box = '\n'.join(("min: "+des["min"], "25%: "+des["25%"], "mean: "+des["mean"], "75%: "+des["75%"], "max: "+des["max"]))<br/>ax[0].text(0.95, 0.95, box, transform=ax[0].transAxes, fontsize=10, va='top', ha="right", bbox=dict(boxstyle='round', facecolor='white', alpha=1))</span><span id="aad9" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### boxplot </strong><br/>ax[1].title.set_text('outliers (log scale)')<br/>tmp_dtf = pd.DataFrame(dtf[x])<br/>tmp_dtf[x] = np.log(tmp_dtf[x])<br/>tmp_dtf.boxplot(column=x, ax=ax[1])<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/d83c414c165b359c36453732132d62f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhXWmvd8Weg3cmWNsVO2XQ.png"/></div></div></figure><p id="4d62" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">平均而言，乘客相当年轻:分布偏向左侧(平均值为30岁，第75百分位为38岁)。加上箱形图中的异常值，左尾部的第一个尖峰表示有大量的儿童。</p><p id="bf91" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我将把分析进行到下一个层次，并研究双变量分布，以了解<em class="op">年龄</em>是否具有预测<em class="op"> Y </em>的预测能力。这就是<strong class="ly jf">绝对(<em class="op"> Y </em>)对数值(<em class="op">年龄</em> ) </strong>的情况，因此我将这样进行:</p><ul class=""><li id="1317" class="np nq je ly b lz mu mc mv lj nr ln ns lr nt mo nu nv nw nx bi translated">将总体(整个观察集)分成2个样本:Y = 1 (存活)和Y = 0 (未存活)的乘客部分。</li><li id="96bc" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">绘制并比较两个样本的密度，如果分布不同，则变量是可预测的，因为两组具有不同的模式。</li><li id="4d3c" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">将数值变量(<em class="op">年龄</em>)分组到箱(子样本)中，并绘制每个箱的组成，如果所有箱中1的比例相似，则该变量不具有预测性。</li><li id="f812" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo nu nv nw nx bi translated">绘制并比较两个样本的箱线图，找出异常值的不同行为。</li></ul><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="f674" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">cat, num = "Y", "Age"</strong></span><span id="bdfd" class="la lb je oe b gy om oj l ok ol">fig, ax = plt.subplots(nrows=1, ncols=3,  sharex=False, sharey=False)<br/>fig.suptitle(x+"   vs   "+y, fontsize=20)<br/>            <br/><strong class="oe jf">### distribution</strong><br/>ax[0].title.set_text('density')<br/>for i in dtf[cat].unique():<br/>    sns.distplot(dtf[dtf[cat]==i][num], hist=False, label=i, ax=ax[0])<br/>ax[0].grid(True)</span><span id="aedf" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### stacked</strong><br/>ax[1].title.set_text('bins')<br/>breaks = np.quantile(dtf[num], q=np.linspace(0,1,11))<br/>tmp = dtf.groupby([cat, pd.cut(dtf[num], breaks, duplicates='drop')]).size().unstack().T<br/>tmp = tmp[dtf[cat].unique()]<br/>tmp["tot"] = tmp.sum(axis=1)<br/>for col in tmp.drop("tot", axis=1).columns:<br/>     tmp[col] = tmp[col] / tmp["tot"]<br/>tmp.drop("tot", axis=1).plot(kind='bar', stacked=True, ax=ax[1], legend=False, grid=True)</span><span id="f033" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### boxplot </strong>  <br/>ax[2].title.set_text('outliers')<br/>sns.catplot(x=cat, y=num, data=dtf, kind="box", ax=ax[2])<br/>ax[2].grid(True)<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/7303e1d387717df21ee93576a67e28f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZhI9R4JiKE_kPDpeVez6YA.png"/></div></div></figure><p id="b48a" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">这3个图只是结论<em class="op">年龄</em>具有预测性的不同角度。年轻乘客的存活率更高:1s分布的左尾有一个尖峰，第一个箱(0-16岁)包含最高百分比的幸存乘客。</p><p id="0897" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">当“视觉直觉”无法说服你时，你可以求助于古老的统计数据来进行测试。在这种分类(<em class="op">Y</em>vs数值(<em class="op">年龄</em>)的情况下，我会使用一个<strong class="ly jf">o</strong><a class="ae oo" href="http://en.wikipedia.org/wiki/F_test#One-way_ANOVA_example" rel="noopener ugc nofollow" target="_blank"><strong class="ly jf">n-way ANOVA检验</strong> </a>。基本上是检验两个或两个以上独立样本的均值是否显著不同，所以如果p值足够小(&lt; 0.05)样本的零假设意味着相等可以被拒绝。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="7774" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">cat, num = "Y", "Age"</strong></span><span id="576e" class="la lb je oe b gy om oj l ok ol">model = smf.<strong class="oe jf">ols</strong>(num+' ~ '+cat, data=dtf).fit()<br/>table = sm.stats.<strong class="oe jf">anova_lm</strong>(model)<br/>p = table["PR(&gt;F)"][0]<br/>coeff, p = None, round(p, 3)<br/>conclusion = "Correlated" if p &lt; 0.05 else "Non-Correlated"<br/>print("Anova F: the variables are", conclusion, "(p-value: "+str(p)+")")</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/9ee7c4a5fa8c3efe835692542c2a90ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*OA5Micu7fM-OhtnPOdne-A.png"/></div></figure><p id="97bf" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">显然，乘客的年龄决定了他们的生存。这是有道理的，因为在生命受到威胁的情况下，当救生艇等生存资源有限时，妇女和儿童的生命将被首先拯救，通常是弃船(代码"<a class="ae oo" href="https://en.wikipedia.org/wiki/Women_and_children_first" rel="noopener ugc nofollow" target="_blank">妇女和儿童优先</a>)。</p><p id="f3b4" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">为了检查第一个结论的有效性，我必须分析<em class="op">性别</em>变量相对于目标变量的行为。这是一个<strong class="ly jf">分类(<em class="op"> Y </em> ) vs分类(<em class="op">性别</em> ) </strong>的例子，所以我将绘制两个条形图，一个是两个类别<em class="op">性别</em>(男性和女性)中1和0的数量，另一个是百分比。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="d9eb" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">x, y = "Sex", "Y"</strong></span><span id="b84f" class="la lb je oe b gy om oj l ok ol">fig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)<br/>fig.suptitle(x+"   vs   "+y, fontsize=20)</span><span id="c72e" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### count</strong><br/>ax[0].title.set_text('count')<br/>order = dtf.groupby(x)[y].count().index.tolist()<br/>sns.catplot(x=x, hue=y, data=dtf, kind='count', order=order, ax=ax[0])<br/>ax[0].grid(True)</span><span id="6f81" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">### percentage</strong><br/>ax[1].title.set_text('percentage')<br/>a = dtf.groupby(x)[y].count().reset_index()<br/>a = a.rename(columns={y:"tot"})<br/>b = dtf.groupby([x,y])[y].count()<br/>b = b.rename(columns={y:0}).reset_index()<br/>b = b.merge(a, how="left")<br/>b["%"] = b[0] / b["tot"] *100<br/>sns.barplot(x=x, y="%", hue=y, data=b,<br/>            ax=ax[1]).get_legend().remove()<br/>ax[1].grid(True)<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/7bc8762ccb0d2d99a26cbd9e7076ba74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C2rzIv-IjCI6p5EsSEKaNQ.png"/></div></div></figure><p id="e4fc" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">200多名女性乘客(占机上女性总数的75%)和约100名男性乘客(不到20%)幸存。换句话说，女性的存活率是75%,男性是20%,因此性别是可以预测的。此外，这证实了他们优先考虑妇女和儿童。</p><p id="a9aa" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">就像之前一样，我们可以测试这两个变量的相关性。由于它们都是分类的，我将使用C <a class="ae oo" href="https://en.wikipedia.org/wiki/Chi-square_test" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">卡方检验:</strong> </a>假设两个变量是独立的(零假设)，它将检验这些变量的列联表的值是否是均匀分布的。如果p值足够小(&lt; 0.05)，可以拒绝零假设，我们可以说这两个变量可能是相关的。可以计算C <a class="ae oo" href="https://en.wikipedia.org/wiki/Cram%C3%A9r's_V" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf"> ramer的V </strong> </a> <strong class="ly jf"> t </strong>这是从该测试得出的相关性度量，它是对称的(就像传统的皮尔逊相关性)，范围在0和1之间(不像传统的皮尔逊相关性，没有负值)。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="2646" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">x, y = "Sex", "Y"</strong></span><span id="cdf7" class="la lb je oe b gy om oj l ok ol">cont_table = pd.crosstab(index=dtf[x], columns=dtf[y])<br/>chi2_test = scipy.stats.<strong class="oe jf">chi2_contingency</strong>(cont_table)<br/>chi2, p = chi2_test[0], chi2_test[1]<br/>n = cont_table.sum().sum()<br/>phi2 = chi2/n<br/>r,k = cont_table.shape<br/>phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))<br/>rcorr = r-((r-1)**2)/(n-1)<br/>kcorr = k-((k-1)**2)/(n-1)<br/>coeff = np.sqrt(phi2corr/min((kcorr-1), (rcorr-1)))<br/>coeff, p = round(coeff, 3), round(p, 3)<br/>conclusion = "Significant" if p &lt; 0.05 else "Non-Significant"<br/>print("Cramer Correlation:", coeff, conclusion, "(p-value:"+str(p)+")")</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/3b5a78769585f056ed75897828a1058e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KS4bwcy6pFpgXreDIwk-RA.png"/></div></div></figure><p id="d85a" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated"><em class="op">年龄</em>和<em class="op">性别</em>都是预测特征的例子，但并不是数据集中的所有列都是这样。例如，<em class="op">舱</em>似乎是一个<strong class="ly jf">无用变量</strong>，因为它不提供任何有用的信息，有太多的缺失值和类别。</p><p id="c264" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">应该对数据集中的每个变量进行这种分析，以决定哪些应该作为潜在特征保留，哪些因为不具有预测性而可以放弃(查看完整代码的链接)。</p><h2 id="b47f" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">特征工程</h2><p id="7756" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">是时候使用领域知识从原始数据创建新要素了。我将提供一个例子:我将尝试通过从<em class="op"> Cabin </em>列中提取信息来创建一个有用的特征。我假设每个舱号开头的字母(即“<strong class="ly jf"> <em class="op"> B </em> </strong> <em class="op"> 96 </em>”)表示某种区域，也许有一些幸运区域靠近救生艇。我将通过提取每个舱室的截面来按组总结观察结果:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="fc38" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## Create new column</strong><br/>dtf["<strong class="oe jf">Cabin_section</strong>"] = dtf["<strong class="oe jf">Cabin</strong>"].apply(lambda x: str(x)[0])</span><span id="195c" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## Plot contingency table<br/></strong>cont_table = pd.crosstab(index=dtf["<strong class="oe jf">Cabin_section"</strong>], <br/>             columns=dtf["<strong class="oe jf">Pclass</strong>"], values=dtf["<strong class="oe jf">Y</strong>"], aggfunc="sum")</span><span id="4432" class="la lb je oe b gy om oj l ok ol">sns.<strong class="oe jf">heatmap</strong>(cont_table, annot=True, cmap="YlGnBu", fmt='.0f',<br/>            linewidths=.5).set_title( <br/>            'Cabin_section vs Pclass (filter: Y)' )</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/6c216a48823f09b8364514e4c51dd629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r9Pw1wtZ9DQunc76InAwmA.png"/></div></div></figure><p id="75c5" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">该图显示了幸存者在客舱各部分和各等级中的分布情况(7名幸存者在A区，35名在B区……)。大多数区段被分配给第一类和第二类，而大多数缺失区段("<em class="op"> n" </em>)属于第三类。我将保留这个新特性，而不是列<em class="op"> Cabin: </em></p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/4ec62398ac8f632b3681b34b2c40a8a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wn2vfFKnu7yqsWkLmZLlcA.png"/></div></div></figure><h2 id="92fe" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">预处理</h2><p id="d738" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">数据预处理是准备原始数据以使其适合机器学习模型的阶段。特别是:</p><ol class=""><li id="90bc" class="np nq je ly b lz mu mc mv lj nr ln ns lr nt mo oz nv nw nx bi translated">每个观察必须用一行来表示，换句话说，不能用两行来描述同一个乘客，因为它们将被模型分别处理(数据集已经是这样的形式，所以✅).而且每一列都应该是一个特征，所以你不应该用<em class="op"> PassengerId </em>作为预测器，这就是为什么这种表叫做“<strong class="ly jf">特征矩阵</strong>”。</li><li id="9e1c" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo oz nv nw nx bi translated">数据集必须<strong class="ly jf">划分为</strong>至少两组:模型应在数据集的重要部分(所谓的“训练集”)上进行训练，并在较小的数据集(“测试集”)上进行测试。</li><li id="3ec3" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo oz nv nw nx bi translated"><strong class="ly jf">缺失值</strong>应该用东西替换，否则你的模型可能会出问题。</li><li id="857a" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo oz nv nw nx bi translated"><strong class="ly jf">分类数据</strong>必须编码，这意味着将标签转换成整数，因为机器学习期望的是数字而不是字符串。</li><li id="171e" class="np nq je ly b lz ny mc nz lj oa ln ob lr oc mo oz nv nw nx bi translated">对数据进行<strong class="ly jf">缩放</strong>是一种很好的做法，这有助于在特定范围内对数据进行标准化，并加快算法的计算速度。</li></ol><p id="4d75" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">好的，让我们从<strong class="ly jf">划分数据集</strong>开始。当把数据分成训练集和测试集时，你必须遵循一个基本规则:训练集中的行不应该出现在测试集中。这是因为模型在训练过程中会看到目标值，并使用它来理解现象。换句话说，模型已经知道训练观察的正确答案，在这些基础上测试就像作弊。我见过很多人推销他们的机器学习模型，声称有99.99%的准确率，但实际上却忽略了这条规则。幸运的是，S <em class="op"> cikit-learn </em>包知道:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="095d" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## split data</strong><br/>dtf_train, dtf_test = <strong class="oe jf">model_selection</strong>.<strong class="oe jf">train_test_split</strong>(dtf, <br/>                      test_size=0.3)</span><span id="3a3b" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## print info</strong><br/>print("X_train shape:", dtf_train.drop("Y",axis=1).shape, "| X_test shape:", dtf_test.drop("Y",axis=1).shape)<br/>print("y_train mean:", round(np.mean(dtf_train["Y"]),2), "| y_test mean:", round(np.mean(dtf_test["Y"]),2))<br/>print(dtf_train.shape[1], "features:", dtf_train.drop("Y",axis=1).columns.to_list())</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/9bae2b2b50595c91a99d03c0dcde28d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5RUyGcVjtyP0Z67rEsLK1g.png"/></div></div></figure><p id="aedd" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">下一步:<em class="op">年龄</em>列包含一些需要处理的<strong class="ly jf">缺失数据</strong> (19%)。在实践中，您可以用一个特定的值来替换丢失的数据，比如9999，它可以跟踪丢失的信息，但会改变变量的分布。或者，你可以使用列的平均值，就像我要做的。我想强调的是，从机器学习的角度来看，首先分成训练和测试，然后仅用训练集的平均值替换<em class="op"> NAs </em>是正确的。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="aee6" class="la lb je oe b gy oi oj l ok ol">dtf_train["Age"] = dtf_train["Age"].<strong class="oe jf">fillna</strong>(dtf_train["Age"].<strong class="oe jf">mean</strong>())</span></pre><p id="b55c" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">仍然有一些<strong class="ly jf">分类数据</strong>应该被编码。两种最常见的编码器是标签编码器(每个唯一的标签映射到一个整数)和一位热编码器(每个标签映射到一个二进制向量)。第一种方法只适用于普通数据。如果应用于没有普通性的列，如<em class="op"> Sex </em>，它会将向量<em class="op">【m</em>ale，female，female，male，male，…】转换为【1，2，2，1，…】,我们会得到那个female &gt; male，平均值为1.5，这是没有意义的。另一方面，One-Hot-Encoder会将之前的示例转换为两个<a class="ae oo" href="https://en.wikipedia.org/wiki/Dummy_variable_(statistics)" rel="noopener ugc nofollow" target="_blank">虚拟变量</a>(二分量化变量):Mal <em class="op"> e [1 </em>，0，0，1，…]和Fem <em class="op"> ale [0 </em>，1，1，0，…]。它的优点是结果是二进制的而不是有序的，并且一切都位于正交向量空间中，但是具有高基数的特性可能会导致维数问题。我将使用One-Hot-Encoding方法，将1个具有n个唯一值的分类列转换为n-1个虚拟列。让我们以编码<em class="op">性别</em>为<em class="op">T21为例:</em></p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="dadc" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## create dummy</strong><br/>dummy = pd.get_dummies(dtf_train["<strong class="oe jf">Sex</strong>"], <br/>                       prefix="Sex",drop_first=True)<br/>dtf_train= pd.concat([dtf_train, dummy], axis=1)<br/>print( dtf_train.filter(like="Sex", axis=1).head() )</span><span id="a54a" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## drop the original categorical column</strong><br/>dtf = dtf_train.drop("<strong class="oe jf">Sex</strong>", axis=1)</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/95da715165894e174ece18bcabaeffd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*OF-cFsJWSVw5tA8DeMCiDQ.png"/></div></figure><p id="0b21" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">最后但同样重要的是，我将<strong class="ly jf">缩放特征</strong>。有几种不同的方法可以做到这一点，我将只介绍最常用的方法:标准缩放器和最小最大缩放器。第一种假设数据呈正态分布，并对其进行重新调整，使分布以0为中心，标准差为1。然而，当计算缩小特征值范围的经验平均值和标准偏差时，离群值具有影响，因此该缩放器不能在离群值存在时保证平衡的特征尺度。另一方面，最小最大缩放器重新缩放数据集，使所有特征值都在同一范围内(0–1)。它受离群值的影响较小，但压缩了一个狭窄范围内的所有内联值。因为我的数据不是正态分布的，所以我将使用最小最大缩放器:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="e28a" class="la lb je oe b gy oi oj l ok ol">scaler = <strong class="oe jf">preprocessing</strong>.<strong class="oe jf">MinMaxScaler</strong>(feature_range=(0,1))<br/>X = scaler.fit_transform(dtf_train.drop("Y", axis=1))</span><span id="7499" class="la lb je oe b gy om oj l ok ol">dtf_scaled= pd.DataFrame(X, columns=dtf_train.drop("Y", axis=1).columns, index=dtf_train.index)<br/>dtf_scaled["Y"] = dtf_train["Y"]<br/>dtf_scaled.head()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/eab93edce1b9eda3527143c28aa18746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k0r2CHiCk9A16p-tfyefSA.png"/></div></div></figure><h2 id="50c7" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">特征选择</h2><p id="cc32" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">特征选择是选择相关变量的子集来构建机器学习模型的过程。它使模型更容易解释，并减少过度拟合(当模型适应训练数据过多，并且在训练集之外表现不佳时)。</p><p id="2be9" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">在数据分析期间，我已经通过排除不相关的列进行了第一次“手动”特征选择。现在会有一点不同，因为我们假设矩阵中的所有特性都是相关的，我们想去掉不必要的特性。当一个特性不是必需的时候。答案很简单:当有更好的对等物，或者做同样工作但更好的对等物时。</p><p id="49c8" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我用一个例子来解释:<em class="op"> Pclass </em>与<em class="op"> Cabin_section </em>高度相关，因为，正如我们之前看到的，某些部分位于一等舱，而其他部分位于二等舱。让我们计算相关矩阵来看看:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="b5d7" class="la lb je oe b gy oi oj l ok ol">corr_matrix = dtf.copy()<br/>for col in corr_matrix.columns:<br/>    if corr_matrix[col].dtype == "O":<br/>         corr_matrix[col] = corr_matrix[col].factorize(sort=True)[0]</span><span id="179e" class="la lb je oe b gy om oj l ok ol">corr_matrix = corr_matrix.<strong class="oe jf">corr</strong>(method="pearson")<br/>sns.heatmap(corr_matrix, vmin=-1., vmax=1., annot=True, fmt='.2f', cmap="YlGnBu", cbar=True, linewidths=0.5)<br/>plt.title("pearson correlation")</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/588fd55ca48e3b40322600cdeea81d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ASqGwKzh9LfKyEHpwi82OQ.png"/></div></div></figure><p id="be47" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated"><em class="op"> Pclass </em>和<em class="op"> Cabin_section </em>中的一个可能是不必要的<em class="op"> </em>，我们可以决定丢弃它并保留最有用的一个(即具有最低p值的一个或最能降低熵的一个)。</p><p id="564d" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我将展示两种不同的方法来执行自动特征选择:首先，我将使用正则化方法<strong class="ly jf"> </strong>并将其与之前已经提到的ANOVA测试进行比较，然后我将展示如何从集成方法中获得特征重要性。</p><p id="d93b" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated"><a class="ae oo" href="https://en.wikipedia.org/wiki/Lasso_(statistics)" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">套索正则化</strong> </a>是一种回归分析方法，执行变量选择和正则化，以提高准确性和可解释性。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="543b" class="la lb je oe b gy oi oj l ok ol">X = dtf_train.drop("Y", axis=1).values<br/>y = dtf_train["Y"].values<br/>feature_names = dtf_train.drop("Y", axis=1).columns</span><span id="45fd" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## Anova</strong><br/>selector = <strong class="oe jf">feature_selection.SelectKBest</strong>(score_func=  <br/>               feature_selection.f_classif, k=10).fit(X,y)<br/>anova_selected_features = feature_names[selector.get_support()]<br/><br/><strong class="oe jf">## Lasso regularization</strong><br/>selector = <strong class="oe jf">feature_selection.SelectFromModel</strong>(estimator= <br/>              linear_model.LogisticRegression(C=1, penalty="l1", <br/>              solver='liblinear'), max_features=10).fit(X,y)<br/>lasso_selected_features = feature_names[selector.get_support()]<br/> <br/><strong class="oe jf">## Plot<br/></strong>dtf_features = pd.DataFrame({"features":feature_names})<br/>dtf_features["anova"] = dtf_features["features"].apply(lambda x: "anova" if x in anova_selected_features else "")<br/>dtf_features["num1"] = dtf_features["features"].apply(lambda x: 1 if x in anova_selected_features else 0)<br/>dtf_features["lasso"] = dtf_features["features"].apply(lambda x: "lasso" if x in lasso_selected_features else "")<br/>dtf_features["num2"] = dtf_features["features"].apply(lambda x: 1 if x in lasso_selected_features else 0)<br/>dtf_features["method"] = dtf_features[["anova","lasso"]].apply(lambda x: (x[0]+" "+x[1]).strip(), axis=1)<br/>dtf_features["selection"] = dtf_features["num1"] + dtf_features["num2"]<br/>sns.<strong class="oe jf">barplot</strong>(y="features", x="selection", hue="method", data=dtf_features.sort_values("selection", ascending=False), dodge=False)</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/e124811fdc3ce486a618fef7107fff7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JbLl3-xAk70ChrwOYf05_Q.png"/></div></div></figure><p id="9529" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">蓝色要素是通过方差分析和LASSO选择的要素，其他要素是通过两种方法中的一种选择的。</p><p id="f73f" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated"><a class="ae oo" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">随机森林</strong> </a>是一种集成方法，由许多决策树组成，其中每个节点都是单个要素上的一个条件，旨在将数据集一分为二，以便相似的响应值最终出现在同一组中。特征重要性是根据每个特征减少树中熵的多少来计算的。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="1bb8" class="la lb je oe b gy oi oj l ok ol">X = dtf_train.drop("Y", axis=1).values<br/>y = dtf_train["Y"].values<br/>feature_names = dtf_train.drop("Y", axis=1).columns.tolist()</span><span id="a486" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## Importance</strong><br/>model = ensemble.<strong class="oe jf">RandomForestClassifier</strong>(n_estimators=100,<br/>                      criterion="entropy", random_state=0)<br/>model.fit(X,y)<br/>importances = model.<strong class="oe jf">feature_importances_</strong></span><span id="df11" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## Put in a pandas dtf</strong><br/>dtf_importances = pd.DataFrame({"IMPORTANCE":importances, <br/>            "VARIABLE":feature_names}).sort_values("IMPORTANCE", <br/>            ascending=False)<br/>dtf_importances['cumsum'] =  <br/>            dtf_importances['IMPORTANCE'].cumsum(axis=0)<br/>dtf_importances = dtf_importances.set_index("VARIABLE")<br/>    <br/><strong class="oe jf">##</strong> <strong class="oe jf">Plot</strong><br/>fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False)<br/>fig.suptitle("Features Importance", fontsize=20)<br/>ax[0].title.set_text('variables')<br/>    dtf_importances[["IMPORTANCE"]].sort_values(by="IMPORTANCE").plot(<br/>                kind="barh", legend=False, ax=ax[0]).grid(axis="x")<br/>ax[0].set(ylabel="")<br/>ax[1].title.set_text('cumulative')<br/>dtf_importances[["cumsum"]].plot(kind="line", linewidth=4, <br/>                                 legend=False, ax=ax[1])<br/>ax[1].set(xlabel="", xticks=np.arange(len(dtf_importances)), <br/>          xticklabels=dtf_importances.index)<br/>plt.xticks(rotation=70)<br/>plt.grid(axis='both')<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/3f5da971c94b0dfa7ed76b91094dc097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Le11nI9ztW4j33LqwGecNQ.png"/></div></div></figure><p id="6ef6" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">很有意思的是，<em class="op">年龄</em>和<em class="op">票价、</em>这两个这次最重要的特征，以前并不是最重要的特征，相反<em class="op">客舱_ E段</em>、<em class="op"> F </em>和<em class="op"> D </em>在这里似乎并不太有用。</p><p id="a407" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">就我个人而言，我总是尽量少用一些功能，所以在这里我选择了以下几个，并继续进行机器学习模型的设计、训练、测试和评估:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="8edc" class="la lb je oe b gy oi oj l ok ol">X_names = ["Age", "Fare", "Sex_male", "SibSp", "Pclass_3", "Parch",<br/>"Cabin_section_n", "Embarked_S", "Pclass_2", "Cabin_section_F", "Cabin_section_E", "Cabin_section_D"]</span><span id="34db" class="la lb je oe b gy om oj l ok ol">X_train = dtf_train[X_names].values<br/>y_train = dtf_train["Y"].values</span><span id="2e1c" class="la lb je oe b gy om oj l ok ol">X_test = dtf_test[X_names].values<br/>y_test = dtf_test["Y"].values</span></pre><p id="002e" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">请注意，在使用测试数据进行预测之前，您必须对其进行预处理，就像我们对训练数据所做的那样。</p><h2 id="47a4" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">模型设计</h2><p id="6213" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">最后，是时候建立机器学习模型了。首先，我们需要选择一种算法，它能够从训练数据中学习如何通过最小化一些误差函数来识别目标变量的两个类别。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/284d3d42aeeaa4f313e2b25343334cc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rlwt7lMKqkgth0mhGK0LIw.png"/></div></div><p class="ph pi gj gh gi pj pk bd b be z dk translated">来源:<a class="ae oo" href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a></p></figure><p id="ff5c" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我建议总是尝试一个<a class="ae oo" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">渐变提升</strong> </a>算法(像XGBoost)。这是一种机器学习技术，它以弱预测模型的集合的形式产生预测模型，通常是决策树。基本上，它类似于一个随机的森林，不同之处在于每棵树都符合前一棵树的误差。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/02deb3ce2928935cb5df21ff45afac87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Quo8-G6HK9KnyI8fHjhJSg.png"/></div></div><p class="ph pi gj gh gi pj pk bd b be z dk translated">来源:<a class="ae oo" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a></p></figure><p id="0c20" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">有很多超参数，没有什么是最好的通用规则，所以你只需要找到更适合你的数据的正确组合。您可以手动进行不同的尝试，或者让计算机通过GridSearch(尝试每种可能的组合，但需要时间)或RandomSearch(随机尝试固定次数的迭代)来完成这项繁琐的工作。我将尝试对我的<strong class="ly jf">超参数调整</strong>进行随机搜索:机器将通过训练数据迭代n次(1000次)以找到参数组合(在下面的代码中指定),最大化用作KPI(准确性，正确预测数与输入样本总数的比率)的评分函数:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="a92c" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## call model</strong><br/>model = ensemble.<strong class="oe jf">GradientBoostingClassifier</strong>()</span><span id="a587" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## define hyperparameters combinations to try<br/></strong>param_dic = {'<strong class="oe jf">learning_rate</strong>':[0.15,0.1,0.05,0.01,0.005,0.001],      <em class="op">#weighting factor for the corrections by new trees when added to the model<br/></em>'<strong class="oe jf">n_estimators</strong>':[100,250,500,750,1000,1250,1500,1750],  <em class="op">#number of trees added to the model</em><br/>'<strong class="oe jf">max_depth</strong>':[2,3,4,5,6,7],    <em class="op">#maximum depth of the tree</em><br/>'<strong class="oe jf">min_samples_split</strong>':[2,4,6,8,10,20,40,60,100],    <em class="op">#sets the minimum number of samples to split</em><br/>'<strong class="oe jf">min_samples_leaf</strong>':[1,3,5,7,9],     <em class="op">#the minimum number of samples to form a leaf<br/></em>'<strong class="oe jf">max_features</strong>':[2,3,4,5,6,7],     <em class="op">#square root of features is usually a good starting point</em><br/>'<strong class="oe jf">subsample</strong>':[0.7,0.75,0.8,0.85,0.9,0.95,1]}       <em class="op">#the fraction of samples to be used for fitting the individual base learners. Values lower than 1 generally lead to a reduction of variance and an increase in bias.</em></span><span id="8317" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## random search</strong><br/>random_search = model_selection.<strong class="oe jf">RandomizedSearchCV</strong>(model, <br/>       param_distributions=param_dic, n_iter=1000, <br/>       scoring="accuracy").fit(X_train, y_train)</span><span id="c3ac" class="la lb je oe b gy om oj l ok ol">print("Best Model parameters:", random_search.best_params_)<br/>print("Best Model mean accuracy:", random_search.best_score_)</span><span id="22b3" class="la lb je oe b gy om oj l ok ol">model = random_search.best_estimator_</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pm"><img src="../Images/e4840d9d2003b13faec0dbbbe578d4c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Unw_2tw2VtqwxDMQmmbPiA.png"/></div></div></figure><p id="6d0d" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">酷，这是最好的模型，平均精度为0.85，因此在测试集上可能有85%的预测是正确的。</p><p id="9eb3" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我们还可以使用<strong class="ly jf"> k倍交叉验证</strong>来验证该模型，该过程包括将数据分成k次训练和验证集，并且针对每次分割对模型进行训练和测试。它用于检查模型通过一些数据进行训练的能力，以及预测未知数据的能力。</p><p id="6066" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我想澄清一下，我称<strong class="ly jf">验证集</strong>为一组用于调整分类器超参数的例子，从分割训练数据中提取。另一方面，<strong class="ly jf">测试集</strong>是一个模拟模型在生产中的表现，当它被要求预测以前从未见过的观察时。</p><p id="8008" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">通常为每个折叠绘制一个<strong class="ly jf"> ROC曲线</strong>，该图说明了二元分类器的能力如何随着其区分阈值的变化而变化。它是通过在各种阈值设置下绘制真阳性率(正确预测的1)与假阳性率(预测的1实际上是0)来创建的。<a class="ae oo" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve" rel="noopener ugc nofollow" target="_blank"><strong class="ly jf">AUC</strong></a><strong class="ly jf"/>(ROC曲线下的面积)表示分类器将随机选择的阳性观察值(<em class="op"> Y=1 </em>)排序高于随机选择的阴性观察值(<em class="op"> Y=0 </em>)的概率。</p><p id="0ffc" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">现在，我将展示一个10次折叠的示例(k=10):</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="af22" class="la lb je oe b gy oi oj l ok ol">cv = model_selection.StratifiedKFold(n_splits=10, shuffle=True)<br/>tprs, aucs = [], []<br/>mean_fpr = np.linspace(0,1,100)<br/>fig = plt.figure()</span><span id="1c3c" class="la lb je oe b gy om oj l ok ol">i = 1<br/>for train, test in cv.split(X_train, y_train):<br/>   prediction = model.fit(X_train[train],<br/>                y_train[train]).predict_proba(X_train[test])<br/>   fpr, tpr, t = metrics.roc_curve(y_train[test], prediction[:, 1])<br/>   tprs.append(scipy.interp(mean_fpr, fpr, tpr))<br/>   roc_auc = metrics.auc(fpr, tpr)<br/>   aucs.append(roc_auc)<br/>   plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = <br/>            %0.2f)' % (i, roc_auc))<br/>   i = i+1<br/>   <br/>plt.plot([0,1], [0,1], linestyle='--', lw=2, color='black')<br/>mean_tpr = np.mean(tprs, axis=0)<br/>mean_auc = metrics.auc(mean_fpr, mean_tpr)<br/>plt.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = <br/>         %0.2f )' % (mean_auc), lw=2, alpha=1)<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.title('K-Fold Validation')<br/>plt.legend(loc="lower right")<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pn"><img src="../Images/94d47619c9ea2a1e3991f5f24bb82559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hMM9fION4eYgN6CyHKE00g.png"/></div></div></figure><p id="2e16" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">根据这一验证，在对测试进行预测时，我们应该预期AUC分数在0.84左右。</p><p id="1a7f" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">出于本教程的目的，我认为性能很好，我们可以继续使用随机搜索选择的模型。一旦选择了正确的模型，就可以在整个训练集上对其进行训练，然后在测试集上进行测试。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="4cd9" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## train</strong><br/>model.<strong class="oe jf">fit</strong>(X_train, y_train)</span><span id="7f8a" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## test</strong><br/>predicted_prob = model.<strong class="oe jf">predict_proba</strong>(X_test)[:,1]<br/>predicted = model.<strong class="oe jf">predict</strong>(X_test)</span></pre><p id="7cf4" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">在上面的代码中，我做了两种预测:第一种是观察值为1的概率，第二种是标签(1或0)的预测。为了得到后者，你必须决定一个概率阈值，对于这个阈值，一个观察可以被认为是1，我使用默认的阈值0.5。</p><h2 id="ea76" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">估价</h2><p id="335c" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">关键时刻到了，我们要看看所有这些努力是否值得。重点是研究模型做出了多少正确的预测和错误类型。</p><p id="c3c3" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我将使用以下常用指标来评估该模型:准确性、AUC、<a class="ae oo" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank"> <strong class="ly jf">精度和召回</strong> </a>。我已经提到了前两个，但我认为其他的更重要。精度是模型在所有预测的1(或0)中正确预测的1(或0)的分数，因此它可以被视为预测1(或0)时的一种置信度。召回是模型在测试集中所有1(或0)中正确预测的1(或0)的部分，基本上它是真正的1率。将精确度和召回率与调和平均值相结合，就得到F1分数。</p><p id="febf" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">让我们看看模型在测试集上的表现:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="36cc" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## Accuray e AUC</strong><br/>accuracy = metrics.<strong class="oe jf">accuracy_score</strong>(y_test, predicted)<br/>auc = metrics.<strong class="oe jf">roc_auc_score</strong>(y_test, predicted_prob)<br/>print("Accuracy (overall correct predictions):",  round(accuracy,2))<br/>print("Auc:", round(auc,2))<br/>    <br/><strong class="oe jf">## Precision e Recall</strong><br/>recall = metrics.<strong class="oe jf">recall_score</strong>(y_test, predicted)<br/>precision = metrics.<strong class="oe jf">precision_score</strong>(y_test, predicted)<br/>print("Recall (all 1s predicted right):", round(recall,2))<br/>print("Precision (confidence when predicting a 1):", round(precision,2))<br/>print("Detail:")<br/>print(metrics.<strong class="oe jf">classification_report</strong>(y_test, predicted, target_names=[str(i) for i in np.unique(y_test)]))</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/88dd8657970406033727964ea5544dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_W0WnLbAmeKYUBTMWBe1Ig.png"/></div></div></figure><p id="b077" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">正如预期的那样，模型的总体准确率在85%左右。它以84%的精度正确预测了71%的1，以85%的精度正确预测了92%的0。为了更好地理解这些指标，我将把结果分解成一个<a class="ae oo" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a>:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="6fc8" class="la lb je oe b gy oi oj l ok ol">classes = np.unique(y_test)<br/>fig, ax = plt.subplots()<br/>cm = metrics.<strong class="oe jf">confusion_matrix</strong>(y_test, predicted, labels=classes)<br/>sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)<br/>ax.set(xlabel="Pred", ylabel="True", title="Confusion matrix")<br/>ax.set_yticklabels(labels=classes, rotation=0)<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/b21b64d0aebb464d415a51d692b565ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*9G51Re_0gPV5444sUeK1ng.png"/></div></figure><p id="a15e" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我们可以看到，该模型预测了85 (70+15)个1，其中70个是真阳性，15个是假阳性，因此在预测1时，它的精度为70/85 = 0.82。另一方面，该模型在测试集中的所有96个(70+26)1中得到了70个1，因此其召回率为70/96 = 0.73。</p><p id="9997" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">选择阈值0.5来决定预测是1还是0导致了这种结果。换一个会不一样吗？当然可以，但是没有一个阈值可以让准确率和召回率都达到最高分，选择一个阈值意味着在这两个指标之间进行折衷。我将通过绘制测试结果的ROC曲线和精确回忆曲线来说明我的意思:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="1905" class="la lb je oe b gy oi oj l ok ol">classes = np.unique(y_test)<br/>fig, ax = plt.subplots(nrows=1, ncols=2)</span><span id="3a9b" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## plot ROC curve</strong><br/>fpr, tpr, thresholds = metrics.<strong class="oe jf">roc_curve</strong>(y_test, predicted_prob)<br/>roc_auc = metrics.auc(fpr, tpr)     <br/>ax[0].plot(fpr, tpr, color='darkorange', lw=3, label='area = %0.2f' % roc_auc)<br/>ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')<br/>ax[0].hlines(y=recall, xmin=0, xmax=1-cm[0,0]/(cm[0,0]+cm[0,1]), color='red', linestyle='--', alpha=0.7, label="chosen threshold")<br/>ax[0].vlines(x=1-cm[0,0]/(cm[0,0]+cm[0,1]), ymin=0, ymax=recall, color='red', linestyle='--', alpha=0.7)<br/>ax[0].set(xlabel='False Positive Rate', ylabel="True Positive Rate (Recall)", title="Receiver operating characteristic")     <br/>ax.legend(loc="lower right")<br/>ax.grid(True)</span><span id="a747" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## annotate ROC thresholds</strong><br/>thres_in_plot = []<br/>for i,t in enumerate(thresholds):<br/>     t = np.round(t,1)<br/>     if t not in thres_in_plot:<br/>         ax.annotate(t, xy=(fpr[i],tpr[i]), xytext=(fpr[i],tpr[i]), <br/>              textcoords='offset points', ha='left', va='bottom')<br/>         thres_in_plot.append(t)<br/>     else:<br/>         next</span><span id="f7f1" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## plot P-R curve</strong><br/>precisions, recalls, thresholds = metrics.<strong class="oe jf">precision_recall_curve</strong>(y_test, predicted_prob)<br/>roc_auc = metrics.auc(recalls, precisions)<br/>ax[1].plot(recalls, precisions, color='darkorange', lw=3, label='area = %0.2f' % roc_auc)<br/>ax[1].plot([0,1], [(cm[1,0]+cm[1,0])/len(y_test), (cm[1,0]+cm[1,0])/len(y_test)], linestyle='--', color='navy', lw=3)<br/>ax[1].hlines(y=precision, xmin=0, xmax=recall, color='red', linestyle='--', alpha=0.7, label="chosen threshold")<br/>ax[1].vlines(x=recall, ymin=0, ymax=precision, color='red', linestyle='--', alpha=0.7)<br/>ax[1].set(xlabel='Recall', ylabel="Precision", title="Precision-Recall curve")<br/>ax[1].legend(loc="lower left")<br/>ax[1].grid(True)</span><span id="07fd" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## annotate P-R thresholds<br/></strong>thres_in_plot = []<br/>for i,t in enumerate(thresholds):<br/>    t = np.round(t,1)<br/>    if t not in thres_in_plot:<br/>         ax.annotate(np.round(t,1), xy=(recalls[i],precisions[i]), <br/>               xytext=(recalls[i],precisions[i]), <br/>               textcoords='offset points', ha='left', va='bottom')<br/>         thres_in_plot.append(t)<br/>    else:<br/>         next<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/f29c4970fdd980811435030b41b51f70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NXe_UCC-88xREtQ58kovqg.png"/></div></div></figure><p id="f667" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">这些曲线的每一点都代表用不同阈值(曲线上印的数字)获得的混淆矩阵。我可以使用0.1的阈值，获得0.9的召回率，这意味着该模型将正确预测90%的1，但精度将下降到0.4，这意味着该模型将预测大量的假阳性。因此，这实际上取决于用例的类型，尤其是假阳性是否比假阴性的成本更高。</p><p id="0bda" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">当数据集是平衡的，并且项目涉众没有指定度量标准时，我通常选择最大化F1分数的阈值。方法如下:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="d6fd" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## calculate scores for different thresholds</strong><br/>dic_scores = {'accuracy':[], 'precision':[], 'recall':[], 'f1':[]}<br/>XX_train, XX_test, yy_train, yy_test = model_selection.train_test_split(X_train, y_train, test_size=0.2)<br/>predicted_prob = model.fit(XX_train, yy_train).predict_proba(XX_test)[:,1]</span><span id="049b" class="la lb je oe b gy om oj l ok ol">thresholds = []<br/>for threshold in np.arange(0.1, 1, step=0.1):<br/>    predicted = (predicted_prob &gt; threshold)<br/>    thresholds.append(threshold)<br/>        dic_scores["accuracy"].append(metrics.accuracy_score(yy_test, predicted))<br/>dic_scores["precision"].append(metrics.precision_score(yy_test, predicted))<br/>dic_scores["recall"].append(metrics.recall_score(yy_test, predicted))<br/>dic_scores["f1"].append(metrics.f1_score(yy_test, predicted))<br/>        <br/><strong class="oe jf">## plot<br/></strong>dtf_scores = pd.DataFrame(dic_scores).set_index(pd.Index(thresholds))    <br/>dtf_scores.plot(ax=ax, title="Threshold Selection")<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pr"><img src="../Images/a8e861e49ebf16839dd50cf7313965cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1EjGMz7axpOjUzLJPtY9dQ.png"/></div></div></figure><p id="8e6f" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">在继续这篇长教程的最后一节之前，我想说我们还不能说这个模型是好是坏。精度0.85，高吗？相比什么？你需要一个<strong class="ly jf">基线</strong>来比较你的模型。也许你正在做的项目是关于建立一个新的模型来取代一个可以用作基线的旧模型，或者你可以在同一训练集上训练不同的机器学习模型，并在测试集上比较性能。</p><h2 id="78cd" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">可解释性</h2><p id="3ca1" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">你分析并理解了数据，你训练了一个模型并测试了它，你甚至对性能感到满意。你以为你完了吗？不对。项目利益相关者很有可能不关心你的指标，不理解你的算法，所以你必须证明你的机器学习模型不是黑盒。</p><p id="f55b" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated"><em class="op">石灰</em>包可以帮助我们建造一个<strong class="ly jf">讲解器</strong>。为了举例说明，我将从测试集中随机观察，看看模型预测了什么:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="e8ee" class="la lb je oe b gy oi oj l ok ol">print("True:", y_test[4], "--&gt; Pred:", predicted[4], "| Prob:", np.max(predicted_prob[4]))</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ps"><img src="../Images/4a8fe8dad03fd0bb4181f667c1bb65ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oosf8SK7ZFcwOuV-rx_4hw.png"/></div></div></figure><p id="39c7" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">模型认为这个观察值是1，概率为0.93，事实上这个乘客确实活了下来。为什么？让我们使用解释器:</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="58fa" class="la lb je oe b gy oi oj l ok ol">explainer = lime_tabular.LimeTabularExplainer(training_data=X_train, feature_names=X_names, class_names=np.unique(y_train), mode="classification")<br/>explained = explainer.explain_instance(X_test[4], model.predict_proba, num_features=10)<br/>explained.as_pyplot_figure()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pt"><img src="../Images/9e64e733badbcf23ba07eab9bfa1ee40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qk-uB_ckNlatkomYt0U04w.png"/></div></div></figure><p id="470d" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">该特定预测的主要因素是乘客是女性(性别_男性= 0)、年轻(年龄≤ 22)并且乘坐头等舱旅行(Pclass_3 = 0和Pclass_2 = 0)。</p><p id="df9c" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">混淆矩阵是显示测试进行情况的一个很好的工具，但是我也绘制了<strong class="ly jf">分类区域</strong>来直观地帮助我们了解模型正确预测了哪些观察结果，以及它错过了哪些观察结果。为了绘制二维数据，需要进行一定程度的降维(通过获得一组主要变量来减少特征数量的过程)。我将给出一个例子，使用<a class="ae oo" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> PCA </a>算法将数据总结为两个变量，这些变量是通过特征的线性组合获得的。</p><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="f71e" class="la lb je oe b gy oi oj l ok ol"><strong class="oe jf">## PCA</strong><br/>pca = decomposition.PCA(n_components=2)<br/>X_train_2d = pca.fit_transform(X_train)<br/>X_test_2d = pca.transform(X_test)</span><span id="c22f" class="la lb je oe b gy om oj l ok ol"><strong class="oe jf">## train 2d model</strong><br/>model_2d = ensemble.GradientBoostingClassifier()<br/>model_2d.fit(X_train, y_train)<br/>    <br/><strong class="oe jf">## plot classification regions</strong><br/>from matplotlib.colors import ListedColormap<br/>colors = {np.unique(y_test)[0]:"black", np.unique(y_test)[1]:"green"}<br/>X1, X2 = np.meshgrid(np.arange(start=X_test[:,0].min()-1, stop=X_test[:,0].max()+1, step=0.01),<br/>np.arange(start=X_test[:,1].min()-1, stop=X_test[:,1].max()+1, step=0.01))<br/>fig, ax = plt.subplots()<br/>Y = model_2d.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)<br/>ax.contourf(X1, X2, Y, alpha=0.5, cmap=ListedColormap(list(colors.values())))<br/>ax.set(xlim=[X1.min(),X1.max()], ylim=[X2.min(),X2.max()], title="Classification regions")<br/>for i in np.unique(y_test):<br/>    ax.scatter(X_test[y_test==i, 0], X_test[y_test==i, 1], <br/>               c=colors[i], label="true "+str(i))  <br/>plt.legend()<br/>plt.show()</span></pre><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mp"><img src="../Images/778b5ac9c09c7591b4d2a2cc3fd1efd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gS5PdcX1sk1yQgYQnRSGcg.png"/></div></div></figure><h2 id="c821" class="la lb je bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">结论</h2><p id="c4fa" class="pw-post-body-paragraph lw lx je ly b lz ma kf mb mc md ki me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">这篇文章是演示<strong class="ly jf">如何用数据科学处理分类用例</strong>的教程。我以泰坦尼克号数据集为例，经历了从数据分析到机器学习模型的每一步。</p><p id="368c" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">在探索部分，我分析了单个分类变量、单个数值变量以及它们如何相互作用的情况。我举了一个从原始数据中提取特征的特征工程的例子。关于预处理，我解释了如何处理缺失值和分类数据。我展示了选择正确特征的不同方法，如何使用它们来构建机器学习分类器，以及如何评估性能。在最后一节，我就如何提高你的机器学习模型的可解释性给出了一些建议。</p><p id="c2a8" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">一个重要的注意事项是，我还没有介绍在您的模型被批准部署之后会发生什么。请记住，您需要构建一个管道来自动处理您将定期获得的新数据。</p><p id="b133" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">现在，您已经知道如何处理数据科学用例，您可以将这些代码和方法应用于任何类型的二进制分类问题，执行您自己的分析，构建您自己的模型，甚至解释它。</p><p id="a191" class="pw-post-body-paragraph lw lx je ly b lz mu kf mb mc mv ki me lj mw mg mh ln mx mj mk lr my mm mn mo im bi translated">我希望你喜欢它！如有问题和反馈，或者只是分享您感兴趣的项目，请随时联系我。</p><blockquote class="pu"><p id="3199" class="pv pw je bd px py pz qa qb qc qd mo dk translated">👉<a class="ae oo" href="https://linktr.ee/maurodp" rel="noopener ugc nofollow" target="_blank">我们来连线</a>👈</p></blockquote></div><div class="ab cl kt ku hx kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="im in io ip iq"><blockquote class="qe qf qg"><p id="9b95" class="lw lx op ly b lz mu kf mb mc mv ki me qh mw mg mh qi mx mj mk qj my mm mn mo im bi translated">本文是系列<strong class="ly jf">用Python进行机器学习</strong>的一部分，参见:</p></blockquote><div class="is it gp gr iu mz"><a rel="noopener follow" target="_blank" href="/machine-learning-with-python-regression-complete-tutorial-47268e546cea"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">Python机器学习:回归(完整教程)</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">数据分析和可视化、特征工程和选择、模型设计和测试、评估和解释</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="qk l nk nl nm ni nn ja mz"/></div></div></a></div><div class="is it gp gr iu mz"><a rel="noopener follow" target="_blank" href="/clustering-geospatial-data-f0584f0b04ec"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">聚类地理空间数据</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">使用交互式地图绘制机器学习和深度学习聚类</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="ql l nk nl nm ni nn ja mz"/></div></div></a></div><div class="is it gp gr iu mz"><a rel="noopener follow" target="_blank" href="/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">Python深度学习:神经网络(完整教程)</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">用TensorFlow建立、绘制和解释人工神经网络</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="qm l nk nl nm ni nn ja mz"/></div></div></a></div><div class="is it gp gr iu mz"><a rel="noopener follow" target="_blank" href="/modern-recommendation-systems-with-neural-networks-3cc06a6ded2c"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jf gy z fp ne fr fs nf fu fw jd bi translated">基于神经网络的现代推荐系统</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">使用Python和TensorFlow构建混合模型</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="qn l nk nl nm ni nn ja mz"/></div></div></a></div></div></div>    
</body>
</html>