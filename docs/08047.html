<html>
<head>
<title>NLP Sentiment Analysis Handbook</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 情感分析手册</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-sentiment-analysis-for-beginners-e7897f976897?source=collection_archive---------12-----------------------#2020-06-14">https://towardsdatascience.com/nlp-sentiment-analysis-for-beginners-e7897f976897?source=collection_archive---------12-----------------------#2020-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cb9f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">逐步了解文本块、NLTK、Scikit-Learn 和 LSTM 网络的方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0e7a38e0e76665386148a59cca1c830a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7MGvhKkPpY8DfnGH"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@rvignes?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Romain Vignes </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="3719" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="7b33" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">自然语言处理(NLP)是机器学习的领域，专注于语言的生成和理解。它的主要目标是让机器能够理解人类，以自然的方式与人类交流和互动。</p><p id="778d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">自然语言处理有许多任务，如文本生成、文本分类、机器翻译、语音识别、情感分析等。对于 NLP 的初学者来说，查看这些任务以及处理这些任务所涉及的所有技术可能会令人望而生畏。而且事实上，对于一个新手来说，确切地知道从哪里开始以及如何开始是非常困难的。</p><p id="c333" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在所有的 NLP 任务中，我个人认为情感分析(SA)可能是最容易的，这使得它成为任何想开始进入 NLP 的人最合适的起点。</p><p id="4b67" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本文中，我编译了各种如何执行 SA 的技术，从简单的 TextBlob 和 NLTK 到更高级的 Sklearn 和长短期记忆(LSTM)网络。</p><p id="0741" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">读完这篇文章后，你可以理解以下内容:</p><ol class=""><li id="c6c6" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated"><strong class="lt iu">在 SA: TextBlob 和 NLTK 中使用的工具包</strong></li><li id="2ec6" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><strong class="lt iu">SA 中使用的算法</strong>:朴素贝叶斯、SVM、逻辑回归和 LSTM</li><li id="3159" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><strong class="lt iu">术语</strong>如停用词移除、词干、单词包、语料库、标记化等。</li><li id="96ae" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">创建一个<strong class="lt iu">字云</strong></li></ol><h2 id="5572" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">这篇文章的流程:</h2><ol class=""><li id="434f" class="ms mt it lt b lu lv lx ly ma ns me nt mi nu mm mx my mz na bi translated">数据清理和预处理</li><li id="bdd8" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">文本 Blob</li><li id="1f82" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">算法:逻辑回归，朴素贝叶斯，SVM 和 LSTM</li></ol><p id="1b17" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们开始吧！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/f9c0dbb84a4b15c81102d23a3e9a8c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f8ubELzWeACYiXegM4SAYg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">远程办公</p></figure></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><h1 id="5e63" class="kz la it bd lb lc od le lf lg oe li lj jz of ka ll kc og kd ln kf oh kg lp lq bi translated">问题定式化</h1><p id="000d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我将介绍由 3000 个句子组成的<code class="fe oi oj ok ol b">sentiment</code>数据集，这些句子来自对<code class="fe oi oj ok ol b">imdb.com</code>、<code class="fe oi oj ok ol b">amazon.com</code>和<code class="fe oi oj ok ol b">yelp.com</code>的评论。每个句子根据它是来自正面评论(标记为<code class="fe oi oj ok ol b">1</code>)还是负面评论(标记为<code class="fe oi oj ok ol b">0</code>)来标记。</p><p id="3420" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">数据可以从<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences" rel="noopener ugc nofollow" target="_blank">网站</a>下载。或者也可以从<a class="ae ky" href="https://github.com/joetrankang/NLP.git" rel="noopener ugc nofollow" target="_blank">这里</a>下载(强烈推荐)。文件夹<code class="fe oi oj ok ol b">sentiment_labelled_sentences</code>(包含数据文件<code class="fe oi oj ok ol b">full_set.txt</code>)应该和你的笔记本在同一个目录下。</p></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><h1 id="ac50" class="kz la it bd lb lc od le lf lg oe li lj jz of ka ll kc og kd ln kf oh kg lp lq bi translated">加载和预处理数据</h1><h2 id="1c6a" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">设置和导入库</h2><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="cf18" class="ng la it ol b gy oq or l os ot">%matplotlib inline<br/>import string<br/>import numpy as np<br/>import matplotlib<br/>import matplotlib.pyplot as plt<br/>matplotlib.rc('xtick', labelsize=14) <br/>matplotlib.rc('ytick', labelsize=14)</span></pre><p id="bc5c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们加载数据并查看前 10 条评论</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="da27" class="ng la it ol b gy oq or l os ot">with open("sentiment_labelled_sentences/full_set.txt") as f:<br/>    content = f.readlines()</span><span id="639b" class="ng la it ol b gy ou or l os ot">content[0:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/f1061e960e1102023a9bec40cd0a2401.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Lt-bg1q64kjEifwUJ-Rng.png"/></div></div></figure><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="d6e2" class="ng la it ol b gy oq or l os ot">## Remove leading and trailing white space<br/>content = [x.strip() for x in content]</span><span id="6e05" class="ng la it ol b gy ou or l os ot">## Separate the sentences from the labels<br/>sentences = [x.split("\t")[0] for x in content]<br/>labels = [x.split("\t")[1] for x in content]</span><span id="f5ad" class="ng la it ol b gy ou or l os ot">sentences[0:10]<br/>labels[0:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/8d3ba452b84f03498f0943ddea248776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y9pNnwYGTbVYfeZIXucqjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">分开句子和标签</p></figure><p id="2582" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以在这里停下来。但对我来说，我更喜欢把 y 变换成(-1，1)的形式，其中-1 代表负，1 代表正。</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="e088" class="ng la it ol b gy oq or l os ot">## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'<br/>y = np.array(labels, dtype='int8')<br/>y = 2*y - 1</span></pre><p id="960c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">请注意，到目前为止，我们还没有对单词做任何修改！！！下一节重点是句子中的单词。</p><h2 id="c32d" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">预处理文本数据</h2><p id="0857" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">要将数据输入到任何模型中，输入的<strong class="lt iu">数据必须为<strong class="lt iu">矢量形式</strong>。我们将进行以下转换:</strong></p><ul class=""><li id="3780" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm ox my mz na bi translated">删除标点和数字</li><li id="cbca" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm ox my mz na bi translated">将所有单词转换成小写</li><li id="69c3" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm ox my mz na bi translated">删除<em class="oy">停用词</em>(例如，the，a，that，this，it，…)</li><li id="58dc" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm ox my mz na bi translated">将文本符号化</li><li id="73ef" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm ox my mz na bi translated">使用单词袋表示法将句子转换成向量</li></ul><p id="fbd1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我将在这里解释一些术语。</p><ol class=""><li id="69f7" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated"><code class="fe oi oj ok ol b">Stop words</code>:对手头的任务不感兴趣的常用词。这些通常包括冠词，如“a”和“the”，代词，如“I”和“他们”，以及介词，如“to”和“from”，…</li></ol><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="5031" class="ng la it ol b gy oq or l os ot">## Demonstrate ##<br/>def removeStopWords(stopWords, txt):<br/>    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])<br/>    return newtxt</span><span id="56d5" class="ng la it ol b gy ou or l os ot"><br/>stoppers = ['a', 'is', 'of','the','this','uhm','uh']<br/>removeStopWords(stoppers, "this is a test of the stop word removal code")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/576511edf2391f0223a2a9cef215620a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJKO6kWlZtqgK0yaPQTXzQ.png"/></div></div></figure><p id="1fc6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">或者，如果我们确实想要使用一套完整的常用停用词，我们可以使用<code class="fe oi oj ok ol b">NLTK</code></p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="115a" class="ng la it ol b gy oq or l os ot">from nltk.corpus import stopwords</span><span id="2b8e" class="ng la it ol b gy ou or l os ot">stops = stopwords.words("English")</span><span id="dc40" class="ng la it ol b gy ou or l os ot">removeStopWords(stops, "this is a test of the stop word removal code.")</span></pre><p id="95f8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="oy">同样的结果</em></p><p id="0f9d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.<code class="fe oi oj ok ol b">Corpus</code>:简单的文字集合。语序<strong class="lt iu">事关</strong>。“不伟大”和“伟大”是不同的</p><p id="de41" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.<code class="fe oi oj ok ol b">Document-Term Matrix</code>或<code class="fe oi oj ok ol b">Bag of Words</code> (BOW)只是文本句子(或文档)的矢量表示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/50f48850dceee5f0fbf2d6c7f421743f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XhGGFY8Yx6Owm-UteYAc_g.png"/></div></div></figure><p id="b4dd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">像这样表示一组要素的一种常见方法称为一键矢量。例如，假设我们文本集中的词汇是:</p><p id="2f9d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe oi oj ok ol b"><em class="oy">today, here, I, a, fine, sun, moon, bird, saw</em></code></p><p id="11f2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们要为之造弓的句子是:</p><p id="e24e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe oi oj ok ol b"><em class="oy">I saw a bird today </em></code></p><p id="92be" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe oi oj ok ol b"><em class="oy">1 0 1 1 0 0 1 1</em></code></p><p id="718c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了创造一个单词包，我们需要将一个长句或一份文件分解成更小的部分。这个过程叫做<code class="fe oi oj ok ol b">Tokenization</code>。最常见的标记化技术是将文本分解成单词。我们可以在 Scikit-Learn 中使用<code class="fe oi oj ok ol b">CountVectorizer</code>来实现这一点，其中每一行代表一个不同的文档，每一列代表一个不同的单词。</p><p id="fbc3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在让我们把所有东西放在一起，组成我们的数据集</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="38e5" class="ng la it ol b gy oq or l os ot">def full_remove(x, removal_list):<br/>    for w in removal_list:<br/>        x = x.replace(w, ' ')<br/>    return x</span><span id="5c63" class="ng la it ol b gy ou or l os ot">## Remove digits ##<br/>digits = [str(x) for x in range(10)]<br/>remove_digits = [full_remove(x, digits) for x in sentences]</span><span id="944f" class="ng la it ol b gy ou or l os ot">## Remove punctuation ##<br/>remove_punc = [full_remove(x, list(string.punctuation)) for x in remove_digits]</span><span id="24e2" class="ng la it ol b gy ou or l os ot">## Make everything lower-case and remove any white space ##<br/>sents_lower = [x.lower() for x in remove_punc]<br/>sents_lower = [x.strip() for x in sents_lower]</span><span id="70cf" class="ng la it ol b gy ou or l os ot">## Remove stop words ##<br/>from nltk.corpus import stopwords<br/>stops = stopwords.words("English")</span><span id="f3eb" class="ng la it ol b gy ou or l os ot">def removeStopWords(stopWords, txt):<br/>    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])<br/>    return newtxt</span><span id="f36e" class="ng la it ol b gy ou or l os ot">sents_processed = [removeStopWords(stops,x) for x in sents_lower]</span></pre><p id="7f1e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们看看现在我们的句子是什么样子的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/8b48a5b557eff0102a7a629fc50b017d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mOI9urASroU4754PSkFISQ.png"/></div></div></figure><p id="f50b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">嗯，等一下！去掉很多停用词会让很多句子失去意义。例如，“<code class="fe oi oj ok ol b">way plug us unless go converter</code>”对我来说没有任何意义。这是因为我们使用 NLTK 删除了所有常见的英语停用词。为了克服这个意义问题，让我们创建一套自己的停用词。</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="15c6" class="ng la it ol b gy oq or l os ot">stop_set = ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from']</span><span id="59aa" class="ng la it ol b gy ou or l os ot">sents_processed = [removeStopWords(stop_set,x) for x in sents_lower]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/950635f237c79fcd3f165f301c5afb0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*17cZBltVfref0lsgtEUTlQ.png"/></div></div></figure><p id="3198" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">就此打住，转到标记化就可以了。然而，人们可以继续使用<code class="fe oi oj ok ol b">stemming</code>。<code class="fe oi oj ok ol b">stemming</code>的目标是去掉单词中的前缀和后缀，将单词转换成它的基本形式，<code class="fe oi oj ok ol b">e.g. studying-&gt;study, beautiful-&gt;beauty, cared-&gt;care</code>，…在 NLTK 中，有两种流行的<code class="fe oi oj ok ol b">stemming</code>技术，称为 porter 和 lanscaster。[参考:<a class="ae ky" href="https://www.datacamp.com/community/tutorials/stemming-lemmatization-python?utm_source=adwords_ppc&amp;utm_campaignid=9942305733&amp;utm_adgroupid=100189364546&amp;utm_device=c&amp;utm_keyword=&amp;utm_matchtype=b&amp;utm_network=g&amp;utm_adpostion=&amp;utm_creative=332602034349&amp;utm_targetid=aud-299261629574:dsa-929501846124&amp;utm_loc_interest_ms=&amp;utm_loc_physical_ms=9062542&amp;gclid=Cj0KCQjwuJz3BRDTARIsAMg-HxXScOjSXlUZMrEMQYLQWaEnrFIECMWrUZF3rnIWap5OyoW5QvtevvoaAjdkEALw_wcB" rel="noopener ugc nofollow" target="_blank">数据营</a></p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="0fcb" class="ng la it ol b gy oq or l os ot">import nltk<br/>def stem_with_porter(words):<br/>    porter = nltk.PorterStemmer()<br/>    new_words = [porter.stem(w) for w in words]<br/>    return new_words<br/>    <br/>def stem_with_lancaster(words):<br/>    porter = nltk.LancasterStemmer()<br/>    new_words = [porter.stem(w) for w in words]<br/>    return new_words    </span><span id="9e84" class="ng la it ol b gy ou or l os ot">## Demonstrate ##    <br/>str = "Please don't unbuckle your seat-belt while I am driving, he said"</span><span id="bef1" class="ng la it ol b gy ou or l os ot">print("porter:", stem_with_porter(str.split()))<br/><br/>print("lancaster:", stem_with_lancaster(str.split()))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/c69275975b23657670135ec926804647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VGrwkb-Ysqgv13t-LwecAg.png"/></div></div></figure><p id="eb35" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们试试我们的<code class="fe oi oj ok ol b">sents_processed</code>看看它是否有意义。</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="5348" class="ng la it ol b gy oq or l os ot">porter = [stem_with_porter(x.split()) for x in sents_processed]</span><span id="9fdf" class="ng la it ol b gy ou or l os ot">porter = [" ".join(i) for i in porter]</span><span id="b295" class="ng la it ol b gy ou or l os ot">porter[0:10]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/cd761fc9a69f723393d060c59f4dc2d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4PCyTXg-N8kgY7BH6AYkTg.png"/></div></div></figure><p id="3cfb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">出现一些奇怪的变化，例如非常-&gt;真实，质量-&gt;质量，价值-&gt;价值，…</p><p id="1ec7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.<code class="fe oi oj ok ol b">Term Document Inverse Document Frequency</code> (TD/IDF)。这是在多个文档的上下文中，一个单词在一个文档中的<em class="oy">相对重要性的度量。在我们这里，多重审查。</em></p><p id="aea7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们从 TD 部分开始——这只是文档中单词的<strong class="lt iu">归一化频率</strong>:</p><p id="15c0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe oi oj ok ol b">(word count in document) / (total words in document)</code><br/><br/>IDF 是所有文档中单词唯一性的加权。以下是 TD/IDF 的完整公式:</p><p id="958d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe oi oj ok ol b">td_idf(t,d) = wc(t,d)/wc(d) / dc(t)/dc()</code></p><p id="6d51" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中:<br/>—<code class="fe oi oj ok ol b">wc(t,d)</code>=【t 项在文档 d <br/>中的出现次数】— <code class="fe oi oj ok ol b">wc(d)</code> =【文档 d】中的字数<br/>—<code class="fe oi oj ok ol b">dc(t)</code>=【t 项在集合中至少出现 1 次的文档数】— <code class="fe oi oj ok ol b">dc()</code> =【集合中的文档数】</p><p id="480f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，让我们创建一个单词包，并规范文本</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="ad31" class="ng la it ol b gy oq or l os ot">from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.feature_extraction.text import TfidfTransformer</span><span id="703b" class="ng la it ol b gy ou or l os ot">vectorizer = CountVectorizer(analyzer = "word", <br/>                             preprocessor = None, <br/>                             stop_words =  'english', <br/>                             max_features = 6000, ngram_range=(1,5))</span><span id="c549" class="ng la it ol b gy ou or l os ot">data_features = vectorizer.fit_transform(sents_processed)<br/>tfidf_transformer = TfidfTransformer()<br/>data_features_tfidf = tfidf_transformer.fit_transform(data_features)<br/>data_mat = data_features_tfidf.toarray()</span></pre><p id="c552" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在<code class="fe oi oj ok ol b">data_mat</code>是我们的文档术语矩阵。输入已准备好投入模型。让我们创建训练集和测试集。在这里，我将数据分为 2500 个句子的训练集和 500 个句子的测试集(其中 250 个是正面的，250 个是负面的)。</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="d9c0" class="ng la it ol b gy oq or l os ot">np.random.seed(0)<br/>test_index = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))</span><span id="adfb" class="ng la it ol b gy ou or l os ot">train_index = list(set(range(len(labels))) - set(test_index))</span><span id="cbec" class="ng la it ol b gy ou or l os ot">train_data = data_mat[train_index,]<br/>train_labels = y[train_index]</span><span id="19ea" class="ng la it ol b gy ou or l os ot">test_data = data_mat[test_index,]<br/>test_labels = y[test_index]</span></pre></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><h1 id="4927" class="kz la it bd lb lc od le lf lg oe li lj jz of ka ll kc og kd ln kf oh kg lp lq bi translated">文本 Blob</h1><ol class=""><li id="a16f" class="ms mt it lt b lu lv lx ly ma ns me nt mi nu mm mx my mz na bi translated"><code class="fe oi oj ok ol b"><strong class="lt iu">TextBlob</strong></code>:语言学研究者根据他们的专业领域给单词的情感贴上标签。单词的情感可以根据它在句子中的位置而变化。<code class="fe oi oj ok ol b">TextBlob</code>模块允许我们利用这些标签。<code class="fe oi oj ok ol b">TextBlod</code>找到<strong class="lt iu">它可以分配<strong class="lt iu">极性</strong>和<strong class="lt iu">主观性</strong>的所有单词和短语</strong>，然后<strong class="lt iu">将它们全部平均</strong></li><li id="e068" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><strong class="lt iu">情感标签</strong>:语料库中的每个单词都根据极性和主观性进行标记(还有更多标签，但我们现在将忽略它们)。一个语料库的情感是这些的平均值。</li></ol><ul class=""><li id="9436" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm ox my mz na bi translated"><strong class="lt iu"> <em class="oy">极性</em> </strong>:一个字有多正或多负。-1 是非常消极的。+1 很正。</li><li id="4609" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm ox my mz na bi translated"><strong class="lt iu"> <em class="oy">主观性</em> </strong>:一个词有多主观，或者说有多固执己见。0 是事实。+1 在很大程度上是一种观点。</li></ul><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="bb13" class="ng la it ol b gy oq or l os ot">from textblob import TextBlob</span><span id="3a6c" class="ng la it ol b gy ou or l os ot">#Create polarity function and subjectivity function</span><span id="7d92" class="ng la it ol b gy ou or l os ot">pol = lambda x: TextBlob(x).sentiment.polarity</span><span id="ac9f" class="ng la it ol b gy ou or l os ot">sub = lambda x: TextBlob(x).sentiment.subjectivity</span><span id="e4e3" class="ng la it ol b gy ou or l os ot">pol_list = [pol(x) for x in sents_processed]</span><span id="a463" class="ng la it ol b gy ou or l os ot">sub_list = [sub(x) for x in sents_processed]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/83c6dc3d64285efe48a20c64900ab2f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E8-VraIQjRpUYGiI_eGKdQ.png"/></div></div></figure><p id="eb0e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是一种基于规则的方法，决定了评论的情绪(极性和主观性)。</p><p id="a1c8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下一节将介绍各种算法。</p></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><h1 id="5aeb" class="kz la it bd lb lc od le lf lg oe li lj jz of ka ll kc og kd ln kf oh kg lp lq bi translated">逻辑回归</h1><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="b642" class="ng la it ol b gy oq or l os ot">from sklearn.linear_model import SGDClassifier</span><span id="bfa3" class="ng la it ol b gy ou or l os ot">## Fit logistic classifier on training data<br/>clf = SGDClassifier(loss="log", penalty="none")<br/>clf.fit(train_data, train_labels)</span><span id="1673" class="ng la it ol b gy ou or l os ot">## Pull out the parameters (w,b) of the logistic regression model<br/>w = clf.coef_[0,:]<br/>b = clf.intercept_</span><span id="8d04" class="ng la it ol b gy ou or l os ot">## Get predictions on training and test data<br/>preds_train = clf.predict(train_data)<br/>preds_test = clf.predict(test_data)</span><span id="709f" class="ng la it ol b gy ou or l os ot">## Compute errors<br/>errs_train = np.sum((preds_train &gt; 0.0) != (train_labels &gt; 0.0))<br/>errs_test = np.sum((preds_test &gt; 0.0) != (test_labels &gt; 0.0))</span><span id="3fbf" class="ng la it ol b gy ou or l os ot">print("Training error: ", float(errs_train)/len(train_labels))<br/>print("Test error: ", float(errs_test)/len(test_labels))</span><span id="0189" class="ng la it ol b gy ou or l os ot"><strong class="ol iu">Training error:  0.0116<br/>Test error:  0.184</strong></span></pre><p id="3c75" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="oy">影响力大的词</em> </strong></p><p id="927a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">决定一个句子是否为肯定的，哪些词最重要？作为对此的第一近似，我们简单地取其系数在<code class="fe oi oj ok ol b">w</code>中具有最大正值的单词。</p><p id="ba0b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">同样，我们看那些在<code class="fe oi oj ok ol b">w</code>中系数具有最大负值的单词，我们认为这些单词对负面预测有影响。</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="78ed" class="ng la it ol b gy oq or l os ot">## Convert vocabulary into a list:<br/>vocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])</span><span id="02f6" class="ng la it ol b gy ou or l os ot">## Get indices of sorting w<br/>inds = np.argsort(w)</span><span id="78d2" class="ng la it ol b gy ou or l os ot">## Words with large negative values<br/>neg_inds = inds[0:50]<br/>print("Highly negative words: ")<br/>print([str(x) for x in list(vocab[neg_inds])])</span><span id="aced" class="ng la it ol b gy ou or l os ot">## Words with large positive values<br/>pos_inds = inds[-49:-1]<br/>print("Highly positive words: ")</span><span id="bbe3" class="ng la it ol b gy ou or l os ot">print([str(x) for x in list(vocab[pos_inds])])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/e457428660ae8460c8578d10f2a54f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EWb8oYyv9kDHvpRzCOUP1Q.png"/></div></div></figure><p id="b866" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="oy">创建字云</em> </strong></p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="b68f" class="ng la it ol b gy oq or l os ot">from wordcloud import WordCloud<br/>wc = WordCloud(stopwords=stop_set, background_color="white", colormap="Dark2",<br/>               max_font_size=150, random_state=42)</span><span id="d3b5" class="ng la it ol b gy ou or l os ot">#plt.rcParams['figure.figsize'] = [16, 6]</span><span id="0543" class="ng la it ol b gy ou or l os ot">wc.generate(" ".join(list(vocab[neg_inds])))</span><span id="0aea" class="ng la it ol b gy ou or l os ot">plt.imshow(wc, interpolation="bilinear")<br/>plt.axis("off")    <br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/2648a2fa933b0bd9c65298c6587ab77f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6pd0UtRdCAXi_qy-nij5Kw.png"/></div></div></figure></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><h1 id="9fea" class="kz la it bd lb lc od le lf lg oe li lj jz of ka ll kc og kd ln kf oh kg lp lq bi translated">朴素贝叶斯</h1><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="2142" class="ng la it ol b gy oq or l os ot">from sklearn.naive_bayes import MultinomialNB<br/>nb_clf = MultinomialNB().fit(train_data, train_labels)</span><span id="a39a" class="ng la it ol b gy ou or l os ot">nb_preds_test = nb_clf.predict(test_data)<br/>nb_errs_test = np.sum((nb_preds_test &gt; 0.0) != (test_labels &gt; 0.0))<br/>print("Test error: ", float(nb_errs_test)/len(test_labels))</span><span id="24c5" class="ng la it ol b gy ou or l os ot"><strong class="ol iu">Test error:  0.174</strong></span></pre><p id="5d34" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们来做一些预测案例。[1]表示正，而[-1]表示负</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="5618" class="ng la it ol b gy oq or l os ot">print(nb_clf.predict(vectorizer.transform(["<strong class="ol iu">It's a sad movie but very good</strong>"])))</span><span id="8a6d" class="ng la it ol b gy ou or l os ot"><strong class="ol iu">[1]</strong></span><span id="a1bd" class="ng la it ol b gy ou or l os ot">print(nb_clf.predict(vectorizer.transform(["<strong class="ol iu">Waste of my time</strong>"])))</span><span id="4aec" class="ng la it ol b gy ou or l os ot"><strong class="ol iu">[-1]</strong></span><span id="f24e" class="ng la it ol b gy ou or l os ot">print(nb_clf.predict(vectorizer.transform(["<strong class="ol iu">It is not what like</strong>"])))</span><span id="f381" class="ng la it ol b gy ou or l os ot"><strong class="ol iu">[-1]</strong></span><span id="d9e4" class="ng la it ol b gy ou or l os ot">print(nb_clf.predict(vectorizer.transform(["<strong class="ol iu">It is not what I m looking for</strong>"])))</span><span id="9d6c" class="ng la it ol b gy ou or l os ot"><strong class="ol iu">[1]</strong></span></pre><p id="9745" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后一个测试用例有问题。这应该是负面评论，但模型预测是正面的。</p></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><h1 id="cf98" class="kz la it bd lb lc od le lf lg oe li lj jz of ka ll kc og kd ln kf oh kg lp lq bi translated">SVM</h1><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="649c" class="ng la it ol b gy oq or l os ot">from sklearn.linear_model import SGDClassifier</span><span id="ba83" class="ng la it ol b gy ou or l os ot">svm_clf = SGDClassifier(loss="hinge", penalty='l2')<br/>svm_clf.fit(train_data, train_labels)</span><span id="ca7a" class="ng la it ol b gy ou or l os ot">svm_preds_test = svm_clf.predict(test_data)<br/>svm_errs_test = np.sum((svm_preds_test &gt; 0.0) != (test_labels &gt; 0.0))</span><span id="fa51" class="ng la it ol b gy ou or l os ot">print("Test error: ", float(svm_errs_test)/len(test_labels))</span><span id="28da" class="ng la it ol b gy ou or l os ot"><strong class="ol iu">Test error:  0.2</strong></span></pre><p id="6524" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">再次，让我们做一些预测</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="93cd" class="ng la it ol b gy oq or l os ot">print(svm_clf.predict(vectorizer.transform(["<strong class="ol iu">This is not what I like</strong>"])))</span><span id="55c7" class="ng la it ol b gy ou or l os ot"><strong class="ol iu">[-1]</strong></span><span id="418a" class="ng la it ol b gy ou or l os ot">print(svm_clf.predict(vectorizer.transform(["<strong class="ol iu">It is not what I am looking for</strong>"])))</span><span id="db2c" class="ng la it ol b gy ou or l os ot"><strong class="ol iu">[-1]</strong></span><span id="d775" class="ng la it ol b gy ou or l os ot">print(svm_clf.predict(vectorizer.transform(["<strong class="ol iu">I would not recommend this movie</strong>"])))</span><span id="0ef5" class="ng la it ol b gy ou or l os ot"><strong class="ol iu">[1]</strong></span></pre><p id="1cc4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">SVM 可以正确预测评论“这不是我要找的”。然而，它无法预测评论“我不推荐这部电影”。</p></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><h1 id="12f6" class="kz la it bd lb lc od le lf lg oe li lj jz of ka ll kc og kd ln kf oh kg lp lq bi translated">LSTM 网络公司</h1><p id="d71e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">关于 LSTM 网络的详细讨论可以在<a class="ae ky" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="0fb9" class="ng la it ol b gy oq or l os ot">from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import LSTM<br/>from keras.layers import SpatialDropout1D<br/>from keras.layers.embeddings import Embedding<br/>from keras.preprocessing import sequence<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.callbacks import EarlyStopping</span><span id="7d27" class="ng la it ol b gy ou or l os ot">max_review_length = 200</span><span id="e228" class="ng la it ol b gy ou or l os ot">tokenizer = Tokenizer(num_words=10000,  #max no. of unique words to keep<br/>                      filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~', <br/>                      lower=True #convert to lower case<br/>                     )<br/>tokenizer.fit_on_texts(sents_processed)</span></pre><p id="af46" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">截断并填充输入序列，使它们长度相同</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="6df2" class="ng la it ol b gy oq or l os ot">X = tokenizer.texts_to_sequences(sents_processed)<br/>X = sequence.pad_sequences(X, maxlen= max_review_length)<br/>print('Shape of data tensor:', X.shape)</span><span id="c5b2" class="ng la it ol b gy ou or l os ot"><strong class="ol iu">Shape of data tensor: (3000, 200)</strong></span></pre><p id="e1cd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">回想一下，y 是 1 和-1 的向量。现在我把它改成一个有 2 列的矩阵，分别代表-1 和 1。</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="9d5b" class="ng la it ol b gy oq or l os ot">import pandas as pd<br/>Y=pd.get_dummies(y).values<br/>Y</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/989d7d48c6b787eb7803e89de9a96564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*jmSgJ5m5LuOfXRytX0VG4w.png"/></div></figure><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="4790" class="ng la it ol b gy oq or l os ot">np.random.seed(0)<br/>test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))<br/>train_inds = list(set(range(len(labels))) - set(test_inds))</span><span id="6699" class="ng la it ol b gy ou or l os ot">train_data = X[train_inds,]<br/>train_labels = Y[train_inds]</span><span id="70e5" class="ng la it ol b gy ou or l os ot">test_data = X[test_inds,]<br/>test_labels = Y[test_inds]</span></pre><p id="865f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="oy">创建网络</em> </strong></p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="622b" class="ng la it ol b gy oq or l os ot">EMBEDDING_DIM = 200</span><span id="a20d" class="ng la it ol b gy ou or l os ot">model = Sequential()<br/>model.add(Embedding(10000, EMBEDDING_DIM, input_length=X.shape[1]))<br/>model.add(SpatialDropout1D(0.2))<br/>model.add(LSTM(250, dropout=0.2,return_sequences=True))<br/>model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))<br/>model.add(Dense(2, activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>print(model.summary())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/bcc4c085f4d9fe4ed106a2759923161f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2I97TX1AmGUL5e7zTi3XeA.png"/></div></div></figure><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="f1e3" class="ng la it ol b gy oq or l os ot">epochs = 2<br/>batch_size = 40</span><span id="e0a9" class="ng la it ol b gy ou or l os ot">model.fit(train_data, train_labels, <br/>          epochs=epochs, <br/>          batch_size=batch_size,<br/>          validation_split=0.1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/92a0b0662b1c61fa02099951cbed5b07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tkvyqdn9h5fbXWjKjQjRCA.png"/></div></div></figure><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="8de3" class="ng la it ol b gy oq or l os ot">loss, acc = model.evaluate(test_data, test_labels, verbose=2,<br/>                            batch_size=batch_size)</span><span id="bef9" class="ng la it ol b gy ou or l os ot">print(f"loss: {loss}")<br/>print(f"Validation accuracy: {acc}")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/5b394cb24455590334c3d9b4ebd6d817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cgrVMxPf7B6go0Cx_qz-7w.png"/></div></div></figure><p id="63b0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在迄今为止训练的所有模型中，LSTM 表现最好，即逻辑斯蒂、朴素贝叶斯和 SVM。现在让我们看看它如何预测一个测试用例</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="8cc8" class="ng la it ol b gy oq or l os ot">outcome_labels = ['Negative', 'Positive']</span><span id="a41d" class="ng la it ol b gy ou or l os ot">new = ["<strong class="ol iu">I would not recommend this movie</strong>"]<br/>    <br/>seq = tokenizer.texts_to_sequences(new)<br/>padded = sequence.pad_sequences(seq, maxlen=max_review_length)<br/>pred = model.predict(padded)<br/>print("Probability distribution: ", pred)<br/>print("Is this a Positive or Negative review? ")<br/>print(outcome_labels[np.argmax(pred)])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/35c5c9b1151bcf23c262216be7734c77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sa-FQCfwqSRPYZAE271ySA.png"/></div></div></figure><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="cd01" class="ng la it ol b gy oq or l os ot">new = ["<strong class="ol iu">It is not what i am looking for</strong>"]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/e1dfce7cf10aec33965ddaf6de38bc18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HcMuz950suxGjuZc94nAwQ.png"/></div></div></figure><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="5166" class="ng la it ol b gy oq or l os ot">new = ["<strong class="ol iu">This isn't what i am looking for</strong>"]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/359b8bbe0f8307af4fa1a3b0c5838444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wjpNaO0va74RPEoLOZz9YQ.png"/></div></div></figure><p id="3c83" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这种情况下，否定和肯定的概率差别不大。LSTM 模型认为这是积极的。</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="f444" class="ng la it ol b gy oq or l os ot">new = ["<strong class="ol iu">I wouldn't recommend this movie</strong>"]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/b019d1f72a391b3b575f067722aba4ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5erQH_AQwFsf9nOkKr7VjQ.png"/></div></div></figure><p id="06cc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个评论也是如此。因此，这意味着我们的模型不能区分<code class="fe oi oj ok ol b">n't</code>和<code class="fe oi oj ok ol b">not</code>。一个可能的解决方案是，在预处理步骤中，不是删除所有标点符号，而是将所有的<code class="fe oi oj ok ol b">n't</code>简写改为<code class="fe oi oj ok ol b">not</code>。这可以简单地用 Python 中的<code class="fe oi oj ok ol b">re</code>模块来完成。您可以亲自查看一下，看看我们的模型预测是如何改进的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/37135a8c1644d1f31b747fa15af8971c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*79Dd5VHTLnsc55atA0cxng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将“不”改为“不”并再次运行模型后</p></figure><p id="c80d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">就是这样！希望你们喜欢这篇文章，并从中有所收获。如果你有任何问题，请在下面的评论区写下来。谢谢你的阅读。祝您愉快，保重！！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/b0d3e0207f96406051aebe61938e0c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7CuVf4FMelLefwbE"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@lux17?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卢卡斯·克拉拉</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div></div>    
</body>
</html>