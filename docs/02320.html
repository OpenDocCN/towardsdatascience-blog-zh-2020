<html>
<head>
<title>Creating Word Embeddings: Coding the Word2Vec Algorithm in Python using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">创建单词嵌入:使用深度学习在 Python 中编码 Word2Vec 算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8?source=collection_archive---------1-----------------------#2020-03-05">https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8?source=collection_archive---------1-----------------------#2020-03-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="233c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用深度学习理解单词嵌入创作背后的直觉</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d3c67f3b236f9033acfa39dbbc849644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OEmWDt4eztOcm5pr2QbxfA.png"/></div></div></figure><p id="5ad2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当我在写另一篇展示如何在文本分类目标中使用单词嵌入的文章时，我意识到我总是使用从外部来源(例如<a class="ae lq" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a>)下载的预训练单词嵌入。我开始思考如何从头开始创建单词嵌入，因此这就是这篇文章的诞生。我的主要目标是让人们通过我的代码片段阅读这篇文章，并深入理解创建单词的向量表示背后的逻辑。</p><p id="2bf8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">完整的代码可以在这里找到:</p><p id="786e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">https://github.com/Eligijus112/word-embedding-creation<a class="ae lq" href="https://github.com/Eligijus112/word-embedding-creation" rel="noopener ugc nofollow" target="_blank"/></p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="9f69" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">单词 embeddings 的创建的简短版本可以概括为以下流程:</p><p id="75da" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">阅读文本<strong class="kw iu"> - &gt; </strong>预处理文本<strong class="kw iu"> - &gt; </strong>创建(X，Y)数据点<strong class="kw iu"> - &gt; </strong>创建一个热编码(X，Y)矩阵<strong class="kw iu"> - &gt; </strong>训练一个神经网络<strong class="kw iu"> - &gt; </strong>从输入层提取权重</p><p id="045d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这篇文章中，我将简要解释每一步。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="0e25" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">来自 wiki: <strong class="kw iu">单词嵌入</strong>是一组<a class="ae lq" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理</a> (NLP)中的<a class="ae lq" href="https://en.wikipedia.org/wiki/Language_model" rel="noopener ugc nofollow" target="_blank">语言建模</a>和<a class="ae lq" href="https://en.wikipedia.org/wiki/Feature_learning" rel="noopener ugc nofollow" target="_blank">特征学习</a>技术的统称，其中来自词汇表的<strong class="kw iu">单词或短语被映射到实数向量。</strong>术语 word2vec 字面翻译为<strong class="kw iu">字到矢量</strong>。举个例子，</p><p id="739f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">“爸爸”= [0.1548，0.4848，…，1.864]</p><p id="1dcb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">"妈妈" = [0.8785，0.8974，…，2.794]</p><p id="eabb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">单词嵌入最重要的特征是语义上相似的单词之间的距离(欧几里德距离、余弦距离或其他距离)比没有语义关系的单词之间的距离要小。例如，像“妈妈”和“爸爸”这样的词应该比“妈妈”和“番茄酱”或“爸爸”和“黄油”更靠近。</p><p id="e634" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用具有一个输入层、一个隐藏层和一个输出层的神经网络来创建单词嵌入。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ly"><img src="../Images/21abf1cef59cf3210bd4a803a1645f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eElnVQPeJx3kQOR4"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">在<a class="ae lq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lq" href="https://unsplash.com/@heftiba?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Toa Heftiba </a>拍摄的照片</p></figure><p id="c798" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">要创建单词嵌入，首先需要的是文本。让我们创建一个简单的例子，用 12 句话陈述一些关于一个虚构的皇室家族的众所周知的事实:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="6963" class="mi mj it me b gy mk ml l mm mn">The future king is the prince</span><span id="5c19" class="mi mj it me b gy mo ml l mm mn">Daughter is the princess</span><span id="d20a" class="mi mj it me b gy mo ml l mm mn">Son is the prince</span><span id="291d" class="mi mj it me b gy mo ml l mm mn">Only a man can be a king</span><span id="1c30" class="mi mj it me b gy mo ml l mm mn">Only a woman can be a queen</span><span id="51c2" class="mi mj it me b gy mo ml l mm mn">The princess will be a queen</span><span id="7817" class="mi mj it me b gy mo ml l mm mn">Queen and king rule the realm</span><span id="7254" class="mi mj it me b gy mo ml l mm mn">The prince is a strong man</span><span id="85c6" class="mi mj it me b gy mo ml l mm mn">The princess is a beautiful woman</span><span id="722f" class="mi mj it me b gy mo ml l mm mn">The royal family is the king and queen and their children</span><span id="6fac" class="mi mj it me b gy mo ml l mm mn">Prince is only a boy now</span><span id="e672" class="mi mj it me b gy mo ml l mm mn">A boy will be a man</span></pre><p id="547b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">计算机不理解国王、王子和男人在语义上比王后、公主和女儿更接近。它看到的都是编码成二进制的字符。那么我们如何让计算机理解某些单词之间的关系呢？<strong class="kw iu">通过创建 X 和 Y 矩阵并使用神经网络。</strong></p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="2424" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当创建用于单词嵌入的训练矩阵时，超参数之一是上下文(w) 的<strong class="kw iu">窗口大小。最小值为 1，因为没有上下文，算法无法工作。让我们看第一句话，假设 w = 2。</strong></p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="f31d" class="mi mj it me b gy mk ml l mm mn"><strong class="me iu">The</strong> <em class="mp">future king</em> is the prince</span></pre><p id="fdfa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">粗体字<strong class="kw iu"/>称为焦点字，左边的 2 个字和右边的 2 个字(因为 w = 2)是所谓的上下文字。所以我们可以开始建立我们的数据点:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="4497" class="mi mj it me b gy mk ml l mm mn">(<strong class="me iu">The</strong>, future), (<strong class="me iu">The</strong>, king)</span></pre><p id="d074" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，如果我们浏览整个句子，我们会得到:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="87f7" class="mi mj it me b gy mk ml l mm mn">(<strong class="me iu">The</strong>, future), (<strong class="me iu">The</strong>, king), <br/>(<strong class="me iu">future</strong>, the), (<strong class="me iu">future</strong>, king), (<strong class="me iu">future</strong>, is)<br/>(<strong class="me iu">king</strong>, the), (<strong class="me iu">king</strong>, future), (<strong class="me iu">king</strong>, is), (<strong class="me iu">king</strong>, the)<br/>(<strong class="me iu">is</strong>, future), (<strong class="me iu">is</strong>, king), (<strong class="me iu">is</strong>, the), (<strong class="me iu">is</strong>, prince),<br/>(<strong class="me iu">the</strong>, king), (<strong class="me iu">the</strong>, is), (<strong class="me iu">the</strong>, prince)<br/>(<strong class="me iu">prince</strong>, is), (<strong class="me iu">prince</strong>, the)</span></pre><p id="b56c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从 6 个单词中，我们能够创建 18 个数据点。在实践中，我们对文本做了一些预处理，删除了停用词，如<strong class="kw iu"> is，the，a 等。</strong>通过扫描整个文本文档并添加数据，我们创建了初始输入，然后可以将其转换为矩阵形式。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">文本预处理功能</p></figure><p id="269a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">给定字符串列表<strong class="kw iu">文本</strong>创建(X，Y)单词对的完整管道:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">数据点的创建</p></figure><p id="75f1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">创建的数据点的第一个条目:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="30e4" class="mi mj it me b gy mk ml l mm mn">['future', 'king'],<br/>['future', 'prince'],<br/>['king', 'prince'],<br/>['king', 'future'],<br/>['prince', 'king'],<br/>['prince', 'future'],<br/>['daughter', 'princess'],<br/>['princess', 'daughter'],<br/>['son', 'prince']<br/>...</span></pre></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="ed52" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在最初创建数据点之后，我们需要为词汇表中的每个唯一单词分配一个唯一的整数(通常称为 index)。这将在创建<strong class="kw iu">独热编码矢量时进一步使用。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">创建独特的单词词典</p></figure><p id="88cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对文本使用上述函数后，我们得到字典:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="3618" class="mi mj it me b gy mk ml l mm mn"><strong class="me iu">unique_word_dict</strong> = {<br/> 'beautiful': 0,<br/> 'boy': 1,<br/> 'can': 2,<br/> 'children': 3,<br/> 'daughter': 4,<br/> 'family': 5,<br/> 'future': 6,<br/> 'king': 7,<br/> 'man': 8,<br/> 'now': 9,<br/> 'only': 10,<br/> 'prince': 11,<br/> 'princess': 12,<br/> 'queen': 13,<br/> 'realm': 14,<br/> 'royal': 15,<br/> 'rule': 16,<br/> 'son': 17,<br/> 'strong': 18,<br/> 'their': 19,<br/> 'woman': 20<br/>}</span></pre></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="23c8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">到目前为止，我们所创建的仍然不是神经网络友好的，因为我们所拥有的数据是成对的<strong class="kw iu">(焦点词，上下文词)</strong>。为了让计算机开始计算，我们需要一种聪明的方法将这些数据点转换成由数字组成的数据点。一个聪明的方法是<strong class="kw iu">一键编码</strong>技术。</p><p id="43de" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一键编码将一个单词转换成一个向量，该向量由 0 和一个表示字符串的坐标组成，等于 1。向量大小等于文档中唯一单词的数量。例如，让我们定义一个简单的字符串列表:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="f991" class="mi mj it me b gy mk ml l mm mn">a = ['blue', 'sky', 'blue', 'car']</span></pre><p id="8506" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有三个独特的词:蓝色，天空和汽车。每个单词一个热表示:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="dbc2" class="mi mj it me b gy mk ml l mm mn">'blue' = [1, 0, 0]<br/>'car' = [0, 1, 0]<br/>'sky' = [0, 0, 1]</span></pre><p id="4704" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，列表可以转换成矩阵:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="3f24" class="mi mj it me b gy mk ml l mm mn">A = <br/>[<br/>1, 0, 0<br/>0, 0, 1<br/>1, 0, 0<br/>0, 1, 0<br/>]</span></pre><p id="f89b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将用完全相同的技术创建两个矩阵，X 和 Y。将使用焦点词创建<strong class="kw iu"> X 矩阵，使用上下文词创建<strong class="kw iu"> Y 矩阵。</strong></strong></p><p id="3b55" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">回想一下我们根据版税文本创建的前三个数据点:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="a6aa" class="mi mj it me b gy mk ml l mm mn">['future', 'king'],<br/>['future', 'prince'],<br/>['king', 'prince']</span></pre><p id="9c45" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">python 中的一键编码 X 矩阵(单词 f<strong class="kw iu">future，future，king </strong>)应该是:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="012c" class="mi mj it me b gy mk ml l mm mn">[array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0.]),<br/> array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0.]),<br/> array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0.])]</span></pre><p id="3d7c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">python 中一键编码的 Y 矩阵(单词<strong class="kw iu"> king，prince，prince </strong>)应该是:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="7b38" class="mi mj it me b gy mk ml l mm mn">[array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0.]),<br/> array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0.]),<br/> array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0.])]</span></pre><p id="cb90" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些矩阵的最终大小将是<strong class="kw iu">n×m，</strong>其中</p><p id="1d25" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> n </strong> -创建的数据点的数量(焦点词和上下文词对)</p><p id="3569" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">m</strong>——唯一字的数量</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">创建 X 和 Y 矩阵</p></figure></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="f914" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们现在有了从焦点单词和上下文单词对构建的 X 和 Y 矩阵。下一步是选择嵌入维度。我将选择维数等于 2，以便稍后绘制单词，并查看相似的单词是否形成簇。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/9a83711f73886e662f2b60d935ef93ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ne_4myEbwCT8rEEcLZ25tg.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">神经网络体系结构</p></figure><p id="958a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">隐层维度就是我们的单词嵌入的大小。输出层激活功能是<strong class="kw iu"> softmax。</strong>隐藏层的激活函数是<strong class="kw iu">线性的。</strong>输入维数等于唯一字的总数(记住，我们的 X 矩阵的维数是 n×21)。每个输入节点将有两个权重将其连接到隐藏层。这些权重就是单词嵌入！在网络的训练之后，我们提取这些权重并去除所有剩余的权重。我们不一定关心输出。</p><p id="dda5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于网络的训练，我们将使用 keras 和 tensorflow:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">训练和获得重量</p></figure><p id="49c6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在网络训练之后，我们可以获得权重并绘制结果:</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="5b4b" class="mi mj it me b gy mk ml l mm mn">import matplotlib.pyplot as plt</span><span id="ba75" class="mi mj it me b gy mo ml l mm mn">plt.figure(figsize=(10, 10))</span><span id="8acd" class="mi mj it me b gy mo ml l mm mn">for word in list(unique_word_dict.keys()):<br/>  coord = embedding_dict.get(word)<br/>  plt.scatter(coord[0], coord[1])<br/>  plt.annotate(word, (coord[0], coord[1]))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/63a92bfe53275a44aeba5f7fc08aed2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ugyK9fI1VJLT_FYkcsfjIg.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">嵌入的可视化</p></figure><p id="a8ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到，在剧情的各个角落，出现了‘男人’，‘未来’，‘王子’，‘男孩’和‘女儿’，‘女人’，‘公主’这样的字眼，形成一簇簇。所有这些都是通过 21 个独特的单词和 12 个句子实现的。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="c760" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通常在实践中，使用预训练的单词嵌入，典型的单词嵌入维数为 100、200 或 300。我个人使用这里存储的嵌入:\<a class="ae lq" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a>。</p></div></div>    
</body>
</html>