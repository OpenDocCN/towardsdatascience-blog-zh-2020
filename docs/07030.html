<html>
<head>
<title>When Your Regression Model’s Errors Contain Two Peaks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当回归模型的误差包含两个峰值时</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/when-your-regression-models-errors-contain-two-peaks-13d835686ca?source=collection_archive---------15-----------------------#2020-05-30">https://towardsdatascience.com/when-your-regression-models-errors-contain-two-peaks-13d835686ca?source=collection_archive---------15-----------------------#2020-05-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b8ddd210427168f417c0b9491dfbedbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rTNWt23UJotdCAfu-oUYuw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>提供)</p></figure><div class=""/><div class=""><h2 id="e115" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">关于处理双峰残差的 Python 教程</h2></div><p id="430b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">原始残差是实际值和由经过训练的回归模型预测的值之间的差值。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lu"><img src="../Images/377e1e71191213551cfa779ca9c07533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uoGLR9T-6_1hIlPhu2d_rg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">残差=实际值-预测值(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>提供)</p></figure><p id="643a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">残差的频率分布图可以很好地判断模型是否正确指定，即模型是否是数据集的正确模型，是否考虑了所有重要的回归变量，以及模型是否以无偏的方式拟合了数据。</p><p id="a428" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您通常希望看到的是以零为中心的正态分布残差图，如下所示。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/a05b8137b8a01e5c3085e9be359501da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*Lc9Kozm7De_8-jc7dPK-9g.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">残差的正态分布频率图(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>提供)</p></figure><p id="b8c8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">残差的正态分布频率图是一个精选的、合适的模型的标志。</p><p id="29db" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但残差图往往是偏斜的，或者有厚尾或者薄尾，有时也不是以零为中心。有办法解决这些问题。有时人们不得不接受某种程度的不正常。</p><p id="ef88" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将向您展示当您的模型的残差变成双峰时该怎么办，即它们有两个峰值而不是一个，如下所示:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b8ddd210427168f417c0b9491dfbedbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rTNWt23UJotdCAfu-oUYuw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">双峰频率分布(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>提供)</p></figure></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h2 id="0826" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">数据集</h2><p id="efec" class="pw-post-body-paragraph ky kz jj la b lb nb kk ld le nc kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">我们将使用为期两年的租赁自行车日常使用数据集。以下是数据集的前 10 行:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ng"><img src="../Images/f1dcd4505d668cfca116b16e497a24d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gxu2mZNpi21QA16PgHTWaw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">租赁自行车使用计数(来源:<a class="ae jg" href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset" rel="noopener ugc nofollow" target="_blank"> UCI 机器学习库</a>)(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>)</p></figure><p id="7b5a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据集中的变量如下:</p><p id="f3c6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">瞬间</strong>:行索引<br/> <strong class="la jk"> dteday </strong>:以 dd-MM-yy 格式进行测量的日期<br/> <strong class="la jk">季节</strong>:当时的天气季节<br/> <strong class="la jk">年</strong>:当时的年份:0=2011，1=2012 <br/> <strong class="la jk">月</strong>:当时的月份:1 日至 12 日<br/> <strong class="la jk">节假日</strong>:测量是否在节假日进行(T16) no=0) <br/> <strong class="la jk"> weekday </strong>:星期几(0 到 6) <br/> <strong class="la jk">工作日</strong>:测量是否在工作日进行(yes=1，no=0) <br/> <strong class="la jk"> weathersit </strong>:当天天气情况:1 =晴朗，少云，部分多云，部分多云。 2 =薄雾+多云，薄雾+碎云，薄雾+少云，薄雾。3 =小雪，小雨+雷雨+散云，小雨+散云。4 =暴雨+冰托盘+雷雨+薄雾，雪+雾。<br/> <strong class="la jk"> temp </strong>:温度，归一化到 39C <br/> <strong class="la jk"> atemp </strong>:真实手感，归一化到 50C <br/> <strong class="la jk">嗡嗡声</strong>:湿度，归一化到 100 <br/> <strong class="la jk">风速</strong>:风速，归一化到 67 <br/> <strong class="la jk">休闲 _ 用户 _ 计数</strong>:休闲自行车租赁者计数<br/> <strong class="la jk">注册 _ 用户 _ 计数</strong>:注册(会员)自行车计数</p><p id="cb0e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以从这里下载数据集<a class="ae jg" href="https://gist.github.com/sachinsdate/413910079ab4ef4332e7a97cae55d13a" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h2 id="9dba" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">回归模型</h2><p id="ba69" class="pw-post-body-paragraph ky kz jj la b lb nb kk ld le nc kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">我们将建立一个回归模型，其中<strong class="la jk">因变量</strong>为<strong class="la jk"> registered_user_count </strong>，而<strong class="la jk">解释变量</strong>或所谓的<strong class="la jk">协变量</strong>如下:<br/> <strong class="la jk">季节、月份、假日、工作日、工作日、天气、温度、atemp、哼声、风速</strong>。</p><blockquote class="nh ni nj"><p id="9742" class="ky kz ma la b lb lc kk ld le lf kn lg nk li lj lk nl lm ln lo nm lq lr ls lt im bi translated">由于我们正在建模计数，我们将使用 Python statsmodels 库中的<strong class="la jk">泊松回归模型</strong>。</p></blockquote><p id="c834" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们从导入所有必需的包开始:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="f71a" class="mi mj jj no b gy ns nt l nu nv"><strong class="no jk">import </strong>pandas <strong class="no jk">as </strong>pd<br/><strong class="no jk">from </strong>patsy <strong class="no jk">import </strong>dmatrices<br/><strong class="no jk">import </strong>numpy <strong class="no jk">as </strong>np<br/><strong class="no jk">import </strong>statsmodels.api <strong class="no jk">as </strong>sm<br/><strong class="no jk">import </strong>statsmodels.stats.stattools <strong class="no jk">as </strong>st<br/><strong class="no jk">import </strong>matplotlib.pyplot <strong class="no jk">as </strong>plt</span></pre><p id="8be5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将数据集加载到 Pandas 数据框中:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="10c5" class="mi mj jj no b gy ns nt l nu nv">df = pd.read_csv(<strong class="no jk">'bike_sharing_dataset_daywise.csv'</strong>, header=0, parse_dates=[<strong class="no jk">'dteday'</strong>], infer_datetime_format=True)</span></pre><p id="e549" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">创建训练和测试数据集:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="50bd" class="mi mj jj no b gy ns nt l nu nv">mask = np.random.rand(len(df)) &lt; 0.8</span><span id="274f" class="mi mj jj no b gy nw nt l nu nv">df_train = df[mask]</span><span id="af77" class="mi mj jj no b gy nw nt l nu nv">df_test = df[~mask]</span><span id="e5e3" class="mi mj jj no b gy nw nt l nu nv">print(<strong class="no jk">'Training data set length='</strong>+str(len(df_train)))</span><span id="8c99" class="mi mj jj no b gy nw nt l nu nv">print(<strong class="no jk">'Testing data set length='</strong>+str(len(df_test)))</span></pre><p id="2ebe" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用<a class="ae jg" href="https://patsy.readthedocs.io/en/latest/quickstart.html" rel="noopener ugc nofollow" target="_blank"> Patsy </a>语法创建回归表达式。我们说<strong class="la jk"> registered_user_count </strong>是因变量，它依赖于~右侧提到的所有变量。</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="27e9" class="mi mj jj no b gy ns nt l nu nv">expr = <strong class="no jk">'registered_user_count </strong>~<strong class="no jk"> season</strong> + <strong class="no jk">mnth</strong> + <strong class="no jk">holiday</strong> + <strong class="no jk">weekday</strong> + <strong class="no jk">workingday</strong> + <strong class="no jk">weathersit</strong> + <strong class="no jk">temp</strong> + <strong class="no jk">atemp</strong> + <strong class="no jk">hum</strong> + <strong class="no jk">windspeed'</strong></span></pre><p id="72c4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">设置 X、y 矩阵:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="8ba8" class="mi mj jj no b gy ns nt l nu nv">y_train, X_train = dmatrices(expr, df_train, return_type=<strong class="no jk">'dataframe'</strong>)</span><span id="7f6c" class="mi mj jj no b gy nw nt l nu nv">y_test, X_test = dmatrices(expr, df_test, return_type=<strong class="no jk">'dataframe'</strong>)</span></pre><p id="10ff" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">建立和训练泊松回归模型:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="f238" class="mi mj jj no b gy ns nt l nu nv">poisson_training_results = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()</span></pre><p id="338c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">poisson_training_results 是一个类型为<a class="ae jg" href="https://www.statsmodels.org/0.6.1/generated/statsmodels.genmod.generalized_linear_model.GLMResults.html" rel="noopener ugc nofollow" target="_blank"> GLMResults </a>的对象。打印模型摘要:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="e872" class="mi mj jj no b gy ns nt l nu nv">print(poisson_training_results.summary())</span></pre><p id="f3c1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这会打印出以下内容:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/b95ed84930ce49764f027839379f83d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RccbombqzvNF91vLG8973g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">拟合泊松回归模型的输出(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>提供)</p></figure><p id="1f79" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">很高兴看到所有的模型系数在 p 值为&lt; 0.001 i.e. at 99.999% confidence level.</p><p id="40eb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">As against linear regression models, models in which the dependent variable is a count, rarely produce normally distributed residual error distributions.</p><p id="0b3d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">So we have to normalize the raw-residuals using other means. Three popular transformations are:</p><ul class=""><li id="ce88" class="ny nz jj la b lb lc le lf lh oa ll ob lp oc lt od oe of og bi translated">The Pearson residual</li><li id="c213" class="ny nz jj la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">The Anscombe residual</li><li id="958d" class="ny nz jj la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">The Deviance residual</li></ul><p id="e008" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Statsmodels makes all three kinds of residual errors available to us via GLMResults.<strong class="la jk"> resid_pearson </strong>时都具有统计显著性。<strong class="la jk"> resid_anscombe </strong>，以及 GLMResults。<strong class="la jk"> resid_deviance </strong>变量</p><p id="e0a9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">原始残差在 GLMResults 中可用。<strong class="la jk"> resid_response </strong>。</p><p id="f9a3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将打印出所有 4 种残差的偏度和峰度，看看哪一种是最正态分布的。完美的正态分布的偏斜度为零，峰度为 3.0。</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="2d5d" class="mi mj jj no b gy ns nt l nu nv"><strong class="no jk">raw_residual_skewness </strong>= st.<strong class="no jk">robust_skewness</strong>(poisson_training_results.<strong class="no jk">resid_response</strong>)[0]</span><span id="7887" class="mi mj jj no b gy nw nt l nu nv"><strong class="no jk">pearson_residual_skewness </strong>= st.<strong class="no jk">robust_skewness</strong>(poisson_training_results.<strong class="no jk">resid_pearson</strong>)[0]</span><span id="331d" class="mi mj jj no b gy nw nt l nu nv"><strong class="no jk">anscobe_residual_skewness </strong>= st.<strong class="no jk">robust_skewness</strong>(poisson_training_results.<strong class="no jk">resid_anscombe</strong>)[0]</span><span id="0fdd" class="mi mj jj no b gy nw nt l nu nv"><strong class="no jk">deviance_residual_skewness </strong>= st.<strong class="no jk">robust_skewness</strong>(poisson_training_results.<strong class="no jk">resid_deviance</strong>)[0]<br/><br/><strong class="no jk">raw_residual_kurtosis </strong>= st.<strong class="no jk">robust_kurtosis</strong>(poisson_training_results.<strong class="no jk">resid_response</strong>)[0]</span><span id="ca34" class="mi mj jj no b gy nw nt l nu nv"><strong class="no jk">pearson_residual_kurtosis </strong>= st.<strong class="no jk">robust_kurtosis</strong>(poisson_training_results.<strong class="no jk">resid_pearson</strong>)[0]</span><span id="2671" class="mi mj jj no b gy nw nt l nu nv"><strong class="no jk">anscobe_residual_kurtosis </strong>= st.<strong class="no jk">robust_kurtosis</strong>(poisson_training_results.<strong class="no jk">resid_anscombe</strong>)[0]</span><span id="3e4d" class="mi mj jj no b gy nw nt l nu nv"><strong class="no jk">deviance_residual_kurtosis </strong>= st.<strong class="no jk">robust_kurtosis</strong>(poisson_training_results.<strong class="no jk">resid_deviance</strong>)[0]<br/><br/>residual_stats = [<br/>[<strong class="no jk">'Raw residual'</strong>, raw_residual_skewness, raw_residual_kurtosis],[<strong class="no jk">'Pearson\'s residual'</strong>, pearson_residual_skewness, pearson_residual_kurtosis],<br/>[<strong class="no jk">'Anscombe residual'</strong>, anscobe_residual_skewness, anscobe_residual_kurtosis],<br/>[<strong class="no jk">'Deviance residual'</strong>, deviance_residual_skewness, deviance_residual_kurtosis]<br/>                ]<br/><br/>residual_stats_df = pd.DataFrame(residual_stats, columns=[<strong class="no jk">'Residual'</strong>, <strong class="no jk">'Skewness'</strong>, <strong class="no jk">'Kurtosis'</strong>])<br/><br/>print(residual_stats_df)</span></pre><p id="f86a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这会打印出下表:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8a6ed13f5328a83e7843e16af80a036c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*C_lUkG5lUSqXLwuWtAP_kQ.png"/></div></figure><p id="55a6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们正在寻找偏度最接近零、峰度最接近 3.0 的残差类型。考虑到该表中所有值的接近程度，很难从该表中选择合适的残差类型。我们将选择皮尔逊残差，因为它的偏斜度最接近于零。</p><p id="0e61" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们绘制皮尔逊残差的频率分布图:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="4bbd" class="mi mj jj no b gy ns nt l nu nv">poisson_training_results.resid_pearson.<strong class="no jk">hist</strong>(bins=50)</span><span id="812a" class="mi mj jj no b gy nw nt l nu nv">plt.show()</span></pre><p id="3d8b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们看到以下双峰分布！</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/02dda5e671a1577135b31d94db7429f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ONwWwev-5A5a-e3NGe_UQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">回归误差(皮尔逊残差)似乎具有双峰分布(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>提供)</p></figure><p id="cf50" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里发生了什么导致了双峰回归误差？</p><p id="6575" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当回归误差为双峰时，可能会出现以下几种情况:</p><blockquote class="nh ni nj"><p id="de71" class="ky kz ma la b lb lc kk ld le lf kn lg nk li lj lk nl lm ln lo nm lq lr ls lt im bi translated">因变量是二元变量，如赢/输、死/活、涨/跌等。但是你的回归模型可能会生成预测，一个连续变化的实值。所以如果你编码了(赢=1，输=0)，或者(死=0，活=1)等等。并且您的回归模型在 0.5 附近的狭窄范围内生成预测值，例如 0.55、0.58、0.6、0.61 等，那么回归误差将在零的一侧(当真实值为 0 时)或零的另一侧(当真实值为 1 时)达到峰值。</p><p id="0bb2" class="ky kz ma la b lb lc kk ld le lf kn lg nk li lj lk nl lm ln lo nm lq lr ls lt im bi translated">如果您没有为数据集选择正确的回归模型，和/或您缺少关键的解释变量，而没有这些变量，大多数预测都徘徊在 0.5 左右，这又会发生。</p></blockquote><p id="e23c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的例子中，因变量是计数，我们使用了一个适合计数的模型，即泊松模型，因此可以排除上述情况。</p><blockquote class="nh ni nj"><p id="22d4" class="ky kz ma la b lb lc kk ld le lf kn lg nk li lj lk nl lm ln lo nm lq lr ls lt im bi translated">双峰残差的另一个原因是，人们可能遗漏了一个二元回归变量，该变量以下列方式影响输出值:</p><p id="be78" class="ky kz ma la b lb lc kk ld le lf kn lg nk li lj lk nl lm ln lo nm lq lr ls lt im bi translated">当变量值为 0 时，输出范围在一定范围内。</p><p id="3174" class="ky kz ma la b lb lc kk ld le lf kn lg nk li lj lk nl lm ln lo nm lq lr ls lt im bi translated">当变量的值为 1 时，输出将呈现一个全新的值范围，这是以前的范围中所没有的。</p></blockquote><p id="3240" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实证明，在我们的例子中确实如此！我们忽略了二元变量 yr(观察年份),它有两个值:0 = 2011 年，1 = 2012 年。</p><p id="d904" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们来看看 yr 是如何影响因变量 registered_user_count 的。我们将绘制注册用户数与您的年数的关系图:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="5537" class="mi mj jj no b gy ns nt l nu nv">df.plot.scatter(<strong class="no jk">'yr'</strong>, <strong class="no jk">'registered_user_count'</strong>)</span><span id="15d3" class="mi mj jj no b gy nw nt l nu nv">plt.show()</span></pre><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/47d525e0ac36c4d12a9f4d288becfb3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GWDb2ssfTLO8ama9wI57Hw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">注册用户数与年份(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>提供)</p></figure><p id="17c8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">位于红圈的<strong class="la jk">注册用户计数</strong>的所有值仅在年份=1 时出现，即 2012 年。由于我们在模型中忽略了 yr，我们的模型无法解释更高值计数的存在。每次实际值在 5000–7000 范围内时，它都会产生系统误差，导致残差图中出现第二个峰值。</p><p id="25be" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果这个理论是正确的，将 yr 加入到模型中应该可以解决残差的双峰问题。因此，让我们将 yr 添加到回归表达式中，并再次构建和训练模型:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="e9d3" class="mi mj jj no b gy ns nt l nu nv"><em class="ma">#Set up the regression expression. This time, include yr.<br/></em>expr = <strong class="no jk">'registered_user_count ~ yr + season + mnth + holiday + weekday + workingday + weathersit + temp + atemp + hum + windspeed'<br/><br/></strong><em class="ma">#Set up the X and y matrices for the training and testing data sets<br/></em>y_train, X_train = <strong class="no jk">dmatrices</strong>(expr, df_train, return_type=<strong class="no jk">'dataframe'</strong>)</span><span id="0f39" class="mi mj jj no b gy nw nt l nu nv">y_test, X_test = dmatrices(expr, df_test, return_type=<strong class="no jk">'dataframe'</strong>)<br/><br/><em class="ma">#Using the statsmodels GLM class, train the Poisson regression model on the training data set<br/></em><strong class="no jk">poisson_training_results </strong>= sm.<strong class="no jk">GLM</strong>(y_train, X_train, family=sm.families.<strong class="no jk">Poisson</strong>()).<strong class="no jk">fit</strong>()<br/><br/><em class="ma">#print out the training summary<br/></em>print(poisson_training_results.<strong class="no jk">summary</strong>())</span></pre><p id="0988" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这会产生以下输出:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/b4148b55e20926155a169e634faa35c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N0ATwBJSfhp0DSxdqYwngw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">泊松回归模型结果(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>)</p></figure><p id="ed42" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一次，请注意 yr 是回归变量之一，其系数在 99.999%的置信水平下具有统计显著性。</p><p id="3040" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，该模型的对数似然比是-52683，这比没有 yr 的先前模型的对数似然比(-114530)大得多。对数似然是一种拟合优度的度量。在这种情况下，它表明添加 yr 大大提高了模型的拟合优度。这与我们的理论一致，yr 的缺失阻止了我们的模型解释所有这些高值计数。</p><p id="c61f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到目前为止一切顺利。</p><p id="247d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们检查包含 yr 的修正模型的所有 4 种残差:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="8783" class="mi mj jj no b gy ns nt l nu nv"><strong class="no jk">deviance_residual_skewness </strong>= st.<strong class="no jk">robust_skewness</strong>(poisson_training_results.<strong class="no jk">resid_deviance</strong>)[0]<br/><br/><strong class="no jk">raw_residual_kurtosis </strong>= st.<strong class="no jk">robust_kurtosis</strong>(poisson_training_results.<strong class="no jk">resid_response</strong>)[0]</span><span id="e767" class="mi mj jj no b gy nw nt l nu nv"><strong class="no jk">pearson_residual_kurtosis </strong>= st.<strong class="no jk">robust_kurtosis</strong>(poisson_training_results.<strong class="no jk">resid_pearson</strong>)[0]</span><span id="f654" class="mi mj jj no b gy nw nt l nu nv"><strong class="no jk">anscobe_residual_kurtosis </strong>= st.<strong class="no jk">robust_kurtosis</strong>(poisson_training_results.<strong class="no jk">resid_anscombe</strong>)[0]</span><span id="48df" class="mi mj jj no b gy nw nt l nu nv"><strong class="no jk">deviance_residual_kurtosis </strong>= st.<strong class="no jk">robust_kurtosis</strong>(poisson_training_results.<strong class="no jk">resid_deviance</strong>)[0]<br/><br/>residual_stats = [<br/>[<strong class="no jk">'Raw residual'</strong>, raw_residual_skewness, raw_residual_kurtosis],[<strong class="no jk">'Pearson\'s residual'</strong>, pearson_residual_skewness, pearson_residual_kurtosis],<br/>[<strong class="no jk">'Anscombe residual'</strong>, anscobe_residual_skewness, anscobe_residual_kurtosis],<br/>[<strong class="no jk">'Deviance residual'</strong>, deviance_residual_skewness, deviance_residual_kurtosis]<br/>                ]<br/><br/>residual_stats_df = pd.DataFrame(residual_stats, columns=[<strong class="no jk">'Residual'</strong>, <strong class="no jk">'Skewness'</strong>, <strong class="no jk">'Kurtosis'</strong>])<br/><br/><strong class="no jk">print</strong>(residual_stats_df)</span></pre><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/b1bed49c1e9b7330e6dc7394f5caa6f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*TWURDOF6ntiOtlVn5XpelQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>提供)</p></figure><p id="010b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">皮尔逊残差再次以接近于零的偏斜度在所有产品中表现出色。虽然，原始残差的峰度最接近 3.0。</p><p id="ea57" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们绘制修正模型的皮尔逊残差的频率分布图:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="c2c1" class="mi mj jj no b gy ns nt l nu nv">df.plot.scatter(<strong class="no jk">'yr'</strong>, <strong class="no jk">'registered_user_count'</strong>)</span><span id="eacb" class="mi mj jj no b gy nw nt l nu nv">plt.show()</span></pre><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/f308b5c6c92a88fda26751429077870d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AHeGUqZTYhr2pTn7eUDZ6g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">模型的皮尔逊残差的频率分布(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>提供)</p></figure><p id="ce78" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一次，随着 yr 变量的加入，双峰现象在很大程度上消失了。</p><p id="2972" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们并排看两个图，左边没有 yr，右边有 yr:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/1a69c0d23775444345bccfc9a49953f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vsy6FgWXkQS8bu2G3Iy0wA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">回归残差(左=没有 yr 的模型，右=有 yr 的模型)(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>提供)</p></figure><p id="56af" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过引入一个重要的二元回归变量，我们成功地修正了残差中的大部分双峰。添加缺失的变量也大大提高了模型的拟合优度。</p><p id="f435" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们获取模型对测试数据集的预测，我们还将绘制预测计数与实际计数的对比图:</p><pre class="lv lw lx ly gt nn no np nq aw nr bi"><span id="dd50" class="mi mj jj no b gy ns nt l nu nv"><em class="ma">#fetch the predictions<br/></em><strong class="no jk">poisson_predictions </strong>= poisson_training_results.<strong class="no jk">get_prediction</strong>(X_test)</span><span id="b1c1" class="mi mj jj no b gy nw nt l nu nv"><em class="ma">#.summary_frame() returns a pandas DataFrame<br/></em>predictions_summary_frame = poisson_predictions.<strong class="no jk">summary_frame</strong>()</span><span id="e806" class="mi mj jj no b gy nw nt l nu nv"><em class="ma">#print the predictions<br/></em><strong class="no jk">print</strong>(predictions_summary_frame)</span><span id="bb1e" class="mi mj jj no b gy nw nt l nu nv"><em class="ma">#The </em><strong class="no jk"><em class="ma">mean</em></strong><em class="ma"> column contains the predicted count<br/></em>predicted_counts=predictions_summary_frame[<strong class="no jk">'mean'</strong>]</span><span id="7cee" class="mi mj jj no b gy nw nt l nu nv"><em class="ma">#get the actual count from y_test<br/></em>actual_counts = y_test[<strong class="no jk">'registered_user_count'</strong>]<br/><br/><em class="ma">#Plot the predicted counts versus the actual counts for the test data.<br/></em>fig = plt.figure()</span><span id="6c8f" class="mi mj jj no b gy nw nt l nu nv">fig.suptitle(<strong class="no jk">'Predicted versus actual user counts'</strong>)</span><span id="47e8" class="mi mj jj no b gy nw nt l nu nv">predicted, = plt.plot(X_test.index, predicted_counts, <strong class="no jk">'go-'</strong>, label=<strong class="no jk">'Predicted counts'</strong>)</span><span id="1cf9" class="mi mj jj no b gy nw nt l nu nv">actual, = plt.plot(X_test.index, actual_counts, <strong class="no jk">'ro-'</strong>, label=<strong class="no jk">'Actual counts'</strong>)</span><span id="747c" class="mi mj jj no b gy nw nt l nu nv">plt.legend(handles=[predicted, actual])</span><span id="fb52" class="mi mj jj no b gy nw nt l nu nv">plt.show()</span></pre><p id="7b27" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们得到如下的情节:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/45c39346ec6cd9afca821f9e5bc7f87b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gC5Mxd2VVDPDRUTGPMGWaw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">预测数量与实际数量的对比(图片由<a class="ae jg" href="https://sachin-date.medium.com/" rel="noopener">作者</a>提供)</p></figure><p id="b56e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是本文中使用的完整源代码:</p><figure class="lv lw lx ly gt iv"><div class="bz fp l di"><div class="ot ou l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">源代码</p></figure><p id="ad88" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以从这里下载数据集<a class="ae jg" href="https://gist.github.com/sachinsdate/413910079ab4ef4332e7a97cae55d13a" rel="noopener ugc nofollow" target="_blank"/>。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h2 id="2f7f" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">进一步阅读的建议</h2><div class="is it gp gr iu ov"><a rel="noopener follow" target="_blank" href="/testing-for-normality-using-skewness-and-kurtosis-afd61be860"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd jk gy z fp pa fr fs pb fu fw ji bi translated">使用偏度和峰度检验正态性</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">…以及使用综合 K 平方和 Jarque–Bera 正态性检验的分步指南</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj ja ov"/></div></div></a></div><div class="is it gp gr iu ov"><a rel="noopener follow" target="_blank" href="/an-illustrated-guide-to-the-poisson-regression-model-50cccba15958"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd jk gy z fp pa fr fs pb fu fw ji bi translated">泊松回归模型图解指南</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">和使用 Python 的泊松回归教程</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pk l pg ph pi pe pj ja ov"/></div></div></a></div></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h2 id="5a08" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">数据集的引用:</h2><p id="1de7" class="pw-post-body-paragraph ky kz jj la b lb nb kk ld le nc kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated"><em class="ma"> Fanaee-T，Hadi 和 Gama，Joao，“结合集合检测器和背景知识的事件标记”，《人工智能进展》(2013):第 1–15 页，Springer Berlin Heidelberg，doi:10.1007/s 13748–013–0040–3。</em></p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><p id="7c41" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ma">感谢阅读！如果你喜欢这篇文章，请关注我的</em><a class="ae jg" href="https://timeseriesreasoning.medium.com" rel="noopener"><strong class="la jk"><em class="ma">Sachin Date</em></strong></a><em class="ma">获取关于回归和时间序列分析的技巧、操作方法和编程建议。</em></p></div></div>    
</body>
</html>