<html>
<head>
<title>Load files faster into BigQuery</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将文件更快地加载到 BigQuery 中</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/load-files-faster-into-bigquery-94355c4c086a?source=collection_archive---------10-----------------------#2020-06-30">https://towardsdatascience.com/load-files-faster-into-bigquery-94355c4c086a?source=collection_archive---------10-----------------------#2020-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6a22" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">针对摄取的 CSV、GZIP、AVRO 和拼花文件类型进行基准测试</h2></div><p id="3666" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated">oogle 云平台的 BigQuery 是一个用于分析的托管大规模数据仓库。它支持 JSON、CSV、PARQUET、OCR 和 AVRO 文件格式来导入表格。这些文件类型各有利弊，我已经谈过为什么我更喜欢这里的数据科学工作流的<a class="ae ln" rel="noopener" target="_blank" href="/loading-files-into-bigquery-6de1ff63df35">拼花</a>。但是还有一个问题:</p><p id="bfd5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lo">哪个文件扩展名能让我们最快地加载到 BigQuery 中？</em></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/4e1180ba7d8ee51a3cbec7b56069b4d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B9g05UuHJSqgwbhIjR0OUQ.jpeg"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">让我们希望我们的文件加载速度会比拍摄这张照片的速度更快……—照片由安德斯·吉尔登在 Unsplash 上拍摄</p></figure><blockquote class="mf"><p id="bc50" class="mg mh it bd mi mj mk ml mm mn mo ld dk translated">要获得所有媒体文章的完整访问权限，包括我的文章，请考虑在此订阅<a class="ae ln" href="https://niczky12.medium.com/membership" rel="noopener"/>。</p></blockquote><h1 id="5331" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">实验</h1><blockquote class="nh ni nj"><p id="2e93" class="ki kj lo kk b kl km ju kn ko kp jx kq nk ks kt ku nl kw kx ky nm la lb lc ld im bi translated"><em class="it">由于 AVRO 压缩数据的方式，它推荐快速摄取时间，但是现在谁会相信文档呢？我想亲眼看看哪种文件格式会胜出。</em></p></blockquote><p id="002d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我很少使用 OCR，我在工作中主要使用 Python，这意味着我可以轻松地编写 CSV 和带压缩的拼花文件，所以我决定测试的文件类型列表是:</p><ul class=""><li id="af38" class="nn no it kk b kl km ko kp kr np kv nq kz nr ld ns nt nu nv bi translated">CSV —完全没有压缩的逗号分隔文件</li><li id="2897" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">CSV。GZIP——同上，但压缩了 GZIP。</li><li id="9790" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">PARQUET——一种柱状存储格式，压缩速度很快，熊猫本身就支持这种格式。</li><li id="0fde" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">AVRO——GCP 推荐用于快速加载的二进制格式。</li></ul><p id="a79d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于上述每种格式，我进行了 3 次实验:</p><ul class=""><li id="2b49" class="nn no it kk b kl km ko kp kr np kv nq kz nr ld ns nt nu nv bi translated">导入小文件(5k 行)</li><li id="c0bc" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">导入大文件(50k 行)</li><li id="d07c" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">导入大量较大的文件(20x50k 行)</li></ul><p id="502d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了模拟一个更真实的例子，我为列创建了一些不同的数据类型，以查看 BigQuery 如何处理它们。我使用了浮点数、整数、日期时间、高熵的长字符串和低熵的短字符串(每种数据类型有 50 列)。</p><h1 id="034e" class="mp mq it bd mr ms mt mu mv mw mx my mz jz ob ka nb kc oc kd nd kf od kg nf ng bi translated">创建数据</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oe"><img src="../Images/19433bfc24daf9afab9bf5c34eebff5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Op8H4yaK4L_DfwGua5jfWA.jpeg"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">这是我想象中的数据挖掘工作——照片由<a class="ae ln" href="https://unsplash.com/@dominik_photography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">张秀坤·万尼</a>在<a class="ae ln" href="/s/photos/mine?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="47b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我总是更喜欢不需要你从互联网上获取数据的脚本，因为谁知道这些数据什么时候会消失，所以我决定使用<code class="fe of og oh oi b">sklearn.datasets</code>模块来生成许多随机浮动列(250 个)并将它们转换成字符串、时间等。让我们开始吧:</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="9c95" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lo">随意窃取上面的内容，用它来生成其他你喜欢的数据集。</em></p><p id="e147" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然有了一个方便的小函数，那就来做一些文件吧。在撰写本文时，除了 AVRO 之外，我将在所有地方使用<code class="fe of og oh oi b">pandas</code>,在<code class="fe of og oh oi b">pandas</code>没有官方的 AVRO 支持😥。</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oj ok l"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">我在这里通过重复小数据集在技术上作弊，但这就是生活…</p></figure><p id="3a91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，如果我的数学是正确的，我们应该有 8 个文件准备好发送到云。</p><h1 id="fa4a" class="mp mq it bd mr ms mt mu mv mw mx my mz jz ob ka nb kc oc kd nd kf od kg nf ng bi translated">比较文件大小</h1><p id="d0e4" class="pw-post-body-paragraph ki kj it kk b kl ol ju kn ko om jx kq kr on kt ku kv oo kx ky kz op lb lc ld im bi translated">在我们开始将文件上传到 Google 云存储并将它们接收到 BigQuery 之前，让我们快速查看一下它们的大小。这将影响你在 GCS 中的上传时间和存储成本(但不是在 BigQuery 中！).我知道这听起来并不重要，但它确实很重要，所以<em class="lo">我懒得告诉你这些</em>——是的，我现在读了很多苏斯博士的书😃。</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="d6d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些是漂亮的酒吧🍫：</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oq"><img src="../Images/e8c96f173bc2bdf6ff6821d4e73b3ca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zr4RRmgfXUdKa6z88-5S2Q.png"/></div></div></figure><p id="5037" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里需要注意一些重要的事情:</p><ul class=""><li id="7141" class="nn no it kk b kl km ko kp kr np kv nq kz nr ld ns nt nu nv bi translated">如果你还在使用拨号调制解调器或者你的宽带速度慢得像蜗牛，不要使用 CSV…</li><li id="f8c0" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">当谈到小文件时，看看 GZIP 和所有其他人相比是多么的小。</li><li id="e201" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">正如在 5k 行文件上看到的，PARQUET 的开销最大，但是当有重复数据时，它会逐列压缩表，因此存储重复值变得非常便宜。对于较大的文件，可以考虑使用拼花地板。</li></ul><h1 id="f341" class="mp mq it bd mr ms mt mu mv mw mx my mz jz ob ka nb kc oc kd nd kf od kg nf ng bi translated">准备 GCP 环境</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi or"><img src="../Images/05fa501db42629688f609c17d18d7a3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7vvWe--Dqxwr0higd6DwWA.jpeg"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">为快速加载做好准备——照片由<a class="ae ln" href="https://unsplash.com/@gabrielalenius?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Gabriel Alenius </a>在<a class="ae ln" href="/s/photos/getting-dressed?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="dbdc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下一步是创建一个存储桶，我们可以上传我们珍贵的小文件。</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="c835" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将 GCS bucket 和 BigQuery 实例设置在同一个区域非常重要——否则，BigQuery 将拒绝加载文件。我还为我的表设置了一个超时，因为我容易忘记事情，也不太热衷于为谷歌保存我的随机值表付费😆。</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oj ok l"/></div></figure><h1 id="103d" class="mp mq it bd mr ms mt mu mv mw mx my mz jz ob ka nb kc oc kd nd kf od kg nf ng bi translated">锤击 BigQuery</h1><p id="e812" class="pw-post-body-paragraph ki kj it kk b kl ol ju kn ko om jx kq kr on kt ku kv oo kx ky kz op lb lc ld im bi translated">让我们回顾一下:</p><ul class=""><li id="634d" class="nn no it kk b kl km ko kp kr np kv nq kz nr ld ns nt nu nv bi translated">generated☑随机文件</li><li id="6e85" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">储水桶创造了☑</li><li id="b198" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">大查询数据集已初始化☑</li><li id="7bf3" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">上传到 gcs 的文件</li></ul><p id="dc31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们只需将文件导入 BigQuery，看看谁会胜出。嗯，差不多了。问题是，由于网络流量和 BigQuery 执行的其他任务，文件摄取时间可能会有微小的变化，所以让我们将每个文件上传几次，看看它们平均起来如何，而不是只比较每个文件的一次摄取。Python 的功能提供了帮助:</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="3380" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我将文件复制到一个新位置(可选地，复制多次),然后创建<code class="fe of og oh oi b">repeat</code>多个 BigQuery 表。这里需要注意的一个技巧是负载是异步的，所以首先我触发作业，然后用<code class="fe of og oh oi b">job.result()</code>等待结果。</p><p id="339b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于所有文件都有不同的格式，我们还需要创建一些不同的<code class="fe of og oh oi b">JobConfig</code>,以便 BigQuery 知道如何处理它们。</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="f6f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们开始锤打🔨先处理小文件的 BigQuery:</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="4289" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果并不令人惊讶。AVRO 名列前茅，而其他人都差不多，我猜是因为开销的原因，镶木地板落后了？</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oq"><img src="../Images/b1d86923deea1e8b4ea0c5d7bb0e0923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-DzYj6QD_Wz17TWy4npxRQ.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">AVRO 速度很快。</p></figure><p id="5ff4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我还用大文件重复了这个实验——代码在我的 GitHub 上。以下是大文件(50k 行)的结果:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oq"><img src="../Images/96c0ee78464384355901930ff8a8ffa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EScsnGukYGY63jGUwEX19Q.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">拼花地板在这里做得更好，但远没有 AVRO 快。</p></figure><p id="2daa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了让它更有趣，我重复了上面的步骤，但是每种文件类型有 20 个文件来模拟一些更大的工作负载。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oq"><img src="../Images/e4068253204e89e66ff301a28936ce13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sRJT8ejadFftEE6gVv9a8w.png"/></div></div></figure><p id="2288" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面可以看出，对于较小的表格，文件格式并不重要。但是随着文件变得越来越大，您应该考虑使用内置压缩的格式。在实际工作负载中加载压缩的 GZIP 文件将比未压缩的 CSV 文件快得多，我假设这主要是由于文件的大小。</p><p id="0990" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AVRO 加载时间确实令人印象深刻，几乎是 CSV 文件速度的两倍。</p><p id="c066" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PARQUET 做得也不差，在大文件方面排在第二位。</p><h1 id="0a4e" class="mp mq it bd mr ms mt mu mv mw mx my mz jz ob ka nb kc oc kd nd kf od kg nf ng bi translated">烦恼😖</h1><p id="63eb" class="pw-post-body-paragraph ki kj it kk b kl ol ju kn ko om jx kq kr on kt ku kv oo kx ky kz op lb lc ld im bi translated">在进行这个实验时，我遇到了几件令我烦恼的事情。我在这里抱怨这些。</p><p id="d3ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，为什么 BigQuery 和 Storage Python SDKs 不一致？要删除一个数据集，您可以使用<code class="fe of og oh oi b">client.delete_dataset</code>，但是要删除一个存储桶，您必须使用<code class="fe of og oh oi b">blob.delete</code>:</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="2285" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其次，当我为每种格式加载 10x 文件时，PARQUET 比其他任何格式都慢<strong class="kk iu">很多</strong>。奇怪的是，它比 20 倍拼花文件还慢。如果有人知道为什么(或者如果你在我的代码中发现了一个 bug)请在评论中告诉我。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oq"><img src="../Images/db24a916e48251140411526269dce0c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8J1pfawwkzmFGvrnpYnj_A.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">加载 10x 文件产生奇怪的拼花结果</p></figure><h1 id="b7ac" class="mp mq it bd mr ms mt mu mv mw mx my mz jz ob ka nb kc oc kd nd kf od kg nf ng bi translated">结论</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oq"><img src="../Images/e4068253204e89e66ff301a28936ce13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sRJT8ejadFftEE6gVv9a8w.png"/></div></div></figure><p id="f74d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AVRO 是最快的！如果需要尽快将文件加载到 BigQuery 中，可以使用 AVRO。但是，如果您在 GCP 之外生成文件(或者如果您需要在 GCS 上保存文件的副本)，请考虑使用 PARQUET，因为它的文件大小要小得多，加载速度也相对较快。</p></div><div class="ab cl os ot hx ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="im in io ip iq"><p id="7838" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lo">以上所有代码可在</em> <a class="ae ln" href="https://github.com/niczky12/medium/blob/master/tech/bigquery/benchmarks/file_loads.py" rel="noopener ugc nofollow" target="_blank"> <em class="lo"> GitHub 这里</em> </a> <em class="lo">获得。</em></p><p id="f603" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lo">关于我写的更多 BigQuery，请查看我的文章《为什么我更喜欢拼花而不是 CSV: </em></p><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/loading-files-into-bigquery-6de1ff63df35"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd iu gy z fp ph fr fs pi fu fw is bi translated">将文件加载到 BigQuery</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">使用 Python 将拼花和 CSV 文件导入 GCP BigQuery</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq lz pc"/></div></div></a></div><p id="169e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lo">或者如果你想尝试一下，看看这篇关于如何用 BigQuery 免费计算斐波那契数的文章:</em></p><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/fibonacci-series-with-user-defined-functions-in-bigquery-f72e3e360ce6"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd iu gy z fp ph fr fs pi fu fw is bi translated">BigQuery 中的斐波那契数列</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">在 BigQuery 中使用用户定义的 JavaScript 函数来计算 Fibonacci</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pr l pn po pp pl pq lz pc"/></div></div></a></div></div></div>    
</body>
</html>