<html>
<head>
<title>How to explain a Regression model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何解释回归模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-explain-a-regression-model-244882e6cc0c?source=collection_archive---------28-----------------------#2020-06-03">https://towardsdatascience.com/how-to-explain-a-regression-model-244882e6cc0c?source=collection_archive---------28-----------------------#2020-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1046" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">高 R 平方值本身就好吗？为什么 R 平方越高，特征越多？回归系数的含义是什么？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a63fcaa5d24c5cc92efa981f253100ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MHf-USUccwGCDlez"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾萨克·史密斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="4ca5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">作者注:这篇文章是线性模型系列的一部分，它没有解释线性模型的所有属性，因为这会使文章太长。请务必继续阅读，以免错过本系列的下一篇文章。</strong></p><p id="41f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我主要使用<a class="ae ky" href="http://statsmodels" rel="noopener ugc nofollow" target="_blank"> statsmodels </a>包进行回归分析，因为它提供了开箱即用的回归模型的详细摘要。在本文中，我将解释摘要中基本术语的含义。</p><p id="2720" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">这里有几个你可能会感兴趣的链接:</strong></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="36b0" class="ma mb it lw b gy mc md l me mf">- <a class="ae ky" href="https://www.humanfirst.ai/" rel="noopener ugc nofollow" target="_blank">Labeling and Data Engineering for Conversational AI and Analytics</a></span><span id="4cac" class="ma mb it lw b gy mg md l me mf">- <a class="ae ky" href="https://imp.i115008.net/c/2402645/880006/11298" rel="noopener ugc nofollow" target="_blank">Data Science for Business Leaders</a> [Course]</span><span id="dff9" class="ma mb it lw b gy mg md l me mf">- <a class="ae ky" href="https://imp.i115008.net/c/2402645/788201/11298" rel="noopener ugc nofollow" target="_blank">Intro to Machine Learning with PyTorch</a> [Course]</span><span id="2a1c" class="ma mb it lw b gy mg md l me mf">- <a class="ae ky" href="https://imp.i115008.net/c/2402645/803127/11298" rel="noopener ugc nofollow" target="_blank">Become a Growth Product Manager</a> [Course]</span><span id="cf8c" class="ma mb it lw b gy mg md l me mf">- <a class="ae ky" href="https://amzn.to/3ncTG7D" rel="noopener ugc nofollow" target="_blank">Deep Learning (Adaptive Computation and ML series)</a> [Ebook]</span><span id="c588" class="ma mb it lw b gy mg md l me mf">- <a class="ae ky" href="https://aigents.co/skills" rel="noopener ugc nofollow" target="_blank">Free skill tests for Data Scientists &amp; Machine Learning Engineers</a></span></pre><p id="3897" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mh">上面的一些链接是附属链接，如果你通过它们购买，我会赚取佣金。请记住，我链接课程是因为它们的质量，而不是因为我从你的购买中获得的佣金。</em></p><h1 id="9aba" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">什么总结？</h1><p id="8b1e" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">当我们用 statsmodels 包训练一个模型时，我们可以调用 summary 函数来产生如下图所示的输出。输出类似于 R 在训练回归模型时产生的输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/da23b2fc2e9f26cb5685c95d65cfabd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*724vUSK-7ZJW5R5g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用 statsmodels 训练的回归模型的摘要。</p></figure><h1 id="6238" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">没听说过 statsmodels？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/ad909d34d225405d6eda4d14387a6e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zezO_jm6QxGDhXr2l0VJeQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.statsmodels.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> statsmodels </a>标志</p></figure><p id="d731" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="http://statsmodels.org/" rel="noopener ugc nofollow" target="_blank"> statsmodels </a>是一个 Python 包，用于许多不同统计模型的估计，以及进行统计测试和统计数据探索。</p><p id="7d05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您错过了我之前关于 statsmodels 的文章，我将它与 sklearn 进行了比较:</p><div class="ng nh gp gr ni nj"><a rel="noopener follow" target="_blank" href="/are-you-still-using-sklearn-for-regression-analysis-fb06bb06ce96"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd iu gy z fp no fr fs np fu fw is bi translated">你还在用 sklearn 做回归分析吗？</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">谈到 Python 中的经典机器学习算法，sklearn 是第一个首选包——还有其他包…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">towardsdatascience.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx ks nj"/></div></div></a></div><h1 id="8b73" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">摘要的解释</h1><p id="3d17" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated"><a class="ae ky" href="http://statsmodels" rel="noopener ugc nofollow" target="_blank"> statsmodels </a>索引页面显示了如何训练普通最小二乘回归模型的简单示例:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c546" class="ma mb it lw b gy mc md l me mf">import numpy as np</span><span id="620e" class="ma mb it lw b gy mg md l me mf">import statsmodels.api as sm</span><span id="1f3e" class="ma mb it lw b gy mg md l me mf">import statsmodels.formula.api as smf</span><span id="46ff" class="ma mb it lw b gy mg md l me mf"># Load data<br/>dat = sm.datasets.get_rdataset("Guerry", "HistData").data</span><span id="6b59" class="ma mb it lw b gy mg md l me mf"># Fit regression model (using the natural log of one of the regressors)<br/>results = smf.ols('Lottery ~ Literacy + np.log(Pop1831)', data=dat).fit()</span><span id="d0e3" class="ma mb it lw b gy mg md l me mf"># Inspect the results<br/>results.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/88cacac74a265bc123667a875d239770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1C8_TUrnoNwE33hHDzHE5w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归模型的详细总结。</p></figure><p id="3235" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">以上术语是什么意思？</strong></p><p id="ecd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从简单的术语开始:</p><ul class=""><li id="4b87" class="nz oa it lb b lc ld lf lg li ob lm oc lq od lu oe of og oh bi translated">离开变量是模型正在学习的目标变量(上面公式中的彩票)，</li><li id="6bb9" class="nz oa it lb b lc oi lf oj li ok lm ol lq om lu oe of og oh bi translated">模型是普通的最小二乘法，因为我们使用 smf.ols 函数，</li><li id="f25a" class="nz oa it lb b lc oi lf oj li ok lm ol lq om lu oe of og oh bi translated">观察值是训练集中的样本数，</li><li id="088e" class="nz oa it lb b lc oi lf oj li ok lm ol lq om lu oe of og oh bi translated">Df 模型显示了模型中特征的数量。上面模型中的识字率和 Pop1831。这不包括常数，该常数在将 statsmodels 与公式一起使用时自动添加。</li></ul><h2 id="90bc" class="ma mb it bd mj on oo dn mn op oq dp mr li or os mt lm ot ou mv lq ov ow mx ox bi translated">r 平方</h2><p id="755d" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">默认情况下，回归模型会报告 R 平方和可调 R 平方指标。</p><p id="f842" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">r 平方是衡量数据与拟合回归线接近程度的指标。r 平方可以是正数，也可以是负数。当拟合完美时，R 的平方为 1。<strong class="lb iu">注意，向模型添加特征不会减少 R 平方。这是因为当添加更多特征时，模型可以找到与之前相同的拟合。更多情况下，添加要素时，R 平方会偶然增加。</strong></p><p id="72e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">skearn 有一个计算 R 平方的函数。在以下情况下，R 平方为负:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8394" class="ma mb it lw b gy mc md l me mf">from sklearn.metrics import r2_score</span><span id="6947" class="ma mb it lw b gy mg md l me mf">y_true = [1, 2, 3]<br/>y_pred = [3, 2, 1]<br/>r2_score(y_true, y_pred)</span><span id="ca87" class="ma mb it lw b gy mg md l me mf">-3.0 # r-squared</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/ad3700c7cf3c1382ee21b9234115e72b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TAULGh47-Tt6blP3KECHvw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">产生上述负 R 平方的数据点图。</p></figure><h2 id="d876" class="ma mb it bd mj on oo dn mn op oq dp mr li or os mt lm ot ou mv lq ov ow mx ox bi translated">调整后的 R 平方</h2><p id="fe5d" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">调整后的 R 平方通过针对模型中的要素数量进行调整来解决 R 平方的问题。向模型中添加更多要素将增加 R 平方，但可能会减少调整后的 R 平方。经调整的 R 平方的这一属性可用于找到给出最佳准确度的特征。Adj. R-squared 介于 0 到 1 之间，其中 1 表示模型解释了响应数据在其平均值附近的所有可变性。</p><p id="8e6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">sklearn 没有计算调整后的 R 平方的函数，因为它也需要样本和特征的数量。我们可以用下面的函数来计算:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="5672" class="ma mb it lw b gy mc md l me mf">def adj_r2(r2, n_samples, n_features):<br/>    return 1-(1-r2) * (n_samples-1) / (n_samples-n_features-1)</span></pre><p id="edd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意，高调整的 R 平方并不意味着你的模型是好的。</strong>拟合回归模型时，我们需要检查残差图。线性回归的假设之一是<strong class="lb iu">同方差</strong>，这意味着残差的方差对于 x 的任何值都是相同的，我打算在下一篇关于回归模型的文章中写残差图。</p><h2 id="89cc" class="ma mb it bd mj on oo dn mn op oq dp mr li or os mt lm ot ou mv lq ov ow mx ox bi translated">系数是如何计算的？</h2><p id="2e05" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">普通最小二乘回归(OLS)通过计算获得解析解:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/05da7e8e744199988385e4252f33224e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PiLJb72X_MCP2k5IkXFkcw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算普通最小二乘回归系数的方程。</p></figure><p id="a80d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们自己试着拟合一下模型吧。首先，我们需要转换功能:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="ed5b" class="ma mb it lw b gy mc md l me mf">dat.loc[:, 'intercept'] = 1<br/>dat['Pop1831'] = dat['Pop1831'].apply(np.log)</span></pre><p id="ebb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们分离特征和目标变量:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="46d0" class="ma mb it lw b gy mc md l me mf">x = dat["intercept Literacy Pop1831".split()].values.T<br/>y = dat['Lottery'].values.T</span></pre><p id="feb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算等式的第一部分:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7792" class="ma mb it lw b gy mc md l me mf">a = np.inner(x, x)</span><span id="d69c" class="ma mb it lw b gy mg md l me mf">array([[8.60000000e+01, 3.37600000e+03, 5.04663925e+02],<br/>       [3.37600000e+03, 1.58156000e+05, 1.98401100e+04],<br/>       [5.04663925e+02, 1.98401100e+04, 2.97314045e+03]])</span></pre><p id="930d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算等式的第二部分:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="81ed" class="ma mb it lw b gy mc md l me mf">b = np.inner(x, y)</span><span id="0e95" class="ma mb it lw b gy mg md l me mf">array([  3741.       , 133414.       ,  21572.9563115])</span></pre><p id="ac44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过寻找线性矩阵方程的最小二乘解来计算系数:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="33cb" class="ma mb it lw b gy mc md l me mf">coef = np.linalg.lstsq(a, b)[0]</span><span id="4424" class="ma mb it lw b gy mg md l me mf">array([246.43413487,  -0.48892344, -31.31139219])</span></pre><p id="7a6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以确认这些系数与 OLS 模型中的相同。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/41eb641aafe84f7678cb02a2e8c63cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L0hAOd4k7KFEzE5a926zlg.png"/></div></div></figure><h2 id="3ed6" class="ma mb it bd mj on oo dn mn op oq dp mr li or os mt lm ot ou mv lq ov ow mx ox bi translated">系数是什么意思？</h2><p id="26e4" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">系数的符号告诉我们一个特征和一个目标变量之间是正相关还是负相关。</p><p id="0a95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正系数表示当特征增加时，目标的平均值也增加。负系数表示随着特征值的减小，目标趋于减小。</p><p id="284c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们把系数形象化。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="5909" class="ma mb it lw b gy mc md l me mf"># Extract the coefficients from the model<br/>df_coef = results.params.to_frame().rename(columns={0: 'coef'})</span><span id="0deb" class="ma mb it lw b gy mg md l me mf"># Visualize the coefficients<br/>ax = df_coef.plot.barh(figsize=(14, 7))<br/>ax.axvline(0, color='black', lw=1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/ce204e46ed5fc4085862ad4f24ad04df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o_Pblc_BITmeh-XHTnpoTg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归模型的系数。</p></figure><h1 id="3b40" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">在你走之前</h1><p id="dea3" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在<a class="ae ky" href="https://twitter.com/romanorac" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我，在那里我定期<a class="ae ky" href="https://twitter.com/romanorac/status/1328952374447267843" rel="noopener ugc nofollow" target="_blank">发布关于数据科学和机器学习的</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/b5d426b68cc5a21b1a35d0a157ebc4f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*69rP1pwjJi9mLSFE"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@cmhedger?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">考特尼·海杰</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure></div></div>    
</body>
</html>