<html>
<head>
<title>On Optimization of Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度神经网络的优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1?source=collection_archive---------22-----------------------#2020-06-14">https://towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1?source=collection_archive---------22-----------------------#2020-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f07d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对经典发展的随机漫步，使深度学习成为最终的学习机器</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/67a8199674c9c399810b344ff41ac1c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qfdsh_bJRIJ-Sla4TulrWA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:A. Amini等.<a class="ae ky" href="https://arxiv.org/pdf/1805.04829.pdf" rel="noopener ugc nofollow" target="_blank">端到端控制的空间不确定性采样</a>。NeurIPS贝叶斯深度学习2018</p></figure><p id="537b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>在一个流行的图像分类基准上的成功让深度学习在2011年成为焦点。自那时以来，它在许多领域取得了非凡的成就。</p><p id="b080" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">最值得注意的是，深度学习对计算机视觉、语音识别和自然语言处理(NLP)有着特殊的影响，单枪匹马地让人工智能再次焕发生机。</em></p><p id="730c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">表面上，人们可以将功劳归于大型数据集和计算资源的高可用性。但从根本上来说，它们只是为神经网络提供燃料，让其更进一步。</p><p id="7ef4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是什么帮助神经网络理解了庞大的数据集？</p><p id="a175" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">在过去十年中，使神经网络成为最终学习机器的少数但非凡的发展到底是什么？</em></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="326b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">早在1998年，Yann LeCun等人发表了一章关于<a class="ae ky" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="lv">高效反向传播</em> </strong> </a>的文章。为了有效地训练神经网络，作者提出了一些实用的指导方针。</p><p id="273e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">这些指南强调随机学习、样本混排、标准化输入、激活函数、网络初始化和自适应学习速率。</em></p><p id="62e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，没有标准化的工具来应用这些现成的想法。要聚合一个深层网络是极其困难的，更不用说在看不见的数据上推广它了。</p><p id="6b7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">幸运的是，这些指南为未来几年开发更正式的工具铺平了道路。</strong></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><blockquote class="md me mf"><p id="170b" class="kz la lv lb b lc ld ju le lf lg jx lh mg lj lk ll mh ln lo lp mi lr ls lt lu im bi translated">神经网络在努力解决什么问题？</p></blockquote><p id="98b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练深度神经网络的许多困难是由于网络初始化不良和较低层的梯度不足造成的。</p><p id="39f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Hinton等人和Yoshio等人探索了贪婪分层训练的思想。在这一范式中引人注目的两篇论文是“<a class="ae ky" href="https://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="lv"/></strong></a><strong class="lb iu"><em class="lv"/></strong>和“<a class="ae ky" href="http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="lv">深度网络的贪婪分层训练</em></strong></a><strong class="lb iu"><em class="lv"/></strong>。这两篇论文首次成功训练了更深层次的网络。</p><p id="5f62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，前面提到的仍然需要额外的努力来一层一层地训练网络，然后对整个网络进行微调。</p><p id="7542" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了获得我们今天看到的深度学习的巨大成功，需要开发正式的技术，而不仅仅是黑客。</p><p id="0f4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要能够跨领域、在各种环境下工作的工具。我们需要一个生态系统来训练神经网络。</p><p id="6f8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，在少数精英的所有指导原则和非凡努力下，我们已经走过了深度学习可以展示其真正潜力的临界点。</p><p id="64ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是深度学习生态系统中一些重要的发展，这些发展使它成为每个工程师工具箱中家喻户晓的名字。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="9e33" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">激活功能</h2><p id="0fb7" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">非线性是神经网络的动力。没有它，神经网络就缺乏表达能力，只能简单地归结为输入数据的线性变换。</p><p id="cda8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，涉及指数(sigmoid，双曲线正切等)的激活。)用于实现非线性已经有了它们的问题。一旦到达饱和区域，它们就变得极其不愿意允许梯度流，这极大地抑制了网络的学习潜力。</p><p id="ee3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">幸运的是，AlexNet前来救援，并首次展示了整流线性单元(ReLU)作为激活函数的未触及潜力。</em></p><p id="90c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ReLU有助于更快地训练网络(由于其不饱和的性质),并使得以端到端的方式训练更深的网络成为可能。</p><p id="ec01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ReLU成为深度学习工具包的默认激活选择。从优化的角度来看，它解决了长期存在的所谓消失梯度的问题。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="8647" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">规范化初始化</h2><p id="f59c" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">优化神经网络是一个高度非凸的问题。很少有什么事情比网络权重初始化不好更有害的了。</p><p id="d400" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">确定一个好的初始化策略的挑战首先在有影响力的工作“理解训练深度前馈神经网络的困难”中被探索和解决。</p><p id="fd9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，作者推导出每一层的均匀分布的适当范围。该分布用于对网络初始化的权重进行采样。它通常被称为泽维尔的初始化。</p><p id="507e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">遗憾的是，作者为得出采样分布范围所做的假设导致了Xavier初始化与ReLUs的糟糕结合(记住激活的默认选择！).幸运的是，论文“<a class="ae ky" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html" rel="noopener ugc nofollow" target="_blank">深入研究整流器</a>”通过调整适合ReLUs的均匀分布范围，提供了一个解决方案。该修复使其工作得如此之好，以至于这篇论文成为ImageNet分类的获奖作品。这种初始化通常被称为明凯初始化。</p><p id="9aaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">Xavier和明凯的初始化都提供了开箱即用的初始化工具包。现在，当事情没有按计划进行时，研究人员又少了一件可以指责的事情！</strong></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="5024" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">权重正则化</h2><p id="672c" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在给定大量参数的情况下，神经网络可以很容易地(过度)适应训练示例。这自然要求网络正规化。</p><p id="e9fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">权重衰减(也称为L2正则化)是机器学习中常用的正则化技术之一。不幸的是，这可能会限制网络发挥其全部潜力。</p><p id="8194" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Hinton等人引入了<a class="ae ky" href="http://jmlr.org/papers/v15/srivastava14a.html" rel="noopener ugc nofollow" target="_blank"> dropout </a>的概念，这可能是迄今为止网络正规化最优雅的解决方案。提出的关键思想是在训练中随机丢弃单位(因此得名dropout)。这有助于避免特征的共同适应，从而大大减少过度拟合。</p><p id="9a9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">从优化的角度来看，对于任何给定的训练示例，只有一部分网络权重被更新。</em></p><p id="411f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这在集成方法的精神上是相似的，其中每个学习者仅暴露于部分训练数据。最终，最终训练的网络是具有共享权重的(指数数量的)网络的集合。</p><p id="6f4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Dropout提供了一种现成的解决方案来规范深度神经网络。</strong></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="92fe" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">特征标准化</h2><p id="fa40" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">数据规范化或标准化的概念在机器学习中并不新鲜。数据标准化可以有效优化学习算法。</p><blockquote class="nh"><p id="b089" class="ni nj it bd nk nl nm nn no np nq lu dk translated">深度学习只是一堆层，将它的输入转换并传递给下一层。因此，没有充分的理由将数据规范化限制在第一层。</p></blockquote><p id="6016" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">对每一层的输入进行规范化的想法最初是在名为<a class="ae ky" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">批处理规范化</a>的论文中正式提出的。这项工作在ImageNet分类基准测试中取得了最佳性能，以快一个数量级的训练时间打破了所有以前的记录。</p><p id="25d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征标准化可能是加速深度网络优化的最有效的技术。批处理规范化是这个范例中的明显赢家！</p><p id="40c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">批处理规范化也适用于各种学习速率，并减少了对显式网络规范化的需求。它为加快培训提供了现成的解决方案。</strong></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="2d3b" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">网络体系结构</h2><p id="ab78" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">随着深度而来的是抽象的力量。这导致更好的概括和测试误差的改进。不幸的是，梯度流不足成为训练更深网络的瓶颈。</p><p id="fde7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一次<strong class="lb iu"> ResNets </strong>前来救援。微软研究院的研究人员在他们名为<a class="ae ky" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">图像识别深度残差学习</a>的里程碑式论文中提出了残差连接的想法。</p><p id="ddef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">捷径连接和剩余学习的想法并不新鲜。人们可以将它与其他经典著作相比较，包括<a class="ae ky" href="https://arxiv.org/abs/1505.00387" rel="noopener ugc nofollow" target="_blank">高速公路网络</a>和<a class="ae ky" href="https://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener ugc nofollow" target="_blank">长短期记忆(LSTM)网络</a>。在深度学习之外，梯度增强决策树(GBDT)还学习每个连续树中的剩余函数。</p><p id="8526" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，深度剩余学习论文的作者首次展示了如何通过身份映射的方式有效地实现剩余学习！</p><p id="4c3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">本文中的关键假设是剩余函数更容易学习。这可以通过简单地提供从层输入到层输出的直接连接来实现。这导致向前传递期间更好的信息流和向后传递中有效的梯度流。</em></p><p id="59e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从优化的角度来看，它简单地消除了梯度消失的问题，并使理论上训练非常深的网络成为可能。</p><p id="941e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Residual connections提供了一个现成的解决方案，可帮助您更深入地了解情况。</strong></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="edaa" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">优化者</h2><p id="799e" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">前述工具提供了获得网络参数更新的适当梯度的必要元素。最终，我们需要设计一个有效的策略来利用这些梯度。</p><p id="ffbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">这一次，灵感以</em> <strong class="lb iu"> <em class="lv">动量</em> </strong> <em class="lv">的形式来自物理学。</em></p><p id="ab08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最常用的优化器之一是随机梯度下降(SGD)。不幸的是，SGD有内在的局限性，因为它只采用一阶信息。它无法与使用二阶信息(即所谓的动量)的优化器竞争。</p><p id="6749" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过利用二阶动态，优化器可以有效地获得每个参数的自适应学习率。这反过来有助于极快的收敛，如下图所示，作者是亚历克·拉德福德。这里，SGD与其他使用动量的优化器进行比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/25bb3cf8e7c92e753c2f8368baf920dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*XVFmo9NxLnwDr3SxzKy-rA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://twitter.com/alecrad" rel="noopener ugc nofollow" target="_blank">亚历克·拉德福德</a></p></figure><blockquote class="nh"><p id="55c6" class="ni nj it bd nk nl nx ny nz oa ob lu dk translated">如何直观地思考动量？</p></blockquote><p id="82ec" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">想象两个球，一个在斜坡上滚动，另一个在碗中滚动。如果两个球都听天由命，会发生什么？</p><p id="36a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于第一个碗，它不断增加其速度(换句话说，动量)，而对于第二个碗，在几次摆动后，它将简单地在碗的底部静止不动。</p><p id="d9a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样的事情也会发生在参数上。如果重量倾向于向某一特定方向移动，动量就会增加(就像一个滚球在向下的斜坡上)。通过考虑这种势头，人们可以更快地更新权重，并以更少的步骤收敛。相反，如果重量主要围绕某一点波动(就像碗内滚动的球)，这就破坏了动量。这将有效地将学习率设置为接近于零，最终将参数设置为其局部最优。</p><p id="b7bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一范式中最引人注目的作品是<a class="ae ky" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> Adam:一种随机优化的方法</a>。在撰写本文时，它被引用了超过40k次(与ResNet大致相同)，是深度学习从业者中明显的赢家和优化器的共同选择。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="b73b" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">结束语</h2><p id="e20d" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">早在1998年就有一章关于高效反向传播的文章发表，这激发了许多上述当代工具的发展。</p><p id="50d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">我们花了将近20年的时间开发工具，在广泛的环境中优雅地解决几个培训问题。幸运的是，随着所有这些进步，训练神经网络不再是一门艺术</em></p><p id="6472" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个勤奋的工程师可以简单地用几行Python代码将不同的部分组合在一起，几乎可以保证获得好的结果。</p><p id="6043" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">这真正将</strong> <a class="ae ky" href="https://cloud.google.com/blog/products/gcp/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">黄瓜农民场景</strong> </a> <strong class="lb iu">带入了生活，导致深度学习的广泛适应，该领域不再局限于少数精英。</strong></p></div></div>    
</body>
</html>