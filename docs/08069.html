<html>
<head>
<title>Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语言模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/language-models-1a08779b8e12?source=collection_archive---------34-----------------------#2020-06-14">https://towardsdatascience.com/language-models-1a08779b8e12?source=collection_archive---------34-----------------------#2020-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1620" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">语言模型是许多自然语言任务的基础。本文是对统计语言模型的直观介绍</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/54208a9038d2f2a131ac1f6d3e513545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oJ0JIe9O0B0swpOuldXmIg.jpeg"/></div></div></figure><h1 id="2ddd" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">定义</h1><p id="ca69" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我在一次电话会议中，有人说“项目A的风险增加了__”我听不到of后面的那个词，但我知道那个词是什么。我相信你也知道这个词。</p><p id="43b0" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">我们有一个语言模型。我们的内部语言模型告诉我们，句子“项目A有延迟的高风险”的概率比句子“项目A有高风险的水”的概率高得多</p><p id="5009" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">这篇文章是关于统计学习语言模型(LM)——它们是什么，它们是如何被评估的，以及它们是如何被学习的。语言建模本身并没有直接的实际用途，但它是机器翻译和自动语音识别等现实应用中的一个重要组成部分。翻译系统可能会生成同一目标句子的多种翻译，语言模型会对所有句子进行评分，以选择最有可能的一个。</p><p id="9912" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">从形式上来说，语言建模的任务就是简单地给任何单词序列分配一个概率。或者，我们也可以把这个问题提出来作为猜词问题。填空:<strong class="ll ir"> <em class="mk">狗____ </em> </strong>。下面的等式从数学上显示了这种等价性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/2447baa37f0523fc5260122e27d4a32a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*O3NtI15baga79rC3ilDHxA.png"/></div></figure><h1 id="e9f6" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">衡量绩效</h1><p id="134a" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">您将如何衡量该模型的性能？最常见的<strong class="ll ir">内在</strong>指标是困惑。困惑衡量语言模型在预测一个看不见的单词序列中的下一个单词时有多困惑。Ravi Charan 的<a class="ae mo" rel="noopener" target="_blank" href="/the-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc">博客</a>中有一个很好的关于困惑的中级概述。</p><p id="e478" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">但是，对于大多数实际目的来说，外在的措施更有用。LM的外在度量是使用LM的底层任务的准确性。例如，使用给定语言模型的翻译任务的BLEU分数。</p><p id="534e" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">困惑是特定于语料库的度量。只有在相同的语料库上计算度量时，我们才能比较两个LMs的复杂度。困惑的改善并不能保证外在指标如BLEU评分的改善。</p><h1 id="d240" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">构建语言模型</h1><p id="180d" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">语言模型从一个<strong class="ll ir">马尔可夫假设</strong>开始。这是一个简化的假设，即第k+1个字依赖于前k个字。二阶假设导致二元模型。使用现有语料库的最大似然估计(MLE)来训练模型。MLE方法只是工作计数的一部分。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/dcbc9f8a098b05a5e059913e384e4f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*V9KAVMmzWQBovLAkxEebrw.png"/></div></figure><p id="bb80" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">使用传统的n元语言模型有一些优点。</p><ol class=""><li id="96ed" class="mq mr iq ll b lm mf lp mg ls ms lw mt ma mu me mv mw mx my bi translated">它们很容易在大型语料库上训练</li><li id="9162" class="mq mr iq ll b lm mz lp na ls nb lw nc ma nd me mv mw mx my bi translated">他们在大多数任务中表现惊人的好！！</li></ol><p id="7232" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">然而，它们也有一些缺点</p><ol class=""><li id="c50e" class="mq mr iq ll b lm mf lp mg ls ms lw mt ma mu me mv mw mx my bi translated">零概率:如果我们有一个包含两个单词的三元语言模型，并且拥有10000个单词的词汇量。我们有10个三胞胎。如果我们的训练数据有10个⁰词，那么在训练数据中有许多三元组将永远不会被观察到，因此基本MLE将把零概率分配给那些事件。零概率意味着无限的困惑。为了克服这个问题，在平滑技术家族下开发了许多技术。在<a class="ae mo" href="https://dl.acm.org/doi/pdf/10.3115/981863.981904?download=true" rel="noopener ugc nofollow" target="_blank">的这篇论文</a>中对这些技术进行了很好的概述。</li><li id="6213" class="mq mr iq ll b lm mz lp na ls nb lw nc ma nd me mv mw mx my bi translated"><strong class="ll ir">指数增长</strong>:第二个挑战是n元文法的数量以词汇量的n次方指数增长。一个10，000单词的词汇表将有10个三元组，一个100，000单词的词汇表将有10个⁵三元组。</li><li id="c6b6" class="mq mr iq ll b lm mz lp na ls nb lw nc ma nd me mv mw mx my bi translated">一般化:MLE技术的最后一个问题是缺乏一般化。如果模型在训练数据中看到术语“白马”，但没有看到“黑马”，MLE将把零概率分配给“黑马”。(幸运的是，它也会将零概率分配给紫马)</li></ol><h1 id="dd21" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">神经语言模型</h1><p id="ed76" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">非线性神经网络模型解决了传统语言模型的一些缺点。例如，与传统模型相比，神经LM的参数数量增加缓慢。最早的这种模型之一是由Bengio等人在2003年提出的。在一篇名为<a class="ae mo" href="http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank">一种神经概率语言模型</a>的经典论文中，他们展示了使用RNN学习单词表示的基本结构。</p><p id="ba18" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">引用这篇论文，他们提出了三个关键观点—</p><ol class=""><li id="e9c8" class="mq mr iq ll b lm mf lp mg ls ms lw mt ma mu me mv mw mx my bi translated">将词汇表中的每个单词与分布式单词特征向量(n维实值向量)相关联</li><li id="3a9a" class="mq mr iq ll b lm mz lp na ls nb lw nc ma nd me mv mw mx my bi translated">根据序列中这些单词的特征向量来表达单词序列的联合概率函数，以及</li><li id="00a8" class="mq mr iq ll b lm mz lp na ls nb lw nc ma nd me mv mw mx my bi translated">同时学习单词特征向量和概率函数的参数。</li></ol><h1 id="60ef" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">LMs的副产品是单词表示</h1><p id="4965" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">语言模型可以在原始文本上训练，比如来自维基百科的文本。为了训练k阶语言模型，我们从运行的文本中取出(k + 1)个字，并将第(k + 1)个字视为监督信号。因此，我们可以从任何语言的各种在线/数字化数据中生成大量的训练数据。</p><p id="1b17" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">使用神经模型学习语言模型的一个特别重要的副产品是如下所示的单词矩阵。我们不仅更新训练参数，还更新单词矩阵。然后，字矩阵可以用于各种不同的监督任务。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/da54396b1021344669f8a883d87c6222.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*dxtu6Nru5fN7tPRHcQIX6Q.png"/></div></figure><h1 id="fc12" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">结论</h1><p id="2c96" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">几乎所有的NLP任务都使用语言模型。语言模型用于语音识别、机器翻译、词性标注、语法分析、光学字符识别、手写识别和信息检索。</p><p id="c2bf" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">传统的语言模型在这些用例中表现得相当好。深度学习时代带来了新的语言模型，这些模型在几乎所有的任务中都优于传统模型。典型的深度学习模型是在大型数据语料库上训练的(<a class="ae mo" href="https://www.msn.com/en-us/news/technology/openai-e2-80-99s-gigantic-gpt-3-hints-at-the-limits-of-language-models-for-ai/ar-BB14RvVk" rel="noopener ugc nofollow" target="_blank"> GPT-3是在从网络上搜集的一万亿单词的文本上训练的</a>)，具有很大的学习能力(GPT-3有1750亿个参数)，并使用新颖的训练算法(注意力网络，BERT)。</p><p id="f0ae" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">尽管学习LMs的机制已经发生了变化，但是LMs背后的基本直觉仍然是一样的。</p></div></div>    
</body>
</html>