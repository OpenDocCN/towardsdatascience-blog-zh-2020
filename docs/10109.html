<html>
<head>
<title>How to build a Neural Network from Scratch with Numpy! (Part I)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 Numpy 从零开始构建神经网络！(第一部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-neural-network-implementation-part-i-eb31f4ea470?source=collection_archive---------43-----------------------#2020-07-16">https://towardsdatascience.com/a-neural-network-implementation-part-i-eb31f4ea470?source=collection_archive---------43-----------------------#2020-07-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b29ef902d861dcbf95a22338af90682b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HEwLXz__e_d1h9USa2tyCw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图一。我关于反向传播的笔记。我拍的照片。</p></figure><div class=""/><div class=""><h2 id="6e66" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">仅使用 Numpy 的前馈实现！</h2></div><p id="525b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">神经网络是<strong class="kz jj">深度学习的核心。</strong>它们为伯特或 GPT-3 等卓越模型奠定了基础。为了对<strong class="kz jj">变形金刚</strong>的功能有一个非常肤浅的了解，我认为有必要学习如何从零开始建立一个神经网络，使用现有的最基本的数学工具。</p><p id="dbee" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">基于此，我决定投资一些时间和代码<strong class="kz jj">我自己的神经网络实现</strong>。你可以在我的 Github 页面(<a class="ae lt" href="https://github.com/next-manuelmartin5/neural-network-implementation" rel="noopener ugc nofollow" target="_blank"> <strong class="kz jj"> neuralnet </strong> </a>)看看完整的项目。是的…我知道，这不是一个非常原始的名字👻！</p><p id="383c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我将在几篇不同的博文中讨论这个话题。</p><h1 id="87c8" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated"><strong class="ak">理论第一</strong></h1><p id="6d4b" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">我想做的第一件事，是向前和向后传播背后的精确的数学运算。我不想看也不想复制其他帖子的实现，所以我选择了<strong class="kz jj"><em class="mr"/></strong><em class="mr"/><strong class="kz jj"><em class="mr">桑德罗·斯坎西</em> </strong>(来自斯普林格出版社)的《深度学习入门》，这本书似乎正适合这个任务。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/2552e6aefb8b9e2aaaecf973db1147a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/0*O1TdpqkrHQXZL9kX.jpg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><em class="mx">图二。桑德罗·斯坎西的《深度学习导论》封面。图片来自</em> <a class="ae lt" href="https://www.amazon.es/Introduction-Deep-Learning-Intelligence-Undergraduate-ebook/dp/B079M3D7JF" rel="noopener ugc nofollow" target="_blank"> <em class="mx">亚马逊</em> </a></p></figure><p id="c5b8" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在第四章有一个关于浅层前馈神经网络的精彩解释。我研究了一下，先用手开发了方程，这样开发的时候就不用挤破头了。(事实上，帖子顶部的图片是我的真实笔记，我花了一个下午的时间把整个算法做对了)。当你对自己想要创造的东西有了一个明确的想法时，最好开始编码。此外，当你进行科学编程时。</p><p id="e69e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">您会注意到，我的项目明显带有“<em class="mr">Keras API-based”</em>偏见。我从它那里借用了一些概念来封装元素，比如<em class="mr">层</em>抽象。最后，是最合理的方式来揭露神经网络的建设。</p><h1 id="4c1b" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">基本概念</h1><p id="22cf" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">现在，我将总结实现网络的这一系列文章中将要涉及的主题。将有数学表达式和相关代码的解释。</p><p id="2a1a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这一部分将涉及的主题有:</p><p id="a19c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">前馈传播</strong></p><ul class=""><li id="3c3e" class="my mz ji kz b la lb ld le lg na lk nb lo nc ls nd ne nf ng bi translated">层权重</li><li id="924a" class="my mz ji kz b la nh ld ni lg nj lk nk lo nl ls nd ne nf ng bi translated">激活功能</li><li id="f2b1" class="my mz ji kz b la nh ld ni lg nj lk nk lo nl ls nd ne nf ng bi translated">向量化偏置吸收</li><li id="6f5e" class="my mz ji kz b la nh ld ni lg nj lk nk lo nl ls nd ne nf ng bi translated">全正向传播</li></ul><p id="acfe" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">以下几点将在即将到来的第二部分中解释！😄</p><ul class=""><li id="a5ce" class="my mz ji kz b la lb ld le lg na lk nb lo nc ls nd ne nf ng bi translated">优化:梯度下降</li><li id="a4fa" class="my mz ji kz b la nh ld ni lg nj lk nk lo nl ls nd ne nf ng bi translated">损失函数</li><li id="cf05" class="my mz ji kz b la nh ld ni lg nj lk nk lo nl ls nd ne nf ng bi translated">反向传播和链式法则</li><li id="31f5" class="my mz ji kz b la nh ld ni lg nj lk nk lo nl ls nd ne nf ng bi translated">权重初始化</li></ul><h1 id="e7a9" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">正向传播</h1><p id="3c64" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">从这一步开始，我们将关注一个<strong class="kz jj">二进制分类问题</strong>，使用一个基本的前馈架构作为构建代码的基础。我们会使用这个设置，因为这是机器学习中<strong class="kz jj">最常见的问题</strong>之一，也是最容易解释这个概念的。</p><p id="8c45" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">正如下面图 3 的描述，我们的网络在输入层由 2 个神经元组成，这意味着网络将接受 2 个分量的向量，然后是隐藏层的 3 个和输出层的 1 个。我们将在后面看到，通过使用激活函数，输出神经元将返回输入观察值属于这一类或那一类的概率。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8b670986948c42401799b83b35ad3c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*z1JgB4IAfHm6US38DiRaBg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 3。浅层前馈神经网络</p></figure><h2 id="da10" class="nn lv ji bd lw no np dn ma nq nr dp me lg ns nt mg lk nu nv mi lo nw nx mk ny bi translated"><strong class="ak"> <em class="mx">层权重</em> </strong></h2><p id="95c9" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">每条线和每个箭头表示权重矩阵，该权重矩阵关于每一层将神经元相互关联。这些权重将<strong class="kz jj">考虑</strong>前一个输入或层输出的“多少”将被转移到下一个。我们可以用数学方法写出这些矩阵，如下所示。</p><p id="c4c2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">表示输入层和隐藏层之间的权重关系的矩阵将被表示为θIH，隐藏层和输出层之间的权重被表示为θHO。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/120c69ec7671ed938db7e3b29b47c8da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3dop4yyXbUPkw7YtIprtvw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 4。每层的权重矩阵。</p></figure><p id="b4aa" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然后，为了明确定义从每一层到下一层的转换，我们需要定义什么是激活函数。</p><h2 id="b3e9" class="nn lv ji bd lw no np dn ma nq nr dp me lg ns nt mg lk nu nv mi lo nw nx mk ny bi translated"><strong class="ak"> <em class="mx">激活功能</em> </strong></h2><p id="d3c6" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">基于生物启发模型来阅读激活函数的定义是很常见的。但是让我们忽略一部分，因为我认为这有点令人困惑。</p><blockquote class="oa ob oc"><p id="dd6c" class="kx ky mr kz b la lb kj lc ld le km lf od lh li lj oe ll lm ln of lp lq lr ls im bi translated"><em class="ji">关于生物学和哲学基础的一些注释可以在论文</em> <a class="ae lt" href="https://arxiv.org/pdf/1702.07800.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ji">“关于深度学习的起源</em> </a> <em class="ji">”中阅读，该论文用非常精辟和系统的方法解释了这些初级概念化。</em></p></blockquote><p id="16d6" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我更喜欢将激活函数视为非线性的(实际上，除了在<a class="ae lt" href="https://en.wikipedia.org/wiki/Perceptron" rel="noopener ugc nofollow" target="_blank">感知器</a>上，总是非线性的)转换，这种转换将它们的输出压缩到某个范围内(近似概率分布)，并最终允许神经网络证明它们是<strong class="kz jj">通用函数近似器<em class="mr">。</em> </strong> <em class="mr"> </em>这个理论解释了为什么神经网络表现出如此<strong class="kz jj">高的表示能力</strong>。</p><blockquote class="oa ob oc"><p id="1978" class="kx ky mr kz b la lb kj lc ld le km lf od lh li lj oe ll lm ln of lp lq lr ls im bi translated">关于<em class="ji">普适近似理论</em>的一个很酷的解释可以在这里读到<a class="ae lt" rel="noopener" target="_blank" href="/the-approximation-power-of-neural-networks-with-python-codes-ddfc250bdb58">。</a></p></blockquote><p id="fbdb" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在我的实现中，我选择了<em class="mr"> sigmoid </em>激活函数，它很长时间以来一直是标准的激活函数(尽管最近它已经被 ReLu 及其所有派生函数所取代)。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi og"><img src="../Images/65e5bb18cdffd37dc0d2129ed3474f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z4dEjWPUix53Qy7MJHATJw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 5。乙状结肠激活功能。</p></figure><p id="6254" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">代码实现如下所示。</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h2 id="668c" class="nn lv ji bd lw no np dn ma nq nr dp me lg ns nt mg lk nu nv mi lo nw nx mk ny bi translated"><strong class="ak"> <em class="mx">偏置吸收</em> </strong></h2><p id="43dc" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">现在，我们有了几乎所有的工具来了解如何计算前向传播。让我们先看看表达式在 2 层前馈架构上是什么样子的:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/9d47705c934080fcc23384e4992d7af7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EyRjVxkwbE6VoEMM78jJvQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 6。浅层网络上的前向传播。</p></figure><p id="ec50" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">可以清楚地看到，正向传播的形式似乎很简单。这只是一种<strong class="kz jj">类型的功能组合</strong>。</p><p id="64a8" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">*在这篇文章中，我们继承了用神经元和它们的联系来展示什么是神经网络的传统，但最终如果你看一下上面的表达式，就很容易理解网络的基础是什么。如果应用于输入 X，表达式的输出将是该输入属于第一类的概率。</p><blockquote class="oa ob oc"><p id="b58b" class="kx ky mr kz b la lb kj lc ld le km lf od lh li lj oe ll lm ln of lp lq lr ls im bi translated"><em class="ji">为什么</em>θIH 和θHO 有<strong class="kz jj"> b </strong>下标？→因为我们正在利用<strong class="kz jj">偏置吸收</strong>实现正向和反向传播。这不是一个非常流行的概念，但我认为它在计算方面更有效，因为它允许完全矢量化的实现。</p></blockquote><p id="0297" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">偏置吸收基本上是将<strong class="kz jj">偏置项视为各层的另一个输入</strong>，并迫使它们为 1。然后，权重将被叠加到图层权重中，以便对其进行估计以符合数据。</p><p id="63b6" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">*我们之所以需要使用偏差项，是因为它提高了模型适应不同数据点的灵活性。</p><p id="8c5f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">让我们用一个<strong class="kz jj">简单的例子来解释这个:</strong></p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/0b42d0de8ccce96f2209c85524ea3312.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*Hi0OrJJWrVKVCfNEJQ2ILw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 7。单层感知器</p></figure><p id="5c03" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">想象具有上面在图 7 中定义的架构:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/c2ded8d54c0cd55795c0ca348997d381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*D_z7lyOXs3-qqhlr4AsYjw.png"/></div></figure><p id="4094" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">如果我们对输入向量和权重应用偏置吸收，</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/9bcc94e40736554116dbe261491d3663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*Hrgs-B13nTsgT23xDdic2w.png"/></div></figure><p id="a9a2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然后，把这个想法带到我们最初的问题和网络架构中，</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/a1b283a59e8567446e678dc628004863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0fxp1j45mGGi1ZtnrIIkw.png"/></div></div></figure><p id="8c5a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">注意，我们已经在两个矩阵中添加了一个<strong class="kz jj">新行来处理点积，我们将在下一步解释。</strong></p><h2 id="76c8" class="nn lv ji bd lw no np dn ma nq nr dp me lg ns nt mg lk nu nv mi lo nw nx mk ny bi translated"><strong class="ak"> <em class="mx">正向传播的定义</em> </strong></h2><p id="3c63" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">现在，我们有了一切来显示如何计算前向传播。让我们看一下代码。</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="5c0b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">函数<em class="mr"> add_bias </em>基本上是将一列 1 叠加到给定的向量上，以允许层内的偏置吸收。然后在<em class="mr"> FullyConnectedLayer </em>类中，我们有<em class="mr">激活</em>，它是之前定义的<em class="mr"> sigmoid </em>函数的封装，还有<em class="mr">权重</em>属性，它应该由一个更高级别的类<em class="mr"> NeuralNet </em>(将在后面介绍)通过使用方法<em class="mr"> initialize_weights </em>来初始化。在这一点上，只要认为权重是从正态分布中随机初始化的。</p><p id="cc20" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">虽然我在上面的要点中删除了一些样板代码，你会注意到有一些痕迹。这是因为它应该处理不同的初始化技术或激活功能，但对所有这些功能保持相同的接口。走 <a class="ae lt" href="https://github.com/next-manuelmartin5/neural-network-implementation/blob/develop/neuralnet/layers.py" rel="noopener ugc nofollow" target="_blank"> <em class="mr">这里</em> </a> <em class="mr">为完整代码。</em></p><p id="f64c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然后，我们有了<strong class="kz jj"> <em class="mr">前进</em> </strong>的方法。该方法应该在承载它的层上执行传播。将这一行代码转换成数学表达式将会产生类似于:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/f026fbdb6a9763e5bbc546839cc15a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwrCmZ8Hpuz3u9cszrBIWw.png"/></div></div></figure><p id="51b0" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">由<em class="mr"> yFCL </em>表示的最后一个表达式，它只是<em class="mr"> FullyConnectedLayer </em>的首字母缩略词，它所做的正是<em class="mr"> forward </em>方法所显示的。它应用输入<em class="mr"> x </em>和权重矩阵<em class="mr">θIHb 的点积。</em></p><p id="aef5" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="mr"> * </em>您可能已经注意到，在权重和输入顺序方面，该表达式与图 6 中的表达式并不完全相同。只要你的向量和矩阵适当地适应矩阵乘积，这是完全有效的。</p><p id="9f07" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">注意形状</strong>对于正确实施来说，这是非常重要的一点。您应该知道输入所需的形状，以及它们将如何在网络的各层之间相乘和转换。</p><p id="7006" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这只是对单层转发的解释，但是，<strong class="kz jj">如何处理输入<em class="mr">信号</em>端到端的传播，直到网络的末端？</strong>如果我们回想起展示 2 层网络上完全正向传播的数学表达式的图:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/9d47705c934080fcc23384e4992d7af7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EyRjVxkwbE6VoEMM78jJvQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 6。浅层网络上的前向传播。</p></figure><p id="8ee9" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这是一个简单的编码操作，可以毫不费力地推广到 N 层。这是它在 Python 中的样子。</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="2f5b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">简单总结一下我们在这里做的事情，<em class="mr"> _build_layers </em>方法负责设置在实例化类<em class="mr"> NeuralNet </em>时定义的层。您应该做类似于以下的事情:</p><pre class="mt mu mv mw gt op oq or os aw ot bi"><span id="497a" class="nn lv ji oq b gy ou ov l ow ox">nn = NeuralNet(<br/>    layer_shapes=(<br/>        (2, 3),<br/>        (3, 1)<br/>    )<br/>)</span></pre><p id="8613" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">通过这样做，你只是在每层之间定义权重的初始形状。然后，在<em class="mr"> __init__ </em>中，那些形状被转换成吸收的 bias 版本，也就是在初始形状上增加一个新行(就像我们之前解释过的一样！).</p><pre class="mt mu mv mw gt op oq or os aw ot bi"><span id="a269" class="nn lv ji oq b gy ou ov l ow ox">def set_bias_as_weight(shape):<br/>    return shape[0] + 1, shape[1]</span></pre><p id="742a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在有了合适的权重形状后，通过使用一种叫做<a class="ae lt" href="https://medium.com/@prateekvishnu/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528" rel="noopener"> <em class="mr">和普通</em> </a>初始化的技术，它们被<strong class="kz jj">初始化</strong>。每一层都集成在我称之为<em class="mr">层链</em>中(类似于 Keras 提供的<em class="mr">顺序</em>类)。它基本上是一个增强的<em class="mr">列表</em>对象，具有一些功能来简化整个层的操作。</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="de4e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">你可以在<em class="mr"> NeuralNet </em>类中看到我们如何使用这个类<strong class="kz jj">来遍历所有层以执行完整的传播</strong>。另外，澄清一下，这个实现是<strong class="kz jj">完全矢量化的</strong>。这样做的根本原因是训练基于神经网络的模型是通过<em class="mr">梯度下降以<strong class="kz jj">分批迭代</strong>的方式完成的。这给了我们利用并行处理和矩阵运算来优化代码效率空间。因此，尽管我们在整篇文章中一直在使用单个观察示例，但是如果不是一个观察，而是将 N 个观察堆叠到同一个向量<em class="mr"> x </em>中，所有的计算也将有效地工作。</em></p><p id="32c8" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="mr">正向</em>方法的输出将用于<strong class="kz jj"><em class="mr"/></strong><em class="mr"/>反向传播，以计算我们相对于目标产生的误差，并且一旦我们训练了我们的神经网络，也用于进行推断。</p><p id="c658" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这就是第一部分的全部内容！希望你喜欢它，一切都很清楚！第二部分见。</p></div></div>    
</body>
</html>