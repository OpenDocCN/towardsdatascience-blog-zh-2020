<html>
<head>
<title>Let’s Code Convolutional Neural Network in plain NumPy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们用简单的数字编码卷积神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lets-code-convolutional-neural-network-in-plain-numpy-ce48e732f5d5?source=collection_archive---------6-----------------------#2020-06-01">https://towardsdatascience.com/lets-code-convolutional-neural-network-in-plain-numpy-ce48e732f5d5?source=collection_archive---------6-----------------------#2020-06-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0c64" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">神经网络的奥秘</h2></div><div class="kf kg gp gr kh ki"><a href="https://github.com/SkalskiP/ILearnDeepLearning.py" rel="noopener  ugc nofollow" target="_blank"><div class="kj ab fo"><div class="kk ab kl cl cj km"><h2 class="bd ir gy z fp kn fr fs ko fu fw ip bi translated">SkalskiP/ILearnDeepLearning.py</h2><div class="kp l"><h3 class="bd b gy z fp kn fr fs ko fu fw dk translated">这个存储库包含与神经网络和深度学习相关的小项目。主题是紧密的…</h3></div><div class="kq l"><p class="bd b dl z fp kn fr fs ko fu fw dk translated">github.com</p></div></div><div class="kr l"><div class="ks l kt ku kv kr kw kx ki"/></div></div></a></div><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ld le l"/></div></figure><p id="b04d" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">我们生活在一个迷人的时代，深度学习(DL)不断应用于我们生活的新领域，并经常给停滞不前的行业带来革命性的变化。与此同时，像<a class="ae mk" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>和<a class="ae mk" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>这样的开源框架让每个人都有机会使用最先进的工具和算法。这些库强大的社区和简单的API使尖端模型触手可及成为可能，即使没有使这一切成为可能的深入的数学知识。</p><p id="1fc7" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，对神经网络[NN]内部发生的事情的理解对于架构选择、超参数调整或性能优化等任务非常有帮助。因为我相信没有什么比弄脏自己的手更能教会你的了，<strong class="lh ir">我将向你展示如何创建一个卷积神经网络[CNN]，它能够对</strong><a class="ae mk" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"><strong class="lh ir">【MNIST】</strong></a><strong class="lh ir">图像进行分类，准确率高达90%，仅使用</strong><a class="ae mk" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"><strong class="lh ir">NumPy</strong></a><strong class="lh ir">。</strong></p><blockquote class="ml mm mn"><p id="94c0" class="lf lg mo lh b li lj jr lk ll lm ju ln mp lp lq lr mq lt lu lv mr lx ly lz ma ij bi translated"><strong class="lh ir">注:</strong>卷积神经网络是一种深度神经网络，最常用于分析图像。</p></blockquote><p id="f496" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本文主要面向对DL框架有一些经验的人。然而，如果你只是一个初学者——进入神经网络的世界——请不要害怕！这一次，我不打算分析任何数学方程式。老实说，我甚至不打算把它们写下来。而是尽量给大家一个直觉，在这些知名库的掩护下发生了什么。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/f30bdf5685cabf37c9dcbbb83d8756fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBR2ZCkZbLd18wp3s_ZY1Q.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated"><strong class="bd nc">图一。</strong>卷积神经网络架构</p></figure><h2 id="2691" class="nd ne iq bd nf ng nh dn ni nj nk dp nl lo nm nn no ls np nq nr lw ns nt nu nv bi translated">介绍</h2><p id="f9e9" class="pw-post-body-paragraph lf lg iq lh b li nw jr lk ll nx ju ln lo ny lq lr ls nz lu lv lw oa ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di">正如</span>已经提到的，我们的主要目标是基于上图所示的架构构建一个CNN，并在MNIST图像数据集上测试其功能。然而，这一次我们不会使用任何流行的DL框架。相反，我们将利用NumPy——一个强大但低级的Python线性代数库。当然，这种方法将显著地使我们的工作复杂化，但是同时，它将允许我们理解在我们的模型的每个阶段正在发生什么。在此过程中，我们将创建一个包含所有必要层的简单库，因此您将能够继续尝试并解决其他分类问题。</p><blockquote class="ml mm mn"><p id="3529" class="lf lg mo lh b li lj jr lk ll lm ju ln mp lp lq lr mq lt lu lv mr lx ly lz ma ij bi translated"><strong class="lh ir">注意:</strong><strong class="lh ir"/>是一个手写数字的大型数据库，通常用作图像识别算法的基准。每张黑白照片都是28x28 px。</p></blockquote><h2 id="8e41" class="nd ne iq bd nf ng nh dn ni nj nk dp nl lo nm nn no ls np nq nr lw ns nt nu nv bi translated">数字图像、张量和形状</h2><p id="029b" class="pw-post-body-paragraph lf lg iq lh b li nw jr lk ll nx ju ln lo ny lq lr ls nz lu lv lw oa ly lz ma ij bi mb translated">让我们停下来分析一下数字图像的结构，因为它会直接影响我们的设计决策。事实上，数码照片是巨大的数字矩阵。每个这样的数字代表单个像素的亮度。在RGB模型中，彩色图像由对应于三个颜色通道(红色、绿色和蓝色)的三个这样的矩阵组成。另一方面，要表现灰度图像——就像我们在MNIST数据集中看到的那样——我们只需要一个这样的矩阵。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ob"><img src="../Images/af3d2ba149cebba258e341d73af93c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yo5DrwucZ_tRKR5DD5bBjA.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated"><strong class="bd nc">图二。</strong>数字图像背后的数据结构</p></figure><p id="da2e" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在线性代数中，这些结构化的多维矩阵称为<strong class="lh ir">张量</strong>。张量维度由它们的<strong class="lh ir">形状</strong>描述。例如，单个MNIST图像的形状是<code class="fe oc od oe of b">[28, 28, 1]</code>，其中连续的值表示高度、宽度和颜色通道的数量。</p><h2 id="5004" class="nd ne iq bd nf ng nh dn ni nj nk dp nl lo nm nn no ls np nq nr lw ns nt nu nv bi translated">顺序模型</h2><p id="c2e5" class="pw-post-body-paragraph lf lg iq lh b li nw jr lk ll nx ju ln lo ny lq lr ls nz lu lv lw oa ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"/>顺序模型是一种连续层形成线性流的模型——第一层的结果用作第二层的输入，依此类推。模型在这个管弦乐队中充当指挥，负责控制各层之间的数据流。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi og"><img src="../Images/df58b3423fac5c2ce3e07fed6449fed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*7O3HgFUz_7yeHerUxk0h7A.gif"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated"><strong class="bd nc">图3。</strong>顺序模型数据流</p></figure><p id="ee5e" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有两种流量类型— <strong class="lh ir">向前和向后</strong>。我们使用<strong class="lh ir">正向传播来基于已经积累的知识和作为输入<code class="fe oc od oe of b">X</code>提供的新数据进行预测</strong>。另一方面，<strong class="lh ir">反向传播就是将我们的预测</strong> <code class="fe oc od oe of b"><strong class="lh ir">Y_hat</strong></code> <strong class="lh ir">与真实值</strong> <code class="fe oc od oe of b"><strong class="lh ir">Y</strong></code> <strong class="lh ir"> </strong>进行比较，并得出结论。因此，我们网络的每一层都必须提供两个方法:<code class="fe oc od oe of b">forward_pass</code>和<code class="fe oc od oe of b">backward_pass</code>，这两个方法都可以被模型访问。一些层——密集层和卷积层——也将具有收集知识和学习的能力。它们保留自己的张量，称为权重，并在每个时期结束时更新它们。简单来说，模型训练的<strong class="lh ir">单个时期由三个元素组成:向前和向后传递以及权重更新</strong>。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="oh le l"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">顺序模型—点击<a class="ae mk" href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/model/sequential.py" rel="noopener ugc nofollow" target="_blank">此处</a>查看完整代码</p></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><a href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/model/sequential.py"><div class="gh gi oi"><img src="../Images/ecdb2139af093eae3a038858b06c093e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*8YfJcBhQ0ssAhG8PJDQ2pg.gif"/></div></a></figure><h2 id="e842" class="nd ne iq bd nf ng nh dn ni nj nk dp nl lo nm nn no ls np nq nr lw ns nt nu nv bi translated">盘旋</h2><p id="f56c" class="pw-post-body-paragraph lf lg iq lh b li nw jr lk ll nx ju ln lo ny lq lr ls nz lu lv lw oa ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"> C </span> onvolution是一种操作，我们采用一个小的数字矩阵(称为内核或过滤器)并将其传递到我们的图像上，以根据过滤器值对其进行变换。将我们的内核放在一个选中的像素上后，我们从过滤器中取出每一个值，并与图像中相应的值成对相乘。最后，我们将所有内容相加，并将结果放在输出矩阵的正确位置。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi oj"><img src="../Images/d814fe4eab00bec4ecd630502d295369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*SBVATujRwx3ehLYuf6gCQg.gif"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated"><strong class="bd nc">图4。</strong>卷积层正向传递</p></figure><p id="35c9" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">很简单吧？对吗？嗯，通常情况下，事情会更复杂一点。为了加快计算速度，一个层通常一次处理多个图像。因此，我们传递一个形状为<code class="fe oc od oe of b">[n, h_in, w_in, c]</code>的四维张量作为输入。这里的<code class="fe oc od oe of b">n</code>对应于并行处理的图像数量，即所谓的<strong class="lh ir">批量</strong>。其余的尺寸都很标准——宽度、高度和通道数量。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ob"><img src="../Images/229ea07a921309a8eed89826fa04104c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nXIh5vciNkZa8Fe-mjnzWQ.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated"><strong class="bd nc">图5。</strong>卷积超过体积</p></figure><p id="cb27" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，通常，输入张量可以有多个通道。上面，你可以看到一个对彩色图像执行卷积的层的例子。这个过程叫做体积卷积。<strong class="lh ir">在这种情况下，最重要的规则是，滤镜和图像必须有相同数量的通道。</strong>我们的操作非常类似于标准卷积，但这次我们将三维张量中的数字对相乘。</p><p id="4386" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，为了使图层尽可能地通用，每个图层通常都包含多个滤镜。<strong class="lh ir">我们分别对每个核进行卷积，将结果一个接一个地堆叠起来，然后将它们组合成一个整体。</strong>卷积层前向传递产生具有<code class="fe oc od oe of b">[n, h_out, w_out, n_f]</code>形状的四维张量，其中<code class="fe oc od oe of b">n_f</code>对应于在给定层中应用的滤波器的数量。让我们看看下面的图像，对这些维度有更多的直觉。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ok"><img src="../Images/e9f9bb84e53925bfbbd959499a68e992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4siyxIL6r-_l54ThEcGI4g.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated"><strong class="bd nc">图六。</strong>卷积张量形状</p></figure><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="oh le l"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">卷积层——点击<a class="ae mk" href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/convolutional.py" rel="noopener ugc nofollow" target="_blank">此处</a>查看完整代码</p></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><a href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/convolutional.py"><div class="gh gi oi"><img src="../Images/5abf291d7ca17dfbad7530cc0ce73858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*STf6zrExRHrsYvJ-E4zV2g.gif"/></div></a></figure><h2 id="1bc8" class="nd ne iq bd nf ng nh dn ni nj nk dp nl lo nm nn no ls np nq nr lw ns nt nu nv bi translated">最大池化</h2><p id="75e5" class="pw-post-body-paragraph lf lg iq lh b li nw jr lk ll nx ju ln lo ny lq lr ls nz lu lv lw oa ly lz ma ij bi mb translated">人们普遍认为，分辨率越高，照片质量越好。毕竟，图片中可见对象的平滑边缘会使整个场景对人眼更具吸引力。有趣的是，很多时候，更多的像素并不能转化为更详细的图像理解。看起来电脑根本不在乎。<strong class="lh ir">存储这些冗余像素称为过表示。</strong>通常，即使张量体积的显著减少也不会影响实现的预测的质量。</p><blockquote class="ml mm mn"><p id="6cf9" class="lf lg mo lh b li lj jr lk ll lm ju ln mp lp lq lr mq lt lu lv mr lx ly lz ma ij bi translated"><strong class="lh ir">注:</strong>现在标准的智能手机摄像头能够产生12Mpx的图像。这样的图像由3600万个数字组成的共色张量来表示。</p></blockquote><p id="db86" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ir">池层的主要任务是减少我们张量的空间大小。</strong>我们这样做是为了限制需要训练的参数数量，从而缩短整个训练过程。这种效果是通过将张量分成几个部分，然后在每个部分分别应用我们选择的函数来实现的。该函数必须以这样一种方式定义，即对于每个部分，它都返回一个值。根据我们的选择，我们可以处理，例如，最大或平均池。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ol"><img src="../Images/db3f31e22db51ed73cbf6ee15baf099d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*9z25jHn_4QSUgIj6CIUEHQ.gif"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated"><strong class="bd nc">图7。</strong>最大池层</p></figure><p id="d7e8" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">上面的可视化显示了一个简单的最大池操作。在正向传播过程中，我们迭代每个部分并找到它的最大值。我们复制这个数字并保存在输出中。同时，我们也记住了我们选择的数字的位置。因此，创建了两个张量，一个是输出，然后传递到下一层，另一个是掩膜，将在反向传播过程中使用。汇集层将张量从原始形状<code class="fe oc od oe of b">[n, h_in, w_in, c]</code>转换为<code class="fe oc od oe of b">[n, h_out, w_out, c]</code>。这里，<code class="fe oc od oe of b">h_in</code>和<code class="fe oc od oe of b">h_out</code>之间的比率由步幅和<code class="fe oc od oe of b">pool_size</code>超参数定义。</p><p id="204b" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当通过池层反向传播时，我们从微分张量开始，并尝试扩展它的维度。首先，我们创建一个形状为<code class="fe oc od oe of b">[n, h_in, w_in, c]</code>的空张量并用零填充它。然后，使用缓存的掩膜张量将输入值重新定位到先前被最大数占据的位置。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="oh le l"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">最大池层—点击<a class="ae mk" href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/pooling.py" rel="noopener ugc nofollow" target="_blank">此处</a>查看完整代码</p></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><a href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/pooling.py"><div class="gh gi oi"><img src="../Images/2bea462616017717d3ffcc7c38a7a7bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*PoHxZSnhqrW1SfVx0oOa3Q.gif"/></div></a></figure><h2 id="8de6" class="nd ne iq bd nf ng nh dn ni nj nk dp nl lo nm nn no ls np nq nr lw ns nt nu nv bi translated">拒绝传统社会的人</h2><p id="252a" class="pw-post-body-paragraph lf lg iq lh b li nw jr lk ll nx ju ln lo ny lq lr ls nz lu lv lw oa ly lz ma ij bi mb translated">这是最流行的调整和防止神经网络过拟合的方法之一。这个想法很简单——辍学层的每个单元都被赋予了在训练中被暂时忽略的概率。然后，在每次迭代中，我们根据分配的概率随机选择我们丢弃的神经元。下面的可视化显示了一个遭受脱落的图层示例。我们可以看到，在每次迭代中，随机神经元是如何被去激活的。结果，权重矩阵中的值变得更加均匀分布。该模型平衡了风险，避免将所有筹码押在一个数字上。在推断过程中，dropout层被关闭，因此我们可以访问所有参数。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi om"><img src="../Images/466f432c90149760fad84d867c3c2ae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*4AExi8XAAQ9qCAEKgE0VQA.gif"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated"><strong class="bd nc">图8。</strong>脱落层</p></figure><blockquote class="ml mm mn"><p id="b2c5" class="lf lg mo lh b li lj jr lk ll lm ju ln mp lp lq lr mq lt lu lv mr lx ly lz ma ij bi translated"><strong class="lh ir">注意:</strong>当我们的模型过于接近有限的一组数据点时，就会发生过度拟合。给定一组新数据，像这样的模型将很难概括，而且很可能失败。</p></blockquote><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="oh le l"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">脱落层—点击<a class="ae mk" href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/dropout.py" rel="noopener ugc nofollow" target="_blank">此处</a>查看完整代码</p></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><a href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/dropout.py"><div class="gh gi oi"><img src="../Images/ca539029e59cb19b865f4fd0ed378a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*0qwtwla9qLCd6Wt37T-QrA.gif"/></div></a></figure><h2 id="6913" class="nd ne iq bd nf ng nh dn ni nj nk dp nl lo nm nn no ls np nq nr lw ns nt nu nv bi translated">变平</h2><p id="379e" class="pw-post-body-paragraph lf lg iq lh b li nw jr lk ll nx ju ln lo ny lq lr ls nz lu lv lw oa ly lz ma ij bi mb translated">这无疑是我们在旅途中实现的最简单的一层。然而，它在卷积层和密集连接层之间起着至关重要的连接作用。顾名思义，在正向传递过程中，它的任务是将输入扁平化，从多维张量变为向量。我们将在反向过程中反向执行此操作。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi on"><img src="../Images/77676c8755c3cc1dc2cc8b32b5610d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*xiA6u9TO6rkOaO7ESOgxEQ.gif"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated"><strong class="bd nc">图九。</strong>展平图层</p></figure><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="oh le l"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">展平图层——点击<a class="ae mk" href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/flatten.py" rel="noopener ugc nofollow" target="_blank">此处</a>查看完整代码</p></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><a href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/flatten.py"><div class="gh gi oi"><img src="../Images/c5c28fa918359b0f080076c199be65ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*RvU_RWNOUL5Wp-Io46bOzw.gif"/></div></a></figure><h2 id="5056" class="nd ne iq bd nf ng nh dn ni nj nk dp nl lo nm nn no ls np nq nr lw ns nt nu nv bi translated">激活</h2><p id="b992" class="pw-post-body-paragraph lf lg iq lh b li nw jr lk ll nx ju ln lo ny lq lr ls nz lu lv lw oa ly lz ma ij bi mb translated">管理我们将使用的所有函数，有几个简单但强大的函数。激活函数可以用一行代码编写，但它们赋予了神经网络迫切需要的非线性和表达能力。<strong class="lh ir">如果没有激活，NN将变成线性函数的组合，因此它本身就是一个线性函数</strong>。我们的模型表达能力有限，不超过逻辑回归。非线性元件允许学习过程中更大的灵活性和复杂函数的创建。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="oh le l"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">ReLU层—点击<a class="ae mk" href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/activation/relu.py" rel="noopener ugc nofollow" target="_blank">此处</a>查看完整代码</p></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><a href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/activation/relu.py"><div class="gh gi oi"><img src="../Images/ce18d990baf5a0ed310ed29f59b09376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*RlMb6ISFRtJcaZ7A4BLvMw.gif"/></div></a></figure><h2 id="f0b5" class="nd ne iq bd nf ng nh dn ni nj nk dp nl lo nm nn no ls np nq nr lw ns nt nu nv bi translated">稠密的</h2><p id="06b0" class="pw-post-body-paragraph lf lg iq lh b li nw jr lk ll nx ju ln lo ny lq lr ls nz lu lv lw oa ly lz ma ij bi mb translated">与激活功能类似，密集层是深度学习的基础。您可以仅使用这两个组件创建全功能神经网络，就像您在下图中看到的那样。不幸的是，尽管有明显的通用性，但它们有一个相当大的缺点——计算量很大。每个致密层神经元都连接到前一层的每个单元。像这样的密集网络需要大量的可训练参数。这在处理图像时尤其成问题。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ob"><img src="../Images/264e2a7d06cf183c1862ba43b5507b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rnpg5-w2_TWb6Ec50R2sIg.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated"><strong class="bd nc">图10。</strong>密集连接的神经网络</p></figure><p id="abef" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">幸运的是，这种层的实现非常容易。前向传递归结为将输入矩阵乘以权重并添加偏差——一行NumPy代码。权重矩阵的每个值代表图10中可见的网络神经元之间的一个箭头。反向传播稍微复杂一点，但仅仅是因为我们必须计算三个值:<code class="fe oc od oe of b">dA</code> —激活导数、<code class="fe oc od oe of b">dW</code> —权重导数和<code class="fe oc od oe of b">db</code> —偏差导数。正如所承诺的，我不打算在这篇文章中张贴数学公式。最重要的是，计算这些微分非常简单，不会给我们带来任何问题。如果你想挖得更深一点，并且不怕面对线性代数，我鼓励你阅读我的另一篇<a class="ae mk" rel="noopener" target="_blank" href="/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba">文章</a>，在那里我详细解释了密集层向后传递的所有曲折。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi oo"><img src="../Images/6cc02927e4d750911410ea28c14906ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cvNW0xGhXEYIy1SUayXK_Q.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated"><strong class="bd nc">图11。</strong>致密层</p></figure><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="oh le l"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">密集层——点击<a class="ae mk" href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/dense.py" rel="noopener ugc nofollow" target="_blank">此处</a>查看完整代码</p></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><a href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/dense.py"><div class="gh gi oi"><img src="../Images/ca1deb350159fd368752bed23a46dbd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*oEphkyD01bwzCf6YRYK72A.gif"/></div></a></figure><h2 id="9a1c" class="nd ne iq bd nf ng nh dn ni nj nk dp nl lo nm nn no ls np nq nr lw ns nt nu nv bi translated">结论</h2><p id="5871" class="pw-post-body-paragraph lf lg iq lh b li nw jr lk ll nx ju ln lo ny lq lr ls nz lu lv lw oa ly lz ma ij bi mb translated">我希望我的文章拓宽了你的视野，增加了你对神经网络内部发生的数学运算的理解。我承认，通过准备这篇文章中使用的代码、注释和可视化，我学到了很多。如果你有任何问题，欢迎在文章下留言或通过社交媒体联系我。</p><p id="f9a7" class="pw-post-body-paragraph lf lg iq lh b li lj jr lk ll lm ju ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这篇文章是“神经网络的奥秘”系列的另一部分，如果你还没有机会，请考虑阅读<a class="ae mk" rel="noopener" target="_blank" href="/lets-code-a-neural-network-in-plain-numpy-ae7e74410795">其他文章</a>。此外，如果你喜欢我目前的工作，请在<a class="ae mk" href="https://twitter.com/PiotrSkalski92" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae mk" href="https://medium.com/@piotr.skalski92" rel="noopener"> Medium </a>和<a class="ae mk" href="https://www.kaggle.com/skalskip" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上关注我。看看我正在做的其他项目，比如<a class="ae mk" href="https://github.com/SkalskiP/make-sense" rel="noopener ugc nofollow" target="_blank">make sense</a>——小型计算机视觉项目的在线标记工具。最重要的是，保持好奇心！</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi oi"><img src="../Images/6ed1c92106d60e5a0281a501b58d0d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*eJFTnh2YtJD6QjfnifOM4g.gif"/></div></div></figure></div></div>    
</body>
</html>