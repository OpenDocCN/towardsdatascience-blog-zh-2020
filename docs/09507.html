<html>
<head>
<title>5 Must-Know Dimensionality Reduction Techniques via Prince</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过 Prince 的 5 个必须知道的降维技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-must-know-dimensionality-reduction-techniques-via-prince-e6ffb27e55d1?source=collection_archive---------4-----------------------#2020-07-07">https://towardsdatascience.com/5-must-know-dimensionality-reduction-techniques-via-prince-e6ffb27e55d1?source=collection_archive---------4-----------------------#2020-07-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5536" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">减少功能对您的数据科学项目有好处</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e616ec1d0eb9d4457b54b1c7fd231cae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*coFQGc68VvLd415o"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Alexander Schimmeck 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="595a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">答</span>根据<a class="ae ky" href="https://en.wikipedia.org/wiki/Dimensionality_reduction" rel="noopener ugc nofollow" target="_blank">维基百科</a>的说法，降维是将高维空间数据转换到低维空间。</p><p id="cbfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，降维将数据从大量特征转换成少量特征。<strong class="lb iu">我们先说从一百个特性变成两个特性。</strong></p><p id="8548" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们为什么需要降维呢？如果我们的机器学习模型有许多功能，这不是很好吗？嗯，技术上来说是的，但只是到某一点。更多的特性可能会增加你的度量，但它会在某个地方达到峰值，然后下降。</p><p id="1000" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，当我们拥有太多功能时，会出现一些问题，包括:</p><ul class=""><li id="5ba7" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated"><strong class="lb iu">较高数量的特征会增加数据的方差，这可能会导致过度拟合— </strong>尤其是在观察数量少于现有特征数量的情况下。</li><li id="39a6" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><strong class="lb iu">数据<strong class="lb iu">之间的密度</strong>和<strong class="lb iu">距离</strong>变得不那么有意义</strong>，这意味着<strong class="lb iu">数据之间的距离是等距的或者是相同的相似/不同的</strong>。这影响了<strong class="lb iu">聚类</strong>和<strong class="lb iu">异常值检测</strong>，因为数据中的关键信息被低估了。</li><li id="5543" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><strong class="lb iu">组合爆炸</strong>或<strong class="lb iu">大量值</strong>会导致<strong class="lb iu">计算上难以处理的</strong>问题，这个过程需要太长时间才能完成。</li></ul><p id="42ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和其他一些问题，但你明白了。<strong class="lb iu">太多功能没用。这就是为什么我们需要降维技术。</strong></p><p id="9b30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以认为降维技术是建模之前的一个中间步骤；无论你的目标是聚类还是预测。您是否只想看到隐藏的潜在变化或使用结果作为另一个特征是您的自由裁量权。</p><p id="633c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">说到结果，所有这些 <strong class="lb iu">降维技术的<strong class="lb iu">结果都是奇异值分解或者 SVD </strong>。这个术语在这篇文章中可以找到。我不会在这里过多讨论，因为 SVD 只是把矩阵分解作为一个过程来处理，而不是主要的焦点。</strong></p><p id="0025" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">头脑:知识和商业理解永远胜过任何一种技术</strong>。尽管降维的目的是在特征空间中找到相关性，但是具有许多相关特征比变换不那么相关的特征更好。</p><p id="1aae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很少有 Python 包来处理降维问题；一个例子是<a class="ae ky" href="https://github.com/MaxHalford/Prince" rel="noopener ugc nofollow" target="_blank">王子</a>套餐。在本文中，我将概述 prince 包中可用的五种降维技术。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="eed0" class="mz na it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">维度缩减技术</h1><p id="0d25" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">王子包标榜自己是 Python <a class="ae ky" href="https://www.wikiwand.com/en/Factor_analysis" rel="noopener ugc nofollow" target="_blank">因子分析</a>库。虽然不是所有的维度技术都是因子分析方法，但有些是相关的。这就是为什么王子包，包括技术，也与因素分析。</p><p id="41bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">prince 包可用于降维的技术有:</p><ul class=""><li id="4311" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1404.1100.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">主成分分析</strong> (PCA) </a></li><li id="5ff6" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><a class="ae ky" href="http://statmath.wu.ac.at/courses/CAandRelMeth/caipA.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">对应分析</strong> (CA) </a></li><li id="60bf" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><a class="ae ky" href="https://core.ac.uk/download/pdf/6591520.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">多重对应分析</strong> (MCA) </a></li><li id="e93a" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><a class="ae ky" href="https://www.utdallas.edu/~herve/Abdi-MFA2007-pretty.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">多因素分析</strong> (MFA) </a></li><li id="7c37" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><a class="ae ky" href="https://github.com/MaxHalford/Prince#factor-analysis-of-mixed-data-famd" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">混合数据的因子分析</strong> (FAMD) </a></li></ul><p id="4255" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我不会详细解释每种技术的理论，因为我计划创建一篇关于它的更详细的文章。相反，我将给出这些技术的一个简短概述，以及您应该在什么时候将它与示例一起应用。</p><p id="c4d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">何时使用该技术取决于特性，下面是根据 prince 包何时应用该技术的总结表。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/eaff9321dde301c633a9dfa716dcd611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HL1Q0-QjTAcJLJD1_JRqZw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创建的表格</p></figure><p id="bb96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于 prince 的一些注意事项是，prince 包使用了奇异值分解的随机版本(SVD )。这比使用更普遍的完全方法要快得多，但是结果可能包含很小的内在随机性。大多数时候，你不需要担心，但是<strong class="lb iu">如果你想要可重复的结果，那么你应该设置</strong> <code class="fe nx ny nz oa b"><strong class="lb iu">random_state</strong></code> <strong class="lb iu">参数</strong>。</p><p id="ac43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVD 的随机化版本是一种迭代方法，Prince 的每种应用 SVD 的算法都拥有一个<code class="fe nx ny nz oa b">n_iter</code>参数，该参数控制用于计算 SVD 的迭代次数。一般来说，该算法收敛非常快，因此建议<strong class="lb iu">使用较低的</strong> <code class="fe nx ny nz oa b"><strong class="lb iu">n_iter</strong></code> <strong class="lb iu">，尽管较高的数值可以给出更精确的结果，但计算时间较长</strong>。</p><p id="61ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，在我们开始之前，让我们先安装软件包。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="9c51" class="of na it oa b gy og oh l oi oj">pip install prince</span></pre></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h2 id="73fc" class="of na it bd nb ok ol dn nf om on dp nj li oo op nl lm oq or nn lq os ot np ou bi translated">1.主成分分析</h2><p id="8134" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">我认为主成分分析是降维概念中介绍最多和教科书式的模型。PCA 是现代数据分析中的标准工具，因为它是一种简单的非参数方法，用于从混乱的数据集中提取相关信息。</p><p id="4273" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主成分分析旨在减少复杂的信息，并提供隐藏在高维数据下的简化结构。PCA <strong class="lb iu">的主要优势在于计算每个维度对描述数据集可变性的重要性。</strong>例如，六个维度的数据可能在一个维度上存在大部分变异。如果你想了解更多关于 PCA 及其局限性的信息，你可以在这里阅读教程论文。</p><p id="5062" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，我们什么时候用 PCA 来降维呢？根据 prince 准则，当所有尺寸都是数值(全数值特征)时为<strong class="lb iu">。让我们用数据集示例来尝试一下。</strong></p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="5eb9" class="of na it oa b gy og oh l oi oj">#Importing the necessary package</span><span id="34b0" class="of na it oa b gy ov oh l oi oj">import pandas as pd<br/>import seaborn as sns<br/>from prince import PCA</span><span id="340e" class="of na it oa b gy ov oh l oi oj">#Dataset preparation with only numerical features</span><span id="0f43" class="of na it oa b gy ov oh l oi oj">mpg = sns.load_dataset('mpg')</span><span id="c6a3" class="of na it oa b gy ov oh l oi oj">mpg.dropna(inplace=True)<br/>mpg_test = mpg.drop(['name', 'origin'], axis =1, inplace = True)<br/>mpg_test.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/2d825c2e88e43d2a8d2949bb3540b2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*byewyPy7GlW1yhuh0HSYpw.png"/></div></div></figure><p id="8dfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了包含所有数字特征的 mpg 数据集。让我们尝试将主成分分析应用到数据集中，以将特征减少为两个主要成分。请注意，您可以设置与输入到模型中的特征数量一样多的主成分。在这种情况下，它是 7，但这意味着根本没有降维。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="a613" class="of na it oa b gy og oh l oi oj">#Setup our PCA, n_components control the number of the dimension<br/>pca =PCA(n_components = 2, n_iter = 3, random_state = 101)</span><span id="ba44" class="of na it oa b gy ov oh l oi oj">Training and transform our data<br/>pca.fit(mpg_test)<br/>mpg_pca = pca.transform(mpg_test)<br/>mpg_pca.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/89fdd0cf9a8a8414acd38429ef7dfd87.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*6SfuMNQIT9nZWKBw_C7iQQ.png"/></div></figure><p id="facb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样，我们将七维数据集缩减为二维(两个 PC)，但是这个二维解释了原始维度的多少变化？</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="6053" class="of na it oa b gy og oh l oi oj">pca.explained_inertia_</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/2491a0df2d77b98b5bfe10bebbf4ae31.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*bFVBNoThjhxnGAn0pDM7hQ.png"/></div></figure><p id="ba41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">似乎 PC1 解释了大约 71%，PC2 解释了大约 12%，所以我们的二维特征解释了原始维度的大约 83%。那还不算太坏。我们可以试着想象我们的两台电脑以及附加标签。以‘原点’特性为标签来试试吧。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="baba" class="of na it oa b gy og oh l oi oj">ax = pca.plot_row_coordinates(mpg_test, <br/>     color_labels=mpg['origin'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/495ef5bca27c80283a5b4d64b0111a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*KGR6dv2qRw6l5I48mvOdtw.png"/></div></figure><p id="760b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">二维特征有助于区分美国和其他国家，尽管欧洲和日本的标签有点困难。</p><p id="a053" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们试试 prince 提供的另一种方法 PCA。我们可以获得原始变量和主成分之间的相关性。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="8ce4" class="of na it oa b gy og oh l oi oj">pca.column_correlations(mpg_test)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/a0567ca286632c5bb5022e6299aec344.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*10uE4epF6DXLmuMcqnHwow.png"/></div></figure><p id="8417" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不确定是哪种相关方法，但我假设是皮尔逊相关。我们还可以知道每个观察值对每个主成分的贡献有多大。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="d6fd" class="of na it oa b gy og oh l oi oj">pca.row_contributions(mpg_test).head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/aee41dd80ce0292939c88fc0737c5ee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*NlbhVjvmtj0F8kFyMXKi5g.png"/></div></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h2 id="a378" class="of na it bd nb ok ol dn nf om on dp nj li oo op nl lm oq or nn lq os ot np ou bi translated">2.对应分析</h2><p id="966d" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">CA 是一种降维技术，传统上应用于<strong class="lb iu">列联表</strong>。它以与 PCA 相似的方式转换数据，其中心结果是 SVD。需要列联表中数据的 CA 的固有属性意味着<strong class="lb iu">更适合将 CA 应用于分类特征</strong>。让我们用一个数据集示例来尝试一下。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="8d96" class="of na it oa b gy og oh l oi oj">#Creating Contigency tables</span><span id="e0a2" class="of na it oa b gy ov oh l oi oj">flights = sns.load_dataset('flights')<br/>pivot = flight.pivot_table(values = 'passengers', index ='year' ,columns =  'month' )</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/2532f6d4c351cb412bd5d7bf73b8bf6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fr8FQj_heclOMiQy-rhRig.png"/></div></div></figure><p id="0c0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们的飞行数据列联表，我们可以看到这里有如此多的信息。我们将使用 CA 来降低维度，并从中提取额外的洞察力。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="6e37" class="of na it oa b gy og oh l oi oj">#Preparing CA</span><span id="a3fa" class="of na it oa b gy ov oh l oi oj">from prince import CA<br/>ca = CA(n_components=2,n_iter=3,random_state=101 )</span><span id="3e0a" class="of na it oa b gy ov oh l oi oj">#Fitting the data<br/>ca.fit(pivot)</span></pre><p id="f711" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与 PCA 不同，没有数据转换，所以我们需要从类本身访问结果。让我们尝试获取行值(行坐标)。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="c197" class="of na it oa b gy og oh l oi oj">ca.row_coordinates(pivot)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/ca63f7e6974e0f9f9ca43115eb0ce87b.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*DsIdyBux5fPcaIIwceDAPQ.png"/></div></div></figure><p id="c5c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上是列联表相对于行(在本例中是年)的变动信息。如果您想要从列的角度获得信息，我们也可以用下面的代码来完成。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="59f8" class="of na it oa b gy og oh l oi oj">ca.column_coordinates(pivot)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/f3b7619ded2bcd4614ca4818689e0ee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*7UJrJyALWZ0fKkSLyom0Gw.png"/></div></figure><p id="b983" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想画出坐标，你可以用下面的代码。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="ec5e" class="of na it oa b gy og oh l oi oj">ax = ca.plot_coordinates(X = pivot, figsize = (6,6))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/45f925a490e4bb72ccc344d4b82e6cbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*q4cfeKWn6wN67VXMTUAamw.png"/></div></figure><p id="70d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过绘制坐标，我们可以知道数据实际在哪里。我们可以看到数据倾向于聚集在一起。这就是我们如何从列联表中获得隐藏的洞察力。</p><p id="8017" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另外，您可以使用下面的代码访问 CA 解释的变体。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="87d6" class="of na it oa b gy og oh l oi oj">ca.explained_inertia_</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/69a3366200992bab79769577fd6a612c.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*WqIMNXucbU60Dw0SVfjU4Q.png"/></div></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h2 id="f594" class="of na it bd nb ok ol dn nf om on dp nj li oo op nl lm oq or nn lq os ot np ou bi translated">3.多重对应分析</h2><p id="2a89" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated"><strong class="lb iu"> MCA 是 CA 对两个以上分类特征(三个或三个以上)的扩展</strong>。这意味着 MCA 适用于特定的分类特征。MCA 的思想是将 CA 应用到数据集的独热编码版本中。现在让我们尝试使用一个数据集示例。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="17bd" class="of na it oa b gy og oh l oi oj">#Dataset preparation</span><span id="135b" class="of na it oa b gy ov oh l oi oj">tips = sns.load_dataset('tips')<br/>tips.drop(['total_bill', 'tip'], axis =1, inplace = True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/0b01b4a20ce1e43a3ba75063929204fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*Tqf_dYm831UtvUL04iUtGA.png"/></div></figure><p id="e5bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上是仅包含我们将使用的分类特征的数据集(大小特征被视为分类特征)。让我们尝试将 MCA 应用于数据集。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="88b6" class="of na it oa b gy og oh l oi oj">from prince import MCA<br/>mca = MCA(n_components = 2, n_iter = 3, random_state = 101)</span><span id="6c56" class="of na it oa b gy ov oh l oi oj">mca.fit(tips)<br/>tips_mca = mca.transform(tips)<br/>tips_mca.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/0fafeea975ea693c04d9968924ed1d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*gTDNPt623_NszItiXNVDOA.png"/></div></figure><p id="0396" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果类似于 PCA 或 CA 结果，两个主成分以 SVD 结果作为值。就像以前的技术一样，我们可以将坐标绘制成二维图形。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="1071" class="of na it oa b gy og oh l oi oj">mca.plot_coordinates(X = tips)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/e136ef18ef7801ef5d3a22e86846b6a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*iIC8ix2E9TvyJfr2crU2LA.png"/></div></figure><p id="ae8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MCA 使用与 CA 相同的方法来获取它的列和行坐标，上面的图总结了这一切。我们可以看到彩色的是列坐标，灰度的是行坐标。我们可以看到似乎有两种数据聚类，左侧和右侧。</p><p id="8799" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，您可以使用下面的代码访问模型解释的差异。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="e16b" class="of na it oa b gy og oh l oi oj">mca.explained_inertia_</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/8d55a63a00516f6c87439abac4a931af.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*ucmyctmeOlGfYVVf97z7rQ.png"/></div></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h2 id="1659" class="of na it bd nb ok ol dn nf om on dp nj li oo op nl lm oq or nn lq os ot np ou bi translated">4.多因素分析</h2><p id="e495" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">多因素分析(MFA)寻找所有特征中存在的共同结构。<strong class="lb iu">当您有一组数字或分类特征</strong>时，使用 MFA。这是因为 MFA 用于分析由几个组特征描述的一组观察值。MFA 的主要目标是整合描述相同观测值的不同特征组。</p><p id="01b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于数据集示例，我将使用 prince 包中的教程。在教程数据集中，是关于三个专家对六种不同的葡萄酒给出他们的意见。对每种葡萄酒的意见都被记录为一个特征，我们希望在对每种葡萄酒有一个总体了解的同时，考虑每位专家的不同意见。MFA 适用于这种分析。</p><p id="e2ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们试着建立数据。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="b283" class="of na it oa b gy og oh l oi oj">X = pd.DataFrame(<br/>data=[<br/>         [1, 6, 7, 2, 5, 7, 6, 3, 6, 7],<br/>         [5, 3, 2, 4, 4, 4, 2, 4, 4, 3],<br/>         [6, 1, 1, 5, 2, 1, 1, 7, 1, 1],<br/>         [7, 1, 2, 7, 2, 1, 2, 2, 2, 2],<br/>         [2, 5, 4, 3, 5, 6, 5, 2, 6, 6],<br/>            [3, 4, 4, 3, 5, 4, 5, 1, 7, 5]<br/>     ],<br/>     columns=['E1 fruity', 'E1 woody', 'E1 coffee',<br/>              'E2 red fruit', 'E2 roasted', 'E2 vanillin', 'E2 woody',<br/>              'E3 fruity', 'E3 butter', 'E3 woody'],<br/>     index=['Wine {}'.format(i+1) for i in range(6)]<br/>)<br/>X['Oak type'] = [1, 2, 2, 2, 1, 1]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/e2067afbe8501b219a2df739f5a08537.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TEgr5TYXAyyBvwaQp869WQ.png"/></div></div></figure><p id="ed94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上是我们的数据集，对每种葡萄酒的看法，以橡木类型作为额外的分类变量。现在，在 prince 中使用 MFA，我们需要指定组类别。所以，让我们创建一个。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="16ed" class="of na it oa b gy og oh l oi oj">groups = {<br/>  'Expert #{}'.format(no+1): [c for c in X.columns if c.startswith('E{}'.format(no+1))] for no in range(3)}<br/>groups</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/099d5eb20ddeb5ff6e9ff07031733fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*0xt28s4oWN8ePcR9ZKncvQ.png"/></div></figure><p id="cfcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在小组成员在场的情况下，让我们尝试应用 MFA 来降低维度。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="c984" class="of na it oa b gy og oh l oi oj">from prince import MFA<br/>mfa = MFA(groups = groups, n_components = 2, n_iter = 3, random_state = 101)</span><span id="6d56" class="of na it oa b gy ov oh l oi oj">mfa.fit(X)<br/>mfa.transform(X)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/e211555b2d81aeb28073aee59ee64b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*mTkIIqmWq9jV43m8d2jRSg.png"/></div></figure><p id="36f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们获取每种葡萄酒的全局行坐标，在这里我们已经将特征简化为二维。就像之前一样，我们可以尝试绘制行坐标。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="a403" class="of na it oa b gy og oh l oi oj">mfa.plot_row_coordinates(X=X, labels = X.index, color_labels=['Oak type {}'.format(t) for t in X['Oak type']])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/a141b1b5b6b9d5eca9c6b1864ceba4c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*0ZYDrNs3K4j8Jn6qDir1LQ.png"/></div></figure><p id="5275" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到一个清晰的葡萄酒分离和密切的每种葡萄酒的橡木类型作为标签。如果你愿意，我们也可以得到组中每个数据的行坐标。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="a4b5" class="of na it oa b gy og oh l oi oj">mfa.partial_row_coordinates(X)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/7444d3bb23232ea1d2b43f3fc4ba0321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*7kuqRCtFC6XUWuYzYXiu-g.png"/></div></figure><p id="b2d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以画出这些组的行坐标。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="0cf5" class="of na it oa b gy og oh l oi oj">mfa.plot_partial_row_coordinates(X =X , color_labels=['Oak type {}'.format(t) for t in X['Oak type']])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/ff894a7bb9ce83d22e4dcf23fb9314ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*TyU5tAECnp4acqwRp6-8yw.png"/></div></figure><p id="624f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您需要访问模型解释的方差时，您可以像前面的技术一样访问它。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="f74f" class="of na it oa b gy og oh l oi oj">mfa.explained_inertia_</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/a64a9fd9558fde38ae0785dd97ecc65b.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*qMB8IDxhgLwjJZnaF-jCtQ.png"/></div></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h2 id="03a7" class="of na it bd nb ok ol dn nf om on dp nj li oo op nl lm oq or nn lq os ot np ou bi translated"><strong class="ak"> 5。混合数据的因子分析(FAMD) </strong></h2><p id="41c8" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">最后，FAMD 是一种致力于对包含定量和定性特征的数据集进行降维的技术。<strong class="lb iu">这意味着 FAMD 适用于具有分类和数值特征的数据。</strong>通过考虑混合类型的特征来分析观察值之间的相似性是可能的。此外，我们可以探索所有功能之间的关联。</p><p id="ff92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">粗略地说，FAMD 算法可以被看作是 PCA 和 MCA 的混合。</p><p id="80f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用一个数据集示例来尝试一下。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="3713" class="of na it oa b gy og oh l oi oj">#Using the tips dataset, change the size feature to string object<br/>tips = sns.load_dataset('tips')<br/>tips['size'] = tips['size'].astype('object')</span><span id="c577" class="of na it oa b gy ov oh l oi oj">from prince import FAMD<br/>famd = FAMD(n_components =2, n_iter = 3, random_state = 101)</span><span id="a174" class="of na it oa b gy ov oh l oi oj">#I leave out tips as I want the sex feature as the label<br/>famd.fit(tips.drop('sex', axis =1))<br/>famd.transform(tips)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/828e4d0ed9648c614b57bf56abd5ba7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*Huanf4PwrfJxwm_9eGHQZg.png"/></div></figure><p id="d96f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像之前一样，我们通过将维度缩减为二维来获得全局行坐标。我们也可以画出行坐标。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="1ec3" class="of na it oa b gy og oh l oi oj">ax = famd.plot_row_coordinates(tips,color_labels=['Sex {}'.format(t) for t in tips['sex']] )</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/55805b09b56d31d61ef9cdde9b1a223b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*L9Sh7VAeUZRPF85Zjmo4uA.png"/></div></figure><p id="7056" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">FAMD 在他们的分析中使用了 PCA 和 MCA 技术。这就是为什么坐标的构建块由分类特征和数字特征组成。如果您想访问这两个分析结果，我们也可以这样做。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="46c1" class="of na it oa b gy og oh l oi oj">famd.partial_row_coordinates(tips)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/61150e1c4aba8b17579502e6398948aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*d-3wsDbc3NAobrdrRZNBzg.png"/></div></figure><p id="6761" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以画出来。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="69f5" class="of na it oa b gy og oh l oi oj">ax = famd.plot_partial_row_coordinates(tips, color_labels=['Sex {}'.format(t) for t in tips['sex']])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/864ef82cc5a4be1e886e5984681bd95a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*tTjvlURY67J1R6cwLsgTmg.png"/></div></figure><p id="0ffe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，如果你想知道模型解释了多少方差。您也可以用下面的代码来访问它。</p><pre class="kj kk kl km gt ob oa oc od aw oe bi"><span id="855d" class="of na it oa b gy og oh l oi oj">famd.explained_inertia_</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/8b12146598bcf8fcf3d4838ff9c1bb47.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*NS-Ivl-qzIj-8bT2JQnOqw.png"/></div></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="6860" class="mz na it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated"><strong class="ak">结论</strong></h1><p id="256b" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">在本文中，我解释了 prince 包提供的五种不同的降维技术。它包括:</p><ol class=""><li id="ec2e" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu px mk ml mm bi translated">主成分分析</li><li id="da17" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu px mk ml mm bi translated">对应分析</li><li id="6fc6" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu px mk ml mm bi translated">多重对应分析</li><li id="ee54" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu px mk ml mm bi translated">多因素分析</li><li id="4c22" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu px mk ml mm bi translated">FAMD(混合数据的因子分析)</li></ol><p id="74b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">何时使用这些技术取决于你所拥有的特性。</p><p id="b460" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望有帮助！</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="c0b2" class="mz na it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">如果你喜欢我的内容，并想获得更多关于数据或作为数据科学家的日常生活的深入知识，请考虑在这里订阅我的<a class="ae ky" href="https://cornellius.substack.com/welcome" rel="noopener ugc nofollow" target="_blank">时事通讯。</a></h1><blockquote class="py"><p id="e0c3" class="pz qa it bd qb qc qd qe qf qg qh lu dk translated">如果您没有订阅为中等会员，请考虑通过<a class="ae ky" href="https://cornelliusyudhawijaya.medium.com/membership" rel="noopener">我的推荐</a>订阅。</p></blockquote></div></div>    
</body>
</html>