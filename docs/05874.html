<html>
<head>
<title>Reinforcement Learning for everyone</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每个人的强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-for-everyone-9c1163f61440?source=collection_archive---------36-----------------------#2020-05-14">https://towardsdatascience.com/reinforcement-learning-for-everyone-9c1163f61440?source=collection_archive---------36-----------------------#2020-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8d63" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">RL已经在AI社区流行起来，但是大多数人还不知道它是关于什么的。来读吧，不管你是什么背景！</h2></div><p id="dbaa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你在技术领域工作，尤其是在人工智能/机器学习领域，你可能已经习惯了人们不理解你的工作。这就是我的生活，我不得不说我为此感到有点自豪:因为这意味着我学到了许多大多数人不知道的东西，而且我对这个话题足够深入，能够理解对大多数人来说非常复杂的事情。然而，每当你想与你关心的人分享一项成就时，这也有点令人沮丧，因为他们无法理解所有这些模糊之处。</p><p id="1fdb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是我决定写这篇文章的原因，希望我能把我的观点传达给任何对ML特别是强化学习感兴趣的人，不管你的背景如何。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/4aa8e6ecdff9f75ad6bd6261970095ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BrDOaoeWv0mc0qHbFGA8jQ.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">不，机器学习无法像电影中那样建造终结者——然而[图片来自<a class="ae lv" href="https://www.publicdomainpictures.net/en/view-image.php?image=107453&amp;picture=terminator" rel="noopener ugc nofollow" target="_blank"> PublicDomainPictures </a>。]</p></figure><h1 id="b63c" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">巴甫洛夫条件反射</h1><p id="4c17" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">尽管当你想到人工智能时，你会想到未来，但我们的故事始于100多年前，19世纪90年代，在伊凡·巴甫洛夫的实验室里。他在研究狗的唾液分泌，为此，他测量了狗看到食物时唾液分泌的多少，在吃食物之前是T4。他已经对这些实验感到兴奋，但他意识到一些意想不到的事情:狗甚至会在看到任何食物之前<em class="le">分泌唾液。当他们注意到巴甫洛夫的助手向他们走来时，他们会开始垂涎三尺。注意到这一点后，他测试了如果在喂狗之前，他会按铃会发生什么(或者实际上是根据<a class="ae lv" href="https://en.wikipedia.org/wiki/Classical_conditioning" rel="noopener ugc nofollow" target="_blank">维基百科</a>的节拍器)，你可能猜到了:它们也开始垂涎三尺，<strong class="kk iu">因为它们已经知道铃响后，食物就会来了</strong>。这种铃声被称为条件刺激，因为狗不会因为铃声响而流口水，而是因为它知道食物会跟着铃声走。</em></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/14cabf19cd4008b9be38b2515650ddac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/1*Hm_fRS6xVUK8UOP9IOoXOQ.gif"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">最终，巴甫洛夫也受到了狗的影响。]</p></figure><h1 id="520f" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">不错！但是我来这里是为了学习…</h1><p id="e780" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">事实证明，RL是基于心理学的基本原理。在RL中，一个智能体基于一个被称为奖励的“条件刺激”来学习如何行为。RL的设置如下:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mu"><img src="../Images/0f25a41914b2a7e8c8f7cf1d1213cb52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oe1BVErDE6IJRDXQK3IbkA.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">RL的框架及其元素【自制。]</p></figure><p id="5695" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们有一个<strong class="kk iu">代理</strong>，它位于一个<strong class="kk iu">环境</strong>中并与之交互，在这个环境中，它可以执行<strong class="kk iu">动作</strong>，同时观察环境的<strong class="kk iu">状态</strong>并接收对其动作的<strong class="kk iu">奖励</strong>。此外，我们在离散时间中解决我们的问题，所以我们的时间线是由<strong class="kk iu">步骤</strong>构成的；在每一步中，代理观察环境的状态，对环境执行改变环境状态的动作，并接收对其动作的奖励。</p><p id="b2c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使它更简单，让我们考虑一个例子:我们的代理将是一个机器人，我们希望它学会走路，直到一个目标区域，所以当它到达这个区域时，我们会给它一些不错的奖励。为了到达这个目标区域，机器人可以使用不同的动作，例如:右转、左转、前进和后退。机器人将在每一步开始尝试随机的动作组合，直到它到达我们希望它到达的位置。一旦发生这种情况，我们改变它的位置并重新开始，这就像一个“<em class="le">对</em>的游戏”，因为机器人已经实现了它的目标，没有其他动作是可能的。这段时间从机器人处于随机位置开始，直到它到达目标区域，这被称为一个<strong class="kk iu">集</strong>，我们将重复这些集，直到机器人知道它需要做什么才能获得好的奖励。在这之后，机器人会一直做同样的事情:导航到目标区域，因为它知道这样做，它会得到很好的回报。</p><h1 id="906d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">但是，代理<em class="mv">如何学习</em>？</h1><p id="4e59" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">你可能已经朝着正确的方向思考了:<em class="le">数学</em>！代理的行为是由它的<strong class="kk iu">策略</strong>定义的，根据我们使用的方法，它可以用不同的方式来表示:一个表，一个函数，甚至是神经网络。</p><p id="e04a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在RL最基本的情况下，称为表格Q-learning，代理保存一个表，其中每个状态一行，每个动作一列，如图所示。该表告诉代理在给定状态下执行某个操作的预期结果，因此当环境状态发生变化时，代理会检查与给定状态对应的行，并可以选择过去回报最高的操作。每个动作和状态的值称为Q值。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mw"><img src="../Images/f104efcd7bd8c27983ce9bbee4190962.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mGhORBlVLmJf3fwqhb5Xyw.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">使用表格Q-learning时的策略。</p></figure><p id="24e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该表是表格Q-learning中的一个策略。每个Q值被初始化为0，然后在每一步之后用更新规则更新其值，该更新规则基于采取行动之后收到的奖励和新状态的“有多好”。我在这篇文章中跳过了数学，以避免你们中的许多人可能不感兴趣的技术问题，但是如果你想看数学和这些值如何更新的所有细节，你可以在这里看到它们。</p><p id="b59e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代理将重复情节几次，用它的新体验(状态、动作、新状态、奖励)更新它在每个步骤中的策略。一段时间后，代理人将学会一种策略，这种策略会在一集里产生很好的回报，就像一个人将学会如何玩视频游戏以获得高分一样。</p><p id="045d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事实上，视频游戏是尝试RL代理的非常好的环境，这就是为什么它们是RL最常见的用例之一。然而，视频游戏的状态通常被定义为游戏的每一帧，所以我们处理的状态空间太大，无法用表格Q-learning来管理，所以这是使用神经网络而不是Q-table的地方。这就是我们所说的深度RL或者深度Q学习，因为使用了深度神经网络。在接下来的视频中，从两分钟的论文中，你可以看到谷歌的DeepMind的Deep Q-learning的代理在玩雅达利游戏。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="mx my l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">DeepMind的深度Q学习</p></figure><h1 id="f503" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">就是这样！</h1><p id="dc84" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">还不算太糟糕，对吧？我希望您现在对RL是什么以及它是如何工作的有一个(非常)宽泛的概念。如果您仍然对RL感兴趣，并且想要更深入地了解它，那么网上有一些令人惊奇的资料可以帮助您正确地理解它，并在代码中实现它！这些是我最喜欢的资源:</p><ul class=""><li id="a9ec" class="mz na it kk b kl km ko kp kr nb kv nc kz nd ld ne nf ng nh bi translated"><strong class="kk iu">RL圣经:《强化学习:导论》出自萨顿&amp;巴尔托(在亚马逊</strong> <a class="ae lv" href="https://amzn.to/33r3cLM" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">纸面上得到美国</strong></a><strong class="kk iu"/><a class="ae lv" href="https://amzn.to/2RkYI3V" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">英国</strong></a><strong class="kk iu"/><a class="ae lv" href="https://amzn.to/2GMyyor" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">德</strong></a><strong class="kk iu"/><a class="ae lv" href="https://amzn.to/3khE0ON" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">IT</strong></a><strong class="kk iu"/><a class="ae lv" href="https://amzn.to/3miY4lG" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">FR</strong></a>这本神奇的书包含了你需要理解和开始使用RL的所有信息，包括代码示例。</li><li id="760d" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated"><a class="ae lv" href="https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">如何用Python和OpenAI Gym实现RL的教程</strong> </a>。如果你想实现你的第一个RL解决方案，这是一个很好的教程。</li><li id="d424" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated"><strong class="kk iu">你还在怀疑RL是不是适合你的方法吗？我也许能帮你解决问题。</strong></li></ul><p id="01ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的阅读！</p></div></div>    
</body>
</html>