<html>
<head>
<title>Cost functions for Regression and its Optimization Techniques in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中回归的代价函数及其优化技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cost-functions-of-regression-and-its-optimization-techniques-in-machine-learning-2f5931cd33f1?source=collection_archive---------13-----------------------#2020-07-17">https://towardsdatascience.com/cost-functions-of-regression-and-its-optimization-techniques-in-machine-learning-2f5931cd33f1?source=collection_archive---------13-----------------------#2020-07-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0849" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入探讨回归的成本函数及其优化技术:Python 中的漫游</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a107cddc305a2b29fe29c4dee01cf2a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zsPVQNw1ytflVuV7"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">亚历山大·密尔斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="dd09" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">价值函数</h1><p id="7aa6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">成本函数用于衡量机器学习模型的性能。缺乏成本函数的机器学习模型是无用的。成本函数有助于分析机器学习模型的性能。成本函数基本上将预测值与实际值进行比较。成本函数的适当选择有助于模型的可信度和可靠性。</p><h2 id="b2f5" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">损失函数与成本函数</h2><ul class=""><li id="cb4e" class="mz na it lt b lu lv lx ly ma nb me nc mi nd mm ne nf ng nh bi translated">定义在单个数据实例上的函数称为损失函数。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/3ca9e8fd8ee6b7996001022d73d779cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*TtJ9LjicBwJ1ddrMD99eug.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归的绝对损失</p></figure><ul class=""><li id="874f" class="mz na it lt b lu nj lx nk ma nl me nm mi nn mm ne nf ng nh bi translated">在整个数据实例上定义的函数称为成本函数。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/7740917385f0ff66cc721d9f03e9780e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*hJ8y-Z5CJiQUMgE9LG0cag.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归的平均绝对误差</p></figure><h1 id="ac8f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">回归的成本函数</h1><p id="e885" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">回归任务处理连续数据。可用于回归的成本函数有:</p><ul class=""><li id="7f71" class="mz na it lt b lu nj lx nk ma nl me nm mi nn mm ne nf ng nh bi translated">绝对平均误差</li><li id="9f44" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">均方误差</li><li id="4b66" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">均方根误差</li><li id="9fd7" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">均方根对数误差</li></ul><h2 id="f30d" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">绝对平均误差</h2><p id="97cb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">平均绝对误差(MAE) </strong>是实际值和预测值之间的平均绝对差值。</p><ul class=""><li id="8734" class="mz na it lt b lu nj lx nk ma nl me nm mi nn mm ne nf ng nh bi translated">MAE 对异常值更稳健。对异常值不敏感是因为它不会惩罚由异常值引起的高误差。</li><li id="9407" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">MAE 的缺点是它在零点不可微，并且许多损失函数优化算法涉及微分以找到参数的最佳值。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/0e62ec1824d2209b41ae60fe8d3f6c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lp3cA5fYLtMT1ruvr5h3GQ.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="d12d" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">均方误差</h2><p id="fa8a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">均方差(MSE) </strong>是实际值和预测值之间的均方差。MSE 通过平方误差来惩罚由异常值引起的高误差。优化算法受益于惩罚，因为它有助于找到参数的最佳值。</p><ul class=""><li id="33d1" class="mz na it lt b lu nj lx nk ma nl me nm mi nn mm ne nf ng nh bi translated">MSE 的缺点是对异常值非常敏感。当高误差(由目标中的异常值引起)被平方时，它甚至变成更大的误差。</li><li id="a3c4" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">MSE 可用于不希望出现高误差的情况。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/63860b43b10bbeaa75e2d5cc5c87eb92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J0D080xocWZGZ4ADH5NWIQ.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="9f8a" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">均方根误差</h2><p id="3c0e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">均方根误差(RMSE) </strong>是实际值和预测值之差的均方根。RMSE 可以用在我们想要惩罚高错误的情况下，但没有 MSE 那么多。</p><ul class=""><li id="c6f0" class="mz na it lt b lu nj lx nk ma nl me nm mi nn mm ne nf ng nh bi translated">RMSE 对异常值也非常敏感。RMSE 中的平方根确保误差项被罚，但不像 MSE 那么多。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/13afe7f49d824ab14574673411b6f5c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c61g9CvHR4xYO1tbNBScAw.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="68f3" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">均方根对数误差</h2><p id="125f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">均方根对数误差(RMSLE) </strong>非常类似于 RMSE，但是在计算实际值和预测值之间的差异之前应用对数。大误差和小误差被同等对待。RMSLE 可用于目标未被标准化或缩放的情况。</p><ul class=""><li id="8278" class="mz na it lt b lu nj lx nk ma nl me nm mi nn mm ne nf ng nh bi translated">与 RMSE 相比，RMSLE 对异常值不太敏感。它减轻了由于日志的存在而导致的高错误的惩罚。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/158a8952a63f300d709c2d2e01aacfd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GyLbnf_moZPHsyToi5hNjQ.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h1 id="7ce6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">成本函数优化算法</h1><p id="50e1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">成本函数优化算法试图通过找到成本函数的全局最小值来找到模型参数的最优值。可用的各种算法有，</p><ul class=""><li id="af07" class="mz na it lt b lu nj lx nk ma nl me nm mi nn mm ne nf ng nh bi translated">梯度下降</li><li id="aea8" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">RMS Prop</li><li id="440b" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</li></ul><h2 id="c131" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">加载预处理的数据</h2><p id="7431" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">你输入给人工神经网络的数据必须经过彻底的预处理，以产生可靠的结果。训练数据已经过预处理。所涉及的预处理步骤是，</p><ul class=""><li id="8d8c" class="mz na it lt b lu nj lx nk ma nl me nm mi nn mm ne nf ng nh bi translated">老鼠归罪</li><li id="4a96" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">对数变换</li><li id="6be4" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">平方根变换</li><li id="0fcf" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">顺序编码</li><li id="3ea1" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">目标编码</li><li id="44a0" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">z 分数标准化</li></ul><blockquote class="oa ob oc"><p id="3cee" class="lr ls od lt b lu nj ju lw lx nk jx lz oe of mc md og oh mg mh oi oj mk ml mm im bi translated">有关上述步骤的详细实现，请参考我的 Kaggle 笔记本上的数据预处理。<a class="ae ky" href="https://www.kaggle.com/srivignesh/data-preprocessing-for-house-price-prediction" rel="noopener ugc nofollow" target="_blank">笔记本链接</a></p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="5f3a" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">用人工神经网络训练模型</h2><p id="afec" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">更多细节请参考我的 Kaggle 笔记本上<a class="ae ky" href="https://www.kaggle.com/srivignesh/introduction-to-ann-in-tensorflow" rel="noopener ugc nofollow" target="_blank">关于 Tensorflow </a>中的 ANN 的介绍。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="4590" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">梯度下降</h2><p id="cbe6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">梯度下降算法利用成本函数的梯度来寻找参数的最佳值。梯度下降是一种迭代算法。它试图找到一个全局最小值。</p><p id="f310" class="pw-post-body-paragraph lr ls it lt b lu nj ju lw lx nk jx lz ma of mc md me oh mg mh mi oj mk ml mm im bi translated">在每次迭代测试中，</p><ul class=""><li id="a84e" class="mz na it lt b lu nj lx nk ma nl me nm mi nn mm ne nf ng nh bi translated">找到数据的成本。</li><li id="1c10" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">计算成本函数相对于权重和偏差的偏导数。</li><li id="6f96" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">然后通过利用成本函数和学习率𝛼.的梯度来更新权重和偏差𝛼的值可以从 0.0 到 1.0。𝛼的值越大，找到成本函数的全局最小值所采取的步骤就越多。</li><li id="a07f" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">继续上述步骤，直到完成指定次数的迭代或达到全局最小值。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/6a7a6783825c0d66b6230704dc14656d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_vY-BtfUK4wTnt_avlUKA.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="45e6" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">均方根误差</h2><p id="5f5c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">RMS Prop 是一种非常类似于梯度下降的优化算法，但是梯度被平滑和平方，然后被更新以很快获得成本函数的全局最小值。</p><p id="2fd5" class="pw-post-body-paragraph lr ls it lt b lu nj ju lw lx nk jx lz ma of mc md me oh mg mh mi oj mk ml mm im bi translated">在每次迭代测试中，</p><ul class=""><li id="309f" class="mz na it lt b lu nj lx nk ma nl me nm mi nn mm ne nf ng nh bi translated">找到数据的成本。</li><li id="1017" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">计算成本函数相对于权重和偏差的偏导数。</li><li id="d9f0" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">权重和偏差参数被平滑，然后通过利用成本函数和𝛼(学习率)的梯度来更新。</li><li id="b02e" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">继续上述步骤，直到完成指定次数的迭代或达到全局最小值。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/e41f26982314e61e8a009690100c3754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1UL3xGPa2Jfymy0-nFAdJA.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="b531" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">Adam(自适应矩估计)</h2><p id="875d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Adam(自适应矩估计)是通过将梯度下降与动量和 RMS Prop 相结合而出现的算法。</p><p id="ff34" class="pw-post-body-paragraph lr ls it lt b lu nj ju lw lx nk jx lz ma of mc md me oh mg mh mi oj mk ml mm im bi translated">在每次迭代测试中，</p><ul class=""><li id="ee21" class="mz na it lt b lu nj lx nk ma nl me nm mi nn mm ne nf ng nh bi translated">找到数据的成本。</li><li id="82bf" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">计算成本函数相对于权重和偏差的偏导数。</li><li id="95c4" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">使用 RMS Prop 和带动量的梯度下降中使用的技术来平滑权重和偏差，然后通过利用成本函数和𝛼(学习率)的梯度来更新权重和偏差。</li><li id="a579" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">继续上述步骤，直到完成指定次数的迭代或达到全局最小值。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/2fe49e0fa65d42bd22799269f58e75eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SbfKEgz_GQGNCW0Wx00K0Q.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/06ad2d683dc662d845b60275eb3c8182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r7wZVNd_KY1YBWk4kgPm0A.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h1 id="3e7e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><ul class=""><li id="c229" class="mz na it lt b lu lv lx ly ma nb me nc mi nd mm ne nf ng nh bi translated">平均绝对误差对异常值是稳健的，而均方误差对异常值是敏感的</li><li id="c253" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">梯度下降算法试图找到参数的最佳值，从而找到成本函数的全局最小值。</li><li id="ba32" class="mz na it lt b lu np lx nq ma nr me ns mi nt mm ne nf ng nh bi translated">RMS Prop 和 Adam 等算法可被视为梯度下降算法的变体。</li></ul></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><p id="fb8c" class="pw-post-body-paragraph lr ls it lt b lu nj ju lw lx nk jx lz ma of mc md me oh mg mh mi oj mk ml mm im bi translated"><strong class="lt iu">在我的 Kaggle 笔记本里找到这个帖子:</strong><a class="ae ky" href="https://www.kaggle.com/srivignesh/cost-functions-of-regression-its-optimizations" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/srivignesh/cost-functions-of-regression-its-optimizations</a></p><p id="207e" class="pw-post-body-paragraph lr ls it lt b lu nj ju lw lx nk jx lz ma of mc md me oh mg mh mi oj mk ml mm im bi translated"><strong class="lt iu">参考文献:</strong></p><p id="477d" class="pw-post-body-paragraph lr ls it lt b lu nj ju lw lx nk jx lz ma of mc md me oh mg mh mi oj mk ml mm im bi translated">[1]吴恩达，<a class="ae ky" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习专精</a>。</p></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><p id="3ac5" class="pw-post-body-paragraph lr ls it lt b lu nj ju lw lx nk jx lz ma of mc md me oh mg mh mi oj mk ml mm im bi translated"><em class="od">在</em><a class="ae ky" href="https://www.linkedin.com/in/srivignesh-rajan-123569151/" rel="noopener ugc nofollow" target="_blank"><em class="od">LinkedIn</em></a><em class="od">，</em><a class="ae ky" href="https://twitter.com/RajanSrivignesh" rel="noopener ugc nofollow" target="_blank"><em class="od">Twitter</em></a><em class="od">上联系我！</em></p><p id="1ada" class="pw-post-body-paragraph lr ls it lt b lu nj ju lw lx nk jx lz ma of mc md me oh mg mh mi oj mk ml mm im bi translated"><strong class="lt iu">快乐的机器学习！</strong></p><h2 id="23c6" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">谢谢你！</h2></div></div>    
</body>
</html>