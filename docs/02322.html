<html>
<head>
<title>Building a ResNet in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在喀拉斯建立一个 ResNet</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba?source=collection_archive---------3-----------------------#2020-03-05">https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba?source=collection_archive---------3-----------------------#2020-03-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5d03" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用 Keras 函数 API 构造残差神经网络</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3048680a82c58944d10872bbd3027553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cIKFeG7ZIl9D-VnSF0KAZA.png"/></div></div></figure><h1 id="623a" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">什么是残差神经网络？</h1><p id="d1a3" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">原则上，神经网络应该得到更好的结果，因为它们有更多的层。一个更深层次的网络可以学到比它更浅层次的网络所能学到的任何东西，而且(可能)不止这些。对于给定的数据集，如果网络无法通过向其添加更多图层来了解更多信息，那么它只需了解这些附加图层的身份映射。这样，它保留了前几层的信息，不会比更浅的层差。如果找不到比这更好的东西，网络至少应该能够学习身份映射。</p><p id="543a" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">但实际上，事情并不是这样的。更深的网络更难优化。网络每增加一层，训练的难度就会增加；我们用来寻找正确参数的优化算法变得更加困难。随着我们添加更多的层，网络得到更好的结果，直到某一点；然后，随着我们继续添加额外的层，精度开始下降。</p><p id="b9c6" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">残余网络试图通过增加所谓的<em class="mn">跳跃连接</em>来解决这个问题。上图中描绘了一个跳过连接。正如我之前所说，更深层次的网络至少应该能够学习身份映射；这就是 skip 连接所做的:它们添加从网络中的一个点到一个转发点的身份映射，然后让网络学习额外的𝐹(𝑥).如果网络没有更多可以学习的东西，那么它就将𝐹(𝑥学习为 0。原来网络比身份映射更容易学习到更接近 0 的映射。</p><p id="6aae" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">如上图所示的具有跳跃连接的块被称为<em class="mn">残差块</em>，残差神经网络(ResNet)就是这些块的串联。</p><p id="9c33" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">一个有趣的事实是，我们的大脑具有类似于残余网络的结构，例如，皮层第六层神经元从第一层获得输入，跳过中间层。</p><h1 id="b2bb" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">Keras 函数式 API 简介</h1><p id="477e" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">如果你正在阅读这篇文章，你可能已经熟悉了顺序类，它允许你通过一层一层地堆叠来轻松地构建神经网络，就像这样:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="7d14" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu">from</strong> keras.models <strong class="mp iu">import</strong> Sequential<br/><strong class="mp iu">from</strong> keras.layers <strong class="mp iu">import</strong> Dense, Activation<br/><br/>model <strong class="mp iu">=</strong> Sequential([<br/>    Dense(32, input_shape<strong class="mp iu">=</strong>(784,)),<br/>    Activation('relu'),<br/>    Dense(10),<br/>    Activation('softmax'),<br/>])</span></pre><p id="45f5" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">但是这种建立神经网络的方式不足以满足我们的需求。对于顺序类，我们不能添加跳过连接。Keras 也有 Model 类，它可以和 functional API 一起使用来创建层，以构建更复杂的网络架构。<br/>构造时，类<code class="fe my mz na mp b">keras.layers.Input</code>返回一个张量对象。Keras 中的层对象也可以像函数一样使用，用张量对象作为参数调用它。返回的对象是一个张量，然后可以作为输入传递给另一个层，依此类推。</p><p id="0531" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">举个例子:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="76dd" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu">from</strong> keras.layers <strong class="mp iu">import</strong> Input, Dense<br/><strong class="mp iu">from</strong> keras.models <strong class="mp iu">import</strong> Model<br/><br/>inputs <strong class="mp iu">=</strong> Input(shape<strong class="mp iu">=</strong>(784,))<br/>output_1 <strong class="mp iu">=</strong> Dense(64, activation<strong class="mp iu">=</strong>'relu')(inputs)<br/>output_2 <strong class="mp iu">=</strong> Dense(64, activation<strong class="mp iu">=</strong>'relu')(output_1)<br/>predictions <strong class="mp iu">=</strong> Dense(10, activation<strong class="mp iu">=</strong>'softmax')(output_2)<br/><br/>model <strong class="mp iu">=</strong> Model(inputs<strong class="mp iu">=</strong>inputs, outputs<strong class="mp iu">=</strong>predictions)<br/>model.compile(optimizer<strong class="mp iu">=</strong>'adam',<br/>              loss<strong class="mp iu">=</strong>'categorical_crossentropy',<br/>              metrics<strong class="mp iu">=</strong>['accuracy'])<br/>model.fit(data, labels)</span></pre><p id="a93b" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">但是上面的代码仍然构建了一个有序的网络，所以到目前为止还没有真正使用这种奇特的函数语法。这种语法的真正用途是在使用所谓的<em class="mn">合并层</em>时，通过它可以合并更多的输入张量。这些层的几个例子是:<code class="fe my mz na mp b">Add</code>、<code class="fe my mz na mp b">Subtract</code>、<code class="fe my mz na mp b">Multiply</code>、<code class="fe my mz na mp b">Average</code>。我们在建造剩余的积木时需要的是<code class="fe my mz na mp b">Add</code>。</p><p id="ce0f" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">使用<code class="fe my mz na mp b">Add</code>的例子:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="8632" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu">from</strong> keras.layers <strong class="mp iu">import</strong> Input, Dense, Add<br/><strong class="mp iu">from</strong> keras.models <strong class="mp iu">import</strong> Model<br/><br/>input1 <strong class="mp iu">=</strong> Input(shape<strong class="mp iu">=</strong>(16,))<br/>x1 <strong class="mp iu">=</strong> Dense(8, activation<strong class="mp iu">=</strong>'relu')(input1)<br/>input2 <strong class="mp iu">=</strong> Input(shape<strong class="mp iu">=</strong>(32,))<br/>x2 <strong class="mp iu">=</strong> Dense(8, activation<strong class="mp iu">=</strong>'relu')(input2)<br/><br/>added <strong class="mp iu">=</strong> Add()([x1, x2])<br/><br/>out <strong class="mp iu">=</strong> Dense(4)(added)<br/>model <strong class="mp iu">=</strong> Model(inputs<strong class="mp iu">=</strong>[input1, input2], outputs<strong class="mp iu">=</strong>out)</span></pre><p id="c214" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">这绝不是对 Keras functional API 的全面指导。如果你想了解更多，请参考<a class="ae nb" href="https://keras.io/getting-started/functional-api-guide/" rel="noopener ugc nofollow" target="_blank">文件</a>。</p><h1 id="3c06" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">让我们实现一个 ResNet</h1><p id="9231" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">接下来，为了进行比较，我们将实现一个 ResNet 和它的普通(没有跳过连接)对应物。</p><p id="2371" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们将在这里构建的 ResNet 具有以下结构:</p><ul class=""><li id="a463" class="nc nd it lo b lp mi ls mj lv ne lz nf md ng mh nh ni nj nk bi translated">形状输入(32，32，3)</li><li id="265b" class="nc nd it lo b lp nl ls nm lv nn lz no md np mh nh ni nj nk bi translated">1 个 Conv2D 层，带 64 个滤波器</li><li id="d0da" class="nc nd it lo b lp nl ls nm lv nn lz no md np mh nh ni nj nk bi translated">具有 64、128、256 和 512 个滤波器的 2、5、5、2 个残差块</li><li id="5d1c" class="nc nd it lo b lp nl ls nm lv nn lz no md np mh nh ni nj nk bi translated">平均池 2D 层，池大小= 4</li><li id="acf6" class="nc nd it lo b lp nl ls nm lv nn lz no md np mh nh ni nj nk bi translated">展平图层</li><li id="0c08" class="nc nd it lo b lp nl ls nm lv nn lz no md np mh nh ni nj nk bi translated">具有 10 个输出节点的密集层</li></ul><p id="dbfb" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">它总共有 30 个 conv+致密层。所有内核大小都是 3x3。我们在 conv 层之后使用 ReLU 激活和批处理规范化。<br/>除了跳过连接外，普通版本是相同的。</p><p id="7640" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们首先创建一个辅助函数，它将一个张量作为输入，并向其添加 relu 和批量归一化:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="e6e7" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu">def</strong> relu_bn(inputs: Tensor) <strong class="mp iu">-&gt;</strong> Tensor:<br/>    relu <strong class="mp iu">=</strong> ReLU()(inputs)<br/>    bn <strong class="mp iu">=</strong> BatchNormalization()(relu)<br/>    <strong class="mp iu">return</strong> bn</span></pre><p id="182b" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">然后我们创建一个函数来构造一个残差块。它接受一个张量<code class="fe my mz na mp b">x</code>作为输入，并通过 2 个 conv 层；让我们把这两个 conv 层的输出称为<code class="fe my mz na mp b">y</code>。然后把输入的<code class="fe my mz na mp b">x</code>加到<code class="fe my mz na mp b">y</code>，加上 relu 和批量归一化，然后返回结果张量。当参数为<code class="fe my mz na mp b">downsample == True</code>时，第一个 conv 层使用<code class="fe my mz na mp b">strides=2</code>将输出大小减半，我们使用输入为<code class="fe my mz na mp b">x</code>的<code class="fe my mz na mp b">kernel_size=1</code>的 conv 层，使其形状与<code class="fe my mz na mp b">y</code>相同。<code class="fe my mz na mp b">Add</code>层要求输入张量具有相同的形状。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="be57" class="mt kv it mp b gy mu mv l mw mx"><strong class="mp iu">def</strong> residual_block(x: Tensor, downsample: bool, filters: int,                                        kernel_size: int <strong class="mp iu">=</strong> 3) <strong class="mp iu">-&gt;</strong> Tensor:<br/>    y <strong class="mp iu">=</strong> Conv2D(kernel_size<strong class="mp iu">=</strong>kernel_size,<br/>               strides<strong class="mp iu">=</strong> (1 <strong class="mp iu">if</strong> <strong class="mp iu">not</strong> downsample <strong class="mp iu">else</strong> 2),<br/>               filters<strong class="mp iu">=</strong>filters,<br/>               padding<strong class="mp iu">=</strong>"same")(x)<br/>    y <strong class="mp iu">=</strong> relu_bn(y)<br/>    y <strong class="mp iu">=</strong> Conv2D(kernel_size<strong class="mp iu">=</strong>kernel_size,<br/>               strides<strong class="mp iu">=</strong>1,<br/>               filters<strong class="mp iu">=</strong>filters,<br/>               padding<strong class="mp iu">=</strong>"same")(y)<br/><br/>    <strong class="mp iu">if</strong> downsample:<br/>        x <strong class="mp iu">=</strong> Conv2D(kernel_size<strong class="mp iu">=</strong>1,<br/>                   strides<strong class="mp iu">=</strong>2,<br/>                   filters<strong class="mp iu">=</strong>filters,<br/>                   padding<strong class="mp iu">=</strong>"same")(x)<br/>    out <strong class="mp iu">=</strong> Add()([x, y])<br/>    out <strong class="mp iu">=</strong> relu_bn(out)<br/>    <strong class="mp iu">return</strong> out</span></pre><p id="e28c" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated"><code class="fe my mz na mp b">create_res_net()</code>函数把所有东西放在一起。以下是这方面的完整代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="5545" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">简单网络以类似的方式构建，但是它没有跳跃连接，我们也不使用<code class="fe my mz na mp b">residual_block()</code>助手函数；一切都在<code class="fe my mz na mp b">create_plain_net()</code>里面完成。<br/>平原网络的代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><h1 id="9701" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">在 CIFAR-10 上进行培训并查看结果</h1><p id="c984" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">CIFAR-10 是一个包含 10 个类别的 32x32 rgb 图像的数据集。它包含 50k 训练图像和 10k 测试图像。<br/>以下是每个班级 10 张随机图片的样本:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/223c59f58aca94b3e085833517ec78b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_0K4gcB8-J8oXyPoUwqJpA.png"/></div></div></figure><p id="6a68" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们将在这个数据集上训练 ResNet 和 plain net 20 个时期，然后比较结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="c9d0" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">在配有 1 个 NVIDIA Tesla K80 的机器上，每个 ResNet 和 PlainNet 的训练时间约为 55 分钟。ResNet 和 PlainNet 在训练时间上没有显著差异。<br/>我们得到的结果如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/e939239277460e4ced03cbab8b6c0f2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KMHpKJS4Y85wiDUXr1EQXw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/19562f76d38f8dbcd1e5624e0f748675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*ldP-CKf9ZvjhveDW0AUchw.png"/></div></figure><p id="8049" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">因此，通过在该数据集上使用 ResNet，我们在验证准确性方面获得了<strong class="lo iu"> 1.59% </strong>的提升。在更深的网络中，差异应该更大。请随意尝试，看看你得到的结果。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h1 id="cb42" class="ku kv it bd kw kx oc kz la lb od ld le jz oe ka lg kc of kd li kf og kg lk ll bi translated">参考</h1><ol class=""><li id="f120" class="nc nd it lo b lp lq ls lt lv oh lz oi md oj mh ok ni nj nk bi translated"><a class="ae nb" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a></li><li id="4f3e" class="nc nd it lo b lp nl ls nm lv nn lz no md np mh ok ni nj nk bi translated"><a class="ae nb" href="https://en.wikipedia.org/wiki/Residual_neural_network" rel="noopener ugc nofollow" target="_blank">残差神经网络—维基百科</a></li><li id="c0e2" class="nc nd it lo b lp nl ls nm lv nn lz no md np mh ok ni nj nk bi translated"><a class="ae nb" href="https://keras.io/getting-started/functional-api-guide/" rel="noopener ugc nofollow" target="_blank">功能 API 指南— Keras 文档</a></li><li id="d3ac" class="nc nd it lo b lp nl ls nm lv nn lz no md np mh ok ni nj nk bi translated"><a class="ae nb" href="https://keras.io/models/model/" rel="noopener ugc nofollow" target="_blank">模型(功能 API) — Keras 文档</a></li><li id="d7e2" class="nc nd it lo b lp nl ls nm lv nn lz no md np mh ok ni nj nk bi translated"><a class="ae nb" href="https://keras.io/layers/merge/" rel="noopener ugc nofollow" target="_blank">合并层— Keras 文档</a></li><li id="64e7" class="nc nd it lo b lp nl ls nm lv nn lz no md np mh ok ni nj nk bi translated"><a class="ae nb" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 和 CIFAR-100 数据集</a></li></ol><p id="7ead" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我希望这些信息对你有用，感谢你的阅读！</p><p id="5a86" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">这篇文章也贴在我自己的网站<a class="ae nb" href="https://www.nablasquared.com/building-a-resnet-in-keras/" rel="noopener ugc nofollow" target="_blank">这里</a>。随便看看吧！</p></div></div>    
</body>
</html>