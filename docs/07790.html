<html>
<head>
<title>Cross-Attention is what you need!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">交叉注意力才是你需要的！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491?source=collection_archive---------13-----------------------#2020-06-10">https://towardsdatascience.com/cross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491?source=collection_archive---------13-----------------------#2020-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="cd89" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="http://cvpr2020.thecvf.com/" rel="noopener ugc nofollow" target="_blank">新成绩</a> / CVPR 2020</h2><div class=""/><div class=""><h2 id="17f6" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">FusAtNet:用于高光谱和激光雷达分类的基于双注意的光谱空间多模态融合网络</h2></div><blockquote class="kr ks kt"><p id="ba51" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">FusAtNet:用于高光谱和激光雷达分类的基于双注意的光谱空间多模态融合网络</p><p id="fa24" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx jd"> <em class="it">萨蒂扬·莫拉</em>等。al，</strong>IEEE/CVF计算机视觉和模式识别会议(CVPR)研讨会，2020年，第92–93页</p></blockquote></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="b072" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">如今，随着传感技术的进步，多模态数据正变得易于用于各种应用，特别是在遥感(RS)中，其中许多数据类型如多光谱(MSI)、超光谱(HSI)、激光雷达等。都是可用的。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/5c94dc0c1e1eeae13083c1d21ab2f6c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G05aJkxufRWGHbKqVO1dbw.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">今天，多模态数据很容易获得！</p></figure><p id="70a6" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">这些多源数据集的有效融合变得越来越重要，因为这些多模态特征已被证明能够生成高度精确的土地覆盖图。然而，考虑到数据中涉及的冗余和多种模态之间的大范围差异，RS环境下的融合并不是微不足道的。此外，不同模态的特征提取模块之间很难交互，这进一步限制了它们的语义相关性。</p><blockquote class="kr ks kt"><p id="3964" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为什么<strong class="kx jd">单一融合表示很重要？</strong></p></blockquote><p id="3ca5" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">组合多模态图像的几个优点包括:</p><ol class=""><li id="1615" class="mr ms it kx b ky kz lb lc ly mt lz mu ma mv lq mw mx my mz bi translated">生成丰富、融合的表示有助于选择任务相关的特征</li><li id="3677" class="mr ms it kx b ky na lb nb ly nc lz nd ma ne lq mw mx my mz bi translated">改进分类，提高可信度，减少歧义</li><li id="f774" class="mr ms it kx b ky na lb nb ly nc lz nd ma ne lq mw mx my mz bi translated">补充缺失或有噪声的数据</li><li id="600c" class="mr ms it kx b ky na lb nb ly nc lz nd ma ne lq mw mx my mz bi translated">减少数据大小</li></ol><p id="3c62" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">有趣的是，今天大多数常见的方法往往只是使用早期连接、CNN提取的特征级连接或多流决策级融合方法等方法，完全忽略了跨域特征。视觉<strong class="kx jd">一个<em class="kw">注意</em> </strong> <em class="kw">，</em>深度学习研究人员的工具箱中最近增加的一个是 <strong class="kx jd"> <em class="kw">在多模态领域中基本上未被探索。</em>T19】</strong></p><blockquote class="kr ks kt"><p id="22d4" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">一个问题出现了:<strong class="kx jd">如何最好地融合这些模态，形成一个联合的、丰富的表示，可以用于下游的任务？</strong></p></blockquote><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/e9e63703008a7e80d88b1915ded2bcb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lKaprmQaD-60ZEVeNj4nwg.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">基于多模态融合的分类任务的一般示意图。目标是有效地组合两种模态(这里是HSI和LiDAR ),使得结果表示具有丰富的、融合的特征，这些特征对于精确分类来说是足够相关和鲁棒的。</p></figure><p id="1fd1" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">一个理想的融合方法是将两种模态协同地结合起来，并确保结果反映输入模态的显著特征。</p><h1 id="39ed" class="nf ng it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">一个新概念:交叉注意</h1><p id="2f99" class="pw-post-body-paragraph ku kv it kx b ky nx kd la lb ny kg ld ly nz lg lh lz oa lk ll ma ob lo lp lq im bi translated">在这项工作中，我们提出了“交叉注意”的新概念，并在土地覆盖分类的背景下提出了基于注意的HSI-LiDAR融合。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi oc"><img src="../Images/d31c4a0a8f27a16e084596358990a325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L4FX9C9xTSFcn9q9nuUqRw.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">多通道融合中的自我注意与交叉注意。自我注意模块(左)仅在单一通道上工作，其中隐藏表征和注意屏蔽都来自同一通道(HSIs)。另一方面，在交叉注意模块(右)中，注意掩模来自不同的模态(LiDAR ),并且被利用来增强来自第一模态的潜在特征</p></figure><p id="64b5" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">交叉注意是一种新颖且直观的融合方法，其中来自一种模态(此处为LiDAR)的注意掩模被用于突出显示另一种模态(此处为HSI)中提取的特征。注意，这不同于自我注意，在自我注意中，来自HSI的注意屏蔽被用来突出它自己的光谱特征。</p><h2 id="11da" class="od ng it bd nh oe of dn nl og oh dp np ly oi oj nr lz ok ol nt ma om on nv iz bi translated"><strong class="ak"> FusAtNet:在实践中使用交叉注意</strong></h2><p id="3c31" class="pw-post-body-paragraph ku kv it kx b ky nx kd la lb ny kg ld ly nz lg lh lz oa lk ll ma ob lo lp lq im bi translated">本文提出了一个用于HSIs和LiDAR数据的集体土地覆盖分类的特征融合和提取框架FusAtNet。所提出的框架有效地利用了HSI通道，使用“自我注意”机制来生成强调其自身光谱特征的注意图。类似地，同时使用“交叉注意”方法来利用激光雷达导出的注意图，该注意图强调HSI的空间特征。然后，这些注意的光谱和空间表示与原始数据一起被进一步探索，以获得特定于模态的特征嵌入。由此获得的面向模态的联合光谱-空间信息随后被用于执行土地覆盖分类任务。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi oo"><img src="../Images/0caa9bf534d672d81fac11eeadbc25b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jap_bqVoPzvb4VDe_K2Ysg.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><strong class="bd op">fusat net示意图(呈现在休斯顿数据集上)。</strong>首先，高光谱训练样本XH被送到特征提取器FHS获取潜在表征，并送到光谱注意模块生成光谱注意掩模。同时，相应的激光雷达训练样本XL被发送到空间注意模块AT以获得空间注意掩模。注意屏蔽被单独乘以潜在HSI表示以获得MS和MT。MS和MT然后与XH和XL连接并被发送到模态特征提取器FM和模态注意模块AM。然后将两者的输出相乘得到FSS，然后将其发送到分类模块C进行像素分类。</p></figure><h1 id="0854" class="nf ng it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">结果</h1><p id="a128" class="pw-post-body-paragraph ku kv it kx b ky nx kd la lb ny kg ld ly nz lg lh lz oa lk ll ma ob lo lp lq im bi translated">在三个HSI-LiDAR数据集上的实验评估表明，所提出的方法达到了最先进的分类性能，包括在现有最大的HSI-LiDAR基准数据集Houston上，为多模态特征融合分类开辟了新的途径。</p><blockquote class="kr ks kt"><p id="f635" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx jd">休斯顿</strong></p></blockquote><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi oq"><img src="../Images/307d71a5a892c8065953a614f9efa1d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jbiHCWouAN3xvhSE71Komg.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><strong class="bd op">带有分类地图的休斯顿超光谱和激光雷达数据集。</strong>(a)HSI的真彩色合成，(b)激光雷达图像，(c)地面实况。(d) SVM (H)，(e) SVM (H+L)，(f)两个分支的CNN (H)，(g)两个分支的CNN (H+L)，(h) FusAtNet (H)，(i) FusAtNet (H+L)</p></figure><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi or"><img src="../Images/75f757d0ab0b25e5e9ccd88ca32ae98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wtHr8NLD2h8hljtkrU6ORA.jpeg"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">休斯顿数据集上的精度分析(单位为%)。‘H’仅代表HSI，而‘H+L’代表融合的HSI和激光雷达</p></figure><blockquote class="kr ks kt"><p id="718d" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx jd">特伦托</strong></p></blockquote><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi os"><img src="../Images/42eb8859e4a9fdc7eab167c035f1558d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGQCVoqf3az5w5_X1IeFQA.jpeg"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><strong class="bd op">带有分类地图的Trento高光谱和激光雷达数据集。</strong>(a)HSI的真彩色合成，(b)激光雷达图像，(c)地面实况。(d) SVM (H)，(e) SVM (H+L)，(f)两个分支的CNN (H)，(g)两个分支的CNN (H+L)，(h) FusAtNet (H)，(i) FusAtNet (H+L)</p></figure><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ot"><img src="../Images/53232c0752b1d979c33054aa5cbce486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HMCeVMsk0F8Ln0Y_EiP7FA.jpeg"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><strong class="bd op">Trento数据集上的精度分析(以百分比表示)。</strong>‘H’仅代表HSI，而‘H+L’代表融合的HSI和激光雷达</p></figure><blockquote class="kr ks kt"><p id="9bcd" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx jd"> MUUFL </strong></p></blockquote><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ou"><img src="../Images/58278aec2fc235427ce4d996edef3448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9NqK8dEoVmcZaoJJ0MGmw.jpeg"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><strong class="bd op">带有分类地图的MUUFL高光谱和激光雷达数据集。</strong>(a)HSI的真彩色合成，(b)激光雷达图像，(c)地面实况。(d) SVM (H)，(e) SVM (H+L)，(f)两个分支的CNN (H)，(g)两个分支的CNN (H+L)，(h) FusAtNet (H)，(i) FusAtNet (H+L)</p></figure><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ov"><img src="../Images/25c9ce3a84fa0c85dd9d070a5c5b4a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5YiV_oQ5iYN21sNdpRSaIA.jpeg"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><strong class="bd op">mu ufl数据集的精度分析(单位为%)。</strong> H仅代表HSI，而H+L代表融合的HSI和激光雷达</p></figure><p id="730f" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">可以清楚地看到，对于所有情况，我们的方法在所有途径中以显著的优势优于所有现有技术方法，无论是OA(休斯顿、特伦托和穆弗勒数据集的相应准确度为89.98%、99.06%和91.48%)、AA(相应值为94.65%、98.50%和78.58%)还是κ。很容易观察到，在类/生产者的准确度的情况下，对于大多数类，我们的方法的性能优于其他方法，而对于少数类，我们的方法仅略微超过其他方法。对于休斯顿数据集，可以注意到，与其他方法相比，我们的方法对于“商业”类(92.12%)的准确性有显著提高。这可以归因于这样的事实，即商业区域通常具有可变的布局，并且频繁的海拔变化被基于LiDAR的注意力地图有效地捕捉到。在休斯顿分类地图中也观察到，诸如SVM和双分支CNN的方法倾向于将阴影区域分类为水(在地图的右边部分),因为它们的色调较暗。我们的方法也在很大程度上缓解了这个问题。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/3b2946ac4c800dc2d29d14e526eb1948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/1*g2N2m-aQ_yjeJafdmALKrQ.gif"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><strong class="bd op">从FusAtNet获得的分类图往往噪声较小，并且具有平滑的类间过渡(此处为MUUFL数据集)</strong>HSI(左上)、激光雷达图像(左下)、分类图(右)的真彩色合成图。</p></figure><p id="3b0c" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">类似地，在Trento数据集的情况下,“道路”类显示出显著的准确性提高(93.32%)。该增量也是由于道路轮廓相对于其高度的变化。</p><p id="7cfc" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">此外，例如在MUUFL中，可以从视觉上验证从FusAtNet获得的分类图往往噪声更小，并且具有平滑的类间过渡。</p><blockquote class="kr ks kt"><p id="607b" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx jd">消融</strong></p></blockquote><p id="7c79" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">我们进一步进行了不同的消融研究，以突出我们模型的各个方面。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ox"><img src="../Images/9e8eb1c2e48f8a3d05e5a8055d5b30d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w-UrV1PVXb-lrshO3boFjA.jpeg"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><strong class="bd op">表4: </strong>通过改变所有数据集上的注意力层进行消融研究(精确度为%)</p></figure><p id="a029" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">表4表示在网络中描述的各种注意层存在的情况下的性能，证明了对所有三个注意模块的需要。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi oy"><img src="../Images/2bf1332945fb22f82c62fa0efcd771b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wSXdiBYlRW2FISvwA1drNQ.jpeg"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><strong class="bd op">表5: </strong>有和无数据增强的训练消融研究(准确度百分比)</p></figure><p id="56b7" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">表5显示了数据扩充的性能。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi oz"><img src="../Images/1cf92328c072d83b5f665b1d424f9f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NgN2Rpf2a8O2vMMf8EjJwg.jpeg"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><strong class="bd op">表6: </strong>通过改变MUUFL数据集上训练样本的分数来对性能建模(精度百分比)</p></figure><p id="958f" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">表6表示训练数据减少后的性能。</p><p id="d2cc" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated">除了明显的观察结果，即数据扩充&amp;训练数据的增加增加了准确性，有趣的是注意到<strong class="kx jd"> <em class="kw">我们的网络仅用50%的训练数据就开始胜过现有的SOTA方法。</em>T9】</strong></p><h2 id="8f57" class="od ng it bd nh oe of dn nl og oh dp np ly oi oj nr lz ok ol nt ma om on nv iz bi translated">结论</h2><p id="2fc1" class="pw-post-body-paragraph ku kv it kx b ky nx kd la lb ny kg ld ly nz lg lh lz oa lk ll ma ob lo lp lq im bi translated">总之，我们的工作是在土地覆盖分类的背景下为HSI-LiDAR 融合引入注意力学习概念的<strong class="kx jd">首批方法之一。在这方面，<strong class="kx jd">我们引入了基于“交叉注意”</strong>的概念，在模态间进行特征学习，这是一种新颖直观的融合方法，它利用来自一个模态(此处为激光雷达)的注意来突出另一个模态(HSI)中的特征。我们<strong class="kx jd">在三个基准HSI-LiDAR数据集上展示了最先进的分类性能</strong>，优于所有现有的深度融合策略。</strong></p><p id="3790" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld ly lf lg lh lz lj lk ll ma ln lo lp lq im bi translated"><em class="kw"/><strong class="kx jd"><em class="kw">发表为</em> </strong> <em class="kw">(点击下方)</em></p><blockquote class="kr ks kt"><p id="4278" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><a class="ae pa" href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w6/Mohla_FusAtNet_Dual_Attention_Based_SpectroSpatial_Multimodal_Fusion_Network_for_Hyperspectral_CVPRW_2020_paper.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd"> <em class="it"> FusAtNet:用于高光谱和激光雷达分类的基于双注意的光谱空间多模态融合网络</em> </strong> </a></p><p id="bc86" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx jd"> <em class="it">萨蒂扬·莫赫拉</em>，</strong>希瓦姆·潘德，比普拉·班纳吉，苏哈西斯·乔杜里；IEEE/CVF计算机视觉和模式识别会议(CVPR)研讨会，2020年，第92–93页</p></blockquote></div></div>    
</body>
</html>