<html>
<head>
<title>Linear Regression Model Selection through Zellner’s g prior</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于策尔纳 g 先验的线性回归模型选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-model-selection-through-zellners-g-prior-da5f74635a03?source=collection_archive---------41-----------------------#2020-07-27">https://towardsdatascience.com/linear-regression-model-selection-through-zellners-g-prior-da5f74635a03?source=collection_archive---------41-----------------------#2020-07-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/1d8402885a3c9088a04d2a695be6c109.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sm2om9iNsfZbPbGGK3PgEA.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">信用:<a class="ae kc" href="https://pixabay.com/es/illustrations/barco-mar-oc%C3%A9ano-remo-madera-5404195/" rel="noopener ugc nofollow" target="_blank"> Baggeb </a>上<a class="ae kc" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="8965" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性回归是复杂模型的构建模块，由于其简单性和易解释性而被广泛使用。</p><p id="145d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常，线性回归中模型选择的经典方法是选择具有最高<a class="ae kc" href="https://en.wikipedia.org/wiki/Coefficient_of_determination" rel="noopener ugc nofollow" target="_blank"> R </a>的模型，或者通过<a class="ae kc" href="https://en.wikipedia.org/wiki/Akaike_information_criterion" rel="noopener ugc nofollow" target="_blank"> Akaike 信息</a>标准在复杂性和拟合优度之间找到正确的平衡。相比之下，在贝叶斯推理中，我们非常依赖于分布，因为我们在进行估计时会得到一个分布。</p><p id="14db" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将使用<a class="ae kc" href="https://en.wikipedia.org/wiki/G-prior" rel="noopener ugc nofollow" target="_blank">策尔纳的 g 先验</a>来执行模型选择。此外，虽然用 R 和 Python 构建 ML 模型并不常见，但我们将利用 R 包<code class="fe lb lc ld le b">bas</code>和<code class="fe lb lc ld le b">learnbayes</code>来说明计算。</p><h1 id="82ab" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">解开贝叶斯回归模型</h1><p id="a44e" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">在<a class="ae kc" href="https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_linear_regression" rel="noopener ugc nofollow" target="_blank">多元线性回归</a>中，我们感兴趣的是通过一组<a class="ae kc" href="https://stats.stackexchange.com/questions/409843/what-is-covariate" rel="noopener ugc nofollow" target="_blank">协变量</a><strong class="kf ir"><em class="mi">【x₁】、…</em></strong><em class="mi">来描述一个目标变量<strong class="kf ir"> <em class="mi"> y </em> </strong>的可变性。</em>这些变量中的每一个通常都有不同的贡献，由相对于每一个变量的系数的权重给出<strong class="kf ir"><em class="mi">【β₁】、</em> </strong>也就是说，</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/a09fc454e89892ccf93d5d86d7faf03c.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*6t0IyOm22fQ6XVf_xVujPA.png"/></div></figure><p id="6f58" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中α是<a class="ae kc" href="https://statisticsbyjim.com/regression/interpret-constant-y-intercept-regression/" rel="noopener ugc nofollow" target="_blank">截距</a>(当协变量为零时<em class="mi"> y </em>的值)，我们假设<a class="ae kc" href="https://online.stat.psu.edu/stat500/lesson/9/9.2/9.2.3" rel="noopener ugc nofollow" target="_blank">等方差</a><strong class="kf ir"><em class="mi"/></strong>。如果考虑<a class="ae kc" href="https://en.wikipedia.org/wiki/Least_squares" rel="noopener ugc nofollow" target="_blank">最小二乘</a>方法(经典统计学)，人们将通过<a class="ae kc" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#:~:text=In%20statistics%2C%20maximum%20likelihood%20estimation,observed%20data%20is%20most%20probable." rel="noopener ugc nofollow" target="_blank">最大似然估计</a>来估计未知参数<strong class="kf ir"> ( <em class="mi"> α、</em> </strong> <em class="mi"> βᵢ </em> <strong class="kf ir"> <em class="mi">、σ </em> ) </strong>的值。然而，贝叶斯方法让你将每一个系数视为随机变量。因此，目标是获得这些系数可能取值的概率模型，其优点是获得我们对这些系数值的不确定性的估计。</p><p id="b736" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了实现这一点，贝叶斯方法非常灵活。<strong class="kf ir">首先</strong>，人们对这些值可能是什么设置了一个先验信念，也就是说，在看到数据<strong class="kf ir"> <em class="mi"> y </em> </strong>之前，我们反映了我们对参数值的了解。例如，假设我们对它们一无所知，那么我们可以考虑一个<a class="ae kc" href="https://stats.stackexchange.com/questions/27813/what-is-the-point-of-non-informative-priors" rel="noopener ugc nofollow" target="_blank">无信息先验</a></p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/78befd8bca57439a99654a39404da668.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*qaNUjTdiu7jss92VzdFPSA.png"/></div></figure><p id="2503" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当人们把这个表达式理解为<strong class="kf ir"> ( <em class="mi"> α，β，σ </em> ) </strong>的<a class="ae kc" href="https://en.wikipedia.org/wiki/Joint_probability_distribution" rel="noopener ugc nofollow" target="_blank">联合分布</a>与方差<strong class="kf ir"><em class="mi">【σ】</em></strong>的倒数成正比时，这叫做<strong class="kf ir"> <em class="mi"> </em>先验分布<em class="mi">。</em> </strong>注意，β是一个向量，其分量为β₁，…，βₙ.</p><p id="7f25" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">然后是</strong>，之后是<strong class="kf ir"> <em class="mi"> </em> </strong>观察数据<strong class="kf ir"> <em class="mi"> y，</em> </strong>我们设定如何描述给定的参数和协变量。因为我们持有线性回归假设</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/37ee50318c69b7c9ad522974ce8c8b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*m4wlGh3mO_cXVRp3jJkpRw.png"/></div></figure><p id="c8d8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者给定<em class="mi"> x </em>，α，β，σ，数据似乎符合一个<a class="ae kc" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">正态分布</a>，均值为α + β <em class="mi"> x </em>，方差为<em class="mi"> </em> σ，这叫做<strong class="kf ir">似然</strong>。请注意，<em class="mi"> x </em>是一个向量，其分量为<em class="mi"> x </em> ₁，…，<em class="mi"> x </em> ₙ.</p><p id="dfa0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">最后</strong>，<a class="ae kc" href="https://www.youtube.com/watch?v=XQoLVl31ZfQ" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>陈述<strong class="kf ir"/><strong class="kf ir">后验分布</strong>与似然性和先验分布的乘积成正比，</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/f1e759d81378d27b2e3ac6b51da40f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*CSQcrg5SSKmKpvZUst2WIg.png"/></div></figure><p id="a21d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">了解了这些组件，您可能会注意到选择先验有点主观。出于这个原因，策尔纳介绍了一种评估一个人对先前的选择有多确定的方法。</p><h1 id="b53d" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">策尔纳的方法</h1><p id="7515" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">1986 年，阿诺德·策尔纳提出了一种在回归模型中引入主观信息的简单方法。想法是用户指定β系数的位置，例如，假设β系数都是酉的，</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/d7c5b3606d93fd320446216c4cb620f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*ITFT47PnU9uZqhEqw5qE3A.png"/></div></figure><p id="553e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，一个人对这个假设的信念将由常数<strong class="kf ir"> <em class="mi"> g </em> </strong>来反映，该常数反映了先验中相对于数据的信息量。因此，为<strong class="kf ir"> <em class="mi"> g </em> </strong>选择一个较小的值，意味着一个人对这个猜想有更强的信念。相反，选择更大的值<strong class="kf ir"> <em class="mi"> g </em> </strong>具有与选择<strong class="kf ir"> ( <em class="mi"> α，β，σ </em> ) </strong>的无信息先验相似的效果，因为随着<strong class="kf ir"><em class="mi"/></strong>g 趋于无穷大，其对先验的影响消失，更多细节参见【2】。</p><h1 id="b7dc" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">基于策尔纳 g 先验的模型选择</h1><p id="3e91" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">如果我们对响应变量<strong class="kf ir"> <em class="mi"> y </em> </strong>有 n 个预测值，那么就有 2ⁿ个可能的回归模型。泽尔纳的 g 先验可用于在 2ⁿ候选模型中选择最佳模型。</p><p id="6c11" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，假设模型具有相同的先验概率，即，对于每个模型，我们为β系数分配 0 的先验猜测和相同的 g 值。然后，我们可以通过计算<a class="ae kc" href="https://stats.stackexchange.com/questions/394648/differences-between-prior-distribution-and-prior-predictive-distribution" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> <em class="mi">先验预测分布</em> </strong> </a> <strong class="kf ir"> <em class="mi"> </em> </strong>(数据<em class="mi"> y </em>在所有可能的参数值上平均的分布)来比较回归模型。</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/d032afde06be9e5c7f3b10dab6065cde.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*bgMLimPnHm6sLy2NjLMEnA.png"/></div></figure><p id="6de7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，如果这个积分没有封闭形式的<a class="ae kc" href="https://en.wikipedia.org/wiki/Closed-form_expression" rel="noopener ugc nofollow" target="_blank">或者有几个维度，那么它可能很难计算。一种近似结果的方法是拉普拉斯方法，更多细节参见[6]和[1]的第 8 章。</a></p><p id="dafe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">曾经，我们计算过每个值<strong class="kf ir"><em class="mi">【p(y)</em></strong>对于每个<strong class="kf ir"> <em class="mi"> m </em> </strong> <em class="mi"> </em>模型，我们需要一种方法来比较这些模型。例如，当只比较它们中的两个时，我们可以计算它们先前预测密度的比率，</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/980d15516b181143d8d5cddb025c538f.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*ouy6DpyEbVc0D2Yq0KK92Q.png"/></div></figure><p id="6f77" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是所谓的<strong class="kf ir">贝叶斯因子。</strong>比值越大，模型<em class="mi"> j </em>对模型<em class="mi"> k 的支持度越大，</em>你可以在[1]的第八章和[5]的第六章阅读更多关于贝叶斯因子的内容。</p><p id="b5af" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我们正在比较 2ⁿ模型，我们可以计算每个模型的<strong class="kf ir">后验概率</strong>，</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/c4c93e766c0f0bada46d1056c8e47014.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*2MYChT-wEvhEv2--HV3GZg.png"/></div></figure><p id="afdb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们将选择概率最高的模型。在接下来的几节中，我们将借助 R 包<code class="fe lb lc ld le b">BAS</code>和<code class="fe lb lc ld le b">learnbayes</code>来看看这个过程。</p><h1 id="d45f" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">代码示例</h1><p id="5760" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">为了运行 R 和 Python，我们将使用 Jupyter 的 Docker image<a class="ae kc" href="https://hub.docker.com/r/jupyter/datascience-notebook/" rel="noopener ugc nofollow" target="_blank">data science-notebook</a>。关于使用 Anaconda 的替代方法，请查看这个<a class="ae kc" href="https://stackoverflow.com/questions/61622624/issues-when-attempting-to-use-rpy2/62986815#62986815" rel="noopener ugc nofollow" target="_blank"> Stackoverflow 问题</a>。</p><p id="0ab3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae kc" href="https://docs.docker.com/get-docker/" rel="noopener ugc nofollow" target="_blank">安装 Docker </a>后，运行你的终端</p><pre class="mk ml mm mn gt mp le mq mr aw ms bi"><span id="e98b" class="mt lg iq le b gy mu mv l mw mx">docker run -it -p 8888:8888 -p 4040:4040 -v D:/user/your_user_name:/home/jovyan/work jupyter/datascience-notebook</span></pre><p id="6a01" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果这是第一次运行这个命令，Docker 会自动拉图片 datascience-notebook。注意，应该启用文件共享来挂载一个本地目录作为 Docker 容器的一个卷，更多内容请参见<a class="ae kc" href="https://thenewstack.io/docker-basics-how-to-share-data-between-a-docker-container-and-host/" rel="noopener ugc nofollow" target="_blank">这里的</a>。</p><p id="6394" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 Jupyter 中，我们将使用 R 包<code class="fe lb lc ld le b">learnbayes</code>和<code class="fe lb lc ld le b">bas</code>来计算具有 g 先验的线性回归模型。此外，我们还使用了来自《汽车碰撞事故》的 Kaggle 数据集，这是 seaborn 默认提供的。</p><figure class="mk ml mm mn gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="1780" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">按照[1]第 9 章中的例子，我们将使用<code class="fe lb lc ld le b">LearnBayes</code>包中的函数<code class="fe lb lc ld le b">bayes_model_selection</code>来计算每个模型的后验概率。</p><p id="be00" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的目标变量<strong class="kf ir"> <em class="mi"> y </em> </strong>将是<strong class="kf ir"> total </strong>“每十亿英里中涉及致命碰撞的驾驶员数量”。为简单起见，我们将前四个变量作为协变量。</p><figure class="mk ml mm mn gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="03dc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，<code class="fe lb lc ld le b">bayes_model_selection</code>函数非常直观，因为我们只需提供目标变量、协变量和β的先验猜测中的置信值<strong class="kf ir"> <em class="mi"> g </em> </strong>。</p><p id="fcd1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我们选择了值<code class="fe lb lc ld le b">g=100</code>，因为我们不太确定之前的猜测。考虑到该值越高，估计值就越接近最小二乘估计值，您可以随意使用该值。</p><p id="6e68" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过访问模型上的<code class="fe lb lc ld le b">mod.prob</code>，我们可以可视化结果。在下面的代码中，我们按照后验概率的较高值进行排序。</p><figure class="mk ml mm mn gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/c6429a79e7a2f185262316ac7a4c5d5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*lUL6A4ZgVzVe-2rX-WprRQ.png"/></div></figure><p id="ca42" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从表中我们看到，最可能的模型认为协变量为<em class="mi">未 _ 分心</em>和<em class="mi">未 _ 先前</em>。还有，贡献最高的个体变量是<em class="mi"> no_previous </em>和<em class="mi">超速。</em></p><h1 id="2cd1" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">如果有大量的协变量呢？</h1><p id="e6ea" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">当处理大量的协变量，也就是大量的模型时，<a class="ae kc" href="https://homepage.divms.uiowa.edu/~jghsh/clyde_ghosh_littman_2010_jcgs.pdf" rel="noopener ugc nofollow" target="_blank">贝叶斯自适应采样</a>算法是一个很好的选择。它的工作原理是对模型进行采样，而不从可能的模型空间中进行替换。为了说明这一点，我们现在考虑数据集中的所有变量，我们有 2⁶可能的模型可供选择。</p><figure class="mk ml mm mn gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="c937" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，具有较高后验概率或较高边际可能性的模型是最有可能的，即，具有协变量集<strong class="kf ir">酒精</strong>、<strong class="kf ir">未分心</strong>和<strong class="kf ir">否先前</strong>的模型。</p><h1 id="9b02" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">主要发现</h1><p id="8295" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">总结一下，让我们概括一些重要的发现:</p><ul class=""><li id="8e1e" class="nb nc iq kf b kg kh kk kl ko nd ks ne kw nf la ng nh ni nj bi translated">在经典统计学中，人们得到点估计，而在贝叶斯统计学中，人们得到参数可能取值的概率分布。</li><li id="87a9" class="nb nc iq kf b kg nk kk nl ko nm ks nn kw no la ng nh ni nj bi translated">先验是信念，可能性是证据，后验是最终的知识。</li><li id="f4ab" class="nb nc iq kf b kg nk kk nl ko nm ks nn kw no la ng nh ni nj bi translated">策尔纳的 g 先验反映了一个人对先验信念的信心。</li><li id="bcda" class="nb nc iq kf b kg nk kk nl ko nm ks nn kw no la ng nh ni nj bi translated">当有大量模型可供选择时，可以考虑使用 BAS 算法。</li></ul><p id="f085" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们已经看到，模型选择的贝叶斯方法与经典方法一样直观且易于实现，同时还能让您更深入地了解模型的内部工作。</p><p id="4380" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有什么问题吗？留下评论。感谢阅读，如果你喜欢这篇文章，请随意分享。</p><h1 id="1fed" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">参考</h1><p id="7d48" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">[1]艾伯特，吉姆。贝叶斯计算与 R 第二版。斯普林格，2009 年。</p><p id="83eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]马林，J.M 罗伯特，Ch。回归和变量选择。网址:<a class="ae kc" href="https://www.ceremade.dauphine.fr/~xian/BCS/Breg.pdf" rel="noopener ugc nofollow" target="_blank">https://www.ceremade.dauphine.fr/~xian/BCS/Breg.pdf</a></p><p id="aa81" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]克莱德，梅里斯；Ghosh，Joyee 里特曼，迈克尔。变量选择和模型平均的贝叶斯自适应抽样。网址:<a class="ae kc" href="https://homepage.divms.uiowa.edu/~jghsh/clyde_ghosh_littman_2010_jcgs.pdf" rel="noopener ugc nofollow" target="_blank">https://home page . div ms . uio wa . edu/~ jghsh/Clyde _ ghosh _ littman _ 2010 _ jcgs . pdf</a></p><p id="2b3c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4]阿诺德·策尔纳。用 g-先验分布评估先验分布和贝叶斯回归分析。1986</p><p id="fb0f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5]戈什、贾扬塔；德兰帕迪、莫汉；萨曼塔，塔帕斯。贝叶斯分析导论:理论与方法。斯普林格，2006 年。</p><p id="8ccf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[6]贝叶斯模型在 R. URL 中的简易拉普拉斯近似:<a class="ae kc" href="https://www.r-bloggers.com/easy-laplace-approximation-of-bayesian-models-in-r/" rel="noopener ugc nofollow" target="_blank">https://www . r-bloggers . com/Easy-la place-approximation-of-Bayesian-models-in-r/</a></p><p id="3664" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[7] Rpy2 文档。网址:<a class="ae kc" href="https://rpy2.github.io/doc/latest/html/index.html" rel="noopener ugc nofollow" target="_blank">https://rpy2.github.io/doc/latest/html/index.html</a></p><p id="ac0b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[8]机器学习中模型评估和选择的终极指南。Neptune . ai . URL:<a class="ae kc" href="https://neptune.ai/blog/the-ultimate-guide-to-evaluation-and-selection-of-models-in-machine-learning" rel="noopener ugc nofollow" target="_blank">https://Neptune . ai/blog/the-ultimate-guide-to-evaluation-and-selection-of-models-in-machine-learning</a></p></div></div>    
</body>
</html>