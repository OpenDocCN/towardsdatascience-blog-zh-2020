<html>
<head>
<title>Data Preprocessing And Interpreting Results: The Heart Of Machine Learning: Part 2- PCA, Feature Selection And Result Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据预处理和解释结果:机器学习的核心:第 2 部分-主成分分析，特征选择和结果分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8?source=collection_archive---------35-----------------------#2020-06-28">https://towardsdatascience.com/data-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8?source=collection_archive---------35-----------------------#2020-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/400bf6882d5a6146059be8375a05d185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mpyrgqwMjfclV2oN1U2VIA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">卢克·切瑟在<a class="ae jd" href="/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="e8da" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">如何选择特征进行预测？</h2></div><p id="0a7e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在<a class="ae jd" href="https://medium.com/@myac.abhijit/data-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-1-eda-49ce99e36655" rel="noopener">第 1 部分</a>，我们已经看了我们的数据集和探索性数据分析的概念。在这一部分，我们将看看主成分分析或 PCA 和特征选择程序。我们开始吧。</p><p id="a9b1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">到目前为止，我们已经单独可视化了几个特征以及它们之间的相关性。但是对我们来说，将整个数据可视化是非常重要的。为此，我们需要将这 30 维数据投影到 2D 特征地图中。因此，需要降维。这就是 PCA 的用武之地。</p><p id="dfb8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们进入主成分分析之前，让我们探讨一下方差。</p><h2 id="b28f" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">差异</h2><blockquote class="mk ml mm"><p id="fd90" class="kv kw mn kx b ky kz kh la lb lc kk ld mo lf lg lh mp lj lk ll mq ln lo lp lq ij bi translated"><strong class="kx jh">方差是对集合分散或展开程度的度量，这是“平均值”(均值或中值)所不具备的。例如，如果我告诉你一个数据集的方差为零，你就知道它的每个成员都是相同的。如果方差很高，特别是与平均值的比率，这就告诉你数据在其成员之间有很大的不相似性。</strong></p></blockquote><p id="3def" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果方差非常低，我们基本上可以从该特性或特性集获得所有相似类型的数据。机器学习通常针对不同类别中数据点的不同行为。因此，如果它非常低，点预计会聚集在相同的点周围，这导致它们很难区分。因此方差通常被认为是最大似然数据集的信息。</p><p id="0357" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">方差由下式给出:</p><blockquote class="mk ml mm"><p id="bc0e" class="kv kw mn kx b ky kz kh la lb lc kk ld mo lf lg lh mp lj lk ll mq ln lo lp lq ij bi translated">方差= Sum [i=1 到 n]((x { I }-均值(x)))</p></blockquote><p id="28db" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在 PCA 中，我们试图创建 n 个复合特征，它们能够最好地表示包含在我们的数据集的 30 个特征中的信息。这 n 个特征称为主成分。n 的值取决于用户。这 n 个特征都不是原始特征。这些特征被开发为不同特征的组合。</p><p id="5743" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们开始理解和应用 PCA 之前，我们必须注意一件事。PCA 总是在缩放的数据上进行。那么，让我们来看看缩放。</p><h2 id="032c" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">缩放比例</h2><p id="19bb" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">我们使用的数据有几个特点。现在，这些特征根据特征具有不同的范围，例如一些特征具有 0 到 1 之间的十进制值，而其他特征具有 100-1000 之间的值。</p><p id="2069" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们使用像逻辑回归这样的分类器，就会产生很多问题。</p><blockquote class="mk ml mm"><p id="33f6" class="kv kw mn kx b ky kz kh la lb lc kk ld mo lf lg lh mp lj lk ll mq ln lo lp lq ij bi translated">y = w1x 1+w2x 2+……………………+wnxn+b</p></blockquote><p id="bfc8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">逻辑回归画一条线，用这个公式表示。现在，如果 x1 从 0 到 1，x2 从 100 到 1000，显然 x2 会有很大的压力和重量。为了避免这种情况，进行了缩放，使得所有的值都在固定的范围内。它只是将值更改为一个比例，但保持分布不变。</p><p id="8798" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有两种主要的缩放类型</p><ol class=""><li id="662f" class="mw mx jg kx b ky kz lb lc le my li mz lm na lq nb nc nd ne bi translated"><strong class="kx jh">标准比例:</strong>由下式给出</li></ol><blockquote class="mk ml mm"><p id="a8ea" class="kv kw mn kx b ky kz kh la lb lc kk ld mo lf lg lh mp lj lk ll mq ln lo lp lq ij bi translated">标度=(x-均值(x))/标准差(x))</p></blockquote><p id="7381" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它给出-1 到+1 之间的值。</p><p id="c419" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2 <strong class="kx jh">。最小-最大缩放:</strong>由下式给出</p><blockquote class="mk ml mm"><p id="1038" class="kv kw mn kx b ky kz kh la lb lc kk ld mo lf lg lh mp lj lk ll mq ln lo lp lq ij bi translated">Scale=(x- min(x))/(max(x)-min(x))</p></blockquote><p id="3d2e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它给出 0 到 1 之间的值。</p><h2 id="e619" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">主成分分析</h2><p id="53b8" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">让我们回到 PCA。我们将在这里使用标准的定标器。让我们先来看看应用程序，然后我们再来看解释。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="fd3b" class="lr ls jg nk b gy no np l nq nr">from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>scale=scaler.fit(X)<br/>X_scaled=scaler.transform(X)<br/>X_scaled_df=pd.DataFrame(X_scaled,columns=X.columns)<br/>from sklearn.decomposition import PCA<br/>pca = PCA(n_components=2)<br/>PC = pca.fit_transform(X_scaled_df)<br/>p_Df = pd.DataFrame(data = PC<br/>             , columns = ['principal component 1', 'principal component 2'])<br/>p_Df.head()</span></pre><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/ddc48c83fddc594b96a8753214d53052.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*szk89959aE9KiVgihTt28A.png"/></div></figure><p id="d2be" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">降维后，这两个分量形成为我们的 n 分量是 2。现在，我们将把这作为我们的特征集，并相应地加入我们的目标集，我们将尝试可视化。由于它有两个数据特征，现在可以很容易地在 2D 平面上表示出来。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="96eb" class="lr ls jg nk b gy no np l nq nr">Y_l=Y.iloc[:]['diagnosis'].tolist()<br/>joined=p_Df<br/>joined['Diagnosis']=Y_l<br/>joined.head()</span></pre><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/0b25d02dc72dbfe6aab160eb103ed364.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*dlIuSX6oDfzIEXabJsn2lA.png"/></div></figure><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="08d4" class="lr ls jg nk b gy no np l nq nr">import matplotlib.pyplot as plt<br/>fig = plt.figure(figsize = (10,10))<br/>ax = fig.add_subplot(1,1,1) <br/>ax.set_xlabel('PC 1', fontsize = 12)<br/>ax.set_ylabel('PC 2', fontsize = 12)<br/>ax.set_title('PCA', fontsize = 15)<br/>diags=['M','B']<br/>colors=['r','b']<br/>for diag, color in zip(diags,colors):<br/>    i = joined['Diagnosis'] == diag<br/>    ax.scatter(joined.loc[i, 'principal component 1']<br/>               , joined.loc[i, 'principal component 2']<br/>               , c = color<br/>               , s = 50)<br/>ax.legend(diags)<br/>ax.grid()</span></pre><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ebb57463485c91d07fdab4611498ce63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*xNHskvE38VSTI_79OioDag.png"/></div></figure><p id="ebef" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是得到的主成分分析分布。如果我们以最好的方式表示 2D 的 30D 特征集，这就是我们的完整数据集表示。但是，这总是不可行的。让我们看看为什么。</p><p id="8f3f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">PCA 是基于无监督算法的。问题是，这些成分是如何获得的？答案来了。</p><p id="46de" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们有一个 30 维的数据集。这意味着我们可以在一个 30 维的平面上绘制我们的点，每个维度代表一个特征。比方说，然后我们根据我们的目标分类给这些点着色。“M”是红色的。“B”是蓝色的。现在，在这一时刻，我们试图绘制一个 2D 平面，它最适合于根据目标类来划分点。我们使用 2D，因为 n_components 是 2。现在我们开始在 2D 平面上对这些 30D 点进行投影。因此，我们将 30 长度的元组减少到 2 长度的元组。现在，如果我们思考，我们会非常清楚，由于维数减少，我们也会丢失一些信息。因此，方差减小。x 轴和 y 轴值是主分量。因此，我们的目标是如何减少方差损失。为了做到这一点，我们通常从许多选择中挑选出可投影的 2D 平面。</p><p id="2859" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">详细地说，首先，选择一个最能区分这些点的 2D 平面。投影这些点并计算方差损失，然后选择与前一个平面正交的另一个平面并再次计算其方差。如此继续下去，直到获得许多选择，然后使用损失最小的平面。</p><p id="8353" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">毕竟，主成分分析并不总是很有信心代表所有的信息。这可以用方差比来测试。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="d324" class="lr ls jg nk b gy no np l nq nr">pca.explained_variance_ratio_</span><span id="9442" class="lr ls jg nk b gy nv np l nq nr">array([0.44272026, 0.18971182])</span></pre><p id="2b07" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这没有给出非常清楚的表示，因为这两个分量仅表示这里所表示的总信息的 44+18=62%。这不是一个准确的表示，因为它仅表示总信息量的 62%。也就是说，它不能用两个主成分来表示。这是一个估计值，但据说如果比率之和小于 85%，则表示不太正确，因为缺少很多信息。</p><p id="d447" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">尽管如此，还是可以从这个操作中提取很多信息。比如，如果我们看这个图表，我们可以看到，如果我们画一条直线，那么它可以很容易地分类。因此，对于这个问题，逻辑回归将比 KNN 型分类器发挥更好的作用。</p><p id="81ae" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这几乎是关于 PCA 的所有内容，让我们进入下一个主题特性选择。</p><h2 id="b237" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">特征选择</h2><p id="6152" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">我们有时会面临有很多特点的问题。它们甚至是像 99 100 个特征这样的大数字。现在，这些特征中的一些在模型预测中不是很有用。但是它们增加了特征集的维数，从而使其难以分析，增加了训练的时间，也降低了准确性。所以这些特征必须被去除。因此，在本节中，我们将讨论完成这项任务的几种方法。</p><h2 id="ca8b" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">随机森林特征重要性</h2><p id="9328" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">在这种方法中，我们将数据拟合到一个随机森林模型，并获得特征重要性。从而获得最佳特征。正如我们所知，随机森林是基于决策树算法的，因此它们可以非常容易地模拟非线性，并提供非常清晰的特征重要性的图像。让我们看看应用程序。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="7525" class="lr ls jg nk b gy no np l nq nr">from sklearn.ensemble import RandomForestClassifier<br/>rfc = RandomForestClassifier()<br/>X_n=X.values<br/>Y_n=Y.values<br/>rfc.fit(X_n,Y_n)<br/>importance = rfc.feature_importances_</span></pre><p id="fdaa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从而拟合决策树模型，获得特征重要度。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="d2fa" class="lr ls jg nk b gy no np l nq nr">import matplotlib.pyplot as plt; plt.rcdefaults()<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="496d" class="lr ls jg nk b gy nv np l nq nr">objects = X.columns<br/>y_pos = np.arange(len(objects))<br/>performance = importance</span><span id="62c6" class="lr ls jg nk b gy nv np l nq nr">fig, ax = plt.subplots(figsize=(20, 20))<br/>plt.barh(y_pos, performance, align='center', alpha=0.5)<br/>fontsize=14,<br/>plt.yticks(y_pos, objects,fontsize=20)<br/>plt.xticks(fontsize=20)<br/>plt.xlabel('Contributions')<br/>plt.title('Feature Contributions',fontsize=20)</span><span id="b01b" class="lr ls jg nk b gy nv np l nq nr">plt.show()</span></pre><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/ed9a06754ee584279625e5f9cfff3918.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4NQPNfM0g1LiXP8hi6ibFw.png"/></div></div></figure><p id="dc95" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是从随机森林算法中获得的特征重要性。</p><h2 id="4e23" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">逻辑回归特征重要性</h2><p id="afae" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">这是另一种在将数据拟合到逻辑回归模型后使用特征重要性的方法。如果数据本质上是线性的，这种方法非常有效。众所周知，逻辑回归的分类线由一个线性方程给出，如:</p><p id="c71f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Y=w1x1+w2x2+w3x3…………+wnxn</p><p id="9c2f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里的权重是相应的特征重要性。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="5759" class="lr ls jg nk b gy no np l nq nr">from sklearn.linear_model import LogisticRegression<br/>model = LogisticRegression(solver='liblinear', random_state=0)<br/>model.fit(X_n, Y_n)<br/>Weights=np.hstack((model.intercept_[:,None], model.coef_))<br/>k=X.columns<br/>k.append('bias')</span></pre><p id="2ff6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，模型是合适的，并且在权重列表中获得了重要度。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/651a40ae1a685af9274b183d65287b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hkrtLkl5U-8REtu6a5k2Lw.png"/></div></div></figure><p id="c140" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是由逻辑回归生成的特征重要性分布。</p><h2 id="5595" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">Lasso 回归要素重要性</h2><p id="4ec4" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">这是一种非常常用的方法，用于获取特征重要性。这是通过将数据拟合到 lasso 回归模型并获得特征重要性来实现的。Lasso 回归适用于 L1 正则化的策略。</p><p id="4838" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">L1 正则化用于稀疏数据，其中有许多特征，但并非所有特征都是预测所必需的。在这种情况下，L1 正则化将不是非常重要的特征的特征重要性归零。Lasso 使用相同的策略。</p><p id="38da" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">需要注意的一点是 Lasso 不能用于分类“M”和“B”类，它们必须转换成整数。</p><p id="9ca8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们检查一下应用程序</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="8bbe" class="lr ls jg nk b gy no np l nq nr">from sklearn.linear_model import Lasso<br/>import numpy as np<br/>lasso = Lasso(alpha=0.0001)<br/>i=0<br/>y_d=[]<br/>while i&lt;len(Y_n):<br/>    if Y_n[i][0]=='M':<br/>        y_d.append([1])<br/>    else:<br/>        y_d.append([0])<br/>    i+=1<br/>y_d=np.array(y_d)<br/>lasso.fit(X_n, y_d)</span><span id="4f1d" class="lr ls jg nk b gy nv np l nq nr">l=(lasso.coef_)</span></pre><p id="3872" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，我们将数据拟合到 lasso 模型，并加载特征重要性。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="9206" class="lr ls jg nk b gy no np l nq nr">import matplotlib.pyplot as plt; plt.rcdefaults()<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="f3dd" class="lr ls jg nk b gy nv np l nq nr">objects = X.columns<br/>y_pos = np.arange(len(objects))<br/>performance = l</span><span id="b218" class="lr ls jg nk b gy nv np l nq nr">fig, ax = plt.subplots(figsize=(20, 20))<br/>plt.barh(y_pos, performance, align='center', alpha=0.5)<br/>fontsize=14,<br/>plt.yticks(y_pos, objects,fontsize=20)<br/>plt.xticks(fontsize=20)<br/>plt.xlabel('Contributions')<br/>plt.title('Feature Contributions',fontsize=20)</span><span id="287a" class="lr ls jg nk b gy nv np l nq nr">plt.show()</span></pre><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/b695e10053322a4b0f7cfc6ccb588cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gu9WfslAuPvzeSX3dfvmEw.png"/></div></div></figure><p id="c361" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是 lasso 获得的特征重要性列表，我们可以看到一些特征的权重被降低到零，所以在模型的训练中，它们不会起任何作用，因为它们的权重为 0。</p><h2 id="002f" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">递归特征消除</h2><p id="1f9c" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">递归特征消除是迄今为止设计的选择最佳特征的最佳方式。它基于一种递归算法。它首先考虑所有特征，建立模型预测，然后找到最不重要的特征。现在，它删除这些特征，重建模型，并再次检查删除特征的效果。这个过程包括一个 k 倍交叉验证步骤，以消除集合中任何种类的不平衡问题。我们设置流程使用的估计器或模型。</p><p id="cf86" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">实施:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="f79c" class="lr ls jg nk b gy no np l nq nr">from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.model_selection import StratifiedKFold<br/>from sklearn.feature_selection import RFECV<br/>rfc = RandomForestClassifier(random_state=101)<br/>rfecv = RFECV(estimator=rfc, step=1, cv=StratifiedKFold(10), scoring='accuracy')<br/>rfecv.fit(X_n,Y_n)</span></pre><p id="3dc1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，我们使用随机森林分类器作为估计器。我们在每一步都使用 10 倍交叉验证。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="8f23" class="lr ls jg nk b gy no np l nq nr">features=rfecv.n_features_<br/>features<br/>-&gt; 24</span></pre><p id="070c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这用于获得消除后最终获得的特征的数量。我们可以看到，在剔除之后，获得了 30 个特征中的 24 个。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="4fd4" class="lr ls jg nk b gy no np l nq nr">n_p=np.where(rfecv.support_ == False)[0]</span></pre><p id="05dc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这获得了在该过程中已经消除的特征。让我们想象一下结果。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="06fe" class="lr ls jg nk b gy no np l nq nr">feat=X.columns<br/>to_drop=[]<br/>for i in n_p:<br/>    to_drop.append(feat[i])<br/>x_imp=X.drop(to_drop,axis=1)<br/>import matplotlib.pyplot as plt; plt.rcdefaults()<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="ef73" class="lr ls jg nk b gy nv np l nq nr">objects = x_imp.columns<br/>y_pos = np.arange(len(objects))<br/>performance = n</span><span id="3a0f" class="lr ls jg nk b gy nv np l nq nr">fig, ax = plt.subplots(figsize=(20, 20))<br/>plt.barh(y_pos, performance, align='center', alpha=0.5)<br/>fontsize=14,<br/>plt.yticks(y_pos, objects,fontsize=20)<br/>plt.xticks(fontsize=20)<br/>plt.xlabel('Contributions')<br/>plt.title('Feature Contributions',fontsize=20)</span><span id="719d" class="lr ls jg nk b gy nv np l nq nr">plt.show()</span></pre><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/7fdcb0e90c5e1ba915eec7995a38121c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SRTKb9i4fcDa1d2zZxP54Q.png"/></div></div></figure><p id="0c2a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些是 RFE 之后最后留下的特征和它们的重要性。</p><p id="0024" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们已经介绍了几乎所有用于获得重要特性的重要过程。</p><p id="9c0d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么，让我们进入下一部分，结果分析。</p><h2 id="1819" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">结果分析</h2><p id="da15" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">机器学习的另一个非常重要的部分是结果分析或解释。现在，通常情况下，我们理解结果意味着高准确性，但在机器学习的情况下并非如此。在机器学习中，准确性取决于许多其他因素。问题是为什么？让我们来看一个例子。假设有一个包含 100 封电子邮件的数据集，其中有 10 封是垃圾邮件。因此，我们形成 80%的训练集和 20%的测试集。现在，假设训练集有 9 封垃圾邮件，测试集有 1 封垃圾邮件。我们训练了我们的模型，但是由于垃圾邮件的实例较少，它没有正确地学习它。因此，该模型将所有内容分类为非垃圾邮件。现在，如果我们使用我们的测试数据集来评估我们的模型，那么它将给出 90%的准确性，因为 9 个实际上不是垃圾邮件。但是我们的模式实际上失败了。所以，我们不能只把准确性作为一个评价指标。</p><p id="ed26" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">已经开发了几种方法来测量结果，最常用的是 ROC-AUC 曲线和精确回忆。</p><h2 id="f0a6" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">ROC- AUC 曲线</h2><p id="54f2" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">ROC 或接收器操作特性曲线用于可视化二元分类器的性能。它显示了真阳性率和假阳性率之间的权衡。</p><blockquote class="mk ml mm"><p id="899f" class="kv kw mn kx b ky kz kh la lb lc kk ld mo lf lg lh mp lj lk ll mq ln lo lp lq ij bi translated">真阳性率=真阳性/(真阳性+假阴性)</p><p id="d6b5" class="kv kw mn kx b ky kz kh la lb lc kk ld mo lf lg lh mp lj lk ll mq ln lo lp lq ij bi translated">假阳性率=假阳性/(真阴性+假阳性)</p></blockquote><p id="ebd9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，它是如何工作的？分类模型实际上产生一个预测百分比，该百分比表示给定样本属于给定类别的可能性有多大。预测百分比可以是 0 到 100 之间的任何值。现在，它有一个阈值。在阈值以下，样本被分类为 0 类，在阈值以上，它被认为是 1 类。所以，假阳性，真阳性都取决于这个阈值。如果我们降低阈值，分类为 1 类的样本数量增加，因此假阳性增加，否则假阴性增加。ROC 曲线实际上滚动这个阈值，并相应地指出所有的真阳性和假阳性率。</p><p id="0a5e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">AUC 代表“ROC 曲线下的面积”也就是说，AUC 测量整个 ROC 曲线下的整个二维面积。AUC 提供了对所有可能的分类阈值的综合绩效衡量。因此，AUC 越大，模型正确标记类别的机会就越大。所以，更多的是准确性。</p><p id="7929" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我发现<a class="ae jd" rel="noopener" target="_blank" href="/understanding-auc-roc-curve-68b2303cc9c5">有一个很棒的帖子</a>，它解释了我在这里使用的图片。</p><blockquote class="mk ml mm"><p id="ca20" class="kv kw mn kx b ky kz kh la lb lc kk ld mo lf lg lh mp lj lk ll mq ln lo lp lq ij bi translated"><strong class="kx jh">一个优秀的模型具有接近 1 的 AUC，这意味着它具有良好的可分性度量。差模型的 AUC 接近 0，这意味着它具有最差的可分性度量。事实上，这意味着它是往复的结果。它预测 0 是 1，1 是 0。当 AUC 为 0.5 时，意味着模型没有任何类别分离能力。</strong></p></blockquote><p id="3cd0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">根据型号的容量，可能有 4 种情况。</p><p id="9efe" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">案例 1:</p><div class="nf ng nh ni gt ab cb"><figure class="ny is nz oa ob oc od paragraph-image"><img src="../Images/edf9b63d622667a9f40bddaa12309b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*ZeDY1DcWPuYWB17Wm8vJ2A.png"/></figure><figure class="ny is oe oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/ce98e7ba6de7912b666b4b7468b2a7ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*yW7Lk2o6QLNSWaq0vm7RGw.png"/></div></figure></div><p id="58ca" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是模型正确预测每个点的理想情况。在实际数据中，正类和负类是不相交的集合，也就是说，一个点可以是真正的，也可以是真负的，但不能同时是真正的和真负的。在这里，我们可以看到模型正确地覆盖和分类了所有的点。因此，曲线下的面积是完整的 2D 盒面积，即 1，精度是 100%</p><p id="3eb8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">案例二:</p><div class="nf ng nh ni gt ab cb"><figure class="ny is of oa ob oc od paragraph-image"><img src="../Images/83dec66cb4b6f9b22f883a286f1769ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*jULiIS9po34fQZo5kLHQEg.png"/></figure><figure class="ny is og oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/f14342209b11d1f9c0f64255949ce95f.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*maiqbgRlrT_UreZ_8RN5Lw.png"/></div></figure></div><p id="c824" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些图表显示了模型的实际行为。在这里，我们可以看到有一个重叠部分，其中模型预测了一些错误的样本，即样本正类作为负类，反之亦然。重叠部分越小，模型的精确度越高。这里我们可以看到 AUC 是 0.7，而不是 1。</p><p id="2b49" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">案例 3:没有技能分类器</p><div class="nf ng nh ni gt ab cb"><figure class="ny is oh oa ob oc od paragraph-image"><img src="../Images/5b999920fa480c69ff729e05911bbd9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*rRw6O-Z9Pe5GNiik4z0rUA.png"/></figure><figure class="ny is oi oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/bbd50d81e449e997406a0dbc760fa0ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*uipMmsecy47X-FBBzVADTg.png"/></div></figure></div><p id="2a75" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是一个模型基本没学到东西的案例。所以，它一直在平等的基础上预测两个类。所以，一半时间它分类正确，一半时间它是错误的。</p><p id="1716" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">案例 4:</p><div class="nf ng nh ni gt ab cb"><figure class="ny is oj oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/561186a1801f0bc18cb27f37d9e3353d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*Sul_UQ6wky-ae41sA3pvCQ.png"/></div></figure><figure class="ny is ok oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/2464c2f2985b8337d69b93abdc2b6636.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*0YN0-i4P9KxQX8KYtrZcmQ.png"/></div></figure></div><p id="4f34" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这种情况下，该模型将所有正类分类为负类，将负类分类为正类。所以，它做了一个完全错误的分类。因此，AUC 值为 0，准确率为 0%</p><p id="05b5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是 ROC-AUC 曲线。让我们转向精确回忆。</p><h2 id="48d9" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">精确召回</h2><p id="7bc9" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">这些度量在用于评估二元分类模型的应用机器学习中也是有用的。</p><p id="720d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">精度由下式给出:</p><blockquote class="mk ml mm"><p id="1b1c" class="kv kw mn kx b ky kz kh la lb lc kk ld mo lf lg lh mp lj lk ll mq ln lo lp lq ij bi translated">精度=真阳性/(真阳性+假阳性)</p></blockquote><p id="acd6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">召回由以下人员发出:</p><blockquote class="mk ml mm"><p id="28f8" class="kv kw mn kx b ky kz kh la lb lc kk ld mo lf lg lh mp lj lk ll mq ln lo lp lq ij bi translated">召回= TP / (TP + FN)</p></blockquote><p id="dfed" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这两个指标在不平衡数据集的情况下非常有用。</p><p id="e88d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果数据集有太多的正类实例而几乎没有负类实例，那么模型将倾向于预测正类，因此假阳性更多，因此精度低。如果数据集有太多的负类实例而几乎没有正类实例，那么模型将倾向于预测负类，因此会有更多的假阴性，因此召回率低。</p><p id="81ba" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，在这两种情况下，这些指标可以给我们一个非常清晰的概念。让我们看看精确回忆曲线。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/71cc5b4ce393da1328872f16005b3083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t37xKeRd5NaeKclSGJcAfQ.png"/></div></div></figure><p id="cef1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是精确回忆曲线的样子。精确回忆不考虑真正的否定。所以只关注正面类。现在，这些图表有一个无技能分类器基线作为参考。在 0.5 处是一条直线。曲线是在其基础上测量的，就像如果模型分类最好，则它是一个凸形的并且远离基线，否则它更接近。</p><p id="077e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们已经研究了不同类型的性能矩阵。所以让我们在乳腺癌数据集上实现它。</p><p id="6a8b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我在这里使用了逻辑回归。</p><p id="80b8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据集的逻辑回归结果。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="3dc8" class="lr ls jg nk b gy no np l nq nr">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)<br/>from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>scale=scaler.fit(X_train)<br/>X_train_scaled=scale.transform(X_train)<br/>from sklearn.linear_model import LogisticRegression<br/>model = LogisticRegression(solver='liblinear', random_state=0)<br/>model.fit(X_train_scaled, y_train)</span></pre><p id="5617" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，这里的数据适合我们的模型，模型准备好进行预测。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="fdd7" class="lr ls jg nk b gy no np l nq nr">x_test_scaled=scale.transform(X_test)<br/>y_pred=model.predict(x_test_scaled)<br/>from sklearn.metrics import accuracy_score<br/>accuracy_score(y_test, y_pred)</span></pre><p id="f35f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的模型给出了 97%的准确率。这是一个非常高的精度，这意味着我们的主成分分析足够好。</p><p id="b72f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh"> ROC-AUC 曲线</strong></p><p id="2a76" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们获得 ROC- AUC 曲线:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="ee01" class="lr ls jg nk b gy no np l nq nr">from sklearn.metrics import roc_curve<br/>from sklearn.metrics import roc_auc_score<br/>import numpy as np<br/>i=0<br/>y_pred_i=[]<br/>y_test_i=[]<br/>while i&lt;len(y_pred):<br/>    if y_pred[i]=='M':<br/>        y_pred_i.append([1])<br/>    else:<br/>        y_pred_i.append([0])<br/>    i+=1<br/>y_pred_n=np.array(y_pred_i)<br/>i=0<br/>while i&lt;len(y_pred):<br/>    if y_test[i]=='M':<br/>        y_test_i.append([1])<br/>    else:<br/>        y_test_i.append([0])<br/>    i+=1<br/>y_test_n=np.array(y_test_i)</span><span id="435d" class="lr ls jg nk b gy nv np l nq nr">roc_auc_score(y_test_n, y_pred_n)</span></pre><p id="b0c2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">ROC 曲线下的 ROC AUC 分数或面积为 0.98</p><p id="bdc9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们检查图表。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="baaa" class="lr ls jg nk b gy no np l nq nr">from sklearn import metrics<br/>import matplotlib.pyplot as plt<br/>fpr, tpr, _ = metrics.roc_curve(y_test_n,  y_pred_n)<br/>auc = metrics.roc_auc_score(y_test_n, y_pred_n)<br/>plt.plot(fpr,tpr,label="breast_cancer, auc="+str(auc))<br/>plt.legend(loc=4)<br/>plt.show()</span></pre><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi om"><img src="../Images/20345fbd2dfd694fbcb976d4f75cea2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*zp2DlRJox39COodtGUbUaw.png"/></div></figure><p id="4255" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是我们得到的 ROC 曲线。</p><p id="3e0c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们来看看精确回忆曲线。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="64ae" class="lr ls jg nk b gy no np l nq nr">from sklearn.metrics import precision_recall_curve<br/>lr_precision, lr_recall, _ = precision_recall_curve(y_test_n, y_pred_n)<br/>plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')<br/>plt.xlabel('Recall')<br/>plt.ylabel('Precision')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e2474de585b5243d69b8c28f614f2224.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*6mlkFjEGm2cgol8mPgHa8Q.png"/></div></figure><p id="a00e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是获得的精度-召回曲线。正如我们所看到的，它离基线很远。因此，我们的数据集非常好，精度令人满意。</p><h2 id="ea7b" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">结论</h2><p id="62b6" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">在本文中，我们已经看到了 PCA、特征选择方法和性能度量。这里是<a class="ae jd" href="https://github.com/abr-98/EDA-and-PCA-demo" rel="noopener ugc nofollow" target="_blank"> Github 链接</a>。希望这些文章有所帮助。</p></div></div>    
</body>
</html>