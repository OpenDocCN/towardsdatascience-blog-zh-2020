<html>
<head>
<title>Everything you need to know about Min-Max normalization: A Python tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于最小-最大归一化，您需要知道的一切:Python 教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79?source=collection_archive---------2-----------------------#2020-05-28">https://towardsdatascience.com/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79?source=collection_archive---------2-----------------------#2020-05-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dbda" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这篇文章中，我解释了什么是最小-最大缩放，何时使用它，以及如何使用 scikit-learn 在 Python 中实现它，但也可以从<strong class="ak">T5】scratch</strong>手动实现。</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/6b2c2b736312a147bdd173616f4640d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*44jKK-vMeP4EGGvyIXPypg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者用 Python 创建的图。</p></figure><h1 id="6e99" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="8d78" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这是我关于在机器学习(ML)模型拟合之前经常使用的<strong class="lt iu">标准化</strong>技术的第二篇文章。在我的第一篇<a class="ae mn" href="https://medium.com/@seralouk/how-scikit-learns-standardscaler-works-996926c2c832" rel="noopener">文章</a>中，我使用 scikit-learn 的<code class="fe mo mp mq mr b">StandardScaler</code>函数介绍了标准化技术。如果您不熟悉标准化技术，只需点击<a class="ae mn" rel="noopener" target="_blank" href="/how-scikit-learns-standardscaler-works-996926c2c832">这里</a>即可在 3 分钟内学会要领。</p><p id="85bd" class="pw-post-body-paragraph lr ls it lt b lu ms ju lw lx mt jx lz ma mu mc md me mv mg mh mi mw mk ml mm im bi translated">在<strong class="lt iu">现在的</strong>帖子中，我将使用 scikit-learn(函数名:<code class="fe mo mp mq mr b"><a class="ae mn" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" rel="noopener ugc nofollow" target="_blank">MinMaxScaler</a></code>)解释第二个最著名的<strong class="lt iu">归一化</strong>方法，即<strong class="lt iu"> <em class="mx">最小-最大缩放</em> </strong>。</p><h1 id="1202" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">方法的核心</h1><p id="b29f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">另一种标准化输入特征/变量的方法是<strong class="lt iu">最小-最大</strong>缩放器(除了<em class="mx">标准化</em>缩放特征使其具有<code class="fe mo mp mq mr b">μ=0</code>和<code class="fe mo mp mq mr b">σ=1</code>)。通过这样做，所有特征将被转换到<strong class="lt iu">范围</strong><strong class="lt iu">【0，1】</strong>中，这意味着<strong class="lt iu">特征</strong> / <strong class="lt iu">变量</strong>的<strong class="lt iu">最小值</strong>和<strong class="lt iu">最大值</strong>将分别为<strong class="lt iu"> 0 </strong>和<strong class="lt iu"> 1 </strong>。</p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><p id="37e2" class="pw-post-body-paragraph lr ls it lt b lu ms ju lw lx mt jx lz ma mu mc md me mv mg mh mi mw mk ml mm im bi translated">如果你想在交互式路线图和活跃的学习社区的支持下自学数据科学，看看这个资源:【https://aigents.co/learn<a class="ae mn" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank"/></p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="b638" class="kz la it bd lb lc nf le lf lg ng li lj jz nh ka ll kc ni kd ln kf nj kg lp lq bi translated">为什么要在模型拟合之前进行归一化？</h1><p id="5c9e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">规范化/标准化背后的主要思想总是相同的。在<strong class="lt iu">不同的</strong><strong class="lt iu"/><strong class="lt iu">尺度下<strong class="lt iu">测量的<strong class="lt iu">变量</strong>对模型拟合</strong> &amp;模型学习函数的贡献不相等，可能最终产生<strong class="lt iu">偏差</strong>。因此，为了处理这个潜在的问题，通常在模型拟合之前使用特征标准化，例如<strong class="lt iu">最小最大</strong>缩放。</strong></p><p id="3da6" class="pw-post-body-paragraph lr ls it lt b lu ms ju lw lx mt jx lz ma mu mc md me mv mg mh mi mw mk ml mm im bi translated"><em class="mx">这个可以</em> <strong class="lt iu"> <em class="mx">非常有用</em> </strong> <em class="mx">对于一些 ML 模型像多层感知器(</em><strong class="lt iu"><em class="mx">【MLP】</em></strong><em class="mx">)，其中</em><strong class="lt iu"><em class="mx"/></strong><em class="mx">可以更</em><strong class="lt iu"><em class="mx"/></strong><em class="mx">甚至</em> <strong class="lt iu"> <em class="mx">更快</em></strong></p><p id="a1a2" class="pw-post-body-paragraph lr ls it lt b lu ms ju lw lx mt jx lz ma mu mc md me mv mg mh mi mw mk ml mm im bi translated"><strong class="lt iu"> <em class="mx">注</em> </strong>:基于树的模型通常不依赖于缩放，但非树模型模型如 SVM、LDA 等。往往非常依赖它。</p><h1 id="6a07" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">数学公式</h1><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/c8bdcbcc8b792d5f1fd4a7485ecb42bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*eO8b-xIGhnLgsQol3O6ksw.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">最小-最大缩放的数学公式。作者创造的形象。这里，x 表示单个特征/变量向量。</p></figure><h1 id="8c3a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Python 工作示例</h1><p id="4746" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这里我们将使用通过 scikit-learn 获得的著名的<code class="fe mo mp mq mr b">iris</code>数据集。</p><p id="f5ce" class="pw-post-body-paragraph lr ls it lt b lu ms ju lw lx mt jx lz ma mu mc md me mv mg mh mi mw mk ml mm im bi translated"><strong class="lt iu"> <em class="mx">提醒</em> </strong> : scikit-learn 函数期望输入一个 numpy 数组<code class="fe mo mp mq mr b">X</code>，其维数为<code class="fe mo mp mq mr b">[samples, features/variables]</code>。</p><pre class="kk kl km kn gt nl mr nm nn aw no bi"><span id="6bd2" class="np la it mr b gy nq nr l ns nt">from sklearn.datasets import load_iris<br/>from sklearn.preprocessing import MinMaxScaler<br/>import numpy as np</span><span id="fe8e" class="np la it mr b gy nu nr l ns nt"># use the iris dataset<br/>X, y = load_iris(return_X_y=True)<br/>print(X.shape)<br/># (150, 4) # 150 samples (rows) with 4 features/variables (columns)</span><span id="e10a" class="np la it mr b gy nu nr l ns nt"># build the scaler model<br/>scaler = MinMaxScaler()</span><span id="3988" class="np la it mr b gy nu nr l ns nt"># fit using the train set<br/>scaler.fit(X)</span><span id="2cc4" class="np la it mr b gy nu nr l ns nt"># transform the test test<br/>X_scaled = scaler.transform(X)</span><span id="6db8" class="np la it mr b gy nu nr l ns nt"># Verify minimum value of all features<br/>X_scaled.min(axis=0)<br/># array([0., 0., 0., 0.])</span><span id="f3d3" class="np la it mr b gy nu nr l ns nt"># Verify maximum value of all features<br/>X_scaled.max(axis=0)<br/># array([1., 1., 1., 1.])</span><span id="fa41" class="np la it mr b gy nu nr l ns nt"># Manually normalise without using scikit-learn<br/>X_manual_scaled = (X — X.min(axis=0)) / (X.max(axis=0) — X.min(axis=0))</span><span id="1087" class="np la it mr b gy nu nr l ns nt"># Verify manually VS scikit-learn estimation<br/>print(np.allclose(X_scaled, X_manual_scaled))<br/>#True</span></pre><h1 id="f1b7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">视觉示例中的变换效果</h1><pre class="kk kl km kn gt nl mr nm nn aw no bi"><span id="7bd2" class="np la it mr b gy nq nr l ns nt">import matplotlib.pyplot as plt</span><span id="fb78" class="np la it mr b gy nu nr l ns nt">fig, axes = plt.subplots(1,2)</span><span id="2483" class="np la it mr b gy nu nr l ns nt">axes[0].scatter(X[:,0], X[:,1], c=y)<br/>axes[0].set_title("Original data")</span><span id="1f39" class="np la it mr b gy nu nr l ns nt">axes[1].scatter(X_scaled[:,0], X_scaled[:,1], c=y)<br/>axes[1].set_title("MinMax scaled data")</span><span id="6f54" class="np la it mr b gy nu nr l ns nt">plt.show()</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nv"><img src="../Images/2011b61271ca2ee44203a0e5065a4850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EvtTPZ7BtvvIlt4muuBeEA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">Iris 数据集前两个特征的最小最大缩放效果。图由作者用 Python 制作。</p></figure><p id="9c50" class="pw-post-body-paragraph lr ls it lt b lu ms ju lw lx mt jx lz ma mu mc md me mv mg mh mi mw mk ml mm im bi translated">很明显，在<strong class="lt iu">最小-最大缩放</strong>(右图)之后，特性值在和<strong class="lt iu">范围【0，1】</strong>内<strong class="lt iu">。</strong></p><h1 id="1bdd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">scikit-learn 网站的另一个可视化示例</h1><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nw"><img src="../Images/28da93900f406b19f0d633c6ea4762bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GkLMjpUYVYWQdy69psVbEw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">最小最大缩放效果。图摘自 scikit-learn <a class="ae mn" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html" rel="noopener ugc nofollow" target="_blank">文档</a>:<a class="ae mn" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn . org/stable/auto _ examples/preprocessing/plot _ all _ scaling . html</a></p></figure><h1 id="50b1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><ul class=""><li id="515d" class="nx ny it lt b lu lv lx ly ma nz me oa mi ob mm oc od oe of bi translated">使用最小最大值缩放时要记住的一件重要事情是，它受到我们数据中最大值和最小值的<strong class="lt iu">高度影响</strong>，因此如果我们的数据包含<strong class="lt iu">异常值</strong>，它就会有<strong class="lt iu">偏差</strong>。</li><li id="e4f7" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><code class="fe mo mp mq mr b">MinMaxScaler</code>重新调整数据集，使所有特征值都在范围[0，1]内。<strong class="lt iu">这是以独立的方式按功能完成的</strong>。</li><li id="69f9" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><code class="fe mo mp mq mr b">MinMaxScaler</code>缩放可能<strong class="lt iu">在一个<strong class="lt iu">窄的</strong>范围内压缩</strong>所有<strong class="lt iu">内联者</strong>。</li></ul><h1 id="362c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">如何处理异常值</h1><ul class=""><li id="6b13" class="nx ny it lt b lu lv lx ly ma nz me oa mi ob mm oc od oe of bi translated">手动方式(不推荐):目视检查数据，并使用异常值剔除统计方法剔除异常值。</li><li id="ccc9" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated">推荐方法:使用<code class="fe mo mp mq mr b"><a class="ae mn" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html" rel="noopener ugc nofollow" target="_blank">RobustScaler</a></code>来缩放特征，但在这种情况下，使用对异常值稳健的<strong class="lt iu">统计。该缩放器根据<strong class="lt iu">分位数</strong>范围<strong class="lt iu">范围</strong>(默认为<strong class="lt iu"> IQR </strong>:四分位间距)移除<strong class="lt iu">中值</strong>和<strong class="lt iu">缩放</strong>数据。<em class="mx">IQR 是第一个四分位数(第 25 个四分位数)和第三个四分位数(第 75 个四分位数)之间的范围。</em></strong></li></ul><p id="3d62" class="pw-post-body-paragraph lr ls it lt b lu ms ju lw lx mt jx lz ma mu mc md me mv mg mh mi mw mk ml mm im bi translated">今天就到这里吧！希望你喜欢这第一个帖子！下一个故事下周开始。敬请关注&amp;注意安全。</p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><blockquote class="ol om on"><p id="e953" class="lr ls mx lt b lu ms ju lw lx mt jx lz oo mu mc md op mv mg mh oq mw mk ml mm im bi translated">——我的邮件列表只需 5 秒:<a class="ae mn" href="https://seralouk.medium.com/subscribe" rel="noopener">https://seralouk.medium.com/subscribe</a></p><p id="d38d" class="lr ls mx lt b lu ms ju lw lx mt jx lz oo mu mc md op mv mg mh oq mw mk ml mm im bi translated">-成为会员支持我:<a class="ae mn" href="https://seralouk.medium.com/membership" rel="noopener">https://seralouk.medium.com/membership</a></p></blockquote></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="293f" class="kz la it bd lb lc nf le lf lg ng li lj jz nh ka ll kc ni kd ln kf nj kg lp lq bi translated"><strong class="ak">最新帖子</strong></h1><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/roc-curve-explained-using-a-covid-19-hypothetical-example-binary-multi-class-classification-bab188ea869c"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">用新冠肺炎假设的例子解释 ROC 曲线:二分类和多分类…</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">在这篇文章中，我清楚地解释了什么是 ROC 曲线以及如何阅读它。我用一个新冠肺炎的例子来说明我的观点，我…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi kt ou"/></div></div></a></div><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">支持向量机(SVM)解释清楚:分类问题的 python 教程…</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">在这篇文章中，我解释了支持向量机的核心，为什么以及如何使用它们。此外，我还展示了如何绘制支持…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pj l pf pg ph pd pi kt ou"/></div></div></a></div><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">PCA 清楚地解释了——如何、何时、为什么使用它以及特性的重要性:Python 指南</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">在这篇文章中，我解释了什么是 PCA，何时以及为什么使用它，以及如何使用 scikit-learn 在 Python 中实现它。还有…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pk l pf pg ph pd pi kt ou"/></div></div></a></div><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/how-and-why-to-standardize-your-data-996926c2c832"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">Scikit-Learn 的标准定标器如何工作</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">在这篇文章中，我将解释为什么以及如何使用 scikit-learn 应用标准化</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pl l pf pg ph pd pi kt ou"/></div></div></a></div></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="da3d" class="kz la it bd lb lc nf le lf lg ng li lj jz nh ka ll kc ni kd ln kf nj kg lp lq bi translated">请继续关注并支持我</h1><p id="c2ea" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果你喜欢这篇文章并且觉得它有用，请<strong class="lt iu">关注</strong>我和<strong class="lt iu">为</strong>我的故事鼓掌支持我！</p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="05da" class="kz la it bd lb lc nf le lf lg ng li lj jz nh ka ll kc ni kd ln kf nj kg lp lq bi translated">资源</h1><p id="21b9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这里并排查看所有 scikit-learn 规范化方法:<a class="ae mn" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/preprocessing/plot _ all _ scaling . html</a></p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="0daa" class="kz la it bd lb lc nf le lf lg ng li lj jz nh ka ll kc ni kd ln kf nj kg lp lq bi translated">参考</h1><p id="4ed4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1]<a class="ae mn" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . preprocessing . minmax scaler . html</a></p><p id="38b6" class="pw-post-body-paragraph lr ls it lt b lu ms ju lw lx mt jx lz ma mu mc md me mv mg mh mi mw mk ml mm im bi translated">[2]<a class="ae mn" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/preprocessing/plot _ all _ scaling . html</a></p><p id="4448" class="pw-post-body-paragraph lr ls it lt b lu ms ju lw lx mt jx lz ma mu mc md me mv mg mh mi mw mk ml mm im bi translated">[3]<a class="ae mn" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . preprocessing . robust scaler . html</a></p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="aea1" class="kz la it bd lb lc nf le lf lg ng li lj jz nh ka ll kc ni kd ln kf nj kg lp lq bi translated">和我联系</h1><ul class=""><li id="ad5f" class="nx ny it lt b lu lv lx ly ma nz me oa mi ob mm oc od oe of bi translated"><strong class="lt iu">领英</strong>:<a class="ae mn" href="https://www.linkedin.com/in/serafeim-loukas/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/serafeim-loukas/</a></li><li id="fd43" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><strong class="lt iu">研究之门</strong>:<a class="ae mn" href="https://www.researchgate.net/profile/Serafeim_Loukas" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/profile/Serafeim_Loukas</a></li><li id="5584" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><strong class="lt iu">EPFL</strong>T22】简介:<a class="ae mn" href="https://people.epfl.ch/serafeim.loukas" rel="noopener ugc nofollow" target="_blank">https://people.epfl.ch/serafeim.loukas</a></li><li id="b61c" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><strong class="lt iu">堆栈</strong>溢出:<a class="ae mn" href="https://stackoverflow.com/users/5025009/seralouk" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/users/5025009/seralouk</a></li></ul></div></div>    
</body>
</html>