# 构建强化学习政策的轴心

> 原文：<https://towardsdatascience.com/constructing-axes-for-reinforcement-learning-policy-1dc727415ad9?source=collection_archive---------24----------------------->

## 向研究界最不透明的框架迈了一小步。

强化学习(RL)是一种与环境交互的框架，用于学习策略以采取行动并实现某些目标，在许多领域都有所发展。有些领域是有意义的，有些永远不会起作用，而大多数注定会落在中间的某个地方。RL 之所以如此受欢迎，是因为它的优雅: ***我们知道，大多数生物都是通过与它们的环境*** 互动来学习的(但是，我们应该注意到，进化实际上是对这些我们不知道如何建模的行为的强先验)。问题是 RL 在个人和群体层面上对我们的社会产生了有害的影响(见我写的关于[推荐系统](https://democraticrobots.substack.com/p/recommendations-are-a-game-a-dangerous))——这不是一篇描述这些问题的文章，而是在取笑我们如何以更精确的方式解决它们。

然而，推荐系统极大地提醒了我们，将社会建模为强化学习问题是多么困难(有时甚至是倒退)。从工程师的角度来看，推荐系统是一个 RL 循环，其中消费者(我们)是环境，应用程序是代理，金钱是回报。令人不安的是，科技公司将探索你的内在工作方式，作为他们研究的环境。工程问题最终变成了设计师/用户和代理人之间的*游戏*——RL 传统上对行为的调整和突然、意想不到的变化非常敏感。在列举了几个现实世界的 RL 的例子之后，我提出了三个初始轴来开始关于 RL 策略的讨论:

1.  模拟目标动态的能力，
2.  目标动力学抽象的封闭性，
3.  关于目标动态的现有规定。

我参加了一个阅读小组，该小组由伯克利的毕业生组成，他们在计算和工程领域获得了参与和扩展奖学金([googes](https://geesegraduates.org/blog/))和长期网络安全中心( [CLTC](https://cltc.berkeley.edu/) )，希望最终能就这个主题发表一份白皮书(感谢合作者)。

![](img/e6359c74cef7d8295f7d3eedc907fcfd.png)

照片由[**Kendall Hoopes**](https://www.pexels.com/@ken123films?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)从 [**Pexels**](https://www.pexels.com/photo/architecture-building-capitol-dawn-616852/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)

# 真实世界 RL:现在和未来

RL 正以积极的方式在现实世界中使用，我乐观地认为它将在比最初预期更多的领域发挥作用。这里有几个例子可以让你开始思考:

## 示例:电网

电力系统已经被证明是非常易于强化学习的。它可以用于管理网络负载，计划不同工厂的生产，等等。许多论文利用动力学和故障保险的精确模拟，以便学习过程不会导致任何奇怪的行为。在电气工程中研究功率动力学的历史为新的基于学习的技术提供了明确的界限和故障保险。电网背后的这些主题就是为什么一些大型技术公司已经将 RL 应用于数据中心运营的优化。该系统在其抽象中是封闭的，具有精确的建模动态和合理的建模目标。【参见 [1](https://livrepository.liverpool.ac.uk/3034527/1/RevisedManuscript_ReinforcementLearning.pdf) 、 [2](https://ieeexplore.ieee.org/abstract/document/8834806) 、 [3](https://www.sciencedirect.com/science/article/pii/S0378779620304193)

## 示例:自动交易

自动化交易可能是 RL 最符合逻辑的问题空间——奖励功能就是赚钱，不停赚钱。行动是购买和销售，但如何在底层环境上放置一个边界框—它不仅仅是将资金转移到所有最终用户的银行账户。另一个问题是，行动空间是如此多样和难以约束(约束是由人类过去犯下的错误设定的，以及[一闪而过的崩溃](https://en.wikipedia.org/wiki/2010_flash_crash))，动态非常未知。不确定性并没有阻止人们在这个领域赚很多钱，但我不认为我们已经结束了股票市场似乎脱离现实的闪电崩盘和其他时期。[这里有一篇关于](https://dennybritz.com/blog/ai-trading)领域的好博文。我认为人们将继续在这一领域赚钱，除非真的出现问题，否则我们不会看到太多的监管，货币市场到最终用户的动态将无法捕捉(人们不总是喜欢谈论他们的钱在哪里)。

## 例如:医疗

用于医疗的 RL 很可能看起来像一台计算机，它计算出如何制造新药或管理以前未研究过的组合(在离线时尚中听起来不错)，但它也可以采取多种形式，并故意模糊(突出挑战)。当前的医疗政策可以为未来的数据驱动的医疗政策提供信息，但这可能过于乐观了。具体来说，我提出医学治疗，因为 RL 在医学之外的应用领域有很多东西需要学习。医学临床试验将任何新方法与当前的护理标准进行比较。在 RL 中实现这一标准将是从当前以人为中心的医疗系统(或其他流程)到数据驱动系统的转变不会降低个人层面的性能(即使给某人送一台计算机更便宜)。这种性能比较应该在不考虑可及性的情况下进行(人类医生更昂贵，因为他们稀缺),并防止过早采用计算机医生。如果你对数据医学是否准备好了感兴趣，[这篇博客调查了](https://www.nabla.com/blog/gpt-3/)Reddit-corpus GPT 3 对医学思想的直觉，结果并不理想。

# RL 策略的轴

这是我第一次草拟 RL 政策。这样的政策*不应该*禁止在这些轴之一的极端区域研究系统中的 RL，但是它应该要求额外的监督或更好的数据实践。减缓 RL 反馈循环的一个令人兴奋的领域是将一些研究领域限制为离线 RL——将记录的数据提取到策略中并偶尔更新而不是持续更新的过程。

## 轴 1:对目标代理建模的能力

考虑从电力系统到医疗的不同:我们知道[麦克斯韦方程](https://en.wikipedia.org/wiki/Maxwell%27s_equations)以及它们在工程系统中如何工作，但我们不知道人体(相对而言)。对电力系统背后的物理进行建模，让机器学习工程师受到物理现实的限制——我们知道何时某些动作会关闭电源，并且它们会从动作空间中移除(尽管这确实会使学习问题稍微困难一些)。

这个界限确保我们不会对我们所控制的系统的底层造成伤害。

## 轴 2:目标端点处抽象的准确性

我对现实世界中的强化学习最大的不满是，当目标终端用户，例如社交网络中的新闻源，显然不仅仅是单个用户。这个轴看起来和上面很像，但是我故意把它分开。例如，运行成本更高的数据中心和电网的潜在危害是非常不同的(尽管我们不一定知道谷歌倒闭实际上会造成多大的危害)。这条轴线还可以进一步细化，但我认为一些尺度感是重要的。我们可以将 RL 应用于我们的智能冰箱，但可能不会应用于食品配送基础设施。

这个框让我们考虑控制算法界限之外的潜在损害的规模。

## 轴 3:脚手架的现有规定

这个医学例子强调了现有的规范性行为和法规帮助定义问题空间的潜力。电力系统在这一点上处于中间位置:作为一家公用事业公司有助于约束一些行为。其他例子就更离谱了。RL 政策和其他技术诉讼应该依靠这个现有的脚手架。我知道食品药品监督管理局(FDA)和其他政府机构是通过修补历史错误而建立的，我认为这是一个难以前进的趋势，但我支持建立一个数据和算法管理局。

这个轴让我们记住，当环境和代理是相对自由的系统(咳咳，社交媒体)时，应用 RL 会有更多的潜在危害。

## 示例:运输

谷歌地图算法可能会被 RL 改进(也许他们是？让我知道)。运输和路线有很好的抽象、奖励和动态。首先，考虑一下医药与交通:医药是一个更受监管、更正式的规范结构。人们很高兴将 RL 应用于交通运输——交通运输对于提取奖励功能来说已经成熟，并且比药物等具有更规范内容的东西更容易。规范性内容与人们想要保持“人性”的强烈程度相关。交通是一个例子，其中大规模的 RL 可以转化为复杂的多智能体交互和意外问题(例如，自动驾驶汽车使行人无法使用通常不行驶的道路)。

运输我会把安全放在轴 1 上(本质上是遍历一个图)，中等放在轴 2 上(不确定自主路由会如何进行，因为它是一个如此庞大的系统)，安全放在轴 3 上(我们已经有很多汽车要遵循的规则)。

# 大规模 RL 成为多代理 RL

多主体强化学习(MARL)可以以多种方式定义，但它是带有个体微调的大规模强化学习的发展方向。讨论一些记录在案的 MARL 关键挑战是很有意义的(参见[概述](https://arxiv.org/abs/1911.10635)):

1.  **非唯一学习目标**:每个智能体都有不同的目标，我们如何优化全局？
2.  **非平稳性**:数据分布在每一步都会发生难以置信的变化，因为个体不再控制一切。
3.  **可扩展性问题**:如果智能体考虑到附近每个智能体的行为，复杂性会呈指数级增长。
4.  **变化的信息结构**:代理和环境数据在代理之间是异构的(我认为这是最接近解决的一个)。

我有兴趣通过合作与竞争游戏(只有一些是零和游戏)、顺序与并行 MDP、集中与去集中控制等等来阐述我对社交媒体世界的看法。从研究的角度来看，我对 MARL 越来越感兴趣，因为这是大多数基于应用的算法正在对我们做的事情。MARL 目前的共识是，几乎任何复杂的奖励函数都是难以处理的，所以这是所有科技公司决定进行的实验！

最后那句话是故意很多，为了某种戏剧而翻转了主宾顺序。但是，看起来确实是这样的:科技公司每天决定着我们越来越多的互动，尽管它们在优化利润，但每个个体代理人(我们)都有我们的奖励功能。下游效应正在显现。

我相信我会在未来更多地触及这一点，就像我过去在推荐系统的帖子中所做的那样。这个选举季更加强调我在社区和公益建筑技术方面的工作需求，也许我应该从改善心理健康和长期导向的社交网络开始。

我们都需要更多的正能量和更多的讨论。

*这是我关于机器人学&自动化、* [*民主化自动化*](http://robotic.substack.com) *的免费时事通讯的一个高峰。*

[](https://robotic.substack.com/) [## 自动化大众化

### 一个关于机器人和人工智能的博客，让它们对每个人都有益，以及即将到来的自动化浪潮…

robotic.substack.com](https://robotic.substack.com/)