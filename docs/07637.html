<html>
<head>
<title>Manually computing the coefficients for an OLS regression using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 手动计算 OLS 回归的系数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/manually-computing-coefficients-for-an-ols-regression-using-python-50d8e413de?source=collection_archive---------34-----------------------#2020-06-08">https://towardsdatascience.com/manually-computing-coefficients-for-an-ols-regression-using-python-50d8e413de?source=collection_archive---------34-----------------------#2020-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8ce8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这是一个关于如何在不使用任何统计软件包的情况下计算 OLS 回归系数的简单演练(带有代码示例)。</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/bf7bd3ff779e884fdafbf72c9c8c1518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Q1Dh3LCpllo7HlDm1PNPw.png"/></div></div></figure><p id="16db" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我写这篇教程有两个原因。前阵子，我在想<em class="lr">背后到底藏着什么。飞腾()</em>和<em class="lr">。像<a class="ae ls" href="https://scikit-learn.org/stable/user_guide.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>、<a class="ae ls" href="https://www.statsmodels.org/stable/user-guide.html" rel="noopener ugc nofollow" target="_blank"> statsmodels </a>和无数其他库这样的流行统计软件包的 predict() </em>方法。在谷歌上花了相当多的时间后，我没有找到足够简单的教程，不得不自己钻研最近的统计学书籍。第二，虽然我使用 Medium 已经有一段时间了，但这是我第一次在这里写文章，我想看看它会是什么样子😀。</p><p id="8897" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我将首先简要解释回归的逻辑——如果您熟悉基本概念，可以跳过这一部分。了解普通最小二乘(OLS)回归背后的理论和基本代数有助于解释结果以及理解模型的局限性。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="5a4a" class="ma mb it bd mc md me dn mf mg mh dp mi le mj mk ml li mm mn mo lm mp mq mr ms bi translated">什么是 OLS 回归？</h2><p id="0269" class="pw-post-body-paragraph kv kw it kx b ky mt ju la lb mu jx ld le mv lg lh li mw lk ll lm mx lo lp lq im bi translated">回归是一种研究因变量和(一个或多个)自变量之间关系的方法。在一个简单的双轴图表上，单变量 OLS 回归将基本上试图通过我们观察到的数据点绘制一条“最佳拟合线”;多元回归更难想象。但是，不管预测变量的数量是多少，OLS 回归都试图找到方程的系数(通常由字母<em class="lr"> β </em>表示):</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi my"><img src="../Images/e241e989fee69d8f5c08663bcc6d7cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qbn5wcLM9Z6AZZNClxGHdw.png"/></div></div></figure><blockquote class="mz na nb"><p id="1d06" class="kv kw lr kx b ky kz ju la lb lc jx ld nc lf lg lh nd lj lk ll ne ln lo lp lq im bi translated"><em class="it">让我们快速定义我们将使用的符号:</em></p><p id="9118" class="kv kw lr kx b ky kz ju la lb lc jx ld nc lf lg lh nd lj lk ll ne ln lo lp lq im bi translated">Y —因变量<br/> X —自变量<br/> <em class="it"> β </em> — OLS 系数<br/> <em class="it"> ϵ — </em>误差项<br/> k —观测值数量<br/> n —预测值数量</p></blockquote><p id="5c9e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">误差项<em class="lr"> ϵ </em>代表我们(通常)无法完美拟合数据的事实，在我们的模型估计和数据的真实值之间会有“误差”。最小二乘回归的目标是最小化这些平方误差(因此得名):</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nf"><img src="../Images/8bc66f3eb29a438bc84a22d53718f068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XOYLc40TLksQGJJt60Y5w.png"/></div></div></figure><p id="b40a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这些系数是如何计算的？</p><p id="3fab" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">OLS 回归的美妙之处在于，通过以矩阵形式重写上述方程并求一阶导数，可以在数值上获得最佳系数:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nf"><img src="../Images/b1ee76c5ee4c4035482aba5ae342f728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0rcMWHFJk3pAvRk30_hVrA.png"/></div></div></figure><p id="ccf0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">一阶条件给出:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ng"><img src="../Images/08327ad6360ffba6e7f7b916db18fd91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6a3HyMvbe3cRB9rSRKwRNg.png"/></div></div></figure><p id="de09" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">正如我们所见，OLS 系数本身是<em class="lr"> X </em>和<em class="lr">y</em>的线性组合</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="6f0e" class="ma mb it bd mc md me dn mf mg mh dp mi le mj mk ml li mm mn mo lm mp mq mr ms bi translated">Python 实现</h2><p id="e2fc" class="pw-post-body-paragraph kv kw it kx b ky mt ju la lb mu jx ld le mv lg lh li mw lk ll lm mx lo lp lq im bi translated">现在，说到文章的重点。为了与常用的包保持一致，我们将编写两个方法:<em class="lr">。合体()</em>和<em class="lr">。预测()。我们的数据操作将使用<em class="lr"> numpy </em>包进行。如果您从另一个文件导入数据，例如在<em class="lr">中。csv </em>格式，你可以使用<em class="lr"> pandas </em>库来这样做。</em></p><p id="4b49" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们导入模块:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="nh ni l"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">*如果您决定可视化预测，matplotlib 导入将会派上用场</p></figure><p id="f134" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">接下来，我们将为我们的模型创建一个类，并创建一个将 OLS 回归拟合到给定的<em class="lr"> x </em>和<em class="lr"> y </em>变量的方法——这些变量必须作为 numpy 数组传入。根据先前执行的向量形式推导来获得系数(<em class="lr"> np.linalg.inv() </em>是用于矩阵求逆的 numpy 函数，并且@符号表示向量乘法):</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="a9db" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们的<em class="lr">。fit() </em>方法将计算出的系数存储在<em class="lr">selfβas</em>属性中，以允许其他方法稍后访问它们。注意，我们增加了一个可选参数<em class="lr">截距</em>；如果我们决定用截距来拟合我们的模型，该方法将向独立变量的数组中添加一个向量 1。</p><p id="f678" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们现在创建<em class="lr">。</em>预测()(方法:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="nh ni l"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">*注意额外的缩进，因为这个方法是模型类的一部分</p></figure><p id="eac1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们(非常)简单的方法利用向量乘法来获得“预测”值— <em class="lr"> y_hat </em>。</p><p id="58f2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">作为一个额外的(可选的)接触，我们可以添加一个方法来可视化地输出预测(<em class="lr">注意</em>:在当前的形式中，它只对带有截距的单变量回归有效):</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="0eeb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后，让我们执行我们使用样本数据创建的方法(如果您不想生成图形，请删除第 7 行中对<em class="lr"> plot_predictions() </em>的调用，代之以在我们的。<em class="lr">预测()</em>方法):</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="b5bc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果您遵循了所有的步骤，您应该已经成功地计算出了系数，并且生成了类似于下面的内容:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nn"><img src="../Images/232cab378c89bff0d25539761972332f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YXXX28X6QjHeoEgY1wjIcQ.png"/></div></div></figure><p id="bc24" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果你想玩玩代码，完整版可以从 GitHub 库<a class="ae ls" href="https://github.com/rshemet/OLS_coeffs_walkthrough" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="10fe" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">不用说，这是一个非常基本的练习，尽管如此，它有效地说明了 OLS 贝塔来自哪里，以及它们的(数学)意义是什么。弄清楚这一点对我的进一步研究有很大帮助。最后，我前面提到的统计软件包，除了计算的系数之外，通常还会计算各种其他指标——统计显著性、置信区间、R 平方等等。这些都可以用数字来计算，但是一旦你理解了基本概念，我建议你依赖(经过良好测试的)库。</p></div></div>    
</body>
</html>