# 用深度强化学习玩宝藏游戏—第 2/3 部分

> 原文：<https://towardsdatascience.com/playing-treasure-drop-with-deep-reinforcement-learning-part-2-3-e7097478d402?source=collection_archive---------83----------------------->

## 艾玩拼图

![](img/0c79c6ac9f4a5c7761dea3c2db061859.png)

这是代理在与其他玩家在线对战时“看到”的内容。

在这个系列文章中，我解释了我从事的强化学习(RL)项目。最后，你会知道这是怎么回事:

[观看这个人工智能在实时益智游戏中击败人类玩家](https://youtu.be/FhhPJIZnJ1M)

我讨论了问题的独特和值得注意的方面，我使用了什么技术以及为什么。希望对深度强化学习感兴趣的读者会发现所有这些都很有见地。

在[第一部](https://medium.com/p/2eb789c2ff5e/edit)中，对*宝藏掉落*的游戏进行了说明，并对游戏的缩小版使用了表格 q-learning 方法。在获得了表格方法所需的验证和早期成功之后，这里实现了 Deep Q 网络。

# 摘要

在获得了表格方法所需的验证和早期成功之后，使用 Keras 库实现了 Deep Q 网络来解决与第 1 部分相同的问题。然而，由于项目的性质，学习过程变得不稳定。实现了几种方法来缓解该问题。即，优先经验重放、目标 Q 网络、具有学习率查找器的循环学习率。

**第 2/3 部分的 GitHub 存储库:**[TD-deepreinforcementlearning-Part 2](https://github.com/arapfaik/td-deepreinforcementlearning-part2)

# 表格 q-学习的局限性

在[第 1 部分](https://medium.com/p/2eb789c2ff5e/edit)中，q 表适用于尺寸为 3x2(最大可能为 4x2)的电路板。在 4x2 上，q-table 花了很长时间才有收敛值。这是因为在 4x2 中，状态空间具有 4⁹ = 2 个⁸状态。这相当于大约 26 万个州。这对于表格实现来说太多了。原始游戏桌是 4x5，2⁶⁰有不同的可能状态。这使得表格 q 学习对我们的问题不可行。

在许多有实际意义的情况下，包括这个例子，状态比一个表中的条目要多得多。在这些情况下，函数必须是近似的，使用某种更紧凑的参数化函数表示。

对此的解决方案是近似 q 值函数。猜猜什么是好的函数逼近器？是的，你答对了！👏👏**深度神经网络**！本文是关于使用深度神经网络来逼近 q 值函数。它们通常出现在强化学习环境中，被称为深度 Q 学习网络(DQN)。

# 从 Q 表到 DQN 的过渡

这很简单。代码中唯一的不同是在我们更新 q 表的那一行:

agentQTable 类中的 q 表更新，代理的表格实现。

相反，我们将把 td_target 和状态一起馈送给神经网络。理论上讲，下一次网络看到状态时，它会输出正确的 q 值。这种情况适用于较小的棋盘尺寸，但对于较大的棋盘尺寸和游戏的原始尺寸来说就不那么容易了。以下更新将取代上面的表格 q-learning 更新:

# 神经网络

## 体系结构

决定采用 60 x 300 x 300 x 300 x 300 x 300 x 300 x 8 的形状，尽管输入层和输出层因电路板尺寸而异。例如，对于 4x3 游戏，它们分别是 30 和 4。所有层都是完全连接的。

> 任务的理想网络架构必须通过验证集误差引导的实验来找到。
> 
> 古德费勒，本吉奥&库维尔律师事务所。[](https://www.amazon.com/Deep-Learning-NONE-Ian-Goodfellow-ebook/dp/B01MRVFGX4/ref=sr_1_3?dchild=1&keywords=deep+learning&qid=1587862365&sr=8-3)**(2016)**

**在线游戏期间，游戏控制器类从屏幕上收集状态。回合也是从屏幕上推断出来的。然后，将状态作为相应回合的输入馈入模型。输出是 q 值，每个对应于棋盘上的一个动作。具有最高 q 值的动作由代理挑选。**

**![](img/2c39059502c2a504893cef807e5ac66f.png)**

**模型和游戏之间的信息流动图。**

## **优化器和损耗**

**动量= 0.9 的随机梯度下降(SGD)用于所有模型。使用系数 0.5 对渐变进行剪裁。不剪裁梯度有时会导致爆炸结果，神经网络的权重最终是 NaN。**

**损失函数是行动的预测 q 值和贝尔曼方程计算值之间的 MSE。**

# **学习过程的稳定性**

**在这一点上，DQN 可以工作，但是只适用于非常小的电路板。它有时会收敛，有时不会。深度强化学习以稳定著称，因为这是当前的热门研究领域。关于不稳定最流行的术语是“致命三重奏”。如果在一个学习算法中存在特定的三个品质，就说学习算法有很大的机会出现分歧。**

**当使用具有强化学习的深度非线性函数空间时，关于发散是否普遍，以及致命的三元组是否是罪魁祸首，几乎没有指导。**

# **致命三和弦**

**在 DQN 方法中，我们做了三件值得注意的事情。这些是:**

1.  **函数逼近:我们通过深度神经网络逼近 q 值函数，而不是用数学方法制作实际函数，或者存储每个输入的输出值:**
2.  **Bootstrapping:我们部分基于其他估计来更新 q 值估计。我们从另一个猜测中学习一个猜测——我们*引导。***
3.  **政策外学习:当学习发生时，我们并不总是遵循政策。这是因为我们采取的是概率为ε(ε)的随机行动。**

**这三者的结合会产生有害的学习动力，导致函数参数的发散。由于这种分歧的可能性，这三者的结合被称为“致命三重奏”。**

**在为 3x2 游戏训练时，我注意到模型有时收敛，有时不收敛。由于某种原因，学习过程似乎不稳定。我归咎于致命三和弦，虽然还不清楚这是否是问题所在。尽管如此，学习过程具有所有这三个特点。本文的其余部分是关于为了使学习过程稳定而实现的内容。**

# **体验回放**

**在数据科学中，数据的底层分布决定了模型学习的内容。经验回放是一个非常非常流行的技术，只要 RL 和 DQN 走到一起就可以实现。用当前事务更新模型很酷，但是浪费数据。模型只看到一个数据。**

**与此相反，经验回放使用随机的先前交易样本来更新 Q 值。通过保存以前发生的交易的记忆，在每一步中随机抽样。这引入了两个新的超参数:内存大小(max_memory)和每次抽取的样本数量(batch_size)。**

**Mnih 等人(2015)为各种 Atari 游戏上的 DQN 任务将他们的内存缓冲区大小设置为 10⁶。⁴:以此作为起点是有意义的。在项目的后期，我意识到它会影响性能，所以将它调整到了一个较低的数值。至于批量大小，10 似乎最适合这个游戏。**

# **优先体验重放**

**体验重放将在线学习代理从按照他们体验的确切顺序处理过渡中解放出来。优先重放进一步解放了代理，使其不用考虑与他们经历的频率相同的转换。⁵**

**直觉上，有些交易比其他交易更重要。有可能以更明智的方式对交易进行抽样。现在的问题是，哪种交易比其他交易更重要？**

**代理商必须对那些大额交易更加敏感。这有双重优势。首先，代理可以感觉到高回报并采取适当的行动。第二，代理可以看到敌人是否可以在某个移动后获得高奖励，并可以通过限制自己的行动来防止让敌人获得奖励。**

**此时，我们只需要一个度量来相应地分配概率。**

**预测高标量值比预测低标量值有更多的 MSE 损失。这意味着，交易的回报越高，MSE 的损失也就越大。**

**因此，我们可以放心地将损失视为优先级的衡量标准。损失越大，交易越重要。每次从经验缓冲区中提取一个事务时，都会在其上运行一个预测。由预测导致的丢失被更新，并且事务被放回具有新的优先级度量的缓冲器中。**

**在训练迭代次数相同的情况下，经过优先体验回放训练的模型似乎比没有经过优先体验回放训练的模型更胜一筹。**

# **目标 Q 网络**

**这解决了我们学习过程的自举质量。这不是一个万全之策，因为自举仍然存在。然而，它已被证明有助于各种各样的强化学习任务，包括 DQN。**

**该逻辑只是从与被更新的网络不同的网络中引导。换句话说，有两个网络，互相复制。估计值来自一个网络，值在另一个网络(称为目标网络)上更新。在每个 tn_steps(目标网络步骤的简称)处，权重从目标网络复制到原始网络，网络估计从其导出。**

**目标 Q 网络在训练过程的早期阶段具有稳定作用。使用 tn_steps = 1 的超参数。意思是每次游戏后复制目标网络。每场比赛持续 100 步左右。所以 tn_steps = 1 实际上意味着每大约 100 次更新就复制一次目标网络。**

# **学习率**

> **学习率是要调整的最重要的超参数。—吴恩达**

**为了找到最佳的学习速率并系统地调整其他超参数，定义了超参数搜索类。在寻找一个恒定的速率后， **0.009** 被确定为目前最好的。**

## **学习率查找器和循环学习率**

**Leslie Smith 等人描述了一种被称为“超级收敛”的现象，即神经网络的训练速度可能比标准方法快。⁶在这个项目中，在给定相同次数的情况下，以循环学习率训练的代理人似乎比其他人在这个问题上做得更好。**

## **LR 取景器**

**首先，我们做一个叫做 LR 范围测试的东西。它决定了该方法是否适用于该问题。训练以非常小的学习率开始，并随着情节的增加而增加。理想情况下，在某一点上，模型停止改进，精确度达到稳定状态，停止增加。此时的学习率是循环学习率计划中使用的最大学习率。三分之一或四分之一是最低学习率。**

**但是，这个问题没有准确性。相反，我使用的是一集训练中发生的平均损失。SGD 的批量大小设置为 100–300 左右。因此，每次损失都是在 30，000 次转换中产生的。这是需要的，使情节顺利如下。否则会太吵，无法解释。**

**![](img/163a33865aeb99ebe25b229a4f6d3404.png)**

**根据 LR 查找器方法，最大学习率大约在 10^-3.5 到 10^-3.之间**

**从上面的图中，我们可以得出结论，最大学习率可以是 10^-3 和最小学习率。四分之一。**

## **循环学习率计划**

**澄清一下，“循环”这个词只是指学习率从 min_lr 到 max_lr，然后下降，看起来就像下面的图。转折点不一定是一半，也可以早到四分之一。这个想法基于一个叫做[模拟退火](https://en.wikipedia.org/wiki/Simulated_annealing)的概念，在深度学习文献中很流行。**

**![](img/3eeb141b52ad07310f834f2a75942f8b.png)**

# **培养**

**如果您希望自己训练模型，请准备好将东西转移到 AWS。训练一个在 4x5(原始)游戏上玩得好的代理需要几个小时。只是澄清一下，使用 GPU 不会加快速度，因为模型中没有卷积层。你也不需要 AWS 上的超级强大的实例，我在 **c5.xlarge EC2** 实例上做了所有的训练。**

# **模型验证和行动⚔️**

**[链接到 GitHub 知识库](https://github.com/arapfaik/td-deepreinforcementlearning-part2)**

**虽然不是最自动化的方法，但验证模型的一种方法是简单地查看并比较模型预测的等效行动的 q 值。另一种方法是比较镜像态的预测 q 值。为什么这些会是好的启发法的解释在[第 1 部分](https://medium.com/p/2eb789c2ff5e)中解释。将结果与第一部分中的表格模型的输出进行比较，我们看到 DQN 的输出不如表格 q-learning 的输出一致。这是因为我们在做近似计算，而且表的大小要大得多。**

**然而，代理无论如何都会选择具有最高 q 值的动作。即使这些数字之间有很大的差异，在这两种情况下，代理将执行相同(等效)的动作。由于状态沿 y 轴镜像，动作 1 和 6 彼此对应。**

**![](img/1b31a1b78d715d458fa00670af99f673.png)**

**我们的学习算法 TD(0)被证明对于算法的表格实现确定性地收敛于单个答案。表格法往往能找到精确解，即往往能精确找到最优值函数和最优策略。**

**这与这里描述的近似方法不同，近似方法只能找到近似解，但反过来可以有效地应用于更大的问题。如同所有的人工智能一样，适用性的广度和数学的易处理性之间存在矛盾。学习最优策略的代理人做得很好，但在实践中这很少发生。对于这项任务，只有在计算成本极高的情况下才能生成最优策略。我们只能近似到不同的程度。**

**模型的手动验证显然很耗时，而且根本不实用。因此，下一部分将解释启发式指标以及损失和事件的绘制。**

# **下一步是什么？**

**测量和绘图是深度学习研究的关键。第三部分介绍了绘图方法。此外，代理是最终确定的，并对玩家在线挑战。接受来自屏幕的输入，点击适当的点，代理能够击败大多数玩家。**

**[链接到第 3 部分](https://medium.com/p/e4a2992112a1)**

# **关于我**

**我是一名数据科学家，住在旧金山。热衷于从数据中寻找答案。在 Linkedin 上找到我:[梅尔·萨卡里亚](http://linkedin.com/in/sakarya)**

# **参考**

**[1]:萨顿&巴尔托。 [*【强化学习】*](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation-ebook/dp/B07JN1QFW5/ref=sr_1_2?crid=352FL757QDCAK&dchild=1&keywords=sutton+and+barto+reinforcement+learning&qid=1587862288&sprefix=sutton+barto+rein%2Caps%2C249&sr=8-2) (2018)**

**[2]:古德费勒，本吉奥&库维尔。[](https://www.amazon.com/Deep-Learning-NONE-Ian-Goodfellow-ebook/dp/B01MRVFGX4/ref=sr_1_3?dchild=1&keywords=deep+learning&qid=1587862365&sr=8-3)**(2016)****

****[3]:哈塞尔特等人 [*深度强化学习与致命三重奏*](https://arxiv.org/abs/1812.02648) *。arXiv:1812.02648v1 [cs。2018 年 12 月 6 日*****

***[4]: Hasselt 等 [*通过深度强化学习的人级控制*](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning) 。《自然》518，第 529-533 页。2015***

***[5]: Schaul 等人 [*优先化经验回放*](https://arxiv.org/abs/1511.05952) 。arXiv:1511.05952v4 [cs。2016 年 2 月 25 日***

***[6]: Smith et al. [*超收敛:利用大学习率非常快速的训练神经网络*](https://arxiv.org/abs/1708.07120) *。*arXiv:1708.07120 v3【cs。2018 年 5 月 17 日***