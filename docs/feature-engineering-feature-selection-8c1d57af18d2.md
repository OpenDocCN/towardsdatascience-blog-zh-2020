# ç‰¹å¾å·¥ç¨‹å’Œç‰¹å¾é€‰æ‹©

> åŽŸæ–‡ï¼š<https://towardsdatascience.com/feature-engineering-feature-selection-8c1d57af18d2?source=collection_archive---------18----------------------->

## ðŸ“ˆPython for finance ç³»åˆ—

## å¦‚ä½•å°†çŽ°ä»£æœºå™¨å­¦ä¹ åº”ç”¨äºŽä½“ç§¯æ‰©æ•£åˆ†æž(VSA)

![](img/d5d8fa97c4fefcbc78908f87fdcd0405.png)

[å†œæ—º](https://unsplash.com/@californong?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)åœ¨ [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) ä¸Šçš„ç…§ç‰‡

**è­¦å‘Š** : *è¿™é‡Œæ²¡æœ‰ç¥žå¥‡çš„å…¬å¼æˆ–åœ£æ¯ï¼Œå°½ç®¡ä¸€ä¸ªæ–°çš„ä¸–ç•Œå¯èƒ½ä¼šä¸ºä½ æ‰“å¼€å¤§é—¨ã€‚*

## ðŸ“ˆPython For Finance ç³»åˆ—

1.  [è¯†åˆ«å¼‚å¸¸å€¼](https://medium.com/python-in-plain-english/identifying-outliers-part-one-c0a31d9faefa)
2.  [è¯†åˆ«å¼‚å¸¸å€¼â€”ç¬¬äºŒéƒ¨åˆ†](https://medium.com/better-programming/identifying-outliers-part-two-4c00b2523362)
3.  [è¯†åˆ«å¼‚å¸¸å€¼â€”ç¬¬ä¸‰éƒ¨åˆ†](https://medium.com/swlh/identifying-outliers-part-three-257b09f5940b)
4.  [ç¨‹å¼åŒ–çš„äº‹å®ž](/data-whispering-eebb77a422da)
5.  [ç‰¹å¾å·¥ç¨‹&ç‰¹å¾é€‰æ‹©](https://medium.com/@kegui/feature-engineering-feature-selection-8c1d57af18d2)
6.  [æ•°æ®è½¬æ¢](/data-transformation-e7b3b4268151)
7.  [ç»†å¾®å·®åˆ«ç‰¹å¾](https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55)
8.  [æ•°æ®æ ‡ç­¾](/the-triple-barrier-method-251268419dcd)
9.  [å…ƒæ ‡ç­¾å’Œå †å ](/meta-labeling-and-stacking-f17a7f9804ec)

åœ¨è¿™äº›ç³»åˆ—çš„å‰å‡ ç¯‡æ–‡ç« çš„åŸºç¡€ä¸Šï¼Œè¿™ä¸€æ¬¡æˆ‘ä»¬å°†æŽ¢ç´¢é‡‘èžå¸‚åœºä¸­çœŸæ­£çš„æŠ€æœ¯åˆ†æžã€‚å¾ˆé•¿ä¸€æ®µæ—¶é—´é‡Œï¼Œæˆ‘ä¸€ç›´ç€è¿·äºŽ TA çš„å†…åœ¨é€»è¾‘ï¼Œå«åšé‡å·®åˆ†æž(VSA)ã€‚æˆ‘æ²¡æœ‰å‘çŽ°ä»»ä½•å…³äºŽåœ¨è¿™ä¸ªæ—¶å€™åº”ç”¨çŽ°ä»£æœºå™¨å­¦ä¹ æ¥è¯æ˜ŽæŒä¹…æŠ€æœ¯çš„æ–‡ç« ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘è¯•å›¾æŠ›å‡ºä¸€æ¡å°é±¼åŽ»æŠ“ä¸€æ¡é²¸é±¼ã€‚å¦‚æžœæˆ‘èƒ½åœ¨è¿™ä¸ªé¢†åŸŸå¼•èµ·ä¸€äº›æ³¨æ„ï¼Œæˆ‘åœ¨è¿™ç¯‡æ–‡ç« ä¸ŠèŠ±è´¹çš„æ—¶é—´æ˜¯å€¼å¾—çš„ã€‚

ç‰¹åˆ«æ˜¯ï¼Œåœ¨æˆ‘è¯»äº†å¤§å«Â·hÂ·éŸ¦æ–¯çš„ã€Šå³å°†å‘ç”Ÿçš„äº¤æ˜“ã€‹ä¹‹åŽï¼Œä»–åœ¨ä¹¦ä¸­æè¿°é“:

> â€œä½ åº”è¯¥èƒ½å¤Ÿå€¾å¬ä»»ä½•å¸‚åœºå¯¹è‡ªèº«çš„çœ‹æ³•ï¼Œè€Œä¸æ˜¯åˆ†æžä¸€ç³»åˆ—æŒ‡æ ‡æˆ–ç®—æ³•ã€‚â€

å¯†åˆ‡å€¾å¬å¸‚åœºï¼Œæ­£å¦‚ä¸‹é¢è¿™å¥è¯æ‰€è¯´çš„ï¼Œæ­£å¦‚æˆ‘ä»¬ä¸å¯èƒ½é¢„æµ‹æœªæ¥ä¸€æ ·ï¼Œæˆ‘ä»¬ä¹Ÿå¾ˆéš¾å¿½è§†å³å°†å‘ç”Ÿçš„äº‹æƒ…ã€‚å…³é”®æ˜¯æ•æ‰å³å°†å‘ç”Ÿçš„äº‹æƒ…ï¼Œå¹¶è·Ÿéšæ½®æµã€‚

![](img/a777b542c2438d7808b5d61989f4817e.png)

ä½†æ˜¯å¦‚ä½•çœ‹å¾…å³å°†å‘ç”Ÿçš„äº‹æƒ…ï¼Œç†æŸ¥å¾·Â·å¨ç§‘å¤«å¾ˆä¹…ä»¥å‰å‘è¡¨çš„ä¸€ç¯‡å£°æ˜Žç»™å‡ºäº†ä¸€äº›çº¿ç´¢:

> â€œæˆåŠŸçš„è¯»ç£å¸¦[è¯»å›¾è¡¨]æ˜¯å¯¹åŠ›çš„ç ”ç©¶ã€‚å®ƒéœ€è¦åˆ¤æ–­å“ªä¸€æ–¹æœ‰æœ€å¤§çš„å¸å¼•åŠ›çš„èƒ½åŠ›ï¼Œä¸€ä¸ªäººå¿…é¡»æœ‰å‹‡æ°”åŽ»æ”¯æŒé‚£ä¸€æ–¹ã€‚å°±åƒåœ¨ä¼ä¸šæˆ–ä¸ªäººçš„ç”Ÿæ´»ä¸­ä¸€æ ·ï¼Œæ¯ä¸€æ¬¡æ‘‡æ‘†éƒ½ä¼šå‡ºçŽ°ä¸´ç•Œç‚¹ã€‚åœ¨è¿™äº›å…³é”®æ—¶åˆ»ï¼Œä¼¼ä¹Žä»»ä½•ä¸€è¾¹ç¾½æ¯›çš„é‡é‡éƒ½ä¼šå†³å®šå½“å‰çš„è¶‹åŠ¿ã€‚ä»»ä½•ä¸€ä¸ªèƒ½å‘çŽ°è¿™äº›ç‚¹çš„äººéƒ½ä¼šèµ¢å¾—å¾ˆå¤šï¼Œå¤±åŽ»å¾ˆå°‘ã€‚â€

ä½†æ˜¯å¦‚ä½•è§£é‡Šå¸‚åœºè¡Œä¸ºå‘¢ï¼Ÿç†æŸ¥å¾·Â·å¨ç§‘å¤«[å¯¹å¸‚åœºåŠ›é‡çš„ä¸€ä¸ªé›„è¾©çš„æè¿°å¾ˆæœ‰å¯å‘æ€§:](https://en.wikipedia.org/wiki/Richard_Wyckoff)

> â€œå¸‚åœºå°±åƒä¸€ä¸ªç¼“æ…¢æ—‹è½¬çš„è½®å­:è½®å­æ˜¯ä¼šç»§ç»­æœåŒä¸€ä¸ªæ–¹å‘æ—‹è½¬ï¼Œé™æ­¢ä¸åŠ¨è¿˜æ˜¯å€’è½¬ï¼Œå®Œå…¨å–å†³äºŽä¸Žå®ƒçš„è½®æ¯‚å’Œè¸æ¿æŽ¥è§¦çš„åŠ›é‡ã€‚å³ä½¿å½“æŽ¥è§¦è¢«æ‰“ç ´ï¼Œæ²¡æœ‰ä»€ä¹ˆå½±å“å®ƒçš„è¿‡ç¨‹ï¼Œè½¦è½®ä¿ç•™ä¸€å®šçš„å†²åŠ›æ¥è‡ªæœ€è¿‘çš„ä¸»å¯¼åŠ›é‡ï¼Œå¹¶æ—‹è½¬ï¼Œç›´åˆ°å®ƒåœä¸‹æ¥æˆ–å—åˆ°å…¶ä»–å½±å“ã€‚â€

David H. Weis ç»™å‡ºäº†ä¸€ä¸ªæžå¥½çš„ä¾‹å­ï¼Œè¯´æ˜Žäº†å¦‚ä½•è§£è¯»æ£’çº¿ï¼Œå¹¶å°†å…¶ä¸Žå¸‚åœºè¡Œä¸ºè”ç³»èµ·æ¥ã€‚é€šè¿‡ä»–å¯¹ä¸€ä¸ªå‡è®¾çš„é…’å§è¡Œä¸ºçš„æž„å»ºï¼Œæ¯ä¸€ä¸ªé…’å§éƒ½å˜å¾—æ´»è·ƒèµ·æ¥ï¼Œäº‰ç›¸å‘ä½ è®²è¿°ä»–ä»¬çš„æ•…äº‹ã€‚

![](img/d7eb81331692b4ddb2eea6db34fba814.png)

å‡è®¾çš„è¡Œä¸º

æœ‰å…³åˆ†æžçš„æ‰€æœ‰ç»†èŠ‚ï¼Œè¯·å‚è€ƒå¤§å«çš„ä¹¦ã€‚

![](img/50fefbda0fd08efd33ba90b7adf6c063.png)

åœ¨è¿™æœ¬ä¹¦æ­£å¼å‘è¡Œä¹‹å‰è´­ä¹°äº†å®ƒï¼Œå¹¶å¾—åˆ°äº†å¤§å«çš„ç­¾åã€‚

åœ¨æˆ‘ä»¬æ·±å…¥ç ”ç©¶ä»£ç ä¹‹å‰ï¼Œæœ€å¥½ç»™å‡ºä¸€äº›å…³äºŽä½“ç§¯æ‰©æ•£åˆ†æž(VSA)çš„èƒŒæ™¯çŸ¥è¯†ã€‚VSA æ˜¯é€šè¿‡è·Ÿè¸ªä¸“ä¸šäº¤æ˜“è€…ï¼Œå³æ‰€è°“çš„åšå¸‚å•†ï¼Œç ”ç©¶é‡ä»·å…³ç³»æ¥é¢„æµ‹å¸‚åœºèµ°å‘ã€‚å¯¹å¸‚åœºè¡Œä¸ºçš„æ‰€æœ‰è§£é‡Šéƒ½éµå¾ª 3 æ¡åŸºæœ¬æ³•åˆ™:

*   ä¾›æ±‚æ³•åˆ™
*   åŠªåŠ›ä¸Žç»“æžœçš„æ³•åˆ™
*   å› æžœå®šå¾‹

åœ¨ VSA çš„å‘å±•å²ä¸Šï¼Œè¿˜æœ‰ä¸‰ä½å¤§åé¼Žé¼Žçš„äººç‰©ã€‚

*   æ°è¥¿Â·åˆ©å¼—èŽ«å°”
*   ç†æŸ¥å¾·Â·å¨ç§‘å¤«
*   æ±¤å§†Â·å¨å»‰å§†æ–¯

å¤§é‡çš„å­¦ä¹ èµ„æ–™å¯ä»¥åœ¨ç½‘ä¸Šæ‰¾åˆ°ã€‚å¯¹äºŽåˆå­¦è€…ï¼Œæˆ‘æŽ¨èä»¥ä¸‹ä¸¤æœ¬ä¹¦ã€‚

1.  [æŽŒæ¡å¸‚åœº](https://www.amazon.com/Master-Markets-Tom-Williams/dp/B001GF0LAM)æ±¤å§†Â·å¨å»‰å§†æ–¯è‘—
2.  å³å°†å‘ç”Ÿçš„äº¤æ˜“

å¦å¤–ï¼Œå¦‚æžœä½ åªæƒ³å¿«é€Ÿæµè§ˆä¸€ä¸‹è¿™ä¸ªè¯é¢˜ï¼Œè¿™é‡Œæœ‰ä¸€ç¯‡æ¥è‡ª[çš„å…³äºŽ VSA çš„å¥½æ–‡ç« ](https://school.stockcharts.com/doku.php?id=market_analysis:the_wyckoff_method)ã€‚

æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ çš„ä¸€å¤§ä¼˜åŠ¿åœ¨äºŽä¸éœ€è¦ç‰¹å¾å·¥ç¨‹ã€‚VSA çš„åŸºæœ¬å¦‚å…¶åæ‰€è¨€ï¼Œæˆäº¤é‡ã€ä»·å·®çš„å¹…åº¦ã€ä½ç½®çš„å˜åŒ–ä¸Žè‚¡ä»·çš„å˜åŒ–å¯†åˆ‡ç›¸å…³ã€‚

è¿™äº›ç‰¹å¾å¯ä»¥å®šä¹‰ä¸º:

![](img/42542cec6cb56aa8cc71ae843d942c5e.png)

é…’å§çš„å®šä¹‰

*   éŸ³é‡:éžå¸¸ç›´æŽ¥
*   èŒƒå›´/ä»·å·®:æœ€é«˜ä»·å’Œæ”¶ç›˜ä»·ä¹‹é—´çš„å·®å¼‚
*   æ”¶ç›˜ä»·ç›¸å¯¹äºŽåŒºé—´:æ”¶ç›˜ä»·æ˜¯æŽ¥è¿‘ä»·æ ¼æŸ±çš„é¡¶éƒ¨è¿˜æ˜¯åº•éƒ¨ï¼Ÿ
*   è‚¡ç¥¨ä»·æ ¼çš„å˜åŒ–:éžå¸¸ç›´æŽ¥

ç½‘ä¸Šæœ‰å¾ˆå¤šå…³äºŽ VSA çš„èµ„æ–™ã€‚æˆ‘å‘çŽ°è¿™ 7 ä¸ªçŽ°åœºäº¤æ˜“ç³»åˆ—è§†é¢‘ç›¸å½“ä¸é”™ã€‚

è¿˜æœ‰è¿™ä¸ªã€‚

æ›´å¤šå†…å®¹å¯ä»¥åœ¨ YouTube ä¸Šæ‰¾åˆ°ã€‚

ç†æŸ¥å¾·Â·å¨ç§‘å¤«åˆ›é€ äº†è®¸å¤šâ€œT4â€æœ¯è¯­ï¼Œå¦‚â€œåŠ›é‡çš„è±¡å¾â€(SOS)ã€â€œè½¯å¼±çš„è±¡å¾â€(SOW)ç­‰..ç„¶è€Œï¼Œè¿™äº›æœ¯è¯­ä¸­çš„å¤§å¤šæ•°çº¯ç²¹æ˜¯è¿™ 4 ä¸ªåŸºæœ¬ç‰¹å¾çš„ç»„åˆã€‚æˆ‘ä¸è®¤ä¸ºï¼Œåœ¨æ·±åº¦å­¦ä¹ çš„æƒ…å†µä¸‹ï¼Œè¿‡åº¦è®¾è®¡åŠŸèƒ½æ˜¯ä¸€ä»¶æ˜Žæ™ºçš„äº‹æƒ…ã€‚è€ƒè™‘åˆ°æ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿ä¹‹ä¸€æ˜¯å®ƒå®Œå…¨è‡ªåŠ¨åŒ–äº†æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ä¸­è¿‡åŽ»æœ€å…³é”®çš„æ­¥éª¤:ç‰¹å¾å·¥ç¨‹ã€‚æˆ‘ä»¬éœ€è¦åšçš„äº‹æƒ…æ˜¯å‘Šè¯‰ç®—æ³•çœ‹å“ªé‡Œï¼Œè€Œä¸æ˜¯ä¸€æ­¥ä¸€æ­¥åœ°ç…§çœ‹ä»–ä»¬ã€‚äº‹ä¸å®œè¿Ÿï¼Œè®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ä»£ç ã€‚

# 1.æ•°æ®å‡†å¤‡

ä¸ºäº†ä¸€è‡´æ€§ï¼Œåœ¨æ‰€æœ‰çš„[ðŸ“ˆPython for finance ç³»åˆ—](https://medium.com/swlh/identifying-outliers-part-three-257b09f5940b)ï¼Œæˆ‘ä¼šå°½é‡é‡ç”¨ç›¸åŒçš„æ•°æ®ã€‚å…³äºŽæ•°æ®å‡†å¤‡çš„æ›´å¤šç»†èŠ‚å¯ä»¥åœ¨[è¿™é‡Œ](https://medium.com/python-in-plain-english/identifying-outliers-part-one-c0a31d9faefa)ï¼Œåœ¨[è¿™é‡Œ](https://medium.com/@kegui/identifying-outliers-part-two-4c00b2523362)å’Œ[è¿™é‡Œ](https://medium.com/swlh/identifying-outliers-part-three-257b09f5940b)æ‰¾åˆ°ã€‚

```
*#import all the libraries*
import pandas as pd
import numpy as np
import seaborn as sns 
import yfinance as yf  *#the stock data from Yahoo Finance*import matplotlib.pyplot as plt #set the parameters for plotting
plt.style.use('seaborn')
plt.rcParams['figure.dpi'] = 300#define a function to get data
def get_data(symbols, begin_date=None,end_date=None):
    df = yf.download('AAPL', start = '2000-01-01',
                     auto_adjust=True,#only download adjusted data
                     end= '2010-12-31') 
    #my convention: always lowercase
    df.columns = ['open','high','low',
                  'close','volume'] 

    return df
prices = get_data('AAPL', '2000-01-01', '2010-12-31')   
prices.head()
```

![](img/20b81e6b675fa63be7ee9e570ba516da.png)

## âœTipï¼

*æˆ‘ä»¬è¿™æ¬¡ä¸‹è½½çš„æ•°æ®æ˜¯é€šè¿‡è®¾ç½®* `*auto_adjust=True*` *è°ƒæ•´* `*yfinance*` *çš„æ•°æ®ã€‚å¦‚æžœä½ èƒ½å¾—åˆ°åˆ†ç¬”æˆäº¤ç‚¹çš„æ•°æ®ï¼Œå°½ä¸€åˆ‡åŠžæ³•ã€‚å¦‚æžœæœ‰é©¬ç§‘æ–¯Â·æ™®æ‹‰å¤š*çš„ [é‡‘èžæœºå™¨å­¦ä¹ è¿›å±•](https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089) *ä¸­é˜è¿°çš„åˆ†ç¬”æˆäº¤ç‚¹æ•°æ®å°±æ›´å¥½äº†ã€‚åæ­£ 10 å¹´è°ƒæ•´åŽçš„æ•°æ®åªç»™å‡º 2766 ä¸ªæ¡ç›®ï¼Œç¦»â€œå¤§æ•°æ®â€è¿˜å·®å¾—è¿œã€‚*

![](img/aade114a28dbd313f869c0d43bb5ddeb.png)

# 2.ç‰¹å¾å·¥ç¨‹

å°† VSA ä¸ŽçŽ°ä»£æ•°æ®ç§‘å­¦ç›¸ç»“åˆçš„å…³é”®æ˜¯ï¼Œé€šè¿‡é˜…è¯»å’Œè§£é‡Šæ£’çº¿è‡ªèº«çš„è¡Œä¸ºï¼Œäººä»¬(å¸Œæœ›æ˜¯ç®—æ³•)å¯ä»¥æž„å»ºä¸€ä¸ªå¸‚åœºè¡Œä¸ºçš„æ•…äº‹ã€‚è¿™ä¸ªæ•…äº‹å¯èƒ½ä¸å®¹æ˜“è¢«äººç±»ç†è§£ï¼Œä½†å´ä»¥ä¸€ç§å¤æ‚çš„æ–¹å¼è¿ä½œã€‚

æˆäº¤é‡ç»“åˆä»·æ ¼åŒºé—´å’Œæ”¶ç›˜ä½ç½®å¾ˆå®¹æ˜“ç”¨ä»£ç æ¥è¡¨ç¤ºã€‚

*   éŸ³é‡:éžå¸¸ç›´æŽ¥
*   èŒƒå›´/ä»·å·®:æœ€é«˜ä»·å’Œæ”¶ç›˜ä»·ä¹‹é—´çš„å·®å¼‚

```
def price_spread(df):
    return (df.high - df.low)
```

*   æ”¶ç›˜ä»·ç›¸å¯¹äºŽåŒºé—´:æ”¶ç›˜ä»·æ˜¯æŽ¥è¿‘ä»·æ ¼æŸ±çš„é¡¶éƒ¨è¿˜æ˜¯åº•éƒ¨ï¼Ÿ

```
def close_location(df):
    return (df.high - df.close) / (df.high - df.low)#o indicates the close is the high of the day, and 1 means close
#is the low of the day and the smaller the value, the closer the #close price to the high.
```

*   è‚¡ç¥¨ä»·æ ¼çš„å˜åŒ–:éžå¸¸ç›´æŽ¥

çŽ°åœ¨åˆ°äº†æ£˜æ‰‹çš„éƒ¨åˆ†ï¼Œ

> "ä»Žæ›´å¤§çš„è§’åº¦æ¥çœ‹ï¼Œä¸€äº›ä»·æ ¼æ¡æœ‰äº†æ–°çš„å«ä¹‰."

è¿™æ„å‘³ç€è¦çœ‹åˆ°å®Œæ•´çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸åŒçš„æ—¶é—´å°ºåº¦ä¸‹è§‚å¯Ÿè¿™ 4 ä¸ªåŸºæœ¬ç‰¹å¾ã€‚

è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸åŒçš„æ—¶é—´è·¨åº¦é‡å»ºä¸€ä¸ªé«˜(H)ï¼Œä½Ž(L)ï¼Œæ”¶ç›˜(C)å’Œä½“ç§¯(V)é…’å§ã€‚

```
def create_HLCV(i): 
'''
#i: days
#as we don't care about open that much, that leaves volume, 
#high,low and close
'''     df = pd.DataFrame(index=prices.index) df[f'high_{i}D'] = prices.high.rolling(i).max()
    df[f'low_{i}D'] = prices.low.rolling(i).min()
    df[f'close_{i}D'] = prices.close.rolling(i).\
                        apply(lambda x:x[-1]) 
    # close_2D = close as rolling backwards means today is 
    #literally, the last day of the rolling window.
    df[f'volume_{i}D'] = prices.volume.rolling(i).sum()

    return df
```

ä¸‹ä¸€æ­¥ï¼Œæ ¹æ®ä¸åŒçš„æ—¶é—´å°ºåº¦åˆ›å»ºè¿™ 4 ä¸ªåŸºæœ¬ç‰¹å¾ã€‚

```
def create_features(i):
    df = create_HLCV(i)
    high = df[f'high_{i}D']
    low = df[f'low_{i}D']
    close = df[f'close_{i}D']
    volume = df[f'volume_{i}D']

    features = pd.DataFrame(index=prices.index)
    features[f'volume_{i}D'] = volume
    features[f'price_spread_{i}D'] = high - low
    features[f'close_loc_{i}D'] = (high - close) / (high - low)
    features[f'close_change_{i}D'] = close.diff()

    return features
```

æˆ‘æƒ³æŽ¢ç´¢çš„æ—¶é—´è·¨åº¦æ˜¯ 1ã€2ã€3 å¤©å’Œ 1 å‘¨ã€1 ä¸ªæœˆã€2 ä¸ªæœˆã€3 ä¸ªæœˆï¼Œå¤§è‡´æ˜¯[1ã€2ã€3ã€5ã€20ã€40ã€60]å¤©ã€‚çŽ°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›é€ ä¸€å¤§å †åŠŸèƒ½ï¼Œ

```
def create_bunch_of_features():
    days = [1,2,3,5,20,40,60]
    bunch_of_features = pd.DataFrame(index=prices.index)
    for day in days:
        f = create_features(day)
        bunch_of_features = bunch_of_features.join(f)

    return bunch_of_featuresbunch_of_features = create_bunch_of_features()
bunch_of_features.info()
```

![](img/b27b3b1c5041a74686cfaa23cf53b126.png)

ä¸ºäº†è®©äº‹æƒ…å®¹æ˜“ç†è§£ï¼Œæˆ‘ä»¬çš„ç›®æ ‡ç»“æžœå°†åªæ˜¯ç¬¬äºŒå¤©çš„å›žæŠ¥ã€‚

```
# next day's returns as outcomes
outcomes = pd.DataFrame(index=prices.index)
outcomes['close_1'] = prices.close.pct_change(-1)
```

# 3.ç‰¹å¾é€‰æ‹©

è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™äº›ç‰¹å¾æ˜¯å¦‚ä½•ä¸Žç»“æžœï¼Œå³ç¬¬äºŒå¤©çš„å›žæŠ¥ç›¸å…³è”çš„ã€‚

```
corr = bunch_of_features.corrwith(outcomes.close_1)
corr.sort_values(ascending=False).plot.barh(title = 'Strength of Correlation');
```

![](img/f0ad8469218b73928e38f6985b097bac.png)

å¾ˆéš¾è¯´æœ‰ä»€ä¹ˆå…³è”ï¼Œå› ä¸ºæ‰€æœ‰çš„æ•°å­—éƒ½è¿œä½ŽäºŽ 0.8ã€‚

```
corr.sort_values(ascending=False)
```

![](img/0c07e3246f1458e5c97654e9985dc6cc.png)

æŽ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›ç‰¹æ€§æ˜¯å¦‚ä½•ç›¸äº’å…³è”çš„ã€‚

```
corr_matrix = bunch_of_features.corr()
```

æˆ‘æ²¡æœ‰åˆ¶ä½œçƒ­å›¾ï¼Œè€Œæ˜¯å°è¯•ä½¿ç”¨ Seaborn çš„ Clustermap æŒ‰è¡Œæˆ–åˆ—è¿›è¡Œèšç±»ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ä»»ä½•æ¨¡å¼å‡ºçŽ°ã€‚Seaborn çš„ Clustermap åŠŸèƒ½éžå¸¸é€‚åˆåˆ¶ä½œç®€å•çš„çƒ­å›¾ä»¥åŠåœ¨è¡Œå’Œ/æˆ–åˆ—ä¸Šéƒ½æœ‰æ ‘çŠ¶å›¾çš„åˆ†å±‚èšç±»çƒ­å›¾ã€‚è¿™å°†é‡æ–°ç»„ç»‡è¡Œå’Œåˆ—çš„æ•°æ®ï¼Œå¹¶æ˜¾ç¤ºå½¼æ­¤ç›¸é‚»çš„ç±»ä¼¼å†…å®¹ï¼Œä»¥ä¾¿æ›´æ·±å…¥åœ°ç†è§£æ•°æ®ã€‚ä¸€ä¸ªå¾ˆå¥½çš„å…³äºŽèšç±»å›¾çš„æ•™ç¨‹å¯ä»¥åœ¨[è¿™é‡Œ](https://blog.tdwi.eu/hierarchical-clustering-in-python/)æ‰¾åˆ°ã€‚è¦èŽ·å¾—ä¸€ä¸ªèšç±»å›¾ï¼Œå®žé™…ä¸Šåªéœ€è¦ä¸€è¡Œä»£ç ã€‚

```
sns.clustermap(corr_matrix)
```

![](img/8f52368acd3ebe119f7913d5aafb3a20.png)

å¦‚æžœä½ ä»”ç»†è§‚å¯Ÿå›¾è¡¨ï¼Œå¯ä»¥å¾—å‡ºä¸€äº›ç»“è®º:

1.  ä»·æ ¼å·®ä»·ä¸Žäº¤æ˜“é‡å¯†åˆ‡ç›¸å…³ï¼Œè¿™ä¸€ç‚¹åœ¨å›¾è¡¨çš„ä¸­å¿ƒå¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ã€‚
2.  å’Œåœ¨ä¸åŒæ—¶é—´è·¨åº¦ä¸Šå½¼æ­¤ç›¸å…³çš„æŽ¥è¿‘çš„ä½ç½®ï¼Œå¦‚å³ä¸‹è§’æ‰€ç¤ºã€‚
3.  ä»Žå·¦ä¸Šè§’çš„æ·¡é¢œè‰²æ¥çœ‹ï¼Œå¯†åˆ‡çš„ä»·æ ¼å˜åŒ–ç¡®å®žä¸Žå®ƒæœ¬èº«æˆå¯¹ï¼Œè¿™éžå¸¸æœ‰æ„ä¹‰ã€‚ç„¶è€Œï¼Œå®ƒæœ‰ç‚¹éšæœºï¼Œå› ä¸ºåœ¨ä¸åŒçš„æ—¶é—´å°ºåº¦ä¸Šæ²¡æœ‰èšç±»æ¨¡å¼ã€‚æˆ‘å¸Œæœ› 2 å¤©çš„å˜åŒ–åº”è¯¥ä¸Ž 3 å¤©çš„å˜åŒ–é…å¯¹ã€‚

æŽ¥è¿‘çš„ä»·æ ¼å·®å¼‚çš„éšæœºæ€§å¯ä»¥å½’å› äºŽè‚¡ç¥¨ä»·æ ¼æœ¬èº«çš„ç‰¹å¾ã€‚ç®€å•çš„ç™¾åˆ†æ¯”å›žæŠ¥å¯èƒ½æ˜¯ä¸€ä¸ªæ›´å¥½çš„é€‰æ‹©ã€‚è¿™å¯ä»¥é€šè¿‡å°†å…³é—­ `diff()`ä¿®æ”¹ä¸ºå…³é—­`pct_change()`æ¥å®žçŽ°ã€‚

```
def create_features_v1(i):
    df = create_HLCV(i)
    high = df[f'high_{i}D']
    low = df[f'low_{i}D']
    close = df[f'close_{i}D']
    volume = df[f'volume_{i}D']

    features = pd.DataFrame(index=prices.index)
    features[f'volume_{i}D'] = volume
    features[f'price_spread_{i}D'] = high - low
    features[f'close_loc_{i}D'] = (high - close) / (high - low)
    #only change here
    features[f'close_change_{i}D'] = close.pct_change()

    return features
```

å†åšä¸€éã€‚

```
def create_bunch_of_features_v1():
    days = [1,2,3,5,20,40,60]
    bunch_of_features = pd.DataFrame(index=prices.index)
    for day in days:
        f = create_features_v1(day)#here is the only difference
        bunch_of_features = bunch_of_features.join(f)

    return bunch_of_featuresbunch_of_features_v1 = create_bunch_of_features_v1()#check the correlation
corr_v1 = bunch_of_features_v1.corrwith(outcomes.close_1)
corr_v1.sort_values(ascending=False).plot.barh( title = 'Strength of Correlation')
```

![](img/84f45390a623b6f52aa0ebb9143a9092.png)

æœ‰ç‚¹ä¸åŒï¼Œä½†ä¸å¤šï¼

```
corr_v1.sort_values(ascending=False)
```

![](img/8c162b1afd54a4c06269ce5f7a0d8510.png)

ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ä¼šæ€Žæ ·ï¼Ÿ

```
corr_matrix_v1 = bunch_of_features_v1.corr()
sns.clustermap(corr_matrix_v1, cmap='coolwarm', linewidth=1)
```

![](img/52d994da1cb73ecc02bb62c70145edf6.png)

å—¯ï¼Œæ¨¡å¼ä¿æŒä¸å˜ã€‚è®©æˆ‘ä»¬å°†é»˜è®¤æ–¹æ³•ä»Žâ€œå¹³å‡â€æ”¹ä¸ºâ€œç—…æˆ¿â€ã€‚è¿™ä¸¤ç§æ–¹æ³•ç±»ä¼¼ï¼Œä½†â€œwardâ€æ›´åƒ K-MEANs èšç±»ã€‚å…³äºŽè¿™ä¸ªè¯é¢˜çš„å¾ˆå¥½çš„æ•™ç¨‹å¯ä»¥åœ¨[è¿™é‡Œ](https://blog.tdwi.eu/hierarchical-clustering-in-python/)æ‰¾åˆ°ã€‚

![](img/d6e5988f97949542bef918f8ecabdbc7.png)

```
sns.clustermap(corr_matrix_v1, cmap='coolwarm', linewidth=1,
                         method='ward')
```

![](img/8c157574622ace871c793727b204d1e9.png)

ä¸ºäº†é€‰æ‹©ç‰¹æ€§ï¼Œæˆ‘ä»¬å¸Œæœ›æŒ‘é€‰é‚£äº›ä¸Žç›®æ ‡ç»“æžœæœ‰æœ€å¼ºã€æœ€æŒä¹…å…³ç³»çš„ç‰¹æ€§ã€‚åŒæ—¶ï¼Œå°½é‡å‡å°‘æ‰€é€‰è¦ç´ ä¸­çš„é‡å æˆ–å…±çº¿æ€§ï¼Œä»¥é¿å…å™ªéŸ³å’Œè®¡ç®—æœºèƒ½åŠ›çš„æµªè´¹ã€‚å¯¹äºŽé‚£äº›åœ¨èšç±»ä¸­é…å¯¹åœ¨ä¸€èµ·çš„ç‰¹å¾ï¼Œæˆ‘åªæŒ‘é€‰ä¸Žç»“æžœæœ‰æ›´å¼ºç›¸å…³æ€§çš„ç‰¹å¾ã€‚é€šè¿‡æŸ¥çœ‹èšç±»å›¾ï¼Œå¯ä»¥æŒ‘é€‰å‡ºä¸€äº›ç‰¹å¾ã€‚

```
deselected_features_v1 = ['close_loc_3D','close_loc_60D',
                       'volume_3D', 'volume_60D',
                       'price_spread_3D','price_spread_60D',
                       'close_change_3D','close_change_60D']selected_features_v1 = bunch_of_features.drop \
(labels=deselected_features_v1, axis=1)
```

æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†çœ‹ä¸€çœ‹é…å¯¹å›¾ï¼Œä¸€ä¸ª[é…å¯¹å›¾](https://seaborn.pydata.org/generated/seaborn.pairplot.html)æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•æ¥è¯†åˆ«åŽç»­åˆ†æžçš„è¶‹åŠ¿ï¼Œå…è®¸æˆ‘ä»¬çœ‹åˆ°å•ä¸ªå˜é‡çš„åˆ†å¸ƒå’Œå¤šä¸ªå˜é‡ä¹‹é—´çš„å…³ç³»ã€‚åŒæ ·ï¼Œæˆ‘ä»¬éœ€è¦çš„åªæ˜¯ä¸€è¡Œä»£ç ã€‚

```
sns.pairplot(selected_features_v1)
```

![](img/c9b2bb866c38e06156f4c946e0054451.png)

å›¾è¡¨é“ºå¤©ç›–åœ°ï¼Œå¾ˆéš¾çœ‹æ¸…ã€‚æˆ‘ä»¬ä»¥ä¸€ä¸ªå°ç¾¤ä½“ä¸ºä¾‹ã€‚

```
selected_features_1D_list = ['volume_1D', 'price_spread_1D',\         'close_loc_1D', 'close_change_1D']selected_features_1D = selected_features_v1\
                       [selected_features_1D_list]sns.pairplot(selected_features_1D)
```

![](img/9b7ef51a34d36552c6ad3cf84d810c1c.png)

æˆ‘ç«‹å³æ³¨æ„åˆ°ä¸¤ä»¶äº‹ï¼Œä¸€æ˜¯æœ‰å¼‚å¸¸å€¼ï¼Œå¦ä¸€ä¸ªæ˜¯åˆ†å¸ƒä¸æŽ¥è¿‘æ­£å¸¸ã€‚

çŽ°åœ¨è®©æˆ‘ä»¬æ¥å¤„ç†ç¦»ç¾¤å€¼ã€‚ä¸ºäº†ä¸€æ°”å‘µæˆï¼Œæˆ‘ä¼šå°†ç»“æžœä¸Žç‰¹å¾ç»“åˆèµ·æ¥ï¼Œå¹¶ä¸€èµ·åˆ é™¤å¼‚å¸¸å€¼ã€‚

```
features_outcomes = selected_features_v1.join(outcomes)
features_outcomes.info()
```

![](img/2c958a142d8645f574560c3897b7bdc6.png)

æˆ‘å°†ä½¿ç”¨ä¸Žè¿™é‡Œæè¿°çš„ã€è¿™é‡Œæè¿°çš„å’Œè¿™é‡Œæè¿°çš„ç›¸åŒçš„æ–¹æ³•æ¥ç§»é™¤å¼‚å¸¸å€¼ã€‚

```
stats = features_outcomes.describe()
def get_outliers(df, i=4): 
    #i is number of sigma, which define the boundary along mean
    outliers = pd.DataFrame()

    for col in df.columns:
        mu = stats.loc['mean', col]
        sigma = stats.loc['std', col]
        condition = (df[col] > mu + sigma * i) | (df[col] < mu - sigma * i) 
        outliers[f'{col}_outliers'] = df[col][condition]

    return outliersoutliers = get_outliers(features_outcomes, i=1)
outliers.info()
```

![](img/4f2b8e799f58bd6cf8cd5dd97d04e1e8.png)

æˆ‘è®¾ç½® 1 ä¸ªæ ‡å‡†å·®ä½œä¸ºè¾¹ç•Œï¼ŒæŠŠå¤§éƒ¨åˆ†çš„ç¦»ç¾¤å€¼æŒ–å‡ºæ¥ã€‚ç„¶åŽç§»é™¤æ‰€æœ‰å¼‚å¸¸å€¼å’Œ NaN å€¼ã€‚

```
features_outcomes_rmv_outliers = features_outcomes.drop(index = outliers.index).dropna()
features_outcomes_rmv_outliers.info()
```

![](img/8dc9357f5f6efa9addbf2ce17ec42c6c.png)

å‰”é™¤å¼‚å¸¸å€¼åŽï¼Œæˆ‘ä»¬å¯ä»¥å†æ¬¡ç»˜åˆ¶é…å¯¹å›¾ã€‚

```
sns.pairplot(features_outcomes_rmv_outliers, vars=selected_features_1D_list);
```

![](img/23b6aa8510a2b32c5d28b857093ae75d.png)

çŽ°åœ¨ï¼Œæƒ…å†µçœ‹èµ·æ¥å¥½äº†å¾ˆå¤šï¼Œä½†å‡ ä¹Žæ— æ³•å¾—å‡ºä»»ä½•æœ‰ç”¨çš„ç»“è®ºã€‚è¿™å°†æ˜¯å¾ˆå¥½çš„ï¼Œçœ‹çœ‹å“ªäº›æ™¯ç‚¹æ˜¯å‘ä¸‹ç§»åŠ¨ï¼Œå“ªäº›æ˜¯å‘ä¸Šç§»åŠ¨ä¸Žè¿™äº›åŠŸèƒ½ç›¸ç»“åˆã€‚æˆ‘å¯ä»¥æå–è‚¡ç¥¨ä»·æ ¼å˜åŒ–çš„ä¿¡å·ï¼Œå¹¶åœ¨å›¾ä¸Šå¢žåŠ ä¸€ä¸ªé¢å¤–çš„ç»´åº¦ã€‚

```
features_outcomes_rmv_outliers['sign_of_close'] = features_outcomes_rmv_outliers['close_1'].apply(np.sign)
```

çŽ°åœ¨ï¼Œè®©æˆ‘ä»¬é‡æ–°ç»˜åˆ¶`pairplot()`,ç¨å¾®è°ƒæ•´ä¸€ä¸‹ï¼Œä½¿å›¾å½¢æ›´åŠ æ¼‚äº®ã€‚

```
sns.pairplot(features_outcomes_rmv_outliers, 
             vars=selected_features_1D_list,
             diag_kind='kde',
             palette='husl', hue='sign_of_close',
             markers = ['*', '<', '+'], 
             plot_kws={'alpha':0.3});#transparence:0.3
```

![](img/eab7e27cb96b63a36a4ae7bdb3242a3f.png)

çŽ°åœ¨ï¼Œçœ‹èµ·æ¥å¥½å¤šäº†ã€‚æ˜¾ç„¶ï¼Œå½“ä»·æ ¼ä¸Šæ¶¨æ—¶ï¼Œå®ƒä»¬(è“ç‚¹)å¯†åº¦æ›´å¤§ï¼Œèšé›†åœ¨æŸä¸ªä½ç½®ã€‚è€Œåœ¨æƒ…ç»ªä½Žè½çš„æ—¶å€™ï¼Œå®ƒä»¬ä¼šå››å¤„è”“å»¶ã€‚

***å¦‚æžœä½ èƒ½ç»™è¿™å¯¹æ­æ¡£çš„æƒ…èŠ‚æä¾›ä¸€äº›çº¿ç´¢ï¼Œå¹¶åœ¨ä¸‹é¢ç•™ä¸‹ä½ çš„è¯„è®ºï¼Œæˆ‘å°†ä¸èƒœæ„Ÿæ¿€ï¼Œè°¢è°¢ã€‚***

ä»¥ä¸‹æ˜¯æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ‰€æœ‰ä»£ç çš„æ‘˜è¦:

```
*#import all the libraries*
import pandas as pd
import numpy as np
import seaborn as sns 
import yfinance as yf  *#the stock data from Yahoo Finance*import matplotlib.pyplot as plt #set the parameters for plotting
plt.style.use('seaborn')
plt.rcParams['figure.dpi'] = 300#define a function to get data
def get_data(symbols, begin_date=None,end_date=None):
    df = yf.download('AAPL', start = '2000-01-01',
                     auto_adjust=True,#only download adjusted data
                     end= '2010-12-31') 
    #my convention: always lowercase
    df.columns = ['open','high','low',
                  'close','volume'] 

    return dfprices = get_data('AAPL', '2000-01-01', '2010-12-31')#create some features
def create_HLCV(i):#as we don't care open that much, that leaves volume, 
#high,low and closedf = pd.DataFrame(index=prices.index)
    df[f'high_{i}D'] = prices.high.rolling(i).max()
    df[f'low_{i}D'] = prices.low.rolling(i).min()
    df[f'close_{i}D'] = prices.close.rolling(i).\
                        apply(lambda x:x[-1]) 
    # close_2D = close as rolling backwards means today is 
    # literly the last day of the rolling window.
    df[f'volume_{i}D'] = prices.volume.rolling(i).sum()

    return dfdef create_features_v1(i):
    df = create_HLCV(i)
    high = df[f'high_{i}D']
    low = df[f'low_{i}D']
    close = df[f'close_{i}D']
    volume = df[f'volume_{i}D']

    features = pd.DataFrame(index=prices.index)
    features[f'volume_{i}D'] = volume
    features[f'price_spread_{i}D'] = high - low
    features[f'close_loc_{i}D'] = (high - close) / (high - low)
    features[f'close_change_{i}D'] = close.pct_change()

    return featuresdef create_bunch_of_features_v1():
'''
the timespan that i would like to explore 
are 1, 2, 3 days and 1 week, 1 month, 2 month, 3 month
which roughly are [1,2,3,5,20,40,60]
'''
    days = [1,2,3,5,20,40,60]
    bunch_of_features = pd.DataFrame(index=prices.index)
    for day in days:
        f = create_features_v1(day)
        bunch_of_features = bunch_of_features.join(f)

    return bunch_of_featuresbunch_of_features_v1 = create_bunch_of_features_v1()#define the outcome target
#hereï¼Œ to make thing easy to understand, i will only try to predict #the next days's return
outcomes = pd.DataFrame(index=prices.index)# next day's returns
outcomes['close_1'] = prices.close.pct_change(-1)#decide which features are abundant from cluster map
deselected_features_v1 = ['close_loc_3D','close_loc_60D',
                       'volume_3D', 'volume_60D',
                       'price_spread_3D','price_spread_60D',
                       'close_change_3D','close_change_60D']
selected_features_v1 = bunch_of_features_v1.drop(labels=deselected_features_v1, axis=1)#join the features and outcome together to remove the outliers
features_outcomes = selected_features_v1.join(outcomes)
stats = features_outcomes.describe()#define the method to identify outliers
def get_outliers(df, i=4): 
    #i is number of sigma, which define the boundary along mean
    outliers = pd.DataFrame()

    for col in df.columns:
        mu = stats.loc['mean', col]
        sigma = stats.loc['std', col]
        condition = (df[col] > mu + sigma * i) | (df[col] < mu -   sigma * i) 
        outliers[f'{col}_outliers'] = df[col][condition]

    return outliersoutliers = get_outliers(features_outcomes, i=1)#remove all the outliers and Nan value
features_outcomes_rmv_outliers = features_outcomes.drop(index = outliers.index).dropna()
```

æˆ‘çŸ¥é“è¿™ç¯‡æ–‡ç« å¤ªé•¿äº†ï¼Œæˆ‘æœ€å¥½æŠŠå®ƒç•™åœ¨è¿™é‡Œã€‚åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†è¿›è¡Œæ•°æ®è½¬æ¢ï¼Œçœ‹çœ‹æˆ‘æ˜¯å¦æœ‰åŠžæ³•è§£å†³åˆ†å¸ƒé—®é¢˜ã€‚æ•¬è¯·æœŸå¾…ï¼

# å‚è€ƒ

1.  å³å°†å‘ç”Ÿçš„äº¤æ˜“
2.  ç½—æ´›ç£å¸¦[pseudã€‚]ï¼Œç£å¸¦é˜…è¯»çš„ç ”ç©¶(ä¼¯çµé¡¿ï¼Œä½›è’™ç‰¹å·ž:å¼—é›·æ³½ï¼Œ1910)ï¼Œ95ã€‚