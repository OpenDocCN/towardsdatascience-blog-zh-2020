<html>
<head>
<title>Blindness detection (Diabetic retinopathy) using Deep learning on Eye retina images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用眼睛视网膜图像上的深度学习进行失明检测(糖尿病视网膜病变)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/blindness-detection-diabetic-retinopathy-using-deep-learning-on-eye-retina-images-baf20fcf409e?source=collection_archive---------4-----------------------#2020-05-21">https://towardsdatascience.com/blindness-detection-diabetic-retinopathy-using-deep-learning-on-eye-retina-images-baf20fcf409e?source=collection_archive---------4-----------------------#2020-05-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="03f3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用卷积神经网络(使用 Python)自动化该过程，以在为时已晚之前加速患者的失明检测</h2></div><h1 id="88e3" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">目录</h1><ol class=""><li id="062b" class="la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">使用深度学习来检测失明</li><li id="edc2" class="la lb it lc b ld ls lf lt lh lu lj lv ll lw ln lo lp lq lr bi translated">评估指标(二次加权 kappa)</li><li id="a425" class="la lb it lc b ld ls lf lt lh lu lj lv ll lw ln lo lp lq lr bi translated">图像处理和分析</li><li id="4eff" class="la lb it lc b ld ls lf lt lh lu lj lv ll lw ln lo lp lq lr bi translated">使用多任务学习实现 arXiv.org 研究论文(前 1%解决方案)</li><li id="3716" class="la lb it lc b ld ls lf lt lh lu lj lv ll lw ln lo lp lq lr bi translated">其他迁移学习模式</li><li id="c9dc" class="la lb it lc b ld ls lf lt lh lu lj lv ll lw ln lo lp lq lr bi translated">未来的工作</li><li id="8df9" class="la lb it lc b ld ls lf lt lh lu lj lv ll lw ln lo lp lq lr bi translated">链接到 github 代码和 linkedin 个人资料</li><li id="684a" class="la lb it lc b ld ls lf lt lh lu lj lv ll lw ln lo lp lq lr bi translated">使用的参考文献</li></ol></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi me"><img src="../Images/c86d9f1b0acf30ae3b36bfab1bf25d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*AzCf55_nrj0DeEWsw9NITg.jpeg"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图片来源—<a class="ae mq" href="http://images2.fanpop.com/image/photos/10300000/Cas-Blindness-castiel-10370846-640-352.jpg" rel="noopener ugc nofollow" target="_blank">http://images 2 . fan pop . com/image/photos/10300000/Cas-Blindness-casti El-10370846-640-352 . jpg</a></p></figure><h1 id="e6bb" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">1.使用深度学习来检测失明</h1><p id="1085" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">本案例研究基于<strong class="lc iu"> APTOS 2019 失明检测</strong>基于此处的 kaggle 挑战赛——<a class="ae mq" href="https://www.kaggle.com/c/aptos2019-blindness-detection/overview" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/APTOS 2019-失明检测/概述</a>。</p><p id="414e" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">数百万人患有<a class="ae mq" href="https://www.mayoclinic.org/diseases-conditions/diabetic-retinopathy/symptoms-causes/syc-20371611" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">糖尿病视网膜病变</strong> </a>，这是工作年龄成人失明的主要原因。<strong class="lc iu">印度 Aravind 眼科医院</strong>希望在难以进行医学筛查的农村地区发现并预防这种疾病。目前，技术人员前往这些农村地区采集图像，然后依靠训练有素的医生来检查图像并提供诊断。</p><p id="00d9" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">这里的目标是通过技术扩大他们的努力；以获得自动筛查疾病图像并提供病情严重程度的信息的能力。我们将通过建立一个<a class="ae mq" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">卷积神经网络</strong> </a>模型来实现这一点，该模型可以自动查看患者的眼睛图像，并估计患者失明的严重程度。这种自动化过程可以减少大量时间，从而大规模筛选治疗糖尿病性视网膜病变的过程。</p><p id="5634" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">我们得到了<strong class="lc iu"> 3200 张</strong>眼睛图像及其对应的严重程度等级，严重程度等级为<strong class="lc iu">【0，1，2，3，4】</strong>之一。该数据将用于训练模型，并对测试数据进行预测。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e98c7e89ae3c73bbdee1e49f75c836fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*pFN1iHWCkXOT-jyk1cV5Lw.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">失明严重程度量表(5 级)</p></figure><p id="b246" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">数据集中的样本眼睛图像如下—</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nk"><img src="../Images/2bec1fca145a7b76eeb0864e4d27bb64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FaDnfa7m1C_2xt8MJDS2fg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">对应于每个失明等级严重性(0-4)的眼睛图像</p></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="fd57" class="ki kj it bd kk kl np kn ko kp nq kr ks jz nr ka ku kc ns kd kw kf nt kg ky kz bi translated">2.评估指标(二次加权 kappa)</h1><p id="706d" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated"><strong class="lc iu">二次加权 kappa </strong>衡量两个评级之间的<strong class="lc iu">一致性。该指标通常从 0(评分者之间的随机一致)到 1(评分者之间的完全一致)不等。如果评定者之间的一致程度偶然低于预期，该指标可能会低于 0。在由<strong class="lc iu">人类评价人</strong>分配的分数和<strong class="lc iu">预测分数</strong>之间计算二次加权 kappa。</strong></p><h2 id="a648" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">科恩卡帕的直觉</h2><p id="da8c" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">要理解这个指标，我们需要理解<strong class="lc iu">科恩的 kappa ( </strong>维基百科— <a class="ae mq" href="https://en.wikipedia.org/wiki/Cohen%27s_kappa#Weighted_kappa" rel="noopener ugc nofollow" target="_blank">链接</a>)的概念。该指标说明了除了我们从<strong class="lc iu">混淆矩阵</strong>中观察到的一致性之外，偶然出现的一致性。我们可以用一个简单的例子来理解这一点—</p><p id="8de5" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">假设我们想从下面的协议表中计算科恩的 kappa，这基本上是一个混淆矩阵。从下表中我们可以看出，在总共 50 项观察中，“A”和“B”评定者同意(20 项是+ 15 项否)= 35 项观察。所以，观察到的一致是<strong class="lc iu">P(o)</strong>= 35/50 =<strong class="lc iu">0.7</strong></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi og"><img src="../Images/22b1aa01cc697b5e458ef8663ddcfe1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*cWcvpg9S5Ni36jpmC0J9MA.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">来源——维基百科</p></figure><p id="6ea4" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">我们还需要找到多大比例的评级是由于随机的机会，而不是由于' A '和' B '之间的实际协议。我们可以看到，' A '说是 25/50 = 0.5 次，' B '说是 30/50 = 0.6 次。所以两个人随机同时说‘是’的概率是 0.5*0.6 = <strong class="lc iu"> 0.3 </strong>。同样，他们两个随机同时说‘不’的概率是 0.5*0.4 = <strong class="lc iu"> 0.2 </strong>。这样，随机一致的概率就是 0.3 + 0.2 = <strong class="lc iu"> 0.5 </strong>。让我们称之为 P(e)。</p><p id="b832" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">所以，Cohens kappa 会是<strong class="lc iu"> 0.4 </strong>(公式如下)。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/ce9c313011a8abdf612399f00392f701.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*SpsRx7RcH5y7czwqYTvh7A.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">来源——维基百科，科恩斯卡帕公式</p></figure><p id="957e" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">我们得到的值是<strong class="lc iu"> 0.4，</strong>我们可以用下表来解释。你可以在本博客中阅读更多关于这一指标的解读——<a class="ae mq" rel="noopener" target="_blank" href="/inter-rater-agreement-kappas-69cd8b91ff75"><strong class="lc iu">https://towardsdatascience . com/inter-rater-agreement-kappas-69 CD 8 b 91 ff 75</strong></a>。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi oi"><img src="../Images/f9f4dd4d4065293602042ab409f83bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fG7fNTtgsYXlMlr0OBk12g.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">来源—<a class="ae mq" rel="noopener" target="_blank" href="/inter-rater-agreement-kappas-69cd8b91ff75">https://towards data science . com/inter-rater-agreement-kappas-69 CD 8 b 91 ff 75</a></p></figure><p id="1c8e" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">值<strong class="lc iu"> 0.4 </strong>将意味着我们有一个公平/适度的协议。</p><h2 id="85c9" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">序数类中二次权的直觉——二次加权 kappa</h2><p id="e59e" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">当涉及到多类时，我们可以扩展相同的概念，并在这里引入<strong class="lc iu">权重</strong>的概念。引入权重背后的直觉是，它们是<strong class="lc iu">序数变量，</strong>，这意味着类别‘1’和‘2’之间的一致优于类别‘1’和‘3’，因为它们在序数尺度上更接近。在我们的失明严重度案例中，输出是<strong class="lc iu">序数</strong>(失明严重度，0 代表无失明，4 代表最高)。为此，引入了二次标度的权重。序数类越接近，它们的权重越高。下面是很好解释这个概念的博文链接—<a class="ae mq" href="https://medium.com/x8-the-ai-community/kappa-coefficient-for-dummies-84d98b6f13ee" rel="noopener"><strong class="lc iu">https://medium . com/x8-the-ai-community/kappa-coefficient-for-dummies-84d 98 b 6 f 13 ee</strong></a>。</p><div class="mf mg mh mi gt ab cb"><figure class="oj mj ok ol om on oo paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><img src="../Images/4b629cbd44d9b60a62642eb38910843d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*ZFAssQfrjhgPMo_H34enLQ.jpeg"/></div></figure><figure class="oj mj op ol om on oo paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><img src="../Images/6e5f75af35a5f103ce906b330bc6eadd.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*3_TfQxUfhlbxz7HoVZlw5g.jpeg"/></div><p class="mm mn gj gh gi mo mp bd b be z dk oq di or os translated">来源—<a class="ae mq" href="https://medium.com/x8-the-ai-community/kappa-coefficient-for-dummies-84d98b6f13ee" rel="noopener">https://medium . com/x8-the-ai-community/kappa-coefficient-for-dummies-84d 98 b 6 f 13 ee</a></p></figure></div><p id="e371" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">正如我们所见，R1 和 R4 的权重为 0.44(因为它们比 R1 和 R2 的权重 0.94 大 3 个等级，因为它们更接近)。在计算观察概率和随机概率时，这些权重乘以相应的概率。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="99a7" class="ki kj it bd kk kl np kn ko kp nq kr ks jz nr ka ku kc ns kd kw kf nt kg ky kz bi translated">3.图像处理和分析</h1><h2 id="4fc5" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">输出变量的类别分布</h2><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ot"><img src="../Images/c3ed3e2459d9da8bd738238e0a7b223c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c8mVx9RAdykGyzUINL_7KQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">阶级不平衡——分布</p></figure><p id="828f" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">正如我们所看到的，在训练数据集中存在类别不平衡，大多数情况下值为“0 ”,在“1”和“3”类别中最少。</p><h2 id="33bb" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">视觉化眼睛图像</h2><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ou ov l"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">用于生成可视化效果的代码段</p></figure><p id="8c70" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">这些基本上是使用<a class="ae mq" href="https://en.wikipedia.org/wiki/Fundus_photography" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">眼底照相</strong> </a>拍摄的眼睛视网膜图像。正如我们所见，图像包含伪像，其中一些是失焦，曝光不足，或曝光过度等。此外，一些图像具有低亮度和低闪电条件，因此难以评估图像之间的差异。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ow"><img src="../Images/03fa06e11dcbb1db25ec49f15d3ab568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*myMf57-4e1205q0gM92HGA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">样本 15 幅眼睛图像</p></figure><p id="51dd" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">下图是一只眼睛的视网膜图像样本，该眼睛可能患有<strong class="lc iu">糖尿病视网膜病变</strong>。源链接— kaggle <a class="ae mq" href="https://www.kaggle.com/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy" rel="noopener ugc nofollow" target="_blank">内核链接 T21<strong class="lc iu">。<a class="ae mq" href="https://www.eyeops.com/" rel="noopener ugc nofollow" target="_blank">原文出处—<strong class="lc iu">https://www.eyeops.com/</strong>T25】</a></strong></a></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ox"><img src="../Images/edf7123788e92f185bed01bd95518b82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FuiFkh2X8NtJbC6BjGNSGg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">img 来源—<a class="ae mq" href="https://www.kaggle.com/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/ratthachat/aptos-眼部预处理-糖尿病视网膜病变</a>，【https://www.eyeops.com/】T2</p></figure><p id="a41c" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">如我们所见，我们正试图找出那些出血/渗出物等。在有博士学位的高年级学生中。</p><h2 id="91a7" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">图像处理</h2><p id="f166" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">为了对图像进行调整，制作更清晰的图像，使模型能够更有效地学习特征，我们将使用 python 中的<strong class="lc iu"> OpenCV </strong>库(<strong class="lc iu"> cv2 </strong>)进行一些图像处理技术。</p><p id="5ac5" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">我们可以应用<a class="ae mq" href="https://www.tutorialspoint.com/opencv/opencv_gaussian_blur.htm" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">高斯模糊</strong> </a> <strong class="lc iu"> </strong>来突出图像中的特征。在高斯模糊操作中，图像与高斯滤波器卷积，高斯滤波器是去除高频分量的低通滤波器。</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ou ov l"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">代码片段—高斯模糊</p></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi oy"><img src="../Images/fafa8dcbef8a7bb78799ccf7168e5303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yM3ephTPk6xVa-V3Y3mFGA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">高斯模糊之前/之后</p></figure><p id="d00b" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">这个写得很棒的内核(<a class="ae mq" href="https://www.kaggle.com/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">链接</strong> </a>)介绍了从灰度图像中进行<strong class="lc iu">圆形裁剪的思想。</strong>在下面的代码部分实现相同的功能:-</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ou ov l"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">代码片段—圆形裁剪</p></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi oz"><img src="../Images/73ffa7b5d0eb707fdcb55187d8b4553c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yd32apfEIz5GejuoGEfBiQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">之前/之后(模糊+裁剪)</p></figure><p id="b704" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">正如我们所看到的，我们现在能够更清楚地看到图像中的独特模式。以下是对 15 个图像样本进行的图像处理。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi pa"><img src="../Images/08f66a4aef3fe01eddeea4bb4f893c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5gUL5tFqDcrDGXxQe4dMvA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图像处理后</p></figure><h2 id="6903" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">多重处理和调整图像大小，以保存在目录中</h2><p id="572d" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">应用上述操作后，我们需要将新图像保存在文件夹中，以便以后使用。原始图像每个大约 3 MB，整个图像文件夹占用 20 GB 空间。我们可以通过调整图像大小来减少这种情况。使用多核线程/多处理，我们可以在一分钟内完成这项任务。我们可以使用具有 6 个内核的<strong class="lc iu">线程池</strong>(因为我有 8 个内核的 CPU)来实现这一点，并使用(512x512)的图像大小来实现这一点(<strong class="lc iu"> IMG 大小</strong>)。</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ou ov l"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">代码片段—使用线程池的多重处理</p></figure><h2 id="e21f" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">TSNE 可视化</h2><p id="4b91" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">为了理解这些图像在各自的类别(失明严重程度)中是否是可分离的，我们可以首先使用 TSNE 在二维空间中对其进行可视化。我们可以首先将 RGB 等级的图像转换为灰度等级的图像，然后将这些图像展平以生成一个矢量表示，该矢量表示可以用作该图像的特征表示。</p><p id="4a4e" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated"><strong class="lc iu">RGB(256 x256 x3)——&gt;灰度(256 x256)——&gt;展平(65536) </strong></p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ou ov l"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">代码片段— TSNE</p></figure><p id="64b1" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">困惑是需要调整以获得良好结果的超参数。迭代之后，我们可以使用 TSNE 图来表示困惑= 40。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/d29def8f56aea6f73d62275b384cc2e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*M3xMudz2tz6QHInLyZIuHQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">TSNE 情节，困惑= 40</p></figure><p id="db8f" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">从上面的图中我们可以看出，类“0”与其他类有些分离，而其他类之间的区别仍然很模糊。</p><h2 id="c510" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">图像增强</h2><p id="3f30" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">这是最常用的程序之一，通过从数据集创建额外的图像来生成数据的鲁棒性，以使其通过旋转翻转、裁剪、填充等对新数据进行良好的概括。使用 keras <strong class="lc iu"> ImageDataGenerator </strong>类</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="6f1d" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">上述代码生成了应用增强后获得的样本图像—</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/9f61dd8073106998aa60952f3123d85f.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*DFb9PNla2D_VQzfKBrLrjw.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图像放大(示例)</p></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="538e" class="ki kj it bd kk kl np kn ko kp nq kr ks jz nr ka ku kc ns kd kw kf nt kg ky kz bi translated">4.使用多任务学习实现 arXiv.org 研究论文(前 1%解决方案)</h1><p id="fd8b" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">在 arXiv.org 上发现的一篇研究论文——<a class="ae mq" href="https://arxiv.org/pdf/2003.02261.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="lc iu">研究论文链接</strong> </a> <strong class="lc iu"> </strong>在 2943 个中获得第 54 位(前 1%)涉及多任务学习的详细方法来解决这个问题陈述。下面是使用 Python 复制研究论文代码的尝试。在 github 上没有找到这篇研究论文的代码参考。我已经在一篇<strong class="lc iu">研究论文(summary doc)中总结了研究论文。pdf </strong>文件— <a class="ae mq" href="https://github.com/debayanmitra1993-data/Blindness-Detection-Diabetic-Retinopathy-" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu"> github 链接</strong> </a>。总结以下研究论文中的指针和代码实现—</p><h2 id="fbca" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">使用的数据集</h2><p id="546e" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">实际的研究论文使用了来自其他来源的多个数据集，比如—</p><ol class=""><li id="1d3b" class="la lb it lc b ld ne lf nf lh pd lj pe ll pf ln lo lp lq lr bi translated">来自糖尿病视网膜病变的 35，216 张图像，2015 年挑战—<a class="ae mq" href="https://www.kaggle.com/c/diabetic-retinopathy-detection/overview/timeline" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/Diabetic-Retinopathy-detection/overview/timeline</a></li><li id="2360" class="la lb it lc b ld ls lf lt lh lu lj lv ll lw ln lo lp lq lr bi translated">印度糖尿病视网膜病变图像数据集(IDRiD) (Sahasrabuddhe 和 Meriaudeau，2018 年)=使用 413 张图像</li><li id="ab5e" class="la lb it lc b ld ls lf lt lh lu lj lv ll lw ln lo lp lq lr bi translated">梅西多数据集(Google Brain，2018)数据集</li><li id="76d0" class="la lb it lc b ld ls lf lt lh lu lj lv ll lw ln lo lp lq lr bi translated">完整的数据集由 18590 张眼底照片组成，由 Kaggle 竞赛的组织者分为 3662 张训练，1928 张验证和 13000 张测试图像</li></ol><p id="215b" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">然而，由于不容易获得所有数据集，我们只能使用现有的 APTOS 2019 数据集来完成这项任务。</p><h2 id="fcec" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">图像预处理和增强</h2><p id="8d57" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">多种图像预处理技术，如图像大小调整、裁剪，被用来从眼睛图像中提取出与众不同的特征(如以上章节中所讨论的)。</p><p id="5cd6" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">使用的图像增强包括:光学失真、网格失真、分段仿射变换、水平翻转、垂直翻转、随机旋转、随机移动、随机缩放、RGB 值移动、随机亮度和对比度、加性高斯噪声、模糊、锐化、浮雕、随机伽玛和剪切等。</p><p id="d4ce" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">然而，我们可以使用类似于上面的替代图像增强(使用参考— <a class="ae mq" href="https://github.com/dimitreOliveira/APTOS2019BlindnessDetection/tree/master/Model%20backlog" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">链接</strong> </a>)。</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ou ov l"/></div></figure><h2 id="385f" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">使用的模型架构</h2><p id="0b45" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">正如我们在下面看到的，研究论文使用了多任务学习模型(它并行进行回归、分类、有序回归的训练)。这样，它可以使用单个模型，并且由于第一层无论如何都会学习相似的特征，所以实现该架构以减少训练时间(而不是训练 3 个单独的模型)。对于编码器部分，我们可以使用任何现有的 CNN 架构— <strong class="lc iu"> ResNet50，EfficientNetB4，EfficientNetB5 ( </strong>和 ensemble 这些)。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/136e089081699e851b0ee0638ee51899.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*ZszXez7QYMj1gbdnFC5ukA.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">来源—<a class="ae mq" href="https://arxiv.org/pdf/2003.02261.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.02261.pdf</a></p></figure><p id="f3e4" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">上述架构的代码实现如下(用 keras 表示)—</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ou ov l"/></div></figure><h2 id="5467" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">创建多输出-自定义图像数据生成器</h2><p id="6b1a" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">我们需要创建一个自定义的 ImageDataGenerator 函数，因为我们有 3 个输出(引用— <a class="ae mq" href="https://classifai.net/blog/multiple-outputs-keras/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">链接</strong> </a>)。请注意，此函数<strong class="lc iu">产生</strong> 3 个输出(回归、分类、有序回归)代码如下—</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="cc1a" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">请注意，对于每个类(0，1，2，3，4)，顺序回归编码按如下方式进行。sklearn 中的<strong class="lc iu">multilabel binary izer</strong>实现了这个任务，如上面的代码要点所示。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/4e27533cbb54ea83f082ee8410bfc32d.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*xlqqRi7na9u2BXrnq2F5sQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">来源—<a class="ae mq" href="https://www.kaggle.com/c/aptos2019-blindness-detection/discussion/98239" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/aptos 2019-失明-检测/讨论/98239 </a></p></figure><h2 id="6589" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">第一阶段(预培训)</h2><p id="d1a7" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">这包括使所有层可训练，并使用现有的 Imagenet 权重作为 ResNet50 编码器的权重初始化器。模型仅被训练到 3 个输出头。使用以下损失函数编译模型(使用 SGD 优化器、余弦衰减调度程序的 20 个时期)—</p><pre class="mf mg mh mi gt pi pj pk pl aw pm bi"><span id="8e99" class="nu kj it pj b gy pn po l pp pq">model.compile(<br/>optimizer = optimizers.SGD(lr=LEARNING_RATE),</span><span id="63c5" class="nu kj it pj b gy pr po l pp pq">loss={'regression_output': 'mean_absolute_error',<br/>'classification_output':'categorical_crossentropy',<br/>'ordinal_regression_output' : 'binary_crossentropy'},</span><span id="8983" class="nu kj it pj b gy pr po l pp pq">metrics = ['accuracy'])</span></pre><div class="mf mg mh mi gt ab cb"><figure class="oj mj ps ol om on oo paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><img src="../Images/e907ddb37e3b5ccf9880b870e3991878.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*oWG9udlKPVKsuI1Zy8ECcw.jpeg"/></div></figure><figure class="oj mj pt ol om on oo paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><img src="../Images/406f686ca4676e85c240386d2f3aa48c.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*BivNAjC_tNVoi9x2YD7oZQ.jpeg"/></div></figure><figure class="oj mj pu ol om on oo paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><img src="../Images/ac0bf84f795e6607fa160ae7924b7cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*U7KildJ8ujw4eEXEJbYYBg.jpeg"/></div><p class="mm mn gj gh gi mo mp bd b be z dk pv di pw os translated">预训练阶段的历元(vs)损失图</p></figure></div><h2 id="514a" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">第二阶段(主要训练)</h2><p id="b68c" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">在这个阶段，事情发生了变化。</p><ol class=""><li id="2f6c" class="la lb it lc b ld ne lf nf lh pd lj pe ll pf ln lo lp lq lr bi translated">首先，损失函数从交叉熵变为<strong class="lc iu">聚焦损失</strong>。你可以在这里阅读更多关于焦损的内容——<a class="ae mq" href="https://medium.com/adventures-with-deep-learning/focal-loss-demystified-c529277052de" rel="noopener">https://medium . com/adventures-with-deep-learning/Focal-loss-demystified-c 529277052 de</a>。简而言之，焦点丢失在处理<strong class="lc iu">等级不平衡</strong>方面做得更好，正如我们任务中的情况。</li></ol><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi px"><img src="../Images/293ab0383b9a276702140110b08c3a42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rkcrsgUZ7kE0aWRD8-WVVg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">来源—<a class="ae mq" href="https://medium.com/adventures-with-deep-learning/focal-loss-demystified-c529277052de" rel="noopener">https://medium . com/adventures-with-deep-learning/focal-loss-demystified-c 529277052 de</a></p></figure><p id="bdc4" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">焦点损失参考代码—<a class="ae mq" href="https://github.com/umbertogriffo/focal-loss-keras" rel="noopener ugc nofollow" target="_blank">https://github.com/umbertogriffo/focal-loss-keras</a></p><pre class="mf mg mh mi gt pi pj pk pl aw pm bi"><span id="1468" class="nu kj it pj b gy pn po l pp pq">model.compile(<br/>optimizer = optimizers.Adam(lr=LEARNING_RATE),</span><span id="9d3a" class="nu kj it pj b gy pr po l pp pq">loss={<br/>'regression_output': mean_squared_error, <br/>'classification_output': categorical_focal_loss(alpha=.25,gamma=2),<br/>'ordinal_regression_output' : binary_focal_loss(alpha=.25,gamma=2)<br/>},</span><span id="6dc6" class="nu kj it pj b gy pr po l pp pq">metrics = ['accuracy'])</span></pre><p id="924e" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">2.第二，第二阶段的训练分两个阶段进行。第一子阶段包括冻结模型网络中的所有编码器层。这样做是为了预热权重(使用 Imagenet 权重初始化对小数据集进行迁移学习)。第二个子阶段包括解冻和训练所有层。</p><pre class="mf mg mh mi gt pi pj pk pl aw pm bi"><span id="dd3c" class="nu kj it pj b gy pn po l pp pq"># SUB STAGE 1 (2nd Stage) - Freeze Encoder layers</span><span id="aa77" class="nu kj it pj b gy pr po l pp pq"><strong class="pj iu">for</strong> layer <strong class="pj iu">in</strong> model.layers:<br/>    layer.trainable = <strong class="pj iu">False</strong><br/><strong class="pj iu">for</strong> i <strong class="pj iu">in</strong> range(-14,0):<br/>  model.layers[i].trainable = <strong class="pj iu">True</strong></span><span id="e24d" class="nu kj it pj b gy pr po l pp pq"># SUB STAGE 2(2nd Stage) - Unfreeze All layers</span><span id="ab1b" class="nu kj it pj b gy pr po l pp pq"><strong class="pj iu">for</strong> layer <strong class="pj iu">in</strong> model.layers:<br/>    layer.trainable = <strong class="pj iu">True</strong></span></pre><p id="bb54" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">图表如下所示—</p><div class="mf mg mh mi gt ab cb"><figure class="oj mj py ol om on oo paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><img src="../Images/9b9e4e71a99749a5f15b789982a9f018.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*K4Vtu4SeO1aFQjJCvA4Czw.jpeg"/></div></figure><figure class="oj mj pz ol om on oo paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><img src="../Images/2bf52fbf20440cdbc12a5a7381c50a27.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*AwAu4esVbgJ2TNNA2xls4Q.jpeg"/></div></figure><figure class="oj mj qa ol om on oo paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><img src="../Images/2d06c806071e9e3a0e48c60e1d1c0c33.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*CVmvAqkTK1YuwDPeKBzusg.jpeg"/></div><p class="mm mn gj gh gi mo mp bd b be z dk qb di qc os translated">主要训练阶段的历元(vs)损失图</p></figure></div><h2 id="7820" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">第三阶段(岗位培训)</h2><p id="318b" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">这包括从 3 个头部(分类、回归、有序回归)获得输出，并将其传递给单个密集神经元(线性激活)以最小化均方误差(50 个时期)</p><pre class="mf mg mh mi gt pi pj pk pl aw pm bi"><span id="b2ce" class="nu kj it pj b gy pn po l pp pq">train_preds = model.predict_generator(<br/>complete_generator, steps=STEP_SIZE_COMPLETE,verbose = 1<br/>)</span><span id="b3d4" class="nu kj it pj b gy pr po l pp pq">train_output_regression = np.array(train_preds[0]).reshape(-1,1)<br/>train_output_classification = np.array(np.argmax(train_preds[1],axis = -1)).reshape(-1,1)<br/>train_output_ordinal_regression = np.array(np.sum(train_preds[2],axis = -1)).reshape(-1,1)</span><span id="bb6f" class="nu kj it pj b gy pr po l pp pq">X_train = np.hstack((<br/>train_output_regression,<br/>train_output_classification,<br/>train_output_ordinal_regression))</span><span id="dc69" class="nu kj it pj b gy pr po l pp pq">model_post = Sequential()<br/>model_post.add(Dense(1, activation='linear', input_shape=(3,)))</span><span id="8160" class="nu kj it pj b gy pr po l pp pq">model_post.compile(<br/>optimizer=optimizers.SGD(lr=LEARNING_RATE), loss='mean_squared_error',<br/>metrics=['mean_squared_error'])</span></pre><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi qd"><img src="../Images/bfb508d599a00cb7866234c594d48377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ob2vK_6agiT4935WBfbamg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">培训后—纪元(vs)损失</p></figure><h2 id="6314" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated">测试数据的模型评估</h2><p id="3b45" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">因为我们有回归输出，所以我们可以进行最近整数舍入来得到最终的类标签。</p><p id="7e97" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">我们在测试数据上获得的最终二次加权 kappa 分数是<strong class="lc iu"> 0.704 </strong>(这表明模型预测和人类评分者之间的基本一致)。</p><p id="fdbe" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">在测试数据上获得的归一化混淆矩阵(matplotlib 的代码已从— <a class="ae mq" href="https://github.com/dimitreOliveira/APTOS2019BlindnessDetection/tree/master/Model%20backlog" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">链接</strong> </a>中引用)</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi qe"><img src="../Images/097180a1380368c1fac4a0918e6373b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HHkmhQtMoPV4X9zzJC1sjQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">测试数据—标准化混淆矩阵</p></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="6a15" class="ki kj it bd kk kl np kn ko kp nq kr ks jz nr ka ku kc ns kd kw kf nt kg ky kz bi translated">5.其他迁移学习模式</h1><p id="d1e3" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">在处理小数据集(与 ImageNet 数据集没有相似性)时，常用的迁移学习方法是首先使用现有的 ImageNet 权重作为初始值(冻结前几层)，然后重新训练模型。</p><p id="4929" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">我们可以使用类似的实现。一个简单的<strong class="lc iu"> ResNet50 </strong>架构在这样使用的时候会给出很好的结果(参考— <a class="ae mq" href="https://github.com/dimitreOliveira/APTOS2019BlindnessDetection/tree/master/Model%20backlog" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">链接</strong> </a>)</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="9a03" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">我们可以训练上述模型 2-5 个时期(只有最后 5 层是可训练的，它们基本上是在<strong class="lc iu"> ResNet50 </strong>之后的层)。</p><p id="0568" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">然后，我们可以使所有层可训练，并训练整个模型。</p><pre class="mf mg mh mi gt pi pj pk pl aw pm bi"><span id="a6ba" class="nu kj it pj b gy pn po l pp pq"><strong class="pj iu">for</strong> layer <strong class="pj iu">in</strong> model.layers:<br/>    layer.trainable = <strong class="pj iu">True</strong></span></pre><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi qf"><img src="../Images/fd4787a9c3e845a2128ddf69d3f123c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BLyEP1jcXG_V-8aZlcjUag.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">ResNet50(迁移学习)—纪元(vs)丢失</p></figure><p id="90b2" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">正如我们所看到的，仅在 20 个时期内，我们就获得了良好的准确度分数——在验证数据集上接近 92%。</p><p id="b9b7" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">该模型在测试数据上给出了<strong class="lc iu"> 0.83 </strong>的二次加权 kappa(这是一个很好的一致性分数)。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi qg"><img src="../Images/fe2d89fe494e3504c4f2e799737ab0fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YlRTaM6PC7EJked_xaazYw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">标准化混淆矩阵(测试数据)</p></figure><p id="9b53" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">同样，我们可以用本页提到的其他<strong class="lc iu">keras . applications</strong>——<a class="ae mq" href="https://keras.io/api/applications/" rel="noopener ugc nofollow" target="_blank"><strong class="lc iu">link</strong></a><strong class="lc iu">做类似的实现。</strong>以下是在其中一些模型上实现时获得的结果(仅模型架构发生变化，其他参数保持不变)。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/750e95263a0ab5c5f485a8bf1a763985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*BBRnw8fyDLCPL8pHDWHCYA.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">模型性能—摘要</p></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="626b" class="ki kj it bd kk kl np kn ko kp nq kr ks jz nr ka ku kc ns kd kw kf nt kg ky kz bi translated">6.未来的工作</h1><h2 id="18b9" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated"><strong class="ak">优化器实验</strong></h2><p id="aded" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">正如我们所看到的，在主要的培训阶段，培训损失并没有减少。我们可以解决这个问题的方法之一是将优化器从 Adam 改为<strong class="lc iu">修正 Adam 优化器</strong>。(更多关于 RAdam 的内容在这里—<a class="ae mq" href="https://www.pyimagesearch.com/2019/10/07/is-rectified-adam-actually-better-than-adam/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2019/10/07/is-rectified-Adam-actually-better-Adam/</a>)。此外，我们可以用<strong class="lc iu"> SGD (Momentum) </strong>优化器和<strong class="lc iu">权重衰减方法</strong>，更多的<strong class="lc iu">学习率调度器</strong>来检查模型性能的改进。</p><h2 id="b21b" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated"><strong class="ak">规则化方法实验</strong></h2><p id="3d72" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">此外，我们还可以在模型上使用更多的正则化来帮助更好地训练模型——一些技术可能涉及<strong class="lc iu">标签平滑</strong>(向目标标签添加噪声)——这将允许模型更好地概括并防止过度拟合。此外，我们可以尝试 L2 正则化来提高模型的泛化能力。</p><h2 id="030d" class="nu kj it bd kk nv nw dn ko nx ny dp ks lh nz oa ku lj ob oc kw ll od oe ky of bi translated"><strong class="ak">集合实验和 K 倍交叉验证</strong></h2><p id="5f1b" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">该研究论文还提到了跨各种架构使用集成技术，如<strong class="lc iu"> EfficientNetB4、5EfficientNetB5、SE-ResNeXt50 </strong>等，并使用<strong class="lc iu">分层交叉验证(5 折)</strong>来提高模型性能和泛化能力。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="9a6d" class="ki kj it bd kk kl np kn ko kp nq kr ks jz nr ka ku kc ns kd kw kf nt kg ky kz bi translated">7.链接到 Github 代码和 linkedin 个人资料</h1><p id="7798" class="pw-post-body-paragraph mr ms it lc b ld le ju mt lf lg jx mu lh mv mw mx lj my mz na ll nb nc nd ln im bi translated">所有代码都在我的 Github 库— <a class="ae mq" href="https://github.com/debayanmitra1993-data/Blindness-Detection-Diabetic-Retinopathy-" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">链接</strong> </a>里。如果你想讨论这方面的进一步实验，你可以在我的 Linkedin 个人资料上联系我— <a class="ae mq" href="https://www.linkedin.com/in/debayan-mitra-63282398/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">链接</strong> </a>。你也可以通过<strong class="lc iu">debayanmitra1993@gmail.com</strong>联系我。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="c39f" class="ki kj it bd kk kl np kn ko kp nq kr ks jz nr ka ku kc ns kd kw kf nt kg ky kz bi translated">6.使用的参考文献</h1><div class="qi qj gp gr qk ql"><a href="https://github.com/dimitreOliveira/APTOS2019BlindnessDetection" rel="noopener  ugc nofollow" target="_blank"><div class="qm ab fo"><div class="qn ab qo cl cj qp"><h2 class="bd iu gy z fp qq fr fs qr fu fw is bi translated">dimitrioliveira/aptos 2019 盲检</h2><div class="qs l"><h3 class="bd b gy z fp qq fr fs qr fu fw dk translated">这个知识库的目标是使用比赛数据建立机器学习模型来处理图像数据…</h3></div><div class="qt l"><p class="bd b dl z fp qq fr fs qr fu fw dk translated">github.com</p></div></div><div class="qu l"><div class="qv l qw qx qy qu qz mk ql"/></div></div></a></div><div class="qi qj gp gr qk ql"><a href="https://www.kaggle.com/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy" rel="noopener  ugc nofollow" target="_blank"><div class="qm ab fo"><div class="qn ab qo cl cj qp"><h2 class="bd iu gy z fp qq fr fs qr fu fw is bi translated">APTOS:糖尿病视网膜病变中的眼睛预处理</h2><div class="qs l"><h3 class="bd b gy z fp qq fr fs qr fu fw dk translated">使用 Kaggle 笔记本探索和运行机器学习代码|使用来自多个数据源的数据</h3></div><div class="qt l"><p class="bd b dl z fp qq fr fs qr fu fw dk translated">www.kaggle.com</p></div></div><div class="qu l"><div class="ra l qw qx qy qu qz mk ql"/></div></div></a></div><div class="qi qj gp gr qk ql"><a href="https://www.kaggle.com/tanlikesmath/intro-aptos-diabetic-retinopathy-eda-starter" rel="noopener  ugc nofollow" target="_blank"><div class="qm ab fo"><div class="qn ab qo cl cj qp"><h2 class="bd iu gy z fp qq fr fs qr fu fw is bi translated">介绍 APTOS 糖尿病视网膜病变(EDA &amp; Starter)</h2><div class="qs l"><h3 class="bd b gy z fp qq fr fs qr fu fw dk translated">使用 Kaggle 笔记本探索和运行机器学习代码|使用来自多个数据源的数据</h3></div><div class="qt l"><p class="bd b dl z fp qq fr fs qr fu fw dk translated">www.kaggle.com</p></div></div><div class="qu l"><div class="rb l qw qx qy qu qz mk ql"/></div></div></a></div><p id="7783" class="pw-post-body-paragraph mr ms it lc b ld ne ju mt lf nf jx mu lh ng mw mx lj nh mz na ll ni nc nd ln im bi translated">研究论文—<a class="ae mq" href="https://arxiv.org/pdf/2003.02261.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.02261.pdf</a></p></div></div>    
</body>
</html>