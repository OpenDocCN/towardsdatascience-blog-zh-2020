<html>
<head>
<title>Machine Reads Bertrand Russell</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器阅读伯特兰·罗素</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-reads-bertrand-russell-ff4eb63bc25c?source=collection_archive---------35-----------------------#2020-01-20">https://towardsdatascience.com/machine-reads-bertrand-russell-ff4eb63bc25c?source=collection_archive---------35-----------------------#2020-01-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/c406389367ee587ac1c322c057d950c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X813KnoZQ147oLt6"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Giammarco Boscaro 在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="b4e8" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用词云、主题建模和文本相似性的自然语言处理(NLP)应用</h2></div><p id="ecf0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我的一个秘密目标就是多看伯特兰·罗素的作品；罗素是一个多才多艺的作家，他涉猎广泛，包括哲学、逻辑、社会问题和数学。自然地，我认为让我的电脑替我做一些阅读会有所帮助，特别是当我们仍然可以告诉电脑做什么的时候。因此，让我们从<a class="ae jd" href="https://www.gutenberg.org/" rel="noopener ugc nofollow" target="_blank">古腾堡</a>那里抓取一些文本，并教机器生成一些见解。</p><p id="85a5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，我们需要导入要分析的文本。为此，我推荐使用<a class="ae jd" href="https://pypi.org/project/Gutenberg/" rel="noopener ugc nofollow" target="_blank"> Python 的 Gutenberg 包</a>而不是通用的文件导入包，因为它允许轻松删除 Gutenberg 在每个文档中相当长的披露。如果我们只处理一个文本，这没什么大不了的，因为删除可以手动完成，但在这种情况下，我们会带来十多个文本。要在 Google 的 Colab 上设置 Gutenberg:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="0a8e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">整个代码被调整为在<a class="ae jd" href="https://colab.research.google.com/notebooks/welcome.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab </a>上运行，所以如果您正在您的机器上运行它，忽略您可能已经在本地拥有的包的片段。正如你可能已经知道的，Colab 设置是临时的，每次我们都需要上传默认情况下没有提供给所有用户的东西。如果在您的计算机上使用 Jupyter 笔记本，您可能希望按照以下方式调整窗口宽度(根据您自己的喜好更改百分比):</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="lv lw l"/></div></figure><h2 id="a4ff" class="lx ly jg bd lz ma mb dn mc md me dp mf le mg mh mi li mj mk ml lm mm mn mo mp bi translated">拉塞尔胸怀大志</h2><p id="61bf" class="pw-post-body-paragraph kv kw jg kx b ky mq kh la lb mr kk ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">预处理和词云</p><p id="f5c8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本教程中，我们将使用<a class="ae jd" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK </a>、<a class="ae jd" href="https://github.com/amueller/word_cloud" rel="noopener ugc nofollow" target="_blank"> Wordcloud </a>和 Scikit-learn 库，它们都有自己的停用词列表(非常常见的词和其他不会给我们的分析增加太多价值的词)。为了保持一致，我们在整个教程中只使用 scikit-learn 版本，但包括一些我不认为非常有用的额外单词:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="77f8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设已经安装了 Gutenberg 包，让我们导入文本:</p><pre class="lr ls lt lu gt mv mw mx my aw mz bi"><span id="87bb" class="lx ly jg mw b gy na nb l nc nd">import os                       <br/>from gutenberg.acquire import load_etext                       <br/>from gutenberg.cleanup import strip_headers                                               path = os.getcwd()                       </span><span id="b31b" class="lx ly jg mw b gy ne nb l nc nd">text_list = [5827, 690, 2529, 25447, 4776, 44932, 37090, 17350, 55610, 52091]                                               </span><span id="4429" class="lx ly jg mw b gy ne nb l nc nd">#writes all texts into one file in the TextsPub directory                      os.mkdir(path + '/TextsPub')<br/>with open(path + '/TextsPub/Russell.txt', 'w') as f:<br/>    for text in text_list:<br/>        text = strip_headers(load_etext(text)).strip()<br/>        f.write(text)</span><span id="871b" class="lx ly jg mw b gy ne nb l nc nd">#writes texts into separate files in the TextsPub directory<br/>for text in text_list:<br/>    with open(f"{path+'/TextsPub'}/{text}", "w") as f:<br/>        f.write(strip_headers(load_etext(text)).strip())</span></pre><p id="eb25" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">利用上一篇关于 Plutarch 的文章<a class="ae jd" rel="noopener" target="_blank" href="/reimagining-plutarch-with-nlp-part-1-24e82fc6556">中的代码，</a>让我们进行<a class="ae jd" href="https://gist.github.com/mlai-demo/4ed02f2bba7e9faf1691f05eab8510d2" rel="noopener ugc nofollow" target="_blank">文本清理和字数统计，</a>我们将删除一些停用词、短词、数字、语法，对文本进行标记，并统计最常用的词:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/4c07002ca0cc8570a10c641b18da07c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LewOGtelX7apRtLL3l4PUQ.png"/></div></div></figure><p id="d48e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们检查一下有多少单词，其中有多少是独特的:</p><pre class="lr ls lt lu gt mv mw mx my aw mz bi"><span id="3409" class="lx ly jg mw b gy na nb l nc nd">unique = set(words)<br/>print("The text is {} words long and {} unique words".format(len(words), len(unique)))</span></pre><p id="0b71" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们得到 216，893 个单词，其中 15，377 个是唯一的。使用这个预处理过的文本，为了使见解更有意义，让我们用<a class="ae jd" href="https://en.wikipedia.org/wiki/Lemmatisation" rel="noopener ugc nofollow" target="_blank">对</a>进行词汇化(即去除词尾变化，返回单词的基本形式)并绘制一个单词云:</p><pre class="lr ls lt lu gt mv mw mx my aw mz bi"><span id="2ec0" class="lx ly jg mw b gy na nb l nc nd">nltk.download('wordnet') #lexical database for English<br/>from nltk.stem import WordNetLemmatizer <br/>                                              <br/>with open(path + '/TextsPub/Russell_tokens.txt') as f, open(path + '/TextsPub/Russell_lemma.txt', 'w') as out_f:    <br/>    text = f.read()<br/>    tokens = word_tokenize(text)<br/>    lemma = WordNetLemmatizer()<br/>    lemmed = [lemma.lemmatize(word) for word in tokens]<br/>    #print(lemmed[:100])<br/>    new_lem_text = ' '.join(lemmed)<br/>    out_f.write(new_lem_text)</span><span id="9342" class="lx ly jg mw b gy ne nb l nc nd">unique_lem = set(lemmed)<br/>print("The lemmatized text is {} words long and {} unique words".format(len(lemmed), len(unique_lem)))</span></pre><p id="e48b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们会看到，虽然整个文本有相同的 216，893 个单词，但独特的单词减少到了 13，656 个。</p><p id="9550" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们将<a class="ae jd" href="https://gist.github.com/mlai-demo/4ebd22e29211162d39679edc0ef05a83" rel="noopener ugc nofollow" target="_blank">词条化的文本放入词云</a>:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/fe8384b31ebb9c4ceedf6f3a04976e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nTx8bqoP2dqr7QK3DwfaLQ.png"/></div></div></figure><p id="00f5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">“thing”这个词变得非常突出，大概是由于将“thing”和“things”结合在一起，所以我们不妨检查一下它的使用频率:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="49c8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的输出表明“thing”现在是最频繁的；光看“云”这个词是看不出来的:</p><pre class="lr ls lt lu gt mv mw mx my aw mz bi"><span id="55c6" class="lx ly jg mw b gy na nb l nc nd">[('thing', 1301), ('point', 1234), ('relation', 1146), ('space', 1140), ('world', 1044), ('fact', 1025), ('object', 930), ('men', 895), ('time', 870), ('knowledge', 856), ('case', 844), ('belief', 805), ('state', 775), ('form', 773), ('law', 756), ('certain', 747), ('number', 744), ('question', 737), ('different', 732), ('life', 727), ('present', 718), ('sense', 692), ('view', 677), ('word', 673), ('possible', 668), ('mean', 665), ('man', 664), ('make', 663), ('way', 655), ('thought', 651), ('true', 635), ('mind', 624), ('matter', 613), ('know', 598), ('given', 579), ('geometry', 560), ('work', 549), ('desire', 542), ('doe', 539), ('kind', 535)]</span></pre><p id="85e4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">罗素的顶级词汇是概括和抽象的:事物、点、关系、空间、世界、事实等。总的来说，顶级词汇代表了科学、哲学和政治词汇的混合，表明了文本中固有的多样性。</p><h2 id="3a1a" class="lx ly jg bd lz ma mb dn mc md me dp mf le mg mh mi li mj mk ml lm mm mn mo mp bi translated">布尔什维克主义与几何学非常不同</h2><p id="3ff4" class="pw-post-body-paragraph kv kw jg kx b ky mq kh la lb mr kk ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">主题建模和文本相似性</p><p id="af5f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们继续之前，让我们将古腾堡索引中的 10 个文本复制并重命名为类似于实际名称的名称，然后只提取和排序没有路径和文件扩展名的文本标题:</p><pre class="lr ls lt lu gt mv mw mx my aw mz bi"><span id="7757" class="lx ly jg mw b gy na nb l nc nd">['Analysis_Mind',<br/> 'Bolshevism',<br/> 'Foundations_Geometry',<br/> 'Free_Thought',<br/> 'Knowledge',<br/> 'Mysticism_Logic',<br/> 'Political_Ideals',<br/> 'Problems_Philosophy',<br/> 'Roads_Freedom',<br/> 'Why_Fight']</span></pre><p id="ef94" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有相当多的主题建模技术，如<a class="ae jd" href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" rel="noopener ugc nofollow" target="_blank">非负矩阵分解</a>(NMF)<a class="ae jd" href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" rel="noopener ugc nofollow" target="_blank">潜在语义分析</a>(截断奇异值分解)<a class="ae jd" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank">潜在狄利克雷分配</a>和<a class="ae jd" href="https://escholarship.org/uc/item/34g5207t" rel="noopener ugc nofollow" target="_blank">稀疏主成分分析</a> —出于篇幅的考虑，在本教程中我们将触及第一个，并在<a class="ae jd" href="https://github.com/mlai-demo/NLP_Russell" rel="noopener ugc nofollow" target="_blank"> Github 代码</a>中涵盖其他三个。</p><p id="1e9a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所有的方法都非常不同，所以检查和操作它们的组成部分是非常重要的，因为人们试图实现更好的结果。例如，scikit-learn 为 NMF 显示了以下内容:</p><pre class="lr ls lt lu gt mv mw mx my aw mz bi"><span id="a4ba" class="lx ly jg mw b gy na nb l nc nd"><em class="nh">class </em>sklearn.decomposition.NMF<!-- -->(<em class="nh">n_components=None</em>, <em class="nh">init=None</em>, <em class="nh">solver='cd'</em>, <em class="nh">beta_loss='frobenius'</em>, <em class="nh">tol=0.0001</em>, <em class="nh">max_iter=200</em>, <em class="nh">random_state=None</em>, <em class="nh">alpha=0.0</em>, <em class="nh">l1_ratio=0.0</em>, <em class="nh">verbose=0</em>, <em class="nh">shuffle=False</em>)</span></pre><p id="7eb5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如对于 beta-loss(测量 NMF 和目标矩阵之间的距离)，我们有三种选择:除了默认的<a class="ae jd" href="http://mathworld.wolfram.com/FrobeniusNorm.html" rel="noopener ugc nofollow" target="_blank"> Frobenius 范数</a>，我们还可以尝试<a class="ae jd" href="https://medium.com/@samsachedina/demystified-kullback-leibler-divergence-3971f956ef34" rel="noopener"> Kullback-Leibler 散度</a>和<a class="ae jd" href="https://en.wikipedia.org/wiki/Itakura%E2%80%93Saito_distance" rel="noopener ugc nofollow" target="_blank">板仓-斋藤距离</a>。解算器本身也可以改变，从默认的坐标下降到乘法更新解算器(板仓-斋藤需要)。</p><p id="a638" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">出于缩短本教程的错误原因，我们这次将跳过术语化——但是技术与上面显示的一样。让我们对文本进行矢量化，并创建一个 numpy 数组，因为上面提到的一些方法不能处理稀疏矩阵(NMF 对这两种方法都可以，但稀疏需要更长的时间来处理):</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="d1de" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，让我们创建主题:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="60f1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是我们得到的结果:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/3641c51fdec38cf854c600af075f30ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lNiRXvEiqHXdSniicvH7Ew.png"/></div></div></figure><p id="19c9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看各种文本是如何处理四大主题的:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="ca10" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是输出:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/f25cf03af93f675aa2e58fe05267716d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*sACfJaRFQlV6sw_5qLEc0Q.png"/></div></figure><p id="cfdb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">(幸运的是)在布尔什维克主义和几何学的文本之间没有重叠；几何学的主题与精神分析、知识、神秘主义和逻辑的文本有很好的匹配。与此同时，处理布尔什维克主义的文件与关于政治理想、人类为何而战和自由之路的文本分享了热门话题。</p><p id="bd8e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后但同样重要的是，让我们运行<a class="ae jd" href="https://iq.opengenus.org/tf-idf/" rel="noopener ugc nofollow" target="_blank"> TF-IDF(术语频率-逆文档频率)</a>来测量这十个文本之间的成对相似性:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="598b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在将包含相似性得分的矩阵转换成 Pandas 数据框架后，我们可以很容易地直观检查结果:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/62f8f85cf02386edced4486f1c8db3ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9dAwI3t5u0-F9CxD2-yPaQ.png"/></div></div></figure><p id="5398" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">是的，几何和布尔什维克主义还是很不一样的！</p><h2 id="b370" class="lx ly jg bd lz ma mb dn mc md me dp mf le mg mh mi li mj mk ml lm mm mn mo mp bi translated">结论</h2><p id="ea4f" class="pw-post-body-paragraph kv kw jg kx b ky mq kh la lb mr kk ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">在这个简短的教程中，我们利用流行的 NLP 技术从文本中提取信息，包括词云、主题建模和文本相似性。我们利用了 NLTK、Wordcloud 和 Scikit——一路学习。本教程涉及的代码(以及更多)可从 Github 上获得。</p><h2 id="26d6" class="lx ly jg bd lz ma mb dn mc md me dp mf le mg mh mi li mj mk ml lm mm mn mo mp bi translated">参考资料:</h2><ul class=""><li id="0aa9" class="nl nm jg kx b ky mq lb mr le nn li no lm np lq nq nr ns nt bi translated"><a class="ae jd" href="https://liferay.de.dariah.eu/tatom/" rel="noopener ugc nofollow" target="_blank">人文社会科学主题模型文本分析</a></li><li id="cb42" class="nl nm jg kx b ky nu lb nv le nw li nx lm ny lq nq nr ns nt bi translated"><a class="ae jd" href="https://www.manning.com/books/natural-language-processing-in-action" rel="noopener ugc nofollow" target="_blank">自然语言处理在行动</a></li></ul></div></div>    
</body>
</html>