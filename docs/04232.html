<html>
<head>
<title>How to Containerize Models Trained in Spark: MLLib, ONNX and more</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将Spark中训练的模型容器化:MLLib、ONNX等等</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-containerize-models-trained-in-spark-f7ed9265f5c9?source=collection_archive---------21-----------------------#2020-04-17">https://towardsdatascience.com/how-to-containerize-models-trained-in-spark-f7ed9265f5c9?source=collection_archive---------21-----------------------#2020-04-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="51a4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">将ML运送到生产部门</h2><div class=""/><div class=""><h2 id="56bb" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">以经济高效的方式包装Spark模型的替代方案</h2></div><p id="b33d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们要说出现代应用中的两个趋势，其中一个应该提到<strong class="kt jd">容器</strong>和<strong class="kt jd">机器学习</strong>。越来越多的应用程序正在利用容器化的微服务架构来提高弹性、容错能力和可扩展性，无论是在内部还是在云中。同时，机器学习越来越成为任何应用的基本要求。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/1a46086785329afc201e5eeb6a18aa0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WwCHIfcqyrG92poY6XPYDQ.png"/></div></div></figure><p id="90e2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，这两件事并不总是能和谐相处，Spark就是这种情况。通过使用Cloudera、Azure Databricks、AWS EMR或任何其他工具，Spark现在已经成为大数据分析的行业事实标准。<strong class="kt jd">但是如果您想在集群之外的某个地方使用您在Spark中训练的模型呢？</strong></p><h2 id="b9f5" class="lz ma it bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq iz bi translated">您的选择，<strong class="ak">按菜单点菜</strong></h2><p id="3284" class="pw-post-body-paragraph kr ks it kt b ku mr kd kw kx ms kg kz la mt lc ld le mu lg lh li mv lk ll lm im bi translated">对此有几个选项:</p><ol class=""><li id="9147" class="mw mx it kt b ku kv kx ky la my le mz li na lm nb nc nd ne bi translated">如果您<strong class="kt jd">使用TensorFlow、PyTorch、Scikit-Learn等工具训练您的模型，</strong>那么您可以将您的模型打包成不同的可移植格式。因为这些框架并不直接依赖于Spark，所以您可以开始了。我不会在这篇文章中谈论这个选项。</li><li id="a7ec" class="mw mx it kt b ku nf kx ng la nh le ni li nj lm nb nc nd ne bi translated">如果<strong class="kt jd">使用MLLib </strong>(比如在pyspark.ml.*)训练模型，那么可以将模型导出为可移植的格式，比如<strong class="kt jd"> ONNX </strong>，然后使用ONNX运行时来运行模型。这有一些限制，因为不是所有的模型都支持ONNX。</li><li id="6ca1" class="mw mx it kt b ku nf kx ng la nh le ni li nj lm nb nc nd ne bi translated">如果您<strong class="kt jd">使用MLLib </strong>训练您的模型，那么您可以通过创建一个<em class="nk">无集群</em> Spark上下文对象来持久化您的模型并从容器内部加载它。</li><li id="879d" class="mw mx it kt b ku nf kx ng la nh le ni li nj lm nb nc nd ne bi translated"><strong class="kt jd">(2022年2月更新)</strong>使用MLFlow持久化您的模型，并使用MLModel规范打包它。MLFlow目前支持Spark，它能够使用MLModel规范打包您的模型。您可以使用MLFlow将您的模型部署到任何您想要的地方。关于这个策略的更多细节请看我的帖子:<a class="ae nl" href="https://santiagof.medium.com/effortless-models-deployment-with-mlflow-2b1b443ff157" rel="noopener">使用MLFlow </a>轻松部署。</li></ol><blockquote class="nm nn no"><p id="8b83" class="kr ks nk kt b ku kv kd kw kx ky kg kz np lb lc ld nq lf lg lh nr lj lk ll lm im bi translated">ONNX是机器学习模型的序列化和规范的开放标准。由于该格式描述了计算图(输入、输出和操作)，所以它是独立的。它专注于深度学习，主要由微软和脸书倡导。在TensorFlow和PyTorch等许多框架中得到支持。</p></blockquote><h2 id="6315" class="lz ma it bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq iz bi translated">Spark中的模型是如何工作的？</h2><p id="d869" class="pw-post-body-paragraph kr ks it kt b ku mr kd kw kx ms kg kz la mt lc ld le mu lg lh li mv lk ll lm im bi translated">在Spark中，<a class="ae nl" href="https://spark.apache.org/docs/latest/ml-pipeline.html" rel="noopener ugc nofollow" target="_blank">一个ML模型由一个</a> <code class="fe ns nt nu nv b"><a class="ae nl" href="https://spark.apache.org/docs/latest/ml-pipeline.html" rel="noopener ugc nofollow" target="_blank">Transformer</a></code>表示，它将一个具有特征的输入<code class="fe ns nt nu nv b">DataFrame</code>转换成另一个具有预测的<code class="fe ns nt nu nv b">DataFrame</code>。一个<code class="fe ns nt nu nv b">Transformer</code>还可以输出另一组特征。因为在机器学习中，你通常必须执行几个步骤来产生想要的预测，Spark有一个<code class="fe ns nt nu nv b">PipelineModel</code>对象，它是一系列步骤或<code class="fe ns nt nu nv b">Transformers</code>(注意，管道对象有点复杂，但为了本文的简单起见，我们将假设它是这样)。它将多个<code class="fe ns nt nu nv b">Transformers</code>链接在一起，以指定一个ML工作流。</p><p id="2df4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">考虑下面这个来自<a class="ae nl" href="https://spark.apache.org/docs/1.6.1/ml-guide.html" rel="noopener ugc nofollow" target="_blank">spark.apache.org</a>的例子，其中我们有一个包含三个阶段的<code class="fe ns nt nu nv b">PipelineModel</code>:一个<strong class="kt jd">标记器</strong>(获取文本并返回单词)、一个<strong class="kt jd">散列函数</strong>(获取单词并返回向量)和一个<strong class="kt jd">逻辑回归模型</strong>(获取特征并返回预测)。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nw"><img src="../Images/cc1dd716809caed42e169e77e0d09a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mdXxEnp3swnpzIK56TMatg.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">文本分析的PipelineModel示例。来源:<a class="ae nl" href="https://spark.apache.org/docs/1.6.1/ml-guide.html" rel="noopener ugc nofollow" target="_blank">spark.apache.org</a></p></figure><p id="88ca" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">您可以通过使用方法<code class="fe ns nt nu nv b">fit()</code>训练管道来获得PipelineModel。这里有一个例子:</p><pre class="lo lp lq lr gt ob nv oc od aw oe bi"><span id="8f9a" class="lz ma it nv b gy of og l oh oi">tokenizer = Tokenizer(inputCol="text", outputCol="words")<br/>hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")<br/>lr = LogisticRegression(maxIter=10, regParam=0.01)<br/>pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])</span><span id="2b63" class="lz ma it nv b gy oj og l oh oi">model = pipeline.fit(training)</span></pre><p id="0467" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在的问题是，如何在Spark之外运行这个PipelineModel对象？</p><h1 id="dcf7" class="ok ma it bd mb ol om on me oo op oq mh ki or kj mk kl os km mn ko ot kp mq ou bi translated">选项1:导出到ONNX并使用ONNX运行时运行模型</h1><p id="d14f" class="pw-post-body-paragraph kr ks it kt b ku mr kd kw kx ms kg kz la mt lc ld le mu lg lh li mv lk ll lm im bi translated">ONNX是为深度学习模型设计的，然而，它在某些方面支持更“传统”的机器学习技术。Spark通常用于那些更传统的方法。<a class="ae nl" href="https://github.com/onnx/onnxmltools/tree/master/onnxmltools/convert/sparkml" rel="noopener ugc nofollow" target="_blank">在Spark中这包括</a>:</p><ul class=""><li id="1e25" class="mw mx it kt b ku kv kx ky la my le mz li na lm ov nc nd ne bi translated">矢量器和编码(字符串索引、OneHotEncoding、Word2Vec)</li><li id="b5fa" class="mw mx it kt b ku nf kx ng la nh le ni li nj lm ov nc nd ne bi translated">定标器(定标器、输入器、二进制化器、存储桶)</li><li id="75fe" class="mw mx it kt b ku nf kx ng la nh le ni li nj lm ov nc nd ne bi translated">模型:线性模型，随机森林，梯度推进，朴素贝叶斯，SVM，主成分分析</li></ul><p id="4f71" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">要将您的模型导出为ONNX格式，您需要首先安装<a class="ae nl" href="https://pypi.org/project/onnxmltools/" rel="noopener ugc nofollow" target="_blank"> onnxmltools </a> <strong class="kt jd">，该工具目前仅适用于PySpark </strong>。安装库的方式取决于您使用的平台。在我的例子中，我使用的是Azure Databricks，所以我将使用集群中的Install library选项来安装这个库。</p><p id="d73c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一旦安装了库，就可以通过以下方式将管线模型导出到ONNX中:</p><pre class="lo lp lq lr gt ob nv oc od aw oe bi"><span id="5a66" class="lz ma it nv b gy of og l oh oi">from onnxmltools import convert_sparkml<br/>from onnxmltools.convert.sparkml.utils import buildInitialTypesSimple</span><span id="97a5" class="lz ma it nv b gy oj og l oh oi">initial_types = buildInitialTypesSimple(test_df.drop("label"))<br/>onnx_model = convert_sparkml(model, 'Pyspark model', initial_types, spark_session = spark)</span></pre><p id="94e5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在哪里，</p><ul class=""><li id="0330" class="mw mx it kt b ku kv kx ky la my le mz li na lm ov nc nd ne bi translated"><code class="fe ns nt nu nv b">buildInitialTypesSimple</code>函数创建一个来自模型(特征)的所有预期输入的列表。它接受一个样本数据帧作为参数。在我的例子中，我使用了test_df数据帧，删除了label列，只保留了所有的特性。</li><li id="7e23" class="mw mx it kt b ku nf kx ng la nh le ni li nj lm ov nc nd ne bi translated">型号是安装管道的名称(<code class="fe ns nt nu nv b">PipelineModel</code>)</li><li id="c9c3" class="mw mx it kt b ku nf kx ng la nh le ni li nj lm ov nc nd ne bi translated">“Pyspark模型”是模型的描述</li><li id="5e6c" class="mw mx it kt b ku nf kx ng la nh le ni li nj lm ov nc nd ne bi translated">initial_types是预期的输入要素名称和类型。正如我所说的，这可以通过使用<code class="fe ns nt nu nv b">buildInitialTypesSimple</code>函数来提供，或者通过手工构建来提供，比如[ ('education '，string sensortype([1，1])]</li><li id="088c" class="mw mx it kt b ku nf kx ng la nh le ni li nj lm ov nc nd ne bi translated"><code class="fe ns nt nu nv b">spark_session=spark</code>将SparkSession上下文传递给该方法。</li></ul><blockquote class="nm nn no"><p id="8419" class="kr ks nk kt b ku kv kd kw kx ky kg kz np lb lc ld nq lf lg lh nr lj lk ll lm im bi translated">注意:<strong class="kt jd">这最后一个参数没有记录在GitHub </strong>库中，但是在调试<strong class="kt jd">之后，我需要指定它以避免使用空的spark上下文。在我的例子中，我使用的是Azure Databricks，所以我不知道这是否特定于Databricks。</strong></p></blockquote><p id="8a8d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一旦模型被转换，您可以通过以下方式将其保存到文件中:</p><pre class="lo lp lq lr gt ob nv oc od aw oe bi"><span id="e9f7" class="lz ma it nv b gy of og l oh oi">with open(os.path.join("/tmp/", "model.onnx"), "wb") as f:<br/>    f.write(onnx_model.SerializeToString())</span></pre><p id="430d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">您可以使用Python中的ONNX运行时轻松加载该模型，如下所示:</p><pre class="lo lp lq lr gt ob nv oc od aw oe bi"><span id="43e8" class="lz ma it nv b gy of og l oh oi">import onnxruntime <br/>session = onnxruntime.InferenceSession(model_file_path, None)<br/>output = session.get_outputs()[0]<br/>inputs = session.get_inputs()</span><span id="adc5" class="lz ma it nv b gy oj og l oh oi">input_data= {i.name: v for i, v in zip(inputs, input_sample.values.reshape(len(inputs),1,1).astype(np.float32))}}</span><span id="97b3" class="lz ma it nv b gy oj og l oh oi">results = session.run([output.name], input_data)</span></pre><blockquote class="nm nn no"><p id="995d" class="kr ks nk kt b ku kv kd kw kx ky kg kz np lb lc ld nq lf lg lh nr lj lk ll lm im bi translated"><strong class="kt jd">注</strong> : <code class="fe ns nt nu nv b">input_data</code>是以键为特征名，以张量(1，1)为值的字典。<code class="fe ns nt nu nv b">reshape</code>函数将输入转换为一个形状为(feature_count，1，1)的张量数组，这是所期望的。将值转换为<code class="fe ns nt nu nv b">float32</code>也很重要。</p></blockquote><p id="0bd7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，您可以将这个模型放在docker容器中，并安装Python和以下库:</p><ul class=""><li id="682d" class="mw mx it kt b ku kv kx ky la my le mz li na lm ov nc nd ne bi translated"><a class="ae nl" href="https://pypi.org/project/onnxmltools/" rel="noopener ugc nofollow" target="_blank"> onnxruntime </a></li></ul><p id="7d0d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">您的评分文件将使用<code class="fe ns nt nu nv b">onnxruntime.InferenceSession()</code>方法加载模型。你通常只做一次。另一方面，您的计分例程将调用<code class="fe ns nt nu nv b">session.run()</code>。</p><p id="6ebc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在下面的GitHub链接中，我有<a class="ae nl" href="https://github.com/santiagxf/portable-sparkml/blob/master/score_onnx.py" rel="noopener ugc nofollow" target="_blank">一个样本评分文件。</a></p><h2 id="f348" class="lz ma it bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq iz bi translated"><strong class="ak">ONNX在Spark中的局限性:</strong></h2><p id="fc44" class="pw-post-body-paragraph kr ks it kt b ku mr kd kw kx ms kg kz la mt lc ld le mu lg lh li mv lk ll lm im bi translated">在撰写本文时，Spark-ONNX转换器缺少以下特性:</p><ul class=""><li id="7e41" class="mw mx it kt b ku kv kx ky la my le mz li na lm ov nc nd ne bi translated">缺少功能哈希、TFIDF、RFormula、NGram、SQLTransformer和某些模型(聚类、FP、ALS等)的导出程序</li><li id="8190" class="mw mx it kt b ku nf kx ng la nh le ni li nj lm ov nc nd ne bi translated">仅支持PySpark</li><li id="7143" class="mw mx it kt b ku nf kx ng la nh le ni li nj lm ov nc nd ne bi translated">一些问题:ONNX规范不支持Tokenizer</li></ul><h1 id="4cbd" class="ok ma it bd mb ol om on me oo op oq mh ki or kj mk kl os km mn ko ot kp mq ou bi translated">选项2:打包PipelineModel并使用Spark上下文运行它</h1><p id="5c51" class="pw-post-body-paragraph kr ks it kt b ku mr kd kw kx ms kg kz la mt lc ld le mu lg lh li mv lk ll lm im bi translated">在容器内部运行PipelineModel的另一种方法是导出模型，并在容器内部创建一个Spark上下文，即使没有可用的集群。</p><p id="8e8a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你想持久化这个PipelineModel，这个对象实现了接口<code class="fe ns nt nu nv b">MLWritable</code>，它扩展了方法<code class="fe ns nt nu nv b">save(path)</code>和<code class="fe ns nt nu nv b">write().overwrite().save(path)</code>。此方法将模型保存在如下所示的文件夹结构中:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ow"><img src="../Images/0ffe23e2f55131005f034ebedd1f6e34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*78SXjNUxhRkPn5RgNgej_w.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">piepline模型的文件夹结构，由VectorAssembler和GradientBoostedRegressor组成。</p></figure><p id="fb8e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这种结构中，PipelineModel保存在一个文件夹结构中，其中详细描述了管道中的所有步骤。在这种情况下，我创建的这个模型有两个阶段:一个VectorAssembler和一个GradientBoostedRegressor。与您可能习惯的相反，所有的结构都需要加载和恢复训练好的模型。如果你使用的是一个模型注册中心，比如MLFlow或者Azure机器学习服务，我建议你把目录压缩到一个归档文件中。下面一行有助于做到这一点:</p><pre class="lo lp lq lr gt ob nv oc od aw oe bi"><span id="5aa3" class="lz ma it nv b gy of og l oh oi">import shutil</span><span id="89c0" class="lz ma it nv b gy oj og l oh oi">model.write().overwrite().save(model_path)<br/>path_drv = shutil.make_archive(model_name, format='zip', base_dir=model_path)</span></pre><p id="cffe" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">请注意<code class="fe ns nt nu nv b">shutil.make_archive</code>将在本地文件系统的驱动节点中创建文件。</p><h2 id="ffe9" class="lz ma it bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq iz bi translated">如何在您的容器内使用它？</h2><p id="1996" class="pw-post-body-paragraph kr ks it kt b ku mr kd kw kx ms kg kz la mt lc ld le mu lg lh li mv lk ll lm im bi translated">因为您将直接加载Spark模型，所以您需要在容器映像中安装<code class="fe ns nt nu nv b">pyspark</code> Python库。然后，在您的评分脚本中，您将创建一个spark会话，将归档文件解压缩到一个文件夹中，并加载PipelineModel对象。</p><pre class="lo lp lq lr gt ob nv oc od aw oe bi"><span id="3401" class="lz ma it nv b gy of og l oh oi">import pyspark<br/>from pyspark.ml import PipelineModel</span><span id="d8e0" class="lz ma it nv b gy oj og l oh oi">spark = pyspark.sql.SparkSession<br/>               .builder.appName("pyspark_runtime").getOrCreate()</span><span id="7eb0" class="lz ma it nv b gy oj og l oh oi">model_unpacked = "./" + model_name<br/>shutil.unpack_archive(model_path, model_unpacked)</span><span id="b964" class="lz ma it nv b gy oj og l oh oi">trainedModel = PipelineModel.load(model_unpacked)</span></pre><p id="4812" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">变量<code class="fe ns nt nu nv b">spark</code>和<code class="fe ns nt nu nv b">trainedModel</code>必须在所有程序中可用。我个人认为这两个变量是全局变量。</p><p id="10ab" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，通过以下方式运行您的模型:</p><pre class="lo lp lq lr gt ob nv oc od aw oe bi"><span id="fc51" class="lz ma it nv b gy of og l oh oi">input_df = spark.createDataFrame(pandas_df)</span><span id="e4f3" class="lz ma it nv b gy oj og l oh oi">predictions = trainedModel.transform(input_df).collect()<br/>preds = [x['prediction'] for x in predictio</span><span id="11ec" class="lz ma it nv b gy oj og l oh oi">print('[INFO] Results was ' + json.dumps(preds))</span></pre><p id="9cfe" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我在下面的GitHub链接中有一个样本评分文件。</p><h1 id="0ead" class="ok ma it bd mb ol om on me oo op oq mh ki or kj mk kl os km mn ko ot kp mq ou bi translated">结论</h1><p id="8e82" class="pw-post-body-paragraph kr ks it kt b ku mr kd kw kx ms kg kz la mt lc ld le mu lg lh li mv lk ll lm im bi translated">在本文中，我们回顾了两种不同的方法来移植在Spark集群中训练的模型，以便在容器中运行它们。使用开放标准ONNX或在Spark上下文中加载持久模型。如果你想看到例子的完整代码，你可以查看GitHub库。还有一个Databricks记事本可以生成ONNX文件和MLLib zip文件。希望有帮助。</p></div></div>    
</body>
</html>