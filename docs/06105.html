<html>
<head>
<title>Building a Neural Network with a Single Hidden Layer using Numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Numpy构建单隐层神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-neural-network-with-a-single-hidden-layer-using-numpy-923be1180dbf?source=collection_archive---------9-----------------------#2020-05-18">https://towardsdatascience.com/building-a-neural-network-with-a-single-hidden-layer-using-numpy-923be1180dbf?source=collection_archive---------9-----------------------#2020-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e84b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用Numpy实现具有单个隐藏层的两类分类神经网络</p><p id="d79a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<a class="ae ko" rel="noopener" target="_blank" href="/build-a-simple-neural-network-using-numpy-2add9aad6fc8?source=your_stories_page---------------------------">上一篇</a>中，我们讨论了如何使用NumPy制作一个简单的神经网络。在本帖中，我们将讨论如何制作一个具有隐藏层的深度神经网络。</p><ol class=""><li id="1cec" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated"><strong class="js iu">导入库</strong></li></ol><p id="1155" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将导入一些基本的python库，如numpy、matplotlib(用于绘制图形)、sklearn(用于数据挖掘和分析工具)等。这是我们需要的。</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="773e" class="lh li it ld b gy lj lk l ll lm">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.model_selection import train_test_split</span></pre><p id="9c8c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 2。数据集</strong></p><p id="03fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用钞票数据集，该数据集涉及在给定从照片中获取的几个测量值的情况下预测给定钞票是否是真实的。这是一个二元(2类)分类问题。有1，372个具有4个输入变量和1个输出变量的观察值。更多详情请参见<a class="ae ko" href="http://archive.ics.uci.edu/ml/datasets/banknote+authentication" rel="noopener ugc nofollow" target="_blank">链接。</a></p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="4645" class="lh li it ld b gy lj lk l ll lm">data = np.genfromtxt(‘data_banknote_authentication.txt’, delimiter = ‘,’)<br/>X = data[:,:4]<br/>y = data[:, 4]</span></pre><p id="bca7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以使用散点图来可视化数据集。我们可以看到两类(真实和非真实)是可分的。我们的目标是建立一个模型来拟合这些数据，也就是说，我们希望建立一个神经网络模型来定义区域是真实的还是不真实的。</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="7dde" class="lh li it ld b gy lj lk l ll lm">plt.scatter(X[:, 0], X[:, 1], alpha=0.2,<br/> c=y, cmap=’viridis’)<br/>plt.xlabel(‘variance of wavelet’)<br/>plt.ylabel(‘skewness of wavelet’);</span></pre><figure class="ky kz la lb gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi ln"><img src="../Images/0fdfb456843706e089ebf9127861bd56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rde2-gYfSfqZVVLV4P7lZw.png"/></div></div></figure><p id="9dbf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，让我们将数据分为训练集和测试集。这可以使用sk learn<em class="lv">train _ test _ split()</em>函数来完成。选择20%的数据用于测试，80%用于训练。此外，我们将检查训练集和测试集的大小。这将有助于以后设计我们的神经网络模型。</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="b97c" class="lh li it ld b gy lj lk l ll lm">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span><span id="3565" class="lh li it ld b gy lw lk l ll lm">X_train = X_train.T<br/>y_train = y_train.reshape(1, y_train.shape[0])</span><span id="0f93" class="lh li it ld b gy lw lk l ll lm">X_test = X_test.T<br/>y_test = y_test.reshape(1, y_test.shape[0])</span><span id="defe" class="lh li it ld b gy lw lk l ll lm">print (‘Train X Shape: ‘, X_train.shape)<br/>print (‘Train Y Shape: ‘, y_train.shape)<br/>print (‘I have m = %d training examples!’ % (X_train.shape[1]))<br/><br/>print ('\nTest X Shape: ', X_test.shape)</span></pre><figure class="ky kz la lb gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lx"><img src="../Images/6771a4ab036427284afd5be1b5171553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAqKtklsnvs5OcZghJq0cw.png"/></div></div></figure><p id="af3c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3。神经网络模型</strong></p><p id="956b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">构建神经网络的一般方法是:</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="f659" class="lh li it ld b gy lj lk l ll lm">1. Define the neural network structure ( # of input units,  # of hidden units, etc). <br/>2. Initialize the model's parameters<br/>3. Loop:<br/>    - Implement forward propagation<br/>    - Compute loss<br/>    - Implement backward propagation to get the gradients<br/>    - Update parameters (gradient descent)</span></pre><p id="e1d5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将构建一个具有单一隐藏层的神经网络，如下图所示:</p><figure class="ky kz la lb gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi ly"><img src="../Images/7199eb5af9eac8c65207db9ff1e5ec2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uzTQjsb5sKNED_45Cu5aUA.png"/></div></div></figure><p id="c2f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3.1定义结构</strong></p><p id="a0f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要定义输入单元的数量、隐藏单元的数量和输出层。输入单位等于数据集中的要素数量(4)，隐藏层设置为4(为此)，问题是我们将使用单一图层输出的二进制分类。</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="77d1" class="lh li it ld b gy lj lk l ll lm">def <strong class="ld iu">define_structure</strong>(X, Y):<br/>    input_unit = X.shape[0] # size of input layer<br/>    hidden_unit = 4 #hidden layer of size 4<br/>    output_unit = Y.shape[0] # size of output layer<br/>    return (input_unit, hidden_unit, output_unit)</span><span id="1da9" class="lh li it ld b gy lw lk l ll lm">(input_unit, hidden_unit, output_unit) = <strong class="ld iu">define_structure</strong>(X_train, y_train)<br/>print("The size of the input layer is:  = " + str(input_unit))<br/>print("The size of the hidden layer is:  = " + str(hidden_unit))<br/>print("The size of the output layer is:  = " + str(output_unit))</span></pre><figure class="ky kz la lb gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lz"><img src="../Images/097c15de3a1f0efd693ae1d416dcdfcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Eayp7GKwrYW3-5Q3C6A0g.png"/></div></div></figure><p id="b9f8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3.2初始化模型参数</strong></p><p id="c0e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要初始化权重矩阵和偏置向量。当偏差设置为零时，权重被随机初始化。这可以使用下面的函数来完成。</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="510c" class="lh li it ld b gy lj lk l ll lm">def <strong class="ld iu">parameters_initialization</strong>(input_unit, hidden_unit, output_unit):<br/>    np.random.seed(2) <br/>    W1 = np.random.randn(hidden_unit, input_unit)*0.01<br/>    b1 = np.zeros((hidden_unit, 1))<br/>    W2 = np.random.randn(output_unit, hidden_unit)*0.01<br/>    b2 = np.zeros((output_unit, 1))<br/>    parameters = {"W1": W1,<br/>                  "b1": b1,<br/>                  "W2": W2,<br/>                  "b2": b2}<br/>    <br/>    return parameters</span></pre><p id="5b7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3.3.1正向传播</strong></p><p id="eb6b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于正向传播，给定一组输入特征(X)，我们需要计算每一层的激活函数。对于隐藏层，我们使用<strong class="js iu"> tanh </strong>激活函数:</p><figure class="ky kz la lb gt lo gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/f3767c6521f7b1684a5d8139a99b3b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*mhWIgRP9hphQLyFMKVgO_A.png"/></div></figure><figure class="ky kz la lb gt lo gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/eba1afaf39f8eed185d50f4fe2f192ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*UqIf10cyDcqB-5NUQNIAUg.png"/></div></figure><p id="0a8a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样，对于输出层，我们使用sigmoid激活函数。</p><figure class="ky kz la lb gt lo gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/a934d9cdb6fa5dc275881f81d1dbce56.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*bM-DJpSISMjAGEamnmis5g.png"/></div></figure><figure class="ky kz la lb gt lo gh gi paragraph-image"><div class="gh gi md"><img src="../Images/8f99b142e777a89456f84c29b2d284cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*k5yZN1cv4qi0L_9Lc9Uzyg.png"/></div></figure><p id="ef34" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以使用下面的代码来实现向前传播。</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="601f" class="lh li it ld b gy lj lk l ll lm">def <strong class="ld iu">sigmoid</strong>(z):<br/>    return 1/(1+np.exp(-z))</span><span id="747e" class="lh li it ld b gy lw lk l ll lm">def <strong class="ld iu">forward_propagation</strong>(X, parameters):<br/>    W1 = parameters['W1']<br/>    b1 = parameters['b1']<br/>    W2 = parameters['W2']<br/>    b2 = parameters['b2']<br/>    <br/>    Z1 = np.dot(W1, X) + b1<br/>    A1 = np.tanh(Z1)<br/>    Z2 = np.dot(W2, A1) + b2<br/>    A2 = sigmoid(Z2)<br/>    cache = {"Z1": Z1,"A1": A1,"Z2": Z2,"A2": A2}<br/>    <br/>    return A2, cache</span></pre><p id="1054" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.3.2计算成本</p><p id="8e4b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将计算交叉熵成本。在上一节中，我们计算了A2。使用A2，我们可以使用以下公式计算交叉熵成本。</p><figure class="ky kz la lb gt lo gh gi paragraph-image"><div class="gh gi me"><img src="../Images/d22632929230d0e612f9287c0890480c.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*ejahWVN7FidYqwPzBQYn1g.png"/></div></figure><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="ad3f" class="lh li it ld b gy lj lk l ll lm">def <strong class="ld iu">cross_entropy_cost</strong>(A2, Y, parameters):<br/>    # number of training example<br/>    m = Y.shape[1] <br/>    # Compute the cross-entropy cost<br/>    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1 - A2))<br/>    cost = - np.sum(logprobs) / m<br/>    cost = float(np.squeeze(cost))<br/>                                    <br/>    return cost</span></pre><p id="e4b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3.3.3反向传播</strong></p><p id="2cfa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要计算不同参数的梯度，如下所示。</p><figure class="ky kz la lb gt lo gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/95a52f8673fa6251252af73885a06132.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*OOKSEES8CRYkeMZ5qVTIqQ.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图片提供:<a class="ae ko" href="https://www.coursera.org/learn/neural-networks-deep-learning/" rel="noopener ugc nofollow" target="_blank">吴恩达</a></p></figure><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="1430" class="lh li it ld b gy lj lk l ll lm">def <strong class="ld iu">backward_propagation</strong>(parameters, cache, X, Y):<br/>    #number of training example<br/>    m = X.shape[1]<br/>    <br/>    W1 = parameters['W1']<br/>    W2 = parameters['W2']<br/>    A1 = cache['A1']<br/>    A2 = cache['A2']<br/>   <br/>    dZ2 = A2-Y<br/>    dW2 = (1/m) * np.dot(dZ2, A1.T)<br/>    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)<br/>    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))<br/>    dW1 = (1/m) * np.dot(dZ1, X.T) <br/>    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)<br/>    <br/>    grads = {"dW1": dW1, "db1": db1, "dW2": dW2,"db2": db2}<br/>    <br/>    return grads</span></pre><p id="8082" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3.3.4梯度下降(更新参数)</strong></p><p id="dcb7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要使用梯度下降规则更新参数，即</p><figure class="ky kz la lb gt lo gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/bbb2ea5678acb752985381225c5f947d.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/format:webp/1*eBzTEHvFYZW5q2Oty-aA6w.png"/></div></figure><p id="5cdd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<strong class="js iu"> 𝛼 </strong>是学习率<strong class="js iu"> 𝜃 </strong>是参数。</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="2200" class="lh li it ld b gy lj lk l ll lm">def <strong class="ld iu">gradient_descent</strong>(parameters, grads, learning_rate = 0.01):<br/>    W1 = parameters['W1']<br/>    b1 = parameters['b1']<br/>    W2 = parameters['W2']<br/>    b2 = parameters['b2']<br/>   <br/>    dW1 = grads['dW1']<br/>    db1 = grads['db1']<br/>    dW2 = grads['dW2']<br/>    db2 = grads['db2']</span><span id="3180" class="lh li it ld b gy lw lk l ll lm">    W1 = W1 - learning_rate * dW1<br/>    b1 = b1 - learning_rate * db1<br/>    W2 = W2 - learning_rate * dW2<br/>    b2 = b2 - learning_rate * db2<br/>    <br/>    parameters = {"W1": W1, "b1": b1,"W2": W2,"b2": b2}<br/>    <br/>    return parameters</span></pre><p id="0ff1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 4。神经网络模型</strong></p><p id="d294" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，把所有的功能放在一起，我们可以建立一个只有一个隐藏层的神经网络模型。</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="617c" class="lh li it ld b gy lj lk l ll lm">def <strong class="ld iu">neural_network_model</strong>(X, Y, hidden_unit, num_iterations = 1000):<br/>    np.random.seed(3)<br/>    input_unit = <strong class="ld iu">define_structure</strong>(X, Y)[0]<br/>    output_unit = <strong class="ld iu">define_structure</strong>(X, Y)[2]<br/>    <br/>    parameters = <strong class="ld iu">parameters_initialization</strong>(input_unit, hidden_unit, output_unit)<br/>   <br/>    W1 = parameters['W1']<br/>    b1 = parameters['b1']<br/>    W2 = parameters['W2']<br/>    b2 = parameters['b2']<br/>    <br/>    for i in range(0, num_iterations):<br/>        A2, cache = <strong class="ld iu">forward_propagation</strong>(X, parameters)<br/>        cost = <strong class="ld iu">cross_entropy_cost</strong>(A2, Y, parameters)<br/>        grads = <strong class="ld iu">backward_propagation</strong>(parameters, cache, X, Y)<br/>        parameters = <strong class="ld iu">gradient_descent</strong>(parameters, grads)<br/>        if i % 5 == 0:<br/>            print ("Cost after iteration %i: %f" %(i, cost))</span><span id="1031" class="lh li it ld b gy lw lk l ll lm">    return parameters</span><span id="d3f9" class="lh li it ld b gy lw lk l ll lm">parameters = <strong class="ld iu">neural_network_model</strong>(X_train, y_train, 4, num_iterations=1000)</span></pre><figure class="ky kz la lb gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lx"><img src="../Images/1efa51d0becb001cbe722d4c6de49c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BijLxY8e4qQjPsLARK4KOw.png"/></div></div></figure><p id="d674" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 5。预测</strong></p><p id="055f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用学习到的参数，我们可以通过使用前向传播来预测每个示例的类。</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="d03c" class="lh li it ld b gy lj lk l ll lm">def <strong class="ld iu">prediction</strong>(parameters, X):<br/>    A2, cache = forward_propagation(X, parameters)<br/>    predictions = np.round(A2)<br/>    <br/>    return predictions</span></pre><p id="071a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果<em class="lv">激活&gt; 0.5，</em>则预测为1否则为0。</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="b865" class="lh li it ld b gy lj lk l ll lm">predictions = <strong class="ld iu">prediction</strong>(parameters, X_train)<br/>print ('Accuracy Train: %d' % float((np.dot(y_train, predictions.T) + np.dot(1 - y_train, 1 - predictions.T))/float(y_train.size)*100) + '%')</span><span id="73ac" class="lh li it ld b gy lw lk l ll lm">predictions = <strong class="ld iu">prediction</strong>(parameters, X_test)<br/>print ('Accuracy Test: %d' % float((np.dot(y_test, predictions.T) + np.dot(1 - y_test, 1 - predictions.T))/float(y_test.size)*100) + '%')</span></pre><figure class="ky kz la lb gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lz"><img src="../Images/0376f1544d550ae36b986e0314a53ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Jx3bsA5zvAoV6ebkJA2RA.png"/></div></div></figure><p id="06fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们所看到的，训练精度约为97%，这意味着我们的模型正在工作，并且以高概率拟合训练数据。测试准确率在96%左右。给定简单的模型和小的数据集，我们可以认为它是一个好的模型。</p><blockquote class="ml mm mn"><p id="6c91" class="jq jr lv js b jt ju jv jw jx jy jz ka mo kc kd ke mp kg kh ki mq kk kl km kn im bi translated">在这里成为Medium会员<a class="ae ko" href="https://medium.com/@rmesfrmpkr/membership" rel="noopener">，支持独立写作，每月5美元，可以完全访问Medium上的每个故事。</a></p></blockquote></div></div>    
</body>
</html>