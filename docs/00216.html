<html>
<head>
<title>How to Use NLP to Find a Tech Job and Win a Hackathon</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用NLP找到技术工作并赢得黑客马拉松</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-nlp-to-find-a-tech-job-and-win-a-hackathon-7aa270ec608d?source=collection_archive---------16-----------------------#2020-01-07">https://towardsdatascience.com/how-to-use-nlp-to-find-a-tech-job-and-win-a-hackathon-7aa270ec608d?source=collection_archive---------16-----------------------#2020-01-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/92872cc8930eefda098cc6c69075b134.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*HVDptrJ5JpivEbEbOgC32A.png"/></div></figure><div class=""/><div class=""><h2 id="68b0" class="pw-subtitle-paragraph jx iz ja bd b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko dk translated">使用网络抓取、NLP和Flask创建一个技术工作搜索网络应用程序</h2></div><p id="f618" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我和我的前端web开发人员Brandon Franks最近在一次30人团队、30小时的黑客马拉松中赢得了“解决的最有趣的数据科学问题”奖。我们的获奖作品是一款Web应用程序，它从Indeed.com收集职位列表，使用自然语言处理(NLP)进行处理，并提供美国前20大科技城市的四大科技职位的职位总数、平均工资范围和前10大技术技能的汇总</p><p id="4a6a" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这四个宽泛的技术职位——数据科学家、网络开发人员、UX设计人员和iOS开发人员——包括了与该职位相关的所有职位。因此，网页开发人员包括网页开发人员、前端工程师、PHP开发人员和许多其他被Indeed.com认为是网页开发人员相关的职位。</p><p id="8561" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">前20大科技城市是根据计算机和数学职业的最高平均工资计算美国前20大都会区(MSA)而确定的。Indeed.com搜索以城市为半径25英里，因此搜索结果包括城市的周边地区。</p><p id="ec11" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">首先，让我们看看成品:</p><p id="45df" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><a class="ae ll" href="https://techjobsearch.herokuapp.com" rel="noopener ugc nofollow" target="_blank"> <strong class="kr jb">点击此处查看科技求职网应用</strong> </a></p><p id="fa5e" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">虽然Web应用程序使用起来非常简单，但它需要大量的幕后数据科学来构建汇总结果。让我们一步一步地完成网页抓取、NLP和应用程序构建的过程，以便在黑客马拉松30小时的时限内将网站变得生动起来。</p><p id="018d" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr jb">关于代码的一句话</strong></p><p id="0217" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">该项目的所有代码、数据和相关文件都可以在my <a class="ae ll" href="https://github.com/JimKing100/techsearch" rel="noopener ugc nofollow" target="_blank"> GitHub </a>访问。自述文件提供了repo目录和文件的详细信息。网页抓取和自然语言处理使用Jupyter笔记本。</p><p id="da04" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr jb">网页抓取</strong></p><p id="f800" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">Web抓取代码使用<a class="ae ll" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>从Indeed.com站点定位并抓取数据。在抓取之前，请务必检查robots.txt文件，以确保所抓取的数据是允许的。我们收集的初始数据:</p><p id="997c" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">1.职位名称<br/> 2。公司<br/> 3。位置<br/> 4。工资<br/> 5。职位描述<br/> 6。作业计数</p><p id="ae98" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">让我们用一段代码来演示这个基本过程。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="d99b" class="lv lw ja lr b gy lx ly l lz ma">def extract_job_title(soup):<br/>    jobs = []<br/>    for div in soup.find_all(name="div", attrs={"class":"row"}):<br/>        for a in div.find_all(name="a", attrs={"data-tn-element":"jobTitle"}):<br/>            jobs.append(a["title"])<br/>    return(jobs)</span><span id="9635" class="lv lw ja lr b gy mb ly l lz ma">city_url = "<a class="ae ll" href="https://www.indeed.com/jobs?q=" rel="noopener ugc nofollow" target="_blank">https://www.indeed.com/jobs?q=</a>" + title_name + \<br/>           "&amp;l=" + city_name + "%2C+" + st_name + \<br/>           "&amp;start=" + str(start)<br/>page = requests.get(city_url)<br/>soup = BeautifulSoup(page.text, "html.parser")</span><span id="9689" class="lv lw ja lr b gy mb ly l lz ma">job_title_list.extend(extract_job_title(soup))</span></pre><p id="a098" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在上面的代码中，<strong class="kr jb"> city_url </strong>是使用职位、城市和州构建的。然后html页面被检索并存储在<strong class="kr jb">页面</strong>变量中。然后，BeautifulSoup将页面解析成html组件，并将它们存储在Beautiful soup对象<strong class="kr jb"> soup </strong>(一个嵌套的数据结构)中。</p><p id="30f3" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">然后调用<strong class="kr jb"> extract_job_title </strong>函数，并将其传递给soup对象。<strong class="kr jb"> soup.find_all </strong>方法定位适当的html并导航到与该值相关联的唯一html组件。在这种情况下，职位被发现为子组件<strong class="kr jb">&lt;a data-TN-element = " Job Title "&gt;</strong>in<strong class="kr jb">&lt;div class = " row "&gt;</strong>。你的浏览器检查成为一个必不可少的工具时，网页抓取！</p><p id="7afe" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">刮擦是费时的。在4个职位和20个城市的80种排列中，每种排列大约需要3个小时才能获得100个职位。因此，Web应用程序不是实时的，需要对数据进行预处理并以某种形式存储。因此，从抓取创建的结果数据帧被下载到一个. csv文件和一个sqlite DB表中。的。csv文件因其易用性和速度最终被应用程序使用。</p><p id="3427" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为每个数据元素创建了类似的函数，并保存到下面的原始数据数据框中:</p><figure class="lm ln lo lp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi mc"><img src="../Images/922f9b032ad0d1eb8cfd5749842e9fd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0QvdDkdomkxXDAX9cJ0wKg.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">事实上_df数据帧—原始刮擦数据</p></figure><p id="9682" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">刮刀的完整代码scraper.ipynb可以在<a class="ae ll" href="https://github.com/JimKing100/techsearch/blob/master/preprocess/scraper.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="079a" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr jb">自然语言处理</strong></p><p id="974b" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们现在有了来自网络搜集的原始数据，但我们仍然需要为每个宽泛的职位确定顶级技术术语。职位描述是一大块文本，因此自然语言处理是从每个职位描述中提取关键技术术语的理想方法。一般步骤如下:</p><ol class=""><li id="8c8b" class="ml mm ja kr b ks kt kv kw ky mn lc mo lg mp lk mq mr ms mt bi translated"><em class="mu">标记化</em> —获取原始文本，并将其简化为不含非字母数字字符(标记)的小写单词列表。</li><li id="f1f9" class="ml mm ja kr b ks mv kv mw ky mx lc my lg mz lk mq mr ms mt bi translated"><em class="mu">去掉停用词</em>——去掉我们可以忽略的常用词(如“我”、“和”、“the”等)。).</li><li id="7620" class="ml mm ja kr b ks mv kv mw ky mx lc my lg mz lk mq mr ms mt bi translated"><em class="mu">引理化</em> —将记号转换成它们的基本形式，称为引理(例如，游程、running和ran都是引理游程的形式)。</li><li id="3432" class="ml mm ja kr b ks mv kv mw ky mx lc my lg mz lk mq mr ms mt bi translated"><em class="mu">减少</em> —通过大约100个技术术语的技术术语字典运行基本令牌，以减少基本令牌的数量。</li><li id="7ad2" class="ml mm ja kr b ks mv kv mw ky mx lc my lg mz lk mq mr ms mt bi translated"><em class="mu">计数和排名</em> —计数基本科技令牌，对其进行排名，并将其作为列表存储在数据帧中。</li></ol><p id="fa6b" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr jb">标记化、删除停用词并词条化</strong></p><p id="8ffe" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">令牌化过程需要两个步骤。清理文本的预处理步骤和实际的标记化步骤。</p><p id="6dcb" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">以下代码片段中的文本清理使用了BeautifulSoup、replace、regex、sub和lower等方法的组合，将文本转换为小写字母数字单词。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="457f" class="lv lw ja lr b gy lx ly l lz ma"># Clean the text<br/>def clean_text(text):<br/>    text = text.replace('\n', ' ')                # remove newline<br/>    text = BeautifulSoup(text, "lxml").get_text() # remove html<br/>    text = text.replace('/', ' ')                 # remove forward slashes<br/>    text = re.sub(r'[^a-zA-Z ^0-9]', '', text)    # letters and numbers only<br/>    text = text.lower()                           # lower case<br/>    text = re.sub(r'(x.[0-9])', '', text)         # remove special characters<br/>    return text</span><span id="9e2f" class="lv lw ja lr b gy mb ly l lz ma">df['description'] = df.apply(lambda x: clean_text(x['description']), axis=1)</span></pre><p id="6d6f" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">然后使用<a class="ae ll" href="https://spacy.io/usage/spacy-101" rel="noopener ugc nofollow" target="_blank"> spacy </a>分词器对文本进行分词。Spacy是用于高级NLP的免费开源库。在下面的代码片段中，spacy <strong class="kr jb">记号赋予器</strong>用英语语言模型(大号)初始化，并添加任何额外的停用词。在这种情况下，只有“year”被添加到停用字词中，因为我们将在代码的后面删除许多其他字词。</p><p id="5786" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">初始化后，使用<strong class="kr jb"> tokenizer.pipe </strong>将文本从<strong class="kr jb">描述</strong>列读入<strong class="kr jb">文档</strong>。对于每个文档，<strong class="kr jb"> token.lemma_ </strong>被附加到一个列表中。这为数据帧的每一行产生了一个词条列表。最终的列表随后被添加为名为<strong class="kr jb">令牌</strong>的列。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="66eb" class="lv lw ja lr b gy lx ly l lz ma"># Initialize the tokenizer<br/>nlp = spacy.load("en_core_web_lg")<br/>tokenizer = Tokenizer(nlp.vocab)<br/>STOP_WORDS = nlp.Defaults.stop_words.union(['year'])</span><span id="cbab" class="lv lw ja lr b gy mb ly l lz ma"># Tokenizer pipe removing stop words and blank words and lemmatizing<br/>tokens = []</span><span id="b14b" class="lv lw ja lr b gy mb ly l lz ma">for doc in tokenizer.pipe(df['description'], batch_size=500):<br/>    <br/>    doc_tokens = []<br/>    for token in doc:<br/>        if (token.lemma_ not in STOP_WORDS) &amp; (token.text != ' '):<br/>            doc_tokens.append(token.lemma_)</span><span id="fa89" class="lv lw ja lr b gy mb ly l lz ma">tokens.append(doc_tokens)</span><span id="23db" class="lv lw ja lr b gy mb ly l lz ma">df['tokens'] = tokens</span></pre><p id="1b33" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr jb">将令牌简化为关键技术术语</strong></p><p id="0549" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了过滤掉尽可能多的“噪音”，创建了一个大约100个技术术语的列表，并通过该列表过滤令牌。这将为每个工作列表生成一个技术术语标记列表。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="3c38" class="lv lw ja lr b gy lx ly l lz ma"># Tech terms list<br/>tech_terms = ['python', 'r', 'sql', 'hadoop', 'spark', 'java', 'sas', 'tableau',<br/>              'hive', 'scala', 'aws', 'c', 'c++', 'matlab', 'tensorflow', 'excel',<br/>              'nosql', 'linux', 'azure', 'scikit', 'machine learning', 'statistic',<br/>              'analysis', 'computer science', 'visual', 'ai', 'deep learning',<br/>              'nlp', 'natural language processing', 'neural network', 'mathematic',<br/>              'database', 'oop', 'blockchain',<br/>              'html', 'css', 'javascript', 'jquery', 'git', 'photoshop', 'illustrator',<br/>              'word press', 'seo', 'responsive design', 'php', 'mobile', 'design', 'react',<br/>              'security', 'ruby', 'fireworks', 'json', 'node', 'express', 'redux', 'ajax',<br/>              'java', 'api', 'state management',<br/>              'wireframe', 'ui prototype', 'ux writing', 'interactive design',<br/>              'metric', 'analytic', 'ux research', 'empathy', 'collaborate', 'mockup', <br/>              'prototype', 'test', 'ideate', 'usability', 'high-fidelity design',<br/>              'framework',<br/>              'swift', 'xcode', 'spatial reasoning', 'human interface', 'core data',<br/>              'grand central', 'network', 'objective-c', 'foundation', 'uikit', <br/>              'cocoatouch', 'spritekit', 'scenekit', 'opengl', 'metal', 'api', 'iot',<br/>              'karma']<br/>              <br/>df['tokens_filtered'] = df.apply(lambda x: list(set(x['tokens']) &amp; set(tech_terms)), axis=1)</span></pre><p id="3187" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr jb">计数和排名</strong></p><p id="2f05" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">对于每一个职位和城市，科技词汇都会被计算和排名。然后，在数据帧的列表中按排名顺序放置前十个术语。按职位和城市<strong class="kr jb"> final_df </strong>汇总的最终数据框架如下:</p><figure class="lm ln lo lp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi na"><img src="../Images/13da6e2b88a072f9bec17c334b6e622f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*POAGzgxRiVjQibwXK-zWuQ.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">final_df数据帧Web应用程序中使用的数据</p></figure><p id="b5f3" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">该数据帧被下载到一个. csv文件中，并提供我们将在Web应用程序中使用的数据。</p><p id="54ce" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">NLP的完整代码，nlp.ipynb，可以在找到。</p><p id="fe2f" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr jb">烧瓶App </strong></p><p id="8470" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">Flask是一个简单的Python Web开发框架，非常适合像这样的小型项目。本文将不讨论使用Flask的细节，但是将提供代码的概述。</p><p id="ae0e" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">代码的基本布局如下:</p><figure class="lm ln lo lp gt iv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/61d880701d3047eccbb67b0bccc88942.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*m362oG20bz2C34gtzF0OKw.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">烧瓶布局</p></figure><p id="482f" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">pip文件为应用程序提供环境和依赖关系。<strong class="kr jb"> __init__。py </strong>文件保存初始化代码，<strong class="kr jb">静态</strong>目录包含图像、css、javascript等静态内容，<strong class="kr jb">模板</strong>目录包含html文件。烧瓶应用程序的主要代码包含在<strong class="kr jb"> app.py </strong>中，如下所示:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="0806" class="lv lw ja lr b gy lx ly l lz ma"># Import Flask package</span><span id="2ea9" class="lv lw ja lr b gy mb ly l lz ma">from flask import Flask, request, render_template<br/>import pandas as pd<br/>import re<br/>import string</span><span id="3f24" class="lv lw ja lr b gy mb ly l lz ma">def create_app():<br/>    # Create Flask web server, makes the application<br/>    app = Flask(__name__)</span><span id="2644" class="lv lw ja lr b gy mb ly l lz ma"># Routes determine location<br/>    <a class="ae ll" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route("/")<br/>    def home():<br/>        return render_template('index.html')</span><span id="1e02" class="lv lw ja lr b gy mb ly l lz ma"><a class="ae ll" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route("/search", methods=['GET'])<br/>    def input():<br/>        return render_template('search.html')</span><span id="354a" class="lv lw ja lr b gy mb ly l lz ma"><a class="ae ll" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route("/output", methods=['POST'])<br/>    def output():<br/>        df = pd.read_csv('<a class="ae ll" href="https://raw.githubusercontent.com/JimKing100/techsearch/master/data/scrape_results1.csv'" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/JimKing100/techsearch/master/data/scrape_results1.csv'</a>)<br/>        df = df.drop(df.columns[0], axis=1)</span><span id="7d46" class="lv lw ja lr b gy mb ly l lz ma">title = request.values['title']<br/>        city = request.values['city']<br/>        result_df = df.loc[(df['job'] == title) &amp; (df['city'] == city)]<br/>        r_title = result_df['job'].iloc[0]<br/>        r_city = result_df['city'].iloc[0]<br/>        r_count = result_df['counts'].iloc[0]<br/>        r_lsalary = result_df['low_salary'].iloc[0]<br/>        r_hsalary = result_df['high_salary'].iloc[0]</span><span id="5bf2" class="lv lw ja lr b gy mb ly l lz ma">r_skills = re.sub('['+string.punctuation+']', '', result_df['skills'].iloc[0]).split()</span><span id="a82a" class="lv lw ja lr b gy mb ly l lz ma">return render_template('response.html',<br/>                               title='Search Results',<br/>                               r_title=r_title,<br/>                               r_city=r_city,<br/>                               r_count=r_count,<br/>                               r_lsalary=r_lsalary,<br/>                               r_hsalary=r_hsalary,<br/>                               r_skills=r_skills<br/>                               )</span><span id="e806" class="lv lw ja lr b gy mb ly l lz ma"><a class="ae ll" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route("/about")<br/>    def about():<br/>        return render_template('about.html')</span><span id="3652" class="lv lw ja lr b gy mb ly l lz ma">return app</span></pre><p id="db97" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">Flask使用routes为函数分配URL。因此“/”路由呈现索引或主页，“/about”路由呈现about页面，依此类推。当用户从搜索页面的下拉列表中选择一个职位和城市并点击搜索按钮时，POST方法调用“/output”路径，执行实际的搜索并呈现结果页面。</p><p id="fd8b" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">Python代码相当简单。加载包含抓取和NLP的最终结果的数据帧，请求职务和城市的值，提取基于职务和城市的数据子集，并将结果传递给页面呈现器。</p><p id="707c" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这总结了创建一个获胜的黑客马拉松项目的整个过程！</p><p id="9014" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我欢迎建设性的批评和反馈，请随时给我发私信。</p><p id="97f4" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在推特上关注我</p><p id="24a4" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这篇文章最初出现在我的<a class="ae ll" href="https://jimking100.github.io/2020-01-05-Post-8/" rel="noopener ugc nofollow" target="_blank"> GitHub页面</a>网站上</p><p id="6676" class="pw-post-body-paragraph kp kq ja kr b ks kt kb ku kv kw ke kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">【Indeed.com】科技工作数据来源</p></div></div>    
</body>
</html>