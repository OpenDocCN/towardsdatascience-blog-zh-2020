<html>
<head>
<title>Statistics in ML: Why Sample Variance Divided by n Is Still a Good Estimator</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML中的统计学:为什么样本方差除以n仍然是一个好的估计量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/statistics-in-ml-estimating-population-variance-1f484184f247?source=collection_archive---------22-----------------------#2020-04-20">https://towardsdatascience.com/statistics-in-ml-estimating-population-variance-1f484184f247?source=collection_archive---------22-----------------------#2020-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b56c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解为什么我们在样本方差中使用(n1 ),以及为什么使用n仍能给出一个很好的总体方差估计量。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ecaf5759cd398c5ff8e07350cf36eb86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDm1kbC3crP2GG6YkP8u6A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">皮沙贝</a>的<a class="ae kv" href="https://pixabay.com/users/tama66-1032521/" rel="noopener ugc nofollow" target="_blank">彼得H </a></p></figure><p id="ce7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文讨论我们如何估计正态分布的总体方差，通常表示为<strong class="ky ir"><em class="ls">【σ</em></strong>。通常，我们使用样本方差:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/edd9bd5b31908da3d306f9898a1d7faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*9ra0KAuNVG2ApruOD1EGgg.png"/></div></figure><p id="070c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，<strong class="ky ir">我们为什么要用<em class="ls"> (n - 1) </em>而不是<em class="ls"> n </em> </strong>来除平方和，其中<strong class="ky ir"> <em class="ls"> n </em> </strong>代表样本大小，从而得到样本方差，这一点并不直观。在统计学中，这通常被称为<a class="ae kv" href="https://en.wikipedia.org/wiki/Bessel%27s_correction" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">贝塞尔修正</strong> </a>。另一个可行的估计量是用平方和除以样本量得到的，它是总体方差的<strong class="ky ir">最大似然估计量</strong> (MLE):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/edfd7494e087855015e2b1eb87da5f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*EpniXe1O5c3V7HcO2O4Ohw.png"/></div></figure><p id="2df1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">哪个比较好？</strong></p><p id="9b08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过应用<a class="ae kv" href="https://www.wikiwand.com/en/Bias%E2%80%93variance_tradeoff" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">偏差-方差分解</strong> </a> <strong class="ky ir"> </strong>和<strong class="ky ir"> </strong> <a class="ae kv" href="https://www.wikiwand.com/en/Cochran%27s_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">柯克兰定理</strong> </a>，本文试图解决这些问题。它的结论是:</p><ul class=""><li id="f131" class="lv lw iq ky b kz la lc ld lf lx lj ly ln lz lr ma mb mc md bi translated">我们在样本方差中使用<em class="ls">(n1)</em>，因为我们想要获得总体方差的无偏估计量。</li><li id="8c17" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">将平方和除以n仍然给了我们一个很好的估计值，因为它具有更低的<strong class="ky ir">均方误差</strong> (MSE)。</li></ul><p id="d215" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还将回顾一个用Python实现的实验，从数值上验证我们的结论。敬请期待！</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><p id="5d0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我们有一个样本<em class="ls"> x₁ </em>、<em class="ls"> x₂ </em>、…、<em class="ls"> xi、</em>其中所有<em class="ls"> xi </em>都是<strong class="ky ir">独立同分布</strong> ( <em class="ls"> iid </em>)根据<em class="ls"> N(μ，σ ) </em>。我们正在考虑总体方差<em class="ls"> σ </em>的两种估计量:样本方差估计量和MLE估计量。</p><h1 id="ab5b" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated"><strong class="ak">评估估计量:偏差、方差和均方误差</strong></h1><p id="eec6" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">我们将首先引入一些度量来评估这些估计量，即<strong class="ky ir">偏差</strong>、<strong class="ky ir">方差</strong>和MSE。假设我们正在估计总体参数<strong class="ky ir"> <em class="ls"> θ </em> </strong>并且我们的估计量是数据的函数，那么我们可以定义:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/ac62f26e9cb27d048421c3ba8a017db7.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*dzMZCZIpZjxiBoAd7kC3Pg.png"/></div></figure><p id="ecde" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">直观地说，偏差衡量我们的估计偏离潜在参数的程度。因为我们的估计值会随着数据而变化，所以方差衡量的是它们偏离不同数据集平均值的期望值。MSE是一个综合的度量，可以分解为<strong class="ky ir">(偏差+方差)</strong>如下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/402ddfe287e26d4aaa63c7497a10f52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eFMi29iIqUs0T_e-d9kqVA.png"/></div></div></figure><p id="a636" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面几节中，我们将应用柯克兰定理来推导我们两个估计量的偏差和方差，并进行比较。</p><h1 id="4791" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">采样离散</h1><p id="176c" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">科克伦定理通常用于证明<a class="ae kv" href="https://www.wikiwand.com/en/Analysis_of_variance" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">方差分析</strong> </a> (ANOVA)中使用的统计数据的概率分布。我们将跳过证明，简单地将其应用于我们的情况。</p><p id="655f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">科克伦定理表明，从标准正态生成的一组<em class="ls"> iid </em>随机变量的平方和具有<a class="ae kv" href="https://www.wikiwand.com/en/Chi-squared_distribution" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">卡方分布</strong> </a>和<em class="ls"> (n - 1) </em>自由度。换句话说:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/629ce41096b342ceee5d72a7aecfa8fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*VaLrqTdT_NeOrlPTc0RtAw.png"/></div></figure><p id="f225" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c7f847737cb9332194a20c23ac805d31.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*AOOvJ5Hpb3CXW33-UC_cIA.png"/></div></figure><p id="7d15" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由此可见，样本方差是总体方差的理想无偏估计量。然后我们可以写出它的方差和MSE:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ea35752b5b1993787602585d9983194c.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*xhsJckDEyiPJU7ksHuI4Kg.png"/></div></figure><h1 id="68ec" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">极大似然估计量</h1><p id="d870" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">以同样的方式，我们可以导出总体方差的MLE估计的偏差、方差和均方误差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e48a8c1f44e7c441bba528025331901a.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*L9AORJ25R-weY-yoRXX6UA.png"/></div></figure><p id="cf7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由此我们知道了为什么我们一般会用平方和除以<em class="ls"> (n - 1) </em>来计算样本方差。MLE估计量是总体方差的有偏估计量，它引入了向下的偏差(低估了参数)。偏差的大小与总体方差成正比，并且随着样本量的增加而减小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ded8d1ea3ac4f0ed0de784da898e4b07.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*zFwcSSti6nb9NAPQMNNFAA.png"/></div></figure><p id="1bc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们发现极大似然估计具有较小的方差。差距的大小与人口方差成正比。此外，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d2e0f70144ecc1412c22b20d28559397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*A4HfJhMcpjwKnaHAEVfvWw.png"/></div></figure><p id="362a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，随着样本容量的增大，MLE估计量对样本方差的优势减小。写出MSE并比较:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/bd5c051d344986894cb8101843f50d5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*Ysrinz4K8diZFxm41LeF9w.png"/></div></figure><p id="ffa1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们发现极大似然估计量也有一个较小的均方误差。正如预期的那样，缺口的大小与总体方差成正比，并且随着样本量的增加而减小。</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="28ba" class="mq mr iq bd ms mt nw mv mw mx nx mz na jw ny jx nc jz nz ka ne kc oa kd ng nh bi translated">数值实验</h1><p id="2824" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">在本节中，我们将验证上面得出的结论。我们将从<em class="ls"> N(0，σ ) </em>中生成100，000个大小为<em class="ls"> n </em>的样本。<strong class="ky ir"> <em class="ls"> X </em> </strong>形状为<em class="ls"> n × 100000 </em>，每个列向量代表一个形状为<em class="ls"> n × 1 </em>的样本。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="8161" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将从<em class="ls"> n </em> = 10和<em class="ls"> σ </em> = 1开始。请注意参数<code class="fe od oe of og b">ddof</code>的使用，因为它指定了从估计量的样本量中减去什么。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/b6874fc5a4852ce67fff3aee53e9509c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aT1ZdJNcMCN3mVurr_Tmpw.png"/></div></div></figure><p id="9e20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如所料，最大似然估计引入了向下偏差，而样本方差估计的向下偏差可以忽略不计。同时，极大似然估计具有较低的方差和均方误差。</p><p id="f2c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将改变样本大小和总体方差的配置，看看样本方差估计量和最大似然估计量之间的偏差、方差和均方误差会发生什么变化:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/160bc1e2cad4b6c393dcc5a5e0c81d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DSiQMCupX-h0o7S-nthVog.png"/></div></div></figure><p id="358a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如所料，样本方差估计量和最大似然估计量之间的偏差、方差和均方误差随着总体方差的增加而增加，随着样本量的增加而急剧减少。检查我用来生成可视化效果的代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><p id="d134" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章是何坤宇写的。Kunyu目前是芝加哥大学计算分析专业的硕士生。他发现理解统计建模和机器学习技术、将它们应用于真实世界的数据并帮助创建金融服务行业的端到端解决方案是一件有趣的事情。在LinkedIn上联系昆玉！🐷</p><div class="oj ok gp gr ol om"><a href="https://www.linkedin.com/in/kunyuhe/" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ir gy z fp or fr fs os fu fw ip bi translated">昆玉何-即将上任的全球量化策略非周期分析师-美银美林…</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">芝加哥大学正在接受理学硕士计算分析项目培训的数据科学家。对…充满热情</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">www.linkedin.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa kp om"/></div></div></a></div></div></div>    
</body>
</html>