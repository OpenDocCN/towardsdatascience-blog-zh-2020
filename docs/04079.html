<html>
<head>
<title>Latent Stochastic Differential Equations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">潜在随机微分方程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/latent-stochastic-differential-equations-a0bac74ada00?source=collection_archive---------30-----------------------#2020-04-14">https://towardsdatascience.com/latent-stochastic-differential-equations-a0bac74ada00?source=collection_archive---------30-----------------------#2020-04-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="83df" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" rel="noopener" target="_blank" href="https://towardsdatascience.com/event-talks/home">活动讲座</a></h2><div class=""/><div class=""><h2 id="1adf" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">大卫·杜文瑙德| TMLS2019</h2></div><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="kt ku l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">来自多伦多机器学习峰会的演讲:【https://torontomachinelearning.com/ T2】</p></figure><h2 id="570a" class="la lb iq bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu iw bi translated">关于演讲者:</h2><p id="746e" class="pw-post-body-paragraph lv lw iq lx b ly lz ka ma mb mc kd md lj me mf mg ln mh mi mj lr mk ml mm mn ij bi translated">David Duvenaud 是多伦多大学计算机科学和统计学的助理教授。他是加拿大研究生成模型的主席。他的博士后研究是在哈佛大学完成的，在那里他从事超参数优化、变分推理和化学设计。他在剑桥大学获得博士学位，与邹斌·格拉马尼和卡尔·拉斯姆森一起研究贝叶斯非参数。大卫在谷歌研究院的机器视觉团队呆了两个夏天，还联合创立了能源预测和交易公司 Invenia。David 是 Vector Institute 的创始成员，也是 ElementAI 的研究员。</p><h2 id="8e4f" class="la lb iq bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu iw bi translated">关于演讲:</h2><p id="ea80" class="pw-post-body-paragraph lv lw iq lx b ly lz ka ma mb mc kd md lj me mf mg ln mh mi mj lr mk ml mm mn ij bi translated">许多真实世界的数据是不定期采样的，但是大多数时间序列模型需要定期采样的数据。连续时间潜变量模型可以解决这个问题，但直到现在，只有确定性模型，如潜在常微分方程，是有效的反向传播训练。我们将伴随灵敏度方法推广到随机微分方程，构造了一个 SDE，它在时间上向后运行并计算所有必要的梯度，以及一个通用算法，该算法允许随机微分方程通过具有恒定存储成本的反向传播来训练。我们还给出了函数空间中基于梯度的随机变分推理的有效算法，所有算法都使用了自适应黑盒 SDE 解算器。最后，我们将展示对时间序列数据应用潜在随机微分方程的初步结果，并讨论无限深度贝叶斯神经网络的原型。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/af5224206b9ddcbd469e00e399e73be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ty8whOPARUrSSpURHYGKHA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><a class="ae kz" href="https://youtu.be/EAsXp8NaCR8" rel="noopener ugc nofollow" target="_blank">潜在随机微分方程</a></p></figure></div></div>    
</body>
</html>