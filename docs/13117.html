<html>
<head>
<title>Confidence intervals for XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost 的置信区间</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/confidence-intervals-for-xgboost-cac2955a8fde?source=collection_archive---------7-----------------------#2020-09-09">https://towardsdatascience.com/confidence-intervals-for-xgboost-cac2955a8fde?source=collection_archive---------7-----------------------#2020-09-09</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><div class=""/><div class=""><h2 id="299a" class="pw-subtitle-paragraph js iu iv bd b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj dk translated">构建正则化分位数回归目标</h2></div><blockquote class="kk kl km"><p id="a413" class="kn ko kp kq b kr ks jw kt ku kv jz kw kx ky kz la lb lc ld le lf lg lh li lj io bi translated">更新:发现我关于渐变提升的新书，<a class="ae lk" href="https://amzn.to/3GqteUN" rel="noopener ugc nofollow" target="_blank">实用渐变提升</a>。这是用 python 中的许多例子对渐变增强的深入探究。</p></blockquote><div class="ll lm gp gr ln lo"><a href="https://amzn.to/3GqteUN" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab fo"><div class="lq ab lr cl cj ls"><h2 class="bd iw gy z fp lt fr fs lu fu fw iu bi translated">实用的渐变增强:深入探究 Python 中的渐变增强</h2><div class="lv l"><h3 class="bd b gy z fp lt fr fs lu fu fw dk translated">这本书的梯度推进方法是为学生，学者，工程师和数据科学家谁希望…</h3></div></div><div class="lw l"><div class="lx l ly lz ma lw mb mc lo"/></div></div></a></div><p id="be7e" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">梯度增强方法是一种非常强大的工具，可对大型数据集、非线性依赖于大量要素的复杂变量快速执行精确预测。潜在的数学原理在我的另一篇文章中有解释:</p><div class="ll lm gp gr ln lo"><a rel="noopener follow" target="_blank" href="/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9"><div class="lp ab fo"><div class="lq ab lr cl cj ls"><h2 class="bd iw gy z fp lt fr fs lu fu fw iu bi translated">用不到 200 行 python 代码 DIY XGBoost 库</h2><div class="lv l"><h3 class="bd b gy z fp lt fr fs lu fu fw dk translated">XGBoost 解释了梯度推进方法和惠普调整，通过建立自己的梯度推进库…</h3></div><div class="mg l"><p class="bd b dl z fp lt fr fs lu fu fw dk translated">towardsdatascience.com</p></div></div><div class="lw l"><div class="mh l ly lz ma lw mb mc lo"/></div></div></a></div><p id="578a" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">而且实现方式多种多样:<a class="ae lk" href="https://xgboost.ai/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>、<a class="ae lk" href="https://catboost.ai/" rel="noopener ugc nofollow" target="_blank"> CatBoost </a>、<a class="ae lk" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html" rel="noopener ugc nofollow" target="_blank">GradientBoostingRegressor</a>，各有千秋，这里讨论<a class="ae lk" rel="noopener" target="_blank" href="/catboost-vs-light-gbm-vs-xgboost-5f93620723db">这里讨论</a>或者<a class="ae lk" rel="noopener" target="_blank" href="/boosting-showdown-scikit-learn-vs-xgboost-vs-lightgbm-vs-catboost-in-sentiment-classification-f7c7f46fd956">这里讨论</a>。这些实现都有一个共同之处，那就是能够选择一个给定的目标进行最小化训练。更有趣的是，XGBoost 和 CatBoost 提供了对自定义目标函数的简单支持。</p><p id="7d6e" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated"><strong class="kq iw">为什么我需要定制物镜？</strong></p><p id="2f58" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">大多数实现都提供了标准的目标函数，如最小二乘法、最小偏差法、休伯法、RMSE 法……但有时，您正在处理的问题需要更具体的解决方案来达到预期的精度水平。使用自定义物镜通常是我最喜欢的调整模型的方法。</p><p id="ae1d" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">请注意，您可以使用超参数调节来帮助找到最佳物镜。参见我关于这个主题的两篇论文:</p><div class="ll lm gp gr ln lo"><a rel="noopener follow" target="_blank" href="/tuning-xgboost-with-xgboost-writing-your-own-hyper-parameters-optimization-engine-a593498b5fba"><div class="lp ab fo"><div class="lq ab lr cl cj ls"><h2 class="bd iw gy z fp lt fr fs lu fu fw iu bi translated">用 XGBoost 调优 XGBoost:编写自己的 Hyper Parameters 优化引擎</h2><div class="mg l"><p class="bd b dl z fp lt fr fs lu fu fw dk translated">towardsdatascience.com</p></div></div><div class="lw l"><div class="mi l ly lz ma lw mb mc lo"/></div></div></a></div><div class="ll lm gp gr ln lo"><a rel="noopener follow" target="_blank" href="/automl-for-fast-hyperparameters-tuning-with-smac-4d70b1399ce6"><div class="lp ab fo"><div class="lq ab lr cl cj ls"><h2 class="bd iw gy z fp lt fr fs lu fu fw iu bi translated">用 SMAC 进行快速超参数调整的 AutoML</h2><div class="lv l"><h3 class="bd b gy z fp lt fr fs lu fu fw dk translated">使用 AutoML 在高维空间中寻找你的路径</h3></div><div class="mg l"><p class="bd b dl z fp lt fr fs lu fu fw dk translated">towardsdatascience.com</p></div></div><div class="lw l"><div class="mj l ly lz ma lw mb mc lo"/></div></div></a></div><p id="92fb" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">你能给我们举个例子吗？</p><p id="d266" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">当然可以！最近，我一直在寻找一种方法，将我们的一个模型的预测与置信区间联系起来。简单提醒一下，置信区间有两个特征:</p><ol class=""><li id="0d32" class="mk ml iv kq b kr ks ku kv md mm me mn mf mo lj mp mq mr ms bi translated">区间[x_l，x_u]</li><li id="790a" class="mk ml iv kq b kr mt ku mu md mv me mw mf mx lj mp mq mr ms bi translated">置信水平<em class="kp"> C </em>确保 C%的时间，我们想要预测的值将位于该区间。</li></ol><p id="b1a3" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">例如，我们可以说地球平均温度的 99%置信区间是[-80，60]。</p><p id="8ec8" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">将置信区间与预测相关联可以让我们量化预测的可信度。</p><p id="7b61" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated"><strong class="kq iw">你如何计算置信区间？</strong></p><p id="2626" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">你需要训练两个模特:</p><ul class=""><li id="07ff" class="mk ml iv kq b kr ks ku kv md mm me mn mf mo lj my mq mr ms bi translated">一个代表区间的上限</li><li id="62ca" class="mk ml iv kq b kr mt ku mu md mv me mw mf mx lj my mq mr ms bi translated">一个代表区间的下限</li></ul><p id="19d6" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">你猜怎么着？您需要特定的指标来实现:分位数回归目标。scikit-learn<a class="ae lk" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html" rel="noopener ugc nofollow" target="_blank">GradientBoostingRegressor</a>和<a class="ae lk" href="https://catboost.ai/" rel="noopener ugc nofollow" target="_blank"> CatBoost </a>实现都提供了一种使用分位数回归目标函数计算这些值的方法，但两者都使用了这种回归的非平滑标准定义:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/de3625a7c9aa88117f8bdcbaa607ff23.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*FfdYOsQX9UuSB7FQgDF_9g.png"/></div></figure><p id="cf1a" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">其中<em class="kp"> t_i </em>为第 I 个真值，<em class="kp"> a_i </em>为第 I 个预测值。<em class="kp"> w_i </em>是用于衡量误差的可选权重。并且<em class="kp">α</em>定义了分位数。</p><p id="2c7b" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">例如，使用这个目标函数，如果您将<em class="kp"> alpha </em>设置为 0.95，95%的观察值低于预测值。相反，如果将<em class="kp"> alpha </em>设置为 0.05，只有 5%的观测值低于预测值。90%的真实值介于这两个预测值之间。</p><p id="d448" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">让我们使用下面的代码来绘制它，对于范围[-10，10]和各种 alphas:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="c842" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">正如你在下面的结果图中看到的，这个目标函数是连续的，但它的导数不是连续的。(0，0)中有一个奇点，<em class="kp">即</em>。就误差而言，这是一个 C_0 函数，但不是 C_1 函数。这是一个问题，因为梯度增强方法需要 C_2 类的目标函数，即可以微分两次来计算梯度和 hessian 矩阵。</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/a3d07bfa10cd4f274a93edacc0d1c019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t0I74oFDIL98daFxMe698w.png"/></div></div></figure><p id="dafa" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">如果您熟悉 MAE 目标，您应该已经认识到这些分位数回归函数只是 MAE、缩放和旋转函数。如果你不是，下面的截图应该可以说服你:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nn"><img src="../Images/12cb3a55d23a46892a53df21b985fe91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qk8-UMzggWNAL0cpwsxxgg.png"/></div></div></figure></div><div class="ab cl no np hz nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="io ip iq ir is"><p id="5243" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated"><strong class="kq iw">对数目标</strong></p><p id="a643" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">提醒一下，MAE 目标的公式很简单</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/21a55109b7cbd9ba367cac34f3283b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*tKVN-rCu8oc1s1twnU552Q.png"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">MAE 目标公式</p></figure><p id="2519" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">上图还显示了 MAE 的正则化版本，logcosh 目标。如您所见，该目标非常接近 MAE，但却是平滑的，即其导数是连续且可微的。因此，它可以用作任何梯度增强方法中的目标，并且与默认的不可微方法相比，提供了合理的收敛速度。</p><p id="b306" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">因为它是 MAE 的非常接近的近似值，如果我们设法缩放和旋转它，我们将得到分位数回归目标函数的两次可微近似值。</p><p id="3696" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">你可能已经注意到，平均风速曲线和对数余弦曲线之间有轻微的偏移。我们将在下面详细解释这一点。</p><p id="8641" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">对数曲线的公式很简单:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/e9b3ce23fc8d1370ae028342952ec40f.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*UUfR9YStoEGLfP5Hdhk9mQ.png"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">对数目标的公式</p></figure><p id="f849" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated"><strong class="kq iw">对数曲线的旋转和缩放</strong></p><p id="2404" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">我们现在需要做的就是找到一种方法来旋转和缩放这个目标，使它成为分位数回归目标的一个很好的近似。这里没什么复杂的。由于 logcosh 与 MAE 相似，我们应用了与分位数回归相同的变化，即我们使用 alpha 对其进行缩放:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/faceca77b4a608a2ff9dc002bb6e5811.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*GUiKoezwapGMmiaNMt_rdQ.png"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">使用对数余弦的平滑分位数回归</p></figure><p id="b1b6" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">这可以用这 12 行代码来完成:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="d478" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">这是可行的，如下所示:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oc"><img src="../Images/2e613a1621a75c21e5cb825c89d662b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WIwZJGx1mIIz-aHxb9cxFA.png"/></div></div></figure><p id="3eb8" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated"><strong class="kq iw">但是等一下！</strong></p><p id="f6a6" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">您可能会好奇为什么将 log 和 cosh 这两个非线性函数结合起来会得到如此简单的近似线性曲线。</p><p id="3be4" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">答案就在 cosh 的公式里:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi od"><img src="../Images/28d85c978f475a303cfcdc81adbec8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*8BWbEWYRjfLp28yAXkJ84g.png"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">科斯公式</p></figure><p id="679c" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">当 x 为正且足够大时，<em class="kp"> cosh </em>可以近似为</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/eaea67ce0677d337e472942cf80d3b8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:114/format:webp/1*qPN_YScbIaRaC6rk6VnvTw.png"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">当 x &gt;&gt; 0 时 cosh 的近似值</p></figure><p id="64d5" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">相反，当 x 足够负时，<em class="kp"> cosh </em>可以近似为</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi of"><img src="../Images/ce87f123665b0d3a1e904da34e1e6f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/format:webp/1*U12LrF6ZBu1IxQIjQx_bQw.png"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">x &lt;&lt; 0</p></figure><p id="edd1" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">We begin to understand how combining these two formulae leads to such linear results. Indeed, as we apply the log to these approximations of cosh, we get :</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi og"><img src="../Images/c705ad1538eb74a1def6023928afe649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*4fzYq8xX-XRT27V867eOeQ.png"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">logcosh simplification for x &gt; &gt; 0 时 cosh 的近似值</p></figure><p id="36f1" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">对于 x &gt;&gt;0。同样代表 x &lt;&lt; 0 :</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/0a01bdc275787dece40e6f6afd9b8607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*Iki-zgcXnA8yBmKq4dsHww.png"/></div></figure><p id="1d0d" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">It is now clear why these two functions closely approximate the MAE. We also get as a side benefit the explanation for the slight gap between the MAE and the logcosh. It’s log(2)!</p></div><div class="ab cl no np hz nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="io ip iq ir is"><p id="a08e" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated"><strong class="kq iw">让我们在一个真实的例子上试试</strong></p><p id="1c0c" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">现在是时候确保我们上面进行的所有理论数学在现实生活中有效了。我们不会在一个简单的窦上评估我们的方法，如 scikit <a class="ae lk" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html" rel="noopener ugc nofollow" target="_blank">这里</a>所提议的；)相反，我们将使用从<a class="ae lk" href="https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page" rel="noopener ugc nofollow" target="_blank"> TLC 行程记录</a>数据集中提取的真实世界数据，该数据包含超过 10 亿次出租车行程。</p><p id="39b3" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">下面的代码片段实现了上面提出的想法。它定义了对数分位数回归目标<strong class="kq iw"> log_cosh_quantile </strong>，计算其梯度和 hessian。这些是最小化目标所必需的。</p><p id="23af" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">如本文开头所述，我们需要训练两个模型，一个用于上限，另一个用于下限。</p><p id="411e" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">代码的剩余部分只是加载数据并执行最少的数据清理，主要是删除异常值。</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="f4da" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">在这段代码中，我们选择计算 90%的置信区间。因此，我们使用<strong class="kq iw">α= 0.95</strong>作为上限，使用<strong class="kq iw">α= 0.05</strong>作为下限。</p><p id="8703" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">超参数调整已经手动完成，使用相当标准的值。当然还可以改进，但是结果已经足够好来说明这篇论文了。</p><p id="272e" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">脚本的最后几行专用于绘制随机构建的测试集的前 150 个预测及其置信区间:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oi"><img src="../Images/97b971f1a2b1cb697001deba858b38d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zqBgWhv-MrMtH5JXJRVuRA.png"/></div></div></figure></div><div class="ab cl no np hz nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="io ip iq ir is"><p id="f810" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">请注意，我们还在脚本末尾包含了一个计数器，用于计算置信区间正确的实数值的数量。在我们的测试集上，24 889 个真实值中有 22 238 个(89.3%)在计算的置信区间内。</p><p id="ef67" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">该模型已经在<a class="ae lk" href="https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page" rel="noopener ugc nofollow" target="_blank"> TLC 行程记录</a>数据集的 2020 年 1 月数据集的前 100 000 行上进行训练。</p><p id="ce61" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated"><strong class="kq iw">结论</strong></p><p id="6a67" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">通过简单的数学，我们已经能够定义一个平滑的分位数回归目标函数，该函数可以插入任何基于目标优化的机器学习算法中。</p><p id="8f34" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">使用这些正则化函数，我们已经能够为我们的预测预测可靠的置信区间。</p><p id="7ede" class="pw-post-body-paragraph kn ko iv kq b kr ks jw kt ku kv jz kw md ky kz la me lc ld le mf lg lh li lj io bi translated">这种方法比这里的<a class="ae lk" rel="noopener" target="_blank" href="/regression-prediction-intervals-with-xgboost-428e0a018b">所示的</a>方法具有无参数的优势。超参数调优已经是优化 ML 模型的一个要求很高的步骤，我们不需要用另一个参数来增加配置空间的大小；)</p></div></div>    
</body>
</html>