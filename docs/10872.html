<html>
<head>
<title>Implementing SGD From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始实施 SGD</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-sgd-from-scratch-d425db18a72c?source=collection_archive---------5-----------------------#2020-07-29">https://towardsdatascience.com/implementing-sgd-from-scratch-d425db18a72c?source=collection_archive---------5-----------------------#2020-07-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fd7a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">无 SKlearn 的随机梯度下降的自定义实现</h2></div><p id="1ef5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在实现随机梯度下降之前，让我们来谈谈什么是梯度下降。</p><p id="61e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">梯度下降算法是一种用于求解优化问题的迭代算法。在几乎每个机器学习和深度学习模型中，梯度下降都被积极地用于改善我们算法的学习。</p><p id="f07a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">读完这篇博客后，你会知道梯度下降算法实际上是如何工作的。在这篇博客的最后，我们将比较我们的自定义 SGD 实现和 SKlearn 的 SGD 实现。</p><h1 id="67e1" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">梯度下降算法是如何工作的？</h1><ol class=""><li id="2b7e" class="lt lu iq kh b ki lv kl lw ko lx ks ly kw lz la ma mb mc md bi translated">选取一个初始随机点<strong class="kh ir"> <em class="me"> x0。</em>T3】</strong></li><li id="695b" class="lt lu iq kh b ki mf kl mg ko mh ks mi kw mj la ma mb mc md bi translated">x1 = x0 - r [(df/dx) of x0]</li><li id="2dfb" class="lt lu iq kh b ki mf kl mg ko mh ks mi kw mj la ma mb mc md bi translated">x2 = x1- r [(df/dx) of x1]</li><li id="0771" class="lt lu iq kh b ki mf kl mg ko mh ks mi kw mj la ma mb mc md bi translated">类似地，我们发现对于 x0，x1，x2 ……。x[k-1]</li></ol><p id="888f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里 r 是学习率，df/dx 是最小化我们损失的梯度函数。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/cb7e9b2e58b6c5daca16bbe2667c75db.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*Nuh8xnKxGm17nPrwQohFaA.png"/></div></figure><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/6a6a842b53b261c5d87f0f4fe6e11f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*n4ftRAKEJu8-gLB3pzY0DA.png"/></div></figure><h1 id="6a7f" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">用小批量实现线性 SGD</h1><p id="b1b9" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">在小批量 SGD 中，在计算关于训练集子集的误差梯度之后，更新参数。</p><blockquote class="mw mx my"><p id="cea4" class="kf kg me kh b ki kj jr kk kl km ju kn mz kp kq kr na kt ku kv nb kx ky kz la ij bi translated">让我们以 Kaggle 的波士顿住房数据集为例。</p></blockquote><p id="b08f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们将导入所有必需的库。</p><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="9944" class="nh lc iq nd b gy ni nj l nk nl"><strong class="nd ir">import</strong> <strong class="nd ir">warnings</strong><br/>warnings.filterwarnings("ignore")<br/><strong class="nd ir">from</strong> <strong class="nd ir">sklearn.datasets</strong> <strong class="nd ir">import</strong> load_boston<br/><strong class="nd ir">from</strong> <strong class="nd ir">random</strong> <strong class="nd ir">import</strong> seed<br/><strong class="nd ir">from</strong> <strong class="nd ir">random</strong> <strong class="nd ir">import</strong> randrange<br/><strong class="nd ir">from</strong> <strong class="nd ir">csv</strong> <strong class="nd ir">import</strong> reader<br/><strong class="nd ir">from</strong> <strong class="nd ir">math</strong> <strong class="nd ir">import</strong> sqrt<br/><strong class="nd ir">from</strong> <strong class="nd ir">sklearn</strong> <strong class="nd ir">import</strong> preprocessing<br/><strong class="nd ir">import</strong> <strong class="nd ir">pandas</strong> <strong class="nd ir">as</strong> <strong class="nd ir">pd</strong><br/><strong class="nd ir">import</strong> <strong class="nd ir">numpy</strong> <strong class="nd ir">as</strong> <strong class="nd ir">np</strong><br/><strong class="nd ir">import</strong> <strong class="nd ir">matplotlib.pyplot</strong> <strong class="nd ir">as</strong> <strong class="nd ir">plt</strong><br/><strong class="nd ir">from</strong> <strong class="nd ir">prettytable</strong> <strong class="nd ir">import</strong> PrettyTable<br/><strong class="nd ir">from</strong> <strong class="nd ir">sklearn.linear_model</strong> <strong class="nd ir">import</strong> SGDRegressor<br/><strong class="nd ir">from</strong> <strong class="nd ir">sklearn</strong> <strong class="nd ir">import</strong> preprocessing<br/><strong class="nd ir">from</strong> <strong class="nd ir">sklearn.metrics</strong> <strong class="nd ir">import</strong> mean_squared_error<br/><strong class="nd ir">from</strong> <strong class="nd ir">sklearn.model_selection</strong> <strong class="nd ir">import</strong> train_test_split</span></pre><p id="f6cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们将加载数据集。这里，X 包含我们拥有的数据集，Y 包含我们需要预测的标签。</p><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="eca9" class="nh lc iq nd b gy ni nj l nk nl">X = load_boston().data<br/>Y = load_boston().target</span></pre><p id="5177" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">记得在缩放之前拆分数据，以避免数据泄漏问题。</p><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="1adc" class="nh lc iq nd b gy ni nj l nk nl"><em class="me"># split the data set into train and test</em><br/>X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)</span></pre><p id="f5a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用标准标量函数来标准化数据集。在这里，我们只拟合训练数据，因为我们不希望我们的模型在此之前看到这些数据，以避免过度拟合。</p><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="eb0e" class="nh lc iq nd b gy ni nj l nk nl">scaler = preprocessing.StandardScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.transform(X_test)</span></pre><p id="a423" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用 pandas 创建数据框架。</p><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="4915" class="nh lc iq nd b gy ni nj l nk nl">X_train = pd.DataFrame(data = X_train, columns=load_boston().feature_names)<br/>X_train['Price'] = list(y_train)  <br/>X_test = pd.DataFrame(data = X_test, columns=load_boston().feature_names)<br/>X_test['Price'] = list(y_test)</span></pre><p id="55f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看我们的 X_train 是什么样子的。</p><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="0574" class="nh lc iq nd b gy ni nj l nk nl">X_train.head()</span></pre><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nm"><img src="../Images/864b3a73f1ec86f07027f5811b40a9d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3VL2kUsCfdTqX6f031Wgcg.png"/></div></div></figure><p id="5316" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是我们需要最小化的线性模型的损失函数。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ea3bbe2f3135f6fa9fbcf4585cdb7947.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*w_wQ4LyJTMQ44SZyPgRQ7w.png"/></div></figure><p id="b308" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们计算损失函数 L w.r.t 权重(W)和截距(b)的梯度。以下是计算梯度的方程式，</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/9723ffff790e2f166074df8d1d83519e.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*fd9EJjmy5WbgyuAOgqlDyg.png"/></div></figure><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/1f4bdec3d19a6837d6a53df28ce939d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*oiiFA1qV06WUoHQGIq8oWA.png"/></div></figure><p id="b89e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在计算梯度后，我们不断改变我们的权重和截距值。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/3ce8f83c1406cd9bf1a17ffa8fb5248a.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*MJNoaIlXDwqMPO5yAgRIBA.png"/></div></figure><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/ec3d3d3ec9a27b3c7e55825cc33d3e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*cpDA8P35FlBwWWTGxUTvWg.png"/></div></figure><p id="03bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们将实现 SGD 函数。</p><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="2d4d" class="nh lc iq nd b gy ni nj l nk nl"><strong class="nd ir">def</strong> sgd_regressor(X, y, learning_rate=0.2, n_epochs=1000, k=40):<br/>    <br/>    w = np.random.randn(1,13)  <em class="me"># Randomly initializing weights</em><br/>    b = np.random.randn(1,1)   <em class="me"># Random intercept value</em><br/>    <br/>    epoch=1<br/>    <br/>    <strong class="nd ir">while</strong> epoch &lt;= n_epochs:<br/>        <br/>        temp = X.sample(k)<br/><br/>        X_tr = temp.iloc[:,0:13].values<br/>        y_tr = temp.iloc[:,-1].values<br/>        <br/>        Lw = w<br/>        Lb = b<br/>        <br/>        loss = 0<br/>        y_pred = []<br/>        sq_loss = []<br/>        <br/>        <strong class="nd ir">for</strong> i <strong class="nd ir">in</strong> range(k):<br/>              <br/>            Lw = (-2/k * X_tr[i]) * (y_tr[i] - np.dot(X_tr[i],w.T) - b)<br/>            Lb = (-2/k) * (y_tr[i] - np.dot(X_tr[i],w.T) - b)<br/>            <br/>            w = w - learning_rate * Lw<br/>            b = b - learning_rate * Lb<br/>            <br/>            y_predicted = np.dot(X_tr[i],w.T)<br/>            y_pred.append(y_predicted)<br/>        <br/>        loss = mean_squared_error(y_pred, y_tr)<br/>            <br/>        print("Epoch: <strong class="nd ir">%d</strong>, Loss: <strong class="nd ir">%.3f</strong>" %(epoch, loss))<br/>        epoch+=1<br/>        learning_rate = learning_rate/1.02<br/>        <br/>    <strong class="nd ir">return</strong> w,b</span></pre><p id="76db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们保持学习率= 0.2，次数= 1000，批量= 40。我们可以相应地改变参数来最小化我们的 MSE。</p><p id="61a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们通过除以 1.02 来不断降低我们的学习率，你可以选择任何你想要的值。</p><p id="84bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我选择了 MSE 作为我的误差度量，我们也可以选择 RMSE。</p><p id="5148" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们创建一个预测函数并计算我们的预测值。</p><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="8e5b" class="nh lc iq nd b gy ni nj l nk nl"><strong class="nd ir">def</strong> predict(x,w,b):<br/>    y_pred=[]<br/>    <strong class="nd ir">for</strong> i <strong class="nd ir">in</strong> range(len(x)):<br/>        temp_ = x<br/>        X_test = temp_.iloc[:,0:13].values<br/>        y = np.asscalar(np.dot(w,X_test[i])+b)<br/>        y_pred.append(y)<br/>    <strong class="nd ir">return</strong> np.array(y_pred)</span><span id="8b80" class="nh lc iq nd b gy nw nj l nk nl">w,b = sgd_regressor(X_train,y_train)<br/>y_pred_customsgd = predict(X_test,w,b)</span></pre><h2 id="db04" class="nh lc iq bd ld nx ny dn lh nz oa dp ll ko ob oc ln ks od oe lp kw of og lr oh bi translated">比较我们的预测值和实际值</h2><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="aa25" class="nh lc iq nd b gy ni nj l nk nl"><strong class="nd ir">from</strong> <strong class="nd ir">matplotlib.pyplot</strong> <strong class="nd ir">import</strong> figure<br/>plt.figure(figsize=(25,6))<br/>plt.plot(y_test, label='Actual')<br/>plt.plot(y_pred_customsgd, label='Predicted')<br/>plt.legend(prop={'size': 16})<br/>plt.show()<br/>print('Mean Squared Error :',mean_squared_error(y_test, y_pred_customsgd))</span></pre><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi oi"><img src="../Images/412a47c2106cf8a274a476b1c941caad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w69BZvAAhfeyDyu_h-Ht9Q.png"/></div></div></figure><p id="2353" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们看到的，我们得到了 26.8 的 MSE，这是相当不错的。</p><h2 id="12b6" class="nh lc iq bd ld nx ny dn lh nz oa dp ll ko ob oc ln ks od oe lp kw of og lr oh bi translated">实施 SKlearn 的 SGD 回归器</h2><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="30b7" class="nh lc iq nd b gy ni nj l nk nl"><strong class="nd ir">from</strong> <strong class="nd ir">sklearn.linear_model</strong> <strong class="nd ir">import</strong> SGDRegressor<br/>clf = SGDRegressor(max_iter=1000, tol=1e-3)<br/>clf.fit(X_train, y_train)<br/>y_pred = clf.predict(X_test)</span></pre><h2 id="c246" class="nh lc iq bd ld nx ny dn lh nz oa dp ll ko ob oc ln ks od oe lp kw of og lr oh bi translated">比较我们的预测值和实际值</h2><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="296b" class="nh lc iq nd b gy ni nj l nk nl"><strong class="nd ir">import</strong> <strong class="nd ir">matplotlib.pyplot</strong> <strong class="nd ir">as</strong> <strong class="nd ir">plt</strong><br/><strong class="nd ir">from</strong> <strong class="nd ir">matplotlib.pyplot</strong> <strong class="nd ir">import</strong> figure<br/>plt.figure(figsize=(25,6))<br/>plt.plot(y_test, label='Actual')<br/>plt.plot(y_pred, label='Predicted')<br/>plt.legend(prop={'size': 16})<br/>plt.show()<br/>print('Mean Squared Error :',mean_squared_error(y_test, y_pred))</span></pre><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi oj"><img src="../Images/2c8826265126e0668a2c3ad855a864f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRyT4i2-LPfQrP2Z_ZAAHQ.png"/></div></div></figure><p id="2f21" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SKlearn 的 SGD 实现比我们的自定义实现要好得多。</p><h1 id="7b4c" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">比较两个 SGD 分类器的预测值</h1><pre class="ml mm mn mo gt nc nd ne nf aw ng bi"><span id="8191" class="nh lc iq nd b gy ni nj l nk nl"><strong class="nd ir">import</strong> <strong class="nd ir">matplotlib.pyplot</strong> <strong class="nd ir">as</strong> <strong class="nd ir">plt</strong><br/><strong class="nd ir">from</strong> <strong class="nd ir">matplotlib.pyplot</strong> <strong class="nd ir">import</strong> figure<br/>plt.figure(figsize=(25,6))<br/>plt.plot(y_pred, label='SGD')<br/>plt.plot(y_pred_customsgd, label='Custom SGD')<br/>plt.legend(prop={'size': 16})<br/>plt.show()<br/>print('Mean Squared Error of Custom SGD :',mean_squared_error(y_test, y_pred_customsgd))<br/>print("Mean Squared Error of SKlearn's SGD :",mean_squared_error(y_test, y_pred))</span></pre><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi ok"><img src="../Images/598eb1b83acd2f09d820e11cfecc6a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A4Fsv7Y33cOFWul1MFuaHg.png"/></div></div></figure><p id="3bc7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嗯，我们的自定义 SGD 做了一个相当不错的工作相比，SKlearn 的，我们总是可以做一些超参数调整，以改善我们的自定义模型。</p><p id="2425" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从头开始实现算法需要很多时间，但是如果我们使用库，那么它对我们来说仍然是黑箱。请在评论区告诉我你对此的看法。</p><p id="8ae5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢你阅读这篇博客，希望你对 SGD 的实际运作有所了解。</p></div></div>    
</body>
</html>