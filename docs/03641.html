<html>
<head>
<title>Tutorial on LSTMs: A Computational Perspective</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTMs教程:计算观点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tutorial-on-lstm-a-computational-perspective-f3417442c2cd?source=collection_archive---------2-----------------------#2020-04-06">https://towardsdatascience.com/tutorial-on-lstm-a-computational-perspective-f3417442c2cd?source=collection_archive---------2-----------------------#2020-04-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ju"><img src="../Images/85387e471e45bb3fbb872f033117efb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9yDB5inawGWuEaEF"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">杰克·纳格兹在<a class="ae kk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="bd7b" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">目录</h1><pre class="jv jw jx jy gt lj lk ll lm aw ln bi"><span id="5806" class="lo km iq lk b gy lp lq l lr ls"><br/>1. <a class="ae kk" href="#6d31" rel="noopener ugc nofollow">Introduction</a></span><span id="afc2" class="lo km iq lk b gy lt lq l lr ls">2. <a class="ae kk" href="#95b4" rel="noopener ugc nofollow">Why Do we need RNNs?</a></span><span id="bf16" class="lo km iq lk b gy lt lq l lr ls">3. <a class="ae kk" href="#7194" rel="noopener ugc nofollow">RNN Training and Inference</a></span><span id="6f2a" class="lo km iq lk b gy lt lq l lr ls">4. <a class="ae kk" href="#8ca2" rel="noopener ugc nofollow">Structure of an RNN</a></span><span id="65b1" class="lo km iq lk b gy lt lq l lr ls">5. <a class="ae kk" href="#3226" rel="noopener ugc nofollow">Time Unrolling</a></span><span id="9b6d" class="lo km iq lk b gy lt lq l lr ls">6. <a class="ae kk" href="#aa74" rel="noopener ugc nofollow">Vanishing Gradient</a></span><span id="ddb8" class="lo km iq lk b gy lt lq l lr ls">7. <a class="ae kk" href="#0d00" rel="noopener ugc nofollow">Long Short-Term Memory (LSTM)</a></span><span id="1087" class="lo km iq lk b gy lt lq l lr ls">8. <a class="ae kk" href="#b10c" rel="noopener ugc nofollow">LSTM equations</a></span><span id="84d0" class="lo km iq lk b gy lt lq l lr ls">9. <a class="ae kk" href="#da46" rel="noopener ugc nofollow">Understanding the LSTM dimensionalities</a></span><span id="ce56" class="lo km iq lk b gy lt lq l lr ls">10. <a class="ae kk" href="#ada8" rel="noopener ugc nofollow">Time Unroll and Multiple Layers</a></span><span id="7c2f" class="lo km iq lk b gy lt lq l lr ls">11. <a class="ae kk" href="#4019" rel="noopener ugc nofollow">Example: Sentiment Analysis using LSTM</a></span><span id="88da" class="lo km iq lk b gy lt lq l lr ls">12. <a class="ae kk" href="#c240" rel="noopener ugc nofollow">Testing your knowledge</a></span></pre></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="6d31" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">介绍</h1><p id="2617" class="pw-post-body-paragraph lu lv iq lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">最近，人们对将深度学习模型嵌入硬件非常感兴趣。当涉及到深度学习模型部署时，尤其是在边缘，能量是至关重要的。Pete Warden在<a class="ae kk" href="https://petewarden.com/2018/06/11/why-the-future-of-machine-learning-is-tiny/" rel="noopener ugc nofollow" target="_blank">“为什么机器学习的未来很小”</a>上发表了一篇关于为什么能源对AI@Edge很重要的博文。程序(或模型)的能量优化只能通过对底层计算的良好理解来完成。在过去几年与深度学习人员——硬件架构师、微内核程序员、模型开发人员、平台程序员和受访者(尤其是受访者)的合作中，我发现人们从定性的角度理解LSTMs，但从定量的角度理解不好。如果你不能很好地理解某事，你就不能优化它。缺乏了解导致LSTMs开始失宠。本教程试图通过方程解释LSTMs所需的计算来弥合定性和定量之间的差距。同时，这也是我从计算的角度巩固对LSTM的理解的一种方式。希望它也能对以不同身份使用LSTMs的其他人有用。</p><p id="e2fb" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">注意:这里的免责声明是，我既不声称自己是LSTMs方面的专家，也不声称自己的理解完全正确。如果有不正确或令人困惑的地方，请随意发表评论。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="95b4" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">为什么我们需要RNNs？</h1><p id="1392" class="pw-post-body-paragraph lu lv iq lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">需要递归神经网络(RNNs ),因为我们希望设计能够识别(或操作)序列的网络。卷积神经网络(CNN)不关心它们识别的图像的顺序。另一方面，RNN用于视频、手写识别等序列。这在图1中用一个高层次的卡通图来说明。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi mx"><img src="../Images/73e90bc30b7843dbbcaa318fa8f34256.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L83_FGCXFXIoFthh0R0Dgg.jpeg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated"><strong class="bd my">图1:反馈网络和前馈网络之间区别的卡通图示。原图来自</strong> <a class="ae kk" href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Feed-Forward_Networks" rel="noopener ugc nofollow" target="_blank"> <strong class="bd my">维基百科</strong> </a> <strong class="bd my">。</strong></p></figure><p id="2ae5" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">简而言之，如果我们试图识别视频、手写或语音等序列，我们就需要RNNs。注意，我们仍然没有谈论LSTMs。我们仍在试图了解RNN。稍后我们将讨论LSTMs。</p><h2 id="8a5c" class="lo km iq bd kn mz na dn kr nb nc dp kv mf nd ne kz mj nf ng ld mn nh ni lh nj bi translated">悬崖笔记版本</h2><blockquote class="nk nl nm"><p id="4c0a" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated">当我们试图处理序列时，需要rnn。T3】</p></blockquote></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="7194" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">RNN训练和推理</h1><p id="2d5d" class="pw-post-body-paragraph lu lv iq lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">如果你跳过了前一部分，我们首先试图理解香草RNN的工作原理。如果你试图理解LSTMs，我鼓励并敦促你通读这一部分。</p><p id="d0e4" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">在本节中，我们将了解以下内容:</p><ol class=""><li id="4fce" class="nr ns iq lw b lx ms mb mt mf nt mj nu mn nv mr nw nx ny nz bi translated">RNN的结构。</li><li id="b938" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">时间展开</li></ol><p id="d944" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">我们将基于这些概念来更好地理解基于LSTM的网络。</p><h1 id="8ca2" class="kl km iq bd kn ko of kq kr ks og ku kv kw oh ky kz la oi lc ld le oj lg lh li bi translated">RNN的结构。</h1><p id="d278" class="pw-post-body-paragraph lu lv iq lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">图2显示了一个简单的RNN结构。该图的灵感来自于<a class="ae kk" href="https://www.deeplearningbook.org/contents/rnn.html" rel="noopener ugc nofollow" target="_blank">深度学习书籍</a>(具体是第10章第373页的图10.3)。</p><p id="d43b" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">图中需要注意一些事情:</p><ul class=""><li id="901d" class="nr ns iq lw b lx ms mb mt mf nt mj nu mn nv mr ok nx ny nz bi translated">我在括号中用红色表示了每个节点的变量。在下一张图和下一节中，我将使用变量(在等式中),所以请花几秒钟时间理解它们。</li><li id="276f" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr ok nx ny nz bi translated">从预期产量到损失的箭头方向不是错别字。</li><li id="38d2" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr ok nx ny nz bi translated">变量<strong class="lw ir"> <em class="nn"> U，V，W </em> </strong>就是这个网络的权重矩阵。</li><li id="675b" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr ok nx ny nz bi translated">反馈路径中的黄色斑点(由绿色箭头指示)表示单位延迟。如果你是DSP的，就把这个当成(z^-1)</li><li id="9c80" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr ok nx ny nz bi translated">反馈(用绿色箭头表示)使这个玩具例子有资格成为RNN。</li></ul><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/21a49ce77f68cfa2df0df9b5b047147a.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*gnUDj7MyXietKjwPPkq8_w.jpeg"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated"><strong class="bd my">图2:RNN结构的例子。这个形象的灵感来源于</strong> <a class="ae kk" href="https://www.deeplearningbook.org/contents/rnn.html" rel="noopener ugc nofollow" target="_blank"> <strong class="bd my">深度学习书籍</strong> </a></p></figure><p id="a07f" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">在我们进入方程之前。让我们看看图表，了解发生了什么。</p><ol class=""><li id="bb56" class="nr ns iq lw b lx ms mb mt mf nt mj nu mn nv mr nw nx ny nz bi translated">在宇宙之初。输入<strong class="lw ir"> 'x(t=0)' </strong>与矩阵<strong class="lw ir"> U </strong>相乘得到<strong class="lw ir"> x(t=0)*U </strong> <strong class="lw ir">。</strong></li><li id="a039" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">上一时间步的反馈乘以矩阵<strong class="lw ir"> W. </strong>由于这是初始阶段，反馈值为零(为简单起见)。因此，反馈值为h(t=-1)*W = 0。因此，乘积是0+x(t=0)*U = x(t=0)*U</li><li id="7a67" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">现在，它与矩阵V相乘，得到x(t=0)*U*V。</li><li id="4fd5" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">对于下一个时间步长，这个值将存储在h(t)中，并且不是一个非零值。</li></ol><p id="b079" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">因此，上述也可以总结为以下等式:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi om"><img src="../Images/2a27925560b38d36118a14595dd90157.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/0*wzFSRDApMwOjwefx"/></div></figure><p id="6ffa" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">在上述方程中，我们忽略了非线性和偏差。将它们添加到等式中，如下所示。不要担心这些看起来很复杂。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi on"><img src="../Images/ff9b095988345b090b9b265fe4a10d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/0*43cf_bTe984Yzecw"/></div></figure><h2 id="badd" class="lo km iq bd kn mz na dn kr nb nc dp kv mf nd ne kz mj nf ng ld mn nh ni lh nj bi translated">悬崖笔记版本</h2><blockquote class="nk nl nm"><p id="1b3c" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"><em class="iq">rnn的结构中有一个反馈环。这使得他们能够处理序列。</em>T3】</strong></p></blockquote><h1 id="3226" class="kl km iq bd kn ko of kq kr ks og ku kv kw oh ky kz la oi lc ld le oj lg lh li bi translated">时间展开</h1><p id="8799" class="pw-post-body-paragraph lu lv iq lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">时间展开是理解RNNs和LSTMs的一个重要概念。</p><p id="072e" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">rnn也可以表示为其自身的时间展开版本。这是表示它们的另一种方式，方程没有变化。时间展开只是另一种表现，而不是一种转换。我们之所以要用这种方式表示它们，是因为这样更容易推导出正向和反向传递方程。时间展开如下图所示:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi oo"><img src="../Images/5903ebffed660b62aaf62a40ff4ef325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Y14Oak7niiOI4q_z4Givg.jpeg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated"><strong class="bd my">展开的RNN画像。形象的灵感来源于</strong> <a class="ae kk" href="https://www.deeplearningbook.org/contents/rnn.html" rel="noopener ugc nofollow" target="_blank"> <strong class="bd my">深度学习书籍</strong> </a></p></figure><p id="b256" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">在左上图中，RNN的结构和我们之前看到的一样。右边是时间展开的表示。</p><p id="b1ff" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">从该图中可以得出一些重要结论:</p><ol class=""><li id="d912" class="nr ns iq lw b lx ms mb mt mf nt mj nu mn nv mr nw nx ny nz bi translated">权重矩阵U，V，W不随时间变化。这意味着一旦训练了RNN，权重矩阵在推断期间是固定的，并且不依赖于时间。换句话说，相同的权重矩阵(U，V，W)用于每个时间步。</li><li id="4e7b" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">浅色阴影的h(..)两边分别表示h(t-1)之前和h(t+1)之后的时间步长。</li><li id="12f0" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">上图显示了RNN的向前(或推断)传球。在每个时间步，都有一个输入和一个相应的输出。</li><li id="e51b" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">在正向传递中，“信息”(或内存)通过变量<em class="nn"> h. </em>传递到下一级</li></ol><h2 id="316c" class="lo km iq bd kn mz na dn kr nb nc dp kv mf nd ne kz mj nf ng ld mn nh ni lh nj bi translated">悬崖笔记版本</h2><blockquote class="nk nl nm"><p id="e1ea" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"><em class="iq">rnn可以表示为时间展开的版本。这只是一个表示，而不是转换。在前向传递中，权重矩阵U、V、W不依赖于时间。</em>T15】</strong></p></blockquote></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="aa74" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">消失梯度</h1><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi op"><img src="../Images/6e062beb751bebf49878384d53e0d485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UsjYj19in3OF6hVa"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated"><a class="ae kk" href="https://unsplash.com/@aaronroth?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚伦·罗斯</a>在<a class="ae kk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="cf7d" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">rnn存在为长距离序列保留上下文的问题。换句话说，rnn不能处理非常长的序列(想想长句或长演讲)。给定输入对隐藏层(以及输出)的影响作为时间(或序列长度)的函数呈指数衰减(或爆发和饱和)。消失梯度问题如下图所示，来自亚历克斯·格雷夫斯的论文。节点的阴影表示网络节点在给定时间对输入的敏感度。阴影越暗，灵敏度越高，反之亦然。如图所示，当我们从时间步长=1快速移动到时间步长=7时，灵敏度会衰减。网络会忘记第一次输入。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/3270f303c6cd32bca40dced3b361fefb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*zZwl8m8K98AzYlQkCMmZuw.png"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">来自<a class="ae kk" href="https://www.cs.toronto.edu/~graves/phd.pdf" rel="noopener ugc nofollow" target="_blank"> Alex Grave论文</a>的图显示了隐藏节点对梯度的敏感性。</p></figure><p id="aebf" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">这是使用LSTMs的主要动机。消失梯度问题导致研究人员多次尝试提出解决方案。其中最有效的是LSTM或长短期记忆，由Hochreiter于1997年提出。</p><h2 id="12a3" class="lo km iq bd kn mz na dn kr nb nc dp kv mf nd ne kz mj nf ng ld mn nh ni lh nj bi translated">悬崖笔记版本</h2><blockquote class="nk nl nm"><p id="bc45" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> <em class="iq">传统的RNNs对于长序列(序列长度大约大于10个时间步长)的输入不敏感。1997年提出的LSTMs仍然是克服rnn这一缺点的最流行的解决方案。</em> </strong></p></blockquote></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="0d00" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">长短期记忆(LSTM)</h1><p id="2485" class="pw-post-body-paragraph lu lv iq lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">LSTMs是由Hochreiter在1997年提出的，作为一种减轻与普通rnn相关的棘手问题的方法。</p><p id="f625" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">一些博客和图片描述了LSTMs。如您所见，在描述LSTMs的方式上有很大的差异。在这篇文章中，我想通过方程式来描述它们。我发现它们通过方程式更容易理解。有很多优秀的博客可以让你直观地理解它们，我强烈推荐你去看看:</p><p id="cbd5" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><a class="ae kk" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">克里斯托弗·奥拉赫的博客。</a></p><p id="cdbf" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">(b) LSTM来自<a class="ae kk" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">维基百科</a></p><p id="5b91" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><a class="ae kk" href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" rel="noopener">阎石的博客</a>上媒</p><p id="a64f" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">(d)来自<a class="ae kk" href="https://www.deeplearningbook.org/contents/rnn.html" rel="noopener ugc nofollow" target="_blank">深度学习书籍</a>的LSTMs</p><p id="73b1" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">(e) <a class="ae kk" href="https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/" rel="noopener ugc nofollow" target="_blank"> Nvidia关于加速LSTMs的博客</a></p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi or"><img src="../Images/229b2b5c985f84fc0bb32c13bcd47f93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*dhsZbCpvn42XYH1ePjModQ.png"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">描述LSTMs的流行博客和论文中的不同图片。<a class="ae kk" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">(一)克里斯多夫·奥拉赫的博客。</a> (b) LSTM来自<a class="ae kk" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">维基百科</a> <a class="ae kk" href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" rel="noopener">阎石的博客</a>关于中(d)lstm来自<a class="ae kk" href="https://www.deeplearningbook.org/contents/rnn.html" rel="noopener ugc nofollow" target="_blank">深度学习书籍</a> (e) <a class="ae kk" href="https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/" rel="noopener ugc nofollow" target="_blank">英伟达的博客</a>关于加速lstm(f)LSTM图来自<a class="ae kk" href="https://www.researchgate.net/figure/Illustration-of-a-Long-Short-Term-Memory-LSTM-unit_fig1_327891190" rel="noopener ugc nofollow" target="_blank">关于人体活动检测的会议论文</a></p></figure><h1 id="b10c" class="kl km iq bd kn ko of kq kr ks og ku kv kw oh ky kz la oi lc ld le oj lg lh li bi translated">LSTM方程</h1><p id="cc97" class="pw-post-body-paragraph lu lv iq lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">下图显示了单个时间步长的LSTM的输入和输出。这是一个时间步长的输入、输出和时间展开表示的方程。LSTM有一个输入<strong class="lw ir"> x(t) </strong>，它可以是CNN的输出或者直接是输入序列。<strong class="lw ir"> h(t-1) </strong>和<strong class="lw ir"> c(t-1) </strong>是来自前一时间步LSTM的输入。o(t) 是该时间步长的LSTM的输出。LSTM还生成<strong class="lw ir"> c(t) </strong>和<strong class="lw ir"> h(t) </strong>用于下一时间步LSTM的消耗。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi os"><img src="../Images/0022426e7c6090ce961ad683668dbdb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ryIzdQtDwrdx_sJHdufrEQ.png"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated"><strong class="bd my"> LSTM输入输出和单个时间步长的相应方程。</strong></p></figure><p id="33f5" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">注意，LSTM方程也产生f(t)，i(t)，c'(t)，这些是LSTM的内部消耗，用于产生c(t)和h(t)。</p><p id="7f3a" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">从上面可以看出一些要点:</p><ol class=""><li id="5cf9" class="nr ns iq lw b lx ms mb mt mf nt mj nu mn nv mr nw nx ny nz bi translated">上述等式仅适用于一次性步骤。这意味着这些方程必须在下一个时间步重新计算。因此，如果我们有10个时间步长的序列，那么对于每个时间步长，上述等式将分别计算10次。</li><li id="8d30" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">权重矩阵(Wf，Wi，Wo，Wc，Uf，Ui，Uo，Uc)和偏差(bf，bi，bo，bc)不依赖于时间。这意味着这些权重矩阵不会从一个时间步长改变到另一个时间步长。换句话说，为了计算不同时间步长的输出，使用相同的权重矩阵。</li></ol><p id="ed28" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">下面的伪代码片段显示了十个时间步长的LSTM时间计算。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="252f" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><em class="nn">说明10个时间步长的LSTM计算的代码片段。</em></p><h2 id="a2d8" class="lo km iq bd kn mz na dn kr nb nc dp kv mf nd ne kz mj nf ng ld mn nh ni lh nj bi translated">悬崖笔记版本</h2><blockquote class="nk nl nm"><p id="3ede" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated">LSTM网络的权重矩阵从一个时间步长到另一个时间步长不变。LSTM由6个方程式组成。如果LSTM正在学习长度为“seq_len”的序列。然后，这六个方程将被计算总共‘seq _ len’。基本上，每一步都要计算方程。</p></blockquote></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="da46" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">理解LSTM维度</h1><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ov"><img src="../Images/be4b5b06f8726b3605a08d43227bd898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UrWHY-wNsyk-aIl2"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">沃洛季米尔·赫里先科在<a class="ae kk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="ca0c" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">理解了LSTM的单个时间步长所需的计算之后，我们进入下一个方面——维度。根据我的经验，LSTM维度是lstm混乱的主要原因之一。另外，这是我最喜欢问的面试问题之一；)</p><p id="6129" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">让我们再看看下图中的LSTM方程。正如你已经知道的，这些是单个时间步长的LSTM方程:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/745575f6449bd7dd17a527b4e28b4a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*6G1yDlrIe0yOcYYklr_6uw.png"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">单时间步长的LSTM方程</p></figure><p id="b0c1" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">让我们从一个简单的x(t)开始。这是输入信号/特征向量/CNN输出。我假设x(t)来自一个嵌入层(想想word2vec)，输入维数为[80x1]。这意味着Wf的维数为[某个值x 80]。</p><blockquote class="nk nl nm"><p id="3268" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir">至此我们已经:</strong></p><p id="58fc" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> x(t)是[80 X 1] —输入假设</strong></p><p id="04dc" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> Wf是[Some_value X 80 ] —矩阵乘法法则。</strong></p></blockquote><p id="2f07" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">让我们做另一个假设，LSTM的输出维数是[12×1]。假设这是输出类的数量。因此，在每个时间步长，LSTM产生大小为[12×1]的输出o(t)。</p><p id="32a2" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">既然o(t)是[12x1]，那么h(t)必须是[12 x 1]，因为h(t)是通过一个元素接一个元素的乘法来计算的(看看最后一个关于如何从o(t)和c(t)计算h(t)的等式)。因为o(t)是[12x1]，那么c(t)必须是[12x1]。如果c(t)是[12x1]，那么f(t)，c(t-1)，i(t)和c'(t)必须是[12x1]。为什么？因为h(t)和c(t)都是通过逐元素乘法来计算的。</p><blockquote class="nk nl nm"><p id="a79f" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir">因此我们有:</strong></p><p id="2171" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> o(t)是[12 X 1] —输出假设</strong></p><p id="5d74" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> h(t)和c(t)是[12x1] —因为h(t)是通过公式中o(t)和tanh(c(t))的逐元素乘法计算的。</strong></p><p id="a235" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> f(t)，c(t-1)，i(t)和c’(t)是[12x1]——因为c(t)是[12 x1]，并且是通过要求相同大小的元素方式运算来估计的。</strong></p></blockquote><p id="1864" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">因为f(t)的维数是[12×1]，那么Wf和x(t)的乘积必须是[12×1]。我们知道x(t)是[80x1](因为我们假设)那么Wf必须是[12x80]。再看f(t)的等式，我们知道偏置项bf是[12x1]。</p><p id="a026" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">因此，我们有:</p><blockquote class="nk nl nm"><p id="8c39" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> x(t)是[80 X 1] —输入假设</strong></p><p id="a7a0" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> o(t)是[12 X 1] —输出假设</strong></p><p id="d9a9" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> h(t)和c(t)是[12x 1]——因为h(t)是通过等式中o(t)和tanh(c(t))的逐元素相乘来计算的。</strong></p><p id="4c8f" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> f(t)，c(t-1)，i(t)和c’(t)是[12x1]——因为c(t)是[12 x1]，并且是通过要求相同大小的元素式运算来估计的。</strong></p><p id="66f4" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> Wf是[12x80] —因为f(t)是[12x1]，x(t)是[80x1] </strong></p><p id="88f3" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> bf是[12x1] —因为所有其他项都是[12x1]。</strong></p></blockquote><p id="3e8e" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">以上内容看起来可能比实际情况要复杂一些。花点时间自己解决它。相信我，没那么复杂。</p><p id="fb4b" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">现在进入令人困惑的部分:)开个玩笑！</p><p id="5661" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">在f(t)的计算中，Uf和h(t-1)的乘积也必须是[12x1]。现在根据前面的讨论我们知道h(t-1)是[12x1]。h(t)和h(t-1)将具有相同的维数[12×1]。因此，Uf将具有[12×12]的维数。</p><p id="3bc6" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">所有Ws (Wf，Wi，Wo，Wc)将具有相同的[12x80]尺寸，所有bias(BF，bi，bc，bo)将具有相同的[12x1]尺寸，所有Us (Uf，Ui，Uo，Uc)将具有相同的[12x12]尺寸。</p><p id="10ed" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">因此:</p><blockquote class="nk nl nm"><p id="ef8d" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> x(t)是[80 X 1] —输入假设</strong></p><p id="48c3" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> o(t)是[12 X 1] —输出假设</strong></p><p id="dc7f" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> Wf、Wi、Wc、Wo的尺寸均为【12x 80】</strong></p><p id="2e21" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> Uf、Ui、Uc、Uo每个都有[12x12]的尺寸</strong></p><p id="95a1" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> bf、bi、bc、bo的尺寸均为【12 x1】</strong></p><p id="19c2" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated"><strong class="lw ir"> ht，ot，ct，ft，it各有一个维度【12 x1】</strong></p></blockquote><p id="c76c" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">LSTM的总权重矩阵大小为</p><p id="c7b9" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">权重_ LSTM = 4 *[12x 80]+4 *[12x 12]+4 *[12x 1]</p><p id="e49f" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">= 4 *[输出尺寸x输入尺寸]+4 *[输出尺寸]+4 *[输入尺寸]</p><p id="ab2a" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi">= 4*[960] + 4*[144] + 4*[12] = 3840 + 576+48= 4,464</p><p id="69b5" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">让我们验证将以下代码粘贴到您的python设置中</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="6125" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">请注意，LSTM的参数数量是4464。这也是我们通过计算得到的结果！</p><p id="21e5" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">在我们进入下一部分之前，我想强调一个关键的方面。LSTMs有两个定义它们的东西:输入维度和输出维度(以及我稍后将谈到的时间展开)。在文献(论文/博客/代码文档)中，术语有很多模糊之处。有些地方称之为单位数、隐藏维数、输出维数、LSTM单位数等。我没有争论哪个是正确的或者哪个是错误的，只是在我看来这些通常意味着同样的事情——输出维度。</p><p id="a57d" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">到目前为止，我们已经查看了权重矩阵的大小。大多数框架将权重矩阵合并存储为单个矩阵。下图说明了这个权重矩阵和相应的维度。</p><p id="c588" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><strong class="lw ir">注意:根据您使用的框架，权重矩阵将以不同的顺序存储。例如，Pytorch可以在Wf或Caffe可以首先存储Wo之前保存Wi。</strong></p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/a6ad2ee851de25dc87ad3bba400e1cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*W3rS-cXXfdN_p8Xc2t1DNQ.png"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">LSTM权重矩阵维度。</p></figure><h2 id="ec5c" class="lo km iq bd kn mz na dn kr nb nc dp kv mf nd ne kz mj nf ng ld mn nh ni lh nj bi translated">悬崖笔记版本</h2><blockquote class="nk nl nm"><p id="3bbf" class="lu lv nn lw b lx ms lz ma mb mt md me no mu mh mi np mv ml mm nq mw mp mq mr ij bi translated">有两个参数定义时间步长的LSTM。输入维度和输出维度。权重矩阵大小为:4 * Output _ Dim *(Output _ Dim+Input _ Dim+1)[感谢<a class="ae kk" href="https://medium.com/@clessvna" rel="noopener"> Cless </a>捕捉到错别字]。当谈到LSTMs时，有很多模糊性——单位的数量、隐藏维度和输出维度。请记住，有两个参数定义了LSTM-输入维度和输出维度。</p></blockquote></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="ada8" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">时间展开和多层</h1><p id="d2e5" class="pw-post-body-paragraph lu lv iq lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">下图中有两个独立的LSTM网络。两个网络都显示为展开三个时间步长。图(A)中的第一个网络是单层网络，而图(B)中的网络是双层网络。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/0c74887d881d22c22ed3fab767e36f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*mDUo0CqB7a0QdqCqibE1Fw.png"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">两个LSTM网络(A)展开三个时间步长的单层LSTM网络(B)展开三个时间步长的双层LSTM网络</p></figure><p id="d3bb" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">在第一个单层网络的情况下，我们初始化<em class="nn"> h </em>和<em class="nn"> c </em>，并且每个时间步产生一个输出，连同<em class="nn"> h </em>和<em class="nn"> c </em>一起被下一个时间步消耗。注意，即使在最后的时间步h(t)和c(t)被丢弃，为了完整起见，我还是显示了它们。正如我们之前讨论的，对于三个时间步长，权重(Ws、Us和bs)是相同的。</p><p id="8f1b" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">双层网络有两个LSTM层。第一层的输出将是第二层的输入。它们都有自己的权重矩阵和各自的<em class="nn"> hs、cs和os。</em>我通过使用上标来表明这一点。</p><h1 id="4019" class="kl km iq bd kn ko of kq kr ks og ku kv kw oh ky kz la oi lc ld le oj lg lh li bi translated">示例:使用LSTM的情感分析</h1><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi oz"><img src="../Images/391d023960086840cb85b26da8a666a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*keNTEzHDzq8KJqnv"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated"><a class="ae kk" href="https://unsplash.com/@tengyart?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">腾雅特</a>在<a class="ae kk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="f520" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">让我们来看看一个非常简单但很现实的LSTM网络，看看这是如何工作的。任务很简单，我们必须想出一个网络来告诉我们一个给定的句子是否定的还是肯定的。为了简单起见，我们假设句子是固定长度的。如果实际句子的字数少于预期长度，则填充零；如果实际句子的字数多于序列长度，则截断句子。在我们的例子中，我们将句子长度限制为3个单词。为什么是3个字？只是一个我喜欢的数字，因为它对我来说更容易画出图表:)。</p><p id="f5b7" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">严肃地说，您可以绘制数据集中一个句子的字数直方图，并根据直方图的形状选择一个值。大于预定单词数的句子将被截断，单词数较少的句子将用零或空单词填充。</p><p id="9e0a" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">不管怎样，回到我们的例子。我们坚持三个词。我们将使用一个嵌入层，将一个英文单词转换成大小为[80x1]的数字向量。为什么是80？因为我喜欢80这个数字:)不管怎样，网络如下图所示。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/f2bf5a380726344ea3c7d71b2a3c22bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*DnUtInkmCS5-YKVVNdTm6g.png"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">使用LSTM的情感分析网络示例。</p></figure><p id="93b2" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">我们将尝试对一个句子进行分类——“我很快乐”。在t=0时，第一个字“I”被嵌入层转换成长度为[80x1]的数字向量。并且穿过LSTM，随后是完全连接的层。然后在时间t=1，第二个单词通过网络，接着是在时间t=2的最后一个单词“happy”。我们希望网络等待整个句子让我们了解感情。我们不希望它过于急切，一字不漏地把感悟告诉我们。这就是为什么在下图中，LSTM的输出只显示在最后一步。Keras将这个参数称为<strong class="lw ir"> return_sequence。</strong>将此项设置为“假”或“真”将决定LSTM和随后的网络是否在每个时间步长或我们示例中的每个单词生成输出。我想在这里强调的一个关键点是，仅仅因为你将返回序列设置为假，并不意味着LSTM方程被修改了。他们仍然在计算每个时间步长的h(t)，c(t)。因此计算量不会减少。</p><p id="fb02" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">以下是Keras在IMDB数据集上进行情感分析的示例:</p><p id="faee" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><a class="ae kk" href="https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py" rel="noopener ugc nofollow" target="_blank">https://github . com/keras-team/keras/blob/master/examples/IMDB _ lstm . py</a></p><h1 id="c240" class="kl km iq bd kn ko of kq kr ks og ku kv kw oh ky kz la oi lc ld le oj lg lh li bi translated">测试你的知识！</h1><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi pb"><img src="../Images/76b4551134b030286cfb8b520b94befe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YEs5CPcXFh5YsAD-"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">安吉丽娜·莉文在<a class="ae kk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="0d36" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">让我们试着巩固一下到目前为止所学的知识。在这一部分，我列出了一些样本/玩具网络的问题。如果我能测试我的理解是否正确，这将有助于我感觉更好，因此这一部分采用了这种格式。或者，您也可以使用这些来准备与LSTMs相关的面试:)</p><p id="6c49" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><strong class="lw ir">样本LSTM网络# 1 </strong></p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/254a5d8b83eead7d131ddf6337782d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*w_WhyO9b7xCATvUKu6mpAg.png"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">LSTM网络# 1示例</p></figure><ol class=""><li id="4f96" class="nr ns iq lw b lx ms mb mt mf nt mj nu mn nv mr nw nx ny nz bi translated">这个网络中有多少个LSTM层？— <strong class="lw ir">网络有一层。不要与多个LSTM盒混淆，它们代表不同的时间步长，只有一层。</strong></li><li id="2494" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如图所示，序列长度或时间步数是多少？— <strong class="lw ir">时间步长的数量为3。看看时间索引。</strong></li><li id="8ced" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如果输入维数，即x(t)是[18x1]，o(t)是[19x1]，那么h(t)，c(t)的维数是多少？— <strong class="lw ir"> h(t)和c(t)的尺寸为【19x 1】</strong></li></ol><p id="f4cd" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><strong class="lw ir">样本LSTM网络# 2 </strong></p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi pd"><img src="../Images/76243fc3323ebe146cd466000049661c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jsQL8k934omb47IalVHIYg.png"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">示例LSTM网络# 2</p></figure><ol class=""><li id="5297" class="nr ns iq lw b lx ms mb mt mf nt mj nu mn nv mr nw nx ny nz bi translated">这个网络中有多少个LSTM层？—<strong class="lw ir">LSTM的总层数为5层。</strong></li><li id="62b8" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如图所示，序列长度或时间步数是多少？— <strong class="lw ir">这个网络的序列长度为1。</strong></li><li id="6f57" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如果x(t)是[45x1]，h1(int)是[25x 1]——C1(int)和o1(t)的维数是多少？— <strong class="lw ir"> c1(t)和o1(t)将与h1(t)具有相同的尺寸，即[25x1] </strong></li><li id="59e4" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如果x(t)是[4x1]，h1(int)是[5x1]，o2(t)的大小是[4x1]。LSTM0和LSTM1的权重矩阵的大小是多少？—<strong class="lw ir">LSTM的权重矩阵为[4 * output _ dim *(input _ dim+output _ dim+1)]。LSTM0的输入尺寸为[4x1]，LSTM0的输出尺寸为[5x1]。LSTM1的输入是LSTM0的输出，因此LSTM1的输入dim与LSTM0的output_dim相同，即[5x1]。LSTM1的输出dim为[4x1]。因此，LSTM0是[4*6*(5+4+1)]=288，而LSTM1是[4*4*(5+4+1)] = 160。</strong></li><li id="771d" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如果x(t)是[10x1]，h1(int)是[7 x1]lst m1的输入维数是多少？— <strong class="lw ir">看上面的解释。在计算之前，我们需要知道LSTM0的输出维数。</strong></li><li id="4d35" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如果x(t)是[6x1]，h1(int)是[4x1]，o2(t)是[3x1]，o3(t)是[5x1]，o4(t)是[9x1]，o5(t)是[10x1]网络的总重量大小是多少？— <strong class="lw ir">单个时间步长的LSTM的权重矩阵为[4 * output _ dim *(input _ dim+output _ dim+1)]。通过估计单个层的输入输出维度来工作。</strong></li></ol><p id="e3e3" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><strong class="lw ir">样本LSTM网络# 3 </strong></p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi pe"><img src="../Images/8b652fa7a0a8f67f782ec58698ddf2c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*ic7f2Uz0FBTPMWKOilKG5A.png"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">样本LSTM网络# 3</p></figure><ol class=""><li id="39f6" class="nr ns iq lw b lx ms mb mt mf nt mj nu mn nv mr nw nx ny nz bi translated">这个网络有多少层？— <strong class="lw ir">有两层</strong></li><li id="a393" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">这个网络中显示的序列长度是多少？— <strong class="lw ir">每层展开2次。</strong></li><li id="15bb" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如果x(t)是[80x1]，h1(int)是[10x1]，那么o(t)，h1(t)，c1(t)，f(t)，i(t)的维数是多少？这些在图中没有显示，但是您应该能够标记出来。— <strong class="lw ir"> o(t)，h1(t)，c1(t)，f1(t)，i1(t)将具有与h1(t)相同的尺寸，即【10x 1】</strong></li><li id="c10f" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如果x(t+1)是[4x1]，o1(t+1)是[5x1]，o2(t+1)是[6x1]。LSTM0和LSTM1的权重矩阵的大小是多少？—<strong class="lw ir">LSTM的权矩阵由4 * output _ dim *(input _ dim+output _ dim+1)给出。LSTM0将是4*5*(4+5+1)，即200。LSTM2将是4*6*(5+5+1) = 264。</strong></li><li id="c32f" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如果x(t+1)是[4x1]，o1(t+1)是[5x1]，o2(t+1)是[6x1]。乘法和累加运算的总数是多少？——<strong class="lw ir">我把这个留给读者。如果有足够多的人问，我会回答:)</strong></li></ol><p id="ff95" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><strong class="lw ir">样本LSTM网络# 4 </strong></p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/59c284d31d0fed09db82f36461bd1d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*RegRL9qexbNbO_KdHERCDA.png"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">样本LSTM网络# 4</p></figure><ol class=""><li id="d678" class="nr ns iq lw b lx ms mb mt mf nt mj nu mn nv mr nw nx ny nz bi translated">有多少层？— <strong class="lw ir">有3层。</strong></li><li id="2406" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如图所示，这个网络展开了多少时间步？— <strong class="lw ir">每层展开3次。</strong></li><li id="738a" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">这个网络总共要执行多少个方程？— <strong class="lw ir">每个LSTM需要6个方程(某些文本合并为5个方程)。然后是6个方程/时间步长/LSTM。因此，6个方程* 3个LSTM层* 3个时间步长= 54个方程</strong></li><li id="2a82" class="nr ns iq lw b lx oa mb ob mf oc mj od mn oe mr nw nx ny nz bi translated">如果x(t)是[10x1]估计LSTM1的权矩阵还需要哪些信息？LSTM2呢？— <strong class="lw ir">一个LSTM的权重矩阵由4 * output _ dim *(input _ dim+out _ dim+1)给出。因此，对于每个LSTM，我们都需要input_dim和output_dim。LSTM的输出是LSTM1的输入。我们有输入维度[10x1]，因此我们需要输出维度或o1(int)维度以及LSTM1的输出维度，即o2(t)。同样，对于LSTM2，我们需要知道o2(t)和o3(t)。</strong></li></ol><h1 id="96e0" class="kl km iq bd kn ko of kq kr ks og ku kv kw oh ky kz la oi lc ld le oj lg lh li bi translated">摘要</h1><p id="dedc" class="pw-post-body-paragraph lu lv iq lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">围绕LSTMs的博客和论文经常在定性的层面上讨论它。在本文中，我试图从计算的角度解释LSTM运算。从计算的角度理解LSTMs是至关重要的，尤其是对于机器学习加速器的设计者来说。</p><h1 id="2741" class="kl km iq bd kn ko of kq kr ks og ku kv kw oh ky kz la oi lc ld le oj lg lh li bi translated">参考资料和其他链接</h1><p id="e51f" class="pw-post-body-paragraph lu lv iq lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated"><a class="ae kk" href="https://medium.com/@manurastogi_74075/yet-another-article-explaining-lstms-f3417442c2cd" rel="noopener"> DL面试准备</a></p><p id="d320" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><a class="ae kk" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">克里斯托弗·奥拉赫的博客。</a></p><p id="521f" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><a class="ae kk" href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" rel="noopener">阎石的博客</a></p><p id="3c25" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">来自<a class="ae kk" href="https://www.deeplearningbook.org/contents/rnn.html" rel="noopener ugc nofollow" target="_blank">深度学习书籍</a>的LSTMs</p><p id="7fed" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><a class="ae kk" href="https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/" rel="noopener ugc nofollow" target="_blank"> Nvidia关于加速LSTMs的博客</a></p><p id="a397" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><a class="ae kk" href="https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/" rel="noopener ugc nofollow" target="_blank">关于机器学习掌握的lst ms</a></p><p id="4722" class="pw-post-body-paragraph lu lv iq lw b lx ms lz ma mb mt md me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">Pete Warden关于<a class="ae kk" href="https://petewarden.com/2018/06/11/why-the-future-of-machine-learning-is-tiny/" rel="noopener ugc nofollow" target="_blank">“为什么机器学习的未来很小”</a>的博客。</p></div></div>    
</body>
</html>