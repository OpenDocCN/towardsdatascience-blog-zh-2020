<html>
<head>
<title>Learning with minibatch Wasserstein</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用迷你批处理学习Wasserstein</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-with-minibatch-wasserstein-d87dcf52efb5?source=collection_archive---------36-----------------------#2020-04-01">https://towardsdatascience.com/learning-with-minibatch-wasserstein-d87dcf52efb5?source=collection_archive---------36-----------------------#2020-04-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/59fb82fb2c03cedf9972dbbf1cddca1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPxy6I_x1FuGHfIXN4TVyw.png"/></div></div></figure><h2 id="146d" class="jc jd je bd b dl jf jg jh ji jj jk dk jl translated" aria-label="kicker paragraph">迷你批次Wasserstein距离属性简介</h2><div class=""/><div class=""><h2 id="7304" class="pw-subtitle-paragraph kk jn je bd b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb dk translated">当我们对迷你批次使用Wasserstein距离时会发生什么？论文<a class="ae lc" href="https://arxiv.org/pdf/1910.04091.pdf" rel="noopener ugc nofollow" target="_blank">的结果来自minibatch Wasserstein的学习:渐近和梯度性质</a>，发表在<a class="ae lc" href="https://www.aistats.org/" rel="noopener ugc nofollow" target="_blank"> AISTATS 2020 </a>会议上。</h2></div><p id="7616" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于许多机器学习应用，如生成模型[1]或领域适应[2]，最优传输已经变得非常流行。在这些应用中，人们希望最小化源数据和目标数据之间的统计距离。为此，瓦瑟斯坦距离成了一项基本资产。它既可以用原始公式[2，3]计算，也可以用对偶公式[1]计算，并依靠小批量进行优化。不幸的是，由于连续函数的梯度计算，对偶可能导致数值不稳定，因此使用原始公式。出于学习目的，可以在[第9.4、4节]中找到对每种配方的利弊的综述。</p><p id="7b0c" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然而，计算迷你批次之间的初始Wasserstein距离<strong class="lf jo"> <em class="lz">并不等同于计算完全测量之间的初始wasser stein距离</em> </strong>。在这个故事中，我将描述<strong class="lf jo"><em class="lz">minibatch wasser stein distance</em></strong>，<strong class="lf jo"> <em class="lz"> </em> </strong>其中mini batch范式对损失的后果被<strong class="lf jo"> <em class="lz">置之不理。</em> </strong>对所呈现结果的全面回顾可见于我们的<a class="ae lc" href="https://arxiv.org/abs/1910.04091" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jo"> <em class="lz"> AISTATS2020论文</em></strong></a><strong class="lf jo"><em class="lz"/></strong>【5】。</p><blockquote class="ma mb mc"><p id="1236" class="ld le lz lf b lg lh ko li lj lk kr ll md ln lo lp me lr ls lt mf lv lw lx ly im bi translated">声明:为了简单起见，我将给出Wasserstein距离的结果，但所有这些结果都可以扩展到所有最优运输变量。我们也考虑一般地面成本。此外，我们将不区分元素集和它们的度量。</p></blockquote><h1 id="34a2" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated">瓦瑟斯坦距离</h1><p id="8bc0" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">基于Kantorovich问题，<strong class="lf jo"> Wasserstein距离通过根据地面度量<strong class="lf jo"><em class="lz"/></strong>寻找测量值α和测量值β之间的<strong class="lf jo">最小位移成本</strong>来测量两个分布之间的</strong>距离。设α (size <em class="lz"> n </em>)和β (size <em class="lz"> n </em>)为两个<em class="lz">离散有界一致测度</em>设<strong class="lf jo"> <em class="lz"> C </em> </strong>为一个度量(size <em class="lz"> n × n </em>)。瓦瑟斯坦距离定义为:</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/5c5260845ff38f94d52be1784f29ea97.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*-343eVxRmYAkOheNhCkJGg.png"/></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">情商。(1):瓦瑟斯坦距离</p></figure><p id="ab80" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">⟨在哪里？,.⟩是Frobenius乘积和<em class="lz"> E </em> <strong class="lf jo"> <em class="lz"> ( </em> </strong> <em class="lz"> α，β)约束的集合</em>。Wasserstein距离<em class="lz">必须在完全测量值α和β之间进行计算。U </em>不幸的是，它在数据数量上有一个立方复杂度<em class="lz"> O(n^3) </em>，使它不适合大数据应用。OT问题的变体出现了，如熵OT或Sinkhorn散度，但它仍然具有平方复杂度。为了克服这种复杂性，可以依靠计算源和目标测量的小批次之间的Wasserstein距离。</p><h1 id="2ecb" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated">小批量Wasserstein距离</h1><p id="1bfc" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">使用小批量策略很有吸引力，因为它在小批量大小上给出了一个立体的复杂性<em class="lz"> O(m^3) </em>。然而，最优运输的原始公式不是一个总和，使用小批量并不等同于等式。(1).实际上，它<strong class="lf jo"> <em class="lz">并不计算瓦瑟斯坦距离</em> </strong>，而是计算从输入测量中采样的迷你批次 上瓦瑟斯坦距离的<strong class="lf jo"> <em class="lz">期望值。形式上，它计算:</em></strong></p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/19d21bd055a98d22723726915d51a8a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*hrOzQYNDDGXdw6-C0r31RA.png"/></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">情商。(2) : <strong class="bd nn"> <em class="no">批次间Wasserstein距离的期望值</em> </strong></p></figure><p id="ec81" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其中<em class="lz"> m </em>为批量。由于它不等同于原始问题，所以理解这个新的损失是很有趣的。我们将回顾运输计划的结果，渐近统计特性，最后，一阶优化方法的梯度特性。</p><h1 id="078b" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated">估计小批量Wasserstein</h1><p id="2890" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">让我们首先设计一个估计量。情商。(2)可以用以下估算器进行估算:</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/797e0e0acc80e241d193dd0b604a0304.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*WZfr8_xptuvvuawTeE5EEQ.png"/></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">情商。(3):Eq的估计量。(2)</p></figure><p id="a8aa" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其中总和取自源和目标测量中所有可能的小批测量A和B。然而，要计算的小批量项目太多了。幸运的是，我们可以依赖子样本量。我们注意到<em class="lz"> D_k，</em>一组基数<em class="lz"> k </em>，其元素是均匀绘制的小批偶。我们定义:</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3faac81f0715c37226318e15c2b69e09.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*j--sgcbX5yjdoXAijcyclg.png"/></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">情商。(4):方程的不完全估计量。(2)</p></figure><p id="0179" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其中<em class="lz"> k </em>是minibatch对的数量。我们还可以为运输计划计算一个类似的估计量，以估计小批量范式对运输计划的影响，更具体地说，是样本之间的联系(本文将详细介绍这一结构)。这个想法是平均样本之间的连接，以获得平均运输计划。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/8eb70774bb34cfe6c9d975c11e25dd61.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*6vj7_KFoOl6xebOZ-33Enw.png"/></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">情商。(5):小批量Wasserstein计划估计量</p></figure><p id="1491" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当然，存在一个不完全的估计量，它遵循与等式1相同的结构。(4):</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2004cd8b3b4b6ed0274df746a7432058.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*nznTMYMCABM3kf_h6XXpaw.png"/></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">情商。(6):小批量Wasserstein计划不完全估计量</p></figure><p id="6144" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于现在我们可以估计我们的数量，我们将给出一个小而有用的例子。</p><h1 id="11ca" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated">1D案例:迷你批次切片Wasserstein</h1><p id="8b89" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">1D案例是一个有趣的特例。这很有趣，因为当数据位于1D时，我们可以获得接近形式的瓦瑟斯坦距离，然后，我们可以很容易地计算加班计划。1D案例也是一个广泛使用的距离的基础，切片瓦瑟斯坦距离。计算全部最小批量OT计划的公式(等式。(5))可以在论文中找到。</p><p id="1a90" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们考虑了源域和目标域的20个数据，采用统一的权重，并绘制了几个ot场景的平均运输计划。实验显示了不同批量的小批量OT计划<em class="lz"> m </em>和<em class="lz">T3】正则化变量(熵+ L2)的OT计划之间的差异。</em></p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/fb1ae6337a086a92165c5eb2e0a72655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHkSNONaXJxbV3jiwwGZ9w.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">1D措施的不同OT问题之间的运输计划[5]</p></figure><p id="d7cd" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们在<strong class="lf jo">小批量瓦瑟斯坦距离</strong>和<strong class="lf jo">正则化瓦瑟斯坦变异之间看到了<strong class="lf jo">相似的效应</strong>。</strong>我们得到<strong class="lf jo">个样本之间的非最优连接</strong>。对于小批量Wasserstein距离，当<em class="lz"> m </em>减少时<strong class="lf jo">连接数量增加。当正则化系数变大时，它类似于熵OT变量。人们还可以注意到，当批量减少时，连接的最高强度降低，这是由于约束。既然我们已经看到了小批量对运输计划的影响，让我们回顾一下损失的性质。</strong></p><h1 id="4471" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated">基本属性</h1><p id="8b08" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">由于我们有了一个新的损失函数，有必要回顾它的优点和缺点，以比较概率分布。它具有以下属性:</p><ul class=""><li id="7ccd" class="nu nv je lf b lg lh lj lk lm nw lq nx lu ny ly nz oa ob oc bi translated">对于<em class="lz"> iid </em>数据，<strong class="lf jo"> <em class="lz"> U </em> </strong>和<strong class="lf jo"><em class="lz">ũ</em></strong>是Eq的无偏估计量。(2)</li><li id="9496" class="nu nv je lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated"><strong class="lf jo"> <em class="lz"> U </em> </strong>和<strong class="lf jo"><em class="lz">ũ</em></strong>的论点是对称的</li><li id="3be8" class="nu nv je lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated"><strong class="lf jo"><em class="lz"/></strong><strong class="lf jo"><em class="lz"/></strong><strong class="lf jo"><em class="lz">严格来说是</em> </strong>阳性</li><li id="097c" class="nu nv je lf b lg od lj oe lm of lq og lu oh ly nz oa ob oc bi translated"><strong class="lf jo"> <em class="lz"> U( </em> </strong> α，α)和<strong class="lf jo"><em class="lz">ũ(</em></strong>α，<strong class="lf jo"> <em class="lz"> </em> </strong>都是<strong class="lf jo"> <em class="lz">严格意义上的</em> </strong>正</li></ul><p id="08ac" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这里有趣的属性是最后一个。对于非平凡测度，我们打破了可分性距离公理。因此，<strong class="lf jo"> <em class="lz"> minibatch Wasserstein距离不是距离</em> </strong>。这是获得数字速度的代价。我们将在梯度流实验中强调这种效应。</p><p id="55b1" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于我们不知道分布<em class="lz"> α </em>和<em class="lz"> β，</em>我们想知道等式。(2)可以用等式有效地估计。(4).</p><h1 id="0511" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated">统计特性</h1><p id="6210" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">我们的不完全估计器定义了一个<strong class="lf jo"> <em class="lz">不完全双样本U-统计量</em> </strong>。U-statistics是由Hoeffding在60年代开发的[6]。使用Hoeffding不等式，可以得到我们的估计量围绕其期望值的偏差界限，概率为1-δ:</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/2335654a3c1f4edf63f4334c8da14ba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*v5Cp15X6V4c0HRDE__wJwQ.png"/></div></figure><p id="b1da" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其中<em class="lz"> M </em>是支撑α和β的大小。这个偏差范围表明，如果我们增加数据<em class="lz"> n </em>和批次<em class="lz"> k </em>的数量，同时保持小批次大小固定，误差会以指数速度收敛到0。值得注意的是，界限不依赖于数据的维度，这在高维优化时是一个有吸引力的属性。</p><p id="a9b9" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于生成模型，我们发现小的<em class="lz"> k </em>足以获得有意义的结果，但是使用小批量会导致更长的训练时间。</p><p id="73ae" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">运输计划和边际利润也有类似的性质。预计运输计划与<em class="lz"> 1/n </em>之间的偏差概率为1-δ:</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/4f4a785df61cdc30ade981d4446fc534.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*rr1oQgmuiT0GiUy4YWTATg.png"/></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">到边缘的距离</p></figure><p id="4eec" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因为我们知道我们的损失有很大的统计性质，我们知道要研究是否可以用现代优化框架将其最小化。</p><h1 id="d1f4" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated">SGD的无偏梯度</h1><p id="34b8" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">众所周知，经验Wasserstein距离相对于连续测量值之间的Wasserstein距离具有偏差梯度[7]。这种偏差使得最小化经验Wasserstein距离不会导致连续测量之间的Wasserstein距离最小化。</p><p id="ee59" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">与Wasserstein距离不同，<strong class="lf jo"><em class="lz">mini batch wasser stein</em></strong>具有很好的属性，即具有<strong class="lf jo"> <em class="lz">无基底梯度</em> </strong>，因此我们可以使用SGD和我们的不完全估计量来最小化连续测量之间的损失。</p><p id="0b53" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这一结果在熵OT变体中得到了证明。与瓦瑟斯坦距离不同，熵OT处处可微。这是我们证明的一个基本要素。它允许我们使用无偏估计量和微分引理来证明无偏梯度。然而，我们在实践中使用迷你批次Wasserstein距离时没有遇到任何问题。</p><h1 id="539f" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated">生成模型</h1><p id="ed68" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">我们举例说明了生成模型的小批量Wasserstein损失的使用。目标是学习生成模型以生成接近目标数据的数据。我们绘制了8000个点，它们遵循2D的8个不同的高斯模式(每个模式1000个点),其中模式形成一个圆。生成数据后，我们使用迷你批次Wasserstein距离和迷你批次Sinkhorn散度作为平方欧几里德成本的损失函数，并将它们与WGAN [1]及其具有梯度惩罚WGAN-GP [8]的变体进行比较。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/6ad679aaf45f4748c5c73f826f672485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QQ-HOts4nAzB-grCjKlDcw.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">高斯模式生成[5]</p></figure><p id="9fec" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们看到，我们能够按照不同的模式生成数据。在CIFAR 10数据集上使用minibatch Sinkhorn散度的更广泛的结果可在[3]中获得。</p><h1 id="a7dd" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated"><strong class="ak">梯度流量</strong></h1><p id="7f79" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">在本节中，我们将介绍小型Wasserstein梯度流的使用。我们考虑来自名人数据集的5000张男性和5000张女性图像，并希望在男性和女性图像之间应用梯度流。梯度流的目的是模拟一个分布，该分布在每次迭代中遵循梯度方向，使最小批次Wasserstein距离最小化。形式上，我们对以下等式进行积分:</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/424dfa648a81477687ef67d40c41aa5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*fFnMgSUKCsjS3IV4adYPgQ.png"/></div></figure><p id="cb73" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于这个实验，我们将批量大小<em class="lz"> m </em>设置为500，将批量对数量<em class="lz"> k </em>设置为10。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/8644744e5bbfe8fb3db50bb0911e9f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMgoISoRHEE3IWzef5Krfg.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">5000个男性图像和5000个女性图像之间的梯度流实验[5]</p></figure><p id="cb1a" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们看到沿着梯度流动的图像的自然演变。然而最后的结果有点模糊。这是因为事实上<strong class="lf jo">小批量Wasserstein距离不是距离</strong>，并且我们<strong class="lf jo">与目标分布</strong>不匹配。</p><h1 id="d790" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated">大规模颜色转移</h1><p id="7dde" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">我们还用我们的方法进行了颜色转移实验。颜色传输的目的是转换源图像的颜色，使其符合目标图像的颜色。最优运输是解决这个问题的一个众所周知的方法[9]。两点云图像之间的传输计划通过使用重心投影给出了传输颜色映射。其思想是使用已开发的小批量运输计划估计器，该估计器具有较小的内存和计算成本。我们使用了两张各为1M像素的图像，几个批次大小和批次数量。据我们所知，这是第一次有一种方法能够处理大规模的颜色转换。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/436460570b5530b13fdfa889d40cdfdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kozJLhWrrgQ0DKF-9IxemA.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">大规模彩色转印[5]</p></figure><p id="7b0b" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以看到，当批量太小时，颜色的多样性下降，就像熵解算器对大正则化参数所做的那样。这显然是由于源样本和目标样本之间的大量连接。然而，即使对于100万像素，1000的批量大小也足以保持良好的颜色多样性。</p><h1 id="3ea2" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated">结论</h1><p id="2800" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">在这篇文章中，我们描述了小批量Wasserstein距离。一个Wasserstein距离变量，其目的是计算迷你批次上的原始Wasserstein距离。我们描述了一个形式，这个损失函数的基本性质，渐近性质，最后，优化程序。然后我们通过三个不同的实验研究了它的用途。关于我们如何改善迷你批次Wasserstein距离还有许多问题，我们将在未来的博客帖子中详细介绍。</p><h1 id="a81d" class="mg mh je bd mi mj mk ml mm mn mo mp mq kt mr ku ms kw mt kx mu kz mv la mw mx bi translated">文献学</h1><p id="a8c5" class="pw-post-body-paragraph ld le je lf b lg my ko li lj mz kr ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">[1]马丁·阿尔乔夫斯基，苏史密斯·钦塔拉，莱昂·博图。<a class="ae lc" href="https://arxiv.org/abs/1701.07875" rel="noopener ugc nofollow" target="_blank">瓦瑟斯坦甘</a>，ICML 2017</p><p id="5059" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[2] BB Damodaran，B Kellenberger，R Flamary，D Tuia，N Courty，“<a class="ae lc" href="https://arxiv.org/abs/1803.10081" rel="noopener ugc nofollow" target="_blank"> DeepJDOT:无监督域适应的深度联合分布最优传输</a>”，ECCV 2018。</p><p id="5c87" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[3]奥德·热纳维、加布里埃尔·佩尔、马尔科·库图里。<a class="ae lc" href="https://arxiv.org/abs/1706.00292" rel="noopener ugc nofollow" target="_blank">用sinkhorn分歧学习生成模型</a>，AISTATS 2018</p><p id="c166" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[4]加布里埃尔·佩尔，马尔科·库图里。<a class="ae lc" href="https://arxiv.org/abs/1803.00567" rel="noopener ugc nofollow" target="_blank">计算最优运输</a>，基础与趋势</p><p id="9cfd" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[5]基利安·法特拉斯、尤尼斯·津、雷米·弗拉芒里、雷米·格里邦瓦尔、尼古拉斯·库蒂。<a class="ae lc" href="https://arxiv.org/abs/1910.04091" rel="noopener ugc nofollow" target="_blank">用minibatch Wasserstein学习:渐近和梯度性质</a>，AISTATS 2020</p><p id="3de8" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[6]瓦西里·赫夫丁。<a class="ae lc" href="https://link.springer.com/chapter/10.1007%2F978-1-4612-0865-5_26" rel="noopener ugc nofollow" target="_blank">有界随机变量和的概率不等式</a>，美国统计协会杂志1963年</p><p id="d32d" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[7]马克·贝勒马尔、伊沃·达尼埃尔卡、威尔·达布尼、沙基尔·穆罕默德、巴拉吉·拉克什米纳拉亚南、斯蒂芬·霍耶、雷米·穆诺斯。<a class="ae lc" href="https://arxiv.org/abs/1705.10743" rel="noopener ugc nofollow" target="_blank">Cramer距离作为有偏wasserstein梯度的解决方案。</a></p><p id="4449" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[8]伊桑·古尔拉贾尼、法鲁克·艾哈迈德、马丁·阿尔约夫斯基、文森特·杜穆林、亚伦·库维尔。<a class="ae lc" href="http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans" rel="noopener ugc nofollow" target="_blank">wasser stein GANs的改进培训</a>，NIPS 2017</p><p id="f89d" class="pw-post-body-paragraph ld le je lf b lg lh ko li lj lk kr ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[9]西拉·费拉丹斯、尼古拉·帕帕达基斯、朱利安·拉宾、加布里埃尔·佩雷、让-弗朗索瓦·奥约尔。<a class="ae lc" href="https://link.springer.com/chapter/10.1007/978-3-642-38267-3_36" rel="noopener ugc nofollow" target="_blank">正则化离散最优运输</a>。计算机视觉中的尺度空间和变分法。</p></div></div>    
</body>
</html>