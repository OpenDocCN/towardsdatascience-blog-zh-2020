<html>
<head>
<title>Complex Web of AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能的复杂网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/complex-web-of-ai-b6dd86156722?source=collection_archive---------25-----------------------#2020-02-27">https://towardsdatascience.com/complex-web-of-ai-b6dd86156722?source=collection_archive---------25-----------------------#2020-02-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="aeb0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为之前关于<a class="ae ko" href="https://medium.com/@siilime/swift-for-tensorflow-820a651e4b14" rel="noopener"> Swift for TensorFlow (S4TF) </a>和<a class="ae ko" rel="noopener" target="_blank" href="/the-strong-arm-of-mobile-ml-5d44713072f4">the hardware powering modern AI</a>的起源的文章的后续，本文是专注于机器学习和网络的两篇文章的第一部分。第二部分将关注作为数据平台的Web，本文将关注人工智能的新兴工具，以及Web将如何成为以一种前所未有的方式连接智能的一流平台</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/c13bc6cc6bceb517eeb456d9f9623220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1TwSSsNt25qGilW4pFPuWg.jpeg"/></div></div></figure><h1 id="477e" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">介绍</h1><p id="6f97" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">虽然人工智能工具在网络上的发展并不是最近的事情，但我们将在2019年9月开始，在日本福冈的一家酒店举办一场研讨会，作为<a class="ae ko" href="https://www.w3.org/2019/09/TPAC/" rel="noopener ugc nofollow" target="_blank">万维网联盟技术全体会议/咨询委员会(W3C TPAC) </a>会议的一部分。在这次会议上，一群行业专家对新兴技术有着独特的观点，这些技术有可能将Web和AI结合在一起，为智能解决方案创建一个无与伦比的开发平台。</p><p id="7a98" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这次会议中，坐着微软Edge团队的一名成员，在他右边坐着苹果Safari开发团队的一名成员，然后是来自英特尔的一名工程师(也是Web / WebNN W3C group 的<a class="ae ko" href="https://webmachinelearning.github.io" rel="noopener ugc nofollow" target="_blank">机器学习的创始人)，以及来自谷歌的两个人，包括</a><a class="ae ko" href="https://www.tensorflow.org/js" rel="noopener ugc nofollow" target="_blank"> TensorFlow.js </a>的一名主要专家。</p><p id="a76e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">里面的对话涵盖了广泛的主题，包括<a class="ae ko" href="https://webassembly.org" rel="noopener ugc nofollow" target="_blank"> WebAssembly </a>、硬件设计和编译工具链，还包括几个关于相关技术的演示，这些技术将不可避免地有助于<em class="me">面向Web的机器学习</em>规范。</p><p id="1eae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">许多材料之前已经共享过，是主要浏览器供应商和更广泛的领域专家社区之间日益丰富的数据共享的一部分，它显示了网络作为一流人工智能平台的发展正在进行的承诺。</p><h1 id="d75c" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">找到与开发者的和谐</h1><p id="e840" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">支撑这场运动的技术并不完全集中在网络上，也不应该完全集中在网络上。至关重要的是，在创建一套跨平台的通用且无处不在的机器学习工具的过程中，经验和发展也是非常重要的，这使得Web易于开发。</p><p id="0a31" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样，TensorFlow针对特定介质有多种排列(<a class="ae ko" href="https://www.tensorflow.org" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>、<a class="ae ko" href="https://www.tensorflow.org/lite" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite </a>、<a class="ae ko" href="https://www.tensorflow.org/js" rel="noopener ugc nofollow" target="_blank"> TensorFlow.js </a>等)。)高级机器学习工具通常需要在缩小到特定问题领域之前呈现更广泛的需求视图，在方法上具有兼容性和通用性，以简化开发人员的入口点。考虑典型的编程范例，如函数式编程(FP)或面向对象编程(OOP):这两种编程范例都在各种语言中使用，以确保易于被开发人员采用，并有一套熟悉的解决问题的方法。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mf"><img src="../Images/c239df2b52ab69c00c301eb9acb8d15a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h5HhgpCuJWLolf5gRar4uA.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">简化低级到高级ML工具分离</p></figure><p id="ad4e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种机器学习方法的简化对于向所有开发者开放可访问性是必不可少的。由于苹果(<a class="ae ko" href="https://developer.apple.com/documentation/coreml" rel="noopener ugc nofollow" target="_blank"> Core ML </a>和谷歌(<a class="ae ko" href="https://developers.google.com/ml-kit/" rel="noopener ugc nofollow" target="_blank"> ML Kit) </a>的专有工具，移动工具变得更加丰富，这两个工具都支持使用Python编写的解决方案，以及像TensorFlow这样的机器学习库，可以毫不费力地移植到分别用<a class="ae ko" href="https://swift.org" rel="noopener ugc nofollow" target="_blank"> Swift </a>和<a class="ae ko" href="https://kotlinlang.org" rel="noopener ugc nofollow" target="_blank"> Kotlin </a>编写的产品上。</p><p id="dacb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2017年4月，谷歌的DeepMind团队一直在开发TensorFlow的高级抽象，<a class="ae ko" href="https://sonnet.dev" rel="noopener ugc nofollow" target="_blank">宣布了这个名为Sonnet的项目的开源，旨在让其他开发者更容易从通用开发模型中受益，同时仍然使用TensorFlow生态系统中固有的高度调整和强大的功能。</a></p><p id="4f37" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当然，这些开发人员工具中的大多数仍然是基于Python的，但Python几乎不是编写机器学习模型的最佳工具。它缺乏静态类型、对定制操作的支持、一流的并发性，并且不能很好地扩展:它本质上仍然是一种解释型的单CPU语言，这对于仍然大量使用C++的科学研究案例来说不是很好。事实上，机器学习中的Python一般只是对C++中执行的计算操作的抽象，需要移植到每个平台，同时还要考虑沿途的硬件架构。随着我们在边缘设备(如智能手机)上构建越来越多的解决方案，以及向ML库中添加新的操作(由于语言的性质，这对于Python来说并不直接)，这变得越来越复杂。目前TensorFlow的运营每年增长约15-20 %,并且已经支持了数千个项目:使这个生态系统可用于网络的任务不可避免地非常复杂。</p><h1 id="f4c7" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">推动模型的发展</h1><p id="0393" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">这就是Swift for TensorFlow最初出现的地方。谷歌的工程师们对Swift作为未来一流ML语言的能力深信不疑，他们为编译工具链做出了贡献，在基于<a class="ae ko" href="https://llvm.org" rel="noopener ugc nofollow" target="_blank"> LLVM </a>的堆栈中添加了<a class="ae ko" href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="noopener ugc nofollow" target="_blank">自动微分</a>等功能，目标是让Swift执行安全、并发和编译的模型，为开发人员提供熟悉和直观的体验，以及模型开发期间的编译时检查和调试。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mk"><img src="../Images/a2c0e28e1f797820f505160ce5910044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bjzzDK9cfZDIoYBjsYodCA.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">简化实现目标性能和工具的Swift方法</p></figure><p id="cf64" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，用Python编写的看似无限的库不会很快迁移到Swift，更重要的是，专注于C++和Python的学术论文和教学材料不会迁移到Swift。重要的是<a class="ae ko" href="https://www.tensorflow.org/swift/tutorials/python_interoperability" rel="noopener ugc nofollow" target="_blank"> Swift支持导入Python模块</a>和类型互操作性(见下面的Swift片段),但是，尽管如此，在未来一段时间内，大多数研究将使用Python。</p><pre class="kq kr ks kt gt ml mm mn mo aw mp bi"><span id="e566" class="mq lc it mm b gy mr ms l mt mu">import Python</span><span id="4ecc" class="mq lc it mm b gy mv ms l mt mu">let np = Python.import("numpy")<br/>print(np)<br/>let zeros = np.ones([2, 3])<br/>print(zeros)</span></pre><p id="43b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Python中缺乏性能就是为什么2017年3月<a class="ae ko" href="https://developers.googleblog.com/2017/03/xla-tensorflow-compiled.html" rel="noopener ugc nofollow" target="_blank">谷歌宣布了XLA(加速线性代数)</a>，这是一个针对TensorFlow的编译器，允许每个操作针对目标架构进行<a class="ae ko" href="https://blog.tensorflow.org/2018/11/pushing-limits-of-gpu-performance-with-xla.html" rel="noopener ugc nofollow" target="_blank">优化，使用JIT编译，用于CPU、GPU，当然还有谷歌的TPU。XLA是TensorFlow核心的一部分，因此它可用于用Python开发的每个模型。为了支持额外的架构(例如，边缘设备上不断增长的npu ), XLA通过使用LLVM IR支持新后端</a>的<a class="ae ko" href="https://www.tensorflow.org/xla/developing_new_backend" rel="noopener ugc nofollow" target="_blank">添加。</a></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mw"><img src="../Images/6b84155d6aaaa8fec97dbcbfe541b448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Gf6cy8S2d5KGIHbunTo_A.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">TensorFlow编译器生态系统</p></figure><p id="a71e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实上，XLA是TensorFlow的一部分，并由LLVM提供支持，这意味着它也可用于其他基于TensorFlow的工具，特别是可以与用于TensorFlow的<em class="me">Swift</em>一起使用。然而，对于像Python这样的解释语言来说，基于JIT的模型编译更多的是一种权宜之计，而不是一种现代机器学习语言的方法。如果我们真的想提高高级现代语言中机器学习的性能，那么编译工具链需要将模型视为代码。</p><h1 id="5a27" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">作为代码的模型</h1><p id="89f4" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">用于TensorFlow  的<a class="ae ko" href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="noopener ugc nofollow" target="_blank"> <em class="me"> Swift令人兴奋的发展之一是来自LLVM的支持，以支持将模型作为代码进行Swift编译(以及通过<em class="me"> Jupyter </em>等工具调试模型的LLDB)。</em></a></p><p id="98d3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">编译期间优化代码是通过中间表示(IR)实现的。由于Swift SIL，Swift拥有了这种能力，这使它成为一种速度惊人的语言，当与类型安全和并发性相结合时，它使开发生产就绪代码变得快速而简单，占用空间最小。<em class="me"> Swift for TensorFlow </em>致力于将编写程序时可用的相同类型的现代和高性能开发引入机器学习，并使每个开发人员都可以使用机器学习，无论解决方案需要多深或多浅。</p><p id="1322" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Swift通过对<a class="ae ko" href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="noopener ugc nofollow" target="_blank">自动区分(AD) </a>的一流语言支持实现了这一目标。下面是一个常见的例子，展示了如何导入一个C函数并使其在Swift中可区分(是的，Swift可以导入C库以及Python)。</p><pre class="kq kr ks kt gt ml mm mn mo aw mp bi"><span id="bca1" class="mq lc it mm b gy mr ms l mt mu">import Glibc<br/><br/>func sillyExp(_ x: Float) -&gt; Float {<br/>    let 𝑒 = Float(M_E)<br/>    print("Taking 𝑒(\(𝑒)) to the power of \(x)!")<br/>    return pow(𝑒, x)<br/>}<br/><br/>@differentiating(sillyExp)<br/>func sillyDerivative(_ x: Float) -&gt; (value: Float, pullback: (Float) -&gt; Float) {<br/>    let y = sillyExp(x)<br/>    return (value: y, pullback: { v in v * y })<br/>}<br/><br/>print("exp(3) =", sillyExp(3))<br/>print("𝛁exp(3) =", gradient(of: sillyExp)(3))</span></pre><p id="6457" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这将我们带到一个世界，在这个世界中，编译工具链不仅仅是在转换为可执行程序的过程中优化人类可读的指令，而是完全支持将机器学习操作优化为目标平台的模型，安全且具有高级别的抽象。</p><p id="ff7b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，这篇文章不是关于Swift的状态，而是关于网络人工智能的状态，所以让我们回到我们目前所处的位置，以及Python社区是如何解决性能缺陷的。</p><p id="9a59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">说到广告，Python社区已经开发了各种工具，最著名的是现在已经不存在的哈佛大学开发的<a class="ae ko" href="https://github.com/hips/autograd" rel="noopener ugc nofollow" target="_blank">亲笔签名。同样，这更多的是对Python缺陷的修补，而不是重大的飞跃，但它是令人印象深刻的Python优化工具生态系统的一部分，当您试图最大限度地利用您的模型时，它变得必不可少。</a></p><p id="fb6f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当核心开发人员在2017年开始为<a class="ae ko" href="https://github.com/google/jax" rel="noopener ugc nofollow" target="_blank"> JAX </a>做出贡献时，亲笔签名就变得过时了，这是一个亲笔签名的基于XLA的工具，用于进一步提高基于Python的机器学习模型的性能。虽然经常有局外人的观点认为JAX正在与<em class="me"> Swift争夺TensorFlow </em>，但现实是两者都可能仍然是高性能机器学习的强有力选择，并将继续愉快地共存，这要归功于它们的共同点。</p><h1 id="7aa1" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">让我们来点诗意的吧</h1><p id="5f46" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">在本文的前面，我提到了对高级抽象的需求，以简化新开发人员的入门。Sonnet非常适合TensorFlow抽象，但从那时起，DeepMind已经用前面提到的JAX增加了TensorFlow的使用，以获得它带来的性能优势，因此Sonnet不再与它们的使用相关。</p><p id="66d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">俳句是DeepMind的项目，用于在JAX之上提供类似OOP的概念，本质上取代了Sonnet作为一个高级API。它简化了JAX的使用，提供了一种熟悉的编程方法，并且可以访问JAX库的纯性能功能。</p><p id="67d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，如果你试图开发具有轻松<em class="me">和</em>性能的模型，那么结合俳句和JAX是一个强有力的方法，并受益于现有Python生态系统的成熟。</p><p id="ebbd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">俳句仍然被认为是alpha版本，但是与Swift的当前状态相比，它可以被认为是足够成熟的，可以在今天使用，只是需要一些温和的谨慎。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="1fb5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到目前为止，我已经从性能工具和开发人员体验(DX)的角度介绍了人工智能开发的现状，但是这些与Web有什么关系呢？</p><h1 id="19d3" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">陷入网中</h1><p id="a2cb" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">2018年<a class="ae ko" href="https://blog.tensorflow.org/2018/03/introducing-tensorflowjs-machine-learning-javascript.html" rel="noopener ugc nofollow" target="_blank">谷歌公布tensor flow . js</a>；流行的JavaScript和Web机器学习库。当时，这是一种学习机器学习的好方法，特别是如果你是一名网络开发人员，但不是一种推动机器学习或人工智能研究边界的伟大工具。</p><p id="c6a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，TensorFlow.js缺乏对许多操作的支持、隐式浏览器支持以及固有的硬件支持。这是一个很好的业余爱好工具，但严肃的数据科学家不会用它来替代云提供的机器学习工具。</p><p id="53e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从那时起，TensorFlow.js发展迅速，实验也在不断增加，但与用于移动机型的<em class="me"> Core ML </em>和<em class="me"> ML Kit </em>，或用于edge设备的<em class="me"> TensorFlow Lite </em>相比，TensorFlow.js甚至还不够。在发布过程中，我从来没有想到会是这样，直到我加入了Web 组的<a class="ae ko" href="https://webmachinelearning.github.io" rel="noopener ugc nofollow" target="_blank">机器学习。</a></p><p id="4b6f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们首先讨论TensorFlow.js的位置，因为有很多夸张的说法，主要是web开发人员，认为这将使JavaScript(和Web)成为开发和交付模型的最佳场所。这与事实相差甚远:TensorFlow.js更接近TensorFlow Lite和Core ML，旨在解决相同的问题。</p><p id="9e36" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你想让你的移动或边缘解决方案更智能、更主动或更个性化，你不必求助于将每一点数据发送到你的云ML服务，然后返回指令。您在目标设备上开发模型，并利用提供的硬件(如GPU或神经处理单元(npu))和操作系统嵌入式工具向最终用户提供计算机视觉、语音识别或个性化功能。网络没有对等物。这是一个没有生产就绪的ML生态系统的平台。</p><p id="e8c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">TensorFlow.js旨在将移动和边缘开发者正在开发的相同工具带到Web上，以便最终用户体验可以以几乎相同的方式得到增强。这本质上为最终用户带来了额外的好处，如隐私优先的解决方案(不再过分热情地将用户数据发送到云服务)，并为开发人员开发具有完全离线功能的渐进式web应用程序(pwa)提供了更多激励。</p><p id="ab2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，我们还没有实现TensorFlow.js，这并不是因为存在不可避免的障碍，这些障碍将不可避免地影响任何让机器学习在网络上与移动和边缘设备平等的尝试。</p><h1 id="0b64" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">编织复杂的网络</h1><p id="0fda" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">我们先来讨论一下TensorFlow.js的实际工作原理。首先，它必须实现与其他TensorFlow库相同的操作。这意味着支持数以千计的操作，每年以难以跟上的速度呈指数级增长。而且TensorFlow.js不仅仅是将Python和C++中的模型翻译成JavaScript(因为那样会很简单，对吧？)，但是它需要来自硬件的操作支持，通过每个浏览器。</p><p id="6074" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">目前，它通过使用WebGL来访问设备的GPU加速来运行指令，这种工作方式相当粗糙。没错，通常用于执行3D转换的Web GPU语言正被用于运行机器学习操作。这是有道理的:GPU长期以来一直在较低层次上这样做，但通过浏览器看到这种情况发生令人兴奋。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ne"><img src="../Images/2ad1372df94d57750178c422ea002c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FM7ZjYNBVFjJXEL39Amuqg.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">TensorFlow.js架构(由TensorFlow.js社区提供)</p></figure><p id="939d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">那为什么这看起来如此不合理呢？世界已经变了。通过编译工具(例如通过LLVM的XLA)和神经处理单元形式的定制硬件(包括运行在所有最新Android和iOS设备上的<a class="ae ko" href="https://cloud.google.com/tpu/" rel="noopener ugc nofollow" target="_blank">谷歌的TPU </a>和各种定制的<a class="ae ko" href="https://www.arm.com/products/silicon-ip-cpu/machine-learning/ethos-n77" rel="noopener ugc nofollow" target="_blank">ARM npu</a>)推动GPU进行ML算术为优化让路。应该没有必要再利用WebGL hacks了。</p><p id="083a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">再来看工装。早些时候，我们谈到了<a class="ae ko" href="https://www.tensorflow.org/xla/" rel="noopener ugc nofollow" target="_blank"> XLA </a>和<a class="ae ko" href="https://llvm.org" rel="noopener ugc nofollow" target="_blank"> LLVM </a>如何通过操作和交付针对目标平台(硬件和软件)优化的编译后的机器代码，为Python和Swift模型提供优化。对于Swift，这是通过Swift SIL实现的，它是Swift程序的中间表示(IR ),使程序能够在构建过程中重新解释。Python没有相应的功能，因此它依赖于实时(JIT)优化——它们是Python模型的优秀助推器，但工作量很大，仍然达不到静态类型编译语言的要求，因为无法保证操作树的哪个分支将执行。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nf"><img src="../Images/f1e46cfdbb8ffaf1a7944832d7b4a75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*siOCw5v-UX3nOX1Gz_fffQ.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">Swift编译器基础设施</p></figure><p id="a697" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">XLA可能会在Python模型上提供15%的改进，而TensorFlow的Swift可以快400%。</p><p id="1181" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就其松散的类型和解释而言，JavaScript看起来更接近Python而不是Swift，因此XLA路线看起来像是基于浏览器的ML的逻辑方式。除了XLA创建机器码，TensorFlow.js的很多好处是它可以在浏览器中运行。</p><p id="d90b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如何将JavaScript编译成浏览器可以理解的优化的机器指令？<a class="ae ko" href="https://en.wikipedia.org/wiki/WebAssembly" rel="noopener ugc nofollow" target="_blank"> WebAssembly </a>对吧？其实不是，简单来说，WebAssembly在自己的内存空间中运行，无法与你的模型交互。毫无疑问，它将为使用ML的各种复杂终端解决方案提供性能改进——事实上<a class="ae ko" href="https://github.com/tensorflow/tfjs/tree/master/tfjs-backend-wasm" rel="noopener ugc nofollow" target="_blank"> TensorFlow.js有一个WASM后端</a>，其他项目如<a class="ae ko" href="https://github.com/Microsoft/onnxjs" rel="noopener ugc nofollow" target="_blank"> ONNX有自己的WASM项目</a>——但这些都是通过CPU或WebGL间接操作的。一般来说，它们比纯JavaScript更快，所以每次开发模型时都应该使用它们，但是要将IR优化的潜力与本机硬件支持相匹配，它们还有很长的路要走。</p><p id="8308" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是<a class="ae ko" href="https://webmachinelearning.github.io/webnn" rel="noopener ugc nofollow" target="_blank">网络神经网络API (WebNN) </a>讨论的由来。要让TensorFlow.js等于<em class="me"> TensorFlow Lite </em>、<em class="me"> Core ML </em>和<em class="me"> ML Kit </em>，就意味着浏览器厂商需要入局。在模型执行上，考虑它们与<em class="me"> Android </em>或<em class="me"> iOS </em>处于同一操作层面。这需要标准化(从来不会很快)和合作(即使有最好的意图也可能很复杂)。</p><p id="72ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们记住，大多数机器学习开发是用一种低效的语言(Python)完成的，然后通过像<em class="me"> Core ML </em>和<em class="me"> TensorFlow Lite </em>这样的工具转换到每个目标平台，这意味着如果他们想充分利用机器学习，每个开发人员都必须知道Python，不管他们开发什么。尽管现代设备内置了对机器学习操作的隐式支持，但像Swift和Kotlin这样的语言无法直接访问，但情况仍然如此。</p><h1 id="f5ff" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">把它扯下来</h1><p id="65e7" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">XLA优化工具目前能够优化由JAX、<a class="ae ko" href="https://julialang.org" rel="noopener ugc nofollow" target="_blank">茱莉亚</a>和<a class="ae ko" href="https://pytorch.org" rel="noopener ugc nofollow" target="_blank">皮托奇</a>构建的模型。它通过另一个称为<a class="ae ko" href="https://www.tensorflow.org/xla/architecture" rel="noopener ugc nofollow" target="_blank"> HLO(或高级优化器)</a>的IR组件来实现这一点。这通过一个可扩展的框架来支持各种架构的各种后端，LLVM IR能够针对各种CPU和GPU进行优化。本质上，XLA·HLO是基于IR的优化的另一个抽象。</p><p id="7aca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">退一步讲，很明显，Swift SIL、XLA和LLVM IR都面临着同样的问题。Rust还有<a class="ae ko" href="https://rust-lang.github.io/rustc-guide/mir/index.html" rel="noopener ugc nofollow" target="_blank"> MIR </a>和<a class="ae ko" href="https://rust-lang.github.io/rustc-guide/hir.html" rel="noopener ugc nofollow" target="_blank"> HIR </a>，利用LLVM IR，连同Go。如果我们能够在现代语言中结合XLA和LLVM IR的方法，我们将能够轻松、安全地开发具有C++性能的模型。<em class="me">tensor flow的Swift</em>承诺了这一点，但是其他语言呢？</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nf"><img src="../Images/54c77c25c4fb2d5f9bc694b2a8c0ffee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h3MvFyUUok_wXugywc55hg.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">LLVM IR支持的各种编译器基础设施</p></figure><p id="60e9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们试图开发一个支持编程语言和机器学习模型的统一编译器工具链，我们很快就会陷入困境。对硬件发展的依赖以及数字抽象和ML开发之间的差异意味着这两个问题的规模完全不一致。抽象成ML建模的中间表示对于这种工作是至关重要的。</p><p id="3d1b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是MLIR(多层次中间代表制)的用武之地。MLIR是Google基于LLVM IR创建一个可扩展工具链的尝试，以允许模型的优化编译。目前MLIR支持各种方言，包括TensorFlow IR、TensorFlow Lite、XLA HLO和LLVM IR，但它为将来添加更多方言提供了基础。Google <a class="ae ko" href="https://mlir.llvm.org" rel="noopener ugc nofollow" target="_blank">向LLVM </a>捐赠了MLIR，因此它可以从LLVM社区在开发强大的指令优化基础设施方面的丰富经验中受益。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ng"><img src="../Images/cb9e60b6f4a42cf33de69f916ac614c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m-lhr6Jjk2-wffr8V9JjGQ.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">将XLA·HLO加入LLVM IR</p></figure><p id="546a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到那时，一种能够为浏览器优化一组通用指令的方言可能并不遥远。WebNN一直在<a class="ae ko" href="https://webmachinelearning.github.io/webnn/" rel="noopener ugc nofollow" target="_blank">寻找用例来进一步实现这一点，以及实现这一点所需的基本操作</a>，在此阶段不要过多关注MLIR。</p><p id="cb67" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这需要超越CPU和GPU。在未来，各种设备拥有不同的npu来完成特定任务并不是不可行的，比如语音识别或计算机视觉。操作系统已经做出了将特定指令输送到哪里的智能决策，但网络仍然对CPU和GPU只有天真的理解。</p><p id="33e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">浏览器供应商已经通过允许为一组指令指定优选的处理器架构来解决这一问题，如果不可用，将选择后备。这将使npu能够在浏览器中使用，并与MLIR的优化操作相结合，这可能意味着直接来自web应用程序的机器学习模型的接近本机的性能，最终使web应用程序获得与移动设备一直受益的ML工具相同的访问级别。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="adee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们离这还有很长的路要走，因为网络事实上不是一个平台。这是一个生态系统。更容易将浏览器描述为平台(即Chrome / Chromium、Firefox / Servo、Safari / WebKit等。)对于web应用程序，但是这些标准的互操作性依赖于合作，对于机器学习，它需要仔细考虑编译器技术和硬件路线图的非常不同的现实。</p><p id="5781" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一个正在进行的讨论，可能是一段时间以来网络上最大的变化。它将为ML驱动的web解决方案带来隐私承诺，并开创一个web应用程序和便携式产品的新时代，这些应用程序和产品刚刚进入现代移动设备。</p><p id="21f8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于图路线的可能排列，使用解释的IR优化模型的当前方式是低效的。机器学习已经是耗电大户，因此降低执行模型所需的能耗是任何未来ML工具的一个基本基准。这可以通过转移到C++来实现，或者通过将大多数ML开发转移到可以提供类似性能的语言来实现，这些语言具有更容易的入口点，允许每个人访问能够实现高效模型训练和执行的工具。后者将是更可取的。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="9346" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">MLIR和WebNN将改善性能、工具和网络内外的机器学习选项，但已经有了高级机器学习工具，尚未被大多数开发人员利用。通常采用的方法是将性能问题归咎于算法，而大多数流行的库已经支持针对特定架构的优化。使用像XLA(和JAX)这样的功能已经可以在培训和模型执行的可靠生产性能方面产生影响。尽可能使用WebAssembly也将在Web上提供优势。</p><p id="abcb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">MLIR和WebNN正在发展，但是开发人员不仅要学习他们正在使用的库的层和算法API，还要学习如何从提供的各种工具中获得性能增益。这意味着了解目标硬件以及如何打开它。</p><p id="6586" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然本文的大部分内容都集中在TensorFlow上，但是对所提到的工具所做的部分努力是关于跨高级工具的统一性和通用性，以简化研究和开发收益。这意味着像<em class="me"> PyTorch </em>和<em class="me"> ONNX </em>这样的项目正朝着同一个方向前进，并且有一定程度的互操作性和兼容性，这将使开发人员更容易选择工具。</p></div></div>    
</body>
</html>