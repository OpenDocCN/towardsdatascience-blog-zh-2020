<html>
<head>
<title>Chunking in NLP: decoded</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的分块:解码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/chunking-in-nlp-decoded-b4a71b2b4e24?source=collection_archive---------8-----------------------#2020-04-11">https://towardsdatascience.com/chunking-in-nlp-decoded-b4a71b2b4e24?source=collection_archive---------8-----------------------#2020-04-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="99a7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">它在文本处理中起什么作用</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="b2b1" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">当我开始学习文本处理时，我坚持了很长时间的一个话题是组块。(我知道，很难相信🙆)通常，我们可以在网上找到许多文章，从容易到难的主题，但当谈到这个特定的主题时，我觉得没有一篇文章可以对组块有全面的理解，然而下面的一篇文章是我迄今为止研究的所有与该主题相关的文章或视频的合并。</p><p id="7ca4" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">所以下面是我对组块的理解。</p><h1 id="b72d" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">那么，什么是组块？</h1><p id="4761" class="pw-post-body-paragraph kp kq it kr b ks md ju ku kv me jx kx ky mf la lb lc mg le lf lg mh li lj lk im bi translated">组块是从非结构化文本中提取短语的过程，这意味着分析句子以识别成分(名词组、动词、动词组等)。)然而，它没有具体说明它们的内部结构，也没有说明它们在主句中的作用。</p><p id="48b9" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">它工作在词性标注之上。它使用词性标签作为输入，并提供组块作为输出。</p><blockquote class="mi mj mk"><p id="fa3e" class="kp kq ml kr b ks kt ju ku kv kw jx kx mm kz la lb mn ld le lf mo lh li lj lk im bi translated">简而言之，组块意味着将单词/标记分组为组块</p></blockquote><h1 id="8181" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">为什么需要？</h1><p id="1a26" class="pw-post-body-paragraph kp kq it kr b ks md ju ku kv me jx kx ky mf la lb lc mg le lf lg mh li lj lk im bi translated">我曾经认为，在我进一步了解这些主题之前，通常文本处理是通过简单地将句子分解成单词来完成的。所以简单的打断文字并没有太大的帮助。知道这个句子涉及到一个人、一个日期、一个地点等等是非常重要的..(不同实体)。所以他们单独是没有用的。</p><blockquote class="mi mj mk"><p id="c62d" class="kp kq ml kr b ks kt ju ku kv kw jx kx mm kz la lb mn ld le lf mo lh li lj lk im bi translated">组块可以将句子分解成比单个单词更有用的短语，并产生有意义的结果。</p><p id="c040" class="kp kq ml kr b ks kt ju ku kv kw jx kx mm kz la lb mn ld le lf mo lh li lj lk im bi translated">当你想从文本中提取信息时，比如地点、人名，组块是非常重要的。(实体提取)</p></blockquote><p id="51ce" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi mp translated">让我们从头开始理解它。</p><p id="9626" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">一个句子通常遵循由以下部分组成的层次结构。</p><h2 id="4bdc" class="my lm it bd ln mz na dn lr nb nc dp lv ky nd ne lx lc nf ng lz lg nh ni mb nj bi translated">句子→从句→短语→单词</h2><p id="9597" class="pw-post-body-paragraph kp kq it kr b ks md ju ku kv me jx kx ky mf la lb lc mg le lf lg mh li lj lk im bi translated">词组组成短语，有五大类。</p><ul class=""><li id="2ff7" class="nk nl it kr b ks kt kv kw ky nm lc nn lg no lk np nq nr ns bi translated">名词短语</li><li id="0a02" class="nk nl it kr b ks nt kv nu ky nv lc nw lg nx lk np nq nr ns bi translated">动词短语</li><li id="cf13" class="nk nl it kr b ks nt kv nu ky nv lc nw lg nx lk np nq nr ns bi translated">形容词短语</li><li id="f9e1" class="nk nl it kr b ks nt kv nu ky nv lc nw lg nx lk np nq nr ns bi translated">副词短语</li><li id="22a9" class="nk nl it kr b ks nt kv nu ky nv lc nw lg nx lk np nq nr ns bi translated">介词短语</li></ul><p id="fe2b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu"> <em class="ml">短语结构规则:</em> </strong></p><p id="2642" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">S -&gt; NP VP</p><p id="fcc7" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">NP--&gt; { Det N，Pro，PN}</p><p id="bd1b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">VP -&gt; V (NP) (PP) (Adv)</p><p id="38b0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">PP -&gt; P NP</p><p id="6f0e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">AP -&gt; A (PP)</p><figure class="nz oa ob oc gt od gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi ny"><img src="../Images/f5ccaf773a98762c5db086df436f0c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gb67U-4kCIm5ISIQwR2leQ.png"/></div></div><p class="ok ol gj gh gi om on bd b be z dk translated">组块:短语被分成组块(来源:<a class="ae oo" href="https://www.nltk.org/book/ch07.html" rel="noopener ugc nofollow" target="_blank">https://www.nltk.org</a>)</p></figure><blockquote class="op"><p id="3e02" class="oq or it bd os ot ou ov ow ox oy lk dk translated">在深入研究组块之前，先简要了解一下语法树和语法规则是有好处的。</p></blockquote><p id="5cf1" class="pw-post-body-paragraph kp kq it kr b ks oz ju ku kv pa jx kx ky pb la lb lc pc le lf lg pd li lj lk im bi translated">正如我们所见，这里整个句子被分成两个不同的名词短语。</p><p id="301e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">现在让我们用python实验来理解这个概念。</p><ol class=""><li id="9fb2" class="nk nl it kr b ks kt kv kw ky nm lc nn lg no lk pe nq nr ns bi translated"><strong class="kr iu">基于正则表达式的分块</strong></li></ol><p id="de1a" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">基于正则表达式模式的分块器代码段</p><figure class="nz oa ob oc gt od gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/a9210859eabc4735d84cf995bde138c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*8zzU0zIpueQ1xJpfeWz4uQ.png"/></div><p class="ok ol gj gh gi om on bd b be z dk translated">解析树(名词短语基于给定的正则表达式生成)</p></figure><p id="77f1" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这里，我们介绍了一个语法。<br/>其中NP(名词短语)由<br/> DT组合而成？→一个或零个限定词<br/> JJ* →零个或多个形容词<br/> NN →名词</p><p id="1452" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们用NLTK定义的正则表达式解析器来解析这个语法。正如我们所看到的，整个句子S被分成几个组块，并用树状结构表示。基于定义的语法，创建内部树状结构。所以你可以定义你的语法，基于这个句子将被分块。</p><p id="ae0f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu"> 2。基于标签的训练分块器</strong></p><p id="11b9" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我使用了“conll2000”语料库来训练chunker。conll2000语料库使用IOB标签定义组块。</p><p id="9298" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">它指定了块的开始和结束位置，以及它的类型。<br/>POS标签员可以接受这些IOB标签的培训</p><p id="4ff7" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">块标签使用IOB格式。<br/> IOB : Inside，Outside，Beginning <br/>标签前的B前缀表示，它是块的开始<br/> I前缀表示它在块内<br/> O标签表示令牌不属于任何块</p><pre class="nz oa ob oc gt pg ph pi pj aw pk bi"><span id="4c54" class="my lm it ph b gy pl pm l pn po">#Here conll2000 corpus for training shallow parser model</span><span id="b452" class="my lm it ph b gy pp pm l pn po">nltk.download('conll2000')<br/>from nltk.corpus import conll2000</span><span id="5526" class="my lm it ph b gy pp pm l pn po">data= conll2000.chunked_sents()<br/>train_data=data[:10900]<br/>test_data=data[10900:]</span><span id="04b7" class="my lm it ph b gy pp pm l pn po">print(len(train_data),len(test_data))<br/>print(train_data[1])</span></pre><figure class="nz oa ob oc gt od gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/756a40a8f87f7d679cf3c7273d1924ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*KN4XXFzbwHhoXhFdZ0QKwA.png"/></div><p class="ok ol gj gh gi om on bd b be z dk translated">“conll2000”数据集的记录</p></figure><p id="13c9" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">tree2conlltags，conlltags2tree是分块实用函数。</p><p id="c00b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">→`<strong class="kr iu">tree 2 conltags</strong>`，获取三元组(每个令牌的单词、标签、块标签)。然后，这些元组最终用于训练标签器，并且它学习POS标签的IOB标签。</p><p id="8791" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">→ ` <strong class="kr iu"> conlltags2tree </strong>`从这些令牌三元组生成解析树<br/> Conlltags2tree()是tree2conlltags()的反转。我们将使用这些函数训练我们的解析器</p><pre class="nz oa ob oc gt pg ph pi pj aw pk bi"><span id="29da" class="my lm it ph b gy pl pm l pn po">from nltk.chunk.util import tree2conlltags,conlltags2tree</span><span id="5a76" class="my lm it ph b gy pp pm l pn po">wtc=tree2conlltags(train_data[1])<br/>wtc</span></pre><figure class="nz oa ob oc gt od gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/5938079f1b9ce854cc7cb50455d153d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*41AKdc6d6QWji_UCY7Wmlg.png"/></div></figure><pre class="nz oa ob oc gt pg ph pi pj aw pk bi"><span id="6305" class="my lm it ph b gy pl pm l pn po">tree=conlltags2tree(wtc)<br/>print(tree)</span></pre><figure class="nz oa ob oc gt od gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/216cb8c4be4c7437b04800d33dfcc66c.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*Ya0hLqwyPFVLvyxsqabyAg.png"/></div></figure><pre class="nz oa ob oc gt pg ph pi pj aw pk bi"><span id="3a84" class="my lm it ph b gy pl pm l pn po">def conll_tag_chunks(chunk_sents):<br/>    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]<br/>    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]</span><span id="30ad" class="my lm it ph b gy pp pm l pn po">def combined_tagger(train_data, taggers, backoff=None):<br/>    for tagger in taggers:<br/>        backoff = tagger(train_data, backoff=backoff)<br/>    return backoff</span></pre><h1 id="050f" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">tagger是什么？</h1><p id="72c1" class="pw-post-body-paragraph kp kq it kr b ks md ju ku kv me jx kx ky mf la lb lc mg le lf lg mh li lj lk im bi translated">它读取文本并给每个单词分配一个POS标签。(单词、标签)</p><p id="1f07" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu"> Unigram tagger </strong>:确定POS时，只使用一个单词。(基于单词上下文的标记器)</p><p id="66fd" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><code class="fe pt pu pv ph b">UnigramTagger</code>、<code class="fe pt pu pv ph b">BigramTagger</code>和<code class="fe pt pu pv ph b">TrigramTagger</code>是继承自基类<code class="fe pt pu pv ph b">NGramTagger</code>的类，基类本身继承自<code class="fe pt pu pv ph b">ContextTagger</code>类，后者继承自<code class="fe pt pu pv ph b">SequentialBackoffTagger</code>类</p><p id="da4a" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们现在将定义一个类<code class="fe pt pu pv ph b">NGramTagChunker</code>，它将接受带标签的句子作为训练输入，获取它们的<strong class="kr iu">(单词、词性标签、组块标签)WTC三元组</strong>并训练一个带有<code class="fe pt pu pv ph b">UnigramTagger</code>的<code class="fe pt pu pv ph b">BigramTagger</code>作为补偿标签。</p><p id="ed5d" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们还将定义一个parse()函数来对一个新句子执行浅层解析。</p><pre class="nz oa ob oc gt pg ph pi pj aw pk bi"><span id="bcb4" class="my lm it ph b gy pl pm l pn po">from nltk.tag import UnigramTagger, BigramTagger<br/>from nltk.chunk import ChunkParserI</span><span id="b16b" class="my lm it ph b gy pp pm l pn po">#Define the chunker class<br/>class NGramTagChunker(ChunkParserI):<br/>  def __init__(self,train_sentences,tagger_classes=[UnigramTagger,BigramTagger]):<br/>    train_sent_tags=conll_tag_chunks(train_sentences)<br/>    self.chunk_tagger=combined_tagger(train_sent_tags,tagger_classes)</span><span id="664a" class="my lm it ph b gy pp pm l pn po">def parse(self,tagged_sentence):<br/>    if not tagged_sentence:<br/>      return None<br/>    pos_tags=[tag for word, tag in tagged_sentence]<br/>    chunk_pos_tags=self.chunk_tagger.tag(pos_tags)<br/>    chunk_tags=[chunk_tag for (pos_tag,chunk_tag) in chunk_pos_tags]<br/>    wpc_tags=[(word,pos_tag,chunk_tag) for ((word,pos_tag),chunk_tag) in zip(tagged_sentence,chunk_tags)]<br/>    return conlltags2tree(wpc_tags)</span><span id="6ecb" class="my lm it ph b gy pp pm l pn po">#train chunker model<br/>ntc=NGramTagChunker(train_data)</span><span id="8df3" class="my lm it ph b gy pp pm l pn po">#evaluate chunker model performance<br/>print(ntc.evaluate(test_data))</span></pre><figure class="nz oa ob oc gt od gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/fb73436782eda9e096348c36fe842d63.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*0iFFVmhI6L6SKqrnoSWRng.png"/></div></figure><p id="1422" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">现在，我们将利用这个模型对我们的新闻标题样本进行浅层解析和分块。</p><pre class="nz oa ob oc gt pg ph pi pj aw pk bi"><span id="7df5" class="my lm it ph b gy pl pm l pn po">import pandas as pd<br/>sentence='No new emoji may be released in 2021 due to COVID-19 pandemic word'<br/>nltk_pos_tagged=nltk.pos_tag(sentence.split())<br/>pd.DataFrame(nltk_pos_tagged,columns=['word','POS tag'])</span></pre><figure class="nz oa ob oc gt od gh gi paragraph-image"><div class="gh gi px"><img src="../Images/b7ff4e07bd2da0a58119aab3d8bc3b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*JTHxQZC567GDK56LZzIvnQ.png"/></div></figure><pre class="nz oa ob oc gt pg ph pi pj aw pk bi"><span id="b2d6" class="my lm it ph b gy pl pm l pn po">chunk_tree=ntc.parse(nltk_pos_tagged)<br/>print(chunk_tree)</span></pre><figure class="nz oa ob oc gt od gh gi paragraph-image"><div class="gh gi py"><img src="../Images/0c0b965b0423780e637ff28525ba411d.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*pcZKXBE4AUYqJcObNR2kFw.png"/></div><p class="ok ol gj gh gi om on bd b be z dk translated">语法树</p></figure><pre class="nz oa ob oc gt pg ph pi pj aw pk bi"><span id="3d63" class="my lm it ph b gy pl pm l pn po">chunk_tree</span></pre><figure class="nz oa ob oc gt od gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi pz"><img src="../Images/f58941baa18a4ce2ecfd63d746be6dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mg108Ptm0UasbqlKq20aIg.png"/></div></div><p class="ok ol gj gh gi om on bd b be z dk translated">解析树</p></figure><p id="902f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">你也可以根据需要定义基于分类器的分块器。你可以在这里了解更多。</p><p id="8d3a" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><a class="ae oo" href="https://www.geeksforgeeks.org/nlp-classifier-based-chunking-set-1/?ref=rp" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/NLP-classifier-based-chunking-set-1/？ref=rp </a></p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h2 id="b315" class="my lm it bd ln mz na dn lr nb nc dp lv ky nd ne lx lc nf ng lz lg nh ni mb nj bi translated">分块的另一个子过程叫做“分块”</h2><p id="0165" class="pw-post-body-paragraph kp kq it kr b ks md ju ku kv me jx kx ky mf la lb lc mg le lf lg mh li lj lk im bi translated">我们创建一个令牌序列，它不包含在块中。所以这是寻找洞察力或背景。(我不在这一部分讨论)</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="4c19" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">感谢您的阅读。🙏</p><p id="cff2" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我试图最大程度地涵盖这个话题。欢迎建议。</p><p id="d77e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">作为荣誉奖，我要感谢<a class="qa qb ep" href="https://medium.com/u/6278d12b0682?source=post_page-----b4a71b2b4e24--------------------------------" rel="noopener" target="_blank">迪潘然(DJ)萨卡尔</a>。我一直跟着他的教程从零开始学NLP。</p><blockquote class="mi mj mk"><p id="695d" class="kp kq ml kr b ks kt ju ku kv kw jx kx mm kz la lb mn ld le lf mo lh li lj lk im bi translated">页（page的缩写）这是我的第一篇技术文章。希望随着学习的进步，我会继续写作。</p></blockquote><h2 id="2965" class="my lm it bd ln mz na dn lr nb nc dp lv ky nd ne lx lc nf ng lz lg nh ni mb nj bi translated">参考</h2><ul class=""><li id="85b5" class="nk nl it kr b ks md kv me ky qc lc qd lg qe lk np nq nr ns bi translated">https://youtu.be/b4nbE-pG_TM<a class="ae oo" href="https://youtu.be/b4nbE-pG_TM" rel="noopener ugc nofollow" target="_blank"/></li><li id="f189" class="nk nl it kr b ks nt kv nu ky nv lc nw lg nx lk np nq nr ns bi translated"><a class="ae oo" rel="noopener" target="_blank" href="/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72">https://towards data science . com/a-从业者-指南-自然语言-处理-第一部分-处理-理解-文本-9f4abfd13e72 </a></li><li id="b7ec" class="nk nl it kr b ks nt kv nu ky nv lc nw lg nx lk np nq nr ns bi translated"><a class="ae oo" href="https://www.geeksforgeeks.org/nlp-chunking-and-chinking-with-regex/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/NLP-chunking-and-chinking-with-regex/</a></li><li id="7e9d" class="nk nl it kr b ks nt kv nu ky nv lc nw lg nx lk np nq nr ns bi translated"><a class="ae oo" href="https://www.geeksforgeeks.org/nlp-training-tagger-based-chunker-set-1/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/NLP-training-tagger-based-chunker-set-1/</a></li></ul></div></div>    
</body>
</html>