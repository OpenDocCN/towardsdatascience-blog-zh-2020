<html>
<head>
<title>Object Detection using YOLOv3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用YOLOv3的对象检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-detection-using-yolov3-9112006d1c73?source=collection_archive---------23-----------------------#2020-06-21">https://towardsdatascience.com/object-detection-using-yolov3-9112006d1c73?source=collection_archive---------23-----------------------#2020-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9fe2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用YOLOv3和OpenCV实时检测物体的旅程</h2></div><p id="fc37" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">深度学习彻底改变了计算机视觉领域。神经网络被广泛应用于几乎所有的尖端技术，如特斯拉的自动驾驶功能。他们表现得太好了，有时会导致道德问题和冲突。我们今天不会深入讨论这些。让我们来关注一下计算机视觉的一个子类，叫做“检测”。</p><p id="d494" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">探测一个物体是什么意思？当我们看到一个物体时，我们可以准确地指出它在哪里，并轻松地确定它是什么。然而对于计算机来说，任务并不简单。多年来，这一直是一个活跃的研究领域，今天仍然如此。在过去的十年里，随着深度学习的出现(而不是复兴)，我们能够取得良好的结果，在一定程度上已经可以在实时场景中使用它。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/38814dbfece14a3dff18b9ee661d1638.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*kJcsE7xrRzbbb4-qCkMWgg.jpeg"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图片来自<a class="ae lx" href="https://en.wikipedia.org/wiki/Object_detection" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><h2 id="929a" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">概观</h2><p id="4bad" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">有几种用于检测的神经网络结构</p><ul class=""><li id="b238" class="mw mx it kk b kl km ko kp kr my kv mz kz na ld nb nc nd ne bi translated">R-CNN系列架构</li><li id="9600" class="mw mx it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated">单发探测器</li><li id="a6fc" class="mw mx it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated">YOLO——你只能看一次</li></ul><p id="0d8f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们今天将看到YOLOv3的实现(最初YOLO架构的一个变种),但不会详细介绍它是如何工作的。之所以选择Python(❤语，是因为它得到了OpenCV等库的大力支持。值得一提的另一个要点是，YOLO模型不如R-CNN模型准确，但它们很快，很容易适用于实时应用。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h2 id="8eed" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">履行</h2><p id="70a1" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">让我们从导入必要的库开始。OpenCV库将是我们在本教程中最好的朋友，因为它有几个用于操作图像的有用函数，以及一些有用的模块，比如‘dnn’。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="a84c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于我们将使用预先训练好的模型，我们必须下载某些文件。“权重”文件、“配置”文件和“可可名”文件。权重和配置文件可以在<a class="ae lx" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank">链接</a>中找到，coco-names文件可以从<a class="ae lx" href="https://github.com/pjreddie/darknet/blob/master/data/coco.names" rel="noopener ugc nofollow" target="_blank">这里</a>下载/复制。有几种预先训练好的模型可用，我们将使用“yolov 3–416”模型。在MS COCO数据集上训练模型，该数据集上存在80类对象。</p><p id="191c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下载完所有文件后，就该创建和加载我们的模型了。正如您在下面看到的，dnn模块有几个内置的函数在这方面帮助我们。我们的模型被训练识别的对象的名称在“coco.names”文件中给出，我们将该文件存储在一个名为classes的列表中。我们还在<strong class="kk iu"> getLayerNames() </strong>和<strong class="kk iu">getUnconnectedOutLayers()</strong>函数的帮助下检索输出层的名称，并将它们存储在output_layers列表中。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="4c3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在必须通过模型传入图像。但是，我们不能直接这样做，因为我们的模型期望我们的图像具有特定的形状。这就是<strong class="kk iu"> cv2.dnn.blobFromImage() </strong>函数派上用场的地方。它帮助我们重塑我们的形象，同时也使它们正常化，并以适当的顺序重新排列颜色通道。</p><p id="4622" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后将图像提供给模型，并执行向前传递。其输出为我们提供了一个检测列表。从该列表中，获得每个检测到的对象的一组边界框坐标，如下所示。我们使用置信度阈值来过滤掉弱检测。我使用的置信度阈值的默认值是“0.5”。所有的包围盒坐标、它们的类别id和它们相应的置信度值分别存储在列表“盒子”、“类别id”和“置信度”中。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="20e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经获得了图像中物体的位置，是时候画出它们的边界框并标记它们了。函数<strong class="kk iu"> draw_boxes() </strong>为我们做了这件事。我们在旅途中可能会遇到的一个问题是，这些物体有时可能会被探测到不止一次。为了避免这种情况，我们将采用非最大值抑制(也称为非最大值抑制)。我使用的NMS阈值的默认值是“0.4”。这是下面的<strong class="kk iu"> cv2.dnn.NMSBoxes() </strong>函数所执行的。我们最后使用<strong class="kk iu"> cv2.imshow() </strong>函数显示输出图像。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="999e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然我们已经看到了所有需要的组件，现在让我们把它们粘在一起，在一个图像文件中执行对象检测。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="0cc8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以从文件和网络摄像头的视频中执行相同的任务，如下所示。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="098f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">右图显示了同一个人周围的多个边界框。在使用NMS之后，我们获得了左边的图像作为输出。重复的包围盒已经被处理。</p><div class="lm ln lo lp gt ab cb"><figure class="nm lq nn no np nq nr paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><img src="../Images/a11cb2d01f33cc7a8f4a736905812762.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*PiVTxELjlYkbkGTmQc08ZQ.png"/></div></figure><figure class="nm lq nw no np nq nr paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><img src="../Images/84a5b32d53e0e06d56aee071a365dc16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*r6PrtUPr2thU3uxZRQZ2XA.png"/></div><p class="lt lu gj gh gi lv lw bd b be z dk nx di ny nz translated">样本输出—从<a class="ae lx" href="http://cocodataset.org/" rel="noopener ugc nofollow" target="_blank">http://cocodataset.org/</a>获得的测试图像</p></figure></div><p id="3625" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过下面的链接，可以在我的GitHub库中找到这篇文章的全部代码以及一个清晰的界面。</p><p id="3e3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lx" href="https://github.com/GSNCodes/YOLOv3_Object_Detection_OpenCV" rel="noopener ugc nofollow" target="_blank">https://github.com/GSNCodes/YOLOv3_Object_Detection_OpenCV</a></p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="0c57" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">深度学习的最新进展为研究和探索开辟了许多途径。如果你想深入研究这个问题，我鼓励你这样做。创造和创新新技术，但要合乎道德。我希望这篇文章对你有所帮助，我很高兴成为你旅程的一部分:)</p><p id="07bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">G.SowmiyaNarayanan </p><p id="42d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">附言:——</strong></p><p id="b7d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我关于媒介的第一篇文章，我欢迎任何批评来改进我的工作，以便我能更好地满足未来像你这样的探险家的需要。欢迎评论，让我知道你的想法。你也可以在<a class="ae lx" href="https://www.linkedin.com/in/sowmiyanarayanan-g/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上和我联系。</p><p id="bb6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">内存字节:- </strong></p><p id="11ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="oa">“若无变化，则无变化。”</em></p></div></div>    
</body>
</html>