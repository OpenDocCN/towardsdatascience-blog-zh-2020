<html>
<head>
<title>Foreground Image Segmentation with FgSegNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 FgSegNet 的前景图像分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/foreground-image-segmentation-with-fgsegnet-9ecbe3d194ab?source=collection_archive---------21-----------------------#2020-08-16">https://towardsdatascience.com/foreground-image-segmentation-with-fgsegnet-9ecbe3d194ab?source=collection_archive---------21-----------------------#2020-08-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7e23" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">具有类似自动编码器结构的多尺度 CNN</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4d095f8a6cf7c340eff36b82cac0a09d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5dbcx6-VpMUlmYZ_LMU2NQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">FgSegNet 的架构图；它使用三个 CNN 模型和一个 TCNN。[1]</p></figure><h1 id="1957" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">介绍</h1><p id="f60b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">一个有待解决的棘手问题是前景图像分割(从不同的角度来看)，即<strong class="ls iu"><em class="mm"/></strong>(或<strong class="ls iu"><em class="mm"/></strong>)。这个任务听起来可能很琐碎:<strong class="ls iu">创建一个二进制蒙版，其中只有移动/重要对象的像素被标记</strong>。然而，当现实世界的可变性被引入到图片中时，这可能变得特别困难(没有双关语)。例如，<strong class="ls iu"> <em class="mm">一个真正鲁棒的图像分割模型必须考虑以下所有</em> </strong>:</p><ul class=""><li id="2985" class="mn mo it ls b lt mp lw mq lz mr md ms mh mt ml mu mv mw mx bi translated">(背景)景色的微妙变化</li><li id="f94a" class="mn mo it ls b lt my lw mz lz na md nb mh nc ml mu mv mw mx bi translated">忽略移动的树木、树叶、雪、雨、阴影等。</li><li id="c8dd" class="mn mo it ls b lt my lw mz lz na md nb mh nc ml mu mv mw mx bi translated">处理光线不好的情况</li><li id="3178" class="mn mo it ls b lt my lw mz lz na md nb mh nc ml mu mv mw mx bi translated">处理相机抖动和/或运动</li><li id="6347" class="mn mo it ls b lt my lw mz lz na md nb mh nc ml mu mv mw mx bi translated">摄像机视野内模糊区域的伪装物体</li><li id="4b9c" class="mn mo it ls b lt my lw mz lz na md nb mh nc ml mu mv mw mx bi translated"><em class="mm">这个清单还在继续……</em></li></ul><p id="fcd7" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated">完成这项任务的最初几个方法之一(在当时)是相当健壮的，本质上是统计学。具体来说，它涉及使用<strong class="ls iu">多个高斯模型来映射输入</strong>的每个像素的每个颜色值(即 RGB)<strong class="ls iu">的分布。如果像素的颜色值在特定帧中与其高斯分布不匹配，则可以确定该像素持有前景对象。然而，这种方法仍然很容易受到上述挑战的影响，但在当时(1999 年)[2]仍然是鲁棒图像分割的一个突破。</strong></p><p id="9299" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated">快进到最近的时代，现在我们有足够的计算能力和数据让卷积神经网络(CNN)和其他复杂的模型相当精确地运行，更不用说简单的前馈网络了。不出所料，<strong class="ls iu"> <em class="mm">现在比以往</em> </strong>有更多基于深度学习的背景减法。</p><blockquote class="ng nh ni"><p id="0df6" class="lq lr mm ls b lt mp ju lv lw mq jx ly nj nd mb mc nk ne mf mg nl nf mj mk ml im bi translated"><em class="it">我们来看看</em> <strong class="ls iu">前景分割网络</strong> <em class="it">，或者</em> <strong class="ls iu"> FgSegNet </strong> <em class="it">，这是一种最近提出的、性能最好的神经网络架构，它使用多个 CNN 和一个转置 CNN (TCNN)来实现背景减法。[1] </em></p></blockquote><h1 id="e3c5" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">理论</h1><h2 id="9dd9" class="nm kz it bd la nn no dn le np nq dp li lz nr ns lk md nt nu lm mh nv nw lo nx bi translated">CNN 和 TCNNs</h2><p id="56e1" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> <em class="mm"> FgSegNet </em> </strong>在其架构内使用 3 个卷积神经网络(CNN)和一个转置 CNN (TCNN)。具体来说，该架构为其每个 CNN 使用一个<strong class="ls iu">预训练的 VGG-16 模型。</strong></p><p id="6109" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated">综上所述，CNN 广泛用于图像特征提取，因此在图像分类中工作良好。卷积层如何在 CNN 内工作的前提是通过<strong class="ls iu">内核</strong>(具有初始化的但可改变的值/权重的小 2D 矩阵)<strong class="ls iu">，其跟踪图像输入并聚集内核值和相应输入像素值之间的乘积</strong>。换句话说，内核<strong class="ls iu"> <em class="mm">在运行时会对输入</em> </strong>进行卷积，这就是 CNN 中的 C。卷积层在实际应用中更容易理解，所以这里有一张内核操作的 GIF 图片。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/4c384c269336c554455c83154ba9ceec.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*B0O-fI8qdwB5wNBO.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CNN 内核的例子。蓝绿色是完整的输出，蓝色是图像输入，阴影/轮廓是内核。内核将其相对于输入的每个值相乘，然后相加，产生一个像素作为输出。[3]</p></figure><p id="5f4c" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated"><strong class="ls iu"> <em class="mm">那么什么是转置 CNN 呢？把它想成是 CNN 应该做的完全相反的事情。CNN 自然产生的输出通常小于其输入；这与 TCNNs 相反，<strong class="ls iu">，因为它的内核执行与 CNN</strong>相反的操作。</em></strong></p><p id="48d5" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated">根据上面的架构图，<strong class="ls iu"> <em class="mm"> FgSegNet </em> </strong>首先将输入馈入三个 CNN，连接三个输出，并将其作为输入馈入 TCNN。最终输出是二进制掩码。</p><h2 id="2b65" class="nm kz it bd la nn no dn le np nq dp li lz nr ns lk md nt nu lm mh nv nw lo nx bi translated">为什么是“类自动编码器”？</h2><p id="1a60" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">论文称<strong class="ls iu"> <em class="mm"> FgSegNet </em> </strong>为“<strong class="ls iu">编解码型网络模型</strong>”，这是名副其实的。首先，<strong class="ls iu">架构类似于自动编码器</strong>的架构:输入首先被漏斗化/瓶颈化成一个更小的特征图(编码)，然后在模型的后半部分结束时被扩展回其原始形状(解码)。CNN 本质上也是编码器，因为它们通过内核操作提取特征。也就是说，<strong class="ls iu">tcnn</strong>(与 CNN 相反)<strong class="ls iu">将是解码器</strong>也是事实。</p><p id="001f" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated">我推测，在前景提取等任务中使用编码器的目的是<strong class="ls iu">精确定位图像帧中容易发生变化的“重要”特征，用通俗的话来说，就是使用压缩信息输出带有 TCNN 的遮罩</strong>。让 3 个不同形状的 CNN 并行工作也支持这一概念，并且<strong class="ls iu">允许该模型更加通用于不同大小的前景对象</strong>。</p><h1 id="60b7" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">表演</h1><h2 id="04aa" class="nm kz it bd la nn no dn le np nq dp li lz nr ns lk md nt nu lm mh nv nw lo nx bi translated">数据集</h2><p id="9059" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">前景提取器模型的一个突出数据集是<a class="ae nz" href="http://changedetection.net/" rel="noopener ugc nofollow" target="_blank"> CDnet2014 数据集</a>。<strong class="ls iu"> CDnet2014 包括模型的 11 种不同挑战场景</strong>(即恶劣天气、相机抖动、夜间视频等。)，每个场景包含 4 到 6 个视频序列。数据集包括地面真实图像/遮罩，标记每一帧的所有前景对象和阴影。<strong class="ls iu"> <em class="mm"> FgSegNet </em> </strong>在多个图像数据集上进行测试，其中一个是 CDnet2014。</p><h2 id="daef" class="nm kz it bd la nn no dn le np nq dp li lz nr ns lk md nt nu lm mh nv nw lo nx bi translated">模型实现</h2><p id="5eed" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> <em class="mm"> FgSegNet </em> </strong>是使用 Keras 和 Tensorflow 框架构建的。它的所有层(除了最后一层)都使用 ReLU 激活层，而 VGG-16 CNN 的多个池层被替换为 dropout 层。除了辍学，该模型利用 L2 权重正则化。RMSProp 被用作具有学习率降低器的优化器，当验证损失停止改善 6 个时期时，学习率降低器被激活。<strong class="ls iu"> <em class="mm">最后，该模型允许选择用 50 或 200 帧图像进行训练。</em>T15】</strong></p><blockquote class="ng nh ni"><p id="4cdb" class="lq lr mm ls b lt mp ju lv lw mq jx ly nj nd mb mc nk ne mf mg nl nf mj mk ml im bi translated">要了解更多关于模型实现的信息，请点击<a class="ae nz" href="https://arxiv.org/abs/1801.02225" rel="noopener ugc nofollow" target="_blank">这里</a>阅读其论文。</p></blockquote><h2 id="0702" class="nm kz it bd la nn no dn le np nq dp li lz nr ns lk md nt nu lm mh nv nw lo nx bi translated">估价</h2><p id="48d8" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> <em class="mm"> FgSegNet </em> </strong>是 CDnet2014 数据集评估期间表现最好的模型之一。当用 200 帧训练时，达到的平均 F 分数是 0.9734(分数范围从 0[差]到 1[最好])，用 50 帧训练时是 0.9545。下面是一个场景分类的例子，以及由模型生成的基本事实和掩码。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/8a1e374e11c66f42d3aa3755500244f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*llsqVC0cW4svXVdG5oLHxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(a)是原始图像。(b)是基本事实。(c)由 FgsegNet 生成。[1]</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2d8d15b7ed7dd02db7e4cfc08b15a6c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*gxV9RA9f6x9PAz_y7yDgUg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像行从上到下依次显示原始图像、地面实况和模型生成的遮罩。[1]</p></figure><p id="0962" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated">正如你所看到的，模型生成的遮罩<strong class="ls iu">非常令人印象深刻</strong>，尤其是对于跟随<em class="mm">基线</em>的三个类。</p><p id="d9c3" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated">在左侧，您可以找到模型意外执行的一些实例。然而，即使对人类来说，分割这些样本看起来也很麻烦。</p><p id="ab42" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated"><strong class="ls iu"> <em class="mm">总的来说，FgSegNet 给人印象非常深刻，是一个很好的前景提取任务。</em>T29】</strong></p><blockquote class="ng nh ni"><p id="fc1f" class="lq lr mm ls b lt mp ju lv lw mq jx ly nj nd mb mc nk ne mf mg nl nf mj mk ml im bi translated">如果你想看看，这里是 github 对<strong class="ls iu"> <em class="it"> FgSegNet </em> </strong>源代码的回购:</p></blockquote><div class="oc od gp gr oe of"><a href="https://github.com/lim-anggun/FgSegNet" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">lim-anggun/FgSegNet</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">这个库包含以下论文的源代码和训练集:“前景分割使用…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">github.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot ks of"/></div></div></a></div><h1 id="fd91" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">参考</h1><p id="a6c3" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">[1] L. Ang Lim 和 H. Yalim Keles，<a class="ae nz" href="https://arxiv.org/abs/1801.02225" rel="noopener ugc nofollow" target="_blank">使用三重卷积神经网络进行多尺度特征编码的前景分割</a> (2018)，arXiv-1801。</p><p id="e3de" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated">[2] C. Stauffer 和 W. E. L. Grimson，<a class="ae nz" href="https://ieeexplore.ieee.org/document/784637" rel="noopener ugc nofollow" target="_blank">用于实时跟踪的自适应背景混合模型</a> (1999)，IEEE 计算机学会会议。</p><p id="8563" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated">[3] V. Dumoulin 和 F. Visin，<a class="ae nz" href="https://arxiv.org/abs/1603.07285" rel="noopener ugc nofollow" target="_blank">深度学习卷积算法指南</a> (2016)，arXiv-1603。</p></div></div>    
</body>
</html>