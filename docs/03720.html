<html>
<head>
<title>Examples of Using Apache Spark with PySpark Using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python将Apache Spark与PySpark结合使用的示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/examples-of-using-apache-spark-with-pyspark-using-python-f36410457012?source=collection_archive---------18-----------------------#2020-04-07">https://towardsdatascience.com/examples-of-using-apache-spark-with-pyspark-using-python-f36410457012?source=collection_archive---------18-----------------------#2020-04-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="0cfd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Apache Spark 是技术领域最热门的新趋势之一。这可能是最有可能实现大数据和机器学习联姻成果的框架。</p><p id="2e6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它运行速度很快(由于内存操作，比传统的<a class="ae ko" href="https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm" rel="noopener ugc nofollow" target="_blank"> Hadoop MapReduce </a>快100倍)，提供健壮的、分布式的、容错的数据对象(称为)，并通过补充包(如<a class="ae ko" href="https://spark.apache.org/mllib/" rel="noopener ugc nofollow" target="_blank"> Mlib </a>和<a class="ae ko" href="https://spark.apache.org/graphx/" rel="noopener ugc nofollow" target="_blank"> GraphX </a>)与机器学习和图形分析的世界完美集成。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/4e955b1538237676bdbb666b5a6df5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/0*NYLdG4jsG2WrVKxq.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated"><a class="ae ko" href="https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/" rel="noopener ugc nofollow" target="_blank">资料来源:Exxact </a></p></figure><p id="da3c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Spark在<a class="ae ko" href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html" rel="noopener ugc nofollow" target="_blank"> Hadoop/HDFS </a>上实现，大部分用<a class="ae ko" href="https://www.scala-lang.org/" rel="noopener ugc nofollow" target="_blank"> Scala </a>编写，一种类似Java的函数式编程语言。事实上，Scala需要在您的系统上安装最新的Java，并在JVM上运行。然而，对于大多数初学者来说，Scala并不是他们进入数据科学世界首先要学习的语言。幸运的是，Spark提供了一个奇妙的Python集成，称为<strong class="js iu"> PySpark </strong>，它允许Python程序员与Spark框架进行交互，并学习如何大规模操作数据，以及如何在分布式文件系统上处理对象和算法。</p><p id="2a0d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本文中，我们将学习PySpark的基础知识。有很多概念(不断发展和引入)，因此，我们只关注一些简单例子的基本原理。我们鼓励读者在这些基础上，自己探索更多。</p><h1 id="3075" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">Apache Spark的短暂历史</h1><p id="681a" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">Apache Spark于2009年作为加州大学伯克利分校AMPLab的一个研究项目启动，并于2010年初开源。这是加州大学伯克利分校的一个班级项目。想法是建立一个集群管理框架，它可以支持不同类型的集群计算系统。该系统背后的许多想法在多年来的各种研究论文中都有介绍。发布后，Spark成长为一个广泛的开发人员社区，并于2013年转移到Apache Software Foundation。今天，该项目是由来自数百个组织的数百名开发人员组成的社区合作开发的。</p><h1 id="f1f8" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">Spark不是一种编程语言</h1><p id="d7c9" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">需要记住的一点是，Spark不是像Python或Java那样的编程语言。它是一个通用的分布式数据处理引擎，适用于各种环境。它对于大规模和高速的大数据处理特别有用。</p><p id="2b62" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">应用程序开发人员和数据科学家通常将Spark集成到他们的应用程序中，以快速查询、分析和转换大规模数据。一些最常与Spark相关联的任务包括:跨大型数据集(通常为万亿字节大小)的ETL和SQL批处理作业；处理来自物联网设备和节点的流数据、来自各种传感器、各种金融和交易系统的数据；以及电子商务或IT应用程序的机器学习任务。</p><p id="1542" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在其核心，Spark建立在Hadoop/HDFS框架之上，用于处理分布式文件。它主要是用Scala实现的，Scala是Java的一种函数式语言变体。有一个核心的Spark数据处理引擎，但在此之上，还有许多为SQL类型的查询分析、分布式机器学习、大规模图形计算和流数据处理而开发的库。Spark以简单接口库的形式支持多种编程语言:Java、Python、Scala和r。</p><h1 id="ae63" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">Spark使用MapReduce范式进行分布式处理</h1><p id="a512" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">分布式处理的基本思想是将数据块分成可管理的小块(包括一些过滤和排序)，使计算接近数据，即使用大型集群的小节点来完成特定的任务，然后将它们重新组合起来。分割部分被称为“映射”动作，重组被称为“缩减”动作。他们共同创造了著名的“MapReduce”范式，这是谷歌在2004年左右推出的(见<a class="ae ko" href="https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf" rel="noopener ugc nofollow" target="_blank">原文)。</a></p><p id="3c75" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，如果一个文件有100条记录要处理，那么100个映射器可以一起运行，每个映射器处理一条记录。或者也许50个映射器可以一起运行，每个处理两个记录。在所有的映射器完成处理之后，框架在将结果传递给reducers之前对它们进行洗牌和排序。映射器仍在运行时，缩减器无法启动。具有相同键的所有地图输出值都被分配给一个reducer，然后该reducer聚合该键的值。</p><h1 id="99cb" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">如何设置PySpark</h1><p id="431d" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">如果您已经熟悉Python和库(如Pandas和Numpy ),那么PySpark是一个很好的扩展/框架，可以通过在后台利用Spark的能力来创建更具可伸缩性、数据密集型的分析和管道。</p><p id="96df" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">安装和设置PySpark环境(在一台独立的机器上)的确切过程有些复杂，可能会因您的系统和环境而略有不同。目标是使用PySpark包让您的常规Jupyter数据科学环境在后台与Spark一起工作。</p><p id="4fc0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://medium.com/free-code-camp/how-to-set-up-pyspark-for-your-jupyter-notebook-7399dd3cb389" rel="noopener"> <strong class="js iu">这篇关于Medium的文章</strong> </a>提供了关于逐步设置过程的更多细节。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi me"><img src="../Images/eef80311a3970ab8dabc76e62fac9c9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*LIiBrtIUa9PANrYz.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated"><a class="ae ko" href="https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/" rel="noopener ugc nofollow" target="_blank">资料来源:Exxact </a></p></figure><p id="2226" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">或者，您可以使用Databricks设置来练习Spark。这家公司是由Spark的最初创建者创建的，拥有一个优秀的现成环境，可以使用Spark进行分布式分析。</p><p id="7880" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但想法总是一样的。您在许多节点上以固定的小块分发(和复制)大型数据集。然后，您将计算引擎靠近它们，以便整个操作是并行的、容错的和可扩展的。</p><p id="50dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过使用PySpark和Jupyter notebook，您可以学习所有这些概念，而无需在AWS或Databricks平台上花费任何东西。您还可以轻松地与SparkSQL和MLlib进行接口，以进行数据库操作和机器学习。如果您事先已经理解了这些概念，那么开始处理现实生活中的大型集群将会容易得多！</p><h1 id="4b90" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">弹性分布式数据集(RDD)和SparkContext</h1><p id="bee4" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">许多Spark程序都围绕着弹性分布式数据集(RDD)的概念，这是一个可以并行操作的容错元素集合。SparkContext驻留在驱动程序中，通过集群管理器管理工作节点上的分布式数据。使用PySpark的好处是，所有这些复杂的数据分区和任务管理都在后台自动处理，程序员可以专注于特定的分析或机器学习工作本身。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi me"><img src="../Images/a86066c82961cab8eea9b526666dbf0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*CR4GHanRwNdYlHtZ.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated"><a class="ae ko" href="https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/" rel="noopener ugc nofollow" target="_blank">来源:Exxact </a></p></figure><p id="b2dc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="mf"> rdd-1 </em></p><p id="f6cf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">创建rdd有两种方法——在驱动程序中并行化现有集合，或者引用外部存储系统中的数据集，例如共享文件系统、HDFS、HBase或任何提供Hadoop InputFormat的数据源。</p><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="9c05" class="ml lc it mh b gy mm mn l mo mp">from pyspark import SparkContext import numpy as np sc=SparkContext(master="local[4]") lst=np.random.randint(0,10,20) A=sc.parallelize(lst)</span></pre><p id="e27d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="mf">注意参数</em>中的‘4’。它表示4个计算核心(在您的本地机器中)将用于这个SparkContext对象</strong>。如果我们检查RDD对象的类型，我们得到如下结果，</p><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="0b53" class="ml lc it mh b gy mm mn l mo mp">type(A) &gt;&gt; pyspark.rdd.RDD</span><span id="9ae3" class="ml lc it mh b gy mq mn l mo mp">A.collect() &gt;&gt; [4, 8, 2, 2, 4, 7, 0, 3, 3, 9, 2, 6, 0, 0, 1, 7, 5, 1, 9, 7]</span></pre><p id="8b81" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是A不再是简单的Numpy数组。我们可以使用glom()方法来检查分区是如何创建的。</p><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="0ab5" class="ml lc it mh b gy mm mn l mo mp">A.glom().collect() &gt;&gt; [[4, 8, 2, 2, 4], [7, 0, 3, 3, 9], [2, 6, 0, 0, 1], [7, 5, 1, 9, 7]]</span></pre><p id="3245" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在停止SC，用2个内核重新初始化它，看看重复这个过程会发生什么。</p><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="af98" class="ml lc it mh b gy mm mn l mo mp">sc.stop() sc=SparkContext(master="local[2]") A = sc.parallelize(lst) A.glom().collect() &gt;&gt; [[4, 8, 2, 2, 4, 7, 0, 3, 3, 9], [2, 6, 0, 0, 1, 7, 5, 1, 9, 7]]</span></pre><p id="f1e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">RDD现在分布在两个块上，而不是四个！您已经了解了分布式数据分析的第一步，即控制如何将您的数据划分为更小的块以供进一步处理</p><h1 id="c2d3" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">RDD &amp;派斯帕克基本操作的一些例子</h1><h1 id="f741" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数数元素</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="bc6a" class="ml lc it mh b gy mm mn l mo mp">&gt;&gt; 20</span><span id="fd0e" class="ml lc it mh b gy mq mn l mo mp">A.first() &gt;&gt; 4 A.take(3) &gt;&gt; [4, 8, 2]</span></pre><h1 id="c8c8" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">使用distinct删除重复项</h1><p id="7442" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated"><strong class="js iu">注意</strong>:这个操作需要一个<strong class="js iu">洗牌</strong>来检测跨分区的复制。所以，这是一个缓慢的操作。不要过度。</p><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="6e19" class="ml lc it mh b gy mm mn l mo mp">A_distinct=A.distinct() A_distinct.collect() &gt;&gt; [4, 8, 0, 9, 1, 5, 2, 6, 7, 3]</span></pre><h1 id="4209" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">使用reduce方法对所有元素求和</h1><p id="9502" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">注意这里使用了lambda函数，</p><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="2568" class="ml lc it mh b gy mm mn l mo mp">A.reduce(lambda x,y:x+y) &gt;&gt; 80</span><span id="f155" class="ml lc it mh b gy mq mn l mo mp">A.sum() &gt;&gt; 80</span></pre><h1 id="728c" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">通过归约寻找最大元素</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="d71e" class="ml lc it mh b gy mm mn l mo mp">A.reduce(lambda x,y: x if x &gt; y else y) &gt;&gt; 9</span></pre><h1 id="8471" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">在文本块中查找最长的单词</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="c5bb" class="ml lc it mh b gy mm mn l mo mp">words = 'These are some of the best Macintosh computers ever'.split(' ') wordRDD = sc.parallelize(words) wordRDD.reduce(lambda w,v: w if len(w)&gt;len(v) else v) &gt;&gt; 'computers'</span></pre><h1 id="5fbf" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">使用过滤器进行基于逻辑的过滤</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="934d" class="ml lc it mh b gy mm mn l mo mp"># Return RDD with elements (greater than zero) divisible by 3 A.filter(lambda x:x%3==0 and x!=0).collect() &gt;&gt; [3, 3, 9, 6, 9]</span></pre><h1 id="9375" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">编写与reduce()一起使用的常规Python函数</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="e449" class="ml lc it mh b gy mm mn l mo mp">def largerThan(x,y): """ Returns the last word among the longest words in a list """ if len(x)&gt; len(y): return x elif len(y) &gt; len(x): return y else: if x &lt; y: return x else: return y wordRDD.reduce(largerThan) &gt;&gt; 'Macintosh'</span></pre><p id="ea71" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意这里的x &lt; y does a lexicographic comparison and determines that Macintosh is larger than computers !</p><h1 id="ae8b" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">Mapping operation with a lambda function with PySpark</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="9fc3" class="ml lc it mh b gy mm mn l mo mp">B=A.map(lambda x:x*x) B.collect() &gt;&gt; [16, 64, 4, 4, 16, 49, 0, 9, 9, 81, 4, 36, 0, 0, 1, 49, 25, 1, 81, 49]</span></pre><h1 id="5cfa" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">Mapping with a regular Python function in PySpark</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="1b75" class="ml lc it mh b gy mm mn l mo mp">def square_if_odd(x): """ Squares if odd, otherwise keeps the argument unchanged """ if x%2==1: return x*x else: return x A.map(square_if_odd).collect() &gt;&gt; [4, 8, 2, 2, 4, 49, 0, 9, 9, 81, 2, 6, 0, 0, 1, 49, 25, 1, 81, 49]</span></pre><h1 id="319c" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">groupby returns a RDD of grouped elements (iterable) as per a given group operation</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="f6f3" class="ml lc it mh b gy mm mn l mo mp">result=A.groupBy(lambda x:x%2).collect() sorted([(x, sorted(y)) for (x, y) in result]) &gt;&gt; [(0, [0, 0, 0, 2, 2, 2, 4, 4, 6, 8]), (1, [1, 1, 3, 3, 5, 7, 7, 7, 9, 9])]</span><span id="a59a" class="ml lc it mh b gy mq mn l mo mp">B.histogram([x for x in range(0,100,10)]) &gt;&gt; ([0, 10, 20, 30, 40, 50, 60, 70, 80, 90], [10, 2, 1, 1, 3, 0, 1, 0, 2])</span></pre><h1 id="6f70" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">Set operations</h1><p id="d699" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">Check out <a class="ae ko" href="https://github.com/tirthajyoti/Spark-with-Python/blob/master/SparkContext%20and%20RDD%20Basics.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">这个Jupyter笔记本</strong> </a>有更多的例子。</p><p id="9137" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">惰性评估是一种评估/计算策略，它为计算任务准备了执行流水线的详细的逐步内部图，但是将最终执行延迟到绝对需要的时候。这一策略是Spark加速许多并行化大数据操作的核心。</p><p id="4c95" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们在这个例子中使用两个CPU内核，</p><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="5ac7" class="ml lc it mh b gy mm mn l mo mp">sc = SparkContext(master="local[2]")</span></pre><h1 id="9da3" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">用一百万个元素做一个RDD</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="9e85" class="ml lc it mh b gy mm mn l mo mp">%%time rdd1 = sc.parallelize(range(1000000)) &gt;&gt; CPU times: user 316 µs, sys: 5.13 ms, total: 5.45 ms, Wall time: 24.6 ms</span></pre><h1 id="5c05" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">一些计算功能—需要时间</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="e477" class="ml lc it mh b gy mm mn l mo mp">from math import cos def taketime(x): [cos(j) for j in range(100)] return cos(x)</span></pre><h1 id="12be" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">检查taketime函数花费了多少时间</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="2056" class="ml lc it mh b gy mm mn l mo mp">%%time taketime(2) &gt;&gt; CPU times: user 21 µs, sys: 7 µs, total: 28 µs, Wall time: 31.5 µs &gt;&gt; -0.4161468365471424</span></pre><h1 id="ab39" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">现在对函数进行映射操作</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="79c2" class="ml lc it mh b gy mm mn l mo mp">%%time interim = rdd1.map(lambda x: taketime(x)) &gt;&gt; CPU times: user 23 µs, sys: 8 µs, total: 31 µs, Wall time: 34.8 µs</span></pre><p id="6686" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="mf">为什么每个函数需要45.8 us，但一百万元素RDD的地图操作也需要类似的时间？</em></p><p id="6486" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">由于懒惰评估，即在之前的步骤中没有进行任何计算，只制定了一个执行计划</strong>。变量interim不指向数据结构，而是指向一个执行计划，用依赖图表示。依赖图定义了rdd如何相互计算。</p><h1 id="5e67" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">reduce方法的实际执行</h1><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="ef4a" class="ml lc it mh b gy mm mn l mo mp">%%time print('output =',interim.reduce(lambda x,y:x+y)) &gt;&gt; output = -0.28870546796843666 &gt;&gt; CPU times: user 11.6 ms, sys: 5.56 ms, total: 17.2 ms, Wall time: 15.6 s</span></pre><p id="f836" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们还没有保存(具体化)任何中间结果，所以另一个简单的操作(例如，计数元素&gt; 0)将花费几乎相同的时间。</p><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="2dc2" class="ml lc it mh b gy mm mn l mo mp">%%time print(interim.filter(lambda x:x&gt;0).count()) &gt;&gt; 500000 &gt;&gt; CPU times: user 10.6 ms, sys: 8.55 ms, total: 19.2 ms, Wall time: 12.1 s</span></pre><h1 id="7a6a" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">缓存以减少类似操作的计算时间(消耗内存)</h1><p id="6b7b" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">还记得我们在上一步中构建的依赖图吗？我们可以像以前一样使用cache方法运行相同的计算，告诉依赖图规划缓存。</p><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="e58c" class="ml lc it mh b gy mm mn l mo mp">%%time interim = rdd1.map(lambda x: taketime(x)).cache()</span></pre><p id="769c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一次计算不会改进，但是它缓存了中间结果，</p><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="11ac" class="ml lc it mh b gy mm mn l mo mp">%%time print('output =',interim.reduce(lambda x,y:x+y)) &gt;&gt; output = -0.28870546796843666 &gt;&gt; CPU times: user 16.4 ms, sys: 2.24 ms, total: 18.7 ms, Wall time: 15.3 s</span></pre><p id="96be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在在缓存结果的帮助下运行相同的过滤方法，</p><pre class="kq kr ks kt gt mg mh mi mj aw mk bi"><span id="01ac" class="ml lc it mh b gy mm mn l mo mp">%%time print(interim.filter(lambda x:x&gt;0).count()) &gt;&gt; 500000 &gt;&gt; CPU times: user 14.2 ms, sys: 3.27 ms, total: 17.4 ms, Wall time: 811 ms</span></pre><p id="55bb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">哇！计算时间从之前的12秒下降到不到1秒！这样，延迟执行的缓存和并行化就是Spark编程的核心特性。</p><h1 id="1f01" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">Dataframe和SparkSQL</h1><p id="3982" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">除了RDD，Spark框架中的第二个关键数据结构是。如果您使用过Python Pandas或R DataFrame，这个概念可能会很熟悉。</p><p id="466f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据帧是指定列下的行的分布式集合。它在概念上相当于关系数据库中的一个表、一个带有列标题的Excel表或R/Python中的一个数据框，但是在底层有更丰富的优化。数据帧可以从各种来源构建，例如:结构化数据文件、Hive中的表、外部数据库或现有的rdd。它还与RDD有一些共同的特征:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi me"><img src="../Images/1bd00c5f26e6ea5f0d2b9f0a737c7bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*tLuEP6XhI1kkKSx6.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated"><a class="ae ko" href="https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/" rel="noopener ugc nofollow" target="_blank">资料来源:Exxact </a></p></figure><ul class=""><li id="0b30" class="mr ms it js b jt ju jx jy kb mt kf mu kj mv kn mw mx my mz bi translated">本质上是不可变的:我们可以创建一次数据框架/ RDD，但不能改变它。并且我们可以在应用变换之后变换数据帧/ RDD。</li><li id="d343" class="mr ms it js b jt na jx nb kb nc kf nd kj ne kn mw mx my mz bi translated">惰性评估:这意味着直到一个动作被执行，一个任务才被执行。分布式:RDD和DataFrame都分布在自然界。</li></ul><h1 id="e61f" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据框架的优势</h1><ul class=""><li id="9bcc" class="mr ms it js b jt lz jx ma kb nf kf ng kj nh kn mw mx my mz bi translated">数据帧是为处理大量结构化或半结构化数据而设计的。</li><li id="efd6" class="mr ms it js b jt na jx nb kb nc kf nd kj ne kn mw mx my mz bi translated">Spark数据帧中的观察结果组织在命名列下，这有助于Apache Spark理解数据帧的模式。这有助于优化这些查询的执行计划。</li><li id="346d" class="mr ms it js b jt na jx nb kb nc kf nd kj ne kn mw mx my mz bi translated">Apache Spark中的DataFrame能够处理数Pb的数据。</li><li id="df5c" class="mr ms it js b jt na jx nb kb nc kf nd kj ne kn mw mx my mz bi translated">DataFrame支持多种数据格式和数据源。</li><li id="1171" class="mr ms it js b jt na jx nb kb nc kf nd kj ne kn mw mx my mz bi translated">它有对不同语言的API支持，比如Python，R，Scala，Java。</li></ul><h1 id="5886" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据框架基础示例</h1><p id="f62a" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated"><a class="ae ko" href="https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_basics.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">火花数据帧基础知识</strong> </a> <a class="ae ko" href="https://github.com/tirthajyoti/Spark-with-Python/blob/masterDataFrame_operations_basics.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">火花数据帧操作</strong> </a></p><p id="3b0c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有关数据帧的基础知识和典型使用示例，请参见以下Jupyter笔记本，</p><h1 id="c5dd" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">SparkSQL有助于弥补PySpark的不足</h1><p id="0306" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">关系数据存储易于构建和查询。用户和开发人员通常更喜欢用类似人类的可读语言(如SQL)编写易于解释的声明性查询。然而，随着数据的数量和种类开始增加，关系方法无法很好地扩展以构建大数据应用程序和分析系统。</p><p id="97e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">借助Hadoop和MapReduce范式，我们在大数据分析领域取得了成功。这很强大，但是通常很慢，并且给用户提供了一个低级的、<strong class="js iu">过程化的编程接口</strong>,要求人们为非常简单的数据转换编写大量代码。然而，一旦Spark发布，它就真正彻底改变了大数据分析的方式，专注于内存计算、容错、高级抽象和易用性。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/7d5ba5bd0b3cd4fdffa7d64e22739b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5NNLSgI9lhwVbyAL.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated"><a class="ae ko" href="https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/" rel="noopener ugc nofollow" target="_blank">来源:Exxact </a></p></figure><p id="ec3d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Spark SQL本质上试图在我们之前提到的两种模型——关系模型和过程模型——之间架起一座桥梁。Spark SQL通过DataFrame API工作，可以对外部数据源和Spark内置的分布式集合执行关系操作——大规模！</p><p id="b038" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为什么Spark SQL这么快，这么优化？原因是因为一个新的可扩展优化器，<strong class="js iu"> Catalyst </strong>，它基于Scala中的函数式编程结构。Catalyst支持基于规则和基于成本的优化。虽然在过去已经提出了可扩展的优化器，但是它们通常需要复杂的特定于领域的语言来指定规则。通常，这导致了很长的学习曲线和维护负担。相比之下，Catalyst使用Scala编程语言的标准特性，例如模式匹配，让开发人员可以使用完整的编程语言，同时仍然可以轻松地指定规则。</p><p id="d24c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_SQL_query.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> SparkSQL数据库操作基础知识</strong> </a></p><p id="2c27" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">关于SparkSQL数据库操作的介绍，可以参考下面的Jupyter笔记本:</p><h1 id="6aad" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">你将如何在你的项目中使用PySpark？</h1><p id="8b73" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">我们讲述了Apache Spark生态系统的基础知识，以及它是如何工作的，并给出了一些使用Python接口PySpark的核心数据结构RDD的基本例子。此外，还讨论了DataFrame和SparkSQL以及代码笔记示例的参考链接。</p><p id="1034" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将Apache Spark与Python结合使用，还有很多东西需要学习和尝试。PySpark网站是一个很好的参考网站，他们会定期更新和增强——所以请关注它。</p><p id="2cc0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，如果您对使用Apache Spark进行大规模分布式机器学习感兴趣，请查看PySpark生态系统的<a class="ae ko" href="https://spark.apache.org/mllib/" rel="noopener ugc nofollow" target="_blank"> MLLib部分。</a></p><p id="0c84" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="mf">原载于2020年4月7日</em><a class="ae ko" href="https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/" rel="noopener ugc nofollow" target="_blank"><em class="mf">【https://blog.exxactcorp.com】</em></a><em class="mf">。</em></p></div></div>    
</body>
</html>