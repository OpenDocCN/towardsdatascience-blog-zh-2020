<html>
<head>
<title>Forget the hassles of Anchor boxes with FCOS: Fully Convolutional One-Stage Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">忘记锚盒与FCOS的争论:完全卷积的一阶段目标检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/forget-the-hassles-of-anchor-boxes-with-fcos-fully-convolutional-one-stage-object-detection-fc0e25622e1c?source=collection_archive---------11-----------------------#2020-03-29">https://towardsdatascience.com/forget-the-hassles-of-anchor-boxes-with-fcos-fully-convolutional-one-stage-object-detection-fc0e25622e1c?source=collection_archive---------11-----------------------#2020-03-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/7f4d8e03b11e4531c994dded9660d74f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*foOy4FKuX7niXgLxuBPFFQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">信用:<a class="ae jd" href="https://arxiv.org/pdf/1904.01355.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS </a></p></figure><div class=""/><p id="435e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文详细解释了一种新的目标检测技术，该技术在ICCV 19年发表的论文<a class="ae jd" href="https://arxiv.org/pdf/1904.01355.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS </a>:全卷积一阶段目标检测中提出。我决定总结这篇论文，因为它提出了一种非常直观和简单的技术来解决对象检测问题。留下来了解它是如何工作的。</p><h1 id="f5b4" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">内容</h1><ol class=""><li id="e59f" class="lz ma jg kf b kg mb kk mc ko md ks me kw mf la mg mh mi mj bi translated">基于锚的检测器</li><li id="217d" class="lz ma jg kf b kg mk kk ml ko mm ks mn kw mo la mg mh mi mj bi translated">FCOS提出的想法</li><li id="81c3" class="lz ma jg kf b kg mk kk ml ko mm ks mn kw mo la mg mh mi mj bi translated">多级检测</li><li id="2adb" class="lz ma jg kf b kg mk kk ml ko mm ks mn kw mo la mg mh mi mj bi translated">FCOS的中心</li><li id="40e3" class="lz ma jg kf b kg mk kk ml ko mm ks mn kw mo la mg mh mi mj bi translated">基于锚的检测器的实验和比较</li><li id="1f58" class="lz ma jg kf b kg mk kk ml ko mm ks mn kw mo la mg mh mi mj bi translated">结论</li></ol><h1 id="568f" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">基于锚的检测器</h1><p id="b3a0" class="pw-post-body-paragraph kd ke jg kf b kg mb ki kj kk mc km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">我们现在使用的每一种著名的物体检测方法(Fast-RCNN，YOLOv3，SSD，RetinaNet等。)使用锚点。这些主播基本都是预定义的训练样本。它们有不同的比例，以方便不同种类的对象及其比例。然而，正如你从它们的定义中清楚理解的那样，使用锚涉及到许多超参数。例如，图像的每个部分的锚的数量、框的尺寸的比例、图像应该被分成的部分的数量。最重要的是，这些超参数影响最终结果，即使是最微小的变化。此外，哪个边界框被认为是负样本还是正样本由另一个称为并集上的交集(IoU)的超参数决定。IoU值极大地改变了哪些盒子将被考虑。下面是一个简单的图像，描述了在Yolov3中锚定框的使用:</p><figure class="mt mu mv mw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ms"><img src="../Images/debfbaff7617015fb3b15b043bda35a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*56UGqknHwtMc9RHRRobL8Q.png"/></div></div></figure><p id="3962" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们一直在使用这种方法，原因只有一个，那就是延续以前方法使用的理念。第一个物体检测器从经典计算机视觉的早期检测模型中借用了滑动窗口的概念。但是，既然我们拥有多个GPU的计算能力，就没有必要使用滑动窗口了。</p><h1 id="b430" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">FCOS:提议的想法</h1><p id="d1fc" class="pw-post-body-paragraph kd ke jg kf b kg mb ki kj kk mc km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">这就引出了一个问题，为什么还要使用锚点，为什么不像分割那样执行对象检测，即像素方式。这正是本文所要提出的。到目前为止，通过使用滑动窗口方法，在图像的逐像素值和检测的对象之间没有直接的联系。现在让我们正式看看这种方法是如何工作的。</p><p id="e04d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设<strong class="kf jh"> Fᵢ </strong>为总跨度<strong class="kf jh"> s </strong>的骨干CNN<strong class="kf jh">I</strong>层的<strong class="kf jh"> Fᵢ⁰ </strong>特征图。此外，我们将图像的地面实况包围盒定义为<strong class="kf jh"> Bᵢ = ( x⁰ᵢ，y⁰ᵢ，x ᵢ，y ᵢ，cᵢ ) ∈ R₄ × {1，2 … C} </strong>。这里是<strong class="kf jh"> C </strong>是班级人数。这里<strong class="kf jh"> (x⁰ᵢ，y⁰ᵢ)</strong><strong class="kf jh">(xᵢ，y ᵢ) </strong>分别表示左上角右下角。对于特征图上的每个位置<strong class="kf jh"> (x，y) </strong>，我们可以将其指向原始图像中的一个像素。这与我们在语义分割中所做的事情类似(尽管不完全相同)。我们将特征图上的<strong class="kf jh"> (x，y) </strong>映射到感受野中心附近的点<strong class="kf jh"> (floor(s/2) + x*s，floor(s/2) + y*s) </strong>。我鼓励用户使用大小为(8，8)的示例图像和大小为(4，4)的特征图来真正理解这种映射。在这种映射的帮助下，我们能够将图像中的每个像素作为训练样本进行关联。这意味着，每个位置<strong class="kf jh"> (x，y) </strong>可以是正样本或负样本之一，这取决于以下条件:它落在地面真实(GT from now)边界框中，并且为该位置计算的类标签是该GT边界框的类标签。</p><p id="a215" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们知道了GT边界框中的一个点，我们需要评估这个框的尺寸。这是通过对四个值<strong class="kf jh"> (l*、t*、r*、b*)的回归来完成的。</strong>它们被定义为:</p><p id="ca41" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">l * =x-x⁰ᵢ；t * =y-y⁰ᵢ；r * = x⁰ᵢ-x；b* = y⁰ᵢ-y </strong></p><p id="eb58" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，正如您将看到的，这些值的基于回归的计算是整个检测算法的损失函数的一部分。</p><p id="f7d7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，因为没有锚点，所以不需要计算锚点和GT边界框之间的IoU来获得可以训练回归器的正样本。相反，给出正样本的每个位置(通过在GT框内并具有正确的类)可以是边界框尺寸回归的一部分。这是FCOS比基于锚的检测器工作得更好的可能原因之一，即使在使用更少数量的参数之后。</p><p id="99c1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于特征图中的每个位置，我们计算分类分数，对于每个正样本位置，我们进行回归。因此，总损失函数变为:</p><figure class="mt mu mv mw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mx"><img src="../Images/22674f909b6b57dd2199e7ada0cc099b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7GaKBudnIzBSCuvDJCwYgA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">信用:<a class="ae jd" href="https://arxiv.org/pdf/1904.01355.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS </a></p></figure><p id="a578" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于本文，<strong class="kf jh"> λ </strong>的值取为1。</p><p id="686e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RHS的第一部分是位置分类<strong class="kf jh"> (x，y) </strong>。RetinaNet中使用的标准焦点loos也在这里使用。RHS的第二部分是回归包围盒。对于不是正样本的位置，它等于零。</p><figure class="mt mu mv mw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi my"><img src="../Images/7c7d72e3f1d1cd6c26e3846c1b67177a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tFNFTeeOysKvs_13r0Xm6A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">信用:<a class="ae jd" href="https://arxiv.org/pdf/1904.01355.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS </a></p></figure><h1 id="d2dd" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">用FCOS进行多层预测</h1><p id="4d8f" class="pw-post-body-paragraph kd ke jg kf b kg mb ki kj kk mc km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">多级预测基本上意味着使用不同级别的特征图进行对象检测。这类似于RetinaNet中使用的FPNs(特征金字塔网络)的概念。探测器头应用于几个级别的特征图，P3，P4，P5，P6，P7如上图所示。这有助于我们检测图像中不同大小的对象。这有助于解决重叠GT边界框的问题。如果两个不同大小的边界框相互重叠会怎样。这是用多级预测来处理的。与基于锚的检测器不同，基于锚的检测器将不同大小的锚框分配给不同的特征级别，从而在不同的级别分离不同大小的重叠GT边界框，像FCOS这样的无锚检测器通过在不同的特征地图级别限制回归来实现这一点。它们为所有要素级别(P3至P7)定义了一个值mᵢ，该值设置为0、64、128、256、512和无穷大，P2为0，因此没有要素级别。基本上是特征级别I需要回归的最大距离。例如，特征级别P7需要回归其中<strong class="kf jh"> max(l*，r*，t*，b*) &lt; infinity </strong>但是大于下一个m的值，即512的盒子。同样，它也适用于其他功能级别。此外，如果一个位置即使在多级检测后仍被分配给一个以上的GT箱，它将被自动分配给两个GT箱中较小的一个。</p><h1 id="18e7" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">FCOS的中心</h1><figure class="mt mu mv mw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mz"><img src="../Images/fcea53a882cd940444c78fea95598c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H2mXwukYZcmcTrYHFdWnCw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">信用:<a class="ae jd" href="https://arxiv.org/pdf/1904.01355.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS </a></p></figure><p id="3e64" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了两个传统的探测头，分类和回归，FCOS建议使用第三个头，称为中心头。顾名思义，它是对为其回归的边界框内的正样本位置的中心性的度量。这是为了提高无锚检测器的性能，并使它们与基于锚的检测器不相上下。从经验上来说，我们发现，就特征在盒内的位置而言，具有偏斜特征位置的低级回归盒会妨碍整体结果。因此，上面的项是使用BCE损失对每个回归的盒子进行计算的，因为它的范围是从0到1。这个损失被加到上面讨论的最终损失函数中。测试时，在对推断出的边界框进行排名之前，将该值乘以分类分数以获得最终分数。</p><h1 id="91aa" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">实验和与基于锚的最新技术的比较</h1><figure class="mt mu mv mw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/f4f20c5ce45e704c20c826bd1cd84797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZErSbauR6tYlLCOcimMJQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">信用:<a class="ae jd" href="https://arxiv.org/pdf/1904.01355.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS </a></p></figure><p id="f6cb" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上表显示了与RetinaNet的比较，retina net是一个最先进的基于锚点的模型。具有多级预测和中心分支的原始实现在其他参数方面优于RetinaNet，例如。两种型号的nms阈值设置相同。上表中提到的“改进”是在最初提交后增加的。它们包括以下内容:将中心性分支移动到回归分支而不是分类分支，控制采样，这基本上是添加基于超参数的去除坏样本以及中心性分支。其他补充请点击<a class="ae jd" href="https://github.com/yqyao/FCOS_PLUS" rel="noopener ugc nofollow" target="_blank">这个</a>链接。</p><figure class="mt mu mv mw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nb"><img src="../Images/da4fa88ba7ad7d4096a38f48cb60a0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y2d2rinGw5nZkmCgAGqbDg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">信用:<a class="ae jd" href="https://arxiv.org/pdf/1904.01355.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS </a></p></figure><p id="e1f4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上表很有趣，因为它强调了中心性分支带来的改进。基于锚的模型采用IoU阈值，同时对正负锚盒进行分类。中心性分支有助于消除这些超短波。</p><p id="eeb4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">需要注意的非常重要的一点是，重要的超参数如学习率、NMS抑制阈值等。直接取自RetinaNet。通过专门针对无锚模型进行更好的超参数调整，有可能获得更好的结果。</p><h1 id="d3ea" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="accb" class="pw-post-body-paragraph kd ke jg kf b kg mb ki kj kk mc km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">FCOS是一个伟大的想法，使对象检测问题更简单，更快。这是对远离基于锚的滑动窗口的目标检测思想的鼓励。这可以在解决这个棘手的计算机视觉问题上带来巨大的成果。</p><p id="16ea" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">这就是所有的乡亲😃</strong></p><p id="62d4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">你可以在我的</strong><a class="ae jd" href="https://github.com/vandit15" rel="noopener ugc nofollow" target="_blank"><strong class="kf jh">Github</strong></a><strong class="kf jh">上看更多深度学习相关的东西，关注我的</strong><a class="ae jd" href="https://www.linkedin.com/in/vandit-jain15/" rel="noopener ugc nofollow" target="_blank"><strong class="kf jh">Linkedin</strong></a><strong class="kf jh">。</strong></p><p id="26d1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">我之前的一些文章:</strong></p><div class="ip iq gp gr ir nc"><a rel="noopener follow" target="_blank" href="/everything-you-need-to-know-about-auto-deeplab-googles-latest-on-segmentation-181425d17cd5"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd jh gy z fp nh fr fs ni fu fw jf bi translated">关于Auto-Deeplab你需要知道的一切:谷歌关于细分的最新消息</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">搜索图像分割模型</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">towardsdatascience.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq ix nc"/></div></div></a></div><div class="ip iq gp gr ir nc"><a rel="noopener follow" target="_blank" href="/everything-you-need-to-know-about-mobilenetv3-and-its-comparison-with-previous-versions-a5d5e5a6eeaa"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd jh gy z fp nh fr fs ni fu fw jf bi translated">关于MobileNetV3及其与以前版本的比较，您需要了解的一切</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">论文综述:寻找MobilenetV3，ICCV 19</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">towardsdatascience.com</p></div></div><div class="nl l"><div class="nr l nn no np nl nq ix nc"/></div></div></a></div><div class="ip iq gp gr ir nc"><a rel="noopener follow" target="_blank" href="/self-supervised-gans-using-auxiliary-rotation-loss-60d8a929b556"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd jh gy z fp nh fr fs ni fu fw jf bi translated">使用辅助旋转损耗的自监督GANs</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">弥合有监督和无监督图像生成之间的差距</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">towardsdatascience.com</p></div></div><div class="nl l"><div class="ns l nn no np nl nq ix nc"/></div></div></a></div><div class="ip iq gp gr ir nc"><a rel="noopener follow" target="_blank" href="/handling-class-imbalanced-data-using-a-loss-specifically-made-for-it-6e58fd65ffab"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd jh gy z fp nh fr fs ni fu fw jf bi translated">使用专门为其制作的损失来处理类不平衡数据</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">通过添加大约10行代码，在严重的类不平衡数据上获得超过4%的准确性提升。</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">towardsdatascience.com</p></div></div><div class="nl l"><div class="nt l nn no np nl nq ix nc"/></div></div></a></div></div></div>    
</body>
</html>