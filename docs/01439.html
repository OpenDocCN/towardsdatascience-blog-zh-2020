<html>
<head>
<title>MiCT-Net for Human Action Recognition in Videos</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于视频中人体动作识别的MiCT-Net</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mict-net-for-human-action-recognition-in-videos-3a18e4f97342?source=collection_archive---------31-----------------------#2020-02-08">https://towardsdatascience.com/mict-net-for-human-action-recognition-in-videos-3a18e4f97342?source=collection_archive---------31-----------------------#2020-02-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0b9c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用跨域剩余连接混合3D和2D卷积</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3a980395cc79113555da9f5ef99c35d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wbb8NxbZgqwGAHvviyKozg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾莉娜·格鲁布尼亚克在<a class="ae ky" href="https://unsplash.com/s/photos/neural-network?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="763a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近，微软的一组研究人员发表了一篇论文[1]，介绍了一种混合3D/2D卷积神经网络架构，用于视频中的人体动作识别。该论文报告了在UCF-101和HMDB-51数据集上的最新性能，同时通过使用比以前工作少一半的3D卷积降低了模型复杂性。</p><p id="d260" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者观察到，3D ConvNets在这项任务中表现出令人失望的性能，因为它们很难训练，并且它们的内存要求限制了它们的深度。他们的架构被称为混合卷积管道网络或MiCT-Net，解决了将2D-CNN主干网的效率与关键位置引入的额外3D残差卷积相结合的想法，以生成更深入、更丰富的特征地图。</p><p id="e62e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者的源代码是不公开的，这篇文章是我试图复制他们的一些工作的结果，这使我使用ResNet主干在PyTorch上实现了MiCT-Net，并将其命名为MiCT-ResNet。我的代码可以在这个<a class="ae ky" href="https://github.com/fmahoudeau/MiCT-Net-PyTorch" rel="noopener ugc nofollow" target="_blank">库</a>上获得，并且可以在你自己的项目中免费使用。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="b26d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">混合三维/2D卷积管</h1><p id="c93d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">MiCT模块是基于2D和3D卷积可以相互补充以提高整体网络性能的观察而构建的。</p><p id="7856" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一方面，2D·康文内特可以学习深度空间表示，但却无法从视频中学习时间结构，而这种时间结构是区分相似类别所必需的。</p><p id="a735" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，3D ConvNets在提取时空特征方面很有效，但很难堆叠成深度网络，因为解空间的指数增长使得它们很难优化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/22061fcbb53c2c9cdc022689534c9517.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fhwEXKVnmKDszMnw0ZJ4SA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从[1]中借用的MiCT块的图示，它利用跳过连接来组合3D和2D卷积。</p></figure><p id="930e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者的想法是通过将有限数量的3D卷积层与2D-CNN主干混合，结合两个世界的最佳之处。如上所述，3D和2D卷积以两种方式混合:</p><p id="90ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在2D卷积的输入和输出之间添加了3D卷积。这种3D卷积在时间级别上增加了另一个级别的特征学习。这两个分支的特征然后用跨域元素式求和来合并。该操作降低了时空融合的复杂性，因为3D卷积分支仅学习剩余的时间特征，即视频中物体和人的运动，而空间特征通过2D卷积学习。</p><p id="9a28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，在求和之后附加2D卷积，以在每一轮时空融合期间提取更深的特征。</p><h1 id="a8db" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated"><strong class="ak"> MiCT块实现</strong></h1><p id="ed1c" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">网络输入是表示为大小为<strong class="lb iu"> NxCxDxHxW </strong>的5维张量的小批量视频剪辑，其中<strong class="lb iu"> N </strong>表示小批量大小，<strong class="lb iu"> C </strong>频道数量，<strong class="lb iu"> D </strong>剪辑持续时间，<strong class="lb iu"> H </strong>和<strong class="lb iu"> W </strong>分别表示空间域中的高度和宽度。</p><p id="da53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当3D和2D卷积对不同大小的张量进行运算时，第一个问题立即出现:3D卷积需要5D输入张量，而2D卷积需要4D输入张量。同样，3D和2D卷积的输出不能直接相加。</p><p id="deb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们需要在跳过连接和融合操作之前，例如，从5D到4D来回变换张量。幸运的是，5D视频张量只不过是图像序列的小批量，可以堆叠在一起形成一个更大的4D小批量，大小为<strong class="lb iu"> (NxD)xCxHxW </strong>。</p><p id="78e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> _to_4d_tensor </strong>函数实现了这种变换，并通过<strong class="lb iu"> depth_stride </strong>参数指定了可选的时间下采样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/e8edde083b468eaa48ad603af8938581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Why_qYzEIADAs6bBk-5EIg.png"/></div></div></figure><p id="72ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> _to_5d_tensor </strong>函数执行逆变换，采用4D输入张量和指定要恢复的序列长度的深度参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/fa0112d6880f9370e336a44575dbc575.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uzlj7Zf-7sRU3jMJ_wPpgg.png"/></div></div></figure><p id="b94b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经准备好使用下一个代码片段来研究MiCT块的向前传递。第一部分在填充输入张量之后执行3D卷积，以使3D卷积处理当前帧和接下来的几帧。接下来，我们执行第一次2D卷积，并将结果与5D空间中的3D卷积的输出相融合。最后，我们执行第二次2D卷积，并将结果作为5D张量返回，以备下一个MiCT块处理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/2533084b0377e201f0ea55fb19d812c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jrc7nQ8oFYAdoNmYdA9FLQ.png"/></div></div></figure><h1 id="6bcf" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">MiCT-ResNet架构</h1><p id="d8fa" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">这篇论文使用了一个从《盗梦空间》中得到灵感的自定义主干。它由4个MiCT块组成，每个块包含几个初始块。我选择使用ResNet backbone，以便能够与3D-ResNet比较结果，并受益于ImageNet上预先训练的权重。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/5b785d321b9cb21030b1f90a27dd08f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D-mUwkwwTx0z3jQJHYDSYw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MiCT-ResNet-18架构本质上是一个增加了五个3D剩余卷积的ResNet-18。</p></figure><p id="ee6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上面使用最浅的ResNet主干ResNet-18所示，该架构使用五个3D卷积，一个在网络入口处，一个在四个主要ResNet块的开始处。BasicBlock是标准的ResNet块。为了清楚起见，省略了每个卷积之后的批标准化和ReLU层。</p><p id="0251" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您不熟悉PyTorch中的ResNet实现，这篇<a class="ae ky" href="https://medium.com/@erikgaas/resnet-torchvision-bottlenecks-and-layers-not-as-they-seem-145620f93096" rel="noopener">文章</a>提供了一步一步的引导，让您快速上手。</p><h1 id="6cc3" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">UCF-101数据集</h1><p id="9f4f" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><a class="ae ky" href="https://www.crcv.ucf.edu/data/UCF101.php" rel="noopener ugc nofollow" target="_blank">UCF-101</a>【3】是著名的逼真动作视频的动作识别数据集，收集自YouTube，有101个动作类别。所有视频的大小都是320x240，每秒25帧。</p><p id="efaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的一点是，尽管每个类别有13320个视频和100多个剪辑，但对于这项任务来说，这是一个相对较小的数据集，容易过度拟合。所有的剪辑都取自只有2.5k的清晰视频。例如，同一个人弹钢琴的一个视频被剪切成7个剪辑。这意味着与每个片段中的动作由不同的人在不同的闪电条件下执行相比，差异要小得多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/541a810e6b461d63caec2fd2e927bfb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YG2AAXeG6j8v_mHAYGkNrA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从[3]中借用了15个人类动作的示例帧。</p></figure><h1 id="4345" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">实验环境</h1><p id="01a7" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">实验的目标是在训练数据有限的情况下比较MiCT-ResNet和3D-ResNet的性能。大规模数据集(如动力学)的预训练不在范围之内。</p><p id="7ce0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了便于结果比较，两个网络都基于ResNet-18主干网，时间跨度为16，空间跨度为32。权重由ImageNet预训练权重初始化。对于3D-ResNet，通过沿时间维度重复N次2D滤波器的权重来引导3D滤波器，并通过除以N来重新缩放它们</p><p id="82c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了支持每批大量视频剪辑的训练，模型的输入大小设置为160x160。沿着时间维度对每个视频进行随机下采样，并且随机选择一组16个连续帧。该序列根据需要循环以获得16个帧剪辑。在测试时，选择视频的前16帧。</p><p id="6827" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SGD优化器的学习率为1e-2，批处理大小为128。应用权重衰减、丢失和数据增加来减少过拟合。关于培训程序的更多细节可以在<a class="ae ky" href="https://github.com/fmahoudeau/MiCT-Net-PyTorch" rel="noopener ugc nofollow" target="_blank">资源库</a>中找到。</p><h1 id="5377" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">结果</h1><p id="008e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">根据标准的前1名和前5名精度对模型进行评估。所有结果是UCF-101的三个标准分裂的平均值。<strong class="lb iu"> MiCT-ResNet-18领先1.5个点，同时快了3.1倍</strong>，这证实了作者的方法的有效性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/c352304743c1346a0d4e9b1fb4743fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3QASl6AMQeRjSo-f_nwqbA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">性能对比。内存大小是针对一个批次大小给出的。</p></figure><p id="7545" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第二个实验中，MiCT-ResNet的时间跨度从16减少到4，并且在不同数量的剪辑长度上测试该网络。对于300帧的序列，实现了MiCT-ResNet-18的<strong class="lb iu"> 69.3 Top-1(交叉验证)精度</strong>和MiCT-ResNet-34的<strong class="lb iu"> 72.8 Top-1(交叉验证)精度</strong>的最佳结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/74063daf30b2c6c618820f2b70d1da3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*urQB9Jq_oP8MW3MIBl9G8A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MiCT-ResNet-18和MiCT-ResNet-34验证精度与视频长度的函数关系。</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="9a03" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">最后</h1><p id="237d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们看到，在UCF-101数据集上，与深度3D卷积网络相比，混合3D和2D卷积是提高性能的好策略。MiCT-ResNet提供了更高的准确性和更快的推理速度。</p><p id="985e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，不可能对这两种架构的相对性能给出明确的结论。与微软在MiCT-Net上的工作平行，谷歌DeepMind的另一个团队已经表明[2]在像Kinetics这样的非常大的视频数据集上预先训练3D-ConvNets大大提高了它们在像UCF-101这样的迁移学习任务上的性能。</p><p id="f189" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，如果这两种架构都在ImageNet和Kinetics上进行了预训练，那么如何比较这两种架构仍是一个未决问题。让我知道，如果你有访问动力学数据集，并愿意提供答案！</p><p id="ab67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><p id="fd3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]周，孙，查志军，曾文伟.MiCT:用于人体动作识别的混合3D/2D卷积管，2018年6月。</p><p id="0105" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] J .卡雷拉和a .齐塞曼。Quo Vadis，动作识别？新模型和动力学数据集，2018年2月。</p><p id="aef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] K. Soomro，A. Roshan Zamir和M. Shah，<a class="ae ky" href="https://www.crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf" rel="noopener ugc nofollow" target="_blank"> UCF101:来自野外视频的101个人类行动类别的数据集</a>，CRCV-TR-12–01，2012年11月。</p></div></div>    
</body>
</html>