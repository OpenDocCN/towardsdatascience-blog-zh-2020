<html>
<head>
<title>MiCT-Net for Human Action Recognition in Videos</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于视频中人体动作识别的 MiCT-Net</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mict-net-for-human-action-recognition-in-videos-3a18e4f97342?source=collection_archive---------31-----------------------#2020-02-08">https://towardsdatascience.com/mict-net-for-human-action-recognition-in-videos-3a18e4f97342?source=collection_archive---------31-----------------------#2020-02-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0b9c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用跨域剩余连接混合 3D 和 2D 卷积</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3a980395cc79113555da9f5ef99c35d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wbb8NxbZgqwGAHvviyKozg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾莉娜·格鲁布尼亚克在<a class="ae ky" href="https://unsplash.com/s/photos/neural-network?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="763a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近，微软的一组研究人员发表了一篇论文[1]，介绍了一种混合 3D/2D 卷积神经网络架构，用于视频中的人体动作识别。该论文报告了在 UCF-101 和 HMDB-51 数据集上的最新性能，同时通过使用比以前工作少一半的 3D 卷积降低了模型复杂性。</p><p id="d260" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者观察到，3D ConvNets 在这项任务中表现出令人失望的性能，因为它们很难训练，并且它们的内存要求限制了它们的深度。他们的架构被称为混合卷积管道网络或 MiCT-Net，解决了将 2D-CNN 主干网的效率与关键位置引入的额外 3D 残差卷积相结合的想法，以生成更深入、更丰富的特征地图。</p><p id="e62e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者的源代码是不公开的，这篇文章是我试图复制他们的一些工作的结果，这使我使用 ResNet 主干在 PyTorch 上实现了 MiCT-Net，并将其命名为 MiCT-ResNet。我的代码可以在这个<a class="ae ky" href="https://github.com/fmahoudeau/MiCT-Net-PyTorch" rel="noopener ugc nofollow" target="_blank">库</a>上获得，并且可以在你自己的项目中免费使用。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="b26d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">混合三维/2D 卷积管</h1><p id="c93d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">MiCT 模块是基于 2D 和 3D 卷积可以相互补充以提高整体网络性能的观察而构建的。</p><p id="7856" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一方面，2D·康文内特可以学习深度空间表示，但却无法从视频中学习时间结构，而这种时间结构是区分相似类别所必需的。</p><p id="a735" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，3D ConvNets 在提取时空特征方面很有效，但很难堆叠成深度网络，因为解空间的指数增长使得它们很难优化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/22061fcbb53c2c9cdc022689534c9517.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fhwEXKVnmKDszMnw0ZJ4SA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从[1]中借用的 MiCT 块的图示，它利用跳过连接来组合 3D 和 2D 卷积。</p></figure><p id="930e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者的想法是通过将有限数量的 3D 卷积层与 2D-CNN 主干混合，结合两个世界的最佳之处。如上所述，3D 和 2D 卷积以两种方式混合:</p><p id="90ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 2D 卷积的输入和输出之间添加了 3D 卷积。这种 3D 卷积在时间级别上增加了另一个级别的特征学习。这两个分支的特征然后用跨域元素式求和来合并。该操作降低了时空融合的复杂性，因为 3D 卷积分支仅学习剩余的时间特征，即视频中物体和人的运动，而空间特征通过 2D 卷积学习。</p><p id="9a28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，在求和之后附加 2D 卷积，以在每一轮时空融合期间提取更深的特征。</p><h1 id="a8db" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated"><strong class="ak"> MiCT 块实现</strong></h1><p id="ed1c" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">网络输入是表示为大小为<strong class="lb iu"> NxCxDxHxW </strong>的 5 维张量的小批量视频剪辑，其中<strong class="lb iu"> N </strong>表示小批量大小，<strong class="lb iu"> C </strong>频道数量，<strong class="lb iu"> D </strong>剪辑持续时间，<strong class="lb iu"> H </strong>和<strong class="lb iu"> W </strong>分别表示空间域中的高度和宽度。</p><p id="da53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当 3D 和 2D 卷积对不同大小的张量进行运算时，第一个问题立即出现:3D 卷积需要 5D 输入张量，而 2D 卷积需要 4D 输入张量。同样，3D 和 2D 卷积的输出不能直接相加。</p><p id="deb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们需要在跳过连接和融合操作之前，例如，从 5D 到 4D 来回变换张量。幸运的是，5D 视频张量只不过是图像序列的小批量，可以堆叠在一起形成一个更大的 4D 小批量，大小为<strong class="lb iu"> (NxD)xCxHxW </strong>。</p><p id="78e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> _to_4d_tensor </strong>函数实现了这种变换，并通过<strong class="lb iu"> depth_stride </strong>参数指定了可选的时间下采样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/e8edde083b468eaa48ad603af8938581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Why_qYzEIADAs6bBk-5EIg.png"/></div></div></figure><p id="72ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> _to_5d_tensor </strong>函数执行逆变换，采用 4D 输入张量和指定要恢复的序列长度的深度参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/fa0112d6880f9370e336a44575dbc575.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uzlj7Zf-7sRU3jMJ_wPpgg.png"/></div></div></figure><p id="b94b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经准备好使用下一个代码片段来研究 MiCT 块的向前传递。第一部分在填充输入张量之后执行 3D 卷积，以使 3D 卷积处理当前帧和接下来的几帧。接下来，我们执行第一次 2D 卷积，并将结果与 5D 空间中的 3D 卷积的输出相融合。最后，我们执行第二次 2D 卷积，并将结果作为 5D 张量返回，以备下一个 MiCT 块处理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/2533084b0377e201f0ea55fb19d812c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jrc7nQ8oFYAdoNmYdA9FLQ.png"/></div></div></figure><h1 id="6bcf" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">MiCT-ResNet 架构</h1><p id="d8fa" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">这篇论文使用了一个从《盗梦空间》中得到灵感的自定义主干。它由 4 个 MiCT 块组成，每个块包含几个初始块。我选择使用 ResNet backbone，以便能够与 3D-ResNet 比较结果，并受益于 ImageNet 上预先训练的权重。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/5b785d321b9cb21030b1f90a27dd08f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D-mUwkwwTx0z3jQJHYDSYw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MiCT-ResNet-18 架构本质上是一个增加了五个 3D 剩余卷积的 ResNet-18。</p></figure><p id="ee6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上面使用最浅的 ResNet 主干 ResNet-18 所示，该架构使用五个 3D 卷积，一个在网络入口处，一个在四个主要 ResNet 块的开始处。BasicBlock 是标准的 ResNet 块。为了清楚起见，省略了每个卷积之后的批标准化和 ReLU 层。</p><p id="0251" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您不熟悉 PyTorch 中的 ResNet 实现，这篇<a class="ae ky" href="https://medium.com/@erikgaas/resnet-torchvision-bottlenecks-and-layers-not-as-they-seem-145620f93096" rel="noopener">文章</a>提供了一步一步的引导，让您快速上手。</p><h1 id="6cc3" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">UCF-101 数据集</h1><p id="9f4f" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><a class="ae ky" href="https://www.crcv.ucf.edu/data/UCF101.php" rel="noopener ugc nofollow" target="_blank">UCF-101</a>【3】是著名的逼真动作视频的动作识别数据集，收集自 YouTube，有 101 个动作类别。所有视频的大小都是 320x240，每秒 25 帧。</p><p id="efaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的一点是，尽管每个类别有 13320 个视频和 100 多个剪辑，但对于这项任务来说，这是一个相对较小的数据集，容易过度拟合。所有的剪辑都取自只有 2.5k 的清晰视频。例如，同一个人弹钢琴的一个视频被剪切成 7 个剪辑。这意味着与每个片段中的动作由不同的人在不同的闪电条件下执行相比，差异要小得多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/541a810e6b461d63caec2fd2e927bfb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YG2AAXeG6j8v_mHAYGkNrA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从[3]中借用了 15 个人类动作的示例帧。</p></figure><h1 id="4345" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">实验环境</h1><p id="01a7" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">实验的目标是在训练数据有限的情况下比较 MiCT-ResNet 和 3D-ResNet 的性能。大规模数据集(如动力学)的预训练不在范围之内。</p><p id="7ce0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了便于结果比较，两个网络都基于 ResNet-18 主干网，时间跨度为 16，空间跨度为 32。权重由 ImageNet 预训练权重初始化。对于 3D-ResNet，通过沿时间维度重复 N 次 2D 滤波器的权重来引导 3D 滤波器，并通过除以 N 来重新缩放它们</p><p id="82c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了支持每批大量视频剪辑的训练，模型的输入大小设置为 160x160。沿着时间维度对每个视频进行随机下采样，并且随机选择一组 16 个连续帧。该序列根据需要循环以获得 16 个帧剪辑。在测试时，选择视频的前 16 帧。</p><p id="6827" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SGD 优化器的学习率为 1e-2，批处理大小为 128。应用权重衰减、丢失和数据增加来减少过拟合。关于培训程序的更多细节可以在<a class="ae ky" href="https://github.com/fmahoudeau/MiCT-Net-PyTorch" rel="noopener ugc nofollow" target="_blank">资源库</a>中找到。</p><h1 id="5377" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">结果</h1><p id="008e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">根据标准的前 1 名和前 5 名精度对模型进行评估。所有结果是 UCF-101 的三个标准分裂的平均值。<strong class="lb iu"> MiCT-ResNet-18 领先 1.5 个点，同时快了 3.1 倍</strong>，这证实了作者的方法的有效性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/c352304743c1346a0d4e9b1fb4743fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3QASl6AMQeRjSo-f_nwqbA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">性能对比。内存大小是针对一个批次大小给出的。</p></figure><p id="7545" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第二个实验中，MiCT-ResNet 的时间跨度从 16 减少到 4，并且在不同数量的剪辑长度上测试该网络。对于 300 帧的序列，实现了 MiCT-ResNet-18 的<strong class="lb iu"> 69.3 Top-1(交叉验证)精度</strong>和 MiCT-ResNet-34 的<strong class="lb iu"> 72.8 Top-1(交叉验证)精度</strong>的最佳结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/74063daf30b2c6c618820f2b70d1da3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*urQB9Jq_oP8MW3MIBl9G8A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MiCT-ResNet-18 和 MiCT-ResNet-34 验证精度与视频长度的函数关系。</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="9a03" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">最后</h1><p id="237d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们看到，在 UCF-101 数据集上，与深度 3D 卷积网络相比，混合 3D 和 2D 卷积是提高性能的好策略。MiCT-ResNet 提供了更高的准确性和更快的推理速度。</p><p id="985e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，不可能对这两种架构的相对性能给出明确的结论。与微软在 MiCT-Net 上的工作平行，谷歌 DeepMind 的另一个团队已经表明[2]在像 Kinetics 这样的非常大的视频数据集上预先训练 3D-ConvNets 大大提高了它们在像 UCF-101 这样的迁移学习任务上的性能。</p><p id="f189" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，如果这两种架构都在 ImageNet 和 Kinetics 上进行了预训练，那么如何比较这两种架构仍是一个未决问题。让我知道，如果你有访问动力学数据集，并愿意提供答案！</p><p id="ab67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><p id="fd3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]周，孙，查志军，曾文伟.MiCT:用于人体动作识别的混合 3D/2D 卷积管，2018 年 6 月。</p><p id="0105" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] J .卡雷拉和 a .齐塞曼。Quo Vadis，动作识别？新模型和动力学数据集，2018 年 2 月。</p><p id="aef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] K. Soomro，A. Roshan Zamir 和 M. Shah，<a class="ae ky" href="https://www.crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf" rel="noopener ugc nofollow" target="_blank"> UCF101:来自野外视频的 101 个人类行动类别的数据集</a>，CRCV-TR-12–01，2012 年 11 月。</p></div></div>    
</body>
</html>