<html>
<head>
<title>Building a Text Normalizer using NLTK ft. POS tagger</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 NLTK ft 构建文本规范化器。POS 标签</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-text-normalizer-using-nltk-ft-pos-tagger-e713e611db8?source=collection_archive---------19-----------------------#2020-03-19">https://towardsdatascience.com/building-a-text-normalizer-using-nltk-ft-pos-tagger-e713e611db8?source=collection_archive---------19-----------------------#2020-03-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="d0b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">标记化和文本规范化是自然语言处理技术的两个最基本的步骤。文本规范化是将单词转换成其词根或基本形式以标准化文本表示的技术。请注意，基本形式不一定是有意义的词，我们将在本博客结束时看到这一点。这些基本形式被称为词干，这个过程被称为<strong class="js iu">词干</strong>。一种专门的获取词干的方法叫做<strong class="js iu">词汇化</strong>，它根据单词所属的词类(POS)家族使用规则。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/2251f3023358fd743b21460b9ba536ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lkfhya7nAKqP-U3Q"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@ilumire?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jelleke Vanooteghem </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="7564" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><strong class="ak">为什么文本规范化很重要？</strong></h1><p id="0627" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">文本规范化对于信息检索(IR)系统、数据或文本挖掘应用以及 NLP 管道是至关重要的。为了促进更快和更有效的信息检索系统，索引和搜索算法需要不同的单词形式——派生的<a class="ae le" href="https://en.wikipedia.org/wiki/Morphological_derivation" rel="noopener ugc nofollow" target="_blank">或屈折的</a><a class="ae le" href="https://en.wikipedia.org/wiki/Inflection" rel="noopener ugc nofollow" target="_blank">——简化为它们的规范化形式。此外，它还有助于节省磁盘空间和语际文本匹配。</a></p><h1 id="ed1e" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">词干化与词汇化</h1><p id="c1d8" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">推导<em class="mi">词条</em>的过程处理单词所属的语义、形态和词性(POS ),而<em class="mi">词干</em>指的是一种粗略的启发式过程，它砍掉单词的词尾，希望在大多数时候都能正确实现这一目标，并且通常包括去除派生词缀。这就是为什么在大多数应用程序中，词汇化比词干化有更好的性能，尽管缩小到一个决策需要更大的视野。</p><p id="0495" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 Python 的 NLTK 库中，<a class="ae le" href="https://www.nltk.org/api/nltk.stem.html" rel="noopener ugc nofollow" target="_blank"> nltk.stem </a>包同时实现了词干分析器和词汇分析器。</p><blockquote class="mj"><p id="f6d2" class="mk ml it bd mm mn mo mp mq mr ms kn dk translated">够了！现在让我们深入研究一些文本规范化代码…</p><p id="6a18" class="mk ml it bd mm mn mo mp mq mr ms kn dk translated">会出什么问题呢？？！</p></blockquote><p id="2113" class="pw-post-body-paragraph jq jr it js b jt mt jv jw jx mu jz ka kb mv kd ke kf mw kh ki kj mx kl km kn im bi translated"><em class="mi">注:</em> <a class="ae le" href="https://github.com/royn5618/Medium_Blog_Codes/blob/master/Text_Normalization_ft_POS_Tagger.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="mi">这里的</em> </a> <em class="mi">是 GitHub 上完整的 jupyter 笔记本。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi my"><img src="../Images/f69d7a2dab68a8700e2e3033eff71507.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3DE5a0ihxj3d5J74"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae le" href="https://unsplash.com/@max_duz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Max Duzij </a>拍照</p></figure><p id="4f42" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第一步:标记化</strong></p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="74bb" class="ne lg it na b gy nf ng l nh ni"><strong class="na iu">import</strong> nltk<br/><strong class="na iu">import</strong> string</span><span id="ca65" class="ne lg it na b gy nj ng l nh ni">hermione_said = '''Books! And cleverness! There are more important things - friendship and bravery and - oh Harry - be careful!'''</span><span id="1a5f" class="ne lg it na b gy nj ng l nh ni"><em class="mi">## Tokenization</em></span><span id="4e77" class="ne lg it na b gy nj ng l nh ni"><strong class="na iu">from</strong> nltk <strong class="na iu">import</strong> sent_tokenize, word_tokenize<br/>sequences = sent_tokenize(hermione_said)<br/>seq_tokens = [word_tokenize(seq) for seq in sequences]</span><span id="ac06" class="ne lg it na b gy nj ng l nh ni"><em class="mi">## Remove punctuation</em></span><span id="15ab" class="ne lg it na b gy nj ng l nh ni">no_punct_seq_tokens = []<br/><strong class="na iu">for</strong> seq_token <strong class="na iu">in</strong> seq_tokens:<br/>    no_punct_seq_tokens.append([token for token in seq_token if token not in string.punctuation])</span><span id="18b5" class="ne lg it na b gy nj ng l nh ni">print(no_punct_seq_tokens)</span><span id="2884" class="ne lg it na b gy nj ng l nh ni"><strong class="na iu">Output:</strong><br/>[['Books'],<br/> ['And', 'cleverness'],<br/> ['There',<br/>  'are',<br/>  'more',<br/>  'important',<br/>  'things',<br/>  'friendship',<br/>  'and',<br/>  'bravery',<br/>  'and',<br/>  'oh',<br/>  'Harry',<br/>  'be',<br/>  'careful']]</span></pre><blockquote class="mj"><p id="dddb" class="mk ml it bd mm mn nk nl nm nn no kn dk translated">太棒了。</p></blockquote><p id="7e61" class="pw-post-body-paragraph jq jr it js b jt mt jv jw jx mu jz ka kb mv kd ke kf mw kh ki kj mx kl km kn im bi translated"><strong class="js iu">第二步:标准化技术</strong></p><p id="b06a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们从词干开始:</p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="ea2e" class="ne lg it na b gy nf ng l nh ni"># Using Porter Stemmer implementation in nltk<br/><strong class="na iu">from</strong> nltk.stem <strong class="na iu">import</strong> PorterStemmer<br/>stemmer = PorterStemmer()<br/>stemmed_tokens = [stemmer.stem(token) <strong class="na iu">for</strong> seq <strong class="na iu">in</strong> no_punct_seq_tokens <strong class="na iu">for</strong> token <strong class="na iu">in</strong> seq]<br/>print(stemmed_tokens)</span><span id="7ef1" class="ne lg it na b gy nj ng l nh ni"><strong class="na iu">Output:</strong><br/>['book',<br/> 'and',<br/> 'clever',<br/> 'there',<br/> 'are',<br/> 'more',<br/> '<strong class="na iu">import</strong>',<br/> 'thing',<br/> 'friendship',<br/> 'and',<br/> '<strong class="na iu">braveri</strong>',<br/> 'and',<br/> 'oh',<br/> '<strong class="na iu">harri</strong>',<br/> 'be',<br/> 'care']</span></pre><blockquote class="mj"><p id="903f" class="mk ml it bd mm mn nk nl nm nn no kn dk translated"><strong class="ak"> <em class="np">于是我们有了第一个问题——</em></strong>在 16 个令牌中，高亮显示的 3 个并不好看！</p></blockquote><p id="a0e4" class="pw-post-body-paragraph jq jr it js b jt mt jv jw jx mu jz ka kb mv kd ke kf mw kh ki kj mx kl km kn im bi translated">所有这三个都是粗略的启发式过程的产物，但是“Harry”到“harri”是误导性的，尤其是对于 NER 应用程序，并且对于“import”来说“重要的”是信息丢失。“勇敢”到“勇敢”是一个词干的例子，它没有任何意义，但仍然可以在信息检索系统中用于索引。一个更好的例子是——争论，争论，争论——所有这些都变成了“争论”,保留了原始单词的意思，但它本身在英语词典中没有任何意义。</p><p id="ab69" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，让我们来假设一下:</p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="d82f" class="ne lg it na b gy nf ng l nh ni"><strong class="na iu">from</strong> nltk.stem.wordnet <strong class="na iu">import</strong> WordNetLemmatizer<br/><strong class="na iu">from</strong> nltk.corpus <strong class="na iu">import</strong> wordnet</span><span id="49d0" class="ne lg it na b gy nj ng l nh ni">lm = WordNetLemmatizer()<br/>lemmatized_tokens = [lm.lemmatize(token) <strong class="na iu">for</strong> seq <strong class="na iu">in </strong>no_punct_seq_tokens <strong class="na iu">for</strong> token <strong class="na iu">in</strong> seq]<br/>print(lemmatized_tokens)</span><span id="7b20" class="ne lg it na b gy nj ng l nh ni">Output:<br/>['<strong class="na iu">Books</strong>',<br/> 'And',<br/> 'cleverness',<br/> 'There',<br/> 'are',<br/> 'more',<br/> '<strong class="na iu"><em class="mi">important</em></strong>',<br/> 'thing',<br/> 'friendship',<br/> 'and',<br/> '<strong class="na iu"><em class="mi">bravery</em></strong>',<br/> 'and',<br/> 'oh',<br/> '<strong class="na iu"><em class="mi">Harry</em></strong>',<br/> 'be',<br/> 'careful']</span></pre><p id="2216" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里，前面三个词干不正确的单词看起来更好。但是，“书”应该像“物”之于“物”一样成为“书”。</p><blockquote class="mj"><p id="0fea" class="mk ml it bd mm mn mo mp mq mr ms kn dk translated"><strong class="ak"> <em class="np">这是我们的第二个问题。</em>T15】</strong></p></blockquote><p id="6e02" class="pw-post-body-paragraph jq jr it js b jt mt jv jw jx mu jz ka kb mv kd ke kf mw kh ki kj mx kl km kn im bi translated">现在，WordNetLemmatizer.lemma()接受一个参数<strong class="js iu"> pos </strong>来理解单词/令牌的 pos 标签，因为单词形式可能相同，但上下文或语义不同。例如:</p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="ca74" class="ne lg it na b gy nf ng l nh ni">print(lm.lemmatize("Books", pos="n"))<br/>Output: 'Books'</span><span id="7ae7" class="ne lg it na b gy nj ng l nh ni">print(lm.lemmatize("books", pos="v"))<br/>Output: 'book'</span></pre><blockquote class="mj"><p id="c634" class="mk ml it bd mm mn nk nl nm nn no kn dk translated">因此，从 POS tagger 获得帮助似乎是一个方便的选择，我们将继续使用 POS tagger 来解决这个问题。</p></blockquote><p id="b8ff" class="pw-post-body-paragraph jq jr it js b jt mt jv jw jx mu jz ka kb mv kd ke kf mw kh ki kj mx kl km kn im bi translated">但在此之前，我们先来看看用哪个——stemmer？lemmatizer？两者都有？要回答这个问题，我们需要后退一步，回答如下问题:</p><ul class=""><li id="3ac7" class="nq nr it js b jt ju jx jy kb ns kf nt kj nu kn nv nw nx ny bi translated">问题陈述是什么？</li><li id="9b25" class="nq nr it js b jt nz jx oa kb ob kf oc kj od kn nv nw nx ny bi translated">哪些功能对解决问题陈述很重要？</li><li id="5e04" class="nq nr it js b jt nz jx oa kb ob kf oc kj od kn nv nw nx ny bi translated">这个特性会增加计算和基础设施的开销吗？</li></ul><p id="523d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的例子是 NLP 从业者在处理数百万代币的基本形式时面临的更大挑战的一个简单尝试。此外，在处理庞大的语料库时保持精度，并进行额外的检查，如词类标记(在这种情况下)、NER 标记、匹配单词包中的标记(BOW)以及拼写纠正，这些都是计算开销很大的。</p><p id="a08b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">步骤 3: POS 标签员进行救援</strong></p><p id="cfdf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们对已经词干化和词汇化的标记应用 POS tagger 来检查它们的行为。</p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="152f" class="ne lg it na b gy nf ng l nh ni"><strong class="na iu">from</strong> nltk.tag <strong class="na iu">import</strong> StanfordPOSTagger<br/><br/>st = StanfordPOSTagger(path_to_model, path_to_jar, encoding=’utf8')</span><span id="0e4e" class="ne lg it na b gy nj ng l nh ni">## Tagging Lemmatized Tokens</span><span id="ec48" class="ne lg it na b gy nj ng l nh ni">text_tags_lemmatized_tokens = st.tag(lemmatized_tokens)<br/>print(text_tags_lemmatized_tokens)</span><span id="8e69" class="ne lg it na b gy nj ng l nh ni">Output:<br/>[('Books', '<strong class="na iu"><em class="mi">NNS</em></strong>'),<br/> ('And', 'CC'),<br/> ('cleverness', 'NN'),<br/> ('There', 'EX'),<br/> ('are', 'VBP'),<br/> ('more', 'RBR'),<br/> ('important', 'JJ'),<br/> ('thing', 'NN'),<br/> ('friendship', 'NN'),<br/> ('and', 'CC'),<br/> ('bravery', 'NN'),<br/> ('and', 'CC'),<br/> ('oh', 'UH'),<br/> ('Harry', '<strong class="na iu"><em class="mi">NNP</em></strong>'),<br/> ('be', 'VB'),<br/> ('careful', 'JJ')]</span><span id="16d0" class="ne lg it na b gy nj ng l nh ni"><br/>## Tagging Stemmed Tokens</span><span id="9682" class="ne lg it na b gy nj ng l nh ni">text_tags_stemmed_tokens = st.tag(stemmed_tokens)<br/>print(text_tags_stemmed_tokens)</span><span id="6b52" class="ne lg it na b gy nj ng l nh ni">Output:<br/>[('book', '<strong class="na iu"><em class="mi">NN</em></strong>'),<br/> ('and', 'CC'),<br/> ('clever', 'JJ'),<br/> ('there', 'EX'),<br/> ('are', 'VBP'),<br/> ('more', 'JJR'),<br/> ('import', 'NN'),<br/> ('thing', 'NN'),<br/> ('friendship', 'NN'),<br/> ('and', 'CC'),<br/> ('braveri', 'NN'),<br/> ('and', 'CC'),<br/> ('oh', 'UH'),<br/> ('harri', '<strong class="na iu"><em class="mi">NNS</em></strong>'),<br/> ('be', 'VB'),<br/> ('care', 'NN')]</span></pre><blockquote class="mj"><p id="77be" class="mk ml it bd mm mn nk nl nm nn no kn dk translated">发现突出的差异？</p></blockquote><ul class=""><li id="9953" class="nq nr it js b jt mt jx mu kb oe kf of kj og kn nv nw nx ny bi translated">第一个标记——<strong class="js iu">书</strong>——带词干的标记是 NN(名词)，带词干的标记是 NNS(复数名词)，比较具体。</li><li id="5dfe" class="nq nr it js b jt nz jx oa kb ob kf oc kj od kn nv nw nx ny bi translated">第二，<strong class="js iu"> Harry — </strong>错误地源于<em class="mi"> harri </em>，结果词性标注者未能正确地将其识别为专有名词。另一方面，词汇化的标记正确地将<em class="mi">哈利</em>分类，并且词性标注者明确地将其识别为专有名词。</li><li id="a718" class="nq nr it js b jt nz jx oa kb ob kf oc kj od kn nv nw nx ny bi translated">最后，<strong class="js iu"> harri </strong>和<strong class="js iu">braveri</strong>——尽管这些词在英语词汇词典中没有任何位置，但它们应该被归类为 FW。这个我现在没有解释。</li></ul><p id="e043" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">步骤 4:为令牌标记构建 POS 映射器</strong></p><p id="a13c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我首先创建了如下的 POS 映射器。该映射器用于根据树库位置标签代码到 wordnet 的参数。</p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="192e" class="ne lg it na b gy nf ng l nh ni"><strong class="na iu">from</strong> nltk.corpus.reader.wordnet <strong class="na iu">import</strong> VERB, NOUN, ADJ, ADV</span><span id="c4d7" class="ne lg it na b gy nj ng l nh ni">dict_pos_map = {<br/>    # Look for NN in the POS tag because all nouns begin with NN<br/>    'NN': NOUN,<br/>    # Look for VB in the POS tag because all nouns begin with VB<br/>    'VB':VERB,<br/>    # Look for JJ in the POS tag because all nouns begin with JJ<br/>    'JJ' : ADJ,<br/>    # Look for RB in the POS tag because all nouns begin with RB<br/>    'RB':ADV  <br/>}</span></pre><p id="e178" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">步骤 5:在解决问题的同时构建规格化器</strong></p><ol class=""><li id="e9ae" class="nq nr it js b jt ju jx jy kb ns kf nt kj nu kn oh nw nx ny bi translated">识别专有名词，跳过处理并保留大写字母</li><li id="e571" class="nq nr it js b jt nz jx oa kb ob kf oc kj od kn oh nw nx ny bi translated">识别单词的词性标签所属的词性家族——NN、VB、JJ、RB，并传递正确的变元化参数</li><li id="d9d9" class="nq nr it js b jt nz jx oa kb ob kf oc kj od kn oh nw nx ny bi translated">获取词汇化记号的词干。</li></ol><p id="7e8a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是最后的代码。我使用 st.tag_sents()来保持序列的顺序(逐句嵌套的标记)</p><p id="e5cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">带词干</strong></p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="ace6" class="ne lg it na b gy nf ng l nh ni">normalized_sequence = []<br/><strong class="na iu">for</strong> each_seq <strong class="na iu">in</strong> st.tag_sents(sentences=no_punct_seq_tokens):<br/>    normalized_tokens = []<br/>    <strong class="na iu">for</strong> tuples <strong class="na iu">in</strong> each_seq:<br/>        temp = tuples[0]<br/>        <strong class="na iu">if</strong> tuples[1] == "NNP" <strong class="na iu">or</strong> tuples[1] == "NNPS":<br/>            <strong class="na iu">continue</strong><br/>        <strong class="na iu">if</strong> tuples[1][:2] <strong class="na iu">in</strong> dict_pos_map.keys():<br/>            temp = lm.lemmatize(tuples[0].lower(), <br/>                                pos=dict_pos_map[tuples[1][:2]])<br/>        temp = stemmer.stem(temp)<br/>        normalized_tokens.append(temp)<br/>    normalized_sequence.append(normalized_tokens)<br/>normalized_sequence</span><span id="cd72" class="ne lg it na b gy nj ng l nh ni"><strong class="na iu">Output:</strong><br/>[['book'],<br/> ['and', 'clever'],<br/> ['there',<br/>  'be',<br/>  'more',<br/>  'import',<br/>  'thing',<br/>  'friendship',<br/>  'and',<br/>  'braveri',<br/>  'and',<br/>  'oh',<br/>  'be',<br/>  'care']]</span></pre><p id="54b0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">不带词干</strong></p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="c968" class="ne lg it na b gy nf ng l nh ni">normalized_sequence = []<br/>for each_seq in st.tag_sents(sentences=no_punct_seq_tokens):<br/>    normalized_tokens = []<br/>    for tuples in each_seq:<br/>        temp = tuples[0]<br/>        if tuples[1] == "NNP" or tuples[1] == "NNPS":<br/>            continue<br/>        if tuples[1][:2] in dict_pos_map.keys():<br/>            temp = lm.lemmatize(tuples[0].lower(), <br/>                                pos=dict_pos_map[tuples[1][:2]])<br/>        normalized_tokens.append(temp)<br/>    normalized_sequence.append(normalized_tokens)<br/>normalized_sequence</span><span id="a336" class="ne lg it na b gy nj ng l nh ni"><strong class="na iu">Output:</strong></span><span id="ac7f" class="ne lg it na b gy nj ng l nh ni">[['book'],<br/> ['And', 'cleverness'],<br/> ['There',<br/>  'be',<br/>  'more',<br/>  'important',<br/>  'thing',<br/>  'friendship',<br/>  'and',<br/>  'bravery',<br/>  'and',<br/>  'oh',<br/>  'be',<br/>  'careful']]</span></pre><p id="e3ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个过程中，我还有两个重要的不同之处，一是源于进口，二是源于勇敢。虽然“braveri”相当明确，但“import”却是一个挑战，因为它可能意味着不同的东西。这就是为什么，一般来说，词干化比词元化具有更差的性能，因为它增加了搜索词的歧义性，因此检测到实际上并不相似的相似性。</p><p id="1c9d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="mi">注:</em> <a class="ae le" href="https://github.com/royn5618/Medium_Blog_Codes/blob/master/Text_Normalization_ft_POS_Tagger.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="mi">这里的</em> </a> <em class="mi">是 GitHub 上完整的 jupyter 笔记本。</em></p><p id="a985" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[1]<a class="ae le" href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html" rel="noopener ugc nofollow" target="_blank">https://NLP . Stanford . edu/IR-book/html/html edition/stemming-and-lemma tization-1 . html</a></p></div></div>    
</body>
</html>