<html>
<head>
<title>Hyperparameter Tuning: A Practical Guide and Template</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调谐:实用指南和模板</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-a-practical-guide-and-template-b3bf0504f095?source=collection_archive---------18-----------------------#2020-02-20">https://towardsdatascience.com/hyperparameter-tuning-a-practical-guide-and-template-b3bf0504f095?source=collection_archive---------18-----------------------#2020-02-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/c9e99e199fff6c22b0b5ccdf72b4b6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sSvAOeHB5skY0L7GA-iZbw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">FreeImages.com<a class="ae kf" href="https://www.freeimages.com/photo/mixpanel-1241075" rel="noopener ugc nofollow" target="_blank">上<a class="ae kf" href="https://www.freeimages.com/photographer/nemrah-42227" rel="noopener ugc nofollow" target="_blank">哈蒙·德夫</a>的照片</a></p></figure><p id="3141" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di"> Y </span>你已经煞费苦心地完成了你的<a class="ae kf" rel="noopener" target="_blank" href="/exploratory-data-analysis-eda-a-practical-guide-and-template-for-structured-data-abfbf3ee3bd9">探索性数据分析</a>，数据处理和最后😎，该造型了！但是你很快意识到，你有相当多的<a class="ae kf" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">超参数</a>需要调整，你可能要花上一千年的时间在这个巨大的超参数空间中搜索最优设置。</p><p id="1af6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">生命短暂，因此，懒惰的我决定编写不止一个，而是两个代码模板，以自动化和加快机器学习和深度学习模型的超参数调整过程。点击<a class="ae kf" href="#b597" rel="noopener ugc nofollow">此处</a>获取代码链接(或滚动至底部)，但请继续阅读，了解更多关于超参数调整的信息，并理解代码的作用。</p><h1 id="4560" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">超参数调谐的情况有哪些</h1><p id="1157" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">在我们深入研究这两个代码模板之前，有必要首先列出我们可能面临的超参数调优的背景。</p><p id="9518" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本质上有两种情况:</p><ol class=""><li id="d436" class="mq mr it ki b kj kk kn ko kr ms kv mt kz mu ld mv mw mx my bi translated">试验次数&gt;超参数次数；和</li><li id="74cb" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld mv mw mx my bi translated">试验次数</li></ol><p id="442f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中每个试验都是对一组超参数的评估。</p><p id="0331" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">情况1通常(但不总是)适用于机器学习。训练时间不是特别长，因此我们可以进行大量试验来覆盖超参数空间(即，探索超参数的许多可能组合)。面临的挑战是，如果超参数空间很大，我们可能仍然需要大量的试验和很长时间才能达到最佳设置。因此，如果算法能够智能地操纵超参数空间，将有助于加快优化。</p><p id="b4c3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，在深度学习中，训练时间通常要长得多；可能几天甚至几周。因此，我们没有能力进行多次试验，这就是情况2。在这种情况下，我们需要通过并行运行许多试验来提高我们的能力，而不是针对全部数量的时期训练所有试验，该算法应该在训练期间尽早淘汰最没有希望的试验候选，以将计算资源重新分配给其他更有希望的试验。</p><p id="6d2d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑到上述情况，我们现在来看看各种超参数调整算法以及为我们的目的而选择的算法。</p><h1 id="b8f6" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">超参数调整算法</h1><p id="bb45" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">下表显示了超参数调整算法的高级概述。</p><figure class="nf ng nh ni gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/51778e927c9d608e5c0ebd89c86e8638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oNZNEfBbQBWYujNTlDxzqg.png"/></div></div></figure><p id="39bc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们不会详细讨论每一个算法，但是如果你有兴趣了解更多，下面有其他资源的链接供你参考。</p><h2 id="e3d8" class="nj lo it bd lp nk nl dn lt nm nn dp lx kr no np mb kv nq nr mf kz ns nt mj nu bi translated">网格搜索和随机搜索</h2><p id="34ac" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">网格搜索系统地评估网格中指定的每个超参数组合的模型，而随机搜索从可能参数值的分布中对每个超参数集进行采样。</p><figure class="nf ng nh ni gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/fdc4e14455879da657300857ef8070ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*iCGf6jSeDR2m_4NjC3TrxA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:<a class="ae kf" href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">伯格斯特拉等人(2012年)</a></p></figure><p id="deea" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面由James Bergstra和Yoshua Bengio在他们的<a class="ae kf" href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank"> 2012年论文</a>中的著名图表中，我们看到左边的网格搜索和右边的随机搜索，都有九个试验(黑点)和两个参数。每个方块上方的绿色区域显示了通过改变重要参数值的函数增益，每个方块左侧的黄色区域显示了由不重要参数引起的增益。该图说明了随机搜索可能更彻底地探索参数空间，并导致发现更优的设置。</p><h2 id="55dd" class="nj lo it bd lp nk nl dn lt nm nn dp lx kr no np mb kv nq nr mf kz ns nt mj nu bi translated">贝叶斯优化(Parzen估计器树)</h2><p id="1756" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">虽然随机搜索比网格搜索好，但还是有可能做得更好。</p><p id="354a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，在超参数空间中，同一区域中的点往往会返回相似的性能。贝叶斯优化利用了这一事实，并以较高的可能性对性能较好的点的区域进行采样。Parzen估计树(TPE)是采用贝叶斯优化方法的算法之一。</p><figure class="nf ng nh ni gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nw"><img src="../Images/a7b78ca0370aeaab8d45fedeb06a5b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BaYQTFh9R9T3sf2c7tUfTQ.png"/></div></div></figure><p id="62d1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上图说明了贝叶斯优化搜索超参数空间的方式。假设红色区域是最佳损失发生的地方，每个白色圆圈是一次尝试，数字代表搜索的顺序。最初，超参数搜索类似于随机搜索，但随着贝叶斯优化算法用过去试验提供的信息更新自身，搜索逐渐开始向红色区域移动。这本质上是贝叶斯优化优于随机搜索的直觉。</p><p id="46b2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在T4的一篇论文中，作者也是James Bergstra等人，TPE算法被证明能够找到比随机搜索更好的超参数配置。</p><figure class="nf ng nh ni gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/0f38194d3340dac046d7d98358a3362c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-EgBNBrabYHYf2CtVZ9EeA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:<a class="ae kf" href="http://proceedings.mlr.press/v28/bergstra13.pdf" rel="noopener ugc nofollow" target="_blank">伯格斯特拉等人(2013年)</a></p></figure><p id="e853" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上图中的灰点表示使用随机搜索的n次试验中的最低误差，而绿点是TPE算法对于相应的n次试验的最低误差。因此，该论文在他们的实验中显示，TPE通常发现超参数配置，这些配置返回比随机搜索更低的验证错误。</p><h2 id="d669" class="nj lo it bd lp nk nl dn lt nm nn dp lx kr no np mb kv nq nr mf kz ns nt mj nu bi translated">异步连续减半算法(ASHA)</h2><p id="f5cd" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">然而，以并行的方式运行贝叶斯优化是很重要的。在深度学习中，模型训练通常需要很长时间，并行计算尤其重要，ASHA利用了这一点。</p><p id="b333" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了理解ASHA是如何工作的，我们需要介绍两个组件。第一，连续减半算法(SHA ),第二，异步方面。</p><p id="4a49" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="ny">【SHA】</em></strong>逐次减半算法</p><p id="8e44" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设有64个超参数配置需要评估，SHA将以如下方式制定策略。</p><figure class="nf ng nh ni gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nz"><img src="../Images/1d68e6851ff8f5e705672b164f053bff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tj3QabtL4wRZGGlAY0Nfag.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">对于每个梯级，将剩余1/配置缩减系数</p></figure><p id="0658" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考上表，SHA将首先评估所有64个超参数配置，每个配置1个历元。这64项试验的性能将相互比较，试验的前1/缩减系数将被“提升”。在我们的示例中，我们任意将缩减因子设置为4，因此1/4 * 64 = 16次试验将被“提升”到下一个梯级，在那里将运行另外3个时期。在每个下一个梯级进行类似的比较，直到只剩下1次试验。</p><p id="b605" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">来自SHA的关键见解是，我们不需要完全评估全部64个时期的所有试验来选择最佳超参数配置。通常情况下，最没有希望的试验可以在几个时期后确定，我们可以提前停止这些试验，将我们的资源重新分配给其他更有希望的试验。</p><p id="02b9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="ny">异步算法</em> </strong></p><p id="0afe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不幸的是，由于SHA需要等待一个梯级中的所有试验完成后才能继续下一个梯级，因此在存在掉队者(即运行速度比其他试验慢的试验)的情况下，很难实现最佳并行化。并且假设我们有10个计算节点，并且每次试验分配一个计算节点，SHA也将导致越来越多的剩余计算节点没有被利用，因为对于更高的梯级，试验的数量减少(例如，梯级3处的6个空闲节点)。</p><p id="9e85" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，ASHA通过尽可能提升配置来调整SHA算法，而不是等待所有试验都在一个梯级中完成。参考上表中的相同示例，ASHA将每评估4次试验，就将一次试验提升至下一级。</p><p id="2800" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据本文<a class="ae kf" href="https://arxiv.org/pdf/1810.05934.pdf" rel="noopener ugc nofollow" target="_blank">的研究</a>，与其他可以在并行环境中运行的现有算法相比，ASHA的性能通常不亚于甚至更好。</p><p id="db38" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我从论文中得到的直觉是，ASHA可能会表现得更好，因为它能够充分利用并行计算来评估给定时间范围内尽可能多的配置。对于其他算法，它们可能无法在掉队的情况下充分利用并行计算。</p><h2 id="78ef" class="nj lo it bd lp nk nl dn lt nm nn dp lx kr no np mb kv nq nr mf kz ns nt mj nu bi translated">进化算法</h2><p id="1373" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">其他类型的算法包括基于进化的算法，如基于群体的训练。欲知详情，请参考<a class="ae kf" href="https://deepmind.com/blog/article/population-based-training-neural-networks" rel="noopener ugc nofollow" target="_blank"> DeepMind的博文</a>。</p><h1 id="3a6e" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">我们的代码模板</h1><p id="be27" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">在回顾了关于超参数调优算法的现有文献并回忆了我们的两种情况后，我决定为每种情况使用两个单独的代码模板。</p><p id="8287" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一个代码模板适合更简单的情况1，我们不需要并行计算，所以我写了一些可以在Google Colab中轻松运行的东西。第二个代码模板将迎合情况2，利用Google云平台进行分布式试验。</p><p id="2464" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在算法方面，第一个代码使用贝叶斯优化，而第二个代码使用ASHA。</p><h1 id="23c9" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">情况1的代码模板</h1><figure class="nf ng nh ni gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oa"><img src="../Images/61dff2f996bd18b35f4726af1bd0179c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EIOXBmpNOoy96CzMDbTmJQ.png"/></div></div></figure><p id="ebed" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> Hyperopt </strong>是一个超参数优化库，实现了贝叶斯优化的TPE。远视的主要贡献者之一是詹姆斯·伯格斯特拉。</p><p id="54d4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://colab.research.google.com/drive/1NanPjXhgXgxiOtoV7ltGA4hKS-6z-3cX" rel="noopener ugc nofollow" target="_blank">我们的代码模板</a>使用Hyperopt库，可以很容易地在Google Colab中运行，有两个主要部分。</p><p id="0007" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="ny"> 1。</em>车型配置探究</strong></p><p id="0192" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本节中，我们将设置我们想要研究的模型的配置。例如，下面的代码块为XGBoost分类器设置超参数空间。</p><pre class="nf ng nh ni gt ob oc od oe aw of bi"><span id="bc56" class="nj lo it oc b gy og oh l oi oj"># XGB Classifier parameters</span><span id="03af" class="nj lo it oc b gy ok oh l oi oj">model_params = {<br/>'learning_rate': hp.choice('xgbc.learning_rate', np.arange(0.05, 0.31, 0.05)),</span><span id="29d8" class="nj lo it oc b gy ok oh l oi oj">'max_depth': hp.choice('xgbc.max_depth', np.arange(5, 16, 1, dtype=int)),</span><span id="c57a" class="nj lo it oc b gy ok oh l oi oj">'min_child_weight': hp.choice('xgbc.min_child_weight', np.arange(1, 8, 1, dtype=int)),</span><span id="8456" class="nj lo it oc b gy ok oh l oi oj">'colsample_bytree': hp.choice('xgbc.colsample_bytree', np.arange(0.3, 0.8, 0.1)),</span><span id="869a" class="nj lo it oc b gy ok oh l oi oj">'subsample': hp.uniform('xgbc.subsample', 0.8, 1),</span><span id="4b85" class="nj lo it oc b gy ok oh l oi oj">'n_estimators': 100,<br/>}</span></pre><p id="f3ff" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了方便起见，我们为3个主要模型设置了模板，每个模型都有一个分类器和回归器:</p><ul class=""><li id="e539" class="mq mr it ki b kj kk kn ko kr ms kv mt kz mu ld ol mw mx my bi translated">XGBoost</li><li id="14d2" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated">LightGBM</li><li id="a14c" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated">随机森林</li></ul><p id="ce86" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要根据您的目的修改代码模板，只需相应地修改相关的代码块。</p><p id="460f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="ny"> 2。远视类对象</em> </strong></p><p id="7abb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这一部分包含我们的主要代码。</p><p id="0e85" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要执行超参数优化，只需实例化<em class="ny"> HyperOpt </em>类对象并调用<em class="ny"> optimize </em>方法。此外，您还可以通过在<em class="ny"> experiment_space </em>字典中包含其他模型来执行模型搜索。</p><pre class="nf ng nh ni gt ob oc od oe aw of bi"><span id="a1bb" class="nj lo it oc b gy og oh l oi oj"># set dictionary containing hyperparameter space defined<br/>experiment_space = {'model_type': hp.choice('model_type',[rf_clf_params, xgb_clf_params, lgbm_clf_params])}</span><span id="7d2d" class="nj lo it oc b gy ok oh l oi oj"># Perform optimal model search<br/>param_search = HyperOpt(X, y, cv=5)<br/>param_search.optimize(space=experiment_space, max_evals=2)</span></pre><p id="9b2b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，在上面的代码中，我们已经将<em class="ny"> max_evals </em>设置为2，但是您也可以将其设置为其他的试验次数。或者，将其设置为<em class="ny">‘auto’</em>，让代码搜索最佳设置，直到损失度量在预定义的试验次数内没有改善。</p><h1 id="c805" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">情况2的代码模板</h1><figure class="nf ng nh ni gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/684ed40c946a0f2fb3f61e2a5e178cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wXrnpLURPNo4_nupJ0EBfA.png"/></div></div></figure><p id="7b3b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> Ray Tune </strong>是一个Python库，用于任何规模的超参数调谐，允许我们轻松地执行多节点分布式计算，以同时评估各种超参数配置。</p><p id="5812" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的代码参考了Ray Tune的GitHub，由两个文件组成，可以在这里<a class="ae kf" href="https://github.com/jiahao87/ray_tune_hyperparameter_tuning" rel="noopener ugc nofollow" target="_blank">下载</a>。</p><ol class=""><li id="2540" class="mq mr it ki b kj kk kn ko kr ms kv mt kz mu ld mv mw mx my bi translated"><em class="ny"> cluster_config_cpu.yml </em>(分布式集群的配置)</li><li id="27bc" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld mv mw mx my bi translated"><em class="ny"> tune_cifar10.py </em>(超参数调整脚本)</li></ol><p id="c184" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将利用谷歌云进行分布式计算，并以CIFAR-10为例。请注意，谷歌云为新客户提供了<a class="ae kf" href="https://cloud.google.com/free/docs/gcp-free-tier" rel="noopener ugc nofollow" target="_blank">免费试用</a>。</p><p id="c3d3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的CIFAR-10示例中，我们能够在一个小时内使用40个CPU评估100个配置，而这通常需要一天的时间。所有这些都是在创建谷歌云账户后通过5个主要步骤实现的。</p><p id="e01d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1.设置Google云端认证</p><ul class=""><li id="f851" class="mq mr it ki b kj kk kn ko kr ms kv mt kz mu ld ol mw mx my bi translated"><a class="ae kf" href="https://cloud.google.com/docs/authentication/getting-started#creating_a_service_account" rel="noopener ugc nofollow" target="_blank">创建服务帐户</a>并下载包含您的密钥的JSON文件</li><li id="850a" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated"><a class="ae kf" href="https://cloud.google.com/docs/authentication/getting-started#setting_the_environment_variable" rel="noopener ugc nofollow" target="_blank">设置环境变量</a>指向下载的JSON文件的目录</li></ul><p id="43e0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.启用以下API:</p><ul class=""><li id="588f" class="mq mr it ki b kj kk kn ko kr ms kv mt kz mu ld ol mw mx my bi translated"><a class="ae kf" href="https://support.cloudability.com/hc/en-us/articles/360022463493-Enabling-Google-s-Cloud-Resource-Manager-API" rel="noopener ugc nofollow" target="_blank">云资源管理器API </a></li><li id="d296" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated">身份和访问管理(IAM) API</li><li id="974f" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated">计算引擎API</li></ul><p id="3792" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.将项目ID复制粘贴到<em class="ny"> cluster_config_cpu.yml </em>配置文件中的<em class="ny"> project_id </em></p><figure class="nf ng nh ni gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi on"><img src="../Images/8420b001a34a3090c48baf7794de0ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CgJs3CHMQYoxIi3tdepqFA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">在配置文件的project_id下填写您的项目ID</p></figure><p id="4f59" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.通过在终端中运行来启动集群:</p><pre class="nf ng nh ni gt ob oc od oe aw of bi"><span id="db60" class="nj lo it oc b gy og oh l oi oj">ray up -y cluster_config_cpu.yml</span></pre><p id="77a0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5.通过在终端中执行以下命令开始超参数调整试验:</p><pre class="nf ng nh ni gt ob oc od oe aw of bi"><span id="b9fc" class="nj lo it oc b gy og oh l oi oj">ray submit cluster_config_cpu.yml tune_cifar10.py</span><span id="7268" class="nj lo it oc b gy ok oh l oi oj"># To trial run scripts, add argument smoke-test<br/># ray submit cluster_config_cpu.yml tune_cifar10.py --smoke-test</span></pre><p id="4614" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在超参数调整过程中，您将在终端中看到状态更新，如下图所示。每次迭代(<em class="ny"> iter </em>)后，将为每次试验报告指标(<em class="ny"> acc，总时间</em>)。</p><figure class="nf ng nh ni gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oo"><img src="../Images/3b7b0fc9869b8ccd26da2f74b70d09c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wyZ8QOm8nGuAdT_N50gyUA.png"/></div></div></figure><p id="6cf9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的示例中，我们调整了三个超参数:</p><ul class=""><li id="a644" class="mq mr it ki b kj kk kn ko kr ms kv mt kz mu ld ol mw mx my bi translated">丢弃层将元素设置为零的概率(<em class="ny"> dropout_p </em>)</li><li id="88a4" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated">第二全连接层中的节点数(<em class="ny"> dense2_nodes </em></li><li id="0683" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated">第二卷积层输出的通道数(<em class="ny">输出通道</em>)</li></ul><p id="5804" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">超参数空间可以在脚本顶部的<em class="ny"> tune_cifar10.py </em>中与其他主要配置一起设置。</p><figure class="nf ng nh ni gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi op"><img src="../Images/3d6fda4e2e9354b4d119400de76a19cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Lcz1VdIBtMfwZTywvWW4Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">要在tune_cifar10.py中设置的主要配置</p></figure><p id="f8ca" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使代码适合您的目的，您可能希望修改<em class="ny"> ConvNet </em>类(用于模型架构)和<em class="ny"> get_data_loaders </em>函数(用于数据集加载)。</p><p id="8d1b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要设置使用的cpu类型和数量，请修改<em class="ny"> cluster_config_cpu.yml </em>中的<em class="ny"> head_node </em>、<em class="ny"> worker_nodes </em>和<em class="ny"> max_workers </em>。或者，要启动gpu，请参见<em class="ny"> cluster_config_gpu.yml </em>。</p><p id="6eb0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在超参数调优过程结束时，将报告最佳配置，并且可以从Google云集群的头节点下载相应的模型检查点。</p><figure class="nf ng nh ni gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oq"><img src="../Images/51233b5ba4b122e389177c96a85490af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6sxp2G9djhDOkqyhEBQ-9Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">每个试验的最佳模型将作为model_xxx.pth保存在头群集节点的/home/Ubuntu/ray _ results/train model下的每个试验文件夹中</p></figure><p id="e5fe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">快速提示:</p><ul class=""><li id="2b91" class="mq mr it ki b kj kk kn ko kr ms kv mt kz mu ld ol mw mx my bi translated">如果您正在启动大量的CPU，请确保您的head节点是足够强大的机器类型。</li><li id="7384" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated">默认情况下，Google Cloud对您可以启动的CPU和GPU数量有配额限制。但是您可以<a class="ae kf" href="https://cloud.google.com/compute/quotas#request_quotas" rel="noopener ugc nofollow" target="_blank">编辑您的配额</a>。</li></ul></div><div class="ab cl or os hx ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="im in io ip iq"><p id="5fcc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过上面的插图，我们希望展示如何为机器学习和深度学习模型执行超参数调整。</p><p id="b597" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您错过了代码模板链接，它们也在这里:</p><ul class=""><li id="11f3" class="mq mr it ki b kj kk kn ko kr ms kv mt kz mu ld ol mw mx my bi translated"><a class="ae kf" href="https://colab.research.google.com/drive/1NanPjXhgXgxiOtoV7ltGA4hKS-6z-3cX" rel="noopener ugc nofollow" target="_blank"> Google Colab笔记本</a>针对情况1</li><li id="d45a" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated"><a class="ae kf" href="https://github.com/jiahao87/ray_tune_hyperparameter_tuning" rel="noopener ugc nofollow" target="_blank">情况2的Github存储库</a></li></ul><p id="7a3d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读，我希望代码和文章是有用的。如果您有任何问题或建议，请随时发表评论。</p><h1 id="f29e" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">参考</h1><ul class=""><li id="655b" class="mq mr it ki b kj ml kn mm kr oy kv oz kz pa ld ol mw mx my bi translated"><a class="ae kf" href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">http://jmlr . csail . MIT . edu/papers/volume 13/bergstra 12a/bergstra 12a . pdf</a></li><li id="5f17" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated"><a class="ae kf" href="http://proceedings.mlr.press/v28/bergstra13.pdf" rel="noopener ugc nofollow" target="_blank">http://proceedings.mlr.press/v28/bergstra13.pdf</a></li><li id="1ec8" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated"><a class="ae kf" href="https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/" rel="noopener ugc nofollow" target="_blank">https://blog . ml . CMU . edu/2018/12/12/massively-parameter-optimization/</a></li><li id="35e4" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated"><a class="ae kf" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank">https://github.com/hyperopt/hyperopt</a></li><li id="1340" class="mq mr it ki b kj mz kn na kr nb kv nc kz nd ld ol mw mx my bi translated"><a class="ae kf" href="https://ray.readthedocs.io/en/latest/tune-usage.html" rel="noopener ugc nofollow" target="_blank">https://ray.readthedocs.io/en/latest/tune-usage.html</a></li></ul></div></div>    
</body>
</html>