<html>
<head>
<title>Understanding Natural Language Processing: how AI understands our languages</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解自然语言处理:人工智能如何理解我们的语言</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-nlp-how-ai-understands-our-languages-77601002cffc?source=collection_archive---------14-----------------------#2020-02-03">https://towardsdatascience.com/understanding-nlp-how-ai-understands-our-languages-77601002cffc?source=collection_archive---------14-----------------------#2020-02-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b762" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让我们来一次自然语言处理的进化之旅…</h2></div><blockquote class="ki"><p id="84e6" class="kj kk it bd kl km kn ko kp kq kr ks dk translated">“语言是文化的路线图。它告诉你它的人民来自哪里，他们将去哪里。”</p><p id="8acb" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">—丽塔·梅·布朗</p></blockquote><p id="2503" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls ks im bi translated">由Google AI Research提出，<strong class="la iu">B</strong>I direction<strong class="la iu">E</strong>N coder<strong class="la iu">R</strong>presentations from<strong class="la iu">T</strong>transformers(<strong class="la iu">BERT</strong>)是一个<strong class="la iu">S</strong>state<strong class="la iu">o</strong>f<strong class="la iu">T</strong>he<strong class="la iu">A</strong>rt(<strong class="la iu">SOTA</strong>)模型中的<strong class="la iu">N</strong>natural<strong class="la iu">Google以其先进的算法为我们提供了许多方便而强大的工具。随着自然语言处理领域的前沿研究，谷歌搜索和谷歌翻译是几乎每天都在使用的两大服务，并且几乎成为我们思维的延伸。</strong></p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/1ef91339d24dc64e4dc8fafb2792d5f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kNVYStDNliSJbuky.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">机器人看书，图片来自<a class="ae lt" href="https://shakespeareoxfordfellowship.org/shakespeare-by-the-numbers-what-stylometrics-can-and-cannot-tell-us/robot-reading-books/" rel="noopener ugc nofollow" target="_blank">莎士比亚牛津奖学金</a></p></figure><p id="8909" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">在本文中，我们将了解NLP的发展以及它是如何变成今天这个样子的。我们将首先浏览机器学习之前的NLP简史。之后，我们将深入研究神经网络的进展及其在自然语言处理领域的应用，特别是神经网络中的<strong class="la iu">R</strong>e current<strong class="la iu">N</strong>eural<strong class="la iu">N</strong>网络(<strong class="la iu"> RNN </strong>)。最后，我们将走进SOTA的模型，如<a class="ae lt" href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="la iu"/>H<strong class="la iu">A</strong>T<strong class="la iu">N</strong>网络(<strong class="la iu">韩</strong> ) </a>和<strong class="la iu">B</strong>I方向<strong class="la iu">E</strong>N编码器<strong class="la iu"> R </strong>代表从<strong class="la iu"> T </strong>变压器(<strong class="la iu"> BERT </strong></p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="c75e" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">自然语言处理简史</h1><blockquote class="ki"><p id="b8fc" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">“学习另一种语言不仅仅是学习相同事物的不同词汇，而是学习思考事物的另一种方式。”</p><p id="232b" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">——弗洛拉·刘易斯</p></blockquote><p id="4c1c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls ks im bi translated">NLP领域的第一个想法可能早在17世纪就有了。笛卡尔和莱布尼茨提出了一种由通用数字代码创建的字典，用于在不同语言之间翻译文本。凯夫·贝克、阿塔纳斯·珂雪和乔安·约阿希姆·贝歇耳随后开发了一种基于逻辑和图像学的明确的通用语言。</p><p id="55f6" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">1957年，诺姆·乔姆斯基发表了《句法结构》。这部专著被认为是20世纪语言学中最重要的研究之一。该专著用短语结构规则和句法树构建了一个正式的语言结构来分析英语句子。"无色的绿色想法疯狂地沉睡."根据相结构规则构造的名句，语法正确但毫无意义。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c66a512a38579947e480c77963cb9e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/0*Doq4lVJuyPWgvEvZ.jpeg"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">“无色的绿色想法疯狂地沉睡”，图片来自<a class="ae lt" href="https://steemit.com/steemiteducation/@rasamuel/colorless-green-ideas-sleep-furiously" rel="noopener ugc nofollow" target="_blank"> steemit </a></p></figure><p id="3994" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">直到20世纪80年代，大多数NLP系统都是基于复杂的手写规则。直到后来，由于计算能力和可用训练数据的增加，机器学习算法开始发挥作用。最著名的机器学习算法之一是RNN，一种基于神经网络的架构。随着用机器处理自然语言的需求不断增加，新的模型近年来一直在快速迭代。伯特是现在的SOTA，也许几年后会被取代，谁知道呢？</p><p id="b0b8" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">我们刚刚复习了英语和西班牙语等拉丁词根语言的历史。其他语言如汉语、印地语和阿拉伯语则完全不同。英语可以用一套简单的规则来描述，而汉语不同，它的语法极其复杂，有时模糊得无法用逻辑元素来定义。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi np"><img src="../Images/c33f6141176bc562e3f7c0e629ed9f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lV7bUyTP5OBsTJZ-"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk">Example of Chinese Paragraphs, Image from <a class="ae lt" href="http://www.bauhinia.org/index.php/zh-CN/analyses/461" rel="noopener ugc nofollow" target="_blank">智经研究中心</a></p></figure><p id="ec5d" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">汉语语法的复杂性和现代机器是基于逻辑电路的事实，也许是为什么流行的编程语言通常是英语的原因。过去曾有过几次用中文编写程序语言的尝试，如<a class="ae lt" href="http://www.dywt.com.cn/" rel="noopener ugc nofollow" target="_blank">易</a>和<a class="ae lt" href="https://github.com/wenyan-lang/wenyan" rel="noopener ugc nofollow" target="_blank">燕文</a>。这些语言非常类似于我们日常使用的编程语言，如Basic和c。这些尝试并不能证明中文是一种更好的编程语言。</p><p id="5267" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">不同词根的语言之间的翻译也更加困难。手语到我们日常语言的翻译也遇到了许多障碍。我猜这就是我们试图建造巴别塔所要付出的代价。为了惩罚试图建造这座塔的人类，据说上帝决定通过让我们说不同的语言来分裂人类。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/7d917c5f57779a1a0cca0990f62512e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*D6vdn4uSqeyrTgCD.jpg"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">巴别塔，图片来自<a class="ae lt" href="https://www.icr.org/article/what-happened-at-tower-babel/" rel="noopener ugc nofollow" target="_blank"> iCR </a></p></figure><p id="b922" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">但随着时间的推移，我们克服了无数的障碍，走到了现在。随着技术的进步，语言障碍越来越不成问题。我们完全可以购买下面视频中的翻译棒，预订我们的日本之旅，而不需要事先了解任何日语。</p><p id="894e" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">NLP算法也在许多其他领域帮助我们，例如自动字幕、用户体验研究、可访问性，甚至这篇文章的写作，因为如果没有<a class="ae lt" href="https://app.grammarly.com/" rel="noopener ugc nofollow" target="_blank">语法</a>的帮助，我的英语会很糟糕。现在，让我们深入那些令人敬畏的技术背后的计算机算法。</p><h1 id="d1bc" class="mw mx it bd my mz nr nb nc nd ns nf ng jz nt ka ni kc nu kd nk kf nv kg nm nn bi translated">抽象语法树、上下文无关语法和编译器</h1><blockquote class="ki"><p id="d626" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">"无色的绿色想法疯狂地沉睡."</p><p id="7307" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">—诺姆·乔姆斯基</p></blockquote><p id="bc3d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls ks im bi translated">我们现代化的软件产业是建立在自然语言处理的基础上的。语法树的应用之一是我们的编译器。如果没有它，我们将不得不处理机器代码，而不是像Python和JavaScript这样简单易学的编程语言。想象一下用二进制机器指令来编码我们的机器学习算法，yuck…由诺姆·乔姆斯基发明，<strong class="la iu">A</strong>bstract<strong class="la iu">S</strong>yntax<strong class="la iu">T</strong>REE(<strong class="la iu">AST</strong>和<strong class="la iu">C</strong>ontext-<strong class="la iu">F</strong>REE<strong class="la iu">G</strong>rammar(<strong class="la iu">CFG</strong>)被用来描述和分析我们用来编码的编程语言。</p><blockquote class="nw nx ny"><p id="ebf4" class="ky kz nz la b lb mk ju ld le ml jx lg oa mm lj lk ob mn ln lo oc mo lr ls ks im bi translated">警告:下面的例子是出于教育目的而简化的。如果你真的想知道编译器到底是如何工作的，请参考更专业的文档。</p></blockquote><h2 id="12db" class="od mx it bd my oe of dn nc og oh dp ng lh oi oj ni ll ok ol nk lp om on nm oo bi translated">抽象语法树</h2><p id="0322" class="pw-post-body-paragraph ky kz it la b lb op ju ld le oq jx lg lh or lj lk ll os ln lo lp ot lr ls ks im bi translated">编译器使用CFG来解释以人类可读编程语言编写的代码。它将分解代码的逻辑，并从中解释出递归逻辑。为了理解代码是如何分解成递归逻辑的，最好将流程表示为AST。下面是从代码中构造出的AST的图示。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/e95d266af2bea2ff76d6535b6915738d.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/1*r0WGRf6PAK6Qoeu-vnN3iQ.gif"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">抽象语法树解释一行代码</p></figure><h2 id="26bf" class="od mx it bd my oe of dn nc og oh dp ng lh oi oj ni ll ok ol nk lp om on nm oo bi translated">上下文无关语法</h2><p id="d4fe" class="pw-post-body-paragraph ky kz it la b lb op ju ld le oq jx lg lh or lj lk ll os ln lo lp ot lr ls ks im bi translated">CFG用于描述输入语言和输出标记之间的转换规则。在定义编译器的文件中，它通常以如下所示的方式编写:</p><pre class="lv lw lx ly gt ov ow ox oy aw oz bi"><span id="a7ec" class="od mx it ow b gy pa pb l pc pd">if_stm   : expr = left_bkt cond_stm right_bkt<br/>cond_stm : expr = expr and_stm expr    |<br/>                  expr or_stm expr     |<br/>                  not_stm expr         |<br/>                  num less_than num    |<br/>                  num greater_than num<br/>num      : var | const  </span></pre><p id="29b0" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">这些CFG基本上定义了if和condition语句的规则。例如，一个条件语句可以是多个条件语句(“expr”表示其本身，在本例中为“cond_stm”)与and语句(“and_stm”，在代码中表示为“&amp;&amp;”)，或者是由“&gt;”或“ Wikipedia 等比较器连接的简单数字，更多关于编译器的信息，如著名的另一个编译器(YACC)可以在<a class="ae lt" href="http://dinosaur.compilertools.net/yacc/" rel="noopener ugc nofollow" target="_blank">这里找到</a>。</p><h1 id="e728" class="mw mx it bd my mz nr nb nc nd ns nf ng jz nt ka ni kc nu kd nk kf nv kg nm nn bi translated">自然语言处理的神经构建模块:单词嵌入、RNN和LSTM</h1><blockquote class="ki"><p id="5278" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">“模仿人脑的复杂性，受神经启发的计算机将以类似于神经元和突触通信的方式工作。它可能会学习或发展记忆。”<br/> —纳耶夫·阿尔·罗德汉</p></blockquote><p id="05f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls ks im bi translated">高级NLP算法是用各种神经网络构建的。我在我的<a class="ae lt" rel="noopener" target="_blank" href="/understanding-alphago-how-ai-thinks-and-learns-advanced-d70780744dae"> Alpha Go文章</a>中对神经网络有更详细的解释，包括与神经网络相关的历史。神经网络是<a class="ae lt" href="https://en.wikipedia.org/wiki/Directed_acyclic_graph" rel="noopener ugc nofollow" target="_blank">有向无环图</a>由人工神经元的连接层组成。输入图层中的值逐层传播到输出图层。这些基于神经科学家如何看待我们大脑工作的模型在最近几年显示出了一些竞争性的表现。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pe"><img src="../Images/81a971082ee95bb9fa2a9eeac17d720a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OolBDcBWZi0qnvOy.gif"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">人工神经网络的前向传播</p></figure><h2 id="f98a" class="od mx it bd my oe of dn nc og oh dp ng lh oi oj ni ll ok ol nk lp om on nm oo bi translated">单词嵌入</h2><p id="c3e5" class="pw-post-body-paragraph ky kz it la b lb op ju ld le oq jx lg lh or lj lk ll os ln lo lp ot lr ls ks im bi translated">神经网络以及其他机器学习模型通常以数字向量的形式接受输入。但是我们的英语单词不是数字。这就是为什么我们有单词嵌入。单词嵌入指的是将单词和短语从词汇表转换成数字向量的语言建模和特征学习技术。一个例子可能是用神经网络在数千个段落中运行，以收集哪些单词更经常与另一个单词一起出现，从而给它们更接近的值。</p><p id="3225" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">一个著名的单词嵌入模型集合可能是Word2vect。该模型基于浅层神经网络，并假设段落中彼此接近的单词也共享相似的语义值。下面是解释它的视频。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="pf pg l"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">理解Word2Vec，视频来自<a class="ae lt" href="https://www.youtube.com/watch?v=QyrUentbkvw" rel="noopener ugc nofollow" target="_blank"> Youtube </a></p></figure><h2 id="1daa" class="od mx it bd my oe of dn nc og oh dp ng lh oi oj ni ll ok ol nk lp om on nm oo bi translated">递归神经网络(RNN)</h2><p id="179a" class="pw-post-body-paragraph ky kz it la b lb op ju ld le oq jx lg lh or lj lk ll os ln lo lp ot lr ls ks im bi translated">RNN是神经网络的一种变体，最擅长处理顺序数据。声波、股票历史和自然语言等数据被认为是连续的。RNN是一种神经网络，在处理顺序数据的过程中，输出被反馈到网络中，允许它也考虑过去的状态。</p><p id="9a40" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">在我们的自然语言中，句子的上下文并不总是由单词的上下文来表示。例如，“其他评论者认为食物很棒，但我认为不好”这句话有两个积极意义的词(棒极了)。人工神经网络对序列没有任何线索，只是总结一切，但RNN能够捕捉句子中的倒装句，如“但是”和“不是”，并根据它进行调整。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/eb1b9601fc5de9293e97b1d1ab777233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/1*x66PrhYu6RryOxItOLPlyQ.gif"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">安正在处理一个句子</p></figure><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/b1228a5eed74c713892695b55f7c9984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/1*bk8nWr-WIssM4voxwu3rQQ.gif"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">RNN处理了一句话</p></figure><h2 id="418b" class="od mx it bd my oe of dn nc og oh dp ng lh oi oj ni ll ok ol nk lp om on nm oo bi translated">长短期记忆(LSTM)</h2><blockquote class="ki"><p id="600d" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">"如果不忘记，根本不可能活着。"</p><p id="ff33" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">― <strong class="ak">弗里德里希·尼采</strong></p></blockquote><p id="f300" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls ks im bi translated">在RNN句子处理的例子中，您可能想知道为什么大多数单词都是不相关的。这正是我们需要LSTM的原因。LSTM引入了一个“忘记层”，它决定应该保留或忘记的信息，使模型在大量数据下更容易训练。LSTM有三种单元状态，即遗忘状态、更新状态和输出状态，每种状态有不同的用途。</p><p id="2893" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated"><strong class="la iu">忘记状态</strong></p><p id="bd33" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">在遗忘状态期间，LSTM将从输入和先前状态中检索信息。然后使用sigmoid函数σ来决定是否应该忘记之前的状态。sigmoid函数将输出0到1之间的值。通过将它与前一状态的输出相乘，它将决定前一状态的多少应该被遗忘。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/bc520a85ae486ed820326cf3b3353e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*xP9Xx8fx0DSWnrKdAXA4xQ.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">LSTM忘记状态</p></figure><p id="7f92" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated"><strong class="la iu">更新状态</strong></p><p id="3de8" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">在更新状态期间，LSTM将尝试更新状态值。一个超正切函数<em class="nz"> tanh </em>将柔和地强制该值位于0和1之间，这样它就不会累积成一些疯狂的数字。另一个sigmoid函数用于确定有多少将被添加到单元状态中。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/7c37d9ea03a1a90e4dc27257e9e04f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*MuQrL-NZDa0NShrUxH4C_w.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">LSTM更新状态</p></figure><p id="0c78" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated"><strong class="la iu">输出状态</strong></p><p id="c633" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">最后，单元格状态将由另一个超正切函数处理。然后使用一个sigmoid函数来确定将有多少输入到输出中。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/02af30a0eae01e59831cfcb50da30740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*CmECYOTLuDlHApq4r9UI2A.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">LSTM输出状态</p></figure><p id="5944" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">更多关于LSTM及其数学训练的详细解释可以在这里找到。</p><h1 id="7c56" class="mw mx it bd my mz nr nb nc nd ns nf ng jz nt ka ni kc nu kd nk kf nv kg nm nn bi translated">文本分类:与韩</h1><p id="2399" class="pw-post-body-paragraph ky kz it la b lb op ju ld le oq jx lg lh or lj lk ll os ln lo lp ot lr ls ks im bi translated">恭喜你！我们终于走到了像韩和伯特这样的模式。我们先讨论韩，并不是因为一个模型优于另一个，只是因为韩更容易理解，可能有助于伯特的理解。由大学和微软研究院在2016年提出的，韩证明了它在文本分类方面的能力。能够对Yelp评论等文本进行分类有助于各种领域，如用户体验研究和支持票证管理。</p><p id="a571" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">我们先从说起，它是韩的基石。不要担心，它与我们在上一节中学习的LSTM非常相似。在此之后，我们将能够了解韩的建筑。</p><h2 id="b9f9" class="od mx it bd my oe of dn nc og oh dp ng lh oi oj ni ll ok ol nk lp om on nm oo bi translated">门控循环单元(GRU)</h2><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pj"><img src="../Images/930a62d3e3268a58bb968154875f4446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lB7od5QKCbIcMYQV"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">GRU，图片来自<a class="ae lt" href="https://www.bustle.com/articles/95190-is-gru-in-minions-heres-why-the-despicable-me-supervillain-isnt-the-focus-of-the-new" rel="noopener ugc nofollow" target="_blank">喧闹</a></p></figure><blockquote class="ki"><p id="d813" class="kj kk it bd kl km kn ko kp kq kr ks dk translated">“我开玩笑的！虽然是真的。反正过得好。”</p><p id="7f8b" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">— GRU</p></blockquote><p id="ba7d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls ks im bi translated"><strong class="la iu"> GRU </strong>代表<strong class="la iu">G</strong>ate<strong class="la iu">R</strong>e current<strong class="la iu">U</strong>nits。与LSTM相比，它们的功能较弱，但模型更简单。然而，在某些情况下，会比表现得更好，这一点我们在了解韩的时候会讲得更多。这就是为什么研究人员通常对这两种设备都进行实验，看哪一种效果最好。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/85f0a43959896c4620341d503f252bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*LfA_sZgN6EZOLgylrGGJ0g.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">GRU解释道</p></figure><p id="fc34" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">GRU通过sigmoid函数使用输入和先前状态来做出所有决定。总共有2个决定:</p><ol class=""><li id="0f67" class="pl pm it la b lb mk le ml lh pn ll po lp pp ks pq pr ps pt bi translated">第一个决定是前一个状态是否将与输入状态合并。</li><li id="ab88" class="pl pm it la b lb pu le pv lh pw ll px lp py ks pq pr ps pt bi translated">第二个决定是这个混合状态(或普通输入状态)或前一个状态是否会进入下一个状态并作为输出。</li></ol><p id="83e1" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">更多关于GRU的变体及其详细的数学模型可以在<a class="ae lt" href="https://arxiv.org/pdf/1701.05923.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="18d0" class="od mx it bd my oe of dn nc og oh dp ng lh oi oj ni ll ok ol nk lp om on nm oo bi translated">分层注意网络</h2><blockquote class="ki"><p id="7883" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">在由单词、句子、段落和故事组成的大军中，字母表中的每个字母都是坚定忠诚的战士。一个字母掉了，整个语言都变得含糊不清。”<br/>——<strong class="ak">维拉·拿撒勒</strong></p></blockquote><p id="a631" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls ks im bi translated">传统的RNN和LSTM很难解释大量的文本，其中一些关键词彼此相距很远。这就是为什么HAN的注意机制从上下文向量<em class="nz"> u </em>中生成重要性权重α的原因。这个重要性权重然后被用于选择性地过滤掉值得关注的输出。</p><p id="9656" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">韩运用两个主要层次对文本进行分类，即词层和句层。字向量首先被送入由双向gru组成的编码器。双向GRU就是方向相反的gru堆叠在一起。然后，输出用于计算注意力和上下文向量，输出将被输入到句子层的编码器中，以经历类似的过程。最终的输出向量将被相加并馈入softmax层。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pz"><img src="../Images/3931e980b36b7f5c6cdbb40de10d00c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ybWTyEt0VZoLPNQpiRNQ9A.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">分层注意网络(韩)解释说</p></figure><p id="e193" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">关于韩的详细解释，其数学细节及其在Yelp评论上的表现，请参考<a class="ae lt" href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" rel="noopener ugc nofollow" target="_blank">原文</a>。</p><h1 id="3623" class="mw mx it bd my mz nr nb nc nd ns nf ng jz nt ka ni kc nu kd nk kf nv kg nm nn bi translated">语言理解:变压器和伯特</h1><p id="066a" class="pw-post-body-paragraph ky kz it la b lb op ju ld le oq jx lg lh or lj lk ll os ln lo lp ot lr ls ks im bi translated">最后，我们做到了如题所示。为了充分理解这一节，我们可能必须理解广泛的数学推理。这些可以在<a class="ae lt" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">变形金刚</a>和<a class="ae lt" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">伯特</a>的原始论文中找到。在本文中，我们将只讨论基本架构。</p><h2 id="8281" class="od mx it bd my oe of dn nc og oh dp ng lh oi oj ni ll ok ol nk lp om on nm oo bi translated">编码器-解码器架构</h2><p id="40d9" class="pw-post-body-paragraph ky kz it la b lb op ju ld le oq jx lg lh or lj lk ll os ln lo lp ot lr ls ks im bi translated">首先，让我们看看编码器-解码器的架构。在机器翻译中，经常使用编码器-解码器结构，因为源文本和目标文本并不总是一一匹配。编码器首先用于从输入消息产生输出值，解码器将利用该输出值产生输出消息。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/eabf49887708dc30f090a783f004a995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/1*YpV2FQZJ5MI3jalaUaJZdw.gif"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">利用编码器-解码器架构进行翻译</p></figure><h2 id="7449" class="od mx it bd my oe of dn nc og oh dp ng lh oi oj ni ll ok ol nk lp om on nm oo bi translated">变压器</h2><blockquote class="ki"><p id="e7cc" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">“你需要的只是关注”</p><p id="7010" class="kj kk it bd kl km kt ku kv kw kx ks dk translated">—变压器原始论文的标题</p></blockquote><p id="7c0a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls ks im bi translated">2017年，来自谷歌的研究人员提出了一种完全基于自我关注的架构。自我关注意味着模型通过解释输入来自行决定关注，而不是从外部获取关注分数。</p><p id="9251" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">有3种不同类型的向量，查询向量Q、密钥向量K和值向量v。这些向量可以被理解为数据搜索机制。</p><ol class=""><li id="f6b8" class="pl pm it la b lb mk le ml lh pn ll po lp pp ks pq pr ps pt bi translated">查询(Q)是我们寻找的那种信息。</li><li id="bb9f" class="pl pm it la b lb pu le pv lh pw ll px lp py ks pq pr ps pt bi translated">Key (K)是查询的相关性。</li><li id="8549" class="pl pm it la b lb pu le pv lh pw ll px lp py ks pq pr ps pt bi translated">值(V)是实际输入。</li></ol><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi qb"><img src="../Images/aea7a69fc56c8b24b213db198143d3d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pTcEVUquGy6s4-w2kpR-AQ.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">缩放的点积注意&amp;多头注意，图片来自<a class="ae lt" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">变形金刚</a>原纸</p></figure><p id="cc28" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">缩放的点积注意力首先将Q和K相乘，然后在图中所示的一些转换之后将其与V相乘。多头注意力将从缩放的点积注意力中检索到的头连接起来。Mask用于过滤掉一些值。这个机制解释起来有点复杂，<a class="ae lt" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">这里的</a>是一篇对注意力机制提供更详细解释的文章。</p><p id="b9fd" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">转换器是基于多头注意力的编码器-解码器架构。由于转换器使用简化的模型，不再考虑像RNN这样的位置信息，因此输入嵌入和位置嵌入都应用于输入，以确保模型也捕捉位置信息。添加&amp; Norm是<a class="ae lt" href="https://arxiv.org/pdf/1909.04653.pdf" rel="noopener ugc nofollow" target="_blank">剩余连接</a>和<a class="ae lt" href="https://en.wikipedia.org/wiki/Batch_normalization" rel="noopener ugc nofollow" target="_blank">批量规格化</a>。前馈只是简单的前馈神经网络重塑向量。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi qc"><img src="../Images/b4f7889ec06faa027400198688d37b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*ScJ66oV71wtmlMp0wmma4Q.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">变形金刚模型架构，图片来自变形金刚的<a class="ae lt" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></figure><h2 id="8fdb" class="od mx it bd my oe of dn nc og oh dp ng lh oi oj ni ll ok ol nk lp om on nm oo bi translated">变压器的双向编码器表示(BERT)</h2><p id="e150" class="pw-post-body-paragraph ky kz it la b lb op ju ld le oq jx lg lh or lj lk ll os ln lo lp ot lr ls ks im bi translated">2018年，来自谷歌的研究人员推出了一种名为BERT的语言表示模型。BERT模型架构是一个多层双向变压器编码器。构建框架有两个步骤——预训练和微调。</p><ol class=""><li id="4972" class="pl pm it la b lb mk le ml lh pn ll po lp pp ks pq pr ps pt bi translated">预训练:在预训练期间，模型在不同的预训练任务中根据未标记的数据进行训练。</li><li id="a596" class="pl pm it la b lb pu le pv lh pw ll px lp py ks pq pr ps pt bi translated">微调:在微调过程中，首先用预先训练的参数初始化BERT模型，然后使用来自下游任务的标记数据微调所有参数。</li></ol><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi qd"><img src="../Images/8a6ca07281015b2e65d9e8283716b6e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Am94REGphhYAqGO4888jw.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">BERT预训练和微调程序，图片来自BERT的<a class="ae lt" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></figure><p id="5c33" class="pw-post-body-paragraph ky kz it la b lb mk ju ld le ml jx lg lh mm lj lk ll mn ln lo lp mo lr ls ks im bi translated">模型在微调后提前了<a class="ae lt" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank"> 11个NLP任务</a>的SOTA。有关BERT的模式详情，请参考<a class="ae lt" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">原始文件</a>。</p><h1 id="fcd1" class="mw mx it bd my mz nr nb nc nd ns nf ng jz nt ka ni kc nu kd nk kf nv kg nm nn bi translated">话说到最后…</h1><p id="28ca" class="pw-post-body-paragraph ky kz it la b lb op ju ld le oq jx lg lh or lj lk ll os ln lo lp ot lr ls ks im bi translated">我计划写关于自然语言处理和计算机视觉的文章。因为我已经在我的<a class="ae lt" rel="noopener" target="_blank" href="/understanding-alphago-how-ai-thinks-and-learns-advanced-d70780744dae">上一篇文章</a>中写了关于卷积神经网络的内容，所以我决定先写一些关于递归神经网络的内容。还有更多关于卷积神经网络的内容，我已经在我的<a class="ae lt" rel="noopener" target="_blank" href="/understanding-cv-how-ai-sees-our-world-a977b90bf612">计算机视觉文章</a>中阐述过了。我还打算写关于生成模型和自动化机器学习的文章。人工智能领域有无数的奇迹，跟随我在遥远的未来看到更多！</p></div></div>    
</body>
</html>