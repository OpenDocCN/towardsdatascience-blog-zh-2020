<html>
<head>
<title>Generating Handwritten Sequences Using LSTMs and Mixed Density Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 LSTMs 和混合密度网络生成手写序列</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-handwritten-sequences-using-lstms-and-mixed-density-networks-c3a2b4a65539?source=collection_archive---------16-----------------------#2020-01-25">https://towardsdatascience.com/generating-handwritten-sequences-using-lstms-and-mixed-density-networks-c3a2b4a65539?source=collection_archive---------16-----------------------#2020-01-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/519ea25c83236db21296e7f2164fe06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yz7UJa-40eNhySY_4wkd7w.jpeg"/></div></div></figure><div class=""/><p id="e665" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于每个人都在年初提出了一个解决方案，所以我会尽量少在博客上发帖😀。已经过去一个多月了，我想在这里分享一些话😀。在这篇博客中，我们将讨论一篇由 Alex Graves(DeepMind)提出的关于用递归神经网络生成序列<a class="ae kw" href="https://arxiv.org/abs/1308.0850" rel="noopener ugc nofollow" target="_blank">的有趣论文。我还将使用 Tensorflow 和 Keras 来实现这篇论文。</a></p><h2 id="f283" class="kx ky jb bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">问题定义</h2><p id="9e27" class="pw-post-body-paragraph jy jz jb ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">这里的数据集是手写笔画的数学表示形式。所以笔画序列中的一个点是一个长度=3 的向量。第一个值是一个二进制数字，表示笔是否在该点上升起。第二个值是 x 坐标相对于序列中前一个 x 值的偏移量，同样，第三个值是 y 坐标的偏移量。问题是，给定数据集，我们需要一个可以无条件生成手写笔画的模型(通过给定一个种子值随机生成，就像 GANs 一样)。</p><h2 id="864f" class="kx ky jb bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">混合密度网络</h2><p id="7cc9" class="pw-post-body-paragraph jy jz jb ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">在进入实现部分之前，我想讨论一种特殊的网络，称为 MDNs 或混合密度网络。监督问题的目标是对要素和标注的生成器或联合概率分布进行建模。但是在大多数情况下，这些分布被认为是时不变的，例如在图像或文本分类的情况下。但是，当概率分布本身不稳定时，比如手写笔画或汽车的路径坐标，正常的神经网络在这种情况下表现很差。混合密度网络假设数据集是各种分布的混合物，并倾向于学习这些分布的统计参数，从而获得更好的性能。<strong class="ka jc">迷茫？</strong></p><figure class="lw lx ly lz gt is gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/be3f791deefb0b2471af6ea5bc40bc43.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*msXphvqTSGLdLRQE.gif"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">来源:https://media1.giphy.com/media/3o7btPCcdNniyf0ArS/giphy.gif<a class="ae kw" href="https://media1.giphy.com/media/3o7btPCcdNniyf0ArS/giphy.gif" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="7a58" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好吧，好吧，我们会进入更多的细节。</p><p id="4f33" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们举一个下图所示的例子:</p><figure class="lw lx ly lz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi me"><img src="../Images/f8ad8206562313a1dd6d8942081372d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*obGUKsSrBwFMAprU.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">来源:<a class="ae kw" href="https://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.6-MixDensityNetworks.pdf" rel="noopener ugc nofollow" target="_blank">https://cedar . buffalo . edu/~ Sri Hari/CSE 574/chap 5/chap 5.6-mixdensitynetworks . pdf</a></p></figure><p id="b2bf" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在第一张图中，给定θ1 和θ2，当被问及预测机械臂的位置时，我们有一个独特的解决方案。现在看第二个图，问题反过来了。给定 x1 和 x2，当被问及预测θ参数时，我们得到两个解。在大多数情况下，我们会遇到像第一张图这样的问题，其中数据分布可以被假设为来自单个高斯分布。而对于像第二幅图这样的情况，如果我们使用传统的神经网络，它的性能不会下降。我很快会告诉你为什么。</p><p id="9860" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这并不是说具有均方损失或交叉熵损失的神经网络不考虑(混合分布)的事情。它考虑了这一点，但给出了一个结果分布，它是所有这些混合物的平均值，这篇<a class="ae kw" href="https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>建议我们不需要平均值，而是最可能的混合物成分来模拟统计参数。因此<strong class="ka jc"> MDN </strong>进入了画面。</p><p id="ab5f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它看起来怎么样？</p><p id="9438" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，MDN 只不过是一个预测统计参数的神经网络，而不是预测回归问题的类或值的概率。让我们假设数据是<strong class="ka jc"> M </strong>正态分布的混合物，因此最终预测将是混合物的<strong class="ka jc"> M </strong>概率权重、<strong class="ka jc"> M </strong>平均值和<strong class="ka jc"> M </strong>方差。取样时，我们取最可能的混合物成分的参数。我们将在后面详细讨论其结构和实现。</p><h2 id="8475" class="kx ky jb bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">长短期记忆网络</h2><figure class="lw lx ly lz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mf"><img src="../Images/402fb0efe41e3f0e153854defe820400.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*keS2VnfeQE0AngYN4Xcqkg.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">来源:<a class="ae kw" href="https://i.stack.imgur.com/SjiQE.png" rel="noopener ugc nofollow" target="_blank">https://i.stack.imgur.com/SjiQE.png</a></p></figure><p id="b4e4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">虽然这不是什么新鲜事，但我可能会遇到一些新人。因此，我将在这里给出一个关于 LSTMs 的基本概念。因此，LSTMs 只不过是一种特殊的神经网络，具有三个门，分别是输入门、遗忘门和输出门。LSTM 细胞中最重要的东西是它的记忆——如上图所示。</p><p id="a7e2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">遗忘门:</strong>一个 LSTM 细胞会输出隐藏状态和本细胞的记忆，反馈给它旁边的细胞。一个遗忘门决定了从前一个存储器传入当前单元的信息量。它基本上采用以前的输出和当前的输入，并输出一个遗忘概率。</p><p id="6437" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">输入门:</strong>它接受与遗忘门相同的输入，并输出一个概率，该概率决定了有助于记忆的单元状态的信息量。</p><p id="7e74" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">输出门:</strong>它再次接受与上述相同的输入，并给出一个概率，该概率预测决定要传递给下一个单元的信息量的概率。</p><p id="3edf" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">LSTMs 非常擅长处理顺序输入，如前面讨论的文本、时间序列数据集或笔画数据集，并为样本数据序列提供健壮的特征向量。Alex Graves 因此建议将它们用于手写笔画，以找到笔画序列的特征向量，然后将该特征向量馈送到 MDN 网络，以找到统计参数，如下图所示:</p><figure class="lw lx ly lz gt is gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/589aacc8e741e73a41e1625eb80856e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/0*UXeO0gAVgJShzCOk"/></div></figure><p id="b0a9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我希望我们现在可以进入实施阶段了。我将使用 Keras 来创建网络，并在其后端使用 Tensorflow 来定义损失函数。</p><p id="d637" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设 x 和 y 点偏移遵循二元正态分布的混合，笔抬起遵循伯努利分布(很明显，对吗？).我将使用两个二元正态分布的混合来演示解决方案，伯努利分布可以用一个概率值来表示，对吗？</p><p id="69da" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">哦！我忘了告诉你损失函数。它由下面给出的等式定义:</p><figure class="lw lx ly lz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/f43d6f345fdef8f0de8253ec348df98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SJ7nOkYBwbJDBbFY.png"/></div></div></figure><p id="5e58" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中，π是混合物成分的概率，e 是确定伯努利分布参数的冲程终点的概率。</p><p id="6e84" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在让我们完成我们被雇来的任务😮….🤫….😃代码代码代码:</p><p id="f84f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">输入将是 3 个长度向量的序列，每个点的输出是笔画序列中的下一个点，如下图所示:</p><figure class="lw lx ly lz gt is gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/06a4c2b06b459c743a8323c223cd0e6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/0*djbQsh28SRzQz9VW"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">来源:<a class="ae kw" href="https://image.slidesharecdn.com/rnnerica-180423124525/95/rnn-and-its-applications-31-638.jpg?cb=1524487703" rel="noopener ugc nofollow" target="_blank">https://image . slidesharecdn . com/rnne Rica-180423124525/95/rnn-and-its-applications-31-638 . jpg？cb=1524487703 </a></p></figure><h2 id="3191" class="kx ky jb bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">车型(LSTM + MDN)</h2><pre class="lw lx ly lz gt mj mk ml mm aw mn bi"><span id="da56" class="kx ky jb mk b gy mo mp l mq mr">import numpy <strong class="mk jc">as</strong> np<br/>import numpy<br/>import tensorflow <strong class="mk jc">as</strong> tf<br/>import tensorflow.keras <strong class="mk jc">as</strong> keras<br/>import tensorflow.keras.backend <strong class="mk jc">as</strong> K<br/>import keras.layers.Input <strong class="mk jc">as</strong> Input<br/>import keras.layers.LSTM <strong class="mk jc">as</strong> LSTM<br/>from tensorflow.keras.models import Model<br/><br/><em class="ms"># nout = 1 eos + 2 mixture weights + 2*2 means \<br/># + 2*2 variances + 2 correlations for bivariates<br/></em><br/><strong class="mk jc">def</strong> <strong class="mk jc">build_model</strong>(ninp<strong class="mk jc">=</strong>3, nmix<strong class="mk jc">=</strong>2):<br/>    inp <strong class="mk jc">=</strong> Input(shape<strong class="mk jc">=</strong>(None, ninp), dtype<strong class="mk jc">=</strong>'float32')<br/>    l,h,c <strong class="mk jc">=</strong> LSTM(400, return_sequences<strong class="mk jc">=</strong>True, \<br/>    return_state<strong class="mk jc">=</strong>True)(inp)<br/>    l1 ,_,_<strong class="mk jc">=</strong> LSTM(400, return_sequences<strong class="mk jc">=</strong>True, \<br/>    return_state<strong class="mk jc">=</strong>True)(l, initial_state<strong class="mk jc">=</strong>[h,c])<br/>    <br/>    output <strong class="mk jc">=</strong> keras<strong class="mk jc">.</strong>layers<strong class="mk jc">.</strong>Dense(nmix<strong class="mk jc">*</strong>6 <strong class="mk jc">+</strong> 1)(l1)<br/>    model <strong class="mk jc">=</strong> Model(inp,output)<br/>    <br/>    <strong class="mk jc">return</strong> model</span></pre><h2 id="3e49" class="kx ky jb bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">损失函数</h2><pre class="lw lx ly lz gt mj mk ml mm aw mn bi"><span id="6a4b" class="kx ky jb mk b gy mo mp l mq mr"><strong class="mk jc">def</strong> <strong class="mk jc">seqloss</strong>():<br/>    <strong class="mk jc">def</strong> <strong class="mk jc">pdf</strong>(x1, x2, mu1, mu2, s1, s2,rho):<br/>        norm1 <strong class="mk jc">=</strong> tf<strong class="mk jc">.</strong>subtract(x1, mu1)<br/>        norm2 <strong class="mk jc">=</strong> tf<strong class="mk jc">.</strong>subtract(x2, mu2)<br/>        s1s2 <strong class="mk jc">=</strong> tf<strong class="mk jc">.</strong>multiply(s1, s2)<br/>        z <strong class="mk jc">=</strong> tf<strong class="mk jc">.</strong>square(tf<strong class="mk jc">.</strong>div(norm1, s1)) <strong class="mk jc">+</strong> \<br/>            tf<strong class="mk jc">.</strong>square(tf<strong class="mk jc">.</strong>div(norm2, s2)) <strong class="mk jc">-</strong> \<br/>            2 <strong class="mk jc">*</strong> tf<strong class="mk jc">.</strong>div(tf<strong class="mk jc">.</strong>multiply(rho, tf<strong class="mk jc">.</strong>multiply(norm1, norm2)), s1s2)<br/>        negRho <strong class="mk jc">=</strong> 1 <strong class="mk jc">-</strong> tf<strong class="mk jc">.</strong>square(rho)<br/>        result <strong class="mk jc">=</strong> tf<strong class="mk jc">.</strong>exp(tf<strong class="mk jc">.</strong>div(<strong class="mk jc">-</strong>z, 2 <strong class="mk jc">*</strong> negRho))<br/>        denom <strong class="mk jc">=</strong> 2 <strong class="mk jc">*</strong> np<strong class="mk jc">.</strong>pi <strong class="mk jc">*</strong> tf<strong class="mk jc">.</strong>multiply(s1s2, tf<strong class="mk jc">.</strong>sqrt(negRho))<br/>        result <strong class="mk jc">=</strong> tf<strong class="mk jc">.</strong>div(result, denom)<br/>        <strong class="mk jc">return</strong> result<br/>        <br/>    <strong class="mk jc">def</strong> <strong class="mk jc">loss</strong>(y_true, pred):<br/>    <br/>        prob <strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>sigmoid(pred[0][:,0]); pi <strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>softmax(pred[0][:,1:3])<br/>        <br/>        x <strong class="mk jc">=</strong> y_true[0][:,1]; y <strong class="mk jc">=</strong> y_true[0][:,2]; penlifts <strong class="mk jc">=</strong> y_true[0][:,0]<br/>        <br/>        m11 <strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>sigmoid(pred[0][:,3]); m12 <strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>sigmoid(pred[0][:,4])<br/>        s11<strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>exp(pred[0][:,5]); s12 <strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>exp(pred[0][:,6])<br/>        rho1 <strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>tanh(pred[0][:,7])<br/>        pdf1 <strong class="mk jc">=</strong> tf<strong class="mk jc">.</strong>maximum(tf<strong class="mk jc">.</strong>multiply(pdf(x, y, m11, m12, s11, s12, rho1),pi[:,0]), K<strong class="mk jc">.</strong>epsilon())<br/>        <br/>        <strong class="mk jc">for</strong> i <strong class="mk jc">in</strong> range(1,2):<br/>            m11 <strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>sigmoid(pred[0][:,3<strong class="mk jc">+</strong>5<strong class="mk jc">*</strong>i]); m12 <strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>sigmoid(pred[0][:,4<strong class="mk jc">+</strong>5<strong class="mk jc">*</strong>i])<br/>            s11 <strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>exp(pred[0][:,5<strong class="mk jc">+</strong>5<strong class="mk jc">*</strong>i]); s12 <strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>exp(pred[0][:,6<strong class="mk jc">+</strong>5<strong class="mk jc">*</strong>i])<br/>            rho1 <strong class="mk jc">=</strong> K<strong class="mk jc">.</strong>tanh(pred[0][:,7<strong class="mk jc">+</strong>5<strong class="mk jc">*</strong>i])<br/>            pdf1 <strong class="mk jc">+=</strong> tf<strong class="mk jc">.</strong>maximum(tf<strong class="mk jc">.</strong>multiply(pdf(x, y, m11, m12, s11, s12, rho1),pi[:,i]), K<strong class="mk jc">.</strong>epsilon())<br/><br/>        <br/>        loss1 <strong class="mk jc">=</strong> tf<strong class="mk jc">.</strong>math<strong class="mk jc">.</strong>reduce_sum(<strong class="mk jc">-</strong>tf<strong class="mk jc">.</strong>log(pdf1))<br/>        pos <strong class="mk jc">=</strong> tf<strong class="mk jc">.</strong>multiply(prob, penlifts)<br/>        neg <strong class="mk jc">=</strong> tf<strong class="mk jc">.</strong>multiply(1<strong class="mk jc">-</strong>prob, 1<strong class="mk jc">-</strong>penlifts)<br/>        loss2 <strong class="mk jc">=</strong> tf<strong class="mk jc">.</strong>math<strong class="mk jc">.</strong>reduce_mean(<strong class="mk jc">-</strong>tf<strong class="mk jc">.</strong>log(pos<strong class="mk jc">+</strong>neg))<br/>        final_loss <strong class="mk jc">=</strong> loss1<strong class="mk jc">+</strong>loss2<br/>        <br/>        <strong class="mk jc">return</strong> final_loss<br/><br/>    <strong class="mk jc">return</strong> loss</span></pre><p id="77a9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我在一个小的笔画数据集上随机训练了 2 个时期的模型，因为笔画的长度是变化的。我得到的结果如下所示:</p><figure class="lw lx ly lz gt is gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/575679d53b1cc0d8314f6ebb8ddc5374.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/0*NJ-GN9xXWuCnEHjn.png"/></div></figure><p id="9fd6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我认为，如果你增加混合、数据集和时期的数量，你会得到更好的结果。我希望我的博客是一个愉快的阅读，如果你有任何疑问或建议，请联系我。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="0e8a" class="nb ky jb bd kz nc nd ne lc nf ng nh lf ni nj nk li nl nm nn ll no np nq lo nr bi translated">参考</h1><ol class=""><li id="4ee7" class="ns nt jb ka b kb lq kf lr kj nu kn nv kr nw kv nx ny nz oa bi translated">【https://arxiv.org/abs/1308.0850 T4】</li><li id="ed11" class="ns nt jb ka b kb ob kf oc kj od kn oe kr of kv nx ny nz oa bi translated"><a class="ae kw" href="https://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.6-MixDensityNetworks.pdf" rel="noopener ugc nofollow" target="_blank">https://cedar . buffalo . edu/~ Sri Hari/CSE 574/chap 5/chap 5.6-mixdensitynetworks . pdf</a></li><li id="d0e8" class="ns nt jb ka b kb ob kf oc kj od kn oe kr of kv nx ny nz oa bi translated"><a class="ae kw" href="https://github.com/Grzego/handwriting-generation" rel="noopener ugc nofollow" target="_blank">https://github.com/Grzego/handwriting-generation</a></li></ol></div></div>    
</body>
</html>