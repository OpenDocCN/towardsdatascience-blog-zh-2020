<html>
<head>
<title>Machine Learning’s Obsession with Kids’ TV Show Characters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习对儿童电视剧角色的痴迷</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learnings-obsession-with-kids-tv-show-characters-728edfb43b3c?source=collection_archive---------27-----------------------#2020-07-16">https://towardsdatascience.com/machine-learnings-obsession-with-kids-tv-show-characters-728edfb43b3c?source=collection_archive---------27-----------------------#2020-07-16</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><div class=""/><div class=""><h2 id="1ef3" class="pw-subtitle-paragraph js iu iv bd b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj dk translated">埃尔默、伯特和玛吉(辛普森饰)不仅仅是你在成长过程中最喜欢的电视角色——他们也是机器学习和自然语言处理模型</h2></div><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi kk"><img src="../Images/aac44b8c1968a0d830ee2c295f9320fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tRiSxCGouHAutoJz"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">斯蒂芬·格雷奇在<a class="ae la" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b15f" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">巴特.埃尔莫。伯特。科米。玛吉。他们有什么共同点？</p><p id="461d" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">他们都是我们许多人年轻时看过的电视节目中受人喜爱的虚构人物。但这还不是全部——它们也都是人工智能模型。</p><p id="b2ca" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">2018 年，艾伦研究所的研究人员发表了语言模型<a class="ae la" href="https://arxiv.org/pdf/1802.05365.pdf" rel="noopener ugc nofollow" target="_blank"> ELMo </a>。主要作者马特·皮特斯说，该团队为他们的模型集思广益了许多首字母缩写词，ELMo 立即成为了一个<a class="ae la" href="https://www.theverge.com/2019/12/11/20993407/ai-language-models-muppets-sesame-street-muppetware-elmo-bert-ernie" rel="noopener ugc nofollow" target="_blank">“异想天开但令人难忘的”</a>选择。</p><p id="b48d" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">起初只是一个内部玩笑的事情已经成为一种全面发展的趋势。</p><p id="6985" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Google AI 随后推出了<a class="ae la" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT </a>，这是一个非常强大并且现在广泛使用的基于 Transformer 的语言模型。然后，更多的还有:<a class="ae la" href="https://arxiv.org/pdf/1904.09223.pdf" rel="noopener ugc nofollow" target="_blank">厄尼</a>、<a class="ae la" href="https://arxiv.org/pdf/1906.01604.pdf" rel="noopener ugc nofollow" target="_blank"> KERMIT </a>、<a class="ae la" href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener ugc nofollow" target="_blank">巴特</a>、<a class="ae la" href="https://arxiv.org/pdf/1905.12616.pdf" rel="noopener ugc nofollow" target="_blank">格罗弗</a>等。OpenAI <a class="ae la" href="https://twitter.com/jackclarkSF/status/1187824098916753408" rel="noopener ugc nofollow" target="_blank">差点</a>把 GPT-2 命名为“Snuffleupagus，或简称 Snuffy”就在上个月，脸书·艾出版了《玛吉与 T21》。</p><p id="9895" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这篇文章对所有这些模型进行了概述，当然，下面是它们的角色灵感:</p><h1 id="324f" class="lx ly iv bd lz ma mb mc md me mf mg mh kb mi kc mj ke mk kf ml kh mm ki mn mo bi translated">ELMo (2018):来自语言模型的嵌入</h1><p id="b931" class="pw-post-body-paragraph lb lc iv ld b le mp jw lg lh mq jz lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">开创这一趋势的是一种深度上下文化的单词表示方法，它能够捕捉更多关于单词的特征(语法、语义等等)。</p><p id="8aa3" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">将单词表示为向量(“单词嵌入”)的一个大挑战是，无论在什么上下文中使用，单词都将由相同的向量表示。然而，“当前”在“你的<em class="mu">当前</em>工作是什么？”对比“那是一条很强的河流<em class="mu">海流</em>”——我们不能只用一个固定的表示法来表示两个“<em class="mu">海流”</em>！</p><p id="8bd4" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，<strong class="ld iw">上下文化的</strong>单词嵌入被创建来捕捉单词在其表示中的上下文。ELMo 不是一次只阅读一个单词，而是在给每个单词分配嵌入之前阅读整个句子的上下文，这是使用双向 LSTM 完成的。</p><p id="dd8e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">ELMo 是使用语言建模的自然语言处理(NLP)研究的一大进步。对于 ELMo 的图解说明，我强烈推荐这个<a class="ae la" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">资源</a>。</p><p id="bd24" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在机器学习和 NLP 之外，Elmo 是儿童节目<em class="mu">芝麻街</em>中一个可爱的毛茸茸的红色布偶。埃尔莫喜欢惊喜、披萨和泡泡浴，并且<a class="ae la" href="https://lifestyle.howstuffworks.com/family/activities/how-elmo-works.htm" rel="noopener ugc nofollow" target="_blank">获得了金像奖最长傻笑奖</a>。它于 1980 年首次出现在银幕上。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi mv"><img src="../Images/720f29cfe1fe50a36ca4d80d9805ee3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0ZTa9aurqNv2Dysc"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">左:芝麻街的 Elmo 右:<a class="ae la" href="https://arxiv.org/pdf/1802.05365.pdf" rel="noopener ugc nofollow" target="_blank">埃尔莫</a></p></figure><h1 id="a284" class="lx ly iv bd lz ma mb mc md me mf mg mh kb mi kc mj ke mk kf ml kh mm ki mn mo bi translated">BERT (2019):变压器的双向编码器表示</h1><p id="fbe7" class="pw-post-body-paragraph lb lc iv ld b le mp jw lg lh mq jz lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">谷歌通过<a class="ae la" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">引入</a> <a class="ae la" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT </a>进一步转变了 NLP 中的预训练，这是一种新的基于 Transformer 的语言模型，首次允许<strong class="ld iw"/><strong class="ld iw">双向</strong>和无监督表示。</p><p id="2a10" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">深度双向意味着在捕捉上下文嵌入时，BERT 使用每个单词的上一个和下一个上下文来表示它。(相比较而言，ELMo 是浅双向的。)简单地根据每个单词的前一个和下一个单词来调节每个单词可能会有问题，因此 BERT 随机屏蔽一些单词，并双向调节每个单词以预测被屏蔽的单词。</p><p id="233d" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在最初的版本中，BERT 已经在问答和自然语言理解任务中取得了令人印象深刻的成果。BERT 和其他基于变压器的架构是过去一年 NLP 研究的基石。</p><p id="6d0d" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在机器学习和 NLP 之外，Bert 是<em class="mu">芝麻街</em>上一个友好的黄色角色。闲暇时，他喜欢读无聊的故事，吃燕麦粥，研究鸽子。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi mv"><img src="../Images/71e15c36da1f1c1728e64986489e65c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lp39J-JHe4KpeHuW"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">左:芝麻街的伯特；右:<a class="ae la" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">伯特</a></p></figure><h1 id="632c" class="lx ly iv bd lz ma mb mc md me mf mg mh kb mi kc mj ke mk kf ml kh mm ki mn mo bi translated">厄尼(2019):双重麻烦</h1><p id="a5a0" class="pw-post-body-paragraph lb lc iv ld b le mp jw lg lh mq jz lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">你不能没有伯特最好的伙伴恩尼——好事研究人员开发了<a class="ae la" href="https://arxiv.org/pdf/1904.09223.pdf" rel="noopener ugc nofollow" target="_blank">恩尼</a>(孙等人)、<a class="ae la" href="https://arxiv.org/pdf/1905.07129.pdf" rel="noopener ugc nofollow" target="_blank">恩尼</a>(张等人)，甚至<a class="ae la" href="https://arxiv.org/abs/1907.12412" rel="noopener ugc nofollow" target="_blank">恩尼 2.0 </a>！</p><p id="cb79" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">第一个<a class="ae la" href="https://arxiv.org/pdf/1904.09223.pdf" rel="noopener ugc nofollow" target="_blank"> ERNIE </a>(通过知识整合增强表示)提出了一个语言模型，将 BERT 的单词屏蔽策略扩展到<strong class="ld iw">实体级和短语级屏蔽</strong>。这样做，这个 ERNIE 可以在训练过程中隐式地学习短语、实体以及它们之间的关系的先验知识。</p><p id="094b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">不到两个月后，第二个<a class="ae la" href="https://arxiv.org/pdf/1905.07129.pdf" rel="noopener ugc nofollow" target="_blank"> ERNIE </a>(带有信息实体的增强语言表示)出版了。这个 ERNIE 提出了一个语言模型，它结合了<strong class="ld iw">知识图</strong>来优化获取尽可能多的信息。知识图是表示数据点和将它们链接在一起的关系的强大方法。</p><p id="475e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在机器学习和 NLP 之外，厄尼是一个麻烦制造者，他的人生使命是在<em class="mu">芝麻街上惹恼伯特。他非常喜欢他的橡胶鸭子，曾经说过一句著名的话“我听不见你说什么，我耳朵里有根香蕉！”</em></p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi mv"><img src="../Images/e140eaac0522e60051f254ffba212236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*B0KtsjMI_Lj3tRwj"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">左:芝麻街的厄尼；右:<a class="ae la" href="https://arxiv.org/pdf/1904.09223.pdf" rel="noopener ugc nofollow" target="_blank">厄尼</a></p></figure><h1 id="06aa" class="lx ly iv bd lz ma mb mc md me mf mg mh kb mi kc mj ke mk kf ml kh mm ki mn mo bi translated">KERMIT (2019):通过插入变换进行的 Kontextuell 编码器表示</h1><p id="cdf5" class="pw-post-body-paragraph lb lc iv ld b le mp jw lg lh mq jz lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">KERMIT 是一个基于插入的生成架构，它将联合分布、分解(即它的边界)和条件一起建模。研究人员发现，KERMIT 在一些特定的任务中表现出色，包括机器翻译。</p><p id="82e1" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">如果你像我一样好奇，“kontextuell”在瑞典语中是“上下文”的意思。我们知道研究人员在这个命名上是相当故意的，因为在论文的后面，他们写道“然后，<em class="mu">像它的朋友</em> ELMo (Peters 等人，2018)，BERT (Devlin 等人，2019)，和 ERNIE (Sun 等人，2019)，我们也可以使用……”</p><p id="ff4a" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在机器学习和 NLP 之外，Kermit 是一只标志性的唱歌青蛙，它已经为许多布偶作品增光添彩(<em class="mu"> Sam and Friends，芝麻街，布偶秀</em>等)。)和迷因(<a class="ae la" href="https://www.12news.com/article/news/year-in-review/here-are-the-top-memes-of-the-2010s/75-cc27ea6b-4a22-475b-b382-4d9f5547f056" rel="noopener ugc nofollow" target="_blank">但那不关我的事</a>、<a class="ae la" href="https://www.theguardian.com/technology/2016/nov/30/evil-kermit-perfect-meme-terrible-times" rel="noopener ugc nofollow" target="_blank">邪柯密特</a>等。).克米特于 1955 年首次亮相，是该榜单上年龄最大的电视角色。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi mv"><img src="../Images/a97639715d5bd05a093c84e5824ff2bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f5KdhRZuz3vLfhXb"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">左:科米蛙；右:<a class="ae la" href="https://arxiv.org/pdf/1906.01604.pdf" rel="noopener ugc nofollow" target="_blank"> KERMIT </a></p></figure><h1 id="bcbe" class="lx ly iv bd lz ma mb mc md me mf mg mh kb mi kc mj ke mk kf ml kh mm ki mn mo bi translated">BART (2019):双向和自回归变压器</h1><p id="a08f" class="pw-post-body-paragraph lb lc iv ld b le mp jw lg lh mq jz lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">脸书·艾在伯特、GPT 和之前的 NLP 预训练工作的基础上创建了<a class="ae la" href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener ugc nofollow" target="_blank"> BART </a>，这是一个用于文本生成和理解的新的预训练模型<strong class="ld iw">，它结合了双向和自动回归</strong>转换器。</p><p id="7508" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">BERT 在其双向编码器中使用掩蔽，这意味着掩蔽/丢失的单词是独立预测的。因此，BERT 不能用于文本生成。</p><p id="79a4" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">相比之下，GPT 是自回归的，这意味着它在向前的方向上从给定上下文的一组单词中预测未来的单词。结果，GPT 无法学习双向互动。</p><p id="9810" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">BART 将这些基本思想结合在一起:1)用掩码替换随机文本跨度的文档被双向编码，然后 2)用自回归解码器预测文档未被修改的可能性。</p><p id="69a3" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在机器学习和 NLP 之外，巴特是电视上最知名的年轻反叛者之一。你可以在《辛普森一家》中找到巴特和他没完没了的恶作剧电话。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi mv"><img src="../Images/6e9ace92627f00d2e097f598cd484d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MeyyeTYxwtSZJPiL"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">左图:《辛普森一家》中的巴特；右:<a class="ae la" href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener ugc nofollow" target="_blank">巴特</a></p></figure><h1 id="c3c2" class="lx ly iv bd lz ma mb mc md me mf mg mh kb mi kc mj ke mk kf ml kh mm ki mn mo bi translated">格罗弗(2019)</h1><p id="7bc3" class="pw-post-body-paragraph lb lc iv ld b le mp jw lg lh mq jz lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">近年来，NLP 领域发展非常迅速，以实现高度可信的摘要和翻译。然而，这些技术也可以用于不太积极的目的，例如人工智能生成的假新闻和宣传。</p><p id="f232" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">为了解决这个问题，研究人员创造了 GROVER，这是一个公开的生成器，可以写真实的可控假新闻。GROVER 的目的是让其他人可以针对它进行实践，以开发更好的技术来区分人工智能生成的假新闻和真实的人类撰写的新闻。当时，最好的鉴别器只能以 73%的准确率区分人工智能生成的假新闻和真实新闻。</p><p id="fedc" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">(在一个令人困惑的命名决定中，这里没有首字母缩写——它被称为 GROVER 只是因为。)</p><p id="ce47" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在机器学习和 NLP 之外，Grover 是一个毛茸茸的蓝色芝麻街人物，他喜欢帮助(或试图帮助)别人。希望格罗弗能像帮助他的朋友一样，帮助人工智能世界解决虚假信息。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi mv"><img src="../Images/cc31cea5278a51b3e4937fc34470075f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UZnUDnDxtVjZWz1c"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">左:芝麻街的格罗弗；右:<a class="ae la" href="https://arxiv.org/pdf/1905.12616.pdf" rel="noopener ugc nofollow" target="_blank">格罗弗</a></p></figure><h1 id="d12f" class="lx ly iv bd lz ma mb mc md me mf mg mh kb mi kc mj ke mk kf ml kh mm ki mn mo bi translated">MARGE (2020):多语言自动编码器，检索和生成</h1><p id="51cd" class="pw-post-body-paragraph lb lc iv ld b le mp jw lg lh mq jz lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">最近由脸书·艾、<a class="ae la" href="https://arxiv.org/abs/2006.15020" rel="noopener ugc nofollow" target="_blank">发表的 MARGE </a>是一个新的“预训练的序列到序列模型，通过无监督的多语言多文档解释目标学习。”</p><p id="f6c5" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">更简单地说，它是一个预先训练好的语言模型，通过 1) <strong class="ld iw">检索其他语言中的</strong>相关文本，2) <strong class="ld iw">通过在原始文本中寻找模式来重构</strong>原始文本来生成文本。</p><p id="ecc0" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">研究人员发现，MARGE 可以成功地执行释义、翻译、多文档摘要和信息检索任务，而无需任何微调。MARGE 在<a class="ae la" href="https://en.wikipedia.org/wiki/BLEU" rel="noopener ugc nofollow" target="_blank"> BLEU </a>(双语评估替补)上的得分高达 35.8，这是一个衡量语言翻译的指标，对于一个没有微调的模型来说，这被认为是相当高的。</p><p id="db59" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在机器学习和 NLP 之外，Marge 是《辛普森一家》中的一个虚构角色。玛吉是三个孩子(包括巴特)的母亲，她也因身材高挑、蓝色(蓝色？👀)头发。</p><p id="b53e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">(有了巴特和玛吉，也许脸书·艾非常喜欢《辛普森一家》？)</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi mv"><img src="../Images/28ba74e544649ac3ce2b63e80dd12790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RWVjeXsL3wKTPNiA"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">左图:《辛普森一家》中的玛吉；右:<a class="ae la" href="https://arxiv.org/abs/2006.15020" rel="noopener ugc nofollow" target="_blank">玛姬</a></p></figure><h1 id="7707" class="lx ly iv bd lz ma mb mc md me mf mg mh kb mi kc mj ke mk kf ml kh mm ki mn mo bi translated">最后的想法</h1><p id="9133" class="pw-post-body-paragraph lb lc iv ld b le mp jw lg lh mq jz lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">研究人员以这种有趣、无害的方式向以前的作品致敬，这很酷。也许未来最先进的机器学习模型将被命名为赫敏或祖寇——我热切期待人工智能研究人员扩展到其他小说领域的那一天。</p><p id="58c4" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">如果有其他以虚构人物命名的 AI 模型，<a class="ae la" href="https://twitter.com/catyeo18" rel="noopener ugc nofollow" target="_blank">告诉我</a>！</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ca"><img src="../Images/f290ee6201243ff65726c1902e43abb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*L0_sv08ykbDaAVjv"/></div></div></figure></div><div class="ab cl mw mx hz my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="io ip iq ir is"><p id="d678" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">感谢您的阅读！<a class="ae la" href="https://medium.com/fair-bytes" rel="noopener">订阅</a>阅读更多关于人工智能的研究、资源和问题。</p><div class="nd ne gp gr nf ng"><a href="https://medium.com/fair-bytes/how-biased-is-gpt-3-5b2b91f1177" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iw gy z fp nl fr fs nm fu fw iu bi translated">GPT-3 有多偏？</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">尽管它的表现令人印象深刻，但世界上最新的语言模型反映了性别、种族和…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">medium.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu ku ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://medium.com/fair-bytes/we-need-to-change-how-image-datasets-are-curated-b325642394df" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iw gy z fp nl fr fs nm fu fw iu bi translated">我们需要改变图像数据集的管理方式</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">为什么许多黄金标准的计算机视觉数据集，如 ImageNet，有缺陷</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">medium.com</p></div></div><div class="np l"><div class="nv l nr ns nt np nu ku ng"/></div></div></a></div><p id="2390" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">凯瑟琳·杨(Catherine Yeo)是哈佛大学计算机科学专业的本科生，她对人工智能/人工智能/自然语言处理、公平和道德以及其他相关领域感兴趣。随意提出想法或者在 Twitter 上跟她打招呼<a class="ae la" href="https://twitter.com/catherinehyeo" rel="noopener ugc nofollow" target="_blank"><em class="mu">@ catherinehyeo</em></a><em class="mu">。</em></p></div></div>    
</body>
</html>