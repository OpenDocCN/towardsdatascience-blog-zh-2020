<html>
<head>
<title>What is activation function ?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是激活功能？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-activation-function-1464a629cdca?source=collection_archive---------20-----------------------#2020-06-09">https://towardsdatascience.com/what-is-activation-function-1464a629cdca?source=collection_archive---------20-----------------------#2020-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/dbd269e421b5607d0308d5a29d27f0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACbDS2jrLif4hrTcUNA4cg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:米里亚姆·埃斯帕奇</p></figure><div class=""/><div class=""><h2 id="8392" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated"><em class="ky">神经网络最重要的部分之一</em></h2></div><p id="6c9a" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb jk"> <em class="lv">激活函数定义了给定一个输入或一组输入时一个神经元/节点的输出</em> </strong> <em class="lv">。这是对生物神经元刺激的模拟。</em></p><p id="9ecb" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">激活函数到下一层(浅层神经网络中:输入层和输出层，深层网络中到下一个隐层)的输出称为前向传播(信息传播)。它被认为是一个神经网络的非线性变换。</p><blockquote class="lw lx ly"><p id="5782" class="kz la lv lb b lc ld kk le lf lg kn lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">一个笔记本的全部代码都在这里:<a class="ae jg" href="https://github.com/Christophe-pere/Activation_functions" rel="noopener ugc nofollow" target="_blank"> GitHub </a></p></blockquote><h1 id="160d" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">有一个常用的激活功能列表:</h1><ul class=""><li id="71f6" class="mu mv jj lb b lc mw lf mx li my lm mz lq na lu nb nc nd ne bi translated"><strong class="lb jk">二进制</strong></li><li id="76e6" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb jk">线性</strong></li><li id="51a0" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb jk">乙状结肠</strong></li><li id="375d" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">谭</li><li id="db3a" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb jk"> ReLU </strong></li><li id="da72" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb jk">泄漏的 ReLU (LReLU) </strong></li><li id="4b3e" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb jk">参数 ReLU (PReLU) </strong></li><li id="5542" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb jk">指数线性单位(eLU) </strong></li><li id="dc0c" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb jk"> ReLU-6 </strong></li><li id="17e4" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb jk"> Softplus </strong></li><li id="b7e4" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb jk">软设计</strong></li><li id="17ee" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb jk"> Softmax </strong></li><li id="c175" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb jk">唰</strong></li></ul><h2 id="e88d" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated"><em class="ky">二进制</em></h2><p id="9b81" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">二元激活函数是最简单的。它基于二进制分类器，如果值为负，输出为 0，否则为 1。把这个激活函数看作二元分类中的一个阈值。</p><p id="e855" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">二元激活函数的代码是:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="b68c" class="nk md jj oe b gy oi oj l ok ol">def binary_active_function(x):<br/>    return 0 if x &lt; 0 else 1</span></pre><p id="077b" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个函数的输出是什么？</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="4c95" class="nk md jj oe b gy oi oj l ok ol">for i in [-5, -3, -1, 0, 2, 5]:<br/>    print(binary_active_function(i))</span><span id="4fa5" class="nk md jj oe b gy om oj l ok ol">output:<br/>    0<br/>    0<br/>    0<br/>    1<br/>    1<br/>    1</span></pre><p id="615b" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者视觉上:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/1a36a44202526a3b21d5eb159eacef6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*eNlGynI-EhkEOdJuGISZIg.jpeg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">二元激活函数</p></figure><p id="a80b" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优点:</p><ul class=""><li id="0fd7" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">二元分类</li></ul><p id="1048" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缺点:</p><ul class=""><li id="4b50" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">在多标签分类中不起作用</li><li id="dbfc" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">梯度计算的导数始终为 0，因此无法更新权重</li></ul><h2 id="be76" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">线性激活函数</h2><p id="e29c" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">二元函数之后的下一步是用线性函数代替阶跃。输出与输入成正比。</p><p id="54d0" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对应的代码是:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="91e3" class="nk md jj oe b gy oi oj l ok ol">def linear_active_function(a, x):<br/>    return a*x</span></pre><p id="46d9" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以针对不同的<em class="lv">a</em>值来计算它:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="55dc" class="nk md jj oe b gy oi oj l ok ol">$ x = numpy.linspace(-10, 10, 5000)<br/>$ y_1 = [linear_active_function(1, i) for i in x] # a = 1<br/>$ y_2 = [linear_active_function(2, i) for i in x] # a = 2<br/>$ y_1<br/>&gt; [-10.0, -9.9, -9.8, -9.7, ..., 9.7, 9.8, 9.9, 10.0]</span></pre><p id="a6a7" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们绘制 a = 1、2、4 和 10 的结果:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/6f957fd377949fcc73abb14db6d42ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*KyDwKyFLqhYPHYO0RyBkYA.jpeg"/></div></figure><p id="25bb" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优点:</p><ul class=""><li id="d232" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">二元和多元分类</li><li id="a326" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">高度可解释性</li></ul><p id="2022" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缺点:</p><ul class=""><li id="765d" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">导数对应于"<em class="lv"> a </em>"，因此在后推过程中权重和偏置的更新将是恒定的。</li><li id="9586" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">如果梯度总是相同，则效率不高。</li></ul><h2 id="9a6b" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">乙状结肠的</h2><p id="c387" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">Sigmoid 是使用最多的激活功能，带有<em class="lv"> ReLU </em>和<em class="lv"> tanh </em>。这是一个非线性激活函数，也称为<em class="lv">逻辑函数</em>。该激活函数输出在 0 和 1 之间变化。神经元的所有输出都将是正的。</p><p id="ea60" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相应的代码如下:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="66e9" class="nk md jj oe b gy oi oj l ok ol">def sigmoid_active_function(x):<br/>    return 1./(1+numpy.exp(-x))</span></pre><p id="5dd3" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快速计算:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="ca10" class="nk md jj oe b gy oi oj l ok ol">$ x = numpy.linspace(-10, 10, 5000)<br/>$ y = [sigmoid_active_function(i) for i in x] <br/>$ y<br/>&gt; [4.5397868702434395e-05, 4.5854103946941324e-05, ... , 0.9999532196250409, 0.9999536850759906, 0.9999541458960531]</span></pre><p id="c62b" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们画出结果:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/9d1c65205a9ffb790a4738798b70bba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*AwaxfGiult4hmOuUzwWELw.jpeg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Sigmoid 激活函数</p></figure><p id="71b2" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缺点:</p><ul class=""><li id="716a" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">这个函数的问题是每个神经元的输出会饱和。大于 1 的值表示为 1，小于 0 的值表示为 0。</li><li id="7c05" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">sigmoid 函数的最佳灵敏度在中心点(0，0.5)附近。</li><li id="116b" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">在饱和期间出现了一个大问题，算法在这个位置期间无法学习(这是<em class="lv">消失梯度问题</em>的来源，对应于梯度中没有方向)。</li></ul><h2 id="a5ca" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">双曲正切</h2><p id="0542" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">正切双曲函数(tanh)与 sigmoï函数的形式相似。Tanh 在 0 中是对称的，值在-1 和 1 的范围内。作为乙状结肠，它们在中心点(0，0)非常敏感，但是它们对于非常大的数(正和负)饱和。这种对称性使它们比 sigmoid 函数更好。</p><p id="bfe9" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">应用双曲正切函数的相应代码是:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="b83c" class="nk md jj oe b gy oi oj l ok ol">def tanh_active_function(x):<br/>    return 2*sigmoid_active_function(2*x)-1</span></pre><p id="b956" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算<em class="lv"> y </em>值:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="d958" class="nk md jj oe b gy oi oj l ok ol">$ x = numpy.linspace(-10, 10, 5000)<br/>$ y = [tanh_active_function(i) for i in x] <br/>$ y<br/>&gt; [-0.9999999958776927, -0.9999999957944167, ... , 0.9999999956227836, 0.9999999957094583, 0.9999999957944166]</span></pre><p id="033e" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以及相应的结果:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/6e343abebff423d7c5593b45e0589a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*KamlOIPheb0Y_6IeRQcogA.jpeg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">tanh 激活函数</p></figure><p id="dcea" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优点:</p><ul class=""><li id="047c" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">范围在-1 和 1 之间</li><li id="468e" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">梯度比 sigmoid 更强(导数更陡)</li></ul><p id="3417" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缺点:</p><ul class=""><li id="2e66" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">像 sigmoid 一样，tanh 也有一个消失梯度问题</li><li id="3632" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">浸透</li></ul><h2 id="56d0" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">热卢</h2><p id="48d2" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">开发了<em class="lv">整流线性单元</em>以避免大正数的饱和。非线性允许保存和学习数据中的模式，线性部分(&gt;0-也称为分段线性函数)使它们易于解释。</p><p id="7522" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的函数显示了如何实现 ReLU 函数:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="eb43" class="nk md jj oe b gy oi oj l ok ol">def relu_active_function(x):<br/>    return numpy.array([0, x]).max()</span></pre><p id="18bd" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> y </em>计算:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="2313" class="nk md jj oe b gy oi oj l ok ol">$ x = numpy.linspace(-10, 10, 5000)<br/>$ y = [relu_active_function(i) for i in x] <br/>$ y<br/>&gt; [0.0, 0.0, ... , 9.97, 9.98, 9.99]</span></pre><p id="8154" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果是:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/ed56440e5a51d40c01c35731eb10ced8.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*jsv6AjBZUdFsGvLLb2ubeA.jpeg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">ReLU 激活功能</p></figure><p id="d00b" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优点:</p><ul class=""><li id="7038" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">易于实施且速度非常快</li><li id="d7fb" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">真 0 值</li><li id="c009" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">当激活函数是线性时，优化是容易的</li><li id="0448" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">最常用于神经网络生态系统</li></ul><p id="5596" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缺点:</p><ul class=""><li id="cc0e" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">当 x = 0 时，函数不可微。这个点的梯度下降无法计算，但实际上这没有影响。线性部分对应于值为 1 的斜率，负部分等于零。</li><li id="bf7e" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">“垂死的 ReLU 问题”:如果输出为 0，则对应于神经元的不活跃部分。当神经元不活动时没有梯度，因此如果大部分神经元没有被激活，则可能导致模型的较差性能</li><li id="ffe4" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">不适用于 RNN 类算法(RNN、LSTM、GRU)</li></ul><h2 id="c23e" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">泄漏 ReLU</h2><p id="71de" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">该激活功能是 ReLU 激活功能的修改，以避免“死亡问题”。该函数返回线性斜率，其中 a=0.01，这允许用梯度流保持激活神经元。</p><p id="279d" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参见下面的代码:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="5392" class="nk md jj oe b gy oi oj l ok ol">def leaky_relu_active_function(x):<br/>    return 0.01*x if x &lt; 0 else x</span></pre><p id="149a" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算 y 轴来绘制结果:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="ac6f" class="nk md jj oe b gy oi oj l ok ol">$ x = numpy.linspace(-10, 10, 5000)<br/>$ y = [leaky_relu_active_function(i) for i in x] <br/>$ y<br/>&gt; [-0.1, -0.0999, ... , 9.97, 9.98, 9.99]</span></pre><p id="1655" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制结果图:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/7e9904e64db041ac369b593f011ca1b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*RFZS84GNgN1Uj6Ni8xI8jQ.jpeg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">泄漏 ReLU 激活功能</p></figure><p id="ae52" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优点:</p><ul class=""><li id="23c2" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">纠正“将死的 ReLU 问题”</li><li id="c700" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">零件 y=x 的 ReLU 激活功能的相同组成部分</li></ul><h2 id="5e07" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">参数 ReLU</h2><p id="03a9" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">在泄漏 ReLU 之后，创建了另一个激活函数来避免“死亡 ReLU 问题”，即参数化的 ReLU。系数 a 不锁定在 0.01(泄漏 ReLU)，但可以自由估计。它是 ReLU 的推广，算法学习整流器参数。</p><p id="423e" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代码:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="db8e" class="nk md jj oe b gy oi oj l ok ol">def parametric_relu_active_function(a, x):<br/>    return a*x if x &lt; 0 else x</span></pre><p id="3360" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算不同<em class="lv"> a </em>值的结果:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="caf0" class="nk md jj oe b gy oi oj l ok ol">$ x   = numpy.linspace(-10, 10, 5000)<br/>$ y_1 = [parametric_relu_active_function(0.25, i) for i in x] <br/>$ y_2 = [parametric_relu_active_function(0.5, i) for i in x]<br/>$ y_3 = [parametric_relu_active_function(0.75, i) for i in x]<br/>$ y_4 = [parametric_relu_active_function(1, i) for i in x]<br/>$ y_1<br/>&gt; [-2.5, -2.4975, ... , 9.97, 9.98, 9.99]</span></pre><p id="5a88" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制 a = 0.25，0.5，0.75，1:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/6b62645009736491dfab1851e57383c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*ZDCVAOhacOE_o1qINnp0-Q.jpeg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">参数 ReLU 激活函数</p></figure><p id="2e7a" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果 a = 0，参数 ReLU 相当于 ReLU 激活函数。如果 a=0.01，参数 ReLU 对应于泄漏 ReLU。</p><p id="503c" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优点:</p><ul class=""><li id="6fc1" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">推广 ReLU 激活函数</li><li id="8fce" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">避免“垂死的 ReLU 问题”</li><li id="16cf" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">神经网络学习参数“<em class="lv"> a </em></li></ul><h2 id="0177" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">指数线性单位</h2><p id="b9c7" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">eLU 是 ReLU 函数的另一种变体。函数的负部分由缓慢平滑的指数函数处理。</p><p id="3921" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相应的功能:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="b65e" class="nk md jj oe b gy oi oj l ok ol">def elu_active_function(a, x):<br/>    return a*(numpy.exp(x)-1) if x &lt; 0 else x</span></pre><p id="e9dd" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> y </em>计算:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="8e57" class="nk md jj oe b gy oi oj l ok ol">$ x   = numpy.linspace(-10, 10, 5000)<br/>$ y_1 = [elu_active_function(0.1, i) for i in x] <br/>$ y_2 = [elu_active_function(1, i) for i in x]<br/>$ y_3 = [elu_active_function(2, i) for i in x]<br/>$ y_4 = [elu_active_function(5, i) for i in x]<br/>$ y_1<br/>&gt; [-0.09999546000702375, -0.09999541437933579, ... , 9.97, 9.98, 9.99]</span></pre><p id="b5fc" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制 a = 0.1，1，2，4 的结果:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/00a71737c4afa8e51b29f98bfe18344e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*MkQMHKIjTJP9-21S8QCKjg.jpeg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">eLU 激活功能</p></figure><p id="2ff0" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优点:</p><ul class=""><li id="9b39" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">ELU 慢慢变得平滑，直到其输出等于-α，而 RELU 急剧平滑。</li><li id="8e0a" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">ELU 是 ReLU 的有力替代者。</li><li id="474b" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">与 ReLU 不同，eLU 可以产生负输出。</li></ul><p id="0f90" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缺点:</p><ul class=""><li id="358f" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">对于 x &gt; 0，可以用输出范围[0，inf]吹爆激活。</li></ul><h2 id="323b" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">ReLU-6</h2><p id="c2db" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">ReLU 函数的另一个变体是 ReLU-6，6 是手动固定的任意参数。优点是将大正数的输出整形为 6 值。</p><p id="d9fb" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相应的代码:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="b7a9" class="nk md jj oe b gy oi oj l ok ol">def relu_6_active_function(x):<br/>    return numpy.array([0, x]).max() if x&lt;6 else 6</span></pre><p id="d96d" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">y 计算:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="8e82" class="nk md jj oe b gy oi oj l ok ol">$ y = [relu_6_active_function(i) for i in x]</span></pre><p id="0127" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制结果图:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/9bcf8d53b77253081dba59374c4d626b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eaqZALcLhzewAjXw4IE84Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">ReLU-6 激活功能</p></figure><h2 id="5549" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">Softplus</h2><p id="5521" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">softplus 激活功能是 sigmoid 和 tanh 功能的替代功能。此功能有限制(上限、下限)，但 softplus 在范围(0，+inf)内。</p><p id="5013" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相应的代码:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="ffec" class="nk md jj oe b gy oi oj l ok ol">def softplus_active_function(x):<br/>    return math.log(1+numpy.exp(x))</span></pre><p id="32f7" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> y </em>计算:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="0942" class="nk md jj oe b gy oi oj l ok ol">$ y = [softplus_active_function(i) for i in x]</span></pre><p id="fa9e" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制结果图:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/16ee65ceb21cf1b55a6dea5a60a6f08d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pw9OIpgpp6nSEOimr3jFZA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Softplus 激活功能</p></figure><h2 id="7908" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">软设计</h2><p id="128a" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">这种激活函数是 tanh 的一种变体，但在实践中并不常用。tanh 和 softsign 函数密切相关，tanh 指数收敛，而 softsign 多项式收敛。</p><p id="420e" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相应的代码:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="9f4c" class="nk md jj oe b gy oi oj l ok ol">def softsign_active_function(x):<br/>    return x / (1 + abs(x) )</span><span id="5c84" class="nk md jj oe b gy om oj l ok ol">$ y = [softsign_active_function(i) for i in x]</span></pre><p id="ec18" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制结果图:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/ed0f61840f6fa1324042e6660ab88f69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h2az0fkyPeuj7FrfrqXDFw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">软设计激活功能</p></figure><h2 id="40ca" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">Softmax</h2><p id="c4d9" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">softmax 激活函数不同于其他函数，因为它计算概率分布。输出的总和等于 1。</p><p id="8645" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相应的代码:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="93cf" class="nk md jj oe b gy oi oj l ok ol">def softmax_active_function(x):<br/>    return numpy.exp(x)/numpy.sum(numpy.exp(x))</span></pre><p id="dc91" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算输出是不同的，因为它是考虑到指数和的概率分布，函数需要所有的<em class="lv"> x </em>点来计算输出<em class="lv"> y </em>。</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="b1db" class="nk md jj oe b gy oi oj l ok ol">$ x = [0.8, 1.2, 2.4, 4.6]<br/>$ y = softmax_active_function(x)<br/>$ y<br/>&gt; [0.01917691, 0.02860859, 0.09498386, 0.85723064]<br/>$ numpy.sum(y)<br/>&gt; 1.0</span></pre><h2 id="6246" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">嗖嗖</h2><p id="d0f9" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">Swish 是较新的激活功能，由谷歌在 2017 年发布，它提高了 ReLU 在更深模型上的性能。这个函数是 sigmoid 函数的变体，因为它可以表示为:x*sigmoid(x)。</p><blockquote class="lw lx ly"><p id="3956" class="kz la lv lb b lc ld kk le lf lg kn lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">Swish 具有在零点的单侧有界性、光滑性和非单调性，这些性质可能在 Swish 和类似激活函数的观察功效中起作用。<br/> <a class="ae jg" href="https://arxiv.org/pdf/1710.05941v1.pdf" rel="noopener ugc nofollow" target="_blank"> SWISH:一个自门控激活函数，Prajit Ramachandran，Barret Zoph，Quoc V. Le，2017 </a></p></blockquote><p id="fc48" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相应的代码:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="251c" class="nk md jj oe b gy oi oj l ok ol">def swish_active_function(x):<br/>    return x/(1+numpy.exp(-x))</span></pre><p id="5036" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="2c4b" class="nk md jj oe b gy oi oj l ok ol">def swish_active_function(x):<br/>    return x*sigmoid_active_function(x)</span></pre><p id="42ee" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算值:</p><pre class="nz oa ob oc gt od oe of og aw oh bi"><span id="d9f1" class="nk md jj oe b gy oi oj l ok ol">$ x = numpy.linspace(-10, 10, 5000)<br/>$ y = [swish_active_function(i) for i in x]<br/>$ y<br/>&gt; [-0.0004539786870243439, -0.0004967044303692657, ..., 9.699405586525717, 9.799456604457717, 9.89950329556963]</span></pre><p id="43db" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制结果图:</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/11e7f04966a58cbc1c6833b6ef5b061e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*kOIP5Ilqa9n8c7sSo2wocw.jpeg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Swish 激活功能</p></figure><p id="0725" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优点:</p><ul class=""><li id="17c1" class="mu mv jj lb b lc ld lf lg li oo lm op lq oq lu nb nc nd ne bi translated">与 ReLU 相比，每个点都是可微的</li></ul><h2 id="6ad7" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">结论</h2><p id="2d4a" class="pw-post-body-paragraph kz la jj lb b lc mw kk le lf mx kn lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">这里展示了一些激活函数(最流行的)及其代码和表示。希望这种视觉化可以让每个人理解神经元的输出，并决定哪种功能更适合他们处理的问题。</p><p id="e04d" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一个图对应于一个图形中的激活功能堆栈。</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/c3dcdbe7aabc72b6124ff7d699c7ecc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HkFYxgsPBofMpy0FjkF8FA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">所有激活功能(无 softmax)</p></figure><h2 id="0b0e" class="nk md jj bd me nl nm dn mi nn no dp mm li np nq mo lm nr ns mq lq nt nu ms nv bi translated">参考</h2><ul class=""><li id="288f" class="mu mv jj lb b lc mw lf mx li my lm mz lq na lu nb nc nd ne bi translated"><a class="ae jg" href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html" rel="noopener ugc nofollow" target="_blank">https://medium.com/r/?URL = https % 3A % 2F % 2f ml-cheat sheet . readthedocs . io % 2 fen % 2 flatest % 2 factivation _ functions . html</a></li><li id="08bb" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/rectified-linear-activation-function-for-deep-learning-neural-networks/</a></li><li id="b587" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">伊恩·古德菲勒，约书亚·本吉奥和亚伦·库维尔，深度学习(2016 年)，麻省理工学院</li><li id="b99d" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://ieeexplore.ieee.org/author/37397823100" rel="noopener ugc nofollow" target="_blank"> Kevin Jarrett </a>等，<a class="ae jg" href="https://ieeexplore.ieee.org/document/5459469" rel="noopener ugc nofollow" target="_blank">物体识别最好的多阶段架构是什么？</a> (2009)，电气和电子工程师协会</li><li id="e650" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">Vinod Nair 和 Geoffrey Hinton，<a class="ae jg" href="https://dl.acm.org/citation.cfm?id=3104425" rel="noopener ugc nofollow" target="_blank">整流线性单元改进受限玻尔兹曼机器</a> (2010)，美国计算机学会</li><li id="67aa" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">Xavier Glorot，Antoine Bordes，Yoshua Bengio <strong class="lb jk"> <em class="lv">，</em> </strong> <a class="ae jg" href="http://proceedings.mlr.press/v15/glorot11a" rel="noopener ugc nofollow" target="_blank">深度稀疏整流器神经网络</a> (2011)，机器学习研究论文集</li><li id="ec6e" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">Andrew L. Maas、Awni Y. Hannun 和 Andrew Y. Ng，<a class="ae jg" href="http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf" rel="noopener ugc nofollow" target="_blank">整流器非线性改善神经网络声学模型</a> (2013)，斯坦福</li><li id="fe38" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">泽维尔·格洛特，约舒阿·本吉奥，<strong class="lb jk"> <em class="lv"> </em> </strong> <a class="ae jg" href="http://proceedings.mlr.press/v9/glorot10a.html" rel="noopener ugc nofollow" target="_blank">理解训练深度前馈神经网络的难度</a> (2010)，《机器学习研究论文集》</li><li id="d330" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K" rel="noopener ugc nofollow" target="_blank">何</a>等，<a class="ae jg" href="https://arxiv.org/abs/1502.01852" rel="noopener ugc nofollow" target="_blank">深入研究整流器:在 ImageNet 分类上超越人类水平</a> (2015).</li><li id="ad81" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">激活功能，维基百科</a></li><li id="352a" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失渐变问题，维基百科</a></li><li id="e9f2" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank">整流器(神经网络)，维基百科</a></li><li id="8d92" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Piecewise_linear_function" rel="noopener ugc nofollow" target="_blank">分段线性函数，维基</a></li><li id="d7d2" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7" rel="noopener">https://medium . com/@丹青/a-practical-guide-to-relu-b 83 ca 804 f1 f 7</a></li><li id="7124" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shang%2C+W" rel="noopener ugc nofollow" target="_blank">温岭尚</a>、<a class="ae jg" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sohn%2C+K" rel="noopener ugc nofollow" target="_blank"> Kihyuk Sohn </a>、<a class="ae jg" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Almeida%2C+D" rel="noopener ugc nofollow" target="_blank">迪奥戈阿尔梅达</a>、<a class="ae jg" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee%2C+H" rel="noopener ugc nofollow" target="_blank"> Honglak Lee </a>、<a class="ae jg" href="https://arxiv.org/abs/1603.05201" rel="noopener ugc nofollow" target="_blank">通过级联整流线性单元理解和改进卷积神经网络</a> (2016)、</li><li id="aeaf" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> Softmax 函数，维基百科</a></li><li id="13f2" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://medium.com/@himanshuxd/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e" rel="noopener">https://medium . com/@ himanshuxd/activation-functions-sigmoid-relu-leaky-relu-and-soft max-basics-for-neural-networks-and-deep-8d 9c 70 eed 91 e</a></li><li id="e2b0" class="mu mv jj lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><a class="ae jg" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ramachandran%2C+P" rel="noopener ugc nofollow" target="_blank">普拉吉特·拉玛钱德朗</a>，<a class="ae jg" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zoph%2C+B" rel="noopener ugc nofollow" target="_blank">巴雷特·佐夫</a>，<a class="ae jg" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Le%2C+Q+V" rel="noopener ugc nofollow" target="_blank">阔克诉勒</a>，<a class="ae jg" href="https://arxiv.org/abs/1710.05941" rel="noopener ugc nofollow" target="_blank">寻找激活函数</a> (2017)，arxiv.org</li></ul></div></div>    
</body>
</html>