<html>
<head>
<title>Modelling tabular data with Neural Networks and Transfer Learning! How and Why?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用神经网络和迁移学习对表格数据建模！怎么做，为什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fast-and-accurate-learning-with-transfer-learning-on-tabular-data-how-and-why-dfe4e752bb2d?source=collection_archive---------13-----------------------#2020-03-25">https://towardsdatascience.com/fast-and-accurate-learning-with-transfer-learning-on-tabular-data-how-and-why-dfe4e752bb2d?source=collection_archive---------13-----------------------#2020-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ff7097ee8ae8437aa0ae4f7effd572d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jJSdp1xVHN6fTHlJI12BSQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">SuperTML(<a class="ae jg" href="https://arxiv.org/abs/1903.06246" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1903.06246</a>)</p></figure><div class=""/><p id="f0f3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">我们如何微调自然图像的分类器来执行表格数据的机器学习任务？答案叫SuperTML。</em></p><h1 id="30b4" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">介绍</h1><p id="4167" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">一般来说，我们可以将我们的数据分为<strong class="ki jk">非结构化</strong>数据(那些可以用不统一的格式维护的数据，如图像和文本)和<strong class="ki jk">结构化</strong>数据(常见的表格)。在第一类中，大部分赢家是深度学习模型(CNN、RNNs等)。然而，在后一种情况下，基于提升树的算法(如XGBoost，LightGBM，CatBoost)占据主导地位。</p><p id="0da9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，当数据科学家处理表格数据时，他们花费大约60%-80%的时间进行数据预处理步骤(清理数据、探索数据分析(EDA)、可视化图形等)。相比之下，在非结构化数据的任务中，例如图像分类，我们只需<strong class="ki jk">将我们的图像从它们离开的离散空间去量化到连续空间，以便能够执行反向传播。神经网络的另一个主要优势是执行迁移学习的能力——我们不是从随机初始化开始，而是对数百万数据使用预先训练的模型，希望(主要)第一层卷积已经捕获了数据的一些重要的一般概念。</strong></p><p id="2157" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">问题是:是否有另一种新颖的方式来制作表格，即<br/>更容易实现，<br/>帮助我们节省一些时间，<br/>归档很好的结果？</p><blockquote class="mi"><p id="8bb6" class="mj mk jj bd ml mm mn mo mp mq mr ld dk translated">完整的代码可以在<a class="ae jg" href="https://github.com/ioangatop/super_tml" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p></blockquote><h1 id="32e2" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq ms ls lt lu mt lw lx ly mu ma mb mc bi translated">方法:SuperTML</h1><p id="070a" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">受最近NLP研究的启发，<strong class="ki jk">超级字符</strong>方法的<strong class="ki jk">二维嵌入</strong>能够在大型数据集基准上实现最先进的结果，<em class="le"> Sun等人。艾尔。(</em>【https://arxiv.org/abs/1903.06246】<em class="le">)</em>借用这个概念来解决表格机器学习(TML)的问题！</p><p id="8045" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个被称为SuperTML的想法既超级简单又疯狂；它由两个步骤组成:</p><ul class=""><li id="ecc0" class="mv mw jj ki b kj kk kn ko kr mx kv my kz mz ld na nb nc nd bi translated">创建二维嵌入；<br/>将表格数据中的特征投影到生成的图像上</li></ul><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ne"><img src="../Images/7bdb3ecd71b3a5f78d46cd5a06f943b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CzUdmT81rcYl4UsrGOFVIQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">将表格数据投影到图像上。(<a class="ae jg" href="https://arxiv.org/abs/1903.06246" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1903.06246</a>)</p></figure><ul class=""><li id="2ce7" class="mv mw jj ki b kj kk kn ko kr mx kv my kz mz ld na nb nc nd bi translated">使用<strong class="ki jk">预先训练的CNN </strong>模型(在ImageNet上？！)对生成的SuperTML图像进行微调。</li></ul><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/a807d7ba3b47688ae9e8475ba8c1a0e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QcDtLjgtq41kk3LTb3fP4g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">深度神经网络分类器在ImageNet上的性能。(<a class="ae jg" href="https://www.eejournal.com/article/neural-net-inference-benchmarks/" rel="noopener ugc nofollow" target="_blank">https://www . ee journal . com/article/neural-net-inference-benchmarks/</a>)</p></figure><p id="c95c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">也许这个过程中最疯狂的部分是，图像(或二维嵌入)现在具有以下属性:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/2a7de9cb6d1a7eb553b6e53db2c5a97b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9R4ZtTECaDAGAdv3sjZPqg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">SuperTML数据属性。(【https://arxiv.org/abs/1903.06246】T2</p></figure><ul class=""><li id="bc34" class="mv mw jj ki b kj kk kn ko kr mx kv my kz mz ld na nb nc nd bi translated">如果某些特性比其他特性更重要(先验知识)，只需<em class="le">增加数量的大小！</em></li><li id="7766" class="mv mw jj ki b kj nl kn nm kr nn kv no kz np ld na nb nc nd bi translated">缺失数据？只是用“？”替换它们(!！)</li><li id="bd58" class="mv mw jj ki b kj nl kn nm kr nn kv no kz np ld na nb nc nd bi translated">分类特征？就按原样(作为字符串)放好了</li></ul><blockquote class="mi"><p id="8ea2" class="mj mk jj bd ml mm nq nr ns nt nu ld dk translated">重述:<br/><strong class="ak">核心思想</strong>:表格数据可以嵌入二维矩阵(一个图像)<br/><strong class="ak">问题:</strong>这种方法行得通吗？如果是，为什么？！</p></blockquote><p id="4f10" class="pw-post-body-paragraph kg kh jj ki b kj nv kl km kn nw kp kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">很多读者大概和我有一样的反应:“这不可能行得通！这工作太疯狂了！”。并为实验和评估部分创建了一个不错的事务</p><h1 id="1296" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">实验</h1><p id="29d1" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">在本节中，除了论文中介绍的结果，我将分享我的实现结果(代码在此处<a class="ae jg" href="https://github.com/ioangatop/super_tml" rel="noopener ugc nofollow" target="_blank">可用</a>)。</p><p id="1d2d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将探索3个数据集，两个非常小(DNN模型容易过度拟合)和一个大的。下表显示了它们的详细信息:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/faa6283a42bb9aee49d85c726f9ce196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AJ7AiRN2_zk0qtCcEBePVg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">(<a class="ae jg" href="https://arxiv.org/abs/1903.06246" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1903.06246</a>)</p></figure><p id="7e35" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下内容表明了神经网络适应任何给定任务的能力，并且非常疯狂:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/0365ab9d81f8e3b6595e7fed05e3fa88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DHDetiVVAoLuD6s5-oDp_g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">(<a class="ae jg" href="https://arxiv.org/abs/1903.06246" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1903.06246</a>)</p></figure><p id="d8b3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那些仍然没有被打动的人，这篇论文展示了一个非常具有挑战性的任务的结果，希格斯玻色子挑战。30个特征的数据集；25，000个训练/ 55，000个测试样本不足以阻止SuperTML并允许它大幅度攀升到顶端(更多详细信息，请参考论文)！</p><blockquote class="oc od oe"><p id="6b38" class="kg kh le ki b kj kk kl km kn ko kp kq of ks kt ku og kw kx ky oh la lb lc ld im bi translated">一个超级琐碎的想法，非常容易实现，以0.170的优势超过了kaggle竞赛(需要大量的功能工程)的获胜者！就凭一张图片！</p></blockquote><h1 id="ed57" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">为什么会这样？？</h1><p id="e5ed" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated"><em class="le">作者的意见来了...</em></p><p id="c7a8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">似乎该算法学习特征(数字、文本)的视觉表示，并且<strong class="ki jk">学习</strong>与来自另一个样本的它们的类似物进行比较。</p><p id="b9ac" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它知道4接近5，但知道数字之间的相对距离；即使对于标准算法，4.5和4.7的距离与4.1和4.3具有相同的值(或权重)，该算法通过数据学习它们的关系，现在我们也许可以认为这个距离是<em class="le">学习的</em>(我们可以认为该算法学习自己的算法)！</p><h1 id="46ee" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">概观</h1><p id="4f47" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated"><strong class="ki jk">优点<br/></strong>简单的想法易于使用和操作<br/>无需数据标准化，无需分类特征特殊处理无需通过网格搜索进行昂贵的微调<br/>利用最好的CNN分类器<br/>无需在小数据集上过度拟合</p><p id="df6d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">缺点:<br/></strong>确保图像上的特征不重叠<br/>数值在数字的形状背后有一些隐藏的关系，比如6.01和5.999(到目前为止确实阻止了我们！)</p><h1 id="c81d" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">最后的问题</h1><p id="377f" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">所以，</p><blockquote class="mi"><p id="648e" class="mj mk jj bd ml mm mn mo mp mq mr ld dk translated"><strong class="ak">我们真的需要表格数据中的数值吗</strong></p><p id="d691" class="mj mk jj bd ml mm mn mo mp mq mr ld dk translated">运筹学</p><p id="a4f0" class="mj mk jj bd ml mm mn mo mp mq mr ld dk translated"><strong class="ak">特性的关系就够了？</strong>🤔</p></blockquote><p id="3d81" class="pw-post-body-paragraph kg kh jj ki b kj nv kl km kn nw kp kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">如前所述，完整的代码可以在这里找到<a class="ae jg" href="https://github.com/ioangatop/super_tml" rel="noopener ugc nofollow" target="_blank">。你可以随意摆弄它，尝试更多疯狂的想法！</a></p><p id="23d5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下次再见，保重！</p><p id="1235" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">论文:<a class="ae jg" href="https://arxiv.org/abs/1903.06246" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1903.06246</a>T22】代号:<a class="ae jg" href="https://github.com/ioangatop/super_tml" rel="noopener ugc nofollow" target="_blank">https://github.com/ioangatop/super_tml</a></p></div></div>    
</body>
</html>