<html>
<head>
<title>MachineRay: Using AI to Create Abstract Art</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MachineRay:用人工智能创造抽象艺术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machineray-using-ai-to-create-abstract-art-39829438076a?source=collection_archive---------8-----------------------#2020-08-03">https://towardsdatascience.com/machineray-using-ai-to-create-abstract-art-39829438076a?source=collection_archive---------8-----------------------#2020-08-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="de72" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我如何使用公共领域绘画训练 GAN</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4ba93a088ad8174676ad3bb3ab3057c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IVvYgMRVZdugiUweOo2e3Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">机器阵列的样本输出</strong></p></figure><p id="a068" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在过去的三个月里，我一直在探索人工智能(AI)和机器学习(ML)的最新技术来创作抽象艺术。在我的调查中，我了解到创作抽象绘画需要三样东西:(A)源图像，(B)ML 模型，以及(C)在高端 GPU 上训练模型的大量时间。在我讨论我的工作之前，让我们先来看看一些先前的研究。</p><p id="76ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我关于人工智能如何用于创造性努力的系列文章的第一部分。第二部分是关于如何使用 ML 为新的故事生成情节，这里有<a class="ae lv" rel="noopener" target="_blank" href="/got-writers-block-it-s-plotjam-to-the-rescue-e555db9f3272">的</a>。</p><h1 id="d08d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">背景</h1><h2 id="92df" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">人工神经网络</h2><p id="c482" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">早在 1943 年，沃伦麦卡洛克和沃尔特皮茨就为神经网络(NNs)创建了一个计算模型[1]。他们的工作导致了对大脑中生物过程和人工智能中神经网络使用的研究。理查德·纳吉菲在这篇<a class="ae lv" rel="noopener" target="_blank" href="/the-differences-between-artificial-and-biological-neural-networks-a8b46db828b7">文章</a>中讨论了人工神经网络(ann)和生物大脑之间的差异。他描述了一个恰当的类比，我将在这里总结一下:<strong class="lb iu">神经对于大脑就像飞机对于鸟类一样</strong>。尽管这些技术的发展受到了生物学的启发，但是实际的实现是非常不同的！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/db57c193e3424e675ca9de94ed9552a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Eg6r1r2-FKFyIfu4jDb_Bw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">视觉类比</strong>mikemacmarketin CC BY 2.0 的神经网络芯片图稿、biologycorner CC BY-NC 2.0 的大脑模型、Moto@Club4AG CC BY 2.0 的平面照片、ksblack99 CC PDM 1.0 的鸟类照片</p></figure><p id="5a85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人工神经网络和生物大脑都从外部刺激中学习，以理解事物和预测结果。一个关键的区别是，人工神经网络处理浮点数，而不仅仅是神经元的二进制触发。对于人工神经网络，它是数字输入和数字输出。</p><p id="b285" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图显示了典型人工神经网络的结构。左边的输入是包含输入刺激的数值。输入层连接到一个或多个包含先前学习记忆的隐藏层。输出层，在这种情况下只有一个数字，连接到隐藏层中的每个节点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/5b64e5d2fe1aa6451ad2c33f8d460984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NM9ymEXwSGTmfXF__dP9EQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">典型人工神经网络图</strong></p></figure><p id="6024" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个内部箭头代表数字权重，这些数字权重用作乘数，在网络中从左到右处理图层时修改图层中的数字。用输入值和预期输出值的数据集来训练该系统。权重最初被设置为随机值。对于训练过程，系统多次运行训练集，调整权重以实现预期的输出。最终，系统不仅能从训练集中正确预测输出，还能预测未知输入值的输出。这就是机器学习(ML)的本质。<strong class="lb iu">智慧在砝码中</strong>。关于人工神经网络培训过程的更详细的讨论可以在康纳麦当劳的帖子中找到，这里<a class="ae lv" rel="noopener" target="_blank" href="/machine-learning-fundamentals-ii-neural-networks-f1e7b2cb3eef"/>。</p><h2 id="03c5" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">生成对抗网络</h2><p id="12aa" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">2014 年，蒙特利尔大学的 Ian Goodfellow 和七名合著者提交了一篇关于生成性对抗网络(GANs)的<a class="ae lv" href="https://arxiv.org/pdf/1406.2661.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>[2]。他们想出了一种训练两个人工神经网络的方法，这两个人工神经网络可以有效地相互竞争，以创建像照片、歌曲、散文，是的，还有绘画这样的内容。第一个人工神经网络称为生成器，第二个称为鉴别器。生成器试图创建逼真的输出，在这种情况下，是一幅彩色绘画。鉴别器试图从训练集中辨别真实的绘画，而不是从生成器中辨别伪造的绘画。这是 GAN 架构的样子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/bf06ad84e772a120d70333f70bd203ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4_g-1dP_ALpkrTKlLkGFw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">生成性对抗网络</strong></p></figure><p id="2911" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一系列随机噪声被送入发生器，然后发生器使用其训练的权重来生成结果输出，在这种情况下，是彩色图像。通过在处理预期输出为 1 的真实绘画和预期输出为-1 的伪造绘画之间交替来训练鉴别器。在每幅画被发送到鉴别器后，它会发回详细的反馈，说明为什么这幅画不是真实的，生成器会根据这些新知识调整其权重，以尝试下次做得更好。<strong class="lb iu">GAN 中的两个网络以对抗的方式被有效地一起训练</strong>。生成器在试图将假图像冒充为真实图像方面变得更好，鉴别器在确定哪个输入是真实的，哪个是假的方面变得更好。最终，生成器在生成逼真的图像方面变得非常出色。你可以在 Shweta Goyal 的帖子<a class="ae lv" href="https://medium.com/analytics-vidhya/gans-a-brief-introduction-to-generative-adversarial-networks-f06216c7200e" rel="noopener">这里</a>阅读更多关于 GANs 的内容，以及他们使用的数学。</p><h2 id="1ce6" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">用于大图像的改进的 GANs</h2><p id="6629" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">尽管上述基本 GAN 适用于小图像(即 64×64 像素)，但对于大图像(即 1024×1024 像素)存在问题。由于像素的非结构化性质，基本 GAN 架构难以收敛到大图像的良好结果。它从树上看不到森林。英伟达的研究人员开发了一系列改进的方法，允许用更大的图像训练 GANs。第一种叫做<a class="ae lv" href="https://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf" rel="noopener ugc nofollow" target="_blank"/>【3】。</p><blockquote class="ni nj nk"><p id="7973" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated">关键的想法是逐步增加生成器和鉴别器:从低分辨率开始，我们添加新的层，随着训练的进行，这些层可以模拟越来越精细的细节。这既加快了训练的速度，又极大地稳定了训练，使我们能够拍摄出前所未有的高质量图像。——<strong class="lb iu">特罗·卡拉斯</strong>等人。艾尔。，英伟达</p></blockquote><p id="1074" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NVIDIA 的团队继续他们的工作，使用 GANs 生成大的、真实的图像，将他们的架构命名为<a class="ae lv" href="https://arxiv.org/pdf/1812.04948.pdf" rel="noopener ugc nofollow" target="_blank">StyleGAN</a>【4】。他们以渐进增长的 GANs 作为基础模型，并添加了一个风格映射网络，将不同分辨率的风格信息注入到生成器网络中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/d0d5baff89d4ac367fcd741484f8a4c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hqClvDA-rmM9s6IJq-Eh3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky"> StyleGAN 组件图</strong></p></figure><p id="e791" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该团队利用<a class="ae lv" href="https://arxiv.org/pdf/1912.04958.pdf" rel="noopener ugc nofollow" target="_blank"> StyleGAN </a> 2 进一步改善了图像创建结果，使 GAN 能够高效地创建高质量的图像，减少不必要的伪影[5]。你可以在 Akria 的帖子“<a class="ae lv" href="https://medium.com/analytics-vidhya/from-gan-basic-to-stylegan2-680add7abe82" rel="noopener">从 GAN basic 到 StyleGAN2 </a>”中了解更多关于这些发展的信息。</p><h1 id="7700" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">以前的工作，创造艺术与甘斯</h1><p id="f58d" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">自 GAN 于 2014 年推出以来，研究人员一直在寻求使用 GAN 来创作艺术。任伟·谭等人在 2017 年发表了一个名为 ArtGAN 的系统的描述。艾尔。来自日本长野信州大学[6]。他们的论文建议延长 GANs…</p><blockquote class="ni nj nk"><p id="5fce" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated">…综合生成更具挑战性和更复杂的图像，如具有抽象特征的艺术品。这与大多数当前的解决方案形成对比，当前的解决方案侧重于生成自然图像，如室内、鸟、花和脸。——<strong class="lb iu">任伟谭</strong>等。艾尔。信州大学</p></blockquote><p id="1775" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Drew Flaherty 在澳大利亚布里斯班的昆士兰科技大学为他的硕士论文进行了一项关于使用 GANs 创作艺术的更广泛的调查[7]。他尝试了各种 GAN，包括基本 GAN、<a class="ae lv" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank">cycle gan</a>【8】、<a class="ae lv" href="https://arxiv.org/pdf/1809.11096.pdf" rel="noopener ugc nofollow" target="_blank">BigGAN</a>【9】、<a class="ae lv" href="https://phillipi.github.io/pix2pix/" rel="noopener ugc nofollow" target="_blank"> Pix2Pix </a>和 StyleGAN。在他尝试的所有方法中，他最喜欢斯泰勒根。</p><blockquote class="ni nj nk"><p id="f699" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated">这项研究的最佳视觉效果来自 StyleGAN。…考虑到模型仅进行了部分训练，输出的视觉质量相对较高，早期迭代的渐进改进显示了更明确的线条、纹理和形式、更清晰的细节以及更全面的构图。昆士兰科技大学的德鲁·弗莱厄蒂</p></blockquote><p id="b3f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了他的实验，弗莱厄蒂使用了从各种来源收集的大量艺术作品，包括<a class="ae lv" href="https://www.WikiArt.org" rel="noopener ugc nofollow" target="_blank">WikiArt.org</a>、<a class="ae lv" href="https://artsandculture.google.com/" rel="noopener ugc nofollow" target="_blank">谷歌艺术项目</a>、<a class="ae lv" href="http://saatchiart.com" rel="noopener ugc nofollow" target="_blank">萨奇艺术</a>和<a class="ae lv" href="https://www.tumblr.com/" rel="noopener ugc nofollow" target="_blank"> Tumblr </a>博客。他指出，并非所有的源图像都在公共领域，但他讨论了合理使用原则及其对 ML 和 AI 的影响。</p><h1 id="2e55" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">机器射线</h1><h2 id="1223" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">概观</h2><p id="c8ec" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">在我的名为 MachineRay 的实验中，我从 WikiArt.org 收集了一些抽象画的图像，对它们进行处理，然后以 1024x1024 的尺寸输入到 StyleGAN2 中。我用 Google Colab 在 GPU 上训练了 GAN 三周。然后，我通过调整纵横比来处理输出图像，并通过另一个人工神经网络进行超分辨率调整。生成的图像宽或高为 4096 像素，具体取决于长宽比。这是组件图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/75cb8bae94f6754893714fac839064ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NcOSjQsts7SS2dv8LjGp0w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">机床部件图</strong></p></figure><h2 id="99b0" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">收集源图像</h2><p id="5708" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">为了收集源图像，我写了一个 Python 脚本来收集 WikiArt.org 的抽象画。请注意，我过滤了图像，只得到被标记为“抽象”流派的绘画，以及被标记为公共领域的图像。这些包括 1925 年之前出版的图像或 1950 年之前去世的艺术家创作的图像。该系列中的顶级艺术家包括瓦西里·康丁斯基、特奥·凡·杜斯堡、保罗·克利、卡齐米尔·马列维奇、亚诺什·马蒂斯·托奇、贾科莫·巴拉和皮特·蒙德里安。下面是 Python 代码的一个片段，完整的源文件是<a class="ae lv" href="https://github.com/robgon-art/MachineRay/blob/master/scrape-wikiart-by-artist.py" rel="noopener ugc nofollow" target="_blank">这里是</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="6380" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我收集了大约 900 张图片，但是我删除了那些有代表性的或者太小的图片，把数量减少到了 850 张。这是源图像的随机抽样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/f61b20c1df6b6ef3479df3861a809963.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n9pucyKojjRnRljewyql4A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">公共领域中来自 WikiArt.org 的抽象画</strong>的随机样本</p></figure><h2 id="2b6c" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">移除框架</h2><p id="9041" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">正如你在上面看到的，一些画在图像中保留了它们的木制框架，但是一些画的框架被裁剪掉了。例如，你可以在亚瑟·多佛的<em class="nl">风暴云</em>中看到这个框架。为了使源图像一致，并允许 GAN 专注于绘画的内容，我使用 Python 脚本自动移除了帧。下面是片段，完整的脚本是<a class="ae lv" href="https://github.com/robgon-art/MachineRay/blob/master/remove_frames.py" rel="noopener ugc nofollow" target="_blank">这里是</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="e8a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该代码打开每幅图像，并在边缘周围寻找与大部分绘画颜色不同的正方形区域。一旦找到边缘，图像将被裁剪以忽略该帧。下面是一些拆框前后的源画图片。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/771a8d2d419cf2f0fd06b7f2bb851b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fRsdFGVhuoto27cdLhZetw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">自动裁剪的图片</strong>来自 WikiArt.org 公共领域</p></figure><h2 id="21ee" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">图像增强</h2><p id="3387" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">虽然 850 张图片看起来很多，但对于训练一只 GAN 来说，这还远远不够。如果没有足够多种类的图像，GAN 可能会过度拟合模型，这将产生较差的结果，或者更糟糕的是，陷入可怕的“模型崩溃”状态，这将产生几乎相同的图像。</p><p id="5612" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">StyleGAN2 具有从左到右随机镜像源图像的内置功能。因此，这将有效地将样本图像的数量增加一倍，达到 1，700 个。这是更好的，但仍然不是很大。我使用了一种称为图像增强的技术，将图像数量增加了 7 倍，达到 11，900 张。下面是我使用的图像增强的代码片段。完整的源文件是这里的<a class="ae lv" href="https://github.com/robgon-art/MachineRay/blob/master/prep_images.py" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="b289" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该增强使用随机旋转、缩放、裁剪和温和的颜色校正来在图像样本中创建更多种类。请注意，在应用图像增强之前，我将图像的大小调整为 1024 乘 1024。我将在这篇文章中进一步讨论长宽比。以下是一些图像增强的例子。原文在左边，右边还有六个附加变奏。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/07104854d6f52620688489f16eb0490d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m6tbiU65xsJxXZDuZrxOxg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图像增强的例子</strong>在公共领域描绘来自 WikiArt.org 的图像</p></figure><h2 id="84c4" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">训练 GAN</h2><p id="43a4" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">我使用 Google Colab Pro 运行了培训。使用这项服务，我可以在高端 GPU 上运行长达 24 小时，这是一款 16 GB 内存的 NVIDIA Tesla P10。我还使用 Google Drive 来保留运行之间正在进行的工作。训练 GAN 花了大约 13 天的时间，通过系统发送了 500 万幅源图像。这是随机抽样的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/7a86b02d6d2a296cf78b43a21f8b5c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ubrrvn9O9zMS6nWj0F1OxA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">【MachineRay 的样本输出</p></figure><p id="f3f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以从上面的 28 幅图像样本中看到，MachineRay 创作了各种风格的画作，尽管它们之间有一些视觉上的共性。在源图像中有风格的提示，但是没有精确的拷贝。</p><h2 id="362a" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">调整纵横比</h2><p id="7229" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">虽然原始源图像有各种长宽比，从较薄的肖像形状到较宽的风景形状，但我将它们都做成方形，以帮助训练 GAN。为了让输出图像有多种长宽比，我在放大之前增加了一个新的长宽比。我没有选择一个纯粹随机的纵横比，而是创建了一个函数，它根据源图像中纵横比的统计分布来选择纵横比。这是分布图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/0c1942d8ca1eafcdcd792cf6eac06d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ZxZhVqPxGfoSvgpDLi_sg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">宽高比分布</strong>来自 WikiArt.org 公共领域的图片。</p></figure><p id="9166" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图描绘了所有 850 幅源图像的纵横比。它的范围从大约 0.5 到大约 2.0，前者是较窄的 1:2 比率，后者是较宽的 2:1 比率。图表显示了四个源图像，以指示它们在图表上的水平位置。下面是我的 Python 代码，它根据源图像的分布将 0 到 850 之间的一个随机数映射成一个纵横比。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="cf6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我调整了上面的 MachineRay 输出，使下面的图片具有不同的纵横比。你可以看到这些图像看起来更自然，不那么同质，只有这一点小小的变化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/e6a1d9e605a41d5ac991179d352328fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wwSxQGZgJkxPEXcl53EJeA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">具有不同纵横比的 MachineRay 的样本输出</strong></p></figure><h2 id="1250" class="mo lx it bd ly mp mq dn mc mr ms dp mg li mt mu mi lm mv mw mk lq mx my mm mz bi translated">超分辨率尺寸调整</h2><p id="762f" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">从 MachineRay 生成的图像的最大高度或宽度为 1024 像素，这对于在计算机上查看是可以的，但是对于打印来说是不可以的。在 300 DPI 时，它只能以大约 3.5 英寸的尺寸打印。这些图像可以放大，但是如果以 12 英寸打印，看起来会很柔和。有一种使用人工神经网络来调整图像大小并保持清晰特征的技术，称为图像超分辨率(ISR)。更多关于超分辨率的信息，请点击这里查看 Bharath Raj 的帖子。</p><p id="923b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">德国 Idealo 公司有一个很好的开源<a class="ae lv" href="http://github.com/idealo/image-super-resolution" rel="noopener ugc nofollow" target="_blank"> ISR 系统</a>，带有预先训练好的模型。他们的 GANs 模型使用在照片上训练过的 GANs 进行 4 倍的尺寸调整。我发现在 ISR 之前给图像添加一点随机噪声会产生一种绘画效果。下面是我用来对图像进行后处理的 Python 代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="d9ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里看到添加噪声和图像超分辨率调整的结果。注意纹理细节看起来有点像笔触。</p><div class="kj kk kl km gt ab cb"><figure class="nx kn ny nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/85bc0eb4bd8570a49390811f190e8ae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*g5-O0PAiUw8noJNcPAXhyQ.jpeg"/></div></figure><figure class="nx kn ny nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/bf195cb886243677d43d87df85b68001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*KCxgHyfbir7VlVCi71EseQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk od di oe of translated"><strong class="bd ky">左图:添加噪声和 ISR 后的样本图像。右图:细节特写</strong></p></figure></div><p id="6e89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看附录 A 中的图库，查看 MachineRay 的高分辨率输出示例。</p><h1 id="278a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">后续步骤</h1><p id="1234" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">其他工作可能包括以大于 1024x1024 的尺寸运行 GAN。将代码移植到张量处理单元(TPUs)而不是 GPU 上运行会使训练运行得更快。此外，来自 Idealo 的 ISR GAN 可以使用绘画而不是照片进行训练。这可能会给图像添加更真实的绘画效果。</p><h1 id="69da" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">感谢</h1><p id="74ee" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">我要感谢詹尼弗·林和奥利弗·斯特瑞普对这个项目的帮助和反馈。</p><h1 id="04b3" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">源代码</h1><p id="e7e0" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">这个项目的所有源代码都可以在<a class="ae lv" href="https://github.com/robgon-art/MachineRay" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。一个用于生成图像的 Google Colab 在这里<a class="ae lv" href="https://colab.research.google.com/github/robgon-art/MachineRay/blob/master/MachineRay_Image_Generation_2.ipynb" rel="noopener ugc nofollow" target="_blank">可用</a>。源代码在<a class="ae lv" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY-NC-SA 许可证</a>下发布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/b315d79ed198829e5f3eea208c7bd3f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:176/format:webp/1*RMfS4ENCi8EwpCTYHCyuvA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">署名-非商业性使用-类似分享</strong></p></figure><h1 id="3593" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">参考</h1><p id="f70a" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">[1] W .麦卡洛克，w .皮茨，“神经活动中内在的思想逻辑演算”，<em class="nl">数学生物物理学通报</em>。5(4):115-133，1943 年 12 月</p><p id="42bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]伊恩，好伙计。“生成性对抗网络。”让·普吉-阿巴迪、迈赫迪·米尔扎、徐炳、戴维·沃德-法利、谢尔吉尔·奥泽尔、亚伦·库维尔、约舒阿·本吉奥，第一版，2014 年 6 月</p><p id="b2a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] T. Karras、T. Aila、S. Laine 和 J. Lehtinen，“为提高质量、稳定性和变化性而逐步种植甘蔗”，CoRR，第 abs/1710.1 卷，2017 年 10 月</p><p id="f8ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] T. Karras，S. Laine，T. Aila，“基于风格的生成对抗网络生成器架构”，CVPR2019，2019 年 3 月</p><p id="195e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] T. Karras、S. Laine、M. Aittala、J. Hellsten、J. Lehtinen 和 T. Aila，“分析和提高 StyleGAN 的图像质量”，2020 年 3 月</p><p id="8af6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] W. R. Tan、C. S. Chan、H. E. Aguirre 和 K. Tanaka，“ArtGAN:使用条件分类 GAN 的艺术作品合成”，2017 年 4 月</p><p id="5acc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7] D. Flaherty，“机器学习的艺术方法”，昆士兰科技大学，硕士论文，2020 年</p><p id="ab62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[8]朱军，帕克，伊索拉和，“使用循环一致的对抗网络进行不成对的图像到图像翻译”，2018 年 11 月</p><p id="2325" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[9] A. Brock，J. Donahue 和 K. Simonyan，“高保真自然图像合成的大规模 GAN 训练”，2019 年 2 月</p><h1 id="b77d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">附录 A —机器射线结果图库</h1><div class="kj kk kl km gt ab cb"><figure class="nx kn oh nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/712c71ffb822be1374a7b81195350e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*hUJJ83PkDE9NlcZqwiHVHg.jpeg"/></div></figure><figure class="nx kn oi nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/d70865fdd56d224e2290abdb14ccac4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*6O-dg5QL-x9w7gtLFVsLiw.jpeg"/></div></figure></div><div class="ab cb"><figure class="nx kn oj nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/0b88531c46a0615f1cd52972d0231141.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*23Vy-3RNf_nl_ICgSsmhoQ.jpeg"/></div></figure><figure class="nx kn ok nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/b03351efea4dd7ec6b8acbdabb262fce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*Sj_R45Gyzw-C-Uzbs9bcDg.jpeg"/></div></figure></div><div class="ab cb"><figure class="nx kn ol nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/8cac4a3a5501915ffec231f5ef45844d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*dEHVD34MBSBY5fiaAW-dCg.jpeg"/></div></figure><figure class="nx kn om nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/f77cb1b1595793f0c196ec64c9b7dc57.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*GVBz7UFD58Bzcr2MNYoHng.jpeg"/></div></figure></div><div class="ab cb"><figure class="nx kn on nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/5913f0e9626e348722536913f7e44eb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*_S_8JlRhc5_S7OnW98JNAg.jpeg"/></div></figure><figure class="nx kn oo nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/4e167ba93c300366e9390ef8c0305006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*UFxGj5ibi-4InTDVV0ioBQ.jpeg"/></div></figure></div><div class="ab cb"><figure class="nx kn op nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/1bc032ef667b84cd6f9558d878805b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*7L1wlYwK4hPntaoXdgmeFw.jpeg"/></div></figure><figure class="nx kn oq nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/547894d5d8a15a541d7758d9e7b3d2dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*Q-_UUoJOuyjETJXAkmNQDw.jpeg"/></div></figure></div><div class="ab cb"><figure class="nx kn or nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/e7cbb6ad26d7125cac28d98ca396d652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*b06aWWGxKebM8CZasDSaEg.jpeg"/></div></figure><figure class="nx kn os nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/09dc70d16a0cf001d0915469007cd38d.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*mbz6e30bB9NNoCbA9RR-Zg.jpeg"/></div></figure></div><div class="ab cb"><figure class="nx kn ot nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/64e1db6dd3c8e200519323af2ff83ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*t9uwfEHPmau-XmmyAGnymg.jpeg"/></div></figure><figure class="nx kn ou nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/c522adb8beb276ba3c052c078b2b5602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*c9bNQVcBxUV2DMxev-07zg.jpeg"/></div></figure></div></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><p id="7d39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了无限制地访问 Medium 上的所有文章，<a class="ae lv" href="https://robgon.medium.com/membership" rel="noopener">成为会员</a>，每月支付 5 美元。非会员每月只能看三个锁定的故事。</p></div></div>    
</body>
</html>