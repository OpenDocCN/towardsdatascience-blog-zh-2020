<html>
<head>
<title>Decision Trees — How to draw them on paper</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树——如何在纸上画出它们</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-how-to-draw-them-on-paper-e2597af497f0?source=collection_archive---------17-----------------------#2020-04-04">https://towardsdatascience.com/decision-trees-how-to-draw-them-on-paper-e2597af497f0?source=collection_archive---------17-----------------------#2020-04-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ju"><img src="../Images/72e40860801a01f486f43f46375d5ce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*10kIeOZeMCgGRDjPF5gdng.png"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自<a class="ae kk" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=576847" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="9ded" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">本文将讨论:</p><ul class=""><li id="430b" class="lj lk iq kn b ko kp ks kt kw ll la lm le ln li lo lp lq lr bi translated">决策树——监督机器学习中一种著名的分类算法</li><li id="e19a" class="lj lk iq kn b ko ls ks lt kw lu la lv le lw li lo lp lq lr bi translated">决策树背后的基础知识以及如何在没有计算机的情况下开发决策树</li></ul><p id="7916" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">你应该已经知道的:</p><ul class=""><li id="4683" class="lj lk iq kn b ko kp ks kt kw ll la lm le ln li lo lp lq lr bi translated">统计和概率基础</li></ul><p id="0271" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">说到机器学习中的分类问题，像逻辑回归、<a class="ae kk" href="https://wildregressor.blogspot.com/2020/03/linear-discriminant-analysis-basics.html" rel="noopener ugc nofollow" target="_blank">判别分析</a>等算法。是那些浮现在人们脑海中的。还有一种非常直观、易于解释和理解的分类算法，称为决策树算法。基于树的模型可以用于回归和分类，但是我们在这里只讨论分类的情况。下面画了一个典型的决策树，让你熟悉这个概念:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/9398591e5042eeb160f911571e515647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/0*s45Y4TH7fPWIepYz.png"/></div></figure><p id="07b9" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">上面的决策树显示了在一个随机的家庭中找到一台电视的机会。显而易见，上面的树表明，如果一个人的月收入超过1000美元，他就会在家里拥有一台电视，否则就会失去。上面的树可以看作是根据下面的数据开发的模型的可视化表示。这里的“<strong class="kn ir">月收入“</strong>是预测变量，“<strong class="kn ir">在家看电视”</strong>是反应变量。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ly"><img src="../Images/3a59575837dd5bf2e8bb0010df8f1a21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zCokakiCCWst5pag.png"/></div></div></figure><p id="d78d" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">让我们用一个简单的方法来处理上面的数据集。总共有10个数据点，在月收入栏下，有6个<strong class="kn ir">真值</strong>和4个<strong class="kn ir">假值</strong>。现在，我们先只分析<strong class="kn ir">真</strong>值。在6个<strong class="kn ir">真</strong>值中，有4个在国内<strong class="kn ir">电视</strong>栏下对应行有<strong class="kn ir">是</strong>，有2个有<strong class="kn ir">否</strong>反对。考虑中的行如下所示:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi lz"><img src="../Images/99ef0204acac3c9be4594e2cf363fa1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Zu65D31EsbwEVay2.png"/></div></div></figure><p id="0507" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">上述数据被认为是不纯的，因为存在混合响应值(4 <strong class="kn ir">是</strong>T44】2<strong class="kn ir">否</strong>)对同一预测值<strong class="kn ir">为真。</strong>对我们来说，理想的数据集应该是只有<strong class="kn ir">是</strong>对<strong class="kn ir">真值</strong>或<strong class="kn ir">否</strong>对<strong class="kn ir">真值</strong>的数据集。在这种情况下，决策应该是非常容易和直接的。但是由于回答是混合或不纯的，我们不会在这里做出任何结论，让我们分析月收入栏下的其他值:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ma"><img src="../Images/035e894c322538236cb07e06fa4ea560.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VkntAsohncg6qLeN.png"/></div></div></figure><p id="597f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">上面的数据集也是不纯的，对于<strong class="kn ir">假</strong>的同一个预测值，这里有一个混合响应变量值。由于大多数反对<strong class="kn ir">真</strong>的值是<strong class="kn ir">是</strong>，反对<strong class="kn ir">假</strong>的值是<strong class="kn ir">否</strong>，这意味着发现<strong class="kn ir">是</strong>反对<strong class="kn ir">真</strong>的概率更大，反对<strong class="kn ir">假</strong>的<strong class="kn ir">否</strong>的概率也更大，因此决策树如上图所示。单一预测变量模型看起来很简单。当模型中涉及多个变量时，事情就变得复杂了。我们很快就会看到这一点，但是让我们把注意力转移到上面介绍的一个术语上，叫做杂质。</p><p id="abf9" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ir">杂质及其测量</strong></p><p id="4595" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">杂质意味着我们的数据是多么的不均匀。在上面的例子中，正如已经提到的，如果预测器列中的每个<strong class="kn ir">真</strong>值在响应列中都有一个<strong class="kn ir">是</strong>，事情会变得更加简单，反之亦然。在几乎所有的实际情况下，情况并非如此，我们通常会得到混合数据集。我们需要找到一种测量杂质的方法。已经有两种方法来衡量数据的不纯或不均匀程度:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/226172c2142b1d7781119ff76469f815.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/0*2ThPOnT5vRszHeQJ.png"/></div></figure><p id="003b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">虽然它们都在相同程度上测量杂质，并导致了类似模型的发展，但我们在这里只考虑熵。让我们讨论熵方程中的各种参数。<em class="mc"> Pi </em>是目标/响应变量中第I类的概率，例如，如果目标变量的类分布为[0，0，1，0，1，1，0，1，1]，P(0) = 4/9，P(1) = 5/9。将这个概念应用于表1，我们将得到P(是)= 5/10和P(否)= 5/10。转到另一个参数，注意以2为底的对数函数的使用。如果你想知道更多关于等式中基数2的用法，你应该读读<a class="ae kk" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">这个</strong> </a>。</p><p id="c6fb" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在我们已经有了熵的等式，让我们首先计算所有数据的熵(表1)。因为<strong class="kn ir">是</strong>值出现了5次，并且<strong class="kn ir">否</strong>值也出现了5次，所以我们有:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi md"><img src="../Images/1de9e0fbb9829d3b4430297ad856f82e.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/0*KisWl_KzTWJ9boEW.png"/></div></figure><p id="d6f4" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在让我们计算在<strong class="kn ir">真</strong>分割下的数据的熵(表2)。这里的目标变量是“<strong class="kn ir">在家看电视”</strong>有两个类“<strong class="kn ir">是</strong>”和“<strong class="kn ir">否</strong>”。<strong class="kn ir">是</strong>值出现4次，而<strong class="kn ir">否</strong>出现2次。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi me"><img src="../Images/d659e8ffd0c52322023bce59e0ce8d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/0*XzzEUNRtSAKJ1MMH.png"/></div></figure><p id="4a7f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">假设目标变量只有一个类别(完全纯)，那么熵应该是:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/f384562640e2c6eb17184997b2651f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/0*TnVut0kyUULax_Dt.png"/></div></figure><p id="bf53" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这示出了具有完全同质性的数据集的熵= 0，并且很容易示出完全不纯的数据集(以相等的数量存在的每个类别观察)的熵为1。</p><p id="a9e6" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">同样，在<strong class="kn ir">错误</strong>分割下的数据的熵将是:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/5e131832e8e4fd551c91a0bdf772d240.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/0*i8XsqAOn7o8dYj0c.png"/></div></figure><p id="5769" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">引入收入变量后，数据的净熵可以通过对其拆分下的两个熵值进行加权平均来计算</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/f2e01a536b328a50138f2a4910fbe75c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/0*YewnoXYeadkkYR2d.png"/></div></figure><p id="b05d" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">熵的流动需要被清楚地理解。最初，它是1，<strong class="kn ir">真</strong>分割将它减少到0.917，<strong class="kn ir">假</strong>分割将它减少到0.811。收入变量的净减少量为0.874。这就是我们所期待的，通过变量的引入，熵(异质性的杂质)的减少。总缩减量为1–0.874 = 0.126。这个数(0.126)被称为<strong class="kn ir">信息增益</strong>，是决策树模型开发中非常重要的参数。</p><p id="b7c7" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这就是如何通过熵计算来计算杂质的程度。现在让我们在模型开发中应用这个概念。</p><p id="ef01" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ir">模型开发</strong></p><p id="5e1a" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在，除了上述通过月收入猜测电视拥有量的信息之外，假设后来我们注意到，不仅月收入，还有另一个影响结果的变量，该变量是一个人的位置，他是住在城市还是农村。让更新后的数据集如下所示:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi mi"><img src="../Images/f022142bf56fb8c71b113143deb9cfaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HnwTWNvJRLJynsFV.png"/></div></div></figure><p id="27cd" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">无论我们目前知道什么，上面的数据集给我们带来了一个两难的选择。两难的问题是:我们的决策树是从收入预测器开始还是从位置预测器开始。我们将尝试通过对两个预测值进行熵计算来解决这一难题，但让我们将位置变量与目标变量分开，如下所示:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/77a8f895179a38db237fb4ddc56f2f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/0*YN2r9pHbpqRz_wTI.png"/></div></figure><p id="eb60" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">让我们分别计算收入和位置变量下的熵:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi mk"><img src="../Images/71d2f689c3217208a9dd502fe1e26f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gvs4AQdhEv6yKcfb.png"/></div></div></figure><p id="1349" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如你所见，月收入的熵低于位置变量的熵，我们可以说月收入变量比位置变量更能降低系统的异质性，信息增益为1–0.874 = 0.126。按照信息增益标准，我们的决策树应该再次从月收入变量开始，如下所示:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi me"><img src="../Images/eb1dcff347adee05b2a32758b4c5b6dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/0*hRZHx1TlBkpCvMmg.png"/></div></figure><p id="14bc" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">虽然我们已经找到了第一个变量，但我们不能就此结束。我们还有另一个变量可以使用，它可以提高我们模型的效率。现在下一个问题是把它放在哪里，沿着树。对于位置变量，我们有两个可能的位置&amp;都将传递不同的数据值给它&amp;因此有不同的信息增益。我们将把它放在信息获取最大的地方。让我们计算一下，找出答案。</p><p id="1af5" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ir">真值</strong>栏如下所示:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ml"><img src="../Images/3681cb2e3e8118bd6779b4fc68452306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VNxuCuHlcQu3tBci.png"/></div></div></figure><p id="0fa6" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">月收入一栏下的数值是一样的。根据列值分离location列，我们将得到如下所示的表:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi mm"><img src="../Images/394ab072cee2b04d4bf8d5e164fa1280.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eSDyz90isUSuff9d.png"/></div></div></figure><p id="5486" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">上述计算可以如下图所示:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi mn"><img src="../Images/1efac4c4ccdf6d9a4194e4af8fbf468e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*f2q0DSV3USPmxptJ.png"/></div></div></figure><p id="1535" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果我们将位置变量置于错误分割下，计算和数据表将如下所示:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi mo"><img src="../Images/93a9c45d0c74e60e917ba95978b1fe1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*67esdbwSmVv_OU1R.png"/></div></div></figure><p id="6ab5" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">月收入一栏下的数值是一样的。根据列值分离location列，我们将得到如下所示的表:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi mk"><img src="../Images/fefcc3ccf9bec356630bbcbd58db51f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FPK-CNcNsvX-3YFx.png"/></div></div></figure><p id="e17b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">上述计算可以想象为:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi mp"><img src="../Images/01974f25f129e1041265020a17321fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1HdQWgL2drR8wCkE.png"/></div></div></figure><p id="196f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">因为错误分割下的信息增益小于真实分割，所以决策树看起来像:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi mq"><img src="../Images/82a532594cf34e3fe55d09477d14adcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cKKlYxpVEyRwlqpJ.png"/></div></div></figure><p id="7c44" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">找到<strong class="kn ir">城市</strong>预测值的<strong class="kn ir">是</strong>值的概率大于<strong class="kn ir">否</strong>值，因此有上述安排。</p><p id="ea2b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">上面的树表明，如果收入低于1000美元，家庭中将没有电视，如果收入高于1000美元，则我们必须检查家庭的位置，如果是城市，则可以找到电视，否则就是其他。</p><p id="87b8" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">决策树就是这样发展起来的。决策树的结果通常不如其他分类模型(如逻辑回归或线性判别分析)准确，但当理解或解释系统的需要大于预测的需要时，它们是有用的。</p><p id="6364" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在你已经理解了开发决策树的概念，让我们来理解决策树中使用的各种术语。</p><p id="7c20" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ir">节点</strong>:节点是表示决策树中一个变量的东西，比如收入和位置就是节点。</p><p id="d704" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ir">根节点</strong>:树开始的节点，如上述决策树中的收入。</p><p id="9fbd" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ir">叶节点</strong>:最底层的节点，在这里决定目标变量的值，例如location是上述树中的叶节点。</p><p id="609b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ir">拆分</strong>:将一个节点拆分成两个或多个子节点的过程称为拆分</p><p id="0e31" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">到目前为止，我们所做的任何事情的Python代码如下所示:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi mr"><img src="../Images/6008134e9349bb39a50d82f9ecb55623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vN0LtRobTKdnxg_w.png"/></div></div></figure><p id="c7c2" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果你觉得上面的概念很有趣，你可以进一步阅读下面提到的主题:</p><ul class=""><li id="df18" class="lj lk iq kn b ko kp ks kt kw ll la lm le ln li lo lp lq lr bi translated"><a class="ae kk" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">随机森林</strong> </a></li><li id="4afa" class="lj lk iq kn b ko ls ks lt kw lu la lv le lw li lo lp lq lr bi translated"><a class="ae kk" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">装袋</strong> </a></li><li id="901e" class="lj lk iq kn b ko ls ks lt kw lu la lv le lw li lo lp lq lr bi translated"><a class="ae kk" href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">助推</strong> </a></li></ul><p id="d61a" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">谢谢，</p><p id="c458" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">玩得开心:)</p><p id="9da9" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果你对这篇文章有任何疑问，你可以通过LinkedIn<a class="ae kk" href="https://in.linkedin.com/in/tanvirhurra" rel="noopener ugc nofollow" target="_blank">联系我</a></p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="1578" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="mc">原载于2020年4月4日http://wildregressor.com</em><a class="ae kk" href="https://www.wildregressor.com/2020/04/decision-trees-how-to-draw-them-on-paper.html" rel="noopener ugc nofollow" target="_blank"><em class="mc"/></a><em class="mc">。</em></p></div></div>    
</body>
</html>