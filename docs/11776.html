<html>
<head>
<title>Training Neural Networks for price prediction with TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用张量流训练神经网络进行价格预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-neural-networks-for-price-prediction-with-tensorflow-8aafe0c55198?source=collection_archive---------9-----------------------#2020-08-15">https://towardsdatascience.com/training-neural-networks-for-price-prediction-with-tensorflow-8aafe0c55198?source=collection_archive---------9-----------------------#2020-08-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c885" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使 DNN 更有效地解决回归问题:TensorFlow 和 Keras 实用指南。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0c84b0ecb73aee37469209c718dce9c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yIquFD3fQEy-OS18K9qpdQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型学习曲线的演变</p></figure><p id="fb9b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用深度神经网络来解决回归问题可能看起来有些矫枉过正(而且经常如此)，但对于一些拥有大量高维数据的情况，它们可以胜过任何其他 ML 模型。</p><p id="1ddf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当你学习神经网络时，你通常会从一些图像分类问题开始，如 MNIST 数据集——这是一个显而易见的选择，因为高维数据的高级任务是 dnn 真正蓬勃发展的地方。</p><p id="34cb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">令人惊讶的是，当你试图将你在 MNIST 学到的东西应用到回归任务中时，你可能会挣扎一段时间，直到你的超级先进的 DNN 模型比基本的随机森林回归器更好。有时候你可能永远也到不了那个时刻…</p><p id="8c97" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本指南中，我列出了使用 DNN 解决回归问题时学到的一些关键技巧和诀窍。该数据是一组近 50 个要素，描述了华沙的 25k 处房产。我在上一篇文章中描述了特征选择过程:<a class="ae lu" rel="noopener" target="_blank" href="/feature-selection-and-error-analysis-while-working-with-spatial-data-a9d38af05b88">处理空间数据时的特征选择和错误分析</a>因此，现在我们将重点关注使用所选特征创建预测每平方米房价的最佳模型。</p><p id="0ec4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">本文使用的代码和数据源可以在</strong> <a class="ae lu" href="https://github.com/Jan-Majewski/Project_Portfolio/blob/master/03_Real_Estate_pricing_in_Warsaw/03_04_Training_Neural_Networks.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> GitHub </strong> </a> <strong class="la iu">上找到。</strong></p><h1 id="067b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">1.入门指南</h1><p id="2a26" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">当训练深度神经网络时，我通常遵循以下关键步骤:</p><ul class=""><li id="39a4" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated"><strong class="la iu"> A) </strong>选择默认架构——层数、神经元数、激活</li><li id="ec24" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><strong class="la iu"> B) </strong>正则化模型</li><li id="9dda" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><strong class="la iu"> C) </strong>调整网络架构</li><li id="9599" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><strong class="la iu"> D) </strong>调整学习率和时期数</li><li id="4a38" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">使用回调提取最佳模型</li></ul><p id="8967" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通常创建最终的模型需要几遍所有这些步骤，但是要记住的一件重要的事情是:<strong class="la iu">一次做一件事</strong>。不要试图同时改变架构、规则和学习速度，因为你不知道什么是真正有效的，可能要花几个小时在原地打转。</p><p id="e06b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在你开始为<strong class="la iu">回归任务</strong>构建 dnn 之前，有 3 件关键的事情你必须记住:</p><ul class=""><li id="6bde" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">将你的数据标准化，让训练更有效率</li><li id="de64" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">对所有隐藏层使用<strong class="la iu"> RELU </strong>激活功能——使用默认的 sigmoid 激活，你将一事无成</li><li id="035e" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">对单神经元输出层使用<strong class="la iu">线性</strong>激活函数</li></ul><p id="ad10" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一个重要的任务是选择损失函数。<strong class="la iu">均方误差</strong>或<strong class="la iu">平均绝对误差</strong>是两种最常见的选择。我的目标是最小化平均百分比误差，并在 5%的误差范围内最大化所有建筑的份额，我选择 MAE，因为它对异常值的惩罚更少，也更容易解释——它几乎可以告诉你每个报价平均每平方米偏离实际值多少美元。</p><p id="30d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有一个与我的目标直接相关的函数——<strong class="la iu">平均绝对百分比误差</strong>，但是在用 MAE 测试它之后，我发现训练效率较低。</p><h1 id="d501" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">2.基本 DNN 模型</h1><p id="54f8" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我们从一个具有 5 个隐藏层的基本网络开始，每隔一层神经元的数量递减。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="e6b8" class="nl lw it nh b gy nm nn l no np">tf.keras.backend.clear_session()<br/>tf.random.set_seed(60)</span><span id="9716" class="nl lw it nh b gy nq nn l no np">model=keras.models.Sequential([<br/>    <br/>    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  <br/>    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  <br/>    keras.layers.Dense(units=256,activation='relu'),  <br/>    keras.layers.Dense(units=256,activation='relu'),    <br/>    keras.layers.Dense(units=128,activation='relu'),<br/>    keras.layers.Dense(units=1, activation="linear"),</span><span id="232b" class="nl lw it nh b gy nq nn l no np">],name="Initial_model",)</span><span id="6168" class="nl lw it nh b gy nq nn l no np">model.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5e306c5581048faeac7cb5600ec01e60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*VWlFbGzVNLWks7LxmzvCCA.png"/></div></figure><p id="e6e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们使用 Adam optimizer，并从训练每个模型 200 个时期开始——模型配置的这一部分将保持不变，直到<br/>点 7。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="f249" class="nl lw it nh b gy nm nn l no np">optimizer = keras.optimizers.Adam()</span><span id="e29a" class="nl lw it nh b gy nq nn l no np">model.compile(optimizer=optimizer, warm_start=False, <br/>            loss='mean_absolute_error')</span><span id="702a" class="nl lw it nh b gy nq nn l no np">history = model.fit(X_train, y_train,<br/>                    epochs=200, batch_size=1024,<br/>                    validation_data=(X_test, y_test), <br/>                    verbose=1)</span></pre><h2 id="ed89" class="nl lw it bd lx ns nt dn mb nu nv dp mf lh nw nx mh ll ny nz mj lp oa ob ml oc bi translated">初始模型学习曲线</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/defe146cb550edc44334d318005d1cc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2LRvaAgtZfBI5eO5ZR1IHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">初始模型学习曲线(从时期 10 开始)</p></figure><p id="a58e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的第一个模型被证明是相当失败的，我们在训练数据上有惊人的过度拟合，并且我们的验证损失在纪元 100 之后实际上在增加。</p><h1 id="1fbe" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">3.辍学正规化</h1><p id="0570" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">退出可能是 DNN 正则化的最佳答案，适用于各种规模和架构的网络。在训练期间，应用 Dropout 会在每个时期随机丢弃层中的一部分神经元，这将迫使剩余的神经元更加通用-这将减少过度拟合，因为一个神经元不再能够映射一个特定的实例，因为它在训练期间不会总是在那里。</p><p id="9f34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我建议阅读原始论文，因为它很好地描述了这个想法，并且不需要多年的学术经验来理解它——<a class="ae lu" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank">辍学:防止神经网络过度拟合的简单方法</a></p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="8d6b" class="nl lw it nh b gy nm nn l no np">tf.keras.backend.clear_session()<br/>tf.random.set_seed(60)</span><span id="c218" class="nl lw it nh b gy nq nn l no np">model=keras.models.Sequential([<br/>    <br/>    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  <br/><strong class="nh iu">    keras.layers.Dropout(0.3),</strong><br/>    <br/>    keras.layers.Dense(512, activation='relu'),  <br/>   <strong class="nh iu"> keras.layers.Dropout(0.3),</strong></span><span id="a4f9" class="nl lw it nh b gy nq nn l no np">keras.layers.Dense(units=256,activation='relu'), <br/><strong class="nh iu">    keras.layers.Dropout(0.2),</strong><br/>    <br/>    keras.layers.Dense(units=256,activation='relu'), <br/>    <strong class="nh iu">keras.layers.Dropout(0.2),</strong><br/>    <br/>    keras.layers.Dense(units=128,activation='relu'),<br/>    keras.layers.Dense(units=1, activation="linear"),</span><span id="ea69" class="nl lw it nh b gy nq nn l no np">],name="Dropout",)</span></pre><p id="74e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Dropout 后的(0.x)指定了您想要丢弃的神经元份额，这转化为您想要正则化的程度。我通常在最大的层中从大约(0.3–0.5)的下降开始，然后在更深的层中降低其刚性。这种方法背后的想法是，更深层次网络中的神经元往往有更具体的任务，因此丢弃太多会增加太多的偏差。</p><h2 id="da13" class="nl lw it bd lx ns nt dn mb nu nv dp mf lh nw nx mh ll ny nz mj lp oa ob ml oc bi translated">辍学模型学习曲线</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/1f5f56ef364b60889c079bc9e6d4bdc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ju_Ha_0KTAphGMpzRROM8g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">辍学模型学习曲线(从第 10 个时期开始)</p></figure><p id="a10f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分析修改后模型的学习曲线，我们可以看到我们正朝着正确的方向前进。首先，我们设法从先前模型的验证损失中取得进展(由灰色阈值线标记)，其次，我们似乎用轻微的欠拟合来代替过拟合。</p><h1 id="0008" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">4.用批处理规范化处理濒死/爆炸神经元</h1><p id="bedc" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">当使用 RELU 激活的几个层工作时，我们有很大的神经元死亡的风险，这对我们的性能有负面影响。这可能导致我们在之前的模型中看到的拟合不足，因为我们可能实际上没有使用我们神经元的大部分，这基本上将它们的输出减少到 0。</p><p id="baef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">批量标准化是处理这一问题的最佳方式之一，当应用时，我们对每一批的每一层的激活输出进行标准化，以减少极端激活对参数训练的影响，从而降低消失/爆炸梯度的风险。描述该解决方案的原始论文比参考的前一篇论文更难阅读，但我仍然建议尝试一下— <a class="ae lu" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank">批量标准化:通过减少内部协变量转移来加速深度网络训练</a></p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="adef" class="nl lw it nh b gy nm nn l no np">tf.keras.backend.clear_session()<br/>tf.random.set_seed(60)</span><span id="a035" class="nl lw it nh b gy nq nn l no np">model=keras.models.Sequential([<br/>    <br/>    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'), <br/>  <strong class="nh iu">  keras.layers.BatchNormalization(),</strong><br/>    keras.layers.Dropout(0.3),<br/>    <br/>    keras.layers.Dense(512, activation='relu'),  <br/>   <strong class="nh iu"> keras.layers.BatchNormalization(),</strong><br/>    keras.layers.Dropout(0.3),</span><span id="f333" class="nl lw it nh b gy nq nn l no np">keras.layers.Dense(units=256,activation='relu'), <br/>   <strong class="nh iu"> keras.layers.BatchNormalization(),</strong><br/>    keras.layers.Dropout(0.2),<br/>    <br/>    keras.layers.Dense(units=256,activation='relu'), <br/>  <strong class="nh iu">  keras.layers.BatchNormalization(),</strong><br/>    keras.layers.Dropout(0.2),<br/>    <br/>    keras.layers.Dense(units=128,activation='relu'),<br/>    keras.layers.Dense(units=1, activation="linear"),</span><span id="45f6" class="nl lw it nh b gy nq nn l no np">],name="Batchnorm",)</span></pre><h2 id="6deb" class="nl lw it bd lx ns nt dn mb nu nv dp mf lh nw nx mh ll ny nz mj lp oa ob ml oc bi translated">BatchNorm 模型学习曲线</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/7ae758c63c8ef50ee5a03e32df40759d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N9ppNlz-qDoqEyYcg9VPKg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BatchNorm 模型学习曲线(从第 10 个时期开始)</p></figure><p id="90b5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">添加批量标准化有助于我们让一些神经元复活，这增加了我们的模型方差，将欠拟合变为轻微的过拟合——训练神经网络通常是一场猫捉老鼠的游戏，在最佳偏差和方差之间进行平衡。</p><p id="82de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一个好消息是，我们仍在改进验证错误。</p><h1 id="75e8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">5.将激活函数改为漏 RELU</h1><p id="502d" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">漏 RELU 激活函数是对 RELU 函数的轻微修改，它允许一些负面激活漏过，进一步降低了神经元死亡的风险。漏 RELU 通常需要更长的时间来训练，这就是为什么我们将训练这个模型另外 100 个纪元。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/566786d5b4c8aafb40b5464ad92f8e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*1Vkyq5vbwa_WDcK2rRQSZg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">漏 RELU 激活</p></figure><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="217f" class="nl lw it nh b gy nm nn l no np">tf.keras.backend.clear_session()<br/>tf.random.set_seed(60)</span><span id="69a6" class="nl lw it nh b gy nq nn l no np">model=keras.models.Sequential([<br/>    <br/>    keras.layers.Dense(512, input_dim = X_train.shape[1]), <br/>   <strong class="nh iu"> keras.layers.LeakyReLU(),</strong><br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Dropout(0.3),<br/>    <br/>    keras.layers.Dense(512),  <br/>    <strong class="nh iu">keras.layers.LeakyReLU(),</strong><br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Dropout(0.3),</span><span id="c4c3" class="nl lw it nh b gy nq nn l no np">keras.layers.Dense(units=256), <br/>  <strong class="nh iu">  keras.layers.LeakyReLU(),</strong><br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Dropout(0.2),<br/>    <br/>    keras.layers.Dense(units=256), <br/>   <strong class="nh iu"> keras.layers.LeakyReLU(),</strong><br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Dropout(0.2),<br/>    <br/>    keras.layers.Dense(units=128),<br/>    <strong class="nh iu">keras.layers.LeakyReLU(), </strong><br/>    keras.layers.Dense(units=1, activation="linear"),</span><span id="23b1" class="nl lw it nh b gy nq nn l no np">],name="LeakyRELU",)</span></pre><h2 id="d124" class="nl lw it bd lx ns nt dn mb nu nv dp mf lh nw nx mh ll ny nz mj lp oa ob ml oc bi translated">泄漏 ReLU 模型学习曲线</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/caa3daaf64067834c083472eda3574d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m76Ft98NWlIFcOD9FRF3ew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">泄漏 ReLU 模型学习曲线(从第 10 个时期开始)</p></figure><p id="1525" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">似乎漏 RELU 减少了过度拟合，并给了我们一个更健康的学习曲线，即使在 300 个时代后，我们也可以看到改进的潜力。我们几乎达到了以前模型的最低误差，但我们设法做到了这一点，而没有过度拟合，这给我们留下了增加方差的空间。</p><h1 id="a0e3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">6.用具有 1024 个神经元的附加隐藏层扩展网络</h1><p id="9bb7" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">在这一点上，我对基本模型感到足够满意，可以通过添加另一个具有 1024 个神经元的隐藏层来使网络变得更大。新的层次也有最高的辍学率。由于整体架构的变化，我还对较低级别的辍学率进行了实验。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="6cfa" class="nl lw it nh b gy nm nn l no np">tf.keras.backend.clear_session()<br/>tf.random.set_seed(60)</span><span id="9a01" class="nl lw it nh b gy nq nn l no np">model=keras.models.Sequential([<br/>    <br/><strong class="nh iu">    keras.layers.Dense(1024, input_dim = X_train.shape[1]), <br/>    keras.layers.LeakyReLU(),<br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Dropout(0.4),</strong><br/>    <br/>    keras.layers.Dense(512),  <br/>    keras.layers.LeakyReLU(),<br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Dropout(0.3),</span><span id="bea3" class="nl lw it nh b gy nq nn l no np">keras.layers.Dense(512),  <br/>    keras.layers.LeakyReLU(),<br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Dropout(0.3),<br/>    <br/>    keras.layers.Dense(units=256), <br/>    keras.layers.LeakyReLU(),<br/>    keras.layers.BatchNormalization(),<br/>    keras.layers.Dropout(0.2),<br/>    <br/>    keras.layers.Dense(units=256), <br/>    keras.layers.LeakyReLU(),<br/>    keras.layers.BatchNormalization(),<br/>    <strong class="nh iu">keras.layers.Dropout(0.01),</strong></span><span id="208d" class="nl lw it nh b gy nq nn l no np">keras.layers.Dense(units=128),<br/>    keras.layers.LeakyReLU(), <br/>    <strong class="nh iu">keras.layers.Dropout(0.05),</strong><br/>    keras.layers.Dense(units=1, activation="linear"),</span><span id="b6bd" class="nl lw it nh b gy nq nn l no np">],name="Larger_network",)</span></pre><h2 id="34a1" class="nl lw it bd lx ns nt dn mb nu nv dp mf lh nw nx mh ll ny nz mj lp oa ob ml oc bi translated">更大的网络模型学习曲线</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/e51958b0268ac8b1b87894ab8ce20338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJFMxv2cxugE3api0ZLe2g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更大的网络模型学习曲线(从时期 10 开始)</p></figure><p id="d88b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">扩展网络架构似乎正朝着正确的方向发展，我们略微增加了方差以获得学习曲线，这接近最优平衡。我们还设法使我们的验证损失几乎与过度拟合的 BatchNorm 模型持平。</p><h1 id="1c1f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">7.学习率下降，训练效率提高</h1><p id="f9a7" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">一旦我们对网络架构满意了，学习率就是最重要的超参数，需要调优。我决定使用学习率衰减，这允许我在开始时更快地训练我的模型，然后随着更多的时期降低学习率，以使训练更加精确。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="da88" class="nl lw it nh b gy nm nn l no np"><strong class="nh iu">optimizer = keras.optimizers.Adam(lr=0.005, decay=5e-4)</strong></span></pre><p id="3111" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">选择正确的起始速率和衰减具有挑战性，需要反复试验。在我的例子中，事实证明 Keras 中默认的 Adam 学习率(0.001)有点高。我开始时的学习率是 0.005，经过 400 多次后，学习率降到了 0.001。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/929747a1bbb32c42f68125f15ef0312a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZgAzJ0aVO0C1JFMueYoSNg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">学习率在 400 个时期内衰减</p></figure><h2 id="bd54" class="nl lw it bd lx ns nt dn mb nu nv dp mf lh nw nx mh ll ny nz mj lp oa ob ml oc bi translated">学习率衰减模型学习曲线</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/ce0d38abb8f8d07eccb09cdd2bae13d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_WX2GCPmTQKIOXb9JECi5w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">学习率衰减模型学习曲线(从时期 10 开始)</p></figure><p id="13b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">调整学习率帮助我们最终改善了验证误差结果，同时仍然保持学习曲线健康，没有过度拟合的太大风险——甚至可能还有一些空间来为另一个 100 个时期训练模型。</p><h1 id="856e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">8.使用回调在最佳时期停止训练</h1><p id="fce3" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">在选择我们的最佳模型之前，剩下的最后一项任务是使用回调在最佳时期停止训练。这允许我们在达到最小误差的精确时期检索模型。这种解决方案的最大优点是，如果你想训练 300 或 600 个纪元，你真的不需要担心——如果你的模型开始过度拟合，回调会让你回到最佳纪元。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="a55a" class="nl lw it nh b gy nm nn l no np">checkpoint_name = 'Weights\Weights-{epoch:03d}--{val_loss:.5f}.hdf5' <br/>checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')<br/>callbacks_list = [checkpoint]</span></pre><p id="ad6e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您需要定义您的回调:checkpoint_name 指定您希望在哪里以及如何保存每个纪元的权重，checkpoint 指定回调应该如何表现——我建议监视 val_loss 以进行改进，并且仅当纪元在这方面取得一些进展时才进行保存。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="eeb2" class="nl lw it nh b gy nm nn l no np">history = model.fit(X_train, y_train,<br/>                    epochs=500, batch_size=1024,<br/>                    validation_data=(X_test, y_test), <br/>                   <strong class="nh iu"> callbacks=callbacks_list, </strong><br/>                    verbose=1)</span></pre><p id="c16e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后你需要做的就是在拟合你的模型的同时添加回调。</p><h2 id="f230" class="nl lw it bd lx ns nt dn mb nu nv dp mf lh nw nx mh ll ny nz mj lp oa ob ml oc bi translated">回调模型学习曲线</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/339f9062755ea2af1cd8fd22181f14ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nKLEYKCiLC5oR6WXzz2fjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回调模型学习曲线(从第 10 个时期开始)</p></figure><p id="2b6b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用回调允许我们检索在第 468 个时期训练的最佳模型——接下来的 30 个时期没有改善，因为我们开始过度适应训练集。</p><h1 id="c7cb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">9.模型进化总结</h1><h2 id="4631" class="nl lw it bd lx ns nt dn mb nu nv dp mf lh nw nx mh ll ny nz mj lp oa ob ml oc bi translated">比较模型之间的验证损失</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/cdaf2b85fa6c86ceb0951d114f43da3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOv6bYU1dTitgAXywTOwGA.png"/></div></div></figure><p id="c499" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了达到期望的模型输出，我们花了 7 个步骤。当我们的主要目标是减少过度拟合时，我们设法在几乎每一步都有改进，在 batch_norm 和 1024_layer 模型之间有一个平台。老实说，提炼这 7 个步骤，可能花了我 70 个步骤，所以请记住，训练 DNNs 是一个迭代的过程，如果你的进步停滞了几个小时，不要推迟。</p><h1 id="496f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">10.DNN vs 兰登森林</h1><p id="1bfe" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">最后，与之前的<a class="ae lu" href="https://medium.com/r?url=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-and-error-analysis-while-working-with-spatial-data-a9d38af05b88" rel="noopener">文章</a>中基于相同数据训练的基本随机森林回归器相比，我们的最佳 DNN 表现如何？</p><p id="53d2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在两个关键 KPI 中我们的<strong class="la iu">随机森林</strong>得分如下:</p><ul class=""><li id="0633" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">绝对误差在 5%以内的预测份额= 44.6%</li><li id="2404" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">平均百分比误差= 8.8%</li></ul><p id="1839" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们最好的<strong class="la iu">深度神经网络</strong>得分:</p><ul class=""><li id="6a70" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">绝对误差在 5%以内的预测份额= 43.3% (-1.3 个百分点)</li><li id="f559" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">平均百分比误差= 9.1% (+0.3 个百分点)</li></ul><p id="77ae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以哭了吗？经过几个小时的精心训练，我们先进的神经网络怎么可能没有打败一个随机森林？老实说，有两个关键原因:</p><ul class=""><li id="6f26" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">就训练 DNNs 而言，25k 记录的样本大小仍然很小，我选择尝试这种架构，因为我每个月都在收集新数据，我相信在几个月内，我将达到更接近 100k 的样本，这将为 DNN 带来所需的优势</li><li id="24e9" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">随机森林模型的拟合度非常高，尽管在验证集上有很高的性能，但我不确定它是否也能很好地概括其他属性——在这一点上，我可能仍然会在生产中使用 DNN 模型，因为它更可靠。</li></ul><p id="f233" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总而言之，我建议不要从 DNN 开始解决回归问题。除非您在一个非常复杂的项目中使用数百个 k 样本，否则随机森林回归通常会更快地获得初始结果-如果结果证明是有希望的，您可以继续进行 DNN。训练有效的 DNN 需要更多的时间，如果您的数据样本不够大，它可能永远不会达到随机森林的性能。</p><p id="77c0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[1]:尼提什·斯里瓦斯塔瓦。(2014 年 6 月 14 日)。辍学:防止神经网络过度拟合的简单方法</p><p id="360b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]:谢尔盖·约菲。(2015 年 3 月 2 日)。批量标准化:通过减少内部协变量转移加速深度网络训练</p></div></div>    
</body>
</html>