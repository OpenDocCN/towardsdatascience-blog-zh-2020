<html>
<head>
<title>Random Forest Classifier in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的随机森林分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-random-forest-classifier-cheat-sheet-in-python-fedb84f8cf4f?source=collection_archive---------2-----------------------#2020-05-02">https://towardsdatascience.com/my-random-forest-classifier-cheat-sheet-in-python-fedb84f8cf4f?source=collection_archive---------2-----------------------#2020-05-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ea9c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一次处理分类变量和数值变量的端到端注释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/48405741385a9a92c7aa1fa8acefc5a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZHiiV40ZNlmQV1C6"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@guigui1410?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Guillaume Henrotte </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="0876" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">内容</h1><ol class=""><li id="bd86" class="lr ls it lt b lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">数据预处理(自动处理分类特征和NAs的技巧)</li><li id="9cb0" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">训练射频分类器</li><li id="442e" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">评估分类器(准确度、召回率、精确度、ROC AUC、混淆矩阵、绘图)</li><li id="a367" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">特征重要性</li><li id="cb5d" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">通过随机搜索调整超参数</li></ol><p id="4216" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">现在让我们开始吧，伙计们！</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="088d" class="kz la it bd lb lc nk le lf lg nl li lj jz nm ka ll kc nn kd ln kf no kg lp lq bi translated">数据描述</h1><p id="ce44" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly np mv mw ma nq my mz mc nr nb nc me im bi translated">在本文中，我使用的数据集来自我在一家科技公司为一个数据科学职位进行的真实技术测试。你可以在这里获得数据<a class="ae ky" href="https://github.com/joetrankang/data-is-life.git" rel="noopener ugc nofollow" target="_blank">(点击下载ZIP)。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/d7928df4b0dabaec7206b6c219549186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4B7tX1FQ7LURKLHMyNY2cw.png"/></div></div></figure><blockquote class="nt nu nv"><p id="764b" class="mo mp nw lt b lu mq ju mr lw ms jx mt nx mu mv mw ny mx my mz nz na nb nc me im bi translated">声明:数据中描述的所有信息都不真实。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/d6dc933c7d9694a3645ea14d72cbe8b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xF9sfl5DeT7q7LsYhvc2dQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/5dfbf9d6b46420945d77372670cb734c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0QBYtBMIx89Gop5oECLljA.png"/></div></div></figure><p id="ca1b" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">这里的<code class="fe oc od oe of b">exit_status</code>是响应变量。注意，我们只给出了<code class="fe oc od oe of b">train.csv</code>和<code class="fe oc od oe of b">test.csv</code>。<code class="fe oc od oe of b">test.csv</code>没有<code class="fe oc od oe of b">exit_status</code>，即仅用于预测。因此，方法是我们需要将<code class="fe oc od oe of b">train.csv</code>分成训练和验证集来训练模型。然后用模型预测<code class="fe oc od oe of b">test.csv</code>中的<code class="fe oc od oe of b">exit_status</code>。</p><p id="fbf3" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">这是一个典型的数据科学技术测试，您有大约30分钟的时间来制作详细的jupyter笔记本和结果。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="23ef" class="kz la it bd lb lc nk le lf lg nl li lj jz nm ka ll kc nn kd ln kf no kg lp lq bi translated">1.数据预处理</h1><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="dae0" class="ok la it of b gy ol om l on oo">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="3d77" class="ok la it of b gy op om l on oo">data = pd.read_csv('train.csv')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/2b6a7c7530352633b37289015b6814c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Biat_8kqvKRcyLeGNeYqbg.png"/></div></div></figure><p id="25e0" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">哎呀，我们看到了NaN，让我们检查一下我们有多少NaN</p><h2 id="da7f" class="ok la it bd lb or os dn lf ot ou dp lj ly ov ow ll ma ox oy ln mc oz pa lp pb bi translated"><strong class="ak">检查NAs </strong></h2><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="3330" class="ok la it of b gy ol om l on oo">data.isnull().sum()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/cfa47d179f0c6062e358f09219720221.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*z1EJkbFpC5cSe9nCgfKiIQ.png"/></div></figure><p id="0f73" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">由于总共有2600行，所以这里使用NAs的行数相对较少。但是，我在这里没有删除NAs，因为如果<code class="fe oc od oe of b">test.csv</code>数据集也有NAs，那么删除训练数据中的NAs将无法使我们预测有NAs的客户的行为。但是，如果<code class="fe oc od oe of b">test.csv</code>没有任何NAs，那么我们可以继续并删除训练数据集中的NAs。但是让我们先来看看<code class="fe oc od oe of b">test.csv</code></p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="8285" class="ok la it of b gy ol om l on oo">test = pd.read_csv('test.csv')<br/>test.isnull().sum()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/d827318b8d0812e63146afc3d1caf8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*F3_dww0SeWMCcsKwfQE8bg.png"/></div></figure><p id="8fdc" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">不出所料，<code class="fe oc od oe of b">test.csv</code>中有NAs。因此，我们将NAs视为一个类别，并假设它对响应变量<code class="fe oc od oe of b">exit_status</code>有贡献。</p><h2 id="c7fa" class="ok la it bd lb or os dn lf ot ou dp lj ly ov ow ll ma ox oy ln mc oz pa lp pb bi translated"><strong class="ak">将</strong> <code class="fe oc od oe of b">exit_status</code> <strong class="ak">中的是-否替换为1–0</strong></h2><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="799b" class="ok la it of b gy ol om l on oo">exit_status_map = {'Yes': 1, 'No': 0}<br/>data['exit_status'] = data['exit_status'].map(exit_status_map)</span></pre><p id="f346" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">这一步在后面很有用，因为响应变量必须是一个数字数组才能输入到RF分类器中。正如我前面提到的，RF模型不能读取字符串或任何非数字数据类型。</p><h2 id="0aab" class="ok la it bd lb or os dn lf ot ou dp lj ly ov ow ll ma ox oy ln mc oz pa lp pb bi translated"><strong class="ak">将数据分为X和y </strong></h2><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="3c6a" class="ok la it of b gy ol om l on oo">y = data.pop('exit_status')<br/>X = data.drop('id',axis = 1)</span></pre><p id="e0c9" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated"><code class="fe oc od oe of b">id</code>列不会给我们的工作增加任何意义，因为它不会影响客户是选择留下还是离开，所以我们应该删除它。</p><h2 id="0571" class="ok la it bd lb or os dn lf ot ou dp lj ly ov ow ll ma ox oy ln mc oz pa lp pb bi translated"><strong class="ak">创建培训和验证集</strong></h2><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="f797" class="ok la it of b gy ol om l on oo">seed = 50  # so that the result is reproducible<br/>from sklearn.model_selection import train_test_split</span><span id="1986" class="ok la it of b gy op om l on oo">X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.333, random_state = seed)</span></pre><p id="db48" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">现在，是时候让NA成为一个类别了。在Python中，<code class="fe oc od oe of b">NaN</code>被认为是NAs。编码后，那些<code class="fe oc od oe of b">NaN</code>将被忽略。因此，用<code class="fe oc od oe of b">na</code>代替<code class="fe oc od oe of b">NaN</code>是有用的，它现在是一个叫做‘na’的类别。稍后编码时会考虑到这一点。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="5a71" class="ok la it of b gy ol om l on oo">X_train = X_train.fillna('na')<br/>X_test = X_test.fillna('na')</span></pre><h2 id="2acf" class="ok la it bd lb or os dn lf ot ou dp lj ly ov ow ll ma ox oy ln mc oz pa lp pb bi translated"><strong class="ak">创建分类变量列表进行编码</strong></h2><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="4947" class="ok la it of b gy ol om l on oo">X_train.dtypes</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/696f3b4bb5cb74e75b4b9bc9b1aa1dca.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*Gb9-AgWhWr9jPab-egTftA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">请注意，只有分类变量具有dtype = object</p></figure><p id="804e" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">现在，让我们创建一个分类变量列表</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="c720" class="ok la it of b gy ol om l on oo">features_to_encode = list(X_train.select_dtypes(include = ['object']).columns) </span><span id="b154" class="ok la it of b gy op om l on oo"># Or alternatively, </span><span id="684a" class="ok la it of b gy op om l on oo">features_to_encode = X_train.columns[X_train.dtypes==object].tolist()  </span></pre><h2 id="c45e" class="ok la it bd lb or os dn lf ot ou dp lj ly ov ow ll ma ox oy ln mc oz pa lp pb bi translated"><strong class="ak">为我们创建一个处理分类特征的构造函数</strong></h2><p id="6722" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly np mv mw ma nq my mz mc nr nb nc me im bi translated">这是我最喜欢的一步，因为通过重新创建这个新的构造函数，当传递到模型中时，我不需要对任何X数据帧进行任何转换。<strong class="lt iu"> <em class="nw">这个构造函数会自动处理分类变量，而不会影响数值变量</em> </strong>。多方便啊！！！</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="eaef" class="ok la it of b gy ol om l on oo">from sklearn.preprocessing import OneHotEncoder<br/>from sklearn.compose import make_column_transformer</span><span id="d2a8" class="ok la it of b gy op om l on oo">col_trans = make_column_transformer(<br/>                        (OneHotEncoder(),features_to_encode),<br/>                        remainder = "passthrough"<br/>                        )</span></pre><p id="3629" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated"><code class="fe oc od oe of b">remainder = 'passthrough'</code>允许构造器忽略那些<strong class="lt iu">不</strong>包含在<code class="fe oc od oe of b">features_to_encode</code>中的变量。</p><p id="8b7e" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">现在我们的输入准备好了。让我们训练射频分类器。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="b4fb" class="kz la it bd lb lc nk le lf lg nl li lj jz nm ka ll kc nn kd ln kf no kg lp lq bi translated">2.训练射频分类器</h1><p id="1440" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly np mv mw ma nq my mz mc nr nb nc me im bi translated">让我们首先创建我们的第一个模型。当然可以从<code class="fe oc od oe of b">rf_classifier = RandomForestClassifier()</code>开始。然而，大多数情况下，这个基本模型的性能不会很好(至少从我的经验来看，你的可能会有所不同)。所以我总是从下面的一组参数开始，作为我的第一个模型。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="e9b0" class="ok la it of b gy ol om l on oo">from sklearn.ensemble import RandomForestClassifier</span><span id="2f01" class="ok la it of b gy op om l on oo">rf_classifier = RandomForestClassifier(<br/>                      min_samples_leaf=50,<br/>                      n_estimators=150,<br/>                      bootstrap=True,<br/>                      oob_score=True,<br/>                      n_jobs=-1,<br/>                      random_state=seed,<br/>                      max_features='auto')</span></pre><p id="5c6c" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">我建议总是从模型开始，因为最好使用袋外样本来估计泛化精度。oob误差估计与通过k倍交叉验证获得的误差估计几乎相同。与许多其他非线性估计器不同，随机森林可以适合一个序列，并在此过程中进行交叉验证。</p><p id="85d8" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">现在，让我们通过使用<code class="fe oc od oe of b">Pipeline</code>来组合我们的分类器和我们之前创建的构造器</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="9c31" class="ok la it of b gy ol om l on oo">from sklearn.pipeline import make_pipeline<br/>pipe = make_pipeline(col_trans, rf_classifier)<br/>pipe.fit(X_train, y_train)</span></pre><p id="692d" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated"><code class="fe oc od oe of b">pipe</code>是一个新的黑盒，由2个组件组成:1。一个构造函数来处理带有分类变量的输入，并转换成正确的类型。从构造函数接收这些新转换的输入的分类器。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="1e6d" class="ok la it of b gy ol om l on oo">y_pred = pipe.predict(X_test)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/e05332eb575cfb48da9148870080fe14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xoSvEGZhsOxgwSVRZGkwnw.png"/></div></div></figure></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="3413" class="kz la it bd lb lc nk le lf lg nl li lj jz nm ka ll kc nn kd ln kf no kg lp lq bi translated">3.评估分类器</h1><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="f047" class="ok la it of b gy ol om l on oo">from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve, f1_score</span></pre><p id="0635" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">准确度= (TP + TN) / (TP+TN+FP+FN)</p><p id="2cf1" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">召回= TP / (TP + FN)</p><p id="1cec" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">精度= TP / (TP + FP)</p><p id="ebb0" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">f1-得分= 2 *精度*召回/(精度+召回)</p><p id="c663" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">在本例中，1为正，0为负</p><blockquote class="nt nu nv"><p id="a623" class="mo mp nw lt b lu mq ju mr lw ms jx mt nx mu mv mw ny mx my mz nz na nb nc me im bi translated">我<!-- -->不会解释上面每个术语的意思，因为这篇文章并不是随机森林算法的详细文档。我想我们都知道这些术语的意思。当然，如果你不确定，可以在评论区问我。</p></blockquote><h2 id="036b" class="ok la it bd lb or os dn lf ot ou dp lj ly ov ow ll ma ox oy ln mc oz pa lp pb bi translated"><strong class="ak">准确度(正确分类样本的分数)</strong></h2><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="b758" class="ok la it of b gy ol om l on oo">accuracy_score(y_test, y_pred)</span><span id="52a6" class="ok la it of b gy op om l on oo">print(f"The accuracy of the model is {round(accuracy_score(y_test,y_pred),3)*100} %")</span><span id="fd0c" class="ok la it of b gy op om l on oo"><strong class="of iu">The accuracy of the model is 91.1%</strong></span></pre><h2 id="a6f7" class="ok la it bd lb or os dn lf ot ou dp lj ly ov ow ll ma ox oy ln mc oz pa lp pb bi translated"><strong class="ak">进行概率预测</strong></h2><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="a2cf" class="ok la it of b gy ol om l on oo">train_probs = pipe.predict_proba(X_train)[:,1] <br/>probs = pipe.predict_proba(X_test)[:, 1]</span><span id="ca57" class="ok la it of b gy op om l on oo">train_predictions = pipe.predict(X_train)</span></pre><p id="74cf" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated"><code class="fe oc od oe of b">predict_proba(dataframe)[:,1]</code>给出数据帧中类别标签1的预测概率分布。这对于计算ROC_AUC分数很重要。你可能会问为什么类的标签是1而不是0。以下是我从sklearn文档中得到的内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/cebceaad40dccaeae73d4100b411e99e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N77WRZVkF9eeHkjhM_W7mA.png"/></div></div></figure><p id="6adb" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">对于<code class="fe oc od oe of b">y_score</code>，‘<em class="nw">二进制情况…分数必须是标签</em> <strong class="lt iu"> <em class="nw">大于</em></strong><em class="nw"/>的类的分数。这就是为什么我们需要得到标签1而不是标签0。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="1630" class="ok la it of b gy ol om l on oo">print(f'Train ROC AUC Score: {roc_auc_score(y_train, train_probs)}')</span><span id="d104" class="ok la it of b gy op om l on oo">print(f'Test ROC AUC  Score: {roc_auc_score(y_test, probs)}')<br/></span><span id="24bc" class="ok la it of b gy op om l on oo"><strong class="of iu">Train ROC AUC Score: 0.9678578659647703 <br/>Test ROC AUC Score: 0.967591183178179</strong></span></pre><p id="824b" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">现在，我们需要绘制ROC曲线</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="4f1e" class="ok la it of b gy ol om l on oo">def evaluate_model(y_pred, probs,train_predictions, train_probs):</span><span id="8df5" class="ok la it of b gy op om l on oo">    baseline = {}</span><span id="9325" class="ok la it of b gy op om l on oo">    baseline['recall']=recall_score(y_test,</span><span id="3392" class="ok la it of b gy op om l on oo">                    [1 for _ in range(len(y_test))])</span><span id="b01d" class="ok la it of b gy op om l on oo">    baseline['precision'] = precision_score(y_test,</span><span id="7379" class="ok la it of b gy op om l on oo">                    [1 for _ in range(len(y_test))])</span><span id="c111" class="ok la it of b gy op om l on oo">    baseline['roc'] = 0.5</span><span id="b8c7" class="ok la it of b gy op om l on oo">    results = {}</span><span id="ee81" class="ok la it of b gy op om l on oo">    results['recall'] = recall_score(y_test, y_pred)</span><span id="7bc8" class="ok la it of b gy op om l on oo">    results['precision'] = precision_score(y_test, y_pred)</span><span id="80bb" class="ok la it of b gy op om l on oo">    results['roc'] = roc_auc_score(y_test, probs)</span><span id="fceb" class="ok la it of b gy op om l on oo">    train_results = {}</span><span id="1b4a" class="ok la it of b gy op om l on oo">    train_results['recall'] = recall_score(y_train,       train_predictions)</span><span id="de89" class="ok la it of b gy op om l on oo">    train_results['precision'] = precision_score(y_train, train_predictions)</span><span id="95b2" class="ok la it of b gy op om l on oo">    train_results['roc'] = roc_auc_score(y_train, train_probs)</span><span id="1287" class="ok la it of b gy op om l on oo">    for metric in ['recall', 'precision', 'roc']:  </span><span id="4ffd" class="ok la it of b gy op om l on oo">          print(f'{metric.capitalize()} <br/>                 Baseline: {round(baseline[metric], 2)} <br/>                 Test: {round(results[metric], 2)} <br/>                 Train: {round(train_results[metric], 2)}')</span><span id="711f" class="ok la it of b gy op om l on oo">     <strong class="of iu"># Calculate false positive rates and true positive rates</strong></span><span id="4fda" class="ok la it of b gy op om l on oo">    base_fpr, base_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])</span><span id="054b" class="ok la it of b gy op om l on oo">    model_fpr, model_tpr, _ = roc_curve(y_test, probs)</span><span id="3794" class="ok la it of b gy op om l on oo">    plt.figure(figsize = (8, 6))<br/>    plt.rcParams['font.size'] = 16</span><span id="8341" class="ok la it of b gy op om l on oo">    # Plot both curves</span><span id="396c" class="ok la it of b gy op om l on oo">    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')<br/>    plt.plot(model_fpr, model_tpr, 'r', label = 'model')<br/>    plt.legend();</span><span id="ce53" class="ok la it of b gy op om l on oo">    plt.xlabel('False Positive Rate');<br/>    plt.ylabel('True Positive Rate'); plt.title('ROC Curves');<br/>    plt.show();</span><span id="748c" class="ok la it of b gy op om l on oo">evaluate_model(y_pred,probs,train_predictions,train_probs)</span><span id="d657" class="ok la it of b gy op om l on oo"><strong class="of iu"><br/>Recall Baseline: 1.0 Test: 0.92 Train: 0.93 <br/>Precision Baseline: 0.48 Test: 0.9 Train: 0.91 <br/>Roc Baseline: 0.5 Test: 0.97 Train: 0.97</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/935b87d63afee357208d8881c9167256.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*G-hZUiFHFN8YF97o2I6zqw.png"/></div></figure><p id="e744" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">结果看起来不错。测试和训练结果之间的差异非常小，表明我们的模型没有过度拟合数据。</p><h2 id="8698" class="ok la it bd lb or os dn lf ot ou dp lj ly ov ow ll ma ox oy ln mc oz pa lp pb bi translated"><strong class="ak">混乱矩阵</strong></h2><p id="8e1e" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly np mv mw ma nq my mz mc nr nb nc me im bi translated">人们可以简单地输入<code class="fe oc od oe of b">confusion_matrix(y_test, y_pred)</code>来得到混淆矩阵。然而，让我们采取更高级的方法。在这里，我创建一个函数来绘制混淆矩阵，这个函数<strong class="lt iu">打印并绘制混淆矩阵。</strong>(改编自<a class="ae ky" href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html" rel="noopener ugc nofollow" target="_blank">代码来源</a>)</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="f272" class="ok la it of b gy ol om l on oo">import itertools</span><span id="7728" class="ok la it of b gy op om l on oo">def plot_confusion_matrix(cm, classes, normalize = False,<br/>                          title='Confusion matrix',<br/>                          cmap=plt.cm.Greens): # can change color </span><span id="886c" class="ok la it of b gy op om l on oo">    plt.figure(figsize = (10, 10))<br/>    plt.imshow(cm, interpolation='nearest', cmap=cmap)<br/>    plt.title(title, size = 24)<br/>    plt.colorbar(aspect=4)</span><span id="cd4e" class="ok la it of b gy op om l on oo">    tick_marks = np.arange(len(classes))<br/>    plt.xticks(tick_marks, classes, rotation=45, size = 14)<br/>    plt.yticks(tick_marks, classes, size = 14)</span><span id="6dab" class="ok la it of b gy op om l on oo">    fmt = '.2f' if normalize else 'd'<br/>    thresh = cm.max() / 2.</span><span id="bada" class="ok la it of b gy op om l on oo">    # Label the plot</span><span id="eafa" class="ok la it of b gy op om l on oo">    for i, j in itertools.product(range(cm.shape[0]),   range(cm.shape[1])):</span><span id="a5a8" class="ok la it of b gy op om l on oo">    plt.text(j, i, format(cm[i, j], fmt), <br/>             fontsize = 20,<br/>             horizontalalignment="center",<br/>             color="white" if cm[i, j] &gt; thresh else "black")</span><span id="2cf3" class="ok la it of b gy op om l on oo">    plt.grid(None)<br/>    plt.tight_layout()<br/>    plt.ylabel('True label', size = 18)<br/>    plt.xlabel('Predicted label', size = 18)</span><span id="8a78" class="ok la it of b gy op om l on oo"><br/># Let's plot it out</span><span id="edf0" class="ok la it of b gy op om l on oo">cm = confusion_matrix(y_test, y_pred)<br/>plot_confusion_matrix(cm, classes = ['0 - Stay', '1 - Exit'],<br/>                      title = 'Exit_status Confusion Matrix')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/b8ba39ad961297a70659578f8c64be4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4TMlcutGT8fbOFaBWKXsw.png"/></div></div></figure></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="0f1c" class="kz la it bd lb lc nk le lf lg nl li lj jz nm ka ll kc nn kd ln kf no kg lp lq bi translated">4.特征重要性</h1><p id="3bdf" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly np mv mw ma nq my mz mc nr nb nc me im bi translated">首先，让我们检查模型中有多少个特征重要性值</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="c047" class="ok la it of b gy ol om l on oo">print(rf_classifier.feature_importances_)</span><span id="3e48" class="ok la it of b gy op om l on oo">print(f" There are {len(rf_classifier.feature_importances_)} features in total")</span><span id="1a88" class="ok la it of b gy op om l on oo"><strong class="of iu">[7.41626071e-04 6.12165359e-04 1.42322746e-03 6.93254520e-03  2.93650843e-04 1.96706074e-04 1.85830433e-03 2.67517842e-03  1.02110066e-05 2.99006245e-05 6.15325794e-03 1.66647237e-02  4.49100748e-03 3.37963818e-05 1.87449830e-03 1.00225588e-03  3.72119245e-04 1.39558189e-02 8.28073088e-04 3.41692010e-04  1.71733193e-04 7.60943914e-02 1.09485070e-02 1.78380970e-02  1.63392715e-02 2.93397339e-03 1.46445733e-02 1.34849432e-01  1.33144331e-02 4.42753783e-02 3.13204793e-03 4.97894324e-03  6.17692498e-03 2.70959923e-02 1.61849449e-03 7.57024010e-02  2.31468190e-02 4.66247828e-01]  </strong></span><span id="7a51" class="ok la it of b gy op om l on oo"><strong class="of iu">There are 38 features in total</strong></span></pre><p id="88a4" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">总共有38个特征。但是，X_train只有15列。这是因为模型<code class="fe oc od oe of b">pipe</code>自动对X_train中的分类变量进行编码。例如，X_train中的<code class="fe oc od oe of b">gender</code>列被转换为2列<code class="fe oc od oe of b">Female</code>和<code class="fe oc od oe of b">Male</code>。</p><p id="2bae" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">因此，为了将特性与从<code class="fe oc od oe of b">rf_classifier</code>获得的特性重要性值相匹配，我们需要在编码的X_train中获得所有那些<strong class="lt iu">对应的</strong>列。</p><blockquote class="nt nu nv"><p id="2ffc" class="mo mp nw lt b lu mq ju mr lw ms jx mt nx mu mv mw ny mx my mz nz na nb nc me im bi translated"><strong class="lt iu">问题</strong>:</p><p id="da29" class="mo mp nw lt b lu mq ju mr lw ms jx mt nx mu mv mw ny mx my mz nz na nb nc me im bi translated"><strong class="lt iu">我们只有一个特征重要性数组，但是有分类和数字特征，我们如何知道哪个值属于哪个特征？</strong></p></blockquote><p id="536e" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">还记得我们之前创建的构造函数<code class="fe oc od oe of b">col_trans</code>吗？<code class="fe oc od oe of b">col_trans.fit_transform(X_train)</code>将给出编码的X_train。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="f5c8" class="ok la it of b gy ol om l on oo"># Let's look at the first row<br/>print(col_trans.fit_transform(X_train)[0,:])</span><span id="f621" class="ok la it of b gy op om l on oo"><strong class="of iu">[ 0.    1.    0.    0.    0.    1.    1.    0.    0.    1.    1.    0.   0.    0.    1.    0.    0.    0.    1.    0.    0.    1.    0.    0.   1.    0.    0.    1.    0.    0.    0.    1.    0.    1.    0.   30.  74.75 14.  ]</strong></span><span id="b8d7" class="ok la it of b gy op om l on oo"># And the first row of X_train<br/>X_train.iloc[0,:]                           </span><span id="ed93" class="ok la it of b gy op om l on oo"><strong class="of iu">gender                             Male<br/>age                                 &gt;60<br/>dependents                           No<br/>lifetime                             30<br/>phone_services                      Yes<br/>internet_services                    3G<br/>online_streaming             Major User<br/>multiple_connections                 No<br/>premium_plan                         No<br/>online_protect                       No<br/>contract_plan            Month-to-month<br/>ebill_services                      Yes<br/>default_payment         Online Transfer<br/>monthly_charges                   74.75<br/>issues                               14<br/>Name: 1258, dtype: object</strong></span></pre><p id="0f0b" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">对于X_train，有3个数值变量，值分别为30、70.75和14。对于编码的X_train，这3个数值放在所有分类变量之后。这意味着对于<code class="fe oc od oe of b">rf_classifier.feature_importances_</code> <strong class="lt iu">，首先显示所有编码的分类变量，随后是数值变量</strong></p><p id="46fe" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">好了，现在我们知道了，让我们创建一个合适的编码X_train。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="e3fa" class="ok la it of b gy ol om l on oo">def encode_and_bind(original_dataframe, features_to_encode):</span><span id="0060" class="ok la it of b gy op om l on oo">    dummies = pd.get_dummies(original_dataframe[features_to_encode])<br/>    res = pd.concat([dummies, original_dataframe], axis=1)<br/>    res = res.drop(features_to_encode, axis=1)<br/>    return(res)</span><span id="5981" class="ok la it of b gy op om l on oo">X_train_encoded = encode_and_bind(X_train, features_to_encode)</span></pre><p id="911f" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">函数<code class="fe oc od oe of b">encode_and_bind</code>对分类变量进行编码，然后将它们与原始数据帧相结合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/c7dce12cc5c3f90ded1590e09f31c67b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sk7O_rWkaURJQxeSaGbSHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">x _火车_编码</p></figure><p id="5fa0" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">酷，现在我们有38列，这和之前在<code class="fe oc od oe of b">rf_classifier.feature_importances_</code>中显示的38个特性完全一样。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="40ca" class="ok la it of b gy ol om l on oo">feature_importances = list(zip(X_train_encoded, rf_classifier.feature_importances_))</span><span id="d349" class="ok la it of b gy op om l on oo"><strong class="of iu"># Then sort the feature importances by most important first</strong><br/>feature_importances_ranked = sorted(feature_importances, key = lambda x: x[1], reverse = True)</span><span id="3b76" class="ok la it of b gy op om l on oo"><strong class="of iu"># Print out the feature and importances<br/></strong>[print('Feature: {:35} Importance: {}'.format(*pair)) for pair in feature_importances_ranked];</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/c02a87cb96b053ebc0e55d374698f786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TzdJzq5GHIHGfwE7BSNoJA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一些特征及其重要性分数(降序)</p></figure><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="a473" class="ok la it of b gy ol om l on oo"><strong class="of iu"># Plot the top 25 feature importance</strong></span><span id="5331" class="ok la it of b gy op om l on oo">feature_names_25 = [i[0] for i in feature_importances_ranked[:25]]<br/>y_ticks = np.arange(0, len(feature_names_25))<br/>x_axis = [i[1] for i in feature_importances_ranked[:25]]</span><span id="aced" class="ok la it of b gy op om l on oo">plt.figure(figsize = (10, 14))<br/>plt.barh(feature_names_25, x_axis)   #horizontal barplot<br/>plt.title('Random Forest Feature Importance (Top 25)',<br/>          fontdict= {'fontname':'Comic Sans MS','fontsize' : 20})</span><span id="de42" class="ok la it of b gy op om l on oo">plt.xlabel('Features',fontdict= {'fontsize' : 16})<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/c8e8501bd3cc86df140cc0d8eb3ef218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*16a9ndleXErWwQw9BbNVsg.png"/></div></div></figure></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="78c9" class="kz la it bd lb lc nk le lf lg nl li lj jz nm ka ll kc nn kd ln kf no kg lp lq bi translated">5.使用RandomSearchCV调整超参数</h1><p id="50b4" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly np mv mw ma nq my mz mc nr nb nc me im bi translated">由于模型性能非常好，具有很高的准确度、精确度和召回率，因此实际上几乎不需要对模型进行调优。但是，如果我们的第一个模型表现不佳，可以采取以下步骤来调整模型。</p><p id="0c47" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">让我们看看目前使用的参数</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="3ee9" class="ok la it of b gy ol om l on oo">from pprint import pprint</span><span id="049d" class="ok la it of b gy op om l on oo">print('Parameters currently in use:\n')<br/>pprint(rf_classifier.get_params())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/6d7f6d7563f88632adca1d1942e4f77e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*deG-CroyDe-KJf9HrkSIzg.png"/></div></div></figure><p id="40d4" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">现在，我为模型创建了一个参数网格，以便<strong class="lt iu">随机</strong>挑选和训练，因此命名为随机搜索。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="5953" class="ok la it of b gy ol om l on oo"><strong class="of iu">from sklearn.model_selection import RandomizedSearchCV</strong></span><span id="b35b" class="ok la it of b gy op om l on oo">n_estimators = [int(x) for x in np.linspace(start = 100, stop = 700, num = 50)]</span><span id="bec2" class="ok la it of b gy op om l on oo">max_features = ['auto', 'log2']  # Number of features to consider at every split</span><span id="9f33" class="ok la it of b gy op om l on oo">max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]   # Maximum number of levels in tree</span><span id="d8b2" class="ok la it of b gy op om l on oo">max_depth.append(None)</span><span id="8b03" class="ok la it of b gy op om l on oo">min_samples_split = [2, 5, 10]  # Minimum number of samples required to split a node</span><span id="28d4" class="ok la it of b gy op om l on oo">min_samples_leaf = [1, 4, 10]    # Minimum number of samples required at each leaf node</span><span id="500e" class="ok la it of b gy op om l on oo">bootstrap = [True, False]       # Method of selecting samples for training each tree<br/></span><span id="e37b" class="ok la it of b gy op om l on oo">random_grid = {'n_estimators': n_estimators,</span><span id="a6ee" class="ok la it of b gy op om l on oo">               'max_features': max_features,</span><span id="2e0c" class="ok la it of b gy op om l on oo">               'max_depth': max_depth,</span><span id="a69c" class="ok la it of b gy op om l on oo">               'min_samples_split': min_samples_split,</span><span id="d672" class="ok la it of b gy op om l on oo">               'min_samples_leaf': min_samples_leaf,</span><span id="1040" class="ok la it of b gy op om l on oo">               'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),</span><span id="c4c2" class="ok la it of b gy op om l on oo">               'bootstrap': bootstrap}</span></pre><p id="096a" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">现在，我首先创建一个基础模型，然后使用随机网格根据ROC_AUC得分选择最佳模型，因此有了<code class="fe oc od oe of b">scoring = 'roc_auc'</code>。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="f18a" class="ok la it of b gy ol om l on oo"><strong class="of iu"># Create base model to tune</strong></span><span id="8efc" class="ok la it of b gy op om l on oo">rf = RandomForestClassifier(oob_score=True)</span><span id="09e0" class="ok la it of b gy op om l on oo"><strong class="of iu"># Create random search model and fit the data</strong></span><span id="97e9" class="ok la it of b gy op om l on oo">rf_random = RandomizedSearchCV(</span><span id="cbf9" class="ok la it of b gy op om l on oo">                        estimator = rf,</span><span id="5d63" class="ok la it of b gy op om l on oo">                        param_distributions = random_grid,</span><span id="8e7d" class="ok la it of b gy op om l on oo">                        n_iter = 100, cv = 3,</span><span id="7405" class="ok la it of b gy op om l on oo">                        verbose=2, random_state=seed, </span><span id="d0e6" class="ok la it of b gy op om l on oo">                        scoring='roc_auc')</span><span id="e163" class="ok la it of b gy op om l on oo">rf_random.fit(X_train_encoded, y_train)</span><span id="c850" class="ok la it of b gy op om l on oo">rf_random.best_params_</span><span id="f949" class="ok la it of b gy op om l on oo">rf_random.best_params_</span><span id="d86c" class="ok la it of b gy op om l on oo"><strong class="of iu">{'n_estimators': 206,<br/> 'min_samples_split': 5,<br/> 'min_samples_leaf': 10,<br/> 'max_leaf_nodes': 44,<br/> 'max_features': 'auto',<br/> 'max_depth': 90,<br/> 'bootstrap': True}</strong></span></pre><p id="0aff" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">我们将用三重交叉验证进行100次迭代。关于参数的更多信息可以在<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="76e9" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated"><strong class="lt iu">或者</strong>，我们可以再次使用<code class="fe oc od oe of b">pipe</code>，这样我们就不需要编码数据了</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="196d" class="ok la it of b gy ol om l on oo">rf = RandomForestClassifier(oob_score=True, n_jobs=-1)</span><span id="05c9" class="ok la it of b gy op om l on oo">rf_random = RandomizedSearchCV(</span><span id="5007" class="ok la it of b gy op om l on oo">                estimator = rf,</span><span id="66a1" class="ok la it of b gy op om l on oo">                param_distributions = random_grid,</span><span id="9460" class="ok la it of b gy op om l on oo">                n_iter = 50, cv = 3,</span><span id="3bb6" class="ok la it of b gy op om l on oo">                verbose=1, random_state=seed,</span><span id="1018" class="ok la it of b gy op om l on oo">                scoring='roc_auc')</span><span id="21ce" class="ok la it of b gy op om l on oo">pipe_random = make_pipeline(col_trans, rf_random)</span><span id="9c96" class="ok la it of b gy op om l on oo">pipe_random.fit(X_train, y_train)</span><span id="a27b" class="ok la it of b gy op om l on oo">rf_random.best_params_</span></pre><p id="8e24" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">请注意，这两种方法给出的答案可能略有不同。这是由于选择参数的随机性。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="ec4f" class="ok la it of b gy ol om l on oo"><strong class="of iu"># To look at nodes and depths of trees use on average</strong></span><span id="3f8d" class="ok la it of b gy op om l on oo">n_nodes = []</span><span id="e96d" class="ok la it of b gy op om l on oo">max_depths = []</span><span id="4ef3" class="ok la it of b gy op om l on oo">for ind_tree in best_model.estimators_:</span><span id="b4d9" class="ok la it of b gy op om l on oo">       n_nodes.append(ind_tree.tree_.node_count)</span><span id="3696" class="ok la it of b gy op om l on oo">       max_depths.append(ind_tree.tree_.max_depth)</span><span id="64a8" class="ok la it of b gy op om l on oo">print(f'Average number of nodes {int(np.mean(n_nodes))}')   print(f'Average maximum depth {int(np.mean(max_depths))}')  </span><span id="e6a5" class="ok la it of b gy op om l on oo"><strong class="of iu">Average number of nodes 82 <br/>Average maximum depth 9</strong></span></pre><p id="bd19" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated"><strong class="lt iu">评估最佳车型</strong></p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="9a8d" class="ok la it of b gy ol om l on oo"><strong class="of iu"># Use the best model after tuning</strong></span><span id="4a98" class="ok la it of b gy op om l on oo">best_model = rf_random.best_estimator_</span><span id="38c5" class="ok la it of b gy op om l on oo">pipe_best_model = make_pipeline(col_trans, best_model)</span><span id="24e5" class="ok la it of b gy op om l on oo">pipe_best_model.fit(X_train, y_train)</span><span id="d00b" class="ok la it of b gy op om l on oo">y_pred_best_model = pipe_best_model.predict(X_test)</span></pre><p id="4053" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">第3节中使用的相同代码可以再次应用于ROC和混淆矩阵。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="e9d0" class="ok la it of b gy ol om l on oo">train_rf_predictions = pipe_best_model.predict(X_train)</span><span id="9d19" class="ok la it of b gy op om l on oo">train_rf_probs = pipe_best_model.predict_proba(X_train)[:, 1]</span><span id="2455" class="ok la it of b gy op om l on oo">rf_probs = pipe_best_model.predict_proba(X_test)[:, 1]</span><span id="05eb" class="ok la it of b gy op om l on oo"><strong class="of iu"># Plot ROC curve and check scores</strong></span><span id="b5c2" class="ok la it of b gy op om l on oo">evaluate_model(y_pred_best_model, rf_probs, train_rf_predictions, train_rf_probs)</span><span id="040d" class="ok la it of b gy op om l on oo"><strong class="of iu">Recall Baseline: 1.0 Test: 0.94 Train: 0.95 <br/>Precision Baseline: 0.48 Test: 0.9 Train: 0.91 <br/>Roc Baseline: 0.5 Test: 0.97 Train: 0.98</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/8abc306e8792eaec0a01b7b03f518e86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*d7NfEXB1uUJFAnHUPwb3DA.png"/></div></figure><p id="3cc7" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">这组参数使模型的性能比我们的原始模型稍好一些。差别并不大。这是可以理解的，因为我们的原始模型已经表现得很好，得分更高(准确度、精确度、召回率)。因此，调整一些超参数可能不会给模型带来任何显著的改进。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="2992" class="ok la it of b gy ol om l on oo"><strong class="of iu"># Plot Confusion matrix</strong></span><span id="2ef1" class="ok la it of b gy op om l on oo">plot_confusion_matrix(confusion_matrix(y_test, y_pred_best_model), classes = ['0 - Stay', '1 - Exit'],</span><span id="9a11" class="ok la it of b gy op om l on oo">title = 'Exit_status Confusion Matrix')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/110fcb5c4af73ff8d459630441eb3934.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*56MdSeZnETOrfV6OvuZavQ.png"/></div></div></figure></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="6de3" class="kz la it bd lb lc nk le lf lg nl li lj jz nm ka ll kc nn kd ln kf no kg lp lq bi translated">对<code class="fe oc od oe of b">test.csv</code>数据使用最佳模型</h1><p id="50b7" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly np mv mw ma nq my mz mc nr nb nc me im bi translated">现在我们有了最好的模型，让我们用它来进行预测，并编译我们的最终答案来提交。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="1761" class="ok la it of b gy ol om l on oo">test = pd.read_csv('test.csv')<br/>test_withoutID = test.copy().drop('id', axis = 1)<br/>test_withoutID = test_withoutID.fillna('na')</span><span id="8c30" class="ok la it of b gy op om l on oo">final_y = pipe_best_model.predict(test_withoutID)<br/>#<!-- -->pipe<!-- --> model only takes in dataframe without ID column.<br/></span><span id="b92f" class="ok la it of b gy op om l on oo">final_report = test<br/>final_report['exit_status'] = final_y<br/>final_report = final_report.loc[:,['id','exit_status']]</span><span id="a997" class="ok la it of b gy op om l on oo"># Replace 1-0 with Yes-No to make it interpretable</span><span id="672c" class="ok la it of b gy op om l on oo">final_report= final_report.replace(1, 'Yes')<br/>final_report= final_report.replace(0, 'No')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/fac9b576573e83e625acaea6703b0863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b4Du72imUiuFubOxEiVm2w.png"/></div></div></figure><p id="784d" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">我们都看到了<code class="fe oc od oe of b">No</code>。让我们检查以确保该列中也有<code class="fe oc od oe of b">Yes</code>。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="0baf" class="ok la it of b gy ol om l on oo">final_report.exit_status.value_counts()</span><span id="d0a9" class="ok la it of b gy op om l on oo"><strong class="of iu">No     701<br/>Yes    638<br/>Name: exit_status, dtype: int64</strong></span></pre><p id="5a38" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">好了，我们安全了~</p><h1 id="a09e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">保存并提交文件</h1><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="63c8" class="ok la it of b gy ol om l on oo">final_report.to_csv('submissions.csv', index=False)</span></pre></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="7bae" class="kz la it bd lb lc nk le lf lg nl li lj jz nm ka ll kc nn kd ln kf no kg lp lq bi translated">结论</h1><p id="3e8e" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly np mv mw ma nq my mz mc nr nb nc me im bi translated">我希望这也能成为你们有用的参考指南。您可以使用本指南来准备一些技术测试，或者将它作为一个备忘单来复习如何用Python实现随机森林分类器。我一定会继续更新，因为我发现了更多有用的函数或技巧，我认为它们可以帮助每个人快速解决许多数据科学问题，而不需要在StackOverflow这样的平台上查找，特别是在时间有限的技术测试中。</p><p id="bd90" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">如果还有其他你认为重要的技巧或功能，请在下面的评论区告诉我。此外，我欢迎任何人的建设性反馈。</p><p id="6973" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">谢谢你的阅读。祝您愉快，编程愉快！</p></div></div>    
</body>
</html>