# 透过树看到森林

> 原文：<https://towardsdatascience.com/seeing-the-forest-through-the-trees-45deafe1a6f0?source=collection_archive---------42----------------------->

## 一窥决策树和随机森林算法的内部工作原理。

![](img/60053a226e91a2a03f8a9efa18872785.png)

在 [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上 [veeterzy](https://unsplash.com/@veeterzy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 拍摄的照片

## 决策树如何工作

形象地说，决策树就像一个流程图，其中父节点代表一个属性的测试，叶节点代表分配给该叶节点的数据点的最终类别。

![](img/fcfcf5972e7789865cfbe664d4d3b6b1.png)

图 1-学生样本分布

在上图中，从学生成绩数据集中随机抽取了 13 名学生。散点图显示了基于两个属性的样本分布:

1.  举手次数:学生在课堂上举手提问或回答问题的次数。
2.  visitedResources:学生访问课程内容的次数。

我们的目的是手动构建一个决策树，该决策树能够*最好地将样本数据点分成*不同的类别——L，M，H，其中:

**L** =较低性能类别

**M** =中等(平均)性能类别

**H** =高性能类别

![](img/af517ce5aff098a5580e10f072e52a8e.png)

选项 A

一种选择是在点标记 70 处沿着属性 visitedResources 分割数据。

这“完美地”将 H 类与其他类区分开来。

![](img/8134d362b65e3e83fee279410e512fe4.png)

选项 B

另一种选择是在点标记 41 处沿同一属性 visitedResources 进行分割。

任何类别都无法实现“完美”的分离。

![](img/1cf509f9cce289a95dec1e8ab3ca999f.png)

选项 C

另一个选项是在点标记 38 处沿属性“举手”进行分割。

这“完美地”将 L 类与其他类区分开来。

选项 A 和 C 在分离至少一个类方面做得更好。假设我们选择选项 A，结果决策三将是:

![](img/1903306eea5caf273f3c239f4448696b.png)

左边的分支只有 H 班的学生，因此不能再分开了。在右边的分支上，结果节点在 M 和 L 班各有四名学生。

![](img/f0418acccea3ed2b50b384c691e11ced.png)

请记住，这是我们分离练习的当前状态。

如何最好地将剩余的学生(数据点)划分到相应的班级？是的，你猜对了——多画几条线！

![](img/550112a4aee33c32257eb305b3e3bf74.png)

一种选择是在点标记 38 处沿着属性“举手”进行分割。

同样，可以绘制任意数量的分割线，然而，这个选项似乎产生了一个好的结果，所以，我们将使用它。

分割后得到的决策树如下所示:

![](img/5a44f087de0a9f125e764a1eabeaf5b8.png)

显然，数据点被完美地分成了适当的类，因此不需要进一步的逻辑分离。

## 迄今吸取的经验教训:

1.  用 ML 的说法，这个建立一个对给定数据集进行最佳分类的决策树的过程被解释或称为*学习*。
2.  这个学习过程是迭代的。
3.  根据所做的拆分属性选择和允许的树深度，可以从同一个数据集中导出不同预测准确度级别的几个决策树。

在手动构建决策树的过程中，我们了解到分隔线可以沿着数据集中可用的任何属性在任何点绘制。问题是，在任何给定的决策节点，哪种可能的属性和分离点会更好地将数据集分离为所需或接近所需的类或类别？确定这个问题答案的工具是基尼系数。

## 基尼杂质

假设我们有一个新生，我们根据班级的概率分布将这个新生随机分为三个班级。基尼系数是对新的随机学生(变量)进行错误分类的可能性的度量。这是一个概率度量，因此它的范围在 0 和 1 之间。

![](img/ecfffaf896d2d7e2d6c76297c747fa26.png)

我们的样本数据集中总共有 13 名学生，H、M 和 L 类的概率分布分别为 5/13、4/13 和 4/13。

以下公式用于计算基尼系数杂质:

![](img/0c1dfd4ca4fe5095b365beaadc35c1ba.png)

上述公式在我们的示例中应用时变成:

![](img/d17cbfaa59cc4d633a055a92bfb21693.png)

因此，在任何分割之前，决策树的根节点处的 gini 杂质将被计算为:

![](img/3604b26b8a87098b885511242690167b.png)

回想一下前面讨论的根节点处的拆分选项 A 和 C，让我们比较一下这两个选项的基尼系数，看看为什么选择 A 作为更好的拆分选项。

![](img/f4ebe258f7c8b68647860cc1e6f8e38c.png)

选项 A

![](img/a0af2c748cdd75815f47241918a34cc0.png)

选项 C

因此，用分割选项 A 去除的杂质数量—基尼系数为:0.66–0.3 =**0.36**。而拆分选项 C 的值为:0.66–0.37 =**0.29**。

很明显，基尼系数 0.36>0.29，因此，选项 A 是一个更好的分割选择，告知早先选择 A 而不是 c 的决定。

在一个节点上，所有的学生都是一个班的，比如说 H，基尼系数总是等于零，这意味着没有杂质。这意味着一个完美的分类，因此，没有进一步的分裂是必要的。

## 随机森林

我们已经看到，可以从同一个数据集生成许多决策树，并且这些树在正确预测未知示例方面的性能可能会有所不同。此外，使用单一的树模型(决策树)很容易导致过度拟合。

问题变成了:我们如何确保构建尽可能好的性能树？对此的答案是智能地构造尽可能多的树，并使用平均来提高预测精度和控制过拟合。这种方法称为随机森林。它是随机的，因为每个树不是使用所有的训练数据集而是数据集和属性的随机样本来构造的。

我们将使用 Scikit-learn python 包中的随机森林算法实现来演示如何训练、测试随机森林模型，以及可视化构成森林的一棵树。

在本练习中，我们将训练一个随机森林模型，根据学生在课堂/学习过程中的参与程度来预测(分类)学生所属的学术表现类别(班级)。

在本练习的[数据集](https://www.kaggle.com/aljarah/xAPI-Edu-Data)中，学生的参与被定义为四个变量的衡量标准，它们是:

1.  **举手:**学生在课堂上举手提问或回答问题的次数(数字:0-100)
2.  **访问过的资源:**学生访问课程内容的次数(数字:0–100)
3.  **查看公告:**学生查看新闻公告的次数(数字:0–100)
4.  **讨论组:**学生参加讨论组的次数(数字:0-100)

在下面的示例摘录中，前四(4)个数字列对应于前面定义的学生参与度，最后一列——类别，代表学生的表现。一名学生可以参加三(3)个班中的任何一个——低、中、高。

![](img/a7804468ed74a11807a25a7bd41df75c.png)

图 1:数据集摘录:学生参与度和表现类

基本数据准备步骤:

1.  加载数据集
2.  清理或预处理数据。该数据集中的所有要素都已采用正确的格式，并且不存在缺失值。根据我的经验，在 ML 项目中很少出现这种情况，因为通常需要某种程度的清理或预处理。
3.  编码标签。这是必要的，因为该数据集中的标签(类)是分类的。
4.  将数据集分成训练集和测试集。

上述所有步骤的实现如下面的代码片段所示:

接下来，我们将创建一个 RandomForest 实例，并使模型适合(构建树)训练集。

其中:

1.  **n_estimators** =组成森林的树木数量
2.  **标准** =为决策树挑选最佳属性分割选项时使用的方法。在这里，我们看到基尼系数被使用。
3.  这是树木深度的上限。如果在这个深度，没有得到明确的分类，模型将认为该层的所有节点是叶节点。此外，对于每个叶节点，数据点被分类为该节点中的多数类。

注意，最佳 n 估计量和 max_depth 组合只能通过试验几种组合来确定。实现这一点的一种方法是使用网格搜索方法。

## 模型评估

虽然存在几种评估模型的度量标准，但我们将使用其中一种(如果不是最基本的话)——准确性。

在**训练集上的准确率:72.59%** ，**测试集上的准确率:68.55%——**可以更好，但不是一个坏的基准。

## 想象森林中最好的树

随机森林模型中的最优树可以很容易地可视化，使工程师、科学家和商业专家对模型的决策流程有所了解。

下面的代码片段从上面训练的模型中提取并可视化了最佳树:

![](img/e02820363ced5cfcbd5210b247c25981.png)

从随机森林中提取的决策树。

## 结论:

在本文中，我们成功地了解了决策树的工作原理，了解了如何使用 gini 杂质进行属性分割选择，如何将几个决策树组合起来形成随机森林，最后，通过训练随机森林模型来演示随机森林算法的用法，以根据学生在课堂/学习过程中的参与程度将他们分类到学术表现类别中。

*感谢阅读。*