<html>
<head>
<title>Reviewing Essential Concepts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习基本概念</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reviewing-essential-concepts-from-part-1-e28234ee7f4f?source=collection_archive---------32-----------------------#2020-07-12">https://towardsdatascience.com/reviewing-essential-concepts-from-part-1-e28234ee7f4f?source=collection_archive---------32-----------------------#2020-07-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4e53" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/deep-r-l-explained" rel="noopener" target="_blank">深度强化学习讲解— 12 </a></h2><div class=""/><div class=""><h2 id="1f8e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">数学符号已更新</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fa0f6596bcb8f7d913d0517d5a751d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDhcpkgdzkisARJU93i5xw.png"/></div></div></figure><p id="d3cb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这篇文章是<em class="lz"/><a class="ae ma" rel="noopener" target="_blank" href="/value-iteration-for-q-function-ac9e508d85bd"><em class="lz">深度强化学习讲解</em></a><em class="lz"/>系列的新部分的序言，在这里我们将介绍强化学习经典方法的实现，如蒙特卡罗、SARSA 或 Q-learning 等。在本帖中，我们将回顾和更新以前帖子中介绍的数学符号。</p><blockquote class="mb mc md"><p id="dd6e" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><a class="ae ma" href="https://medium.com/aprendizaje-por-refuerzo/3-funciones-de-valor-y-la-ecuaci%C3%B3n-de-bellman-7b0ebfac2be1" rel="noopener">本出版物的西班牙语版本</a></p></blockquote><div class="mh mi gp gr mj mk"><a href="https://medium.com/aprendizaje-por-refuerzo/3-funciones-de-valor-y-la-ecuaci%C3%B3n-de-bellman-7b0ebfac2be1" rel="noopener follow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">3.英勇勋章和贝尔曼勋章</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">访问第 3 页的自由介绍</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">medium.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="2446" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">数学符号复习</h1><p id="2413" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">在<a class="ae ma" rel="noopener" target="_blank" href="/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a">帖子 2 </a>中，我们已经看到，我们可以使用马尔可夫决策过程(MDP)作为我们希望通过强化学习来解决的问题的正式定义。MDP 由 5 个参数<strong class="lf jd"> <em class="lz"> &lt; S，A，R，p，γ &gt;，</em> </strong>定义，其中每个参数表示:</p><ul class=""><li id="4a72" class="nw nx it lf b lg lh lj lk lm ny lq nz lu oa ly ob oc od oe bi translated"><strong class="lf jd"><em class="lz"/></strong>—一组状态</li><li id="01a8" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated"><strong class="lf jd"> <em class="lz">一个</em> </strong> —一组动作</li><li id="b710" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated"><strong class="lf jd"> <em class="lz"> R </em> </strong> —奖励功能</li><li id="37bc" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated"><strong class="lf jd"> <em class="lz"> p </em> </strong> —过渡函数</li><li id="a1df" class="nw nx it lf b lg of lj og lm oh lq oi lu oj ly ob oc od oe bi translated"><strong class="lf jd"> <em class="lz"> γ </em> </strong> —贴现因子</li></ul><blockquote class="mb mc md"><p id="58d7" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">请记住，我们倾向于使用理查德·萨顿和安德鲁·g·巴尔托所著的教科书<a class="ae ma" href="http://www.incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">强化学习:导论</strong> </a>中的符号。这本书是一个优秀的强化学习基础介绍的经典文本。</p></blockquote><p id="8f5b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们在以前的文章中介绍的主要定义和数学符号是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f63fb489356c991416ac8e160db67a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/0*jyfcVlhMoIbBBuff.png"/></div></figure><h1 id="61cd" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">连续任务的折扣率</h1><p id="3407" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">在继续之前，让我们简单地补充一下贴现率在一个<strong class="lf jd">连续任务</strong>中的表现，这在以前的帖子中没有涉及。</p><h2 id="c5e6" class="ol na it bd nb om on dn nf oo op dp nj lm oq or nl lq os ot nn lu ou ov np iz bi translated">继续任务示例</h2><p id="6df3" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">在本系列的第 1 部分中，我们使用了一个临时任务，即<a class="ae ma" href="https://gym.openai.com/envs/FrozenLake-v0/" rel="noopener ugc nofollow" target="_blank">冰湖</a>环境，这是一个来自<a class="ae ma" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI Gym </a>的简单网格世界环境，是一个用于开发和比较 RL 算法的工具包。在这一节中，我们将使用另一个环境介绍一个持续的任务，即<a class="ae ma" href="https://gym.openai.com/envs/CartPole-v0" rel="noopener ugc nofollow" target="_blank">车杆平衡问题</a>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/607abcf130089b0a5f25bfde6eb50aa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*bd170zVwQZvoYUfutMnXrQ.gif"/></div><p class="ox oy gj gh gi oz pa bd b be z dk translated">使用角度和角速度的车杆平衡(<a class="ae ma" href="https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947" rel="noopener">来源</a>)</p></figure><p id="43e1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如前图所示，一辆手推车沿水平轴放置在无摩擦的轨道上，一根柱子固定在手推车的顶部。目标是通过向左或向右移动小车来防止杆倒下，并且不要从轨道上掉下来。</p><p id="947e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">通过对手推车施加+1(左)或-1(右)的力来控制该系统。钟摆开始直立，目标是防止它翻倒。柱子保持直立的每个时间步提供+1 的奖励，包括该集的最后一步。当柱子偏离垂直方向超过 15 度，或者手推车偏离中心超过 2.4 个单位时，该集结束。</p><p id="2267" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该环境在每个时间点的观察空间是 4 个数字的阵列。在每个时间步，你可以观察它的位置，速度，角度和角速度。这些是这个世界的可观察状态。您可以在本文档的<a class="ae ma" href="https://github.com/openai/gym/wiki/CartPole-v0" rel="noopener ugc nofollow" target="_blank">中查找这些数字的含义。请注意小车速度和尖端极点速度的最小值(-Inf)和最大值(Inf)。由于数组中对应于每个索引的条目可以是任何实数，这意味着状态空间是无限的！</a></p><p id="3e72" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在任何状态下，小车只有两种可能的动作:<em class="lz">向左移动</em>或<em class="lz">向右移动</em>。换句话说，车极点的状态空间有四维连续值，而动作空间有一维两个离散值。</p><h2 id="22c2" class="ol na it bd nb om on dn nf oo op dp nj lm oq or nl lq os ot nn lu ou ov np iz bi translated">贴现率</h2><p id="552d" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">在我们继续任务的例子中，哪个贴现率会鼓励代理尽可能长时间地保持极点平衡？</p><p id="d0e9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在任何贴现率<em class="lz"> γ &gt; 0 的情况下，</em>代理人在极点尚未倒下的每个时间步获得正奖励。因此，代理将尽可能长时间地保持极点平衡。</p><p id="24bb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然而，想象一下，奖励信号被修改为仅在一集结束时给代理人奖励。换句话说，除了最后一个时间步，当剧集结束时，每个时间步的奖励都是 0，然后代理人获得+1 的奖励。</p><p id="d30b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在这种情况下，如果折扣率为<em class="lz"> γ= </em> 1，代理人将总是获得+1 的奖励(无论它在该集期间选择什么行动)，因此奖励信号不会向代理人提供任何有用的反馈。</p><p id="c999" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果折扣率为<em class="lz"> γ </em> &lt; 1，代理将尝试尽快终止该集(通过快速放下杆或离开轨道边缘)。因此，在这种情况下，我们必须重新设计奖励信号！</p><p id="4794" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个问题的解决方案是由策略决定的，这意味着为了追求一个目标，代理需要学习一系列的动作。在下一节中，我们将进一步讨论这个问题解决方案的正式定义。</p><h1 id="103a" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">政策</h1><p id="d15d" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated"><strong class="lf jd">策略</strong>是代理用来基于当前状态确定下一个动作的策略(例如，一些规则集)。一般用<strong class="lf jd">【𝜋(𝑎|𝑠】、</strong>希腊字母 pi 表示，一个策略<strong class="lf jd"> </strong>是决定下一个动作<strong class="lf jd"><em class="lz"/></strong>采取给定状态<strong class="lf jd"><em class="lz"/></strong>。</p><p id="1707" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最简单的策略是从环境状态集<strong class="lf jd"><em class="lz"/></strong>到可能动作集<strong class="lf jd"><em class="lz"/></strong>的映射。我们称这种策略为<strong class="lf jd">确定性策略</strong>。但是在第二篇文章中，我们也介绍了𝜋(𝑎|𝑠政策可以被定义为概率，而不是具体的行动。换句话说，这是一个<strong class="lf jd">随机策略</strong>，它对代理在给定状态下可以采取的行动有一个概率分布。</p><p id="96fb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">随机策略将允许代理随机选择行动。更正式地说，我们将随机策略定义为接受环境状态<strong class="lf jd"><em class="lz"/></strong>和动作<strong class="lf jd"><em class="lz"/></strong>并返回代理在状态 S:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/79b5c44c57bdb8ed1d4ee9fcea7d812c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FX7BL1Ee5eEhb0Ie_6M1uQ.png"/></div></div></figure><p id="a152" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在学习过程中，随着代理获得更多的经验，策略𝜋可能会改变。例如，代理可以从随机策略开始，其中所有动作的概率是一致的；与此同时，代理将有希望学会优化其策略，以达到最优策略。</p><p id="e6b7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在我们知道了如何指定一个策略，我们可以采取什么步骤来确保代理的策略是最好的？我们将使用在第二篇文章中已经介绍过的状态值函数和动作值函数。</p><h2 id="61f0" class="ol na it bd nb om on dn nf oo op dp nj lm oq or nl lq os ot nn lu ou ov np iz bi translated">价值函数</h2><p id="52db" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated"><strong class="lf jd">状态-价值函数</strong>，也被称为<strong class="lf jd">价值函数</strong>，甚至是<strong class="lf jd"> V 函数</strong>，衡量每个状态的好坏，它告诉我们如果我们从那个状态开始，我们未来可以预期的总回报。</p><p id="aaf4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于每个状态<strong class="lf jd"> <em class="lz"> s </em> </strong>，状态值函数告诉我们期望的贴现回报<strong class="lf jd"> <em class="lz"> G </em> </strong>，如果代理在该状态 s 开始，然后使用策略为所有时间步骤选择其动作。重要的是要注意，状态值函数将总是对应于特定的策略，所以如果我们改变策略，我们改变状态值函数。出于这个原因，我们通常用小写的<strong class="lf jd"> <em class="lz"> v </em> </strong>来表示函数，在下标中用相应的策略<strong class="lf jd"> 𝜋 </strong>来表示，并正式定义为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/38f6cb1df7f1e9c4b8a52d4d6b177a06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ytnxMagW_tsxSBk4yghvow.png"/></div></div></figure><p id="945a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其中𝔼[ ]表示给定代理遵循策略<strong class="lf jd"> 𝜋 </strong>时随机变量的期望值，而<em class="lz"> t </em>是任意时间步长。正如我们在第 8 篇文章中介绍的，它用于𝔼[.的预期因为环境转移函数可能以随机的方式起作用。</p><p id="9533" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">同样在<a class="ae ma" rel="noopener" target="_blank" href="/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a"> post 2 </a>中，我们将状态-值函数的定义扩展到状态-动作对，为每个状态-动作对定义一个值，这被称为<strong class="lf jd">动作-值函数，</strong>也被称为<strong class="lf jd">Q-函数</strong>或简称为<strong class="lf jd"> Q. </strong>它定义了在一个策略<strong class="lf jd">下，在状态<strong class="lf jd"><em class="lz"/></strong>中采取动作<strong class="lf jd"><em class="lz"/></strong>的值</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/23ffc6f8aea613ba4bb8bd680fc6fab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QBrt_UYe3GS9EFReRBT2Iw.png"/></div></div></figure><blockquote class="mb mc md"><p id="e8ff" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">在整个系列中，我们将交替使用值函数的大写或小写符号:<strong class="lf jd"><em class="it"/></strong>或<strong class="lf jd"> <em class="it"> v(s) </em> </strong>和<strong class="lf jd"> <em class="it"> Q(s，a) </em> </strong>或<strong class="lf jd"> <em class="it"> q(s，a) </em> </strong></p></blockquote><h2 id="da9d" class="ol na it bd nb om on dn nf oo op dp nj lm oq or nl lq os ot nn lu ou ov np iz bi translated">贝尔曼期望方程</h2><p id="e9ff" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">对于一个一般的 MDP，我们必须根据一个<em class="lz">期望</em>来工作，因为直接的回报和下一个状态很难被确定地预测。的确，我们在前面的帖子里看到，奖励<em class="lz"> r </em>和下一个状态<strong class="lf jd">T5】s’</strong>都是根据 MDP 的单步动态来选择的。在这种情况下，当从(条件)概率分布<strong class="lf jd">p(<em class="lz">s’，r </em> ∣ <em class="lz"> s，a </em> ) </strong>中得出<em class="lz"> r </em>和<em class="lz"> s </em>时，<strong class="lf jd">贝尔曼期望方程</strong>根据<em class="lz">期望的</em>即时报酬和<em class="lz">期望的</em>来表达任意状态<em class="lz"> s </em>的值</p><p id="71a4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于一般情况，当代理的策略<em class="lz"> π </em>为<strong class="lf jd">随机</strong>时，代理在状态<em class="lz"> s </em>时以概率<em class="lz">π</em>(<em class="lz">a</em>∣<em class="lz">s</em>)选择行动<em class="lz"> a </em>，贝尔曼期望方程可以表示为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/f49b1699b530a691951a8c87ac73428b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hz0P02r1yNwyAb2v3SO6dA.png"/></div></div></figure><p id="d242" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在这种情况下，我们把下一个状态的奖励和贴现值之和(<em class="lz">r</em>+<em class="lz">γvπ</em>(<em class="lz">s</em>’)乘以其对应的概率<em class="lz">π</em>(<em class="lz">a</em>∣<em class="lz">s</em>)<em class="lz">p</em>(<em class="lz">s【t57’，<em class="lz">r</em>∣<em class="lz">s<em class="lz"/></em></em></p><p id="ce60" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们还有作用值函数的贝尔曼方程:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/0df1ed8e051f4d8f73ba45ab8cc0b4a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3vokEmJbydAOdPj8xLa7TQ.png"/></div></div></figure><h2 id="00bb" class="ol na it bd nb om on dn nf oo op dp nj lm oq or nl lq os ot nn lu ou ov np iz bi translated">最优策略</h2><p id="5295" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">代理人的目标是在长期内最大化总的累积报酬。使总累积报酬最大化的策略称为<strong class="lf jd">最优策略</strong>。在第 8 篇文章中，我们介绍了“最优”价值函数。</p><p id="dc5c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当且仅当对于所有的<em class="lz"> s </em> ∈S，<em class="lz">vπ</em>′(<em class="lz">s</em>)≥<em class="lz">vπ</em>(<em class="lz">s</em>)时，策略<em class="lz"> π </em>被定义为优于或等于策略<em class="lz"> π </em>最优策略肯定存在，但可能不是唯一的。</p><p id="0aeb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所有最优政策都有相同的状态值函数<strong class="lf jd"><em class="lz">v</em>∑</strong>，称为<strong class="lf jd">最优状态值函数</strong>。最佳状态值函数的更正式的定义可以是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/c81c2a54b9cf650575cb66614e6d273a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZCusCYEeviGmPv1ksrV83g.png"/></div></div></figure><p id="5d0a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于动作值函数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/80a67392ba15beb3573a408c64ac7f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_eNjanx1Gz9fjB4WNDSlg.png"/></div></div></figure><p id="bcc2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所有最优政策都有相同的行动价值函数<strong class="lf jd"><em class="lz">q</em>∑</strong>，称为<strong class="lf jd">最优行动价值函数</strong>。</p><p id="0fcf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这种最优行动值对于获得最优策略非常有用。代理通过与环境交互来估计它。一旦代理确定了最优动作值函数<em class="lz">q</em>∫，它可以通过设置以下参数快速获得最优策略<em class="lz">π</em>∫:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/6805ac5dd4c39d7b2e6877ec060ba8e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_6aO2J2ezBJWsIrf15VABQ.png"/></div></div></figure><p id="59bc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">正如我们在第 8 篇文章<a class="ae ma" rel="noopener" target="_blank" href="/the-bellman-equation-59258a0d3fa7">中看到的，贝尔曼方程用于在算法中找到价值函数的最优值来计算它们。更正式的表达可以是:</a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/bf1bf67f769575004d7982b1779e4580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EpYSA2KO47gz7NApQRgC-A.png"/></div></div></figure><h1 id="6208" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">下一步是什么？</h1><p id="b5f1" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">我们已经到达这篇文章的结尾了！。<a class="ae ma" rel="noopener" target="_blank" href="/monte-carlo-methods-9b289f030c2e">在接下来的</a>这篇文章中，我们将介绍蒙特卡罗方法，这是一种估计价值函数和发现最优策略的学习方法。</p><p id="8eb7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下期<a class="ae ma" rel="noopener" target="_blank" href="/monte-carlo-methods-9b289f030c2e">见</a>！</p></div><div class="ab cl pj pk hx pl" role="separator"><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po"/></div><div class="im in io ip iq"><h1 id="e706" class="mz na it bd nb nc pq ne nf ng pr ni nj ki ps kj nl kl pt km nn ko pu kp np nq bi translated">深度强化学习讲解系列</h1><p id="c09a" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated"><strong class="lf jd">由</strong> <a class="ae ma" href="https://www.upc.edu/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> UPC 巴塞罗那理工大学</strong> </a> <strong class="lf jd">和</strong> <a class="ae ma" href="https://www.bsc.es/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">巴塞罗那超级计算中心</strong> </a></p><p id="4cf4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一个轻松的介绍性<a class="ae ma" href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener ugc nofollow" target="_blank">系列</a>以一种实用的方式逐渐向读者介绍这项令人兴奋的技术，它是人工智能领域最新突破性进展的真正推动者。</p><div class="mh mi gp gr mj mk"><a href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">深度强化学习解释-乔迪托雷斯。人工智能</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">本系列的内容</h3></div></div><div class="mt l"><div class="pv l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="89c0" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">关于这个系列</h1><p id="87a4" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">我在五月开始写这个系列，在巴塞罗那的<strong class="lf jd">封锁期。</strong>老实说，由于封锁，在业余时间写这些帖子帮助了我<a class="ae ma" href="https://twitter.com/hashtag/StayAtHome?src=hashtag_click" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> #StayAtHome </strong> </a>。感谢您当年阅读这份刊物；它证明了我所做的努力。</p><p id="74d2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">免责声明</strong> —这些帖子是在巴塞罗纳封锁期间写的，目的是分散个人注意力和传播科学知识，以防对某人有所帮助，但不是为了成为 DRL 地区的学术参考文献。如果读者需要更严谨的文档，本系列的最后一篇文章提供了大量的学术资源和书籍供读者参考。作者意识到这一系列的帖子可能包含一些错误，如果目的是一个学术文件，则需要对英文文本进行修订以改进它。但是，尽管作者想提高内容的数量和质量，他的职业承诺并没有留给他这样做的自由时间。然而，作者同意提炼所有那些读者可以尽快报告的错误。</p></div></div>    
</body>
</html>