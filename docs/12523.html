<html>
<head>
<title>Creating A Chess AI using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习创建国际象棋人工智能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-a-chess-ai-using-deep-learning-d5278ea7dcf?source=collection_archive---------0-----------------------#2020-08-29">https://towardsdatascience.com/creating-a-chess-ai-using-deep-learning-d5278ea7dcf?source=collection_archive---------0-----------------------#2020-08-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4d4f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用神经网络解码世界上最古老的游戏…</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/503fe09dc7c6090b5b72ee346651bded.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*73SfeqLGzbvmhqnzwt9XCw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">哈桑·帕夏在<a class="ae ky" href="https://unsplash.com/s/photos/chess?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2f3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当加里·卡斯帕罗夫被 IBM 的深蓝国际象棋算法废黜时，该算法没有使用机器学习，或者至少以我们今天定义机器学习的方式使用。</p><p id="dbcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文旨在通过使用神经网络，一种更新形式的机器学习算法，使用神经网络来创建一个成功的国际象棋人工智能。</p><h1 id="5b74" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">概念:</h1><p id="a6a9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">使用超过 20，000 个实例的国际象棋数据集(联系 victorwtsim@gmail.com 获取数据集)，当给定一个棋盘时，神经网络应该输出一个移动。</p><p id="43a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是代码的<a class="ae ky" href="https://github.com/victorsimrbt" rel="noopener ugc nofollow" target="_blank"> github </a>回购:</p><h1 id="290e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">代码:</h1><h2 id="67ae" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">第一步|准备:</h2><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="cefe" class="ms lw it nf b gy nj nk l nl nm">import os<br/>import chess<br/>import numpy as np<br/>import pandas as pd<br/>from tensorflow import keras<br/>from tensorflow.keras import layers</span></pre><p id="5054" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些库是创建程序的先决条件:操作系统和 pandas 将访问数据集，python-chess 是测试神经网络的“即时”棋盘。Numpy 是执行矩阵操作所必需的。Keras 的任务是创建神经网络。</p><h2 id="21eb" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">第 2 步|访问数据:</h2><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="14a1" class="ms lw it nf b gy nj nk l nl nm">os.chdir('XXXXXXXXXXX')<br/>df = pd.read_csv('chess_normalized.csv')<br/>data = df['moves'].tolist()[:500]<br/>split_data = []<br/>indice = 500</span></pre><p id="8a5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个项目，我们需要的只是数据集中每个棋局的 pgn。请更改 os.chdir 函数的目录路径，以访问数据集所在的目录。</p><h2 id="cb0d" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">第三步|热门词典:</h2><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="77ef" class="ms lw it nf b gy nj nk l nl nm">chess_dict = {<br/>    'p' : [1,0,0,0,0,0,0,0,0,0,0,0],<br/>    'P' : [0,0,0,0,0,0,1,0,0,0,0,0],<br/>    'n' : [0,1,0,0,0,0,0,0,0,0,0,0],<br/>    'N' : [0,0,0,0,0,0,0,1,0,0,0,0],<br/>    'b' : [0,0,1,0,0,0,0,0,0,0,0,0],<br/>    'B' : [0,0,0,0,0,0,0,0,1,0,0,0],<br/>    'r' : [0,0,0,1,0,0,0,0,0,0,0,0],<br/>    'R' : [0,0,0,0,0,0,0,0,0,1,0,0],<br/>    'q' : [0,0,0,0,1,0,0,0,0,0,0,0],<br/>    'Q' : [0,0,0,0,0,0,0,0,0,0,1,0],<br/>    'k' : [0,0,0,0,0,1,0,0,0,0,0,0],<br/>    'K' : [0,0,0,0,0,0,0,0,0,0,0,1],<br/>    '.' : [0,0,0,0,0,0,0,0,0,0,0,0],<br/>}</span><span id="c1da" class="ms lw it nf b gy nn nk l nl nm">alpha_dict = {<br/>    'a' : [0,0,0,0,0,0,0],<br/>    'b' : [1,0,0,0,0,0,0],<br/>    'c' : [0,1,0,0,0,0,0],<br/>    'd' : [0,0,1,0,0,0,0],<br/>    'e' : [0,0,0,1,0,0,0],<br/>    'f' : [0,0,0,0,1,0,0],<br/>    'g' : [0,0,0,0,0,1,0],<br/>    'h' : [0,0,0,0,0,0,1],<br/>}</span><span id="5e7b" class="ms lw it nf b gy nn nk l nl nm">number_dict = {<br/>    1 : [0,0,0,0,0,0,0],<br/>    2 : [1,0,0,0,0,0,0],<br/>    3 : [0,1,0,0,0,0,0],<br/>    4 : [0,0,1,0,0,0,0],<br/>    5 : [0,0,0,1,0,0,0],<br/>    6 : [0,0,0,0,1,0,0],<br/>    7 : [0,0,0,0,0,1,0],<br/>    8 : [0,0,0,0,0,0,1],<br/>}</span></pre><p id="4fe8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一键编码是必要的，以确保没有特征或某些实例的权重高于其他特征或实例，从而在数据中产生偏差并阻碍网络的学习。pgn 值中的每个移动和配置都被改变成矩阵，在适当的列中具有 1。</p><h2 id="2868" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">步骤 4|准备数据的初步功能:</h2><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="a761" class="ms lw it nf b gy nj nk l nl nm">def make_matrix(board): <br/>    pgn = board.epd()<br/>    foo = []  <br/>    pieces = pgn.split(" ", 1)[0]<br/>    rows = pieces.split("/")<br/>    for row in rows:<br/>        foo2 = []  <br/>        for thing in row:<br/>            if thing.isdigit():<br/>                for i in range(0, int(thing)):<br/>                    foo2.append('.')<br/>            else:<br/>                foo2.append(thing)<br/>        foo.append(foo2)<br/>    return foo</span><span id="381e" class="ms lw it nf b gy nn nk l nl nm">def translate(matrix,chess_dict):<br/>    rows = []<br/>    for row in matrix:<br/>        terms = []<br/>        for term in row:<br/>            terms.append(chess_dict[term])<br/>        rows.append(terms)<br/>    return rows</span></pre><p id="bc40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个函数是将板翻译成 ascii 形式然后翻译成矩阵的两个初步函数。</p><h2 id="8d38" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">第 5 步|创建数据:</h2><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="7bdc" class="ms lw it nf b gy nj nk l nl nm">for point in data[:indice]:<br/>    point = point.split()<br/>    split_data.append(point)<br/>    <br/>data = []<br/>for game in split_data:<br/>    board = chess.Board()<br/>    for move in game:<br/>        board_ready = board.copy()<br/>        data.append(board.copy())<br/>        board.push_san(move)<br/>trans_data = []<br/>for board in data:<br/>    matrix = make_matrix(board)<br/>    trans = translate(matrix,chess_dict)<br/>    trans_data.append(trans)<br/>pieces = []<br/>alphas = []<br/>numbers = []</span></pre><p id="259a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于神经网络的输入，板本身就足够了:板本身保存时态数据，尽管顺序不正确。添加多个板来形成临时数据对我可怜的 8GB 内存来说计算量太大了。</p><p id="425a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将原始输入作为移动将移除时态数据，并阻止卷积层从数据中提取特征。在第一行中，变量 indice 是可选的。我添加了这个变量来减少数据大小，以便在向上扩展之前测试网络和数据是否正常工作。</p><h2 id="3849" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">第 6 步|转换数据:</h2><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="91b3" class="ms lw it nf b gy nj nk l nl nm">true_data = flatten(split_data)<br/>for i in range(len(true_data)):<br/>    try:<br/>        term = flatten(split_data)[i]<br/>        original = term[:]<br/>        term = term.replace('x','')<br/>        term = term.replace('#','')<br/>        term = term.replace('+','')<br/>        if len(term) == 2:<br/>            piece = 'p' <br/>        else:<br/>            piece = term[0]<br/>        alpha = term[-2]<br/>        number = term[-1]<br/>        pieces.append(chess_dict[piece])<br/>        alphas.append(alpha_dict[alpha])<br/>        numbers.append(number_dict[int(number)])<br/>    except:<br/>        pass</span></pre><p id="33e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这段代码通过使用 try except 函数并删除 check 或 checkmate 的所有额外符号，删除了所有不能一键编码的实例。</p><p id="9ae4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，这意味着程序永远也学不会城堡。</p><h2 id="ee4f" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">步骤 7|创建神经网络:</h2><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="9e38" class="ms lw it nf b gy nj nk l nl nm">board_inputs = keras.Input(shape=(8, 8, 12))</span><span id="37c8" class="ms lw it nf b gy nn nk l nl nm">conv1= layers.Conv2D(10, 3, activation='relu')<br/>conv2 = layers.Conv2D(10, 3, activation='relu')<br/>pooling1 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid", data_format=None,)<br/>pooling2 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid", data_format=None,)<br/>flatten = keras.layers.Flatten(data_format=None)</span><span id="da09" class="ms lw it nf b gy nn nk l nl nm">x = conv1(board_inputs)<br/>x = pooling1(x)<br/>x = conv2(x)<br/>x = flatten(x)<br/>piece_output = layers.Dense(12,name = 'piece')(x)</span><span id="d2e2" class="ms lw it nf b gy nn nk l nl nm">model_pieces = keras.Model(inputs=board_inputs, outputs=piece_output, name="chess_ai_v3")<br/>earlystop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=250, verbose=0, mode='auto', baseline=None, restore_best_weights=True)<br/>model_pieces.compile(<br/>    loss=keras.losses.mse,<br/>    optimizer=keras.optimizers.Adam(),<br/>    metrics=None,<br/>)<br/>model_pieces.fit(trans_data[:len(pieces)],pieces[:len(pieces)],batch_size=64, epochs=100,callbacks = [earlystop])<br/>clear_output()</span><span id="3d8a" class="ms lw it nf b gy nn nk l nl nm">board_inputs = keras.Input(shape=(8, 8, 12))</span><span id="53bb" class="ms lw it nf b gy nn nk l nl nm">conv1= layers.Conv2D(10, 3, activation='relu')<br/>conv2 = layers.Conv2D(10, 3, activation='relu')<br/>pooling1 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid", data_format=None,)<br/>pooling2 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid", data_format=None,)<br/>flatten = keras.layers.Flatten(data_format=None)</span><span id="a196" class="ms lw it nf b gy nn nk l nl nm">x = conv1(board_inputs)<br/>x = pooling1(x)<br/>x = conv2(x)<br/>x = flatten(x)<br/>alpha_output = layers.Dense(7,name = 'alpha')(x)</span><span id="e2af" class="ms lw it nf b gy nn nk l nl nm">model_alpha = keras.Model(inputs=board_inputs, outputs=alpha_output, name="chess_ai_v3")<br/>earlystop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=250, verbose=0, mode='auto', baseline=None, restore_best_weights=True)<br/>model_alpha.compile(<br/>    loss=keras.losses.mse,<br/>    optimizer=keras.optimizers.Adam(),<br/>    metrics=None,<br/>)<br/>model_alpha.fit(trans_data[:len(alphas)],alphas[:len(alphas)],batch_size=64, epochs=100,callbacks = [earlystop])<br/>clear_output()</span><span id="9069" class="ms lw it nf b gy nn nk l nl nm">board_inputs = keras.Input(shape=(8, 8, 12))</span><span id="18da" class="ms lw it nf b gy nn nk l nl nm">conv1= layers.Conv2D(10, 3, activation='relu')<br/>conv2 = layers.Conv2D(10, 3, activation='relu')<br/>pooling1 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid", data_format=None,)<br/>pooling2 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid", data_format=None,)<br/>flatten = keras.layers.Flatten(data_format=None)</span><span id="912a" class="ms lw it nf b gy nn nk l nl nm">x = conv1(board_inputs)<br/>x = pooling1(x)<br/>x = conv2(x)<br/>x = flatten(x)<br/>numbers_output = layers.Dense(7,name = 'number')(x)</span><span id="272f" class="ms lw it nf b gy nn nk l nl nm">model_number = keras.Model(inputs=board_inputs, outputs=numbers_output, name="chess_ai_v3")<br/>earlystop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=250, verbose=0, mode='auto', baseline=None, restore_best_weights=True)<br/>model_number.compile(<br/>    loss=keras.losses.mse,<br/>    optimizer=keras.optimizers.Adam(),<br/>    metrics=None,<br/>)</span><span id="fde1" class="ms lw it nf b gy nn nk l nl nm">model_number.fit(trans_data[:len(numbers)],numbers[:len(numbers)],batch_size=64, epochs=100,callbacks = [earlystop])<br/>clear_output()</span></pre><p id="67f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该神经网络是一个卷积神经网络，具有从数据中提取特征的最大池。这种神经网络结构对于要预测的三个变量中的每一个都是重叠的:棋子、alpha(列)和 number(行)。</p><p id="f990" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有一些我在设计神经网络时无法避免的致命缺点:</p><ul class=""><li id="2f7e" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">神经网络可能会预测不合法的举动</li><li id="ec3a" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">神经网络的拓扑为预测每个特征创建了一个断开点</li></ul><p id="82c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你认为你能解决它，请随意使用这段代码来改进我的程序！</p><h2 id="abed" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">第八步|做预测:</h2><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="d379" class="ms lw it nf b gy nj nk l nl nm">new_chess_dict = {}<br/>new_alpha_dict = {}<br/>new_number_dict = {}<br/>for term in chess_dict:<br/>    definition = tuple(chess_dict[term])<br/>    new_chess_dict[definition] = term<br/>    new_chess_dict[term] = definition<br/>    <br/>for term in alpha_dict:<br/>    definition = tuple(alpha_dict[term])<br/>    new_alpha_dict[definition] = term<br/>    new_alpha_dict[term] = definition<br/>    <br/>for term in number_dict:<br/>    definition = tuple(number_dict[term])<br/>    new_number_dict[definition] = term<br/>    new_number_dict[term] = definition</span><span id="1748" class="ms lw it nf b gy nn nk l nl nm">data = np.reshape(trans_data[0],(1,8,8,12))<br/>pred = model_pieces.predict(data)<br/>def translate_pred(pred):<br/>    translation = np.zeros(pred.shape)<br/>    index = pred[0].tolist().index(max(pred[0]))<br/>    translation[0][index] = 1<br/>    return translation[0]<br/>piece = translate_pred(model_pieces.predict(data))<br/>alpha = translate_pred(model_alpha.predict(data))<br/>number = translate_pred(model_alpha.predict(data))<br/>piece_pred = new_chess_dict[tuple(piece)]<br/>alpha_pred = new_alpha_dict[tuple(alpha)]<br/>number_pred = new_number_dict[tuple(number)]<br/>move =str(piece_pred)+str(alpha_pred)+str(number_pred)</span></pre><p id="64b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了解码来自各个神经网络的预测，必须创建反向字典:这意味着采用一次性编码并将其翻译成字符串。这在通过颠倒术语和定义而创建的 new_chess_dict、new_alpha_dict 和 new_number_dict 字典中有详细描述。</p><p id="ab82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这最后一点代码，程序就完成了！</p><h1 id="25ad" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论:</h1><p id="edab" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">虽然神经网络在进行预测方面起作用，但它经常预测非法移动，因为移动范围对于非法移动范围是连续的。我不能为此创造一个新的解决方案，但我想到了一个新的方法来实现一个具有不同算法的象棋人工智能:遗传算法！敬请关注！</p><h1 id="0650" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">我的链接:</h1><p id="a665" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如果你想看更多我的内容，点击这个<a class="ae ky" href="https://linktr.ee/victorsi" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">链接</strong> </a>。</p></div></div>    
</body>
</html>