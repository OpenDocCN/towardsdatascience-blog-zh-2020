<html>
<head>
<title>Sequence-to-Sequence Models: Attention Network using Tensorflow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">序列对序列模型:使用 Tensorflow 2 的注意力网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe?source=collection_archive---------46-----------------------#2020-09-14">https://towardsdatascience.com/sequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe?source=collection_archive---------46-----------------------#2020-09-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d958" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第 2 部分:序列到序列模型:从 RNN 到变压器</h2></div><p id="1670" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本系列教程的<a class="ae le" rel="noopener" target="_blank" href="/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639">第 1 部分</a>中，我们讨论了带有简单编码器-解码器网络的序列间模型。简单网络更容易理解，但它也有其局限性。</p><h2 id="3a6e" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">简单编码器-解码器网络的局限性</h2><p id="c829" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">如果您还记得第 1 部分，解码器仅基于编码器的最后隐藏输出进行解码。<strong class="kk iu"> <em class="md">这意味着，为了让普通的编码器-解码器网络正常工作，编码器需要确保在最后的隐藏状态输出中编码所有必要的信息。</em> </strong>这对于短序列很有效，但对于长序列效果不佳。</p><p id="d99e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是为什么<code class="fe me mf mg mh b">attention</code>是序列到序列模型中的一个关键概念[1]。</p><h2 id="1703" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">注意力是如何工作的</h2><p id="6602" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">注意机制的目标是向解码器提供上下文信息，以便解码器能够以更高的准确度解码。注意力网络表示上下文向量和整个输入序列之间的关系，而不是依赖编码器的最后隐藏状态中的单个上下文向量。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/f5c7a4ce6cd1171a2d8eaad1fcf2da9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*O-xKW4z-HWg1AC0vVFe3vg.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图 1:<em class="mu">bahda nau 注意的编码器-解码器模型[1] </em></p></figure><ul class=""><li id="f9cc" class="mv mw it kk b kl km ko kp kr mx kv my kz mz ld na nb nc nd bi translated">我们计算一个上下文向量<strong class="kk iu"> <em class="md"> c </em> </strong>作为输入序列的隐藏状态的总和，用<code class="fe me mf mg mh b">alignment scores</code>加权。</li><li id="eba6" class="mv mw it kk b kl ne ko nf kr ng kv nh kz ni ld na nb nc nd bi translated"><code class="fe me mf mg mh b">alignment scores</code>给位置<strong class="kk iu"><em class="md"/></strong>处的输入<strong class="kk iu"> <em class="md"> x </em> </strong>和位置<strong class="kk iu"><em class="md"/></strong>处的输出<strong class="kk iu"><em class="md"/></strong>分配一个<strong class="kk iu"> <em class="md">分数</em>。</strong></li></ul><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nj"><img src="../Images/50c654b9008be1faf6aba5dbc11ee9e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u8ioPUMvSv_uk0ngL-Zf6A.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图 2:注意力等式[4]</p></figure><ul class=""><li id="6d96" class="mv mw it kk b kl km ko kp kr mx kv my kz mz ld na nb nc nd bi translated">不同的注意机制以不同的方式计算这个<code class="fe me mf mg mh b">score</code>。</li><li id="fe2b" class="mv mw it kk b kl ne ko nf kr ng kv nh kz ni ld na nb nc nd bi translated">解码器使用这些注意力分数来决定在每个解码时间步长对输入给予多大程度的关注</li></ul><p id="790d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">计算注意力得分主要有四种不同的方法——加法(Bahdanau 的注意力)和乘法(Luong 的注意力)、自我注意力和键值注意力。这里我们将重点关注 Bahdanua 的注意力。</p><h2 id="743c" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">巴丹瑙注意了</h2><p id="f23d" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">Bahdanau 等人[1]提出了原始的注意力机制，该机制使用一个隐藏层前馈网络来计算注意力对准分数[2]</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/17e8ca92890068e0a9547d68394a4fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*Td8oBJ4teww59DvNn4pK7w.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图 3 —注意力得分计算</p></figure><p id="e599" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<strong class="kk iu"><em class="md">v</em></strong><strong class="kk iu"><em class="md">W</em></strong>是学习到的——注意网络的参数。<strong class="kk iu"> <em class="md"> W₁ </em> </strong>和<strong class="kk iu"> <em class="md"> W₂ </em> </strong>是分别学习当前隐藏状态<strong class="kk iu"><em class="md"/></strong>的变换和编码器输出<strong class="kk iu"> <em class="md"> s </em> </strong>的独立矩阵。</p><p id="9f0d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你有点困惑，不要担心。我们会写一个方法让 Bahdanau 注意，让事情变得更清楚。</p><h2 id="87c4" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">使用 Tensorflow 2.0 实现 Bahdanau 注意力</h2><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="e3b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<code class="fe me mf mg mh b">BahdanauAttention</code>初始化器中，你会看到我们正在初始化三个<code class="fe me mf mg mh b">Dense</code>层— <code class="fe me mf mg mh b">W1</code>、<code class="fe me mf mg mh b">W2</code>和<code class="fe me mf mg mh b">V</code>。如图 3 所示，这些<code class="fe me mf mg mh b">Dense</code>层将用于计算正向传播中的分数，也称为<code class="fe me mf mg mh b">call</code>方法中的分数。</p><p id="46b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们向<code class="fe me mf mg mh b">W1</code>和<code class="fe me mf mg mh b">W2</code>层传递什么？<code class="fe me mf mg mh b">W1</code>和<code class="fe me mf mg mh b">W2</code>分别将当前隐藏状态和编码器输出作为输入。在<code class="fe me mf mg mh b">call</code>方法内部——隐藏状态和编码器输出分别由<code class="fe me mf mg mh b">query</code>和<code class="fe me mf mg mh b">value</code>表示。一旦我们使用来自<code class="fe me mf mg mh b">Decoder</code>网络的<code class="fe me mf mg mh b">BahdanauAttention</code>，你会对分数计算有更清晰的了解。</p><p id="fd27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，当我们通过 softmax 层传递<code class="fe me mf mg mh b">score</code>时，我们计算<code class="fe me mf mg mh b">attention_weights</code>。由于 softmax 返回多类分类问题中目标类的概率分布[5]，因此<code class="fe me mf mg mh b">attention_weights</code>本质上代表了解码器在解码过程中关注了哪个单词。</p><p id="8f8c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们计算<code class="fe me mf mg mh b">context_vector</code>，解码器将使用它来预测最可能的输出。</p><h2 id="0a0d" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">编码器和解码器:</h2><p id="d846" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated"><code class="fe me mf mg mh b">Encoder</code>等级与<a class="ae le" rel="noopener" target="_blank" href="/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639">第一部分</a>中描述的等级相同。除了添加了如下所示的注意机制之外，<code class="fe me mf mg mh b">Decoder</code>类也非常相似。</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="eba4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你会注意到在<code class="fe me mf mg mh b">line #19</code>我们正在初始化<code class="fe me mf mg mh b">attention</code>层。在<code class="fe me mf mg mh b">call</code>方法的前向传播过程中，我们用当前的隐藏状态和编码器输出实例化了<code class="fe me mf mg mh b">attention</code>层，该输出转化为<code class="fe me mf mg mh b">BahdanauAttention</code>类中的<code class="fe me mf mg mh b">query</code>和<code class="fe me mf mg mh b">value</code>。<code class="fe me mf mg mh b">attention</code>层输出<code class="fe me mf mg mh b">context_vector</code>和<code class="fe me mf mg mh b">attention_weights</code>(第<code class="fe me mf mg mh b">20</code>行)。<code class="fe me mf mg mh b">context_vector</code>与解码器输入<code class="fe me mf mg mh b">x</code>连接(第#行<code class="fe me mf mg mh b">26</code>)。该连接的结果然后通过<code class="fe me mf mg mh b">gru</code>单元和一个完全连接的层(线# <code class="fe me mf mg mh b">35</code>)。这里的<code class="fe me mf mg mh b">Decoder</code>类也输出<code class="fe me mf mg mh b">attention_weights</code>，稍后您可以使用它来可视化解码器关注的地方[3]</p><h2 id="76ea" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">把所有的放在一起</h2><p id="def1" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">端到端的工作流程与我们在<a class="ae le" rel="noopener" target="_blank" href="/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639">第一部分</a>中描述的相同——数据清理、定义<code class="fe me mf mg mh b">Encoder</code>和<code class="fe me mf mg mh b">Decoder</code>类、训练模型、推理和评估。关于工作代码，请参考 TensorFlow 示例代码<a class="ae le" href="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb" rel="noopener ugc nofollow" target="_blank">这里的</a>。</p><h1 id="b783" class="nr lg it bd lh ns nt nu lk nv nw nx ln jz ny ka lq kc nz kd lt kf oa kg lw ob bi translated">参考:</h1><ol class=""><li id="def0" class="mv mw it kk b kl ly ko lz kr oc kv od kz oe ld of nb nc nd bi translated">联合学习对齐和翻译的神经机器翻译<a class="ae le" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1409.0473</a></li><li id="e9c3" class="mv mw it kk b kl ne ko nf kr ng kv nh kz ni ld of nb nc nd bi translated">NLP 深度学习最佳实践<a class="ae le" href="https://ruder.io/deep-learning-nlp-best-practices/index.html#attention" rel="noopener ugc nofollow" target="_blank">https://ruder . io/Deep-Learning-NLP-Best-Practices/index . html #注意</a></li><li id="1dcb" class="mv mw it kk b kl ne ko nf kr ng kv nh kz ni ld of nb nc nd bi translated">有注意力的神经机器翻译<a class="ae le" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/tutorials/text/NMT _ with _ Attention</a></li><li id="d5b5" class="mv mw it kk b kl ne ko nf kr ng kv nh kz ni ld of nb nc nd bi translated"><a class="ae le" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2018/06/24/attention-attention . html</a></li><li id="58ed" class="mv mw it kk b kl ne ko nf kr ng kv nh kz ni ld of nb nc nd bi translated">Softmax 的直观解释<a class="ae le" href="https://www.machinecurve.com/index.php/2020/01/08/how-does-the-softmax-activation-function-work/" rel="noopener ugc nofollow" target="_blank">https://www . machine curve . com/index . PHP/2020/01/08/how-the-soft max-activation-function-work/</a></li></ol></div></div>    
</body>
</html>