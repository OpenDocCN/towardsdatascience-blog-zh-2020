<html>
<head>
<title>First time fine-tuning the BERT framework</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">首次微调 BERT 框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/first-time-using-and-fine-tuning-the-bert-framework-for-classification-799def68a5e4?source=collection_archive---------24-----------------------#2020-07-24">https://towardsdatascience.com/first-time-using-and-fine-tuning-the-bert-framework-for-classification-799def68a5e4?source=collection_archive---------24-----------------------#2020-07-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ju"><img src="../Images/95df47dfe8eb2d72182c8870f9890b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IDieEjD-w5s6h3I5SKViMA.jpeg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">照片由 Dmitry Ratushny 拍摄</p></figure><h1 id="ba82" class="kk kl iq bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">语言之旅的开始</h1><p id="9f8b" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">当我决定参加一个正在进行的<a class="ae mg" href="https://www.kaggle.com/c/nlp-getting-started/overview" rel="noopener ugc nofollow" target="_blank">比赛</a>时，我对 NLP 领域的兴趣就开始了，比赛的目的是识别给定的推文是否是关于任何灾难的。我在语言处理领域没有任何经验，在几次互联网搜索后，我开始了解一些数据的文本预处理，如<strong class="lk ir">标记化和词条化</strong>，使用<a class="ae mg" href="https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.Xxrv1_gzZhE" rel="noopener ugc nofollow" target="_blank"><strong class="lk ir">tfidfttransformer 和 TfidfVectorizer </strong> </a>进行特征提取，然后简单地使用<strong class="lk ir">朴素贝叶斯</strong>进行分类(得分= 0.77)。我在此期间参加了一个<a class="ae mg" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习专业课程</a>并了解了<strong class="lk ir"> RNN </strong>并决定使用<strong class="lk ir"> LTSM 模型</strong>来完成这项任务，并获得了更好的结果(得分= 0.79987，前 40%)。在那门课程中，提到了迁移学习以及它如何成为任何任务的有力工具。我想为什么不在我现在拥有的数据集上尝试一下。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="b120" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">伯特的发现</h1><p id="f569" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">我在 NLP 中搜索了不同的框架，并了解了 BERT。据说它是 Google 在 NLP 领域最强大和最有影响力的模型之一，在一个大型的未标记数据集上进行训练，在 11 个单独的 NLP 任务上取得了最先进的结果。它可以根据您的需要进行微调，从而使其更加强大。我决定使用这个框架，并根据我的数据集对它进行微调。我在搜索如何使用这个框架，偶然发现了<a class="ae mg" href="https://huggingface.co/transformers/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lk ir">拥抱脸变形金刚</strong> </a> <strong class="lk ir"> </strong>，它为自然语言理解(NLU)和自然语言生成(NLG)提供了通用架构(BERT、GPT-2、罗伯塔、XLM、DistilBert、XLNet…)，拥有 100 多种语言中超过 32 个预训练模型，以及 TensorFlow 2.0 和 PyTorch 之间的深度互操作性。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><blockquote class="mm mn mo"><p id="73b7" class="li lj mp lk b ll mq ln lo lp mr lr ls ms mt lv lw mu mv lz ma mw mx md me mf ij bi translated">经过大量的实验和阅读文档，我能够对这个模型进行微调，并希望这种体验也能在某种程度上帮助你。</p></blockquote></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="40b9" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">我们开始吧</h1><p id="d515" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><strong class="lk ir">首先，我们来看看给出的数据</strong></p><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="05cf" class="nd kl iq mz b gy ne nf l ng nh">data = pd.read('train.csv')<br/>data.head()</span></pre><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ni"><img src="../Images/d637ee60207a1efc8361cfeff287f814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ur9cQLaWtoHH8t6iGhuzqw.png"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">代码片段的输出</p></figure><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="3fe3" class="nd kl iq mz b gy ne nf l ng nh">data.describe(include = 'all')</span></pre><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nj"><img src="../Images/09c4550b1ada77d77ca2d17daf1b4864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XnKWxNFRqc8PrQ5lWcruvA.png"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">代码片段的输出</p></figure><p id="eae4" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">同样，还有一个“test.csv ”,我们必须预测它的推文。我们可以合并两个数据集，并对它们进行一些必要的操作。我们可以删除关键字和位置列，这样我们就可以只根据给定的推文进行预测。</p><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="3644" class="nd kl iq mz b gy ne nf l ng nh">df1 = pd.read_csv('content/drive/My Drive/disaster tweets/train.csv')</span><span id="5a8b" class="nd kl iq mz b gy nk nf l ng nh">df2 = pd.read_csv('content/drive/My Drive/disaster tweets/test.csv')</span><span id="ebfa" class="nd kl iq mz b gy nk nf l ng nh">combined = pd.concat([df1,df2], axis=0)</span><span id="2b42" class="nd kl iq mz b gy nk nf l ng nh">combined = combined.drop(['keyword','location'], axis=1)</span></pre><p id="a38d" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">我没有预处理或清理数据(例如，删除标点符号或 HTML 标签等。)因为我只是想看看如何使用这个框架。我确信清理和处理数据将会得到更好的结果。</p><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="14bf" class="nd kl iq mz b gy ne nf l ng nh"><strong class="mz ir">from transformers import BertForSequenceClassification, AdamW   </strong> <em class="mp">#importing appropriate class for classification</em><br/><strong class="mz ir">import</strong> <strong class="mz ir">numpy</strong> <strong class="mz ir">as</strong> <strong class="mz ir">np</strong><br/><strong class="mz ir">import</strong> <strong class="mz ir">pandas</strong> <strong class="mz ir">as</strong> <strong class="mz ir">pd</strong><br/><strong class="mz ir">import</strong> <strong class="mz ir">torch</strong><br/><strong class="mz ir">model = BertForSequenceClassification.from_pretrained('bert-base-uncased')</strong>   <em class="mp">#Importing the bert model</em><br/><strong class="mz ir">model.train()</strong>     <em class="mp">#tell model its in training mode so that some<br/>layers(dropout,batchnorm) behave accordingly</em></span></pre><blockquote class="mm mn mo"><p id="1d8a" class="li lj mp lk b ll mq ln lo lp mr lr ls ms mt lv lw mu mv lz ma mw mx md me mf ij bi translated"><strong class="lk ir">符号化和编码</strong></p></blockquote><p id="1a94" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">为了使用 BERT 模型，我们必须首先对我们的文本进行记号化和编码，BERT 记号化器在拥抱面部转换器中提供。</p><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="722f" class="nd kl iq mz b gy ne nf l ng nh">from transformers import BertTokenizer</span><span id="a6e8" class="nd kl iq mz b gy nk nf l ng nh">tokenizer = BertTokenizer.from_pretrained(‘bert-base-uncased’)</span><span id="6c83" class="nd kl iq mz b gy nk nf l ng nh">encoded = tokenizer(combined.text.values.tolist(), padding=True, truncation=True, return_tensors='pt')</span></pre><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nl"><img src="../Images/2fdb9a1f7f9c59fcb8f315222e1f5a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxQlPE-1bcrMsGVNt6hU2w.png"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">代码片段的输出</p></figure><p id="fce3" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">编码器以字典<em class="mp"> ('input_ids '，' attention_mask '，' token_type_ids') </em>的形式为单个 tweet 给出三个张量，供模型使用。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="ea69" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">现在，让我们将张量分成不同的变量(我们只需要' input_ids '和' attention_mask ')，并在测试和训练格式中再次分解组合的数据。</p><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="cde3" class="nd kl iq mz b gy ne nf l ng nh">input_id = encoded['input_ids']<br/>attention_mask = encoded['attention_mask']</span><span id="c08c" class="nd kl iq mz b gy nk nf l ng nh">train_id = input_id[:len(df1)]<br/>train_am = attention_mask[:len(df1)]<br/>test_id = input_id[len(df1):]<br/>test_am = attention_mask[len(df1):]</span><span id="ed31" class="nd kl iq mz b gy nk nf l ng nh">train = combined.iloc[:len(df1)]<br/>test = combined.iloc[len(df1):]</span></pre><p id="20ae" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">出于训练和测试的目的，让我们将训练数据分成两部分，用于模型的训练和测试。</p><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="4d28" class="nd kl iq mz b gy ne nf l ng nh">Xtrain = train.iloc[:6800]<br/>Xtest =  train.iloc[6800:]</span><span id="ce32" class="nd kl iq mz b gy nk nf l ng nh">Xtrain_id = train_id[:6800]<br/>Xtrain_am = train_am[:6800]<br/>Xtest_id = train_id[6800:]<br/>Xtest_am = train_am[6800:]</span><span id="9c86" class="nd kl iq mz b gy nk nf l ng nh">labels = torch.tensor(Xtrain.target.values.tolist())<br/>labels = labels.type(torch.LongTensor)<br/>labels.shape</span></pre></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><blockquote class="mm mn mo"><p id="220e" class="li lj mp lk b ll mq ln lo lp mr lr ls ms mt lv lw mu mv lz ma mw mx md me mf ij bi translated"><strong class="lk ir">微调模型</strong></p></blockquote><p id="b242" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">现在，让我们把重点放在模型上。我们将使用 PyTorch 来训练模型(也可以使用 TensorFlow)。首先，我们将配置我们的优化器(Adam)，然后我们将批量训练我们的模型，以便我们的机器(CPU，GPU)不会崩溃。</p><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="ad1b" class="nd kl iq mz b gy ne nf l ng nh">optimizer = AdamW(model.parameters(), lr=1e-5)<br/>n_epochs = 1 <br/>batch_size = 32 <br/><br/>for epoch in range(n_epochs):<br/><br/>    permutation = torch.randperm(Xtrain_id.size()[0])<br/><br/>    for i in range(0,Xtrain_id.size()[0], batch_size):<br/>        optimizer.zero_grad()<br/><br/>        indices = permutation[i:i+batch_size]<br/>        batch_x, batch_y,batch_am = Xtrain_id[indices],   labels[indices], Xtrain_am[indices]<br/><br/>        outputs = model(batch_x, attention_mask=batch_am, labels=batch_y)<br/>        loss = outputs[0]<br/>        loss.backward()<br/>        optimizer.step()</span></pre><p id="7de1" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">这里输出给我们一个包含交叉熵损失和模型最终激活的<strong class="lk ir">元组</strong>。例如，这是两个张量的输出</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nm"><img src="../Images/9aabc27c336aeeea0d6a8d4de1a4a53c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oOZ5r5KKXKAwDBAQ0J4WlQ.png"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">模型给出的输出</p></figure><p id="7502" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">在 softmax 激活功能的帮助下，我们可以使用这些激活来分类灾难推文。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="0985" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">现在，让我们在训练集中剩余数据的帮助下测试模型。首先，我们必须将模型置于测试模式，然后从模型中获取输出。</p><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="7d19" class="nd kl iq mz b gy ne nf l ng nh">model.eval()     <strong class="mz ir"><em class="mp">#model in testing mode</em></strong><br/>batch_size = 32<br/>permutation = torch.randperm(Xtest_id.size()[0])<br/><br/>for i in range(0,Xtest_id.size()[0], batch_size):<br/><br/>  indices = permutation[i:i+batch_size]<br/>  batch_x, batch_y, batch_am = Xtest_id[indices], labels[indices], Xtest_am[indices]<br/><br/>        <br/>  outputs = model(batch_x, attention_mask=batch_am, labels=batch_y)<br/>  loss = outputs[0]<br/>  print('Loss:' ,loss)</span></pre><p id="b963" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">您还可以通过将输出与标签进行比较来获得准确性指标，并计算(正确预测)tweets 总数)* 100。</p><p id="cb65" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">现在，让我们预测我们的实际测试数据，我们必须找到其输出。</p><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="f728" class="nd kl iq mz b gy ne nf l ng nh">import torch.nn.functional as F  <strong class="mz ir"><em class="mp">#for softmax function</em></strong><em class="mp">    </em><br/>batch_size = 32<br/>prediction = np.empty((0,2)) <strong class="mz ir"><em class="mp">#empty numpy for appending our output</em></strong><br/>ids = torch.tensor(range(original_test_id.size()[0]))<br/>for i in range(0,original_test_id.size()[0], batch_size):<br/>  indices = ids[i:i+batch_size]<br/>  batch_x1, batch_am1 = original_test_id[indices], original_test_am[indices]<br/>  pred = model(batch_x1, batch_am1) <strong class="mz ir"><em class="mp">#Here only activation is given</em></strong><em class="mp"> as output</em><br/>  pt_predictions = F.softmax(pred[0], dim=-1)  <em class="mp">#applying softmax activation function</em><br/>  prediction = np.append(prediction, pt_predictions.detach().numpy(), axis=0) <strong class="mz ir"><em class="mp">#appending the prediction</em></strong></span></pre><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/8af7594d689d0ac84da4bf22d98a0ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*P5fu4NhozpSWClmmk9nHkw.png"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">谓词的形状</p></figure><p id="2d6e" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">如我们所见，预测有两列，<strong class="lk ir">预测[:，0] </strong>给出标签为 0 的概率，而<strong class="lk ir">预测[:，1] </strong>给出标签为 1 的概率。我们可以使用<a class="ae mg" href="https://numpy.org/doc/stable/reference/generated/numpy.argmax.html" rel="noopener ugc nofollow" target="_blank"> argmax 函数</a>来找到合适的标签。</p><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="ebab" class="nd kl iq mz b gy ne nf l ng nh">sub = np.argmax(prediction, axis=1)</span></pre><p id="c8ee" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">然后通过排列这些标签，我们可以得到我们的预测。</p><pre class="jv jw jx jy gt my mz na nb aw nc bi"><span id="b864" class="nd kl iq mz b gy ne nf l ng nh">submission = pd.DataFrame({'id': test.id.values, 'target':sub})</span></pre><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi no"><img src="../Images/822ea637355096454217d9bea945b2a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*sY4GHN4EtEQYDaUetOxYsA.png"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">提交数据集</p></figure><p id="7e6a" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">使用这个模型，我得到了 0.83695 分，甚至没有清理或处理数据，就进入了前 12%。因此，我们可以看到这个框架是多么强大，它可以用于各种目的。你也可以在这里看到代码<a class="ae mg" href="https://github.com/suyash2104/Disaster-detector" rel="noopener ugc nofollow" target="_blank"/>。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="d5f1" class="pw-post-body-paragraph li lj iq lk b ll mq ln lo lp mr lr ls lt mt lv lw lx mv lz ma mb mx md me mf ij bi translated">我希望我的经验可以在某种程度上帮助你，也让我知道还可以做些什么来提高性能(因为我也是 NLP :P 的新手)。</p></div></div>    
</body>
</html>