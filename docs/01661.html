<html>
<head>
<title>Comparing a variety of Naive Bayes classification algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">比较各种朴素贝叶斯分类算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparing-a-variety-of-naive-bayes-classification-algorithms-fc5fa298379e?source=collection_archive---------6-----------------------#2020-02-15">https://towardsdatascience.com/comparing-a-variety-of-naive-bayes-classification-algorithms-fc5fa298379e?source=collection_archive---------6-----------------------#2020-02-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="825b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">数据科学模型</h2><div class=""/><div class=""><h2 id="b1f3" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">文本分类公式的综合列表</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/82e0746e1bdcdc7d593f2d75bb08d6a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cbeOc7SHRPyVjZCm"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae lh" href="https://unsplash.com/@fempreneurstyledstock?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Leone Venter </a>拍摄的照片</p></figure><p id="412d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> N </span> aive Bayes算法是著名的监督分类算法之一。它基于<a class="ae lh" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>，速度非常快，对于文本分类来说足够好。我认为没有必要描述<a class="ae lh" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank">背后的理论</a>，尽管如此，我们将介绍一些概念，然后将重点放在不同实现的比较上。</p><h1 id="ce9d" class="mn mo it bd mp mq mr ms mt mu mv mw mx ki my kj mz kl na km nb ko nc kp nd ne bi translated">1.概念</h1><p id="b593" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">首先，我们采用贝叶斯公式本身:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/7e7bd82f8ac82763dcefea7db73db6a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*rZ0SVT7RQhzQLyPbj0Mshw.png"/></div></figure><p id="07b8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是什么意思？既然可能会混淆，我们就换个眼神。我们将从垃圾邮件的角度讨论事件的类别。所以现在我们有两类事件:垃圾邮件和非垃圾邮件。</p><p id="b444" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们将A改为收到垃圾邮件的事件(<strong class="lk jd"> y </strong>)，将B改为由一组单词(x1，x2，…)组成的消息。所以:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/b5e532a7fe188a07e26897d12872331a.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*JrzywrHZ1MWJwhnmM4f5Ew.png"/></div></figure><p id="b271" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在更明显了:包含单词(x1，x2，…)的消息是垃圾邮件的概率等于收到垃圾邮件的一般概率，乘以垃圾邮件包含上述单词的概率，再除以任何传入消息包含给定单词的概率。</p><p id="1db8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">朴素贝叶斯定理说，集合(x1，x2，…)中所有事件的概率可以被视为独立的，所以:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/c83b310211249e2b9eb38172fdbf096c.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*G-Uo4DZzULefq2YbT4w2xg.png"/></div></figure><p id="7c8e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这仍然是一个有点复杂的公式，但我们可以简化它。显然，我们需要计算邮件是垃圾邮件的概率和邮件不是垃圾邮件的概率。因为我们需要比较这两个值，而且它们在同一个字段中，并且具有相同的除法器(<strong class="lk jd"> P(x1，x2，…) </strong>)，所以我们只能使用分子:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/9367127bb2492381bb2dabffc7723df0.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*u3kQWcqzrYO_v-TP_q31pw.png"/></div></figure><p id="911d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以从分布中找到从数据集获取垃圾邮件的一般概率。所以，主要的问题是找到每个单词出现在垃圾消息中的条件概率(<strong class="lk jd"> P(xi | y) </strong>)。这几乎是朴素贝叶斯算法实现之间的唯一区别。</p><h1 id="e9ac" class="mn mo it bd mp mq mr ms mt mu mv mw mx ki my kj mz kl na km nb ko nc kp nd ne bi translated">2.履行</h1><p id="5d07" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">我已经从头开始构建了一个算法的实现:</p><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/how-to-build-and-apply-naive-bayes-classification-for-spam-filtering-2b8d3308501"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">如何建立和应用朴素贝叶斯分类进行垃圾邮件过滤</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">有效模型的简单实现</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of lb nr"/></div></div></a></div><p id="0d98" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">不过，现在让我们研究一下<a class="ae lh" href="https://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank"> sklearn库</a>的能力。出于我们的目的，我们将使用Tiago A. Almeida和José María Gómez Hidalgo收集的短信。它是免费的，可以从<a class="ae lh" href="https://archive.ics.uci.edu/ml/datasets/sms+spam+collection" rel="noopener ugc nofollow" target="_blank">UCI机器学习库</a>下载。它包含5572条不同消息的记录以及747条垃圾消息。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/960f0fc5d652bf8d8482bec24a56b1ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/0*Ocb1lpDWB039sVY0.png"/></div></figure><p id="9031" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将每条信息拆分成一组单词，并对标签进行编码。之后，我们将数据集分为训练和测试部分。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="49ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下一步是最重要的:我们将训练部分转向词汇，并计算每个消息中每个词汇的重复次数(在训练和测试部分):</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="5220" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在数据集已经为模型创建做好了准备。</p><h2 id="17c6" class="oj mo it bd mp ok ol dn mt om on dp mx lr oo op mz lv oq or nb lz os ot nd iz bi translated">2.1.高斯朴素贝叶斯</h2><p id="cf45" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">这种方法建立在概率正态分布的假设上。也就是说，垃圾邮件和非垃圾邮件类别的消息中，词汇的出现频率符合高斯定律:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/c103f1ceeab7b179cdf9c530039461db.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*R1H8iSnQtyYyRVTbP_7cZw.png"/></div></figure><p id="cb69" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该公式基于消息类中每个词的频率的平均值(<strong class="lk jd"> μ </strong>和贝塞尔校正方差(<strong class="lk jd"> σ </strong>)。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="f482" class="oj mo it ow b gy pa pb l pc pd">cl_gauss = <strong class="ow jd">sklearn</strong>.naive_bayes.<strong class="ow jd">GaussianNB</strong>()<br/>res_gauss = cl_gauss.fit(X_train_voc, y_train).predict(X_test_voc)<br/>metrics.accuracy_score(y_test, res_gauss) * 100</span></pre><p id="cf1a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果是模型精度<strong class="lk jd"> 91.03% </strong>。这已经足够好了，但是说明了单词并不完全是高斯分布。</p><h2 id="9eea" class="oj mo it bd mp ok ol dn mt om on dp mx lr oo op mz lv oq or nb lz os ot nd iz bi translated">2.2.多项式朴素贝叶斯</h2><p id="a232" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">多项式分类最适合离散值，如字数。所以我们希望它能表现出最好的准确性。在这种情况下，每个事件的概率分布基于以下公式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/116464d0e315b1b8e0bd7116304f09af.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*8Pnwdt82eV67EP6RPC9AmA.png"/></div></figure><p id="6442" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> Ny </strong>是事件的特征总数<strong class="lk jd"> y </strong>(所有垃圾短信中的总字数)<strong class="lk jd"> Nyi </strong> —每个特征的计数(一个词在所有垃圾短信中重复的汇总数)<strong class="lk jd"> n </strong> —特征数(词汇中的字数)<strong class="lk jd"> α </strong>是一个平滑拉普拉斯参数，用于丢弃词汇中不存在的词的影响。同样的公式也适用于非垃圾邮件消息集。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="694a" class="oj mo it ow b gy pa pb l pc pd">cl_multi = <strong class="ow jd">sklearn</strong>.naive_bayes.<strong class="ow jd">MultinomialNB</strong>()<br/>res_multi = cl_multi.fit(X_train_voc, y_train).predict(X_test_voc)<br/>metrics.accuracy_score(y_test, res_multi) * 100</span></pre><p id="bba2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">模型精度<strong class="lk jd"> 99.19% </strong>，是(剧透)包括定制款在内的测试模型中最高的。</p><h2 id="003b" class="oj mo it bd mp ok ol dn mt om on dp mx lr oo op mz lv oq or nb lz os ot nd iz bi translated">2.3.互补朴素贝叶斯</h2><p id="4d5d" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">这种方法几乎与多项式相同，尽管现在我们计算一个单词在该类补集中的出现次数。例如，对于垃圾邮件，我们将计算每个单词在所有非垃圾邮件中的重复次数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/caa4d2b243aa7a50154b4bb99d4446ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*gnlrE3Tk5itsYGiDZi7Ssg.png"/></div></figure><p id="4b47" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Nc —相反类别中的总字数(对于垃圾邮件参数—非垃圾邮件字数)，Nci —相反类别中某个单词的重复次数(对于垃圾邮件中的某个单词—在所有非垃圾邮件中的重复次数)。我们也使用相同的平滑参数。计算基本值后，我们开始处理实际参数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/c308ae3a437f004e7e5762ed8f222749.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*GFvxb03r7yBnYV8LrAJHXA.png"/></div></figure><p id="4cd3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它是k个单词的消息中每个单词的权重。最终决策由以下公式计算得出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/93d3c3e137e9aa72a874f5ef48330225.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*I4T0o3bgRvnurqGKg1Y9SQ.png"/></div></figure><p id="3c21" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，分类结果是邮件中每个单词的权重之和的最小值。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="355d" class="oj mo it ow b gy pa pb l pc pd">cl_compl = <strong class="ow jd">sklearn</strong>.naive_bayes.<strong class="ow jd">ComplementNB</strong>()<br/>res_compl = cl_compl.fit(X_train_voc, y_train).predict(X_test_voc)<br/>metrics.accuracy_score(y_test, res_compl) * 100</span></pre><p id="02fe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们得到了98.12%的正确预测，这是一个非常高的结果。</p><h2 id="3887" class="oj mo it bd mp ok ol dn mt om on dp mx lr oo op mz lv oq or nb lz os ot nd iz bi translated">2.4.伯努利朴素贝叶斯</h2><p id="97f2" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">伯努利公式接近于多项式公式，尽管输入是一组布尔值(该词是否出现在消息中)而不是一组频率。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/bb6264387c6a7ec979cd14743ca66c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*ktdCWtrBDpPFpCJI8eWPuw.png"/></div></figure><p id="e884" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，该算法明确地惩罚特征的不出现(消息中的单词在词汇表中不存在)，而多项式方法对不存在的值使用平滑参数。<code class="fe pj pk pl ow b">sklearn</code>伯努利算法将输入值二进制化，因此无需额外操作。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="8408" class="oj mo it ow b gy pa pb l pc pd">cl_bern = <strong class="ow jd">sklearn</strong>.naive_bayes.<strong class="ow jd">BernoulliNB</strong>()<br/>res_bern = cl_bern.fit(X_train_voc, y_train).predict(X_test_voc)<br/>metrics.accuracy_score(y_test, res_bern) * 100</span></pre><p id="4af5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们已经得到了<strong class="lk jd"> 98.30% </strong>的正确预测</p><h2 id="04d9" class="oj mo it bd mp ok ol dn mt om on dp mx lr oo op mz lv oq or nb lz os ot nd iz bi translated">2.5.分类朴素贝叶斯</h2><p id="9f53" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">分类朴素贝叶斯适用于分类值-无论示例是否具有特征集。在我们的例子中，这意味着词汇表被视为一组特征，而一个单词在消息中的出现被视为与该特征的匹配。所有公式都与多项式方法相同，只是出现次数不同于重复次数。</p><p id="1f9e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于算法需要分类值，我们将单词的频率转换为单词的存在:1-消息包含该单词，0-该单词不在消息中。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="6edb" class="oj mo it ow b gy pa pb l pc pd">X_train_voc_cat= X_train_voc.applymap(<strong class="ow jd">lambda</strong> el: 1 <strong class="ow jd">if</strong> el &gt; 0 <strong class="ow jd">else</strong> 0)<br/>X_test_voc_cat = X_test_voc.applymap(<strong class="ow jd">lambda</strong> el: 1 <strong class="ow jd">if</strong> el &gt; 0 <strong class="ow jd">else</strong> 0)</span></pre><p id="5f1b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们将模型应用于更新的数据集:</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="bbd4" class="oj mo it ow b gy pa pb l pc pd">cl_cat = <strong class="ow jd">sklearn</strong>.naive_bayes.<strong class="ow jd">CategoricalNB</strong>()<br/>res_cat=cl_cat.fit(X_train_voc_cat,y_train).predict(X_test_voc_cat)<br/>metrics.accuracy_score(y_test, res_cat) * 100</span></pre><p id="9bdb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">准确率相当高:<strong class="lk jd"> 98.30% </strong>。</p></div><div class="ab cl pm pn hx po" role="separator"><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr"/></div><div class="im in io ip iq"><p id="e58e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文中，我们熟悉了几种朴素贝叶斯算法，理解了它们背后的数学原理，并测试了公式。结果自说自话。您可以在我的Github上找到包含所有工作模式的Jupyter笔记本:</p><div class="no np gp gr nq nr"><a href="https://github.com/Midvel/medium_jupyter_notes/blob/master/scikit_bayes/scikit-bayes.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">中级/中等_jupyter_notes</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">permalink dissolve GitHub是4000多万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">github.com</p></div></div><div class="oa l"><div class="pt l oc od oe oa of lb nr"/></div></div></a></div><p id="9082" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，看看一个自定义算法的实现，从头开始，没有任何带模型的库:</p><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/how-to-build-and-apply-naive-bayes-classification-for-spam-filtering-2b8d3308501"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">如何建立和应用朴素贝叶斯分类进行垃圾邮件过滤</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">有效模型的简单实现</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of lb nr"/></div></div></a></div></div></div>    
</body>
</html>