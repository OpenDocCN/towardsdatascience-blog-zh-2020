<html>
<head>
<title>Divide Hugging Face Transformers training time by 2 or more with dynamic padding and uniform length batching</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用动态填充和统一长度批处理将拥抱面部变形器训练时间除以2或更多</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e?source=collection_archive---------10-----------------------#2020-05-20">https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e?source=collection_archive---------10-----------------------#2020-05-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4112" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习工程</h2><div class=""/><div class=""><h2 id="6f37" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated"><strong class="ak">减少训练时间有助于在固定的预算时间内进行更多的迭代，从而达到更好的效果。</strong></h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ad96b0bc8c31570692df28e6304a5d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GmoGpZZJCEIdCX3-I58uGA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">上面的照片是由<a class="ae lh" href="https://www.pngwing.com/en/free-png-ncpgf" rel="noopener ugc nofollow" target="_blank">这个</a>(免费用于非商业用途)和<a class="ae lh" href="https://www.pexels.com/fr-fr/photo/851989/" rel="noopener ugc nofollow" target="_blank">那个</a> (Pexel许可，免费用于任何用途)制作的</p></figure><blockquote class="li lj lk"><p id="41e1" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">2020年6月4日更新:根据一项建议，在<a class="ae lh" href="https://app.wandb.ai/pommedeterresautee/speed_training/reports/Divide-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI" rel="noopener ugc nofollow" target="_blank">报告</a>中增加了对<strong class="lo jd">大</strong>口味卡门贝干酪进行的实验。TL；DR:训练时间从4小时减少到1小时30分。</p></blockquote><p id="7811" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi ml translated">我在Lefebvre Sarrut出版社工作，这是一家出版公司，是欧洲法律行业的主要参与者。正如前面的文章中所解释的，我们的需求要求我们在非常大的文本数据集上应用深度学习(法律案件匿名化、文档分类等)。)，这就是为什么我们对让我们所有的(机器学习)工具变得更快如此感兴趣。</p><p id="1ed1" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">本着一篇<a class="ae lh" rel="noopener" target="_blank" href="/why-we-switched-from-spacy-to-flair-to-anonymize-french-legal-cases-e7588566825f">先例文章</a>的精神，这篇文章的目的是探索两个非常<em class="ln">简单的</em>优化，它们可以显著减少<a class="ae lh" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>库的训练时间，而不会对准确性产生负面影响。我们在一个大型的知名NLP数据集(X-NLI的法国部分)上运行了<strong class="lo jd"> 21个实验+ 12个再现性实验</strong>，我们表明，对于模型的<strong class="lo jd">基础</strong>风格，通过简单地使用现成的法国BERT模型(<a class="ae lh" href="https://camembert-model.fr/" rel="noopener ugc nofollow" target="_blank"> CamemBERT </a>)、默认参数、单个消费级GPU和这些优化，我们可以达到最大令牌长度为128的<strong class="lo jd">、最大令牌长度为16分钟的<strong class="lo jd"> 比没有任何优化的<em class="ln"> 56分钟</em>训练获得的分数高出0.5分</strong>，比CamemBERT模型作者为此任务报告的分数高出0.3分。 在相同的模型上，对于493最大令牌长度，增益甚至更令人印象深刻，其中训练时间从没有任何优化的4h38减少到有所有优化的1h01，仍然达到相同的分数。<em class="ln">与</em> <strong class="lo jd"> <em class="ln">大型</em> </strong> <em class="ln">型号达到了类似的训练时间缩减(从4h到1h30为128令牌长度)。</em></strong></p><p id="08d2" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">重现实验的源代码</strong>可从<a class="ae lh" href="https://gist.github.com/pommedeterresautee/1a334b665710bec9bb65965f662c94c8" rel="noopener ugc nofollow" target="_blank">那里</a>获得。</p><blockquote class="li lj lk"><p id="7e85" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">这些优化不是特定于任务/模型/语言的，但前提是下面的代码是为Pytorch编写的。</p><p id="d111" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><em class="it">如果你对这个话题感兴趣，在推特上关注我:</em><a class="ae lh" href="https://twitter.com/pommedeterre33" rel="noopener ugc nofollow" target="_blank"><em class="it">https://twitter.com/pommedeterre33</em></a></p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mu"><img src="../Images/52ecd321d0689be15125b5b19836a68f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*llJdV2uTRbqtPlNpRFz8ig.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">查看<a class="ae lh" href="https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI" rel="noopener ugc nofollow" target="_blank">报告</a>，看看<a class="mv mw ep" href="https://medium.com/u/c4669728a9af?source=post_page-----21bf7129db9e--------------------------------" rel="noopener" target="_blank">权重&amp;偏差</a>是一个你不知道自己需要的强大工具</p></figure><p id="175b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">此外，我们记录了所有关于<a class="mv mw ep" href="https://medium.com/u/c4669728a9af?source=post_page-----21bf7129db9e--------------------------------" rel="noopener" target="_blank">重量&amp;偏差</a>的实验，这是一种范围更大的在线张量板，所以你可以自己分析我们的实验结果，这里是<a class="ae lh" href="https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI" rel="noopener ugc nofollow" target="_blank">的报道</a>。</p><p id="460d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><em class="ln">(注意“等长配料”在报告上被命名为“智能配料”)</em></p><p id="8d5e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">“动态/可变长度填充”</strong>是NLP在2017-2018年的事情，但现在单个神经网络架构使得任何GPU所有者都能够在几乎任何任务上达到SOTA分数，似乎NLP社区已经将重点从寻找最佳批量大小转移到在神奇的架构上构建东西。<strong class="lo jd">“统一长度批处理”</strong>是一个天真的想法，进一步推动我们想要检查的动态填充增益。</p><blockquote class="mx"><p id="7b2c" class="my mz it bd na nb nc nd ne nf ng mh dk translated">在撰写本文时，对于大多数常见任务来说，这两个工具在Transformers库上都不是现成的。</p></blockquote><p id="0762" class="pw-post-body-paragraph ll lm it lo b lp nh kd lr ls ni kg lu mi nj lx ly mj nk mb mc mk nl mf mg mh im bi translated">您只能在文本生成/语言建模任务中找到动态填充。有希望的是，我们将看到这两种技术都很容易在普通任务(分类等)的用户端实现。)并且我们认为大多数NLP从业者应该测试/使用它们。</p><p id="e215" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在过去的一年里，来自<a class="mv mw ep" href="https://medium.com/u/b1574f0c6c5e?source=post_page-----21bf7129db9e--------------------------------" rel="noopener" target="_blank">拥抱脸</a>的变形金刚库成为使用大型预训练语言NLP模型的标准方式。它附带了大量的功能，涵盖了大多数NLP用例，并且有一个完美的API，直到你开始期望它是完美的。自从<a class="ae lh" href="https://github.com/huggingface/transformers/releases/tag/v2.9.0" rel="noopener ugc nofollow" target="_blank">版本2.9 </a>带给我们<a class="ae lh" href="https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py" rel="noopener ugc nofollow" target="_blank">训练器</a>类以来，这种感觉更加强烈，这是一个精心设计的<a class="mv mw ep" href="https://medium.com/u/8536ebfbc90b?source=post_page-----21bf7129db9e--------------------------------" rel="noopener" target="_blank"> William Falcon </a>的Pytorch Lightning训练API适应特定的变压器要求，它将用户从大多数工程方面的训练中解放出来(张量板测井、混合精度、梯度累积、多GPU设置等)。)因此是微调模型的新的默认方式。</p><p id="2a61" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在一切都如此完美的情况下，你倾向于相信一切都被优化到了极致。不要忘记，这个库还很年轻，团队同时在几条战线上工作(最近的例子是与来自Azure DevOps的吴合作，利用ONNX和在一些设置中减少推理时间。当你深入图书馆时，你可能仍然会发现一些有趣的低挂水果。</p><p id="edda" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我们进行了实验，总结了我们在下面和<a class="ae lh" href="https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI" rel="noopener ugc nofollow" target="_blank">那里</a>的发现。</p><p id="1961" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">如果你还不是“填充/注意力面具”专家，你可能想再读一遍这篇为《老式RNN》写的优秀的<a class="ae lh" rel="noopener" target="_blank" href="/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e">文章</a>(摘自<a class="mv mw ep" href="https://medium.com/u/8536ebfbc90b?source=post_page-----21bf7129db9e--------------------------------" rel="noopener" target="_blank">威廉·法尔肯</a> …!).简而言之，在一批序列上训练神经网络要求它们具有完全相同的长度，以构建批矩阵表示。由于现实生活中的NLP数据集总是由可变长度的文本组成，我们经常需要通过截断它们来缩短一些序列，并通过在末尾添加一个重复的假标记(称为“填充”标记)来延长一些序列。因为填充令牌不代表真实的单词/子单词/信号，所以当大多数计算完成时，在计算损失之前，我们通过注意力屏蔽矩阵将填充令牌信号乘以0来擦除它。</p><p id="25d5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">这两种优化背后的主要思想是尽可能避免不使用的计算</strong>:</p><ul class=""><li id="914d" class="nm nn it lo b lp lq ls lt mi no mj np mk nq mh nr ns nt nu bi translated"><strong class="lo jd">动态填充</strong>:我们限制添加填充令牌的数量，以达到每个小批量的最长序列的长度，而不是为整个列车组设置一个固定值。因为添加的令牌的数量在小批之间变化，所以我们称之为"<em class="ln">动态</em>"填充；</li><li id="8696" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated"><strong class="lo jd">统一长度批处理</strong>:我们通过生成由相似长度序列组成的批处理来进一步推动逻辑，因此我们避免了极端情况，即迷你批处理中的大多数序列都很短，我们需要为每个序列添加大量填充标记，因为同一迷你批处理的1个序列非常长。</li></ul><p id="bf98" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">此外，我们检查了第三种选择的好处，<strong class="lo jd">混合精度</strong>，它单独使用或与上述两种技术结合使用的价值。</p><h1 id="7a3b" class="oa ob it bd oc od oe of og oh oi oj ok ki ol kj om kl on km oo ko op kp oq or bi translated">设置</h1><p id="7ca1" class="pw-post-body-paragraph ll lm it lo b lp os kd lr ls ot kg lu mi ou lx ly mj ov mb mc mk ow mf mg mh im bi translated">为了检查这些优化是否与基于transformer的语言模型配合良好，我们在<a class="ae lh" href="https://arxiv.org/pdf/1809.05053.pdf" rel="noopener ugc nofollow" target="_blank"> X-NLI </a>的法语部分运行了14个不同设置的实验。</p><p id="0ff5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">如果你不知道X-NLI，它基本上是一个句子对分类任务，其中模型需要猜测第二个句子与第一个句子相比是否需要/矛盾/中性(3类)。X-NLI训练集是大型英语数据集的机器翻译，测试集由来自同一英语数据集的15种语言(包括法语)的5K对人工翻译组成。</p><p id="e037" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">用于实验的模型是卡门贝塔建筑，由脸书费尔和Inria实验室用法语文本训练而成。存在几种口味，下面的数字与已经发布的第一种相关，在变形金刚中称为<a class="ae lh" href="https://huggingface.co/camembert-base" rel="noopener ugc nofollow" target="_blank">卡门贝干酪基</a>(在130 Gb数据上训练的110万个参数)。<a class="ae lh" href="https://huggingface.co/camembert-base" rel="noopener ugc nofollow" target="_blank"><em class="ln">camembert-large</em></a><em class="ln">实验报告中均有。</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5606c9bb09000d5949d5030fb7485ab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/0*FK3QvRoep5_uyoHS"/></div></figure><blockquote class="li lj lk"><p id="3dec" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">Camembert论文作者在<strong class="lo jd"> 10 epoch </strong> s中达到了81.2%的准确率，提前停止，1e-5学习率，512个令牌的序列长度<a class="ae lh" href="https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md" rel="noopener ugc nofollow" target="_blank">以及其他一些东西</a>。</p></blockquote><p id="5fa3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">对于每个实验，我们将训练限制在1个时期，因为我们只能使用单个GPU来运行所有实验。所有运行都使用相同的种子(不同种子的“再现性”实验除外，如下所述)。除非另有说明，超参数保持默认值，因为我们没有资源对每个实验进行网格搜索。正如我们将看到的，在大多数实验中，我们在X-NLI上击败了作者报告的分数，所以默认设置可能已经足够好了。</p><p id="5b93" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd"> X-NLI主要由短句对组成</strong>:当短句对的两个部分连接成一个序列时，超过97%的短句对长度小于128个标记。这种长度分布对减少培训时间的机会有很大影响。简而言之，在这个数据集上，即使在消费级GPU上，也可以在128个令牌处截断序列来构建“大型”批处理。在一个更平衡的数据集上，比如T2的马可女士，你需要512个单词的限制才能接近SOTA的平均倒数排名。在这两种情况下，这里介绍的两种技术带来了大量的训练时间减少(2倍或更多)，但出于不同的原因，我们将在下面进行分析。在我们对私有数据集使用这些优化时，无论数据集的特征如何，我们总是对训练时间产生显著影响。</p><p id="5899" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">时间和精度是在单个Nvidia 2080 TI GPU上测量的。3个优化选项的每个组合已经运行了2次，一次是将一批64个序列截短为128个记号，第二次是将一批16个序列(2步8个序列和单个梯度累积)截短为493个记号。493是字符数最长序列中的Camembert子词标记数，可能是训练集中标记数最长的标记序列之一。步长已被设置为该GPU在内存中可以占用的最大值。</p><p id="2efa" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">计时包括中间评估(在我们的例子中是每500个小批量)，因为在现实生活中，大多数NLP从业者都是这样进行培训的。</p><h1 id="eb2b" class="oa ob it bd oc od oe of og oh oi oj ok ki ol kj om kl on km oo ko op kp oq or bi translated">结果</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/c7227ed3fed7cd6308075ff49cf875f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BFMchareZHutuAWQ"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">每个组合随时间变化的精确度，报告中的交互式图表<a class="ae lh" href="https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI" rel="noopener ugc nofollow" target="_blank">此处</a></p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/920239ee5bf70ef9a24cd34807dee98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CHRhVCNSrDfaU6Nx"/></div></div></figure><p id="fd54" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">TL；博士:</p><h2 id="7816" class="pa ob it bd oc pb pc dn og pd pe dp ok mi pf pg om mj ph pi oo mk pj pk oq iz bi translated"><strong class="ak">定时</strong></h2><ul class=""><li id="1480" class="nm nn it lo b lp os ls ot mi pl mj pm mk pn mh nr ns nt nu bi translated"><strong class="lo jd">单独使用动态填充可以显著减少训练时间，这可以通过使用统一大小批处理和混合精度来增强；</strong></li><li id="1c1e" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">在一些设置中(小批量+短序列)，混合精度可以产生更长的训练时间，在其他情况下，它是游戏改变者(大批量和/或长序列)。</li></ul><h2 id="0300" class="pa ob it bd oc pb pc dn og pd pe dp ok mi pf pg om mj ph pi oo mk pj pk oq iz bi translated"><strong class="ak">精度</strong></h2><ul class=""><li id="63a6" class="nm nn it lo b lp os ls ot mi pl mj pm mk pn mh nr ns nt nu bi translated"><strong class="lo jd">在14次运行中，有11次在单个时期内获得了81.18%以上的分数(Camembert论文中报道的10个时期的分数，并提前停止)；</strong></li><li id="1a49" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">当我们比较成对的运行(相同的设置，在128处截断与在493处截断)时，不出所料，截断平均具有(小的)准确性成本，即使只有3%的数据集与128令牌截断有关。</li></ul><blockquote class="mx"><p id="fe58" class="my mz it bd na nb po pp pq pr ps mh dk translated"><strong class="ak">通过使用优化和混合精度，我们在16mn训练中击败了4h38训练的分数！</strong></p></blockquote><blockquote class="li lj lk"><p id="5354" class="ll lm ln lo b lp nh kd lr ls ni kg lu lv nj lx ly lz nk mb mc md nl mf mg mh im bi translated"><strong class="lo jd">基础</strong>模型的发现与<strong class="lo jd">大型</strong>模型相同，额外12个实验的测量结果在<a class="ae lh" href="https://app.wandb.ai/pommedeterresautee/speed_training/reports/Divide-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI" rel="noopener ugc nofollow" target="_blank">报告</a>中。</p></blockquote><h1 id="ddce" class="oa ob it bd oc od oe of og oh oi oj ok ki ol kj om kl on km oo ko op kp oq or bi translated">优化机会</h1><h2 id="68e1" class="pa ob it bd oc pb pc dn og pd pe dp ok mi pf pg om mj ph pi oo mk pj pk oq iz bi translated">当你要抛出结果时，避免计算</h2><p id="fb58" class="pw-post-body-paragraph ll lm it lo b lp os kd lr ls ot kg lu mi ou lx ly mj ov mb mc mk ow mf mg mh im bi translated">如上所述，pad令牌信号通过应用注意屏蔽来消除。序列末尾的填充标记越多，执行的未使用的计算就越多。</p><p id="aa44" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在训练器类中，您定义了一个(固定的)序列长度，训练集的所有序列都被填充/截断以达到该长度，没有任何例外。在X-NLI上，最短的序列是10个记号长，如果你提供128个记号长，你将把118个填充记号加到这10个记号序列上，然后在这118个有噪声的记号上执行计算。</p><blockquote class="li lj lk"><p id="47ff" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">最差，如<a class="ae lh" href="https://github.com/google-research/bert/blob/master/README.md" rel="noopener ugc nofollow" target="_blank">原伯特回购自述</a>、<em class="it">“…注意是序列长度的平方。换句话说，一批64个长度为512的序列比一批256个长度为128的序列要贵得多。”</em>。</p></blockquote><p id="fb9e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">小批量由从数据集中取样的少量序列组成。即使在X-NLI中随机选择，小批量中的最长序列也有可能比整个训练集的最大序列长度短。因为学习/梯度下降是在小批量水平执行的，所以我们有机会限制填充效应，更准确地说，我们可以首先在小批量中搜索最长的序列长度，然后相应地填充其他序列。</p><p id="9028" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这些操作可以在<em class="ln"> collate_fn </em>功能中执行。该函数的用途在<a class="ae lh" href="https://pytorch.org/docs/stable/data.html#working-with-collate-fn" rel="noopener ugc nofollow" target="_blank"> Pytorch文档</a>中有所描述，基本上它采用数据集返回的单个示例，并将它们合并以构建张量矩阵，并将其发送至模型。</p><h2 id="094f" class="pa ob it bd oc pb pc dn og pd pe dp ok mi pf pg om mj ph pi oo mk pj pk oq iz bi translated">动态填充</h2><p id="cc8d" class="pw-post-body-paragraph ll lm it lo b lp os kd lr ls ot kg lu mi ou lx ly mj ov mb mc mk ow mf mg mh im bi translated">如上所述，这个想法是在小批量水平而不是数据集水平调整序列长度。这样我们可以限制未使用的计算。该工作在<a class="ae lh" href="https://pytorch.org/docs/stable/data.html" rel="noopener ugc nofollow" target="_blank"> Pytorch数据加载器</a>内进行。让我们提醒一下它是如何工作的:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/c2d647b981e9a465c99a2c61e1362378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZEk6orCJAdIZA3-TbdWM3g.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Pytorch数据加载器内部</p></figure><p id="04eb" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">组件:</p><ul class=""><li id="340d" class="nm nn it lo b lp lq ls lt mi no mj np mk nq mh nr ns nt nu bi translated"><strong class="lo jd"> Dataset() </strong>是访问原始文本数据的砖块，是一个简单的字符串列表或类似数据库连接器的东西；</li><li id="dfd0" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated"><strong class="lo jd"> Sampler() </strong>生成索引，指向数据集中的一个数据点。它遵循一种策略，例如顺序生成(对于测试集)或随机生成(对于训练集)。</li><li id="b8bd" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated"><strong class="lo jd"> collate_fn() </strong>:对于每个小批量，它接收由采样器选择的数据点(来自数据集)并将它们分组到一个张量中(理论上它可以是其他东西，但通常这是您期望的数据加载器输出/模型输入)。</li></ul><p id="97c6" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><em class="ln"> collate_fn </em>是执行动态填充的最佳位置。幸运的是，Pytorch Dataloader在其构造函数中有一个参数来提供我们自己的实现，不需要覆盖任何东西。变形金刚库中的训练师类在其构造函数中有一个类似的参数，我们将使用它。它不是一个函数，而是等待一个“整理器”(一个特定于Transformers的类)的实例，它只有一个目的，包装<em class="ln"> collate </em>方法。</p><p id="3742" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在下面找到一个可能的<em class="ln">排序器</em>类的实现。</p><pre class="ks kt ku kv gt pu pv pw px aw py bi"><span id="ec9f" class="pa ob it pv b gy pz qa l qb qc"><em class="ln"># ...</em></span><span id="6888" class="pa ob it pv b gy qd qa l qb qc">def <strong class="pv jd">pad_seq</strong>(seq: List[int], max_batch_len: int, pad_value: int) -&gt; List[int]:<br/><em class="ln">    # IRL, use pad_sequence<br/>    # </em><a class="ae lh" href="https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pad_sequence.html" rel="noopener ugc nofollow" target="_blank"><em class="ln">https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pad_sequence.html</em></a><br/>    return seq + (max_batch_len - len(seq)) * [pad_value]</span><span id="5399" class="pa ob it pv b gy qd qa l qb qc"><a class="ae lh" href="http://twitter.com/dataclass" rel="noopener ugc nofollow" target="_blank">@dataclass</a><br/>class <strong class="pv jd">SmartCollator</strong>(DataCollator):<br/>    pad_token_id: int</span><span id="ff9c" class="pa ob it pv b gy qd qa l qb qc">    def <strong class="pv jd">collate_batch</strong>(self, batch: List[Features]) -&gt; Dict[str, torch.Tensor]:<br/>        batch_inputs = list()<br/>        batch_attention_masks = list()<br/>        labels = list()<br/>        <em class="ln"># find the max length of the mini batch</em><br/>        max_size = max([len(ex.input_ids) for ex in batch])<br/>        for item in batch:<br/>            <em class="ln"># apply padding at the mini batch level</em><br/>            batch_inputs += [pad_seq(item.input_ids, max_size, self.pad_token_id)]<br/>            batch_attention_masks += [pad_seq(item.attention_mask, max_size, 0)]<br/>            labels.append(item.label)<br/>        <em class="ln"># expected Transformers input format (dict of Tensors)</em><br/>        return {"input_ids": torch.tensor(batch_inputs, dtype=torch.long),<br/>                "attention_mask": torch.tensor(batch_attention_masks, dtype=torch.long),<br/>                "labels": torch.tensor(labels, dtype=torch.long)<br/>                }</span></pre><h2 id="4d8b" class="pa ob it bd oc pb pc dn og pd pe dp ok mi pf pg om mj ph pi oo mk pj pk oq iz bi translated">动态填充有助于减少训练时间吗？</h2><p id="c0b6" class="pw-post-body-paragraph ll lm it lo b lp os kd lr ls ot kg lu mi ou lx ly mj ov mb mc mk ow mf mg mh im bi translated">我们运行了4个实验，按每批大小进行分组，对于每组，我们比较了使用和不使用动态填充的情况。当为以下情况启用时:</p><ul class=""><li id="1734" class="nm nn it lo b lp lq ls lt mi no mj np mk nq mh nr ns nt nu bi translated">16批未截短的序列，时间从4h39减少到0h 59(-79%)；</li><li id="b756" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">64个序列的批次被截短为128个标记，计时从0h56减少到0h48 (-15%)。</li></ul><p id="0f27" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在这两种情况下，计时减少都是显著的，并且对于长序列是4倍强。这是有意义的，在训练集中，97%的例子短于128个令牌，所以对于大多数例子，我们为拥有493个最大序列大小而缴税。通过使用优化，我们只为有用的计算付费。</p><p id="3f97" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">对于128个记号的截断，仍然有增益，因为大多数序列仍然比128个记号小得多，并且BERT复杂度关于其输入长度是二次的，避免的计算成本低得多，并且训练时间减少“仅仅”15%。</p><h2 id="25ab" class="pa ob it bd oc pb pc dn og pd pe dp ok mi pf pg om mj ph pi oo mk pj pk oq iz bi translated">会影响准确性吗？</h2><p id="4ff3" class="pw-post-body-paragraph ll lm it lo b lp os kd lr ls ot kg lu mi ou lx ly mj ov mb mc mk ow mf mg mh im bi translated">我们运行了4个实验，按每批大小进行分组，对于每组，我们比较了使用和不使用动态填充的情况。当为以下情况启用时:</p><ul class=""><li id="13d1" class="nm nn it lo b lp lq ls lt mi no mj np mk nq mh nr ns nt nu bi translated">16批未截短序列，准确率从81.42%提高到82.0%；</li><li id="8e01" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">一批64个序列被截短为128个标记，准确率从81.0%提高到82.0%。</li></ul><p id="db9e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在这两种情况下，动态填充都可以提高精度。</p><h1 id="b441" class="oa ob it bd oc od oe of og oh oi oj ok ki ol kj om kl on km oo ko op kp oq or bi translated">均匀尺寸配料</h1><p id="28c2" class="pw-post-body-paragraph ll lm it lo b lp os kd lr ls ot kg lu mi ou lx ly mj ov mb mc mk ow mf mg mh im bi translated">统一批量生产由简单地构建由相似长度序列制成的批次组成。目的是在与动态填充结合使用时尽可能减少填充。</p><p id="b188" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">有许多方法可以实现它，我们遵循的方法是:</p><ul class=""><li id="ee15" class="nm nn it lo b lp lq ls lt mi no mj np mk nq mh nr ns nt nu bi translated">在一个简单的Python列表中按长度对示例进行排序，</li><li id="0fac" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">随机选择一个索引，</li><li id="7dfd" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">提取示例和后面的<em class="ln"> n </em>个示例(<em class="ln"> n </em>为批次/步长)，</li><li id="df64" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">从列表中删除提取的示例，</li><li id="7a70" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">重复此操作，直到列表中不再有示例。</li></ul><p id="6eb1" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这样，每一批都是由相似长度的序列组成的，但是后面的几批是不同长度的。</p><p id="051d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">天真(简单易懂/不干净)的实现可能看起来像这样:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="qe qf l"/></div></figure><h1 id="a1aa" class="oa ob it bd oc od oe of og oh oi oj ok ki ol kj om kl on km oo ko op kp oq or bi translated">统一大小批量真的能减少训练时间吗？</h1><p id="de19" class="pw-post-body-paragraph ll lm it lo b lp os kd lr ls ot kg lu mi ou lx ly mj ov mb mc mk ow mf mg mh im bi translated">为了减少时间，我们之前已经展示了动态填充带来了大量的训练时间减少，让我们比较使用动态填充和不使用统一大小批处理以及启用两种优化的训练时间。用于:</p><ul class=""><li id="ecc0" class="nm nn it lo b lp lq ls lt mi no mj np mk nq mh nr ns nt nu bi translated">批16个未截短的序列，训练时间从1h01减少到0h 52(-15%)；</li><li id="e6c6" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">一批64个序列被截短为128个标记，训练时间从0h48减少到0h30 (-38%)。</li></ul><p id="3773" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">所以在这两种情况下，我们天真的想法似乎带来了另一个显著的训练时间减少。</p><h1 id="72a2" class="oa ob it bd oc od oe of og oh oi oj ok ki ol kj om kl on km oo ko op kp oq or bi translated">统一尺寸配料是否会以任何方式影响精确度？</h1><p id="766b" class="pw-post-body-paragraph ll lm it lo b lp os kd lr ls ot kg lu mi ou lx ly mj ov mb mc mk ow mf mg mh im bi translated">通常，神经网络是在随机排序的数据点上训练的。均匀尺寸配料限制了这种随机性，因此引入了一种偏差，这种偏差在理论上可能会影响精度。</p><p id="b36f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我们将只比较有和没有统一尺寸配料的设置:</p><ul class=""><li id="94aa" class="nm nn it lo b lp lq ls lt mi no mj np mk nq mh nr ns nt nu bi translated">对于一批16个样品，当激活统一长度配料时，准确度从81.4%增加到81.6%；</li><li id="3b19" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">对于一批64个样品，当激活统一尺寸配料时，准确度从81.0%提高到81.7%。</li></ul><p id="3923" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在这两种情况下，都有改进，我们可以得出结论，对准确性没有负面影响。</p><p id="9c0c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">然而，我们结合多种选择进行了许多实验，根据<a class="mv mw ep" href="https://medium.com/u/c4669728a9af?source=post_page-----21bf7129db9e--------------------------------" rel="noopener" target="_blank">权重&amp;偏差</a>仪表盘，使用统一尺寸配料与准确性呈负相关。手动检查实验对后(有/没有选项)，这种效果不明显。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qg"><img src="../Images/6175f744dc3c3f8a7474992ae5688856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9YRURwcnsxnzLt9V"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qh"><img src="../Images/c3cc08f5ca92a530867725f3a324f7a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_S_lIBdoqMaj5XLe"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(如果您想深入了解，请随时查看<a class="ae lh" href="https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI" rel="noopener ugc nofollow" target="_blank">报告</a>)</p></figure><h1 id="d6c6" class="oa ob it bd oc od oe of og oh oi oj ok ki ol kj om kl on km oo ko op kp oq or bi translated">混合精度</h1><p id="05b5" class="pw-post-body-paragraph ll lm it lo b lp os kd lr ls ot kg lu mi ou lx ly mj ov mb mc mk ow mf mg mh im bi translated">通过Nvidia apex库，Pytorch上的混合精度是可能的。简而言之，在其最常见的模式中，混合精度包括以半精度执行大多数操作，并以全精度累加结果(更多信息请参见<a class="ae lh" href="https://nvidia.github.io/apex/amp.html" rel="noopener ugc nofollow" target="_blank"> apex文档</a>)。</p><p id="1c38" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">Apex以在某些场景中带来改进而闻名，有时它也会带来一些不稳定性(例如，训练期间的损失幅度比没有混合精度时更大)，并且很少避免模型收敛。换句话说，它不是一个银弹，而是一个有趣的工具来测试你的情况。</p><p id="633b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">好消息是Trainer class实现了开箱即用，要利用它，您只需要在命令行中添加正确的标志(“— fp16”)。</p><p id="ac18" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">关于16个长序列的小批量的训练时间，情况是不寻常的。用于:</p><ul class=""><li id="9957" class="nm nn it lo b lp lq ls lt mi no mj np mk nq mh nr ns nt nu bi translated">单独的混合精度通过将训练时间从4h38减少到2h50而使事情变得更好；</li><li id="eb0f" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">混合精度结合动态填充和统一大小批处理，使得训练变慢，从0h52到1h01！</li></ul><p id="502e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">原因可能是在第二种情况下，它增加了开销，并且没有多大帮助，因为大多数批处理只由短序列组成。混合精度对大型矩阵运算帮助最大。</p><p id="a28a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">当应用于64个短序列的小批量时，事情如预期的那样:</p><ul class=""><li id="1140" class="nm nn it lo b lp lq ls lt mi no mj np mk nq mh nr ns nt nu bi translated">单独使用时，时间训练从0h56减少到0h26</li><li id="9a3f" class="nm nn it lo b lp nv ls nw mi nx mj ny mk nz mh nr ns nt nu bi translated">结合其他两个选项，时间从0时30分减少到0时17分</li></ul><p id="2477" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这一次，即使步长由短序列组成，每个包含64个序列，使得矩阵足够大，可以从混合精度中受益。</p><p id="263b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">关于准确性，没有明确的模式。你可以通过查看<a class="mv mw ep" href="https://medium.com/u/c4669728a9af?source=post_page-----21bf7129db9e--------------------------------" rel="noopener" target="_blank">权重&amp;偏差</a> <a class="ae lh" href="https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI" rel="noopener ugc nofollow" target="_blank">报告</a>来自己拿主意。</p><h1 id="8538" class="oa ob it bd oc od oe of og oh oi oj ok ki ol kj om kl on km oo ko op kp oq or bi translated">重复的结果</h1><p id="9140" class="pw-post-body-paragraph ll lm it lo b lp os kd lr ls ot kg lu mi ou lx ly mj ov mb mc mk ow mf mg mh im bi translated">所有的实验都是使用相同的种子进行的。可能我们很幸运，我们的方法达到了精度，但不是用这个种子和这个数据集。</p><p id="44cd" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我们在启用所有优化设置的情况下用不同的种子重新运行16分钟的训练5次，并且再现了准确度/计时。</p><blockquote class="li lj lk"><p id="b1fd" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">对大模型也进行了同样的复制实验。结果是一样的。</p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qi"><img src="../Images/bdaa5d72389f1a95d110a7956fa0c6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LpMZaX87d0X8REpzx7d6ag.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">还是那句话，想要互动图？-&gt; <a class="ae lh" href="https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI" rel="noopener ugc nofollow" target="_blank">报告</a>&lt;—</p></figure><h1 id="5221" class="oa ob it bd oc od oe of og oh oi oj ok ki ol kj om kl on km oo ko op kp oq or bi translated">一个结论？</h1><p id="3f2f" class="pw-post-body-paragraph ll lm it lo b lp os kd lr ls ot kg lu mi ou lx ly mj ov mb mc mk ow mf mg mh im bi translated"><strong class="lo jd">我们已经证明，这两种技术都能在不降低准确性的情况下持续大幅缩短时间</strong>。此外，我们了解到，在具有小批量的数据集上，应该小心混合精度，因为如果没有足够的计算来执行，它可能会导致意外的训练变慢。</p><blockquote class="mx"><p id="25e1" class="my mz it bd na nb nc nd ne nf ng mh dk translated">我们确信这两种技术都是唾手可得的，应该被变形金刚用户广泛使用。</p></blockquote><p id="17a0" class="pw-post-body-paragraph ll lm it lo b lp nh kd lr ls ni kg lu mi nj lx ly mj nk mb mc mk nl mf mg mh im bi translated">总结一个更一般的想法，我们对这些简单的想法所获得的结果感到惊喜。只是为了这个故事，在另一个不相关的实验中，我们注意到X-NLI的法语训练集(它是英语数据集的机器翻译)质量很低(许多例子在法语中是绝对无意义的)，我们想知道翻译质量更好的它是否会提高测试集(这是人工翻译)的准确性。这对我们来说是一个重要的机会，因为如果它成功了，就意味着有大量的法语数据集可以使用。我们在DeepL上花了几块钱，翻译好了很多……而且准确度也没变(我们甚至觉得我们的度量有bug)。<strong class="lo jd">并非所有简单的想法都是平等的！</strong></p></div></div>    
</body>
</html>