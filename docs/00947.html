<html>
<head>
<title>Gradient Boosting from Almost Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">几乎从零开始的梯度提升</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-boosting-from-almost-scratch-c3ed0db2ffdb?source=collection_archive---------31-----------------------#2020-01-27">https://towardsdatascience.com/gradient-boosting-from-almost-scratch-c3ed0db2ffdb?source=collection_archive---------31-----------------------#2020-01-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dae5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过Python实现理解强大的机器学习算法</h2></div><p id="9133" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在过去的一个月里，我一直在慢慢地从头开始学习Joel Grus的《数据科学》第二版，我非常喜欢构建简单版本的机器学习算法。我总是通过实践学得更好，这本书符合这种学习风格。不幸的是，它没有实现梯度推进的一章。虽然我通读了陈天齐的伟大的<a class="ae le" href="https://homes.cs.washington.edu/~tqchen/data/pdf/BoostedTree.pdf" rel="noopener ugc nofollow" target="_blank">幻灯片</a>，我想我会通过自己实现一个简单版本的渐变增强来获得更好的理解。</p><p id="c3d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">梯度推进是一种相对简单的算法。它的工作原理是训练弱模型(最常见的是<a class="ae le" href="https://en.wikipedia.org/wiki/Decision_tree_learning" rel="noopener ugc nofollow" target="_blank">推车</a>)，这些弱模型来自前一次迭代的残差。通过将预测残差乘以0到1之间的学习率与先前的预测相加来生成新的预测。学习率防止模型向实际值迈出太大的一步并超过它们。小的学习率需要更长的时间收敛到最佳状态。找到合适的是一个平衡的行为。</p><p id="7a62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我强烈推荐陈天齐的<a class="ae le" href="https://homes.cs.washington.edu/~tqchen/data/pdf/BoostedTree.pdf" rel="noopener ugc nofollow" target="_blank">幻灯片</a>更彻底的处理渐变提升，或者看一下<a class="ae le" href="https://xgboost.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>的文档，这是他惊人的渐变提升包。</p><p id="47c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我说梯度推进几乎从零开始；我使用scikit-learn的模型作为我的弱学习器，并使用numpy用于某些数学函数及其数组结构。</p><p id="3d78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">梯度推进算法是这样的:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/7ef8783828af2c10ddcd40796e99dbaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*-N8J4xpBCb3ZsAzWzXQvpA.png"/></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/dabbe3a0f37fe888b8d95be9eed163c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*uHVt0H72R6k_EMiI1l8-nA.png"/></div></figure><p id="98bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我使用的第一个模型是scikit-learn的回归树。因此，潜在损失函数是均方误差、弗里德曼调整的均方误差和平均绝对误差。它们每个都包含正则化项，这对我来说是幸运的。scikit-learn的<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html" rel="noopener ugc nofollow" target="_blank">文档</a>中包含了对每一个更全面的解释。</p><p id="4d23" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另外，我用的是他们的<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html" rel="noopener ugc nofollow" target="_blank">岭回归模型</a>，它的损失函数是误差平方和+系数的l2范数乘以一个惩罚系数。你可以在大多数线性模型或机器学习教科书中了解更多信息，或者去<a class="ae le" href="https://en.wikipedia.org/wiki/Tikhonov_regularization" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="3048" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我用来实现boosting算法的代码如下。整篇笔记的链接会在文章的底部提供，作为继续阅读的激励。</p><pre class="lg lh li lj gt ln lo lp lq aw lr bi"><span id="df2b" class="ls lt it lo b gy lu lv l lw lx">import typing<br/>import numpy as np</span><span id="7e71" class="ls lt it lo b gy ly lv l lw lx">def GradBoost(model,<br/>              X_test: np.array,          # testing independent vars<br/>              X_train: np.array,         # training independent vars<br/>              y_train: np.array,         # training dependent var<br/>              boosting_rounds: int = 100,# number of boosting rounds<br/>              learning_rate: float = 0.1,# learning rate<br/>              verbose: bool = True       # shows progress bar<br/>              ) -&gt; np.array: <br/>    '''<br/>    Takes in a model and performs gradient boosting using it.<br/>    '''<br/>    import numpy as np<br/>    <br/>    # initalize guess of our training target variable using the mean<br/>    y_hat_train = np.repeat(np.mean(y_train), len(y_train))<br/>    <br/>    # initialize out of sample prediction with training mean<br/>    y_hat_train_test = np.repeat(np.mean(y_train), len(X_test))</span><span id="8d4d" class="ls lt it lo b gy ly lv l lw lx">    # calculate the training residuals fusing the first guess<br/>    pseudo_resids = y_train - y_hat_train<br/>    <br/>    # performs gradient boosting with a tqdm progress bar<br/>    if verbose:</span><span id="dc70" class="ls lt it lo b gy ly lv l lw lx">        from tqdm import tqdm</span><span id="5f9c" class="ls lt it lo b gy ly lv l lw lx">        # iterates through the boosting round<br/>        for _ in tqdm(range(0, boosting_rounds)):</span><span id="c4b4" class="ls lt it lo b gy ly lv l lw lx">            # fit the model to the pseudo residuals<br/>            model = model.fit(X_train, pseudo_resids)</span><span id="b207" class="ls lt it lo b gy ly lv l lw lx">            # increment y_hat_train with the predicted resids*lr<br/>            y_hat_train += learning_rate * model.predict(X_train)  <br/>     <br/>            # increment the predicted test y as well<br/>            y_hat_train_test += (learning_rate *   <br/>                                 model.predict(X_test))</span><span id="ed12" class="ls lt it lo b gy ly lv l lw lx">            # calculate the pseudo resids for next round<br/>            pseudo_resids = y_train - y_hat_train</span><span id="4a74" class="ls lt it lo b gy ly lv l lw lx">    # performs gradient boosting without a progress bar        <br/>    else:</span><span id="d576" class="ls lt it lo b gy ly lv l lw lx">        # iterates through the boosting round<br/>        for _ in range(0, boosting_rounds):</span><span id="989b" class="ls lt it lo b gy ly lv l lw lx">            # fit the model to the pseudo residuals<br/>            model = model.fit(X_train, pseudo_resids)</span><span id="945b" class="ls lt it lo b gy ly lv l lw lx">            # increment the y_hat_train with the pseudo resids*lr<br/>            y_hat_train += learning_rate * model.predict(X_train)<br/>       <br/>            # increment the predicted test y as well<br/>            y_hat_train_test += (learning_rate * <br/>                                 model.predict(X_test))</span><span id="92ea" class="ls lt it lo b gy ly lv l lw lx">            # calculate the pseudo resids for next round<br/>            pseudo_resids = y_train - y_hat_train</span><span id="101a" class="ls lt it lo b gy ly lv l lw lx">    # return a tuple of the training y_hat and the test y_hat<br/>    return y_hat_train, y_hat_train_test</span></pre><p id="6d33" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们生成一些测试数据，看看这个函数是否有效。我选择模拟数据，所以我知道一些自变量和目标变量之间有关系。我用scikit-learn的make_regression函数生成了1000个观察值，有20个自变量，其中四分之三实际上包含有用的信息。然后，我将数据分成训练和测试数据集。</p><pre class="lg lh li lj gt ln lo lp lq aw lr bi"><span id="059c" class="ls lt it lo b gy lu lv l lw lx">from sklearn.datasets import make_regression</span><span id="5d9b" class="ls lt it lo b gy ly lv l lw lx">X, y = make_regression(n_samples=1000, <br/>                       n_features=20, <br/>                       n_informative=15, <br/>                       n_targets=1, <br/>                       bias=0.0, <br/>                       noise=20,<br/>                       shuffle=True,<br/>                       random_state=13)</span><span id="d794" class="ls lt it lo b gy ly lv l lw lx">X_train = X[0:int(len(X) / 2)]<br/>y_train = y[0:int(len(X) / 2)]</span><span id="2635" class="ls lt it lo b gy ly lv l lw lx">X_test = X[int(len(X) / 2):]<br/>y_test = y[int(len(X) / 2):]</span></pre><p id="1cad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当使用梯度推进时，太少的迭代导致欠拟合模型，太多的迭代导致过拟合模型。在我们进行预测之前，了解训练均方差如何随着更多的提升迭代而演变是有帮助的，因此我们可以尝试并拟合一个校准良好的模型。</p><p id="7b82" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当实际尝试解决一个实际问题时，您将使用网格搜索和k-folds交叉验证来确定推进回合的数量以及其他参数。为了便于演示，我做了一些更简单的事情。我只是绘制了5到100轮的训练均方误差与助推轮数的关系，步长为5，并选择了半任意截止值。</p><p id="cfca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该实现中的回归树深度为3，损失函数被指定为正则化均方误差。岭回归使用三重交叉验证在{0.01，0.1，1，10}中的每一步选择最佳l2范数惩罚系数。两种模型的学习率都设置为0.1。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lz"><img src="../Images/bc445bf895d091c53d9e14d5519a814e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9fhpbcgA7WYUstV7e8_IQg.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">训练MSE vs助推回合</p></figure><p id="8691" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当使用树作为我们的弱学习器时，均方误差快速下降，直到它在30次提升迭代左右达到拐点。在那之后，当它接近100轮的时候，它慢慢地向下倾斜。当使用岭回归模型时，训练均方误差猛跌，直到20轮，并且在30轮之后才真正稳定下来。我们将使用100作为使用树的提升迭代次数，30用于岭回归提升模型。但是首先，让我们看看0次和10次提升迭代的结果。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mi"><img src="../Images/615cb090a390dd70eaa3167403b6625f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6va6LON6Eu_Pw2XelCfX7w.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">第0轮助推时的预测y值与实际y值</p></figure><p id="6384" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的图中，红线是预测值等于实际值的地方，代表完美的预测。测试数据越接近这条线，我们的模型就越准确。如果训练预测是紧密拟合的，但是测试预测到处都是，则模型与训练数据过度拟合。</p><p id="0ce4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在0轮助推的情况下，我们只是猜测平均值。正如你所料，这个天真的“模型”不符合数据。让我们看看10轮助推后事情会如何发展。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mi"><img src="../Images/9bd38ba04f0fa1941c3f9dcfd0b3c911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k5mY_VmrOgTvXgIR48trhQ.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">10轮助推时的预测y值与实际y值</p></figure><p id="9b8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在第10轮，每个模型的预测开始向完美的预测线旋转。这一过程随着学习速度的提高而加快，但也有可能超过标准。对于使用岭回归作为弱学习器的模型，预测云要紧密得多。这可能是因为我们生成的数据包含线性关系，而岭回归是线性模型。现在我想是时候做我们最后的预测和比较了。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mi"><img src="../Images/5c9f95279725b62b8aed55b4aab5c954.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B4n0IQ-6xOPjZbT_g83DTA.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">100轮增强树的预测y值与实际y值，以及30轮增强岭回归的预测y值与实际y值</p></figure><p id="27fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在100次迭代时，我们的提升树模型是不错的，但是它在测试数据集中的表现差于目标变量的平均值。看起来也有点过合身。尽管如此，它还是比10回合模型更适合。我们以岭回归作为弱学习者的模型更适合。同样，这可能是由于模拟数据中的线性结构。说到线性结构，让我们看看这些模型与简单的多元线性回归相比如何。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mj"><img src="../Images/e8f2d5dfe7fe51f6563777589ac83ed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0A9FcI_SLqBccbBTUsEIhA.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">多元线性回归的预测y值与实际y值</p></figure><p id="a08b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该图表明多元线性回归是我们的最佳拟合模型。让我们检查测试集的均方误差以确保正确。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mk"><img src="../Images/1dd8505e74f621c158f42ebaaf040784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*VqLBEQAEgliag1QH8Lt2qw.png"/></div></div></figure><p id="4671" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">多元线性回归胜出！让这成为可视化和调查你的数据的一个提醒。在用算法解决问题之前，请始终仔细考虑形成数据结构的数据生成过程。或者不要。这是你的生活。</p><p id="3d29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看更多的提升回合会发生什么，比方说1000个提升树和100个提升岭回归。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mi"><img src="../Images/3d076e95c568ac8f4f00d3728f500beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VQyZAeRfBpsqFQkOLAyGmg.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">增强树的1000轮增强和增强岭回归的100轮增强时的预测y值与实际y值</p></figure><p id="cc47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">增强的树显然过度拟合了训练数据，但是来自增强的岭回归的结果相当好。同样，数据中的关系是线性的。让我们看看新的均方误差。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/3b7a97a717e441798613dcb0fa67e518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*CGrdMVDNh4djFpYJW3b-kA.png"/></div></figure><p id="54c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在100轮提升时，提升岭回归的表现接近多元线性回归，多元线性回归代表真实的模型(加上几个应被忽略的噪声变量)。事实证明，有时候用算法解决问题是有效的。尽管如此，最好还是了解你的数据。</p><p id="ebd5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望这篇文章能让你更好地了解最强大的机器学习技术之一的内幕(抱歉，这是陈词滥调)。现在，您可以一边去喝咖啡，一边思考您的计算机正在做什么，等待您的网格搜索完成。</p><p id="ad82" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在这里得到我的代码并使用它。</p><p id="ba43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">点击查看这篇文章基于<a class="ae le" href="https://github.com/jkclem/GradBoost/blob/master/example/GradBoost%20Notebook.ipynb" rel="noopener ugc nofollow" target="_blank">的Jupyter笔记本。</a></p><p id="4f57" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在这里找到更多我的文章<a class="ae le" href="https://medium.com/@jkclements2016" rel="noopener"/>。</p></div></div>    
</body>
</html>