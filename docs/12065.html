<html>
<head>
<title>Decision Trees: A step-by-step approach to building DTs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树:构建 DTs 的逐步方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-a-step-by-step-approach-to-building-dts-58f8a3e82596?source=collection_archive---------4-----------------------#2020-08-20">https://towardsdatascience.com/decision-trees-a-step-by-step-approach-to-building-dts-58f8a3e82596?source=collection_archive---------4-----------------------#2020-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/967537ee5e95d699dca5239aa0f90967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vv8nYWi2-WqlbgLg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">马里乌斯·马萨拉尔在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><h1 id="5880" class="kg kh jj bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="661e" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="lg jk">决策树(DTs) </strong>是一种用于分类和回归的非参数监督学习方法。目标是创建一个模型，通过学习从数据特征推断的简单决策规则来预测目标变量的值。决策树通常用于运筹学，特别是决策分析，以帮助确定最有可能达到目标的策略，但也是机器学习中的一种流行工具。</p><h1 id="827f" class="kg kh jj bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">语境</h1><p id="da89" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在本文中，我们将讨论以下主题</p><ol class=""><li id="9d89" class="mc md jj lg b lh me ll mf lp mg lt mh lx mi mb mj mk ml mm bi translated">通常什么是决策树</li><li id="8d85" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">决策树的类型。</li><li id="20c7" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">用于构建决策树的算法。</li><li id="c3fc" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">构建决策树的逐步过程。</li></ol><h1 id="8cbd" class="kg kh jj bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">什么是决策树？</h1><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ab7a7b68a476617860cb28dfa52433bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*9xHwoqJdCbgyCbSR6nB5eA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 1-基于是/否问题的决策树</p></figure><p id="de74" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">上图是一个简单的决策树。如果一个人是非素食者，那么他/她吃鸡肉(最有可能)，否则，他/她不吃鸡肉。一般来说，决策树会问一个问题，并根据答案对人进行分类。该决策树基于是/否问题。在数字数据上构建决策树同样简单。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/5db21d61516c3be4de8033ed450fe307.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*focCyRj3jp8XuuwbuK5AOw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 2-基于数字数据的决策树</p></figure><p id="3f9a" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">如果一个人开车时速超过 80 英里，我们可以认为这是超速，否则不是。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/ea00c5511da55798eedecb5f71e0c6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*lvfBfBcUWloYh1G270oaNQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 3-分级数据的决策树</p></figure><p id="35ae" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">这里有一个更简单的决策树。该决策树基于分级数据，其中 1 表示速度太高，2 表示速度较低。如果一个人超速超过等级 1，那么他/她就是严重超速。如果这个人在速度等级 2 之上但在速度等级 1 之下，那么他/她超速了，但没超速那么多。如果这个人低于速度等级 2，那么他/她在速度限制内驾驶得很好。</p><p id="714b" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">决策树中的分类可以是类别的，也可以是数字的。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/22a54284fe991a4db62286e8798f2e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B5GHTAWaG1z_H01-8iUHvQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 4-复杂 DT</p></figure><p id="acbe" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">这里有一个更复杂的决策树。它结合了数字数据和是/否数据。在很大程度上，决策树很容易使用。你从顶端开始，一路向下，直到你无法再前进。样本就是这样分类的。</p><p id="b66a" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">树的最顶端被称为<strong class="lg jk"> <em class="nd">根节点</em> </strong> <em class="nd"> </em>或者仅仅是<strong class="lg jk"> <em class="nd">根。</em> </strong>中间的节点称为<strong class="lg jk"> <em class="nd">内部节点</em> </strong>。内部节点有指向它们的箭头和远离它们的箭头。末端节点被称为<strong class="lg jk"> <em class="nd">叶节点</em> </strong>或者仅仅是<strong class="lg jk"> <em class="nd">叶节点</em> </strong>。叶节点有指向它们的箭头，但没有远离它们的箭头。</p><p id="0748" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">在上面的图中，根节点用矩形表示，内部节点用圆形表示，叶节点用倒三角形表示。</p><h1 id="f488" class="kg kh jj bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">构建决策树</h1><p id="1813" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">构建决策树有几种算法。</p><ol class=""><li id="a9ce" class="mc md jj lg b lh me ll mf lp mg lt mh lx mi mb mj mk ml mm bi translated">CART-分类和回归树</li><li id="b674" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">ID3-迭代二分法 3</li><li id="02c7" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">C4.5</li><li id="bcdf" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">CHAID 卡方自动交互检测</li></ol><p id="cfec" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们将只讨论 CART 和 ID3 算法，因为它们是主要使用的算法。</p><h1 id="9d8b" class="kg kh jj bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">手推车</h1><p id="3e64" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">CART 是一种 DT 算法，根据因变量(或目标变量)是分类变量还是数值变量，分别生成<strong class="lg jk">二进制</strong> <em class="nd">分类</em>或<em class="nd">回归</em>树。它以原始形式处理数据(不需要预处理)，并且可以在同一个 DT 的不同部分多次使用相同的变量，这可以揭示变量集之间复杂的相互依赖关系。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/9b2a272d7a17d964123ee7f15cbd31dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*6QRxaxZzA5_7YOPMElxfyQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 5-样本数据集</p></figure><p id="cc82" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">现在我们将讨论如何从原始数据表中构建决策树。在上面给出的例子中，我们将建立一个决策树，使用胸痛、良好的血液循环和阻塞动脉的状态来预测一个人是否患有心脏病。</p><p id="a624" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们必须知道的第一件事是哪个特性应该在树的顶部或根节点中。我们先来看看胸痛是如何预测心脏病的。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/4e2e255d92417a31dae328c5c9254c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*EH7Yguljck8-oW45_H5dcg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 6-胸痛作为根节点</p></figure><p id="b842" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">有两个叶节点，分别对应胸痛的两种结果。每一个叶子包含有心脏病和没有心脏病的患者的数目，用于对应的胸痛条目。现在我们对良好的血液循环和阻塞的动脉做同样的事情。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/bfb22451bd67aa2db9b7120a4a652bda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*JQ60zWBTF8sp_fl6C_5eug.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 7-良好的血液循环作为根节点</p></figure><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/f5d2649e07129351b83e2915567a10e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*2dhkbzX29N5C6xjB1x8ikg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 8-阻塞的动脉作为根节点</p></figure><p id="4247" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们可以看到，这三个特征都不能很好地区分心脏病患者和非心脏病患者。值得注意的是，在所有三种情况下，患有心脏病的患者总数是不同的。这样做是为了模拟真实数据集中存在的缺失值。</p><p id="1d4d" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">因为没有一个叶节点不是 100% '有心脏病'就是 100% '没有心脏病'，所以都认为<strong class="lg jk"> <em class="nd">不纯。</em> </strong>为了决定哪种分离最好，我们需要一种方法来测量和比较<strong class="lg jk"> <em class="nd">杂质。</em> </strong></p><p id="e64b" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">CART 算法中用于测量杂质的指标是<strong class="lg jk"> <em class="nd">基尼杂质分数</em> </strong>。计算基尼系数很容易。我们先来计算一下胸痛的基尼杂质。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/4e2e255d92417a31dae328c5c9254c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*EH7Yguljck8-oW45_H5dcg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 9-胸痛分离</p></figure><p id="c84b" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">对于左边的叶子，</p><pre class="mt mu mv mw gt ni nj nk nl aw nm bi"><span id="8d5b" class="nn kh jj nj b gy no np l nq nr"><strong class="nj jk">Gini impurity = 1 - (probability of ‘yes’)² - (probability of ‘no’)²</strong><br/>              = 1 - (105/105+39)² - (39/105+39)²<br/>Gini impurity = 0.395</span></pre><p id="7682" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">类似地，计算右叶节点的 Gini 杂质。</p><pre class="mt mu mv mw gt ni nj nk nl aw nm bi"><span id="62e9" class="nn kh jj nj b gy no np l nq nr"><strong class="nj jk">Gini impurity = 1 - (probability of ‘yes’)² - (probability of ‘no’)²</strong><br/>              = 1 - (34/34+125)² - (125/34+125)²<br/>Gini impurity = 0.336</span></pre><p id="7847" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">现在我们已经测量了两个叶节点的基尼系数，我们可以计算总基尼系数，用胸痛来区分有和没有心脏病的患者。</p><p id="0588" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">叶节点不代表相同数量的患者，因为左叶代表 144 名患者，而右叶代表 159 名患者。因此，总 Gini 杂质将是叶节点 Gini 杂质的加权平均值。</p><pre class="mt mu mv mw gt ni nj nk nl aw nm bi"><span id="48d9" class="nn kh jj nj b gy no np l nq nr">Gini impurity = (144/144+159)*0.395 + (159/144+159)*0.336<br/>              = 0.364</span></pre><p id="7d39" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">同样,“血液循环良好”和“动脉阻塞”的总基尼系数计算如下</p><pre class="mt mu mv mw gt ni nj nk nl aw nm bi"><span id="f3fc" class="nn kh jj nj b gy no np l nq nr">Gini impurity for ‘good blood circulation’ = 0.360<br/>Gini impurity for ‘blocked arteries’ = 0.381</span></pre><p id="11ed" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">“良好的血液循环”在树中具有最低的杂质分数，这象征着它最好地分离了患有和未患有心脏病的患者，因此我们将在根节点使用它。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/59961954049019f96116aef584e55ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TlGCjQBKxmFXLB_rKanGZg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 10-根节点处良好的血液循环</p></figure><p id="bff5" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">现在我们需要弄清楚“胸痛”和“动脉阻塞”如何区分左淋巴结的 164 名患者(37 名有心脏病，127 名无心脏病)。</p><p id="c3c1" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">就像我们之前所做的那样，我们将把这些‘胸痛’患者分开，并计算基尼杂质值。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d79319736e9b31155b19e51fa2594b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*xHuLs5homusnLG3Sus5_ew.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 11-胸痛分离</p></figure><p id="d4e7" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">基尼系数为 0.3。然后我们对“堵塞的动脉”做同样的事情。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/a52b3f14420d24f45ff916facf454536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*7pAUoeGiCyddOS56S7eBkQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 12-阻塞动脉分离</p></figure><p id="b849" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">基尼系数为 0.29。由于“阻塞的动脉”具有最低的基尼系数，我们将在图 10 的左侧节点使用它来进一步分离患者。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/420d742de041e1ec672e45b15eaed93b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g9-exTd8eM0Tk9X-VHRMqA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 13-阻塞动脉分离</p></figure><p id="6281" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们只剩下“胸痛”，所以我们将看到它如何很好地分离左侧淋巴结中的 49 名患者(24 名有心脏病，25 名无心脏病)。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/bc4bc4fc8a8d24d74cc74e07f5b3d239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*NtSEfBf051u1uegq9FxH6Q.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 14-左侧淋巴结的胸痛分离</p></figure><p id="15af" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们可以看到胸痛在区分病人方面做得很好。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/ee0954d6ebcc5a3faaec49f66baafb84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pWcfZ3QIsLTdYSWR6DdLNQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 15-最终胸痛分离</p></figure><p id="4d1a" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">这些是树的这个分支左边的最后的叶子节点。现在，让我们看看当我们尝试使用“胸痛”来分离具有 13/102 个患者的节点时会发生什么。请注意，这个节点中几乎 90%的人没有心脏病。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/abef686213462d7dc456f1cee11ea6e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*kT33VuaPkbiUoVmb4_L41Q.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 16-右侧淋巴结的胸痛分离</p></figure><p id="e4c2" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">这种分离的基尼系数为 0.29。但是在使用胸痛来分离患者之前，母节点的基尼不纯度是</p><pre class="mt mu mv mw gt ni nj nk nl aw nm bi"><span id="9d3c" class="nn kh jj nj b gy no np l nq nr">Gini impurity = 1 - (probability of yes)² - (probability of no)²<br/>              = 1 - (13/13+102)² - (102/13+102)²<br/>Gini impurity = 0.2</span></pre><p id="f7ae" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">如果我们不使用“胸痛”来区分患者，杂质会更低。所以我们将使它成为叶节点。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/ed845f47fe62b3b87481e2d51e4bf7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_7g290xk9QVtnkSivHakLA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 17-左侧已完成</p></figure><p id="20a7" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">此时，我们已经完成了树的整个左侧。按照相同的步骤，计算出树的右侧。</p><ol class=""><li id="d83d" class="mc md jj lg b lh me ll mf lp mg lt mh lx mi mb mj mk ml mm bi translated">计算基尼系数。</li><li id="8dca" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">如果节点本身得分最低，那么就没有必要再分离患者，它就变成了一个叶节点。</li><li id="a654" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">如果分离数据导致改进，则选择杂质值最低的分离。</li></ol><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/52a0d8efa6dd36bbfe50afd467b663c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*17qvtR0E3ZUZv_QhmpBApw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 18-完整的决策树</p></figure><h1 id="63e6" class="kg kh jj bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">ID3</h1><p id="0b41" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">除了用于测量纯度/杂质的方法之外，使用 ID3 算法构建决策树的过程几乎类似于使用 CART 算法。ID3 算法中用于测量纯度的度量被称为<strong class="lg jk"> <em class="nd">熵</em> </strong>。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/b3f7c8f518607b7e0e7f6646898b89da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kUN_DHobRsQ6FeZENVKDzQ.png"/></div></div></figure><p id="baaa" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">熵是一种度量样本子集中某个类的不确定性的方法。假设项目属于子集 S，具有两个类别正和负。熵被定义为表示 x 是正还是负所需的位数。</p><p id="bc50" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">熵总是给出一个介于 0 和 1 之间的数。因此，如果使用属性分离后形成的子集是纯的，那么我们将需要 0 位来判断是正还是负。如果所形成的子集具有相等数量的正项和负项，那么所需的位数将是 1。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/299b65d1b0a6f002396a0b72b32ab319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*tDm_fIjwr0M-WUWVne8N6Q.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 19。熵与 p(+)的关系</p></figure><p id="d21f" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">上图显示了熵和正类概率之间的关系。正如我们所看到的，熵达到 1，这是最大值，这时一个项目有相等的机会成为正的或负的。当 p(+)趋于零(象征 x 为负)或 1(象征 x 为正)时，熵最小。</p><p id="010b" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">熵告诉我们每个子集在分裂后有多纯或不纯。我们需要做的是汇总这些分数，检查拆分是否可行。这是通过<strong class="lg jk">信息增益</strong>完成的。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/a5a3f35dfe561ebdf65dfe9c6775904c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*inkb4Cc_Bv-cNOs_-OxFHg.png"/></div></figure><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/59961954049019f96116aef584e55ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TlGCjQBKxmFXLB_rKanGZg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 20。构建 ID3 树</p></figure><p id="80cd" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">考虑我们上面讨论的 CART 算法的这部分问题。我们需要从<code class="fe ob oc od nj b"><strong class="lg jk">chest pain</strong></code>和<code class="fe ob oc od nj b"><strong class="lg jk">blocked arteries</strong></code>中决定使用哪个属性来分离包含 164 个患者(37 个患有心脏病，127 个没有心脏病)的左侧节点。我们可以将分裂前的熵计算为</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/54ca58ef5ccea6eb3ef4147ea134ecea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*q2pGfLJqtyDlM8X182Qxmg.png"/></div></figure><p id="09f5" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">让我们看看<code class="fe ob oc od nj b"><strong class="lg jk">chest pain</strong></code>如何区分病人</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d79319736e9b31155b19e51fa2594b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*xHuLs5homusnLG3Sus5_ew.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 21-胸痛分离</p></figure><p id="ba36" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">可以计算左侧节点的熵</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi of"><img src="../Images/0dbe5cb3f79991d3949bd76dfa758df2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*I95LvlMWUjrHAx0me_WFtw.png"/></div></figure><p id="0367" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">类似地，右边节点的熵</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/2a788ce3fc9085268e78a85a7e78195f.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*5rGf4t92Cc0of_n8PZSCpQ.png"/></div></figure><p id="3195" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">使用<code class="fe ob oc od nj b"><strong class="lg jk">chest pain</strong></code>分裂后熵的总增益</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/74fcd7c07eb5b418292c303d651426f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*7axNsQQ0EJlOJKjzrAkPkw.png"/></div></figure><p id="667e" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">这意味着，如果在当前情况下，如果我们选择<code class="fe ob oc od nj b"><strong class="lg jk">chest pain</strong></code>来分割患者，我们将获得关于患者是否患有心脏病的 0.098 比特的确定性。对<code class="fe ob oc od nj b"><strong class="lg jk">blocked arteries</strong></code>、<strong class="lg jk">、</strong>做同样的事情，得到的增益是 0.117。既然用<code class="fe ob oc od nj b"><strong class="lg jk">blocked arteries</strong></code> <strong class="lg jk"> </strong>分裂给了我们更多的确定性，那就摘了吧。我们可以对所有节点重复相同的过程，以基于 ID3 算法构建 DT。</p><p id="a8a5" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">注意:可以通过对所需增益值施加最小阈值来决定是将节点分成 2 个还是将其声明为叶节点。如果获得的增益高于阈值，我们可以分裂节点，否则，将其作为叶节点。</p><h1 id="21e8" class="kg kh jj bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">摘要</h1><p id="420d" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">以下是这篇文章的要点</p><ol class=""><li id="bb06" class="mc md jj lg b lh me ll mf lp mg lt mh lx mi mb mj mk ml mm bi translated">决策树背后的一般概念。</li><li id="1b37" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">决策树的基本类型。</li><li id="bd86" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">构建决策树的不同算法。</li><li id="0d29" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">使用 CART 算法构建决策树。</li><li id="86fe" class="mc md jj lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">使用 ID3 算法构建决策树。</li></ol><h1 id="e7f3" class="kg kh jj bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">参考</h1><ol class=""><li id="1542" class="mc md jj lg b lh li ll lm lp oi lt oj lx ok mb mj mk ml mm bi translated">参考 youtube 上的这个播放列表，了解更多关于使用 CART 算法构建决策树的细节。</li></ol><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="c65d" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">2.关于使用 ID3 算法构建决策树的更多细节，请参考 youtube 上的这个播放列表。</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="on om l"/></div></figure><p id="0e6b" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">PS:-我将很快发布另一篇关于回归树和随机森林的文章。<a class="ae jg" href="https://medium.com/@skumar.gokul" rel="noopener"> <strong class="lg jk"> <em class="nd">敬请期待</em> </strong> </a>:)</p><p id="6520" class="pw-post-body-paragraph le lf jj lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">请在这里查看我关于数据科学和机器学习的其他文章<a class="ae jg" href="https://medium.com/@skumar.gokul" rel="noopener"><strong class="lg jk"><em class="nd"/></strong>。</a>欢迎在评论和<a class="ae jg" href="https://www.linkedin.com/in/gokul-s-kumar" rel="noopener ugc nofollow" target="_blank"><strong class="lg jk"><em class="nd">LinkedIn</em></strong></a>上寻求更深入的讨论。</p></div></div>    
</body>
</html>