<html>
<head>
<title>Understanding Text Vectorizations I: How Having a Bag of Words Already Shows What People Think About Your Product</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解文本向量化 I:如何拥有一袋单词已经显示了人们对你的产品的看法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a?source=collection_archive---------44-----------------------#2020-07-25">https://towardsdatascience.com/understanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a?source=collection_archive---------44-----------------------#2020-07-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9f6d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">自然语言处理</h2><div class=""/><div class=""><h2 id="e274" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Sklearn 管道、SHAP 和面向对象编程在情感分析中的应用</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f3df2dffd49f40a38e240656842a0567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eeullBznilNT3W29r5LjYw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@amadorloureiroblanco?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">阿玛多·洛雷罗</a>在<a class="ae lh" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="01cb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">你知道吗，在神经网络变得无处不在之前，我们已经(几乎)解决了情感分析问题？</p><p id="cb1c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在当前自然语言处理(NLP)越来越依赖深度学习模型来产生惊人性能的时代，我们经常忽略最简单类型的文本矢量化技术的重要性——单词袋(BOW)和词频-逆文档频率(TF-IDF)。事实上，通过这两种技术，我们已经能够以超过 80%的准确率预测一段给定文本的情感。换句话说，我们对最先进的深度学习模型所做的一切只是试图从这些分类问题中挤出更多的性能改进。在某种程度上，几乎所有更复杂的模型都依赖于 BOW 和 TF-IDF 的概念。在这篇博文中，我们将在 sklearn pipelines 的帮助下，尝试理解和实现 BOW 和 TF-IDF。</p><h2 id="c4a9" class="mn mo it bd mp mq mr dn ms mt mu dp mv lr mw mx my lv mz na nb lz nc nd ne iz bi translated"><strong class="ak">为什么选择 Sklearn 管道？</strong></h2><p id="d4d5" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">您可能会认为使用管道是一种极端的大材小用。事实上，实现管道需要在面向对象编程(OOP)中大量使用，这可能会使调试变得相当困难。虽然我花了两周的时间来实现 BOW 和 TF-IDF(其间我犯了一些错误，在另一篇博客中会有更多的介绍)，但这绝对值得花时间和精力。通过使用 sklearn 管道，我们能够在单个函数调用中使用原始数据进行转换和训练/预测。例如，如果我们想使用 sklearn 的 TF-IDF 矢量器，训练 logistic 回归模型，我们只需使用以下几行代码。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="6cc1" class="mn mo it nl b gy np nq l nr ns"><strong class="nl jd">from</strong> sklearn.feature_extraction.text <strong class="nl jd">import</strong> CountVectorizer<br/><strong class="nl jd">from</strong> sklearn.linear_model <strong class="nl jd">import</strong> LogisticRegression<br/><strong class="nl jd">from</strong> sentiment_analysis.models.model <strong class="nl jd">import</strong> StreamlinedModel</span><span id="4695" class="mn mo it nl b gy nt nq l nr ns">logistic = StreamlinedModel(<br/>    transformer_description="Bag of words",<br/>    transformer=CountVectorizer,<br/>    model_description="logisitc regression model",<br/>    model=LogisticRegression,<br/>)</span></pre><p id="aae2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe nu nv nw nl b">TfidfVectorizer</code>和<code class="fe nu nv nw nl b">LogisticRegression</code>是来自 sklearn 的模块对象，而<code class="fe nu nv nw nl b">StreamlinedModel</code>是我们将要实现的管道对象。正如你可能已经看到的，有了这个结构，我们将能够很容易地用任何变形金刚和模型交换上述两个对象——这个结构将保持界面的通用性，并使特征提取变得轻而易举。还有一些其他真正重要的原因，我们希望以这种方式构建模型，但这将在另一篇博客文章中讨论。</p><h2 id="cb2f" class="mn mo it bd mp mq mr dn ms mt mu dp mv lr mw mx my lv mz na nb lz nc nd ne iz bi translated">实现<em class="nx">流线型模型</em></h2><p id="d48f" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">要实现<code class="fe nu nv nw nl b">StreamlinedModel</code>类，我们应该回忆一下我们熟悉的大多数 sklearn 包是如何使用的。举个例子，</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="1223" class="mn mo it nl b gy np nq l nr ns">logistic = LogisticRegression()<br/>logistic.fit(X_train, y_train)<br/>logistic.predict(X_test)</span></pre><p id="f9c6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的<code class="fe nu nv nw nl b">StreamlinedModel</code>将有相同的行为，因此我们将在类中添加相同的<code class="fe nu nv nw nl b">.fit</code>和<code class="fe nu nv nw nl b">.predict</code>方法。其他有用的方法包括<code class="fe nu nv nw nl b">.predict_proba</code>和<code class="fe nu nv nw nl b">.score</code>。这些方法可以只是 sklearn 的<code class="fe nu nv nw nl b">BaseEstimator</code>中相同方法的包装器。例如，管道培训模型可以简单地是</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="c85e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们可以构建一个名为<code class="fe nu nv nw nl b">make_model_pipeline</code>的类方法，它在类实例化时使用 4 个参数，并创建一个 sklearn 管道对象。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="84ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个<code class="fe nu nv nw nl b">self.pipeline</code>就是我们可以称之为<code class="fe nu nv nw nl b">.fit</code>和<code class="fe nu nv nw nl b">.predict</code>等的同一个对象。就是这样！我们已经创建了<code class="fe nu nv nw nl b">StreamlinedModel</code>。下面是完整的实现</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h2 id="6556" class="mn mo it bd mp mq mr dn ms mt mu dp mv lr mw mx my lv mz na nb lz nc nd ne iz bi translated">词汇袋模型</h2><p id="cf02" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">由于可以灵活地将变压器转换为我们选择的任何变压器，我们现在可以将相同的接口用于我们定制的 BOW 和 TF-IDF 变压器。出于区分的目的，我们将我们定制的弓变压器称为<code class="fe nu nv nw nl b">WordFrequencyVectorizer</code>。同样的<code class="fe nu nv nw nl b">StreamlinedModel</code>可以以如下方式使用</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="bc5c" class="mn mo it nl b gy np nq l nr ns"><strong class="nl jd">from</strong> models.feature <strong class="nl jd">import</strong> WordFrequencyVectorizer<br/><strong class="nl jd">from</strong> sklearn.linear_model <strong class="nl jd">import</strong> LogisticRegression<br/><strong class="nl jd">from</strong> sentiment_analysis.models.model <strong class="nl jd">import</strong> StreamlinedModel</span><span id="3bd8" class="mn mo it nl b gy nt nq l nr ns">logistic = StreamlinedModel(<br/>    transformer_description="Bag of Words",<br/>    transformer=WordFrequencyVectorizer,<br/>    model_description="logisitc regression model",<br/>    model=LogisticRegression,<br/>)</span></pre><p id="455e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇博文中，我们将重点讨论单词包模型的实现，而将 TF-IDF 留给下一篇。</p><h2 id="bc1f" class="mn mo it bd mp mq mr dn ms mt mu dp mv lr mw mx my lv mz na nb lz nc nd ne iz bi translated">大意</h2><p id="b18d" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">弓模型的想法非常简单。我们希望找到能够强烈反映积极或消极情绪的特定单词的分数。例如，一篇评论包含更多积极的词，如“棒极了”，“棒极了”会被认为比一篇评论包含“一般”这样的词更积极。由于评论都有不同的长度，我们需要用 0 填充每个评论中没有包含的单词。例如，如果我们有以下 3 个评论，</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="10f6" class="mn mo it nl b gy np nq l nr ns">I love dogs, I think they have adorable personalities.<br/>I don't like cats<br/>My favorite pet is a bird</span></pre><p id="3da8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们会让每一个独特的词占据一个特定的位置。那些没有包含在评论中的单词将被填充为 0，而出现不止一次的单词将被增加到它们出现的次数。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">矢量化的评论</p></figure><p id="55fe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们了解了 BOW 模型如何工作的一般概念，是时候看看我们如何将这个概念应用到我们的情感分析任务中了。我们将从从数据源加载评论开始。</p><h2 id="cf58" class="mn mo it bd mp mq mr dn ms mt mu dp mv lr mw mx my lv mz na nb lz nc nd ne iz bi translated">加载评论</h2><p id="68fa" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">我们将在这个项目中使用的数据是来自约翰·霍普斯金大学<a class="ae lh" href="https://www.cs.jhu.edu/~mdredze/datasets/sentiment/" rel="noopener ugc nofollow" target="_blank">多领域情感数据集</a>的评论。一旦我们下载并解压缩数据，我们将看到一系列具有以下结构的文件夹。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="1f54" class="mn mo it nl b gy np nq l nr ns">/reviews  <br/>   |--/books<br/>      |--positive.review<br/>      |--negative.review<br/>   |--/dvd   <br/>      |--positive.review<br/>      |--negative.review<br/>   |--electronics  <br/>      |--positive.review<br/>      |--negative.review<br/>   |--kitchen_&amp;_house_supplies  <br/>      |--positive.review<br/>      |--negative.review</span></pre><p id="7b70" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们希望将所有正面/负面的评论合并在一起，这样我们就不需要分别引用各个文件夹。我们将创建一个名为<code class="fe nu nv nw nl b">LoadReviews</code>的类来实现这个目标。这个类的完整实现如下所示</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="9717" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于评论是以 XML 格式存储的，我们必须使用<code class="fe nu nv nw nl b">BeautifulSoup</code>将评论解析成文本字符串。实际的文本评论存储在标签<code class="fe nu nv nw nl b">review_text</code>中。在阅读文本评论时，我们还会过滤掉不是 Unicode 字符串的字符。静态方法<code class="fe nu nv nw nl b">strip_non_printable</code>完成这样的任务。所有处理过的评论都将存储在类别属性<code class="fe nu nv nw nl b">reviews</code>中。正如您可能注意到的，我们的 BOW 模型将只对单个单词令牌而不是长字符串进行操作，我们将需要采取进一步的措施来将评论预处理成 BOW transformer 可以使用的正确的矢量化格式。</p><h2 id="f432" class="mn mo it bd mp mq mr dn ms mt mu dp mv lr mw mx my lv mz na nb lz nc nd ne iz bi translated">文本预处理</h2><p id="1761" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">为了开始预处理步骤，<strong class="lk jd">我们将首先把每个评论的长 str 格式转换成评论的标记化版本，这可以通过用空格分割字符串</strong>来容易地完成。比如字符串“我爱 nlp”就会变成列表[“我”、“爱”、“NLP”]。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="c3b2" class="mn mo it nl b gy np nq l nr ns">tokens = s.split()</span></pre><p id="7c9c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用 split 可能会导致常见的边缘情况，即单词有标点符号，但中间没有空格，例如“hello”。为了避免这个单词作为“hello”被区别对待，<strong class="lk jd">我们将去掉所有的标点符号，用空白替换它。</strong></p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="3208" class="mn mo it nl b gy np nq l nr ns">s = s.translate(str.maketrans(string.punctuation, <br/>                              " " * len(string.punctuation)))</span></pre><p id="0f54" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">很多没有强烈感情指示的词，如“我”、“他们”、“一个”，但还是会占空格</strong>。我们将继续从唯一单词列表中删除它们，使它们不再占据位置。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="42d5" class="mn mo it nl b gy np nq l nr ns">tokens = [t for t in tokens <br/>          if t not in stopwords.words("english") + [""]]</span></pre><p id="026c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外<strong class="lk jd">，不同形式的相同词语，如“work”到“worked”，需要被视为相同词语</strong>。这将要求我们对所有单词进行词干分析和词序分析，使其恢复到原始形式，这可以通过使用 NLTK 的词干分析器来完成。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="ee64" class="mn mo it nl b gy np nq l nr ns">tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]</span></pre><p id="3401" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们还将删除长度小于 2 个字符的单词，因为它们可能对情感没有什么意义。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="580c" class="mn mo it nl b gy np nq l nr ns">tokens = [t for t in tokens if len(t) &gt; 2]</span></pre><p id="df6f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所有这些步骤都可以打包成一个名为<code class="fe nu nv nw nl b">WordTokenizer</code>的实用程序类，实现如下。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="6223" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">准备好所有这些组件后，我们现在准备实现 BOW 模型。</p><h2 id="0c99" class="mn mo it bd mp mq mr dn ms mt mu dp mv lr mw mx my lv mz na nb lz nc nd ne iz bi translated">BOW 实现</h2><p id="2d22" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">由于模型将被<code class="fe nu nv nw nl b">StreamlinedModel</code>对象用作转换器，我们需要遵循 sklearn 管道构建定制转换器的指导方针。这意味着对象需要有一个<code class="fe nu nv nw nl b">.transform</code>和一个<code class="fe nu nv nw nl b">.fit</code>方法。<code class="fe nu nv nw nl b">.fit</code>方法将负责保存训练中的特定参数的转换，并在预测过程中应用它们。具体来说，在我们的情况下，数据集中包含的唯一单词列表将在训练期间拟合，但在预测期间不会再次重新拟合。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="ab22" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe nu nv nw nl b">word_index_mapping</code>将是对象<code class="fe nu nv nw nl b">ReviewProcessor</code>的一个属性，本质上是所有唯一单词的枚举。如果预测过程中的单词不是唯一单词列表的一部分，我们还会将向量的最后一个位置设置为“未知单词”。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="0d6c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe nu nv nw nl b">.transform</code>方法用于将输入的原始文本转换成词频向量。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="cc34" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe nu nv nw nl b">get_word_frequency_vector</code>方法将接受检查，并根据预先计算的<code class="fe nu nv nw nl b">word_to_index</code>映射增加向量中相应索引处的值。如果输入的审查文本包含未知单词，则<code class="fe nu nv nw nl b">word_frequency_vector</code>的最后一个索引将递增。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="fc99" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">弓形变压器的完整实现如下所示</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="f42a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们已经将 BOW 实现为一个变压器，我们可以使用<code class="fe nu nv nw nl b">StreamlinedModel</code>构建 4 个不同版本的模型并比较性能。</p><h2 id="56d8" class="mn mo it bd mp mq mr dn ms mt mu dp mv lr mw mx my lv mz na nb lz nc nd ne iz bi translated">绩效评估</h2><p id="5ba2" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">正如<code class="fe nu nv nw nl b">StreamlinedModel</code>能够使用任何转换器一样，我们也可以通过交换<code class="fe nu nv nw nl b">model</code>参数来使用不同的机器学习模型。例如，之前我们通过调用<code class="fe nu nv nw nl b">LogisticRegression</code>对象使用了逻辑回归模型，我们可以将它更改为 lightGBM 模型，如下所示</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="c91c" class="mn mo it nl b gy np nq l nr ns"><strong class="nl jd">from</strong> models.feature <strong class="nl jd">import</strong> WordFrequencyVectorizer<br/><strong class="nl jd">import</strong> lightgbm <strong class="nl jd">as</strong> lgb<br/><strong class="nl jd">from</strong> sentiment_analysis.models.model <strong class="nl jd">import</strong> StreamlinedModel</span><span id="10c8" class="mn mo it nl b gy nt nq l nr ns">lgbm = StreamlinedModel(<br/>    transformer_description="Bag of words",<br/>    transformer=WordFrequencyVectorizer,<br/>    model_description="logisitc regression model",<br/>    model=lgb.LGBMClassifier,<br/>)</span></pre><p id="e1ec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将尝试 4 种不同的模型，并收集它们的预测 AUC 分数，以制作柱状图。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/b4cf7fa568a3d104c534452578104d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bDrHEZ1h3P0ulC4KdTafSQ.png"/></div></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="5fc4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用逻辑回归模型已经可以得到 0.82 的 AUC 分数。</p><p id="4f84" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">LightGBM 模型还帮助我们将性能提高到 0.888，显示了树模型比线性模型更好的预测能力。</p><p id="583a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最有趣的结果来自两种不同的朴素贝叶斯模型，它们产生了截然不同的结果，<strong class="lk jd">高斯朴素贝叶斯比多项式朴素贝叶斯表现得更差</strong>。由于高斯朴素贝叶斯模型被设计为对连续变量进行操作，而词频只是离散的，因此低于标准的性能是意料之中的。</p><p id="5496" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可能已经注意到，在这个情感分析任务中，相对简单的多项式朴素贝叶斯模型取得了与相当高级的 lightGBM 模型相当的性能。那么我们的模型是如何做出决定的呢？让我们仔细看看。</p><h2 id="eb97" class="mn mo it bd mp mq mr dn ms mt mu dp mv lr mw mx my lv mz na nb lz nc nd ne iz bi translated">错误分类的正面/负面评论</h2><p id="875b" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">由于有多个评论被错误分类，我们将只显示与正确标签距离最大的评论。为此，我们将对预测概率进行排序，打印出第一个/最后一个(取决于标签)</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="df78" class="mn mo it nl b gy np nq l nr ns"># get the indices of the misclassified reviews<br/>wrong_positive_inds = np.where((y_test == 1) <br/>                             &amp; (y_pred != y_test))[0]<br/>wrong_negative_inds = np.where((y_test == 0) <br/>                             &amp; (y_pred != y_test))[0]</span><span id="2b88" class="mn mo it nl b gy nt nq l nr ns"># most wrong positive review<br/>most_wrong_positive_index = wrong_positive_inds[<br/>    y_prob_lgbm[:, 1][wrong_positive_inds].argmax()<br/>]</span><span id="0f50" class="mn mo it nl b gy nt nq l nr ns"># most wrong negative review<br/>most_wrong_negative_index = wrong_negative_inds[<br/>    y_prob_lgbm[:, 1][wrong_negative_inds].argmin()<br/>]</span></pre><p id="2eb7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最错误的正面评价</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/4d2198c37f47a9f506a9b8eaacdd24bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hh1lAmKWpw8RbuIiiV-dmQ.png"/></div></div></figure><p id="8d09" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是一篇相当长且详细的正面评论，提到了这本书的优点和缺点。在弱点部分，一些负面词汇如“<strong class="lk jd"> <em class="oc">问题</em> </strong>”、“<strong class="lk jd"> <em class="oc">而不是</em> </strong>”被多次提及，这反过来增加了对该复习进行分类的难度。</p><p id="11a3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们来看看最错误的负面评论。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/8b50d1a5675a1ad7acca60d4db0d15f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*npJ0meXEcsEoLq46r-4pgw.png"/></div></div></figure><p id="da98" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个负面评论还包含一个正面词“<strong class="lk jd"> <em class="oc">有趣的</em> </strong>”，没有明显的负面词(可能“<strong class="lk jd"> <em class="oc">过时的</em></strong><em class="oc">”</em>是一个负面词)，使得将这个评论归类为负面的可能性更小。</p><p id="0dbc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么模型认为一篇评论到底哪些词是正面/负面的呢？接下来我们来看看。</p><h2 id="10fe" class="mn mo it bd mp mq mr dn ms mt mu dp mv lr mw mx my lv mz na nb lz nc nd ne iz bi translated">特征重要性</h2><p id="533a" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">在这一点上，我们经常会提出如何在两个性能最好的模型(LightGBM 和多项式朴素贝叶斯)之间进行选择的问题。答案在于互操作性。由于朴素贝叶斯依赖于计数和概率，我们可以很容易地找出哪些词对情感有贡献。然而，LightGBM 模型并不那么简单。由于大量独特的词，树分裂是相当具有挑战性的形象化。我们真的没有运气解释它们吗？</p><p id="3009" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">不完全是。在模型解释包 SHAP 的帮助下，我们可以在一个单独的评论中产生一个特性重要性的可视化(更多关于 SHAP 的信息在另一篇文章中)。在下面的 SHAP 可视化图表中，红色表示预测的情绪接近 1，而蓝色表示预测的情绪为 0。下面是模型预测为正的示例审查</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/266c28f981e7f436b02b577838da3878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FEEkzSXggc6K2uQ3GAGhQg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">一个非常正面的评论的例子</p></figure><p id="f5f6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于该评论包含强烈的积极词汇，如“<strong class="lk jd"> <em class="oc">最佳</em> </strong>”和“<strong class="lk jd"> <em class="oc">享受</em> </strong>”，因此该评论被预测为积极的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/a3c3f33f2539f252afea212092c75bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XtHQHJkp33A-Y6G86W3D8w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">非常负面的评论的例子</p></figure><p id="9ae2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">负面评论通常由负面词汇组成，如“<strong class="lk jd"><em class="oc"/></strong>”、“<strong class="lk jd"> <em class="oc">失败</em> </strong>”或“<strong class="lk jd"> <em class="oc">不好</em> </strong>”。</p><h2 id="be3d" class="mn mo it bd mp mq mr dn ms mt mu dp mv lr mw mx my lv mz na nb lz nc nd ne iz bi translated">结论</h2><p id="8ddd" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">对于情感分析任务来说，单词包可能是最简单的文本矢量化类型，但它已经非常有效了。GitHub 项目可以在这里找到<a class="ae lh" href="https://github.com/chen-bowen/Streamlined_Sentiment_Analysis" rel="noopener ugc nofollow" target="_blank">。下面是我们今天学到的内容。</a></p><ol class=""><li id="e7b9" class="og oh it lk b ll lm lo lp lr oi lv oj lz ok md ol om on oo bi translated">面向对象编程极大地提高了整个工作流程的灵活性。在 sklearn pipelines 的帮助下，我们可以用同一个 BOW transformer 快速迭代和测试多个机器学习模型。</li><li id="2f2b" class="og oh it lk b ll op lo oq lr or lv os lz ot md ol om on oo bi translated">我们必须为底层任务使用正确的模型。高斯朴素贝叶斯模型是唯一不能获得超过 80% AUC 分数的模型。这只是一个错误的模型，因为它被设计为对连续特征进行操作。</li><li id="ae48" class="og oh it lk b ll op lo oq lr or lv os lz ot md ol om on oo bi translated">LightGBM 赢得了比赛(以微弱优势)。基于树的模型能够产生相当大的拟合度和最佳性能，尽管多项朴素贝叶斯的性能仅稍差。</li><li id="3b24" class="og oh it lk b ll op lo oq lr or lv os lz ot md ol om on oo bi translated">LightGBM 需要依靠一些特殊的解释包才能被理解，现在我们正在得到它。随着模型解释在我们的行业中变得越来越重要，我们经常被迫选择更简单的模型来实现互操作性。SHAP 的存在改变了这一切。</li></ol><p id="e5b5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在下一篇文章中，我们将在单词袋模型的基础上，用一个智能加权方案来改进它。下次见！</p></div></div>    
</body>
</html>