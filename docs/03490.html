<html>
<head>
<title>Overfeat Review[1312.6229]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">过量饮食评论[1312.6229]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overfeat-review-1312-6229-4fd925f3739f?source=collection_archive---------29-----------------------#2020-04-02">https://towardsdatascience.com/overfeat-review-1312-6229-4fd925f3739f?source=collection_archive---------29-----------------------#2020-04-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e0bb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">过量进食的理论综述</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/99e863ffb019e2d13c6252b12be3d491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GFL-KOxw7O-BT4TsEa0crg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">演职员表:<a class="ae ky" href="https://en.wikipedia.org/wiki/Object_detection" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Object_detection</a></p></figure><p id="89f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经计划阅读主要的物体探测论文(虽然我已经粗略地阅读了它们中的大部分，但我会详细地阅读它们，好到足以写一篇关于它们的博客)。这些论文与基于深度学习的对象检测相关。随时给建议或询问疑惑会尽我所能帮助大家。我将在下面写下每篇论文的 arxiv 代码，并在下面给出博客(我写的时候会不断更新)和他们论文的链接。任何从这个领域开始的人都可以跳过许多这样的论文。当我读完所有的论文后，我还会写下它们的优先级/重要性(根据理解主题的必要性)。<br/>我写这篇博客是考虑到和我相似并且仍在学习的读者。万一我犯了任何错误(我将通过从各种来源(包括博客、代码和视频)深入理解论文来尽量减少错误)，任何人都可以随意地在博客上强调它或添加评论。我已经提到了我将在博客末尾涉及的论文列表。</p><p id="96f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始吧:)</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="9778" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Overfeat 论文探讨了三个计算机视觉任务分类，定位，检测分别按照难度递增的顺序。每个任务都是下一个任务的子任务。所有任务都使用一个框架和一个共享的功能学习库来解决。</p><h2 id="3083" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">分类</h2><p id="f09b" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">用于分类的架构类似于 alexnet，但有一些改进。作者准备了两种不同的架构:快速和准确。与 alexnet 架构的不同之处包括没有对比度归一化、汇集区域不重叠、由于步幅较小而具有较大的第一层和第二层特征地图。我将添加显示快速和精确架构的表格。训练和推理步骤的执行是不同的。我在这里解释训练步骤，后面解释推理步骤<strong class="lb iu">。</strong>分类器的训练是在 221*221 大小的单一尺度上完成的。通过首先将图像大小调整为 256，然后裁剪为 221，提取了 5 个随机裁剪及其水平翻转。我不会详细讨论训练的细节，比如学习速度，体重下降等等。该架构可以在图 1 中看到。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/00a0e6e53222f2be7b1000ea30c9a952.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kxCEJO3h4MQuUcOn_8H8tg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。过度饮食建筑</p></figure><h2 id="743b" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">ConvNets 和滑动窗口效率(在 convnets 内部应用滑动窗口)</h2><p id="0c62" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们知道滑动窗口方法可以改善分类和定位的结果，但是增加了许多倍的计算。ConvNets 在应用滑动窗口技术方面具有内在的效率，因为它们共享重叠区域的共同计算(大部分将重叠)。我将在 CNN 的内部解释这个滑动窗口是如何工作的。拿这个图做参考。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/ae66da6e9e5b8dd18c88ce36f05b234a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JKkWAVGJ8EJ6tz6pWMsMkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。</p></figure><p id="0f7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你意识到感受野(即使你在这篇博客之后没有阅读它，这是一个重要的概念)。上图分为两个子部分，先考虑第一部分。应用第一次卷积后，输出变为 10*10(如果计算卷积运算的输出尺寸，请阅读<a class="ae ky" href="https://www.quora.com/How-can-I-calculate-the-size-of-output-of-convolutional-layer" rel="noopener ugc nofollow" target="_blank">此</a>)。)之后是最大池层，类似地在后面的层中给出 1*1 的最终输出。这里我们可以直观地说，最终的 1*1 编码了 14*14 输入的信息(基本上这是一个感受域)。没有跳转到第二部分，过滤器大小没有变化，唯一的变化是输入大小(16*16)。当第一个卷积被应用，随后是最大池时，输出大小变成 6*6，在先前的情况下是 5*5，现在当具有 5*5 滤波器大小的卷积将被应用时，输出大小将是 2*2 而不是 1*1。如果您看到 2*2 输出中的蓝点，它会对输入中蓝色部分的所有信息进行编码，而不会看到黄色区域。类似地，2*2 输出的第二个点((输出矩阵上的(0，1)位置)将永远看不到输入的前两列和底部两行(我们的模型的感受野是 14*14，因此输入大小的增加线性地增加了输出大小)。</p><h2 id="88a8" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">多尺度分类</h2><p id="e9a5" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在推理时，为了提高分类精度，他们使用了多视图投票。他们使用培训中讨论的类似策略生成 10 幅不同的图像，并对预测进行平均。但这忽略了图像中的许多区域(因为与滑动窗口方法不同，我们只对每幅图像使用 5 次裁剪)。因此，他们采用了一种更好的方法，解释如下:</p><p id="152e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们使用了 6 种不同的输入比例，从而生成了不同大小的第 5 层要素地图(这些大小请参考下表)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/34416291b931b619c68e3867498c0b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sk0Zfm5Mq0Pbgf3lw8UA7A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图三。多尺度方法的空间维度</p></figure><p id="ad7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里我们可以看到不同尺度下模型输入大小的变化是(36，72)。这里的 36 值(二次采样率)类似于我们在上一节中观察到的 2 像素移动。第 5 层(上表中的预池)的输出将因规模而异。应用 3*3 最大池。考虑输出形状为 17*17 的第一个比例。这里，如果在没有填充的情况下应用 max-pooling(3*3 ),则生成的输出大小将为 5*5。在这里，我们可以观察到最大池的输出没有来自预池化特征地图的最后两行和列的任何输入。由于 CNN 具有本地连接性，在原始图像的情况下，这两个列和行变成大约 30。为了解决这个问题，考虑到起始像素位置为(x{0，1，2}，y{0，1，2}，总共应用了 9 次最大池。因此，现在输出将是 3*3 的形状(前面提到的 9 倍是 2d 的行和列)。作者在图 4 中解释了这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/ad04e658c7e93cfd549a02ae55ca6f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hk2GA1q_Kha0k9X4qrksMQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4。</p></figure><p id="2cc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上一段继续。因此，我们得到的总输出是(5*5)*(3*3)的形状。应用 5×5 卷积来获得最终的分类器图。它的形状是(1*1)*(3*3)*C，这里 C 是因为这是我们得到预测的最后一步。对于其他比例，最大池化后生成的输出地图会有所不同。对于其他规模，它将是类似的，我想你现在可以计算我们如何得到输出尺寸。现在，通过首先在每个尺度上取空间最大值(3*3*C 或 6*9*C 中的最大值)来计算最终输出。然后对所有尺度的输出进行平均，最终得到每个图像的一个类输出。</p><p id="08ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们完成了分类，我们的主要任务是对象检测，我们甚至还没有开始。</p><p id="9c29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">别担心，现在没那么久了。我们几乎涵盖了所有的理论。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="4eb7" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">本地化</h2><p id="e448" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">分类训练网络由回归网络代替分类图层，并对其进行训练以预测每个空间位置和比例的边界框。回归网络从第 5 层获取合并的要素地图，随后是大小为 4096 和 1024 的两个 FC 层。最终输出有 4 个单元(见图 5。).特征提取层(前 5 层)的权重是固定的，并且使用 l2 损失来训练模型。训练是在多个尺度上进行的(不同于在单个尺度上训练的分类网络，在多个尺度上仅生成预测)。这将使预测跨尺度正确匹配，并增加合并预测的可信度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/9d804489b6220ca7d1270c8a4b66d58f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OFm_2reQbwnV8x30wFVTIA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5</p></figure><p id="9f39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了生成对象边界框预测，我们跨所有位置和规模同时运行分类器和回归器网络。由于这些图层共享相同的要素提取图层，因此在计算分类网络后，只需重新计算最终回归图层。</p><p id="49db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于模型将预测多个框(在本地化的情况下将有一个框)，我们需要一些策略来消除所有的坏预测。应用了一个贪婪的合并策略，如图 6 所示(你可以跳过它，因为我没有看到任何其他论文使用这种策略，NMS(将在未来的博客中讨论)使用得更频繁)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/dedf4b451c52acdc0780c5f24d71ebc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fVc3Y1eKlceOeRYnPBaEHw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6</p></figure><p id="a594" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，match_score 被计算为 b1 和 b2 的中心之间的距离之和。box_merge 计算边界框坐标的平均值。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="cba5" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">侦查</h2><p id="5034" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在检测的情况下，与定位任务的主要区别在于当不存在物体时，需要预测背景类别。因此，我们现在从定位网络获得的先前属于某个类别的盒子现在属于背景，因此我们可以移除这些预测，并且仅获得在哪个类别中被有把握地预测的预测。</p><p id="e804" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">过量论文总结到此结束。如果我做错了请重点指出，提出疑问作为评论。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="3a6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献:</strong></p><ol class=""><li id="dd28" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated">https://arxiv.org/pdf/1312.6229<a class="ae ky" href="https://arxiv.org/pdf/1312.6229" rel="noopener ugc nofollow" target="_blank"/></li><li id="350c" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">https://medium . com/coin monks/review-of-over feat-winner-of-ils vrc-2013-localization-task-object-detection-a6f8b 9044754</a></li><li id="1acb" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=JKTzkcaWfuk" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=JKTzkcaWfuk</a></li><li id="b014" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=3U-bZgKFS7g&amp;t=70s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=3U-bZgKFS7g&amp;t = 70s</a></li><li id="ad0f" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> Alexnet </a></li><li id="3667" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">感受野:<a class="ae ky" href="https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807" rel="noopener">https://medium . com/ml review/a-guide-to-receptive-field-algorithm-for-卷积神经网络-e0f514068807 </a></li></ol><h2 id="68fa" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">论文列表:</h2><ol class=""><li id="0b87" class="ng nh it lb b lc mv lf mw li nu lm nv lq nw lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1312.6229.pdf" rel="noopener ugc nofollow" target="_blank"> OverFeat:使用卷积网络的综合识别、定位和检测</a>。←你完成了这篇博客。</li><li id="03b3" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">丰富的特征层次，用于精确的对象检测和语义分割(RCNN)。</a> [ <a class="ae ky" href="https://medium.com/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a" rel="noopener">链接到博客</a> ]</li><li id="1f82" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络中的空间金字塔池。</a> [ <a class="ae ky" href="https://medium.com/@sanchittanwar75/review-spatial-pyramid-pooling-1406-4729-bfc142988dd2" rel="noopener">链接到博客</a></li><li id="00bb" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">快速 R-CNN </a>【链接到博客】</li><li id="6901" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">更快的 R-CNN:用区域提议网络实现实时目标检测。【博客链接】</li><li id="c2de" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的，实时的物体检测。</a>【博客链接】</li><li id="5c77" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank"> SSD:单次多盒探测器</a>。[博客链接]</li><li id="8abc" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">R-FCN:通过基于区域的完全卷积网络的目标检测。【博客链接】</li><li id="c0ac" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1612.03144.pdf" rel="noopener ugc nofollow" target="_blank">用于目标检测的特征金字塔网络。</a>【博客链接】</li><li id="bc79" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1701.06659.pdf" rel="noopener ugc nofollow" target="_blank"> DSSD:解卷积单粒子探测器</a>。[博客链接]</li><li id="6c26" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank">密集物体检测的焦点丢失(视网膜网)。</a>【博客链接】</li><li id="a4d5" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">YOLOv3:一种渐进的改进。[博客链接]</li><li id="9cdd" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1805.09300v3.pdf" rel="noopener ugc nofollow" target="_blank">狙击手:高效多尺度训练</a>。[博客链接]</li><li id="3d61" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1904.04514.pdf" rel="noopener ugc nofollow" target="_blank">标注像素和区域的高分辨率表示。</a>【博客链接】</li><li id="a9f8" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1904.01355v5.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS:全卷积一级目标检测</a>。[博客链接]</li><li id="306a" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1904.07850.pdf" rel="noopener ugc nofollow" target="_blank">物为点</a>。[博客链接]</li><li id="aba3" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">CornerNet-Lite:高效的基于关键点的对象检测。【博客链接】</li><li id="5af3" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1904.08189v3.pdf" rel="noopener ugc nofollow" target="_blank"> CenterNet:用于对象检测的关键点三元组</a>。[博客链接]</li><li id="fb73" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">用于实时目标检测的训练时间友好网络。【博客链接】</li><li id="3817" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1909.03625v1.pdf" rel="noopener ugc nofollow" target="_blank"> CBNet:一种用于目标检测的新型复合主干网络体系结构。</a>【博客链接】</li><li id="9d1b" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1911.09070v2.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet:可扩展且高效的对象检测</a>。[博客链接]</li></ol><p id="c670" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和平…</p></div></div>    
</body>
</html>