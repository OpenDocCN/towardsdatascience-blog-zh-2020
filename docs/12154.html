<html>
<head>
<title>Fast and Restricted Style Transfer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">快速和受限的风格转换</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fast-and-restricted-style-transfer-bbfc383cccd6?source=collection_archive---------45-----------------------#2020-08-21">https://towardsdatascience.com/fast-and-restricted-style-transfer-bbfc383cccd6?source=collection_archive---------45-----------------------#2020-08-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9aea" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">神经类型转移，进化</h2><div class=""/><div class=""><h2 id="160c" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用前馈网络的实时图像风格传递</h2></div><p id="f917" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在他们的开创性工作中，“使用卷积神经网络 的<strong class="kq ja"> <em class="lk">图像风格转移，”Gatys <em class="lk">等人</em>【R1】<em class="lk"/>展示了 CNN 在分离和重新组合图像内容和风格以创建复合艺术图像方面的功效。使用从预先训练的 CNN 的中间层提取的特征，他们定义单独的<strong class="kq ja"> <em class="lk">内容</em> </strong>和<strong class="kq ja"> <em class="lk">风格</em> </strong>损失函数，并将风格转换任务作为一个<strong class="kq ja"> <em class="lk">优化</em> </strong>问题。我们从随机图像开始，并更新像素值，从而最小化各个损失函数。更多详情请参考<a class="ae ll" rel="noopener" target="_blank" href="/slow-and-arbitrary-style-transfer-3860870c8f0e"> <em class="lk">这篇</em> </a> <em class="lk"> </em>的文章。</em></strong></p><p id="6973" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这种方法的一个明显的警告是，它是<strong class="kq ja"> <em class="lk">慢</em> </strong>。调整随机图像需要几次<strong class="kq ja"> <em class="lk">优化</em> </strong>迭代，以使其适应两个不同参考图像的<strong class="kq ja"> <em class="lk">内容</em> </strong>和<strong class="kq ja"> <em class="lk">样式</em> </strong>。为了解决这种低效率，约翰逊<em class="lk">等人</em>【R2】提出了一种<strong class="kq ja"> <em class="lk">前馈</em> </strong>神经网络来<strong class="kq ja"> <em class="lk">近似</em> </strong>求解风格转换优化问题【R1】。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/dc8b919c450a9f5244a635567bb7176b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hjZISafPZ7ryTXOWLmKIHA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图一。概述“实时风格转换和超分辨率的感知损失”中提出的方法。图像取自同一张纸。</p></figure><p id="66b7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">本质上，他们提出了一种<strong class="kq ja"> <em class="lk">图像变换网络</em> </strong> ( <em class="lk"> fw </em>)，这是一种将输入图像<strong class="kq ja"> <em class="lk"> x </em> </strong>变换为输出图像<strong class="kq ja"> <em class="lk"> ŷ </em> </strong> <em class="lk">的深度残差卷积神经网络。</em>样式转移的目标是生成一个图像<strong class="kq ja"> <em class="lk"> ŷ </em> </strong>该图像将目标内容图像<strong class="kq ja"> <em class="lk"> y_c </em> </strong>的内容与目标样式图像<strong class="kq ja"> <em class="lk"> y_s </em> </strong>的样式相结合。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/5e6e01a2c7c8b7b9696150153f33d852.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*_GOIaTY3XEJpjZ3baOhNXQ.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图二。风格传递损失。更多详情请参考前面提到的<a class="ae ll" rel="noopener" target="_blank" href="/slow-and-arbitrary-style-transfer-3860870c8f0e">博客</a>。</p></figure><p id="ff6e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">训练时，对于一个<strong class="kq ja"> <em class="lk">固定</em> </strong>样式的目标<strong class="kq ja"> <em class="lk"> y_s </em> </strong>，图像变换网络以一个<strong class="kq ja"> <em class="lk">随机</em> </strong>目标内容图像<strong class="kq ja"> <em class="lk"> y_c </em> </strong> ( <em class="lk">或</em> <strong class="kq ja"> <em class="lk"> x </em> </strong>)作为输入，生成图像<strong class="kq ja"> <em class="lk"> ŷ.</em> </strong>然后调整变换网络，使风格传递损失最小化(参见图 2)。在几次迭代的过程中，网络看到多个内容图像，但是只有一个固定样式的图像。结果，图像变换网络学会将来自任何 参考图像的<strong class="kq ja"> <em class="lk">的内容与特定<strong class="kq ja"><em class="lk"/></strong>参考图像的样式相结合。</em></strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi md"><img src="../Images/b450d70a57265e2bcdb2e3bbb81fa94a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HuKvsAdRffc2kMVR98SSPg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图三。每个网络一种样式的演示。<a class="ae ll" href="https://ml4a.github.io/ml4a/style_transfer/" rel="noopener ugc nofollow" target="_blank">图片由</a>提供</p></figure><p id="a9ba" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">总之，虽然这种方法以比每图像 优化<strong class="kq ja"> <em class="lk">快三个数量级的速度<strong class="kq ja"><em class="lk"/></strong>执行样式转移，但是它们受限于<strong class="kq ja"><em class="lk"/></strong>到<strong class="kq ja"> <em class="lk">每网络一种样式</em> </strong>。</em></strong></p><h2 id="a3cb" class="me mf iq bd mg mh mi dn mj mk ml dp mm kx mn mo mp lb mq mr ms lf mt mu mv iw bi translated">参考</h2><ol class=""><li id="1ecb" class="mw mx iq kq b kr my ku mz kx na lb nb lf nc lj nd ne nf ng bi translated">利昂·A·加蒂斯、亚历山大·S·埃克和马蒂亚斯·贝奇。<a class="ae ll" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">使用卷积神经网络的图像风格转换</a>。在<em class="lk"> CVPR </em>，2016。</li><li id="852b" class="mw mx iq kq b kr nh ku ni kx nj lb nk lf nl lj nd ne nf ng bi translated">贾斯廷·约翰逊，亚历山大·阿拉希和李菲菲。<a class="ae ll" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">实时风格转换和超分辨率的感知损失</a>。在<em class="lk"> ECCV </em>，2016。</li></ol></div></div>    
</body>
</html>