<html>
<head>
<title>Processing large data files with Python multithreading</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python多线程处理大型数据文件</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/processing-large-data-files-with-python-multithreading-dbb916f6b58d?source=collection_archive---------8-----------------------#2020-05-24">https://towardsdatascience.com/processing-large-data-files-with-python-multithreading-dbb916f6b58d?source=collection_archive---------8-----------------------#2020-05-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2d96" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在左侧车道加速行驶。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d243aa1623ea16a77e1591f6694ab2dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lOXDHhxbqfy0unDn9x56HQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">2020年zapalote.com<a class="ae kv" href="http://zapalote.com" rel="noopener ugc nofollow" target="_blank"/></p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="kw kx l"/></div></figure><p id="42ee" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们花费大量时间等待一些数据准备任务完成——你可能会说，这是数据科学家的命运。我们可以加快速度。这里有两种技术会派上用场:<strong class="la ir">内存映射</strong>文件和<strong class="la ir">多线程</strong>。</p><h1 id="00e3" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak">数据</strong></h1><p id="6a0b" class="pw-post-body-paragraph ky kz iq la b lb mm jr ld le mn ju lg lh mo lj lk ll mp ln lo lp mq lr ls lt ij bi translated">最近，我不得不从Google Books Ngram corpus<a class="ae kv" href="http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html" rel="noopener ugc nofollow" target="_blank">中提取术语和术语频率</a>，我发现自己在想是否有办法加快这项任务。语料库由26个文件组成，总数据量为24GB。我感兴趣的每个文件都包含一个术语和其他元数据，用制表符分隔。将这些文件作为熊猫数据帧读取的暴力方法非常慢。因为我们只需要唯一的术语和它们的匹配计数，所以我想我会尽量让它更快:-)</p><h1 id="a235" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak">内存映射文件</strong></h1><p id="11d5" class="pw-post-body-paragraph ky kz iq la b lb mm jr ld le mn ju lg lh mo lj lk ll mp ln lo lp mq lr ls lt ij bi translated">这种技术并不新鲜。由来已久，起源于Unix(Linux之前！).简而言之，<code class="fe mr ms mt mu b">mmap</code>通过将文件内容加载到内存页面中来绕过通常的I/O缓冲。这对于内存占用量大的计算机非常适用。对于今天的台式机和笔记本电脑来说，这基本上没问题，因为32GB的内存不再是一个深奥的问题。Python库模仿了大多数Unix功能，并提供了一个方便的<code class="fe mr ms mt mu b">readline()</code>函数来一次提取一行字节。</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="1c0f" class="mz lv iq mu b gy na nb l nc nd"># map the entire file into memory<br/>mm = mmap.mmap(fp.fileno(), 0)</span><span id="85a7" class="mz lv iq mu b gy ne nb l nc nd"># iterate over the block, until next newline<br/>for line in iter(mm.readline, b""):<br/>    # convert the bytes to a utf-8 string and split the fields<br/>    term = line.decode("utf-8").split("\t")</span></pre><p id="5367" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><code class="fe mr ms mt mu b">fp </code>是一个文件指针，之前用<code class="fe mr ms mt mu b">r+b</code>访问属性打开过。这就对了，通过这个简单的调整，你已经使文件读取速度提高了一倍(好吧，确切的改进将取决于许多因素，如磁盘硬件等)。</p><h1 id="12d1" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">多线程操作</h1><p id="220c" class="pw-post-body-paragraph ky kz iq la b lb mm jr ld le mn ju lg lh mo lj lk ll mp ln lo lp mq lr ls lt ij bi translated">下一个总是有助于提高速度的技术是增加并行性。在我们的例子中，任务是I/O绑定的。这非常适合于<em class="nf">扩展— </em>即<em class="nf"> </em>添加线程。你会发现关于什么时候在搜索引擎上横向扩展(多处理)更好的讨论。</p><p id="f685" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">Python3有一个很棒的标准库，用于管理线程池并动态地给它们分配任务。所有这一切都通过一个极其简单的API实现。</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="b936" class="mz lv iq mu b gy na nb l nc nd"># use as many threads as possible, default: os.cpu_count()+4<br/>with ThreadPoolExecutor() as threads:<br/>   t_res = threads.map(process_file, files)</span></pre><p id="aeff" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><code class="fe mr ms mt mu b">ThreadPoolExecutor</code>的<code class="fe mr ms mt mu b">max_workers</code>默认值是每个CPU内核5个线程(从Python v3.8开始)。<code class="fe mr ms mt mu b">map()</code> API将接收一个应用于列表中每个成员的函数，并在线程可用时自动运行该函数。哇哦。就这么简单。在不到50分钟的时间里，我已经将24GB的输入转换成了一个方便的75MB的数据集，可以用pandas来分析——瞧。</p><p id="7ad5" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">完整的代码在<a class="ae kv" href="https://gist.github.com/zapalote/30aa2d7b432a08e6a7d95e536e672494" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上。随时欢迎评论和意见。</p><p id="91ae" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">PS:我给每个线程加了一个带<code class="fe mr ms mt mu b">tqdm</code>的进度条。我真的不知道他们是如何设法避免屏幕上的线条混乱的…它非常有效。</p><p id="d6b8" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">更新:两年后，<a class="ae kv" href="https://hackernoon.com/crunching-large-datasets-made-fast-and-easy-the-polars-library" rel="noopener ugc nofollow" target="_blank">这个</a>上来了:-)</p></div></div>    
</body>
</html>