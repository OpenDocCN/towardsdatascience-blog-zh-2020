<html>
<head>
<title>Building Data Processing Pipeline With Apache Beam, Dataflow &amp; BigQuery</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Apache Beam、Dataflow 和 BigQuery 构建数据处理管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/apache-beam-pipeline-for-cleaning-batch-data-using-cloud-dataflow-and-bigquery-f9272cd89eba?source=collection_archive---------5-----------------------#2020-07-12">https://towardsdatascience.com/apache-beam-pipeline-for-cleaning-batch-data-using-cloud-dataflow-and-bigquery-f9272cd89eba?source=collection_archive---------5-----------------------#2020-07-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="301d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">beam 管道的实现，它清理数据并将数据写入 BigQuery 进行分析。</h2></div><p id="8bd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">市场上有各种与大数据相关的技术，如 Hadoop、Apache Spark、Apache Flink 等，维护这些技术对开发人员和企业来说都是一个巨大的挑战。哪个工具最适合批量和流式数据？在我们的用例中，某个特定工具的性能和速度是否足够？您应该如何集成不同的数据源？如果这些问题经常出现在您的业务中，您可能需要考虑 Apache Beam。</p><p id="3979" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://beam.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> Apache Beam </strong> </a>是一个开源的统一模型，用于构建批处理和流数据处理管道。Beam 支持针对 Beam 模型编写管道的多种语言特定的 SDK，如<strong class="kk iu"> Java </strong>、<strong class="kk iu"> Python </strong>、<strong class="kk iu"> Go </strong>以及在分布式处理后端执行它们的 Runners，包括<strong class="kk iu"> Apache Flink </strong>、<strong class="kk iu"> Apache Spark </strong>、<a class="ae le" href="https://cloud.google.com/dataflow" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">Google Cloud data flow</strong></a>和<strong class="kk iu"> Hazelcast Jet </strong>。</p><p id="8ffd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用谷歌云平台产品来运行这一管道，所以你需要利用你的免费提供来使用这些产品，直到他们指定的免费使用限制，新用户也将获得 300 美元，在你的<a class="ae le" href="https://cloud.google.com/free" rel="noopener ugc nofollow" target="_blank">免费试用</a>期间花在谷歌云平台产品上。</p><p id="40ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里我们要用<a class="ae le" href="https://beam.apache.org/get-started/quickstart-py/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> Python SDK </strong> </a>和<strong class="kk iu">云数据流</strong>来运行管道。</p><h2 id="a81b" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><strong class="ak">剖析数据管道</strong></h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/93adf0d167c702ee1852355703d4051f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KSbxc89zsZ-AWLJSsAtPvQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae le" href="https://link.springer.com/chapter/10.1007/978-1-4842-4470-8_40" rel="noopener ugc nofollow" target="_blank">管道的关键概念</a></p></figure><ul class=""><li id="fb68" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated"><strong class="kk iu">管道:</strong>管理准备执行的 PTransforms 和 PCollections 的有向无环图(DAG)。</li><li id="82c2" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu"> PCollection: </strong>表示有界或无界数据的集合。</li><li id="fcb8" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu">p 转换:</strong>将输入 p 集合转换成输出 p 集合。</li><li id="82cb" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu"> PipelineRunner: </strong>表示管道应该在哪里以及如何执行。</li><li id="00d4" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu"> I/O transform: </strong> Beam 附带了许多“io”，即从各种外部存储系统读取数据或向其写入数据的库 p transform。</li></ul><p id="7ad9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在下面剪辑了一些常用的高级转换(Ptransforms ),我们将在我们的管道中使用其中一些。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nc"><img src="../Images/0f94c109e7cc2a4b9e20264504815e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WII3iZ-V8FLUn_pINE5UWA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae le" href="http://shzhangji.com/blog/2017/09/12/apache-beam-quick-start-with-python/" rel="noopener ugc nofollow" target="_blank">管道中的常见转换</a></p></figure><p id="ad75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> ParDo </strong>是普通并行处理的主波束变换，不在上图中。ParDo 处理范例类似于 Map/Shuffle/Reduce 风格算法的“Map”阶段:ParDo 转换考虑输入 PCollection 中的每个元素，对该元素执行一些处理，并向输出 PCollection 发出零个或多个元素。</p><p id="e0ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> Pipe <em class="nd"> '|' </em> </strong>是应用转换的操作符，每个转换可以有选择地提供一个唯一的标签。转换可以被链接，我们可以组成任意形状的转换，在运行时，它们将被表示为 DAG。</p><p id="d3b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述概念是创建 apache beam 管道的核心，所以让我们进一步创建第一个批处理管道，它将清理数据集并将其写入 BigQuery。</p><h2 id="5c58" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">管道的基本流程</h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ne"><img src="../Images/c8cf214df47d3b84ef28653943d18656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ayAEAIrpsqG88IvpSdyvRA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae le" href="https://miro.medium.com/max/1250/1*2RSf9nVv6-MaLhVjW2F2tA.png" rel="noopener">管道流量</a></p></figure><ol class=""><li id="dfe9" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld nf mu mv mw bi translated">从 google 云存储桶(Batch)中读取数据。</li><li id="9f02" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld nf mu mv mw bi translated">应用一些转换，例如通过逗号分隔符分割数据，删除不需要的列，转换数据类型等。</li><li id="46df" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld nf mu mv mw bi translated">将数据写入数据宿(<a class="ae le" href="https://cloud.google.com/bigquery" rel="noopener ugc nofollow" target="_blank"> BigQuery </a>)并进行分析。</li></ol><p id="1eec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里我们将使用 Kaggle 的<a class="ae le" href="https://www.kaggle.com/nickhould/craft-cans" rel="noopener ugc nofollow" target="_blank">精酿啤酒数据集</a>。</p><p id="e84c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">啤酒数据集的描述</strong></p><p id="c327" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> abv </strong>:酒精体积含量，0 为不含酒精，1 为纯酒精<br/> <strong class="kk iu"> ibu </strong>:国际苦味单位，规定一种饮料有多苦<br/> <strong class="kk iu">名称</strong>:啤酒名称<br/> <strong class="kk iu">风格</strong>:啤酒风格(lager、ale、IPA 等。)<br/> <strong class="kk iu"> brewery_id </strong>:生产这种啤酒的啤酒厂的唯一标识符<br/> <strong class="kk iu">盎司</strong>:以盎司为单位的啤酒大小</p><p id="3b65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将把这个数据集上传到 google cloud bucket。</p><p id="2d82" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在运行管道之前，我们需要启用数据流和大查询 API。在 GCP 搜索框中键入数据流 API 并启用它。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ng"><img src="../Images/b38e67d04bc360805692083e08e547a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qljUFT5Kkkuo82O-NUjBAA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">启用 API 按作者分类的图像</p></figure><p id="c95b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，您需要启用 BigQuery API。</p><p id="9edd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据流将使用云桶作为暂存位置来存储临时文件。我们将创建一个云存储桶，并选择最近的位置(区域)。</p><p id="d3f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，如果您在亚洲，您必须为计算(数据流作业)的速度和性能选择亚洲地区。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nh"><img src="../Images/838ad91a25e2fb441745e3cf084282f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BsVooOU2Fj0RQ0wuqVxq7Q.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">创建 GCS 存储桶-按作者分类的图像</p></figure><p id="7272" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用适当的模式创建 BigQuery 数据集和表，作为 data sink，来自 dataflow 作业的输出将驻留在其中。数据集区域将是离您最近的位置。在我们的例子中是南亚 1(孟买)。在 BigQuery 中创建表时，需要提供输出模式(已经在 batch.py 中给出)。</p><p id="6c80" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来打开 cloud shell 编辑器，设置您的项目属性(如果尚未设置),它将克隆包含所有支持文件和数据的 GitHub 存储库。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="f0d4" class="lf lg it nj b gy nn no l np nq">git clone <a class="ae le" href="https://github.com/aniket-g/batch-pipeline-using-apache-beam-python" rel="noopener ugc nofollow" target="_blank">https://github.com/aniket-g/batch-pipeline-using-apache-beam-python</a></span></pre><p id="2e10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">完成后，切换到所有文件所在的目录。</p><p id="b435" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在使用下面给出的命令将 beer.csv 文件复制到我们的 bucket 中。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="750b" class="lf lg it nj b gy nn no l np nq">gsutil cp beers.csv gs://ag-pipeline/batch/</span></pre><p id="b3ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者，您可以通过转到存储桶来上传该 CSV 文件。</p><p id="9759" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要运行管道，您需要在虚拟机上安装 Apache Beam 库。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="96f4" class="lf lg it nj b gy nn no l np nq">sudo pip3 install apache_beam[gcp]</span></pre><p id="616d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">仅此而已。</p><p id="f7af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们将浏览管道代码，了解它是如何工作的。我们将主要关注管道中的 Ptransforms。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="4155" class="lf lg it nj b gy nn no l np nq">def discard_incomplete(data):<br/>    """Filters out records that don't have an information."""<br/>    return len(data['abv']) &gt; 0 and len(data['id']) &gt; 0 and len(data['name']) &gt; 0 and len(data['style']) &gt; 0</span></pre><p id="073b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经过滤掉了那些没有信息或空值的数据。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="9df8" class="lf lg it nj b gy nn no l np nq">def convert_types(data):<br/>    """Converts string values to their appropriate type."""<br/>    data['abv'] = float(data['abv']) if 'abv' in data else None<br/>    data['id'] = int(data['id']) if 'id' in data else None<br/>    data['name'] = str(data['name']) if 'name' in data else None<br/>    data['style'] = str(data['style']) if 'style' in data else None<br/>    data['ounces'] = float(data['ounces']) if 'ounces' in data else None<br/>    return data</span></pre><p id="a6df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的函数将把字符串值转换成适当的数据类型。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="2a1e" class="lf lg it nj b gy nn no l np nq">def del_unwanted_cols(data):<br/>    """Deleting unwanted columns"""<br/>    del data['ibu']<br/>    del data['brewery_id']<br/>    return data</span></pre><p id="5f16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的函数中，我们删除了不需要的列，这些列最终成为干净的数据。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="e405" class="lf lg it nj b gy nn no l np nq">p = beam.Pipeline(options=PipelineOptions())</span><span id="bf82" class="lf lg it nj b gy nr no l np nq">(p | 'ReadData' &gt;&gt; beam.io.ReadFromText('gs://purchases-3/beers.csv', skip_header_lines =1)<br/>       | 'Split' &gt;&gt; beam.Map(lambda x: x.split(','))<br/>       | 'format to dict' &gt;&gt; beam.Map(lambda x: {"sr": x[0], "abv": x[1], "id": x[2], "name": x[3], "style": x[4], "ounces": x[5]}) <br/>       | 'DelIncompleteData' &gt;&gt; beam.Filter(discard_incomplete)<br/>       | 'Convertypes' &gt;&gt; beam.Map(convert_types)<br/>       | 'DelUnwantedData' &gt;&gt; beam.Map(del_unwanted_cols)<br/>       | 'WriteToBigQuery' &gt;&gt; beam.io.WriteToBigQuery(<br/>           '{0}:beer.beer_data'.format(PROJECT_ID),<br/>           schema=SCHEMA,<br/>           write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))<br/>    result = p.run()</span></pre><p id="32be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> beam.io.ReadFromText </strong> —将数据从外部来源读入 PCollection</p><p id="e9f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> beam.map — </strong>的工作方式类似于<strong class="kk iu"> ParDo，</strong>以多种方式应用<strong class="kk iu"> Map </strong>来变换<strong class="kk iu">p 集合</strong>中的每一个元素。Map 接受为 PCollection 中的每个输入元素返回单个元素的函数。</p><p id="9194" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">光束。Filter </strong> —接受一个函数，该函数保留返回<strong class="kk iu"> True </strong>的元素，并过滤掉剩余的元素。</p><p id="c9fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">beam . io . writetobigquery</strong>—对 BigQuerySink 的写转换接受字典的 p 集合。它需要以下参数</p><ul class=""><li id="c4bb" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated">TableReference 可以是 PROJECT:DATASET。表或数据集。表格字符串。</li><li id="1d9f" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">TableSchema 可以是 NAME:TYPE{，NAME:TYPE}* string(例如' month:STRING，event_count:INTEGER ')。</li></ul><p id="3ed2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们使用以下语法，使用数据流运行器运行管道。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="d1da" class="lf lg it nj b gy nn no l np nq">python3 batch.py --runner DataFlowRunner --project aniket-g --temp_location gs://ag-pipeline/batch/temp --staging_location gs://ag-pipeline/batch/stag --region asia-east1 --job_name drinkbeer</span></pre><p id="d3f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目前，Dataflow 为一些不包括亚洲-南方 1 的地区提供<a class="ae le" href="https://cloud.google.com/dataflow/docs/concepts/regional-endpoints." rel="noopener ugc nofollow" target="_blank">地区端点</a>，因此我在地区中选择了亚洲-东方 1。</p><ul class=""><li id="0891" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated"><strong class="kk iu">项目</strong> —您的谷歌云项目的 ID。</li><li id="c50a" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu">运行器</strong> —管道运行器，它将解析你的程序并构建你的管道。如果你想调试你的管道，也可以直接运行。这里我们使用数据流运行器。</li><li id="c78b" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu"> staging_location </strong> —数据流的云存储路径，用于存放执行作业的工人所需的代码包。</li><li id="c86d" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu"> temp_location </strong> —数据流的云存储路径，用于存放在流水线执行期间创建的临时作业文件。</li><li id="85f0" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu">区域</strong> —您可以指定想要运行数据流运行程序的区域。</li><li id="aff5" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu"> job_name </strong>(可选)—给数据流管道起任何名字。</li></ul><p id="73d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在转到数据流，您可以看到您的作业正在以批处理类型运行。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ns"><img src="../Images/74ccaf4738b1abf326bb6bce7300c51c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jQjo2KHho4EN5_jHZa-1CA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">管道状态—按作者分类的图像</p></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/dfef8990951be227a0ed2cd20adc0de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*15dHeI-pjKOibzJd6inTcg.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">DAG —管道步骤—按作者分类的图像</p></figure><p id="466f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦完成并成功，您将在 BigQuery beer_data 表中看到结果。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nu"><img src="../Images/631f1446c1a3c9871570cd103c986418.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8gsBIuB6m6jKHLwL5OfgQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">大查询表-按作者排序的图像</p></figure><p id="06c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们可以查询数据来获得一些见解。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="8fd4" class="lf lg it nj b gy nn no l np nq"># Beer style with highest alcohol by volume<br/>SELECT<br/>  style,<br/>  SUM(abv) AS volume<br/>FROM<br/>  `aniket-g.beer.beer_data`<br/>GROUP BY<br/>  style<br/>ORDER BY<br/>  volume DESC</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nv"><img src="../Images/272fecfb56a31f9dd51ccc841b5cd91c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dk8M6QckmwtgEGMXL4T_GQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">Bigquery Insights —按作者分类的图片</p></figure><p id="b522" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">查看代码:</strong></p><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="6e6c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文的主要目的是演示如何使用 apache beam 创建清理管道。我只使用了一个包含啤酒信息的数据集，而另一个包含啤酒厂信息的数据集可以提供更多的见解。</p><h2 id="337b" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">参考资料:</h2><p id="f5b7" class="pw-post-body-paragraph ki kj it kk b kl ny ju kn ko nz jx kq kr oa kt ku kv ob kx ky kz oc lb lc ld im bi translated"><a class="ae le" href="http://shzhangji.com/blog/2017/09/12/apache-beam-quick-start-with-python/" rel="noopener ugc nofollow" target="_blank">http://shzhangji . com/blog/2017/09/12/Apache-beam-quick-start-with-python/</a></p><p id="fc32" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://beam.apache.org/documentation/programming-guide/" rel="noopener ugc nofollow" target="_blank">https://beam.apache.org/documentation/programming-guide/</a></p></div></div>    
</body>
</html>