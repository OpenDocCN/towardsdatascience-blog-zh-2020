<html>
<head>
<title>Algorithms From Scratch: K-Nearest Neighbors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始的算法:K-最近邻</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algorithms-from-scratch-k-nearest-neighbors-fe19b431a57?source=collection_archive---------32-----------------------#2020-08-20">https://towardsdatascience.com/algorithms-from-scratch-k-nearest-neighbors-fe19b431a57?source=collection_archive---------32-----------------------#2020-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d115" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/algorithms-from-scratch" rel="noopener" target="_blank">从零开始的算法</a></h2><div class=""/><div class=""><h2 id="f186" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从头开始详述和构建 K-NN 算法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a6f6da1320afcb443e032f29f59c2ae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-sam_c4Z2TUgJAIb"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@ninastrehl?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">妮娜·斯特雷尔</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="1fd4" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">介绍</h2><p id="4d05" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml lr mm mn mo lv mp mq mr lz ms mt mu mv im bi mw translated"><span class="l mx my mz bm na nb nc nd ne di">能够执行分类和回归的</span>非参数算法；斯坦福大学教授 Thomas Cover 于 1967 年首次提出了 K 近邻算法的思想。</p><p id="4f03" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">许多人经常把 K-NN 称为懒惰学习器或一种基于实例的学习器，因为所有的计算都推迟到函数求值。我个人认为，当我们开始概念化机器学习算法时，这将 K-最近邻推向了不太复杂的一端。</p><p id="59fc" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">无论我们是在做分类还是回归风格问题，输入都将由原始特征空间中的<em class="nk"> k </em>个最近的训练样本组成。然而，算法的输出当然取决于问题的类型——关于不同输出的更多信息，请参见术语部分。</p><p id="20a2" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">链接到文章中生成的代码…</p><div class="nl nm gp gr nn no"><a href="https://github.com/kurtispykes/ml-from-scratch/tree/master" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd jd gy z fp nt fr fs nu fu fw jc bi translated">kurtispykes/ml-从零开始</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">通过在 GitHub 上创建一个帐户，为 kurtispykes/ml 的从头开发做出贡献。</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">github.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc lb no"/></div></div></a></div></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="c45e" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">术语</h2><p id="e43a" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml lr mm mn mo lv mp mq mr lz ms mt mu mv im bi translated"><strong class="mf jd">K-最近邻分类</strong> →输出将确定类成员，并且通过其邻居的多数投票来进行预测。因此，新实例将被分配到最常见的 k 个最近邻居的类中。</p><p id="8be7" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated"><strong class="mf jd">K-最近邻回归</strong> →输出将确定对象的属性值。因此，新实例将被分类为第<em class="nk"> k 个</em>最近邻居的平均值</p><p id="d01e" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated"><strong class="mf jd">基于实例的学习</strong> →一系列机器学习算法，不执行显式归纳，而是将新的问题实例与存储在内存中的训练中看到的实例进行比较。(来源:<a class="ae lh" href="https://en.wikipedia.org/wiki/Instance-based_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="mf jd">维基百科</strong> </a>)</p><p id="a922" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated"><strong class="mf jd">惰性学习</strong> →一种机器学习方法，在这种方法中，训练数据的概括在理论上被延迟，直到向系统发出查询，这与系统在接收查询之前试图概括训练数据的急切学习相反。(来源:<a class="ae lh" href="https://en.wikipedia.org/wiki/Lazy_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="mf jd">百科</strong> </a>)</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="4093" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">创建模型</h2><p id="7645" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml lr mm mn mo lv mp mq mr lz ms mt mu mv im bi translated">创建 K-NN 算法非常简单。训练阶段实际上是存储训练样本的特征向量和标签，但是我们需要为<em class="nk"> k. </em>通常为<em class="nk">，</em>确定一个正整数。当我们选择一个较大的值<em class="nk"> k </em>时，我们会减少噪声对分类的影响，从而使类别之间的边界不那么明显。最终，<em class="nk"> k </em>的选择很大程度上受到数据的影响，这意味着我们无法知道，直到我们尝试了数据，然而我们可以使用许多不同的试探法来为我们的数据选择<em class="nk"> k </em>。</p><blockquote class="ok ol om"><p id="0f47" class="md me nk mf b mg nf kd mi mj ng kg ml on nh mn mo oo ni mq mr op nj mt mu mv im bi translated"><strong class="mf jd">注</strong>:要阅读关于调整<em class="it"> k </em>超参数的更多信息，请参见维基百科页面的<a class="ae lh" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" rel="noopener ugc nofollow" target="_blank"> <strong class="mf jd">超参数优化</strong> </a>。</p></blockquote><p id="51eb" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">太好了，我们选择了 k。为了对分类任务的新实例进行预测，识别出与新观察最接近的 k 个记录(来自训练数据)。在对 K 个近邻进行评估后，将进行预测——参见术语部分的<strong class="mf jd">K-最近邻分类</strong>,了解如何进行预测。</p><p id="97f9" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">为了识别与新实例最接近的 k 个记录，我们必须对所有实例进行测量。这可以通过多种方式实现，尽管作为一种指导，当我们有连续变量时，许多从业者经常使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>，而对于离散变量则使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">汉明距离</a>。</p><p id="758a" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">图像汉明和欧氏距离</p><p id="bb3f" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated"><strong class="mf jd">分块算法</strong></p><ol class=""><li id="e96b" class="oq or it mf b mg nf mj ng lr os lv ot lz ou mv ov ow ox oy bi translated">计算欧几里德距离</li><li id="ad37" class="oq or it mf b mg oz mj pa lr pb lv pc lz pd mv ov ow ox oy bi translated">定位邻居</li><li id="e214" class="oq or it mf b mg oz mj pa lr pb lv pc lz pd mv ov ow ox oy bi translated">预测</li></ol><p id="eaac" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated"><strong class="mf jd">实现</strong></p><p id="a4ce" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">为了实现我们的 K-最近邻分类算法，我们将使用来自 Scikit-Learn 的<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>。在这项任务中，我们面临的挑战是在给定花朵尺寸的情况下，预测花朵是<code class="fe pe pf pg ph b">setosa</code>、<code class="fe pe pf pg ph b">versicolor</code>还是<code class="fe pe pf pg ph b">virginica</code>——这使它成为一项多类分类任务。</p><blockquote class="ok ol om"><p id="2a69" class="md me nk mf b mg nf kd mi mj ng kg ml on nh mn mo oo ni mq mr op nj mt mu mv im bi translated"><strong class="mf jd">注意</strong>:在这个实现中，我没有执行任何试探法来选择最佳的<em class="it">k</em>——我只是随机选择了一个 k 值。</p></blockquote><pre class="ks kt ku kv gt pi ph pj pk aw pl bi"><span id="d32f" class="li lj it ph b gy pm pn l po pp">import numpy as np<br/>import pandas as pd<br/>from collections import Counter</span><span id="9c02" class="li lj it ph b gy pq pn l po pp">import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="597f" class="li lj it ph b gy pq pn l po pp">from sklearn.metrics import accuracy_score<br/>from sklearn.datasets import load_iris<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import train_test_split</span><span id="c115" class="li lj it ph b gy pq pn l po pp">iris = load_iris()<br/>X, y = iris.data, iris.target</span><span id="24de" class="li lj it ph b gy pq pn l po pp">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1810)</span><span id="7fd5" class="li lj it ph b gy pq pn l po pp">X_train.shape, y_train.shape, X_test.shape, y_test.shape</span><span id="9421" class="li lj it ph b gy pq pn l po pp">((120, 4), (120,), (30, 4), (30,))</span></pre><p id="7e0d" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">我们现在有了数据，并使用了基于维持的交叉验证方案来拆分数据——如果您不熟悉这个术语，请参见下面的链接。</p><div class="nl nm gp gr nn no"><a rel="noopener follow" target="_blank" href="/cross-validation-c4fae714f1c5"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd jd gy z fp nt fr fs nu fu fw jc bi translated">交叉验证</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">验证机器学习模型的性能</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">towardsdatascience.com</p></div></div><div class="nx l"><div class="pr l nz oa ob nx oc lb no"/></div></div></a></div><p id="411f" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">第一步是计算两行之间的欧几里德距离。</p><pre class="ks kt ku kv gt pi ph pj pk aw pl bi"><span id="10ce" class="li lj it ph b gy pm pn l po pp">def euclidean(x1, x2):<br/>    return np.sqrt(np.sum((x1 - x2)**2))</span></pre><p id="b2dc" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">为了测试这个函数，我从 Jason brown lee<a class="ae lh" href="https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/" rel="noopener ugc nofollow" target="_blank">那里取了一些代码，他用这些代码来测试他的距离函数。如果我们有正确的实现，那么我们的输出应该是相同的。</a></p><pre class="ks kt ku kv gt pi ph pj pk aw pl bi"><span id="071a" class="li lj it ph b gy pm pn l po pp"># dataset from <a class="ae lh" href="https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/" rel="noopener ugc nofollow" target="_blank">https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/</a></span><span id="6a5b" class="li lj it ph b gy pq pn l po pp">dataset = [[2.7810836,2.550537003,0],<br/>[1.465489372,2.362125076,0],<br/>[3.396561688,4.400293529,0],<br/>[1.38807019,1.850220317,0],<br/>[3.06407232,3.005305973,0],<br/>[7.627531214,2.759262235,1],<br/>[5.332441248,2.088626775,1],<br/>[6.922596716,1.77106367,1],<br/>[8.675418651,-0.242068655,1],<br/>[7.673756466,3.508563011,1]]</span><span id="390c" class="li lj it ph b gy pq pn l po pp">row0 = dataset[0]</span><span id="412c" class="li lj it ph b gy pq pn l po pp">for row in dataset: <br/>    <strong class="ph jd">print</strong>(euclidean(np.array(row0), np.array(row)))</span><span id="2e43" class="li lj it ph b gy pq pn l po pp">0.0<br/>1.3290173915275787<br/>1.9494646655653247<br/>1.5591439385540549<br/>0.5356280721938492<br/>4.952940611164215<br/>2.7789902674782985<br/>4.3312480380207<br/>6.59862349695304<br/>5.084885603993178</span></pre><p id="9417" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">我们得到完全相同的输出——请随意查看提供的链接。</p><p id="4009" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">如前所述，新观察的<em class="nk"> k </em> —邻居是来自训练数据的<em class="nk"> k </em>最近实例。使用我们的距离函数<code class="fe pe pf pg ph b">euclidean</code>，我们现在可以计算训练数据中的每个观察值与我们已经传递的新观察值之间的距离，并从最接近我们的新观察值的训练数据中选择<em class="nk"> k </em>个实例。</p><pre class="ks kt ku kv gt pi ph pj pk aw pl bi"><span id="2eaa" class="li lj it ph b gy pm pn l po pp">def find_neighbors(X_train, X_test, y_train, n_neighbors): <br/>    <br/>    distances = [euclidean(X_test, x) for x in X_train]<br/>    k_nearest = np.argsort(distances)[:n_neighbors]<br/>    k_nearest_label = [y_train[i] for i in k_nearest]<br/>    <br/>    most_common = Counter(k_nearest_label).most_common(1)[0][0]<br/>    <br/>    return most_common</span></pre><p id="adb7" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">此函数计算新观察到定型数据中所有行的距离，并将其存储在一个列表中。接下来，我们使用 NumPy 模块<code class="fe pe pf pg ph b">np.argsort()</code>找到<em class="nk"> k </em>最低距离的索引—参见<a class="ae lh" href="https://numpy.org/doc/stable/reference/generated/numpy.argsort.html" rel="noopener ugc nofollow" target="_blank">文档</a>。然后我们使用索引来识别 k 个实例的类。之后，我们使用 Pythons 内置模块中的<code class="fe pe pf pg ph b">Counter</code>函数计算<code class="fe pe pf pg ph b">k_nearest_labels </code>列表中实例的数量，并返回最常见的(计数最高的标签)。然而，在我们做出预测之前，我们不会看到它的实际运行，所以让我们来构建预测函数。</p><pre class="ks kt ku kv gt pi ph pj pk aw pl bi"><span id="7269" class="li lj it ph b gy pm pn l po pp">def predict(X_test, X_train, y_train, n_neighbors=3): <br/>    predictions = [find_neighbors(X_train, x, y_train, n_neighbors)                  for x in X_test]<br/>    return np.array(predictions)</span><span id="3380" class="li lj it ph b gy pq pn l po pp">predict(X_test, X_train, y_train, n_neighbors=3)<br/><strong class="ph jd">array</strong>([0, 0, 2, 2, 0, 1, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2, 0, 0, 0, 2,           1, 2, 0, 0, 0, 0, 1, 1, 0, 2])</span></pre><p id="1292" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated">在<code class="fe pe pf pg ph b">predict</code>函数中，我们使用列表理解来为测试集中的每个新实例找到最近的邻居，并返回一个数组。使用 3 个邻居，我们在任务上获得了 100%的准确性，并且我们可以将其与 scikit-learning 实现进行比较，以查看我们是否获得了相同的结果——确实如此。</p><blockquote class="ok ol om"><p id="8800" class="md me nk mf b mg nf kd mi mj ng kg ml on nh mn mo oo ni mq mr op nj mt mu mv im bi translated"><strong class="mf jd">注意</strong>:K 近邻分类器的文档可以在<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p></blockquote><pre class="ks kt ku kv gt pi ph pj pk aw pl bi"><span id="5396" class="li lj it ph b gy pm pn l po pp">knn = KNeighborsClassifier(n_neighbors=3)<br/>knn.fit(X_train, y_train)</span><span id="42d4" class="li lj it ph b gy pq pn l po pp">sklearn_preds = knn.predict(X_test)<br/>preds = predict(X_test, X_train, y_train, n_neighbors=3)</span><span id="9d57" class="li lj it ph b gy pq pn l po pp"><strong class="ph jd">print</strong>(f"My Implementation: {accuracy_score(y_test, preds)}\nScikit-Learn Implementation: {accuracy_score(y_test, sklearn_preds)}")</span><span id="9c29" class="li lj it ph b gy pq pn l po pp">My Implementation: 1.0<br/>Scikit-Learn Implementation: 1.0</span></pre><p id="a0e1" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated"><strong class="mf jd">优点</strong></p><ul class=""><li id="3749" class="oq or it mf b mg nf mj ng lr os lv ot lz ou mv ps ow ox oy bi translated">直观简单</li><li id="48e2" class="oq or it mf b mg oz mj pa lr pb lv pc lz pd mv ps ow ox oy bi translated">没有训练步骤</li><li id="2335" class="oq or it mf b mg oz mj pa lr pb lv pc lz pd mv ps ow ox oy bi translated">可用于分类和回归(以及无监督学习)</li><li id="0826" class="oq or it mf b mg oz mj pa lr pb lv pc lz pd mv ps ow ox oy bi translated">对于多类问题易于实现</li></ul><p id="14f4" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated"><strong class="mf jd">缺点</strong></p><ul class=""><li id="ff5a" class="oq or it mf b mg nf mj ng lr os lv ot lz ou mv ps ow ox oy bi translated">随着数据的增长，算法变得非常慢</li><li id="6743" class="oq or it mf b mg oz mj pa lr pb lv pc lz pd mv ps ow ox oy bi translated">对异常值敏感</li><li id="f851" class="oq or it mf b mg oz mj pa lr pb lv pc lz pd mv ps ow ox oy bi translated">不平衡的数据会导致问题—可以使用加权距离来克服这个问题。</li></ul></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="4f67" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">包裹</h2><p id="9b23" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml lr mm mn mo lv mp mq mr lz ms mt mu mv im bi translated">在本故事中，您了解了 K-最近邻算法、如何在 Python 中从头开始实现 K-最近邻分类算法，以及使用 K-最近邻的利弊。</p><p id="3cc4" class="pw-post-body-paragraph md me it mf b mg nf kd mi mj ng kg ml lr nh mn mo lv ni mq mr lz nj mt mu mv im bi translated"><strong class="mf jd">让我们继续 LinkedIn 上的对话……</strong></p><div class="nl nm gp gr nn no"><a href="https://www.linkedin.com/in/kurtispykes/" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd jd gy z fp nt fr fs nu fu fw jc bi translated">Kurtis Pykes -人工智能作家-走向数据科学| LinkedIn</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">在世界上最大的职业社区 LinkedIn 上查看 Kurtis Pykes 的个人资料。Kurtis 有一个工作列在他们的…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">www.linkedin.com</p></div></div><div class="nx l"><div class="pt l nz oa ob nx oc lb no"/></div></div></a></div></div></div>    
</body>
</html>