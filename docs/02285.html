<html>
<head>
<title>A (Quick) Guide to Neural Network Optimizers with Applications in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络优化器(快速)指南及其在Keras中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4?source=collection_archive---------23-----------------------#2020-03-04">https://towardsdatascience.com/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4?source=collection_archive---------23-----------------------#2020-03-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3b70" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">SGD，Adagrad，Adam等等</h2></div><p id="b51c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated">随着深度学习的快速发展，出现了过多的优化器，人们可以选择来编译他们的神经网络。有这么多优化器，很难选择一个来使用。本文将简要说明各种神经网络优化器之间的区别。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h1 id="b502" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated"><strong class="ak">SGD</strong></h1><p id="06c4" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">与批量梯度下降或普通梯度下降相反，随机梯度下降更新每个训练示例<em class="mr"> x </em>和<em class="mr"> y </em>的参数。SGD以高方差执行频繁更新，导致目标函数大幅波动。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/bfac9c12f88d566134f5c49ba255c527.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*3iHheYFBsFF1ZuBWbLJtKg.png"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">新币波动。<a class="ae ne" href="https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="7421" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SGD的波动使它能够从一个局部极小值跳到一个可能更好的局部极小值，但是使收敛到一个精确的极小值变得复杂。</p><p id="01cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">动量是SGD的一个参数，可以添加到峡谷中以帮助SGD，峡谷是表面在一个维度比另一个维度弯曲得更陡的区域，常见于optima周围。在这些场景中，SGD在峡谷的斜坡周围振荡，沿着局部最优的底部犹豫前进。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/bb8f7ff9b1852d22e67054307461651d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*TJ6G0RSKBnCt5CpbGM6EQw.png"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated"><a class="ae ne" href="https://stats.stackexchange.com/questions/366728/why-doesnt-feature-standardization-make-sgd-with-momentum-redundant" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="82e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">动量有助于在正确的方向上加速SGD，因此抑制了多余的振荡，如图2所示。</p><p id="6ff0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">内斯特罗夫动量是对标准动量的改进——一个盲目跟随斜坡的球是不令人满意的。理想情况下，球会知道它要去哪里，所以它可以在山坡再次向上倾斜之前减速。内斯特罗夫加速梯度(NAG)可以通过在SGD到达上坡区域之前减慢它的速度，帮助减少收敛中不必要的冗余，从而使动量具有预见性。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h1 id="2905" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">阿达格拉德</h1><p id="2c51" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">Adagrad使学习率适应参数，对与频繁出现的特征相关联的参数执行较小的更新(低学习率),对与不频繁出现的特征相关联的参数执行较大的更新(高学习率)。因此，Adagrad在处理稀疏数据方面是有帮助的。</p><p id="8a33" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Adagrad消除了手动调整学习率的需要——大多数实现都保留默认值0.01。然而，Adagrad的算法导致学习率随着每次迭代而收缩，最终变得无穷小，在这个速度下，算法无法获得任何新知识。</p><p id="1b63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Adadelta是Adagrad的扩展，它寻求解决模型学习率收敛到0的问题。RMSprop是Adadelta的另一个版本，它试图解决Adadelta试图解决的相同问题。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="63a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">亚当</strong></p><p id="efe5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Adam是为每个参数计算自适应学习率的另一种方法。除了像Adadelta和RMSprop一样存储以前的渐变，Adam还实现了momentum的一个版本。Adam的行为就像一个有摩擦的重球，更喜欢误差曲面中的平坦极小值，可以被视为RMSprop和SGD的组合，具有动量。</p><p id="9902" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Nadam是另一个优化器，它结合了adam和NAG。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h1 id="fa79" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">形象化</h1><h2 id="1170" class="ng lv it bd lw nh ni dn ma nj nk dp me kr nl nm mg kv nn no mi kz np nq mk nr bi translated">比尔函数</h2><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/25bb3cf8e7c92e753c2f8368baf920dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*XVFmo9NxLnwDr3SxzKy-rA.gif"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">GIF:亚历克·拉德福德</p></figure><p id="ab96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，Adagrad、Adadelta和RMSprop几乎立即朝着正确的方向前进，并非常快地收敛，而具有动量的SGD和NAG则偏离了轨道，让人想起一个球滚下山坡的形象。然而，NAG很快就能通过向前看来纠正它的路线。</p><h2 id="c959" class="ng lv it bd lw nh ni dn ma nj nk dp me kr nl nm mg kv nn no mi kz np nq mk nr bi translated">鞍点</h2><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5ced5286be46c059022fec7161e0f4ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*SjtKOauOXFVjWRR7iCtHiA.gif"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">GIF:亚历克·拉德福德</p></figure><p id="8988" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意SGD(有动量和没有动量)和NAG发现很难突破到最小值，并且卡在中间。然而，新币与势头和唠叨最终逃脱鞍点。阿达格拉德、RMSprop和阿达德尔塔迅速向负斜率下驶去。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h1 id="fee3" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">我应该使用哪种优化器？</h1><p id="f7a9" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">如果输入数据是稀疏的，最好的结果将来自自适应学习率方法。总体而言，Adam可能是深度神经网络的最佳总体选择。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="2629" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你喜欢阅读，看看我在神经网络方面的其他工作:</p><ul class=""><li id="5686" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated"><a class="ae ne" rel="noopener" target="_blank" href="/a-guide-to-neural-network-layers-with-applications-in-keras-40ccb7ebb57a?source=post_stats_page---------------------------">神经网络层指南及其在Keras中的应用</a></li><li id="d159" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated"><a class="ae ne" href="https://medium.com/@andre_ye/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4?source=your_stories_page---------------------------" rel="noopener">神经网络损失函数指南及其在Keras中的应用</a></li></ul></div></div>    
</body>
</html>