# 可解释机器学习指南

> 原文：<https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf?source=collection_archive---------2----------------------->

## 破除深度学习黑箱神话的技术。

> 如果不能简单的解释，说明你理解的不够好。— **阿尔伯特·爱因斯坦**
> 
> **免责声明:**本文借鉴并扩展了(1) Christoph Molnar 关于 [*可解释机器学习*](https://christophm.github.io/interpretable-ml-book/) 的优秀书籍中的材料，我肯定会推荐给好奇的读者，(2)来自哈佛 ComputeFest 2020 的[深度学习可视化研讨会](https://github.com/Harvard-IACS/2020-ComputeFest/tree/master/visualization_workshop)，以及(3)来自哈佛大学的 [CS282R](https://interpretable-ml-class.github.io/) 的材料，由 Ike Lage 和伊马·拉卡茹教授，他们都是该领域的杰出研究人员本文旨在向普通数据科学家浓缩和总结可解释机器学习领域，并激发他们对该主题的兴趣。

机器学习系统越来越多地用于复杂的高风险环境，如医学(如放射学、药物开发)、金融技术(如股票价格预测、数字金融顾问)，甚至法律(如案例总结、诉讼预测)。尽管利用率有所提高，但仍然缺乏足够的技术来解释和诠释这些深度学习算法的决策。在某些领域，算法的决策必须是可解释的，或者归因于法律或法规规定的某些特征(如[解释权](https://en.wikipedia.org/wiki/Right_to_explanation))，或者需要[问责制](https://searchenterpriseai.techtarget.com/definition/algorithmic-accountability)，这可能会很成问题。

算法问责的必要性已被多次强调，其中最著名的案例是谷歌的面部识别算法，该算法将一些黑人标记为大猩猩，以及优步的自动驾驶汽车闯了一个停车标志。由于谷歌无法修复算法并消除导致这一问题的算法偏见，他们通过从谷歌照片的搜索引擎中删除与猴子有关的词语来解决问题。这说明了许多机器学习算法所谓的*黑盒*本质。

由于其预测性，黑盒问题主要与监督机器学习范例相关联。

![](img/ac9a7f7e84aabbb644902a45dc84524a.png)

黑盒算法——谁知道它在做什么？显然，没人。

仅仅精确是不够的。

深度学习领域的学者敏锐地意识到了这个[可解释性和可解释性问题](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence)，尽管一些人(如上面引用的山姆·哈里斯)认为这些模型本质上是[黑盒](https://en.wikipedia.org/wiki/Black_box)，但近年来已经有了一些发展，用于可视化深度神经网络的各个方面，如它们所学习的特征和表示。“信息贫乏”一词被用来指当决策是基于许多个人特征时，由于信息过载而难以提供透明度。自 2015 年以来，机器学习中的可解释性和可解释性领域出现了爆炸式增长，现在有几十篇关于该主题的论文，其中一些可以在参考文献中找到。

正如我们将在本文中看到的那样，这些可视化技术不足以完全解释深度学习算法学习到的复杂表示，但希望你会相信深度学习的黑盒解释不是真的——我们只是需要更好的技术来理解和解释这些模型。

# **黑盒子**

机器学习中的所有算法在某种程度上都是黑盒。机器学习的一个关键思想是模型是数据驱动的——模型是根据数据配置的。这从根本上给我们带来了这样的问题，如 **(1)** 我们应该如何解释模型， **(2)** 如何确保它们在决策中是透明的，以及 **(3)** 确保所述算法的结果是公平的和统计上有效的。

对于像线性回归这样的东西，模型是非常好理解和高度可解释的。当我们转向支持向量机(SVM)或随机森林模型时，事情变得有点困难。从这个意义上说，在机器学习中没有白盒或黑盒算法，可解释性是作为一个光谱或不同灰度的“灰盒”而存在的。

碰巧的是，在我们“灰色”区域的远端是神经网络。在这个灰色区域的更远处是深层神经网络。当你有一个拥有 15 亿个参数的深度神经网络时——就像用于语言建模的 GPT-2 算法一样——解释模型学习到的表示就变得极其困难。

2020 年 2 月，微软发布了现存最大的深度神经网络[【图灵-NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/) 。这个网络包含 170 亿个参数，大约是人脑中 850 亿个神经元的 1/5(尽管在神经网络中，参数代表连接，其中人脑中有约 100 万亿个)。显然，解释一个 170 亿参数的神经网络将非常困难，但它的性能可能远远优于其他模型，因为它可以在海量数据上训练而不会饱和——这是一个更复杂的表示可以由具有更多参数的模型存储的想法。

![](img/8b91f19271b84e53eb29f5921748c757.png)

比较图灵-NLG 与其他深度神经网络，如伯特和 GPT-2。[来源](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)

显然，表示是存在的，我们只是没有完全理解它们，因此我们必须想出更好的技术来解释模型。遗憾的是，这比读取线性回归中的系数更困难！

![](img/8cc5ee1a2705e8c365902552deb51d87.png)

神经网络是强大的模型，但比更简单、更传统的模型更难解释。

通常，我们并不关心一个算法如何得出一个特定的决定，特别是当它们在低风险环境中被操作时。在这些场景中，我们在选择算法时不会受到任何可解释性限制的限制。然而，如果可解释性在我们的算法中很重要——就像它通常在高风险环境中一样——那么我们必须接受准确性和可解释性之间的折衷。

那么有什么技术可以帮助我们更好地解释和理解我们的模型呢？事实证明有很多这样的方法，区分这些不同类型的技术帮助我们检查什么是有帮助的。

**本地对全球**

技术可以是**局部的**，帮助我们研究网络的一小部分，就像观察神经网络中的单个过滤器一样。

技术可以是**全局的**，允许我们从整体上建立一个更好的模型，这可以包括深度神经网络中权重分布的可视化，或者通过网络传播的神经网络层的可视化。

**特定型号与不特定型号的对比**

高度特定于**型号的技术**仅适用于单一型号。例如，层可视化仅适用于神经网络，而部分依赖图可用于许多不同类型的模型，并被描述为**模型不可知**。

特定于模型的技术通常涉及检查算法或中间表示的结构，而模型不可知的技术通常涉及检查输入或输出数据分布。

![](img/c9f9b4f06201300e942b42989cc4d0ab.png)

不同模型可视化技术和可解释性度量之间的区别。[来源](https://github.com/Harvard-IACS/2020-ComputeFest/blob/master/visualization_workshop/Workshop.pdf)

我将在整篇文章中讨论上述所有技术，但是也将讨论在哪里以及如何使用它们来帮助我们洞察我们的模型。

**因为正确的原因而正确**

由于我们缺乏模型可解释性而产生的一个问题是，我们不知道模型已经被训练了什么。最好用一个虚构的例子[来说明这一点](https://www.jefftk.com/p/detecting-tanks)(关于这个故事的真实性还有一些争论，但我们可以从中吸取的教训仍然是有价值的)。

> **捉迷藏**
> 
> 根据 AI 民间传说，在 20 世纪 60 年代，美国陆军对开发一种能够在图像中检测坦克的神经网络算法感兴趣。研究人员开发了一种算法，能够非常准确地做到这一点，每个人都对结果非常满意。
> 
> 然而，当该算法在额外的图像上进行测试时，它的表现非常差。这让研究人员感到困惑，因为研究结果在开发过程中是如此积极。在所有人都摸不着头脑的一段时间后，其中一名研究人员注意到，在查看两组图像时，一组图像中的天空比另一组图像中的天空更暗。
> 
> 很明显，该算法实际上并没有学会探测伪装的坦克，而是在观察天空的亮度！

虽然这个故事加剧了对深度学习的一个常见批评，但事实是，在神经网络中，尤其是深度神经网络中，你并不真正知道模型正在学习什么。

这种强有力的批评和深度学习在学术界和工业界日益增长的重要性导致了对可解释性和可解释性的日益关注。如果一个行业专家不能让他们的客户相信他们理解他们建立的模型在做什么，那么当有大的风险时，比如财务损失或人们的生命安全，真的应该使用它吗？

## 可解释性

在这一点上，你可能会问自己，鉴于可能有无限多种可行的解释，可视化如何帮助我们解释一个模型。定义和衡量可解释性的含义并不是一项简单的任务，而且在如何评价它的问题上几乎没有共识。

可解释性没有数学定义。文献中提出的两个定义是:

> "可解释性是人类能够理解决策原因的程度." **—提姆·米勒**
> 
> "可解释性是人类能够持续预测模型结果的程度."— **后土金**

机器学习模型的可解释性越高，人们就越容易理解为什么会做出某些决定或预测。如果一个模型的决策比另一个模型的决策更容易让人理解，那么这个模型就比另一个模型更容易解释。我们可以开始评估模型可解释性的一种方式是通过 [*可量化的代理*](https://en.wikipedia.org/wiki/Proxy_(statistics)) 。

一个**代理**是与我们感兴趣研究的东西高度相关，但与感兴趣的对象有本质区别的东西。代理往往比感兴趣的对象更容易测量，或者在这种情况下，只是可测量的——而我们感兴趣的对象(如可解释性)可能不可测量。

代理人的想法在许多领域都很普遍，其中一个领域是心理学，它们被用来测量抽象概念。最著名的代表可能是智商(IQ ),它是智力的代表。虽然智商和智力之间的相关性不是 100%，但它足够高，我们可以从测量智商中获得一些有用的信息。没有直接测量智力的已知方法。

一种使用降维来允许我们在低维空间中可视化高维数据的算法为我们提供了可视化数据分布的代理。同样，一组训练图像为我们提供了感兴趣的完整数据分布的代理，但不可避免地会与真实分布有些不同(如果您在构建训练集方面做得很好，它应该不会与给定的测试集相差太多)。

事后解释呢？

事后解释(或事后解释)可能有用，但有时会误导人。这些只是为黑盒的算法行为提供了一个看似合理的合理化，不一定是具体的证据，因此应该谨慎使用。事后合理化可以用可量化的代理来完成，我们将讨论的一些技术可以做到这一点。

# 选择可视化

设计可视化需要我们考虑以下因素:

*   **我们要介绍的观众(谁)** —这是为了调试的目的吗？说服客户吗？说服一个同行审稿人写一篇研究文章？
*   **可视化的目标(是什么)** —我们是在试图理解输入(例如，来自图像的 EXIF 元数据是否被正确读取，以使图像不会从侧面进入 CNN)、输出或我们模型的参数分布吗？我们感兴趣的是输入如何在网络中演变，还是网络的静态特征，如特征图或过滤器？
*   **正在开发的模型** **【如何】** —很明显，如果您没有使用神经网络，您就无法可视化网络层的特征地图。类似地，特征重要性可以用于一些模型，例如 XGBoost 或随机森林算法，但不能用于其他模型。因此，模型选择固有地偏向可以使用的技术，一些技术比其他技术更通用和通用。开发多个模型可以提供更多的多样性。

**深度模型对可视化提出了独特的挑战**:我们可以回答关于模型的相同问题，但是我们的询问方法必须改变！由于这一点的重要性，我们将在本文的剩余部分主要关注深度学习可视化。

## 深度学习可视化的子领域

深度学习可视化文献主要有三个子领域:

1.  **可解释性&可解释性:**有助于理解深度学习模型如何做出决策以及它们的学习表征。
2.  **调试&改进:**帮助模型管理员和开发人员构建和解决他们的模型，希望加速迭代实验过程，最终提高性能。
3.  **教授深度学习:**帮助教育业余用户关于人工智能——更具体地说，机器学习。

## 为什么解释一个神经网络如此困难？

为了理解为什么解释神经网络是困难和不直观的，我们必须理解网络对我们的数据做了什么。

本质上，我们传递到输入层的数据(这可能是一幅图像或一组用于预测变量的相关特征)可以绘制成一些复杂的分布，如下图所示(这只是一个 2D 表示，想象一下 1000 维)。

![](img/876274b51c6f7586df1fceafdd560609.png)

如果我们通过线性分类器运行这些数据，模型会尽最大努力分离数据，但由于我们仅限于仅包含线性函数的假设类，我们的模型将表现不佳，因为大部分数据不是线性可分离的。

![](img/2356a79bdb46f14f9ead85631bd7d928.png)

这就是神经网络的用武之地。神经网络是一个非常特殊的功能。已经证明，具有单个隐藏层的神经网络能够表示所有非线性函数的假设类，只要我们在网络中有足够的节点。这就是所谓的[通用近似定理](https://en.wikipedia.org/wiki/Universal_approximation_theorem)。

事实证明，我们拥有的节点越多，我们可以表示的函数类就越大。如果我们有一个只有十层的网络，并试图用它来分类一百万张图像，网络将很快饱和并达到最大容量。如果我们有 1000 万个参数，随着非线性变换数量的增加，它将能够学习网络的更好表示。我们说这个型号有更大的*型号容量*。

人们使用深度神经网络而不是单层神经网络，因为单层网络中所需的神经元数量随着模型容量呈指数增长。隐藏层的抽象大大减少了对更多神经元的需求，但这是以可解释性为代价的。我们越深入，这个网络就变得越难以解释。

神经网络的非线性变换允许我们将数据重新映射到线性可分的空间中。在神经网络的输出层，我们可以任意使用线性分类器将最初的非线性数据分成两类，如下所示。

![](img/68a048aab33b1394dcb0e226601c209d.png)

使用神经网络将非线性数据集转换为可线性分离的数据集。[来源](https://github.com/Harvard-IACS/2020-ComputeFest/blob/master/visualization_workshop/Lecture/Lecture.ipynb)

问题是，我们如何知道这个多层非线性变换中发生了什么，它可能包含数百万个参数？

想象一下一个 [GAN](https://en.wikipedia.org/wiki/Generative_adversarial_network) 模型(两个网络为了模拟输入数据的分布而相互争斗)在一个 512x512 的图像数据集上工作。当图像被引入神经网络时，每个像素都成为神经网络的一个特征。对于这样大小的图像，特征的数量是 262，144。这意味着我们正在对超过 200，000 个特征执行潜在的 8 或 9 个卷积和非线性变换。这怎么解释呢？

更极端的例子是 1024x1024 图像，这是由 NVIDIA 的 StyleGAN 实现开发的。由于像素数量增加了四倍，图像大小增加了一倍，我们将有超过一百万个特征作为 GAN 的输入。因此，我们现在有一个一百万特征的神经网络，执行卷积运算和非线性激活，并在数十万张图像的数据集上执行这些操作。

希望我已经让你相信解释深层神经网络是非常困难的。尽管神经网络的操作看似简单，但它们可以通过某种形式的[涌现](https://en.wikipedia.org/wiki/Emergence)产生极其复杂的结果。

# 形象化

在本文的剩余部分，我将讨论可用于深度神经网络的可视化技术，因为它们在机器学习的可解释性和可解释性方面提出了最大的挑战。

## 权重直方图

权重直方图通常适用于任何数据类型，所以我选择先介绍这些。权重直方图对于确定深度神经网络中权重的总体分布非常有用。通常，直方图显示给定值相对于其他值的出现次数。如果权重的分布是均匀的，正态分布或呈现某种有序结构可以告诉我们有用的信息。

例如，如果我们想检查我们所有的网络层是否都在从给定的批次中学习，我们可以看到在对该批次进行训练之后权重分布是如何变化的。虽然一开始这可能不是最有用的可视化，但我们仍然可以从权重直方图中获得有价值的洞察力。

下图显示了一个四层网络在 [Tensorboard](https://www.tensorflow.org/tensorboard) — Tensorflow 的主要可视化工具中的权重和偏差直方图。

![](img/da880d0d196c3298bcdc8245a6cdc141.png)

[张量板](https://www.tensorflow.org/tensorboard)中的重量直方图。

对于不熟悉的人来说，还有一个绘制权重分布的工具是[Weights and bias](https://www.wandb.com/)(W&B)，这是一家比较新的公司，专门从事深度学习的实验跟踪。当训练具有数百万个参数的大型网络(如 GAN)时，W & B 提供的实验跟踪非常有助于日志记录，并提供了比 Tensorboard 更多的功能(对学术界的人来说是免费的)。

![](img/2855d9e155d87763efa1b8b47c294e63.png)

[权重和偏差](https://www.wandb.com/articles/monitor-your-pytorch-models-with-five-extra-lines-of-code)中的权重直方图。

## 显著图

回到我们之前讨论的 tank 问题，我们如何对这个网络进行故障排除，以确保分类器检查图像的正确部分来进行预测？一种方法是使用显著图。

显著图是在 2013 年的论文“ [*深入卷积网络内部:可视化图像分类模型和显著图*](https://arxiv.org/pdf/1312.6034.pdf) ”中提出的，以及类别最大化(稍后讨论)。他们背后的想法相当简单。首先，我们计算输出类别相对于输入图像的梯度。这为我们提供了一个指标，表明我们的分类相对于每个输入图像像素的微小变化是如何变化的。如果微小的变化产生了正梯度，那么我们知道该像素的变化增加了输出值。通过可视化梯度，我们可以检查哪些像素对激活最重要，并确保被检查的图像部分对应于感兴趣的对象。

![](img/d11d46b2d64f096f7fae05d6718940f0.png)

显著图提供了输出类的输入敏感度的可视化表示。

显著图为我们提供了一种计算给定图像中给定类别的[空间支持度](https://en.wikipedia.org/wiki/Support_(mathematics))的方法(图像特定类别显著图)。这意味着我们可以查看卷积网络的分类输出，执行反向传播，并查看图像的哪些部分参与了将图像分类为给定类别。

![](img/f1e850b35eae3d80b8cdccbc16356f5b.png)

特定类别图像及其该类别的预期显著性图的示例。[来源](https://arxiv.org/pdf/1312.6034.pdf)

可以使用另一种对显著性方法的简单调整，称为校正显著性。这包括在反向传播步骤中剪切负梯度，以便只传播正梯度信息。因此，仅传达与输出增加相关的信息。您可以在论文[可视化和理解卷积网络](https://arxiv.org/pdf/1311.2901.pdf)中找到更多信息

给定具有像素位置 *i* 和 *j* 以及 *c* 颜色通道(RGB 图像中的红色、蓝色和绿色)的图像，我们反向传播输出以找到对应于每个像素的导数。然后，我们取权重的所有颜色通道的绝对值的最大值，并将其用作显著图 *M* 的第 *ij* 个值。

![](img/fa5f7a88b4f55035eceae0b6eccb5c74.png)

显著图 M 是具有像素位置 I 和 j 的 2D 图像。该图在每个点的值是从所有图像颜色通道 c 的反向传播中找到的导数的最大绝对值

使用 Keras 函数“可视化显著性”和“可视化显著性损失”，可以在 Keras 中轻松实现显著性图的可视化。

## 遮挡贴图

用于辨别图像预测中像素重要性的显著性映射的类似技术是*遮挡映射*。在遮挡贴图中，我们仍然在开发与图像输出相关的贴图。然而，这次我们感兴趣的是遮挡图像的一部分如何影响图像的预测输出。

基于遮挡的方法使用灰色正方形系统地遮挡(阻挡)部分输入图像，并监控分类器输出。下图-显示了一个旨在预测黑色素瘤的图像分类器-清楚地显示了该模型正在定位场景中的对象，因为当对象被遮挡时，正确分类的概率显著下降(热图在黑色素瘤所在的区域变暗，因为遮挡会降低分类器的输出性能)。

![](img/af0cf69a6a4ffe425cce00698597bdcb.png)

分类器显示预测黑色素瘤的分类器的遮挡图。[来源](https://www.linkedin.com/pulse/what-does-ai-know-model-interpretability-occlusion-susan-sheldrick/)

遮挡贴图实现起来相当简单，因为它只需要在给定的像素位置扭曲图像，并将预测输出保存到热图中。阿克谢·舒拉在 GitHub 上的一个很好的实现可以在[这里](https://github.com/akshaychawla/Occlusion-experiments-for-image-segmentation/blob/master/Occlusion%20experiments%20for%20segmentation.ipynb)找到。

## 类别最大化

研究神经网络的一个非常强大的技术是类最大化。这允许我们查看一个类的样本，即会导致分类器的类值在输出中最大化的输入。对于图像数据，我们称之为一个类的图像样本。从数学上讲，这相当于:

![](img/d6b1b8865afa2066fbc96f7a3526e24d.png)

其中 **x*** 对应于类别 *c* 的图像样本。这种符号表示我们想要的图像对于类 *c* 给出了最大可能的输出，这可以解释为什么是完美的 *c* ？

大规模分类网络的输出非常有趣。下面是由 [Nguyen、Yosinski 和 Clune 在他们 2016 年关于深度卷积网络可视化的论文](https://arxiv.org/pdf/1602.03616.pdf)中生成的一些图像。他们在一个深度卷积神经网络上执行类最大化，该网络在 ILSVRC-2013 数据集上进行训练。

![](img/99a4630f1a6a50da250d30c107343462.png)

由深度卷积网络上的类最大化生成的图像。[来源](https://arxiv.org/pdf/1602.03616.pdf)

## 激活最大化

类似于类最大化，激活最大化帮助我们可视化卷积过滤器的范例。类别最大化是激活最大化的子集，由此分类算法的输出 softmax 层被最大化。数学上，激活最大化可以描述为:

![](img/de40e7440bd93812024e6775e939e918.png)

其中 **x*** 对应于深度神经网络中隐含层 *l* 或滤波器 *f* 的样本。这种符号表示我们想要最大化滤波器或层的输入(在卷积网络的情况下是图像)。这在下面针对深度卷积神经网络的 8 层来说明。

![](img/a2dc62ab98405c8997966025f62e5109.png)

深度卷积网络上激活最大化生成的图像。[来源](https://arxiv.org/pdf/1602.03616.pdf)

## LIME(局部可解释的模型不可知解释)

LIME 代表本地可解释的模型不可知解释，甚至有自己的 [Python 包](https://github.com/marcotcr/lime)。因为该方法被设计为模型不可知的，所以它可以应用于许多不同的机器学习模型。它首先出现在马尔科·图利奥·里贝罗和他的同事们的论文中，包括 [*“机器学习的模型不可知可解释性”*](https://arxiv.org/pdf/1606.05386v1.pdf) 和[*’“我为什么要相信你？”:解释任何分类器的预测'*](https://arxiv.org/abs/1602.04938) *，*均发表于 2016 年。

局部代理模型是用于解释黑盒机器学习模型的个体预测的可解释模型。LIME 是本地代理模型的一个实现。

代理模型被训练来近似底层黑盒模型的预测。

LIME 不是训练一个全局代理模型，而是专注于训练局部代理模型来解释个体预测。

![](img/31da8393aae88f079bd7415959a4e726.png)

向人类决策者解释个人预测。 [*来源*](https://arxiv.org/pdf/1602.04938.pdf)

在 LIME 中，我们扰动输入并分析我们的预测如何变化。不管听起来如何，这与遮挡映射和显著性映射非常不同。我们的目标是在由*邻近性度量* πₓ *管理的给定位置，使用来自一组可能模型 *G* 的可解释模型 *g* (例如具有几个系数的线性模型)来近似底层模型 *f* 。*我们还添加了一个正则项*ω*，以确保可解释的模型尽可能简单。这在下面的等式中说明。

![](img/67572842ce9794faa180c10d51d9aaee.png)

实例 x 的解释模型是最小化损失 L(例如，均方误差)的模型 g(例如，线性回归模型)，其测量解释与原始模型 f(例如，xgboost 模型)的预测有多接近，同时模型复杂度ω(g)保持较低(例如，偏好较少的特征)。g 是可能解释的族，例如，所有可能的线性回归模型。邻近度πₓ定义了我们在解释时考虑的实例 x 周围的邻域有多大。

图像的 LIME 与表格数据和文本的 LIME 工作方式不同。直觉上，干扰单个像素没有多大意义，因为不止一个像素对一个类有贡献。随机改变单个像素可能不会改变预测太多。因此，通过将图像分割成“超像素”并关闭或打开超像素来创建图像的变体。

![](img/22b28f91f382e96ba7bb20fa55a69acd.png)

被分割成超像素的猫的图像。[来源](https://stackoverflow.com/questions/55928671/image-boundary-based-on-superpixels)

超像素是具有相似颜色的互连像素，可以通过用用户定义的颜色(如灰色)替换每个像素来关闭。用户还可以指定在每个排列中关闭超像素的概率。

![](img/4d6994798cf6b6898ae720bace7f9210.png)

解释谷歌的 Inception 神经网络做出的一个图像分类预测。预测的前 3 类分别是“电吉他”(p = 0.32)、“木吉他”(p = 0.24)和“拉布拉多”(p = 0.21)。[来源](https://arxiv.org/pdf/1602.04938.pdf)

**保真度度量**(可解释模型逼近黑盒预测的程度，由我们的损失值 *L* 给出)让我们很好地了解了可解释模型在解释感兴趣的数据实例附近的黑盒预测时的可靠性。

LIME 也是为数不多的**用于表格数据、文本和图像**的方法之一。

注意，我们也可以生成全局代理模型，它遵循相同的思想，但用作整个黑盒算法的近似模型，而不仅仅是算法的局部子集。

## 部分相关图

部分相关性图显示了一个或两个特征对机器学习模型的预测结果的边际影响。如果我们使用具有一百个特征的数据集(包括前几天的黄金价值)来分析像黄金这样的金属的市场价格，我们会发现黄金价格对一些特征的依赖性比其他特征高得多。例如，黄金价格可能与石油价格密切相关，而与鳄梨价格没有密切联系。该信息在部分相关性图中可见。

![](img/1f92ce4d706309750d53a1a20cc06493.png)

自行车租赁与温度、湿度和风速的部分相关图示例。我们看到，在这三个变量中，温度对自行车租赁数量的依赖性最强。[来源](https://christophm.github.io/interpretable-ml-book/pdp.html)

请注意，这与线性回归模型不同。如果这是在线性回归模型上执行的，则每个部分相关性图将是线性的。部分依赖图允许我们看到关系的全部复杂性，它可能是线性的、指数的或一些其他复杂的关系。

部分依赖图的主要缺陷之一是，它只能真实地显示涉及一个或两个特征的 2D 解释。因此，对多个变量之间的高阶交互项进行建模是困难的。

还有一个变量独立性的固有假设，但通常不是这样(例如身高和体重之间的相关性，这是医学数据集中的两个常见参数)。由于[多重共线性](https://en.wikipedia.org/wiki/Multicollinearity)，变量之间的这些相关性可能导致其中一个变量冗余或给算法带来问题。当这成为一个问题时，使用累积局部效应(ALE)是更可取的，因为当涉及到共线性时，它不会遭受与部分相关图相同的陷阱。

为了避免过度解释数据稀疏特征区域中的结果，在部分相关性图的底部添加 [rug 图](https://en.wikipedia.org/wiki/Rug_plot)有助于查看数据丰富和数据稀疏区域出现的位置。

## 个体条件期望

ICE 类似于部分依赖关系图，只是为数据集中的每个实例绘制了不同的线条。因此，部分依赖图为我们提供了一个特性变量对输出变量的依赖的平均视图，而 ICE 允许我们看到特性变量的特定于实例的依赖。当交互变量存在时，这是有用的，当查看平均结果时，交互变量可能被掩盖，但是当使用 ICE 时，交互变量变得非常明显。

![](img/0d5e55a598c3bc11be970033af6d6233.png)

自行车租赁关于温度、湿度和风速的单个条件期望图示例。我们看到，每个图在实例之间没有表现出任何异质性，因此不太可能存在任何重要的相互作用项。[来源](https://christophm.github.io/interpretable-ml-book/pdp.html)

存在不同类型的冰图，例如也存在中心冰图和衍生冰图，但是本质上以不同的形式提供相同的信息。

## 沙普利值

沙普利值是由[劳埃德·沙普利](https://en.wikipedia.org/wiki/Lloyd_Shapley)于 1953 年从合作博弈论的一个方面得出的概念。在合作博弈理论中，Shapley 值根据每个玩家在所有排列中的平均贡献来优化他们的支出。当应用于机器学习时，我们假设每个特征都是游戏中的一个玩家，所有人一起工作以最大化预测，这可以被认为是支出。Shapley 值根据每个要素对输出值的贡献将支出的一部分分配给每个要素。

例如，如果您正在查看房价，并且从分析中移除了单个要素，这会如何影响模型预测？如果预测值下降了一个数量，我们可以推断该特征对预测的贡献如此之大。当然，事情并不那么简单，我们必须对每个可能的特征组合进行计算，这意味着我们需要运行 *2ˣ* 模型，其中 *x* 是特征的数量。

**因此，Shapley 值是一个特征值在所有可能的联合中的平均边际贡献。**

![](img/fa6f9f8af13050b5750112b6709af4db.png)

ϕ沙普利值的方程式，来自合作博弈理论。

这个等式可能看起来令人生畏，所以让我们从右到左一点一点地检查它。为了知道我们点 *xᵢ、*的边际贡献，我们使用我们的特征子集 *S* 中不包含特征 *xᵢ、*的所有特征来计算我们的模型的预测值，且我们从仍然存在该特征的子集的预测值中减去该预测值。然后，我们对特征排列的总数进行缩放，然后对所有这些贡献进行求和。因此，我们现在有了一个值，该值实质上是使用每个可能的特征子集的训练模型的特征的平均贡献。

这个讨论可能看起来很抽象，所以举个例子会很有帮助。Christoph 的书中使用的例子是考虑房价的一个很好的例子。如果我们有预测房价的特征，包括(1)公寓的大小(数字)，(2)离附近公园的远近(二元)，以及(3)公寓所在的楼层。为了计算每个特性的 Shapley 值，我们采用每个可能的特性子集，并预测每种情况下的输出(包括没有特性的情况)。然后，我们将每个特征的边际贡献相加。

![](img/22ec617f9281c06b470a8084147a5567.png)

计算简单房价预测模型的 Shapley 值时需要考虑的所有可能的特征排列。[来源](https://christophm.github.io/interpretable-ml-book/shapley.html)

一个参与者可以是单个特征值，例如对于表格数据，但是一个参与者也可以是一组特征值。例如，为了解释图像，可以将像素分组为超像素，并在它们之间分配预测。

据我所知，Python 上没有 Shapley 值的官方包，但有一些可用的存储库已经实现了它，用于机器学习。一个这样的包可以在这里找到。

Shapley 值的主要缺点是，对于大量的特征，由于特征数量的线性增加，可能的排列数量呈指数增加，因此计算非常昂贵和耗时。因此，对于特征数量非常大的应用，Shapley 值通常使用特征排列的子集来近似。

## 锚

马尔科·图利奥·里贝罗、萨梅尔·辛格和卡洛斯·盖斯特林在 2018 年的一篇论文中首次介绍了这一点，他们是创造石灰的同一批研究人员。它也有自己的由 Marco 开发的 [Python 包](https://github.com/marcotcr/anchor)。Python 的 [ALIBI](https://docs.seldon.io/projects/alibi/en/stable/index.html) 包中也有。

锚点解决了局部解释方法的一个关键缺点，如 [LIME](https://arxiv.org/abs/1602.04938) 以线性方式代理模型的局部行为。然而，不清楚该解释在多大程度上在要解释的实例周围的区域中成立，因为模型和数据在该实例的邻域中都可以表现出非线性行为。这种方法很容易导致解释的过度自信和对看不见但相似的例子的误导性结论。anchor 算法通过将覆盖范围(解释适用的区域)合并到优化问题中来解决这个问题。

与 LIME 类似，锚点可以用于文本、表格和图像数据。对于图像，我们首先将它们分割成超像素，同时仍然保持局部图像结构。然后，可解释的表示包括锚中每个超像素的存在或不存在。几种图像分割技术可用于将图像分割成超像素，如 [slic](https://scikit-image.org/docs/dev/api/skimage.segmentation.html?highlight=slic) 或 [quickshift](https://scikit-image.org/docs/dev/api/skimage.segmentation.html?highlight=slic) 。

该算法支持多种标准图像分割算法( [felzenszwalb、slic 和 quickshift](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_segmentations.html#sphx-glr-auto-examples-segmentation-plot-segmentations-py) )，并允许用户提供自定义分割功能。

![](img/5efb2ae2f85f4f25d0fc1277d2123861.png)

当使用初始网络分类时，小猎犬的锚被叠加在其他图像背景上，而预测准确性没有降低。[来源](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf)

## 反事实

反事实是锚的对立面。锚是当存在时足以锚定预测的特征(即，防止其通过改变其他特征而被改变)。在锚点部分，我们看了一个例子，这些锚点是图像的超像素。事实上，图像中不属于锚点的每个超像素都是反事实——我们可以通过改变反事实来改变预测，而不是改变锚点。

反事实是在沃希特等人 2017 年题为 [*“不打开黑盒的反事实解释:自动化决策和 GDPR”*](https://arxiv.org/abs/1711.00399)的论文中首次提出的。反事实的基本思想是，我们希望找到我们能够对最少数量的特征做出的最小改变，以便获得我们想要的期望输出。

**预测的反事实解释描述了将预测改变为预定义输出的特征值的最小变化。**

![](img/41f762b5bd90b2ea46069f9e65e427df.png)

什么是反事实？这是我们的特征空间的最小变化，允许我们跨越一个决策边界。[来源](https://christophm.github.io/interpretable-ml-book/counterfactual.html)

这听起来像是一个欠定义的任务，因为我们有很多方法可以改变我们的实例以满足我们期望的输出。这种现象被称为“罗生门效应”,因此，我们必须将我们的问题转换成优化问题的形式。首先，我们希望确保尽可能少地改变特征，并且尽可能少地改变这些特征，同时还维护可能给定数据联合分布的实例。我们的优化问题的损失函数可以转换为

![](img/e2c5396c2453c37479da11a5a10472d3.png)

作为反事实优化问题的一部分要最小化的损失函数。

损失函数的第一项表示模型预测*f’(x’)*和预期输出*y’之间的二次距离。*第二项表示原始实例和反事实实例之间的距离度量。二次项具有缩放参数，该参数将预测输出的重要性缩放到正常实例 *x* 和反事实实例*x’*之间的距离。

我们使用的距离度量是曼哈顿距离，因为反事实不仅应该接近原始实例，还应该**尽可能少地改变特征**。距离函数被描述为

![](img/0103e4109d54334d1eacf76c9e80eb74.png)

这是使用中间绝对偏差缩放的[曼哈顿距离](https://en.wikipedia.org/wiki/Taxicab_geometry)。

如果我们有一个小的缩放参数，距离度量就更重要，我们更喜欢看到接近正常情况的反事实。如果我们有一个大的尺度参数，预测就变得更加重要，我们对反事实与正常情况的接近程度就不那么在意了。

当我们运行我们的算法时，我们不需要为我们的缩放参数选择一个值。相反，作者建议由用户给出一个容差ϵ，它代表我们可以容忍预测与我们的输出有多远。这表现为

![](img/6cd9427878d3bb65c89fe9e5152f5f4b.png)

我们优化问题的一个附加约束。

我们的优化问题可以简洁地描述为

![](img/b9828203e1273b261155e5f4898d5d98.png)

我们的目标是找到反事实 x ’,使我们的总损失函数最小化，同时改变缩放参数 *λ。*

反事实的优化机制可以描述为“生长球体”方法，其中输入实例 *x* 、输出值*y’*和容差参数 *ϵ* 由用户给出。最初，设定比例参数 *λ* 的一个小值。对当前允许的反事实“范围”内的随机实例进行采样，然后将其用作优化的起点，直到该实例满足上述约束条件(即，如果预测值和输出值之间的差异低于我们的容差)。然后，我们将这个实例添加到我们的反事实列表中，并增加 *λ，*的值，这有效地增加了我们的“球体”的大小。我们递归地这样做，生成一个反事实列表。在程序的最后，我们选择最小化损失函数的反事实。

反事实是在 Python 包 ALIBI 中实现的，你可以在这里阅读(它们也有一个替代描述，可能比我自己的描述更有帮助和清晰)。

## 其他技术

这里还有一些我没有提到的技术，我建议感兴趣的读者参考一下。这些包括但不限于:

[累积局部效应](https://arxiv.org/pdf/1612.08468.pdf)

[特征重要性](/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e)

[降维技术(PCA，t-SNE)](/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b)

[沙普利加法解释(SHAP)](https://github.com/slundberg/shap)

[模型蒸馏](https://arxiv.org/pdf/1503.02531.pdf)

在这个 [GitHub 页面](https://github.com/amarasovic/interpretability-literature)上也可以找到一个很好的关于机器学习可解释性主题的知识库，它涵盖了关于该主题的论文、讲座和其他博客。

# 最终意见

深度学习可视化是一个复杂的话题，最近几年才刚刚开始研究。然而，随着深度学习技术越来越多地融入我们的数据驱动型社会，它将变得更加重要。我们大多数人可能更看重表现而不是理解，但我认为能够解释和说明模型将在未来为个人和公司提供竞争优势，这肯定会有市场。

可视化不是解释或解释深度神经网络结果的唯一方法或最佳方法，但它们肯定是*一种*方法，它们可以为我们提供对复杂网络决策过程的有用见解。

> 问题在于，单一指标(如分类准确率)无法完整描述大多数现实任务 ***—多希-维勒兹和金 2017***

## 时事通讯

关于新博客文章和额外内容的更新，请注册我的时事通讯。

[](https://mailchi.mp/6304809e49e7/matthew-stewart) [## 时事通讯订阅

### 丰富您的学术之旅，加入一个由科学家，研究人员和行业专业人士组成的社区，以获得…

mailchi.mp](https://mailchi.mp/6304809e49e7/matthew-stewart) 

# 参考

这里有我在这篇文章中引用的论文，以及我认为读者可能会发现的关于算法可解释性和可解释性的论文。

**【1】迈向可解释机器学习的严谨科学**——[多希-维勒兹和金，2017](https://arxiv.org/pdf/1702.08608.pdf)

**【2】模型可解释性的神话**——[利普顿，2017](https://arxiv.org/pdf/1606.03490.pdf)

**【3】透明度:动机与挑战** — [韦勒，2019](https://arxiv.org/pdf/1708.01870.pdf)

**【4】**[](https://arxiv.org/abs/1902.00006)****对人类的一种评价——解释性的解释——**[拉赫等人。艾尔。，2019](https://arxiv.org/abs/1902.00006)**

****【5】操纵和测量模型可解释性—** [Poursabzi-Sangdeh，2018](https://arxiv.org/pdf/1802.07810.pdf)**

****【6】使用规则和贝叶斯分析的可解释分类器:构建更好的中风预测模型—** [Letham and Rudin，2015](https://arxiv.org/pdf/1511.01644.pdf)**

****【7】可解释决策集:描述和预测的联合框架** — [Lakkaraju 等。艾尔。，2016 年](https://www-cs-faculty.stanford.edu/people/jure/pubs/interpretable-kdd16.pdf)**

****【8】通过原型进行案例推理的深度学习:解释其预测的神经网络** — [李等。艾尔。，2017 年](https://arxiv.org/pdf/1710.04806.pdf)**

****【9】贝叶斯案例模型:一种基于案例推理和原型分类的生成方法** — [Kim 等人。艾尔。，2014 年](https://beenkim.github.io/papers/KimRudinShahNIPS2014.pdf)**

****【10】学习优化风险分值**——[乌斯顿和鲁丁，2017](https://arxiv.org/abs/1610.00168)**

****【11】可理解的医疗保健模型:预测肺炎风险和住院 30 天再入院**——[卡鲁阿纳等人。艾尔。，2015 年](http://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf)**

****【12】“我为什么要相信你？”解释任何分类器的预测** — [里贝罗等人。艾尔。，2016 年](https://arxiv.org/pdf/1602.04938.pdf)**

****【13】停止解释高风险决策的黑盒机器学习模型，转而使用可解释的模型** — [Rudin，2019](https://arxiv.org/pdf/1811.10154.pdf)**

****【14】神经网络的解释是脆弱的**——[Ghorbani 等人。艾尔。，2019](https://arxiv.org/pdf/1710.10547.pdf)**

****【15】可视化深度神经网络决策:预测差异分析** — [Zintgraf 等。艾尔。，2017](https://arxiv.org/pdf/1702.04595.pdf)**

****【16】显著图的健全性检查**——[阿德巴约等人。艾尔。，2018](https://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps.pdf)**

**【2017】解释模型预测的统一方法——[伦德伯格和李，2017](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)**

****【18】特征归因之外的可解释性:用概念激活向量进行定量测试(TCAV)**—[Kim et al .艾尔。，2018](https://arxiv.org/pdf/1711.11279.pdf)**

****【19】不打开黑盒的反事实解释:自动化决策和 GDPR**——[沃希特等。艾尔。，2018](https://arxiv.org/pdf/1711.00399.pdf)**

****【20】线性分类中的可诉追索权** — [Ustun 等。艾尔。，2018](https://arxiv.org/pdf/1809.06514.pdf)**

****【21】黑箱模型的因果解释**——[赵、哈斯蒂，2018](https://web.stanford.edu/~hastie/Papers/pdp_zhao.pdf)**

****【22】学习具有成本效益且可解释的治疗制度**——[Lakkaraju 和 Rudin，2017 年](http://proceedings.mlr.press/v54/lakkaraju17a/lakkaraju17a.pdf)**

****【23】人在回路的可解释性优先** — [拉赫等人。艾尔。，2018](https://arxiv.org/pdf/1805.11571.pdf)**

****【24】黑盒模型的忠实可定制解释**——[Lakkaraju 等人。艾尔。，2019](https://web.stanford.edu/~himalv/customizable.pdf)**

****【25】通过影响函数理解黑箱预测** — [Koh 和梁，2017](https://arxiv.org/pdf/1703.04730.pdf)**

****【26】简单创造不公平:对公平、刻板印象和可解释性的影响** — [Kleinberg 和 Mullainathan，2019](https://arxiv.org/pdf/1809.04578.pdf)**

****【27】通过深度可视化理解神经网络**——[约辛斯基等，2015](https://arxiv.org/abs/1506.06579)**

****【28】深入卷积网络内部:可视化图像分类模型和显著图** — [Simonyan、Vedaldi 和 Zisserman，2014 年](https://arxiv.org/pdf/1312.6034.pdf)**

****【29】多方面特征可视化:揭示深度神经网络中每个神经元学习的不同类型特征**——[Nguyen，Yosinski，和 Clune，2016](https://arxiv.org/pdf/1602.03616.pdf)**

****【30】****人工智能中的解释:来自社会科学的洞见** — [提姆·米勒，2017](https://doi.org/10.1016/j.artint.2018.07.007)**

****【31】例子还不够，学会批判！对可解释性的批评—** [Kim，Been，Rajiv Khanna，和 Oluwasanmi O. Koyejo，2016 年](https://dl.acm.org/doi/10.5555/3157096.3157352)**

****【32】黑匣子里面是什么？律师和研究人员面临的人工智能挑战**——[Ronald Yu 和 Gabriele Spina Ali，2019](https://www.cambridge.org/core/journals/legal-information-management/article/whats-inside-the-black-box-ai-challenges-for-lawyers-and-researchers/8A547878999427F7222C3CEFC3CE5E01/core-reader)**