<html>
<head>
<title>ONNX: Easily Exchange Deep Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ONNX:轻松交流深度学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/onnx-easily-exchange-deep-learning-models-f3c42100fd77?source=collection_archive---------26-----------------------#2020-09-15">https://towardsdatascience.com/onnx-easily-exchange-deep-learning-models-f3c42100fd77?source=collection_archive---------26-----------------------#2020-09-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a1bc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">ONNX(开放式神经网络交换格式)及其潜在应用的实践演练</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e17636a43f5167a65dd7bb98b1c659fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*inljZli6_6SNIVKX"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@noodlekim?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">面条金姆</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="kz la l"/></div></figure><h1 id="4f30" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">介绍</h1><p id="0a57" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><a class="ae ky" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX(开放式神经网络交换格式)</a>是一种旨在表示任何类型的机器学习和深度学习模型的格式。</p><p id="1791" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">支持框架的一些例子有:PyTorch、TensorFlow、Keras、SAS、Matlab 等等。这样，ONNX 可以更容易地将模型从一个框架转换到另一个框架。此外，使用 ONNX.js，我们可以轻松地在线部署任何以 ONNX 格式保存的模型。</p><p id="b550" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">在图 1 中，使用 ONNX.js 在线部署了一个简单的变型 Autoencoder PyTorch 模型示例，以便按需进行推理。在我的个人网站上的<a class="ae ky" href="https://pierpaolo28.github.io/Projects/ONNX/home.html" rel="noopener ugc nofollow" target="_blank">链接可以找到这种部署模式的完整工作示例。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/72095b6f04763a34593c186b91b19717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*w7zv_HHG8lS5r2-U7wlzAQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:使用 ONNX.js 的在线 VAE</p></figure><p id="e861" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">ONNX 的主要目标是将所有不同的人工智能框架聚集在一起，并尽可能容易地使它们相互通信，以便构建可以在任何类型的平台或硬件上支持的更好的模型。</p><h1 id="af17" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">将模型转换为 ONNX</h1><p id="ca9c" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">将模型转换成 ONNX 格式相对容易。我们所要做的就是，确保我们的训练模型处于评估模式，并创建一个简单的虚拟输入，其形状与我们的模型预期的相同。</p><p id="e918" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">下面是 PyTorch 中的一个简单示例。在这个简单的例子中，我们实例化了一个变化的 Autoencoder 模型，加载其预训练的权重，将其置于评估模式并创建一个示例输入。使用这些参数，我们可以创建 ONNX 文件。</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="62a2" class="na lc it mw b gy nb nc l nd ne">import torch</span><span id="5ce6" class="na lc it mw b gy nf nc l nd ne">pre_trained = VAE(encoder, decoder)<br/>pre_trained.load_state_dict(torch.load('trained.pt'))<br/>pre_trained.eval()</span><span id="f44e" class="na lc it mw b gy nf nc l nd ne">ex_input = torch.zeros(1, 28, 28)</span><span id="95ac" class="na lc it mw b gy nf nc l nd ne">torch.onnx.export(pre_trained, ex_input, "onnx_model.onnx")</span></pre><p id="cd7e" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">然后，我们的 ONNX 文件将由我们的模型的图形表示构成，该图形表示可用于将我们的模型转换为其他类型的框架(通过重新创建图形中的指令)或使用我们训练的模型进行在线推断。ONNX 目前的一个局限是，并不是所有的操作(如定制损失函数、特定的神经网络层等)都被所有的框架所支持。PyTorch 支持的所有操作符列表可在<a class="ae ky" href="https://pytorch.org/docs/stable/onnx.html" rel="noopener ugc nofollow" target="_blank">链接中找到。</a></p><p id="4520" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">在 ONNX 库的开发过程中，开发了不同的操作集版本(<strong class="lv iu"><em class="ng">【op set _ version】</em></strong>)。因此，通过指定我们最喜欢的<strong class="lv iu"> <em class="ng"> opset_version </em> </strong>作为导出函数的参数，我们可以决定我们想要默认可用的操作集。</p><h1 id="d668" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">使用 ONNX.js 部署模型</h1><p id="b188" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">拥有在线模型的一些好处是:</p><ul class=""><li id="e3b4" class="nh ni it lv b lw mp lz mq mc nj mg nk mk nl mo nm nn no np bi translated">我们减少了延迟，因为数据不必从服务器来回发送。</li><li id="e858" class="nh ni it lv b lw nq lz nr mc ns mg nt mk nu mo nm nn no np bi translated">不向服务器发送数据也增加了隐私，因为用户数据不会被远程访问。</li><li id="d2a6" class="nh ni it lv b lw nq lz nr mc ns mg nt mk nu mo nm nn no np bi translated">网站可以只用静态文件来建立，这使得可扩展性不成问题。</li></ul><p id="5793" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">有不同的技术可用于在线部署模型，如 Tensorflow.js 和 ONNX.js。使用 Tensorflow.js 的主要优势之一是可以在线训练模型(使用 ONNX.js 则不是可用选项)。另一方面，ONNX.js 比 Tensorflow.js 更高效，因此在进行在线推理时速度更快。</p><p id="85a3" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">下面显示了一个简单的模板，可以用来开始使用 ONNX.js。在这种情况下，假设存在一个名为<strong class="lv iu"> <em class="ng"> getInputs() </em> </strong>的函数能够自动为我们的模型创建输入数据。</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="104b" class="na lc it mw b gy nb nc l nd ne">&lt;html&gt;<br/>  &lt;head&gt; &lt;/head&gt;<br/>  &lt;body&gt;<br/>    &lt;!-- Loading ONNX.js library --&gt;<br/>    &lt;script      src="https://cdn.jsdelivr.net/npm/onnxjs/dist/onnx.min.js"&gt;<br/>    &lt;/script&gt;<br/>    &lt;script&gt;<br/>      // Creating a session<br/>      const sess = new onnx.InferenceSession();<br/>      // Loading the created ONNX file<br/>      sess.loadModel("onnx_model.onnx").then(() =&gt; {<br/>      // Getting an imput for our model from an example function <br/>      const input_data = getInputs();<br/>      // Feeding the input to our model and fetching the output<br/>      sess.run(input_data).then((output) =&gt; {<br/>      // Storing and displaying the output prediction <br/>      const outputTensor = output.values().next().value;<br/>      console.log(`Model prediction: ${outputTensor.data}.`);<br/>        });<br/>      });<br/>    &lt;/script&gt;<br/>  &lt;/body&gt;<br/>&lt;/html&gt;</span></pre><p id="a362" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">从这个示例模板开始，使用标准的 Javascript 功能，可以开发复杂的应用程序。</p><p id="19fb" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">如果您正在寻找创建模型、将其转换为 ONNX 然后在线部署的完整示例，可以在<a class="ae ky" href="https://github.com/pierpaolo28/Artificial-Intelligence-Projects/tree/master/Online%20Learning/ONNX" rel="noopener ugc nofollow" target="_blank"> this my Github 资源库中找到。</a>此外，使用 ONNX.js 部署的示例模型集合可在 ONNX.js 官方网站<a class="ae ky" href="https://microsoft.github.io/onnxjs-demo/#/" rel="noopener ugc nofollow" target="_blank">此链接获得。</a></p><h1 id="3f14" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">联系人</h1><p id="e376" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">如果你想了解我最新的文章和项目<a class="ae ky" href="https://medium.com/@pierpaoloippolito28?source=post_page---------------------------" rel="noopener">，请通过媒体</a>关注我，并订阅我的<a class="ae ky" href="http://eepurl.com/gwO-Dr?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">邮件列表</a>。以下是我的一些联系人详细信息:</p><ul class=""><li id="8bc7" class="nh ni it lv b lw mp lz mq mc nj mg nk mk nl mo nm nn no np bi translated">领英</li><li id="f9a9" class="nh ni it lv b lw nq lz nr mc ns mg nt mk nu mo nm nn no np bi translated"><a class="ae ky" href="https://pierpaolo28.github.io/blog/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人博客</a></li><li id="4c55" class="nh ni it lv b lw nq lz nr mc ns mg nt mk nu mo nm nn no np bi translated"><a class="ae ky" href="https://pierpaolo28.github.io/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人网站</a></li><li id="8e1c" class="nh ni it lv b lw nq lz nr mc ns mg nt mk nu mo nm nn no np bi translated"><a class="ae ky" href="https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------" rel="noopener" target="_blank">中型简介</a></li><li id="6c31" class="nh ni it lv b lw nq lz nr mc ns mg nt mk nu mo nm nn no np bi translated"><a class="ae ky" href="https://github.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="e2e5" class="nh ni it lv b lw nq lz nr mc ns mg nt mk nu mo nm nn no np bi translated"><a class="ae ky" href="https://www.kaggle.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">卡格尔</a></li></ul></div></div>    
</body>
</html>