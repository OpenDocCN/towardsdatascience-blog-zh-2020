# 构建决策树以及如何处理过度拟合

> 原文：<https://towardsdatascience.com/construct-a-decision-tree-and-how-to-deal-with-overfitting-f907efc1492d?source=collection_archive---------12----------------------->

![](img/15d0cd9d21df5480143e9aaa9acabb44.png)

# 介绍

决策树是一种用于监督学习的算法。它使用树结构，其中有两种类型的节点:决策节点和叶节点。决策节点通过对某个特征提出布尔问题，将数据分成两个分支。一个叶节点代表一个类。训练过程是关于在具有特定值的特定特征上找到“最佳”分割。预测过程是通过在路径上的每个决策节点回答问题从根节点到达叶节点。

# 基尼杂质与熵

术语“最佳”分裂是指在分裂之后，两个分支比任何其他可能的分裂更“有序”。我们如何定义更有序？这取决于我们选择哪个指标。一般来说，有两种类型的指标:基尼系数和熵。这些指标越小，数据集就越“有序”。

这两个指标之间的差异非常微妙。要了解更多，你可以阅读这篇[帖子](https://buk.io/190.2.1.3.2.1.4.210)。在大多数应用中，这两种度量的表现相似。下面是计算每个指标的代码。

# 建造这棵树

培训过程实质上就是构建树。关键的一步是确定“最佳”分割。过程如下:我们尝试在每个特征的每个唯一值处分割数据，并选择产生最少无序的最佳值。现在让我们把这个过程翻译成 Python 代码。

在构建树之前，让我们定义决策节点和叶节点。决策节点指定将根据其进行分割的要素和值。它还指向它的左右子节点。叶节点包括一个类似于`Counter`对象的字典，显示每个类有多少训练样本。这对于计算训练的准确度是有用的(尽管这不是必需的，因为我们可以在模型被训练后通过预测每个例子来获得准确度)。此外，它还会导致到达该叶子的每个示例的结果预测。

给定它的结构，通过递归构造树是最方便的。递归的出口是一个叶节点。当我们无法通过拆分来提高数据的纯度时，就会出现这种情况。如果我们能找到一个“最佳”的分割，这就变成了一个决策节点。接下来，我们对它的左右孩子递归地做同样的事情。

# 预言；预测；预告

现在我们可以通过遍历树直到一个叶节点来预测一个例子。

事实证明，训练准确率为 100%，决策边界看起来很奇怪！显然，模型过度拟合了训练数据。好吧，如果你想一想，如果我们继续分裂直到数据集不能更纯，决策树将会过度拟合数据。换句话说，如果我们不停止分裂，模型将正确地分类每一个例子！训练准确率是 100%(除非有完全相同特征的不同类的例子)，没有意外。

![](img/ccaac4128b7558ec78daead480079322.png)

# 如何应对过度拟合？

从上一节，我们知道了决策树过度拟合的幕后原因。为了防止过度拟合，有两种方法:1 .我们在某个点停止分裂树。2.我们先生成一棵完整的树，然后去掉一些分支。我将用第一种方法作为例子。为了更早的停止分裂，我们需要引入两个超参数进行训练。它们是:树的最大深度和叶子的最小尺寸。让我们重写树构建部分。

现在，我们可以重新训练数据并绘制决策边界。

![](img/669df633436ed1a655184b157583ca56.png)

# 想象这棵树

接下来，我们将通过打印节点来可视化决策树。节点的缩进与其深度成比例。

```
|---feature_1 <= 1.87
|   |---feature_1 <= -0.74
|   |   |---feature_1 <= -1.79
|   |   |   |---feature_1 <= -2.1
|   |   |   |   |---Class: 2
|   |   |   |---feature_1 > -2.1
|   |   |   |   |---Class: 2
|   |   |---feature_1 > -1.79
|   |   |   |---feature_0 <= 1.62
|   |   |   |   |---feature_0 <= -1.31
|   |   |   |   |   |---Class: 2
|   |   |   |   |---feature_0 > -1.31
|   |   |   |   |   |---feature_1 <= -1.49
|   |   |   |   |   |   |---Class: 1
|   |   |   |   |   |---feature_1 > -1.49
|   |   |   |   |   |   |---Class: 1
|   |   |   |---feature_0 > 1.62
|   |   |   |   |---Class: 2
|   |---feature_1 > -0.74
|   |   |---feature_1 <= 0.76
|   |   |   |---feature_0 <= 0.89
|   |   |   |   |---feature_0 <= -0.86
|   |   |   |   |   |---feature_0 <= -2.24
|   |   |   |   |   |   |---Class: 2
|   |   |   |   |   |---feature_0 > -2.24
|   |   |   |   |   |   |---Class: 1
|   |   |   |   |---feature_0 > -0.86
|   |   |   |   |   |---Class: 0
|   |   |   |---feature_0 > 0.89
|   |   |   |   |---feature_0 <= 2.13
|   |   |   |   |   |---Class: 1
|   |   |   |   |---feature_0 > 2.13
|   |   |   |   |   |---Class: 2
|   |   |---feature_1 > 0.76
|   |   |   |---feature_0 <= -1.6
|   |   |   |   |---Class: 2
|   |   |   |---feature_0 > -1.6
|   |   |   |   |---feature_0 <= 1.35
|   |   |   |   |   |---feature_1 <= 1.66
|   |   |   |   |   |   |---Class: 1
|   |   |   |   |   |---feature_1 > 1.66
|   |   |   |   |   |   |---Class: 1
|   |   |   |   |---feature_0 > 1.35
|   |   |   |   |   |---Class: 2
|---feature_1 > 1.87
|   |---Class: 2
```

# 非线性特征

上面的例子清楚地显示了决策树的一个特征:决策边界在特征空间中是线性的。虽然该树能够对不可线性分离的数据集进行分类，但它严重依赖于训练数据的质量，并且其准确性在决策边界附近会降低。解决这个缺点的一个方法是特征工程。类似于逻辑回归中的例子，我们可以扩展特征以包括非线性项。如果我们添加像***【x1 x2】******x1***和 ***x2*** 这样的术语，这里有一个例子。

![](img/2ba7409c291c0027c73e4fa015e5d6a5.png)

# 结论

与其他回归模型不同，决策树不使用正则化来对抗过度拟合。相反，它采用了树木修剪。选择正确的超参数(树深度和叶子大小)也需要实验，例如使用超参数矩阵进行交叉验证。

关于完整的工作流程，包括数据生成和绘制决策边界，您可以访问 my [github](https://github.com/szjunma/ML-Algorithm-with-Python/blob/master/decision_tree/decision_tree.ipynb) 。