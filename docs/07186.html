<html>
<head>
<title>Stochastic Gradient Descent for machine learning clearly explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的随机梯度下降解释清楚</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11?source=collection_archive---------49-----------------------#2020-06-01">https://towardsdatascience.com/stochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11?source=collection_archive---------49-----------------------#2020-06-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6ec5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">随机梯度下降是当今大规模机器学习问题的标准优化方法。它用于从逻辑回归到人工神经网络的各种模型的训练。在本文中，我们将用线性回归来说明梯度下降和随机梯度下降的基本原理。</h2></div><h1 id="3ecf" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">形式化我们的机器学习问题</strong></h1><p id="2552" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">正如你可能知道的，监督机器学习在于找到一个函数，称为决策函数，它最好地模拟数据的输入/输出对之间的关系。为了找到这个函数，我们必须将这个学习问题公式化为一个最优化问题。</p><p id="54b3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">让我们考虑以下任务:找到将输入空间、变量<em class="mb"> X </em>映射到输出空间、变量<em class="mb"> Y </em>的最佳线性函数。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/51f517d42c4edd2e24c95b76b0ac750a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*-ocWvJwIpH47Xo4ygiT1uw.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">Y 变量对 X 变量的绘图(图片作者:<a class="ae mo" href="https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub 链接</a>)</p></figure><p id="1118" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">当我们试图通过线性函数来模拟<em class="mb"> X </em>和<em class="mb"> Y </em>之间的关系时，允许学习算法选择的函数集合如下:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mp"><img src="../Images/90fa1cc8171740c1b9a516064d27a1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gDmzF0ByeQUHTtVBmEL7AA.png"/></div></div></figure><p id="d345" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">术语<em class="mb"> b </em>就是截距，在机器学习中也叫<strong class="lc iu">偏差</strong>。<br/>这组函数就是我们的<strong class="lc iu">假设空间</strong>。但是我们如何选择参数 a，b 的值，以及我们如何判断这是否是一个好的猜测？</p><p id="7bae" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们定义了一个叫做<strong class="lc iu">损失函数</strong>的函数，它在结果<em class="mb"> Y. </em>的上下文中评估我们的选择</p><p id="8e4b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们将损失定义为平方损失(我们可以选择另一个损失函数，如绝对损失) :</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mu"><img src="../Images/4c128fcf5fe2a4a07d21babd191eb832.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KT0UzvcI83kEL0Pmbj-trg.png"/></div></div></figure><p id="201a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">平方损失惩罚实际<em class="mb"> y </em>结果和通过选择参数组<em class="mb"> a、b </em>的值估计的结果之间的差异。这个损失函数在单个点上评估我们的选择，但是我们需要在所有的训练点上评估我们的决策函数。</p><p id="ab0a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">因此，我们计算误差平方的平均值:<strong class="lc iu">均方误差</strong>。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mv"><img src="../Images/f019cdebae23ba1e100354fd9ef6acb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uyRy2s21wqaMo4j6ETKfSg.png"/></div></div></figure><p id="03ea" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">其中<em class="mb"> n </em>是数据点的数量。<br/>该函数取决于定义我们假设空间的参数，称为<strong class="lc iu">经验风险</strong>。</p><p id="689d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="mb"> Rn(a，b) </em>是参数的二次函数，因此其最小值总是存在，但可能不是唯一的。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mw"><img src="../Images/a12edd6327c3cf84336e8be12df0bef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mFTQALf2VzL-G-Y4Py0aaw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">(图片作者:<a class="ae mo" href="https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub 链接</a>)</p></figure><p id="44f3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">最终，我们达到了最初的目标:<strong class="lc iu">将学习问题公式化为优化问题！</strong></p><p id="5216" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">事实上，我们所要做的就是找到决策函数，即<em class="mb"> a，b </em>系数，使这种经验风险最小化。</p><p id="84ed" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这将是我们可能产生的最好的决策函数:我们的<strong class="lc iu">目标函数</strong>。</p><p id="54e1" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在简单线性回归的情况下，我们可以简单地对经验风险进行微分，并计算<em class="mb"> a，b </em>系数来抵消衍生产品。用矩阵符号来计算这个解更容易。方便的是将常量变量 1 包含在<em class="mb"> X </em>中，并将参数<em class="mb"> a </em>和<em class="mb"> b </em>写成单个向量<em class="mb"> β。</em>因此，我们的线性模型可以写成:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mx"><img src="../Images/b9661b7af5f2f00a51a7194e2a06e8f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2aVDGkBSQtyvCsNx256aTA.png"/></div></div></figure><p id="e0e7" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们的损失函数变成了:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi my"><img src="../Images/aaf96b9047a405c43f11ebdf56eb9160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T0cCYoq0Tvm9updnvXtp4w.png"/></div></div></figure><p id="35c5" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">最小化我们的等式的向量<em class="mb">β</em>可以通过求解下面的等式来找到:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mz"><img src="../Images/91519c364d8d849ab86d91f141c0e0e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O5CCuLiZzpvhhZmE9h1SHQ.png"/></div></div></figure><p id="b40d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们的线性回归只有两个预测器(<em class="mb"> a </em>和<em class="mb"> b </em>，这样<strong class="lc iu"> <em class="mb"> X </em> </strong>就是一个<strong class="lc iu"><em class="mb">n</em>X<em class="mb">2</em></strong><em class="mb"/>矩阵(其中<em class="mb"> n </em>是观测值的个数，2 是预测值的个数)。如你所见，要解这个方程，我们需要计算矩阵<strong class="lc iu">(<em class="mb">x^t</em><em class="mb">x)</em></strong>然后求逆。</p><p id="2831" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在机器学习中，观察值的数量通常非常多，预测值的数量也非常多。因此，该操作在计算和存储方面非常昂贵。</p><p id="9c6e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">梯度下降算法是一种迭代优化算法，它允许我们在保持低计算复杂度的同时找到解决方案。我们将在本文的下一部分描述它是如何工作的。</p><h1 id="d64b" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">潜入梯度下降原理</h1><p id="948f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">梯度下降算法可以用下面的类比来说明。想象一下，你半夜在山里迷路了。你什么也看不见，因为一片漆黑，你想回到位于谷底的村庄(你试图找到均方误差函数的局部/全局最小值)。为了生存，你制定了以下策略:</p><ol class=""><li id="f9f9" class="na nb it lc b ld lw lg lx lj nc ln nd lr ne lv nf ng nh ni bi translated">在你当前的位置，你感觉到了山的陡度，找到了坡度最陡的方向。最陡的斜率对应于均方误差的梯度。</li><li id="8ec8" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">你沿着这个方向下山，走一段固定的距离，然后停下来检查你是否还在正确的方向上。这个固定距离就是梯度下降算法的学习速率。如果你走得太久，你可能会错过这个村庄，最终会在山谷另一边的斜坡上。如果你走得不够，到达村庄将需要很长时间，并且有陷入小洞的风险(当地的最小值)。</li><li id="16a0" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">你重复这些步骤，直到满足你设定的标准:例如，两个步骤之间的高度差非常低。</li></ol><p id="e442" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">最终，你会到达谷底，或者你会陷入局部最小值…</p><p id="2a39" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">既然你已经用这个寓言理解了原理，那就让我们深入到梯度下降算法的数学中去吧！<br/>为了找到使均方误差最小的<em class="mb"> a </em>、<em class="mb"> b </em>参数，该算法可以实现如下:</p><ol class=""><li id="26c2" class="na nb it lc b ld lw lg lx lj nc ln nd lr ne lv nf ng nh ni bi translated">初始化<em class="mb"> a </em>和<em class="mb"> b </em>的值，例如<em class="mb"> a </em> =200，b=-200</li><li id="8e5b" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">计算相对于<em class="mb"> a </em>和<em class="mb"> b </em>的均方误差的梯度。坡度是当前位置最陡坡度的方向。</li></ol><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi no"><img src="../Images/9082f03ca5e413f19fb88435d29c48a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4nElTDH-oSn4tsCoUoL5A.png"/></div></div></figure><p id="5040" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">然后通过减去梯度乘以步长来更新<em class="mb"> a </em>和<em class="mb"> b </em>的值:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi np"><img src="../Images/3a4897ff56fecec0bd155721f457ade8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hV-NBO4Rnk2xzvaZZ4Rz-A.png"/></div></div></figure><p id="0fa7" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">用<em class="mb"> η </em>，我们的固定步长。</p><p id="5e97" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">用<em class="mb"> a </em>和<em class="mb"> b </em>的更新值计算均方损失。</p><p id="762d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">重复这些步骤</strong>，直到满足停止标准。例如，均方损失的减少低于阈值<em class="mb"> ϵ.</em></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mw"><img src="../Images/e6099e037b6885ddbf9a0166cdc98203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BtlBnXa63BcO5yQL8aKpkA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">梯度下降算法图解(图片作者:<a class="ae mo" href="https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub Link </a>)</p></figure><p id="8914" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在下面的动画中，您可以看到由梯度下降算法执行的参数<em class="mb"> a </em>的更新，以及我们的线性回归模型的拟合:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nq"><img src="../Images/9fa6b10566d3a3ab12acc9baef7f98db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*b1zIkuw0XZb2TIgbYrDZEA.gif"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">梯度下降算法的动画(图片作者:<a class="ae mo" href="https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub 链接</a>)</p></figure><p id="3b14" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">由于我们正在拟合具有两个预测值的模型，我们可以在 3D 空间中可视化梯度下降算法过程！</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nq"><img src="../Images/b0a866da9fa2456c45e28c71d04ae6ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*TloU2nSZduwncOre0ECY1A.gif"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">渐变下降 3D 动画(图片作者:<a class="ae mo" href="https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub 链接</a>)</p></figure><h1 id="969e" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">梯度下降:这会扩展到大数据吗？</strong></h1><p id="d494" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在梯度下降算法的每次迭代中，我们必须查看所有的训练点来计算梯度。</p><p id="4411" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">因此，该算法的时间复杂度为 O(n)。计算一个非常大的数据集需要很长时间。也许我们可以计算梯度的估计值，而不是查看所有的数据点:这种算法被称为<strong class="lc iu">迷你批次梯度下降</strong>。</p><p id="24a2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">小批量梯度下降包括使用大小为<em class="mb"> N </em>的随机子集来确定每次迭代的步进方向。</p><ul class=""><li id="54bc" class="na nb it lc b ld lw lg lx lj nc ln nd lr ne lv nr ng nh ni bi translated">对于一个大的数据子集，我们得到一个更好的梯度估计，但算法较慢。</li><li id="b0c4" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nr ng nh ni bi translated">对于一个小的数据子集，我们得到一个更差的梯度估计，但算法计算解决方案更快。</li></ul><p id="4fa6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果我们使用一个大小为<em class="mb"> N=1 </em>的随机子集，称之为<strong class="lc iu">随机梯度下降</strong>。这意味着我们将使用一个随机选择的点来确定步进方向。</p><p id="7b84" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在下面的动画中，蓝线对应随机梯度下降，红线是基本梯度下降算法。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nq"><img src="../Images/e96cee18920d45232add45f89ad83ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ngDh4v1MmXtCALwTmLkDFg.gif"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">随机梯度下降 3D 动画(图片作者:<a class="ae mo" href="https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub 链接</a>)</p></figure><p id="2a3b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我希望这篇文章能帮助你理解这个基本的优化算法，如果你喜欢它，或者如果你有任何问题，请不要犹豫发表评论！</p><p id="cee4" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">你可以在我的 GitHub 上找到我做的实现随机梯度下降和创建动画的代码:<a class="ae mo" href="https://github.com/baptiste-monpezat/stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/baptiste-monpezat/random _ gradient _ descent</a>。</p><p id="7101" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">你也可以在我的博客上找到原帖:<a class="ae mo" href="https://baptiste-monpezat.github.io/blog/stochastic-gradient-descent-for-machine-learning-clearly-explained" rel="noopener ugc nofollow" target="_blank">https://baptiste-monpezat . github . io/blog/random-gradient-descent-for-machine-learning-clearly-explained</a></p></div></div>    
</body>
</html>