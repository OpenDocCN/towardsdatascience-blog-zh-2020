<html>
<head>
<title>How powerful can an ensemble of linear models be?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性模型的集合能有多强大？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-powerful-can-an-ensemble-of-linear-models-be-231824de50e1?source=collection_archive---------35-----------------------#2020-05-28">https://towardsdatascience.com/how-powerful-can-an-ensemble-of-linear-models-be-231824de50e1?source=collection_archive---------35-----------------------#2020-05-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="496b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一组线性模型如何在Kaggle上进入Mercari价格预测挑战排行榜的前6%。</h2></div><p id="13c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated">随着近年来深度学习算法的快速增长，今天它们已经成为人工智能的最先进水平。这让我想知道传统和老式的机器学习技术，如线性回归，支持向量机等，是否仍然足够体面，可以与深度学习技术相抗衡？<br/>为了检查这些经常被忽视的机器学习技术的能力，我将只使用传统的机器学习技术(没有神经网络)来解决一个Kaggle竞赛问题。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/0dd2223623fb839f7384596138c8e505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QLU58lo_X3qE_HLb70MTmg.png"/></div></div></figure><blockquote class="ma mb mc"><p id="0837" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">注意:我将在这个项目中使用python 3.7。</strong></p></blockquote><h2 id="f4d4" class="mg mh it bd mi mj mk dn ml mm mn dp mo kr mp mq mr kv ms mt mu kz mv mw mx my bi translated">博客的鸟瞰图-</h2><p id="9a52" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">该项目分为6个主要步骤-</p><ul class=""><li id="2371" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">业务问题和评估指标</li><li id="d432" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">关于数据</li><li id="3f64" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">探索性数据分析</li><li id="7bd9" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">数据预处理</li><li id="f7bd" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">建模</li><li id="91fc" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">从Kaggle排行榜获取分数。</li></ul></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="fa55" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di"> B </span> <strong class="kk iu"> <em class="ln">业务问题及评估指标</em> </strong></p><p id="58dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">很难知道一件东西到底值多少钱。小细节可能意味着价格上的巨大差异。例如，其中一件毛衣售价335美元，另一件售价9.99美元。你能猜出哪个是哪个吗？</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nz"><img src="../Images/947ba726b082ef96b12e2549213d9f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KR9rv6UCu2XR90ldot_R0g.png"/></div></div></figure><p id="d4df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑到有多少产品在网上销售，产品定价在规模上变得更加困难。服装有很强的季节性定价趋势，并受品牌名称的影响很大，而电子产品的价格根据产品规格而波动。日本最大的社区购物应用Mercari深谙这个问题。他们希望向卖家提供定价建议，但这很难，因为他们的卖家可以在Mercari的市场上出售任何东西或任何一捆东西。<br/>在这场竞赛中，我们需要构建一个算法，自动建议正确的产品价格。我们将提供产品的文本描述，以及包括产品类别名称，品牌名称和项目条件等细节的功能。</p><p id="f6fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本次比赛的评价指标为<a class="ae oa" href="https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError" rel="noopener ugc nofollow" target="_blank">均方根对数误差</a>。RMSLE的计算方法如下:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ob"><img src="../Images/5b3b3420a8e379707de648bca3ddf10f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tZY-2x2IdDbbBoVIKxwQHA.png"/></div></div></figure><p id="7130" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中:<br/><em class="ln"/>是RMSLE值(得分)<br/> <em class="ln"> n </em>是(公共/私有)数据集中的观察总数，<br/> <em class="ln"> pi </em>是价格的预测，<br/> <em class="ln"> ai </em>是<em class="ln"> i </em>的实际销售价格。<br/> <em class="ln"> log(x) </em>是x的自然对数</p><p id="9cd7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="ln">注意，由于这种数据的公共性质，这种竞争是一种“只有内核”的竞争。因此，我们需要构建一个代码，在一台拥有16 GB RAM和4个CPU的机器上一小时内执行。</em>T3】</strong></p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="fb36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di">一个</span> <strong class="kk iu"> <em class="ln">回合的数据</em> </strong></p><p id="1422" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用的数据由Mercari提供，可以通过<a class="ae oa" href="https://www.kaggle.com/c/mercari-price-suggestion-challenge/data" rel="noopener ugc nofollow" target="_blank">这个</a>链接在Kaggle上找到。这些数据列出了Mercari网站上产品的详细信息。<br/>让我们看看网站上的一款产品，以及它在数据集中是如何描述的。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oc"><img src="../Images/d57ebad459c7516b20503e10b5eb0713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F_qskp-MUrFcMys310zPng.jpeg"/></div></div><p class="od oe gj gh gi of og bd b be z dk translated"><a class="ae oa" href="https://www.mercari.com/" rel="noopener ugc nofollow" target="_blank">https://www.mercari.com/</a></p></figure><p id="e9e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">数据集有8个特征:</strong></p><ul class=""><li id="595d" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated"><strong class="kk iu"> Train_id/Test_id: </strong>数据集中的每个项目都有一个唯一的项目id。这将在提交预测价格时使用。</li><li id="db33" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated"><strong class="kk iu">名称:</strong>代表产品名称，为字符串格式。以上产品名称为<em class="ln">【安·兰德源头】</em></li><li id="e91c" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated"><strong class="kk iu">物品状况:</strong>卖家提供的表示物品状况的编号。它可以取1到5之间的值。在我们的例子中，产品的状态是'<em class="ln">好'</em>，所以在数据集中用4表示。</li><li id="66af" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated"><strong class="kk iu">类别名称:</strong>代表物品的类别。对于上面的条目，数据集中提到的类别是<em class="ln">‘其他/书籍/文学&amp;小说’<br/></em>，这个特征的数据类型也是string。</li><li id="8dcc" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated"><strong class="kk iu">品牌名称:</strong>代表该商品所属品牌的名称。上述产品的品牌名称为<em class="ln">‘企鹅兰登书屋’</em>。</li><li id="9ef5" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated"><strong class="kk iu">价格:</strong>表示商品的价格，在我们的例子中，这将是我们需要预测的目标值。单位是美元。以上产品，提供的价格为<em class="ln"> '$9' </em>。</li><li id="af70" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated"><strong class="kk iu">运输:</strong>代表产品可用运输类型的编号。如果运费由卖方支付，运费将为1，如果费用由买方支付，运费将为0。对于上述产品，运输是免费的，因此在数据集中，此功能将为1。</li><li id="0571" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated"><strong class="kk iu">物品描述:</strong>物品的完整描述。对于上述产品，说明中写道，<em class="ln">《源泉》袖珍平装书——安·兰德——百年纪念版——经典文学——书的状况良好，封面和边角有一些磨损(见图片)。</em>该特征已经以预处理的形式出现在所提供的数据集中。</li></ul><p id="7322" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用pandas导入数据，并检查前5个条目。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="edcd" class="mg mh it oi b gy om on l oo op">import pandas as pd<br/>data = pd.read_csv('train.tsv', sep='\t')<br/>df_test = pd.read_csv('test.tsv', sep='\t')<br/>data.head()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oq"><img src="../Images/4fe2aebd473c64b3638886ad5fd88770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*plSzaXxtFbxTSFD5LHTuhQ.png"/></div></div></figure></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="c711" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di">E</span><strong class="kk iu">T3】勘探数据分析(EDA)T5】</strong></p><p id="1828" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这一部分，我们将深入探索和分析数据。我们将逐一介绍这些数据。</p><blockquote class="ma mb mc"><p id="cc06" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">价格</strong></p></blockquote><p id="1004" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我们需要使用其他特征形式的产品信息来预测的目标特征。<br/>让我们使用describe()查看该特性的统计摘要</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="67e3" class="mg mh it oi b gy om on l oo op">data['price'].describe()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e03ae1b7c16c5d56eb146be844ad0d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*P3GA8wWo1S1INdBR6q6Y4w.png"/></div></figure><ul class=""><li id="84ad" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">数据集中大约有148万个产品。最贵的产品定价为2009美元，最便宜的产品定价为3美元，而平均价格为26.75美元</li></ul><p id="185b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们来看看价格直方图。在这里，我使用的箱数是200。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="9005" class="mg mh it oi b gy om on l oo op">plt.hist(data['price'], bins=200)<br/>plt.xlabel('price')<br/>plt.ylabel('frequency')<br/>plt.title('histogram of price')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi os"><img src="../Images/5c524188b6cf1f3b3939c8f712635874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6U-7smRLqG654I2EchjKCw.png"/></div></div></figure><ul class=""><li id="0723" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以观察到分布遵循幂律分布，为了解决这个问题，并使其成为高斯分布，让我们将这些值转换为对数形式，即我们将用log(price+1)替换价格值。</li></ul><p id="e663" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将价格转换为正态分布，因为它是统计学中最著名的分布之一，因为它符合许多自然现象，这使它成为我们可以进行分析的最容易解释的分布之一。将数据转换为正态分布的另一个原因是价格的方差减少了，并且大多数点都以平均值为中心，这使得模型的价格预测更加容易。</p><p id="a607" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我已经把数据转换成了日志形式。下面是日志的直方图(价格+1)。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="becf" class="mg mh it oi b gy om on l oo op">plt.hist(data['price_log'], bins=20)<br/>plt.xlabel('log(price + 1)')<br/>plt.ylabel('frequency')<br/>plt.title('histogram of log of price')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ot"><img src="../Images/df41197b2cf6f7caa472d1c343bd8a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VKo8apbiIIDD5jnnDdcung.png"/></div></div></figure><ul class=""><li id="e9b3" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以观察到，分布现在更容易解释，并试图遵循正态分布。</li><li id="c0c2" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">此外，请注意大多数点是如何以平均值为中心的(平均值接近3)。</li></ul><blockquote class="ma mb mc"><p id="e00a" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">项目条件标识</strong></p></blockquote><p id="2c52" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是表示物品状况的分类特征。让我们使用value_counts()查看更多信息</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="ab68" class="mg mh it oi b gy om on l oo op">data['item_condition_id'].value_counts()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/7a00254977ba3fd6b6dccb5946a3eb2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*od3gkug_DbF-c0qDO3-mJg.png"/></div></figure><ul class=""><li id="9f9b" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">输出告诉我们，这个特性可以占用1到5之间的5个值，并且具有特定条件的项目的数量就在它的旁边。</li></ul><p id="3122" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看这个特性的条形图</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="4296" class="mg mh it oi b gy om on l oo op">sns.barplot(x=data['item_condition_id'].value_counts().keys(),<br/>            y=data['item_condition_id'].value_counts())<br/>plt.xlabel('item condition type')<br/>plt.ylabel('number of products')<br/>plt.title('bar graph of "item condition type"')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ov"><img src="../Images/53f564c703c78d308414b04ded5e4b86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MQqGF0Uqj4rkEMAPez6yUA.png"/></div></div></figure><ul class=""><li id="fe11" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，大多数项目的条件id为1，只有极少数项目的条件id为5。</li></ul><p id="c39a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们比较具有不同item_condition_id的产品的价格分布</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ow"><img src="../Images/09493325155c4ca87dbd19846121a57a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOpvTg4KdoiYUkMpLVPQjA.png"/></div></div></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ox"><img src="../Images/3146e1cfccd1d4d4366d25bc606cb9e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6uTtP9eKjbiS4vQmqaKxcA.png"/></div></div></figure><ul class=""><li id="e902" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，具有不同item_condition_id的项目的价格分布非常相似。</li></ul><p id="1b3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看具有不同item_condition_id的产品的价格分布的箱线图和小提琴图。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="bbbb" class="mg mh it oi b gy om on l oo op"># plotting box-plot<br/>sns.boxplot(x='item_condition_id', y='price_log', data=data)<br/>plt.show()</span><span id="8669" class="mg mh it oi b gy oy on l oo op"># plotting violin plot<br/>sns.violinplot(x='item_condition_id', y='price_log', data=data)<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oz"><img src="../Images/642d38281508e81c0651d04057402c47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QLHsX6AoirpCRaUy3Hqnjw.png"/></div></div></figure><p id="dfec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">箱线图和violin图还告诉我们，具有不同item_condition_id的商品的价格分布差别不大，而且分布有点向右倾斜。item_condition_id = 5的产品具有最高的中间价格，而item_condition_id = 4的产品具有最低的中间价格。大多数产品的价格在1.5英镑到5.2英镑之间</p><blockquote class="ma mb mc"><p id="a4cf" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">类别名称</strong></p></blockquote><p id="d25e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个文本类型的数据，告诉我们产品的类别。<br/>我们来看看特征类别名称的统计汇总-</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="1a33" class="mg mh it oi b gy om on l oo op">data['category_name'].describe()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi os"><img src="../Images/a864b7517fdfb1c486164daf78436b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jfUM2Vybwzar6wGW3QFADw.png"/></div></div></figure><p id="9010" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些是字符串类型的特征，实际上是三个子类别合并成一个。<br/>让我们考虑上面描述中提到的最频繁出现的类别名称特征“女性/运动服装/裤子、紧身衣、打底裤”。它可以分为3个子类别:<br/> -子类别_1:“女性”<br/> -子类别_2:“运动服装”<br/> -子类别_3:“裤子、紧身衣、打底裤”<br/>为了使该特征的可视化更容易，我将考虑该特征的子类别。让我们把数据分成小类。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="c4b6" class="mg mh it oi b gy om on l oo op"># this is to divide the category_name feature into 3 sub categories<br/>from tqdm import tqdm_notebook<br/>sub_category_1 = []<br/>sub_category_2 = []<br/>sub_category_3 = []</span><span id="4a32" class="mg mh it oi b gy oy on l oo op">for feature in tqdm_notebook(data['category_name'].values):<br/>  fs = feature.split('/')<br/>  a,b,c = fs[0], fs[1], ' '.join(fs[2:])<br/>  sub_category_1.append(a)<br/>  sub_category_2.append(b)<br/>  sub_category_3.append(c)</span><span id="949d" class="mg mh it oi b gy oy on l oo op">data['sub_category_1'] = sub_category_1<br/>data['sub_category_2'] = sub_category_2<br/>data['sub_category_3'] = sub_category_3</span></pre><blockquote class="ma mb mc"><p id="de5d" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">子类别_1 </strong></p></blockquote><p id="c468" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们检查一下统计描述:</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="30f4" class="mg mh it oi b gy om on l oo op">data['sub_category_1'].describe()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/ca7be9070d3f3bae750628c7c30ffc63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*CWomypnKmI1peXK8JMfQaQ.png"/></div></figure><ul class=""><li id="4b22" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">在我们的数据中，大约有140万个这样的函数，可以接受11个不同的值。其中最常见的是女性。</li></ul><p id="594c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们绘制子类别1的条形图</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="4bc7" class="mg mh it oi b gy om on l oo op">sns.barplot(x=data['sub_category_1'].value_counts().keys(), y=data['sub_category_1'].value_counts())<br/>plt.ylabel('number of products')<br/>locs, labels = plt.xticks()<br/>plt.setp(labels, rotation=90)<br/>plt.title('bar-plot of sub_category_1')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pb"><img src="../Images/64b9b1dbe08c5e4355db03a83f54056c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-A2A0l68XqYFIKf0dlVt_A.png"/></div></div></figure><ul class=""><li id="935b" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，大多数项目的子类别1为“女性”，最少的项目为“运动和户外”。</li><li id="998c" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">请注意，没有定义sub_category_1的项目用“无标签”表示。</li></ul><p id="55ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们检查sub_category_1的分布和价格日志</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="4a89" class="mg mh it oi b gy om on l oo op">sns.FacetGrid(data, hue="sub_category_1", height=5).map(sns.distplot, 'price_log').add_legend();<br/>plt.title('comparing the log of price distribution of products with<br/>           sub_category_1\n')<br/>plt.ylabel('PDF of log of price')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pc"><img src="../Images/9e87ab12efeee6ed4d7f7dee9082fbfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vj-JoEqTsgORuijWVnSkg.png"/></div></div></figure><ul class=""><li id="de7f" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，大多数分布都是右偏的，只有一点差异。</li><li id="9cf5" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">“手工制作”子类别略有不同，因为我们可以看到该类别中的一些产品的log(价格)低于2</li></ul><p id="b006" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们来看看子_类别_1的小提琴情节</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pd"><img src="../Images/962bd7ea3a58e4a5aacfe2eb494368fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d7kLtI9HxuPd1h5G6mgqTA.png"/></div></div></figure><ul class=""><li id="a405" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">查看小提琴图，我们可以说，子类别1为“男士”的商品的销售往往更贵，而子类别1为“手工”的商品往往更经济。</li></ul><blockquote class="ma mb mc"><p id="54cc" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">子类别2 </strong></p></blockquote><p id="5f92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们检查sub_category_2的统计描述:</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="90b9" class="mg mh it oi b gy om on l oo op">data['sub_category_2'].describe()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/02ddf3d0b77c82cd7b185e55ac4213e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*GHKDffRHGzbx6gyGbyl0CQ.png"/></div></figure><ul class=""><li id="894f" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">sub_category_2有114个不同的值，让我们分析一下sub_category_2的前20个类别。</li></ul><p id="9a44" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">sub_category_2中前20个类别的条形图</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="c515" class="mg mh it oi b gy om on l oo op">plt.figure(figsize=(12,8))<br/>sns.barplot(x=data['sub_category_2'].value_counts().keys()[:20],<br/>            y=data['sub_category_2'].value_counts()[:20])<br/>plt.ylabel('number of products')<br/>locs, labels = plt.xticks()<br/>plt.setp(labels, rotation=90)<br/>plt.title('bar-plot of top 20 sub_category_2')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pf"><img src="../Images/285718bd629807ac4c5f3e764c0d171c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-WiCIllod2FpJj2z-QV88Q.png"/></div></div></figure><ul class=""><li id="545c" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，大多数商品都有sub_category_2，即“正宗服装”,其次是“化妆品”,然后是“上衣和衬衫”。</li></ul><blockquote class="ma mb mc"><p id="e069" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">子类别3 </strong></p></blockquote><p id="4354" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们检查sub_category_3的统计描述:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pg"><img src="../Images/dd39c04ba84f16a5af907f70c7d62a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G1wl1JPJgrjE1ZldJaKwRQ.png"/></div></div></figure><ul class=""><li id="2eaa" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">sub_category_3有865个不同的值，让我们分析一下sub_category_3的前20个类别的直方图。</li></ul><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ph"><img src="../Images/f6afc0d92a9abbd21f43b2d9e31fff25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eJAARMyWMKv7mNEky4jW0A.png"/></div></div></figure><ul class=""><li id="d005" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，大多数商品都有sub_category_3，如“裤子、紧身衣、打底裤”，后面是“其他”和“面部”。</li></ul><blockquote class="ma mb mc"><p id="cafb" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">品牌名称</strong></p></blockquote><p id="f3a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是另一个文本类型特征，表示产品所属的品牌。让我们来看看特性brand_name的统计摘要。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/348e78ef360915703ee84976820edee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*xts6ErQ_Ke0IivDCUHOThw.png"/></div></figure><ul class=""><li id="f904" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">在这里，我们可以看到共有4089个不同的品牌名称。</li></ul><p id="e0db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看前20个品牌的直方图</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="1808" class="mg mh it oi b gy om on l oo op">plt.figure(figsize=(12,8))<br/>sns.barplot(x=data['brand_name'].value_counts().keys()[:20],<br/>            y=data['brand_name'].value_counts()[:20])<br/>plt.ylabel('number of products')<br/>locs, labels = plt.xticks()<br/>plt.setp(labels, rotation=50)<br/>plt.title('bar-plot of top 20 brands (including products with<br/>           unknown brand)')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pf"><img src="../Images/bd0ee838ea5c2361f07fd5a57ec2fdae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lciLuLRNEM_kynJkkLsIOw.png"/></div></div></figure><ul class=""><li id="a672" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">请注意，这里的“未知”表示没有指定品牌的商品。</li><li id="136a" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">PINK、Nike和Victoria's Secret是网站上商品最多的前三大品牌。</li></ul><p id="d574" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看20大品牌及其平均产品价格的柱状图。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="8a0e" class="mg mh it oi b gy om on l oo op">plt.figure(figsize=(12,8))<br/>sns.barplot(x=df['brand_name'].values[:20],<br/>            y=df['price'].values[:20])<br/>plt.ylabel('average price of products')<br/>locs, labels = plt.xticks()<br/>plt.setp(labels, rotation=50)<br/>plt.title('bar-plot of top 20 brands with their mean product price')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pj"><img src="../Images/6fcc938c3b89c3e62f55aff94da1ed6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XkUZpI9qlRiH_WBCvAjFGw.png"/></div></div></figure><p id="11d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看产品价格最高的前20个品牌的柱状图</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pk"><img src="../Images/470281f19f06bef5659d9efdf4046615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MxZyU5j5VpiG_sLch804KQ.png"/></div></div></figure><blockquote class="ma mb mc"><p id="53dc" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">运输</strong></p></blockquote><p id="4b10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一种数值分类数据类型，可以取2个值，0或1<br/>让我们来看看它的统计描述。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="6051" class="mg mh it oi b gy om on l oo op">data['shipping'].value_counts()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/f1aa1b8a0c94694dc4d59aee1f3550c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*cEWev-hX6tiBJfhvDP4kJQ.png"/></div></figure><ul class=""><li id="60e5" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">发货为0的商品比发货为1的多22%。</li></ul><p id="5135" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来对比一下不同出货的产品价格分布日志。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pm"><img src="../Images/99a578bb30ac65425de95672945c6e44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SfOFnHY5SpS6jhiWBhTTPQ.png"/></div></div></figure><ul class=""><li id="ba4a" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，不同运输的项目的价格分布日志略有差异。</li><li id="7a86" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">运费为1的产品往往价格较低。</li></ul><blockquote class="ma mb mc"><p id="7562" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">项目_描述(正文)</strong></p></blockquote><p id="a813" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是描述产品的文本类型特征。让我们来看看其中的一些。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="e39f" class="mg mh it oi b gy om on l oo op">data['item_description']</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pn"><img src="../Images/f04be3ef71b6b3bf8dd6996a831326f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ho4OUDAoOby4WAyanoUqXQ.png"/></div></div></figure><ul class=""><li id="c9f8" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到这些总共有1482535个。</li></ul><p id="d13f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在执行一些NLP技术后使用这个特性，这些技术将在本博客的后面讨论。<br/>我们可以对该功能做的另一件事是，计算它的字长，即该功能包含每个产品的字数，并对其进行分析。<br/>我们来查看一下物品描述的word_length的统计汇总。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="3dcb" class="mg mh it oi b gy om on l oo op">data['item_description_word_length'].describe()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi po"><img src="../Images/5c4d02c05a0c7d6ca3b1d62f152d31e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YKmRGP0MxxMows0yZyQhKA.png"/></div></div></figure><ul class=""><li id="492f" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到最长的描述有245个字，最短的没有字。平均来说，单词在25个左右</li></ul><p id="3877" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来绘制item_description_word_length的直方图，</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="d816" class="mg mh it oi b gy om on l oo op">plt.hist(data['item_description_word_length'], bins=200)<br/>plt.xlabel('item_description_word_length')<br/>plt.ylabel('frequency')<br/>plt.title('histogram of item_description_word_length')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pp"><img src="../Images/fb4c86e3de6ce04326aeb8036c8743f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fRm22uuLM42OFIh5iSz5ng.png"/></div></div></figure><ul class=""><li id="ebf4" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，字长直方图遵循幂律分布。</li><li id="e0c1" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">这个直方图我用了200个面元。</li></ul><p id="88bf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们通过取单词长度的对数，试着把这个转换成正态分布。这是分布图。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="bda6" class="mg mh it oi b gy om on l oo op">plt.hist(data['log_item_description_word_length'])<br/>plt.xlabel('log(item_description_word_length + 1)')<br/>plt.ylabel('frequency')<br/>plt.title('histogram of log of item_description_word_length')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pq"><img src="../Images/a9b8c22d708b0fc55cd11c479eea01d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zBS-98suZ_EvyBD7yuzqJw.png"/></div></div></figure><ul class=""><li id="1da9" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，这个特征试图遵循正态分布。</li><li id="ea06" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">大多数项目的单词长度在5到20之间。(从antilog获得的值)。</li><li id="ee80" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">我们可以将此作为建模的一个特征。</li></ul><p id="742d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们看看log(item_word_length)如何影响商品的价格</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pr"><img src="../Images/96cbfd19e9a25c3e704b75b68d74a614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9q81XPQaqbcThxuiMpna3Q.png"/></div></div></figure><ul class=""><li id="5e82" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，随着item_word_length从0到50，价格的对数增加，但随后价格趋于下降，除了我们可以在大约190的字长附近观察到的峰值。</li><li id="d4b8" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">此外，单词长度超过100的价格波动更大。</li></ul><blockquote class="ma mb mc"><p id="55b0" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">产品名称</strong></p></blockquote><p id="6132" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，让我们看看最后一个特性，即产品的名称。这也是一个文本类型特征，我们稍后将对其执行NLP，但首先，让我们通过绘制“名称”特征中单词数量的直方图来对其进行一些分析。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="791f" class="mg mh it oi b gy om on l oo op">plt.hist(data['name_length'])<br/>plt.xlabel('name_length')<br/>plt.ylabel('frequency')<br/>plt.title('histogram of name_length')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ps"><img src="../Images/c1d5012e90fb3f55e8bca790f6b0faf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a6FJNlDpETi4rWfojTozrQ.png"/></div></div></figure><ul class=""><li id="15d2" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">这种分布明显是左倾的，最大条目的名称长度大约为25。</li></ul><p id="f08e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看价格是如何随着产品名称的字数而变化的。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="63e2" class="mg mh it oi b gy om on l oo op">df = data.groupby('name_length')['price_log'].mean().reset_index()<br/>plt.figure(figsize=(12,8))<br/>sns.relplot(x="name_length", y="price_log", kind="line", data=df)<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pt"><img src="../Images/9d0a7f0252dee2e41d71ad888d963993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IfkJ_h-YkQIsjapC9RyWMA.png"/></div></div></figure><ul class=""><li id="cd77" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">请注意，我使用的是价格日志，而不是实际价格。</li><li id="04fd" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">我们可以看到，name_length值在10到38之间的分布非常线性，然后有一个急剧的下降和上升。</li></ul></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="14e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di">D</span>T11<em class="ln">数据预处理</em>T14】</p><p id="3526" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这一步中，我们将清理数据，并为建模做好准备。<br/>记住，我们有6个特征，其中有:<br/> - 4个文本特征:名称、描述、品牌名称和类别<br/> - 2个分类特征:运输和商品条件id</p><p id="9144" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从清理文本特征开始，为此，我们将定义一些函数-</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="83a1" class="mg mh it oi b gy om on l oo op">import re<br/>def decontracted(phrase):<br/>    # specific<br/>    phrase = re.sub(r"won't", "will not", phrase)<br/>    phrase = re.sub(r"can\'t", "can not", phrase)<br/>    # general<br/>    phrase = re.sub(r"n\'t", "not", phrase)<br/>    phrase = re.sub(r"\'re", " are", phrase)<br/>    phrase = re.sub(r"\'s", " is", phrase)<br/>    phrase = re.sub(r"\'d", " would", phrase)<br/>    phrase = re.sub(r"\'ll", " will", phrase)<br/>    phrase = re.sub(r"\'t", " not", phrase)<br/>    phrase = re.sub(r"\'ve", " have", phrase)<br/>    phrase = re.sub(r"\'m", " am", phrase)<br/>    return phrase</span></pre><p id="5d2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该功能的工作原理是将单词从“我们将”分解为“我们将”，“不能”分解为“不能”，“我们是”分解为“我们是”等。这一步是必要的，因为我们不希望我们的模型以不同的方式对待像“我们是”和“我们是”这样的短语。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="8de6" class="mg mh it oi b gy om on l oo op">stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've","you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their','theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after','above', 'below', 'to', 'from', 'up', 'down', 'in','out','on','off', 'over', 'under', 'again', 'further','then', 'once', 'here', 'there', 'when', 'where', 'why','how','all', 'any', 'both', 'each', 'few', 'more','most', 'other', 'some', 'such', 'only', 'own', 'same', 'so','than', 'too', 'very', 's', 't', 'can', 'will', 'just','don',"don't",'should',"should've", 'now', 'd', 'll', 'm', 'o','re','ve','y','ain','aren',"aren't",'couldn',"couldn't",'didn',"didn't", 'doesn', "doesn't", 'hadn',"hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't",'ma', 'mightn', "mightn't", 'mustn',"mustn't", 'needn', "needn't",'shan',"shan't",'shouldn',"shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't", '•', '❤', '✨', '$', '❌','♡', '☆', '✔', '⭐','✅', '⚡', '‼', '—', '▪', '❗', '■', '●', '➡','⛔', '♦', '〰', '×', '⚠', '°', '♥', '★', '®', '·','☺','–','➖','✴', '❣', '⚫', '✳', '➕', '™', 'ᴇ', '》', '✖', '▫', '¤','⬆', '⃣', 'ᴀ', '❇', 'ᴏ', '《', '☞', '❄', '»', 'ô', '❎', 'ɴ', '⭕', 'ᴛ','◇', 'ɪ', '½', 'ʀ', '❥', '⚜', '⋆', '⏺', '❕', 'ꕥ', '：', '◆', '✽','…', '☑', '︎', '═', '▶', '⬇', 'ʟ', '！', '✈', '�', '☀', 'ғ']</span></pre><p id="0271" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的代码块中，我定义了一个包含停用词的列表。停用词是不会给句子增加太多语义或字面意义的词。其中大部分是单词或不太重要的单词的压缩表示，如“a”、“at”、“for”等，以及符号。</p><p id="fa53" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们将定义一个函数，该函数获取句子，并使用解串函数和停用词列表来清理和返回处理过的文本。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="084c" class="mg mh it oi b gy om on l oo op">from tqdm import tqdm_notebook<br/>def preprocess_text(text_data):<br/>  preprocessed_text = []<br/>  # tqdm is for printing the status bar<br/>  for sentence in tqdm_notebook(text_data):<br/>    sent = decontracted(sentence)<br/>    sent = sent.replace('\\r', ' ')<br/>    sent = sent.replace('\\n', ' ')<br/>    sent = sent.replace('\\"', ' ')<br/>    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)<br/>    sent = ' '.join(e for e in sent.split() if e.lower() not in<br/>                    stopwords)<br/>    preprocessed_text.append(sent.lower().strip())<br/>  return preprocessed_text</span></pre><p id="8df1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用preprocess_text()函数清理文本数据的时间到了。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="6da3" class="mg mh it oi b gy om on l oo op">df['name'] = df['name'].fillna('') + ' ' +<br/>                       df['brand_name'].fillna('')<br/>df['name'] = preprocess_text(df.name.values)</span><span id="11a2" class="mg mh it oi b gy oy on l oo op">df['text'] = (df['item_description'].fillna('')+<br/>                       ' ' + df['category_name'].fillna(''))<br/>df['text'] = preprocess_text(df.text.values)</span><span id="9c4f" class="mg mh it oi b gy oy on l oo op">df_test['name'] = df_test['name'].fillna('') + ' ' <br/>                  + df_test['brand_name'].fillna('')<br/>df_test['text'] = (df_test['item_description'].fillna('') + ' '<br/>                   + df_test['category_name'].fillna(''))</span></pre><p id="1280" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，df['name']列包含连接和预处理的' name '和' brand_name '功能，同样，df['text']功能包含连接和预处理的' item_description '和' category_name '功能。</p><p id="db2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们继续进一步的过程，但在此之前，我们需要将数据分成训练集和交叉验证集。此外，我们将把目标值(即价格)转换成对数形式，以便它们呈正态分布，并且RMSLE(均方根对数误差)易于计算。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="1a69" class="mg mh it oi b gy om on l oo op">df =  df[['name', 'text', 'shipping', 'item_condition_id']]<br/>X_test = df_test[['name', 'text', 'shipping', 'item_condition_id']]</span><span id="ae89" class="mg mh it oi b gy oy on l oo op">from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler</span><span id="2d35" class="mg mh it oi b gy oy on l oo op">y_scaler = StandardScaler()<br/>X_train, X_cv, y_train, y_cv = train_test_split(df, y,<br/>                               test_size=0.05, random_state=42)<br/>y_train_std =<br/>     y_scaler.fit_transform(np.log1p(y_train.values.reshape(-1, 1)))</span></pre><p id="ce31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在是时候将这些预处理过的文本特征转换成数字表示了。在这个过程中，我将使用TF-IDF矢量器。我们将从特性“名称”开始</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="414f" class="mg mh it oi b gy om on l oo op">from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf</span><span id="9f71" class="mg mh it oi b gy oy on l oo op">tfidf = Tfidf(max_features=350000, token_pattern='\w+', ngram_range=(1,2)) # using only top 350000 tf-idf features (with bi-grams).<br/>X_tr_name = tfidf.fit_transform(X_train['name'])<br/>X_cv_name = tfidf.transform(X_cv['name'])<br/>X_test_name = tfidf.transform(X_test['name'])</span></pre><p id="c3f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来是“文本”功能</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="7679" class="mg mh it oi b gy om on l oo op">tfidf = Tfidf(max_features=350000, token_pattern='\w+', ngram_range=(1,3)) # using only top 350000 tf-idf features (with tri-grams).<br/>X_tr_text = tfidf.fit_transform(X_train['text'])<br/>X_cv_text = tfidf.transform(X_cv['text'])<br/>X_test_text = tfidf.transform(X_test['text'])</span></pre><p id="4a7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们也处理从“shipping”开始的其余分类特征<br/>因为这个特征只取2个值0和1，我们不需要对它们进行某种特殊的编码，让我们保持它们不变。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="112a" class="mg mh it oi b gy om on l oo op">from scipy import sparse<br/>X_tr_ship =<br/>         sparse.csr_matrix(X_train['shipping'].values.reshape(-1,1))<br/>X_cv_ship = sparse.csr_matrix(X_cv['shipping'].values.reshape(-1,1))<br/>X_test_ship =<br/>          sparse.csr_matrix(X_test['shipping'].values.reshape(-1,1))</span></pre><p id="11e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二个分类特征是‘item _ condition _ id’，它也是一个顺序特征。请记住，这些可以接受5个整数值(1–5 ),因此我们也将保持原样。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="095e" class="mg mh it oi b gy om on l oo op">X_tr_condition =<br/> sparse.csr_matrix(X_train['item_condition_id'].values.reshape(-1,1)<br/>                                                               - 1.)<br/>X_cv_condition =<br/>  sparse.csr_matrix(X_cv['item_condition_id'].values.reshape(-1,1)<br/>                                                               - 1.)<br/>X_test_condition =<br/>  sparse.csr_matrix(X_test['item_condition_id'].values.reshape(-1,1)<br/>                                                               - 1.)</span></pre><p id="0504" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，我使用了-1，因为该特性包含5种介于(1–5)之间的值，所以-1会将它们转换为(0–4)的范围。这将在转换为稀疏数据时给我们带来优势。</p><p id="ff19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，作为最后一步，我们将按列堆叠这些特性。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pu"><img src="../Images/aa66f6ca3c9bfb6dc70d6b3132fb6c3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NaNK4CSTlS6OcNde3_Y7-A.png"/></div></div></figure><p id="58bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我现在将把这些预处理过的数据转换成二进制形式，其中的值只能是1或0。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="20e3" class="mg mh it oi b gy om on l oo op">X_tr_binary = (X_tr&gt;0).astype(np.float32)<br/>X_cv_binary = (X_cv&gt;0).astype(np.float32)<br/>X_test_binary = (X_test&gt;0).astype(np.float32)</span></pre><p id="d3a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这一步的优点是，现在我们将有2个具有良好方差的数据集要处理。</p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="b520" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di"> M </span> <strong class="kk iu"> <em class="ln"> odeling </em> </strong></p><p id="496e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是时候用我们的数据测试一些模型了。我们将要尝试的模型是- <br/> -岭回归器<br/> -线性SVR <br/> - SGD回归器<br/> -随机森林回归器<br/> -决策树回归器<br/> - XGBoost回归器</p><blockquote class="ma mb mc"><p id="6637" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">正态数据的岭回归量</strong></p></blockquote><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/aeb6aa125d7142c426070fc0656b308d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*eeIvlwkMNG1wSmj3FR6M2g.gif"/></div><p class="od oe gj gh gi of og bd b be z dk translated">线性回归</p></figure><p id="8929" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用线性回归来寻找最优超平面(上面gif中的红线),使得<strong class="kk iu">损失</strong>或每个点到平面/直线的距离之和的平方最小。我们可以注意到，如果我们考虑在迭代次数=28次时得到的线，损失将是最小的。<br/>岭回归也称为L2正则化线性回归，这意味着它使用权重平方和作为惩罚。添加惩罚项是为了限制模型过度拟合(捕捉噪声)。<br/>岭回归只有1个超参数<strong class="kk iu"> λ </strong>，它与惩罚/正则化项相乘，并决定模型经历的欠拟合程度。λ的值越大，我们欠拟合越多。<br/> alpha就是正则化强度，它必须是一个正浮点数。因此，随着α的增加，欠拟合也会增加。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pw"><img src="../Images/5559e825510a6bf6bf6ad2911cf5d701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1cZF0vMWfRxbVadoXRPtQ.png"/></div></div></figure><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="c2e8" class="mg mh it oi b gy om on l oo op">import matplotlib.pyplot as plt<br/>plt.plot(alpha_list, train_loss, label='train loss')<br/>plt.plot(alpha_list, test_loss, label='test loss')<br/>plt.title('alpha VS RMSLE-loss plot')<br/>plt.xlabel('Hyperparameter: alpha')<br/>plt.ylabel('RMSLE loss')<br/>plt.xscale('log')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi px"><img src="../Images/14688b09ffb5a4e5699b00a32ad9a6ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOCULXcFu5oHAtQOuUbW0A.png"/></div></div></figure><ul class=""><li id="bbaf" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以观察到，随着alpha的降低，模型开始过度拟合。</li><li id="2277" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">α= 1时，试验损失最小。</li></ul><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi py"><img src="../Images/b39562d722f87f01135b0cc1ea1bdc04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-guTkXjJVsoTAUnWqAzXXA.png"/></div></div></figure><p id="e637" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好的，我们的岭在cv数据上返回了0.4232的损失。</p><blockquote class="ma mb mc"><p id="cb9a" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">二元数据的岭回归</strong></p></blockquote><p id="49bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们将对二进制数据使用岭回归</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="fd32" class="mg mh it oi b gy om on l oo op">import matplotlib.pyplot as plt<br/>plt.plot(alpha_list, train_loss, label='train loss')<br/>plt.plot(alpha_list, test_loss, label='test loss')<br/>plt.title('alpha VS RMSLE-loss plot (on binary features)')<br/>plt.xlabel('Hyperparameter: alpha')<br/>plt.ylabel('RMSLE loss')<br/>plt.xscale('log')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pz"><img src="../Images/b447e7fe8fdc62b939eacb74a34fbfc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u9RLEGtoW6t8CNAdP7zy-Q.png"/></div></div></figure><ul class=""><li id="2e49" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以观察到，当α= 100时，损耗最小。</li></ul><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qa"><img src="../Images/fea56624c15cbc0b6d2c70ec743bc67d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jfkNabTLvD_u3bBkTzqA9w.png"/></div></div></figure><p id="898f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的岭回归在cv数据上返回了0.4335的损失。</p><blockquote class="ma mb mc"><p id="7cf0" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">让我们在二进制数据上尝试SGD-Regressor(作为SVR)</strong></p></blockquote><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qb"><img src="../Images/f92aa8271411ada94af42b11b0725e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*wsBakfF2Geh1zgY4HJbwFQ.gif"/></div></div><p class="od oe gj gh gi of og bd b be z dk translated"><a class="ae oa" href="https://gifer.com/en/gifs/gradient" rel="noopener ugc nofollow" target="_blank">https://gifer.com/en/gifs/gradient</a></p></figure><p id="db94" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们快速刷新一下SGD是什么，它是如何工作的。还记得我在岭回归中提到的损失吗？损失有不同的类型，让我们从几何角度来理解。如果回归问题就是寻找最适合我们数据的最优超平面，那么损失仅仅意味着我们的数据与超平面有多大差异。因此，低损失意味着这些点与我们的超平面差别不大，模型表现良好，反之亦然。<br/>在线性回归的情况下，损失是平方损失，它是通过将数据点到超平面的平方距离的总和除以项数而获得的。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/1348d5914e0056f844d894f27589812a.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/0*RRHnDdeeoCMg52HZ.png"/></div></figure><p id="e0ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">损失函数很重要，因为它们定义了超平面是什么样子。还有其他被称为梯度下降的算法，它们利用这些损失函数并更新超平面的参数，以使其完美地拟合数据。这里的目标是最小化损失。SGD是一种优化算法，通过逐步减少损失来更新超平面的参数。这是通过计算损失函数相对于特征的梯度，然后使用这些梯度向最小值下降来实现的。在上图(左部)中，我们可以看到该算法如何通过向山下走正确的一步来达到损失函数的最小值，并且随着每一步都在正确的方向上，参数得到更新，这导致更好的拟合超平面(右部)。要了解更多关于随机梯度下降(SGD)算法的信息，你可以查看<a class="ae oa" href="https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/" rel="noopener ugc nofollow" target="_blank">这篇精彩的博客</a>。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qd"><img src="../Images/e79ec27fdf0393a52bd2717d7206965e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xSejSNMd5xAXxE5M.gif"/></div></div><p class="od oe gj gh gi of og bd b be z dk translated"><a class="ae oa" href="https://aws.amazon.com/blogs/machine-learning/train-faster-more-flexible-models-with-amazon-sagemaker-linear-learner/" rel="noopener ugc nofollow" target="_blank">https://AWS . Amazon . com/blogs/machine-learning/train-faster-more-flexible-models-with-Amazon-sage maker-linear-learner/</a></p></figure><p id="ac29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是一些其他常见损耗，但我们将使用“Huber”、“epsilon_insensitive”和“squared_epsilon_insensitive”来调整该模型的超参数。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qe"><img src="../Images/8f8f778109f5441c0fcc9f4abefe5f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I__GS6M_0shj0oEMEJfNBQ.png"/></div></div></figure><p id="fa96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机搜索交叉验证告诉我们，L2正则化的“平方ε不敏感”损失最适合此数据。顺便说一句，“平方ε不敏感”损失是另一个众所周知的机器学习算法支持向量机使用的损失之一，支持向量机通过利用支持向量来生成更好的拟合超平面，从而使用最大限度技术。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qf"><img src="../Images/769b06202d62e60fd06650b1a195bf31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6oNO9vFNwBL91iQL.jpeg"/></div></div><p class="od oe gj gh gi of og bd b be z dk translated"><a class="ae oa" href="https://www.researchgate.net/figure/Schematic-of-the-one-dimensional-support-vector-regression-SVR-model-Only-the-points_fig5_320916953" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/figure/Schematic-of-the-one-dimension-support-vector-regression-SVR-model-Only-the-points _ fig 5 _ 320916953</a></p></figure><ul class=""><li id="9ced" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">在该图中，虚线称为决策边界，位于虚线上的点称为支持向量，SVR的目标是最大化这些决策边界之间的距离。</li></ul><p id="3c33" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，为什么利润最大化如此重要，以至于SVM成为顶级最大似然算法之一？让我们使用一个简单的分类问题来快速理解这一点，我们需要找到一个分离蓝点和红点的最佳超平面。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qg"><img src="../Images/ce1428c2612c70ad53e4ad0caed23396.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*A_zxELRrMOraaHyV.png"/></div></div><p class="od oe gj gh gi of og bd b be z dk translated"><a class="ae oa" href="https://medium.com/coinmonks/support-vector-regression-or-svr-8eb3acf6d0ff" rel="noopener">https://medium . com/coin monks/support-vector-regression-or-SVR-8e B3 ACF 6d 0 ff</a></p></figure><ul class=""><li id="70b5" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">请看图中由名称<em class="ln">超平面</em>和<em class="ln">最优超平面</em>表示的两个平面。任何人都可以看出，<em class="ln">最优超平面</em>在分离蓝色和红色点方面比另一个平面好得多，并且使用SVM，这个<em class="ln">最优超平面</em>几乎是有保证的。</li></ul><p id="80ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个有趣的事实是,“平方ε不敏感”损失中的平底部分是由于这种利润最大化技巧。可以参考<a class="ae oa" href="https://medium.com/coinmonks/support-vector-regression-or-svr-8eb3acf6d0ff" rel="noopener">本</a>和<a class="ae oa" href="https://en.wikipedia.org/wiki/Support_vector_machine" rel="noopener ugc nofollow" target="_blank">本博客</a>了解更多关于SVR的内容。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qh"><img src="../Images/6c4782d0f5939fedfda2032c7d12bbd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xOwShgO7BXHzmdVWyTrMLw.png"/></div></div></figure><p id="341a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SGD回归器(作为SVR)在cv数据上返回0.4325…的损失。</p><blockquote class="ma mb mc"><p id="fe32" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">让我们在二进制数据上试试SGD回归器(作为线性回归器)</strong></p></blockquote><p id="42cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们将执行所有先前的步骤，但针对的是二进制数据。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qi"><img src="../Images/29922d8937ee5f6cee67b534bf80691e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OpMLYa_iEokmFj8e4TGMaw.png"/></div></div></figure><p id="093d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机搜索交叉验证告诉我们，L2正则化的“平方损失”损失最适合此数据。顺便说一下，L2正则化的平方损失的设置听起来很熟悉，对吗？这正是我们在岭回归模型中使用的。在这里，我们从优化问题的角度来处理这个问题，因为SGDRegressor为我们提供了更多的超参数来调整我们的模型。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qj"><img src="../Images/065ea07bad70cadd9560039e37551ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fNDaUXTXcyFWSfKpOKnW9w.png"/></div></div></figure><p id="62b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SGD回归器(作为线性回归器)在cv数据上返回0.4362的损失。</p><blockquote class="ma mb mc"><p id="b22e" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">正常数据的线性支持向量回归</strong></p></blockquote><p id="378c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们在正常数据上尝试支持向量回归机。这里的超参数是C，也是我们在岭回归中讨论过的α的倒数。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="200f" class="mg mh it oi b gy om on l oo op">import matplotlib.pyplot as plt<br/>plt.plot(C, train_loss, label='train loss')<br/>plt.plot(C, test_loss, label='test loss')<br/>plt.title('alpha VS RMSLE-loss plot (on binary features)')<br/>plt.xlabel('Hyperparameter: C (1/alpha)')<br/>plt.ylabel('RMSLE loss')<br/>plt.xscale('log')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qk"><img src="../Images/5bf27588fb0c44a14f284525fce18c54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-J5Ji1ADJ27YFz0u2lMlQ.png"/></div></div></figure><ul class=""><li id="cf29" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，0.1是超参数C的最佳超参数值，它使我们的测试损失最小。</li></ul><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ql"><img src="../Images/5df2ef507d774009365c4d15ce52a999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L35e-55NWoVP3OjtwJNSJg.png"/></div></div></figure><p id="0451" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线性SVR在正常数据的CV上返回0.4326的损失。</p><blockquote class="ma mb mc"><p id="6f1e" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">二进制数据的线性支持向量回归</strong></p></blockquote><p id="2493" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将对二进制数据尝试支持向量回归机。这里的超参数也是C，也是我们在岭回归中讨论过的α的倒数。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="97ec" class="mg mh it oi b gy om on l oo op">import matplotlib.pyplot as plt<br/>plt.plot(C, train_loss, label='train loss')<br/>plt.plot(C, test_loss, label='test loss')<br/>plt.title('alpha VS RMSLE-loss plot (on binary features)')<br/>plt.xlabel('Hyperparameter: C (1/alpha)')<br/>plt.ylabel('RMSLE loss')<br/>plt.xscale('log')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qm"><img src="../Images/01b1dc099c579ac6f7485115086fb9ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2XsJL6XbIwDKWNpesnFvfg.png"/></div></div></figure><ul class=""><li id="7311" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，0.01是超参数C的最佳超参数值，它使我们的测试损失最小。</li></ul><p id="a08e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线性SVR在二进制数据的cv上返回0.4325的损失。</p><h2 id="9453" class="mg mh it bd mi mj mk dn ml mm mn dp mo kr mp mq mr kv ms mt mu kz mv mw mx my bi translated">基于树的模型</h2><p id="d2c2" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated"><strong class="kk iu"> </strong></p><p id="e517" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意:我尝试的另一个维度技术是truncated-SVD，但是它需要大量的RAM(超过16 GB)来进行计算，并且因为这是一个内核挑战，所以使用完整的数据没有多大意义。</p><p id="29a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="ln">为基于树的模型选择顶级特征:</em> </strong></p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="9a21" class="mg mh it oi b gy om on l oo op">from sklearn.feature_selection import SelectFromModel</span><span id="3dd1" class="mg mh it oi b gy oy on l oo op">from sklearn.linear_model import SGDRegressor<br/>regressor = Ridge(alpha=100)<br/>selection = SelectFromModel(regressor)<br/>selection.fit(X_tr_binary, y_train_std.ravel())</span><span id="fb7b" class="mg mh it oi b gy oy on l oo op">X_train_top = selection.transform(X_tr_binary)<br/>X_cv_top = selection.transform(X_cv_binary)<br/>X_test_top = selection.transform(X_test_binary)</span></pre><blockquote class="ma mb mc"><p id="f447" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">决策树</strong></p></blockquote><p id="1792" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的第一个基于树的模型是决策树，在我们的数据集上使用它之前，让我们先快速了解它是如何工作的。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="ab gu cl qn"><img src="../Images/289224725f94e46ee933a2dde242ffba.png" data-original-src="https://miro.medium.com/v2/0*cant-HQdfMju-GxG"/></div><p class="od oe gj gh gi of og bd b be z dk translated"><a class="ae oa" href="https://www.datasciencecentral.com/profiles/blogs/the-complete-guide-to-decision-trees" rel="noopener ugc nofollow" target="_blank">https://www . datascience central . com/profiles/blogs/the-complete-guide-to-decision-trees</a></p></figure><p id="9f1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">决策树由简单的if-else语句组成，使用这些条件，它们决定如何预测给定名称、条件等的产品价格。从几何学上讲，它们使用几个平行于轴的超平面来拟合数据。<br/>在训练树的同时，树通过使用和验证训练数据来学习这些if-else语句。当它被训练时，它使用这些学习到的if-else条件来预测测试数据的值。<br/>但是它是如何决定如何拆分数据或者在拆分数据和构建完整的树时考虑什么特征的呢？<br/>嗯，它使用一种叫做熵的东西来构建树，熵是一种确定性的度量。<br/>决策树有几个超参数，但我们将只考虑其中两个重要的- <br/> - <em class="ln"> max_depth: </em>它表示决策树的最大深度。因此，如果max_depth假定为4，则在训练时，构造的树的深度不会超过4。<br/>-<em class="ln">min _ samples _ split:</em>表示执行分割或考虑if-else条件时必须存在的最小数据点数。因此，如果min_samples_split假定为32，则在训练时，如果所构造的树看到的数据点少于32个，则它不会应用if-else条件。</p><p id="fb78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以上两个超参数都限制了决策树的不足或过度捕捞。高的max_depth和低的min_samples_split值使决策树更容易过度拟合，反之亦然。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/d8f945744ddf3e62be0602de585c6af2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*DzCdcC-xomLcLd1B.png"/></div><p class="od oe gj gh gi of og bd b be z dk translated"><a class="ae oa" href="https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/tree/plot _ tree _ regression . html</a></p></figure><ul class=""><li id="5e63" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">在此图中，我们可以看到一个经过训练的决策树算法如何尝试拟合数据，请注意拟合线是如何由轴平行线组成的。</li><li id="d291" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">我们还可以注意到，max_depth值越大的决策树也越容易捕捉到噪声点。</li></ul><p id="c91d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我不会在这个博客中深入讨论决策树的内部工作原理，因为这会使它变得太长，要了解更多关于决策树的内部工作原理，你可以查看这个很棒的<a class="ae oa" rel="noopener" target="_blank" href="/entropy-how-decision-trees-make-decisions-2946b9c18c8">博客</a>。</p><p id="bb85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用RandomSearchCV对我们的决策树执行一些超参数调整，并检查什么是我们的树的最佳超参数。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qo"><img src="../Images/00219ab1891f4e77bea5028b65df98d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ZinFL3ECLWXGNC6Lo768A.png"/></div></div></figure><p id="071f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">返回的最佳超参数值是max_depth=64和min_samples_split = 64。现在让我们检查在这些超参数上训练决策树之后获得的损失。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qp"><img src="../Images/e08d15c2fb92e0200e946ced2b6b9622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0cbEiS0XSgNC82nrDjfkaw.png"/></div></div></figure><p id="b8bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑到训练需要14分钟，损失值并不是很大。到目前为止，我们的线性模型已经超过了决策树模型。</p><blockquote class="ma mb mc"><p id="b3cb" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">随机森林—(最大深度=3，n估计值=100) </strong></p></blockquote><p id="23ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们使用另一个非常棒的基于树的模型，或者我应该说模型来模拟我们的数据。<br/>随机森林是由多个模型组成的集合。这个想法是使用数据的随机部分来训练多个模型，然后使用来自这些多个模型的平均预测作为最终值。这是有意义的，因为使用完整数据的随机部分训练几个模型会创建在不同方面存在一定程度偏差的模型。现在，从所有这些模型中取平均预测值，最终会得到一个更好的预测值。</p><p id="b4ae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机森林这个名称来自于我们在从训练数据集中随机采样数据<em class="ln">时使用的Bootstrap采样，由于我们使用多个决策树作为我们的基础模型，所以它有单词<em class="ln">森林</em>。</em></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qq"><img src="../Images/97e8f29f353342abbe5176ca0da7e118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*N2HUznk6Rrtpg1iR.png"/></div></div><p class="od oe gj gh gi of og bd b be z dk translated"><a class="ae oa" rel="noopener" target="_blank" href="/random-forest-and-its-implementation-71824ced454f">https://towards data science . com/random-forest-and-its-implementation-71824 ced 454 f</a></p></figure><p id="95d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上图显示了随机森林如何使用随机采样数据训练不同的基础学习者，分别表示为树1、树2…然后收集并平均这些树的预测。</p><p id="b19e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机森林有多个超参数，但对于我们的数据，我们将只使用2: <br/> <em class="ln"> - n_estimator: </em>这表示我们希望随机森林模型拥有的基础模型的数量。<br/> <em class="ln"> - max_depth: </em>表示每个基础模型即决策树的最大深度。</p><p id="5715" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们训练一个随机森林模型，并对其执行一些超参数调整。</p><p id="6e1b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个模型的训练时间大约是23分钟。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qr"><img src="../Images/4b6f064f8c7b1ec05b445c84659396fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ehM9Y9BQGveU8-yBmny4A.png"/></div></div></figure><p id="7eb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，这个模型在给定的数据集上表现不好，结果一点也不好。</p><blockquote class="ma mb mc"><p id="df66" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">随机森林—(最大深度=4，n估计值=200) </strong></p></blockquote><p id="6f20" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里我使用了相同的模型，但是在架构上做了一些改变。我已经把最大深度增加到了4，基础学习者的数量增加到了200。<br/>我们来看看模特表现如何。</p><p id="6e1e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型的训练时间约为65分钟。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qo"><img src="../Images/848dc9206d9577b7b3f03297d824eba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7_d_JkciaKqEK7U04Zyzeg.png"/></div></div></figure><p id="ceac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果略好于之前的随机森林模型，但仍然不接近我们的线性模型。</p><blockquote class="ma mb mc"><p id="069c" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu"> XGBoost — (max_depth=4，n_estimators=200) </strong></p></blockquote><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/9170d2d398bb2b4e7e418ba6c0ab1f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*CVSyne5ZJ7MnDeAU.gif"/></div><p class="od oe gj gh gi of og bd b be z dk translated">【https://github.com/bgreenwell】</p></figure><p id="aba9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我们将尝试的最后一个基于树的模型，它被称为XGBoost。XGBoost是GBDT的略微增强版本，它也是一种集合建模技术。在梯度增强中，目的是减少方差或减少数据集的欠拟合行为。让我们看看它是如何工作的。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi qt"><img src="../Images/c54278935a76e206db620a99cb914083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*ctuJT4dyWDY18QCm.png"/></div><p class="od oe gj gh gi of og bd b be z dk translated"><a class="ae oa" href="http://uc-r.github.io/gbm_regression" rel="noopener ugc nofollow" target="_blank">http://uc-r.github.io/gbm_regression</a></p></figure><p id="62bf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在GBDT，我们首先使用训练数据训练我们的第一个基本模型，这通常是一个高偏差决策树，然后我们从该模型中获取预测值，并计算由预测值与实际值的差异定义的误差。现在我们训练我们的第二个基础学习者，但是这次我们不仅仅使用训练数据，我们还使用从我们的第一个基础学习者获得的误差，并且我们再次从这个模型中获取预测值并计算误差。这一直持续到覆盖了所有的基础学习者，当我们一个接一个地训练基础学习者时，我们注意到误差值慢慢地减小。你可以在这里阅读更多关于GBDT <a class="ae oa" href="http://uc-r.github.io/gbm_regression" rel="noopener ugc nofollow" target="_blank">的信息。<br/> XGBoost是GBDT的一个稍微修改的版本，它使用像随机森林中的行采样和列采样这样的技术来构造基础学习器。</a></p><p id="e6cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们快速检查一下XGBoost的代码，我将使用2个超参数:<br/> - n_estimators:它表示作为决策树模型的基础学习者的数量。<br/> - max_depth:表示基础学习器决策树的最大深度。</p><p id="6011" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型花了大约27分钟来训练。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qu"><img src="../Images/0caaf362bada76c8136b3401798928c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GpBSeek8lu_plTioQc1r0Q.png"/></div></div></figure><p id="3f5d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果没有随机森林差，但也没有线性模型好。</p><blockquote class="ma mb mc"><p id="6487" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu"> XGBoost — (max_depth=6，n_estimators=500) </strong></p></blockquote><p id="2b69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们试试XGBoost，max_depth=6，n_estimators=500。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qv"><img src="../Images/b69475bfc3d29931857f62d66af2ed35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fw1yJ3iF2rCJAn1O5vz9_Q.png"/></div></div></figure><p id="eec9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到一个体面的数量从以前的模型改进，但它需要模型78分钟的训练。</p><h2 id="e1d3" class="mg mh it bd mi mj mk dn ml mm mn dp mo kr mp mq mr kv ms mt mu kz mv mw mx my bi translated">让我们比较不同的型号及其性能:</h2><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qw"><img src="../Images/1b234b92239299e6a362bd03a8633857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P--evq3wbxVgY9Tj7787OQ.png"/></div></div></figure><p id="cbd1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上表中，我们可以看到基于树的模型花费了太多的计算时间，事实上，我用于基于树的数据要小得多，我只使用了从岭回归器中选择的顶部二元特征。因此，新数据只有大约236k个特征，而不是其他线性模型训练的原始700k个特征。我们还可以观察到，我们能够获得的交叉验证数据的最小损失是0.4232…让我们尝试使用集成建模来进一步减少这一损失。</p><p id="3863" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线性模型已经胜过其他基于树的模型，所以我将使用这些来创建一个集合。</p><p id="88c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们把前6个线性模型的结果串联起来。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qx"><img src="../Images/1fc733fd69ba98d06490806a8371c3b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eAS9FC7SP5MMTqR4eOnG_Q.png"/></div></div></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qy"><img src="../Images/b9e4a2ea246e20d0a0e50c3f27226d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QrZtzLNhER9TszchVkSq5w.png"/></div></div></figure><p id="47d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们快速测试一个简单的集合，该集合将这些特征作为输入，并将输出计算为这些值的平均值。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qz"><img src="../Images/4623ff851076d98079bb419aa15afcd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dg9VBOEZjOnmC7TFwOpGXA.png"/></div></div></figure><p id="8919" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以观察到损失略有增加，这意味着这种方法本身不足以产生好的分数。</p><p id="d237" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们检查这些新特征之间的相关性，因为它们都来自线性模型，并且产生类似的损失。如果它们高度相关，它们不会对整体损失有太大改善。</p><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="44de" class="mg mh it oi b gy om on l oo op">import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>plt.figure(figsize=(10,8))<br/>columns = ['y_pred_ridge_binary_tr', 'y_pred_ridge_normal_tr',<br/>           'y_pred_svr_normal_tr','y_pred_svr_binary_tr',<br/>           'y_pred_sgd_lr_binary_tr', 'y_pred_sgd_svr_binary_tr']<br/>df = pd.DataFrame(y_pred_tr_ensemble, columns=columns)<br/>Var_Corr = df.corr()<br/>sns.heatmap(Var_Corr, xticklabels=Var_Corr.columns,<br/>yticklabels=Var_Corr.columns, annot=True)<br/>plt.title('Correlation between different features.')<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ox"><img src="../Images/10a3d6c40653c0c2857efe033a4b736c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4KjE92siYvJcKt61675KQ.png"/></div></div></figure><ul class=""><li id="a810" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，来自基础模型的结果是高度相关的，因此在它们的基础上构建一个集成并不能获得多少分数。</li></ul><p id="530e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解决这个问题，我增加了数据的维度，添加了从线性模型中收集的顶级特征，这些数据是我们用来训练基于树的模型的二进制数据。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ra"><img src="../Images/f621adbeb8b42a845ba1170d920764a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DxQH9MNQ5TsMzAS50wiysw.png"/></div></div></figure><p id="b500" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在是时候在这些新生成的特征上尝试不同的模型，看看我们是否可以改善这种损失。</p><blockquote class="ma mb mc"><p id="da9c" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">让我们试试使用不同超参数的SGD回归器</strong></p></blockquote><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi rb"><img src="../Images/339bbafc14057c2f57023f155f733f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YI8aWb3zJL_p67qa_FmJHw.png"/></div></div></figure><p id="cbdf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的代码块代表了RandomSearchCV返回的最佳超参数。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ql"><img src="../Images/c4b3fbb1eafc045319b0eb01bc8e19ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s24NGK5ZOPD1mOvflcQA_w.png"/></div></div></figure><p id="dde8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">CV损失没有达到标准，因为我们已经损失了0.4232…我们正在寻找比这更低的损失。</p><blockquote class="ma mb mc"><p id="2283" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">让我们在新功能上试试线性支持向量回归机和岭回归机</strong></p></blockquote><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="625c" class="mg mh it oi b gy om on l oo op">import matplotlib.pyplot as plt<br/>ridge_loss = np.array(ridge_loss)<br/>linearsvr_loss = np.array(linearsvr_loss)<br/>plt.plot(alpha, ridge_loss.T[0], label='Ridge train')<br/>plt.plot(alpha, ridge_loss.T[1], label='Ridge test')<br/>plt.plot(alpha, linearsvr_loss.T[0], label='linearsvr train')<br/>plt.plot(alpha, linearsvr_loss.T[1], label='linearsvr test')<br/>plt.xlabel('Hyperparameter: alpha or (1/C)')<br/>plt.ylabel('loss')<br/>plt.xscale('log')<br/>plt.title('Linear SVR and Ridge losses')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi px"><img src="../Images/7c7c29623d5ed454b111e5c5e7a92258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yaalDojG9iH1xgmnkYI89A.png"/></div></div></figure><ul class=""><li id="5b94" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">我们可以看到，在alpha=100000时，岭回归和线性SVR返回的cv损失最小。让我们把模型放在那上面。</li></ul><p id="e396" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用alpha = 100000训练岭回归方程</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi rc"><img src="../Images/fb424e42f1b3110cd326c1abcf04d837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tdz-S5tXH1WjRklGmA9xjA.png"/></div></div></figure><p id="327f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用C = 0.00001训练线性SVR</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi rd"><img src="../Images/6741ea3c18d62924a5baa295ee350e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-0KihDZg6yACp0SJoVs9zg.png"/></div></div></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi re"><img src="../Images/13fc57772b4ba5c34605b02c3309bda5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jcy10PEYybzNB06Lmuo03Q.png"/></div></div></figure><p id="05d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，通过查看上表，我们可以知道山脊和LinearSVR模型产生了最好的结果，所以我们将使用这些来生成一个和我们的系综的最后一层。</p><p id="df2b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用这些模型快速拟合数据，并将输出连接起来，作为最终集合层的输入。</p><p id="14a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将使用先前图层模型生成的输出来创建总体的最终图层。我们将使用一些线性模型，但在此之前，让我们测试简单的平均结果。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi rf"><img src="../Images/5f0dca606a1ccc7c8384e0aaeab3aee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s2SWaJ2bbnKGvM6yMHX9zg.png"/></div></div></figure><p id="41d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果比单独的LinearSVR模型要好，但是到目前为止，岭仍然优于每个模型。</p><p id="fd58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们为最后一层尝试一些线性模型:</p><blockquote class="ma mb mc"><p id="c77f" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">新币回归器</strong></p></blockquote><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ox"><img src="../Images/8e3e72b1337f15e4684fd066423af183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UnARXx06nsf4Ipi8IljAMQ.png"/></div></div></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi rg"><img src="../Images/bb8a53d324b2b06c324bb3d0d0aa5f0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rlb1u_EZ5kasq7Cg0gCvGw.png"/></div></div></figure><blockquote class="ma mb mc"><p id="7336" class="ki kj ln kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">让我们尝试使用岭和线性支持向量回归作为最终的图层模型</strong></p></blockquote><pre class="lp lq lr ls gt oh oi oj ok aw ol bi"><span id="1552" class="mg mh it oi b gy om on l oo op">import matplotlib.pyplot as plt<br/>ridge_loss = np.array(ridge_loss)<br/>linearsvr_loss = np.array(linearsvr_loss)<br/>plt.plot(alpha, ridge_loss.T[0], label='Ridge train')<br/>plt.plot(alpha, ridge_loss.T[1], label='Ridge test')<br/>plt.plot(alpha, linearsvr_loss.T[0], label='linearsvr train')<br/>plt.plot(alpha, linearsvr_loss.T[1], label='linearsvr test')<br/>plt.xlabel('Hyperparameter: alpha or (1/C)')<br/>plt.ylabel('loss')<br/>plt.xscale('log')<br/>plt.title('Linear SVR and Ridge losses')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi rh"><img src="../Images/017052d9403ff55b7e3eb937cac69561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KiLPm2CRbWW-auX7ua3Vgg.png"/></div></div></figure><ul class=""><li id="e691" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">结果接近，但岭回归优于线性回归。</li></ul><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ri"><img src="../Images/0e54ae5d4f83af7f1d2494862bcc9d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CpaKlAOcl47fhvVkVGyGXg.png"/></div></div></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi rj"><img src="../Images/a6fd1c3d8da099899a98a21afb720be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uWD04L7O3hi423rS4sFc6Q.png"/></div></div></figure><p id="7c74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">这里是用于集合的所有模型，以表格形式进行比较。</strong></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qp"><img src="../Images/fbba72f654be4832d269917bd843557e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CyA0T6y9RSgWS0a3JFp5ZQ.png"/></div></div></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/0dd2223623fb839f7384596138c8e505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QLU58lo_X3qE_HLb70MTmg.png"/></div></div></figure></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="efb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di"> F </span> <strong class="kk iu"> <em class="ln">最后，让我们预测测试数据集的价格，并检查我们的组合在Kaggle排行榜上的表现。</em>T15】</strong></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi rk"><img src="../Images/b3863466c75144bef0531d434c7bbf06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q4xHaJNty0OCsrqN4f_ecQ.png"/></div></div></figure><h2 id="0484" class="mg mh it bd mi mj mk dn ml mm mn dp mo kr mp mq mr kv ms mt mu kz mv mw mx my bi translated">在提交预测结果后，我获得了0.42457的分数，相当于Kaggle排行榜上的前6%。</h2><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi rl"><img src="../Images/31df85f29af8cc315f12a2199c96f868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m8PVQnO4QZhA6hev8Lfl2w.png"/></div></div><p class="od oe gj gh gi of og bd b be z dk translated"><a class="ae oa" href="https://www.kaggle.com/c/mercari-price-suggestion-challenge/submissions" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/mercari-price-suggestion-challenge/submissions</a></p></figure></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="69b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di">福</span> <strong class="kk iu"> <em class="ln">未来工作</em> </strong></p><ul class=""><li id="5923" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">这个问题可以使用深度学习算法来解决，如GRU，MLP，伯特，因为大多数特征来自文本描述。我们可以在这些数据上尝试这些最先进的技术，并尝试提高分数。</li><li id="c5bf" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated">代码没有针对多处理进行优化，因此它使用所有4个CPU。我认为这值得一试，因为这样的话，即使是基于树的模型也可以加入到整体中。</li></ul></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="8443" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di">R</span><strong class="kk iu">T3】引用T5】</strong></p><ul class=""><li id="8a33" class="ne nf it kk b kl km ko kp kr ng kv nh kz ni ld nj nk nl nm bi translated">https://www . ka ggle . com/c/mercari-price-suggestion-challenge/discussion/50256</li><li id="0ad2" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated"><a class="ae oa" href="https://www.youtube.com/watch?v=QFR0IHbzA30" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=QFR0IHbzA30</a></li><li id="6973" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated"><a class="ae oa" href="https://youtu.be/_PwhiWxHK8o" rel="noopener ugc nofollow" target="_blank">https://youtu.be/_PwhiWxHK8o</a></li><li id="9f1d" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated"><a class="ae oa" href="https://youtu.be/UHBmv7qCey4" rel="noopener ugc nofollow" target="_blank">https://youtu.be/UHBmv7qCey4</a></li><li id="5581" class="ne nf it kk b kl nn ko no kr np kv nq kz nr ld nj nk nl nm bi translated"><a class="ae oa" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li></ul></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="b7fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di">福</span> <strong class="kk iu"> <em class="ln">遗书</em> </strong></p><p id="d4a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您阅读博客。我希望它对那些有志于做机器学习、整体建模、数据处理、数据可视化项目的人有用。</p><p id="8940" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对这个项目有任何疑问，请在这个项目的回复部分或GitHub repo中留下评论。</p><p id="3961" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">完整的项目可以在我的Github上找到:<br/><a class="ae oa" href="https://github.com/SarthakV7/mercari_kaggle" rel="noopener ugc nofollow" target="_blank">https://github.com/SarthakV7/mercari_kaggle</a><br/>在LinkedIn上找到我:<a class="ae oa" href="http://www.linkedin.com/in/sarthak-vajpayee" rel="noopener ugc nofollow" target="_blank">www.linkedin.com/in/sarthak-vajpayee</a></p><p id="bd8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">和平！☮</p></div></div>    
</body>
</html>