<html>
<head>
<title>Text Classification with CNNs in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 中基于 CNN 的文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classification-with-cnns-in-pytorch-1113df31e79f?source=collection_archive---------3-----------------------#2020-09-18">https://towardsdatascience.com/text-classification-with-cnns-in-pytorch-1113df31e79f?source=collection_archive---------3-----------------------#2020-09-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2040" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 PyTorch 中实现的 CNN 构建文本分类器的分步指南。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d16d0009c4530381ac39a10afa33872d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*z7sYZWkJX66mC_UM"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">谢尔比·米勒在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="kz"><p id="1c8b" class="la lb it bd lc ld le lf lg lh li lj dk translated">“深度学习不仅仅是添加层”</p></blockquote><p id="c5e9" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me lj im bi translated">这个博客的目的是通过实现卷积神经网络来开发一个逐步的文本分类器。所以，这个博客分为以下几个部分:</p><ul class=""><li id="a15c" class="mf mg it lm b ln mh lq mi lt mj lx mk mb ml lj mm mn mo mp bi translated"><strong class="lm iu">简介</strong></li><li id="f6a3" class="mf mg it lm b ln mq lq mr lt ms lx mt mb mu lj mm mn mo mp bi translated"><strong class="lm iu">预处理</strong></li><li id="8990" class="mf mg it lm b ln mq lq mr lt ms lx mt mb mu lj mm mn mo mp bi translated"><strong class="lm iu">车型</strong></li><li id="da28" class="mf mg it lm b ln mq lq mr lt ms lx mt mb mu lj mm mn mo mp bi translated"><strong class="lm iu">培训</strong></li><li id="dc0d" class="mf mg it lm b ln mq lq mr lt ms lx mt mb mu lj mm mn mo mp bi translated"><strong class="lm iu">评估</strong></li></ul><p id="cb55" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">所以，让我们开始吧！</p><h1 id="94ae" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">介绍</h1><p id="f612" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">可以从不同的方法来解决文本分类问题，例如，考虑单词在给定文本中相对于这些单词在完整语料库中的出现的<em class="nv">出现频率。</em></p><p id="2d0a" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">另一方面，存在将文本建模为单词或字符序列的其他方法，这种类型的方法主要利用基于递归神经网络架构的模型。</p><blockquote class="nw nx ny"><p id="c7ed" class="lk ll nv lm b ln mh ju lp lq mi jx ls nz mv lv lw oa mw lz ma ob mx md me lj im bi translated">如果你想了解更多关于 LSTM 递归神经网络的文本分类，看看这个博客:<a class="ae ky" rel="noopener" target="_blank" href="/text-classification-with-pytorch-7111dae111a6">py torch 中 LSTMs 的文本分类</a></p></blockquote><p id="494c" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">然而，还有另一种方法，其中文本被建模为给定空间中单词的<em class="nv">分布</em>。这是通过使用<strong class="lm iu">卷积神经网络</strong>(CNN)实现的。</p><p id="b71e" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">因此，我们将从最后提到的方法开始，我们将使用基于 CNN 的架构，建立一个模型来分类文本，考虑组成文本的一组单词在空间中的分布。</p><p id="1e16" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">开始吧！</p><h1 id="e559" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated"><strong class="ak">预处理</strong></h1><p id="e370" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">本模型所用数据取自 Kaggle 大赛:<a class="ae ky" href="https://www.kaggle.com/c/nlp-getting-started" rel="noopener ugc nofollow" target="_blank"> <em class="nv">真实与否？NLP 与灾难推文</em> </a></p><p id="9175" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">数据集的第一行如图 1 所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/e4d48d836060e9caa082c747eac23d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwgJDgHbiKoTO2QmG1Va5w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。数据集头|按作者分类的图像</p></figure><p id="d6fb" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">我们可以看到，需要创建一个预处理管道来<em class="nv">加载文本</em>，<em class="nv">清理它</em>，<em class="nv">标记它</em>，<em class="nv">填充它</em>和<em class="nv">分割成训练和测试</em>组。</p><p id="9e4f" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated"><strong class="lm iu">载入文字。</strong>由于我们要处理的文本已经在我们的存储库中，我们只需要在本地调用它并删除一些无用的列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 1。加载数据功能</p></figure><p id="10e0" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated"><strong class="lm iu">干净的文字</strong>。在这种情况下，我们需要从文本中删除特殊符号和数字。我们将只使用小写单词。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 2。清除文本功能</p></figure><p id="3092" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated"><strong class="lm iu">词语虚化</strong>。对于标记化，我们将使用来自<em class="nv"> nltk </em>库的<em class="nv"> word_tokenize </em>函数(一种非常简单的句子标记化方法)。在此之后，我们将需要<em class="nv">生成一个字典</em>，其中包含数据集中最常用的单词<em class="nv"> x </em>(这是为了降低问题的复杂性)。因此，正如您在代码 3 的第 3 行中看到的，应用了标记化。在第 14 行中，选择了最常见的"<em class="nv"> x </em>"单词，在第 16 行中构建了单词字典(正如您所看到的，字典从索引 1 开始，这是因为我们保留了索引 0 来应用填充)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 3。标记化和构建词汇功能</p></figure><p id="901d" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">到目前为止，每条 tweet 都已经标记化了，但是我们需要将每个单词标记转换成数字格式，因此我们将使用代码 3 中生成的字典将每个单词转换成基于索引的表示形式。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 4。从 word 到 idx 函数</p></figure><p id="3c54" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated"><strong class="lm iu">填充</strong>。正如你所想象的，不是所有的 tweetss 都一样长，但是每条 tweet 都有相同的字数是很重要的。这就是我们引入填充的原因。填充是为了标准化每条 tweet 的长度。在这种情况下，我们将用于填充的值将是数字零(我们在构建词汇词典时保留的索引)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 5。填充功能</p></figure><p id="6587" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated"><strong class="lm iu">分割介绍训练和测试</strong>。这个预处理管道的最后一步是将数据分为训练和测试。为此，我们将使用<em class="nv"> scikit learn </em>提供的功能。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 6。分割数据功能</p></figure><p id="bbe7" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">完整的预处理类如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 7。预处理类</p></figure><p id="b3f6" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">很好，到目前为止，我们已经完成了所有的预处理，我们已经有了我们的训练和测试集，是时候看看模型了！</p><h1 id="706a" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">模型</h1><p id="3a0b" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">实现的模型将使用<em class="nv"> n 元单词</em>，即不同的核大小将应用于同一个句子(指基于 n 元单词的作文)。然后，使用<em class="nv"> max pooling </em>函数减少这些内核的每个输出。最后，这些输出中的每一个将被<em class="nv">连接</em>在<em class="nv">单张量</em>中，以被引入线性层，该线性层将被激活函数过滤以获得最终结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/f0edad242e0e16f585bcf7c85f9f3e9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RM8kQOnrIIYl97jL30LVRg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。建筑模型|作者图片</p></figure><blockquote class="nw nx ny"><p id="ad69" class="lk ll nv lm b ln mh ju lp lq mi jx ls nz mv lv lw oa mw lz ma ob mx md me lj im bi translated">这里有完整的实现:<a class="ae ky" href="https://github.com/FernandoLpz/Text-Classification-CNN-PyTorch" rel="noopener ugc nofollow" target="_blank">https://github . com/FernandoLpz/Text-Classification-CNN-py torch</a></p></blockquote><p id="8704" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">正如我们所见，卷积层没有堆叠。每个卷积层都由一个特定的核大小定义，这个大小就是所讨论的“n 元文法”的定义。此外，每个卷积的每个输出都使用最大池减少。最后，每个张量被连接起来形成一个单一的张量，它将被引入到线性层。</p><p id="3d83" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">现在让我们看看如何使用 PyTorch 框架在代码中完成所有这些工作。首先，我们需要创建神经网络的构造器，为此，我们将定义一些重要的参数以及卷积、最大池和线性层。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 8。文本分类器构造器</p></figure><p id="d156" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">从第 16 行到第 19 行，我们为每个卷积定义了<em class="nv">个不同的内核</em>(记住在这种情况下，<em class="nv">内核大小</em>充当了<em class="nv"> n-gram </em>的大小)。</p><p id="eff5" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">第 22 行指的是对每层卷积的输出通道数量<em class="nv">的定义。第 24 行是指滑动窗口(内核)时要考虑的跳转次数。</em></p><p id="949e" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">在第 27 行定义了<em class="nv">嵌入层</em>。我们可以看到输入单词的数量(词汇量的大小)，包含了一个"<em class="nv"> +1 </em>"这是因为我们考虑的索引是指填充，在这种情况下它是索引 0。</p><p id="51ed" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">从第 30 行到第 33 行，我们定义了每个<em class="nv">卷积层</em>。同样，从第 36 行到第 39 行，我们定义了每个<em class="nv">最大池层</em>。</p><p id="eb83" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">最后在第 42 行，我们定义了<em class="nv">线性层</em>。值得注意的是，该层的<em class="nv">输入元素</em>的数量由一个函数定义。这是因为通过应用<em class="nv">卷积</em>和<em class="nv">最大池</em>(在不同的内核大小下)，修改了输出张量的大小。同样，这样的输出张量将被连接并简化为一维张量，(<em class="nv">展平</em>)。这就是为什么我们实现了一个函数来计算线性层的输入大小。</p><p id="cef2" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">每个<em class="nv">卷积</em>和<em class="nv">最大汇集</em>层的输出张量的大小由以下函数定义:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/c879a2b1ca203dbb414d25e63ae294b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H4rtFwnp6o8kBdeX6Polpw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3。输出矢量大小| <a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="411c" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">因此，计算线性图层输入大小的函数由以下公式确定:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 9。计算线性图层的输入大小</p></figure><p id="e29a" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">到目前为止，我们已经定义了构造函数。是时候继续前进了，让我们开始吧！</p><p id="ac02" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated"><em class="nv"> forward </em>函数将获取<em class="nv">分词</em>的向量，并通过<em class="nv">嵌入层</em>。随后，每个<em class="nv">嵌入语句</em>将通过<em class="nv">卷积</em>和<em class="nv">最大池层</em>的每一层，最后，得到的向量将被<em class="nv">串接</em>和<em class="nv">归约</em>以引入到<em class="nv">线性层</em>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 10。正向功能</p></figure><p id="b3f5" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">在第 4 行，输入向量通过<em class="nv">嵌入层</em>。在第 7、12、17 和 22 行中，<em class="nv">卷积</em>运算(具有不同的<em class="nv">内核大小</em>)被应用于嵌入序列。在第 27 行上，每个输出向量被连接，在第 28 行上，该向量被减少到一维(<em class="nv">展平</em>)。随后，<em class="nv">展平向量</em>通过一个线性层，其激活函数为<em class="nv"> sigmoid </em>。</p><p id="1b9b" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">所以完整的架构看起来会是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/22d5b61361bba52940e0873ee74b8291.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*51dkqMhE21qKtzkEwl5PqA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3。文本分类模型|按作者分类的图片</p></figure><p id="5119" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">太好了，我们已经看到了如何定义神经网络的架构以及前向函数，是时候看看训练函数了，让我们开始吧！</p><h1 id="d8dc" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated"><strong class="ak">培训</strong></h1><p id="4e48" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">正如我们在预处理阶段看到的，训练和测试数据已经准备好实现了。然而，我们需要将它们转换成基于 torch 的数据类型，并创建批处理生成器。为此，我们将利用<em class="nv"> PyTorch </em>框架提供的<em class="nv">数据集</em>和<em class="nv">数据加载器</em>模块，实现如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 11。数据处理程序类</p></figure><p id="0942" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">为了使用该类，我们只需要实例化然后初始化数据加载器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 12。数据生成程序</p></figure><p id="fb83" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">对于训练阶段，我们必须定义数据加载器(我们在上一步中已经完成了)并定义优化器(在本例中，我们使用的是<em class="nv"> RMSprop </em>优化器)。一旦一切准备就绪，我们就可以开始训练循环了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 13。培训阶段</p></figure><p id="ea8e" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">在第 15 行，我们对每个<em class="nv">时期</em>进行迭代。在第 20 行，我们使用<em class="nv">数据加载器</em>对每个<em class="nv">批次</em>进行迭代。在第 17 行<em class="nv">模型被设置为训练模式</em>(这意味着梯度将被更新)。在第 25 行，模型被输入。在第 28 行计算了误差。在第 29 行，存放梯度的变量被清除。在第 34 行计算梯度。在第 37 行，参数被更新。</p><p id="2e67" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">最后，保存预测，并调用评估函数来获得训练和测试数据的准确性。所以，让我们在下一节看到这一点！</p><h1 id="bc67" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">估价</h1><p id="6db5" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">太好了，我们终于进入评估环节了。让我们看看这是怎么回事。</p><p id="a429" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">这次我们将使用准确性作为衡量模型性能的标准。虽然有预定义的函数可以直接计算，但这次我们将手动计算，即计算<em class="nv">真阳性</em>以及<em class="nv">假阳性</em>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码 14。评价函数</p></figure><p id="88ab" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">在这种情况下，我们将 0.5 定义为阈值，以确定结果类的值是正还是负。</p><p id="8d85" class="pw-post-body-paragraph lk ll it lm b ln mh ju lp lq mi jx ls lt mv lv lw lx mw lz ma mb mx md me lj im bi translated">恭喜你！我们已经到了这个教程博客的结尾。该准则欢迎任何建议和/或评论。随意分叉或克隆！</p><blockquote class="nw nx ny"><p id="cce0" class="lk ll nv lm b ln mh ju lp lq mi jx ls nz mv lv lw oa mw lz ma ob mx md me lj im bi translated">这里有完整的实现:<a class="ae ky" href="https://github.com/FernandoLpz/Text-Classification-CNN-PyTorch" rel="noopener ugc nofollow" target="_blank">https://github . com/FernandoLpz/Text-class ification-CNN-py torch</a></p></blockquote><h1 id="ff2b" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">结论</h1><p id="a929" class="pw-post-body-paragraph lk ll it lm b ln nq ju lp lq nr jx ls lt ns lv lw lx nt lz ma mb nu md me lj im bi translated">在这篇教程博客中，我们学习了如何使用基于卷积的神经网络架构实现 PyTorch 框架来生成文本分类模型。</p></div></div>    
</body>
</html>