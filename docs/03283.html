<html>
<head>
<title>Simple Explanation of Transformers in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP中变压器的简单说明</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-explanation-of-transformers-in-nlp-da1adfc5d64f?source=collection_archive---------12-----------------------#2020-03-29">https://towardsdatascience.com/simple-explanation-of-transformers-in-nlp-da1adfc5d64f?source=collection_archive---------12-----------------------#2020-03-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c68a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">NLP中变压器的简单易懂的解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2719572d838f4aad489f0ce547eaaaed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H1MTpwr3VjfB4B6ajOBj3Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯·劳顿在<a class="ae ky" href="https://unsplash.com/s/photos/transformation?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="73e0" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">先决条件:</h2><p id="58d2" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">S <a class="ae ky" rel="noopener" target="_blank" href="/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a">序列-2-序列-模型-关注机制</a></p><p id="b4b1" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/intuitive-explanation-of-neural-machine-translation-129789e3c59f"> Seq2Seq-神经机器翻译</a></p><p id="b9eb" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">在本帖中，我们将解决以下与Transformer相关的问题</p><ul class=""><li id="aa0d" class="mt mu it lx b ly mo mb mp li mv lm mw lq mx mn my mz na nb bi translated"><em class="nc">我们为什么需要变压器，Sequence2Sequence模型有哪些挑战？</em></li><li id="c35c" class="mt mu it lx b ly nd mb ne li nf lm ng lq nh mn my mz na nb bi translated"><em class="nc">变压器及其详细架构</em></li><li id="8162" class="mt mu it lx b ly nd mb ne li nf lm ng lq nh mn my mz na nb bi translated"><em class="nc">深入探讨变形金刚中使用的术语，如位置编码、自我关注、多头自我关注、掩蔽多头自我关注</em></li><li id="8a11" class="mt mu it lx b ly nd mb ne li nf lm ng lq nh mn my mz na nb bi translated"><em class="nc">可以使用变压器的NLP任务</em></li></ul><h2 id="4aba" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">序列2序列(Seq2Seq)的挑战</h2><ul class=""><li id="5b46" class="mt mu it lx b ly lz mb mc li ni lm nj lq nk mn my mz na nb bi translated"><strong class="lx iu">顺序计算</strong>:在Seq2Seq中，我们以顺序方式在每一步向编码器输入一个单词，以便在解码器中一次生成一个单词的输出。我们可以通过并行化操作来实现计算效率，这在Seq2Seq建模中是不可能的</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/427aa872ddab967745bd33c7b8f3dd52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*14M77qM-Vdif7EfieK0YYw.png"/></div></div></figure><ul class=""><li id="ffa9" class="mt mu it lx b ly mo mb mp li mv lm mw lq mx mn my mz na nb bi translated"><strong class="lx iu">长期依赖</strong>:长期依赖是Seq2Seq的一个问题，因为需要对一个长句子执行大量操作，如下所示。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/7469fb3d7149c2fe6c56a47c8d9db81f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7NdNgobiuUKb4HoA7WeCbA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">“它”这个词指的是“冠状病毒”或“国家”。</p></figure><p id="4a1a" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><em class="nc">让我们深入变压器的架构和变压器的关键概念，了解变压器如何应对这些挑战</em></p><h2 id="d435" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">变压器架构</h2><p id="7fab" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">与Seq2Seq不同，Transformer具有6个编码器和6个解码器的堆栈；编码器<strong class="lx iu">包含两个子层:多头自关注层和全连接前馈网络</strong>。</p><p id="633a" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><strong class="lx iu">解码器包含三个子层，一个多头自关注层，一个对编码器输出执行多头自关注的附加层，以及一个完全连接的前馈网络。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/daae7b58a8d4de719575f25679365d9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*xVgpgLKz0wmv0O3JfFC9Tw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">高级变压器架构</p></figure><p id="42cc" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">编码器和解码器中的每个子层都有一个残差连接，然后是层归一化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/9bcf332a7219a4ccca52dffe418b81c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*1kV8wtm1pG2ax9Jlib_L6A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">编码器和解码器的单一实例</p></figure><h2 id="94d0" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">编码器和解码器的输入</h2><p id="c542" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">使用学习嵌入将编码器/解码器的所有输入和输出令牌转换成向量。这些输入嵌入然后被传递到位置编码。</p><p id="3614" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><strong class="lx iu">位置编码</strong></p><p id="4367" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">转换器的架构不包含任何递归或卷积，因此没有词序的概念。输入序列的所有字都被馈送到网络，没有特殊的顺序或位置，因为它们都同时流经编码器和解码器堆栈。</p><p id="de4d" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">要理解一个句子的意思，理解单词的位置和顺序是必不可少的。</p><p id="f743" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><strong class="lx iu">位置编码被添加到模型中，以帮助注入关于句子中单词的相对或绝对位置的信息</strong></p><p id="28ec" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">位置编码与输入嵌入具有相同的维数，因此两者可以相加。</p><h2 id="b3aa" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">自我关注</h2><p id="fa0f" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">简单地说，注意力是为了更好地理解句子中单词的含义和上下文。</p><blockquote class="np"><p id="9627" class="nq nr it bd ns nt nu nv nw nx ny mn dk translated">自我注意，有时被称为内部注意，是一种与单个序列的不同位置相关的注意机制，以便计算该序列的表示</p></blockquote><p id="21ad" class="pw-post-body-paragraph lv lw it lx b ly nz ju ma mb oa jx md li ob mf mg lm oc mi mj lq od ml mm mn im bi translated"><strong class="lx iu">自我关注层将所有位置与恒定数量的顺序执行操作连接起来，因此比递归层更快</strong></p><p id="cc01" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><strong class="lx iu">转换器中的注意功能被描述为将查询和一组键和值对映射到输出</strong>。查询、键和值都是向量。<strong class="lx iu">使用句子</strong>中每个单词的标度点积注意力来计算注意力权重。最终得分是这些值的加权和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/5e5546298f0c6f100b365d0c3b1b5ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*WmIvqElftp2_Ee0oNIX09w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/7181-attention-is-all-you-need . pdf</a></p></figure><p id="ff60" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><em class="nc">我们用一句话来理解这个吧，</em> <strong class="lx iu"> <em class="nc">“我享受自然。”</em>T15】</strong></p><p id="04c8" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">输入是查询、键和值。向量的维数是64，因为这导致稳定的梯度。</p><p id="5709" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><strong class="lx iu">第一步:点积</strong></p><p id="b9ff" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">为句子中的每个单词取查询和关键字的点积。点积决定了谁更关注输入句子中的其他单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f2d4711ba8a9b548f8cda96462516cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*k3uw0wJFdpHCekRyCdvLXg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第一步:点积</p></figure><p id="6fe1" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><strong class="lx iu">第二步:缩放</strong></p><p id="78d9" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">通过除以关键向量维数的平方根来缩放点积。维度为64；因此我们将点积除以8。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6a395b41a6136487780db45bec98558b.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*FbVkF4UCH4_33dJfQDtM1g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤2:缩放点积</p></figure><p id="c562" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">第三步:应用软最大值</p><p id="b47f" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">Softmax将缩放值规格化。应用Softmax后，所有值都是正的，加起来等于1</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a3f6e6a64160afcb3d08ec388abf5d19.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*2fg4pEinDs6ih1vdlTYgdQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤3:应用Softmax来归一化缩放值</p></figure><p id="a838" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><strong class="lx iu">第四步:计算值的加权和</strong></p><p id="708e" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">我们应用标准化分数和值向量之间的点积，然后计算总和</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/2de9ccc139b617b8a3d6b82727f401c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*hVjvT2r1E6e8MIxuHC1Q7Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤4:值的加权和</p></figure><p id="fd1a" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">自我关注的完整方程式</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/02b5a4ab6c2e426beb0030c5b1abc293.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*LVdLFpZS8-5Db6xvvwewjA.png"/></div></figure><p id="a0d9" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">对句子中的每个单词重复这些步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/7fffe9d0d0bf2580445b1e67a7b51695.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*lBGkrgBbacy3I541l08byQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">句子中所有单词的注意力权重</p></figure><h2 id="3691" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">多头注意力</h2><p id="5e6d" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">变形金刚使用多个注意力头，而不是使用单一的注意力功能，在单一的注意力功能中，注意力可以由实际的单词本身支配。</p><p id="8768" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">每个注意力头部具有应用于相同输入表示的不同线性变换。<strong class="lx iu">变压器使用8个不同的注意头，并行独立计算。使用八个不同的注意头，我们有八组不同的查询、键和值，还有八组编码器和解码器，每组都是随机初始化的</strong></p><blockquote class="np"><p id="f7f3" class="nq nr it bd ns nt nu nv nw nx ny mn dk translated">"多头注意力允许模型在不同的位置共同注意来自不同表征子空间的信息."</p></blockquote><figure class="ol om on oo op kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9de09d8c6225226ee5980ef10db27b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*jTYrbq8ByKukKdpJDcCSgg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多头关注；来源:<a class="ae ky" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/7181-attention-is-all-you-need . pdf</a></p></figure><p id="5384" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">利用多头注意力头，输入表征的每个部分与输入表征的其他部分相互作用，以获得更好的含义和上下文。由于多头注意力在不同位置观察不同的表征子空间，这也有助于学习长期依赖性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/7469fb3d7149c2fe6c56a47c8d9db81f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7NdNgobiuUKb4HoA7WeCbA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">“它”这个词指的是“冠状病毒”或“国家”。</p></figure><p id="8019" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">使用多头注意力，我们得到上面句子中的单词“它”指的是“冠状病毒”。</p><h2 id="6cea" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">掩蔽的多头注意力</h2><p id="222a" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">解码器屏蔽了多头注意力，它屏蔽或阻止解码器输入进入后续步骤。在训练期间，解码器的多头注意力隐藏了未来的解码器输入。</p><p id="b0e3" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">对于使用转换器将句子“我享受自然”从英语翻译成印地语的机器翻译任务，解码器将考虑所有输入单词“我享受自然”来预测第一个单词。</p><p id="20d1" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">下表显示了解码器如何阻止后续步骤的输入</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/5aa305e759f47304e9de2fa5ac863491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*eB6uboyzH8TLvC-KKmxazQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练期间掩蔽的多头注意力解码器输入和预测</p></figure><p id="0bd7" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">编码器和解码器中的每个子层都有一个残差连接，然后是层归一化。</p><h2 id="360d" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">残差连接和图层归一化有什么帮助？</h2><p id="581d" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated"><strong class="lx iu">剩余连接</strong>是“跳过连接”，允许梯度流过网络，而不通过非线性激活函数。残留连接有助于避免消失或爆炸梯度问题。为了使剩余连接工作，模型中每个子层的输出应该是相同的。变压器中的所有子层产生维度512的输出。</p><p id="95f1" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><strong class="lx iu">图层标准化:</strong>标准化每个特征的输入，独立于其他示例，如下所示。层标准化减少了前馈神经网络的训练时间。在层标准化中，我们在单个训练案例上计算层中神经元的所有总输入的均值和方差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/ff581161c9431dfa32039086c79d5519.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*8wuiEQtx2a8_OxJWT5rvIw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图层规范化</p></figure><h2 id="0678" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">全连接层</h2><p id="e668" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">变换器中的编码器和解码器都有一个完全连接的前馈网络，它有两个线性变换，中间包含一个ReLU激活。</p><h2 id="6e6b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">解码器的线性和软最大层</h2><p id="68ac" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">解码器的最后一层应用线性变换和softmax函数来转换解码器输出，以预测输出概率</p><h2 id="e3f3" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">变压器的特点</h2><p id="e6f1" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">seq2seq模型的缺点由Transformer解决</p><ul class=""><li id="d82a" class="mt mu it lx b ly mo mb mp li mv lm mw lq mx mn my mz na nb bi translated"><strong class="lx iu">并行计算</strong> : Transformer的架构去除了Seq2Seq模型中使用的自回归模型，完全依靠自我关注来理解输入和输出之间的全局依赖关系。自我关注对并行计算有很大帮助</li><li id="0ee3" class="mt mu it lx b ly nd mb ne li nf lm ng lq nh mn my mz na nb bi translated"><strong class="lx iu">操作次数减少</strong>:变形金刚的操作次数不变，因为注意力权重在多头注意力中是平均的</li><li id="fee2" class="mt mu it lx b ly nd mb ne li nf lm ng lq nh mn my mz na nb bi translated"><strong class="lx iu">长程相关性</strong>:影响长程相关性学习的因素基于信号在网络中必须经过的前向和后向路径的长度。输入和输出序列中任何位置组合之间的路径越短，就越容易了解长程相关性。自我注意层将所有位置与学习长程依赖的恒定数量的顺序执行的操作连接起来。</li></ul><h2 id="b06c" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">变压器处理的NLP任务</h2><ul class=""><li id="5223" class="mt mu it lx b ly lz mb mc li ni lm nj lq nk mn my mz na nb bi translated">抽象文本摘要</li><li id="5b3d" class="mt mu it lx b ly nd mb ne li nf lm ng lq nh mn my mz na nb bi translated">神经机器翻译</li></ul><h2 id="c6e1" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">结论:</h2><p id="8901" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">变压器具有基于自关注机制的简单网络架构，并且不完全依赖于递归和卷积。计算是并行执行的，这使得变压器效率更高，并且需要更少的训练时间</p><h2 id="0f0b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">参考资料:</h2><p id="4b1b" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1607.06450.pdf" rel="noopener ugc nofollow" target="_blank">吉米·巴雷、杰米·瑞安·基罗斯和杰弗里·e·辛顿的图层规范化</a></p><p id="b0d1" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><a class="ae ky" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的一切</a></p><div class="os ot gp gr ou ov"><a href="http://jalammar.github.io/illustrated-transformer/" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">图示的变压器</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">讨论:黑客新闻(65分，4条评论)，Reddit r/MachineLearning (29分，3条评论)翻译…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">jalammar.github.io</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj ks ov"/></div></div></a></div><p id="df22" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated"><a class="ae ky" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p></div></div>    
</body>
</html>