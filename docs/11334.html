<html>
<head>
<title>Introduction to generative and discriminative models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生成模型和判别模型介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-generative-and-discriminative-models-9c9ef152b9af?source=collection_archive---------15-----------------------#2020-08-06">https://towardsdatascience.com/introduction-to-generative-and-discriminative-models-9c9ef152b9af?source=collection_archive---------15-----------------------#2020-08-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/aaef7b7e707d15b405a4844aee000edf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LF7MUP7jrGqPAzVZ"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">Pawel Nolbert 在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="35d8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">简介</strong></p><p id="b1b9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最近我在工作中做了一个演示，在那里我解释了如何使用<a class="ae kf" href="https://en.wikipedia.org/wiki/Conditional_random_field" rel="noopener ugc nofollow" target="_blank">条件随机场(CRF) </a>解决一些问题。由于我的同事对 CRF 知之甚少，我有几张幻灯片专门介绍这个算法背后的理论。当我准备理论幻灯片时，我觉得有责任将 CRF 与概念上类似的算法<a class="ae kf" href="https://en.wikipedia.org/wiki/Hidden_Markov_model" rel="noopener ugc nofollow" target="_blank">隐马尔可夫模型(HMM) </a>进行比较。已知 CRF 是判别模型，而 HMM 是生成模型。我必须更新我对监督机器学习方法分类的知识，特别是生成模型。现在我想简单地分享一下我对生成模式和判别模式之间区别的理解。</p><p id="eb8c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="le">生成模型</em> </strong>是一大类机器学习算法，通过对联合分布<em class="le"> P(y，x) </em>建模来进行预测。</p><p id="cadf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="le">判别模型</em> </strong>是一类有监督的机器学习模型，通过估计条件概率<em class="le"> P(y|x)进行预测。</em></p><p id="6e06" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使用生成模型，应该解决更多的未知数:必须估计每个类的概率和给定类的观察概率。这些概率用来计算联合概率，最后联合概率可以作为条件概率的替代来进行预测。</p><figure class="lg lh li lj gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lf"><img src="../Images/432d5d3cb50ce20c290b058aee1e6332.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_nXM1QK_jEGjH_-imJDQ5A.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">为了估计条件概率 P(y|x ),生成模型比判别模型有更多的步骤</p></figure><p id="7180" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">判别模型采取了一种更简单的方式:它只是直接估计条件概率。</p><p id="a484" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每种模式都有许多优点和缺点。我只是注意到生成模型可以用来生成新的样本，但是它需要更多的数据。给定相同数量的数据，判别模型通常优于生成模型，但是它不知道特征之间的依赖性，因为它与预测无关。因此，判别模型不能产生新的样本。</p><p id="7639" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们仔细看看生成模型的概念。</p><p id="0ac6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">生成模型</strong></p><blockquote class="lk ll lm"><p id="b248" class="kg kh le ki b kj kk kl km kn ko kp kq ln ks kt ku lo kw kx ky lp la lb lc ld im bi translated"><em class="it">正如我前面所展示的，要做出预测，条件分布</em> P(y|x) <em class="it">就足够了。但由于</em> P(y|x) = P(y，x) / P(x) <em class="it">，其中</em> P(x) <em class="it">为常数对于给定的</em> x <em class="it">和</em> <em class="it">所有可能的</em> y <em class="it">，使用联合分布</em> P(y，x) <em class="it">进行预测是有效的。</em></p></blockquote><p id="4e5f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过模拟联合分布<em class="le"> P(y，x) </em>意味着对于每一对(<em class="le">、</em>)，概率<em class="le"> P(易、)</em>是已知的(被模拟)。一开始，我甚至有点难以理解这是怎么可能的——<strong class="ki iu"><em class="le">X</em></strong>的可能值的范围可能是巨大的，所以建议每个<em class="le"/>的概率是不现实的，更不用说一对(<em class="le">易</em>、<em class="le">)了。应该怎么做呢？</em></p><p id="4342" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">首先。</strong>贝叶斯定理！它将联合概率<em class="le"> P(y，x) </em>的计算分解为另外两种类型的概率的计算:类概率<em class="le">P(y)</em>和给定类的观察概率<em class="le"> P(x|y)。</em></p><p id="fb9e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le"> P(y，x) = P(y) * P(x|y) </em></p><p id="4284" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它有什么好处？这样至少更容易计算出概率<em class="le"> P(y) </em>，因为它可以通过计算类别频率从数据集进行估计。<em class="le"> P(x|y) </em>比较棘手，因为通常<em class="le"> x </em>不只是一个特征，而是一组特征:<em class="le"> x = xi，…，xn </em>，它们之间可能存在依赖关系<em class="le">。</em></p><p id="17bf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">P(x | y)=пP(Xi | y，x1，xi-1，xi+1，xn) </em></p><p id="6279" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常特征之间的依赖关系是未知的，特别是当它们出现在复杂的星座中时(<em class="le"> y，x1，xi-1，xi+1，xn) </em>)。</p><p id="385d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么估计<em class="le"> P(x|y) </em>应该怎么做呢？为此，有以下诀窍:</p><p id="c92b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">第二。</strong>胡乱假设！或者仅仅是一些假设，使得<em class="le"> P(x|y) </em>的估计易于处理。朴素贝叶斯分类器可以作为具有这种假设的生成模型的完美例子，这使得<em class="le"> P(x|y) </em>的计算更容易。也就是说，它具有特征之间的独立性假设<em class="le"> xi，…，xn。</em></p><p id="f2b1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">P(x | y)=пP(Xi | y)</em></p><p id="3ca3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过这种放松，对<em class="le"> P(x|y) </em>的估计是易处理的，因为每个<em class="le"> P(xi|y) </em>都可以通过独立于其他特征找到离散特征<em class="le"> xi </em>的频率或者使用高斯分布来估计，如果特征<em class="le"> xi </em>是连续的。</p><p id="03e8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">结论</strong></p><p id="21be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在你可以看到，为了使用生成模型，你应该准备好估计两种类型的概率<em class="le"> P(y) </em>和<em class="le"> P(x|y)。</em>同时，判别模型直接估计条件概率<em class="le"> P(y|x) </em>，这通常更有效，因为人们不估计特征之间的依赖性，因为这些关系不一定有助于目标变量的预测。</p></div></div>    
</body>
</html>