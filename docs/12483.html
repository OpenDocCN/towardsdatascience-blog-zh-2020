<html>
<head>
<title>Text Summarization Using Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度神经网络的文本摘要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-summarization-using-deep-neural-networks-e7ee7521d804?source=collection_archive---------7-----------------------#2020-08-28">https://towardsdatascience.com/text-summarization-using-deep-neural-networks-e7ee7521d804?source=collection_archive---------7-----------------------#2020-08-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="65e5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于 seq2seq 模型的文摘综述</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7f9ad72d5e4f7596b155b027404aad7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Q9QKpaqTz9wWZ-5V"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@californong?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">农旺</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="968b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">简介</strong></h1><p id="a87e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">每天产生的文本数据量在复杂性和数量上都在快速增长。社交媒体、新闻文章、电子邮件、短信(清单还在继续..)，产生海量信息，浏览冗长的文字材料变得繁琐(也很无聊！).谢天谢地，随着深度学习的进步，我们可以建立模型来缩短长文本，并产生清晰连贯的摘要，以节省时间并有效地理解关键点。</p><p id="14d9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们可以将文本摘要大致分为两种类型:</p><p id="d97c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> 1。提取摘要:</strong>这种技术包括从输入的句子中提取重要的单词/短语。基本思想是通过从输入句子中选择最重要的单词来创建摘要</p><p id="9fb8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> 2。抽象概括:</strong>这种技术包括生成全新的短语来捕捉输入句子的意思。潜在的想法是把重点放在形式上——旨在生成语法摘要，因此需要高级语言建模技术。</p><p id="a2c7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在本文中，我们将使用 PyTorch 建立一个序列 2 序列(编码器-解码器)模型，使用 GRU 进行简单的点积注意力，并评估他们的注意力得分。我们将进一步研究像 BLEU、ROUGE 这样的指标来评估我们的模型。</p><p id="5d73" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">使用的数据集:<br/> </strong>我们将使用 wikihow 数据集，该数据集包含大约 200，000 对长序列文章及其标题。该数据集是可用于摘要的大规模数据集之一，文章的长度变化很大。这些文章在写作风格上非常多样化，这使得摘要问题更具挑战性和趣味性。</p><p id="81cb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp">关于数据集的更多信息:</em><a class="ae kv" href="https://arxiv.org/abs/1810.09305" rel="noopener ugc nofollow" target="_blank"><em class="mp">https://arxiv.org/abs/1810.09305</em></a></p><h1 id="ac3b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">数据预处理</strong></h1><p id="c093" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">预处理和清理是一个重要的步骤，因为在不干净和混乱的数据上建立模型会反过来产生混乱的结果。在将数据输入模型之前，我们将应用以下清理技术:</p><ol class=""><li id="6469" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">将所有文本转换为小写以便进一步处理</li><li id="c205" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">解析 HTML 标签</li><li id="ffd6" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">正在删除()和[]之间的文本</li><li id="0904" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">缩略映射—替换单词的缩略版本(例如，不能被替换为不能等等)</li><li id="ff6a" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">删除撇号</li><li id="c13a" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">删除标点符号和特殊字符</li><li id="3fb8" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">使用 nltk 库删除停用词</li><li id="a10f" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">仅保留长单词，即长度大于 3 的单词</li></ol><p id="52f1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们将首先以字典的形式定义缩写:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="c57c" class="nj kx iq nf b gy nk nl l nm nn">## Find the complete list of contractions on my Github Repo</span><span id="6603" class="nj kx iq nf b gy no nl l nm nn">contraction_mapping = {"ain't": "is not", "aren't": "are not"}<br/>stop_words = set(stopwords.words('english'))</span><span id="05a1" class="nj kx iq nf b gy no nl l nm nn"><strong class="nf ir">def</strong> text_cleaner(text,num):</span><span id="9a3d" class="nj kx iq nf b gy no nl l nm nn">  str = text.lower()<br/>  str = BeautifulSoup(str, "lxml").text<br/>  str = re.sub("[\(\[].*?[\)\]]", "", str)<br/>  str = ' '.join([contraction_mapping[t] <strong class="nf ir">if</strong> t <strong class="nf ir">in</strong> contraction_mapping      <strong class="nf ir">else</strong> t <strong class="nf ir">for</strong> t <strong class="nf ir">in</strong> str.split(" ")])<br/>  str = re.sub(r"'s\b","",str)<br/>  str = re.sub("[^a-zA-Z]", " ", str)<br/>  <br/>  <strong class="nf ir">if</strong>(num==0):<br/>      tokens = [w <strong class="nf ir">for</strong> w <strong class="nf ir">in</strong> str.split() <strong class="nf ir">if</strong> <strong class="nf ir">not</strong> w <strong class="nf ir">in</strong> stop_words]<br/>  <strong class="nf ir">else</strong>:<br/>      tokens=str.split()</span><span id="3536" class="nj kx iq nf b gy no nl l nm nn">  long_words=[]<br/>  <br/>  <strong class="nf ir">for</strong> i <strong class="nf ir">in</strong> tokens:<br/>      <strong class="nf ir">if</strong> len(i)&gt;3:          <em class="mp">#removing short words</em><br/>          long_words.append(i)<br/>  <br/>  <strong class="nf ir">return</strong> (" ".join(long_words)).strip()</span><span id="2455" class="nj kx iq nf b gy no nl l nm nn"><em class="mp">#calling the above function</em><br/>clean_text = []<br/>clean_summary = []</span><span id="e9e6" class="nj kx iq nf b gy no nl l nm nn"><strong class="nf ir">for</strong> t <strong class="nf ir">in</strong> df['text']:<br/>    clean_text.append(text_cleaner(t,0))</span><span id="d313" class="nj kx iq nf b gy no nl l nm nn"><strong class="nf ir">for</strong> t <strong class="nf ir">in</strong> df['headline']:<br/>    clean_summary.append(text_cleaner(t,0))</span></pre><h1 id="b507" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">深度模型设计</strong></h1><p id="0b65" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在将训练数据输入我们的深度模型之前，我们将每个单词表示为一个热点向量。然后，我们将需要每个单词的唯一索引，以将其用作网络的输入和输出。</p><p id="aef0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为此，我们将创建一个助手类<em class="mp"> Lang </em>，它具有<em class="mp">单词- &gt;索引</em>和<em class="mp">索引- &gt;单词</em>映射以及每个单词的计数。<br/>为了读取模型中的数据，我们以列表的形式创建了输入和输出对(Pairs- &gt; [Input，Output])</p><h1 id="6003" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak"> Seq2seq 模型，注意使用 GRU 和教师强制</strong></h1><p id="3f86" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将使用 seq2seq(编码器-解码器架构)模型，关注简单的点积。选择这种架构的基本思想是，我们手头有一个多对多的问题(n 个单词作为输入，m 个单词作为输出)。下图显示了该模型的详细架构图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/64c448ddae5b65dbe9f5f22bab6787b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQgKImpmNxUwJICZdybqsQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="ed48" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">该架构中有四个主要组件:</p><blockquote class="nq nr ns"><p id="7256" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><strong class="lq ir">编码器:</strong><em class="iq">seq 2 seq 模型的编码器层从输入文本中提取信息，并将其编码成单个向量，即上下文向量。<br/>基本上，对于每个输入单词，编码器都会生成一个隐藏状态和一个向量，将这个隐藏状态用于下一个输入单词。</em></p><p id="8502" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><em class="iq">我们使用</em> <strong class="lq ir"> <em class="iq"> GRU(门控递归单元)</em> </strong> <em class="iq">用于编码器层，以便捕捉长期依赖性——减轻使用普通 RNNs 时遇到的消失/爆炸梯度问题。<br/>GRU 单元一次读取一个字，并使用更新和复位门，计算隐藏状态内容和单元状态。</em></p><p id="3056" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><strong class="lq ir">我发现下面两个链接对获得更多关于 GRU 工作的信息很有用:</strong> <br/> <a class="ae kv" href="https://arxiv.org/abs/1412.3555" rel="noopener ugc nofollow" target="_blank">门控递归神经网络序列建模的经验评估</a> <br/> <a class="ae kv" href="https://d2l.ai/chapter_recurrent-modern/gru.html" rel="noopener ugc nofollow" target="_blank">门控递归单元(GRU) </a></p><p id="f0e7" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><strong class="lq ir">解码器:</strong><em class="iq">seq 2 seq 模型的解码器层使用编码器最后的隐藏状态即上下文向量，生成输出字。一旦句子被编码，解码过程就开始，并且在每个步骤/时间，解码器被给予隐藏状态和输入令牌。<br/>在初始时间戳/状态，第一隐藏状态是上下文向量，输入向量是 SOS(字符串开始)。当到达 EOS(句子结束)时，解码过程结束。<br/>SOS 和 EOS 标记分别显式添加在每个句子的开头和结尾。</em></p><p id="083f" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><strong class="lq ir">注意机制:</strong> <em class="iq">利用编码器-解码器架构，将编码后的上下文向量传递给解码器，生成输出句子。</em>现在<em class="iq">如果输入的句子很长，单个上下文向量无法捕捉所有重要信息怎么办。</em> <strong class="lq ir"> <em class="iq">这就是注意力进入画面的地方！！！</em>T15】</strong></p><p id="1233" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><em class="iq">使用注意力的主要直觉是让模型聚焦/注意输入文本最重要的部分。</em>塞翁失马，焉知非福，<em class="iq">也有助于克服消失渐变问题。<br/>注意力有不同的类型——加法、乘法，但是，我们将使用基本的点积注意力作为我们的模型。</em></p><p id="bfee" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><strong class="lq ir"> 1。注意力分数</strong>首先通过计算编码器(h)和解码器(s)隐藏状态<br/> <strong class="lq ir"> 2 的点积来计算。</strong>这些注意力分数通过 Softmax 层转换成<strong class="lq ir">分布(α) </strong>。<br/> <strong class="lq ir"> 3。</strong>然后计算隐藏状态(z) 的<strong class="lq ir">加权和。<br/> <strong class="lq ir"> 4。</strong>该 z 然后与 s 连接，并通过 softmax 层使用<strong class="lq ir">“贪婪算法”</strong>(通过计算 argmax)生成单词</strong></p><p id="81d6" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><em class="iq">在这种架构中，我们不是直接使用最后一个编码器隐藏状态的输出，而是提供所有编码器隐藏状态的加权组合。这有助于模型关注长序列中的重要单词。</em></p><p id="df5a" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">支持方程</em> </strong></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/9fa2a55a5ba0f68a4e3638b179d21356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GT2jMNtRNx78qjDM8vRZgQ.png"/></div></div></figure><blockquote class="nq nr ns"><p id="0fbf" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">教师强制:</em> </strong> <em class="iq">一般来说，对于递归神经网络，一个状态的输出作为输入馈入下一个状态。这个过程导致收敛缓慢，从而增加了训练时间。</em></p><p id="ec6a" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">什么是老师强制<br/> </em> </strong> <em class="iq">老师强制通过向模型馈送实际值/地面真实值来解决这个收敛缓慢的问题。这种技术背后的基本直觉是，不是将解码器预测的输出作为下一个状态的输入，而是将基本事实或实际值馈送给模型。如果该模型预测了一个错误的单词，则可能导致所有被预测的其他单词都不正确的情况。因此，如果模型预测/生成了错误的单词，教师强制输入实际值，从而校正模型。</em></p><p id="10c0" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><em class="iq">教师强制是一种快速有效的训练 RNNs 的方法，然而，当生成的序列与训练过程中看到的不同时，这种方法可能导致</em> <strong class="lq ir">更脆弱/不稳定的模型<em class="iq"> </em> </strong> <em class="iq">。<br/>为了处理此类问题，我们将采用一种方法，随机选择使用地面实况输出或前一时间步生成的输出作为当前时间步的输入。</em></p><p id="51e5" class="lo lp mp lq b lr mk jr lt lu ml ju lw nt mm lz ma nu mn md me nv mo mh mi mj ij bi translated"><em class="iq">下面的代码片段展示了教师强制的动态实现</em></p></blockquote><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="edb1" class="nj kx iq nf b gy nk nl l nm nn">teacher_forcing_ratio = 0.5</span><span id="1b7c" class="nj kx iq nf b gy no nl l nm nn">use_teacher_forcing = <strong class="nf ir">True</strong> <strong class="nf ir">if</strong> random.random() &lt; teacher_forcing_ratio <strong class="nf ir">else</strong> <strong class="nf ir">False</strong><br/><br/>    <strong class="nf ir">if</strong> use_teacher_forcing:<br/>        <em class="mp"># Teacher forcing: Feed the target as the next input</em><br/>        <strong class="nf ir">for</strong> di <strong class="nf ir">in</strong> range(target_length):<br/>            decoder_output, decoder_hidden, decoder_attention = decoder(<br/>                decoder_input, decoder_hidden, encoder_outputs)<br/>            loss += criterion(decoder_output, target_tensor[di])<br/>            decoder_input = target_tensor[di]  <em class="mp"># Teacher forcing</em><br/><br/>    <strong class="nf ir">else</strong>:<br/>        <em class="mp"># W/O teacher forcing: use own predictions as the next input</em><br/>        <strong class="nf ir">for</strong> di <strong class="nf ir">in</strong> range(target_length):<br/>            decoder_output, decoder_hidden, decoder_attention = decoder(<br/>                decoder_input, decoder_hidden, encoder_outputs)<br/>            topv, topi = decoder_output.topk(1)<br/>            decoder_input = topi.squeeze().detach()  <em class="mp"># detach from history as input</em><br/><br/>            loss += criterion(decoder_output, target_tensor[di])<br/>            <strong class="nf ir">if</strong> decoder_input.item() == EOS_token:<br/>                <strong class="nf ir">break</strong></span></pre><h1 id="04e9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">模型评估指标</h1><p id="9a4c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">对于我们的文本摘要问题，可以有多个正确的答案，因为我们没有一个正确的输出，所以我们可以使用不同的参数来评估我们的模型，如召回率、精确度、F 分数。下面是我们将使用的一些指标:</p><p id="e18c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> BLEU(双语评估替角):</strong>该度量的基础是精度，其值在[0，1]之间，其中 1 表示完全匹配，0 表示完全不匹配。这个度量基本上是通过比较作为参考句子的一部分的机器生成的单词的数量与机器生成的输出中的单词总数来计算的。让我们借助一个例子来理解这一点:</p><p id="129e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">参考句子:<em class="mp">门被锁上</em> <br/>机器输出:<em class="mp">The</em><br/>BLEU Score =<strong class="lq ir">1</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/9eaea901f43ae1b2e47c024785d5679f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bIcZ_CDUibBBsdTSU3Bmdg.png"/></div></div></figure><p id="5022" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">上面的机器输出是一个极端的情况，但是，为了克服这个问题，BLEU 分数是以这样一种方式计算的，即生成的句子中的每个单词将被<strong class="lq ir"> <em class="mp">剪切</em> </strong>到该单词在参考句子中出现的次数。<br/>这基本上确保了如果一个单词出现的次数比它在参考句子中出现的次数多——在评估相似度时不会被考虑。<br/>应用上述规则后，我们得到修改后的<strong class="lq ir"> <em class="mp"> BLEU 分数为 1/3 </em> </strong></p><p id="6c29" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp">我们再来看看另一个极端的例子:</em></p><p id="61f4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">参考句子:<em class="mp">门被锁上</em> <br/>机器输出:<em class="mp"/><br/><em class="mp">BLEU 得分:</em> <strong class="lq ir"> <em class="mp"> 1(即使应用以上规则)</em> </strong></p><p id="a096" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这种情况下，在生成的句子的长度小于参考句子的情况下，引入<strong class="lq ir"> <em class="mp">简洁罚分(BP) </em> </strong>，即，如果生成的句子小于参考句子，则对 BLEU 分数有罚分，然而，当生成的句子长于参考句子时，不引入罚分。</p><p id="0b7a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">简洁罚分</strong>定义为-</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/333cce208a7c42fd849600f833b0abbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*x7sABHf9Qy9GhcEx_U8eDg.png"/></div></figure><p id="681e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中，<br/> r 是有效参考语料长度<br/> c 是生成/候选句子的长度</p><p id="78fe" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个指标是由 Kishore Papineni 在 2002 年首次提出的。有关此指标的更多详细信息，请参考以下链接:<br/> <a class="ae kv" href="https://www.aclweb.org/anthology/P02-1040.pdf" rel="noopener ugc nofollow" target="_blank"> BLEU:一种自动评估机器翻译的方法</a></p><p id="22f5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> ROUGE(面向回忆的理解，用于 Gisting 评估):</strong></p><p id="a374" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">ROUGE 基本上是一种<strong class="lq ir">面向回忆的措施</strong>，它通过比较参考句子中机器生成的单词数量与参考句子中的总单词数量来工作。</p><p id="1109" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个指标更直观，因为每次我们向池中添加一个引用，我们就扩展了交替摘要的空间。因此，当我们有多个引用时，应该首选这个指标。</p><p id="ef5b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">ROUGE 的实现与 BELU 非常相似，但是，还有其他底层实现，如 LCS(最长公共子序列)和 skip-gram 等。我们可以使用 python 库 ROUGE 直接使用<strong class="lq ir"> ROUGE-N 实现</strong>。</p><p id="1714" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">更多的细节，你可以参考下面的文章:<br/> <a class="ae kv" href="https://www.aclweb.org/anthology/W04-1013.pdf" rel="noopener ugc nofollow" target="_blank"> ROUGE:一个自动评估摘要的包</a></p><p id="4643" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们已经看到了基于精度的指标(BLEU)和面向召回的指标(ROUGE)，但是，如果我们希望我们的评估基于召回和精度，我们可以使用<strong class="lq ir"> <em class="mp"> F-Score </em> </strong>作为评估度量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/9aa954f89ea2991011b44cdc40b5084b.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*VJex_QlRPQItrf9WW8Yfrg.png"/></div></figure><p id="b994" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">结果:</strong>实现代码可以在我的<a class="ae kv" href="https://github.com/shivamduseja/" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。</p><p id="cad5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">该模型在 Google Colab Pro(T4 和 P100 GPU - 25GB 高内存虚拟机)上运行了约 6-7 个小时，它似乎在较短的摘要(~50 个单词)上运行良好。然而，可以通过<strong class="lq ir"> <em class="mp">进一步调整超参数</em> </strong>(学习率、优化器、损失函数、隐藏层、动量、迭代等)来优化模型。)</p><h1 id="96e0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">下一步…</h1><ol class=""><li id="7a7a" class="mq mr iq lq b lr ls lu lv lx oa mb ob mf oc mj mv mw mx my bi translated">双向/堆叠 GRU 单元可用于提高性能</li><li id="cbaa" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">实施不同类型的注意机制——倍增注意、多头注意等。</li><li id="9bc0" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">可以使用波束搜索而不是贪婪算法来选择输出字</li></ol><h1 id="7db7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><ol class=""><li id="6fa2" class="mq mr iq lq b lr ls lu lv lx oa mb ob mf oc mj mv mw mx my bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的一切</a></li><li id="000e" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated"><a class="ae kv" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/intermediate/seq 2 seq _ translation _ tutorial . html</a></li><li id="62f2" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">【https://www.aclweb.org/anthology/W04-1013.pdf T4】</li></ol></div></div>    
</body>
</html>