<html>
<head>
<title>Logistic Regression and Decision Boundary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归和决策边界</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-and-decision-boundary-eab6e00c1e8?source=collection_archive---------4-----------------------#2020-05-19">https://towardsdatascience.com/logistic-regression-and-decision-boundary-eab6e00c1e8?source=collection_archive---------4-----------------------#2020-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4d63" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解逻辑回归及其在分类中的效用</h2></div><p id="3f27" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">逻辑回归的基本应用是确定二元分类问题的决策边界。尽管基线是识别二元决策边界，但是该方法可以很好地应用于具有多个分类类别或多类别分类的场景。</p><h1 id="829d" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">决策边界是什么？</h1><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/5428a4b2ab85bc6b13e63f1dd601f952.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aE8XLyApqvaQA9B7MWjjlA.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">决策界限</p></figure><p id="dd5f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上图中，虚线可以被识别为决策边界，因为我们将在边界的每一侧观察不同类的实例。我们在逻辑回归中的意图是决定一个合适的决策边界，以便我们能够预测一个新的特征集可能对应于哪个类。关于逻辑回归的有趣事实是利用 sigmoid 函数作为目标类估计量。让我们看看这个决定背后的直觉。</p><h1 id="7dea" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">Sigmoid 函数</h1><p id="ef32" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">参数<strong class="kh ir"> z </strong>的 sigmoid 函数可以表示如下。请注意，函数总是位于 0 到 1 的范围内，边界是渐近的。这也给了我们一个完美的概率输出表示。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mo"><img src="../Images/72c93c8fcc5e701bcdc520fe3c5a547e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T9I5Ba3xSJTDGVNOYD-rig.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">Sigmoid 函数</p></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mp"><img src="../Images/0232c8d48ce912e5a324c8ec71e42bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vOlauLoy06nmNc1xOoLkMw.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">sigmoid 函数的绘图。请注意，x=0 时，y=0.5</p></figure><h2 id="83b9" class="mq lc iq bd ld mr ms dn lh mt mu dp ll ko mv mw ln ks mx my lp kw mz na lr nb bi translated">将二元分类建模为概率函数</h2><p id="51e1" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">现在我们知道我们的 sigmoid 函数位于 0 和 1 之间，我们可以如下表示类概率。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nc"><img src="../Images/d702d1c6f35cf268185b7ba13611c566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NptQhpt7mRlK7MhLhsrikQ.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">逻辑回归的数学模型</p></figure><p id="f8aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里<strong class="kh ir"> θ </strong>代表估计的参数向量，<strong class="kh ir"> X </strong>是考虑的变量向量。</p><pre class="lu lv lw lx gt nd ne nf ng aw nh bi"><span id="1292" class="mq lc iq ne b gy ni nj l nk nl">X = X0, X1 ... Xn    &lt;- n Features and X0=1<br/>θ = θ0, θ1 ... θn    &lt;- Parameters to be estimated</span></pre><h1 id="bfa2" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">参数估计和成本函数</h1><p id="2636" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">现在我们知道了概率的函数估计，我们将想出一种方法来估计由<strong class="kh ir"> θ </strong>向量表示的参数。在这个练习中，让我们考虑下面的例子。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mp"><img src="../Images/1a5a0c18adf6f102b7f83f7380cb4a38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*inq0-DIKHwydMV2uFQ6kXg.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">具有两个要素的数据集</p></figure><p id="def0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们有一个包含两个要素和两个类的数据集。我们的目标是为这两个特征找到合适的值<strong class="kh ir"> θ </strong>。这可以建模如下。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nm"><img src="../Images/fd161e056c2cd8b57e91350fafb84897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G4VwspEmOapDtagmTCeW6A.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">成本函数表示</p></figure><p id="122d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> h(θ) </strong>是概率估计或假设函数。</p><h2 id="90f4" class="mq lc iq bd ld mr ms dn lh mt mu dp ll ko mv mw ln ks mx my lp kw mz na lr nb bi translated">损失/成本函数</h2><p id="4332" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">暂时让我们假设我们可以使用<strong class="kh ir">均方根误差(RMS) </strong>，类似于线性回归。你可以参考下面的文章获得更多的见解。</p><div class="nn no gp gr np nq"><a rel="noopener follow" target="_blank" href="/calculus-behind-linear-regression-1396cfd0b4a9"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd ir gy z fp nv fr fs nw fu fw ip bi translated">线性回归背后的微积分</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">理解线性回归的数学方面</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe md nq"/></div></div></a></div><p id="5213" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为简单起见，我将绘制成本函数相对于<strong class="kh ir"> θ[0] </strong>的变化，这是我们估计量的偏差。这是基于我们的目标变量<strong class="kh ir"> y </strong>的表示如下:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi of"><img src="../Images/6033e4e174f371f97e22df1cb0e41062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pcYiqLzJ_zrr5m-3HYwzRw.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">使用<strong class="bd og"> θ </strong>向量的最终估计</p></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mp"><img src="../Images/4837310b1fcc783c2ce39816e313e4cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qruywufsvsB3OyT6dltE_g.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">均方根误差的损耗<strong class="bd og"> θ[1] </strong>和<strong class="bd og"> θ[2] </strong>为<strong class="bd og">【0，0】</strong></p></figure><p id="3481" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到有两个局部最优解。这是出乎意料的，是由我们的 sigmoid 函数的行为引起的。因此，成本函数表示如下，这与我们的预期完全匹配。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oh"><img src="../Images/110f44935cecbe1e999e66cd66e5307c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xvr9CXgOHirdzdYZgjIX-A.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">更好的损失/成本函数</p></figure><p id="cc5a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个分段函数，在不同的<strong class="kh ir"> y </strong>值下有不同的定义。这个想法是指数地惩罚错误的分类。例如，对于标签<strong class="kh ir"> y=1 </strong>如果模型预测<strong class="kh ir"> h(x)=0 </strong>，我们将使第一个方程达到无穷大，反之亦然。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mp"><img src="../Images/92a68f446965c99241e03de95a121ea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n1lMNu3lRZDinkSv8xlQTw.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">新的损失/成本函数的行为</p></figure><p id="4e66" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们手头有了一个更好的损失函数，让我们看看如何估计这个数据集的参数向量<strong class="kh ir"> θ </strong>。</p><h1 id="b186" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">逻辑回归的梯度下降</h1><p id="7797" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">因为我们知道损失函数，我们需要计算损失函数的导数来更新梯度。可以这样做。给定 sigmoid 函数的导数的性质，整个操作变得极其简单。它会给我们留下下面的损失函数。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oi"><img src="../Images/70e405250130e4027e1fea95d99c47b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*94cnb4alGub8bFVFcCmM2g.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">逻辑回归的损失函数</p></figure><p id="70e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，这正是我在上面引用的文章中讨论的线性回归损失/成本函数。因为我已经实现了算法，所以在本文中让我们使用 python sklearn 包的逻辑回归器。</p><h1 id="4f77" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">使用 sklearn 逻辑回归模块</h1><p id="05c4" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">用法非常简单。然而，理解估计的参数是很重要的。模型拟合可以如下进行。这里<strong class="kh ir"> X </strong>是一个二维向量，而<strong class="kh ir"> y </strong>是一个二元向量。</p><pre class="lu lv lw lx gt nd ne nf ng aw nh bi"><span id="1766" class="mq lc iq ne b gy ni nj l nk nl">from sklearn.linear_model import LogisticRegression</span><span id="1430" class="mq lc iq ne b gy oj nj l nk nl">clf = LogisticRegression(random_state=0).fit(X, y)</span></pre><p id="667e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">估计的参数可以确定如下。</p><pre class="lu lv lw lx gt nd ne nf ng aw nh bi"><span id="0ef0" class="mq lc iq ne b gy ni nj l nk nl">print(clf.coef_)<br/>print(clf.intercept_)</span><span id="c91e" class="mq lc iq ne b gy oj nj l nk nl">&gt;&gt;&gt; [[-3.36656909  0.12308678]]<br/>&gt;&gt;&gt; [-0.13931403]</span></pre><p id="d59a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">系数是特征的乘数。我们用索引 1 和 2 中的<strong class="kh ir"> θ </strong>向量来表示它们。截距是模型的偏差值。</p><p id="c83a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">拟合后逻辑回归的使用可以如下进行。</p><pre class="lu lv lw lx gt nd ne nf ng aw nh bi"><span id="f24c" class="mq lc iq ne b gy ni nj l nk nl">clf.predict_proba([[ 0.8780991 ,  0.89551051]])<br/>&gt;&gt;&gt; array([[0.95190421, 0.04809579]])</span></pre><p id="4e4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是对每个类的预测。注意总概率等于 1。使用下面的实现也可以达到同样的效果。<strong class="kh ir">h(θ，xi) </strong>是利用学习到的<strong class="kh ir">θ</strong>参数的<strong class="kh ir">假设</strong>函数。</p><pre class="lu lv lw lx gt nd ne nf ng aw nh bi"><span id="21dc" class="mq lc iq ne b gy ni nj l nk nl">def h(theta, xi):<br/>    return 1/(1 + np.exp(-1*np.dot(xi, theta)))</span></pre><p id="6354" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，我使用了<code class="fe ok ol om ne b">np.dot()</code>来获得矩阵或向量乘法，这比使用<code class="fe ok ol om ne b">for</code>循环要有效得多。这也叫<em class="on">矢量化</em>。我们可以使用下面的函数调用来获得 p(y=1)的估计值。</p><pre class="lu lv lw lx gt nd ne nf ng aw nh bi"><span id="f8f9" class="mq lc iq ne b gy ni nj l nk nl">h([-0.13931403, -3.36656909,  0.12308678], [1, 0.8780991 ,  0.89551051])</span></pre><p id="fc7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我已经使用我们的截距值作为第一个元素的<strong class="kh ir">θ</strong>参数和其余的顺序。我已经为对应于学习偏差的特征向量附加了 1。</p><p id="0fdf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们可以如下绘制我们的边界。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mp"><img src="../Images/2a574959d99c2c299ec5a423539de885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QNolqdBHwLtb50Hg8R2pUA.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">判别边界</p></figure><h1 id="3116" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">逻辑回归的扩展</h1><p id="ba8d" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">逻辑回归可以很容易地扩展到预测 2 类以上。然而，您将不得不构建<strong class="kh ir"> k </strong>分类器来预测<strong class="kh ir"> k </strong>多个类中的每一个，并针对每个类使用<strong class="kh ir"> i </strong> vs 其他<strong class="kh ir"> k-1 </strong>类来训练它们。</p><h2 id="3260" class="mq lc iq bd ld mr ms dn lh mt mu dp ll ko mv mw ln ks mx my lp kw mz na lr nb bi translated">重要事实</h2><ol class=""><li id="f812" class="oo op iq kh b ki mj kl mk ko oq ks or kw os la ot ou ov ow bi translated">逻辑回归是一种快速的机器学习技术</li><li id="d440" class="oo op iq kh b ki ox kl oy ko oz ks pa kw pb la ot ou ov ow bi translated">除了我们讨论过的简单梯度下降之外，大多数实现都使用了更快的优化器</li><li id="0c60" class="oo op iq kh b ki ox kl oy ko oz ks pa kw pb la ot ou ov ow bi translated">检查决策边界的存在总是明智的。你可能需要像主成分分析或 SNE 霸王龙这样的技术。</li></ol><p id="764e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你喜欢阅读这篇关于逻辑回归的文章。你可以在这里找到 Jupyer 笔记本<a class="ae pc" href="https://gist.github.com/anuradhawick/a64b8c3c8700d3816c283f03c3775b75" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="efe4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为好奇者提供一些有趣的读物；</p><div class="nn no gp gr np nq"><a rel="noopener follow" target="_blank" href="/what-can-you-do-with-gnns-5dbec638b525"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd ir gy z fp nv fr fs nw fu fw ip bi translated">你能用 GNNs 做什么</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">图形神经网络的操作、效用和优势</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="pd l ob oc od nz oe md nq"/></div></div></a></div><p id="98ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">干杯！</p></div></div>    
</body>
</html>