<html>
<head>
<title>Introduction to Encoder-Decoder Models — ELI5 Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编码器-解码器模型简介— ELI5路</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-encoder-decoder-models-eli5-way-2eef9bbf79cb?source=collection_archive---------9-----------------------#2020-01-05">https://towardsdatascience.com/introduction-to-encoder-decoder-models-eli5-way-2eef9bbf79cb?source=collection_archive---------9-----------------------#2020-01-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ac31" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">ELI5项目机器学习</h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/64a25c0c91289d3bc4eeea97aafd31bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*V_CW96WNQSM_Clhr"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ko" href="https://unsplash.com/@pafuxu?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Kouji鹤</a>拍摄的照片</p></figure><p id="049c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">大家好，欢迎来到我的博客“<strong class="kr jd">编解码模型介绍——Eli 5 Way</strong>”。我叫<a class="ae ko" href="https://www.linkedin.com/feed/" rel="noopener ugc nofollow" target="_blank"> Niranjan Kumar </a>，是好事达印度公司的高级数据科学顾问。</p><p id="8f6b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本文中，我们将讨论编码器-解码器模型的基本概念及其在一些任务中的应用，如语言建模、图像字幕、文本推导和机器音译。</p><blockquote class="ln lo lp"><p id="2690" class="kp kq lq kr b ks kt ku kv kw kx ky kz lr lb lc ld ls lf lg lh lt lj lk ll lm im bi translated"><strong class="kr jd"> <em class="it">引用注:</em> </strong> <em class="it">本文的内容和结构是基于我对四分之一实验室深度学习讲座的理解——</em><a class="ae ko" href="https://padhai.onefourthlabs.in/" rel="noopener ugc nofollow" target="_blank"><em class="it">pad hai</em></a><em class="it">。</em></p></blockquote><p id="2166" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我们讨论编码器-解码器模型的概念之前，我们先回顾一下语言建模的任务。</p><h1 id="1ea5" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">语言建模—概述</h1><p id="7326" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">语言建模是预测下一个单词/字母的任务。与FNN和CNN不同，在序列建模中，当前输出依赖于先前输入，并且输入的长度不固定。</p><p id="d049" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">给定“t-1”个单词，我们感兴趣的是基于先前的单词或信息来预测iᵗʰ单词。让我们看看如何使用递归神经网络解决语言建模问题。</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mx"><img src="../Images/dcd05a1effb1d525afe6b075fe50dd1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zlf1qcFzVggitP3XIpcDsQ.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">语言建模— RNN</p></figure><p id="78a4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们来看看WhatsApp中自动完成的问题。当你打开键盘输入时，你注意到字母<strong class="kr jd"> I </strong>是信息第一个字符的建议。在这个问题中，每当我们键入一个字符，网络就试图根据先前键入的字符来预测下一个可能的字符。</p><p id="1362" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该函数的输入用橙色表示，并表示为一个<strong class="kr jd"> x </strong> ₜ.与输入相关联的权重使用向量<strong class="kr jd"> U </strong>来表示，并且单词的隐藏表示(<strong class="kr jd"> s </strong> ₜ <strong class="kr jd"> ) </strong>作为先前时间步长的输出和当前输入以及偏差的函数来计算。隐藏表示的输出(<strong class="kr jd"> s </strong> ₜ <strong class="kr jd"> ) </strong>由下面的等式给出，</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nc"><img src="../Images/2c823d75647026c4b7a771cee6ad085b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*fDbCF86YiwcTHhG74A9Ydw.png"/></div></div></figure><p id="a467" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一旦我们计算了输入的隐藏表示，来自网络的最终输出(<strong class="kr jd"> y </strong> ₜ)是隐藏表示的softmax函数(表示为o ),以及与其相关联的权重和偏差。</p><h1 id="7b43" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">编码器-解码器模型—语言建模</h1><p id="15ac" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">在这一节中，我们将看到我们是如何在不知道的情况下，在语言建模的问题中使用编码器-解码器模型的。</p><p id="a858" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在语言建模中，我们感兴趣的是根据以前的信息找到iᵗʰ词的概率分布。</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nd"><img src="../Images/d4ad1fbb4691cf8525cfeceee4bb8def.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QA-h4Nra9zDEq4IAv75Cew.png"/></div></div></figure><h1 id="70e7" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">编码器型号</h1><ul class=""><li id="f915" class="ne nf it kr b ks ms kw mt la ng le nh li ni lm nj nk nl nm bi translated">第一个时间步长的RNN输出作为输入与原始输入一起输入到下一个时间步长。</li><li id="d8eb" class="ne nf it kr b ks nn kw no la np le nq li nr lm nj nk nl nm bi translated">在每个时间步长，单词的隐藏表示(<strong class="kr jd"> s </strong> ₜ₋₁ <strong class="kr jd"> ) </strong>被计算为前一时间步长的输出和当前输入以及偏置的函数。</li><li id="7cec" class="ne nf it kr b ks nn kw no la np le nq li nr lm nj nk nl nm bi translated">最终隐藏状态vector(sₜ)包含来自先前隐藏表示和先前输入的所有编码信息。</li><li id="6033" class="ne nf it kr b ks nn kw no la np le nq li nr lm nj nk nl nm bi translated">这里，递归神经网络充当了一个<strong class="kr jd">编码器</strong>。</li></ul><h1 id="c8f1" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">解码器模型</h1><ul class=""><li id="9225" class="ne nf it kr b ks ms kw mt la ng le nh li ni lm nj nk nl nm bi translated">一旦我们将编码的向量传递到输出层，输出层就解码成下一个可能单词的概率分布。</li><li id="4b93" class="ne nf it kr b ks nn kw no la np le nq li nr lm nj nk nl nm bi translated">输出层是一个softmax函数，它将隐藏状态表示和与之关联的权重以及偏差作为输入。</li><li id="7b98" class="ne nf it kr b ks nn kw no la np le nq li nr lm nj nk nl nm bi translated">由于输出层包含了线性变换和偏置运算，因此可以称为简单的前馈神经网络。</li><li id="573d" class="ne nf it kr b ks nn kw no la np le nq li nr lm nj nk nl nm bi translated">前馈神经网络充当<strong class="kr jd">解码器。</strong></li></ul><h1 id="c85b" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">编码器-解码器应用</h1><p id="0e2d" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">在这一节中，我们将讨论编码器-解码器模型的一些应用</p><h1 id="bbc9" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">图像字幕</h1><p id="f6da" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">图像字幕是根据图像上显示的内容自动生成字幕的任务。</p><ul class=""><li id="d749" class="ne nf it kr b ks kt kw kx la ns le nt li nu lm nj nk nl nm bi translated">在图像字幕中，我们将通过卷积神经网络传递图像，并以特征表示向量的形式从图像中提取特征。</li><li id="dea3" class="ne nf it kr b ks nn kw no la np le nq li nr lm nj nk nl nm bi translated">预处理后的特征表示向量通过RNN或LSTM生成字幕。</li></ul><figure class="my mz na nb gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nv"><img src="../Images/4a5347f0b85c54f7b114a55287efa534.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cY_DouxBBGpB7Vp2Ws8Ciw.png"/></div></div></figure><ul class=""><li id="5cc6" class="ne nf it kr b ks kt kw kx la ns le nt li nu lm nj nk nl nm bi translated">CNN用于对图像进行编码</li><li id="9c3c" class="ne nf it kr b ks nn kw no la np le nq li nr lm nj nk nl nm bi translated">然后，RNN被用来从嵌入的句子中<strong class="kr jd">解码</strong></li></ul><h1 id="9bb4" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">文本蕴涵</h1><p id="5515" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">文本蕴涵是确定一段给定文本<em class="lq"> T </em>是否包含另一段称为“假设”的文本的任务。</p><p id="ca92" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">举个例子，</p><p id="cd68" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">输入</strong>:外面下雨。</p><p id="1438" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">输出</strong>:地面潮湿。</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/a7d0dec7b353a75af6565a3d3e7327a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*ZPoMgfHR2Qi8LI_VUDILMA.png"/></div></figure><p id="ce19" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这个问题中，输入和输出都是字符序列。因此，编码器和解码器网络都是RNN或LSTM。</p><h1 id="b536" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">机器音译</h1><p id="cb3e" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">音译——“用另一种语言或文字书写同一个单词”。<strong class="kr jd">翻译</strong>会告诉你另一种语言中单词的意思，而<strong class="kr jd">音译</strong>不会告诉你单词的意思，但会帮你发音。</p><p id="119c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">输入</strong>:印度</p><p id="4256" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">输出</strong> : इंडिया</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1e9eaec0ac0375208a2d51efa15efa73.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*fQKlnIGdxsjyS8dY7DwvOQ.png"/></div></figure><h1 id="6356" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">编码器</h1><ul class=""><li id="3980" class="ne nf it kr b ks ms kw mt la ng le nh li ni lm nj nk nl nm bi translated">输入的每个字符都作为输入被输入到RNN，方法是将字符转换成一个独热向量表示。</li><li id="08b8" class="ne nf it kr b ks nn kw no la np le nq li nr lm nj nk nl nm bi translated">在编码器的最后一个时间步长，所有先前输入的最终隐藏表示将作为输入传递给解码器。</li></ul><h1 id="72c7" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">解码器</h1><ul class=""><li id="856c" class="ne nf it kr b ks ms kw mt la ng le nh li ni lm nj nk nl nm bi translated">解码器模型可以是RNN或LSTM网络，它将对状态表示向量进行解码，并给出每个字符的概率分布。</li><li id="18e0" class="ne nf it kr b ks nn kw no la np le nq li nr lm nj nk nl nm bi translated">softmax函数用于生成每个字符的概率分布向量。这又有助于生成完整的音译单词。</li></ul><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c2af7a31bca0f5811d280a939ff2941e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*zZmgVaPeq7YL6Ou71KzG9g.png"/></div></figure></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="0410" class="lu lv it bd lw lx og lz ma mb oh md me mf oi mh mi mj oj ml mm mn ok mp mq mr bi translated">从这里去哪里？</h1><p id="6231" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">如果想用Keras &amp; Tensorflow 2.0 (Python或R)学习更多关于神经网络的知识。查看来自<a class="ae ko" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank"> Starttechacademy </a>的Abhishek和Pukhraj的<a class="ae ko" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>。他们以一种简单化的方式解释了深度学习的基础。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><p id="09ca" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="lq">推荐阅读—</em><strong class="kr jd">Eli 5项目机器学习</strong></p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/long-short-term-memory-and-gated-recurrent-units-explained-eli5-way-eff3d44f50dd"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jd gy z fp ot fr fs ou fu fw jc bi translated">长短期记忆和门控循环单位的解释——Eli 5方式</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">在这篇文章中，我们将学习LSTM和格鲁工作背后的直觉</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc ki oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/recurrent-neural-networks-rnn-explained-the-eli5-way-3956887e8b75"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jd gy z fp ot fr fs ou fu fw jc bi translated">递归神经网络(RNN)解释ELI5方式</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">使用RNN的序列标记和序列分类</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc ki oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/understanding-convolution-neural-networks-the-eli5-way-785330cd1fb7"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jd gy z fp ot fr fs ou fu fw jc bi translated">理解卷积神经网络ELI5方法</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">了解卷积运算和CNN的</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pe l oz pa pb ox pc ki oo"/></div></div></a></div><h1 id="04db" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">结论</h1><p id="1097" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">在这篇文章中，我们通过RNN和FNN讨论了如何在语言建模的任务中使用基本的编码器-解码器模型。之后，我们讨论了编码器-解码器模型在解决一些复杂任务中的应用，如机器音译、文本蕴涵。</p><p id="43e3" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我的下一篇文章中，我们将讨论注意力机制。因此，请确保您在Medium上<a class="ae ko" href="https://medium.com/@niranjankumarc" rel="noopener">跟随我</a>，以便在它下降时得到通知。</p><p id="90a1" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">直到那时，和平:)</p><p id="68ce" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">NK。</p><h1 id="857e" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">作者简介</h1><p id="7df3" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated"><a class="ae ko" href="https://medium.com/@niranjankumarc" rel="noopener"> Niranjan Kumar </a>是好事达印度公司的高级数据科学顾问。他对深度学习和人工智能充满热情。除了在媒体上写作，他还作为自由数据科学作家为Marktechpost.com写作。点击查看他的文章<a class="ae ko" href="https://www.marktechpost.com/author/niranjan-kumar/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="4df6" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可以在<a class="ae ko" href="https://www.linkedin.com/in/niranjankumar-c/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与他联系，或者在<a class="ae ko" href="https://twitter.com/Nkumar_283" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注他，了解关于深度学习和机器学习的最新文章。</p><p id="0b69" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">免责声明</strong> —这篇文章中可能有一些相关资源的附属链接。你可以以尽可能低的价格购买捆绑包。如果你购买这门课程，我会收到一小笔佣金。</p><p id="ea8c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">参考文献</strong>:</p><ol class=""><li id="d340" class="ne nf it kr b ks kt kw kx la ns le nt li nu lm pf nk nl nm bi translated"><a class="ae ko" href="https://padhai.onefourthlabs.in/" rel="noopener ugc nofollow" target="_blank">深度学习—帕德海</a></li><li id="af2c" class="ne nf it kr b ks nn kw no la np le nq li nr lm pf nk nl nm bi translated"><a class="ae ko" href="https://www.familytreemagazine.com/premium/now-what-translation-vs-transliteration/" rel="noopener ugc nofollow" target="_blank">理解翻译与音译</a></li><li id="51ef" class="ne nf it kr b ks nn kw no la np le nq li nr lm pf nk nl nm bi translated"><a class="ae ko" href="https://www.youtube.com/playlist?list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT" rel="noopener ugc nofollow" target="_blank">深度学习(CS7015) </a></li></ol></div></div>    
</body>
</html>