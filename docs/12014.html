<html>
<head>
<title>Topic Modelling into a Category Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">类别树中的主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modelling-into-a-category-tree-acafad0f0050?source=collection_archive---------21-----------------------#2020-08-19">https://towardsdatascience.com/topic-modelling-into-a-category-tree-acafad0f0050?source=collection_archive---------21-----------------------#2020-08-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eea2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">LDA 和 NMF 的数字组合与 W2V 级联，将 1M+多语言记录分类到 275 节点、5 级深度类别树中。</h2></div><p id="d3a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le"> —这个解决方案</em>在全印<strong class="kk iu"><em class="le">AI Hackathon</em></strong><em class="le">中排名一万+ <em class="le">第四，自动化多标签分类，在 Techgig 的 Code Gladiators 2020 中。</em></em></p><blockquote class="lf lg lh"><p id="001a" class="ki kj le kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated"><em class="it">个人备注:</em></p><p id="ea20" class="ki kj le kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">当我得知以 1 分之差获得黑客马拉松前 3 名时，我的心情很复杂。虽然在决赛队伍中排名第四有点令人放心，尽管我独自参加，但一个充满激情的团队伙伴的空缺相当令人不安。</p></blockquote><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/36699d6c26857a5b15b6845832bfd032.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*wkAeMljYpaZiSt2VjE_lhw.png"/></div></figure><p id="b9ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">黑客马拉松的挑战是用元信息对 100 多万篇多语言文章进行高精度的分类。</strong>元信息表示隐藏在 URL 文本序列中的类别树信息以及每篇文章的标题。您可以浏览一下下面的输入数据集。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lt"><img src="../Images/8979e65970746ced9b6f6665655fccb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QmUpZvm4Khd_JJT2Cy3ZCA.png"/></div></div></figure><p id="bfb9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">上述文章将根据文章主题分类到一个类别层次中。</strong>例如,“莫迪访华”被归类为“Politics^International ”,而“科罗纳票数创历史新高”被归类为“Health^Pandemic".”</p><p id="254a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于上述数据集非常庞大，人工标注文章需要付出巨大的努力。我们的目标是尽可能减少手工劳动。因此，我们的目标是<strong class="kk iu">在上述数据集上进行主题建模。</strong></p><h1 id="6a5e" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated"><strong class="ak">主题建模 vs 分类</strong></h1><ul class=""><li id="8b49" class="mq mr it kk b kl ms ko mt kr mu kv mv kz mw ld mx my mz na bi translated"><strong class="kk iu">主题建模</strong>是一种<strong class="kk iu">‘无监督’</strong>ML 技术，用于将文档分类为不同的主题<strong class="kk iu"> s. </strong></li><li id="b501" class="mq mr it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated"><strong class="kk iu">主题分类</strong>是一种<strong class="kk iu">‘监督’</strong>ML 技术，它消耗人工标记的数据，以在以后做出预测。</li></ul><p id="c4b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，很明显，我们需要做主题建模，因为输入的文章没有事先标记。<strong class="kk iu">但是主题建模不能保证准确的结果，尽管它很快</strong>，因为不需要培训。</p><p id="7280" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">因此，我们需要替代分类方法，并将它们安排为级联回退，同时改进主题建模结果。</strong></p><h2 id="f4e8" class="ng lz it bd ma nh ni dn me nj nk dp mi kr nl nm mk kv nn no mm kz np nq mo nr bi translated"><strong class="ak">解决方案的完整源代码</strong>可以在<strong class="ak">找到</strong> <a class="ae ns" href="https://github.com/AdroitAnandAI/Topic-Modelling-LDA-NMF-W2V" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">这里</strong> </a> <strong class="ak">。</strong></h2><h1 id="9612" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated"><strong class="ak">级联回退管道</strong></h1><ol class=""><li id="e216" class="mq mr it kk b kl ms ko mt kr mu kv mv kz mw ld nt my mz na bi translated"><strong class="kk iu">主要方法:使用描述的类别树分类器</strong></li></ol><p id="92df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">关联节点类别中出现的单词的单词向量&amp;文章描述，使用 Google Word2Vec 模型计算。</strong>这个模型是在 Google News 数据集上进行预训练的，这个数据集类似于我们需要分类的输入数据。</p><p id="2056" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.<strong class="kk iu">回退 1:基于 URL 的类别树分类器</strong></p><p id="acf9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">文件夹的顺序，从左到右体现了与文章相关的类别层次结构。</strong>例如，以下网址应归类为“新闻/政治/BJP 政治”。如果“方法 1”的<strong class="kk iu">置信度值</strong>低于阈值，则触发回退 1。</p><p id="66e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ns" href="https://www.navjivanindia.com/news/bjp-leader-kaushal-kishore-attacks-up-police-over-law-and-order-in-up" rel="noopener ugc nofollow" target="_blank">https://www . navjivanindia . com/news/BJP-leader-kau shal-Kishore-attacks-up-police-over-law-and-order-in-up</a></p><p id="362c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.<strong class="kk iu">回退 2: LDA-NMF 组合模型</strong></p><p id="f6eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">潜在狄利克雷分配(LDA) </strong>是主题建模的经典解决方案。但在实践中，它给出了很大比例的错误分类。因此，<strong class="kk iu">非负矩阵分解(NMF) </strong>也被使用，并在数值上与 LDA 结合，与多类二进制化器一起改进结果。</p><p id="4fa2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.<strong class="kk iu">回退 3:使用描述的类别树分类器</strong></p><p id="fbfa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果上述所有方法都失败了，那么你可以将文章标记为<strong class="kk iu">“未分类”</strong>，或者将“主要方法”的输出作为最后的退路。</p><p id="c5db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你也可以看看这个博客底部的<strong class="kk iu">“改进”</strong>标题，了解替代 fallback 4 的进一步的 Fallback 策略。</p><h1 id="af41" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated"><strong class="ak">解决方案架构</strong></h1><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nu"><img src="../Images/1f09289e39189faa2bcf1bd85e2824c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wcT6P-SioJ3kfKGV2n1Myg.png"/></div></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">文章、URL 和 LDA-NMF 分类器被合并</p></figure><p id="93aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在解决方案实现之前，让我们构建输入类别树。</p><h1 id="beee" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">构建类别树</h1><p id="dc49" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">下面以 csv 文件的形式给出了对文档进行分类的类别树。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/4ba10f974e25f61b576d6f289081ba1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*GCn6iI_QhEBcAzGfUFrxEQ.png"/></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">父子节点由^分隔</p></figure><p id="5dd7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用任何 python 数据结构来表示类别树。<strong class="kk iu"> AnyTree 包用于创建、操纵&amp;遍历类别树输入。</strong></p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="482e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的代码从输入的 cat_tree.csv 文件生成了树结构。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi of"><img src="../Images/f2cadd9a5d03a9bfb0d3026ce136679a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Q350QoVv58V3fO2ZFdNEA.png"/></div></div><p class="nv nw gj gh gi nx ny bd b be z dk translated"><strong class="bd og">类别树</strong></p></figure><p id="23e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类别树结构可以使用 graphviz 可视化<strong class="kk iu">。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oh"><img src="../Images/970ee56f77fbc1e0068c0e83f76b2867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SX2JfLgNxjD2J69GmQ6ORg.png"/></div></div><p class="nv nw gj gh gi nx ny bd b be z dk translated"><strong class="bd og">类别树描述</strong></p></figure><p id="7af3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">非英语文章可以很容易地翻译成“英语”使用谷歌 API。为了节省时间，请识别非英语行，并只将它们提供给下面的函数。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="od oe l"/></div></figure><h1 id="9092" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">正在加载 Google Word2Vec 模型</h1><p id="18a5" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">谷歌的预训练模型包括 300 万个单词和短语的词汇向量，他们从谷歌新闻数据集中训练了大约 1000 亿个单词。向量长度为 300 个特征，训练数据集类似于我们的输入数据集。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="od oe l"/></div></figure><h1 id="e9b4" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated"><strong class="ak">主逻辑:类别树 Classn 使用描述</strong></h1><p id="c58a" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">首先，我们将“长描述”中的单词与节点相关联以找到类别。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oi"><img src="../Images/4669054164767889e711ec58da44db2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mKEQDxYLFPfIb_j6vlw3ow.png"/></div></div></figure><p id="5d09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实现了一种深度算法，即<strong class="kk iu">搜索与文章描述中每个词相关的类别树节点的 GoogleNews 向量。多词分类得到智能处理</strong>不会提高相似度分数。该功能是解决方案的核心。</p><p id="d07e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">对类别树进行广度优先相关遍历</strong>以找出基于 Word2Vec 相似度的最佳类别树分配。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="e400" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的代码足够复杂，可以通过分析文章中的每个单词(与节点类别中的单词相关联)来生成丰富的类别树。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/0fedb652e1163ef1abc93bf3cb308c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*H3kEsb41awmAD0mZ5_p8gw.png"/></div></figure><p id="f99d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">为了组合不同算法的结果，我们需要计算每篇文章的分类置信度。</strong>因此，将(id、树、置信度)信息保存到 CSV 文件中。</p><p id="d044" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用基于列表中数值分布的数学公式计算<strong class="kk iu">置信度得分。该公式计算最高相似性值和第二高相似性值之间的差异，因为当前 2 个值接近时，它表示不明确。</strong></p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="od oe l"/></div></figure><h2 id="82eb" class="ng lz it bd ma nh ni dn me nj nk dp mi kr nl nm mk kv nn no mm kz np nq mo nr bi translated">调整类别树</h2><p id="c5f8" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">具有不同含义的词的多词类别被组合成相似含义的词，使得词向量距离度量不会出错。例如，在单词向量中，“教育”与“金融”相似，因此被重命名为意思相似的单词“学校教育”。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="od oe l"/></div></figure><h1 id="1039" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">后备 1:基于 URL 的类别树分类器</h1><p id="e449" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">没有单一的信息可能足以对一篇文章进行适当的分类。首先，我们使用 URL 链接中嵌入的树信息来查找类别。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ok"><img src="../Images/f623c44d11cbc01bc8b852aba5e1f054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*10uzVN7NsDvDzPH0NG_fLQ.png"/></div></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">文章 ID 和相应的 URL</p></figure><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="3882" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的代码将生成一个对应于每篇文章的 cat-tree 和置信度得分。请注意，当链接中没有足够的信息时，置信度为 0。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/9ac4caff149de274d1d95dae8dcc22e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*MZeNfmZtY7mJDOW6TagNWA.png"/></div></figure><h1 id="33c3" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated"><strong class="ak">回退 2: LDA-NMF 组合模型</strong></h1><p id="2d04" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated"><strong class="kk iu"> LDA </strong>是一个<strong class="kk iu">概率生成</strong>过程，而<strong class="kk iu"> NMF </strong>是一个<strong class="kk iu">线性代数方法</strong>将文档分类到不同的主题。</p><p id="dea7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">任何文档都被认为有与之相关联的潜在混合主题。类似地，主题被认为是可能产生的术语的混合。因此，我们需要计算两组概率分布参数。</p><ol class=""><li id="a662" class="mq mr it kk b kl km ko kp kr om kv on kz oo ld nt my mz na bi translated">给定一个文档 d 的主题 z 的概率:<strong class="kk iu"> P (z | d) </strong></li></ol><p id="bccb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.给定题目 z 的条件 t 的概率:<strong class="kk iu"> P ( t | z) </strong></p><p id="8c51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由此，<strong class="kk iu">的概率<em class="le">文档 d 的</em> </strong> <em class="le">生成</em> <strong class="kk iu"> <em class="le">的术语 t，</em> </strong></p><p id="ccd1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">P (t | d) =所有主题的前两个概率的乘积之和。</p><p id="11e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果有 10 个主题，那么参数数量= 500 个文档 x 10 个主题+ 10 个主题 x 1000 个单词= <strong class="kk iu"> 15K 个参数。</strong>(优于 500 *1000)</p><p id="e981" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">这种矩阵分解的概念叫做潜在狄利克雷分配。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi op"><img src="../Images/2f64f6a6fa3245db441e928cb19b4b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*5PQocYbgDFiDKMqKGvXUaw.png"/></div><p class="nv nw gj gh gi nx ny bd b be z dk translated"><em class="oq"> BoW 模型(左)到 LDA 模型(右)</em></p></figure><p id="c852" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LDA 首先为每篇文章挑选一些主题，并为每个主题挑选一些词，这两者都用<strong class="kk iu">多项式分布表示。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi op"><img src="../Images/6544ff7081ffa8c204aa9ac9ff8b915e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*p5b-ZkrpCzf9y3nXiaD4Eg.png"/></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">贝塔分布:红色部分大概率，蓝色部分小概率。</p></figure><p id="328b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LDA 模型有一个巨大的优势，它也给了我们一堆话题。该算法只会抛出一些主题，我们可能需要手动对它们进行更有意义的分组。</p><p id="eee4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，由于无人监管，无论是 LDA 还是 NMF 都无法单独获得好的结果。<strong class="kk iu">因此，实施了 LDA 和 NMF 输出矩阵归一化后的数值组合</strong>，以计算出最大可能主题。</p><p id="d913" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">进行预处理后，在<strong class="kk iu">“文章描述”上应用<strong class="kk iu">计数矢量器和 TF-IDF-矢量器</strong>计算文档-术语矩阵(DTM)分别馈入 LDA 和 NMF。</strong></p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="od oe l"/></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">LDA 计算</p></figure><p id="ce51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据每个聚类中的前 20 个单词手动标记主题，如下所示。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi or"><img src="../Images/6753835f1b22527101f33192dda5fac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x3V3ep7s6aMKIcd4afJYKw.png"/></div></div></figure><p id="5102" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LDA 的输出是大小为文章数*主题数的矩阵。每个单元格包含一篇文章属于一个主题的概率。</p><p id="b65a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">我们还可以使用 NMF 将 DTM </strong>分解成两个矩阵，如下图，这在概念上与 LDA 模型<strong class="kk iu">类似，只是这里我们使用线性代数而不是概率模型。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi os"><img src="../Images/7b97dac1de32821f7f4428bf49203115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*TytqAW4L8M_F96GJ62KznA.png"/></div></div><p class="nv nw gj gh gi nx ny bd b be z dk translated"><a class="ae ns" href="https://www.researchgate.net/figure/Conceptual-illustration-of-non-negative-matrix-factorization-NMF-decomposition-of-a_fig1_312157184" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="a831" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">NMF 的输出也是一个大小为文章数*主题数的矩阵，解释保持不变。因此，我们可以在 y 轴上组合 LDA 和 NMF 的输出矩阵，以得到一百万* 40 列。</strong>(输入= 1M 行。话题 LDA 和 NMF 各 20 个)</p><p id="ab72" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">取每一行的 argmax()，找出 LDA 和 NMF 最可能的预测</p><p id="7a3d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">注意</strong>:记得在矩阵串联之前做归一化。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="6169" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">组合模型产生更好的分类结果</strong>如下。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oi"><img src="../Images/1e75130f1f28dd902692529b99396a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zAWqvaWDsjqb88URzIAgIA.png"/></div></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">错误分类的置信度得分较低</p></figure><p id="668f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">唯一的缺点是这会导致第一级分类，而不是树形层次结构。所以是这个原因，我决定把它作为第二个退路。</p><p id="0c17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">理想情况下，应使用分级 LDA，并将其作为解决方案管道中文章分类器之后的第一个备用分类器。</strong>文章分类器仍然应该是主要逻辑，因为 Hierarchical-LDA 产生随机类别的词簇，这些词簇可能与我们想要的类别树不同。</p><h1 id="0b04" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">合并描述、URL 和 LDA-NMF 分类器</h1><p id="8d63" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">文章描述分类器和 URL 分类器通过 LDA-NMF 组合模型合并成一个流水线，作为基于相应置信度得分的级联回退。</p><p id="26c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">描述分类器被赋予了很大的权重，因为 LDA-NMF 只给出了类别(而不是类别树),而 URL 通常不包含所需的信息。</strong>如果 3 种方法都失败，则将其标记为“未分类”或使用文章分类器的结果，尽管不明确。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="024a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">免责声明:由于上述解决方案是在 24 小时黑客马拉松中编写的，代码可能并不完美。它可以通过使用下面提到的方法进行改进，并且还需要对代码中的超参数进行更多的调整。</em></p><h1 id="7f1c" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated"><strong class="ak">改进</strong></h1><ol class=""><li id="c8db" class="mq mr it kk b kl ms ko mt kr mu kv mv kz mw ld nt my mz na bi translated"><a class="ae ns" href="https://github.com/joewandy/hlda" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">h-LDA</strong></a><strong class="kk iu">:Hierarchical-LDA</strong>按照主题的层次结构对文档进行分组，并与 LDA-NMF 输出相结合[1]。</li></ol><p id="1580" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 2。知识图:</strong>文章文本的知识图会生成单词的层次关系，类似于类别树。此信息可用于将文章映射到类别树。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ot"><img src="../Images/bee3cc9fefc0001b09efe91fc89dd967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6QYCMGpatHbevfJkxdt_4Q.png"/></div></div></figure><p id="3676" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.<strong class="kk iu"> Guided LDA: </strong>我们也可以使用<a class="ae ns" href="https://www.freecodecamp.org/news/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164/" rel="noopener ugc nofollow" target="_blank"> Guided LDA </a>为歧义类别设置一些种子词，当没有足够的文档来区分时。<strong class="kk iu">种子词将引导模型围绕这些类别收敛。</strong></p><h2 id="551c" class="ng lz it bd ma nh ni dn me nj nk dp mi kr nl nm mk kv nn no mm kz np nq mo nr bi translated">解决方案的<strong class="ak">完整源代码</strong>可以在<a class="ae ns" href="https://github.com/AdroitAnandAI/Topic-Modelling-LDA-NMF-W2V" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</h2><p id="5a50" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">如果您有任何疑问或建议，您可以在此 处<strong class="kk iu">联系我</strong> <a class="ae ns" href="https://www.linkedin.com/in/ananduthaman/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"/></a></p><h1 id="e8bd" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated"><strong class="ak">参考文献</strong></h1><div class="ou ov gp gr ow ox"><a href="https://medium.com/square-corner-blog/inferring-label-hierarchies-with-hlda-2093d0413337" rel="noopener follow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">用 hLDA 推断标签层次结构</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">问题:组织广场支持中心文章</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">medium.com</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl lr ox"/></div></div></a></div><div class="ou ov gp gr ow ox"><a rel="noopener follow" target="_blank" href="/topic-modeling-quora-questions-with-lda-nmf-aff8dce5e1dd"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">主题建模与 LDA 和 Quora 问题</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">潜在狄利克雷分配，非负矩阵分解</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">towardsdatascience.com</p></div></div><div class="pg l"><div class="pm l pi pj pk pg pl lr ox"/></div></div></a></div><div class="ou ov gp gr ow ox"><a href="https://github.com/joewandy/hlda" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">joewandy/hlda</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">层次潜在狄利克雷分配(hLDA)解决了从数据中学习主题层次的问题。的…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">github.com</p></div></div><div class="pg l"><div class="pn l pi pj pk pg pl lr ox"/></div></div></a></div><div class="ou ov gp gr ow ox"><a href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">自然语言处理在线课程</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">新的！纳米学位课程必备知识该课程要求具备 Python，统计学，机器…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">www.udacity.com</p></div></div><div class="pg l"><div class="po l pi pj pk pg pl lr ox"/></div></div></a></div></div></div>    
</body>
</html>