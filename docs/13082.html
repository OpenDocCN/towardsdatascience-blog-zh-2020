<html>
<head>
<title>Spark 3.0 SQL Feature Update| ANSI SQL Compliance, Store Assignment policy, Upgraded query semantics, Function Upgrades</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark 3.0 SQL 功能更新| ANSI SQL 合规性、存储分配策略、升级的查询语义、功能升级</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spark-3-0-sql-feature-update-ansi-sql-compliance-store-assignment-policy-upgraded-query-94d8d8618ddf?source=collection_archive---------36-----------------------#2020-09-08">https://towardsdatascience.com/spark-3-0-sql-feature-update-ansi-sql-compliance-store-assignment-policy-upgraded-query-94d8d8618ddf?source=collection_archive---------36-----------------------#2020-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="6d4e" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">Spark 通过 Spark SQL 增加了许多显著的特性。有些会对数据质量和数据验证等检查产生巨大影响。尽管有很多升级的特性，我还是列出了其中的几个，因为它们会在最常见的情况下使用。</p><h2 id="db99" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">ANSI SQL 投诉功能</h2><p id="bc02" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">对于刚刚开始学习 SQL 命令的 Spark 开发人员来说，查询标识符验证可能会有所帮助。他们可能会使用不应该使用的关键字作为标识符。即使它在 spark 上完全正常工作，这也会使将来使用该代码的其他人感到困惑。</p><p id="7f76" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">Spark 将允许某些不寻常的情况，比如使用一些保留的关键字作为标识符。大概是这样的:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="7279" class="kv kw it ly b gy mc md l me mf">select * from table_1 create where create.column_1= 1</span></pre><p id="f0e6" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这个查询将在 spark <strong class="jz iu">中运行，没有任何问题。create </strong>是最常见的保留关键字，用于使用 SQL 创建表，但是它可以在 spark 中用作标识符，没有任何问题。</p><p id="a923" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">为了克服这个问题，并在运行时进行查询验证，Spark 现在符合 ANSI SQL 标准。catalyst 解析器级别增加了验证。要启用 ANSI 模式查询验证，请将属性<strong class="jz iu"> spark.sql.ansi.enabled </strong>切换为<strong class="jz iu"> true。</strong>对于上面显示的相同查询，Spark 现在将抛出异常。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="b00b" class="kv kw it ly b gy mc md l me mf">Error in SQL statement: ParseException:  no viable alternative at input 'create'(line 1, pos 38)</span><span id="13e3" class="kv kw it ly b gy mg md l me mf">== SQL == <br/>select * from table_1 create  where create.column_1= 1 <br/>----------------------^^^</span><span id="438b" class="kv kw it ly b gy mg md l me mf">com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.catalyst.parser.ParseException:</span></pre><p id="8481" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">以下是 Spark 3.0 中标准化的关键字:</p><figure class="lt lu lv lw gt mh"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="5cc2" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">要禁用 ANSI 标准验证，伪造<strong class="jz iu"> spark.sql.ansi.enabled. </strong></p><h2 id="dfe7" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated"><strong class="ak">店铺分配政策</strong></h2><p id="c87b" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">引入该特性是为了在从 SQL 类环境迁移的过程中进行严格的数据质量检查。在 Spark 2.4 和更低版本中，Below Insert table 语句(将字符串摄取到整数列中)将在没有任何运行时异常的情况下执行。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/6436d9ffc55b8cf88fe7e7a47a7d2721.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6pf5xLlqMR-uRhfZd_9zSw.png"/></div></div></figure><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mr"><img src="../Images/0a6214d0d63c4386f82148fc95d63072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9MlTBlaOtlw-9uR5yrqe3g.png"/></div></div></figure><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi ms"><img src="../Images/d0bff833ed079a24004694c784cd4d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pJhPGCdLgzp2u_gtINAfpA.png"/></div></div></figure><p id="81e6" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这在 Spark 3.0 中也能很好地工作。但是通过将属性"<strong class="jz iu">spark . SQL . storeasignmentpolicy "</strong>设置为<strong class="jz iu"> 'ANSI '，</strong> casting <strong class="jz iu"> </strong>异常被抛出。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mt"><img src="../Images/1ee3e6240b39ef6836ca539ea83f9248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*McAcM4rJmnEzguq38msyzw.png"/></div></div></figure><p id="b078" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这无疑改善了迁移过程中的数据质量检查。</p><h2 id="055c" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">“from_json”(失败 _ 快速/许可)中的可选模式</h2><p id="4a91" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">from_json 方法用于解析列中的 json 类型值。在 Spark 3.0 中，它支持类似 spark.read.json 的 PERMISSIVE/FAIL_FAST 模式。因此，如果 json 值无法解析或格式错误，它将引发如下异常:</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mu"><img src="../Images/060c618cbea7b34e84d11f1e9a633b03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zn5O_tnoVfh_xHqCvqOqVw.png"/></div></div></figure><h2 id="f7dc" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">指数符号</h2><p id="f7e4" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">在 Spark 3.0 中，用科学记数法写的数字(例如，1E11 )将被解析为 Double。在 Spark 及以下版本中，它们被解析为十进制。要恢复 Spark 3.0 之前的行为，可以将<strong class="jz iu">Spark . SQL . legacy . exponentliteralasdecimal . enabled</strong>设置为<strong class="jz iu"> false </strong>。</p><p id="cb6f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在 Spark 2.4 中，</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mw"><img src="../Images/58432e236a744af6b4c6ee9a7cdf1fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ajkThEwWmlWyFNgKtRA3Bw.png"/></div></div></figure><p id="a1d5" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在 Spark 3.0 中，将<em class="mv">' Spark . SQL . legacy . exponentliteralasdecimal . enabled '</em>设置为<em class="mv"> true </em>。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mx"><img src="../Images/6a111db8cf6d16bdf65eb7b81d82f55d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hKw8Hu4EYoc0vPcHzteIrg.png"/></div></div></figure><h2 id="04c0" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">负十进制零</h2><p id="2e82" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">在 Spark 版和更低版本中，float/double -0.0 在语义上等同于 0.0，但是在聚合分组键、窗口分区键和连接键中使用时，将-0.0 和 0.0 视为不同的值。这是一个 Bug，在 Spark 3.0 中已经修复。现在，Distinct of (-0.0，0.0)将给出(0.0)。</p><p id="a37f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">样本数据集</strong></p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi my"><img src="../Images/9bd248e94b3df891bb941f916c586c23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3BCM7b96Dsx_CnXcvUBm-w.png"/></div></div></figure><p id="3ec8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">使用 Spark 2.4，您将获得如下内容:</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mz"><img src="../Images/7ff76ee75f4f740315c56c50fd13be03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DIMbiJaarS9cVhksG2h8Lg.png"/></div></div></figure><p id="dacc" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">而在 Spark 3.0 中，没有设置额外的属性，考虑因素将是相同的，无论是正面还是负面。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi na"><img src="../Images/0f9340f5f5cf37fbd55d9603ed8cd8cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pLjM1fr4j3ksB1zQmjSXHQ.png"/></div></div></figure><h2 id="79fc" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">“日期”和“时间戳”的关键字字符串</h2><p id="32f8" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">在 Spark 3.0 中，某些关键字支持将字符串转换为日期。</p><p id="f8ef" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">要从字符串生成“日期”,以下是关键字。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nb"><img src="../Images/91cd5ed7434a869304fa7d40fd6510df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qDmy6PUZWTAYDczb2adhvw.png"/></div></div></figure><blockquote class="nc nd ne"><p id="91a8" class="jx jy mv jz b ka kb kc kd ke kf kg kh nf kj kk kl ng kn ko kp nh kr ks kt ku im bi translated">可能的关键字:</p></blockquote><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="0bc6" class="kv kw it ly b gy mc md l me mf"><strong class="ly iu">select date 'x'</strong></span><span id="f4e6" class="kv kw it ly b gy mg md l me mf"><em class="mv">'x' can have following values:</em></span><span id="20fa" class="kv kw it ly b gy mg md l me mf">&gt; <strong class="ly iu">epoch </strong>-&gt; <em class="mv">1970-01-01 (minimum date)</em></span><span id="3947" class="kv kw it ly b gy mg md l me mf">&gt; <strong class="ly iu">today</strong> <!-- -->-&gt; <em class="mv">the current date in the time zone specified by </em><em class="mv">spark.sql.session.timeZone</em></span><span id="c439" class="kv kw it ly b gy mg md l me mf">&gt; <strong class="ly iu">yesterday </strong>-&gt; <em class="mv">the current date - 1</em></span><span id="b4ef" class="kv kw it ly b gy mg md l me mf">&gt; <strong class="ly iu">tomorrow </strong>-&gt; <em class="mv">the current date + 1</em></span><span id="625e" class="kv kw it ly b gy mg md l me mf">&gt; <strong class="ly iu">now</strong><strong class="ly iu"> </strong>-&gt; <em class="mv">the date of running the current query. It has the same notion as today</em></span></pre><p id="a677" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">类似地，我们有一个从字符串获取时间戳的选项。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi ni"><img src="../Images/e76b79a5885c6ad78f036c5d49ac95d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W7eH2ZOHfGaFJMG9UtvMlQ.png"/></div></div></figure><blockquote class="nc nd ne"><p id="0667" class="jx jy mv jz b ka kb kc kd ke kf kg kh nf kj kk kl ng kn ko kp nh kr ks kt ku im bi translated">可能的关键字:</p></blockquote><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="dbb1" class="kv kw it ly b gy mc md l me mf"><strong class="ly iu">select timestamp 'x'</strong></span><span id="8681" class="kv kw it ly b gy mg md l me mf"><em class="mv">'x' can have following values</em></span><span id="a3f8" class="kv kw it ly b gy mg md l me mf">&gt; <strong class="ly iu">epoch </strong>-&gt; <em class="mv">1970-01-01 00:00:00+00 (Unix system time zero)</em></span><span id="46a7" class="kv kw it ly b gy mg md l me mf">&gt; <strong class="ly iu">today </strong>-&gt; <em class="mv">midnight today</em></span><span id="66df" class="kv kw it ly b gy mg md l me mf">&gt; <strong class="ly iu">yesterday</strong> <!-- -->-&gt; <em class="mv">midnight yesterday</em></span><span id="c21c" class="kv kw it ly b gy mg md l me mf">&gt; <strong class="ly iu">tomorrow </strong>-&gt; <em class="mv">midnight tomorrow</em></span><span id="68a0" class="kv kw it ly b gy mg md l me mf">&gt; <strong class="ly iu">now</strong><strong class="ly iu"> </strong>-&gt; <em class="mv">current query start time</em></span></pre><p id="2535" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在 3.0 中，一些 SQL 函数也得到了更新/添加，这有利于数据聚合和以更简单的方式从中获得洞察力。在下面找到其中的一些。</p><h2 id="9eb0" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">用于以下部分的数据集</h2><figure class="lt lu lv lw gt mh"><div class="bz fp l di"><div class="mi mj l"/></div></figure><h2 id="3478" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">Max_By()，Min_By()</h2><p id="8d9f" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">这些功能是我提到的重要特征之一。这将得到一列的值，相对于其他列的最大值/最小值。</p><p id="6fff" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="mv">用法:</em> <strong class="jz iu"> max_by(x，y) </strong> / <strong class="jz iu"> min_by(x，y) </strong></p><p id="b731" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">→ x =从中提取值的列。</p><p id="b230" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">→ y =获得最大值/最小值的列。</p><p id="fe25" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">从以上数据来看，整体最高价 2.598，整体最低价 2.514。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nj"><img src="../Images/817a4e7a0cae650be122f3bd72d4cf21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZW2vRbPqfqUXecRYi_HTig.png"/></div></div></figure><p id="d2d6" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">但是对于 Max_by/Min_by，最大值= 2.572，这是针对最大订单编号 6，最小值= 2.548，这是针对最小订单编号 1。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nk"><img src="../Images/c362b4ee17678ed3ba40e0281bde5b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wWTmVSzey36qoqGNnyqWrQ.png"/></div></div></figure><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nl"><img src="../Images/8bca20502960420bab9f8c360e3dd6f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*niBhqokoyyfnbznt5f2QDA.png"/></div></div></figure><h2 id="ee32" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">子查询中的 WITH 子句</h2><p id="9fb9" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">使用 with 子句的 CTE 现在可以在子查询中使用。这将提高查询的可读性，并且在使用多个 CTE 时很有意义。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="04e0" class="kv kw it ly b gy mc md l me mf">select * from <br/>(with inner_cte as (select * from sql_functions_test where order_num = 5)<br/>select order_num from inner_cte);</span></pre><h2 id="bc41" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">过滤器(在哪里…)</h2><p id="bbe6" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">此功能仅对基于条件筛选的一组行的聚合有帮助。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nm"><img src="../Images/6420ee9564cb83093257f6ba37205f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjWV9LTSeEuwp-eV_kIBjQ.png"/></div></div></figure><h2 id="4dc2" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">覆盖()</h2><p id="ca39" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">这是“替换”功能的替代功能。但是，它确实比 replace 有一些优势。</p><p id="7a41" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="mv">用途:</em>叠加(<em class="mv">input _ string</em>placement<em class="mv">replace _ string</em>from<em class="mv">start _ position</em>for<em class="mv">number _ of _ positions _ to _ be _ replaced</em></p><p id="2676" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="mv">注意:</em>输入列应该是字符串。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nn"><img src="../Images/4067e440680880fd7e0867f0267856d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gebxYLZ478avoSwPJCeeag.png"/></div></div></figure><h2 id="b542" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">Any ()— Every() — Some()</h2><p id="7041" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">这是为了根据提供的条件表达式验证列。</p><p id="dc04" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="mv">用法:</em> Any(expr)，Every(expr)，Some(expr)</p><p id="5950" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="mv"> EVERY: </em>将返回 true，仅当所有列值都返回 true 时。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi no"><img src="../Images/1ab2575abef5475d22115776304ec447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ergIZyr7eMlqjpUQhzUSzg.png"/></div></div></figure><p id="daeb" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="mv"> ANY/SOME: </em>将返回 true，即使有一个值返回 true。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi np"><img src="../Images/87aeaa50206391b93ce1a8b5863b6cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uifGXBMhIF48d13VyAiYGQ.png"/></div></div></figure><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nq"><img src="../Images/cf6cdbd13888d77d7b4d698423b48203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dufiABEWwoyws8azkhrFJg.png"/></div></div></figure><h2 id="7393" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">Count_if()</h2><p id="6642" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">这类似于过滤(其中..)，这将给出满足所提供条件的列记录的非唯一计数。</p><p id="a67f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="mv">用法:</em> count_if(expr)</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nr"><img src="../Images/acc31924715417364d1f712cd60007ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QDrZTCo-QA6UhL15bQWl9Q.png"/></div></div></figure><h2 id="cf39" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">Bool_and() — Bool_or()</h2><p id="f4c4" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">这是为了验证布尔列，模拟 AND /OR 操作。</p><p id="4ebd" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="mv">用法:</em> bool_and( <em class="mv">列值</em>)，bool_or( <em class="mv">列值</em>)</p><p id="2278" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">考虑样本表，有一个布尔列。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi ns"><img src="../Images/d1ee5fe75a6208031075628ee7990b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B7Y29k1jQW0RARMCts8UrQ.png"/></div></div></figure><p id="19ad" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">仅当所有值都为真时，应用<em class="mv"> BOOL_AND: </em>才返回真。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nt"><img src="../Images/fad7f7667d869f4dcd215de76e0ec562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-slRsoNhCH5gXZn2DpoQRQ.png"/></div></div></figure><p id="d596" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">应用<em class="mv"> BOOL_OR: </em>即使一个值为真也返回真。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nu"><img src="../Images/4097c27263fdd8df7bc266c6f7162283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oXZNVcAgpt54VFgxc5PdoQ.png"/></div></div></figure><h2 id="4a86" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">包扎</h2><p id="cf4a" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">Spark 不仅增加了新功能，还修复了早期版本的一些错误。验证和质量检查比以前更容易了。像 count with filter、max_by 和 min_by 这样的函数将降低执行多个子查询的复杂性。</p><figure class="lt lu lv lw gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nv"><img src="../Images/6a37a779fb20981bb0acfd65c9670d69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OZ06_K9d_Yda5Ik3fD2f-w.png"/></div></div></figure></div></div>    
</body>
</html>