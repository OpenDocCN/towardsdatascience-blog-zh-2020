<html>
<head>
<title>Ultimate PySpark Cheat Sheet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">终极PySpark备忘单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ultimate-pyspark-cheat-sheet-7d3938d13421?source=collection_archive---------5-----------------------#2020-06-14">https://towardsdatascience.com/ultimate-pyspark-cheat-sheet-7d3938d13421?source=collection_archive---------5-----------------------#2020-06-14</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><figure class="it iu gq gs iv iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj is"><img src="../Images/c753bbee3b813343b0aafd9508f86da1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uN3G3dR-_2Brm4OzxfF8Uw.jpeg"/></div></div><p class="jd je gk gi gj jf jg bd b be z dk translated">照片由<a class="ae jh" href="https://unsplash.com/@genessapana?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Genessa pana intet</a>在<a class="ae jh" href="https://unsplash.com/s/photos/spark?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h2 id="ab11" class="ji jj jk bd b dl jl jm jn jo jp jq dk jr translated" aria-label="kicker paragraph">数据科学</h2><div class=""/><div class=""><h2 id="bb5c" class="pw-subtitle-paragraph kq jt jk bd b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh dk translated">PySpark数据帧API的简短指南</h2></div><p id="7c6c" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> S </span> park是当今数据工程、数据科学领域的主要参与者之一。随着处理更多数据的需求不断增加，企业经常将Spark集成到数据堆栈中，以解决快速处理大量数据的问题。由Apache维护，Spark生态系统中的主要商业参与者是Databricks(由Spark的最初创建者所有)。Spark已经被各种公司和机构广泛接受——本地的和云中的。一些最受欢迎的底层使用Spark的云产品有<a class="ae jh" href="https://aws.amazon.com/glue/" rel="noopener ugc nofollow" target="_blank"> AWS Glue </a>、<a class="ae jh" href="https://codelabs.developers.google.com/codelabs/cloud-dataproc-starter/#0" rel="noopener ugc nofollow" target="_blank"> Google Dataproc </a>、<a class="ae jh" href="https://azure.microsoft.com/en-us/services/databricks/" rel="noopener ugc nofollow" target="_blank"> Azure Databricks </a>。</p><p id="0e44" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">没有一种技术，没有一种编程语言对所有用例都足够好。Spark是用于解决大规模数据分析和ETL问题的众多技术之一。<a class="ae jh" href="https://linktr.ee/kovid" rel="noopener ugc nofollow" target="_blank">在Spark上工作了一段时间后</a>，我想到用真实的例子来编写一份备忘单。尽管有很多关于在Scala中使用Spark的参考资料，但除了Datacamp 上的<a class="ae jh" href="https://www.datacamp.com/community/data-science-cheatsheets?page=2" rel="noopener ugc nofollow" target="_blank">之外，我找不到一个像样的备忘单，但我认为它需要更新，需要比一页纸更广泛一点。</a></p><p id="f1c5" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">首先，一个关于Spark如何工作的体面介绍—</p><div class="it iu gq gs iv mn"><a href="https://mapr.com/blog/how-spark-runs-your-applications/" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fp"><div class="mp ab mq cl cj mr"><h2 class="bd ju gz z fq ms fs ft mt fv fx jt bi translated">这就是Spark运行应用程序的方式</h2><div class="mu l"><h3 class="bd b gz z fq ms fs ft mt fv fx dk translated">回想一下之前的Spark 101博客，您的Spark应用程序是作为一组并行任务运行的。在这篇博文中…</h3></div><div class="mv l"><p class="bd b dl z fq ms fs ft mt fv fx dk translated">mapr.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb jb mn"/></div></div></a></div><h1 id="cfa6" class="nc nd jk bd ne nf ng nh ni nj nk nl nm kz nn la no lc np ld nq lf nr lg ns nt bi translated">配置和初始化</h1><p id="d97a" class="pw-post-body-paragraph li lj jk lk b ll nu ku ln lo nv kx lq lr nw lt lu lv nx lx ly lz ny mb mc md in bi translated">在开始编写哪几行代码来启动和运行PySpark笔记本/应用程序之前，您应该了解一点关于<code class="fe nz oa ob oc b">SparkContext</code>、<code class="fe nz oa ob oc b">SparkSession</code>和<code class="fe nz oa ob oc b">SQLContext</code>的知识。</p><ul class=""><li id="f00b" class="od oe jk lk b ll lm lo lp lr of lv og lz oh md oi oj ok ol bi translated"><code class="fe nz oa ob oc b">SparkContext</code> —提供与Spark的连接，能够创建rdd</li><li id="785e" class="od oe jk lk b ll om lo on lr oo lv op lz oq md oi oj ok ol bi translated"><code class="fe nz oa ob oc b">SQLContext</code> —提供与Spark的连接，能够对数据运行SQL查询</li><li id="d465" class="od oe jk lk b ll om lo on lr oo lv op lz oq md oi oj ok ol bi translated"><code class="fe nz oa ob oc b">SparkSession</code> —包罗万象的上下文，包括对<code class="fe nz oa ob oc b">SparkContext</code>、<code class="fe nz oa ob oc b">SQLContext</code>和<code class="fe nz oa ob oc b">HiveContext</code>的覆盖。</li></ul><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><p id="51e9" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">我们将在一些例子中使用MovieLens数据库。这是那个数据库的链接。你可以从Kaggle下载。</p><div class="it iu gq gs iv mn"><a href="https://www.kaggle.com/rounakbanik/the-movies-dataset" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fp"><div class="mp ab mq cl cj mr"><h2 class="bd ju gz z fq ms fs ft mt fv fx jt bi translated">电影数据集</h2><div class="mu l"><h3 class="bd b gz z fq ms fs ft mt fv fx dk translated">超过45，000部电影的元数据。超过270，000名用户的2，600万次评分。</h3></div><div class="mv l"><p class="bd b dl z fq ms fs ft mt fv fx dk translated">www.kaggle.com</p></div></div><div class="mw l"><div class="ox l my mz na mw nb jb mn"/></div></div></a></div><h1 id="0ef1" class="nc nd jk bd ne nf ng nh ni nj nk nl nm kz nn la no lc np ld nq lf nr lg ns nt bi translated">读取数据</h1><p id="5c61" class="pw-post-body-paragraph li lj jk lk b ll nu ku ln lo nv kx lq lr nw lt lu lv nx lx ly lz ny mb mc md in bi translated">Spark支持从各种数据源读取数据，比如CSV、Text、Parquet、Avro、JSON。它还支持从Hive和任何具有可用JDBC通道的数据库中读取数据。以下是如何在Spark中阅读CSV的方法—</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><p id="2c8b" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">在您的Spark之旅中，您会发现有许多方法可以编写相同的代码来获得相同的结果。许多函数都有别名(例如<code class="fe nz oa ob oc b">dropDuplicates</code>和<code class="fe nz oa ob oc b">drop_duplicates</code>)。下面的例子展示了在Spark中读取文件的几种方法。</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><h1 id="e4b6" class="nc nd jk bd ne nf ng nh ni nj nk nl nm kz nn la no lc np ld nq lf nr lg ns nt bi translated">写入数据</h1><p id="60f9" class="pw-post-body-paragraph li lj jk lk b ll nu ku ln lo nv kx lq lr nw lt lu lv nx lx ly lz ny mb mc md in bi translated">一旦你完成了数据转换，你会想把它写在某种持久存储上。这里有一个例子，展示了将一个拼花文件写入磁盘的两种不同方式—</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><p id="8a73" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">显然，基于您的消费模式和需求，您也可以使用类似的命令将其他文件格式写入磁盘。写入Hive表时，可以用<code class="fe nz oa ob oc b">bucketBy</code>代替<code class="fe nz oa ob oc b">partitionBy</code>。</p><div class="it iu gq gs iv mn"><a href="https://luminousmen.com/post/the-5-minute-guide-to-using-bucketing-in-pyspark" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fp"><div class="mp ab mq cl cj mr"><h2 class="bd ju gz z fq ms fs ft mt fv fx jt bi translated">Pyspark中使用分桶的5分钟指南</h2><div class="mu l"><h3 class="bd b gz z fq ms fs ft mt fv fx dk translated">世界上有许多不同的工具，每一种都可以解决一系列问题。他们中的许多人是如何判断…</h3></div><div class="mv l"><p class="bd b dl z fq ms fs ft mt fv fx dk translated">luminousmen.com</p></div></div><div class="mw l"><div class="oy l my mz na mw nb jb mn"/></div></div></a></div><p id="0b85" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated"><code class="fe nz oa ob oc b">bucketBy</code>和<code class="fe nz oa ob oc b">partitionBy</code>背后的思想都是拒绝不需要查询的数据，即修剪分区。这是一个来自传统关系数据库分区的老概念。</p><h1 id="1905" class="nc nd jk bd ne nf ng nh ni nj nk nl nm kz nn la no lc np ld nq lf nr lg ns nt bi translated">创建数据框架</h1><p id="6dfe" class="pw-post-body-paragraph li lj jk lk b ll nu ku ln lo nv kx lq lr nw lt lu lv nx lx ly lz ny mb mc md in bi translated">除了您在上面的<strong class="lk ju">读取数据</strong>部分看到的直接方法<code class="fe nz oa ob oc b">df = spark.read.csv(csv_file_path)</code>之外，还有一种创建数据帧的方法，那就是使用SparkSQL的行构造。</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><p id="7bd1" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">还有一个选项，您可以使用Spark的<code class="fe nz oa ob oc b">.paralellize</code>或<code class="fe nz oa ob oc b">.textFile</code>特性将文件表示为RDD。要将其转换成数据帧，显然需要指定一个模式。这就是<code class="fe nz oa ob oc b">pyspark.sql.types</code>出现的原因。</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><p id="19ae" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">我们将在PySpark中使用大量类似SQL的功能，请花几分钟时间熟悉下面的<a class="ae jh" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><h1 id="5d35" class="nc nd jk bd ne nf ng nh ni nj nk nl nm kz nn la no lc np ld nq lf nr lg ns nt bi translated">修改数据帧</h1><p id="2d1d" class="pw-post-body-paragraph li lj jk lk b ll nu ku ln lo nv kx lq lr nw lt lu lv nx lx ly lz ny mb mc md in bi translated">数据帧抽象出rdd。数据集做同样的事情，但是数据集没有表格、关系数据库表那样的rdd表示。数据帧有。因此，DataFrames支持类似于您通常在数据库表上执行的操作，即通过添加、删除、修改列来更改表结构。Spark提供了DataFrames API中的所有功能。事情是这样的—</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><p id="65aa" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">除了创建新列之外，我们还可以使用以下方法重命名现有的列</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><p id="11e8" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">如果我们必须删除一列或多列，我们可以这样做—</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><h1 id="e74d" class="nc nd jk bd ne nf ng nh ni nj nk nl nm kz nn la no lc np ld nq lf nr lg ns nt bi translated">连接</h1><p id="62fa" class="pw-post-body-paragraph li lj jk lk b ll nu ku ln lo nv kx lq lr nw lt lu lv nx lx ly lz ny mb mc md in bi translated">Spark使用类似SQL的接口背后的整个想法是，有许多数据可以用一个松散的关系模型来表示，即一个没有ACID、完整性检查等的表模型。考虑到这一点，我们可以预期会发生很多连接。Spark完全支持连接两个或多个数据集。这里是如何—</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><h1 id="5331" class="nc nd jk bd ne nf ng nh ni nj nk nl nm kz nn la no lc np ld nq lf nr lg ns nt bi translated">过滤</h1><p id="4673" class="pw-post-body-paragraph li lj jk lk b ll nu ku ln lo nv kx lq lr nw lt lu lv nx lx ly lz ny mb mc md in bi translated">过滤器就像SQL中的子句一样。事实上，你可以在Spark中互换使用<code class="fe nz oa ob oc b">filter</code>和<code class="fe nz oa ob oc b">where</code>。下面是一个在MovieLens数据库电影元数据文件中过滤分级在7.5和8.2之间的电影的示例。</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><p id="3aa0" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">过滤器支持所有类似SQL的特性，例如使用比较运算符、正则表达式和按位运算符进行过滤。</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><p id="c5d9" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">过滤掉空值和非空值是查询中最常见的用例之一。Spark对列对象提供了简单的<code class="fe nz oa ob oc b">isNULL</code>和<code class="fe nz oa ob oc b">isNotNull</code>操作。</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><h1 id="bdfa" class="nc nd jk bd ne nf ng nh ni nj nk nl nm kz nn la no lc np ld nq lf nr lg ns nt bi translated">聚集</h1><p id="cffb" class="pw-post-body-paragraph li lj jk lk b ll nu ku ln lo nv kx lq lr nw lt lu lv nx lx ly lz ny mb mc md in bi translated">聚合是处理大规模数据的巨大工作的核心，因为这通常归结为BI仪表板和ML，这两者都需要某种聚合。使用SparkSQL库，您几乎可以实现在传统关系数据库或数据仓库查询引擎中所能实现的一切。这里有一个例子展示了Spark是如何进行聚合的。</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><h1 id="f834" class="nc nd jk bd ne nf ng nh ni nj nk nl nm kz nn la no lc np ld nq lf nr lg ns nt bi translated">窗口功能和排序</h1><p id="16ea" class="pw-post-body-paragraph li lj jk lk b ll nu ku ln lo nv kx lq lr nw lt lu lv nx lx ly lz ny mb mc md in bi translated">与大多数分析引擎一样，窗口函数已经成为<code class="fe nz oa ob oc b">rank</code>、<code class="fe nz oa ob oc b">dense_rank</code>等的标准。，被大量使用。Spark利用传统的基于SQL的窗口函数语法<code class="fe nz oa ob oc b">rank() over (partition by something order by something_else desc)</code>。</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><p id="6f2c" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">请注意<code class="fe nz oa ob oc b">sort</code>和<code class="fe nz oa ob oc b">orderBy</code>在Spark中可以互换使用，除非是在窗口函数中。</p><figure class="or os ot ou gu iw"><div class="bz fq l di"><div class="ov ow l"/></div></figure><p id="0249" class="pw-post-body-paragraph li lj jk lk b ll lm ku ln lo lp kx lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">这些是我收集的一些例子。很明显，除了一张小抄，还有很多东西值得一试。如果您感兴趣或者在这里没有找到任何有用的东西，可以去看看文档——它相当不错。</p></div></div>    
</body>
</html>