<html>
<head>
<title>Hyper-Parameters Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyper-parameters-optimization-c2f888515d8f?source=collection_archive---------28-----------------------#2020-04-16">https://towardsdatascience.com/hyper-parameters-optimization-c2f888515d8f?source=collection_archive---------28-----------------------#2020-04-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="7e33" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">了解如何让您的深度学习模型更上一层楼！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/52d3bf803ca95c49ac93eaa4b825a158.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FvWI16tqtjrkvXsP"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/photos/Kl1gC0ve620" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/Kl1gC0ve620</a></p></figure><h1 id="eca5" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">介绍</h1><p id="3d77" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">本文的目的是学习如何通过选择最佳超参数来优化神经网络。具体来说，我们将探索:</p><ul class=""><li id="da61" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">网格搜索</li><li id="177b" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">远视</li><li id="6328" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">遗传算法</li></ul><h1 id="0c1d" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">网格搜索</h1><p id="783b" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">网格搜索是在一组参数中寻找最佳参数的最简单方法。本质上是蛮力。让我们看一个简单的例子:</p><p id="9602" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设我们有一个网格，我们想看看哪些参数是最好的。我们有:</p><ul class=""><li id="c887" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">下降:可以从 0 到 0.5 变化，间隔为 0.1</li><li id="cc47" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">学习率:可以在 0.1 到 0.001 之间变化，间隔为 x10</li><li id="943e" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">过滤器数量:从 64 到 256 不等，间隔为 64</li><li id="d9cc" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">过滤器尺寸:从 3 到 7 英寸不等(总是方形过滤器)</li></ul><p id="369c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">嗯，网格搜索会做的如下:</p><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="1241" class="nb lg it mx b gy nc nd l ne nf"><strong class="mx iu"># grid definition</strong><br/>dropouts = [0, 0.1, 0.2, 0.3, 0.4, 0.5]<br/>learning_rates = [0.1, 0.01, 0.001, 0.0001]<br/>n_filters = [64, 128, 192, 256]<br/>filter_sizes = [3, 5, 7]</span><span id="395b" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># variable creation to store the values</strong><br/>log_accuracies = []</span><span id="e436" class="nb lg it mx b gy ng nd l ne nf">from random import uniform<br/>def dummy_net(d, lr, nf, fs):<br/>  print('Executing network with d={}, lr={}, nf={}, fs={}'.format(d, lr, nf, fs))<br/>  return uniform(0,1)</span><span id="43ee" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># counter</strong><br/>i = 1</span><span id="523e" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># grid search</strong><br/>for d in dropouts:<br/>  for lr in learning_rates:<br/>    for nf in n_filters:<br/>      for fs in filter_sizes:<br/>        result_net = dummy_net(d, lr, nf, fs)<br/>        print('[{}] Resultado: {}'.format(i, result_net))<br/>        log_accuracies.append(result_net)<br/>        i += 1</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d53cb1ae6da74e86b29b9960a6bcdf52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*ZlsP52f2rSg-lkTcRbtaiA.png"/></div></figure><p id="c0e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最好的结果是:</p><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="3861" class="nb lg it mx b gy nc nd l ne nf">import numpy as np<br/>idx_max = np.nonzero(log_accuracies==np.max(log_accuracies))<br/>print(idx_max)</span><span id="57a4" class="nb lg it mx b gy ng nd l ne nf">print('Best execution: {}. Accuracy: {}'.format(idx_max[0][0], log_accuracies[idx_max[0][0]]))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e49a348234146322573cae4bf45c46a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*rqmc7HX32gZKfJ3B-ymz_A.png"/></div></figure><p id="a8aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是我们如何评估网络配置的最佳结果。</p><p id="efd1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种方法的问题是我们在网络中有 6×4×4×3 次执行，总共有 288 次执行。如果每次执行最少需要 10 分钟，那么总时间加起来就是 48 小时。</p><p id="849c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们用一个非常简单的网来做一个测试:</p><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="da35" class="nb lg it mx b gy nc nd l ne nf"><strong class="mx iu"># We import the necessary libraries</strong><br/>from keras.datasets import mnist<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Flatten<br/>from keras.layers import Conv2D, MaxPooling2D<br/>from keras import backend as K<br/>from keras.utils import to_categorical<br/>from keras.optimizers import Adam</span><span id="33c7" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># To load the data and convert from vectors to images</strong><br/>img_rows, img_cols = 28, 28<br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()<br/>x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)<br/>x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)<br/>input_shape = (img_rows, img_cols, 1)</span><span id="f430" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># to normalize the data</strong><br/>x_train = x_train.astype('float32')<br/>x_test = x_test.astype('float32')<br/>x_train /= 255<br/>x_test /= 255</span><span id="3d22" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># convert class vectors to binary class matrices</strong><br/>num_classes = 10<br/>y_train = to_categorical(y_train, num_classes)<br/>y_test = to_categorical(y_test, num_classes)</span><span id="5aa0" class="nb lg it mx b gy ng nd l ne nf">def net(do, lr, nf, fs):</span><span id="b741" class="nb lg it mx b gy ng nd l ne nf">model = Sequential()<br/>  model.add(Conv2D(nf, kernel_size=fs, activation='relu', input_shape=input_shape))<br/>  model.add(Conv2D(nf, fs, activation='relu'))<br/>  model.add(MaxPooling2D(pool_size=(2, 2)))<br/>  model.add(Dropout(do))<br/>  model.add(Flatten())<br/>  model.add(Dense(128, activation='relu'))<br/>  model.add(Dropout(do))<br/>  model.add(Dense(10, activation='softmax'))</span><span id="c1fb" class="nb lg it mx b gy ng nd l ne nf">model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr), metrics=['accuracy'])</span><span id="feb0" class="nb lg it mx b gy ng nd l ne nf">model.fit(x_train, y_train,<br/>            batch_size=1024,<br/>            epochs=1,<br/>            verbose=0,<br/>            validation_data=(x_test, y_test))</span><span id="d382" class="nb lg it mx b gy ng nd l ne nf">score = model.evaluate(x_test, y_test, verbose=0)<br/>  <br/>  print('Red con d={}, lr={}, nf={}, fs={}. Loss: {}. Acc: {}.'.format(d, lr, nf, fs, score[0], score[1]))      <br/>  return score[1] # accuracy</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nj"><img src="../Images/ec3f5fb3ccda32653dfa9b2dd07c8010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ey5LNCfIGyADH4xsUKODmA.png"/></div></div></figure><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="808e" class="nb lg it mx b gy nc nd l ne nf"><strong class="mx iu"># grid definition</strong><br/>dropouts = [0, 0.3]<br/>learning_rates = [0.1, 0.01]<br/>n_filters = [32, 64]<br/>filter_sizes = [3, 5]</span><span id="aac0" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># variable creation to store the values</strong><br/>log_accuracies = []</span><span id="9860" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># counter</strong><br/>i = 1</span><span id="a297" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># grid search</strong><br/>for d in dropouts:<br/>  for lr in learning_rates:<br/>    for nf in n_filters:<br/>      for fs in filter_sizes:<br/>        result_net = net(d, lr, nf, fs)<br/>        print('[{}] Resultado: {}'.format(i, result_net))<br/>        log_accuracies.append(result_net)<br/>        i += 1<br/>        <br/><strong class="mx iu"># the best result will be:</strong><br/>import numpy as np<br/>idx_max = np.nonzero(log_accuracies==np.max(log_accuracies))<br/>print(idx_max)</span><span id="a586" class="nb lg it mx b gy ng nd l ne nf">print('Best execution: {}. Accuracy: {}'.format(idx_max[0][0], log_accuracies[idx_max[0][0]]))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nk"><img src="../Images/1bd05553fe7b195a4c5b8a42a10d7e09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nj_lNfUae0EUzmwvIRdD7A.png"/></div></div></figure><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="e2b7" class="nb lg it mx b gy nc nd l ne nf"><strong class="mx iu"># grid definition with the best values</strong><br/>dropouts = [0, 0.3]<br/>learning_rates = [0.01]<br/>n_filters = [32, 64, 128]<br/>filter_sizes = [5, 7]</span><span id="1283" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># variable creation to store the values</strong><br/>log_accuracies = []</span><span id="eb5a" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># counter</strong><br/>i = 1</span><span id="9ea5" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># grid search</strong><br/>for d in dropouts:<br/>  for lr in learning_rates:<br/>    for nf in n_filters:<br/>      for fs in filter_sizes:<br/>        result_net = net(d, lr, nf, fs)<br/>        print('[{}] Resultado: {}'.format(i, result_net))<br/>        log_accuracies.append(result_net)<br/>        i += 1<br/>        <br/><strong class="mx iu"># the best result will be:</strong><br/>import numpy as np<br/>idx_max = np.nonzero(log_accuracies==np.max(log_accuracies))<br/>print(idx_max)<br/>print('Best execution: {}. Accuracy: {}'.format(idx_max[0][0], log_accuracies[idx_max[0][0]]))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nl"><img src="../Images/b1db284b71f8355cccb5c643219c726b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GH2uSumLedUca46RidPdwQ.png"/></div></div></figure><p id="0b2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一个相当好的结果。虽然用更快的方法或者一些启发式的方法会很棒，而不是蛮力。你很幸运，因为有各种各样的方法:</p><ul class=""><li id="4e50" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">留兰香(蟒蛇皮)</li><li id="a1c8" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">BayesOpt(带有 Python 和 Matlab/Octave 接口的 C++)</li><li id="96ac" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">远视(蟒蛇皮)</li><li id="da13" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">SMAC(爪哇)</li><li id="f9f9" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">伦博(Matlab)</li><li id="56f6" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">MOE (C++/Python)</li></ul><p id="b7f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来我们将探索超级选项！</p><h1 id="902a" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">超级选项</h1><p id="9bb6" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">Hyperopt 是一个用 Python 编写的库，它允许您通过更多地关注最有可能提供良好解决方案的值来快速优化函数。</p><p id="beb9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以在这里找到完整的方法:分布式异步超参数优化，【https://github.com/hyperopt/hyperopt】T2。</p><p id="7a50" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它目前实现了两种算法来做到这一点:</p><ul class=""><li id="0603" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">随机搜索</li><li id="f756" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">Parzen 估计树(TPE)</li></ul><p id="88e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，利用 MongoDB，它们可以串行或并行运行。</p><p id="f4b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看一个如何使用它的例子。</p><p id="b707" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们找出 x 的最小值:</p><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="83d3" class="nb lg it mx b gy nc nd l ne nf">from hyperopt import fmin, tpe, hp</span><span id="2d00" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># with 10 iterations</strong><br/>best = fmin(fn=lambda x: x ** 2,<br/>            space=hp.uniform('x', -10, 10),<br/>            algo=tpe.suggest,<br/>            max_evals=10)</span><span id="80d9" class="nb lg it mx b gy ng nd l ne nf">print(best)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/1ec9a2c9eed8f23c110c565b52bac204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pd2PZa4AJslDqhWd7ei8kg.png"/></div></div></figure><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="7b3d" class="nb lg it mx b gy nc nd l ne nf">from hyperopt import fmin, tpe, hp</span><span id="a596" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># with 100 iterations</strong><br/>best = fmin(fn=lambda x: x ** 2,<br/>            space=hp.uniform('x', -10, 10),<br/>            algo=tpe.suggest,<br/>            max_evals=100)</span><span id="6ae1" class="nb lg it mx b gy ng nd l ne nf">print(best)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/cf94e6bf83ecde78f074a37e96721f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYMso2X0ypSDohJqTyPoBw.png"/></div></div></figure><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="d30e" class="nb lg it mx b gy nc nd l ne nf">from hyperopt import fmin, tpe, hp</span><span id="734c" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># with 1000 iterations</strong><br/>best = fmin(fn=lambda x: x ** 2,<br/>            space=hp.uniform('x', -10, 10),<br/>            algo=tpe.suggest,<br/>            max_evals=1000)</span><span id="375b" class="nb lg it mx b gy ng nd l ne nf">print(best)</span></pre><p id="cb01" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们尝试一个更复杂的神经网络:</p><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="2533" class="nb lg it mx b gy nc nd l ne nf"><strong class="mx iu"># we install the necessary packages</strong><br/>!pip install networkx==1.11 # para instala hyperopt correctamente, si no, da errores<br/>!pip install hyperopt</span><span id="f9fa" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># necessary imports</strong><br/>import sys<br/>import time<br/>import numpy as np<br/>from hyperopt import fmin, tpe, hp, STATUS_OK, Trials<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Activation, Flatten<br/>from keras.layers import Conv2D, MaxPooling2D<br/>from keras.constraints import max_norm<br/>from keras.optimizers import Adam<br/>from sklearn.model_selection import train_test_split<br/>from keras.utils import to_categorical<br/>from keras.callbacks import EarlyStopping<br/>from keras.datasets import cifar10</span><span id="00a9" class="nb lg it mx b gy ng nd l ne nf">SEED = 42</span><span id="56aa" class="nb lg it mx b gy ng nd l ne nf">(X_train, y_train), (X_test, y_test) = cifar10.load_data()<br/>validation_split = 0.1<br/>X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_split, random_state=SEED)</span><span id="304d" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># Let's convert the data to float and then divide it by 255 to normalize it<br/># Due to image characteristics they can only get values from 0 to 255</strong><br/>X_train = X_train.astype('float32') / 255.<br/>X_val = X_val.astype('float32') / 255.<br/>X_test = X_test.astype('float32') / 255.</span><span id="59cc" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># let's convert the labels with one-hot encoding</strong><br/>n_classes = 10<br/>y_train = to_categorical(y_train, n_classes)<br/>y_val = to_categorical(y_val, n_classes)<br/>y_test = to_categorical(y_test, n_classes)</span><span id="dec6" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># we define the search space<br/># we'll vary:<br/># - the number of filters in our conv layers<br/># - the dropout percentage<br/># - the number of neurons in the dense layer</strong><br/>space = {<br/>    'n_filters_conv': hp.choice('n_filters_conv', [32, 64, 128]),<br/>    'dropout': hp.uniform('dropout', 0.0, 0.5),<br/>    'neurons_dense': hp.choice('neurons_dense', [256, 512, 1024]), <br/>}</span><span id="044c" class="nb lg it mx b gy ng nd l ne nf">def get_callbacks(pars):<br/>  callbacks = [EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=0, mode='auto')]<br/>  return callbacks</span><span id="9709" class="nb lg it mx b gy ng nd l ne nf">def mi_cnn(pars):<br/>  print ('Parameters: ', pars)<br/>  model = Sequential()<br/> <br/><strong class="mx iu">  # First convolutional block</strong><br/>  model.add(Conv2D(pars['n_filters_conv'], kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))<br/>  model.add(MaxPooling2D(pool_size=(2, 2)))<br/>  model.add(Dropout(pars['dropout']))</span><span id="82b7" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># second convolutional block</strong><br/>  model.add(Conv2D(pars['n_filters_conv'], kernel_size=(3, 3), activation='relu'))<br/>  model.add(MaxPooling2D(pool_size=(2, 2)))<br/>  model.add(Dropout(pars['dropout']))</span><span id="f4c1" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># third convolutional block</strong><br/>  model.add(Conv2D(pars['n_filters_conv'], kernel_size=(3, 3), activation='relu'))<br/>  model.add(MaxPooling2D(pool_size=(2, 2)))<br/>  model.add(Dropout(pars['dropout']))</span><span id="4ee8" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># Classifier block</strong><br/>  model.add(Flatten())<br/>  model.add(Dense(pars['neurons_dense'], activation='relu', kernel_constraint=max_norm(3.)))<br/>  model.add(Dropout(pars['dropout']))<br/>  model.add(Dense(10, activation='softmax'))</span><span id="18cb" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># We compile the model</strong><br/>  model.compile(loss='categorical_crossentropy',<br/>                optimizer=Adam(lr=0.0001, decay=1e-6),<br/>                metrics=['accuracy'])</span><span id="32f0" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># We train the model</strong><br/>  history = model.fit(X_train, <br/>                      y_train,<br/>                      batch_size=128,<br/>                      shuffle=True,<br/>                      epochs=5,<br/>                      validation_data=(X_val, y_val),<br/>                      verbose = 0,<br/>                      callbacks = get_callbacks(pars))</span><span id="b02a" class="nb lg it mx b gy ng nd l ne nf">best_epoch_loss = np.argmin(history.history['val_loss'])<br/>  best_val_loss = np.min(history.history['val_loss'])<br/>  best_val_acc = np.max(history.history['val_acc'])<br/>  <br/>  print('Epoch {} - val acc: {} - val loss: {}'.format(best_epoch_loss, best_val_acc, best_val_loss))<br/>  sys.stdout.flush()<br/>  <br/>  return {'loss': best_val_loss, 'best_epoch': best_epoch_loss, 'eval_time': time.time(), 'status': STATUS_OK, 'model': model, 'history': history}</span><span id="28f4" class="nb lg it mx b gy ng nd l ne nf">trials = Trials()<br/>best = fmin(mi_cnn, space, algo=tpe.suggest, max_evals=10, trials=trials)<br/>print(best)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/70e768c5c04ede38a8e35f27531dedcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFtq59B8RDMeZonEH-FtjQ.png"/></div></div></figure><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="adda" class="nb lg it mx b gy nc nd l ne nf">trials.results</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/7a015634f61c1488e946a299b4f2ab7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*vBxjOTFBbn689nyzqtt_Dw.png"/></div></figure><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="c898" class="nb lg it mx b gy nc nd l ne nf">trials.losses()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/6c49481a3dd1ee6141170c433c2e21aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*c8UFEP4SNUtCcVriHozQQg.png"/></div></div></figure><p id="de19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这样，您可以离开您的配置，去做一些比改变参数更有用的事情，直到您找到正确的配置。</p><p id="105f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是我们不一定要呆在这里，如果我们想要剩余连接，我们也可以改变层数或设置。是的，这意味着我们也可以改变架构！</p><p id="22c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里有一个非常完整的例子:【https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100】T4</p><p id="5b80" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">还有一个你可能会感兴趣的。</p><h1 id="e29b" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">遗传算法</h1><p id="4822" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">本质上，遗传算法是一种受自然进化启发的元启发式研究方法。它们属于进化算法，特别是导向随机搜索算法(进化算法)。</p><p id="4d1a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这听起来可能很复杂，但实际上非常简单。让我们用一个例子来理解它们:</p><p id="212d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">想象一下，我们有一个拼图，我们只剩下一块拼图可以拼了。问题是这个谜题非常特别，因为它让我们能够完成我们的作品。为此，我们有几种机制:</p><ul class=""><li id="1595" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">组合部分片段(交叉或重组)</li><li id="4685" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">修改那些部分的某些部分(突变)</li><li id="525f" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">选择我们所做的最好的作品，从新的和更好的作品中建立(选择)</li></ul><p id="2f93" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，假设我们决定切割 10 块纸板，这是我们最初的 10 块纸板，我们将用它们来测试是否有任何一个完全符合。我们都试过了，在这 10 个中，有 5 个或多或少合适。因此，我们选择了这 5 个，并使用上面解释的机制从中制作了新的:</p><ul class=""><li id="e2c9" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">从选择的五个中，我们通过随机选择的方式将最初的五个中的两个部分组合起来，再取五个。</li><li id="0aea" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">在最初的 5 个和我们创造的新的 5 个中，我们通过稍微修改作品的一个尖端，再去掉 5 个</li></ul><p id="09f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们有 15 个棋子，我们总是想要 10 个，因为如果不是在第 5 次我们这样做了，我们会有很多棋子，所以:</p><ul class=""><li id="df92" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">我们试了 15 件，找到最合适的，然后随机选择 9 件。</li></ul><p id="f2e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看如何在实践中应用这个例子:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/6afef84288827141866248bf130e597c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_7yCaSt9gFHa1nFnIHQ1CQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">按作者分列的数字</p></figure><p id="10f9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如你所见:</p><ul class=""><li id="42b7" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">我们这组片段(群体)中的每一个片段都是一条染色体</li><li id="7559" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">我们的每一部分都是一个基因，所以我们的染色体有 4 个基因</li><li id="244c" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">每个基因可能具有的值或设置被称为等位基因。</li></ul><p id="f010" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这和生物学中的一样，因为这些算法是受自然进化的启发？</p><p id="f27c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">好的，让我们把这些单词和我们的例子联系起来:</p><ul class=""><li id="d719" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">我们需要为我们的拼图洞找到合适的一块</li><li id="c390" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">我们有一组初始的片段(群体),我们不知道它们是否合适</li><li id="69cd" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">我们检查这些部分配合得有多好(使用适应度函数)</li><li id="28e6" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">如果没有一个片段符合我们的要求，我们就修改这些片段(使用操作符:交叉和变异)</li><li id="9a27" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">我们检查新创建的片段在一起的吻合程度(适应度函数)</li><li id="4fbb" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">我们选择我们想要为下一次迭代(选择)保留的片段</li><li id="d3a0" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">我们重新开始。直到我们找到一个符合我们要求精度的零件</li></ul><p id="0503" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们来看看伪算法:</p><p id="df76" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">开始</strong></p><ul class=""><li id="3585" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">生成初始群体</li><li id="4529" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">计算适合度</li></ul><p id="b9d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">重复</strong></p><ul class=""><li id="b03d" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">选择</li><li id="db51" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">交叉</li><li id="4c94" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">变化</li><li id="b7b5" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">计算适合度</li></ul><p id="b869" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">直到种群已经收敛</p><p id="a2ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">停止</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2c24de1e44b7971bc8838865c63478d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*n_M2uxzTqwPSjr8CtNGt-Q.png"/></div></figure><h2 id="5bbe" class="nb lg it bd lh nt nu dn ll nv nw dp lp kb nx ny lt kf nz oa lx kj ob oc mb od bi translated">但是它们是如何工作的呢？</h2><p id="4531" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">我们必须理解几个概念:</p><ul class=""><li id="3041" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">我们的人口如何初始化</li><li id="f0d2" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">交叉是如何工作的</li><li id="2f0e" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">变异是如何发生的</li><li id="9b35" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">选择是如何工作的</li><li id="f44a" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">我们如何定义我们的适应度函数</li></ul><p id="1d37" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先要明白的是，当我们在现实世界中遇到问题，想要在计算机上解决时，我们需要对它进行编码，以便计算机能够理解它。</p><p id="5985" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">比如说:</p><p id="7313" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在现实世界中，染色体是拼图的一部分。在计算机中，染色体是一个有 4 个值的向量(一个表示每个尖端的大小，其中正表示尖端，负表示片上的洞)<br/>这就是所谓的编码。</p><p id="963f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我们知道了这一点，我们将看到操作符是如何工作的。首先，你应该知道有许多类型的交叉、变异和选择，但是这里我们将只从时间的角度来看最简单的那些。</p><p id="8275" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你有兴趣了解更多，网上有很多资料。可以从这里开始:<a class="ae le" href="https://www.tutorialspoint.com/genetic_algorithms/index.htm" rel="noopener ugc nofollow" target="_blank">https://www.tutorialspoint.com/genetic_algorithms/index.htm</a></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/f48f1d49ad3dba3bd1ffa7eb4ad951ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORkThH5AGTeMe-gmi525SA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">按作者分列的数字</p></figure><p id="bb07" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">单点交叉</strong></p><p id="cfa2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的染色体是拼图，它有你在图片中看到的 4 个基因。因此，简单的交叉只是从 4 个基因中随机选择一个点，并将这些部分组合成新的染色体，如图所示。</p><p id="db4d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">理解这一点很重要，因为我们有原始的和重组的染色体。</p><p id="c796" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">均匀突变</strong></p><p id="3a0c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">统一的突变是，对于每个染色体，我们抛硬币。如果很贵，我们就修改随机选择的基因。我们赋予它什么价值？在基因允许的范围内随机选择一个。</p><p id="612f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">选择</strong></p><p id="b58b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于选择，通常使用染色体适应度(也称为可能的解决方案)。在这种情况下，我们将研究随机通用采样，它包括构建一个饼形图，其中每个染色体占据一个与其适应度相对应的空间。然后，我们在“蛋糕”周围建立 N 个固定点，其中 N 是我们要选择的染色体数目。然后，我们“旋转蛋糕”，仿佛是运气轮盘，定点指向的染色体就是被选中的，继续下一次迭代。</p><p id="e142" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你看看，染色体不是从适应度最高到最低排序的。</p><p id="60f6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这一点很重要，因为否则，选择一个高适应性染色体和另一个低适应性染色体的机会将高于选择两个高适应性染色体的机会。毕竟，由于选择点在彼此的前面，选择两条适应度相似的染色体会非常复杂。</p><p id="024e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个运算符有几种工作方式。继续我们的 10 染色体群体的例子:</p><ul class=""><li id="83b2" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">我们选择 N=10 条染色体，也就是说，我们用一个完全新的群体替换先前的群体</li><li id="2cb1" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">我们选择 N=n 条染色体，其中 N&lt;10. In other words, we replace only a part of the old chromosomes.</li></ul><p id="3178" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Okay, so if we select all 10 it’s clear, but if we select n, how do we choose which ones to remove?</p><p id="e1c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Well, the two most common ways are:</p><ul class=""><li id="cb21" class="mi mj it js b jt ju jx jy kb mk kf ml kj mm kn mn mo mp mq bi translated">We remove the oldest chromosomes</li><li id="a00f" class="mi mj it js b jt mr jx ms kb mt kf mu kj mv kn mn mo mp mq bi translated">We remove the chromosomes with the worst fitness</li></ul><p id="dc6b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Finally, there are times when we select the best chromosome (or the k best) to pass if or when the next iteration, that is, there is elitism. We have to be careful with this, because although a priori it seems that elitism is the best and that we should only stay with the best if we did we would be killing one of the greatest virtues of genetics: that they can escape to local minimums!</p><p id="fd5e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Look, here you can see a geneticist in action trying to decide which is the best configuration for a two-wheeled vehicle: <a class="ae le" href="http://rednuht.org/genetic_cars_2/" rel="noopener ugc nofollow" target="_blank">http://rednuht.org/genetic_cars_2/</a></p><p id="f4f3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们自己实现几个例子？</p><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="1de1" class="nb lg it mx b gy nc nd l ne nf"><strong class="mx iu"># example of a GA where we have to find N numbers that add up to X<br/># </strong><a class="ae le" href="https://lethain.com/genetic-algorithms-cool-name-damn-simple" rel="noopener ugc nofollow" target="_blank"><strong class="mx iu">https://lethain.com/genetic-algorithms-cool-name-damn-simple</strong></a></span><span id="5aec" class="nb lg it mx b gy ng nd l ne nf">from random import randint, random<br/>from operator import add<br/>from functools import reduce<br/>import numpy as np</span><span id="45e7" class="nb lg it mx b gy ng nd l ne nf">def individual(length, min, max):<br/><strong class="mx iu">    # we create an individual</strong><br/>    return [ randint(min,max) for x in range(length) ]</span><span id="9e7c" class="nb lg it mx b gy ng nd l ne nf">def population(count, length, min, max):   <br/>    <strong class="mx iu"># we create our population</strong><br/><strong class="mx iu">    # count: number of individuals of each population<br/>    # length: number of values per individual<br/>    # min: minimum allowed for each individual's value<br/>    # max: maximum allowed for each individual's value</strong></span><span id="ec9e" class="nb lg it mx b gy ng nd l ne nf">return [ individual(length, min, max) for x in range(count) ]</span><span id="af4d" class="nb lg it mx b gy ng nd l ne nf">def fitness(individual, target):<br/>    <strong class="mx iu"># we compute the fitness of each individual, the smaller the better</strong><br/>    <br/>    sum = reduce(add, individual, 0)<br/>    return abs(target-sum)</span><span id="b7a6" class="nb lg it mx b gy ng nd l ne nf">def grade(pop, target):<br/>    <strong class="mx iu"># we compute the average of the entire population</strong><br/>    summed = reduce(add, (fitness(x, target) for x in pop))<br/>    return summed / (len(pop) * 1.0)<br/>  <br/>def find_best_solution(pop, target):<br/><strong class="mx iu">    # we find the best solution in the current population and prints it</strong><br/>    res = [fitness(x, target) for x in pop]<br/>    res_min = np.min(res)<br/>    res_min_idx = np.where(res == res_min)[0]<br/>    for n in res_min_idx:<br/>        print('Individual: ', n, 'Valores: ', *pop[n], ' Result: ', np.sum(pop[n]), 'Target; ', target)<br/>    return res_min</span><span id="2505" class="nb lg it mx b gy ng nd l ne nf">def evolve(pop, target, retain=0.2, random_select=0.05, mutate=0.01):<br/>    graded = [ (fitness(x, target), x) for x in pop]<br/>    graded = [ x[1] for x in sorted(graded)]<br/>    retain_length = int(len(graded)*retain)<br/>    parents = graded[:retain_length]<br/>    <br/><strong class="mx iu">    # we add individuals randomnly to promote genetic diversity</strong><br/>    for individual in graded[retain_length:]:<br/>        if random_select &gt; random():<br/>            parents.append(individual)<br/>    <br/><strong class="mx iu">    # we mute some</strong><br/>    for individual in parents:<br/>        if mutate &gt; random():<br/>            pos_to_mutate = randint(0, len(individual)-1)<br/>            individual[pos_to_mutate] = randint(i_min, i_max)<br/>    <br/><strong class="mx iu">    # we reproduce (crossover) our chromossomes (individuals, solutions)</strong><br/>    parents_length = len(parents)<br/>    desired_length = len(pop) - parents_length<br/>    children = []<br/>    while len(children) &lt; desired_length:<br/>        male = randint(0, parents_length-1)<br/>        female = randint(0, parents_length-1)<br/>        if male != female:<br/>            male = parents[male]<br/>            female = parents[female]<br/>            half = round(len(male) / 2)<br/>            child = male[:half] + female[half:]<br/>            children.append(child)        <br/>    parents.extend(children)<br/>    return parents</span><span id="742b" class="nb lg it mx b gy ng nd l ne nf"><strong class="mx iu"># exectute the GA</strong><br/>generations = 20<br/>target = 108<br/>p_count = 20<br/>i_length = 5<br/>i_min = 0<br/>i_max = 100<br/>error_accepted = 1<br/>print('We intiate the population with 20 individuals.')<br/>p = population(p_count, i_length, i_min, i_max)<br/>print('We compute the fitness of those 20 individuals.')<br/>fitness_history = [grade(p, target),]<br/>print('The best individual of the initial population is:')<br/>find_best_solution(p, target)</span><span id="6308" class="nb lg it mx b gy ng nd l ne nf">for i in range(generations):<br/>    p = evolve(p, target, retain=0.2, random_select=0.2, mutate=0.4)<br/>    res = grade(p, target)<br/>    fitness_history.append(res)<br/>    <br/>    res_min = find_best_solution(p, target)<br/>    print('Generation: ', i, ' Average fitness of the population's individuals:', res)<br/>    <br/>    if res_min &lt; error_accepted:<br/>      break</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/3fee05f235ea762b6b7d125115e2a72a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0th7fdEGpyIrl4E7eBx-qQ.png"/></div></div></figure><p id="4c8a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们通过使用 Github 上可用的实现将它应用于神经网络:【https://github.com/jliphard/DeepEvolve<a class="ae le" href="https://github.com/jliphard/DeepEvolve" rel="noopener ugc nofollow" target="_blank"/></p><p id="9c66" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们克隆一个 git 存储库，它已经实现了 GA 来进化神经网络的超参数和架构:</p><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="393f" class="nb lg it mx b gy nc nd l ne nf">!rm -rf DeepEvolve<br/>!git clone <a class="ae le" href="https://github.com/jliphard/DeepEvolve.git" rel="noopener ugc nofollow" target="_blank">https://github.com/jliphard/DeepEvolve.git</a></span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/8147027991361a4ef763a77ecae6c676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*SgmKEi3BAHF9SvXeNDwakw.png"/></div></figure><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="110c" class="nb lg it mx b gy nc nd l ne nf">!ls</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/8a0feec0331240875cefa4da004d2683.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*AWKeZU0pkNtix3HLtBo8PA.png"/></div></figure><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="ca03" class="nb lg it mx b gy nc nd l ne nf">!pip install tqdm</span></pre><p id="260f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看一看正在进行搜索以找到最佳组合的参数:</p><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="31d5" class="nb lg it mx b gy nc nd l ne nf">if dataset == 'mnist_cnn':<br/>        generations = 8 # Number of times to evolve the population.<br/>        all_possible_genes = {<br/>            'nb_neurons': [16, 32, 64, 128],<br/>            'nb_layers':  [1, 2, 3, 4 ,5],<br/>            'activation': ['relu', 'elu', 'tanh', 'sigmoid', 'hard_sigmoid','softplus','linear'],<br/>            'optimizer':  ['rmsprop', 'adam', 'sgd', 'adagrad','adadelta', 'adamax', 'nadam']<br/>        }</span></pre><p id="c3f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们将执行 GA:</p><pre class="kp kq kr ks gt mw mx my mz aw na bi"><span id="4cde" class="nb lg it mx b gy nc nd l ne nf">!python DeepEvolve/main.py</span></pre><h1 id="249d" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">最后的话</h1><p id="b8a4" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">一如既往，我希望你<strong class="js iu"> </strong>喜欢这个帖子，并且你获得了关于如何优化你的神经网络参数的直觉！</p><p id="41d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="oi">如果你喜欢这篇文章，那么你可以看看我关于数据科学和机器学习的其他文章</em> <a class="ae le" href="https://medium.com/@rromanss23" rel="noopener"> <em class="oi">这里</em> </a> <em class="oi">。</em></p><p id="554b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="oi">如果你想了解更多关于机器学习、数据科学和人工智能的知识</em> <strong class="js iu"> <em class="oi">请关注我的 Medium </em> </strong> <em class="oi">，敬请关注我的下一篇帖子！</em></p></div></div>    
</body>
</html>