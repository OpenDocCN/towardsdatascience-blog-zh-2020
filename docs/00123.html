<html>
<head>
<title>Gradient Descent Explanation &amp; Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降解释和实施</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-explanation-implementation-c74005ff7dd1?source=collection_archive---------15-----------------------#2020-01-04">https://towardsdatascience.com/gradient-descent-explanation-implementation-c74005ff7dd1?source=collection_archive---------15-----------------------#2020-01-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="760a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">香草梯度下降算法分步指南</h2></div><p id="9c48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">梯度下降可能是最著名的优化算法，在机器学习的世界中，你肯定已经直接或间接使用过梯度下降。你可能已经知道，通过向梯度的负方向移动微小的步长，可以帮助最小化损失函数。但是具体怎么做呢？除了直接使用包含在包中的梯度方法，我们如何实现我们自己的方法并更仔细地观察梯度下降呢？</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="1041" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">理解渐变</h1><p id="90d0" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">在这篇文章中，让我们一点一点地打破梯度下降，并通过实现我们自己的来加深对它的理解。现在让我们从最简单的例子开始:</p><blockquote class="mi mj mk"><p id="9a1e" class="ki kj ml kk b kl km ju kn ko kp jx kq mm ks kt ku mn kw kx ky mo la lb lc ld im bi translated">假设我们有函数<code class="fe mp mq mr ms b">f(x) = x^2</code>，其中<code class="fe mp mq mr ms b">x</code>的范围从-1到1，给定<code class="fe mp mq mr ms b">x</code>在该范围内随机开始，如何求<code class="fe mp mq mr ms b">f(x)</code>的最小值？</p></blockquote><figure class="mt mu mv mw gt mx"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi na"><img src="../Images/7757351f3327a7e134e5716c21d2d0f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*PNIK7L_pUEgGKkYJtdHvyQ.png"/></div></figure><p id="682f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">显然在这个例子中，最小值位于<code class="fe mp mq mr ms b">x = 0</code>，我们希望</p><ul class=""><li id="ae19" class="nd ne it kk b kl km ko kp kr nf kv ng kz nh ld ni nj nk nl bi translated">使<code class="fe mp mq mr ms b">x</code>向右移动<code class="fe mp mq mr ms b">if x &lt; 0</code></li><li id="a008" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">使<code class="fe mp mq mr ms b">x</code>向左移动<code class="fe mp mq mr ms b">if x &gt; 0</code></li></ul><p id="1268" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么我们如何做到这一点呢？你可能还会注意到函数的梯度</p><ul class=""><li id="89b7" class="nd ne it kk b kl km ko kp kr nf kv ng kz nh ld ni nj nk nl bi translated"><code class="fe mp mq mr ms b">grad &gt; 0</code>当<code class="fe mp mq mr ms b">x &gt; 0</code></li><li id="df23" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated"><code class="fe mp mq mr ms b">grad &lt; 0</code>当<code class="fe mp mq mr ms b">x &lt; 0</code></li></ul><p id="b468" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与<code class="fe mp mq mr ms b">x</code>的移动方向相反！梯度下降利用这一点，让<code class="fe mp mq mr ms b">x</code>向其梯度的相反方向移动。在这种情况下，无论<code class="fe mp mq mr ms b">x</code>在哪里，它都会移动到最小。</p><p id="dcee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下一个问题是<code class="fe mp mq mr ms b">x</code>应该移动多远，这导致了学习率，它是许多机器学习算法中的参数。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c3d1d8b5caedd23896135897ac6b3f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*mASFu34-ifph24JW3o_ksg.png"/></div></figure><p id="f06f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事实上，在上面的例子中，<code class="fe mp mq mr ms b">tanα</code>是梯度，在每一步，我们使<code class="fe mp mq mr ms b">x</code>以步长<code class="fe mp mq mr ms b">tanα * learning_rate</code>移动，其中学习率成为控制下降速度的可调参数。</p><h1 id="5040" class="ll lm it bd ln lo ns lq lr ls nt lu lv jz nu ka lx kc nv kd lz kf nw kg mb mc bi translated">首次实施</h1><p id="6de2" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">记住上面的内容，我们可以让我们的第一个实现找到函数<code class="fe mp mq mr ms b">x^2</code>的最小值。</p><figure class="mt mu mv mw gt mx"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="1eda" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在每次迭代中，<code class="fe mp mq mr ms b">x -= lr*grad_fn(x)</code>使其总是向最小值移动，我们也可以绘制出<code class="fe mp mq mr ms b">x</code>的轨迹:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi na"><img src="../Images/308d24a7341cf48bc25a8b12aa56b6bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*fWhMxlbjDnDePjE-fuR46w.png"/></div></figure><p id="003d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe mp mq mr ms b">x</code>从-1开始，逐渐下降到0。还要注意，它在开始时移动得更快，当接近目标时变慢，这是因为绝对梯度在开始时更高。</p><h1 id="e5ce" class="ll lm it bd ln lo ns lq lr ls nt lu lv jz nu ka lx kc nv kd lz kf nw kg mb mc bi translated">优化参数</h1><p id="8ea0" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">现在让我们来看一个优化参数的例子。假设我们试图用函数优化参数:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/721aad0e31f2b05256bbd03eedd688e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*g54gv6rsq2oOdNnkUBWF3Q.png"/></div></figure><p id="05cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目标是最小化损失<code class="fe mp mq mr ms b">(y — f(x))^2</code>，参数<code class="fe mp mq mr ms b">a</code>和<code class="fe mp mq mr ms b">b</code>的相应梯度为:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7c3c24ce5332495b10e53e4863dda613.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*pGllZKhu6Lgm5fE1jV47gA.png"/></div></figure><p id="b662" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意这里的<code class="fe mp mq mr ms b">x</code>和<code class="fe mp mq mr ms b">y</code>被认为是常数。</p><p id="3f5a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们实现函数优化:</p><figure class="mt mu mv mw gt mx"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="40ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，我们更新每个输入<code class="fe mp mq mr ms b">x, y</code>对的参数，并得到结果:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/44b395e73589b145a4f17d4e71ead2e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*TSUNoSrURl1yg6jyy_uWbw.png"/></div></figure><p id="55bd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以看到，更新过程是不稳定的，对于参数<code class="fe mp mq mr ms b">a</code>，在它移动到最佳值之前，它首先下降(向相反的方向)。这是因为<em class="ml">我们更新了每个输入</em>的参数，因为每个单独的输入都可能在任意方向更新参数。我们能生成更平滑的线条吗？答案导致批量梯度下降。</p><h1 id="abbc" class="ll lm it bd ln lo ns lq lr ls nt lu lv jz nu ka lx kc nv kd lz kf nw kg mb mc bi translated">批量梯度下降</h1><p id="c13f" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">在实际使用情况下，参数不是每次在单个数据点上更新，而是应用批量更新，其中在每次迭代(历元)中，基于一批数据点的平均值更新参数。在这种情况下，我们的更新公式是:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/7f45d26ad8b594b22da3effc63b4bd9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*sztUiOV2CGZJuzwp-VEdbA.png"/></div></figure><p id="d85e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">计算一批梯度的总和，取平均值作为要更新的梯度:</p><figure class="mt mu mv mw gt mx"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="f8f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里需要更高的迭代次数，因为每批只贡献1次更新(我们将所有数据点放入一批，可以有不同的组合)，这一次更新过程如下所示:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi na"><img src="../Images/d8c3ddc9b08a0874ac217d9b1939ea35.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*JmWyqgXxX2AuFcqB2vFXGA.png"/></div></figure><p id="6dc0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更加平滑和稳定，但是代价是需要更多的计算。</p><h1 id="245f" class="ll lm it bd ln lo ns lq lr ls nt lu lv jz nu ka lx kc nv kd lz kf nw kg mb mc bi translated">结论</h1><p id="108b" class="pw-post-body-paragraph ki kj it kk b kl md ju kn ko me jx kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">希望到这里为止，你已经获得了对香草梯度下降稍微好一点的理解。传统的梯度下降法不能保证最优，事实上，当目标函数中有多个流域时，它很容易陷入局部极小，因为每次参数仅基于梯度轻微移动，不允许有随机性。如果你感兴趣，请在这里查看完整的实现。</p><p id="cc25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我将介绍<a class="ae ob" rel="noopener" target="_blank" href="/stochastic-gradient-descent-momentum-explanation-8548a1cd264e"> SGD &amp; Momentum </a>，它为普通梯度下降增加了一些变化，并解决了它的一些问题。</p><p id="fb3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考:</p><ol class=""><li id="7641" class="nd ne it kk b kl km ko kp kr nf kv ng kz nh ld oc nj nk nl bi translated"><a class="ae ob" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank">https://ml-cheat sheet . readthedocs . io/en/latest/gradient _ descent . html</a></li><li id="51bf" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld oc nj nk nl bi translated"><a class="ae ob" href="https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms" rel="noopener ugc nofollow" target="_blank">https://ruder . io/optimization-gradient-descent-descent/index . html # gradient descent optimizationalgorithms</a></li></ol></div></div>    
</body>
</html>