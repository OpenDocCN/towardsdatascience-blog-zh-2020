<html>
<head>
<title>Data Prep with Spark DataFrames</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用火花数据帧进行数据准备</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-prep-with-spark-dataframes-3629478a1041?source=collection_archive---------9-----------------------#2020-01-25">https://towardsdatascience.com/data-prep-with-spark-dataframes-3629478a1041?source=collection_archive---------9-----------------------#2020-01-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4fbf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 PySpark 继续调查金融服务消费者投诉数据库</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6e6dd55f63c0393dc49f6f9abc903b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8BhYHYEJQtOeX5sp"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">当你向消费者金融保护局投诉时，你希望得到的平静——照片由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@stphnwlkr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Stephen Walker </a>拍摄</p></figure><p id="9b59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在<a class="ae ky" rel="noopener" target="_blank" href="/exploring-financial-consumer-complaints-with-spark-48a253f9c830?source=friends_link&amp;sk=66da638afb846bf78032a63afd8acc0c">上周的博客</a>中看到的，三大信用报告机构是美国联邦金融服务消费者投诉数据库中被投诉最多的公司之一。想想你的信用评分被用来做的每一件事都很有趣:获得贷款、租公寓、购买手机套餐。算了吧。与其说有趣，不如说可怕。因此，今天我们将深入挖掘投诉数据库，回答我一直在思考的几个问题:</p><ul class=""><li id="a228" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">收到的投诉数量与星期几有什么关系？</li><li id="4c58" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">这些年来投诉数量有什么变化？</li><li id="77a0" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">投诉数量每月有何变化？</li><li id="a98b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">哪个州的居民抱怨最多？</li><li id="5eae" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">不同州的人提交投诉的比率不同吗？</li><li id="b530" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">哪些产品被投诉最多？</li></ul><p id="c88f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦你有了 Spark 数据框架中的数据(如果没有，查看上周的<a class="ae ky" rel="noopener" target="_blank" href="/exploring-financial-consumer-complaints-with-spark-48a253f9c830?source=friends_link&amp;sk=66da638afb846bf78032a63afd8acc0c">文章</a>)，你就准备好做一些探索和清理了。<a class="ae ky" href="https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame" rel="noopener ugc nofollow" target="_blank"> PySpark 数据框架</a>、<a class="ae ky" href="https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.Column" rel="noopener ugc nofollow" target="_blank"> PySpark 列</a>和<a class="ae ky" href="https://spark.apache.org/docs/2.3.0/api/sql/index.html" rel="noopener ugc nofollow" target="_blank"> PySpark 函数</a>文档肯定会成为你的朋友，因为你在自己的环境中工作(分别是 Ross、Monica 和 Chandler……对不起 Joey，我仍然不确定你在数据科学世界中的位置)。对于我在<a class="ae ky" href="https://www.consumerfinance.gov/data-research/consumer-complaints/" rel="noopener ugc nofollow" target="_blank">美国联邦金融服务消费者投诉数据库</a>上的项目，我将计数、填充和删除 nan，将 date 列转换为 datetime 并提取日期特征，并对照可接受值列表检查值。</p><h1 id="686d" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">使用 Jupyter 笔记本时，更易于使用的输出格式</h1><p id="c741" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在开始之前，我建议您像这样设置 Spark 配置:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="2b2a" class="nl mk it nh b gy nm nn l no np">spark.conf.set('spark.sql.repl.eagerEval.enabled', True)</span></pre><p id="91f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个设置使得输出更像熊猫，而不像命令行 SQL。此后，您不再需要指定 show()来查看输出。或者，您也可以使用<code class="fe nq nr ns nh b">.toPandas()</code>或<code class="fe nq nr ns nh b">.toPandas().T</code>(用于移调)来查看熊猫风格的输出。请记住，仅在足够小以适合内存的数据帧上执行此操作。太大的<code class="fe nq nr ns nh b">pandas</code>数据帧很容易使你的内核崩溃。</p><h1 id="8f39" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">计算 nan 和 Nulls</h1><p id="ab58" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">请注意，在 PySpark 中，NaN 与 Null 不同。这两者也不同于空字符串""，因此您可能希望在任何数据集特定填充值之上检查其中的每一个。</p><p id="91de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像这样检查 nan:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="0ab5" class="nl mk it nh b gy nm nn l no np">from pyspark.sql.functions import isnan, when, count, col</span><span id="17f5" class="nl mk it nh b gy nt nn l no np">df.select([count(when(isnan(c), c)).alias(c) for c in df.columns])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/b0ff21a5bba3858ad322519f8d9cef80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U7IBHmoryR4mKyUq"/></div></div></figure><p id="a2c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以在这里看到，这种格式肯定比标准输出更容易阅读，标准输出不适合长列标题，但它仍然需要向右滚动才能看到剩余的列。</p><p id="b693" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个数据集没有 NaNs，所以我转到 Null。</p><p id="cff2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用以下代码计算空值:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="6c1a" class="nl mk it nh b gy nm nn l no np">from pyspark.sql.functions import when, count, col</span><span id="7921" class="nl mk it nh b gy nt nn l no np">df.select([count(when(col(c).isNull(), c)).alias(c) for c in <br/>           df.columns]).toPandas().T</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/acb4cc9f18cb1235b97eed746a158688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/0*v7IBGAqELotzYeDH"/></div></figure><h1 id="7b1a" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">检查重复</strong></h1><p id="0147" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">为了检查重复项，我比较了 df.count()和 df.distinct()。计数()。在这种情况下，我没有。</p><h1 id="634a" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">处理空值</strong></h1><p id="f010" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">接下来，我决定删除 company_response_to_consumer 中值为 null 的那一行。这里我们看到它和熊猫很像。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="c65c" class="nl mk it nh b gy nm nn l no np">df_clean = df.dropna(subset='company_response_to_consumer')</span></pre><p id="117e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于 consumer _ contracted 列，我决定用 No 替换 null 值，同时为这一更改添加一个 flag 列:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="55cf" class="nl mk it nh b gy nm nn l no np"># add flag column<br/>df_clean = df_clean.withColumn('null_c_disputed', <br/>                            df_clean['consumer_disputed?'].isNull())</span><span id="0368" class="nl mk it nh b gy nt nn l no np"># fill na in consumer_disputed? with 'No'<br/>df_clean = df_clean.fillna('No', subset=’consumer_disputed?’)</span></pre><p id="f03a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，这与 pandas 非常相似，除了用于添加列的新语法“withColumn”)。在下一节中，您将看到添加新列的另一种方法。</p><h1 id="b9eb" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">将字符串转换为日期(时间)并生成日期相关特征</strong></h1><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="1a23" class="nl mk it nh b gy nm nn l no np">from pyspark.sql.functions import (to_date, datediff, date_format, <br/>                                   month)</span><span id="7d55" class="nl mk it nh b gy nt nn l no np"># add datetime columns<br/>df_clean = df_clean.select('*', to_date(df_clean['date_received'], <br/>                          'MM/dd/yyyy').alias('date_received_dt'))<br/>df_clean = df_clean.select('*', <br/>                    to_date(df_clean['date_sent_to_company'], <br/>                    'MM/dd/yyyy').alias('date_sent_to_company_dt'))</span><span id="ee14" class="nl mk it nh b gy nt nn l no np"># drop string date columns<br/>df_clean = df_clean.drop(‘date_received’) \ <br/>                   .drop(‘date_sent_to_company’)</span></pre><p id="eca3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我准备添加我的新功能:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="007e" class="nl mk it nh b gy nm nn l no np"># add time difference between receipt and sent to company<br/>df_clean = df_clean.withColumn('transit_time',<br/>                       datediff(df_clean['date_sent_to_company_dt'],<br/>                       df_clean['date_received_dt']))</span><span id="8cc8" class="nl mk it nh b gy nt nn l no np"># add submitted day of week (1=Monday, 7=Sunday)<br/>df_clean = df_clean.withColumn('dow_submitted', <br/>                              date_format('date_received_dt', 'u') \<br/>                              .alias('dow_submitted'))</span><span id="2d50" class="nl mk it nh b gy nt nn l no np"># add submitted month, year<br/>df_clean = df_clean.withColumn('y_submitted', <br/>                              date_format('date_received_dt', 'y') \ <br/>                              .alias('y_submitted')) \<br/>                   .withColumn('m_submitted', <br/>                               month('date_received_dt') \ <br/>                               .alias('m_submitted'))</span></pre><p id="cc98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这让我能够研究我们数据集的时间方面</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/0069f7a6ae44bd90d54fdc273e2e00b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Bm7fPV0FscnEe8rn"/></div></div></figure><p id="43f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有趣的是，尽管有网上表格，但人们大多在投诉。我想知道这是如何按提交类型划分的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/e8f7c156a652c7ae500ff092c09d5837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JKPuuVE5Oepg34-Y"/></div></div></figure><p id="4568" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里似乎确实有周期性模式的可能性，但仅从这张图表上很难说。未来调查的另一个领域。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/46e32259041f3e10cc659c2b5f95286e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_cinZwydhRw1S0rE"/></div></div></figure><p id="5906" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与可接受值列表相比，来自&lt;75k 2012 to 275k in 2019.</p><h1 id="9864" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">的投诉明显呈上升趋势</strong></h1><p id="81c6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我使用州缩写列表清理了我的 state 列，用一个值“unknown”替换了所有非标准响应。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="427a" class="nl mk it nh b gy nm nn l no np"># add clean state column, drop original column, rename new column<br/>df_clean = df_clean.withColumn(‘state_c’, when(col(‘state’)<br/>                                         .isin(states), <br/>                                         col(‘state’)) \         <br/>                                         .otherwise(‘unknown’)) \<br/>                   .drop(‘state’) \<br/>                   .withColumnRenamed(‘state_c’, ‘state’)</span></pre><p id="a00b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我最终得到了 26k 个“未知”状态的行——这是限制输入的主要原因，但至少现在已经包含了，而不是一大堆随机的错误输入的状态值。</p><p id="074d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这个，我按州分组，并把我现在的 50 行 2 列的数据框架转换成熊猫。然后，我将这个数据框架与我基于州的数据连接起来(谢谢<a class="ae ky" href="https://simplemaps.com/data/us-zips" rel="noopener ugc nofollow" target="_blank"> SimpleMaps </a> CC4.0)，并使用 geopandas(以及来自<a class="ae ky" href="https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html" rel="noopener ugc nofollow" target="_blank">census.gov</a>的 shapefiles)绘制图表。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="3c76" class="nl mk it nh b gy nm nn l no np">state_counts = df_clean.groupby(“state”).count().toPandas()</span></pre><p id="8ab7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 GeoPandas 中，我发现阿拉斯加最西部的岛屿与我的可视化效果不太好，于是选择今天只绘制美国的连续部分——改天再详细介绍如何分离和操作形状优美的多多边形。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/2e5b0ac5f1f8c3038d939f29eda21160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*maZYq7kicGhHxvhO"/></div></div></figure><p id="89ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这仍然不是一个非常有吸引力的投影选择(强迫一个地球仪在一个平面屏幕上)，但我们正在了解大多数投诉来自哪里的要点——人口最多的州。我想公平竞争，并考虑人口数量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/3d87d1c261df15ce4215d512ba9ee594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tS4wx-QosTYU16Yv"/></div></div></figure><p id="f7d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">choropleth 会自动缩放颜色，因此我们必须有一个投诉提交量不成比例的小州。稍加挖掘就会发现，DC 的华盛顿州以每 100 名居民 1.1 起的投诉率领先。</p><h1 id="f449" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">按投诉计数的产品</h1><p id="5ad6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">最后，我执行了另一个<code class="fe nq nr ns nh b">groupby().count()</code>来查看哪些产品是最值得关注的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/9e4d3353a6d95204209f915a32adaf4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d3d2yDqxvVNd37m_"/></div></div></figure><p id="1fb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们发现信用报告在前四个最有问题的产品中出现了两次。这与我们在我之前的博客中发现的相吻合，三大信用报告机构 Equifax、Experian 和 Transunion 是投诉最多的三家金融公司。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="39fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一如既往，你可以在<a class="ae ky" href="https://github.com/allisonhonold/spark-data-prep-blog" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>上查看详情。编码快乐！</p></div></div>    
</body>
</html>