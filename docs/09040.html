<html>
<head>
<title>TensorBoard: Hyperparameter Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量板:超参数优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tensorboard-hyperparameter-optimization-a51ef7af71f5?source=collection_archive---------17-----------------------#2020-06-29">https://towardsdatascience.com/tensorboard-hyperparameter-optimization-a51ef7af71f5?source=collection_archive---------17-----------------------#2020-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3720" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用 TensorBoard 的 HParamas 仪表板为深度学习模型找到最佳超参数。</h2></div><p id="84f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">先决条件:</strong></p><p id="82fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" rel="noopener" target="_blank" href="/tensorboard-a-visualization-suite-for-tensorflow-models-c484dd0f16cf">tensor board——tensor flow 模型的可视化套件</a></p><p id="c33d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">在本文中，您将学习超参数优化，然后使用 TensorBoard 显示超参数优化的结果。</strong></p><p id="d6f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lf">深度神经网络背景下的超参数是什么？</em>T9】</strong></p><p id="4145" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你在深度学习神经网络中的目标是找到节点的权重，这将帮助我们理解图像、任何文本或语音中的数据模式。</p><p id="e728" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，您可以使用为模型提供最佳准确度和精度的值来设计神经网络参数。</p><p id="3383" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lf">那么，这些被称为超参数的参数是什么呢？</em>T13】</strong></p><p id="19ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">用于训练神经网络模型的不同参数称为超参数。这些超参数像旋钮一样被调整，以提高神经网络的性能，从而产生优化的模型。</strong></p><p id="4095" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经网络中的一些超参数是</p><ul class=""><li id="e9d8" class="lg lh it kk b kl km ko kp kr li kv lj kz lk ld ll lm ln lo bi translated"><strong class="kk iu">隐藏层数</strong></li><li id="bd36" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><strong class="kk iu">隐藏层中单元或节点的数量</strong></li><li id="e402" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><strong class="kk iu">学习率</strong></li><li id="da25" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><strong class="kk iu">辍学率</strong></li><li id="1d16" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><strong class="kk iu">历元或迭代</strong></li><li id="aa60" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><strong class="kk iu">像 SGD，Adam，AdaGrad，Rmsprop 等优化者。</strong></li><li id="1e1d" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><strong class="kk iu">激活功能，如 ReLU、sigmoid、leaky ReLU 等。</strong></li><li id="cc34" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><strong class="kk iu">批量大小</strong></li></ul><p id="70a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lf">如何实现超参数优化？</em> </strong></p><blockquote class="lu"><p id="81ce" class="lv lw it bd lx ly lz ma mb mc md ld dk translated">超参数优化是找到超参数值的过程，如优化器、学习率、辍学率等。深度学习算法的一部分，它将提供最好的模型性能。</p></blockquote><figure class="mf mg mh mi mj mk gh gi paragraph-image"><div class="gh gi me"><img src="../Images/6f12fa5e4a09651b666793fa75deefc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*1E65lJj395Ax5XGTCSaw1Q.png"/></div></figure><p id="f766" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以使用以下技术执行超参数优化。</p><ul class=""><li id="71ae" class="lg lh it kk b kl km ko kp kr li kv lj kz lk ld ll lm ln lo bi translated"><strong class="kk iu">手动搜索</strong></li><li id="0461" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><a class="ae le" href="https://medium.com/datadriveninvestor/tuning-artificial-neural-network-b028dcc3b9d0" rel="noopener"> <strong class="kk iu">网格搜索</strong> </a> : <strong class="kk iu">对产生笛卡尔积的指定超参数的所有可能组合进行彻底搜索。</strong></li><li id="40fd" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><strong class="kk iu">随机搜索</strong> : <strong class="kk iu">随机选择超参数，并不是每个超参数组合都尝试</strong>。随着超参数数量的增加，随机搜索是更好的选择，因为它可以更快地获得超参数的良好组合。</li><li id="052c" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><a class="ae le" href="https://medium.com/datadriveninvestor/bayesian-optimization-f5ab0cc653a7" rel="noopener"> <strong class="kk iu">贝叶斯优化</strong></a><strong class="kk iu">:</strong>I<strong class="kk iu">n 合并关于超参数的先验数据，</strong>包括模型的精度或损失<strong class="kk iu">。先验信息有助于确定模型超参数选择的更好近似。</strong></li></ul><p id="c691" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了可视化 TensorBoard 上模型的超参数调整，我们将使用网格搜索技术，其中我们将使用一些超参数，如节点数量、不同的优化器或学习率以及不同的退出率，并查看模型的准确性和损失。</p><p id="b0bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lf">为什么要用 TensorBoard 进行超参数优化？</em>T13】</strong></p><p id="e3e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一张图胜过千言万语，这也适用于复杂的深度学习模型。深度学习模型被认为是一个黑匣子，你发送一些输入数据，模型进行一些复杂的计算，瞧，你现在有你的结果了！！！</p><blockquote class="lu"><p id="09cc" class="lv lw it bd lx ly lz ma mb mc md ld dk translated">TensorBoard 是 Tensorflow 的一个可视化工具包，用于显示不同的指标、参数和其他可视化内容，帮助调试、跟踪、微调、优化和共享您的深度学习实验结果</p></blockquote><p id="39dc" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><strong class="kk iu">有了 TensorBoard，可以在每个历元跟踪模型的精度和损耗；并且还具有不同的超参数值</strong>。<strong class="kk iu">超参数不同值的跟踪精度将帮助您更快地微调模型。</strong></p><p id="4175" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lf">最后，下面是用 Python 实现的代码……</em></strong></p><p id="7762" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用 TensorBoard 使用<a class="ae le" href="https://www.kaggle.com/c/dogs-vs-cats/data" rel="noopener ugc nofollow" target="_blank">猫狗数据集</a>可视化标量、图形和分布。</p><h1 id="3569" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">导入所需的库</h1><p id="36cc" class="pw-post-body-paragraph ki kj it kk b kl nk ju kn ko nl jx kq kr nm kt ku kv nn kx ky kz no lb lc ld im bi translated">导入 TensorFlow 和 TensorBoard HParams 插件以及 Keras 库，用于预处理图像和创建模型。</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="48af" class="ny mt it nu b gy nz oa l ob oc"><strong class="nu iu">import tensorflow as tf<br/>from tensorboard.plugins.hparams import api as hp<br/>import datetime<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D<br/>from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img<br/>import numpy as np</strong></span></pre><p id="767d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我用过 TensorFlow 2.0.0 版本。</p><h1 id="73bc" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">加载 TensorBoard 笔记本扩展</h1><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="4eea" class="ny mt it nu b gy nz oa l ob oc"># Load the TensorBoard notebook extension<br/><strong class="nu iu">%load_ext tensorboard</strong></span></pre><h1 id="45b9" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">创建图像分类深度学习模型</h1><p id="e6b1" class="pw-post-body-paragraph ki kj it kk b kl nk ju kn ko nl jx kq kr nm kt ku kv nn kx ky kz no lb lc ld im bi translated"><strong class="kk iu">设置培训的关键参数</strong></p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="6662" class="ny mt it nu b gy nz oa l ob oc"><strong class="nu iu">BASE_PATH = 'Data\\dogs-vs-cats\\train\\'<br/>TRAIN_PATH='Data\\dogs-vs-cats\\train_data\\'<br/>VAL_PATH='Data\\dogs-vs-cats\\validation_data\\'batch_size = 32 <br/>epochs = 5<br/>IMG_HEIGHT = 150<br/>IMG_WIDTH = 150</strong></span></pre><p id="fafd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">重新缩放并对训练图像应用不同的增强</strong></p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="00f7" class="ny mt it nu b gy nz oa l ob oc"><strong class="nu iu">train_image_generator = ImageDataGenerator(                                                rescale=1./255,                                              rotation_range=45,                                                width_shift_range=.15,                                                height_shift_range=.15,                                                horizontal_flip=True,                                                zoom_range=0.3)</strong></span></pre><p id="3943" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">重新调整验证数据</strong></p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="6b03" class="ny mt it nu b gy nz oa l ob oc"><strong class="nu iu">validation_image_generator = ImageDataGenerator(rescale=1./255)</strong></span></pre><p id="65f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">为训练和验证数据集生成批量归一化数据</strong></p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="9067" class="ny mt it nu b gy nz oa l ob oc"><strong class="nu iu">train_data_gen = train_image_generator.flow_from_directory(batch_size = batch_size,                                                     directory=TRAIN_PATH,                                                     shuffle=True,                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),                                                     class_mode='categorical')val_data_gen = validation_image_generator.flow_from_directory(batch_size = batch_size,                                                              directory=VAL_PATH,                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),                                                              class_mode='categorical')</strong></span></pre><h2 id="69bc" class="ny mt it bd mu od oe dn my of og dp nc kr oh oi ne kv oj ok ng kz ol om ni on bi translated">为网格搜索设置超参数</h2><p id="2743" class="pw-post-body-paragraph ki kj it kk b kl nk ju kn ko nl jx kq kr nm kt ku kv nn kx ky kz no lb lc ld im bi translated">我们通过列出超参数的不同值或值范围，使用四个超参数来运行我们的实验。</p><p id="a88c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">对于离散的超参数，会尝试所有可能的参数组合，对于实值参数，只会使用上下界。</strong></p><ol class=""><li id="2f95" class="lg lh it kk b kl km ko kp kr li kv lj kz lk ld oo lm ln lo bi translated"><strong class="kk iu">第一密集层单元数:256 和 512 </strong></li><li id="481c" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld oo lm ln lo bi translated"><strong class="kk iu">辍学率:范围在 0.1 到 0.2 之间。因此将使用 0.1 和 0.2 的辍学率。</strong></li><li id="cfd1" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld oo lm ln lo bi translated"><strong class="kk iu">优化者:亚当、SGD 和 rmsprop </strong></li><li id="6580" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld oo lm ln lo bi translated"><strong class="kk iu">优化器的学习率:0.001，0.0001，0.0005，</strong></li></ol><p id="4861" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还将指标设置为显示在 TensorBoard 上的精确度</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="0058" class="ny mt it nu b gy nz oa l ob oc">## Create hyperparameters<br/><strong class="nu iu">HP_NUM_UNITS=hp.HParam('num_units', hp.Discrete([ 256, 512]))<br/>HP_DROPOUT=hp.HParam('dropout', hp.RealInterval(0.1, 0.2))<br/>HP_LEARNING_RATE= hp.HParam('learning_rate', hp.Discrete([0.001, 0.0005, 0.0001]))<br/>HP_OPTIMIZER=hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop']))</strong></span><span id="0c07" class="ny mt it nu b gy op oa l ob oc"><strong class="nu iu">METRIC_ACCURACY='accuracy'</strong></span></pre><p id="3417" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">创建和配置日志文件</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="cafd" class="ny mt it nu b gy nz oa l ob oc"><strong class="nu iu">log_dir ='\\logs\\fit\\' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')<br/>with tf.summary.create_file_writer(log_dir).as_default():<br/>    hp.hparams_config(<br/>    hparams=<br/>    [HP_NUM_UNITS, HP_DROPOUT,  HP_OPTIMIZER, HP_LEARNING_RATE],<br/>    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],<br/>    )</strong></span></pre><h1 id="93d2" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">创建、编译和拟合模型</h1><p id="563e" class="pw-post-body-paragraph ki kj it kk b kl nk ju kn ko nl jx kq kr nm kt ku kv nn kx ky kz no lb lc ld im bi translated">超参数不是硬编码的，而是取自<strong class="kk iu"> <em class="lf"> hparams </em> </strong>字典的不同参数:<strong class="kk iu">HP _ dropout 对于 dropout，HP_NUM_UNITS 对于第一个密集层中的单元数，HP_OPTIMIZER 设置不同的优化器。</strong>我们采用使用的优化器，并根据<strong class="kk iu"> HP_LEARNING_RATE 设置学习率。</strong></p><p id="81b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该函数返回最后一个纪元的验证精度。</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="e405" class="ny mt it nu b gy nz oa l ob oc"><strong class="nu iu">def create_model(hparams):<br/>    model = Sequential([<br/>    Conv2D(64, 3, padding='same', activation='relu', <br/>           input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),<br/>    MaxPooling2D(),</strong><br/>    #setting the Drop out value based on HParam<br/>    <strong class="nu iu">Dropout(hparams[HP_DROPOUT]),<br/>    Conv2D(128, 3, padding='same', activation='relu'),<br/>    MaxPooling2D(),<br/>    Dropout(hparams[HP_DROPOUT]),<br/>    Flatten(),<br/>    Dense(hparams[HP_NUM_UNITS], activation='relu'),<br/>    Dense(2, activation='softmax')])</strong><br/>    <br/>    #setting the optimizer and learning rate<br/>    <strong class="nu iu">optimizer = hparams[HP_OPTIMIZER]<br/>    learning_rate = hparams[HP_LEARNING_RATE]<br/>    if optimizer == "adam":<br/>        optimizer = tf.optimizers.Adam(learning_rate=learning_rate)<br/>    elif optimizer == "sgd":<br/>        optimizer = tf.optimizers.SGD(learning_rate=learning_rate)<br/>    elif optimizer=='rmsprop':<br/>        optimizer = tf.optimizers.RMSprop(learning_rate=learning_rate)<br/>    else:<br/>        raise ValueError("unexpected optimizer name: %r" % (optimizer_name,))</strong><br/>    <br/>    # Comiple the mode with the optimizer and learninf rate specified in hparams<br/>    <strong class="nu iu">model.compile(optimizer=optimizer,<br/>              loss='categorical_crossentropy',<br/>              metrics=['accuracy'])</strong><br/>    <br/>    #Fit the model <br/>    <strong class="nu iu">history=model.fit_generator(<br/>    train_data_gen,<br/>    steps_per_epoch=1000,<br/>    epochs=epochs,<br/>    validation_data=val_data_gen,<br/>    validation_steps=1000,<br/>    callbacks=[<br/>        tf.keras.callbacks.TensorBoard(log_dir),  # log metrics<br/>        hp.KerasCallback(log_dir, hparams),# log hparams<br/>        <br/>    ])<br/>    return history.history['val_accuracy'][-1]</strong></span></pre><p id="b9eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于模型的每次运行，记录带有超参数和最终历元精度的<strong class="kk iu"> <em class="lf"> hparams </em> </strong>摘要。我们需要将最后一个时期的验证精度转换为标量值。</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="de49" class="ny mt it nu b gy nz oa l ob oc"><strong class="nu iu">def run(run_dir, hparams):<br/>  with tf.summary.create_file_writer(run_dir).as_default():<br/>    hp.hparams(hparams)</strong>  # record the values used in this trial<br/>    <strong class="nu iu">accuracy = create_model(hparams)</strong><br/>    #converting to tf scalar<br/>    <strong class="nu iu">accuracy= tf.reshape(tf.convert_to_tensor(accuracy), []).numpy()<br/>    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)</strong></span></pre><h2 id="43da" class="ny mt it bd mu od oe dn my of og dp nc kr oh oi ne kv oj ok ng kz ol om ni on bi translated">使用不同的超参数值运行模型</h2><p id="d2c3" class="pw-post-body-paragraph ki kj it kk b kl nk ju kn ko nl jx kq kr nm kt ku kv nn kx ky kz no lb lc ld im bi translated">这里的实验使用网格搜索，并测试第一层的单元数、辍学率、优化器及其学习率的超参数的所有可能组合，精确度用于精确度。</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="0807" class="ny mt it nu b gy nz oa l ob oc"><strong class="nu iu">session_num = 0</strong></span><span id="c016" class="ny mt it nu b gy op oa l ob oc"><strong class="nu iu">for num_units in HP_NUM_UNITS.domain.values:<br/>  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):<br/>    for optimizer in HP_OPTIMIZER.domain.values:<br/>        for learning_rate in HP_LEARNING_RATE.domain.values:<br/>          hparams = {<br/>              HP_NUM_UNITS: num_units,<br/>              HP_DROPOUT: dropout_rate,<br/>              HP_OPTIMIZER: optimizer,<br/>              HP_LEARNING_RATE: learning_rate,<br/>          }<br/>          run_name = "run-%d" % session_num<br/>          print('--- Starting trial: %s' % run_name)<br/>          print({h.name: hparams[h] for h in hparams})<br/>          run('logs/hparam_tuning/' + run_name, hparams)<br/>          session_num += 1</strong></span></pre><h2 id="8bf7" class="ny mt it bd mu od oe dn my of og dp nc kr oh oi ne kv oj ok ng kz ol om ni on bi translated">HParams 仪表板中结果的可视化</h2><p id="a662" class="pw-post-body-paragraph ki kj it kk b kl nk ju kn ko nl jx kq kr nm kt ku kv nn kx ky kz no lb lc ld im bi translated">您可以使用不同的命令查看 HParams TensorBoard 仪表板:在 Jupyter notebook 中或使用 cmd</p><p id="388f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">使用 cmd </strong></p><p id="e9b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您将通过使用以下命令提供存储不同运行日志的目录路径来显示 Hparam 仪表板</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="fae9" class="ny mt it nu b gy nz oa l ob oc">python -m tensorboard.main --logdir="logs/hparam_tuning"</span></pre><p id="ac5a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当按降序对准确度排序时，可以看到优化最多的模型是 256 个单元，辍学率为 0.2，rmsprop 优化器的学习率为 0.0005。</p><figure class="np nq nr ns gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi oq"><img src="../Images/82aa42e406b6079a3a7732c7ed0cc5db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Skaid4UXa6BKfrtDeHwl1w.png"/></div></div></figure><p id="c973" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">使用 Jupyter 笔记本</strong></p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="186b" class="ny mt it nu b gy nz oa l ob oc">%tensorboard --logdir='\logs\hparam_tuning'</span></pre><figure class="np nq nr ns gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi ov"><img src="../Images/d9964bdc2127cc795ce1e36c31580652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M44n_ku38RSOqS3rYE9haA.png"/></div></div></figure><p id="9653" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您也可以查看<strong class="kk iu">平行坐标视图，显示每个超参数的单次运行</strong>并显示精确度</p><figure class="np nq nr ns gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi ow"><img src="../Images/043ea3b84838e62cd197de42a06f72b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3d4ilja_lFoM1SSjRWPcEA.png"/></div></div></figure><p id="71da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Tensorboard Hparams 仪表板有助于找到最佳的超参数，以获得最佳的模型精度</p><h2 id="56ef" class="ny mt it bd mu od oe dn my of og dp nc kr oh oi ne kv oj ok ng kz ol om ni on bi translated">结论:</h2><p id="2817" class="pw-post-body-paragraph ki kj it kk b kl nk ju kn ko nl jx kq kr nm kt ku kv nn kx ky kz no lb lc ld im bi translated">张量板超参数调整提供了一种直观的方式来了解哪些超参数可用于微调深度学习模型以获得最佳精度</p><h2 id="06b7" class="ny mt it bd mu od oe dn my of og dp nc kr oh oi ne kv oj ok ng kz ol om ni on bi translated">参考资料:</h2><p id="913c" class="pw-post-body-paragraph ki kj it kk b kl nk ju kn ko nl jx kq kr nm kt ku kv nn kx ky kz no lb lc ld im bi translated"><a class="ae le" href="https://github.com/tensorflow/tensorboard/issues/3688" rel="noopener ugc nofollow" target="_blank">https://github.com/tensorflow/tensorboard/issues/3688</a></p><p id="2db7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/tensor board/hyperparameter _ tuning _ with _ hparams</a></p></div></div>    
</body>
</html>