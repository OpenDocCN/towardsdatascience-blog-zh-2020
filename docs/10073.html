<html>
<head>
<title>How to Find the Best Predictors for ML Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何找到最大似然算法的最佳预测器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-find-the-best-predictors-for-ml-algorithms-4b28a71a8a80?source=collection_archive---------7-----------------------#2020-07-16">https://towardsdatascience.com/how-to-find-the-best-predictors-for-ml-algorithms-4b28a71a8a80?source=collection_archive---------7-----------------------#2020-07-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1aae" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解特征选择及其各种技术，以提高机器学习算法的预测能力</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d44a7c933d9db072e14a63156b3afb30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4-lgJ20FMZ_AfViFOqglg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://www.shutterstock.com/g/JrCasas" rel="noopener ugc nofollow" target="_blank"> JrCasas </a>在<a class="ae ky" href="http://www.shutterstock.com" rel="noopener ugc nofollow" target="_blank"> Shutterstock </a>拍摄</p></figure><p id="31fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以现在，你已经清理了你的<a class="ae ky" rel="noopener" target="_blank" href="/practical-guide-to-data-cleaning-in-python-f5334320e8e">数据</a>，并准备好将其输入机器学习(ML)算法。但是，如果您有大量的输入要素呢？对于模型学习来说，它们都足够有用和有预测性吗？如果你使用了所有的特性，是否会使你的模型缺乏解释力？拥有大量预测器还可能会增加开发和模型训练时间，同时会占用大量系统内存。</p><p id="35bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征选择技术旨在为模型训练系统地选择输入特征的最佳子集，以预测目标变量。不要将特征选择与其他降维技术(如 PCA 或使用信息论)相混淆。特征选择技术不修改预测器的原始语义，而只是选择它们的一个子集，从而产生一个更易解释的模型。</p><p id="c1b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Sayes 等人认为:“特征选择的目标是多方面的，其中最重要的是:</p><ol class=""><li id="da46" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">为了<strong class="lb iu">避免过拟合</strong>和<strong class="lb iu">提高模型性能</strong>，即在监督分类情况下的预测性能和在聚类情况下的更好的聚类检测，</li><li id="97b3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">提供<strong class="lb iu">更快</strong>和更多<strong class="lb iu">性价比高的车型</strong>，以及</li><li id="ecd4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">更深入地了解</strong>生成数据的底层流程。"</li></ol><p id="4fe1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然特征选择适用于有监督的和无监督的学习，但我在这里将重点放在有监督的特征选择上，其中目标变量是预先已知的。</p><p id="e19c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在监督特征选择的环境中，我们可以定义三大类别:</p><ul class=""><li id="03a4" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu mj mb mc md bi translated">嵌入式方法:有时也称为内在方法，这些技术内置于分类或回归模型的模型训练阶段。例子包括惩罚回归模型(例如，Lasso)和基于树的模型。因为这样的嵌入方法是模型训练的一个组成部分，所以我们不会在这篇文章中讨论这些。</li><li id="fc45" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu mj mb mc md bi translated">过滤方法:这些技术着眼于要素的内在属性，并使用统计技术来评估预测值和目标变量之间的关系。然后，最佳排序或评分特征的子集被用于模型训练。在这篇文章中，我们将回顾这些不同的统计技术。</li><li id="5304" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu mj mb mc md bi translated">包装器方法:这些方法利用 ML 算法作为特性评估过程的一部分，根据特定的性能指标迭代地识别和选择最佳的特性子集。例如，scikit-learn 的<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank">递归特征消除(RFE) </a>类，我们将在本文稍后介绍。</li></ul></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="102c" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">过滤特征选择方法</h1><p id="8180" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在选择输入特征的最佳子集之前，过滤器特征选择技术利用各种统计测试和测量来推断输入和目标变量之间的关系。</p><p id="77e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，这些统计测量是单变量的，因此，没有考虑各种输入特征对之间的相关性。首先从相关矩阵开始剔除冗余的相关特征可能是个好主意。或者，可以利用特定的高级多元滤波器技术，例如，基于相关性的特征选择(CFS)、马尔可夫毯式滤波器(MBF)和快速基于相关性的特征选择(FCBF)。然而，目前还没有得到广泛使用和支持的 python 包支持这些高级多元要素选择技术。</p><h2 id="ba12" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">分类特征选择</h2><p id="b99b" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">可用于分类问题中的分类特征的两种常规统计特征技术是:</p><ol class=""><li id="411c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">独立性卡方检验</li><li id="91b1" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">交互信息</li></ol><p id="8a72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">卡方检验用于确定两个分类变量之间的关系或依赖程度，在我们的示例中，一个是分类输入要素，另一个是分类目标变量。</p><p id="9f04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在概率论和信息论中，两个随机变量的互信息度量它们之间的独立程度。更高的值意味着更高的依赖性。</p><p id="2c51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于编码为整数的分类特征(例如，通过<code class="fe oa ob oc od b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html" rel="noopener ugc nofollow" target="_blank">OrdinalEncoder</a></code>)，scikit-learn 的<code class="fe oa ob oc od b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html" rel="noopener ugc nofollow" target="_blank">SelectKBest</a></code>类与<code class="fe oa ob oc od b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html" rel="noopener ugc nofollow" target="_blank">chi2</a></code>或<code class="fe oa ob oc od b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html" rel="noopener ugc nofollow" target="_blank">mutual_info_classif</a></code>函数结合使用，以识别和选择前<em class="oe"> k </em>个最相关的特征。<code class="fe oa ob oc od b">SelectKBest</code>返回的分数越高，该特征的预测能力就越强。入围的特征然后被馈送到用于模型开发的 ML 算法中。</p><p id="e6dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显示<code class="fe oa ob oc od b">chi2</code>和<code class="fe oa ob oc od b">mutual_info_classif</code>的完整工作示例如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="4278" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意<code class="fe oa ob oc od b">SelectKBest</code>也可以作为<code class="fe oa ob oc od b">Pipeline</code>的一部分用于交叉验证。然而，根据输入特性的基数，当模型在测试集中遇到训练集中没有的新值，因此没有用于编码时，在<code class="fe oa ob oc od b">Pipeline</code>中使用<code class="fe oa ob oc od b">OrdinalEncoder</code>有时是不可行的。在这种情况下，一个解决方法是使用不同的<code class="fe oa ob oc od b">test_size</code>和<code class="fe oa ob oc od b">random_state</code>运行<code class="fe oa ob oc od b">train_test_split</code>多次，然后取所选验证指标的平均值。</p><p id="e901" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于尚未进行数字编码的<code class="fe oa ob oc od b">object</code>类型分类特征(可能因为它们将被<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" rel="noopener ugc nofollow" target="_blank">一次性编码</a>或在稍后阶段转换为<a class="ae ky" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html" rel="noopener ugc nofollow" target="_blank">虚拟变量</a>，SciPy 的<code class="fe oa ob oc od b"><a class="ae ky" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html" rel="noopener ugc nofollow" target="_blank">chi2_contingency</a></code>类用于计算每对分类特征和目标变量的 chi 统计量和 p 值。然后，p 值最低的要素将被列入建模候选名单。</p><p id="7216" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个完整的工作示例如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="1595" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">数字特征选择——分类问题</h2><p id="d459" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">可用于分类问题中的数字特征的两种常规统计特征技术是:</p><ol class=""><li id="cbc5" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">方差分析 F 统计量</li><li id="ad67" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">相互信息(如上所述)</li></ol><p id="8aa0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">方差分析(ANOVA) F 统计量计算两个或多个数据样本均值的方差比率。数字输入特征和分类目标特征之间的比率越高，两者之间的独立性越低，越可能对模型训练有用。参见<a class="ae ky" href="https://statisticsbyjim.com/anova/f-tests-anova/" rel="noopener ugc nofollow" target="_blank">这篇</a>优秀文章，了解 ANOVA F 统计的详细信息。</p><p id="8358" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过 sci-kit learn 的<code class="fe oa ob oc od b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html" rel="noopener ugc nofollow" target="_blank">f_classif</a></code>函数，用于特征选择的 ANOVA F-statistic 在 Python 中类似地实现为独立性的卡方检验。假设我们这次处理的是数字特性，我们可以通过管道轻松实现交叉验证，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="a8cd" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">数字特征选择—回归问题</h2><p id="fbc3" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">啊，这是最简单的特征选择类型:为一个数字目标变量识别最合适的数字输入特征(回归问题)。可能的技术包括:</p><ol class=""><li id="5d25" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">相关统计</li><li id="ab2f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">相互信息(与前面解释的相同，只是使用了不同的<code class="fe oa ob oc od b">score_func</code>:<code class="fe oa ob oc od b">mutual_info_regression</code>)</li></ol><p id="06e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相关性衡量一个变量因另一个变量而发生变化的程度。也许最著名的相关性度量是假设数据为高斯分布的皮尔逊相关性。相关系数的范围在-1(完全负相关)和 1(完全正相关)之间，0 表示变量之间没有任何关系。</p><p id="5631" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了绘制相关矩阵之外，还可以使用 sci-kit learn 的<code class="fe oa ob oc od b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html" rel="noopener ugc nofollow" target="_blank">f_regression</a></code>函数和<code class="fe oa ob oc od b">SelectKBest</code>类，实现基于相关性的自动特征选择，类似于独立性卡方检验或 ANOVA F 统计。此外，就像 ANOVA F-statistic 函数一样，我们可以通过管道轻松实现交叉验证，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="da77" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">包装特征选择</h1><p id="81ec" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">来自 scikit-learn 的<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank">递归特征消除</a> (RFE)是实践中最广泛使用的包装器特征选择方法。RFE 是特征类型不可知的，它通过给定的监督学习模型(估计器)迭代地选择最佳数量的特征。</p><p id="f6eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来自 scikit-learn 文档:“首先，在初始特征集上训练估计器，并且通过<code class="fe oa ob oc od b">coef_</code>属性或<code class="fe oa ob oc od b">feature_importances_</code>属性获得每个特征的重要性。然后，从当前特征集中删除最不重要的特征。该过程在删减集上递归重复，直到最终达到要选择的特征的期望数量。”</p><p id="f9c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，RFE 只能与具有<code class="fe oa ob oc od b">coef_</code>或<code class="fe oa ob oc od b">feature_importances_</code>属性的算法一起使用来评估特征重要性。由于 RFE 是一种包装器特征选择技术，它可以使用任何给定的估计器(可以不同于应用于所选特征的算法)来识别最合适的特征。</p><p id="709b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假定<code class="fe oa ob oc od b">RFE</code>可用于分类和数字特征；在分类和回归问题中，它的实现是相似的。</p><p id="5f02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要调谐的<code class="fe oa ob oc od b">RFE</code>的主要参数是<code class="fe oa ob oc od b">n_features_to_select</code>。使用<code class="fe oa ob oc od b">DecisionTreeClassifier</code>进行特征选择和分类算法，基于<code class="fe oa ob oc od b">RFE</code>确定最佳特征数量的实际演示如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="be84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的代码片段将打印出<code class="fe oa ob oc od b">RFE</code>中使用的每个特性的平均和标准差，如下所示:</p><pre class="kj kk kl km gt oh od oi oj aw ok bi"><span id="f115" class="no ms it od b gy ol om l on oo">&gt;2: 0.710<br/>&gt;3: 0.815<br/>&gt;4: 0.872<br/>&gt;5: 0.884<br/>&gt;6: 0.891<br/>&gt;7: 0.888<br/>&gt;8: 0.888<br/>&gt;9: 0.884</span></pre><p id="3cb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">纯粹基于准确度分数，选择六个特征在这种情况下似乎是理想的。</p><h2 id="f7b0" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">自动特征选择以及对 RFE 多个模型的评估</h2><p id="da01" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">然而，我们怎么知道<code class="fe oa ob oc od b">DecisionTreeClassifier</code>是 RFE 使用的最佳算法呢？如果我们想用 RFE 评估各种算法呢？来帮助我们了。</p><p id="d30b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe oa ob oc od b">RFECV</code>的主要目的是通过自动交叉验证选择最佳数量的特征。由于理想数量的特性将由<code class="fe oa ob oc od b">RFECV</code>自动选择(因此，在最后的代码片段中不需要<code class="fe oa ob oc od b">for</code>循环)，我们可以有效地训练和评估用于<code class="fe oa ob oc od b">RFECV</code>特性选择的多个模型。然后，可以使用具有最佳验证度量的模型向前进行预测。</p><p id="f975" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们看一个实际的例子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="5621" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的代码片段将打印出<code class="fe oa ob oc od b">RFECV</code>中使用的每个模型的准确性得分的平均值和标准值，如下所示:</p><pre class="kj kk kl km gt oh od oi oj aw ok bi"><span id="9388" class="no ms it od b gy ol om l on oo">&gt;LR: 0.891<br/>&gt;DT: 0.882<br/>&gt;RF: 0.888<br/>&gt;XGB: 0.886</span></pre><p id="9ea7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单的逻辑回归模型赢得了包装特征选择。然后我们可以用这个来做预测:</p><pre class="kj kk kl km gt oh od oi oj aw ok bi"><span id="6803" class="no ms it od b gy ol om l on oo"># create pipeline<br/>rfecv = RFECV(estimator = LogisticRegression(), cv = 10, scoring = <br/>    'accuracy')<br/>model = DecisionTreeClassifier()<br/>pipeline = Pipeline(steps=[('features', rfecv), ('model', model)])</span><span id="f53a" class="no ms it od b gy op om l on oo"># fit the model on all available data<br/>pipeline.fit(X, y)</span><span id="4f0e" class="no ms it od b gy op om l on oo"># make a prediction for one example<br/>data = #load or define any new data unseen data that you want to make predictions upon</span><span id="a58a" class="no ms it od b gy op om l on oo">yhat = pipeline.predict(data)<br/>print('Predicted: %.3f' % (yhat))</span></pre></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="e9bd" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">额外小费</h1><p id="a3b5" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">大多数 ML 算法都有一个内置属性，用于查看模型学习过程中使用的特征的相对重要性。一些例子包括:</p><ul class=""><li id="a4e5" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu mj mb mc md bi translated">线性回归:<code class="fe oa ob oc od b">coef_</code>给出系数列表。系数越高，输入特征对目标变量的影响越大</li><li id="4d15" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu mj mb mc md bi translated">逻辑回归:<code class="fe oa ob oc od b">coef_</code>提供系数列表，解释与线性回归相同</li><li id="bb1a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu mj mb mc md bi translated">决策树和随机森林:<code class="fe oa ob oc od b">feature_importanes_</code> —越高越好</li></ul></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="5e6a" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">结论</h1><p id="47c2" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">这次我就这样了。如果您想讨论任何与数据分析、传统机器学习或信用分析相关的问题，请随时联系我。</p><p id="3e42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下次见，摇滚起来！</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="07e4" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">参考</h1><p id="2e12" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">受<a class="ae ky" href="http://www.machinelearningmastery.com" rel="noopener ugc nofollow" target="_blank">机器学习大师</a>的杰森·布朗利博士的启发</p><p id="9257" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] Saeys Y，Inaki I，Larranaga P .生物信息学中的特征选择<br/>技术综述。生物信息学。2007;23(19): 2507- <br/> 2517。</p></div></div>    
</body>
</html>