<html>
<head>
<title>Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于自然语言处理的文本分类:Tf-Idf vs Word2Vec vs BERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794?source=collection_archive---------0-----------------------#2020-07-18">https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794?source=collection_archive---------0-----------------------#2020-07-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bf7a59d41b19f964ce71b34753c515bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgqxDMP5qD1HE_uM33zZrg.png"/></div></div></figure><div class=""/><div class=""><h2 id="bd0f" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">预处理、模型设计、评估、词袋的可解释性、词嵌入、语言模型</h2></div><h2 id="8f03" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">摘要</h2><p id="84c0" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">在本文中，我将使用 NLP 和 Python 解释文本多类分类的 3 种不同策略:老式的<em class="mi">单词袋</em>(使用 Tf-Idf ) <em class="mi">，</em>著名的<em class="mi">单词嵌入(</em>使用 Word2Vec)，以及尖端的<em class="mi">语言模型</em>(使用 BERT)。</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mj"><img src="../Images/5c45d4e86f3251ebafa88ec4ce280e91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*T8WWibd7u8b7gfgeG0LgAA.gif"/></div></div></figure><p id="d857" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated"><a class="ae mt" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"><strong class="lr jf">【NLP(自然语言处理)</strong> </a>是人工智能领域，研究计算机与人类语言之间的交互，特别是如何给计算机编程，以处理和分析大量自然语言数据。NLP 通常用于文本数据的分类。<strong class="lr jf">文本分类</strong>就是根据文本数据的内容给文本数据分配类别的问题。</p><p id="deee" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">有不同的技术可以从原始文本数据中提取信息，并使用它来训练分类模型。本教程比较了老派的<em class="mi">单词袋</em>(与简单的机器学习算法一起使用)、流行的<em class="mi">单词嵌入</em>模型(与深度学习神经网络一起使用)和最先进的<em class="mi">语言模型</em>(与基于注意力的变形金刚的转移学习一起使用)，这些模型已经彻底改变了 NLP 的前景。</p><p id="d290" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我将展示一些有用的 Python 代码，这些代码可以很容易地应用于其他类似的情况(只需复制、粘贴、运行)，并通过注释遍历每一行代码，以便您可以复制这个示例(下面是完整代码的链接)。</p><div class="is it gp gr iu mu"><a href="https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jf gy z fp mz fr fs na fu fw jd bi translated">mdipietro 09/data science _ 人工智能 _ 实用工具</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">github.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni ja mu"/></div></div></a></div><p id="27b4" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我将使用“<strong class="lr jf">新闻类别数据集</strong>”，其中为您提供了从<em class="mi">赫芬顿邮报</em>获得的 2012 年至 2018 年的新闻标题，并要求您将它们分类到正确的类别，因此这是一个多类别分类问题(下面的链接)。</p><div class="is it gp gr iu mu"><a href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jf gy z fp mz fr fs na fu fw jd bi translated">新闻类别数据集</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">根据标题和简短描述识别新闻的类型</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">www.kaggle.com</p></div></div><div class="nd l"><div class="nj l nf ng nh nd ni ja mu"/></div></div></a></div><p id="841e" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">特别是，我将经历:</p><ul class=""><li id="4920" class="nk nl je lr b ls mo lv mp lc nm lg nn lk no mh np nq nr ns bi translated">设置:导入包、读取数据、预处理、分区。</li><li id="e2a3" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">用<em class="mi"> scikit-learn </em>进行特征工程&amp;特征选择&amp;机器学习，用<em class="mi"> lime </em>进行测试&amp;评估，可解释性。</li><li id="4c08" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">单词嵌入:用<em class="mi"> gensim </em>拟合一个 Word2Vec，用<em class="mi"> tensorflow/keras </em>进行特征工程&amp;深度学习，用注意力机制测试&amp;评价，可解释性。</li><li id="afd8" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">语言模型:用<em class="mi">变形金刚</em>进行特征工程，用<em class="mi">变形金刚</em>和<em class="mi">tensor flow/keras</em>测试&amp;评估向预先训练好的 BERT 转移学习。</li></ul></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h2 id="ec62" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">设置</h2><p id="eafa" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">首先，我需要导入以下库:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="a49d" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## for data<br/></strong>import <strong class="og jf">json<br/></strong>import <strong class="og jf">pandas </strong>as pd<br/>import <strong class="og jf">numpy </strong>as np</span><span id="9c21" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for plotting</strong><br/>import <strong class="og jf">matplotlib</strong>.pyplot as plt<br/>import <strong class="og jf">seaborn </strong>as sns</span><span id="a39f" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for processing<br/></strong>import <strong class="og jf">re</strong><br/>import <strong class="og jf">nltk</strong></span><span id="10de" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for bag-of-words</strong><br/>from <strong class="og jf">sklearn </strong>import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing</span><span id="2f48" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for explainer</strong><br/>from <strong class="og jf">lime </strong>import lime_text</span><span id="80df" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for word embedding</strong><br/>import <strong class="og jf">gensim<br/></strong>import gensim.downloader as gensim_api</span><span id="7b35" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for deep learning</strong><br/>from <strong class="og jf">tensorflow</strong>.keras import models, layers, preprocessing as kprocessing<br/>from tensorflow.keras import backend as K</span><span id="73bd" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for bert language model</strong><br/>import <strong class="og jf">transformers</strong></span></pre><p id="9c06" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">数据集包含在一个 json 文件中，所以我将首先用<em class="mi"> json </em>把它读入一个字典列表，然后把它转换成一个<em class="mi"> pandas </em> Dataframe。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="7f3b" class="kt ku je og b gy ok ol l om on">lst_dics = []<br/>with <strong class="og jf">open</strong>('data.json', mode='r', errors='ignore') as json_file:<br/>    for dic in json_file:<br/>        lst_dics.append( json<strong class="og jf">.loads</strong>(dic) )</span><span id="4125" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## print the first one</strong><br/>lst_dics[0]</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi op"><img src="../Images/abad2559dc8105b8f4fda79c7a163672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N7xAYy2MBRJHKMBXnxMi0A.png"/></div></div></figure><p id="6721" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">原始数据集包含超过 30 个类别，但是出于本教程的目的，我将使用 3 个类别的子集:娱乐、政治和技术。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="b876" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## create dtf</strong><br/>dtf = pd.DataFrame(lst_dics)</span><span id="09fd" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## filter categories</strong><br/>dtf = dtf[ dtf["category"].isin(['<strong class="og jf">ENTERTAINMENT</strong>','<strong class="og jf">POLITICS</strong>','<strong class="og jf">TECH</strong>']) ][["category","headline"]]</span><span id="0cbe" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## rename columns</strong><br/>dtf = dtf.rename(columns={"category":"<strong class="og jf">y</strong>", "headline":"<strong class="og jf">text</strong>"})</span><span id="da4f" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## print 5 random rows</strong><br/>dtf.sample(5)</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/dadf349b489276b644508140d4375a2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iurA976CkC9i1Yi1L6hIIw.png"/></div></div></figure><p id="c391" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">为了理解数据集的组成，我将通过用条形图显示标签频率来查看目标的<strong class="lr jf">单变量分布</strong>。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="4dac" class="kt ku je og b gy ok ol l om on">fig, ax = plt.subplots()<br/>fig.suptitle(<strong class="og jf">"y"</strong>, fontsize=12)<br/>dtf[<strong class="og jf">"y"</strong>].reset_index().groupby(<strong class="og jf">"y"</strong>).count().sort_values(by= <br/>       "index").plot(kind="barh", legend=False, <br/>        ax=ax).grid(axis='x')<br/>plt.show()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/c34aa02e00de5b86b9f7cc6ab2d210c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b7hN7kENZzF4wsck1ne0QA.png"/></div></div></figure><p id="a1d5" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">数据集是不平衡的:与其他相比，科技新闻的比例非常小，这将使模型识别科技新闻相当困难。</p><p id="8362" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">在解释和构建模型之前，我将给出一个预处理的例子，通过清理文本、删除停用词和应用词汇化。我将编写一个函数，并将其应用于整个数据集。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="8b11" class="kt ku je og b gy ok ol l om on"><strong class="og jf">'''<br/>Preprocess a string.<br/>:parameter<br/>    :param text: string - name of column containing text<br/>    :param lst_stopwords: list - list of stopwords to remove<br/>    :param flg_stemm: bool - whether stemming is to be applied<br/>    :param flg_lemm: bool - whether lemmitisation is to be applied<br/>:return<br/>    cleaned text<br/>'''</strong><br/>def <strong class="og jf">utils_preprocess_text</strong>(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):<br/>    <strong class="og jf">## clean (convert to lowercase and remove punctuations and   <br/>    characters and then strip)</strong><br/>    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())<br/>            <br/>    <strong class="og jf">## Tokenize (convert from string to list)</strong><br/>    lst_text = text.split()</span><span id="c171" class="kt ku je og b gy oo ol l om on"><strong class="og jf">    ## remove Stopwords</strong><br/>    if lst_stopwords is not None:<br/>        lst_text = [word for word in lst_text if word not in <br/>                    lst_stopwords]<br/>                <br/>    <strong class="og jf">## Stemming (remove -ing, -ly, ...)</strong><br/>    if flg_stemm == True:<br/>        ps = nltk.stem.porter.PorterStemmer()<br/>        lst_text = [ps.stem(word) for word in lst_text]<br/>                <br/>    <strong class="og jf">## Lemmatisation (convert the word into root word)</strong><br/>    if flg_lemm == True:<br/>        lem = nltk.stem.wordnet.WordNetLemmatizer()<br/>        lst_text = [lem.lemmatize(word) for word in lst_text]<br/>            <br/>    <strong class="og jf">## back to string from list</strong><br/>    text = " ".join(lst_text)<br/>    return text</span></pre><p id="3136" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">该函数从语料库中删除一组给定的单词。我可以用<em class="mi"> nltk </em>为英语词汇创建一个通用停用词列表(我们可以通过添加或删除单词来编辑这个列表)。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="72e4" class="kt ku je og b gy ok ol l om on">lst_stopwords = <strong class="og jf">nltk</strong>.corpus.stopwords.words("<strong class="og jf">english</strong>")<br/>lst_stopwords</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/b10919fcef40946289f0de22bb3a3d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k1fsJU_S0_WPZku6gg-qOQ.png"/></div></div></figure><p id="482f" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">现在，我将对整个数据集应用我编写的函数，并将结果存储在一个名为“<em class="mi"> text_clean </em>的新列中，以便您可以选择处理原始语料库或预处理文本。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="8fe2" class="kt ku je og b gy ok ol l om on">dtf["<strong class="og jf">text_clean</strong>"] = dtf["text"].apply(lambda x: <br/>          <strong class="og jf">utils_preprocess_text</strong>(x, flg_stemm=False, <strong class="og jf">flg_lemm=True</strong>, <br/>          <strong class="og jf">lst_stopwords=lst_stopwords</strong>))</span><span id="4fdd" class="kt ku je og b gy oo ol l om on">dtf.head()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/06aef09197a0128cee33e71e25cf5cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-R6djtHnK4cBVqFrjfLgA.png"/></div></div></figure><p id="d7f8" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">如果你对更深入的文本分析和预处理感兴趣，可以查看<a class="ae mt" rel="noopener" target="_blank" href="/text-analysis-feature-engineering-with-nlp-502d6ea9225d">这篇文章</a>。考虑到这一点，我将把数据集划分为训练集(70%)和测试集(30%)，以便评估模型的性能。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="00f2" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## split dataset</strong><br/>dtf_train, dtf_test = model_selection.<strong class="og jf">train_test_split</strong>(dtf, test_size=0.3)</span><span id="4b93" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## get target</strong><br/>y_train = dtf_train[<strong class="og jf">"y"</strong>].values<br/>y_test = dtf_test[<strong class="og jf">"y"</strong>].values</span></pre><p id="e70d" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我们开始吧，好吗？</p><h2 id="a81a" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">词汇袋</h2><p id="96c7" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated"><a class="ae mt" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> <em class="mi">单词袋</em> </a>模型很简单:它从一个文档语料库中构建一个词汇表，并统计这些单词在每个文档中出现的次数。换句话说，词汇表中的每个单词都成为一个特征，一个文档由一个具有相同词汇表长度的向量(一个“单词包”)来表示。例如，让我们用这种方法来表达 3 个句子:</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/06f565b9e55d35887acec308928b263f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m1O25pvl8R5DlkhuJjRrDw.png"/></div></div><p class="ov ow gj gh gi ox oy bd b be z dk translated">特征矩阵形状:<strong class="bd oz"> <em class="pa">文档数量</em></strong><em class="pa"/>x<em class="pa"/><strong class="bd oz"><em class="pa">词汇长度</em> </strong></p></figure><p id="f309" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">可以想象，这种方法会导致一个显著的维数问题:文档越多，词汇表就越大，因此特征矩阵将是一个巨大的稀疏矩阵。因此，在单词袋模型之前通常要进行重要的预处理(单词清理、停用词去除、词干化/词条化)，目的是减少维数问题。</p><p id="1375" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">术语频率不一定是文本的最佳表示。事实上，你可以在语料库中找到出现频率最高但对目标变量几乎没有预测能力的常用词。为了解决这个问题，单词包有一个高级变体，它不是简单的计数，而是使用<strong class="lr jf">术语频率-逆文档频率</strong>(或 T<a class="ae mt" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">f-Idf</a>)<strong class="lr jf">。</strong>基本上，一个词的值随着 count 成正比增加，但是和这个词在语料库中的出现频率成反比。</p><p id="3524" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">让我们从<strong class="lr jf">特征工程开始，</strong>通过从数据中提取信息来创建特征的过程。我将使用限制为 10，000 个单词的 Tf-Idf 矢量器(因此我的词汇长度将为 10k)，捕获单字母词(即“<em class="mi"> new </em>”和“<em class="mi"> york </em>”)和双字母词(即“<em class="mi"> new york </em>”)。我还将提供经典计数矢量器的代码:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="98fe" class="kt ku je og b gy ok ol l om on"><strong class="og jf"><em class="mi">## Count (classic BoW)</em></strong><br/><em class="mi">vectorizer = feature_extraction.text.</em><strong class="og jf"><em class="mi">CountVectorizer</em></strong><em class="mi">(max_features=10000, </em>ngram_range=(1,2))<br/><br/><strong class="og jf"><em class="mi">## Tf-Idf (advanced variant of BoW)</em></strong><br/>vectorizer = feature_extraction.text.<strong class="og jf">TfidfVectorizer</strong>(max_features=10000, ngram_range=(1,2))</span></pre><p id="187b" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">现在，我将在训练集的预处理语料库上使用矢量器来提取词汇并创建特征矩阵。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="1f61" class="kt ku je og b gy ok ol l om on">corpus = dtf_train["<strong class="og jf">text_clean</strong>"]</span><span id="eb41" class="kt ku je og b gy oo ol l om on">vectorizer.fit(corpus)<br/>X_train = vectorizer.transform(corpus)<br/>dic_vocabulary = vectorizer.vocabulary_</span></pre><p id="0d8b" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">特征矩阵<em class="mi"> X_train </em>具有 34，265(训练中的文档数量)x 10，000(词汇长度)的形状，并且它非常稀疏:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="12f1" class="kt ku je og b gy ok ol l om on">sns.<strong class="og jf">heatmap</strong>(X_train.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/65b89a5de416006874c9be2bd115faac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CpZ9fxPY5iSEzgdyS021_Q.png"/></div></div><p class="ov ow gj gh gi ox oy bd b be z dk translated">来自特征矩阵的随机样本(黑色的非零值)</p></figure><p id="9df7" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">为了知道某个单词的位置，我们可以在词汇表中查找它:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="b534" class="kt ku je og b gy ok ol l om on">word = "new york"</span><span id="d66b" class="kt ku je og b gy oo ol l om on">dic_vocabulary[word]</span></pre><p id="931e" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">如果这个单词存在于词汇表中，这个命令打印一个数字<em class="mi"> N </em>，意味着矩阵的第<em class="mi"> N </em>个特征就是这个单词。</p><p id="18fd" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">为了删除一些列并降低矩阵维数，我们可以执行一些<strong class="lr jf">特征选择</strong>，即选择相关变量子集的过程。我将如下进行:</p><ol class=""><li id="d5ae" class="nk nl je lr b ls mo lv mp lc nm lg nn lk no mh pc nq nr ns bi translated">将每个类别视为二进制(例如，“技术”类别对于技术新闻为 1，对于其他类别为 0)；</li><li id="6d94" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh pc nq nr ns bi translated">执行<a class="ae mt" href="https://en.wikipedia.org/wiki/Chi-squared_test" rel="noopener ugc nofollow" target="_blank">卡方检验</a>以确定特征和(二元)目标是否独立；</li><li id="31b3" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh pc nq nr ns bi translated">仅保留卡方检验中具有特定 p 值的要素。</li></ol><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="3c2d" class="kt ku je og b gy ok ol l om on">y = dtf_train["<strong class="og jf">y</strong>"]<br/>X_names = vectorizer.get_feature_names()<br/>p_value_limit = 0.95</span><span id="70ae" class="kt ku je og b gy oo ol l om on">dtf_features = pd.DataFrame()<br/>for cat in np.unique(y):<br/>    chi2, p = feature_selection.<strong class="og jf">chi2</strong>(X_train, y==cat)<br/>    dtf_features = dtf_features.append(pd.DataFrame(<br/>                   {"feature":X_names, "score":1-p, "y":cat}))<br/>    dtf_features = dtf_features.sort_values(["y","score"], <br/>                    ascending=[True,False])<br/>    dtf_features = dtf_features[dtf_features["score"]&gt;p_value_limit]</span><span id="4aaa" class="kt ku je og b gy oo ol l om on">X_names = dtf_features["feature"].unique().tolist()</span></pre><p id="cf35" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我通过保留统计上最相关的特征，将特征的数量从 10，000 个减少到 3，152 个。让我们打印一些:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="9c3d" class="kt ku je og b gy ok ol l om on">for cat in np.unique(y):<br/>   print("# {}:".format(cat))<br/>   print("  . selected features:",<br/>         len(dtf_features[dtf_features["y"]==cat]))<br/>   print("  . top features:", ",".join(<br/>dtf_features[dtf_features["y"]==cat]["feature"].values[:10]))<br/>   print(" ")</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/3737a49e1c9eaf5d037252cc9b67c052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fo0EjcD4Ibo2Jz1y6eNL0A.png"/></div></div></figure><p id="1761" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我们可以通过给定这组新单词作为输入，在语料库上重新装配矢量器。这将产生更小的特征矩阵和更短的词汇表。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="9a1f" class="kt ku je og b gy ok ol l om on">vectorizer = feature_extraction.text.<strong class="og jf">TfidfVectorizer</strong>(vocabulary=X_names)</span><span id="123e" class="kt ku je og b gy oo ol l om on">vectorizer.fit(corpus)<br/>X_train = vectorizer.transform(corpus)<br/>dic_vocabulary = vectorizer.vocabulary_</span></pre><p id="d6e5" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">新的特征矩阵<em class="mi"> X_train </em>具有 is 34，265(训练中的文档数量)x 3，152(给定词汇的长度)的形状。让我们看看矩阵是否不那么稀疏:</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/f683d14f6fcee76d0f2e9a684c8a5f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O8lMt_obkHbMXuOSg1bTRA.png"/></div></div><p class="ov ow gj gh gi ox oy bd b be z dk translated">来自新特征矩阵的随机样本(黑色非零值)</p></figure><p id="3a83" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">是时候训练一个<strong class="lr jf">机器学习模型</strong>并测试它了。我推荐使用朴素贝叶斯算法:一种概率分类器，它利用了<a class="ae mt" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>，这是一种基于可能相关的条件的先验知识使用概率进行预测的规则。该算法最适合这种大型数据集，因为它独立考虑每个特征，计算每个类别的概率，然后预测概率最高的类别。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="a2de" class="kt ku je og b gy ok ol l om on">classifier = naive_bayes.<strong class="og jf">MultinomialNB</strong>()</span></pre><p id="4428" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我将在特征矩阵上训练这个分类器，然后在转换后的测试集上测试它。为此，我需要构建一个<em class="mi"> scikit-learn </em>管道:一个转换列表和一个最终估计器的顺序应用。将 Tf-Idf 矢量器和朴素贝叶斯分类器放在一个管道中，使我们能够在一个步骤中转换和预测测试数据。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="374d" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## pipeline</strong><br/>model = pipeline.<strong class="og jf">Pipeline</strong>([("<strong class="og jf">vectorizer</strong>", vectorizer),  <br/>                           ("<strong class="og jf">classifier</strong>", classifier)])</span><span id="8518" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## train classifier<br/></strong>model["classifier"].fit(X_train, y_train)</span><span id="f0ee" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## test<br/></strong>X_test = dtf_test["text_clean"].values<br/>predicted = model.predict(X_test)<br/>predicted_prob = model.predict_proba(X_test)</span></pre><p id="8016" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我们现在可以<strong class="lr jf">评估单词袋模型的性能</strong>，我将使用以下指标:</p><ul class=""><li id="97f9" class="nk nl je lr b ls mo lv mp lc nm lg nn lk no mh np nq nr ns bi translated">准确性:模型预测正确的比例。</li><li id="1828" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">混淆矩阵:按类别细分正确和错误预测数量的汇总表。</li><li id="e253" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">ROC:说明在不同阈值设置下真阳性率与假阳性率的图表。曲线下的面积(AUC)表示分类器将随机选择的阳性观察值排列为高于随机选择的阴性观察值的概率。</li><li id="c5f5" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">Precision:相关实例在检索到的实例中所占的比例。</li><li id="7682" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">Recall:实际检索到的相关实例总数的一部分。</li></ul><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="4c56" class="kt ku je og b gy ok ol l om on">classes = np.unique(y_test)<br/>y_test_array = pd.get_dummies(y_test, drop_first=False).values<br/>    </span><span id="52c1" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## Accuracy, Precision, Recall</strong><br/>accuracy = metrics.accuracy_score(y_test, predicted)<br/>auc = metrics.roc_auc_score(y_test, predicted_prob, <br/>                            multi_class="ovr")<br/>print("Accuracy:",  round(accuracy,2))<br/>print("Auc:", round(auc,2))<br/>print("Detail:")<br/>print(metrics.classification_report(y_test, predicted))<br/>    <br/><strong class="og jf">## Plot confusion matrix</strong><br/>cm = metrics.confusion_matrix(y_test, predicted)<br/>fig, ax = plt.subplots()<br/>sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, <br/>            cbar=False)<br/>ax.set(xlabel="Pred", ylabel="True", xticklabels=classes, <br/>       yticklabels=classes, title="Confusion matrix")<br/>plt.yticks(rotation=0)</span><span id="4b72" class="kt ku je og b gy oo ol l om on"><br/>fig, ax = plt.subplots(nrows=1, ncols=2)<br/><strong class="og jf">## Plot roc</strong><br/>for i in range(len(classes)):<br/>    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  <br/>                           predicted_prob[:,i])<br/>    ax[0].plot(fpr, tpr, lw=3, <br/>              label='{0} (area={1:0.2f})'.format(classes[i], <br/>                              metrics.auc(fpr, tpr))<br/>               )<br/>ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')<br/>ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], <br/>          xlabel='False Positive Rate', <br/>          ylabel="True Positive Rate (Recall)", <br/>          title="Receiver operating characteristic")<br/>ax[0].legend(loc="lower right")<br/>ax[0].grid(True)<br/>    <br/><strong class="og jf">## Plot precision-recall curve<br/></strong>for i in range(len(classes)):<br/>    precision, recall, thresholds = metrics.precision_recall_curve(<br/>                 y_test_array[:,i], predicted_prob[:,i])<br/>    ax[1].plot(recall, precision, lw=3, <br/>               label='{0} (area={1:0.2f})'.format(classes[i], <br/>                                  metrics.auc(recall, precision))<br/>              )<br/>ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', <br/>          ylabel="Precision", title="Precision-Recall curve")<br/>ax[1].legend(loc="best")<br/>ax[1].grid(True)<br/>plt.show()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/70cd46763c53895ba93d7c08839e0148.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iPL_8iJOuTJ_mrLvftwUEw.png"/></div></div></figure><p id="7722" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">BoW 模型对测试集的正确率为 85%(准确率为 0.85)，但很难识别科技新闻(只有 252 个预测正确)。</p><p id="9d9a" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">让我们试着理解为什么该模型将新闻归入某一类别，并评估这些预测的<strong class="lr jf">解释能力</strong>。lime 包可以帮助我们建立一个解释器。为了举例说明，我将从测试集中随机观察，看看模型预测了什么，为什么。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="31a1" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## select observation<br/></strong>i = 0<br/>txt_instance = dtf_test["<strong class="og jf">text</strong>"].iloc[i]</span><span id="346e" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## check true value and predicted value</strong><br/>print("True:", y_test[i], "--&gt; Pred:", predicted[i], "| Prob:", round(np.max(predicted_prob[i]),2))</span><span id="9694" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## show explanation</strong><br/>explainer = lime_text.<strong class="og jf">LimeTextExplainer</strong>(class_names=<br/>             np.unique(y_train))<br/>explained = explainer.explain_instance(txt_instance, <br/>             model.predict_proba, num_features=3)<br/>explained.show_in_notebook(text=txt_instance, predict_proba=False)</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/4a949622b01ba00b6a67c146d6bca429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*CFZTX1Ud0jOwNZMhrr4cWA.png"/></div></figure><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/aa1be6b6af7588a17b76ad91a4a06444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qcAV1wvucxNogDz_eh3dpQ.png"/></div></div></figure><p id="cf12" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">这是有道理的:单词“<em class="mi">克林顿</em>”和“<em class="mi">共和党</em>”为这个模型指出了正确的方向(政治新闻)，即使单词“<em class="mi">舞台</em>”在娱乐新闻中更常见。</p><h2 id="b371" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">单词嵌入</h2><p id="58f4" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated"><a class="ae mt" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank"> <em class="mi">单词嵌入</em> </a>是将词汇中的单词映射到实数向量的特征学习技术的统称。这些向量是根据出现在另一个单词之前或之后的每个单词的概率分布来计算的。换句话说，相同上下文的单词通常一起出现在语料库中，因此它们在向量空间中也将是接近的。例如，让我们看一下上一个例子中的三个句子:</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pi"><img src="../Images/742e8b09d7c5128e2edd6dd30217c1cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u67szEvNSMqrQeitdahw_A.png"/></div></div><p class="ov ow gj gh gi ox oy bd b be z dk translated">嵌入 2D 向量空间的词</p></figure><p id="efe4" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">在本教程中，我将使用这个家族的第一个模型:谷歌的<a class="ae mt" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"><em class="mi">word 2 vec</em></a><em class="mi"/>(2013)。其他流行的单词嵌入模型还有斯坦福的<a class="ae mt" href="https://en.wikipedia.org/wiki/GloVe_(machine_learning)" rel="noopener ugc nofollow" target="_blank"><em class="mi">GloVe</em></a><em class="mi"/>(2014)<em class="mi"/>和脸书的<a class="ae mt" href="https://en.wikipedia.org/wiki/FastText" rel="noopener ugc nofollow" target="_blank"><em class="mi">fast text</em></a>(2016)。</p><p id="b046" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated"><strong class="lr jf"> Word2Vec </strong>利用语料库中的每个唯一单词产生通常具有数百维的向量空间，使得语料库中共享共同上下文的单词在空间中彼此靠近。这可以使用两种不同的方法来完成:从单个单词开始预测其上下文(<em class="mi"> Skip-gram </em>)或者从上下文开始预测单词(<em class="mi">连续单词包</em>)。</p><p id="3453" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">在 Python 中，你可以像这样从<a class="ae mt" href="https://github.com/RaRe-Technologies/gensim-data" rel="noopener ugc nofollow" target="_blank"><em class="mi">genism-data</em></a><em class="mi"/>加载一个预先训练好的单词嵌入模型:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="fb2e" class="kt ku je og b gy ok ol l om on">nlp = gensim_api.load("<strong class="og jf">word2vec-google-news-300"</strong>)</span></pre><p id="3b3c" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我将使用<em class="mi"> gensim 在训练数据语料库上拟合我自己的 Word2Vec，而不是使用预先训练的模型。</em>在拟合模型之前，需要将语料库转换成 n 元文法列表的列表。在这个特殊的例子中，我将尝试捕获单字母("<em class="mi"> york </em>")、双字母("<em class="mi"> new york </em>")和三字母("<em class="mi"> new york city </em>")。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="eabd" class="kt ku je og b gy ok ol l om on">corpus = dtf_train["<strong class="og jf">text_clean</strong>"]</span><span id="c998" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><br/>## create list of lists of unigrams</strong><br/>lst_corpus = []<br/>for string in corpus:<br/>   lst_words = string.split()<br/>   lst_grams = [" ".join(lst_words[i:i+1]) <br/>               for i in range(0, len(lst_words), 1)]<br/>   lst_corpus.append(lst_grams)</span><span id="ade3" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><br/>## detect bigrams and trigrams</strong><br/>bigrams_detector = gensim.models.phrases.<strong class="og jf">Phrases</strong>(lst_corpus, <br/>                 delimiter=" ".encode(), min_count=5, threshold=10)<br/>bigrams_detector = gensim.models.phrases.<strong class="og jf">Phraser</strong>(bigrams_detector)</span><span id="3024" class="kt ku je og b gy oo ol l om on">trigrams_detector = gensim.models.phrases.<strong class="og jf">Phrases</strong>(bigrams_detector[lst_corpus], <br/>            delimiter=" ".encode(), min_count=5, threshold=10)<br/>trigrams_detector = gensim.models.phrases.<strong class="og jf">Phraser</strong>(trigrams_detector)</span></pre><p id="c423" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">安装 Word2Vec 时，需要指定:</p><ul class=""><li id="8700" class="nk nl je lr b ls mo lv mp lc nm lg nn lk no mh np nq nr ns bi translated">单词向量的目标大小，我用 300；</li><li id="f753" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">窗口，或句子中当前单词和预测单词之间的最大距离，我将使用语料库中文本的平均长度；</li><li id="e0a7" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">训练算法，我将使用 skip-grams (sg=1 ),因为通常它有更好的结果。</li></ul><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="a5f2" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## fit w2v</strong><br/>nlp = gensim.models.word2vec.<strong class="og jf">Word2Vec</strong>(lst_corpus, size=300,   <br/>            window=8, min_count=1, sg=1, iter=30)</span></pre><p id="9959" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我们有自己的嵌入模型，所以我们可以从语料库中选择任何单词，并将其转换为向量。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="c003" class="kt ku je og b gy ok ol l om on">word = "data"<br/>nlp[word].shape</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/f77b6fe4c5b08cb3fada7872b0b3589e.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*nMBpatACacNe1SZYF39j3Q.png"/></div></figure><p id="d20c" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我们甚至可以通过应用任何降维算法(即<a class="ae mt" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank"> TSNE </a>)来使用它将一个单词及其上下文可视化到一个更小的维度空间(2D 或 3D)中。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="d8d1" class="kt ku je og b gy ok ol l om on">word = "data"<br/>fig = plt.figure()</span><span id="408e" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## word embedding</strong><br/>tot_words = [word] + [tupla[0] for tupla in <br/>                 nlp.most_similar(word, topn=20)]<br/>X = nlp[tot_words]</span><span id="a680" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## pca to reduce dimensionality from 300 to 3</strong><br/>pca = manifold.<strong class="og jf">TSNE</strong>(perplexity=40, n_components=3, init='pca')<br/>X = pca.fit_transform(X)</span><span id="c7cd" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## create dtf</strong><br/>dtf_ = pd.DataFrame(X, index=tot_words, columns=["x","y","z"])<br/>dtf_["input"] = 0<br/>dtf_["input"].iloc[0:1] = 1</span><span id="618e" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## plot 3d</strong><br/>from mpl_toolkits.mplot3d import Axes3D<br/>ax = fig.add_subplot(111, projection='3d')<br/>ax.scatter(dtf_[dtf_["input"]==0]['x'], <br/>           dtf_[dtf_["input"]==0]['y'], <br/>           dtf_[dtf_["input"]==0]['z'], c="black")<br/>ax.scatter(dtf_[dtf_["input"]==1]['x'], <br/>           dtf_[dtf_["input"]==1]['y'], <br/>           dtf_[dtf_["input"]==1]['z'], c="red")<br/>ax.set(xlabel=None, ylabel=None, zlabel=None, xticklabels=[], <br/>       yticklabels=[], zticklabels=[])<br/>for label, row in dtf_[["x","y","z"]].iterrows():<br/>    x, y, z = row<br/>    ax.text(x, y, z, s=label)</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mj"><img src="../Images/5c45d4e86f3251ebafa88ec4ce280e91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*T8WWibd7u8b7gfgeG0LgAA.gif"/></div></div></figure><p id="2637" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">这很酷，但是嵌入这个词怎么能用来预测新闻类别呢？嗯，单词向量可以在神经网络中用作权重。这是怎么回事:</p><ul class=""><li id="6309" class="nk nl je lr b ls mo lv mp lc nm lg nn lk no mh np nq nr ns bi translated">首先，将语料库转换成单词 id 的填充序列，以获得特征矩阵。</li><li id="1700" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">然后，创建一个嵌入矩阵，使得 id 为<em class="mi"> N </em>的单词的向量位于第<em class="mi">N</em>行。</li><li id="2904" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">最后，构建一个具有嵌入层的神经网络，该嵌入层用相应的向量对序列中的每个单词进行加权。</li></ul><p id="7f89" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">让我们从<strong class="lr jf">特征工程</strong>开始，通过使用<em class="mi"> tensorflow/keras </em>将给予 Word2Vec 的相同预处理语料库(n 元语法列表的列表)转换成序列列表:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="17e9" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## tokenize text</strong><br/>tokenizer = kprocessing.text.<strong class="og jf">Tokenizer</strong>(lower=True, split=' ', <br/>                     oov_token="NaN", <br/>                     filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n')<br/>tokenizer.fit_on_texts(lst_corpus)<br/>dic_vocabulary = tokenizer.word_index</span><span id="2793" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## create sequence</strong><br/>lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)</span><span id="495c" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## padding sequence</strong><br/>X_train = kprocessing.sequence.<strong class="og jf">pad_sequences</strong>(lst_text2seq, <br/>                    maxlen=15, padding="post", truncating="post")</span></pre><p id="5ade" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">特征矩阵<em class="mi"> X_train </em>的形状为 34，265 x 15(序列数 X 序列最大长度)。让我们想象一下:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="4fcd" class="kt ku je og b gy ok ol l om on">sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)<br/>plt.show()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/f9d1b04f3d2703846bfab65dde0abfb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tQ588zm4i96xfEBCfiUpzQ.png"/></div></div><p class="ov ow gj gh gi ox oy bd b be z dk translated">特征矩阵(34，265 x 15)</p></figure><p id="23eb" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">语料库中的每个文本现在都是长度为 15 的 id 序列。例如，如果文本中有 10 个标记，那么序列由 10 个 id+5 个 0 组成，这是填充元素(而不在词汇表中的单词的 id 是 1)。让我们打印一个来自训练集的文本如何被转换成一个带有填充和词汇的序列。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="8b50" class="kt ku je og b gy ok ol l om on">i = 0<br/><br/><strong class="og jf">## list of text: ["I like this", ...]</strong><br/>len_txt = len(dtf_train["text_clean"].iloc[i].split())<br/>print("from: ", dtf_train["text_clean"].iloc[i], "| len:", len_txt)<br/><br/><strong class="og jf">## sequence of token ids: [[1, 2, 3], ...]</strong><br/>len_tokens = len(X_train[i])<br/>print("to: ", X_train[i], "| len:", len(X_train[i]))<br/><br/><strong class="og jf">## vocabulary: {"I":1, "like":2, "this":3, ...}</strong><br/>print("check: ", dtf_train["text_clean"].iloc[i].split()[0], <br/>      " -- idx in vocabulary --&gt;", <br/>      dic_vocabulary[dtf_train["text_clean"].iloc[i].split()[0]])<br/><br/>print("vocabulary: ", dict(list(dic_vocabulary.items())[0:5]), "... (padding element, 0)")</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/edd044da44c06749ed9f150e8015ef8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vhJWO3cTN2jfrg2upX2kGQ.png"/></div></div></figure><p id="a07a" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">在继续之前，不要忘记在测试集上做同样的特性工程:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="2ebc" class="kt ku je og b gy ok ol l om on">corpus = dtf_test["<strong class="og jf">text_clean</strong>"]</span><span id="220a" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><br/>## create list of n-grams</strong><br/>lst_corpus = []<br/>for string in corpus:<br/>    lst_words = string.split()<br/>    lst_grams = [" ".join(lst_words[i:i+1]) for i in range(0, <br/>                 len(lst_words), 1)]<br/>    lst_corpus.append(lst_grams)<br/>    </span><span id="791a" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## detect common bigrams and trigrams using the fitted detectors</strong><br/>lst_corpus = list(bigrams_detector[lst_corpus])<br/>lst_corpus = list(trigrams_detector[lst_corpus])<br/></span><span id="3d90" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## text to sequence with the fitted tokenizer</strong><br/>lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)</span><span id="51fd" class="kt ku je og b gy oo ol l om on"><br/><strong class="og jf">## padding sequence</strong><br/>X_test = kprocessing.sequence.<strong class="og jf">pad_sequences</strong>(lst_text2seq, maxlen=15,<br/>             padding="post", truncating="post")</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pm"><img src="../Images/30c51e6f881711789149c623f5126eed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6ZZAT-9tmu8PcaY74Un6w.png"/></div></div><p class="ov ow gj gh gi ox oy bd b be z dk translated">x _ 测试(14，697 x 15)</p></figure><p id="a0f8" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我们已经得到了我们的<em class="mi"> X_train </em>和<em class="mi"> X_test </em>，现在我们需要创建嵌入的<strong class="lr jf">矩阵，它将被用作神经网络分类器中的权重矩阵。</strong></p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="319f" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## start the matrix (length of vocabulary x vector size) with all 0s</strong><br/>embeddings = np.zeros((len(dic_vocabulary)+1, 300))</span><span id="b807" class="kt ku je og b gy oo ol l om on">for word,idx in dic_vocabulary.items():<br/>    <strong class="og jf">## update the row with vector</strong><br/>    try:<br/>        embeddings[idx] =  nlp[word]<br/>    <strong class="og jf">## if word not in model then skip and the row stays all 0s</strong><br/>    except:<br/>        pass</span></pre><p id="2aa4" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">该代码生成一个形状为 22，338 x 300 的矩阵(从语料库中提取的词汇长度 x 向量大小)。它可以通过单词 id 导航，可以从词汇表中获得。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="d103" class="kt ku je og b gy ok ol l om on">word = "data"</span><span id="80ad" class="kt ku je og b gy oo ol l om on">print("dic[word]:", dic_vocabulary[word], "|idx")<br/>print("embeddings[idx]:", embeddings[dic_vocabulary[word]].shape, <br/>      "|vector")</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/dd16f543cb696f43146c079609e5d4da.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*EWLnzZ0abpnhH1zR8jq5PQ.png"/></div></figure><p id="f4fc" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">终于到了搭建<strong class="lr jf">深度学习模型</strong>的时候了。我将使用我将构建和训练的神经网络的第一个嵌入层中的嵌入矩阵来对新闻进行分类。输入序列中的每个 id 将被用作访问嵌入矩阵的索引。这个嵌入层的输出将是一个 2D 矩阵，对于输入序列中的每个单词 id 有一个单词向量(序列长度×向量大小)。让我们用句子“<em class="mi">我喜欢这篇文章</em>”作为例子:</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/092d46a775b0cf57dc4259c2e49c2b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KvBp0xzRThA7qTXACT4A-g.png"/></div></div></figure><p id="e0c6" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我的神经网络结构如下:</p><ul class=""><li id="e79b" class="nk nl je lr b ls mo lv mp lc nm lg nn lk no mh np nq nr ns bi translated">如前所述，嵌入层将序列作为输入，将单词向量作为权重。</li><li id="b13f" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">一个简单的注意层，不会影响预测，但它将捕获每个实例的权重，并允许我们构建一个好的解释器(它对于预测是不必要的，只是为了解释，所以你可以跳过它)。注意机制在<a class="ae mt" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">本文</a> (2014)中提出，作为序列模型(即 LSTM)问题的解决方案，以理解长文本的哪些部分实际上是相关的。</li><li id="1d19" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">两层双向 LSTM，在两个方向上对序列中的单词顺序进行建模。</li><li id="e566" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">两个最终的密集层将预测每个新闻类别的概率。</li></ul><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="3c10" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## code attention layer</strong><br/>def <strong class="og jf">attention_layer</strong>(inputs, neurons):<br/>    x = layers.<strong class="og jf">Permute</strong>((2,1))(inputs)<br/>    x = layers.<strong class="og jf">Dense</strong>(neurons, activation="softmax")(x)<br/>    x = layers.<strong class="og jf">Permute</strong>((2,1), name="<strong class="og jf">attention</strong>")(x)<br/>    x = layers.<strong class="og jf">multiply</strong>([inputs, x])<br/>    return x</span><span id="7930" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><br/>## input</strong><br/>x_in = layers.<strong class="og jf">Input</strong>(shape=(15,))</span><span id="d8c6" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## embedding</strong><br/>x = layers.<strong class="og jf">Embedding</strong>(input_dim=embeddings.shape[0],  <br/>                     output_dim=embeddings.shape[1], <br/>                     weights=[embeddings],<br/>                     input_length=15, trainable=False)(x_in)</span><span id="5041" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## apply attention</strong><br/>x = attention_layer(x, neurons=15)</span><span id="70c4" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## 2 layers of bidirectional lstm</strong><br/>x = layers.<strong class="og jf">Bidirectional</strong>(layers.<strong class="og jf">LSTM</strong>(units=15, dropout=0.2, <br/>                         return_sequences=True))(x)<br/>x = layers.<strong class="og jf">Bidirectional</strong>(layers.<strong class="og jf">LSTM</strong>(units=15, dropout=0.2))(x)</span><span id="9ebc" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## final dense layers</strong><br/>x = layers.<strong class="og jf">Dense</strong>(64, activation='relu')(x)<br/>y_out = layers.<strong class="og jf">Dense</strong>(3, activation='softmax')(x)</span><span id="34a7" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## compile</strong><br/>model = models.<strong class="og jf">Model</strong>(x_in, y_out)<br/>model.compile(loss='sparse_categorical_crossentropy',<br/>              optimizer='adam', metrics=['accuracy'])<br/><br/>model.summary()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pp"><img src="../Images/008d3348585bcfd23a1c581509298f2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dfu-2hqEMaBe6YHx1C71Uw.png"/></div></div></figure><p id="9280" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">现在，我们可以训练模型，并在实际测试集上测试它之前，检查用于验证的训练集子集的性能。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="523e" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## encode y</strong><br/>dic_y_mapping = {n:label for n,label in <br/>                 enumerate(np.unique(y_train))}<br/>inverse_dic = {v:k for k,v in dic_y_mapping.items()}<br/>y_train = np.array([inverse_dic[y] for y in y_train])</span><span id="7971" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## train</strong><br/>training = model.fit(x=X_train, y=y_train, batch_size=256, <br/>                     epochs=10, shuffle=True, verbose=0, <br/>                     validation_split=0.3)</span><span id="40ef" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## plot loss and accuracy</strong><br/>metrics = [k for k in training.history.keys() if ("loss" not in k) and ("val" not in k)]<br/>fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)</span><span id="be93" class="kt ku je og b gy oo ol l om on">ax[0].set(title="Training")<br/>ax11 = ax[0].twinx()<br/>ax[0].plot(training.history['loss'], color='black')<br/>ax[0].set_xlabel('Epochs')<br/>ax[0].set_ylabel('Loss', color='black')<br/>for metric in metrics:<br/>    ax11.plot(training.history[metric], label=metric)<br/>ax11.set_ylabel("Score", color='steelblue')<br/>ax11.legend()</span><span id="19f7" class="kt ku je og b gy oo ol l om on">ax[1].set(title="Validation")<br/>ax22 = ax[1].twinx()<br/>ax[1].plot(training.history['val_loss'], color='black')<br/>ax[1].set_xlabel('Epochs')<br/>ax[1].set_ylabel('Loss', color='black')<br/>for metric in metrics:<br/>     ax22.plot(training.history['val_'+metric], label=metric)<br/>ax22.set_ylabel("Score", color="steelblue")<br/>plt.show()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/d7a10cdb9f1d7072551abac81d5ce28d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MdYjdeOju4ez8gcAc-v8qA.png"/></div></div></figure><p id="759d" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">不错！在某些纪元中，精确度达到 0.89。为了完成单词嵌入模型的<strong class="lr jf">评估</strong>，让我们预测测试集并比较之前使用的相同度量(度量的代码与之前相同)。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="f7a4" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## test</strong><br/>predicted_prob = model.predict(X_test)<br/>predicted = [dic_y_mapping[np.argmax(pred)] for pred in <br/>             predicted_prob]</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/4ec9e658756f2ba8a9584f1c13b5064c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a39MMTNXnDaFOKFur2Z7xQ.png"/></div></div></figure><p id="8636" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">该模型的表现与上一个模型一样好，事实上，它也很难对科技新闻进行分类。</p><p id="a333" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">但是这也是<strong class="lr jf">可以解释的吗？是的，它是！我在神经网络中放置了一个注意力层，以提取每个单词的权重，并了解这些权重对分类一个实例的贡献有多大。因此，我将尝试使用注意力权重来构建一个解释器(类似于上一节中看到的解释器):</strong></p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="855e" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## select observation<br/></strong>i = 0<br/>txt_instance = dtf_test["<strong class="og jf">text</strong>"].iloc[i]</span><span id="6749" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## check true value and predicted value</strong><br/>print("True:", y_test[i], "--&gt; Pred:", predicted[i], "| Prob:", round(np.max(predicted_prob[i]),2))</span><span id="81ae" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><br/>## show explanation<br/>### 1. preprocess input<br/></strong>lst_corpus = []<br/>for string in [re.sub(r'[^\w\s]','', txt_instance.lower().strip())]:<br/>    lst_words = string.split()<br/>    lst_grams = [" ".join(lst_words[i:i+1]) for i in range(0, <br/>                 len(lst_words), 1)]<br/>    lst_corpus.append(lst_grams)<br/>lst_corpus = list(bigrams_detector[lst_corpus])<br/>lst_corpus = list(trigrams_detector[lst_corpus])<br/>X_instance = kprocessing.sequence.pad_sequences(<br/>              tokenizer.texts_to_sequences(corpus), maxlen=15, <br/>              padding="post", truncating="post")</span><span id="932d" class="kt ku je og b gy oo ol l om on"><strong class="og jf">### 2. get attention weights</strong><br/>layer = [layer for layer in model.layers if "<strong class="og jf">attention</strong>" in <br/>         layer.name][0]<br/>func = K.function([model.input], [layer.output])<br/>weights = func(X_instance)[0]<br/>weights = np.mean(weights, axis=2).flatten()</span><span id="0d81" class="kt ku je og b gy oo ol l om on"><strong class="og jf">### 3. rescale weights, remove null vector, map word-weight</strong><br/>weights = preprocessing.MinMaxScaler(feature_range=(0,1)).fit_transform(np.array(weights).reshape(-1,1)).reshape(-1)<br/>weights = [weights[n] for n,idx in enumerate(X_instance[0]) if idx <br/>           != 0]<br/>dic_word_weigth = {word:weights[n] for n,word in <br/>                   enumerate(lst_corpus[0]) if word in <br/>                   tokenizer.word_index.keys()}</span><span id="6dcb" class="kt ku je og b gy oo ol l om on"><strong class="og jf">### 4. barplot</strong><br/>if len(dic_word_weigth) &gt; 0:<br/>   dtf = pd.DataFrame.from_dict(dic_word_weigth, orient='index', <br/>                                columns=["score"])<br/>   dtf.sort_values(by="score", <br/>           ascending=True).tail(3).plot(kind="barh", <br/>           legend=False).grid(axis='x')<br/>   plt.show()<br/>else:<br/>   print("--- No word recognized ---")</span><span id="2f30" class="kt ku je og b gy oo ol l om on"><strong class="og jf">### 5. produce html visualization</strong><br/>text = []<br/>for word in lst_corpus[0]:<br/>    weight = dic_word_weigth.get(word)<br/>    if weight is not None:<br/>         text.append('&lt;b&gt;&lt;span style="background-color:rgba(100,149,237,' + str(weight) + ');"&gt;' + word + '&lt;/span&gt;&lt;/b&gt;')<br/>    else:<br/>         text.append(word)<br/>text = ' '.join(text)</span><span id="ce30" class="kt ku je og b gy oo ol l om on"><strong class="og jf">### 6. visualize on notebook<br/></strong>print("<strong class="og jf">\033</strong>[1m"+"Text with highlighted words")<br/>from IPython.core.display import display, HTML<br/>display(HTML(text))</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/ef181ff814438c9b8eb1c2313724e68a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*21iP8FD4XOBDn3XS5LKiJA.png"/></div></figure><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ps"><img src="../Images/f7c8a622fd9c5fd3bc0789735431bb13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CrFeNLHgDzBxAmgGiXR8lg.png"/></div></div></figure><p id="3f7e" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">就像以前一样，单词“<em class="mi">克林顿</em>”和“<em class="mi">共和党</em>”激活了模型的神经元，但这次“<em class="mi">高</em>”和“<em class="mi">班加西</em>”也被认为与预测略有关联。</p><h2 id="8831" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">语言模型</h2><p id="0455" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated"><a class="ae mt" href="https://en.wikipedia.org/wiki/Language_model" rel="noopener ugc nofollow" target="_blank">语言模型</a>，或语境化/动态单词嵌入<strong class="lr jf">，</strong>克服了经典单词嵌入方法的最大限制:多义词歧义消除，一个具有不同含义的单词(例如“<em class="mi"> bank </em>或“<em class="mi"> stick </em>”)仅通过一个向量来识别。最受欢迎的方法之一是 ELMO (2018)，它没有应用固定的嵌入，而是使用双向 LSTM，查看整个句子，然后为每个单词分配一个嵌入。</p><p id="9efe" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">进入变形金刚:谷歌论文<a class="ae mt" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="mi">提出的一种新的建模技术</em></a><em class="mi"/>(2017)<em class="mi"/>中展示了顺序模型(如 LSTM)可以完全被注意力机制取代，甚至获得更好的性能。</p><p id="0e55" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">谷歌的<a class="ae mt" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank"> <strong class="lr jf">伯特</strong> </a> <strong class="lr jf"> </strong>(来自变形金刚的双向编码器表示，2018)结合了 ELMO 上下文嵌入和几个变形金刚，加上它的双向(这对变形金刚来说是一个很大的新奇)。BERT 分配给单词的向量是整个句子的函数，因此，基于上下文，一个单词可以有不同的向量。让我们用<em class="mi">变压器</em>试试吧:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="0f5f" class="kt ku je og b gy ok ol l om on">txt = <strong class="og jf">"bank river"</strong></span><span id="8739" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## bert tokenizer</strong><br/>tokenizer = transformers.<strong class="og jf">BertTokenizer</strong>.<strong class="og jf">from_pretrained</strong>('bert-base-uncased', do_lower_case=True)</span><span id="b7ff" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## bert model</strong><br/>nlp = transformers.<strong class="og jf">TFBertModel</strong>.<strong class="og jf">from_pretrained</strong>('bert-base-uncased')</span><span id="20f4" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## return hidden layer with embeddings</strong><br/>input_ids = np.array(tokenizer.encode(txt))[None,:]  <br/>embedding = nlp(input_ids)<strong class="og jf"><br/></strong>embedding[0][0]</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pt"><img src="../Images/9f42c3aa067e46a5cf7eec8718ebe337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f7-l1PPDu5Q6rgUBzgz12A.png"/></div></div></figure><p id="cfbd" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">如果我们将输入文本更改为“<em class="mi">银行资金</em>”，我们得到的结果是:</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pu"><img src="../Images/bfea07a1c107b72a46ac20bb78cbbd2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V2iWL5kNy9WBzIFPfoVZYQ.png"/></div></div></figure><p id="bd4c" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">为了完成文本分类任务，您可以以 3 种不同的方式使用 BERT:</p><ul class=""><li id="31c1" class="nk nl je lr b ls mo lv mp lc nm lg nn lk no mh np nq nr ns bi translated">训练它从划痕，并使用它作为分类器。</li><li id="32ee" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">提取单词 embedding，并在嵌入层中使用它们(就像我对 Word2Vec 所做的那样)。</li><li id="4e55" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">微调预训练模型(迁移学习)。</li></ul><p id="8eb7" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我将采用后者，并从一个预训练的较轻版本的 BERT 进行迁移学习，称为<a class="ae mt" href="https://huggingface.co/transformers/model_doc/distilbert.html" rel="noopener ugc nofollow" target="_blank">distilt-BERT</a>(6600 万个参数，而不是 1.1 亿个！).</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="ce0e" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## distil-bert tokenizer</strong><br/>tokenizer = transformers.<strong class="og jf">AutoTokenizer</strong>.<strong class="og jf">from_pretrained</strong>('distilbert-base-uncased', do_lower_case=True)</span></pre><p id="9ddd" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">像往常一样，在装配模型之前，有一些特征工程工作要做，但是这次会有点棘手。为了说明我要做的事情，让我们以我们最喜欢的句子“<em class="mi">我喜欢这篇文章</em>”为例，它必须转换为 3 个向量(id、掩码、段):</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pv"><img src="../Images/2f16d8673e765195c46a9def4ed73f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rENCe-2FhlIBIfUstVHRhA.png"/></div></div><p class="ov ow gj gh gi ox oy bd b be z dk translated">形状:3 x 序列长度</p></figure><p id="c6b3" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">首先，我们需要选择序列的最大长度。这次我将选择一个更大的数字(即 50 ),因为 BERT 会将未知单词拆分成子标记，直到找到一个已知的单词。例如，如果给定一个像"<em class="mi"> zzdata </em>"这样的造词，伯特会把它拆分成[" <em class="mi"> z </em>"，"<em class="mi"> ##z </em>"，"<em class="mi"> ##data </em> "]。此外，我们必须在输入文本中插入特殊标记，然后生成掩码和分段。最后，将所有这些放在一个张量中以获得特征矩阵，该矩阵将具有 3(id、掩码、分段)x 语料库中的文档数量 x 序列长度的形状。</p><p id="37c8" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">请注意，我使用原始文本作为语料库(到目前为止，我一直使用<em class="mi"> clean_text </em>列)。</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="19fd" class="kt ku je og b gy ok ol l om on">corpus = dtf_train["<strong class="og jf">text</strong>"]<br/>maxlen = 50</span><span id="f2b8" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><br/>## add special tokens</strong><br/>maxqnans = np.int((maxlen-20)/2)<br/>corpus_tokenized = ["[CLS] "+<br/>             " ".join(tokenizer.tokenize(re.sub(r'[^\w\s]+|\n', '', <br/>             str(txt).lower().strip()))[:maxqnans])+<br/>             " [SEP] " for txt in corpus]<br/><br/><strong class="og jf">## generate masks</strong><br/>masks = [[1]*len(txt.split(" ")) + [0]*(maxlen - len(<br/>           txt.split(" "))) for txt in corpus_tokenized]<br/>    <br/><strong class="og jf">## padding</strong><br/>txt2seq = [txt + " [PAD]"*(maxlen-len(txt.split(" "))) if len(txt.split(" ")) != maxlen else txt for txt in corpus_tokenized]<br/>    <br/><strong class="og jf">## generate idx</strong><br/>idx = [tokenizer.encode(seq.split(" ")) for seq in txt2seq]<br/>    <br/><strong class="og jf">## generate segments</strong><br/>segments = [] <br/>for seq in txt2seq:<br/>    temp, i = [], 0<br/>    for token in seq.split(" "):<br/>        temp.append(i)<br/>        if token == "[SEP]":<br/>             i += 1<br/>    segments.append(temp)</span><span id="3481" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## feature matrix</strong><br/>X_train = [np.asarray(idx, dtype='int32'), <br/>           np.asarray(masks, dtype='int32'), <br/>           np.asarray(segments, dtype='int32')]</span></pre><p id="5aa3" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">特征矩阵<em class="mi"> X_train </em>的形状为 3×34，265×50。我们可以从特征矩阵中检查随机观察值:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="9e38" class="kt ku je og b gy ok ol l om on">i = 0</span><span id="6e88" class="kt ku je og b gy oo ol l om on">print("txt: ", dtf_train["text"].iloc[0])<br/>print("tokenized:", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])<br/>print("idx: ", X_train[0][i])<br/>print("mask: ", X_train[1][i])<br/>print("segment: ", X_train[2][i])</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pw"><img src="../Images/35417be842967bba45244f5d416b1bfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vRUkclqzuGDvmgu1VFs-5A.png"/></div></div></figure><p id="a1fc" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">您可以将相同的代码应用于 dtf_test["text"]以获得<em class="mi"> X_test </em>。</p><p id="1d18" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">现在，我要从预先训练好的 BERT 开始，用迁移学习构建<strong class="lr jf">深度学习模型。基本上，我将使用平均池将 BERT 的输出总结为一个向量，然后添加两个最终的密集层来预测每个新闻类别的概率。</strong></p><p id="f6c7" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">如果您想使用 BERT 的原始版本，下面是代码(记得用正确的标记器重做特性工程):</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="ad59" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## inputs</strong><br/>idx = layers.<strong class="og jf">Input</strong>((50), dtype="int32", name="input_idx")<br/>masks = layers.<strong class="og jf">Input</strong>((50), dtype="int32", name="input_masks")<br/>segments = layers.Input((50), dtype="int32", name="input_segments")</span><span id="e1dc" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## pre-trained bert</strong><br/>nlp = transformers.<strong class="og jf">TFBertModel.from_pretrained</strong>("bert-base-uncased")<br/>bert_out, _ = nlp([idx, masks, segments])</span><span id="454a" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## fine-tuning</strong><br/>x = layers.<strong class="og jf">GlobalAveragePooling1D</strong>()(bert_out)<br/>x = layers.<strong class="og jf">Dense</strong>(64, activation="relu")(x)<br/>y_out = layers.<strong class="og jf">Dense</strong>(len(np.unique(y_train)), <br/>                     activation='softmax')(x)</span><span id="ee3d" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## compile</strong><br/>model = models.<strong class="og jf">Model</strong>([idx, masks, segments], y_out)</span><span id="aaca" class="kt ku je og b gy oo ol l om on">for layer in model.layers[:4]:<br/>    layer.trainable = False</span><span id="cd6e" class="kt ku je og b gy oo ol l om on">model.compile(loss='sparse_categorical_crossentropy', <br/>              optimizer='adam', metrics=['accuracy'])</span><span id="5e3e" class="kt ku je og b gy oo ol l om on">model.summary()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi px"><img src="../Images/c98c8514c367551c81337b688a615c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*riJ2LlNVz_0MJvqAbYG3Bw.png"/></div></div></figure><p id="ee15" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">正如我所说，我将使用更简单的版本，distilt-BERT:</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="6928" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## inputs</strong><br/>idx = layers.<strong class="og jf">Input</strong>((50), dtype="int32", name="input_idx")<br/>masks = layers.<strong class="og jf">Input</strong>((50), dtype="int32", name="input_masks")</span><span id="aff6" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## pre-trained bert with config</strong><br/>config = transformers.DistilBertConfig(dropout=0.2, <br/>           attention_dropout=0.2)<br/>config.output_hidden_states = False</span><span id="d4a7" class="kt ku je og b gy oo ol l om on">nlp = transformers.<strong class="og jf">TFDistilBertModel.from_pretrained</strong>('distilbert-<br/>                  base-uncased', config=config)<br/>bert_out = nlp(idx, attention_mask=masks)[0]</span><span id="cc29" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## fine-tuning</strong><br/>x = layers.<strong class="og jf">GlobalAveragePooling1D</strong>()(bert_out)<br/>x = layers.<strong class="og jf">Dense</strong>(64, activation="relu")(x)<br/>y_out = layers.<strong class="og jf">Dense</strong>(len(np.unique(y_train)), <br/>                     activation='softmax')(x)</span><span id="65af" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## compile</strong><br/>model = models.<strong class="og jf">Model</strong>([idx, masks], y_out)</span><span id="5a09" class="kt ku je og b gy oo ol l om on">for layer in model.layers[:3]:<br/>    layer.trainable = False</span><span id="46a9" class="kt ku je og b gy oo ol l om on">model.compile(loss='sparse_categorical_crossentropy', <br/>              optimizer='adam', metrics=['accuracy'])</span><span id="dcef" class="kt ku je og b gy oo ol l om on">model.summary()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/aaeb3eec736e646710bfbda0a0eb20d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p5sc1h4l3DewgrHvZDrr9g.png"/></div></div></figure><p id="ed0a" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">让我们<strong class="lr jf">训练、测试、评估</strong>这个坏小子(评估的代码相同):</p><pre class="mk ml mm mn gt of og oh oi aw oj bi"><span id="6ad3" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## encode y</strong><br/>dic_y_mapping = {n:label for n,label in <br/>                 enumerate(np.unique(y_train))}<br/>inverse_dic = {v:k for k,v in dic_y_mapping.items()}<br/>y_train = np.array([inverse_dic[y] for y in y_train])</span><span id="04a7" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## train</strong><br/>training = model.fit(x=X_train, y=y_train, batch_size=64, <br/>                     epochs=1, shuffle=True, verbose=1, <br/>                     validation_split=0.3)</span><span id="d959" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## test</strong><br/>predicted_prob = model.predict(X_test)<br/>predicted = [dic_y_mapping[np.argmax(pred)] for pred in <br/>             predicted_prob]</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi py"><img src="../Images/a6d56582bddbbc0fe9e6c54ade57be57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oS2xD1Y0IWPGDg8uVx2dPQ.png"/></div></div></figure><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pz"><img src="../Images/65be0ab75c86cda87d57d03c990a42a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NsiKi7b0JGlCQPLpeVkftA.png"/></div></div></figure><p id="1319" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">BERT 的性能比以前的型号略好，事实上，它比其他型号能识别更多的科技新闻。</p><h2 id="64fd" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">结论</h2><p id="9228" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">这篇文章是演示<strong class="lr jf">如何将不同的 NLP 模型应用到多类分类用例</strong>的教程。我比较了 3 种流行的方法:使用 Tf-Idf 的单词包、使用 Word2Vec 的单词嵌入和使用 BERT 的语言模型。我经历了特征工程&amp;选择，模型设计&amp;测试，评估&amp;可解释性，比较每一步中的 3 个模型(如果可能的话)。</p><p id="9ace" class="pw-post-body-paragraph lp lq je lr b ls mo kf lu lv mp ki lx lc mq lz ma lg mr mc md lk ms mf mg mh im bi translated">我希望你喜欢它！如有问题和反馈，或者只是分享您感兴趣的项目，请随时联系我。</p><blockquote class="qa"><p id="3199" class="qb qc je bd qd qe qf qg qh qi qj mh dk translated">👉<a class="ae mt" href="https://linktr.ee/maurodp" rel="noopener ugc nofollow" target="_blank">我们来连线</a>👈</p></blockquote></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><blockquote class="qk ql qm"><p id="3822" class="lp lq mi lr b ls mo kf lu lv mp ki lx qn mq lz ma qo mr mc md qp ms mf mg mh im bi translated">本文是 Python 的系列文章<strong class="lr jf"> NLP 的一部分，参见:</strong></p></blockquote><div class="is it gp gr iu mu"><a rel="noopener follow" target="_blank" href="/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jf gy z fp mz fr fs na fu fw jd bi translated">带 NLP 的 AI 聊天机器人:语音识别+变形金刚</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">用 Python 构建一个会说话的聊天机器人，与你的人工智能进行对话</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">towardsdatascience.com</p></div></div><div class="nd l"><div class="qq l nf ng nh nd ni ja mu"/></div></div></a></div><div class="is it gp gr iu mu"><a rel="noopener follow" target="_blank" href="/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jf gy z fp mz fr fs na fu fw jd bi translated">使用 NLP 的文本摘要:TextRank vs Seq2Seq vs BART</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">使用 Python、Gensim、Tensorflow、Transformers 进行自然语言处理</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">towardsdatascience.com</p></div></div><div class="nd l"><div class="qr l nf ng nh nd ni ja mu"/></div></div></a></div><div class="is it gp gr iu mu"><a rel="noopener follow" target="_blank" href="/text-analysis-feature-engineering-with-nlp-502d6ea9225d"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jf gy z fp mz fr fs na fu fw jd bi translated">使用自然语言处理的文本分析和特征工程</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">语言检测，文本清理，长度，情感，命名实体识别，N-grams 频率，词向量，主题…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">towardsdatascience.com</p></div></div><div class="nd l"><div class="qs l nf ng nh nd ni ja mu"/></div></div></a></div><div class="is it gp gr iu mu"><a rel="noopener follow" target="_blank" href="/text-classification-with-no-model-training-935fe0e42180"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jf gy z fp mz fr fs na fu fw jd bi translated">用于无模型训练的文本分类的 BERT</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">如果没有带标签的训练集，请使用 BERT、单词嵌入和向量相似度</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">towardsdatascience.com</p></div></div><div class="nd l"><div class="qt l nf ng nh nd ni ja mu"/></div></div></a></div></div></div>    
</body>
</html>