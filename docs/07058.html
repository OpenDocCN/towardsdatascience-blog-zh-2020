<html>
<head>
<title>Predict LeBron James’s Game Results with RNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测勒布朗詹姆斯与 RNN 的比赛结果</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predict-lebron-jamess-game-results-with-rnn-1709f364d4d4?source=collection_archive---------43-----------------------#2020-05-30">https://towardsdatascience.com/predict-lebron-jamess-game-results-with-rnn-1709f364d4d4?source=collection_archive---------43-----------------------#2020-05-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="24b4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="1b82" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">众所周知，RNN 擅长分析序列数据。但是我们能应用它根据游戏历史预测游戏结果吗？</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/92b0dff57a0e09c6f63fc7c74c42675a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gWFZljpvVVlHG8m1"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">埃德加·恰帕罗在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="7ff1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> A </span>递归神经网络(RNN)是序列处理中最常用的深度学习算法之一。RNN 和前馈神经网络之间的区别在于，前者可以“记忆”序列中的时序信息，而后者可以一次性处理整个序列。</p><p id="32c1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，当数据的时间顺序对问题很重要时，我们最好使用 RNN，例如时间序列数据分析或自然语言处理(NLP)。</p><p id="38a1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，一个球员之前的比赛是否能够影响即将到来的比赛的结果仍在争论之中。所以，我想探究一下 RNN 是否适合篮球场上的比赛预测问题。</p><p id="5ca7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇短文中，我试图使用一个深度学习模型，根据之前的比赛统计数据和结果来预测勒布朗·詹姆斯未来的比赛结果。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="20dc" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">数据集</h2><p id="af99" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">我用 Python 刮了勒布朗从<strong class="lk jd"> <em class="nr">赛季 2003–04</em></strong>到<strong class="lk jd"> <em class="nr">赛季 2019–20</em></strong>的<strong class="lk jd">场 1258 场</strong>的比赛统计。对刮痧程序感兴趣的可以参考<a class="ae lh" rel="noopener" target="_blank" href="/scrape-tabular-data-with-python-b1dd1aeadfad">我之前的一篇帖子</a>。</p><p id="8c45" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是数据的样子。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/aa2b94ea01d9235e8e7516fa4cc4aff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Se7kSttDWDk5t2rV_rNUA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">数据头</p></figure><p id="3add" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在<a class="ae lh" rel="noopener" target="_blank" href="/present-the-feature-importance-of-the-random-forest-classifier-99bb042be4cc">我的另一个帖子</a>中，我已经说明了“GmSc”(玩家的游戏分数)、“+/-”(加减分)和“Minutes”(上场时间)是解释相应游戏结果的三个最重要的特征。因此，为了减少输入数据的维度，我只在我们的模型中保留这些游戏统计数据和游戏结果(“赢”)。对于每个数据点，我提取过去 10 场比赛的统计数据(t-9，t-8，…，t-1，t)，目的是预测下一场比赛(t + 1)的比赛结果。</p><p id="36ed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我用前 800 场作为我们的训练数据，后 300 场作为验证数据，后 158 场作为测试数据。</p><p id="b7eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数据的整个预处理如下所示。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="01ad" class="mu mv it nu b gy ny nz l oa ob">data_u = df[["GmSc","+/-","Minutes","Win"]].to_numpy()<br/>mean = data_u[:800].mean(axis=0)<br/>data_u -= mean<br/>std = data_u[:800].std(axis=0)<br/>data_u /= std</span></pre><p id="174d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">标准化参数(平均值和标准偏差)仅从训练数据(前 800 场比赛)中生成，然后将这些参数应用于整个数据集。</p><p id="70da" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">加载整个数据集非常消耗内存，有时甚至是不可能的。所以使用<strong class="lk jd">数据生成器</strong>(在<strong class="lk jd"> Keras </strong>中实现)向模型提供数据是处理这个问题的一个好选择。即使这里我有一个非常小的(1，258 个数据点)数据集，我仍然希望在我的管道中使用一个数据生成器，这将有利于将来的扩展。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="173a" class="mu mv it nu b gy ny nz l oa ob">def generator(data, lookback, delay, start, end, batch_size = 64):<br/>    if end is None:<br/>        end = len(data) - delay - 1<br/>    i = start + lookback<br/>    while True:<br/>        if i + batch_size &gt;= end:<br/>            i = start + lookback<br/>        rows = np.arange(i, min(i + batch_size, end))<br/>        i += len(rows)<br/>        samples = np.zeros((len(rows),<br/>                           lookback,<br/>                           data.shape[-1]))<br/>        res_s = np.zeros((len(rows),))<br/>        <br/>        for j, row in enumerate(rows):<br/>            indices = range(rows[j] - lookback, rows[j])<br/>            samples[j] = data[indices]<br/>            tar_v = data[rows[j] + delay][3]<br/>            if tar_v &gt; 0:<br/>                res_s[j] = 1<br/>            else:<br/>                res_s[j] = 0<br/>        yield samples, res_s</span></pre><p id="8439" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我将<em class="nr">回看</em>设置为 10，这意味着使用之前的 10 场比赛作为输入。我把<em class="nr">延迟</em>设为 1，表示预测下一场比赛结果。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="15ac" class="mu mv it nu b gy ny nz l oa ob">lookback = 10<br/>delay = 1<br/>batch_size = 128<br/>steps_per_epc = int(800/batch_size)</span></pre><p id="5f46" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了定义训练、验证和测试数据生成器，我只需要将开始和结束值提供给生成器函数。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="e38b" class="mu mv it nu b gy ny nz l oa ob">train_generator = generator(data_u,<br/>                           lookback = lookback,<br/>                           delay = delay,<br/>                           start = 0,<br/>                           end = 800,<br/>                           batch_size = batch_size)</span><span id="e1a3" class="mu mv it nu b gy oc nz l oa ob">val_generator = generator(data_u,<br/>                           lookback = lookback,<br/>                           delay = delay,<br/>                           start = 801,<br/>                           end = 1100,<br/>                           batch_size = batch_size)</span><span id="28e8" class="mu mv it nu b gy oc nz l oa ob">test_generator = generator(data_u,<br/>                           lookback = lookback,<br/>                           delay = delay,<br/>                           start = 1101,<br/>                           end = None,<br/>                           batch_size = batch_size)</span></pre><p id="965d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">相应地，我需要指定检查验证和测试数据集所需的步骤数量。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="32c0" class="mu mv it nu b gy ny nz l oa ob">val_steps = (1100 - 801 - lookback)<br/>test_steps = (len(data_u) - 1101 - lookback)</span></pre><p id="a810" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我将构建模型结构。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="c7b5" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">建模结果</h2><p id="6f70" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">首先，我构建了一个具有密集连接层的人工神经网络作为我的基线模型，以便与我的其他模型进行比较。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="92ec" class="mu mv it nu b gy ny nz l oa ob">from keras.models import Sequential<br/>from keras import layers<br/>from keras.optimizers import RMSprop</span><span id="fe09" class="mu mv it nu b gy oc nz l oa ob">model_ann = Sequential()<br/>model_ann.add(layers.Flatten(input_shape = (lookback, data_u.shape[-1])))<br/>model_ann.add(layers.Dense(32,activation = 'relu'))<br/>model_ann.add(layers.Dropout(0.3))<br/>model_ann.add(layers.Dense(1,activation = 'sigmoid'))<br/>model_ann.summary()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi od"><img src="../Images/0adc8ef7d7233160b368667673ce5900.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*9T61xdmlCI4m_0nw90cnBw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">人工神经网络模型结构</p></figure><p id="1fcb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我编译模型，记录拟合过程。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="8c3e" class="mu mv it nu b gy ny nz l oa ob">model_ann.compile(optimizer = RMSprop(lr = 1e-2),<br/>                 loss = 'binary_crossentropy',<br/>                 metrics = ['acc'])<br/>history = model_ann.fit_generator(train_generator,<br/>                                  steps_per_epoch=steps_per_epc,<br/>                              epochs = 20, <br/>                              validation_data = val_generator,<br/>                              validation_steps = val_steps)</span></pre><p id="2e0f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了检查验证数据集的性能，我绘制了损失曲线。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="1ac1" class="mu mv it nu b gy ny nz l oa ob">acc_ = history_dic['loss']<br/>val_acc_ = history_dic['val_loss']<br/>epochs = range(1,21)<br/>#plt.clf()<br/>plt.plot(epochs,acc_, 'bo', label = "training loss")<br/>plt.plot(epochs, val_acc_, 'r', label = "validation loss")<br/>plt.xlabel('Epochs')<br/>plt.ylabel('loss')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/16ef58ad32c64201fb87e680c101aca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*wLVIIP07Olk4XSbci-YR4Q.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">训练和验证人工神经网络损失</p></figure><p id="9366" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如所料，模型在几个时期后变得过度拟合。为了客观地评价该模型，我将其应用于测试集，得到的准确率为 60%。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="fd45" class="mu mv it nu b gy ny nz l oa ob">scores = model_ann.evaluate_generator(test_generator,test_steps) <br/>print("Accuracy = ", scores[1]," Loss = ", scores[0])</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/8d3b81b5853389bc1dc27f00a5e8aeef.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*7BsrzcQZoMUBx8NIh2BO_g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">人工神经网络测试集性能</p></figure><p id="d278" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我通过使用一个<a class="ae lh" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank"> LSTM 层</a>后跟两个紧密连接的层来实现 RNN。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="eef3" class="mu mv it nu b gy ny nz l oa ob">model_rnn = Sequential()<br/>model_rnn.add(layers.LSTM(32,<br/>                        dropout=0.2,<br/>                        recurrent_dropout=0.2,<br/>                        input_shape=(None,data_u.shape[-1])))</span><span id="f24d" class="mu mv it nu b gy oc nz l oa ob">model_rnn.add(layers.Dense(32,activation = 'relu'))<br/>model_rnn.add(layers.Dropout(0.3))<br/>model_rnn.add(layers.Dense(1,activation='sigmoid'))<br/>model_rnn.summary()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6a4ca0eab21ff9f724d5c2228a171530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*rANZ6gZCG9QODoZv-bNHKA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">一层 LSTM 结构</p></figure><p id="107e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">模型训练类似于上面的 ANN。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="4f83" class="mu mv it nu b gy ny nz l oa ob">model_rnn.compile(optimizer = RMSprop(lr = 1e-2),<br/>                 loss = 'binary_crossentropy',<br/>                 metrics = ['acc'])<br/>history = model_rnn.fit_generator(train_generator, <br/>                                  steps_per_epoch=steps_per_epc,<br/>                              epochs = 20, <br/>                              validation_data = val_generator,<br/>                              validation_steps = val_steps)</span></pre><p id="2c62" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">训练集和验证集的性能如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/6b0283436e4c47d237428a3f35a477c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*umgTN8yH4ptWbLUmeutfjg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">1 层 LSTM 损耗</p></figure><p id="c4ec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">过拟合不像人工神经网络那样严重。我还在测试数据上评估了该模型，其准确率为 62.5%。尽管在测试集上的性能优于具有密集连接层的人工神经网络，但是改进是微小的。</p><p id="61b1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了获得更好的性能，我试图通过增加一个递归层来增加模型的复杂性。然而，为了减少计算成本，我用门控递归单元(GRU)代替了 LSTM 层。模型如下所示。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="fd24" class="mu mv it nu b gy ny nz l oa ob">model_rnn = Sequential()<br/>model_rnn.add(layers.GRU(32,<br/>                        dropout=0.2,<br/>                        recurrent_dropout=0.2,<br/>                         return_sequences = True,<br/>                        input_shape=(None,data_u.shape[-1])))<br/>model_rnn.add(layers.GRU(64, activation = 'relu',dropout=0.2,recurrent_dropout=0.2))<br/>model_rnn.add(layers.Dense(32,activation = 'relu'))<br/>model_rnn.add(layers.Dropout(0.3))</span><span id="fb2c" class="mu mv it nu b gy oc nz l oa ob">model_rnn.add(layers.Dense(1,activation = 'sigmoid'))<br/>model_rnn.summary()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/09880261ed521b647156c04dd95b3d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*I60JpqLSF-0PZkSP3hs2mQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">2 层 GRU 结构</p></figure><p id="f3e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">训练集和验证集的性能如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/4f0275bdc64dc0f633b4456a88ccee03.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*HckitRqu9QcfC1ohqmDzPA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">第二层 GRU 损失</p></figure><p id="c1c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在图上没有检测到严重的过度拟合。即使测试数据的准确性已经提高到 64%,改进仍然很小。我开始怀疑 RNN 能否胜任这项工作。</p><p id="f807" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，我通过进一步增加模型的复杂性来做最后的尝试。具体来说，我使递归层成为双向的。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="4ed2" class="mu mv it nu b gy ny nz l oa ob">model_rnn = Sequential()<br/>model_rnn.add(layers.Bidirectional(layers.GRU(32,<br/>                        dropout=0.2,<br/>                        recurrent_dropout=0.2,<br/>                         return_sequences = True),<br/>                        input_shape=(None,data_u.shape[-1])))<br/>model_rnn.add(layers.Bidirectional(layers.GRU(64, activation = 'relu',dropout=0.2,recurrent_dropout=0.2)))</span><span id="cd0e" class="mu mv it nu b gy oc nz l oa ob">model_rnn.add(layers.Dense(32,activation = 'relu'))<br/>model_rnn.add(layers.Dropout(0.3))<br/>model_rnn.add(layers.Dense(1,activation='sigmoid'))<br/>model_rnn.summary()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b9b5ba7c5540da8ee8221b543aa393b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*MyD-wBrJxiMoQfxltSepKQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">双向 RNN 结构</p></figure><p id="c549" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这一次，训练集和验证集的性能如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/72807386da7aaac75dd52eb8c7a56a1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*3qhmKOckH0-b9BCVmR6uHg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">双向损失</p></figure><p id="b9ca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">实际上，在模型开始过拟合之前，这个模型和上一个模型在验证损失上没有太大区别。测试集的准确率也达到了 64%。</p><p id="02f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过探索上面的所有模型，我有点意识到 RNN 可能不太适合 NBA 比赛结果预测问题。确实有几十个超参数可以调整，但是人工神经网络和 RNN 之间的差异太小。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="0969" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">数据诊断</h2><p id="bba5" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">我不想被测试集准确性的值所迷惑，所以我进一步诊断数据集，以检查模型是否真的有效。</p><p id="3a8b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我在训练、验证和测试数据集中检查勒布朗的詹姆斯胜率。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/856471f4026ac2f2031affcb3ed9354b.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*GzAsPyHXhf0wMFoyFbaXBg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">胜率的数据诊断</p></figure><p id="96f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">哎呀！测试数据集中的胜率是 62.7%，这意味着如果我猜测勒布朗会赢得所有比赛，我仍然得到 62.7%的准确率。</p><p id="4b33" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些结果表明，我的模型并不比随机猜测好多少。伤心…</p><p id="4680" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">可能是我没有全面探索超参数空间，也可能是勒布朗的比赛不可预测。但有一点是肯定的，大约 1000 个数据点远远不够。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="b779" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">讨论</h2><p id="8bbd" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">在本文中，我展示了一个为游戏结果预测问题开发失败模型的例子。如果你认为你没有从中学到什么，我想强迫自己列出一些要点。至少你可能会在建模中遇到这些细节。</p><ol class=""><li id="96f0" class="ol om it lk b ll lm lo lp lr on lv oo lz op md oq or os ot bi translated">首先将模型推向过度拟合是很重要的，因为一个具有弱表示能力的模型永远不会解决问题。</li><li id="cf67" class="ol om it lk b ll ou lo ov lr ow lv ox lz oy md oq or os ot bi translated">如果你的模型在大量的时期后仍然不合适，比如说 40 个，你可能需要增加学习率。</li><li id="d72a" class="ol om it lk b ll ou lo ov lr ow lv ox lz oy md oq or os ot bi translated">如果你只接触到有限的数据，没有什么是新奇的。你的复杂模型可能比随机猜测更糟糕。</li></ol></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="c21f" class="mu mv it bd mw mx my dn mz na nb dp nc lr nd ne nf lv ng nh ni lz nj nk nl iz bi translated">参考资料:</h2><ol class=""><li id="eb9b" class="ol om it lk b ll nm lo nn lr oz lv pa lz pb md oq or os ot bi translated">弗朗索瓦·乔莱。<a class="ae lh" href="https://www.manning.com/books/deep-learning-with-python" rel="noopener ugc nofollow" target="_blank">用 Python 进行深度学习。</a></li><li id="9460" class="ol om it lk b ll ou lo ov lr ow lv ox lz oy md oq or os ot bi translated"><a class="ae lh" href="https://www.basketball-reference.com/" rel="noopener ugc nofollow" target="_blank">篮球参考。</a></li></ol></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="b937" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">谢谢你的时间。如果你喜欢读这篇文章，请<a class="ae lh" href="https://medium.com/@jianan.jay.lin" rel="noopener">关注我的媒体</a>。以下是我之前的一些文章。</p><div class="pc pd gp gr pe pf"><a rel="noopener follow" target="_blank" href="/one-potential-cause-of-overfitting-that-i-never-noticed-before-a57904c8c89d"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd jd gy z fp pk fr fs pl fu fw jc bi translated">我以前从未注意到的过度拟合的一个潜在原因</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">当训练数据中的性能比测试数据中的性能好得多时，就会发生过度拟合。默认…</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt lb pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a rel="noopener follow" target="_blank" href="/whos-the-mvp-of-nba-this-season-3e347c66a40a"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd jd gy z fp pk fr fs pl fu fw jc bi translated">谁是本赛季 NBA 的最有价值球员？</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">一个案例研究，展示一个机器学习项目从开始到结束的样子。</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pu l pq pr ps po pt lb pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a rel="noopener follow" target="_blank" href="/consider-multicollinearity-in-my-model-or-not-7aca16e74773"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd jd gy z fp pk fr fs pl fu fw jc bi translated">是否在我的模型中考虑多重共线性？</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">简要讨论是否有必要修复特征空间中的多重共线性。我希望它会…</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pv l pq pr ps po pt lb pf"/></div></div></a></div></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/552ee6e52640270a1abb0aedf97bf13f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MZft85LvvxMw0PlF"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">皮特拉·施瓦兹勒在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div></div>    
</body>
</html>