<html>
<head>
<title>How NLP has evolved for Financial Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理是如何发展成金融情绪分析的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-nlp-has-evolved-for-financial-sentiment-analysis-fb2990d9b3ed?source=collection_archive---------24-----------------------#2020-05-21">https://towardsdatascience.com/how-nlp-has-evolved-for-financial-sentiment-analysis-fb2990d9b3ed?source=collection_archive---------24-----------------------#2020-05-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6dcf219a6a85271f81fd973e978efce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eSmpsOqXS8phHgf01Da9nA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">资料来源:联合国人类住区规划署</p></figure><div class=""/><div class=""><h2 id="a4db" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">我们还需要人类去阅读无聊的财务报表吗？</h2></div><p id="4c4b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">机器学习模型旨在学习其输入数据的良好表示，以执行其任务。近年来，模型学习用自然语言处理(NLP)来表示单词的方式已经发生了变化，在本文中，我们将探讨模型如何理解语言以做出财务决策的显著变化。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lt"><img src="../Images/66ec451bd4699324a93a8ade867b889d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qfZQKnUmFDxTrlUJQApKWg.png"/></div></div></figure><p id="2a93" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们关注NLP在金融市场中的直接应用:自动对文本文档进行情感分类，从而做出快速准确的投资决策，避免人为偏见。</p><p id="4941" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了更有意义地比较不同的技术变革，我们使用了一个<a class="ae ly" href="https://arxiv.org/abs/1307.5336" rel="noopener ugc nofollow" target="_blank">金融短语库</a> [1],其中包含带标签的金融短语(如下例所示):</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lz"><img src="../Images/13b3f79f9860e7d71f1215d6317881ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q0YEqljcIWwi21t8MCXJzg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:好债还是坏账:检测经济文本中的语义指向[1]</p></figure><h1 id="11ef" class="ma mb ji bd mc md me mf mg mh mi mj mk ko ml kp mm kr mn ks mo ku mp kv mq mr bi translated"><strong class="ak">词汇袋/词典方法</strong></h1><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ms"><img src="../Images/72ff786f8fe2d6bebea9fad862f93d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0GaZqM21HsmwU6Qf.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">金融情绪分类的创始人:蒂姆&amp; # 183；拉夫兰和比尔&amp; # 183；麦克唐纳。<a class="ae ly" href="https://www.hillcrestasset.com/behavioral-finance/hillcrest-finance-award/2014-hbfa/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="7613" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">如果我们想让一个8岁的孩子解释上面的短语，我们可以给她一个带有积极、中性和消极标签的单词列表，并让她告诉我们文本中是否包含这些单词——这基本上就是词典方法。最流行的金融词典是拉夫兰·麦克唐纳词典。如果您有兴趣尝试一下，请使用这个运行速度非常快的<a class="ae ly" href="http://kaichen.work/?p=399" rel="noopener ugc nofollow" target="_blank">代码片段</a>。</p><p id="8158" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">使用LM的整体准确性并不高— <strong class="kz jj"> 61% </strong> [1]。因此，尽管我们可以快速地对一个文本序列进行分类，并理解我们为什么要对它们进行分类，但我们仍然非常依赖那些在字典中有一席之地的单词。即使这样，单词也只有一维。也许在更高的维度上，比如300，我们会有更好的结果来代表每一个单词？</p><h1 id="3ed8" class="ma mb ji bd mc md me mf mg mh mi mj mk ko ml kp mm kr mn ks mo ku mp kv mq mr bi translated">嵌入</h1><p id="064d" class="pw-post-body-paragraph kx ky ji kz b la mt kj lc ld mu km lf lg mv li lj lk mw lm ln lo mx lq lr ls im bi translated">嵌入将序列中的每个单词投影到多维空间中。我们可以把嵌入想象成一个宽的查找表——我们有300列，单词充当查找索引。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/c65e1e7922780d09f08a381263e5c38a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sRv4hnK6HAQANp0zIInhQA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">以埃隆·马斯克最近影响市场的推文为例</p></figure><p id="a04d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">SOTA嵌入很可能是ELMO的[2]语境嵌入，即嵌入考虑到单词及其周围的语境。嵌入有三个组成部分:两个来自双向上下文表示，一个是上下文无关的表示。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/674015429c45b4ca2cc1d24641f2378f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bHXcpto23TrLaiOZ"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">如果Elmo在NLP中每次有人用他的名字都会收到一分钱。资料来源:联合国人类住区规划署</p></figure><p id="18b7" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">嵌入本身仅仅是表象——仅仅是表象并不能对情感进行分类。因此，我们必须训练一个模型，使用嵌入作为输入，并学习预测情绪。我找不到任何测试短语库的论文，所以我<a class="ae ly" href="https://gist.github.com/neoyipeng2018/73b61bec5e7034a92ea0a4e393202f7d" rel="noopener ugc nofollow" target="_blank">在这里</a>做了一个快速实验。</p><p id="0dd2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">侧</strong>:没有我想象的那么快——张量流/keras比我记忆中的要更不确定/痛苦。最后，我从一个名为<a class="ae ly" href="https://github.com/plasticityai/magnitude" rel="noopener ugc nofollow" target="_blank"> pymagnitude </a>的即插即用库中取出了这些嵌入，并使用了我可信赖的ol’random forest。</p><p id="5ccf" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">整体准确率上升到了<strong class="kz jj"> 78%。</strong>比LM字典高，但还是不邪乎。嵌入能够从单词创建多维表示，但是即使嵌入是上下文相关的，它也只是单层表示，不管维度有多大。是时候探索一种高维度的多层表示，即深度学习了。</p><h1 id="5fc0" class="ma mb ji bd mc md me mf mg mh mi mj mk ko ml kp mm kr mn ks mo ku mp kv mq mr bi translated">深度学习</h1><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/2ca45438214e6a07ada383447c1b5ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l3E_KVpPH6SuKagE"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">资料来源:联合国人类住区规划署</p></figure><p id="a2ae" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">最近在架构和训练方法方面的进步极大地改善了深度学习模型学习语言和执行下游任务的方式。</p><p id="e306" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">transformer架构和<a class="ae ly" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">BERT</a>【4】无需进一步介绍——参考这篇<a class="ae ly" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">精彩文章</a>了解更多信息——但简而言之，自我关注和令牌屏蔽的使用允许学习真正的双向上下文关系。</p><p id="6002" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">NLP中迁移学习的使用最早是在<a class="ae ly" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">ul mfit</a>【5】中引入的，其中网络首先学习从庞大的文本语料库中预测最可能的单词，然后使用语言建模中预先训练的权重来执行其他任务——这导致了收敛性的改善，并减少了对庞大的带标签数据集的需求。</p><p id="53b2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><a class="ae ly" href="https://arxiv.org/abs/1908.10063" rel="noopener ugc nofollow" target="_blank">有一篇很好的论文，FinBERT </a>，将BERT和迁移学习应用于短语库，达到了<strong class="kz jj"> 97% </strong>的准确率！顺便说一句，如果你有兴趣看看如何使用FinBERT来交易企业申报情绪，我制作了一个视频，带你了解从解析申报到回测结果的整个过程！</p><figure class="lu lv lw lx gt iv"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="39b7" class="ma mb ji bd mc md me mf mg mh mi mj mk ko ml kp mm kr mn ks mo ku mp kv mq mr bi translated">下一步是什么？</h1><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/c7076c7526b6c895550aa6e02303ff9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hHsSJ_wsCIEzXl8l"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">资料来源:联合国人类住区规划署</p></figure><p id="7a1a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们完事了吗？当我们啜饮马提尼酒时，机器能分析金融文件并产生准确客观的情绪吗？我们是否不再需要人类分析师来仔细阅读大量财务报表，并时刻警惕彭博的头条新闻？</p><p id="8e92" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">不完全是。</p><p id="c47f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">当前的BERT模型存在实际限制，如计算成本随输入长度的二次增加——最大输入长度通常上限为512/1024个令牌。<a class="ae ly" href="https://arxiv.org/abs/2001.04451" rel="noopener ugc nofollow" target="_blank">改革者</a>和<a class="ae ly" href="https://arxiv.org/abs/2004.05150" rel="noopener ugc nofollow" target="_blank">龙前</a>是最近的论文，试图通过简化自我关注机制来解决。</p><p id="4888" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">此外，BERT不能做心算——它只包含文本表示。这是机器完全理解和分析通常包含大量数字的金融文档的主要瓶颈。我发现<a class="ae ly" href="https://arxiv.org/abs/1909.00109" rel="noopener ugc nofollow" target="_blank">的一篇论文</a>试图用一个做基础数学的程序来增强BERT——但在BERT能够进行人类水平的推理之前，可能要等待很长时间。* *更新:<a class="ae ly" href="https://arxiv.org/abs/2005.14165?fbclid=IwAR1Cc2FIMZ-Eh-pPLeXNnB3GV7Nd4ER4AuCBbI9xc_gYPoSyFP58X202QuQ" rel="noopener ugc nofollow" target="_blank">open ai发布的GPT-3 </a>无需事先训练就能进行零炮计算！</p><p id="1a6b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">最重要的是，金融文本可能是不明确的，并且通常需要文本本身无法提供的上下文知识BERT实现的高准确性是在人类注释者之间100%一致的短语上。甚至人类都不同意的其他短语使得机器不太可能胜过人类分析师。更有可能的是，一种<a class="ae ly" href="https://www.forbes.com/sites/michaelmolnar/2019/12/12/quantamental-investing-a-fuzzy-term-that-describes-an-inevitable-future/#434505a5ff07" rel="noopener ugc nofollow" target="_blank">量化方法</a>将主导市场，金融分析师通过机器学习得到增强，以做出准确、快速和客观的投资决策。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lt"><img src="../Images/66ec451bd4699324a93a8ade867b889d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qfZQKnUmFDxTrlUJQApKWg.png"/></div></div></figure><h1 id="d30c" class="ma mb ji bd mc md me mf mg mh mi mj mk ko ml kp mm kr mn ks mo ku mp kv mq mr bi translated">你认为金融情绪分析的下一步是什么？留下评论。</h1></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><p id="055f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">[1] Pekka Malo、Ankur Sinha、Pekka Korhonen、Jyrki Wallenius和Pyry Takala。2014.好债还是坏账:探测经济文本中的语义指向。信息科学与技术协会杂志65，4 (2014)，782–796。https://doi.org/10.1002/asi.23062arXiv:arXiv:1307.5336 v2</p><p id="d682" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">[2] Loughran，t .和Mcdonald，b.《什么时候责任不是责任？文本分析、字典和10-k。<em class="nj">《金融杂志》第66期</em>，2011年第1期，第35期第65页。</p><p id="f446" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">[3]马修·彼得斯、马克·诺依曼、莫希特·伊耶、马特·加德纳、克里斯托弗·克拉克、肯顿·李和卢克·塞特勒莫耶。2018a。深层语境化的词语表达。在NAACL。</p><p id="178e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">[4]雅各布·德夫林、张明蔚、肯顿·李和克里斯蒂娜·图塔诺瓦。Bert:用于语言理解的深度双向转换器的预训练。arXiv预印本arXiv:1810.04805，2018。</p><p id="727b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">[5]霍华德和鲁德。用于文本分类的通用语言模型微调。计算语言学协会(ACL)，2018。</p></div></div>    
</body>
</html>