<html>
<head>
<title>Additive Margin Softmax Loss (AM-Softmax)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">附加边际软最大损失(AM-Softmax)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/additive-margin-softmax-loss-am-softmax-912e11ce1c6b?source=collection_archive---------8-----------------------#2020-06-24">https://towardsdatascience.com/additive-margin-softmax-loss-am-softmax-912e11ce1c6b?source=collection_archive---------8-----------------------#2020-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f877" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解L-Softmax、A-Softmax和AM-Softmax</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/106876acbc02aec56a35538fec17364c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*0LtQaUc1XJQ71q3U1O69kA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://arxiv.org/pdf/1704.08063.pdf" rel="noopener ugc nofollow" target="_blank">来源:SphereFace:深度超球面嵌入人脸识别</a></p></figure><p id="f306" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在本文中，我将记录我在阅读<a class="ae ku" href="https://arxiv.org/abs/1801.05599" rel="noopener ugc nofollow" target="_blank">用于人脸验证的附加保证金Softmax</a>论文时，对附加保证金soft max损失或AM-Softmax损失的理解历程。在分类中，将创建一个决策边界来分隔类别。然而，当输出位于判定边界附近时，这可能是一个问题。AM-Softmax旨在通过向决策边界添加余量来解决这一问题，以增加类的可分性，并使相同类之间的距离更紧密。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/6c88c8aa835196f74733210ff8ecb2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*bhEtmMWXVZti9CpDISHTMQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">软最大VS AM-软最大<a class="ae ku" href="https://arxiv.org/abs/1801.05599" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></figure><p id="dbe1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">除了AM-Softmax，我还将讨论早期的作品，这些作品也将余量引入Softmax损耗，以不同的方式实现，如L-Softmax和Angular Softmax，以便更好地理解AM-Softmax。</p><h1 id="f16a" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">目录</h1><ol class=""><li id="9a2e" class="mk ml it kx b ky mm lb mn le mo li mp lm mq lq mr ms mt mu bi translated">先决条件</li><li id="77db" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated">软最大损失</li><li id="5f23" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated">L-Softmax(大余量Softmax)</li><li id="2401" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated">α-最大柔度(角度最大柔度)</li><li id="7de0" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated">AM-Softmax(附加余量Softmax)</li></ol><h1 id="bb77" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">先决条件</strong></h1><ol class=""><li id="fdf5" class="mk ml it kx b ky mm lb mn le mo li mp lm mq lq mr ms mt mu bi translated">深度学习和神经网络基础</li><li id="dbb2" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated">卷积神经网络</li></ol><p id="e5d7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我假设你有深度学习和神经网络的基础知识，特别是CNN，因为这可能是理解AM-Softmax所需要的。</p><h1 id="0e68" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak"> Softmax损失</strong></h1><p id="223b" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated">在深入到AM-Softmax之前，我们先多退一步，刷新一下对Softmax损耗的认识。当我第一次听说最大损失的时候，我很困惑我所知道的，最大损失是一个激活函数，而不是损失函数。</p><p id="1c72" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">简而言之，Softmax损失实际上只是一个Softmax激活加上一个交叉熵损失。Softmax是一个激活函数，它输出每个类的概率，这些概率的总和为1。交叉熵损失就是概率的负对数之和。它们通常在分类中一起使用。您可以在下面看到Softmax和交叉熵的公式，其中<em class="nd"> f </em>是Softmax函数，<em class="nd"> CE </em>是交叉熵损失。因此，Softmax损失只是这两个相加。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/e875f1bcba7fb81dd429a87baa53735a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wM2hLqEmylkmh7rConpfpA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Softmax损失。来源:<a class="ae ku" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" rel="noopener ugc nofollow" target="_blank">劳尔·戈麦斯</a></p></figure><p id="60db" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了更好地理解Softmax损失，我建议阅读Raúl Gómez的一篇文章，因为他在<a class="ae ku" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" rel="noopener ugc nofollow" target="_blank"> <em class="nd">中清楚地解释了这一点，理解分类交叉熵损失、二元交叉熵损失、Softmax损失、逻辑损失、焦点损失以及所有那些令人困惑的名称</em>。</a></p><h1 id="dc8e" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">大幅度软最大(左软最大)</h1><p id="2a31" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated">L-Softmax是在原始Softmax损失的基础上引入余量的第一批论文之一。这里的容限具有与三元组损失函数中的容限相似的概念，在三元组损失函数中，容限将增加类别之间的可分性或距离，并进而最小化相同类别之间的距离。这种类内紧密性和类间可分性将显著提高各种视觉分类和验证任务的性能。</p><p id="b51e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在论文中需要注意的一件重要事情是，当我们使用术语Softmax损失而不仅仅是Softmax激活和交叉熵损失时，我们还包括分类器或全连接层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/72bd6dae51734faf27f4e58c17b315c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*m0xv4Y0fY3UTnS1_ks08jQ.png"/></div></figure><p id="54c7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">根据上面的图像，我们可以将Softmax损失定义如下，其中<em class="nd"> f </em>作为最后一个完全连接的层或分类器的输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/87f0319eb0db75b32619452be4aa2aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*p-5MghuIN5KqTlnGzkPJBg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">软最大损失</p></figure><p id="bba5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">全连接层的输出只是权重和前一层输出加上偏差的乘积。所以f也可以写成<strong class="kx iu"> W </strong> * <strong class="kx iu"> x </strong>如下。(<strong class="kx iu"> b </strong>或bias为简单起见被省略，但如果加上它仍然可以工作)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nl"><img src="../Images/74d853be4c6432bb9402eb0c2a86f14c.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*XxNgtVAwwTtl2usjlEb7wg.png"/></div></div></figure><p id="2573" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">由于<em class="nd"> f </em>是<strong class="kx iu"> W </strong>和<strong class="kx iu"> x </strong>之间的内积，因此也可表示为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0775e16b48e7f0ebc611474ffe7fa97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*s60k4xACoSVqP7stHChE2Q.png"/></div></figure><p id="bab2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">其中θ是矢量<strong class="kx iu"> W </strong>和<strong class="kx iu"> x </strong>之间的角度。因此，Softmax损耗也可以通过代入f定义如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/61970ce4ce064a6802cb792e483b9f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*eTzt5cyM8hSx90vFqa3HoQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">修改的Softmax</p></figure><p id="5993" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了理解L-Softmax背后的直觉，我们将使用一个二元分类的例子。假设我们有一个来自类1的样本<strong class="kx iu"> <em class="nd"> x </em> </strong>。最初的Softmax需要<strong class="kx iu">W1</strong>*<strong class="kx iu">x</strong>&gt;<strong class="kx iu">W2</strong>*<strong class="kx iu">x</strong>才能将<strong class="kx iu"> x </strong>正确分类为1。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ebfa9f1f6a03bc6e3f2d1d46f75d28e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*lffJvF1VmeRE4LiD9xIRTw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">二元分类</p></figure><p id="bdd5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">但是，<strong class="kx iu">W1</strong>*<strong class="kx iu">x</strong>&gt;<strong class="kx iu">W2</strong>*<strong class="kx iu">x</strong>也可以写成如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/74bc3657d2141b5ef7417c317fc05a6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*vJGGiWpZ0o0iUBAJF3PbLw.png"/></div></figure><p id="bfdd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">接下来，我们希望分类更严格，并扩大决策范围。因此，我们需要下面的条件，其中我们将θ1乘以一个正整数，<strong class="kx iu"> m，</strong>和(0 ≤ θ1 ≤ π/m)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/8ceed28f011b302f9b5d4a55bde1fef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*5f6_Ixp_XNPb5FqRnFl1kQ.png"/></div></figure><p id="db9c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这将为我们提供如下所示的决策余量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/065ad121f4f608441cca6f32e627be99.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*8UzBpaCDfZFGg7_31MzEAA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">L-Softmax几何解释</p></figure><p id="c66e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">因此，按照前面的要求。L-Softmax损耗可定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f33709868ba9b5ab985181f32d225b03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*ohTCkvVQiamCTrTwexCf8Q.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">L-Softmax损失公式</p></figure><p id="d5e6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们要求ψ在哪里，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/383bcab93a979bfca761125f6380c842.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*l2UP_xo5k5xMy-Ptu2oKOQ.png"/></div></figure><p id="ee7a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">其中m是正整数，其中m越大，分类余量越大。然后，要求D(θ)是单调递减函数，D( π/m)应等于cos( π/m)。这是因为，我们希望<em class="nd"> cos </em>函数在π之前只有一个递减的值，因此，如果theta &lt; π/m和D(θ)应该是递减函数，我们才使用cos函数。</p><p id="681e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了满足上述要求并简化前向和后向传播，本文构造了一个特定的ψ，如下所示，其中k ∈ [0，m1]，k为整数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ae408f793c9c16c5df9732203183463f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*vYWcb27MScl3gzaaeO4xXA.png"/></div></figure><p id="f88a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">总之，L-Softmax引入了一个可调的裕度，它鼓励学习特征的类内紧密性和类间可分性，这可以显著提高各种视觉分类和验证任务的性能。可以使用参数<em class="nd"> m </em>来控制裕量，其中较大的<em class="nd"> m </em>导致较大的决策裕量。L-Softmax还在几个基准数据集上进行了测试，与当时的其他方法相比，它表现出了更好的性能和更低的错误率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/2d4054b34a231a703466eda0714046d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*XnxoIre7xcheZlYxYvRGWw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://arxiv.org/pdf/1612.02295.pdf" rel="noopener ugc nofollow" target="_blank">来源:卷积神经网络的大幅度软最大损失</a></p></figure><h1 id="0230" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">角度软最大值</h1><p id="3978" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated">2018年在论文中介绍了Angular Softmax，<a class="ae ku" href="https://arxiv.org/pdf/1704.08063.pdf" rel="noopener ugc nofollow" target="_blank"> SphereFace:用于人脸识别的深度超球面嵌入</a>。Angular Softmax非常类似于L-Softmax，因为它旨在实现比最小类间距离更小的最大类内距离。然而，它与L-Softmax的不同之处在于将分类器权重<strong class="kx iu"> W、</strong>归一化为1。这导致了在开集人脸识别数据集中性能的提高，在开集人脸识别数据集中会有在训练期间不存在的人脸。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/2e5292c550bdc2a724dfaf1a873734e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*yT_DuEP3_mPZYmtRFP1pXQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">闭集VS开集<a class="ae ku" href="https://arxiv.org/pdf/1704.08063.pdf" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></figure><p id="84d0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如前所述，A-Softmax与L-Softmax非常相似，只是分类器权重<strong class="kx iu"> W </strong>被归一化为1，偏差被设置为0。因此，之前修改的Softmax公式</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/61970ce4ce064a6802cb792e483b9f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*eTzt5cyM8hSx90vFqa3HoQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">修改的Softmax</p></figure><p id="c2c1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">将更改为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/344d59222772451d6cb0758db72bdf95.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*QnrjLRnbPzPc-DYDvTznMw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">具有归一化权重的修改的Softmax</p></figure><p id="5394" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">因此，如果我们使用相同的二进制分类示例。将样品x正确分类为1类的要求将从</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/74bc3657d2141b5ef7417c317fc05a6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*vJGGiWpZ0o0iUBAJF3PbLw.png"/></div></figure><p id="41fe" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/1b2fea317fd7e550a9d5aa21804bd3c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*7eldBtmwp1KsEsyZOeJt1Q.png"/></div></figure><p id="3fad" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">因为|W1| = |W2|，因此|W1||x| = |W2||x|我们可以从等式中抵消W和x。</p><p id="9df9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然后，引入保证金，它将</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/06f0e8b08a535a47957c551f8cb15b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*tRbyHxytEttigphZryvpew.png"/></div></figure><p id="5bd8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">m越大，决策界限越宽。决策边界方程的比较如下表所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/758aeeea86f6c1e05da975fdb14f48a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*HI07u1ADQSlZj-B7_1p3LQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://arxiv.org/pdf/1704.08063.pdf" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></figure><p id="56c2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">因此，A-Softmax损失最终可定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8ab7d892666eacb9532e868bd7b07f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*rWaVibQbHwKh5uljBVb6_Q.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">最大损失</p></figure><p id="8d4b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/855b1a81f1086337115bd046b2146a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*sljCvS9YAyJTLfJEAdDPkA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/519fb7b077f757d632bb738cc5892fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*3-IFQhmxJ5KPL2-x1SADrg.png"/></div></figure><p id="f38c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">A-Softmax的一个优点是它在超球解释中也能很好地渲染。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/16ed203672d248df9c06660cde819460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*8qF89gjU5FRXl290KCBYUA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://arxiv.org/pdf/1704.08063.pdf" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi of"><img src="../Images/86d1dcdea63790a94315de124fcadce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QuKVA2hE6tMcEm5vwnevag.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">超球解释用不同的<em class="og"/><a class="ae ku" href="https://arxiv.org/pdf/1704.08063.pdf" rel="noopener ugc nofollow" target="_blank"><em class="og">【来源】</em> </a></p></figure><p id="0dca" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">总之，A-Softmax对分类权重进行了归一化处理，使偏差为零，并引入了可通过参数<em class="nd"> m </em>进行控制的角裕量，以学习具有区别性的特征，并具有清晰的几何解释，这将在开集数据集中表现得更好，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7b81c2b9f300105e310ebe53c856ad14.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*ZitPorU99-LAj2oV9_l8Ww.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://arxiv.org/pdf/1704.08063.pdf" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></figure><h1 id="9294" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">最大附加余量(AM-Softmax)</h1><p id="79c0" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated">AM-Softmax随后在<a class="ae ku" href="https://arxiv.org/pdf/1801.05599.pdf" rel="noopener ugc nofollow" target="_blank">用于人脸验证的附加余量soft max</a>论文中提出。它采用了不同的方法来增加softmax损失的利润。它不是像L-Softmax和A-Softmax那样将<em class="nd"> m </em>乘以<em class="nd"> θ </em>，而是通过将<em class="nd"> ψ(θ) </em>改为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/51f2238f05bfa9e49d8cb9f3ffdbb244.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*Az_ocAHttCh6QK3wTOqavA.png"/></div></figure><p id="595e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">与L-Softmax和A-Softmax <em class="nd"> ψ(θ) </em>相比，这要简单得多，AM-Softmax的性能也更好。此外，与A-Softmax类似，AM-Softmax也对权重和偏差进行归一化处理，但引入了一个新的超参数<em class="nd"> s </em>来缩放余弦值。最后，AM-Softmax损耗可以定义如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oj"><img src="../Images/e04d352c1dcfabfe7618457034fa218f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*wrvMyo3JupkAPjAAhnZFHw.png"/></div></div></figure><p id="b756" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">因此，决策边界将形成于</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/7977faefe97df92bfb566aad7910156c.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*Z2pJDMieQB7sEw3YCUmZGA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">AM-Softmax判决边界</p></figure><p id="48c2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">其中，在二元分类示例中，P1是类别1的特征，p2是类别2的特征。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/47a0cc02d1495e780ed0bee803339880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*AlJWCdtYAns7AEAl7RQWzQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">AM-Softmax在二进制分类中的直观解释<a class="ae ku" href="https://arxiv.org/abs/1801.05599" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></figure><p id="bae1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了更好地可视化AM-Softmax损失的影响，使用具有时尚MNIST数据集的7层CNN模型将其与其他损失进行比较。CNN模型的3维特征输出被归一化并绘制在如下所示的超球体(球)中。从可视化中，您可以看到AM-Softmax在对输出进行聚类时的性能与SphereFace (A-Softmax)相似，并且随着裕度的增加，m越大，性能越好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi om"><img src="../Images/3aad4897fdce6989bb4b82b32e97cad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sUfemUEAATCDyvm6MMXk8Q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">超球面几何解释<a class="ae ku" href="https://arxiv.org/abs/1801.05599" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></figure><p id="ffe1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">此外，AM-Softmax和其他损失也在开放集数据集上进行测试，使用Casia-WebFace进行训练，使用LFW和MegaFace进行测试，并移除重叠身份。我们可以看到AM-Softmax优于其他损失，特别是在MegaFace数据集上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/82014e90256cfc97027db82a5741f2d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*TJkcRt4vkvbd-74p8Mj47Q.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://arxiv.org/pdf/1801.05599.pdf" rel="noopener ugc nofollow" target="_blank">【Source】:用于人脸验证的附加边距Softmax】</a></p></figure><h1 id="29d7" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">摘要</h1><p id="dc48" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated">总之，L-Softmax、A-Softmax和AM-Softmax损失都试图通过引入Softmax损失的余量来结合分类和度量学习，并旨在最大化类之间的距离和增加相同类之间的紧密性。在这三个模型中，AM-Softmax被证明在模型性能方面提供了最好的提高，特别是在用于人脸验证的LFW和MegaFace数据集上。</p><p id="31e0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后，我要衷心感谢我团队中的所有同事，尤其是阿克毛、易卜拉欣和英·康咪咪，他们回答了我所有的问题，并帮助我理解了这些概念。</p><p id="d130" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">附言:看看我的另一篇关于圈损的文章。我实际上读到了AM-Softmax，因为它被圈损论文引用了。它引入了一个有趣的统一公式，可以根据输入退化为AM-Softmax或三重态损耗。</p><div class="oo op gp gr oq or"><a href="https://medium.com/vitrox-publication/understanding-circle-loss-bdaa576312f7" rel="noopener follow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">暹罗网，三重损失，圆损失解释。</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">理解“圈损失:对相似性优化的统一观点”</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">medium.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf ko or"/></div></div></a></div><h1 id="5fa6" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">参考</h1><p id="2046" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated"><a class="ae ku" href="https://arxiv.org/pdf/1612.02295.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>刘文伟，温，于，杨。卷积神经网络的大幅度软最大损失。国际机器学习会议，第507–516页，2016年</p><p id="3c6f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><a class="ae ku" href="https://arxiv.org/pdf/1704.08063.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>刘文伟，温，俞正声，李，拉杰，宋。用于人脸识别的深度超球面嵌入。2017年IEEE计算机视觉和模式识别会议论文集。</p><p id="e672" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><a class="ae ku" href="https://arxiv.org/pdf/1801.05599.pdf" rel="noopener ugc nofollow" target="_blank">【3】</a>王凤芳，郑俊杰，刘文伟，刘浩辉。用于人脸验证的附加余量softmax。IEEE信号处理快报，25(7):926–930，2018。</p></div></div>    
</body>
</html>