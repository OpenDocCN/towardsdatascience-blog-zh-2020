<html>
<head>
<title>ScrabbleGAN — Adversarial Generation of Handwritten Text Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ScrabbleGAN——对手写文本图像的生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scrabblegan-adversarial-generation-of-handwritten-text-images-628f8edcfeed?source=collection_archive---------20-----------------------#2020-05-11">https://towardsdatascience.com/scrabblegan-adversarial-generation-of-handwritten-text-images-628f8edcfeed?source=collection_archive---------20-----------------------#2020-05-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="23b7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">文本生成教程(TensorFlow 2)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5a9c776bc635ac1006d6db6ff41754a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WsFF-Mx-XJemtkwBD1l3KQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@moroo" rel="noopener ugc nofollow" target="_blank">莫里茨·施米特</a>在<a class="ae ky" href="https://unsplash.com/photos/UsTERdf7yEY" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="d4fa" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak"> 1。动机</strong></h1><p id="9114" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本文介绍了一个用于对手写文本图像生成的<a class="ae ky" href="https://github.com/Nikolai10/scrabble-gan" rel="noopener ugc nofollow" target="_blank">开源项目</a>，它建立在【1，2】中提出的思想之上，并利用了生成对手网络(GANs【3】)的力量。尽管GANs(特别是cgan[4]和变体)在研究社区中受到了很多关注，但到目前为止，很少有人关注基于序列的图像生成(不要与文本到图像学习任务(如[5]或[6])相混淆)。此外，最近的工作([1，2])没有共享它们的源代码/评估代码，这使得清晰的比较变得困难，因为并不总是清楚哪个组件导致了改进。在这项工作中，模型/软件在<a class="ae ky" href="http://www.fki.inf.unibe.ch/databases/iam-handwriting-database" rel="noopener ugc nofollow" target="_blank"> IAM手写数据库</a>上进行评估，并提供给社区进行进一步实验。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/9bc106934ebdb37aa6cb40884617acc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*VJgCUPL5wslVNfILpisFaQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ScrabbleGAN:第1和第15纪元之间的训练进度(IAM手写数据库)</p></figure><p id="df5a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu"> <em class="mt">注:</em> </strong> <em class="mt">本文不是为了解释最先进的机器学习算法，而是为了突出这个项目中所做的设计决策。技术细节在</em><a class="ae ky" href="https://github.com/Nikolai10/scrabble-gan" rel="noopener ugc nofollow" target="_blank"><em class="mt">GitHub repo</em></a><em class="mt">中有很好的记录，可以在各自的参考文献中找到。</em></p><h1 id="8921" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">2.背景/相关性</h1><p id="21f7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在许多机构(如医疗保健或金融机构)中，手写仍然是数据收集的主要方法。虽然光学字符识别(OCR)现在通常被认为已经解决了，但是(离线)手写识别(HWR)仍然具有挑战性，因此使自动数据处理变得复杂。与印刷文本相比，手写在风格上有很大不同(众所周知，手写就像指纹一样独一无二，不会弄错)，因此需要大规模的训练数据。因此，由于GANs的流行和成功，越来越多的工作集中在扩展现有数据集上。在接下来的部分中，介绍了两篇最近的论文(本文非常依赖于它们)。</p><p id="5563" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在[1]中，作者训练了一个辅助分类器GAN (AC-GAN [7])，以便扩展两个基准数据集(法语、阿拉伯语)。这里的主要思想是将每个单词嵌入到一个128维的特征向量中(类似于[5])，然后将该向量馈入BigGAN [8]网络架构。为了控制生成的图像的文本内容，训练CRNN [9]作为辅助分类器。源自R和D的梯度被平衡(考虑到它们不同的幅度),并最终反向传播到G:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/bc1c20ebe2ea494a8875568490f8f075.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*5ZY6NuVEcbiAhchHaC6YRg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/pdf/1903.00277.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="4a4b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">尽管他们的工作显示出有希望的结果，但是仍然存在一些限制，这些限制在[2]中进行了阐述:</p><ul class=""><li id="f491" class="mv mw it lt b lu mo lx mp ma mx me my mi mz mm na nb nc nd bi translated"><strong class="lt iu">生成任意长度的word图像:</strong> <br/>在[1]中，所有图像都被(白色)填充到相等的大小，同时保持原始的纵横比——超过给定阈值(128×512)的图像被移除。因此，在所有的单词长度上，只能生成相同大小的单词图像。在[2]中，G、D和R都遵循完全卷积范式，这潜在地允许生成完整的句子。</li><li id="d64f" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated"><strong class="lt iu">解开生成过程/嵌入:<br/> </strong>不是像[1]中那样从整个单词表示中生成图像，而是单独生成每个字符(因此<em class="mt"> Scrabble </em>)，使用CNN的重叠感受域属性来考虑附近字母的影响。这主要是因为手写是一个局部过程，每个字母只受其前一个和后一个字母的影响。因此，G (D)可以被视为相同的类条件生成器(真实/虚假分类器)的串联:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/7f2ba9d3ccc5bb3fd5d4458892028109.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*203XZJo3QLRk5wUjifGPkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/pdf/2003.10557.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><ul class=""><li id="10b7" class="mv mw it lt b lu mo lx mp ma mx me my mi mz mm na nb nc nd bi translated"><strong class="lt iu">去除R中的循环头<br/> </strong>作者[2]发现，更好的R不一定带来更好的整体(代)性能。这是因为CRNN网络中的RNN层学习隐含的语言模型，从而能够识别正确的字符，即使它没有被清楚地书写。因此，为了迫使R仅基于视觉特征做出决策，移除了递归头部。</li></ul><h1 id="73e5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">3.方法学</h1><p id="bc08" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这项工作中，使用了<a class="ae ky" href="https://www.tensorflow.org/guide/keras/functional" rel="noopener ugc nofollow" target="_blank">功能API </a>和<a class="ae ky" href="https://www.tensorflow.org/tutorials/customization/custom_training" rel="noopener ugc nofollow" target="_blank">定制训练循环</a>。大致结构基于TensorFlow教程关于<a class="ae ky" href="https://www.tensorflow.org/tutorials/generative/dcgan" rel="noopener ugc nofollow" target="_blank">深度卷积生成对抗网络</a>；下一节重点介绍了主要区别。</p><h2 id="da94" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">加载并准备数据集</h2><p id="1471" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">首先，IAM手写(单词)数据库被转换成以下格式:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="35e0" class="nk la it nx b gy ob oc l od oe"><em class="mt">res/data/iamDB/words-Reading/<br/>    ├── Bucket 1<br/>        ├── img_i.png<br/>        ├── img_i.txt<br/>        ├── ...<br/>    ├── Bucket 2<br/>        ├── ...<br/>    ├── Bucket n</em></span></pre><p id="dffd" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">其中<em class="mt">桶x </em>保存转录长度为<em class="mt"> x </em>的所有样本，并且<em class="mt"> img_i.txt </em>包含<em class="mt"> img_i.png. </em>的基本事实注释(转录)</p><p id="9cf0" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">为了减少噪声，在本工作中只考虑正确分割的单词([a-zA-Z])(<a class="ae ky" href="http://www.fki.inf.unibe.ch/DBs/iamDB/data/ascii/words.txt" rel="noopener ugc nofollow" target="_blank">表示为“ok”</a>)。此外，大小为&gt; <em class="mt">的字bucket_size ( </em>此处为10 <em class="mt"> ) </em>被忽略，以断言len(bucket)&gt;&gt;batch _ size。总之，这种预处理策略产生了80.377个独特的训练样本。</p><p id="ed28" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">所有图像都被重新整形，以匹配[2]: (32，16)提出的每个字符的纵横比。例如，转录长度为10的单词具有形状(32，160) px。</p><p id="fddc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">最后，与<a class="ae ky" href="https://www.tensorflow.org/tutorials/generative/dcgan" rel="noopener ugc nofollow" target="_blank"> DCGAN-Tutorial </a>类似，图像被规范化为[-1，1]，标签通过char_vector(此处为:“abc…zA…Z”)进行编码:例如，“auto”变成[0，20，19，14]。</p><p id="2102" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在每个训练步骤，一批随机单词沿着其编码的转录(相同随机桶的)被产生给相应的模型。</p><h2 id="74b9" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">创建模型</h2><p id="440e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">从根本上说，ScrabbleGAN是著名的BigGAN和CRNN架构的混合体。尽管有一些BigGAN的开源实现(例如<a class="ae ky" href="https://github.com/ajbrock/BigGAN-PyTorch" rel="noopener ugc nofollow" target="_blank"> BigGAN-Pytorch </a>，<a class="ae ky" href="https://github.com/google/compare_gan/blob/master/compare_gan/architectures/resnet_biggan.py" rel="noopener ugc nofollow" target="_blank"> resnet_biggan </a>)，但据我所知，目前还没有“官方”的TF2实现。因此，我决定自己在Tf2中重写一个简化版的<a class="ae ky" href="https://github.com/google/compare_gan" rel="noopener ugc nofollow" target="_blank"> google/compare_gan </a>。与原始作品相似，代码可通过<a class="ae ky" href="https://github.com/google/gin-config" rel="noopener ugc nofollow" target="_blank"> Gin </a>进行配置:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="3e59" class="nk la it nx b gy ob oc l od oe">...<br/>setup_optimizer.r_lr = 2E-4<br/>setup_optimizer.beta_1 = 0.0<br/>setup_optimizer.beta_2 = 0.999<br/>setup_optimizer.loss_fn = @hinge<br/>setup_optimizer.disc_iters=1<br/>...<br/>shared_specs.kernel_reg = @spectral_norm<br/>...<br/>shared_specs.g_bw_attention = 'B3'              <br/>shared_specs.d_bw_attention = 'B1'<br/>...<br/>shared_specs.embed_y = (32, 8192)</span></pre><p id="1d14" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">CRNN-模型基于<a class="ae ky" href="https://github.com/Nikolai10/crnn-tf2" rel="noopener ugc nofollow" target="_blank"> CRNN-Tf2 </a>。所有模型都是利用第2节所述的完全卷积范例实现的，以允许灵活的输入和输出形状。最终的“ScrabbleGAN”模型是通过将这些模型堆叠在一起而获得的(复合模型)。</p><p id="6bc7" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">为了简洁起见，感兴趣的读者可以参考<a class="ae ky" href="https://github.com/Nikolai10/scrabble-gan" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>了解更多信息。</p><h2 id="3f37" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">定义损失和优化器</h2><p id="2601" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这项工作中，基于<a class="ae ky" href="https://github.com/google/compare_gan/blob/master/compare_gan/gans/loss_lib.py" rel="noopener ugc nofollow" target="_blank">compare _ gan/gans/loss _ lib . py</a>，考虑了“非饱和”和“铰链”损耗。对于CRNN部分，照常使用<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/backend/ctc_batch_cost" rel="noopener ugc nofollow" target="_blank"> CTC损耗</a>。</p><h2 id="d589" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">定义训练循环</h2><p id="f087" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">训练循环从生成器接收一组随机种子和来自随机单词列表(这里:<a class="ae ky" href="https://superuser.com/questions/136236/where-on-disk-does-the-macos-system-wide-dictionary-store-the-list-of-words-i-ad" rel="noopener ugc nofollow" target="_blank"> macOS单词列表</a>)的转录作为输入开始，以产生一组假图像。D(实数)和R(实数)是分开计算的，而D(虚数)和R(虚数)现在是通过复合模型计算的。然后为这些模型中的每一个计算损失，并且梯度被用于更新鉴别器、识别器和复合模型。例如，后者更新如下:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="b646" class="nk la it nx b gy ob oc l od oe">recognizer.trainable = <strong class="nx iu">False<br/></strong>discriminator.trainable = <strong class="nx iu">False<br/></strong>gradients_of_generator = gen_tape.gradient(g_loss_balanced, composite_gan.trainable_variables)<br/>generator_optimizer.apply_gradients(zip(gradients_of_generator, composite_gan.trainable_variables))</span></pre><blockquote class="of og oh"><p id="5b62" class="lr ls mt lt b lu mo ju lw lx mp jx lz oi mq mc md oj mr mg mh ok ms mk ml mm im bi translated">注意:在这项工作中，没有梯度平衡导致最好的结果(更多信息可以在各自的参考文献[1，2]中找到)。)</p></blockquote><p id="dd05" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">与[1，2]类似，R只在真实图像上训练，以防止它学习如何识别生成的(和潜在的虚假)图像。</p><h1 id="ce8c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">4.结果</h1><p id="a5e5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">所有实验(不同的模型配置)都是使用<a class="ae ky" href="https://aws.amazon.com/de/ec2/instance-types/p3/" rel="noopener ugc nofollow" target="_blank">AWS p 3.2x大型实例</a>进行的；每个模型被训练15个时期。以下是一些直观的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/915f6c9a4a4571ba96a07539f639d05c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Oy13ewQPvtuFud7a_nQc9g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">与[2]的比较:来自电影《欢乐满人间》的单词“supercalifragilisticepialidocious”(34个字母)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/28443632767a87beb9a51dbc9311d1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*fQ3DfQK-AK_eBgj5LcyW0A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更多的例子</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/64bac854f7d3f7ab0eed4cbd404c1c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*NdFiA4IP-vShed5xVYi34g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更多例子II</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/e717a77eb1fcd8f1e3a96cc1f1011c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*EEaRWF75_SXfOEwGYEjTcw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更多例子III</p></figure><h2 id="0201" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">参考</h2><ol class=""><li id="f285" class="mv mw it lt b lu lv lx ly ma ol me om mi on mm oo nb nc nd bi translated">E.Alonso，B. Moysset，R. Messina，<a class="ae ky" href="https://arxiv.org/pdf/1903.00277.pdf" rel="noopener ugc nofollow" target="_blank">基于序列的手写文本图像对抗生成</a> (2019)，arXiv:1903.00277</li><li id="5c46" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm oo nb nc nd bi translated">南Fogel，H. Averbuch-Elor，S. Cohen，S. Mazor，R. Litman，<a class="ae ky" href="https://arxiv.org/pdf/2003.10557.pdf" rel="noopener ugc nofollow" target="_blank"> ScrabbleGAN:半监督变长手写文本生成</a> (2020)，arXiv:2003.10557</li><li id="d907" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm oo nb nc nd bi translated">I. J. Goodfellow，J. Pouget-Abadie，M. Mirza，B. Xu，D. Warde-Farley，S. Ozair，a .，Y. Bengio，<a class="ae ky" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">生成性对抗性网络</a> (2014)，arXiv:1406.2661</li><li id="909c" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm oo nb nc nd bi translated">米（meter的缩写））米尔扎，s .奥辛德罗，<a class="ae ky" href="https://arxiv.org/abs/1411.1784" rel="noopener ugc nofollow" target="_blank">条件生成对抗网</a> (2014)，arXiv:1411.1784</li><li id="4891" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm oo nb nc nd bi translated">南Reed，Z. Akata，X. Yan，L. Logeswaran，B. Schiele，H. Lee，<a class="ae ky" href="https://arxiv.org/pdf/1605.05396.pdf" rel="noopener ugc nofollow" target="_blank">生成性对抗性文本到图像合成</a> (2016)，arXiv:1605.05396</li><li id="d5d9" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm oo nb nc nd bi translated">T.乔，张，徐，陶，<a class="ae ky" href="https://arxiv.org/abs/1903.05854" rel="noopener ugc nofollow" target="_blank"> MirrorGAN:通过重新描述学习文本到图像的生成</a> (2019)，arXiv:1903.05854</li><li id="9b8e" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm oo nb nc nd bi translated">A.Odena，C. Olah，J. Shlens，<a class="ae ky" href="https://arxiv.org/pdf/1610.09585.pdf" rel="noopener ugc nofollow" target="_blank">使用辅助分类器的条件图像合成GANs </a> (2016)，arXiv:1610.09585</li><li id="5c78" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm oo nb nc nd bi translated">A.Brock，J. Donahue，K. Simonyan，<a class="ae ky" href="https://arxiv.org/abs/1809.11096" rel="noopener ugc nofollow" target="_blank">用于高保真自然图像合成的大规模GAN训练</a> (2018)，arXiv:1809.11096</li><li id="2df9" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm oo nb nc nd bi translated">B.石，x .白，c .姚，<a class="ae ky" href="https://arxiv.org/pdf/1507.05717.pdf" rel="noopener ugc nofollow" target="_blank">基于图像序列识别的端到端可训练神经网络及其在场景文本识别中的应用</a> (2015)，arXiv:1507.05717</li></ol></div></div>    
</body>
</html>