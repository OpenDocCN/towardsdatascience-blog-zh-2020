<html>
<head>
<title>Translational Invariance Vs Translational Equivariance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">平移不变性与平移等价性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/translational-invariance-vs-translational-equivariance-f9fbc8fca63a?source=collection_archive---------3-----------------------#2020-01-17">https://towardsdatascience.com/translational-invariance-vs-translational-equivariance-f9fbc8fca63a?source=collection_archive---------3-----------------------#2020-01-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a0c3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">平移不变性和平移等方差经常被混淆为同一事物，但它们是 CNN 的不同属性。要了解不同之处，请阅读下文。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1057086edf373044c335d958d19d8ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TH5N8CGX5tGKImmT.png"/></div></div></figure><p id="64d5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">卷积神经网络已经成为基于图像和视频的任务(如分类、定位、分割等)的首选架构。他们在以前被认为使用基本图像处理技术很难完成的任务中表现出了超人的水平。它使分类任务变得相对容易执行，而不需要像 CNN 变革之前那样向模型输入手动精选的特征。</p><p id="7ddb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">卷积神经网络的灵感来自诺贝尔奖得主科学家 Hubel 博士和 Wiesel 博士的工作，他们展示了大脑中视觉皮层的工作。他们在一只部分麻醉的猫的视觉皮层中插入微电极，使她不能移动，并在它的视网膜上移动一条亮线。在这个实验中，他们注意到了以下场景:</p><ol class=""><li id="c3e2" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated">当这条线位于视网膜上的特定位置时，神经元就会放电。</li><li id="a9f3" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">神经元的活动根据线的方向而变化。</li><li id="ede7" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">神经元有时只有在线路向特定方向移动时才会放电。</li></ol><p id="4c1e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个经典实验展示了视觉皮层是如何以一种分层的方式处理信息，提取越来越复杂的信息。他们表明，在视觉皮层中有一个代表视野的<strong class="kw iu">地形图</strong>，<strong class="kw iu">，附近的细胞在这里处理来自附近视野的信息。</strong>这给了 CNN 的<strong class="kw iu">稀疏交互</strong>的概念，在这里，网络聚焦于本地信息，而不是获取完整的全球信息。这一特性使得 CNN 在图像相关任务中提供了最先进的性能，因为在图像中，附近的像素比远处的像素具有更强的相关性。此外，他们的工作确定了视觉皮层中的神经元以精确的结构排列。具有相似功能的细胞被组织成<strong class="kw iu">列</strong>，微小的计算机器将信息传递到大脑的更高区域，在那里形成视觉图像。这类似于 CNN 架构的设计方式，其中较低层提取边缘和其他公共特征，而较高层提取更多特定于类别的信息。总之，他们的工作揭示了视觉皮层神经元如何对图像特征进行编码，图像特征是帮助我们建立对周围世界的感知的物体的基本属性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi me"><img src="../Images/ed8ccb54d7aba390ad6615e2fe2c78c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CX1szohzfrU8AN0f.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><strong class="bd mj">图 1 </strong> : Hubel 和 Weisel 的实验直观解释。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/e8df4057a3915876c63aaf33d9c189bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*GRns6U6t6jNF8QpKTdozHA.jpeg"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><strong class="bd mj">图 2: </strong></p></figure><p id="3b80" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">卷积神经网络提供了优于传统全连接层的三个基本优点。首先，它们具有稀疏连接，而不是完全连接的连接，这导致参数减少，并使 CNN 有效地处理高维数据。第二，权重共享发生在相同的权重在整个图像上共享的情况下，导致减少的存储器需求以及平移等变(稍后将解释)。第三，CNN 使用了一个非常重要的二次采样或汇集的概念，其中最突出的像素传播到下一层，丢弃其余的像素。这提供了固定大小的输出矩阵，这通常是分类和对平移、旋转<strong class="kw iu">的不变性所需要的。</strong></p><h2 id="2678" class="ml mm it bd mn mo mp dn mq mr ms dp mt ld mu mv mw lh mx my mz ll na nb nc nd bi translated"><strong class="ak">平移等方差:</strong></h2><p id="ef07" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">平移等方差或仅等方差是卷积神经网络的一个非常重要的属性，在卷积神经网络中，图像中对象的位置不应该是固定的，以便它被 CNN 检测到。这仅仅意味着如果输入改变，输出也会改变。准确地说，如果 f(g(x)) = g(f(x))，则称函数 f(x)与函数 g 等变。如果我们有一个函数 g，它将图像的每个像素向右移动一个像素，即 I'(x，y) = I(x-1，y)。如果我们对图像应用变换 g，然后应用卷积，结果将与我们对 I '应用卷积，然后对输出应用平移 g 的结果相同。当处理图像时，这仅仅意味着如果我们将输入向右移动 1 个像素，那么它的表示也将向右移动 1 个像素。<br/>CNN 的平移等价性是通过权重共享的概念实现的。由于在图像之间共享相同的权重，因此，如果对象出现在任何图像中，则不管它在图像中的位置如何，它都将被检测到。该属性对于诸如图像分类、对象检测等应用非常有用，在这些应用中，对象可能多次出现或者对象可能处于运动中。</p><p id="a562" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">卷积神经网络对于某些其他变换，如图像的缩放或旋转变化，并不是自然等变的。需要其他机制来处理这种转换。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/8d741bf955deea356719bef3422c6ae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0Y8LuG5wPnzO6W3n.jpeg"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><strong class="bd mj">图 3 </strong>。由于翻译等变性质，检测到各种 cat 实例。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/3b3f83ec48404d0808a291eccb2ca0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*YLXu4uUvTSA4Kfo6.jpg"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><strong class="bd mj">图 4。</strong>在同一帧中检测多只狗时看到的平移等方差特性。</p></figure><h2 id="fe49" class="ml mm it bd mn mo mp dn mq mr ms dp mt ld mu mv mw lh mx my mz ll na nb nc nd bi translated">平移不变性:</h2><p id="1f28" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">平移不变性经常与平移等价性混淆，许多人，甚至是专家都分不清两者的区别。<br/>平移不变性使 CNN 具有平移不变性。平移不变性意味着，如果我们平移输入，CNN 仍然能够检测输入所属的类别。<br/>平移不变性是汇集操作的结果。在传统的 CNN 架构中，有三个阶段。在第一阶段，该层对输入执行卷积运算以给出线性激活。在第二阶段，产生的激活通过非线性激活函数，例如 sigmoid、tanh 或 relu。在第三阶段，我们执行池操作来进一步修改输出。<br/>在池化操作中，我们用附近输出的汇总统计来替换某个位置的 convnet 的输出，例如最大池化情况下的最大值。因为在最大池化的情况下，我们用最大值替换输出，因此即使我们稍微改变输入，也不会影响大多数池化输出的值。在不需要物体的精确位置的情况下，平移不变性是一个有用的属性。例如，如果你正在建立一个检测人脸的模型，你需要检测的只是眼睛是否存在，它的确切位置是不必要的。而在分割任务中，需要精确的位置。<br/>池化的使用可以被视为添加了一个强先验，即该层学习的函数必须对平移不变。当先验正确时，可以大大提高网络的统计效率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/5e8817a3e7d8a7b6674b47ac5e069f93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/0*nV_o47J-bsA3ev5O.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><strong class="bd mj">图五。</strong>显示平移不变性，尽管输入向右移动，但表示也移动。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/e6652ee99566c2acf24363476a0950cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NoAQ4ZgofpkK6esl4sMHkA.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><strong class="bd mj">图六。</strong>平移不变性的例子</p></figure><p id="ef42" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">平移不变性和平移等方差的属性在一种称为数据扩充的技术中得到部分利用，当我们拥有较少的训练数据或希望在更丰富的数据集上进行模型训练时，这种技术会很方便。在数据扩充中，我们对从训练集中随机采样的每批数据应用不同的变换，如旋转、翻转、缩放、平移等，并将其提供给模型，以使其对变换更加鲁棒并提高性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/b9b5bbb97077039877979a5827595ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/0*Nuxo8QIdwdYJxIsU.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><strong class="bd mj">图 6: </strong>所示的数据增强技术</p></figure><h2 id="06a1" class="ml mm it bd mn mo mp dn mq mr ms dp mt ld mu mv mw lh mx my mz ll na nb nc nd bi translated">结论:</h2><p id="97e9" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">卷积神经网络正在解决以前被认为无法解决的各种挑战，并且在大多数情况下，击败了人类水平的性能，正如在 ImageNet challenge 中看到的那样，Resnet 比人类表现得更好。使 CNN 如此伟大的概念并不复杂，但非常直观，有逻辑性，易于理解。<br/>我希望你喜欢这篇文章，如果你有任何疑问、建议或要求，请在下面留下你的评论，或者通过<a class="ae no" href="https://twitter.com/Perceptron97" rel="noopener ugc nofollow" target="_blank"> twitter </a>或<a class="ae no" href="https://www.linkedin.com/in/divyanshu-mishra-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。</p><h2 id="c284" class="ml mm it bd mn mo mp dn mq mr ms dp mt ld mu mv mw lh mx my mz ll na nb nc nd bi translated">参考资料:</h2><ol class=""><li id="cb2d" class="lq lr it kw b kx ne la nf ld np lh nq ll nr lp lv lw lx ly bi translated">伊恩·古德菲勒、约舒阿·本吉奥和亚伦·库维尔的深度学习书籍。</li><li id="2f9a" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">人类视觉系统架构图 2 取自 knowingneurons.com。</li><li id="e89d" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">为什么等方差优于过早不变性 Geoffrey Hinton(图 4)。</li><li id="caff" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">图 6 来自 itutorials.com。</li></ol></div></div>    
</body>
</html>