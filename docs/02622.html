<html>
<head>
<title>Diving Deeper into Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/diving-deeper-into-linear-regression-81adaa7b79e5?source=collection_archive---------18-----------------------#2020-03-13">https://towardsdatascience.com/diving-deeper-into-linear-regression-81adaa7b79e5?source=collection_archive---------18-----------------------#2020-03-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ab3c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">OLS，山脊，拉索和超越…</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/15f4a2fb3cf24ace552072fe787d42d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bdjT8V4U-DS4ZAbcKml6qA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@artemverbo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Artem Verbo </a>在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="7010" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> W </span>当我提到“线性回归”时，大多数人都会想到传统的普通最小二乘法 (OLS)回归。如果你不熟悉这个术语，这些等式可能会有帮助…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/899c0d092df4a391b2c98fcd378af1ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*W2U1Punyiw9c-doIiEYgAA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">β_1、β_2:权重；β_0:偏倚；J(β):成本函数</p></figure><p id="4f95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你也想到 OLS 了吗？如果是，那么你就在正确的轨道上。但是线性回归不仅仅是 OLS！首先，让我们更仔细地看看 OLS。</p><h1 id="6573" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">内源性阿片样物质</h1><p id="0765" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">这种技术的名字来自于成本函数。在这里，我们取误差平方和(基础事实和预测之间的差异),并试图将其最小化。通过最小化成本函数，我们获得了向量β(包含偏差和权重)的最优值。在下图中，显示了成本函数的等高线(同心椭圆)。最小化后，我们得到β作为中心点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/93415889422886307e3ab25d3cfaa039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*PlJSFQFVJGMMM6xAyrPfng.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">内源性阿片样物质</p></figure><p id="0926" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">起初，OLS 似乎足以解决任何回归问题。但是随着我们增加特征的数量和数据的复杂性，OLS 倾向于过度拟合训练数据。过度拟合的概念非常广泛，值得单独写一篇文章(你可以找到很多这样的例子)，所以我会给你一个简要的介绍。过度拟合意味着模型已经很好地学习了训练数据，以至于无法进行概括。换句话说，该模型已经学习了训练数据中即使是小规模(不显著)的变化，因此它不能对看不见的(验证和测试)数据产生良好的预测。为了解决过度拟合的问题，我们可以使用许多技术。在我们的成本函数中增加一个正则化项就是这样一种技术。但是我们应该用什么术语呢？我们一般使用以下两种方法之一。</p><h1 id="4761" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">山脉</h1><p id="913c" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在这种情况下，我们将权重的<strong class="lb iu">平方之和添加到我们的最小平方成本函数中。所以现在看起来像这样…</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/395fd6ea072022701186e90e5026a68d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*CMy-1w3X7JiwQgd2iFj5BQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">m:β的 1+维；λ:正则化参数</p></figure><p id="f274" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是这个术语如何防止过度拟合呢？增加这一项相当于对β的可能值增加了一个额外的约束。因为要达到最小成本，β _j 的总和一定不能超过某个值(比如 r)。这种技术可以防止模型对某些特征赋予比其他特征更大的权重，从而解决过度拟合问题。数学上，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/48a08b812cc9e0fa7348fc74f6781ba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/format:webp/1*fufWiVqFKUxjd1q2YLZR5A.png"/></div></figure><p id="9fb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，β应该位于以原点为圆心，半径为√r 的圆内(或上)。这是可视化效果…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/e920d91560731840fe84b68fa24617e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*V_LYzEdMEYDc69oqmtIdJA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">山脉</p></figure><p id="c103" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，由于存在约束(红色圆圈)，β的最终值比在 OLS 中更接近原点。</p><h1 id="78dc" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">套索</h1><p id="c8ab" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">山脊和套索之间的唯一区别是正则化项。这里，我们将权重的绝对值的和添加到我们的最小二乘成本函数中。所以成本函数变成了…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/f9e6699be6e23235af4c8d5be53072da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*TEtN2EegbEyD2vhVP_mTSA.png"/></div></figure><p id="a2f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，约束可以写成…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/bcc949aaabec70379b37609245050042.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*wm-8PIX-WlxQ1H1NmCk53Q.png"/></div></figure><p id="d5d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以把约束想象成正方形而不是圆形。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d4bca7329af69c24aa9500055defd1cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*8hdFNL60miMgNFnNS26JcQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">套索</p></figure><p id="04f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得注意的是，如果轮廓碰到正方形的一个角，则一个特征被完全忽略(权重变为 0)。对于高维特征空间，我们可以用这个技巧<strong class="lb iu">减少特征的数量</strong>。</p><blockquote class="nj nk nl"><p id="3ea7" class="kz la nm lb b lc ld ju le lf lg jx lh nn lj lk ll no ln lo lp np lr ls lt lu im bi translated">注意:在正则化项中，我们不使用偏差(β_0 ),因为只有对应于特征的非常大的权重(β_i，对于 i&gt;0)对过拟合有贡献。偏差项只是一个截距，因此与过拟合没有太大关系。</p></blockquote><p id="d8a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">唷…这是很多关于正规化。上述方法的共同点是:它们的成本函数中都有残差/误差(地面真实预测)。这些误差平行于 y 轴。我们也可以考虑沿 x 轴的误差，并以类似方式进行。见下图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/9671141af53a1387ca471d7d72a54dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GZSRwUZJKZ4T7rD7pyAi3Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">y 误差和 x 误差</p></figure><p id="547a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们使用一种不同的误差呢？</p><h1 id="11db" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">主轴(正交)回归</h1><p id="bc54" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在这种情况下，我们考虑<strong class="lb iu">两个方向</strong> (x 轴和 y 轴)的误差。观察数据点和预测线之间的垂直距离的平方和将被最小化。让我们通过仅取一个特征来形象化这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/574c7a4e6ec52e167264bc9a75255665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sLS_1KCli7wnHFOIwOGJNw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(X_i，Y_i):从最佳拟合线上的(x_i，y_i)绘制的垂线的脚</p></figure><p id="807f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们的模型</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/16ae97aee201a9364aa9b4ef0f645538.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*HE_oMu8GYdwq8caJLabUHw.png"/></div></div></figure><p id="e509" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后通过最小化可以获得回归系数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/cf1afba43fbf4c51c0663238c84f208e.png" data-original-src="https://miro.medium.com/v2/resize:fit:150/format:webp/1*H-cQNp6MX38ep0LQCJ-oMw.png"/></div></figure><p id="cfea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在约束下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/3ccd8b395e2ce9b429ed320def6e8110.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*AlasHF0VKWaqoD2qQSBnNQ.png"/></div></figure><h1 id="78cc" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">简化长轴回归</h1><p id="05da" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">这与上面的方法非常相似，只是略有变化。这里，我们最小化由(X_i，Y_i)和(x_i，y_i)形成的矩形的面积之和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/467d35e079fb1d7ee7808b8edbbaf33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*ftqbfZFEcnYhlgcuvZ7onA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">缩减长轴</p></figure><p id="46ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由 n 个数据点扩展的总面积是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e9edb62972d933e954624a1076350768.png" data-original-src="https://miro.medium.com/v2/resize:fit:158/format:webp/1*sTd6k_Y4-68aBrJq0PDfjA.png"/></div></figure><p id="9cd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的约束条件与正交回归相同。</p><h2 id="760f" class="nx mg it bd mh ny nz dn ml oa ob dp mp li oc od mr lm oe of mt lq og oh mv oi bi translated">什么时候应该使用正交回归？</h2><blockquote class="nj nk nl"><p id="f672" class="kz la nm lb b lc ld ju le lf lg jx lh nn lj lk ll no ln lo lp np lr ls lt lu im bi translated">当研究变量(y)和解释变量(x)都存在不确定性时，应该选择正交和简化的主轴回归。</p></blockquote><p id="687e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正交回归中有趣的一点是，它会产生对称的 y 误差和 x 误差。但是在 OLS，我们没有得到对称，因为我们要么最小化 y 误差，要么最小化 x 误差，而不是两者都最小化。</p><h2 id="57dc" class="nx mg it bd mh ny nz dn ml oa ob dp mp li oc od mr lm oe of mt lq og oh mv oi bi translated">还好奇？看一个我最近做的视频…</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="c224" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢阅读。下次见…学习愉快！</p></div></div>    
</body>
</html>