<html>
<head>
<title>Three aspects of Deep RL: noise, overestimation and exploration</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深层 RL 的三个方面:噪声、高估和勘探</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/three-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b?source=collection_archive---------23-----------------------#2020-05-01">https://towardsdatascience.com/three-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b?source=collection_archive---------23-----------------------#2020-05-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2b8c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">噪音的两面。噪音是有害的，它会导致系统性的高估。然而，噪声可能是有用的，例如用于探索的噪声。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0703b95531067134aaa0fa5d2d37da1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IuisMVfrVo8E99E--nAn0A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">资料来源:123rf.com</p></figure><p id="3bd0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<em class="lu">深度强化学习</em>模型中，我们触及了噪声的各个方面。第一部分讨论<em class="lu">高估</em>，即噪声产生的有害属性。第二部分处理用于<em class="lu">探索</em>的噪声，这是有用的噪声。在附录中，我们将再看一个噪声的例子:<em class="lu">自适应噪声</em>。</p><p id="77fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">第一部分</em>。我们将看到研究人员如何试图克服模型中的高估。第一步是将<em class="lu">动作选择</em>与<em class="lu">动作评估分离。</em>在<em class="lu">双 DQN </em>中实现。第二步与<strong class="la iu">行动者-批评家架构相关:<em class="lu"> </em> </strong>这里我们将<em class="lu">价值神经网络</em>(批评家)与<em class="lu">策略神经网络(行动者)解耦。</em> <a class="ae lv" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> DDPG </em> </a>和<a class="ae lv" href="https://arxiv.org/abs/1802.09477" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> TD3 </em> </a>都采用这种架构。</p><p id="1e53" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">第二部分</em>。探索是学习的一大挑战。<strong class="la iu">的主要问题是<em class="lu">的探索噪音。</em> </strong>我们涉及的车型有<em class="lu"> DQN </em>、<em class="lu">双 DQN、</em>T42、和<em class="lu"> TD3 </em>。使用一些噪声参数的神经网络模型具有更多的探索能力，并且在<em class="lu">深度 RL </em>算法中更加成功。</p><p id="4df9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">附录</em>。我们考虑简单的无梯度算法<em class="lu">爬山</em>。该算法将<strong class="la iu"> <em class="lu">自适应噪声</em> </strong>直接添加到输入变量中，即添加到确定神经网络的<em class="lu">权重矩阵</em>中。</p><h1 id="7177" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">第一部分。努力克服高估</h1><p id="d77a" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated"><em class="lu"> DQN </em>和<em class="lu">双 DQN </em>算法<em class="lu"> </em>在离散动作空间的情况下证明是非常成功的。然而，已知这些算法遭受高估。这种有害的属性比低估要糟糕得多，因为低估是不会累积的。让我们看看研究人员是如何试图克服高估的。</p><p id="8fb3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">高估在<em class="lu">DQN</em>T69】。</strong></p><p id="462f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">问题是在计算目标值<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">Gt</em></strong></code> <em class="lu">时使用了最大化运算符。</em>假设<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">Q</em>(<em class="lu">S</em>_{<em class="lu">t</em>+1<em class="lu">}</em>, <em class="lu">a</em>)</strong></code>的评估值已经被高估。然后从 DQN 关键方程(见下文)中，代理观察到<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">Q</em>(<em class="lu">S</em>_<em class="lu">t</em>, <em class="lu">a</em>)</strong></code> <strong class="la iu"> </strong>的误差也会累积。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/a53815567a12d712f960ac94ac6423ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XgrA9OoIEawFqEy13C7Ffw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd my"> DQN 关键方程 Q(s_t，a_t) </strong></p></figure><p id="69c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，<code class="fe mt mu mv mw b"><a class="ae lv" rel="noopener" target="_blank" href="/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a"><strong class="la iu"><em class="lu">Rt</em></strong></a></code> <a class="ae lv" rel="noopener" target="_blank" href="/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a">是当时的奖励</a><code class="fe mt mu mv mw b"><a class="ae lv" rel="noopener" target="_blank" href="/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a"><strong class="la iu"><em class="lu">t</em></strong></a><strong class="la iu"><em class="lu">;</em></strong></code><code class="fe mt mu mv mw b"><a class="ae lv" rel="noopener" target="_blank" href="/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a"><strong class="la iu">Gt</strong></a></code><a class="ae lv" rel="noopener" target="_blank" href="/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a"><strong class="la iu"><em class="lu"/></strong>是累计奖励</a>也称为<em class="lu">TD-target；</em> <a class="ae lv" rel="noopener" target="_blank" href="/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a"> <em class="lu"> Q(s，a) </em>是形状<code class="fe mt mu mv mw b"><strong class="la iu">[space x action]</strong></code>的<em class="lu">Q</em>-值表</a>。</p><p id="0cec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Thrun 和 Schwartz 在“<a class="ae lv" href="https://www.ri.cmu.edu/publications/issues-in-using-function-approximation-for-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank"><em class="lu">Issues in use Function Approximation for Reinforcement Learning</em></a>”(1993)中观察到，使用函数逼近器(即神经网络)而不仅仅是查找表(这是<em class="lu"> Q </em> -learning 的基本技术)会对输出预测产生一些噪声。他们给出了一个例子，其中高估渐进地导致次优政策。</p><p id="8d0a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">脱钩于<em class="lu">双 DQN </em> </strong> <em class="lu">。</em></p><p id="bc04" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2015 年，哈塞尔特等人。艾尔。在“<a class="ae lv" href="https://arxiv.org/abs/1509.06461" rel="noopener ugc nofollow" target="_blank"> <em class="lu">使用双 Q 学习的深度强化学习</em> </a>”中显示，估计误差会驱使估计值上升并远离真正的最优值。他们假设减少高估的解决方案:<em class="lu">双 DQN </em>。</p><p id="e1f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<em class="lu">双 DQN </em>中所做的重要事情是将动作选择从动作评估中分离出来。让我们弄清楚这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/ef69fd82f478bef4b863721fd8564df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sN4JvavAt7GpaajANOdohg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd my">DQN 和双 DQN Gt 公式</strong></p></figure><ul class=""><li id="6bd6" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated"><em class="lu">DQN</em>Gt 公式:用于<em class="lu">动作选择</em>的<em class="lu"> Q- </em>值<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">Q</em>(<em class="lu">S</em>_<em class="lu">t</em>, <em class="lu">a</em>)</strong></code>(红色)和用于<em class="lu">动作评估</em>(蓝色)<em class="lu"> </em>的<em class="lu"> Q- </em>值<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">Q</em>(<em class="lu">S_t, a</em>)</strong></code>由<strong class="la iu">同一个神经网络</strong>和<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">θ_t</em></strong></code>确定。</li><li id="9641" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><em class="lu"> Gt 双 DQN </em>公式:用于<em class="lu">动作选择</em>的<em class="lu"> Q </em>值和用于<em class="lu">动作评估</em>的<em class="lu"> Q- </em>值由两个<strong class="la iu">不同的神经网络</strong>确定，神经网络的权重向量分别为<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">θ_t </em></strong></code>和<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">θ'_t.</em></strong></code> <strong class="la iu"> <em class="lu"> </em> </strong>这些网络称为<em class="lu">当前</em>和<em class="lu">目标。</em></li></ul><p id="cd7d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，由于缓慢变化的策略，对<em class="lu">当前</em>和<em class="lu">目标</em> <a class="ae lv" href="https://arxiv.org/abs/1802.09477" rel="noopener ugc nofollow" target="_blank">神经网络的值的估计仍然过于相似</a>，并且这仍然导致一致的高估。</p><p id="5243" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">优评建筑<em class="lu">DDPG</em>T58】。</strong></p><p id="1374" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu"> DDPG </em>是<a class="ae lv" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank">第一个尝试使用<em class="lu">Q</em>-连续动作空间<em class="lu"> DQN </em>模型学习技术的算法</a>之一。<em class="lu"> DDPG </em>代表<a class="ae lv" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id1" rel="noopener ugc nofollow" target="_blank"> <em class="lu">深度确定性政策梯度</em> </a>。在这种情况下，我们不能在所有操作中使用<em class="lu"> Q </em>值的最大化算子，但是，我们可以使用函数逼近器，一个代表<em class="lu"> Q </em>值的神经网络。我们假设存在某个函数<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">Q</em>(<em class="lu">s, a</em>)</strong></code>，该函数相对于动作自变量<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">a.</em></strong></code>是可微的，然而，对于给定的状态<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">S_t</em></strong></code> <strong class="la iu"> <em class="lu"> </em> </strong>，在所有动作<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">a</em></strong></code> <strong class="la iu"> <em class="lu"> </em> </strong>上找到<code class="fe mt mu mv mw b"><strong class="la iu">argmax(<em class="lu">Q</em>(<em class="lu">S</em>_<em class="lu">t</em>, <em class="lu">a</em>))</strong></code>意味着我们必须在每个时间步求解优化任务。这是一项非常昂贵的任务。为了克服这一障碍，来自 DeepMind 的一组研究人员在作品“<a class="ae lv" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank"> <em class="lu">用深度强化学习进行连续控制</em> </a>”中使用了演员-评论家架构。他们使用了<strong class="la iu">两个神经网络</strong>:一个和之前一样，在<em class="lu">DQN</em>:<em class="lu">Q</em>——网络代表<em class="lu">Q</em>——值；另一个是<strong class="la iu">演员函数</strong> <strong class="la iu"> 𝜋( <em class="lu"> s </em> ) </strong>，它提供<strong class="la iu"> <em class="lu"> a </em> *，</strong>值函数<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">Q</em>(<em class="lu">s, a</em>)</strong></code>的最大值如下<code class="fe mt mu mv mw b"><strong class="la iu">:</strong></code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/7763c5a162e745008363e1fbf7ef6845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eEWb5WOQEtMAsTORENxZmg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd my">演员功能</strong>t57】𝜋(<em class="np">s</em>)</p></figure><h1 id="622a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">第二部分。探索是学习的一大挑战</h1><p id="9833" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated"><strong class="la iu">为什么要探索？</strong></p><p id="1cad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除了高估之外，<em class="lu">深 RL </em>还有一个问题，难度不亚于。这就是探索。我们不能无条件地相信 q 表的最大值或 a* = 𝜋(s).的值为什么不呢？首先，在训练初期，对应的神经网络还是“年少无知”，其最大值与现实相差甚远。其次，也许<em class="lu">而不是最大</em>值会引导我们在艰苦训练后找到最优策略。</p><blockquote class="nq"><p id="55a8" class="nr ns it bd nt nu nv nw nx ny nz lt dk translated">在生活中，我们经常要解决以下问题:走别人走过的路——风险小，回报少；或者冒着巨大的风险走上一条未知的新路——但是，在某种程度上，大获全胜是可能的。也许它会超级棒，你不知道。</p></blockquote><p id="158e" class="pw-post-body-paragraph ky kz it la b lb oa ju ld le ob jx lg lh oc lj lk ll od ln lo lp oe lr ls lt im bi translated"><strong class="la iu">勘探与开采</strong></p><p id="7a20" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu"> Exploitation </em>是指代理使用积累的知识来选择接下来的动作。在我们的例子中，这意味着对于给定的状态，代理找到最大化<em class="lu">Q</em>-值的以下动作。<em class="lu">探索</em>表示<em class="lu"> </em>随机选择后续动作。</p><p id="3a5f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">没有规则可以决定哪种策略更好:探索还是开发。真正的目标是在这两种策略之间找到真正的平衡。正如我们所见，平衡策略在学习过程中会发生变化。</p><p id="e2f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">探险于<em class="lu"> DQN </em>和<em class="lu">双 DQN </em>和</strong></p><p id="9be6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<em class="lu"> DQN </em>和<em class="lu">双 DQN </em>中保证<em class="lu">充分探索</em>的一种方法是使用<a class="ae lv" rel="noopener" target="_blank" href="/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4">退火</a><code class="fe mt mu mv mw b"><a class="ae lv" rel="noopener" target="_blank" href="/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4"><strong class="la iu">ε</strong></a></code><a class="ae lv" rel="noopener" target="_blank" href="/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4">-贪婪机制</a>。对于第一集，以小概率选择开发，例如<code class="fe mt mu mv mw b"><strong class="la iu">0.02</strong></code>(即，将非常随机地选择动作)，以概率<code class="fe mt mu mv mw b"><strong class="la iu">0.98</strong></code>选择探索。从一定数量的剧集<code class="fe mt mu mv mw b"><strong class="la iu">Mε</strong></code>开始，以最小概率<code class="fe mt mu mv mw b"><strong class="la iu">ε_m,</strong></code>进行探索，例如<code class="fe mt mu mv mw b"><strong class="la iu">ε_m<em class="lu">= </em>0.01,</strong></code>，以概率<code class="fe mt mu mv mw b"><strong class="la iu">0.99.</strong></code> <strong class="la iu"> </strong>选择开发，探索的概率公式<code class="fe mt mu mv mw b"><strong class="la iu">ε</strong></code>可以实现如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/b10950063202139ed0f064017c0a9965.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZVJ2quAcu-HndK-G2jBpCA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd my">退火</strong><code class="fe mt mu mv mw b"><strong class="bd my">ε</strong></code><strong class="bd my">——贪婪机制，探索概率公式</strong> <code class="fe mt mu mv mw b"><strong class="bd my">ε</strong></code></p></figure><p id="2ce2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">i</em></strong></code> <em class="lu"> </em>为集号。设<code class="fe mt mu mv mw b"><strong class="la iu">Mε = 100, ε_m = 0.01.</strong></code> <strong class="la iu"> </strong>那么<strong class="la iu"> </strong>探索的概率<code class="fe mt mu mv mw b"><strong class="la iu">ε</strong></code> <strong class="la iu"> </strong>如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/73fb70ff4427bc27bab2fbb6ffb15892.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9hL4V8gGAeOuRXeH48WaPQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd my">概率从 1 逐渐降低到</strong> <code class="fe mt mu mv mw b"><strong class="bd my">ε_m = </strong></code> <strong class="bd my"> 0.01 </strong></p></figure><p id="7f83" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">探险<em class="lu">DDPG</em>T45】</strong></p><p id="a1b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在具有<em class="lu">连续动作空间的<em class="lu"> RL </em>模型中，应用</em>代替<code class="fe mt mu mv mw b"><strong class="la iu">ε</strong></code>-贪婪机制<a class="ae lv" href="https://arxiv.org/abs/1802.05054" rel="noopener ugc nofollow" target="_blank"><em class="lu"/></a><strong class="la iu"/>。这种方法用于<a class="ae lv" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank"><em class="lu"/></a><em class="lu"/><a class="ae lv" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank"><em class="lu">PPO</em></a><em class="lu"/>等连续控制算法。<em class="lu">【DDPG】</em>(Lilli crap et al .，2015) <em class="lu"> </em>的作者通过将从噪声进程<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">N</em></strong></code> <strong class="la iu"> <em class="lu"> </em> </strong>中采样的噪声添加到演员策略<strong class="la iu"> <em class="lu"> </em> </strong> <code class="fe mt mu mv mw b"><strong class="la iu">𝜋(s)</strong></code>中，构造了无向探索策略<code class="fe mt mu mv mw b"><strong class="la iu">𝜋’</strong></code> <strong class="la iu"> </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7ff2a10c7457b7574e20f209394e350d.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*qKmsB7ALL-qdi0GFNCqSsQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd my">政策<em class="np"> </em> </strong> <code class="fe mt mu mv mw b"><strong class="bd my">𝜋(s) with exploration noise</strong></code></p></figure><p id="7b07" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<strong class="la iu"> <em class="lu"> N </em> </strong>是<a class="ae lv" href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" rel="noopener ugc nofollow" target="_blank">奥恩斯坦-乌伦贝克</a>给出的噪声，相关噪声过程。在<a class="ae lv" href="https://arxiv.org/abs/1802.09477" rel="noopener ugc nofollow" target="_blank">中<em class="lu"> TD3 </em>论文</a>作者(<a class="ae lv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fujimoto%2C+S" rel="noopener ugc nofollow" target="_blank">藤本</a> et。艾尔。，2018)提出使用经典的高斯噪声，这是引用:</p><blockquote class="oi oj ok"><p id="e51e" class="ky kz lu la b lb lc ju ld le lf jx lg ol li lj lk om lm ln lo on lq lr ls lt im bi translated">…我们使用偏离策略的探索策略，添加高斯噪声 N(0；0:1)到每个动作。与 DDPG 的原始实现不同，我们使用不相关的噪声进行探索，因为我们发现从奥恩斯坦-乌伦贝克(乌伦贝克&amp;奥恩斯坦，1930)过程中提取的噪声没有提供性能优势。</p></blockquote><p id="9449" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu"> DDPG </em>的一个常见故障模式是，学习到的<em class="lu">Q</em>-函数开始高估<em class="lu">Q</em>-值，然后策略(actor 函数)导致重大错误。</p><p id="9e07" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">探索中的<em class="lu"> TD3 </em>中的</strong></p><p id="59d5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">名称<em class="lu"> TD3 </em>代表<em class="lu">孪生延迟深度确定性</em>。<em class="lu"> TD3 </em>保留了<em class="lu"> DDPG </em>中使用的演员-评论家架构，并增加了 3 个新属性，极大地帮助克服高估:</p><ul class=""><li id="7a58" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated"><em class="lu"> TD3 </em>维持着一对<strong class="la iu">影评人</strong> <em class="lu"> Q </em> 1 和<em class="lu"> Q </em> 2(因此得名“双胞胎”)以及一个演员。对于每个时间步长，<em class="lu"> TD3 </em>使用两个 Q 值中较小的一个。</li><li id="798d" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><em class="lu"> TD3 </em>更新策略(和目标网络)的频率低于 Q 函数更新(每两次 Q 函数(批评者)更新一次策略更新)</li><li id="ee9e" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated"><em class="lu"> TD3 </em>给目标动作添加探测噪音。<em class="lu"> TD3 </em>使用高斯噪声，而不是像<em class="lu"> DDPG </em>中的奥恩斯坦-乌伦贝克噪声。</li></ul><p id="2964" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">【PyBullet 料斗试验中的探测噪音</p><p id="71de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lv" href="https://pypi.org/project/pybullet/" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> PyBullet </em> </a>是基于<a class="ae lv" href="https://github.com/bulletphysics/bullet3" rel="noopener ugc nofollow" target="_blank"><em class="lu">Bullet Physics SDK</em></a><em class="lu">的用于机器人学的 Python 模块和<em class="lu"> Deep RL </em>。</em>让我们看看 HopperBulletEnv，它是<em class="lu"> PyBullet </em>与铰接体相关的环境之一:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/cc5f495adbef7c23854f4976dcbe37fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJyQTiZTrYiZf6Oa_rEtuw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae lv" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/HopperBulletEnv_v0-TD3" rel="noopener ugc nofollow" target="_blank"> <strong class="bd my">受过训练的特工</strong> </a></p></figure><p id="059d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果达到的分数超过 2500，则 HopperBulletEnv 环境被视为已解决。在使用 HopperBulletEnv 环境的<em class="lu"> TD3 </em>试验中，我得到了<code class="fe mt mu mv mw b"><strong class="la iu">std = 0.1</strong></code>和<code class="fe mt mu mv mw b"><strong class="la iu">std = 0.3</strong></code>的以下结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/b85415124cced44a4ce67d7bb45b39e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eHwyXbOmMZNZRtuzlvBODg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd my">HopperBulletEnv 的两次试验，TD3，噪声标准值= 0.1 和 0.3 </strong></p></figure><p id="dc8a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，<code class="fe mt mu mv mw b"><strong class="la iu">std</strong></code>是<em class="lu"> TD3 </em>中勘探噪声的标准差。在两次试验中，阈值 2500 都没有达到。然而，我注意到以下奇怪之处。</p><ul class=""><li id="5d1a" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated">在试验<code class="fe mt mu mv mw b"><strong class="la iu">std = 0.3</strong></code>中，在 2500 附近有很多值(然而小于 2500 ),同时平均值一直在下降。对此的解释如下:小值的数量多于大值的数量，并且这些数量之间的差异增大。</li><li id="e6ae" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">在<code class="fe mt mu mv mw b"><strong class="la iu">std = 0.1</strong></code>试验中，平均值达到较大值，但总的来说，这些值会降低。同样，这是因为小值的数量比大值的数量多。</li><li id="1f0f" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">在我看来，非常小的值的流行与太大的噪声标准偏差有关。然后我决定把<code class="fe mt mu mv mw b"><strong class="la iu">std</strong></code>减少到<code class="fe mt mu mv mw b"><strong class="la iu">0.02</strong></code>，这足以解决环境问题。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/5639d29c60bc292c9d0e8185f7688525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*WrSYkoW5XI6yTyi4ktkBMg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae lv" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/HopperBulletEnv_v0-TD3" rel="noopener ugc nofollow" target="_blank"> <strong class="bd my"> HopperBulletEnv 带 TD3，噪声标准值= 0.02 </strong> </a></p></figure><h1 id="9a89" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">App。具有自适应噪声的爬山算法</h1><p id="fdc7" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated"><strong class="la iu">张量的先行者</strong></p><p id="041a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们举例说明应用于<a class="ae lv" rel="noopener" target="_blank" href="/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4"> <em class="lu">翻筋斗</em>环境</a>的<em class="lu">爬山</em>算法的特性。这里的神经网络模型非常简单，不使用张量(没有<em class="lu"> PyTorch </em>，没有<em class="lu"> Tensorflow </em>)，神经网络只使用形状最简单的矩阵<code class="fe mt mu mv mw b"><strong class="la iu">[4 x 2]</strong></code>，那就是张量的前身。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="ak">爬山算法中的类策略</strong></p></figure><p id="5d7e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">爬山</em>算法寻求最大化<strong class="la iu">目标函数</strong> <code class="fe mt mu mv mw b"><strong class="la iu">Go</strong></code>，在我们的特定情况下，它是<em class="lu">累积折扣奖励</em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/132bf95487b76cc31818fa89de466fd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Th97M0TUHS4PUS59KxPsEA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd my"> <em class="np">累计折扣奖励</em> </strong></p></figure><p id="dbd1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">γ</em></strong></code> <strong class="la iu"> <em class="lu"> </em> </strong>为<em class="lu">折扣</em>、<code class="fe mt mu mv mw b"><strong class="la iu">0 &lt; <em class="lu">γ &lt; </em>1</strong></code> <em class="lu">、</em>、<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">Rk</em></strong></code>为该集时间步<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">k</em></strong></code> <em class="lu"> </em>获得的奖励。Python 中的目标函数<code class="fe mt mu mv mw b"><strong class="la iu">Go</strong></code>如下所示:</p><pre class="kj kk kl km gt ou mw ov ow aw ox bi"><span id="7b9a" class="oy lx it mw b gy oz pa l pb pc"><strong class="mw iu">discounts </strong><strong class="mw iu">= [gamma**i for i in range(len(rewards)+1)]<br/>Go = sum([a*b for a,b in zip(discounts, rewards)])</strong></span></pre><p id="1bad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一如既往地在<em class="lu">深 RL </em>中，我们试图超过某个阈值。对于<em class="lu">横竿-v0 </em>来说，这个阈值分数是<code class="fe mt mu mv mw b"><strong class="la iu">195</strong></code>，对于<em class="lu">横竿-v1 </em>来说是<code class="fe mt mu mv mw b"><strong class="la iu">475</strong></code>。<em class="lu">爬山</em>是一种简单的无梯度算法(即，我们不使用梯度上升/梯度下降方法)。我们试图通过使用特定的<strong class="la iu">自适应噪声</strong>仅改变目标函数<code class="fe mt mu mv mw b"><strong class="la iu">Go</strong></code>的参数来爬上曲线的顶端。但是，我们的目标函数的自变量是什么呢？</p><p id="ec5d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe mt mu mv mw b"><strong class="la iu">Go</strong></code>的参数是权重矩阵，它决定了我们模型中的神经网络。第 0-5 集的权重矩阵示例如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/a8d28ddd848a792f161d75d37d8a710d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cff-plm8GXEguWDVAXp7QA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd my">第 0-5 集神经网络的权重向量【4x 2】</strong></p></figure><p id="383f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">自适应噪声标度</strong></p><p id="474f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的模型的自适应噪声缩放实现如下。如果目标函数的当前值比为目标函数获得的最佳值更好，我们用<code class="fe mt mu mv mw b"><strong class="la iu">2</strong></code>除以<strong class="la iu">噪声标度</strong>，并且该噪声被添加到权重矩阵中。如果目标函数的当前值比获得的最佳值差，我们<strong class="la iu">将噪声标度</strong>乘以<code class="fe mt mu mv mw b"><strong class="la iu">2</strong></code>，并且该噪声被添加到权重矩阵的获得的最佳值。在这两种情况下，噪声标度被添加了对于矩阵的任何元素都不同的一些随机因子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/b226d7dc0f47dfd76d5a30347229626a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g8jpPATf8eRQ95nTE3ZEVA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd my">噪音等级和剧集评分图</strong></p></figure><p id="15ae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于<a class="ae lv" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/blob/master/CartPole-Policy-Based-Hill-Climbing/CartPole-v1_Hill_Climbing.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="lu">cart pole-v1</em></a><em class="lu">，</em>如果权重矩阵初始化为非零小值(见左上矩阵)，则集数= <code class="fe mt mu mv mw b"><strong class="la iu">112</strong></code>。注意，如果权重矩阵被初始化为零，则情节数从<code class="fe mt mu mv mw b"><strong class="la iu">112</strong></code>增加到<code class="fe mt mu mv mw b"><strong class="la iu">168</strong></code>。同样适用于<a class="ae lv" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/blob/master/CartPole-Policy-Based-Hill-Climbing/CartPole-v0_Hill_Climbing.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> Cartpole-v0 </em> </a>。</p><p id="a9c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关于<em class="lu">具有自适应噪声缩放的 Cartpole-v0/Cartpole-v1 的更多信息，</em> <a class="ae lv" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/CartPole-Policy-Based-Hill-Climbing" rel="noopener ugc nofollow" target="_blank">参见 Github 上的项目</a>。</p><p id="4164" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">噪声等级的更通用公式</strong></p><p id="0a40" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们在上面看到的，噪声比例根据目标函数是低于还是高于获得的最佳值而自适应地增大或减小。这个算法中的噪声尺度是<code class="fe mt mu mv mw b"><strong class="la iu">2</strong></code>。在论文<a class="ae lv" href="https://arxiv.org/abs/1706.01905" rel="noopener ugc nofollow" target="_blank"> <em class="lu">中参数空间噪声的探索</em> </a>作者考虑了更一般的公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/e3d8520218e3fa73bab1fd0f137ded9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_jp4BcJxiYpKkzbRXk4QVg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd my">自适应噪声标度</strong></p></figure><p id="8fe8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">α</em></strong></code> <em class="lu"> </em>是噪声尺度，<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">d</em></strong></code> <em class="lu"> </em>是扰动和非扰动策略之间的某个距离度量，<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">δ</em></strong></code> <em class="lu"> </em>是阈值。在附录 C 中，作者考虑了算法<em class="lu"> DQN </em>、<em class="lu"> DDPG </em>和<em class="lu"> TPRO 的距离函数<code class="fe mt mu mv mw b"><strong class="la iu"><em class="lu">d</em></strong></code> <em class="lu"> </em>的可能形式。</em></p><h1 id="9a77" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">参考文献</strong></h1><p id="5e7e" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">[1] S.Thrun 和 A.Schwartz，使用函数逼近进行强化学习的问题，(1993)，卡内基梅隆大学，机器人研究所<br/> <br/> [2] H.van Hasselt 等人。艾尔。，采用双 Q 学习的深度强化学习(2015)，arXiv:1509.06461</p><p id="093e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3] T.P. Lillicrap 等，深度强化学习的连续控制(2015)，arXiv:1509.02971</p><p id="671b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4]李玉玺，深度强化学习:概述(2018)，arXiv:1701.07274v6</p><p id="e90d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[5] S.Fujimoto 等人，在 Actor-Critic 方法中解决函数近似误差(2018)，arXiv: arXiv:1802.09477v3</p><p id="7619" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[6]带参数噪声的更好探索，openai.com，<a class="ae lv" href="https://openai.com/blog/better-exploration-with-parameter-noise/" rel="noopener ugc nofollow" target="_blank">https://open ai . com/blog/Better-Exploration-with-Parameter-Noise/</a></p><p id="ef79" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[7] M.Plappert 等著《用于探索的参数空间噪声》，OpenAI，arXiv:1706.01905v2，ICLR 2018<br/><br/>【8】B . Mahyavanshi 著《爬山入门|人工智能》，Medium，2019</p><p id="a945" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[9]深度确定性策略梯度，OpenAI，Spinning Up，<a class="ae lv" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html" rel="noopener ugc nofollow" target="_blank">https://Spinning Up . open ai . com/en/latest/algorithms/ddpg . html</a></p><p id="d543" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">【10】机器学习中的随机是什么意思？(2019)，机器学习掌握，<br/><a class="ae lv" href="https://machinelearningmastery.com/stochastic-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/random-in-Machine-Learning/</a><br/><br/>【11】c . Colas et .艾尔。、GEP-PG:深度强化学习算法中的解耦探索与利用(2018)，arXiv:1802.05054</p><p id="164b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[12]<a class="ae lv" href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/</a>奥恩斯坦-乌伦贝克<a class="ae lv" href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" rel="noopener ugc nofollow" target="_blank">_ 过程</a>，奥恩斯坦-乌伦贝克过程</p><p id="9bbd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[13] E.Lindwurm，直觉:探索与利用(2019 年)，走向数据科学</p><p id="c12a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[14] M.Watts，用于新闻推荐的强化学习介绍(DDPG 和 TD3)(2019)，TowardsDataScience</p><p id="9091" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[15] T .斯塔福德，学习的基础:探索-开发权衡(2012 年)，<a class="ae lv" href="https://tomstafford.staff.shef.ac.uk/?p=48" rel="noopener ugc nofollow" target="_blank">https://tomstafford.staff.shef.ac.uk/?p=48</a></p><p id="00f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[16]子弹实时物理模拟(2020)，<a class="ae lv" href="https://pybullet.org/wordpress/" rel="noopener ugc nofollow" target="_blank">https://pybullet.org/wordpress/</a><br/><br/>【17】R . Stekolshchik，一对相互关联的神经网络在 DQN (2020)，走向数据科学</p><p id="048a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[18] R.Stekolshchik，贝尔曼方程在 Deep RL 中是如何工作的？(2020 年)，走向数据科学</p></div></div>    
</body>
</html>