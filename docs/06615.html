<html>
<head>
<title>Visualising the Classification Power of Data using PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用主成分分析可视化数据的分类能力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualising-the-classification-power-of-data-54f5273f640?source=collection_archive---------14-----------------------#2020-05-25">https://towardsdatascience.com/visualising-the-classification-power-of-data-54f5273f640?source=collection_archive---------14-----------------------#2020-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1535" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用主成分分析来研究数据如何区分类(使用Python代码)</h2></div><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="kn ko l"/></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">文章概述</p></figure><p id="a166" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">主成分分析(PCA)是数据科学家使用的一个很好的工具。它可以用来降低特征空间的维数，产生不相关的特征。正如我们将看到的，它还可以帮助您深入了解数据的分类能力。我们将带您详细了解如何以这种方式使用PCA。提供了Python代码片段，完整的项目可以在<a class="ae lp" href="https://github.com/conorosully/medium-articles" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><h1 id="61c7" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">什么是PCA？</h1><p id="ffc7" class="pw-post-body-paragraph kt ku it kv b kw mi ju ky kz mj jx lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">我们将从复习理论开始。如果你想了解PCA是如何工作的<a class="ae lp" rel="noopener" target="_blank" href="/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c">【1】</a><a class="ae lp" href="https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">【2】</a>，我们不会进入太多细节，因为有大量的资源。重要的是要知道PCA是一种降维算法。这意味着它用于减少用于训练模型的特征数量。这是通过从许多特征中构造主成分(PC)来实现的。</p><p id="faac" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">PCs的构造使得第一台PC(即PC1)尽可能解释您的功能中的大多数变化。然后PC2尽可能解释剩余变异中的大部分变异等等。PC1和PC2通常可以解释总特征变化的很大一部分。另一种思考方式是，前两台电脑可以很好地总结这些特性。这是很重要的，因为它允许我们在二维平面上可视化数据的分类能力。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mn"><img src="../Images/ad564a926869235daa1e371340ef3099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p4V1czhFcaITCragtOKrrg.png"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">来源1: <a class="ae lp" href="https://www.flaticon.com/free-icon/analytics_777324" rel="noopener ugc nofollow" target="_blank"> flaticon </a> |来源2: <a class="ae lp" href="https://www.flaticon.com/premium-icon/analytics_500960" rel="noopener ugc nofollow" target="_blank"> flaticon </a> |来源3: <a class="ae lp" href="https://www.flaticon.com/free-icon/idea_166375" rel="noopener ugc nofollow" target="_blank"> flaticon </a></p></figure><h1 id="1ba6" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">资料组</h1><p id="1f5d" class="pw-post-body-paragraph kt ku it kv b kw mi ju ky kz mj jx lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">好的，让我们来看一个实际的例子。我们将使用PCA来探索一个乳腺癌数据集<a class="ae lp" href="http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)" rel="noopener ugc nofollow" target="_blank">【3】</a>，我们使用下面的代码导入该数据集。目标变量是乳腺癌测试的结果——恶性或良性。每次测试，都要采集许多癌细胞。然后对每个癌细胞进行10种不同的测量。这些指标包括像像细胞半径和细胞对称性。为了得到30个特性的最终列表，我们以3种方式汇总这些度量。也就是说，我们计算每个测量的平均值、标准误差和最大(“最差”)值。在图1中，我们仔细观察了其中的两个特征——细胞的<em class="mu">平均对称性</em>和<em class="mu">最差平滑度</em>。</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="mv ko l"/></div></figure><p id="e3f6" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在图1中，我们看到这两个特性有助于区分这两个类。也就是说，良性肿瘤往往更加对称和光滑。仍然有很多重叠，所以只使用这些特征的模型不会做得很好。我们可以创建这样的图来了解每个特征的预测能力。虽然，有30个特征，会有相当多的图需要分析。它们也没有告诉我们数据集作为一个整体的预测性如何。这就是PCA的用武之地。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mw"><img src="../Images/c16c3425429837c3ac1cbb556ea58a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GFOmZvwxvPPg8_mZDtXF9A.png"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">图1:使用两个特征的散点图(来源:作者)</p></figure><h1 id="520d" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">PCA整个数据集</h1><p id="d910" class="pw-post-body-paragraph kt ku it kv b kw mi ju ky kz mj jx lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">让我们从对整个数据集进行PCA开始。我们使用下面的代码来做到这一点。我们从缩放特征开始，因此它们都具有0的平均值和1的方差。这一点很重要，因为PCA通过最大化PCs解释的方差来工作。由于其规模的原因，一些要素往往会有较高的方差。例如，以厘米为单位测量的距离比以千米为单位测量的距离具有更高的方差。没有缩放，PCA将被那些具有高方差的特征“压倒”。</p><p id="a07d" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">缩放完成后，我们将拟合PCA模型，并将我们的特征转换到PCs中。由于我们有30个功能，我们可以有多达30台电脑。对于我们的观想，我们只对前两个感兴趣。您可以在图2中看到这一点，其中使用了PC1和PC2来创建散点图。我们现在可以看到两个不同的集群，它们比图1中的更清晰。</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="mv ko l"/></div></figure><p id="fa92" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">此图可用于为数据的预测力度建立直觉。在这种情况下，它表明使用整个数据集将允许我们分离恶性和良性肿瘤。然而，仍然存在一些异常值(即，不清楚地在聚类中的点)。这并不意味着我们会对这些情况做出不正确的预测。我们应该记住，不是所有的特性差异都在前两个PC中被捕获。基于完整特征集训练的模型可以产生更好的预测。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mw"><img src="../Images/1c9996cfd150423507f6bb00cbf44107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_nTMODngNz9QeZqe6mkg7w.png"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">图2:使用所有特征的PCA散点图(来源:作者)</p></figure><p id="0e3b" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这一点上，我们应该提到这种方法的警告。PC1和PC2可以解释要素中很大一部分差异。然而，事实并非总是如此。在某些情况下，个人电脑可以被认为是你的功能不良总结。这意味着，即使您的数据能够很好地将类分开，您也可能无法获得清晰的聚类，如图2所示。</p><p id="f65a" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们可以使用PCA scree图来确定这是否是一个问题。我们使用下面的代码为这个分析创建了scree图，如图3所示。这是一个条形图，其中每个条形的高度是由相关PC解释的差异百分比。我们看到，PC1和PC2总共只能解释大约20%的特征差异。即使只有20%得到解释，我们仍然得到两个不同的集群。这强调了数据的预测能力。</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="mv ko l"/></div></figure><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mx"><img src="../Images/1d168a1c118089d2a7d65a2b57bbcee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l4Nwa1JN30PjUwvgzITZaQ.png"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">图3:碎石地块(来源:作者)</p></figure><h1 id="e4d3" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">PCA —特征组</h1><p id="e851" class="pw-post-body-paragraph kt ku it kv b kw mi ju ky kz mj jx lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">我们也可以用这个过程来比较不同组的特征。例如，假设我们有两组特征。组1具有基于单元对称性和平滑度特征的所有特征。然而，组2具有基于周长和凹度的所有特征。我们可以使用PCA来获得关于哪一组对于进行预测更有用的直觉。</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="mv ko l"/></div></figure><p id="24e6" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们首先创建两组特征。然后，我们分别对每个组进行主成分分析。这将为我们提供两组PC，我们选择PC1和PC2来代表每组功能。这个过程的结果可以在图4中看到。</p><p id="5fac" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">对于组1，我们可以看到有一些分离，但仍有许多重叠。相反，对于组2，有两个不同的集群。因此，从这些图中，我们可以预期第2组中的特征是更好的预测器。使用组2特征训练的模型应该比使用组1特征训练的模型具有更高的准确度。现在，让我们来检验这个假设。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi my"><img src="../Images/6d135cf49712643be6791898ec5d58f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j5ebtRYTuH8H6weDpYWT0w.png"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">图4:使用特征组的PCA散点图(来源:作者)</p></figure><p id="bf80" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们使用下面的代码来训练使用这两组特征的逻辑回归模型。在每种情况下，我们使用70%的数据来训练模型，剩余的30%用于测试模型。组1的测试集的准确率为74%，相比之下，组2的准确率为97%。因此，第2组中的特征是更好的预测器，这是我们从PCA结果中预期的。</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="mv ko l"/></div></figure><p id="0889" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">最后，我们将看到在开始建模之前，如何使用PCA来更深入地了解您的数据。这将使您了解预期的分类精度。您还将围绕哪些特征具有预测性建立直觉。在选择功能时，这可以给你带来优势。</p><p id="7a47" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如前所述，这种方法并不是完全可靠的。它应该与其他数据勘探图和汇总统计一起使用。对于分类问题，这些可能包括信息值和箱线图。一般来说，在开始建模之前，从尽可能多的不同角度查看数据是一个好主意。下面是另外两个你可能会感兴趣的观想教程。</p><div class="mz na gp gr nb nc"><a rel="noopener follow" target="_blank" href="/finding-and-visualising-non-linear-relationships-4ecd63a43e7e"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd iu gy z fp nh fr fs ni fu fw is bi translated">发现并可视化非线性关系</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">用部分相关图(PDP)、互信息和特征重要性分析非线性关系</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">towardsdatascience.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq ms nc"/></div></div></a></div><div class="mz na gp gr nb nc"><a rel="noopener follow" target="_blank" href="/finding-and-visualising-interactions-14d54a69da7c"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd iu gy z fp nh fr fs ni fu fw is bi translated">发现和可视化交互</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">使用特征重要性、弗里德曼的H-统计量和ICE图分析相互作用</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">towardsdatascience.com</p></div></div><div class="nl l"><div class="nr l nn no np nl nq ms nc"/></div></div></a></div></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="1b3d" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我希望这篇文章对你有帮助！如果你想看更多，你可以成为我的<a class="ae lp" href="https://conorosullyds.medium.com/membership" rel="noopener"> <strong class="kv iu">推荐会员</strong> </a> <strong class="kv iu">来支持我。你可以访问medium上的所有文章，我可以得到你的部分费用。</strong></p><div class="mz na gp gr nb nc"><a href="https://conorosullyds.medium.com/membership" rel="noopener follow" target="_blank"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd iu gy z fp nh fr fs ni fu fw is bi translated">通过我的推荐链接加入Medium康纳·奥沙利文</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">conorosullyds.medium.com</p></div></div><div class="nl l"><div class="nz l nn no np nl nq ms nc"/></div></div></a></div><h2 id="5208" class="oa lr it bd ls ob oc dn lw od oe dp ma lc of og mc lg oh oi me lk oj ok mg ol bi translated">图像来源</h2><p id="4c46" class="pw-post-body-paragraph kt ku it kv b kw mi ju ky kz mj jx lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">所有图片都是我自己的或从<a class="ae lp" href="http://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank">www.flaticon.com</a>获得。在后者的情况下，我拥有他们的<a class="ae lp" href="https://support.flaticon.com/hc/en-us/articles/202798201-What-are-Flaticon-Premium-licenses-" rel="noopener ugc nofollow" target="_blank">保费计划</a>中定义的“完全许可”。</p><h2 id="82c9" class="oa lr it bd ls ob oc dn lw od oe dp ma lc of og mc lg oh oi me lk oj ok mg ol bi translated">参考</h2><p id="1779" class="pw-post-body-paragraph kt ku it kv b kw mi ju ky kz mj jx lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">[1] <a class="om on ep" href="https://medium.com/u/55680478461?source=post_page-----54f5273f640--------------------------------" rel="noopener" target="_blank"> Matt Brems </a>，主成分分析一站式商店(2017)，<a class="ae lp" rel="noopener" target="_blank" href="/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c">https://towardsdatascience . com/A-一站式主成分分析商店-5582fb7e0a9c </a></p><p id="5643" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">[2] L. Pachter什么是主成分分析？(2014)，<a class="ae lp" href="https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">https://liorpachter . WordPress . com/2014/05/26/what-is-principal-component-analysis/</a></p><p id="55f6" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">[3] UCI，乳腺癌威斯康星州(诊断)数据集(2020年)，<a class="ae lp" href="http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)" rel="noopener ugc nofollow" target="_blank">http://archive . ics . UCI . edu/ml/datasets/Breast+Cancer+Wisconsin+(诊断)</a></p></div></div>    
</body>
</html>