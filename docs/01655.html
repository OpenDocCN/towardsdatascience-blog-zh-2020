<html>
<head>
<title>Pytorch [Basics] — Intro to RNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pytorch【基础】—RNN 简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677?source=collection_archive---------0-----------------------#2020-02-15">https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677?source=collection_archive---------0-----------------------#2020-02-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/840b4e62b667c78ce2f768c8b11e4488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Dsdw-L4qVhT1WkyLvtsPg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">如何训练你的神经网络[图片[0]]</p></figure><h2 id="fa2d" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/akshaj-wields-pytorch" rel="noopener" target="_blank">如何训练你的神经网络</a></h2><div class=""/><div class=""><h2 id="c2ac" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">这篇博文将带你了解 PyTorch 中不同类型的 RNN 操作。</h2></div><p id="4888" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这篇博文通过预测一个数字序列，带您了解 PyTorch 中普通 rnn、堆叠 rnn、双向 rnn 和堆叠双向 rnn 的实现。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/426f7bf462aa41e782b74f3addfa217a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/0*C4JCPa92jGkmdlHC"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">RNN 迷因[图片[1]]</p></figure></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="3e6f" class="ml mm jf bd mn mo mp mq mr ms mt mu mv ku mw kv mx kx my ky mz la na lb nb nc bi translated">导入库</h1><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="dd5b" class="ni mm jf ne b gy nj nk l nl nm">import numpy as np<br/><br/>import torch<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>from torch.utils.data import Dataset, DataLoader</span></pre><h1 id="a4f6" class="ml mm jf bd mn mo nn mq mr ms no mu mv ku np kv mx kx nq ky mz la nr lb nb nc bi translated">rnn 的类型</h1><p id="09a3" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated">rnn 主要用于时序数据，如时间序列或 NLP。有多种不同类型的 rnn 用于不同的应用。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/e3cef12c0260f16568a46f092651ded8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gHkt_1WgOn1csK3C.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">不同类型的 RNNs 图片[2]]</p></figure><p id="413b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">对于时间序列-</p><ul class=""><li id="fc26" class="ny nz jf lf b lg lh lj lk lm oa lq ob lu oc ly od oe of og bi translated">预测-多对多或多对一</li><li id="e523" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated">分类-多对一</li></ul><p id="3c32" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">对于 NLP -</p><ul class=""><li id="f3c2" class="ny nz jf lf b lg lh lj lk lm oa lq ob lu oc ly od oe of og bi translated">文本分类:多对一</li><li id="ef74" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated">文本生成:多对多</li><li id="255a" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated">机器翻译:多对多</li><li id="1693" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated">命名实体识别:多对多</li><li id="0182" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated">图像字幕:一对多</li></ul><h2 id="0c99" class="ni mm jf bd mn om on dn mr oo op dp mv lm oq or mx lq os ot mz lu ou ov nb jl bi translated">堆叠 rnn</h2><p id="b95e" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated">为了获得更好的性能，我们经常将 rnn 堆叠在一起。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/e8fa55dfcfca74af8523742a877262b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/0*jQIBe8109Y878Udn.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">堆叠 RNNs[图像[3]]</p></figure><h2 id="267a" class="ni mm jf bd mn om on dn mr oo op dp mv lm oq or mx lq os ot mz lu ou ov nb jl bi translated">双向 RNN</h2><p id="b02a" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated">双向 RNN 基本上使用 2 个 RNN，其中输入序列按正常顺序馈送到 1 个 RNN，反向馈送到另一个 RNN。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ox"><img src="../Images/4083b004f5b2a9755e4b4fb3b6110997.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YRgeyior6VQe0NM8.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">双向 RNNs [Image [4]]</p></figure><h1 id="b934" class="ml mm jf bd mn mo nn mq mr ms no mu mv ku np kv mx kx nq ky mz la nr lb nb nc bi translated">输入数据</h1><p id="428b" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated">这里的数据是:</p><blockquote class="oy"><p id="57ec" class="oz pa jf bd pb pc pd pe pf pg ph ly dk translated"><strong class="ak">【1，2，3，4，5，6，7，8，9，10，11，12，13，14，15，16，17，18，19，20】</strong></p></blockquote><p id="da50" class="pw-post-body-paragraph ld le jf lf b lg pi kp li lj pj ks ll lm pk lo lp lq pl ls lt lu pm lw lx ly ij bi translated">我们把它分成 4 批，序列长度= 5。</p><blockquote class="oy"><p id="8bdd" class="oz pa jf bd pb pc pd pe pf pg ph ly dk translated">[[1，2，3，4，5]，T15，[6，7，8，9，10]，<br/>，[11，12，13，14，15]，T17，[16，17，18，19，20]]</p></blockquote><p id="9542" class="pw-post-body-paragraph ld le jf lf b lg pi kp li lj pj ks ll lm pk lo lp lq pl ls lt lu pm lw lx ly ij bi translated"><strong class="lf jp">批量大小</strong> = 4 <br/> <strong class="lf jp">序列长度</strong> = 5 <br/> <strong class="lf jp">输入大小</strong> = 1(因为，只有一维)</p><p id="640f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在我们的例子中，我们查看 5 (seq_len)的前一个值来预测接下来的 2 个值。</p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="af4d" class="ni mm jf ne b gy nj nk l nl nm">data = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])</span><span id="1982" class="ni mm jf ne b gy pn nk l nl nm">print("Data: ", data.shape, "\n\n", data)</span><span id="9410" class="ni mm jf ne b gy pn nk l nl nm">###################### OUTPUT ######################</span><span id="f3bb" class="ni mm jf ne b gy pn nk l nl nm"><br/>Data:<br/> tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20.]) </span><span id="abb7" class="ni mm jf ne b gy pn nk l nl nm">Data Shape: <br/> torch.Size([20])</span></pre><h1 id="70b5" class="ml mm jf bd mn mo nn mq mr ms no mu mv ku np kv mx kx nq ky mz la nr lb nb nc bi translated">香草 RNN</h1><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="1d73" class="ni mm jf ne b gy nj nk l nl nm"># Number of features used as input. (Number of columns)<br/>INPUT_SIZE = 1</span><span id="be3c" class="ni mm jf ne b gy pn nk l nl nm"># Number of previous time stamps taken into account.<br/>SEQ_LENGTH = 5</span><span id="0ec9" class="ni mm jf ne b gy pn nk l nl nm"># Number of features in last hidden state ie. number of output time-<br/># steps to predict.See image below for more clarity.<br/>HIDDEN_SIZE = 2</span><span id="25ff" class="ni mm jf ne b gy pn nk l nl nm"># Number of stacked rnn layers.<br/>NUM_LAYERS = 1</span><span id="d6ad" class="ni mm jf ne b gy pn nk l nl nm"># We have total of 20 rows in our input. <br/># We divide the input into 4 batches where each batch has only 1<br/># row. Each row corresponds to a sequence of length 5. <br/>BATCH_SIZE = 4</span></pre><h2 id="549c" class="ni mm jf bd mn om on dn mr oo op dp mv lm oq or mx lq os ot mz lu ou ov nb jl bi translated">投入</h2><p id="c907" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated"><code class="fe po pp pq ne b">torch.nn.RNN</code>有两个输入- <code class="fe po pp pq ne b">input</code>和<code class="fe po pp pq ne b">h_0</code> ie。t=0 时的输入序列和隐藏层。如果我们不初始化隐藏层，它将被 PyTorch 自动初始化为全零。</p><ul class=""><li id="cdf2" class="ny nz jf lf b lg lh lj lk lm oa lq ob lu oc ly od oe of og bi translated"><code class="fe po pp pq ne b"><strong class="lf jp">input</strong></code>是输入网络的序列。它的大小应该是<code class="fe po pp pq ne b">(seq_len, batch, input_size)</code>。如果<code class="fe po pp pq ne b">batch_first=True</code>，则输入大小为<code class="fe po pp pq ne b">(batch, seq_len, input_size)</code>。</li><li id="e6e2" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated"><code class="fe po pp pq ne b"><strong class="lf jp">h_0</strong></code>是网络的初始隐藏状态。它的大小为<code class="fe po pp pq ne b">(num_layers * num_directions, batch, input_size)</code>，其中<code class="fe po pp pq ne b">num_layers</code>是堆叠 rnn 的数量。<code class="fe po pp pq ne b">num_directions</code> =双向 rnn 为 2，否则为 1。</li></ul><h2 id="4bac" class="ni mm jf bd mn om on dn mr oo op dp mv lm oq or mx lq os ot mz lu ou ov nb jl bi translated">输出</h2><p id="cbef" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated"><code class="fe po pp pq ne b">torch.nn.RNN</code>有两个输出- <code class="fe po pp pq ne b">out</code>和<code class="fe po pp pq ne b">hidden</code>。</p><ul class=""><li id="8259" class="ny nz jf lf b lg lh lj lk lm oa lq ob lu oc ly od oe of og bi translated"><code class="fe po pp pq ne b"><strong class="lf jp">out</strong></code>是来自最后一个 RNN 层的所有时间步长的 RNN 的输出。它的大小和<code class="fe po pp pq ne b">(seq_len, batch, num_directions * hidden_size)</code>差不多。如果<code class="fe po pp pq ne b">batch_first=True</code>，则输出大小为<code class="fe po pp pq ne b">(batch, seq_len, num_directions * hidden_size)</code>。</li><li id="1baf" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated"><code class="fe po pp pq ne b"><strong class="lf jp">h_n</strong></code>是所有 RNN 层的最后一个时间步长的隐藏值。它的尺寸是<code class="fe po pp pq ne b">(num_layers * num_directions, batch, hidden_size)</code>。<code class="fe po pp pq ne b">h_n</code>不受<code class="fe po pp pq ne b">batch_first=True</code>影响。<a class="ae pr" href="https://github.com/pytorch/pytorch/issues/4145" rel="noopener ugc nofollow" target="_blank"> Github 问题</a>。</li></ul><p id="f83c" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">下图解释得更清楚。这里的<code class="fe po pp pq ne b">batch=1</code>。该图是一个 LSTM，它有两个隐藏参数<code class="fe po pp pq ne b">(h, c)</code>。RNN 和 GRU 都只有<code class="fe po pp pq ne b">h</code>。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/15237fb6ea15b73758c0cb31d506b3a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*tUxl5-C-t3Qumt0cyVhm2g.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">RNN 输入输出【图片[5] <a class="ae pr" href="https://stackoverflow.com/a/48305882/9193380" rel="noopener ugc nofollow" target="_blank">信用点】</a></p></figure><blockquote class="pt pu pv"><p id="b21d" class="ld le pw lf b lg lh kp li lj lk ks ll px ln lo lp py lr ls lt pz lv lw lx ly ij bi translated">重申一下—</p><p id="9285" class="ld le pw lf b lg lh kp li lj lk ks ll px ln lo lp py lr ls lt pz lv lw lx ly ij bi translated"><code class="fe po pp pq ne b"><strong class="lf jp"><em class="jf">out</em></strong></code> <em class="jf">是 RNN 的</em> <strong class="lf jp"> <em class="jf">输出</em> </strong> <em class="jf">从</em> <strong class="lf jp"> <em class="jf">到</em> </strong> <em class="jf">从</em> <strong class="lf jp"> <em class="jf">最后一个 RNN 层</em> </strong> <em class="jf">。<br/> </em> <code class="fe po pp pq ne b"><strong class="lf jp"><em class="jf">h_n</em></strong></code> <em class="jf">是将</em> <strong class="lf jp"> <em class="jf">隐藏</em> </strong> <em class="jf">的值从</em> <strong class="lf jp"> <em class="jf">最后一步</em></strong><em class="jf"/><strong class="lf jp"><em class="jf">的所有 RNN 图层</em> </strong> <em class="jf">。</em></p></blockquote><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="42d3" class="ni mm jf ne b gy nj nk l nl nm"># Initialize the RNN.<br/>rnn = nn.RNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers = 1, batch_first=True)</span><span id="b330" class="ni mm jf ne b gy pn nk l nl nm"># input size : (batch, seq_len, input_size)<br/>inputs = data.view(BATCH_SIZE, SEQ_LENGTH, INPUT_SIZE)</span><span id="215d" class="ni mm jf ne b gy pn nk l nl nm"># out shape = (batch, seq_len, num_directions * hidden_size)<br/># h_n shape  = (num_layers * num_directions, batch, hidden_size)<br/>out, h_n = rnn(inputs)</span></pre><p id="a9bf" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe po pp pq ne b"><strong class="lf jp">input</strong></code>形状= <code class="fe po pp pq ne b">[4, 5, 1]</code> <br/> <code class="fe po pp pq ne b"><strong class="lf jp">out</strong></code>形状= <code class="fe po pp pq ne b">[4, 5, 2]</code> <br/> <code class="fe po pp pq ne b"><strong class="lf jp">h_n</strong></code>形状= <code class="fe po pp pq ne b">[1, 4, 2]</code></p><p id="16c0" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">input</strong></code>中，我们有 4 个批次作为输出，因为我们设置了<code class="fe po pp pq ne b">BATCH_SIZE=4</code>。每批包含 5 行，因为出了<code class="fe po pp pq ne b">SEQ_LENGTH = 5</code>。我们只使用一个特征作为输入<code class="fe po pp pq ne b">INPUT_SIZE = 1</code>。</p><p id="54fb" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">out</strong></code>中，我们从所有 4 个批次中获取值，其中时间步数(seq_len)为 5，预测数为 2。对于每一批，我们预测 2 个输出。</p><p id="414f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">h_n</strong></code>中，我们从单个 RNN 层的最后一个时间步长的 4 个批次中的每一个批次中获取值。</p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="a01a" class="ni mm jf ne b gy nj nk l nl nm">print('Input: ', inputs.shape, '\n', inputs)<br/>print('\nOutput: ', out.shape, '\n', out)<br/>print('\nHidden: ', h_n.shape, '\n', h_n)<br/></span><span id="fe1a" class="ni mm jf ne b gy pn nk l nl nm">###################### OUTPUT ######################</span><span id="e7d4" class="ni mm jf ne b gy pn nk l nl nm">Input:  torch.Size([4, 5, 1]) <br/> tensor([[[ 1.],<br/>         [ 2.],<br/>         [ 3.],<br/>         [ 4.],<br/>         [ 5.]],<br/><br/>        [[ 6.],<br/>         [ 7.],<br/>         [ 8.],<br/>         [ 9.],<br/>         [10.]],<br/><br/>        [[11.],<br/>         [12.],<br/>         [13.],<br/>         [14.],<br/>         [15.]],<br/><br/>        [[16.],<br/>         [17.],<br/>         [18.],<br/>         [19.],<br/>         [20.]]])<br/><br/>Output:  torch.Size([4, 5, 2]) <br/> tensor([[[-0.0819,  0.8100],<br/>         [-0.4311,  0.9332],<br/>         [-0.3162,  0.9748],<br/>         [-0.3979,  0.9875],<br/>         [-0.3675,  0.9944]],<br/><br/>        [[-0.1081,  0.9953],<br/>         [-0.5145,  0.9986],<br/>         [-0.3269,  0.9995],<br/>         [-0.4254,  0.9997],<br/>         [-0.3820,  0.9999]],<br/><br/>        [[-0.1342,  0.9999],<br/>         [-0.5245,  1.0000],<br/>         [-0.3458,  1.0000],<br/>         [-0.4382,  1.0000],<br/>         [-0.3982,  1.0000]],<br/><br/>        [[-0.1601,  1.0000],<br/>         [-0.5328,  1.0000],<br/>         [-0.3648,  1.0000],<br/>         [-0.4506,  1.0000],<br/>         [-0.4143,  1.0000]]], grad_fn=&lt;TransposeBackward1&gt;)<br/><br/>Hidden:  torch.Size([1, 4, 2]) <br/> tensor([[[-0.3675,  0.9944],<br/>         [-0.3820,  0.9999],<br/>         [-0.3982,  1.0000],<br/>         [-0.4143,  1.0000]]], grad_fn=&lt;StackBackward&gt;)</span></pre><p id="9dd8" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在上面的输出中，注意每批<code class="fe po pp pq ne b">out</code>中的最后一行出现在<code class="fe po pp pq ne b">h_n</code>中。</p><ul class=""><li id="9083" class="ny nz jf lf b lg lh lj lk lm oa lq ob lu oc ly od oe of og bi translated"><code class="fe po pp pq ne b">out</code>是每批最后一个 RNN 层所有时间步的输出值。</li><li id="28df" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated"><code class="fe po pp pq ne b">h_n</code>是每批所有 RNN 层在最后一个时间步的隐藏值。</li></ul><h1 id="7999" class="ml mm jf bd mn mo nn mq mr ms no mu mv ku np kv mx kx nq ky mz la nr lb nb nc bi translated">堆叠 RNN</h1><p id="0123" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated">如果我改变<code class="fe po pp pq ne b">num_layers = 3</code>，我们将有 3 个 RNN 层堆叠在一起。看看下面的例子中<code class="fe po pp pq ne b">out</code>和<code class="fe po pp pq ne b">h_n</code>张量是如何变化的。</p><p id="f361" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们现在在<code class="fe po pp pq ne b">h_n</code>张量中有 3 个批次。最后一批包含<code class="fe po pp pq ne b">out</code>张量中每批的结束行。</p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="51eb" class="ni mm jf ne b gy nj nk l nl nm"># Initialize the RNN.<br/>rnn = nn.RNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers = 3, batch_first=True)</span><span id="f482" class="ni mm jf ne b gy pn nk l nl nm"># input size : (batch_size , seq_len, input_size)<br/>inputs = data.view(BATCH_SIZE, SEQ_LENGTH, INPUT_SIZE)</span><span id="0f89" class="ni mm jf ne b gy pn nk l nl nm"># out shape = (batch, seq_len, num_directions * hidden_size)<br/># h_n shape  = (num_layers * num_directions, batch, hidden_size)<br/>out, h_n = rnn(inputs)</span></pre><p id="ba60" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe po pp pq ne b"><strong class="lf jp">input</strong></code>形状= <code class="fe po pp pq ne b">[4, 5, 1]</code> <br/> <code class="fe po pp pq ne b"><strong class="lf jp">out</strong></code>形状= <code class="fe po pp pq ne b">[4, 5, 2]</code> <br/> <code class="fe po pp pq ne b"><strong class="lf jp">h_n</strong></code>形状= <code class="fe po pp pq ne b">[3, 4, 2]</code></p><p id="1b49" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">input</strong></code>中，我们有 4 个批次作为输出，因为我们设置了<code class="fe po pp pq ne b">BATCH_SIZE=4</code>。每批包含 5 行，因为 out <code class="fe po pp pq ne b">SEQ_LENGTH = 5</code>。我们只使用一个特征作为输入<code class="fe po pp pq ne b">INPUT_SIZE = 1</code>。</p><p id="26ae" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">out</strong></code>中，我们从所有 4 个批次中获取值，其中时间步长数(seq_len)为 5，预测数为 2。对于每一批，我们预测 2 个输出。</p><p id="2070" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">h_n</strong></code>中，我们从 3 个堆叠的 RNN 层的最后时间步的 4 个批次中的每一个中获得值。</p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="25ea" class="ni mm jf ne b gy nj nk l nl nm">print('Input: ', inputs.shape, '\n', inputs)<br/>print('\nOutput: ', out.shape, '\n', out)<br/>print('\nHidden: ', h_n.shape, '\n', h_n)<br/></span><span id="61e3" class="ni mm jf ne b gy pn nk l nl nm">###################### OUTPUT ######################</span><span id="b106" class="ni mm jf ne b gy pn nk l nl nm">Input:  torch.Size([4, 5, 1]) <br/> tensor([[[ 1.],<br/>         [ 2.],<br/>         [ 3.],<br/>         [ 4.],<br/>         [ 5.]],<br/><br/>        [[ 6.],<br/>         [ 7.],<br/>         [ 8.],<br/>         [ 9.],<br/>         [10.]],<br/><br/>        [[11.],<br/>         [12.],<br/>         [13.],<br/>         [14.],<br/>         [15.]],<br/><br/>        [[16.],<br/>         [17.],<br/>         [18.],<br/>         [19.],<br/>         [20.]]])<br/><br/>Output:  torch.Size([4, 5, 2]) <br/> tensor([[[ 0.3144, -0.7527],<br/>         [-0.0597, -0.6038],<br/>         [ 0.0896, -0.7646],<br/>         [ 0.0608, -0.6358],<br/>         [ 0.1084, -0.6783]],<br/><br/>        [[ 0.4442, -0.6350],<br/>         [ 0.0949, -0.3948],<br/>         [ 0.2715, -0.5962],<br/>         [ 0.1819, -0.4580],<br/>         [ 0.2529, -0.5213]],<br/><br/>        [[ 0.4907, -0.5688],<br/>         [ 0.1671, -0.2976],<br/>         [ 0.3462, -0.4922],<br/>         [ 0.2388, -0.3768],<br/>         [ 0.3078, -0.4418]],<br/><br/>        [[ 0.5041, -0.5466],<br/>         [ 0.1883, -0.2675],<br/>         [ 0.3684, -0.4576],<br/>         [ 0.2572, -0.3502],<br/>         [ 0.3238, -0.4167]]], grad_fn=&lt;TransposeBackward1&gt;)<br/><br/>Hidden:  torch.Size([3, 4, 2]) <br/> tensor([[[-0.6480, -0.4044],<br/>         [-0.8912, -0.7801],<br/>         [-0.9808, -0.9366],<br/>         [-0.9975, -0.9836]],<br/><br/>        [[-0.7848, -0.0118],<br/>         [-0.8707, -0.1721],<br/>         [-0.8955, -0.2411],<br/>         [-0.9016, -0.2605]],<br/><br/>        [[ 0.1084, -0.6783],<br/>         [ 0.2529, -0.5213],<br/>         [ 0.3078, -0.4418],<br/>         [ 0.3238, -0.4167]]], grad_fn=&lt;StackBackward&gt;)</span></pre><h1 id="f37d" class="ml mm jf bd mn mo nn mq mr ms no mu mv ku np kv mx kx nq ky mz la nr lb nb nc bi translated">双向 RNN</h1><p id="2233" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated">对于双向 RNN，我们设置了<code class="fe po pp pq ne b">bidirectional=True</code>。</p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="15e5" class="ni mm jf ne b gy nj nk l nl nm">rnn = nn.RNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, batch_first=True, num_layers = 1, bidirectional = True)</span><span id="11fa" class="ni mm jf ne b gy pn nk l nl nm"># input size : (batch_size , seq_len, input_size)<br/>inputs = data.view(BATCH_SIZE, SEQ_LENGTH, INPUT_SIZE)</span><span id="e616" class="ni mm jf ne b gy pn nk l nl nm"># out shape = (batch, seq_len, num_directions * hidden_size)<br/># h_n shape  = (num_layers * num_directions, batch, hidden_size)<br/>out, h_n = rnn(inputs)</span></pre><p id="f49a" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe po pp pq ne b"><strong class="lf jp">input</strong></code>形状= <code class="fe po pp pq ne b">[4, 5, 1]</code> <br/> <code class="fe po pp pq ne b"><strong class="lf jp">out</strong></code>形状= <code class="fe po pp pq ne b">[4, 5, 4]</code> <br/> <code class="fe po pp pq ne b"><strong class="lf jp">h_n</strong></code>形状= <code class="fe po pp pq ne b">[2, 4, 2]</code></p><p id="1890" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">input</strong></code>中，我们有 4 个批次作为输出，因为我们设置了<code class="fe po pp pq ne b">BATCH_SIZE=4</code>。每批包含 5 行，因为 out <code class="fe po pp pq ne b">SEQ_LENGTH = 5</code>。我们只使用一个特征作为输入<code class="fe po pp pq ne b">INPUT_SIZE = 1</code>。</p><p id="c983" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">out</strong></code>中，我们从所有 4 个批次中获取值，其中时间步长数(seq_len)为 5，预测数为 2。对于每一批，我们预测 2 个输出。因为这是一个双向 RNN，我们得到了 2 组预测。因此，形状是<code class="fe po pp pq ne b">[4, 5, 4]</code>而不是<code class="fe po pp pq ne b">[4, 5, 2]</code>(我们在上述的单向 RNN 的情况下观察到的<em class="pw">)。</em></p><p id="1d24" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">h_n</strong></code>中，我们从单个 RNN 层的最后时间步的 4 个批次中的每一个批次中获取值。因为这是一个双向 RNN，我们得到了 2 组预测。因此，形状是<code class="fe po pp pq ne b">[2, 4, 2]</code>而不是<code class="fe po pp pq ne b">[1, 4, 2]</code>(我们在以上单向 RNN 的情况下观察到的<em class="pw">)。</em></p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="8439" class="ni mm jf ne b gy nj nk l nl nm">print('Input: ', inputs.shape, '\n', inputs)<br/>print('\nOutput: ', out.shape, '\n', out)<br/>print('\nHidden: ', h_n.shape, '\n', h_n)<br/></span><span id="8e33" class="ni mm jf ne b gy pn nk l nl nm">###################### OUTPUT ######################</span><span id="0ef0" class="ni mm jf ne b gy pn nk l nl nm">Input:  torch.Size([4, 5, 1]) <br/> tensor([[[ 1.],<br/>         [ 2.],<br/>         [ 3.],<br/>         [ 4.],<br/>         [ 5.]],<br/><br/>        [[ 6.],<br/>         [ 7.],<br/>         [ 8.],<br/>         [ 9.],<br/>         [10.]],<br/><br/>        [[11.],<br/>         [12.],<br/>         [13.],<br/>         [14.],<br/>         [15.]],<br/><br/>        [[16.],<br/>         [17.],<br/>         [18.],<br/>         [19.],<br/>         [20.]]])<br/><br/>Output:  torch.Size([4, 5, 4]) <br/> tensor([[[ 0.2184,  0.4086,  0.6418, -0.1677],<br/>         [-0.0222, -0.0095,  0.8794, -0.4927],<br/>         [-0.6716, -0.2802,  0.9585, -0.7248],<br/>         [-0.9387, -0.4152,  0.9846, -0.8646],<br/>         [-0.9841, -0.6164,  0.9789, -0.9192]],<br/><br/>        [[-0.9813, -0.8829,  0.9979, -0.9721],<br/>         [-0.9986, -0.8902,  0.9992, -0.9877],<br/>         [-0.9995, -0.9449,  0.9997, -0.9946],<br/>         [-0.9998, -0.9729,  0.9999, -0.9977],<br/>         [-0.9999, -0.9868,  0.9998, -0.9987]],<br/><br/>        [[-0.9999, -0.9968,  1.0000, -0.9996],<br/>         [-1.0000, -0.9969,  1.0000, -0.9998],<br/>         [-1.0000, -0.9985,  1.0000, -0.9999],<br/>         [-1.0000, -0.9993,  1.0000, -1.0000],<br/>         [-1.0000, -0.9997,  1.0000, -1.0000]],<br/><br/>        [[-1.0000, -0.9999,  1.0000, -1.0000],<br/>         [-1.0000, -0.9999,  1.0000, -1.0000],<br/>         [-1.0000, -1.0000,  1.0000, -1.0000],<br/>         [-1.0000, -1.0000,  1.0000, -1.0000],<br/>         [-1.0000, -1.0000,  1.0000, -1.0000]]], grad_fn=&lt;TransposeBackward1&gt;)<br/><br/>Hidden:  torch.Size([2, 4, 2]) <br/> tensor([[[-0.9841, -0.6164],<br/>         [-0.9999, -0.9868],<br/>         [-1.0000, -0.9997],<br/>         [-1.0000, -1.0000]],<br/><br/>        [[ 0.6418, -0.1677],<br/>         [ 0.9979, -0.9721],<br/>         [ 1.0000, -0.9996],<br/>         [ 1.0000, -1.0000]]], grad_fn=&lt;StackBackward&gt;)</span></pre><p id="1be5" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在让我们试着更详细地理解输出。根据文档，为了分离方向(向前和向后)，我们可以做以下事情-</p><ul class=""><li id="50c1" class="ny nz jf lf b lg lh lj lk lm oa lq ob lu oc ly od oe of og bi translated"><code class="fe po pp pq ne b">out.view(seq_len, batch, num_directions, hidden_size)</code>向前和向后分别为方向 0 和 1。请记住，如果您使用了<code class="fe po pp pq ne b">batch_first=True</code>，那么它将是<code class="fe po pp pq ne b">out.view(batch, seq_len, num_directions, hidden_size)</code>。</li><li id="d153" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated"><code class="fe po pp pq ne b">h_n.view(num_layers, num_directions, batch, hidden_size)</code>向前和向后分别为方向 0 和 1。</li></ul><h2 id="2efd" class="ni mm jf bd mn om on dn mr oo op dp mv lm oq or mx lq os ot mz lu ou ov nb jl bi translated">BiRNN 分离<code class="fe po pp pq ne b"><strong class="ak">out</strong></code></h2><p id="cdff" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated">让我们重塑 BiRNN 的输出，使用<code class="fe po pp pq ne b">out.view(batch, seq_len, num_directions, hidden_size)</code>来分离前向和后向值。</p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="a928" class="ni mm jf ne b gy nj nk l nl nm">out_reshaped = out.view(BATCH_SIZE, SEQ_LENGTH, 2, HIDDEN_SIZE)</span><span id="0efc" class="ni mm jf ne b gy pn nk l nl nm">print("Shape of the output after directions are separated: ", out_reshaped.shape)<br/></span><span id="9259" class="ni mm jf ne b gy pn nk l nl nm">###################### OUTPUT ######################</span><span id="d495" class="ni mm jf ne b gy pn nk l nl nm">Shape of the output after directions are separated:  torch.Size([4, 5, 2, 2])</span></pre><p id="7940" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在的形状是— <code class="fe po pp pq ne b">(batch, seq_len, num_directions, hidden_size)</code>。</p><p id="41a4" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe po pp pq ne b">num_directions</code>是二次元。为了获得前向和后向输出，我们可以做以下事情</p><ul class=""><li id="8d2c" class="ny nz jf lf b lg lh lj lk lm oa lq ob lu oc ly od oe of og bi translated">out_forward = <code class="fe po pp pq ne b">(batch, seq_len, 0, hidden_size)</code></li><li id="1fb4" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated">out_backward = <code class="fe po pp pq ne b">(batch, seq_len, 1, hidden_size)</code></li></ul><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="eb24" class="ni mm jf ne b gy nj nk l nl nm">out_forward = out_reshaped[:, :, 0, :]<br/>out_backward = out_reshaped[:, :, 1, :]</span><span id="ffe3" class="ni mm jf ne b gy pn nk l nl nm">print("Forward output: ", out_forward.shape, "\n", out_forward)<br/>print("\n\nBackward output: ", out_backward.shape, "\n", out_backward)<br/></span><span id="ce43" class="ni mm jf ne b gy pn nk l nl nm">###################### OUTPUT ######################</span><span id="d87e" class="ni mm jf ne b gy pn nk l nl nm">Forward output:  torch.Size([4, 5, 2]) <br/> tensor([[[ 0.2184,  0.4086],<br/>         [-0.0222, -0.0095],<br/>         [-0.6716, -0.2802],<br/>         [-0.9387, -0.4152],<br/>         [-0.9841, -0.6164]],<br/><br/>        [[-0.9813, -0.8829],<br/>         [-0.9986, -0.8902],<br/>         [-0.9995, -0.9449],<br/>         [-0.9998, -0.9729],<br/>         [-0.9999, -0.9868]],<br/><br/>        [[-0.9999, -0.9968],<br/>         [-1.0000, -0.9969],<br/>         [-1.0000, -0.9985],<br/>         [-1.0000, -0.9993],<br/>         [-1.0000, -0.9997]],<br/><br/>        [[-1.0000, -0.9999],<br/>         [-1.0000, -0.9999],<br/>         [-1.0000, -1.0000],<br/>         [-1.0000, -1.0000],<br/>         [-1.0000, -1.0000]]], grad_fn=&lt;SliceBackward&gt;)<br/><br/><br/>Backward output:  torch.Size([4, 5, 2]) <br/> tensor([[[ 0.6418, -0.1677],<br/>         [ 0.8794, -0.4927],<br/>         [ 0.9585, -0.7248],<br/>         [ 0.9846, -0.8646],<br/>         [ 0.9789, -0.9192]],<br/><br/>        [[ 0.9979, -0.9721],<br/>         [ 0.9992, -0.9877],<br/>         [ 0.9997, -0.9946],<br/>         [ 0.9999, -0.9977],<br/>         [ 0.9998, -0.9987]],<br/><br/>        [[ 1.0000, -0.9996],<br/>         [ 1.0000, -0.9998],<br/>         [ 1.0000, -0.9999],<br/>         [ 1.0000, -1.0000],<br/>         [ 1.0000, -1.0000]],<br/><br/>        [[ 1.0000, -1.0000],<br/>         [ 1.0000, -1.0000],<br/>         [ 1.0000, -1.0000],<br/>         [ 1.0000, -1.0000],<br/>         [ 1.0000, -1.0000]]], grad_fn=&lt;SliceBackward&gt;)</span></pre><h2 id="ea46" class="ni mm jf bd mn om on dn mr oo op dp mv lm oq or mx lq os ot mz lu ou ov nb jl bi translated">BiRNN 分离的<code class="fe po pp pq ne b">h_n</code></h2><p id="9b4a" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated">让我们用<code class="fe po pp pq ne b">h_n.view(num_layers, num_directions, batch, hidden_size)</code>重塑隐藏的 BiRNN 来分离向前和向后的值。</p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="fa66" class="ni mm jf ne b gy nj nk l nl nm">h_n_reshaped = h_n.view(1, 2, BATCH_SIZE, HIDDEN_SIZE)</span><span id="c165" class="ni mm jf ne b gy pn nk l nl nm">print("Shape of the hidden after directions are separated: ", h_n_reshaped.shape)<br/></span><span id="cb1d" class="ni mm jf ne b gy pn nk l nl nm">###################### OUTPUT ######################</span><span id="478f" class="ni mm jf ne b gy pn nk l nl nm">Shape of the hidden after directions are separated:  torch.Size([1, 2, 4, 2])</span></pre><p id="58b8" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在的形状是— <code class="fe po pp pq ne b">(num_layers, num_directions, batch, hidden_size)</code>。</p><p id="bf0d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe po pp pq ne b">num_directions</code>是第一维度。要获得向前和向后隐藏，我们可以做以下事情-</p><ul class=""><li id="b701" class="ny nz jf lf b lg lh lj lk lm oa lq ob lu oc ly od oe of og bi translated">hidden_forward = <code class="fe po pp pq ne b">(num_layers, 0, batch, hidden_size)</code></li><li id="4b50" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated">hidden_backward = <code class="fe po pp pq ne b">(num_layers, 1, batch, hidden_size)</code></li></ul><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="d87c" class="ni mm jf ne b gy nj nk l nl nm">h_n_forward = h_n_reshaped[:, 0, :, :]<br/>h_n_backward = h_n_reshaped[:, 1, :, :]</span><span id="0dc1" class="ni mm jf ne b gy pn nk l nl nm">print("Forward h_n: ", h_n_forward.shape, "\n", h_n_forward)<br/>print("\n\nBackward h_n: ", h_n_backward.shape, "\n", h_n_backward)</span><span id="790d" class="ni mm jf ne b gy pn nk l nl nm"><br/>###################### OUTPUT ######################</span><span id="ea7b" class="ni mm jf ne b gy pn nk l nl nm">Forward h_n:  torch.Size([1, 4, 2]) <br/> tensor([[[-0.9841, -0.6164],<br/>         [-0.9999, -0.9868],<br/>         [-1.0000, -0.9997],<br/>         [-1.0000, -1.0000]]], grad_fn=&lt;SliceBackward&gt;)<br/><br/><br/>Backward h_n:  torch.Size([1, 4, 2]) <br/> tensor([[[ 0.6418, -0.1677],<br/>         [ 0.9979, -0.9721],<br/>         [ 1.0000, -0.9996],<br/>         [ 1.0000, -1.0000]]], grad_fn=&lt;SliceBackward&gt;)</span></pre><h1 id="b250" class="ml mm jf bd mn mo nn mq mr ms no mu mv ku np kv mx kx nq ky mz la nr lb nb nc bi translated">堆叠双向 RNN</h1><p id="6581" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated">对于堆叠式双向 RNN，我们设置<code class="fe po pp pq ne b">bidirectional=True</code>和<code class="fe po pp pq ne b">num_layers</code> = 3。</p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="9634" class="ni mm jf ne b gy nj nk l nl nm">rnn = nn.RNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, batch_first=True, num_layers = 3, bidirectional = True)</span><span id="2f0e" class="ni mm jf ne b gy pn nk l nl nm"># input size : (batch_size , seq_len, input_size)<br/>inputs = data.view(BATCH_SIZE, SEQ_LENGTH, INPUT_SIZE)</span><span id="8497" class="ni mm jf ne b gy pn nk l nl nm"># out shape = (batch, seq_len, num_directions * hidden_size)<br/># h_n shape  = (num_layers * num_directions, batch, hidden_size)<br/>out, h_n = rnn(inputs)</span></pre><p id="3713" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe po pp pq ne b"><strong class="lf jp">input</strong></code>形状= <code class="fe po pp pq ne b">[4, 5, 1]</code> <br/> <code class="fe po pp pq ne b"><strong class="lf jp">out</strong></code>形状= <code class="fe po pp pq ne b">[4, 5, 4]</code> <br/> <code class="fe po pp pq ne b"><strong class="lf jp">h_n</strong></code>形状= <code class="fe po pp pq ne b">[6, 4, 2]</code></p><p id="9733" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">input</strong></code>中，我们有 4 个批次作为输出，因为我们设置了<code class="fe po pp pq ne b">BATCH_SIZE=4</code>。每批包含 5 行，因为 out <code class="fe po pp pq ne b">SEQ_LENGTH = 5</code>。我们仅使用单一特征作为输入<code class="fe po pp pq ne b">INPUT_SIZE = 1</code>。</p><p id="e52f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">out</strong></code>中，我们从所有 4 个批次中获取值，其中时间步长数(seq_len)为 5，预测数为 2。对于每一批，我们预测 2 个输出。因为这是一个双向 RNN，我们得到了 2 组预测。因此，形状是<code class="fe po pp pq ne b">[4, 5, 4]</code>而不是<code class="fe po pp pq ne b">[4, 5, 2]</code>(我们在上方的堆叠单向 RNN 的情况下观察到的<em class="pw">)。</em></p><p id="94f1" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<code class="fe po pp pq ne b"><strong class="lf jp">h_n</strong></code>中，我们从单个 RNN 层的最后时间步的 4 个批次中的每一个批次中获取值。因为这是一个双向 RNN，我们得到了 2 组预测。因此，形状是<code class="fe po pp pq ne b">[6, 4, 2]</code>而不是<code class="fe po pp pq ne b">[3, 4, 2]</code>(我们在上方的堆叠单向 RNN 的情况下观察到的<em class="pw">)。</em></p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="ede5" class="ni mm jf ne b gy nj nk l nl nm">print('Input: ', inputs.shape, '\n', inputs)<br/>print('\nOutput: ', out.shape, '\n', out)<br/>print('\nHidden: ', h_n.shape, '\n', h_n)</span><span id="1148" class="ni mm jf ne b gy pn nk l nl nm"><br/>###################### OUTPUT ######################</span><span id="40c1" class="ni mm jf ne b gy pn nk l nl nm">Input:  torch.Size([4, 5, 1]) <br/> tensor([[[ 1.],<br/>         [ 2.],<br/>         [ 3.],<br/>         [ 4.],<br/>         [ 5.]],<br/><br/>        [[ 6.],<br/>         [ 7.],<br/>         [ 8.],<br/>         [ 9.],<br/>         [10.]],<br/><br/>        [[11.],<br/>         [12.],<br/>         [13.],<br/>         [14.],<br/>         [15.]],<br/><br/>        [[16.],<br/>         [17.],<br/>         [18.],<br/>         [19.],<br/>         [20.]]])<br/><br/>Output:  torch.Size([4, 5, 4]) <br/> tensor([[[-0.4175, -0.6278, -0.0101, -0.4025],<br/>         [ 0.1271, -0.5579,  0.2162, -0.4832],<br/>         [-0.2557, -0.6714,  0.3084, -0.4927],<br/>         [ 0.0556, -0.6295,  0.3194, -0.4467],<br/>         [-0.1510, -0.6863,  0.3917, -0.6299]],<br/><br/>        [[-0.4311, -0.6939, -0.2381, -0.6894],<br/>         [ 0.1423, -0.5335, -0.0872, -0.6471],<br/>         [-0.2943, -0.6468,  0.0076, -0.6274],<br/>         [ 0.0392, -0.5691,  0.0595, -0.5576],<br/>         [-0.2070, -0.6238,  0.2187, -0.6570]],<br/><br/>        [[-0.4458, -0.6581, -0.6259, -0.8299],<br/>         [ 0.0999, -0.4501, -0.5715, -0.8090],<br/>         [-0.3441, -0.5669, -0.4723, -0.7729],<br/>         [-0.0133, -0.4705, -0.3131, -0.6745],<br/>         [-0.2617, -0.5444,  0.0042, -0.6820]],<br/><br/>        [[-0.4556, -0.6330, -0.7035, -0.8531],<br/>         [ 0.0780, -0.4118, -0.6690, -0.8358],<br/>         [-0.3608, -0.5393, -0.5730, -0.7989],<br/>         [-0.0285, -0.4442, -0.3958, -0.6973],<br/>         [-0.2739, -0.5259, -0.0447, -0.6868]]], grad_fn=&lt;TransposeBackward1&gt;)<br/><br/>Hidden:  torch.Size([6, 4, 2]) <br/> tensor([[[ 0.9455,  0.5653],<br/>         [ 0.9986, -0.1385],<br/>         [ 1.0000, -0.7900],<br/>         [ 1.0000, -0.9272]],<br/><br/>        [[ 0.1570,  0.2765],<br/>         [ 0.9959,  0.9972],<br/>         [ 1.0000,  1.0000],<br/>         [ 1.0000,  1.0000]],<br/><br/>        [[-0.6463,  0.5301],<br/>         [-0.5393,  0.6556],<br/>         [-0.4089,  0.7277],<br/>         [-0.3732,  0.7372]],<br/><br/>        [[ 0.0474, -0.5973],<br/>         [ 0.0082, -0.9715],<br/>         [-0.1373, -0.9681],<br/>         [-0.2362, -0.9658]],<br/><br/>        [[-0.1510, -0.6863],<br/>         [-0.2070, -0.6238],<br/>         [-0.2617, -0.5444],<br/>         [-0.2739, -0.5259]],<br/><br/>        [[-0.0101, -0.4025],<br/>         [-0.2381, -0.6894],<br/>         [-0.6259, -0.8299],<br/>         [-0.7035, -0.8531]]], grad_fn=&lt;StackBackward&gt;)</span></pre><p id="7015" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在让我们试着更详细地理解输出。根据文档，为了分离方向(向前和向后)，我们可以做以下事情-</p><ul class=""><li id="af26" class="ny nz jf lf b lg lh lj lk lm oa lq ob lu oc ly od oe of og bi translated"><code class="fe po pp pq ne b">out.view(seq_len, batch, num_directions, hidden_size)</code>向前和向后分别为方向 0 和 1。请记住，如果您使用了<code class="fe po pp pq ne b">batch_first=True</code>，那么它将是<code class="fe po pp pq ne b">out.view(batch, seq_len, num_directions, hidden_size)</code>。</li><li id="b17c" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated"><code class="fe po pp pq ne b">h_n.view(num_layers, num_directions, batch, hidden_size)</code>向前和向后分别为方向 0 和 1。</li></ul><h2 id="8695" class="ni mm jf bd mn om on dn mr oo op dp mv lm oq or mx lq os ot mz lu ou ov nb jl bi translated">堆叠 BiRNN 分离<code class="fe po pp pq ne b">out</code></h2><p id="3d62" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated">让我们使用<code class="fe po pp pq ne b">out.view(batch, seq_len, num_directions, hidden_size)</code>对堆叠的 BiRNN 输出进行整形，以分离出正向和反向值。</p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="228f" class="ni mm jf ne b gy nj nk l nl nm">out_reshaped = out.view(BATCH_SIZE, SEQ_LENGTH, 2, HIDDEN_SIZE)</span><span id="c3ae" class="ni mm jf ne b gy pn nk l nl nm">print("Shape of the output after directions are separated: ", out_reshaped.shape)</span><span id="4071" class="ni mm jf ne b gy pn nk l nl nm"><br/>###################### OUTPUT ######################</span><span id="57b1" class="ni mm jf ne b gy pn nk l nl nm">Shape of the output after directions are separated:  torch.Size([4, 5, 2, 2])</span></pre><p id="a991" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在的形状是— <code class="fe po pp pq ne b">(batch, seq_len, num_directions, hidden_size)</code>。</p><p id="129d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe po pp pq ne b">num_directions</code>是二次元。为了获得前向和后向输出，我们可以做以下事情</p><ul class=""><li id="d3c9" class="ny nz jf lf b lg lh lj lk lm oa lq ob lu oc ly od oe of og bi translated">out_forward = <code class="fe po pp pq ne b">(batch, seq_len, 0, hidden_size)</code></li><li id="707f" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated">out_backward = <code class="fe po pp pq ne b">(batch, seq_len, 1, hidden_size)</code></li></ul><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="76ae" class="ni mm jf ne b gy nj nk l nl nm">out_forward = out_reshaped[:, :, 0, :]<br/>out_backward = out_reshaped[:, :, 1, :]</span><span id="d1a3" class="ni mm jf ne b gy pn nk l nl nm">print("Forward output: ", out_forward.shape, "\n", out_forward)<br/>print("\n\nBackward output: ", out_backward.shape, "\n", out_backward)</span><span id="e0d7" class="ni mm jf ne b gy pn nk l nl nm"><br/>###################### OUTPUT ######################</span><span id="bd16" class="ni mm jf ne b gy pn nk l nl nm">Forward output:  torch.Size([4, 5, 2]) <br/> tensor([[[-0.4175, -0.6278],<br/>         [ 0.1271, -0.5579],<br/>         [-0.2557, -0.6714],<br/>         [ 0.0556, -0.6295],<br/>         [-0.1510, -0.6863]],<br/><br/>        [[-0.4311, -0.6939],<br/>         [ 0.1423, -0.5335],<br/>         [-0.2943, -0.6468],<br/>         [ 0.0392, -0.5691],<br/>         [-0.2070, -0.6238]],<br/><br/>        [[-0.4458, -0.6581],<br/>         [ 0.0999, -0.4501],<br/>         [-0.3441, -0.5669],<br/>         [-0.0133, -0.4705],<br/>         [-0.2617, -0.5444]],<br/><br/>        [[-0.4556, -0.6330],<br/>         [ 0.0780, -0.4118],<br/>         [-0.3608, -0.5393],<br/>         [-0.0285, -0.4442],<br/>         [-0.2739, -0.5259]]], grad_fn=&lt;SliceBackward&gt;)<br/><br/><br/>Backward output:  torch.Size([4, 5, 2]) <br/> tensor([[[-0.0101, -0.4025],<br/>         [ 0.2162, -0.4832],<br/>         [ 0.3084, -0.4927],<br/>         [ 0.3194, -0.4467],<br/>         [ 0.3917, -0.6299]],<br/><br/>        [[-0.2381, -0.6894],<br/>         [-0.0872, -0.6471],<br/>         [ 0.0076, -0.6274],<br/>         [ 0.0595, -0.5576],<br/>         [ 0.2187, -0.6570]],<br/><br/>        [[-0.6259, -0.8299],<br/>         [-0.5715, -0.8090],<br/>         [-0.4723, -0.7729],<br/>         [-0.3131, -0.6745],<br/>         [ 0.0042, -0.6820]],<br/><br/>        [[-0.7035, -0.8531],<br/>         [-0.6690, -0.8358],<br/>         [-0.5730, -0.7989],<br/>         [-0.3958, -0.6973],<br/>         [-0.0447, -0.6868]]], grad_fn=&lt;SliceBackward&gt;)</span></pre><h2 id="c0d2" class="ni mm jf bd mn om on dn mr oo op dp mv lm oq or mx lq os ot mz lu ou ov nb jl bi translated">堆叠 BiRNN 分离<code class="fe po pp pq ne b">h_n</code></h2><p id="7922" class="pw-post-body-paragraph ld le jf lf b lg ns kp li lj nt ks ll lm nu lo lp lq nv ls lt lu nw lw lx ly ij bi translated">让我们使用<code class="fe po pp pq ne b">h_n.view(num_layers, num_directions, batch, hidden_size)</code>将堆叠的 BiRNN 隐藏起来，以分离出向前和向后的值。</p><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="8e4a" class="ni mm jf ne b gy nj nk l nl nm">h_n_reshaped = h_n.view(3, 2, BATCH_SIZE, HIDDEN_SIZE)</span><span id="c7c2" class="ni mm jf ne b gy pn nk l nl nm">print("Shape of the hidden after directions are separated: ", h_n_reshaped.shape)</span><span id="3760" class="ni mm jf ne b gy pn nk l nl nm">###################### OUTPUT ######################</span><span id="36df" class="ni mm jf ne b gy pn nk l nl nm">Shape of the hidden after directions are separated:  torch.Size([3, 2, 4, 2])</span></pre><p id="8bb2" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在的形状是— <code class="fe po pp pq ne b">(num_layers, num_directions, batch, hidden_size)</code>。</p><p id="57fa" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe po pp pq ne b">num_directions</code>是第一维度。要获得向前和向后隐藏，我们可以做以下事情-</p><ul class=""><li id="8962" class="ny nz jf lf b lg lh lj lk lm oa lq ob lu oc ly od oe of og bi translated">hidden_forward = <code class="fe po pp pq ne b">(num_layers, 0, batch, hidden_size)</code></li><li id="6d80" class="ny nz jf lf b lg oh lj oi lm oj lq ok lu ol ly od oe of og bi translated">hidden_backward = <code class="fe po pp pq ne b">(num_layers, 1, batch, hidden_size)</code></li></ul><pre class="ma mb mc md gt nd ne nf ng aw nh bi"><span id="f345" class="ni mm jf ne b gy nj nk l nl nm">h_n_forward = h_n_reshaped[:, 0, :, :]<br/>h_n_backward = h_n_reshaped[:, 1, :, :]</span><span id="2a04" class="ni mm jf ne b gy pn nk l nl nm">print("Forward h_n: ", h_n_forward.shape, "\n", h_n_forward)<br/>print("\n\nBackward h_n: ", h_n_backward.shape, "\n", h_n_backward)</span><span id="3a17" class="ni mm jf ne b gy pn nk l nl nm"><br/>###################### OUTPUT ######################</span><span id="c759" class="ni mm jf ne b gy pn nk l nl nm">Forward h_n:  torch.Size([3, 4, 2]) <br/> tensor([[[ 0.9455,  0.5653],<br/>         [ 0.9986, -0.1385],<br/>         [ 1.0000, -0.7900],<br/>         [ 1.0000, -0.9272]],<br/><br/>        [[-0.6463,  0.5301],<br/>         [-0.5393,  0.6556],<br/>         [-0.4089,  0.7277],<br/>         [-0.3732,  0.7372]],<br/><br/>        [[-0.1510, -0.6863],<br/>         [-0.2070, -0.6238],<br/>         [-0.2617, -0.5444],<br/>         [-0.2739, -0.5259]]], grad_fn=&lt;SliceBackward&gt;)<br/><br/><br/>Backward h_n:  torch.Size([3, 4, 2]) <br/> tensor([[[ 0.1570,  0.2765],<br/>         [ 0.9959,  0.9972],<br/>         [ 1.0000,  1.0000],<br/>         [ 1.0000,  1.0000]],<br/><br/>        [[ 0.0474, -0.5973],<br/>         [ 0.0082, -0.9715],<br/>         [-0.1373, -0.9681],<br/>         [-0.2362, -0.9658]],<br/><br/>        [[-0.0101, -0.4025],<br/>         [-0.2381, -0.6894],<br/>         [-0.6259, -0.8299],<br/>         [-0.7035, -0.8531]]], grad_fn=&lt;SliceBackward&gt;)</span></pre></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="cdfd" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">感谢您的阅读。欢迎提出建议和建设性的批评。:)<em class="pw">T17】你可以在<a class="ae pr" href="https://www.linkedin.com/in/akshajverma7/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae pr" href="https://twitter.com/theairbend3r" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上找到我。如果你喜欢这个，看看我的其他博客。</em></p><figure class="ma mb mc md gt is gh gi paragraph-image"><a href="https://www.buymeacoffee.com/theairbend3r"><div class="gh gi qa"><img src="../Images/041a0c7464198414e6ce355f9235099e.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*SGCT6C60o4t58wRqeU2viQ.png"/></div></a></figure></div></div>    
</body>
</html>