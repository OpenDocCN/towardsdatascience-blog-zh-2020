# DiscoBERT:缩短你阅读时间的 BERT

> 原文：<https://towardsdatascience.com/discobert-a-bert-that-shortens-your-reading-time-be49d03e1ff?source=collection_archive---------33----------------------->

## 使用 BERT 来总结那些你不想阅读的长文档

![](img/8025259f01f04015af205665176d9dc8.png)

布雷迪·贝里尼在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

现在，世界联系如此紧密，我们不断受到来自各种不同来源的信息的轰炸，这可能会令人不知所措。社交媒体也改变了信息呈现给我们的方式，Instagram 和 Pinterest 等应用程序主要关注视觉内容，而不是文本。这使得当我们不得不阅读大段文字时，它就不那么吸引人了。

但是，如果这些大篇幅的文本可以转换成仅仅是关键点的摘要，那会怎么样呢？机器学习再次拯救了我们。

多年来，摘要一直是自然语言处理社区感兴趣的任务，有两种广泛使用的方法:**抽象**和**提取**。抽象方法侧重于理解整个文本并一次生成一个单词的摘要。另一方面，抽取式摘要侧重于从文本中选择重要信息(通常是句子),并将它们拼接在一起形成摘要。

抽象的摘要通常比提取的模型更简洁，但可能在语法上不正确，因为基于概率生成单词不是编写摘要的最直观的方式。然而，这两种方法都有一个共同的问题。他们无法跟踪文档中的长期上下文。例如，如果一篇新闻文章在第一段谈到了利奥·梅西，在第四段又提到了他，这两种方法都很难将这些提及的上下文联系起来。对于抽象模型(主要是 Seq2Seq 模型)，这是因为 Seq2Seq 模型比其他模型更重视最近的单词，而提取模型通常意味着几个句子或一个段落，而不是整个文档。

微软 Dynamics 365 AI Research 的研究人员旨在通过提出他们自己的模型来改进这些摘要技术，**用于文本提取的话语感知 BERT**(**disco BERT**)。这个名字透露了一个事实，即它是一个提取模型，但有一个扭曲。与其他提取模型不同，DiscoBERT 提取**基本话语单元** ( **EDUs** ，它们是句子的一部分)，而不是句子用于其摘要。这有助于生成的摘要简洁，因为整个句子可能包含一些 EDUs 中不存在的无关信息。

它还通过创建文档的两个不同的图来解决长期上下文问题。第一张图，****RST 图**，是基于修辞结构理论。这一理论声称，文本的结构可以归结为一棵树的独立单元。以类似的方式，RST 图试图分析这些单位之间的依赖关系，并确定它们在句子中的作用。第二张图，**共指图**，着重于建立对贯穿全文讨论的关键实体(人、组织等)、事件和主题的理解。**

**通过这两个图表，DiscoBERT 了解了文本的重要区域是什么，并在此基础上对 edu 进行了排序。为了生成摘要，DiscoBERT 选择排名前 *N* 的 edu(也考虑了依赖关系)并按时间顺序将它们缝合在一起。**

**研究人员通过在两个新闻数据集上运行 DiscoBERT 来验证它，一个由来自纽约时报的文章组成，另一个来自 CNN 和 DailyMail，它们需要一个模型来总结每篇文章。DiscoBERT 在这两个数据集上实现了新的最先进的性能。**

**"*大多数行业新闻应用在新闻推荐方面做得很好，但我们希望以一种更高效、更容易理解的方式发布新闻。微软动态 365 人工智能研究院高级首席研究经理刘晶晶说，他负责监督这个项目。所以，下次当你坐公交车想快速浏览每日新闻时，让 DiscoBERT 来帮你吧！***

**如果你想了解更多关于 DiscoBERT 的信息，这里有一个[链接](https://arxiv.org/pdf/1910.14142.pdf)到论文，如果你想尝试训练它总结你的故事、报告等等，这里有一个[链接](https://github.com/jiacheng-xu/DiscoBERT)到代码，点击[这里](https://aka.ms/mmai)查看更多我们的出版物和其他工作。**

****参考文献****

**1.威廉·C·曼和桑德拉·A·汤普森，1988，**修辞结构理论:走向语篇组织的功能理论**，《语篇研究的语篇跨学科期刊》，8(3):243–281。**

**2.徐，贾成，柘淦，于成，，**话语感知的神经抽取文本摘要**，*计算语言学协会第 58 届年会论文集*，第 5021–5031 页。2020.**