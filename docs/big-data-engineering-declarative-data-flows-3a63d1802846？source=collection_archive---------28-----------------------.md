# 大数据工程—声明性数据流

> 原文：<https://towardsdatascience.com/big-data-engineering-declarative-data-flows-3a63d1802846?source=collection_archive---------28----------------------->

## 通过使用声明性语言方法来分离功能性和非功能性需求。

![](img/8bce01b035b0989f1f515551f3b1c7ed.png)

迈克·本纳在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

这是大数据环境中的数据工程系列的第 3 部分。它将反映我个人的经验教训之旅，并在我创建的开源工具 [Flowman](https://flowman.readthedocs.io) 中达到高潮，以承担在几个项目中一遍又一遍地重新实现所有锅炉板代码的负担。

*   [第 1 部分:大数据工程—最佳实践](https://medium.com/@kupferk/big-data-engineering-best-practices-bfc7e112cf1a)
*   [第 2 部分:大数据工程— Apache Spark](https://medium.com/@kupferk/big-data-engineering-apache-spark-d67be2d9b76f)
*   第 3 部分:大数据工程—声明性数据流
*   [第 4 部分:大数据工程— Flowman 启动并运行](/big-data-engineering-flowman-up-and-running-cd234ac6c98e)

# 期待什么

本系列是关于用 Apache Spark 构建批处理数据管道的。但是有些方面对于其他框架或流处理也是有效的。最后，我将介绍 [Flowman](https://flowman.readthedocs.io) ，这是一个基于 Apache Spark 的应用程序，它简化了批处理数据管道的实现。

# 功能需求

本系列的第 1 部分已经指出了两种需求的存在，它们适用于几乎所有类型的应用程序:功能性需求和非功能性需求。让我们首先关注第一类需求。

功能需求描述了解决方案首先应该解决的实际问题。它们描述了核心功能以及应该实现什么。这可能是一个数据处理应用程序，需要集成多个数据源并执行聚合，以便为一些数据科学家提供简化的数据模型。但是功能需求的想法也适用于预订不同城市旅行的 Android 应用程序。

功能需求总是由作为最终用户代理的业务专家编写的。他们应该关注要解决的问题，这样开发人员或架构师仍然可以决定可行的解决方案应该是什么样子。当然，在实现之前，应该与业务专家一起验证所选择的方法。

注意我特别写了功能需求应该关注问题(或任务)而不是解决方案。这种微小的差异非常重要，因为它为开发人员留下了更大的设计空间来寻找最佳的可能解决方案，而不是由业务专家给出的特定解决方案，后者可能很难在给定的技术范围内实现。

# 非功能性需求

除了描述软件必须实现的主要任务的功能性需求之外，还有非功能性需求。它们描述了必须实现的一切，但不是核心功能的直接部分。

数据处理管道的典型要求如下:

*   包含有意义的信息、警告和错误消息的日志被推送到中央日志聚合器。
*   关于已处理记录数量的指标被发布并推送到一个中央指标收集器。
*   上游系统在一小时内生成的输入数据的处理时间不会超过一小时。
*   应用程序必须在现有的集群基础设施(可能是 Kubernetes、YARN 或者甚至是 Mesos)中分布式运行
*   随着数据量的预期增长，应用程序应该能够轻松扩展其处理吞吐量。

到目前为止，所有这些要求都非常技术性。特别是在数据处理管道的情况下，我会添加以下要求:

*   业务专家应该能够阅读并(至少粗略地)理解实现的转换逻辑。

这最后一个要求可能是有争议的，但是我发现如果实现了它会非常有帮助。能够与业务专家或利益相关者讨论特定的解决方案，作为一个额外的验证和信任层，是非常宝贵的。幸运的是，在数据处理领域，业务专家通常至少知道 SQL，并且很好地理解典型转换的概念，如过滤、连接和分组聚合。所以我们需要的是一个足够简单易读的逻辑表示。

# 关注点分离

![](img/1e70d84606e420ab36ec4da8c8009718.png)

西蒙·瑞在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

我们刚刚看到，几乎所有的应用程序都必须实现两类不同的需求，功能性需求和非功能性需求。一个简单但重要的观察结果是，在一个项目中，甚至在一个公司中，不同应用程序的功能需求因情况而异(因为每个应用程序都要解决不同的问题)，但非功能需求通常是相同的，至少在特定的应用程序类型中是如此(如“数据处理管道”或“web 应用程序”)。

每个开发人员现在都应该有一个类似“可重用代码”的术语，甚至可能有“关注点分离”的概念。很明显，在大多数非功能性需求都相同的情况下，这是一条可行之路。如果我们能够构建一个解决方案，其中功能性和非功能性需求在不同的层实现，并且如果我们还成功地共享了负责所有这些非功能性需求的代码，那么我们将收获良多:

*   通过非功能性需求的共享实现，所有的应用程序都将从任何改进中受益。
*   由于统一的指标和日志记录，所有应用程序都可以以非常相似的方式集成到整个 IT 环境中。
*   操作也得到简化，因为关于一个应用程序的知识和问题解决策略可以应用于所有其他应用程序，使用相同的共享基础层来解决非功能性需求。
*   作为功能需求的一部分实现的核心业务逻辑没有与技术细节混杂在一起，因此更容易被业务专家理解和验证。

下一节将描述一种基于这些想法构建解决方案的可能方法。

# 声明性数据管道

![](img/10a074b8e4870f4a5171df41955758f8.png)

[斯科特·格雷厄姆](https://unsplash.com/@sctgrhm?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照

通过使用适当的业务逻辑高级描述语言，然后由较低的数据流执行层解析和执行，我们可以获得我上面描述的所有优点。这是我在 Apache Spark 上实现通用数据处理应用程序时选择的路线。

基本上，这个想法是使用简单的 YAML 文件，它包含所有数据转换的纯声明性描述，类似于 Kubernetes 中的部署描述。在我给出一些小例子之前，让我解释一下这种方法的好处:

*   通过构建在 Apache Spark 之上，我们确保获得 Spark 的所有好处，如可扩展性、可伸缩性、丰富的连接器等等。
*   具有潜在更高抽象级别的描述语言允许在类似于原始业务需求的级别上指定逻辑(如构建历史、在复杂的 JSON 文档中提取子树等)，这些在 Spark DataFrame API 级别上不直接可用。
*   使用没有任何流控制的纯声明性语言有助于关注逻辑流，从而防止在单个源文件中混合功能性和非功能性需求。
*   声明性语言还可以用于以低成本提取和提供附加信息，如数据沿袭或输出模式。此信息可用于自动化工作流、模式管理和生成指标。

# 示例数据流

下面几节给出了一个简单的数据流示例。乍一看，这个例子可能看起来有点复杂，但是您会发现它还包含许多有价值的详细信息，这些信息在某种程度上是必需的，并且可以用于上面提到的额外好处之一，如自动模式管理。

对于这个例子，我将建立一个小的数据处理管道，用于处理来自 NOAA 的包含天气测量的"[综合地表数据集](https://www.ncdc.noaa.gov/isd)。这是一个非常有趣的大型数据集，包含了从全球到 1901 年的天气信息。但是它使用了非常复杂的自定义格式(没有 CSV)，其中一些基本的测量可以在固定的位置提取(我们将做的)。在这个例子中，我们不会直接访问他们的服务器，但是我假设一些数据被下载到某个私有位置(比如本地文件系统，S3 或者 HDFS)。

## 1.源位置

首先，在从 NOAA 服务器下载原始测量值之后，我们需要声明原始测量值存储的物理源位置:

```
relations:
  measurements-raw:
    kind: file
    format: text
    location: "s3a://dimajix-training/data/weather/"
    pattern: "${year}"
    schema:
      kind: embedded
      fields:
        - name: raw_data
          type: string
          description: "Raw measurement data"
    partitions:
      - name: year
        type: integer
        granularity: 1
```

这个定义已经包含了大量关于称为“测量值-原始值”的源关系的信息:

*   **种类:文件** —数据存储为文件
*   **格式:文本** —数据存储为文本文件(每行代表一条记录)
*   **位置:…** —数据存储在 S3 的指定位置
*   **模式:${year}** —该位置包含分区，其目录名简单地采用$year 的形式(分区将出现在下面)
*   **模式:…** —数据有一个特定的模式，其中有一个名为“raw_data”的列
*   **分区:…** —按“年”对关系进行分区，即其数据沿“年”轴拆分成更小的块。

您看，这已经包含了很多信息，这些信息都是处理关系所必需的(或者至少是有价值的)。

## 2.读出数据

现在下一步是从上面声明的关系中读入数据。这可以通过以下*映射*规范来实现:

```
mappings:
  measurements-raw:
    kind: read
    relation: measurements-raw
    partitions:
      year: $year
```

我不会像在第一小节中那样遍历所有行——代码应该是不言自明的:该规范指示从先前定义的关系“measurements-raw”中读取单个分区$year。“$year”开头的美元表示这是一个变量，必须在某个地方定义或在执行前显式设置。

## 3.提取测量值

正如我在示例开始时解释的那样，一些度量可以在固定的位置用单个记录提取(其他度量存储在可选的和动态的位置——我们在这里不关心它们)。这可以通过以下*选择映射*来实现:

```
mappings:
  measurements:
    kind: select
    input: measurements-raw
    columns:
      usaf: "SUBSTR(raw_data,5,6)"
      wban: "SUBSTR(raw_data,11,5)"
      date: "SUBSTR(raw_data,16,8)"
      time: "SUBSTR(raw_data,24,4)"
      report_type: "SUBSTR(raw_data,42,5)"
      wind_direction: "SUBSTR(raw_data,61,3)"
      wind_direction_qual: "SUBSTR(raw_data,64,1)"
      wind_observation: "SUBSTR(raw_data,65,1)"
      wind_speed: "CAST(SUBSTR(raw_data,66,4) AS FLOAT)/10"
      wind_speed_qual: "SUBSTR(raw_data,70,1)"
      air_temperature: "CAST(SUBSTR(raw_data,88,5) AS FLOAT)/10"
      air_temperature_qual: "SUBSTR(raw_data,93,1)"
```

现在您可以看到这个映射是如何通过其“**输入**字段引用第一个映射的，然后指定要生成的列列表以及每个列的表达式。

## 4.目标位置

现在，我们希望将提取的数据写入一个适当的 Hive 表，再次按年份进行分区。在指定写操作本身之前，我们首先需要创建一个新的目标关系，数据应该被写入其中。具体规定如下:

```
relations:
  measurements:
    kind: hiveTable
    database: "weather"
    table: "measurements"
    format: parquet
    partitions:
    - name: year
      type: int
    schema:
      kind: mapping
      mapping: measurements
```

这个关系描述了一个名为“天气”的数据库中的一个名为“测量”的配置单元表。数据应该存储为 Parquet 文件，模式应该从映射“measurements”中隐式地推断出来，这是我们在步骤 3 中定义的。

## 5.构建目标

现在我们有了一个输入关系、一些提取逻辑和一个目标关系，我们需要创建一个构建目标，它告诉我们由“measurements”映射生成的所有记录都应该被写入“measurements”目标(它反过来表示具有相同名称的 Hive 表)。这可以通过以下方式完成:

```
targets:
  measurements:
    kind: relation
    relation: measurements
    mapping: measurements
    partition:
      year: $year
```

称为“度量”的构建目标将映射“度量”的结果与关系“度量”结合起来作为它的目标位置。

您可能会问为什么我们需要明确地指定一个关系和一个构建目标。原因很简单，关系是一个逻辑对象，既可以用于读，也可以用于写(甚至可能在同一个应用程序中)。通过将*声明*与*写*和*读*操作分开，我们自动增加了重用单个关系的可能性。

## 6.构建作业

我们就要完成了，但是最后一个小细节还没找到。很可能您为不同的关系创建了多个目标，但是您不想执行所有的目标。因此，最后一步需要一个*构建作业*，它主要包含要构建的目标列表。此外，构建作业也是指定正确执行所需的运行时参数的好地方。在我们的示例中，年份是一个自然的候选，因为您可能希望在不同的年份独立运行相同的数据流:

```
jobs:
  main:
    parameters:
      - name: year
        type: Integer
        default: 2013
    targets:
      - measurements
```

## 摘要

让我们总结一下我们需要为工作数据管道指定哪些细节:

*   **关系。**像在传统数据库中一样，我们需要指定我们想要读取或写入的物理数据源。我选择“关系”这个术语只是因为这正是 Spark 中使用的术语。关系可以指文件、配置单元表、JDBC 源或任何其他物理数据表示。
*   **映射。**转换(和读操作)被建模为映射。虽然我们只使用了两种非常简单的类型(“read”和“select”)，但是您可以想象任何类型的数据转换都可以表示为映射。
*   **目标明确。**数据处理管道通常可与传统的构建管道相媲美——除了它创建新数据而不是应用程序。因此，我重用了构建工具中的一些概念和术语。构建“目标”表示将数据写入关系所需执行的工作量。
*   **乔布斯。**通常(尤其是在批处理中)你不仅只有一个输出，还需要用不同的转换写入多个输出。每个写操作被表示为一个构建目标，这些操作的集合被表示为一个作业。

为了完善“数据构建工具”的概念，Flowman 本身也支持构建阶段，就像它们在 Maven 中一样:

*   **创造。**第一阶段是创建和/或迁移任何关系。这解决了整个“模式管理”部分。
*   **打造。第二阶段将执行构建目标隐含的所有写操作。**
*   **验证。**验证阶段将检查先前执行的写操作是否确实导致一些记录被写入相应的关系。作为单元测试的一部分，验证阶段也可以用来执行一些测试。
*   **截断。**虽然前三个阶段是关于创建关系和数据，截断阶段将通过删除关系的内容来执行一些清理，但是它将保留关系本身(配置单元表、目录等)。
*   **干净。**最后最后一个阶段也将去除物理关系本身。

我们在上面的规范中没有明确提到构建阶段，你甚至不能这样做。构建阶段是数据管道的*执行*的一部分，我将在下一节讨论。

# 执行数据流

我刚刚向您展示了一个简单数据流的小(虽然冗长)示例，它代表了读取、提取和存储数据的非常基本和典型的操作。当然，你不能直接执行这些 YAML 文件。您现在需要的是一个理解上述特定语法的应用程序，然后它可以执行作业、目标、读操作、转换和写操作。

Flowman 就是做这个工作的。这是我为实现这种非常特殊的方法而创建的开源应用程序，在这种方法中，数据流与执行逻辑相分离。它在很大程度上构建于 Apache Spark 之上，如果你有一点 Spark 的经验，你很可能会想象上面的 YAMLs 是如何用 DataFrame API 执行的。

# 设计优势

也许我在这个系列的[第一部分](/big-data-engineering-best-practices-bfc7e112cf1a)和[第二部分](/big-data-engineering-apache-spark-d67be2d9b76f)中提到的话题现在开始对你更有意义了。或者你可能觉得这种方法太复杂了。不管是哪种情况，让我试着指出这种设计相对于直接使用 Apache Spark 的经典应用程序的一些优点。

通过依赖具有特定实体模型(关系、映射、目标和作业)的高级描述语言，您可以确保数据管道的统一方法。这有助于简化操作并遵循最佳实践(至少我是这么理解的)——实际上，这种方法很难不遵循它们。

接下来，当实现数据管道时，您主要在高层次上工作，技术细节和可能的解决方法隐藏在执行层(Flowman)。

这种方法的另一个优点是，许多非功能方面，如模式管理(创建和更改配置单元表)、提供有意义的运行时指标等，可以在执行层实现，而不会弄乱规范。这是作为构建阶段的一部分完成的，构建阶段完全在执行层实现，不需要在数据流规范本身中明确指定。

最后，即使是倾向于技术的业务专家也能比一个完整的 Spark 应用程序(无论是用 Scala、Java、Python 等编写的)更好地理解 YAML 文件，后者也包含许多样板代码。

# 最后的话

这是关于使用 Apache Spark 构建健壮的数据管道的系列文章的第三部分。这一次，我向您展示了以非常统一的方式实现数据处理管道的非常具体的方法。它不包含任何 Spark 代码(你可以在 GitHub 上查看 [Flowmans 源代码)——这代表了我对数据管道应该是什么样子的看法。](https://github.com/dimajix/flowman)

下一部分将介绍如何在本地 Linux 机器上安装和运行 Flowman。