<html>
<head>
<title>Crystal Clear Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">清晰的强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/crystal-clear-reinforcement-learning-7e6c1541365e?source=collection_archive---------25-----------------------#2020-06-24">https://towardsdatascience.com/crystal-clear-reinforcement-learning-7e6c1541365e?source=collection_archive---------25-----------------------#2020-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9f455388fcf4299117964598a03a2004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ifjBF_naONII_uyDXCgSnw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><div class=""/><div class=""><h2 id="cf95" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">强化学习的综合简明概念</h2></div><p id="d1f3" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">强化学习(RL)是人工智能和机器学习的最热门领域，在过去几年中有许多惊人的突破。这篇文章试图给出整个RL光谱的一个简明的视图，而不要过多地进入数学和公式，同时也不要忽略茂密复杂的森林中的树木。</p><p id="1095" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">现在首先让我们了解什么是强化学习或RL？</p><p id="03ce" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">RL意味着在头脑中考虑<strong class="kz jj">长期</strong>结果或<strong class="kz jj">累积</strong> <strong class="kz jj">回报</strong>的情况下采取最佳行动。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lt"><img src="../Images/da56e5bae6b7f321abc528e8e3799259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7TDcDII6iH9tKpMBH3enwA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="90ee" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">强化学习(RL)是通过<strong class="kz jj">与<strong class="kz jj">环境</strong>交互</strong>进行的学习。强化学习<strong class="kz jj">代理</strong>从其<strong class="kz jj">动作</strong>的结果中学习，而不是从明确的教导中学习，它根据其过去的经验(利用)以及新的选择(探索)来选择其动作，这是必不可少的，试错学习就像孩子学习一样。RL-agent接收到的强化信号是一个数字的<strong class="kz jj">奖励</strong>，它编码了一个动作结果的成功，agent试图学习选择那些<strong class="kz jj">随着时间的推移使累积奖励最大化的动作。</strong></p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ly"><img src="../Images/6a78b9ec78cfb07cb0c36b5f703ecebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-L-bqrtDZSOqucJuywPOYw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="1569" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在我们深入研究RL之前，让我们先看看为什么RL在人工智能和机器学习领域如此重要。参考下面的图表，它显示了RL在生活的各个领域中的应用。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lz"><img src="../Images/c890fc388b4d6ea0f1118d1565ac9554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W3Fe5EWa2HjH4CjDZJt54w.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="1e80" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">现在，我将介绍一些经常用来解释强化学习的术语，在深入研究算法和更吸引人的概念之前，人们必须理解这些术语。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ma"><img src="../Images/e7cf90e837f4ab3e9b05c92fab6fe4a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6wAYVUgDsfYBNZ2HKeZiCQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="f674" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在上图中，我列出了名称、标准符号和图示。现在，我将分别对它们进行定义。</p><h2 id="a04c" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">代理人</h2><p id="6762" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">任何使用某种传感器感知其环境并能够在环境中产生行动的东西都被称为代理。代理执行动作，接受观察，并给予奖励。</p><h2 id="1382" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">环境</h2><p id="c192" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">环境是代理交互的总体表示。该媒介不被认为是环境的一部分。环境接受一个动作，发出观察和奖励。</p><h2 id="3829" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">状态</h2><p id="93c1" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">状态描述了当前的情况，并决定了接下来会发生什么。代理可能有状态的局部视图，这被称为<strong class="kz jj">观察</strong>。</p><h2 id="e59e" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">报酬</h2><p id="4c77" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">当一个代理在一个州采取行动时，它会收到一个奖励。这里的术语“奖励”是一个抽象的概念，描述来自环境的反馈。奖励可以是正面的<strong class="kz jj">或负面的</strong>。当奖励为正时，对应的是我们通常意义上的奖励。当奖励为负时，对应的就是我们通常所说的“惩罚”。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/5918bb4cf0f3bcb424c06816ff750dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L0uC-xgcsKxkbvbwqKWBfQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Pixabay，Atari &amp; Tesla图像用于此插图图片</p></figure><p id="3bf2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">奖励是代理在每个时间步从环境中得到的<strong class="kz jj">反馈</strong>。然后，它可以使用这个奖励信号(对于好的行为可以是积极的，对于坏的行为可以是消极的)来推断在一种状态下如何表现。总的来说，目标是解决一个给定的任务，尽可能获得最大的回报。这就是为什么许多算法对代理采取的每一个动作都有一个微小的负奖励，以激励它尽快解决任务。奖励是在RL中至关重要的设计决策，因为这将鼓励RL代理优化行为。例如，以更少的时间赢得象棋比赛或以平均速度驾驶汽车但不发生任何碰撞(汽车可以在空旷的高速公路上加速，而在繁忙的道路上缓慢行驶)。奖励告诉我们什么是好的<strong class="kz jj">直接感觉</strong>。比如雅达利游戏，每一个时间步代理都是得分或者失分的。</p><h2 id="1d30" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">贴现因素</h2><p id="1685" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">现在的奖励<strong class="kz jj">比未来<strong class="kz jj">的奖励</strong>更有价值。贴现因子，通常表示为γ，是一个乘以未来预期<em class="na">回报</em>的因子，在[0，1]的范围内变化。它控制着未来回报相对于眼前回报的重要性。折扣因子越低，未来的回报就越不重要，代理人<em class="na"> </em>将倾向于专注于只产生即时回报的行动。贴现率被限制为小于1的事实是一个使<strong class="kz jj">无限和</strong>有限的数学技巧。这有助于证明某些算法的收敛性。贴现因子用于确定未来报酬</strong>的<strong class="kz jj">现值(类似于由于通货膨胀等因素贴现的未来货币的现值)。</strong></p><h2 id="70e5" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">状态值</h2><p id="7830" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">状态值“V(S)”是从该状态开始的一段时间内代理可以期望积累的奖励总额。与奖励信号不同，价值函数指定了<em class="na">长期</em> 中什么是<strong class="kz jj">好的。</strong></p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/32919b81cb7542524820ae3a3a18959e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7e_bsYcUNOc_DsBY3ykIMA.png"/></div></div></figure><h2 id="f972" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">行动</h2><p id="358d" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">动作是代理在每个状态下可以做的事情。例如，机器人的步幅只能是0.01米到1米。围棋程序只能在19 x 19(即361)个位置中的一个位置放下棋子。自动驾驶汽车可能会向不同的方向行驶。</p><h2 id="470d" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">动作值</h2><p id="054f" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">动作值或状态-动作值函数(Q函数)指定代理在具有策略π的状态下执行特定动作的良好程度。Q函数用Q(S，A)表示。它表示在遵循策略π的状态下采取行动的价值。这里的‘q’指的是一个动作的<strong class="kz jj">质量</strong>。</p><h2 id="f050" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">政策</h2><p id="5111" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">策略是代理从状态中选择的动作。策略是在一种状态下行动的映射，这种状态可以是<strong class="kz jj">确定性的</strong>或<strong class="kz jj">随机性的</strong>。该策略由<strong class="kz jj"> π表示。</strong></p><h2 id="115d" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">环境模型</h2><p id="9ff9" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">这个模型是对环境的模拟。它知道从一个状态到另一个状态的转换，以及对环境的高层次理解。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/4d9700585486be73f21850bf4b79e18b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dW4Qb7--xcfmERxZWteIsA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="d224" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">现在，我将介绍一些基本概念，如勘探、开发、策略上、策略下等。在我们深入研究RL算法之前。</p><h2 id="d4de" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">开采和勘探</h2><p id="ee1c" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">开发和探索的概念与人类的本性有着内在的联系，作为人类，我们更喜欢已知而不是未知。例如，去餐馆，你可以选择去你最喜欢的餐馆，因为你已经喜欢那里的食物，但除非你尝试另一家餐馆，否则你不会知道是否存在更好的餐馆。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nd"><img src="../Images/51bb02f28500b4eb23497e0cb7383d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CALR2iVfPNj-KzY0b-Mztg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="d1fd" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">因此，开发就是去做同样的事情，从一个国家获得最大的价值(这通常被称为贪婪的行为)，而探索就是尝试新的活动，从长远来看，这些活动可能会带来更好的回报，即使眼前的回报可能并不令人鼓舞。在上图中，如果代理人只考虑即时奖励，通过跟随红色路径获得最大奖励，它将在稍后发现具有更高价值的蓝色路径，即使即时奖励较低。这就是为什么需要探索来获得更好的长期回报。</p><h2 id="4019" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">符合政策和不符合政策</h2><p id="fa3c" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">在非策略学习中，我们评估目标策略(π)，同时遵循另一个称为<strong class="kz jj">的策略行为策略</strong> (μ)(这就像机器人遵循视频或基于<strong class="kz jj">另一个代理</strong>获得的经验的代理学习)。“在策略上”表示代理遵循它用来更新目标的相同策略。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ne"><img src="../Images/05a0de43e57ca485ba887dc085b20ca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PeD-bFmQ5usYd6QDUssnRw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><h2 id="e364" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">规划和学习</h2><p id="01b4" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">RL算法可分为<strong class="kz jj">规划</strong>或<strong class="kz jj">学习</strong>算法。在规划中，代理需要知道环境的模型(代理知道转移概率)，然后在该模型下规划最优策略。在学习问题中，代理不了解环境的动态，它需要通过<strong class="kz jj">试错</strong>进行交互，以了解环境，从而得出最优策略或最优状态值。</p><h2 id="5c1a" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">基于模型和无模型</h2><p id="1bef" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">在基于模型的RL问题中，代理知道环境的模型，这意味着它知道转换动态，并且主要目标是计划最优动作。在无模型RL中，代理不知道转换或其他动态；相反，它必须通过与环境的互动来学习什么样的可选动作会产生最佳结果。</p><h1 id="e485" class="nf mc ji bd md ng nh ni mg nj nk nl mj ko nm kp mm kr nn ks mp ku no kv ms np bi translated">强化学习“规划”算法</h1><h2 id="e8ec" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">马尔可夫决策过程(MDP)</h2><p id="cccc" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">马尔可夫决策过程(MDP)是复杂决策过程的数学表示。(MDP)正式描述强化学习的环境。环境是完全可观察的和静止的(意味着规则不随时间改变)。几乎所有的RL问题都可以用MDP来形式化。MDP的定义是:</p><ul class=""><li id="7d71" class="nq nr ji kz b la lb ld le lg ns lk nt lo nu ls nv nw nx ny bi translated">一个状态S，它代表了在一个定义的世界中，一个人可能处于的每一个状态。只有<strong class="kz jj">现在重要，</strong>这意味着转移函数仅取决于当前状态S，而不取决于任何先前状态。</li></ul><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/29a01251d32b65000b26ecdd47ee2fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rLK0R4-cDjX6-HwjTDWJg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><ul class=""><li id="839b" class="nq nr ji kz b la lb ld le lg ns lk nt lo nu ls nv nw nx ny bi translated">一个模型或转移函数P，它是当前状态、所采取的行动以及我们最终所处的状态的函数。这种转变产生了一定的概率，以状态S '结束，在途中获得奖励R，从状态S开始，并采取行动a。</li><li id="9852" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls nv nw nx ny bi translated">奖励是处于一种状态的一个标度值。它告诉我们进入状态的用处。</li></ul><p id="305b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">MDP的最终目标是找到一个政策，它可以告诉我们，对于任何一个州，该采取什么行动(记住，将州映射到行动是我们的政策)。最优策略是使长期预期回报最大化的策略。<strong class="kz jj">一旦找到最优策略，RL问题就解决了</strong>。</p><p id="e3bb" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在RL中，我们考虑如何计算任意策略π的状态值函数V(S)。这在RL文献中被称为<strong class="kz jj">政策评估</strong>(也称为<strong class="kz jj">预测</strong>问题)。通过使新策略相对于原始策略的价值函数变得<strong class="kz jj">贪婪</strong>来改进原始策略的过程被称为<strong class="kz jj">策略改进或控制。</strong></p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/cc43a06183918602709cd149c4b984a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVOQJ9HW6ao29R0FL9Xghg.png"/></div></div></figure><h2 id="06ce" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">广义策略迭代</h2><p id="714b" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">广义策略迭代(GPI)是指让策略评估和策略改进过程相互作用的总体思想，独立于两种方法的粒度和其他细节。这就像梅西和c罗在同一支球队踢一场足球赛。他们每次传球都是为了个人进球而互相竞争，但他们也互相协助为球队赢得比赛。同样，GPI就像一场足球比赛，最终目标是找到最优政策/最优价值函数。在一次扫描中(可能是全部情节，也可能是每一步或几步)，对策略进行评估，以计算出该策略的价值。然后通过对旧政策采取贪婪的行动来改进政策。重复这一过程，直到出现收敛，并找到最优策略或最优值。</p><h2 id="2622" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">贝尔曼方程</h2><p id="ee23" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">特定状态的值等于当前状态的奖励，加上从该点开始从当前状态S过渡到未来状态S’时得到的折扣奖励。</p><h2 id="6112" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">价值迭代</h2><p id="9d51" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">为了求解贝尔曼方程，我们通常从所有状态的任意值开始，然后基于邻居(从当前状态可以到达的所有状态)更新它们。最后，我们重复直到收敛。这个过程叫做<strong class="kz jj">“价值迭代。”</strong>值迭代是一个组合<strong class="kz jj">一个(或多个)</strong>政策评估扫描，然后执行<strong class="kz jj">另一个政策改进扫描</strong>。重复这个过程，直到收敛发生。价值迭代包括:<strong class="kz jj">寻找最优价值函数</strong> +一个<strong class="kz jj">策略抽取</strong>。这两者没有重复，因为一旦价值函数是最优的，那么由此得出的策略也应该是最优的(即，收敛的)。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi og"><img src="../Images/21a6cba2e422634156fc3a71d75178df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J1R5cv86xn_SmcNS0CkeoQ.png"/></div></div></figure><h2 id="692b" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">策略迭代</h2><p id="fae7" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">这是一个GPI过程，其中通过执行单调改进的策略和值函数来改进策略:通过重复策略评估，然后执行策略改进。每一个策略都<strong class="kz jj">保证</strong>是对前一个策略的严格改进(除非它已经是最优的)。因为有限的MDP只有有限数量的策略，所以这个过程必须在有限数量的迭代中<strong class="kz jj">收敛到最优策略</strong>和最优值函数。这种寻找最优策略的方式称为策略迭代。策略迭代包括<strong class="kz jj">策略评估</strong> + <strong class="kz jj">策略改进</strong>，两者反复迭代直到策略收敛。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oh"><img src="../Images/a9cadc76301808f7decb1e99e820c12f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rNFCSpGSOJepKjHrYqsu7A.png"/></div></div></figure><h1 id="938f" class="nf mc ji bd md ng nh ni mg nj nk nl mj ko nm kp mm kr nn ks mp ku no kv ms np bi translated">RL“学习”算法</h1><p id="6cb8" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">在RL问题中，我们的目标是找到每个状态的最优值或者找到最优策略。根据算法是如何制定的，它们大致分为三类。我们还可以根据策略(开或关)选择不同的模式</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/d95df160da53e2b66f1995bfcbe513aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uW79NHhW4vC7NELLYLVhzQ.png"/></div></div></figure><h1 id="5ed9" class="nf mc ji bd md ng nh ni mg nj nk nl mj ko nm kp mm kr nn ks mp ku no kv ms np bi translated">基于值的算法</h1><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/e32559baaf8106d64852d4d9e1c49416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Sgfw0jYqvnyxncVnHl69w.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><h2 id="aa7d" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">蒙特卡洛学习</h2><p id="c1f7" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">蒙特卡罗方法是一个简单的概念，当它与环境交互时，代理学习状态和奖励。在这种方法中，代理生成经验样本，然后基于平均回报，为状态或状态-动作计算值。下面是蒙特卡罗(MC)方法的关键特征:</p><ol class=""><li id="1996" class="nq nr ji kz b la lb ld le lg ns lk nt lo nu ls ok nw nx ny bi translated">没有模型(代理不知道状态MDP转换)</li><li id="c2d0" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">代理人<strong class="kz jj">从<strong class="kz jj">采样的</strong>经验中学习</strong></li><li id="b614" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">通过体验来自所有采样剧集的<strong class="kz jj">平均</strong>回报，学习策略π下的状态值vπ(s )(值=平均回报)</li><li id="fd64" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">仅在<strong class="kz jj">完成一集</strong>之后，值才被更新(因为该算法收敛缓慢，并且更新发生在一集完成之后)</li><li id="5759" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">没有<strong class="kz jj">自举</strong></li><li id="6caf" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">仅可用于<strong class="kz jj">偶发性问题</strong></li></ol><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/e98716cf12d6328e0602c34cc769733c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BwF-fhimL85BosMvVbntjA.png"/></div></div></figure><p id="41ca" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在蒙特卡罗方法中，我们使用代理人根据政策采样的经验回报率，而不是预期回报率。考虑下面的图表，我们看到五个不同的轨迹(轨迹是代理在一集里所走的路径),代理从一集开始到结束都经历了这些轨迹。在这种情况下，for starting状态出现在所有五个样本中，并且该状态的值将是在五个展开中采样的五个值的平均值。</p><p id="4a60" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">有两种MC方法:第一次访问MC和每次访问MC。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/e11b301d7383d37a6e7f5fbf4fa6c7c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hzNunwO9vIAAwBYachYxGQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="480e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在计算一个状态的值时，只计算第一次访问，即使该状态被多次访问。在每次访问MC中，每次在一集内访问该州时，该州的值都会受到访问次数的影响(参考以下示例中的S3州值。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/031ddc49cfbf239d8b29f63cb7153fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CXkdOrE0itv4sxeCW6mJmA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><h2 id="d2d3" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">TD(0)</h2><p id="be65" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">TD(0)是时间差分(TD)学习的最简单形式。在这种形式的TD学习中，在每一步值函数都用下一个状态的值和沿途的<strong class="kz jj">更新后，得到奖励</strong>。这种观察到的回报是保持学习接地气的关键因素，算法在足够数量的采样后收敛(在无穷大的极限内)。所有TD方法都具有以下特征:</p><ol class=""><li id="9596" class="nq nr ji kz b la lb ld le lg ns lk nt lo nu ls ok nw nx ny bi translated">没有模型(代理不知道状态MDP转换)</li><li id="690b" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">代理<strong class="kz jj">从<strong class="kz jj">采样的</strong>经验中学习</strong>(类似于MC)</li><li id="820b" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">像DP一样，TD方法部分基于其他<strong class="kz jj">学习的估计</strong>更新估计，而不等待结果(它们<strong class="kz jj">像DP一样引导</strong>)。</li><li id="c3d2" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">它可以从<strong class="kz jj">不完整的插曲中吸取教训；</strong>因此，该方法同样可以用于<strong class="kz jj">连续问题</strong>。</li><li id="fc87" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">TD将猜测更新为猜测，并根据实际经验修改猜测。</li></ol><p id="b7b6" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">参考下图，其中状态值在每一步之后从下一个状态值更新(<strong class="kz jj">学习率(α) </strong>是一个决定从下一个状态和当前状态之间的差中取多少来更新当前状态的因素)。这种差异被称为<strong class="kz jj"> TD误差</strong>。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/13e3e7de776f307966d0e071d33fed3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7rxsa0-hk2e9w7S7rFgC-A.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><h2 id="2e8d" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">萨尔萨</h2><p id="c529" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">控制<strong class="kz jj">或改进</strong>的TD算法之一是SARSA。SARSA的名字来源于这样一个事实，即代理从一个状态-动作值对<strong class="kz jj">向另一个状态-动作值对</strong>迈出一步，并在此过程中收集奖励R(因此它是S( t)，A( t)，R (t+1)，S (t+1) &amp; A(t+1)元组，它创建了术语<strong class="kz jj"> S，A，R，S，A </strong>)。SARSA是一种<strong class="kz jj">基于策略的</strong>方法。SARSA使用动作值函数Q并遵循策略π。<strong class="kz jj"> GPI </strong>(广义策略迭代)用于基于策略π采取行动(<strong class="kz jj">ε-贪婪</strong>确保探索以及贪婪改进策略)。参考下图，我们可以看到下一个操作可以是基于策略的任何下一个操作。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/d5829f862531cd74b6bb53e675279a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*10RCbzCXFuF2wu5J-Dj-vw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><h2 id="b6ef" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">q学习</h2><p id="2b24" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">Q-learning是一种<strong class="kz jj">非策略</strong>算法。DQN(深度Q-learning)登上了《自然》杂志的头版，它是一种基于Q-Learning的算法(几乎没有额外的技巧)，在雅达利游戏中超越了人类水平的专业知识(我将在未来的帖子中详细介绍DQN)。在Q-learning中，目标策略是<strong class="kz jj">贪婪策略</strong>，行为策略是<strong class="kz jj">ε-贪婪策略</strong>(这确保了探索)。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/48ac0022ebceeff0b9601185413286e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nCK6v1OA0x12d0VC7a-9Mw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="6d64" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">参考下图。接下来的行动是基于一个贪婪的政策。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/37764326f8812fdc79368c0751fda873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GmD5Rk91ellxxTJZ45BpQw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><h2 id="a502" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">预期SARSA</h2><p id="b82c" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">预期SARSA就像Q-learning一样，除了它使用<strong class="kz jj">期望值</strong>(基于概率的平均值)而不是下一个状态-动作对的最大值，同时考虑了每个动作在当前策略下的可能性。给定下一个状态，Q-learning算法确定性地将<strong class="kz jj">向同一方向移动</strong>，而SARSA按照<strong class="kz jj">期望</strong>跟随，因此，它被称为期望SARSA。参考下文，下一步行动基于预期值。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/d2fa14a7eae37a0e65dc5e739b93223d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TOjSjw6KONQGYG5fkoUbqA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="bd1e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">基于SARSA概念的这些控制算法的主要区别如下图所示。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi op"><img src="../Images/2ef3043473eff7dc3ca800b7f4ec65e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vYNdccNYQ9WDbQjOyF7alg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="2477" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">接下来，我们将介绍一些基于时间差概念的算法，但这些算法的值不会立即更新，而是在几步之后更新，或者基于几个状态的平均值进行更新。</p><h2 id="5064" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">n步TD / SARSA</h2><p id="171b" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">在n步TD中，状态值或动作值在一定数量的时间步后更新。在这些情况下，代理在值更新之前收集更多的奖励。在一定数量的步骤之后，根据沿途收集的所有折扣奖励和状态或状态-动作的价值函数来更新价值。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/a232633d7426860f03b24d302121c11c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6tHGkJC6m8ysQPHBYgOlg.png"/></div></div></figure><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/3f0b4554fa847ed980e1c4418d39a969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*STt_NUmVZdKWLKZ4xRAyGw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><h2 id="c5d5" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">TD(λ)</h2><p id="626e" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">TD(λ)是n步TD的扩展。直觉是<em class="na">将所有可能的n步回报平均</em>成一个单一的回报。我们使用随时间呈指数衰减的权重对n步回报进行加权。这是通过引入因子λ(值在0和1之间)并将第n次返回加权为(n-1)次幂λ来实现的。因为我们希望所有这些权重相加为一(得到一个加权平均值)，所以我们将它们归一化。</p><p id="16ef" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们可以看到，当λ = 1时，只保留最后一项，这本质上是蒙特卡罗方法，作为状态，作用过程一直进行到最后，当λ = 0时，该项还原TD (0)方法，对于0</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/5a5ea721f3669ac8e489fd939e7db075.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DOAxZ_5G7MI4gq_ruZyHmA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="8ac1" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这可以使用<strong class="kz jj">资格跟踪</strong>来进一步改进，这是一个绝妙的想法，它允许我们基于对过去发生的事情的记忆来更新每个状态值，并使用当前信息来更新我们到目前为止看到的每个状态的状态值。资格追踪结合了两件事:一是<strong class="kz jj">多频繁</strong>二是<strong class="kz jj">一个州多近</strong>。在RL中，将在下一个状态学到的东西也扩展到先前的所有状态以加速学习将是有用的。为了实现这一目标，需要有一个短期记忆机制来存储在最后步骤中访问过的状态。这就是<strong class="kz jj">资格跟踪</strong>概念出现的地方。λ∈[0，1]是称为<strong class="kz jj"> trace-decay </strong>或<strong class="kz jj"> accumulating trace，</strong>的衰减参数，其定义了每个被访问状态的更新权重。痕迹随时间减少。这允许对不频繁的状态给予小的权重，而对最近访问的状态给予大的权重。对于λ=0，我们有TD(0)的情况，只有前一个预测被更新。TD(1)可以被认为是使用TD框架的MC方法的扩展。在MC方法中，我们需要等到剧集结束时更新状态。在TD(1)中，我们可以在线更新所有之前的状态，并且我们不需要剧集的结尾。现在让我们看看在一个事件中特定的状态跟踪会发生什么。我将考虑一集七次访问，其中访问了五个州。在该事件期间，状态s1s1被访问两次。让我们看看它的踪迹会发生什么变化。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/9d252e02295b4a07d832baa71d016a37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5txo8ElbI2jL4q7-By8oIg.png"/></div></div></figure><p id="9f60" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">参考上图；开始时，迹线等于零。在第一次访问S1(第一步)后，迹线上升到1.0，然后开始衰减。在第二次访问(第3步)之后，将+1加到当前值(0.50)，获得1.50的轨迹。在三个以上的步骤之后，S1被重新访问，使得踪迹值为1.75(注意，这里指数衰减仅仅被显示为在每个步骤中减少0.25，为了说明的自由，实际值将会不同)。</p><p id="29cf" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">TD(λ)如何更新值函数？在TD(λ)中，先前的状态是可访问的，但是它们是基于资格跟踪值更新的。具有小资格轨迹的州将被少量更新<strong class="kz jj">，</strong>，而具有高资格轨迹的州将被大量更新。</p><h2 id="85e1" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">深Q网(DQN)</h2><p id="00ba" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">这是整合深度学习和强化学习的主要成功故事之一，它打开了新算法的闸门，并重新引起了人们对强化学习领域的兴趣。深度神经网络用作Q值的函数逼近器。深度Q学习简单地应用神经网络来近似每个动作和状态的Q函数，这可以节省大量的计算资源，并且潜在地扩展到连续时间动作空间。</p><p id="6456" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">体验回放</strong>:我们运行代理，将最近的10万个过渡(或视频帧)存储到一个缓冲区中，并从这个缓冲区中抽取一个512大小的小批量样本来训练深度网络。当我们从重放缓冲区随机取样时，数据更加相互独立，更接近于同分布(独立同分布)。这使得降雨稳定。</p><p id="1f99" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">目标网络</strong>:我们创建两个深度网络θ-(目标网络)和θ(Q-网络)。我们使用第一个来检索Q值，而第二个包括训练中的所有更新。比如说50，000次更新后，我们将θ-与θ同步。目的是暂时固定Q值目标，这样我们就没有一个移动的目标可以追逐。此外，参数变化不会立即影响θ-，因此即使输入也变得接近i.i.d。</p><p id="0546" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">下面是DQN的图表和步骤。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/d726b51be40209470de4886612ddb659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sBQZOI2-R8ZjrtS6r4ZZ6A.png"/></div></div></figure><ol class=""><li id="132b" class="nq nr ji kz b la lb ld le lg ns lk nt lo nu ls ok nw nx ny bi translated">预处理并将状态提供给DQN，后者将返回状态中所有可能动作的Q值</li><li id="a125" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">使用ε-贪婪策略(<strong class="kz jj">行为策略</strong>)选择一个动作。对于概率ε，我们选择随机动作<em class="na"> a，</em>，对于概率1-ε，我们选择具有最大Q值的动作，例如A= argmax (Q(S，A，θ))</li><li id="53f7" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">在状态S中执行该动作，并移动到新的状态S <em class="na"> ' </em>以获得奖励r。在Atari模拟器的情况下，该状态S '是下一个游戏屏幕的预处理图像。我们将这个过渡存储在我们的重放缓冲器中作为&lt; S，A，R，S’&gt;</li><li id="2c5a" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">接下来，从重放缓冲区中随机抽取一些批次的转换样本，并计算损失，这只是目标Q(奖励+贴现的最大下一个Q值(<strong class="kz jj">贪婪目标策略</strong>))和预测Q之间的平方差。</li><li id="3360" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">根据我们的实际网络参数执行梯度下降，以最小化这种损失</li><li id="4042" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">每N次迭代后，将我们的实际网络参数(θ)复制到目标网络参数(θ-)</li><li id="2308" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">重复这些步骤达<em class="na"> M </em>集数以收敛参数θ</li></ol><h2 id="3703" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">双Q学习</h2><p id="cd3d" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">Q-Learning在一些随机环境中表现很差。由于在Q-learning中使用<strong class="kz jj">Max Q(S’，A’)</strong>导致动作值被大大高估，从而导致性能不佳。主要概念是通过将目标中的max操作分解为<strong class="kz jj">动作选择</strong>和<strong class="kz jj">动作评估</strong>来减少高估。在Doble Q-learning中，我们维护两个Q值函数Q-A和Q-B(在Double Deep Q网络中，这两个通过称为目标网络和Deep Q网络的两个不同的函数近似来支持)，每个都从另一个获得下一个状态的更新。Q-A决定在下一个状态中导致最佳动作值的动作，而Q-B使用Q-A选择的那个动作来决定状态动作的期望值是多少(这里的算法是维护两个网络)。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/0bcdb373bbc22850459088fc0d79b555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YkcVQRPZU9Xzq8FLqwAcbQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><h2 id="984c" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">决斗DQN</h2><p id="235a" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">q值可以分解为两部分:状态值函数V和优势值a。优势函数捕获<strong class="kz jj">在给定状态下，某个动作</strong>与平均动作相比有多好(参见A3C，了解优势函数的直观解释)。同时，正如我们所知，价值函数捕捉到了处于这种状态有多好。</p><p id="5e59" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">决斗Q网络背后的整个思想依赖于将Q函数表示为值和优势函数的总和。</p><p id="53bf" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们简单地用两个网络来学习和的每一部分，然后我们合计它们的输出。参考下图，其中全连接(FC)层有两个头(一个用于V，一个用于所有A)。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/31dbf6ec3974db4c59d9bb9200ff6a81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qP_5Ci2bJuv-ZIvXUp6TdA.png"/></div></div></figure><p id="e833" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">将这种类型的结构添加到网络中允许网络更好地区分彼此的动作，并且显著地改进了学习。在许多状态下，不同动作的值非常相似，采取哪种动作并不重要。这在有许多操作可供选择的环境中尤其重要。</p><h1 id="fbed" class="nf mc ji bd md ng nh ni mg nj nk nl mj ko nm kp mm kr nn ks mp ku no kv ms np bi translated">基于策略的算法</h1><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/1dceb8485fa282a95643bee6a13c8efd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xM48a8RABZRBkWCXfYFV7g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="3f69" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在基于策略的方法中，我们不是学习告诉我们给定一个状态和一个动作的期望回报总和的值函数，而是直接学习将状态映射到动作的策略函数(选择动作而不使用值函数)。政策梯度背后的关键思想是<strong class="kz jj">提高</strong>导致<strong class="kz jj">更高回报</strong>的行动概率，而<strong class="kz jj">降低</strong>导致<strong class="kz jj">更低回报</strong>的行动概率，直到我们得出最优政策。</p><p id="df20" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">策略梯度</strong>方法的目标是直接建模和优化<strong class="kz jj">策略</strong>。策略<em class="na"> π </em> (A|S)通常用参数化函数<em class="na"> θ </em>建模。奖励(目标)函数的值取决于这个策略，然后可以应用各种算法来优化<em class="na"> θ </em>以获得最佳奖励。在<strong class="kz jj">基于策略的</strong>方法中，我们显式地构建一个策略的表示(映射π:S→A ),并在学习期间将其保存在内存中。</p><p id="f3da" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">很自然地期望基于策略的方法在<strong class="kz jj">连续空间</strong>更有用。因为存在无限数量的动作和(或)状态来估计值，因此基于值的方法在连续空间中在计算上<strong class="kz jj">过于昂贵</strong>。例如，在一般化的策略迭代中，策略改进步骤需要对动作空间进行全面扫描，遭受了<strong class="kz jj">维数灾难</strong>。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/4344bd9f83e2ace20bf5cac89213bb82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a54k1tUWzP0yzF4YsDq8AQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><h1 id="2c20" class="nf mc ji bd md ng nh ni mg nj nk nl mj ko nm kp mm kr nn ks mp ku no kv ms np bi translated">加固</h1><p id="bc7c" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated"><strong class="kz jj">加强</strong>(蒙特卡洛政策梯度)依赖于通过蒙特卡洛方法使用事件样本更新政策参数θ的估计回报。因为样本梯度的期望值等于实际梯度，所以加强是可行的。在加强中，代理使用其当前策略收集一集的轨迹τ，并使用它来更新策略参数。下面是加固的步骤。</p><ol class=""><li id="95a8" class="nq nr ji kz b la lb ld le lg ns lk nt lo nu ls ok nw nx ny bi translated">随机初始化策略参数θ。</li><li id="456c" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">在策略π上生成一个轨迹</li><li id="7d3f" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">估算每个时间步的回报，直到最终状态</li><li id="72ca" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">通过一步梯度上升更新政策参数:θ(价值函数的梯度可以计算为<strong class="kz jj">政策梯度(的对数)的梯度期望值乘以奖励— </strong>这是政策梯度的主要技巧)</li><li id="73ac" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">重复2–4直到收敛</li></ol><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/5eb2fbf7747bdb0a59a1f281ade9ac22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbcY2LGj0fVsHCoauTYYfw.png"/></div></div></figure><p id="3f81" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">强化以基于策略的方式训练随机策略。这意味着它通过根据其随机策略的最新版本对行为进行采样来进行探索。动作选择的随机性取决于初始条件和训练程序。在整个训练过程中，策略通常变得越来越不随机，因为更新规则鼓励它利用它已经发现的奖励。这可能导致策略陷入局部最优。下图是对钢筋的解释。我们希望更新有正回报的策略参数，并减少有负回报的策略参数。</p><p id="74fe" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">需要注意的重要一点是，这里没有使用马尔可夫性质，这也可以用于部分观测的MDP，而不需要在算法上做任何改变。</p><p id="ad51" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这种方法的一个主要缺点是方差大和收敛慢。</p><h2 id="a899" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">用基线加强</h2><p id="df56" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">为了减少方差，一个想法是从返回G(S)中减去一个称为基线B(S)的值，其中B(S)不依赖于动作a。从数学上仍然可以看出，政策梯度仍然在预期中给出无偏的结果。</p><p id="4e0a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">所以现在的问题是如何选择B(S)。对基线的选择之一是计算状态值V(S，w)的估计，其中w是通过诸如蒙特卡罗的一些方法学习的参数向量。</p><h1 id="3adf" class="nf mc ji bd md ng nh ni mg nj nk nl mj ko nm kp mm kr nn ks mp ku no kv ms np bi translated">演员-评论家算法</h1><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/4391cf6c4a52e35f43227b0fc977ff2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Ln9amy749DX_9WQBPBmbg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="3f6f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">政策梯度的两个主要组成部分是政策模型和价值函数。除了策略之外，<strong class="kz jj">学习价值函数</strong>是很有意义的，因为知道价值函数可以帮助策略更新，例如通过减少普通策略梯度中的<strong class="kz jj">梯度方差</strong>，并且这正是<strong class="kz jj">参与者-批评家</strong>方法所做的，其中<strong class="kz jj">参与者是策略</strong>并且<strong class="kz jj">批评家是价值函数</strong>。</p><p id="e7b7" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">行动者-批评家方法由两个模型组成，这两个模型可能共享也可能不共享参数:</p><ul class=""><li id="fd50" class="nq nr ji kz b la lb ld le lg ns lk nt lo nu ls nv nw nx ny bi translated"><strong class="kz jj">评论家</strong>更新值函数参数‘w’，根据算法，它可以是动作值Q或状态值v。</li><li id="4d5d" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls nv nw nx ny bi translated"><strong class="kz jj">行动者</strong>按照批评者建议的方向更新策略参数‘θ’。</li></ul><p id="5a44" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">正如我们前面看到的，减少政策梯度方法的方差和增加稳定性的一个方法是用基线减去累积回报(回报)。该基线减法在预期中<strong class="kz jj">是无偏的</strong>。因此，我们在这里做的是通过一些基线来调整回报，这减少了方差。有许多方法可以改进增强算法。</p><h2 id="2d60" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">A3C</h2><p id="6bcc" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated"><strong class="kz jj">异步优势行动者-批评家(A3C) </strong>算法是一种经典的策略梯度方法，特别关注并行训练。这是一个典型的政策梯度方法，特别强调平行培训。该算法使用多个代理，每个代理都有自己的网络参数和环境副本。这些代理与它们各自的环境<strong class="kz jj">异步交互</strong>，在每次交互中学习。一个全球网络控制着每个代理。随着每个代理获得更多的知识，它有助于全球网络的总知识。全球网络的存在允许每个代理具有更多样化的训练数据。这种设置模拟了人类生活的真实环境，因为每个人都从其他人的经验中获得知识，从而使整个“全球网络”变得更好。</p><p id="d015" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">与基于价值迭代法或策略梯度法的一些更直接的技术不同，A3C算法结合了这两种方法的最佳部分，即该算法预测价值函数和最优策略函数。</p><p id="fd84" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">典型地，在实施<strong class="kz jj">策略梯度</strong>(优势的价值)时，代理人也知道回报比其平均值(期望值)好多少。这提供了对环境中的代理的新发现的洞察力，因此学习过程更好。以下表达式提供了advantage函数。</p><p id="516c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">优势(S，A) =行动价值(S，A) —状态价值(S)。</strong></p><p id="7893" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">参考下图，该图解释了具有四个动作的状态的优势函数，并显示了动作的概率(为了解释，这被用作确定性离散动作，但该概念同样适用于随机连续动作空间)。这里，行动A3中的优势值最高，行动者在这种状态下会更多地遵循该行动，如评论家所建议的那样(优势函数)。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/d4a79d6aa0ab9487eb22ee19025bc0c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pCX9e4FBB3_kcWaVCF3yYA.png"/></div></div></figure><p id="acfe" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">请参考下图，该图显示了A3C架构如何与工人和全球网络参数一起工作。与此同时，不同的主体从环境中获取不同的信息，并且所有主体都合作学习对环境和问题的全局理解。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/618ca206f8cbbec859f121db981fefcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8_JqSr9COzfhJuX-hA7Sg.png"/></div></div></figure><h1 id="f8d5" class="nf mc ji bd md ng nh ni mg nj nk nl mj ko nm kp mm kr nn ks mp ku no kv ms np bi translated">A2C</h1><p id="4395" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated"><strong class="kz jj"> A2C </strong>是A3C的“同步”确定性版本；这就是为什么它被命名为“A2C”，去掉了第一个“A”(“异步”)。在A3C中，每个代理独立地与全局参数对话，所以有时特定于线程的代理可能会使用不同版本的策略。因此，聚集更新将<strong class="kz jj">不是最佳的</strong>。为了解决这种不一致，A2C的协调器在更新全局参数之前等待所有并行参与者完成他们的工作。然后在下一次迭代中，并行参与者从相同的策略开始。同步梯度更新使训练<strong class="kz jj">更有凝聚力</strong>，并可能使<strong class="kz jj">收敛更快</strong>。A2C还能够更有效地利用GPU，更好地处理大批量数据，同时实现与A3C相同或更好的性能。</p><p id="ea57" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">请参考下图。所有n个代理线程等待，直到它们都有更新要执行。然后，我们通过对全局网络的一次更新来平均这n个线程上的梯度。事实上，由于批量较大，这应该更有效。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/351178a0eaabbd8e6f6acbbfbad9588c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tz9zcc1_8BEeGU1ctmga9A.png"/></div></div></figure><h2 id="eea2" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">TRPO</h2><p id="3dde" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">为了提高训练的稳定性，我们应该避免一步改变策略太多的参数更新。<strong class="kz jj">信任区域策略优化(TRPO) </strong>通过在每次迭代中对策略更新的大小实施KL发散约束来实现这一思想。TRPO是一种基于策略的算法。TRPO可用于具有离散或连续动作空间的环境。</p><p id="ef80" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">TRPO通过采取尽可能大的步骤来更新策略，以提高性能，同时满足允许新旧策略接近程度的特殊约束。这种约束用<strong class="kz jj"> KL-Divergence </strong>来表示，这是新旧政策概率分布之间距离的一种度量。</p><p id="7c3f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这不同于标准策略梯度，标准策略梯度在参数空间中保持新旧策略接近。但是，即使参数空间中看似很小的差异也会在性能上产生相当大的差异——因此，一个错误的步骤就可能导致策略性能崩溃。这使得对普通策略梯度使用大的步长变得很危险，从而损害了它的采样效率。TRPO很好地避免了这种崩溃，并倾向于快速单调地提高性能。参考下图，这是TRPO如何使用信任区域(循环空间)来限制后续策略的卡通说明，以便新策略不会漂移太多，从而永远不会达到最优策略。红色、黄色和橙色策略在信任区域(由KL-divergence设置)之外，因此无法恢复或解决局部最优。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/6684f970fbda240c516c434769020cbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*umW3fKg-LsCBeghBH1KPuw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于此插图图片的像素图像</p></figure><p id="57ac" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">TRPO旨在最大化服从<em class="na">信任区域约束</em>的目标函数，该约束强制由KL-divergence测量的新旧策略之间的距离在参数δ内足够小。这样，当满足这一硬约束时，新旧政策不会相差太多。请参考下图，了解约束如何适用于两种策略。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/8b4fd81614088b181bc0389afd52f5d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0UHUz6ypf-8kqZ2c9jmpnA.png"/></div></div></figure><h2 id="13e0" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">聚苯醚（Polyphenylene Oxide的缩写）</h2><p id="c9f6" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">TRPO实现起来相当复杂。<strong class="kz jj">近似策略优化(PPO) </strong>通过使用<strong class="kz jj">修剪替代目标</strong>来简化它，同时保留类似的约束和类似的性能。PPO消除了由约束优化产生的计算，因为它提出了一个<strong class="kz jj">裁剪代理目标</strong>函数。它不是强加硬约束，而是将约束形式化为目标函数中的惩罚。</p><p id="f725" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">对于PPO，我们通过将KL散度从一个约束条件转化为一个惩罚项来简化问题，类似于，例如，L1，L2权重惩罚(以防止权重增长到很大的值)。</p><p id="0842" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj"> PPO-Penalty </strong>近似地解决了类似TRPO的KL约束更新，但是惩罚目标函数中的KL-divergence，而不是使其成为硬约束，并且在整个训练中自动调整惩罚系数，以便其被适当地缩放。</p><p id="3e06" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj"> PPO-Clip </strong>在目标中没有KL-divergence项，完全没有约束。而是依赖于目标函数中的专门剪裁来消除新策略远离旧策略的激励。PPO确实将政策比率(更新政策与旧政策的比率)限制在1.0左右的小范围内，其中1.0表示新政策与旧政策相同。</p><h2 id="6693" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">DDPG</h2><p id="4c65" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated"><strong class="kz jj"/>(<strong class="kz jj">深度确定性策略梯度)</strong>，是一种无模型的非策略行动者-批评家算法，它同时学习Q函数(行动值)和策略。它使用非策略数据和贝尔曼方程<strong class="kz jj">学习Q函数</strong>，并使用Q函数<strong class="kz jj">学习策略</strong>。DDPG可以被认为是针对<strong class="kz jj">连续动作空间</strong>的深度Q学习。</p><ul class=""><li id="a004" class="nq nr ji kz b la lb ld le lg ns lk nt lo nu ls nv nw nx ny bi translated">DDPG是一个<strong class="kz jj">非策略</strong>算法(使用重放缓冲)</li><li id="e751" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls nv nw nx ny bi translated">DDPG<strong class="kz jj">只能</strong>用于有连续动作空间的环境</li></ul><p id="f5b5" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">当存在有限数量的离散动作时，最大值不会造成问题，因为我们可以单独计算每个动作的Q值，并直接比较它们，这给了我们最大化Q值的动作。但是当<strong class="kz jj">动作空间是连续的</strong>时，我们不能穷尽地评估该空间(我们必须评估无限数量的动作)，并且因为每次代理想要在环境中采取任何动作时都需要运行它，这是不可行的。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/437d84829c9365d8183d09ee6d6bf351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qvkuR8HrqgXP-An3qwXu-Q.png"/></div></div></figure><p id="4921" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在DDPG，我们使用重放缓冲区来计算目标，然后更新Q函数(梯度下降)，然后更新策略(梯度上升)。我们想学习一个确定性的策略，给出最大化Q函数的动作。由于动作空间是连续的，并且我们假设Q函数相对于动作是可微的，我们执行梯度上升来求解仅相对于策略参数的最大Q值。注意，当我们进行梯度上升以更新策略参数时，Q函数参数在这里被视为常数。</p><h2 id="8a9e" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">TD3:双延迟深度确定性政策梯度</h2><p id="45e2" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">双延迟深度确定性策略梯度(TD3)算法是一种无模型、在线、脱离策略的行动者-批评家算法。DDPG的一个常见故障模式是，学习到的Q函数开始显著高估Q值，然后导致策略破坏，因为它利用了Q函数中的错误。TD3算法是DDPG算法的扩展。</p><p id="6f7a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">TD3比DDPG做了以下改进。</p><ol class=""><li id="c1b7" class="nq nr ji kz b la lb ld le lg ns lk nt lo nu ls ok nw nx ny bi translated">TD3代理学习<strong class="kz jj">两个Q值</strong> ( <strong class="kz jj">“孪生”</strong>)函数，并在策略更新期间使用最小值函数估计。</li><li id="06bc" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">TD3更新策略(和目标网络)<strong class="kz jj">比Q功能更不频繁(“延迟”)</strong>(一个策略更新两个Q功能更新，这就是孪生延迟名称的由来)</li><li id="c85c" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">当更新策略时，TD3代理<strong class="kz jj">向目标动作添加噪声</strong>，这使得策略不太可能利用具有高Q值估计的动作。</li></ol><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/967f1ea342b9b42f14db700268c3a6b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRY8shQVQ95beTFQMJGSww.png"/></div></div></figure><p id="8381" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">上图显示了算法中的步骤。我将在这里尝试以简化的方式描述这些步骤。</p><ol class=""><li id="484e" class="nq nr ji kz b la lb ld le lg ns lk nt lo nu ls ok nw nx ny bi translated">第一步是初始化策略和Q函数的函数近似参数(两个Q参数)，将目标参数设置为主参数(1个用于策略，2个用于Q函数)</li><li id="3293" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">重复许多集以收集状态、动作(我们添加噪声和剪辑)、奖励、下一个状态、终端状态指示符来构建重放缓冲区</li><li id="999b" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">从重放缓冲器中随机抽取一批过渡细节，并计算目标动作A’(S’)(使用<strong class="kz jj">演员目标</strong>网络、剪辑和噪声属性)</li><li id="84c1" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">计算目标Q值，在<strong class="kz jj">批评家目标1 </strong>网络中的Q’1</li><li id="e540" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">计算目标Q值，Q'2在<strong class="kz jj">批评家目标2 </strong>网络中</li><li id="569e" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">从Q'1和Q'2中取Q值的最小值。</li><li id="4f23" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">使用<strong class="kz jj"> Critic Model1中的一步梯度下降计算Q函数的损失(</strong>将步骤6中的Q值作为目标<strong class="kz jj"> ) </strong></li><li id="a8af" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">使用<strong class="kz jj"> Critic Model2中的一步梯度下降法计算Q函数的损失(</strong>将步骤6中的Q值作为目标<strong class="kz jj"> ) </strong></li><li id="6941" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated">采取步骤7和8计算总损失，并更新<strong class="kz jj">评估模型1 </strong>和<strong class="kz jj">评估模型2 </strong>的参数</li><li id="07ca" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated"><strong class="kz jj">每两次迭代中，</strong>我们使用从<strong class="kz jj">评论家模型1 </strong>输出的Q值通过一步梯度上升来更新<strong class="kz jj">演员模型</strong>的策略参数(步骤7)</li><li id="8f81" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls ok nw nx ny bi translated"><strong class="kz jj">每两次迭代中，</strong>我们通过Polyak平均更新一次目标网络的参数(<strong class="kz jj">演员目标，评论家目标1 &amp;评论家目标2 </strong>)。</li></ol><p id="f97d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们继续上述步骤，直到参数收敛。</p><h2 id="bf28" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">软演员兼评论家</h2><p id="ba99" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">软行动者-批评家(SAC)基于<strong class="kz jj">最大熵强化学习</strong>，这是一个框架，旨在既<strong class="kz jj">最大化预期回报</strong>(这是标准的RL目标)又<strong class="kz jj">最大化政策的<em class="na">熵</em> </strong> <em class="na"> ( </em>熵越高的政策越随机)，这直观地意味着最大熵强化学习更喜欢仍然实现高回报的最随机的政策。</p><p id="5850" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">软演员评论家(SAC)结合了<strong class="kz jj">限幅双Q技巧</strong>(像DDPG一样)，SAC的一个中心特征是<strong class="kz jj">熵正则化。</strong>该策略被训练为最大化预期回报和熵之间的权衡，熵是策略中随机性的度量。这与探索-利用权衡有着密切的联系:<strong class="kz jj">增加熵会导致更多的探索</strong>，这可以加速以后的学习。它还可以防止策略过早地收敛到一个<strong class="kz jj">坏的局部最优</strong>。</p><ul class=""><li id="1ef3" class="nq nr ji kz b la lb ld le lg ns lk nt lo nu ls nv nw nx ny bi translated">SAC是一种非策略算法。</li><li id="21aa" class="nq nr ji kz b la oa ld ob lg oc lk od lo oe ls nv nw nx ny bi translated">SAC可以在连续或离散的动作空间中工作。</li></ul><h2 id="7779" class="mb mc ji bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">黑斑羚</h2><p id="c1fc" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated"><strong class="kz jj"> IMPALA(重要性加权行动者-学习者架构)</strong>受流行的A3C架构启发，使用多个分布式行动者来学习代理的参数。每个参与者都使用策略参数的克隆来在环境中操作。</p><p id="9c16" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">IMPALA使用为学习策略和基线函数v而设置的行动者-批评家。该架构由一组行动者组成，重复生成经验轨迹，一个或多个学习者使用从行动者发送来的经验<strong class="kz jj">学习策略</strong>非策略。</p><p id="06b0" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">演员<strong class="kz jj">周期性地</strong>暂停</strong>他们的探索，以与<strong class="kz jj">中央参数服务器</strong>共享他们已经计算的梯度，中央参数服务器通过计算梯度来应用更新。在IMPALA中，<strong class="kz jj">生成经验的过程与<strong class="kz jj">学习</strong>参数策略和基线函数v是分离的</strong>。多个参与者并行生成经验，而学习者使用所有生成的经验优化策略和值函数参数。参与者定期用来自学习者的最新策略更新他们的参数。因为表演和学习是分离的，我们可以添加更多的演员机器来生成更多的时间单位轨迹。由于训练策略和行为策略<strong class="kz jj">不完全同步</strong>，它们之间有一个间隙，因此我们需要称为<strong class="kz jj"> V-trace </strong>的非策略修正。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/9c313daf94a67c93f5810dbceaa4946f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8igroQhQjcagmfCH"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图像参考:深度思维</p></figure><p id="6fd1" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">中央学习器计算梯度，产生具有完全独立的参与者和学习者的模型。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/e8a01b33d50b69f92c328cec9445ae16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*RVU396TwPQko17Ao.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae ph" href="https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html" rel="noopener ugc nofollow" target="_blank">图片来自谷歌人工智能博客</a></p></figure><p id="7954" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了利用现代计算系统的规模，可以使用单个学习器机器或在它们之间执行同步更新的多个学习器来实现重要性加权的参与者-学习器架构。以这种方式分离学习和行动还有增加整个系统吞吐量的优点，因为行动者不再需要像在诸如批处理A2C(如下图所示)的架构中那样等待学习步骤。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pi"><img src="../Images/bdc7a6a97c7b6fe8d1eb836536826b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RTqVZFJ4fPwIu4GAbq6kwA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图像参考:深度思维</p></figure><p id="ab3a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然而，将表演和学习分离会导致参与者的策略落后于学习者。为了补偿这种差异，IMPALA使用了一种称为<strong class="kz jj"> V-trace </strong>的非政策优势演员-评论家公式，用于补偿演员非政策获得的轨迹。</p><h1 id="c524" class="nf mc ji bd md ng nh ni mg nj nk nl mj ko nm kp mm kr nn ks mp ku no kv ms np bi translated">最后一个音符</h1><p id="0c60" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated">强化学习很复杂，但这并不意味着它必须是深奥的。这个领域正在迅速发展；新的想法和论文层出不穷。我尽量避免所有的数学表达式，这些表达式对于理解更复杂的算法是必不可少的。感兴趣的读者可以深入下面的参考列表，享受数学细节和进一步的解释。</p><p id="7c2d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我期待看到您的反馈或分享任何其他更直接的解释或参考任何不同的算法，您可能希望看到作为本文的一部分。</p><p id="9e30" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">感谢阅读。你可以在<a class="ae ph" href="http://www.linkedin.com/in/baijayantaroy" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p></div><div class="ab cl pj pk hx pl" role="separator"><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po"/></div><div class="im in io ip iq"><p id="ebb7" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">只需每月5美元，就可以无限制地获取最鼓舞人心的内容…点击下面的链接，成为媒体会员，支持我的写作。谢谢大家！<br/><a class="ae ph" href="https://baijayanta.medium.com/membership" rel="noopener"><strong class="kz jj"><em class="na">https://baijayanta.medium.com/membership</em></strong></a></p><h1 id="624e" class="nf mc ji bd md ng nh ni mg nj nk nl mj ko nm kp mm kr nn ks mp ku no kv ms np bi translated">参考:</h1><p id="fe0e" class="pw-post-body-paragraph kx ky ji kz b la mu kj lc ld mv km lf lg mw li lj lk mx lm ln lo my lq lr ls im bi translated"><a class="ae ph" href="https://spinningup.openai.com/en/latest/" rel="noopener ugc nofollow" target="_blank">https://spinningup.openai.com/en/latest/</a><br/><a class="ae ph" href="https://github.com/dennybritz/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">https://github.com/dennybritz/reinforcement-learning</a><br/><a class="ae ph" href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" rel="noopener ugc nofollow" target="_blank">https://lilianweng . github . io/lil-log/2018/04/08/policy-gradient-algorithms . html</a></p></div></div>    
</body>
</html>