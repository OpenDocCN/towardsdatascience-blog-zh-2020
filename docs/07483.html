<html>
<head>
<title>Four Common Types of Neural Network Layers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">四种常见类型的神经网络层</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/four-common-types-of-neural-network-layers-c0d3bb2a966c?source=collection_archive---------9-----------------------#2020-06-06">https://towardsdatascience.com/four-common-types-of-neural-network-layers-c0d3bb2a966c?source=collection_archive---------9-----------------------#2020-06-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2f8c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">(以及何时使用它们)</h2></div><p id="458c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a> (NN)是当今许多机器学习(ML)模型的支柱，松散地模仿人脑的神经元，从输入数据中识别模式。因此，多年来已经设计了许多类型的神经网络拓扑，使用不同类型的神经网络层来构建。</p><p id="37f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了今天大量的ML框架和工具，任何有一点ML知识的人都可以很容易地用不同类型的神经网络拓扑建立一个模型。在大多数情况下，这都是关于了解每种类型的神经网络擅长解决什么问题，并优化它们的超参数配置。</p><p id="1cc1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">四种最常见的神经网络层是<em class="lf">全连接</em>、<em class="lf">卷积</em>、<em class="lf">反卷积</em>和<em class="lf">递归、</em>，下面你会发现它们是什么以及如何使用它们。</p><h1 id="5ad8" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">全连接层</h1><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/1618bcfb9fbcc43540b2942fd6989c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*sVvC9YwPFD5RJ9xgxrYHPw.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图像由<a class="ae le" href="https://perceptilabs.com/home" rel="noopener ugc nofollow" target="_blank">感知实验室</a></p></figure><p id="fdb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Fully_connected" rel="noopener ugc nofollow" target="_blank">全连接层</a>将一层中的每个神经元连接到下一层中的每个神经元。全连接层存在于从标准神经网络到<a class="ae le" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a> (CNN)的所有不同类型的神经网络中。</p><p id="6888" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着输入的增长，完全连接的层在计算上可能变得昂贵，导致要执行的矢量运算的组合爆炸，以及潜在的较差的可扩展性。因此，它们通常用于神经网络中的特定目的，例如对图像数据进行分类。</p><p id="1534" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">用例</strong></p><ul class=""><li id="fd96" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">使用全连接神经网络的实验或学习ML。</li><li id="f534" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">为计算机视觉对图像进行分类。</li></ul><p id="7b4e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">通常与全连接层相关的超参数</strong></p><ul class=""><li id="29f2" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">激活功能</li><li id="cebf" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">神经元数量</li><li id="8536" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">拒绝传统社会的人</li></ul><h1 id="aa35" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">卷积层</h1><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi my"><img src="../Images/6102debb500c1de4925719ff6aa7543e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*YIIs6rTS0iWDvhc9ydRQgg.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图像由<a class="ae le" href="https://perceptilabs.com/home" rel="noopener ugc nofollow" target="_blank">感知实验室</a></p></figure><p id="dd9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional" rel="noopener ugc nofollow" target="_blank">卷积层</a>是CNN中一种重要的层类型。它最常见的用途是检测图像中的特征，其中它使用一个<em class="lf">过滤器</em>来扫描图像，一次扫描几个像素，并输出一个<em class="lf">特征图</em>，对发现的每个特征进行分类。</p><p id="73c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">过滤器(有时称为<em class="lf">内核</em>)是一组与输入相乘的n维权重，其中过滤器的维度与输入的维度相匹配(例如，当处理2D图像时是两个维度)。过滤器描述了给定的像素模式表示特征的概率。因此，滤波器权重的数量(即，滤波器的大小)小于输入，并且由层的卷积过程执行的乘法是在匹配滤波器大小的图像“小块”上执行的。</p><p id="c26b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在整个图像上从左到右和从上到下系统地重复乘法以检测特征。滤波器在下一次迭代中移动的像素数称为<em class="lf">步幅</em>。<em class="lf">可以在输入图像周围添加填充</em>，以确保滤波器总是适合给定步幅的图像的总边界。</p><p id="101e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">用例</strong></p><ul class=""><li id="7896" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">用于图像识别和分类的图像分析。</li></ul><p id="a885" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">通常与卷积层相关的超参数</strong></p><ul class=""><li id="e6c6" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">维度</li><li id="64cd" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">补丁大小</li><li id="767f" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">进展</li><li id="e14f" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">要生成的要素地图的数量</li><li id="3a42" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">填充策略</li><li id="47a6" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">激活功能</li><li id="ce5f" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">拒绝传统社会的人</li><li id="77c0" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">联营</li></ul><h1 id="ac58" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">去卷积层</h1><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/c57c48d051e23f38ee13a1ac40bbe8f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*_EIazONs-6k3Dc_W6c4HTA.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图像由<a class="ae le" href="https://perceptilabs.com/home" rel="noopener ugc nofollow" target="_blank">感知实验室</a></p></figure><p id="1c7e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">反卷积图层是一种转置卷积过程，可有效地将数据向上采样至更高的分辨率。这可以包括从卷积层生成的图像数据和/或特征图，或者其他类型的数据。对于图像数据，通过去卷积输出的上采样分辨率可以与原始输入图像相同，也可以不同。</p><p id="7d6c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">常见用例</strong></p><ul class=""><li id="d614" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">向上采样图像</li></ul><p id="8c92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">通常与反褶积层相关的超参数</strong></p><ul class=""><li id="f6e5" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">维度</li><li id="7c65" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">进展</li><li id="34d5" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">要生成的要素地图的数量</li><li id="15b5" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">填充策略</li><li id="7122" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">激活功能</li><li id="329e" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">拒绝传统社会的人</li></ul><h1 id="0d61" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">循环层</h1><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi na"><img src="../Images/4d0c4cb9a6eeb153622caa61711cc0b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*evR7fkBJLxYD4mcbB-4YTw.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图像由<a class="ae le" href="https://perceptilabs.com/home" rel="noopener ugc nofollow" target="_blank">感知实验室</a></p></figure><p id="ce75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个<a class="ae le" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">循环层</a>包括一个“循环”能力，使得它的输入既包括要分析的数据，也包括由该层执行的先前计算的输出。递归层形成了递归神经网络(RNNs)的基础，有效地为它们提供了<em class="lf">内存</em>(即，在迭代过程中保持状态)，而它们的递归性质使RNNs适用于涉及自然语言和时间序列等顺序数据的情况。它们对于将输入映射到不同类型和维度的输出也很有用。</p><p id="f9cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">常见用例</strong></p><ul class=""><li id="6eb0" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">将情绪分为积极情绪和消极情绪。</li><li id="c60e" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">生成图像内容的文本描述。</li><li id="2926" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">将文本段落翻译成另一种语言。</li></ul><p id="bbc7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">通常与重现层相关的超参数</strong></p><ul class=""><li id="2a3f" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">维度</li><li id="cb20" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">递归神经网络类型(<a class="ae le" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank"> LSTM </a>、<a class="ae le" href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" rel="noopener ugc nofollow" target="_blank"> GRU </a>，或标准RNN层)</li><li id="cf0b" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">返回序列</li><li id="acfc" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">拒绝传统社会的人</li></ul><h1 id="34dd" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">结论</h1><p id="0394" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">当谈到机器学习时，神经网络是当前最先进的，有许多拓扑和层类型可供选择。每种类型的神经网络都擅长解决特定领域的问题，每种神经网络都通过优化这些解决方案的超参数进行调整。此外，ML从业者现在可以访问许多ML框架和工具，这使得实现围绕神经网络拓扑结构构建的ML模型比以往任何时候都更容易。</p><p id="5b43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要了解更多信息，请查看我的<a class="ae le" href="https://www.perceptilabs.com/resources/handbook" rel="noopener ugc nofollow" target="_blank">机器学习手册</a>，它提供了关于神经网络和ML其他方面的更多细节。</p></div></div>    
</body>
</html>