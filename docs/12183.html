<html>
<head>
<title>Hyper Parameter Tuning — A Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调谐—教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyper-parameter-tuning-a-tutorial-70dc6c552c54?source=collection_archive---------25-----------------------#2020-08-22">https://towardsdatascience.com/hyper-parameter-tuning-a-tutorial-70dc6c552c54?source=collection_archive---------25-----------------------#2020-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e95e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">不使用代码或数学的超级参数调整方法的简单高级概述</h2></div><p id="5d1c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本教程中，我们将介绍 5 种超参数优化方法:</p><ol class=""><li id="3bcb" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">网格搜索</li><li id="ba2f" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">随机搜索</li><li id="c368" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">贝叶斯优化</li><li id="dd8d" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">连续减半</li><li id="9d8a" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">超波段</li></ol><h1 id="c10b" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">网格搜索</h1><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/458dc42f029ac2194e6a7ccdd9216596.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*RKJHTpEj0KQme6d35bxN3A.jpeg"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">从这篇<a class="ae mt" href="https://www.groundai.com/project/grid-search-random-search-genetic-algorithm-a-big-comparison-for-nas/1" rel="noopener ugc nofollow" target="_blank">遗传算法论文</a></p></figure><p id="0c4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">网格搜索背后的思想非常直观。递增移动一个超参数，同时保持其他参数不变，并记录结果。基本上对所有你怀疑可以优化的超参数都这样做。这是一种非常昂贵和麻烦的优化和调整超参数的方法，今天被认为是进行超参数搜索的效率较低的方法之一。让我们看看如何做得更好。</p><h1 id="5685" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">随机搜索</h1><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/79cba66c3aebe1eaa27e6609df175ef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*gEGALzAFmBoBfJb2_kBgfw.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">来自<a class="ae mt" href="https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">的随机搜索论文</a></p></figure><p id="5550" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然你可能一开始没有想到，但随机选择所有超参数的值实际上是一种更有效的超参数调整方法。在这种情况下，我们不是保持所有超参数不变并以迭代方式调整其中一个，而是在每次试验中随机初始化所有超参数值。这更好，因为事实证明，一些超参数对优化来说比其他的更重要，如果我们不能区分重要的超参数和不重要的超参数，我们能做的下一个最好的事情是在每次试验中随机选择所有的超参数值。这将为重要的超参数提供更高的采样率，因此我们的优化将更加高效。随机搜索与网格搜索的优势在<a class="ae mt" href="https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">这篇</a>文章中有所探讨。</p><h1 id="121c" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">贝叶斯优化</h1><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mv"><img src="../Images/b711ec54f85debcde77abd8f5fd1c341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZgSLiJzTy0GgL6tYBAk75Q.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">来自<a class="ae mt" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank">贝叶斯优化回购</a></p></figure><p id="12b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实际上，高层次的概念非常简单，我们试图用贝叶斯优化做同样的事情，我们总是试图在 ML 中做，那是估计函数，函数太复杂而无法公式化。但是现在我们试图逼近的函数是我们的 ML 算法。在这种情况下，我们可能会使用深度学习或其他形式的 ML，我们只能运行有限数量的试验来测试超参数的不同组合。如果我们能够在选择下一个超参数配置之前智能地近似我们的 ML 算法的结果，我们可能会节省大量的时间和金钱。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi na"><img src="../Images/59f9b79b43c0d78021df63704ba85e30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*YfYSAJ59d4LihU1a2jaFgA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">来自<a class="ae mt" href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf" rel="noopener ugc nofollow" target="_blank"> TPE 论文</a></p></figure><p id="4971" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有几个贝叶斯优化函数，但其关键思想是使用贝叶斯方法来估计一个更好的超参数配置给定以前的一组配置及其结果。在上图中，我们使用了一种叫做<a class="ae mt" href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf" rel="noopener ugc nofollow" target="_blank"> TPE </a>的算法，其基本概念是将我们的试验根据其表现分成两组，即得到较好结果的组和得到较差结果的组。然后，我们基于其属于好分布而不是坏分布的概率来挑选下一组超参数。</p><h1 id="5ffd" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">连续减半</h1><p id="683b" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">在连续减半中，我们开始训练少量时期的大量试验，超参数配置是随机的。然后，我们丢弃表现最差的试验，并且仅继续训练表现最好的试验，我们这样做，直到保留单个超参数配置。</p><h1 id="55e8" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">超波段</h1><p id="a610" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">超带是逐次减半算法的扩展。连续减半的问题是，我们通常无法知道试验次数与时期数之间的正确权衡。在某些情况下，一些超参数配置可能需要更长时间才能收敛，因此开始时进行大量试验，但少量的历元并不理想，在其他情况下，收敛速度很快，试验次数是瓶颈。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/8ac0c5bbabbf22bcc48de6d6665fb7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*eaLW_XGC8r2lFmq0A98AUg.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">摘自<a class="ae mt" href="https://arxiv.org/pdf/1603.06560.pdf" rel="noopener ugc nofollow" target="_blank">超频论文</a></p></figure><p id="5074" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是超级波段的用武之地。Hyperband 本质上只是对最优分配策略的网格搜索。所以在每个单独的试验中，超参数组是随机选择的。</p><p id="0ead" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图像中，您可以看到 hyperband 算法将在 5 次资源分配中连续减半。s=4 以 81 个试验开始其第一轮，为每个试验提供单个历元，然后迭代地丢弃 2/3 的试验，直到剩下一个试验，并训练 81 个历元。在 s=0 时，Hyperband 算法基本上运行一个随机搜索，进行 5 次试验，每次试验都根据最大历元数进行训练。</p><h1 id="a44f" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">扎祖姆尔</h1><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nh"><img src="../Images/b8806f0b95d33b772baf3515ce270aae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C2_GQJx2C95sp3mhQWGxXA.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">ZazuML 开源项目</p></figure><p id="e994" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ZazuML 是我和一些朋友一直在做的一个开源 AutoML 项目。它混合了几种搜索算法，包括前面提到的超波段和随机搜索。</p><p id="3c0d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请查看我们的<a class="ae mt" href="https://github.com/dataloop-ai/ZazuML" rel="noopener ugc nofollow" target="_blank"> Github </a>！</p></div></div>    
</body>
</html>