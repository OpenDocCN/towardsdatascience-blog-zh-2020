# ç”¨äºæ— æ¨¡å‹è®­ç»ƒçš„æ–‡æœ¬åˆ†ç±»çš„ BERT

> åŸæ–‡ï¼š<https://towardsdatascience.com/text-classification-with-no-model-training-935fe0e42180?source=collection_archive---------1----------------------->

![](img/bf7a59d41b19f964ce71b34753c515bf.png)

## å¦‚æœæ²¡æœ‰å¸¦æ ‡ç­¾çš„è®­ç»ƒé›†ï¼Œè¯·ä½¿ç”¨ BERTã€å•è¯åµŒå…¥å’Œå‘é‡ç›¸ä¼¼åº¦

## æ‘˜è¦

æ‚¨æ˜¯å¦å› ä¸ºæ²¡æœ‰å¸¦æ ‡ç­¾çš„æ•°æ®é›†è€Œéš¾ä»¥å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ BERT å’Œ Python è§£é‡Šå¦‚ä½•æ‰§è¡Œä¸€ç§åŸºäºç›¸ä¼¼æ€§çš„â€œæ— ç›‘ç£â€æ–‡æœ¬åˆ†ç±»ã€‚

![](img/f94b673ab86710650fa386a4a26df041.png)

ä½œè€…å›¾ç‰‡

[**ã€NLP(è‡ªç„¶è¯­è¨€å¤„ç†)**](https://en.wikipedia.org/wiki/Natural_language_processing) æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œç ”ç©¶è®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•ç»™è®¡ç®—æœºç¼–ç¨‹ä»¥å¤„ç†å’Œåˆ†æå¤§é‡è‡ªç„¶è¯­è¨€æ•°æ®ã€‚NLP é€šå¸¸ç”¨äºæ–‡æœ¬æ•°æ®çš„åˆ†ç±»ã€‚**æ–‡æœ¬åˆ†ç±»**å°±æ˜¯æ ¹æ®æ–‡æœ¬æ•°æ®çš„å†…å®¹ç»™æ–‡æœ¬æ•°æ®åˆ†é…ç±»åˆ«çš„é—®é¢˜ã€‚ä¸ºäº†æ‰§è¡Œåˆ†ç±»ç”¨ä¾‹ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„æ ‡è®°æ•°æ®é›†ã€‚å¦‚æœä½ æ²¡æœ‰ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

è¿™ç§æƒ…å†µåœ¨ç°å®ä¸–ç•Œä¸­å‘ç”Ÿçš„æ¬¡æ•°æ¯”ä½ æƒ³è±¡çš„è¦å¤šã€‚å¦‚ä»Šï¼Œäººå·¥æ™ºèƒ½è¢«å¤§è‚†å®£ä¼ ï¼Œä»¥è‡³äºä¼ä¸šç”šè‡³åœ¨æ²¡æœ‰æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿæƒ³ä½¿ç”¨å®ƒã€‚ç‰¹åˆ«æ˜¯ï¼Œå¤§å¤šæ•°éæŠ€æœ¯äººå‘˜å¹¶æ²¡æœ‰å®Œå…¨ç†è§£â€œç›®æ ‡å˜é‡â€çš„æ¦‚å¿µï¼Œä»¥åŠå®ƒåœ¨ç›‘ç£æœºå™¨å­¦ä¹ ä¸­æ˜¯å¦‚ä½•ä½¿ç”¨çš„ã€‚é‚£ä¹ˆï¼Œå½“ä½ æœ‰æ–‡æœ¬æ•°æ®ä½†æ²¡æœ‰æ ‡ç­¾æ—¶ï¼Œå¦‚ä½•æ„å»ºä¸€ä¸ªåˆ†ç±»å™¨å‘¢ï¼Ÿåœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†è§£é‡Šä¸€ç§åº”ç”¨ W2V å’Œ BERT é€šè¿‡è¯å‘é‡ç›¸ä¼¼åº¦å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»çš„ç­–ç•¥ã€‚

æˆ‘å°†å±•ç¤ºä¸€äº›æœ‰ç”¨çš„ Python ä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºå…¶ä»–ç±»ä¼¼çš„æƒ…å†µ(åªéœ€å¤åˆ¶ã€ç²˜è´´ã€è¿è¡Œ)ï¼Œå¹¶é€šè¿‡æ³¨é‡Šéå†æ¯ä¸€è¡Œä»£ç ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥å¤åˆ¶è¿™ä¸ªç¤ºä¾‹(ä¸‹é¢æ˜¯å®Œæ•´ä»£ç çš„é“¾æ¥)ã€‚

[](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb) [## mdipietro 09/data science _ äººå·¥æ™ºèƒ½ _ å®ç”¨å·¥å…·

### permalink dissolve GitHub æ˜¯è¶…è¿‡ 5000 ä¸‡å¼€å‘äººå‘˜çš„å®¶å›­ï¼Œä»–ä»¬ä¸€èµ·å·¥ä½œæ¥æ‰˜ç®¡å’Œå®¡æŸ¥ä»£ç ï¼Œç®¡ç†â€¦

github.com](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb) 

æˆ‘å°†ä½¿ç”¨â€œ**æ–°é—»ç±»åˆ«æ•°æ®é›†**ï¼Œå…¶ä¸­ä¸ºæ‚¨æä¾›äº†ä»*èµ«èŠ¬é¡¿é‚®æŠ¥*è·å¾—çš„ 2012 å¹´è‡³ 2018 å¹´çš„æ–°é—»æ ‡é¢˜ï¼Œå¹¶è¦æ±‚æ‚¨å°†å®ƒä»¬åˆ†ç±»åˆ°æ­£ç¡®çš„ç±»åˆ«ï¼Œå› æ­¤è¿™æ˜¯ä¸€ä¸ªå¤šç±»åˆ«åˆ†ç±»é—®é¢˜(ä¸‹é¢çš„é“¾æ¥)ã€‚

[](https://www.kaggle.com/rmisra/news-category-dataset) [## æ–°é—»ç±»åˆ«æ•°æ®é›†

### æ ¹æ®æ ‡é¢˜å’Œç®€çŸ­æè¿°è¯†åˆ«æ–°é—»çš„ç±»å‹

www.kaggle.com](https://www.kaggle.com/rmisra/news-category-dataset) 

ç‰¹åˆ«æ˜¯ï¼Œæˆ‘å°†ç»å†:

*   è®¾ç½®:å¯¼å…¥åŒ…ï¼Œè¯»å–æ•°æ®ã€‚
*   é¢„å¤„ç†:æ¸…ç†æ–‡æœ¬æ•°æ®ã€‚
*   åˆ›å»ºç›®æ ‡é›†ç¾¤:ä½¿ç”¨ Word2Vec å’Œ *gensim* æ„å»ºç›®æ ‡å˜é‡ã€‚
*   ç‰¹å¾å·¥ç¨‹:ç”¨*å˜å½¢é‡‘åˆš*å’Œ BERT *åµŒå…¥å•è¯ã€‚*
*   æ¨¡å‹è®¾è®¡å’Œæµ‹è¯•:é€šè¿‡ä½™å¼¦ç›¸ä¼¼æ€§å°†è§‚å¯Ÿå€¼åˆ†é…ç»™é›†ç¾¤ï¼Œå¹¶è¯„ä¼°æ€§èƒ½ã€‚
*   å¯è§£é‡Šæ€§:ç†è§£æ¨¡å‹å¦‚ä½•äº§ç”Ÿç»“æœã€‚

## è®¾ç½®

é¦–å…ˆï¼Œæˆ‘éœ€è¦å¯¼å…¥ä»¥ä¸‹åŒ…:

```
**## for data** import **json** import **pandas** as pd
import **numpy** as np
from **sklearn** import metrics, manifold**## for processing** import **re**
import **nltk****## for plotting**
import **matplotlib**.pyplot as plt
import **seaborn** as sns**## for w2v**
import **gensim** import gensim.downloader as gensim_api**## for bert**
import **transformers**
```

æ•°æ®é›†åŒ…å«åœ¨ä¸€ä¸ª json æ–‡ä»¶ä¸­ï¼Œæ‰€ä»¥æˆ‘å°†é¦–å…ˆç”¨ *json* æŠŠå®ƒè¯»å…¥ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œç„¶åæŠŠå®ƒè½¬æ¢æˆä¸€ä¸ª *pandas* Dataframeã€‚

```
lst_dics = []
with **open**('data.json', mode='r', errors='ignore') as json_file:
    for dic in json_file:
        lst_dics.append( json**.loads**(dic) )**## print the first one**
lst_dics[0]
```

![](img/bb93abf7957730b12bffc659571c6d92.png)

ä½œè€…å›¾ç‰‡

åŸå§‹æ•°æ®é›†åŒ…å«è¶…è¿‡ 30 ä¸ªç±»åˆ«ï¼Œä½†æ˜¯å‡ºäºæœ¬æ•™ç¨‹çš„ç›®çš„ï¼Œæˆ‘å°†ä½¿ç”¨ 3 ä¸ªç±»åˆ«çš„å­é›†:å¨±ä¹ã€æ”¿æ²»å’ŒæŠ€æœ¯ã€‚

```
**## create dtf**
dtf = pd.DataFrame(lst_dics)**## filter categories**
dtf = dtf[ dtf["category"].isin(['**ENTERTAINMENT**','**POLITICS**','**TECH**'])        ][["category","headline"]]**## rename columns**
dtf = dtf.rename(columns={"category":"**y**", "headline":"**text**"})**## print 5 random rows**
dtf.sample(5)
```

![](img/c331c42eb10537ab4cfb69673ae00f87.png)

ä½œè€…å›¾ç‰‡

å¦‚æ‚¨æ‰€è§ï¼Œæ•°æ®é›†è¿˜åŒ…æ‹¬ä¸€ä¸ªç›®æ ‡å˜é‡ã€‚æˆ‘ä¸ä¼šå°†å®ƒç”¨äºå»ºæ¨¡ï¼Œåªæ˜¯ç”¨äºæ€§èƒ½è¯„ä¼°ã€‚

æ‰€ä»¥æˆ‘ä»¬æœ‰ä¸€äº›åŸå§‹çš„æ–‡æœ¬æ•°æ®ï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯æŠŠå®ƒåˆ†æˆæˆ‘ä»¬ä¸€æ— æ‰€çŸ¥çš„ 3 ç±»(å¨±ä¹ã€æ”¿æ²»ã€ç§‘æŠ€)ã€‚è¿™æ˜¯æˆ‘è®¡åˆ’è¦åšçš„:

*   æ¸…ç†æ•°æ®å¹¶å°†å…¶åµŒå…¥å‘é‡ç©ºé—´ï¼Œ
*   ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºä¸€ä¸ªä¸»é¢˜èšç±»å¹¶å°†å…¶åµŒå…¥å‘é‡ç©ºé—´ï¼Œ
*   è®¡ç®—æ¯ä¸ªæ–‡æœ¬å‘é‡å’Œä¸»é¢˜èšç±»ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œç„¶åå°†å…¶åˆ†é…ç»™æœ€æ¥è¿‘çš„èšç±»ã€‚

![](img/8a7a4969d816d3b7a8ae5a2f85294557.png)

ä½œè€…å›¾ç‰‡

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ç§°ä¹‹ä¸ºâ€œä¸€ç§æ— ç›‘ç£çš„æ–‡æœ¬åˆ†ç±»â€ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸åŸºæœ¬çš„æƒ³æ³•ï¼Œä½†æ˜¯æ‰§è¡Œèµ·æ¥ä¼šå¾ˆæ£˜æ‰‹ã€‚

ç°åœ¨éƒ½å‡†å¤‡å¥½äº†ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ã€‚

## é¢„å¤„ç†

ç»å¯¹çš„ç¬¬ä¸€æ­¥æ˜¯å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†:æ¸…ç†æ–‡æœ¬ã€åˆ é™¤åœç”¨è¯å’Œåº”ç”¨è¯æ±‡åŒ–ã€‚æˆ‘å°†ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ã€‚

```
**'''
Preprocess a string.
:parameter
    :param text: string - name of column containing text
    :param lst_stopwords: list - list of stopwords to remove
    :param flg_stemm: bool - whether stemming is to be applied
    :param flg_lemm: bool - whether lemmitisation is to be applied
:return
    cleaned text
'''**
def **utils_preprocess_text**(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):
    **## clean (convert to lowercase and remove punctuations and   
    characters and then strip)**
    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())

    **## Tokenize (convert from string to list)**
    lst_text = text.split() **## remove Stopwords**
    if lst_stopwords is not None:
        lst_text = [word for word in lst_text if word not in 
                    lst_stopwords]

    **## Stemming (remove -ing, -ly, ...)**
    if flg_stemm == True:
        ps = nltk.stem.porter.PorterStemmer()
        lst_text = [ps.stem(word) for word in lst_text]

    **## Lemmatisation (convert the word into root word)**
    if flg_lemm == True:
        lem = nltk.stem.wordnet.WordNetLemmatizer()
        lst_text = [lem.lemmatize(word) for word in lst_text]

    **## back to string from list**
    text = " ".join(lst_text)
    return text
```

è¯¥å‡½æ•°ä»è¯­æ–™åº“ä¸­åˆ é™¤ä¸€ç»„ç»™å®šçš„å•è¯ã€‚æˆ‘å¯ä»¥ç”¨ *nltk* ä¸ºè‹±è¯­è¯æ±‡åˆ›å»ºä¸€ä¸ªé€šç”¨åœç”¨è¯åˆ—è¡¨(æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ æˆ–åˆ é™¤å•è¯æ¥ç¼–è¾‘è¿™ä¸ªåˆ—è¡¨)ã€‚

```
lst_stopwords = **nltk**.corpus.stopwords.words("**english**")
lst_stopwords
```

![](img/0256ecf866f72fba87f57927ab64ddfb.png)

ä½œè€…å›¾ç‰‡

ç°åœ¨ï¼Œæˆ‘å°†å¯¹æ•´ä¸ªæ•°æ®é›†åº”ç”¨è¯¥å‡½æ•°ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªåä¸ºâ€œ *text_clean* çš„æ–°åˆ—ä¸­ï¼Œæˆ‘å°†æŠŠå®ƒç”¨ä½œè¯­æ–™åº“ã€‚

```
dtf["**text_clean**"] = dtf["text"].apply(lambda x: 
          **utils_preprocess_text**(x, flg_stemm=False, **flg_lemm=True**, 
          **lst_stopwords=lst_stopwords**))dtf.head()
```

![](img/e3aa1bdb08a36810789726250d592321.png)

ä½œè€…å›¾ç‰‡

æˆ‘ä»¬æœ‰äº†é¢„å¤„ç†çš„è¯­æ–™åº“ï¼Œå› æ­¤ä¸‹ä¸€æ­¥æ˜¯æ„å»ºç›®æ ‡å˜é‡ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œ:

![](img/bd39f45e88440464b32913a23517e0de.png)

ä½œè€…å›¾ç‰‡

## åˆ›å»ºç›®æ ‡é›†ç¾¤

æœ¬èŠ‚çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€äº›å¯ä»¥ä»£è¡¨æ¯ä¸ªç±»åˆ«çš„ä¸Šä¸‹æ–‡çš„å…³é”®å­—ã€‚é€šè¿‡è¿›è¡Œä¸€äº›æ–‡æœ¬åˆ†æï¼Œä½ å¯ä»¥å¾ˆå®¹æ˜“åœ°å‘ç°ï¼Œå‡ºç°é¢‘ç‡æœ€é«˜çš„ 3 ä¸ªè¯æ˜¯â€œ*ç”µå½±*â€ã€â€œ*ç‹ç‰Œ*â€å’Œâ€œ*è‹¹æœ*â€(å…³äºè¯¦ç»†çš„æ–‡æœ¬åˆ†ææ•™ç¨‹ï¼Œä½ å¯ä»¥æŸ¥çœ‹[è¿™ç¯‡æ–‡ç« ](/text-analysis-feature-engineering-with-nlp-502d6ea9225d))ã€‚æˆ‘å»ºè®®ä»è¿™äº›å…³é”®è¯å¼€å§‹ã€‚

è®©æˆ‘ä»¬ä»¥æ”¿æ²»ç±»åˆ«ä¸ºä¾‹:å•è¯â€œ *trump* â€å¯ä»¥æœ‰ä¸åŒçš„å«ä¹‰ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦æ·»åŠ å…³é”®å­—æ¥é¿å…å¤šä¹‰æ€§é—®é¢˜(ä¾‹å¦‚ï¼Œâ€œ *donald* â€ã€â€œ *republican* â€ã€â€œ *white house* â€ã€â€œ *obama* â€)ã€‚è¿™é¡¹ä»»åŠ¡å¯ä»¥æ‰‹åŠ¨æ‰§è¡Œï¼Œæˆ–è€…æ‚¨å¯ä»¥ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„ NLP æ¨¡å‹çš„å¸®åŠ©ã€‚æ‚¨å¯ä»¥ä»[*genism-data*](https://github.com/RaRe-Technologies/gensim-data)*ä¸­åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„å•è¯åµŒå…¥æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤º:*

```
*nlp = gensim_api.load("**glove-wiki-gigaword-300**")*
```

*gensim åŒ…æœ‰ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„åŠŸèƒ½ï¼Œå¯ä»¥å°†ä»»ä½•ç»™å®šå•è¯çš„æœ€ç›¸ä¼¼çš„å•è¯è¿”å›åˆ°è¯æ±‡è¡¨ä¸­ã€‚*

```
*nlp.**most_similar**(["**obama**"], topn=3)*
```

*![](img/7c7bd92f9c81a200c71a25923ec56a28.png)*

*ä½œè€…å›¾ç‰‡*

*æˆ‘å°†ä½¿ç”¨å®ƒä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºä¸€ä¸ªå…³é”®å­—å­—å…¸:*

```
***## Function to apply**
def **get_similar_words**(lst_words, top, nlp):
    lst_out = lst_words
    for tupla in nlp.most_similar(lst_words, topn=top):
        lst_out.append(tupla[0])
    return list(set(lst_out)) **## Create Dictionary {category:[keywords]}** dic_clusters = {}dic_clusters["**ENTERTAINMENT**"] = get_similar_words([**'celebrity','cinema','movie','music'**], 
                  top=30, nlp=nlp)dic_clusters[**"POLITICS"**] = get_similar_words([**'gop','clinton','president','obama','republican'**]
                  , top=30, nlp=nlp)dic_clusters["**TECH**"] = get_similar_words([**'amazon','android','app','apple','facebook',
                   'google','tech'**], 
                   top=30, nlp=nlp) **## print some**
for k,v in dic_clusters.items():
    print(k, ": ", v[0:5], "...", len(v))*
```

*![](img/5208d371e0202dc1ea06023b27e2cb8b.png)*

*ä½œè€…å›¾ç‰‡*

*è®©æˆ‘ä»¬é€šè¿‡åº”ç”¨é™ç»´ç®—æ³•(å³ [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) )æ¥å°è¯•åœ¨ 2D ç©ºé—´ä¸­å¯è§†åŒ–é‚£äº›å…³é”®è¯ã€‚æˆ‘ä»¬å¸Œæœ›ç¡®ä¿é›†ç¾¤ä¹‹é—´èƒ½å¤Ÿå¾ˆå¥½åœ°åˆ†ç¦»ã€‚*

```
***## word embedding** tot_words = [word for v in **dic_clusters**.values() for word in v]
X = nlp[tot_words] **## pca**
pca = manifold.**TSNE**(perplexity=40, n_components=2, init='pca')
X = pca.fit_transform(X) **## create dtf**
dtf = pd.DataFrame()
for k,v in **dic_clusters**.items():
    size = len(dtf) + len(v)
    dtf_group = pd.DataFrame(X[len(dtf):size], columns=["x","y"], 
                             index=v)
    dtf_group["cluster"] = k
    dtf = dtf.append(dtf_group) **## plot**
fig, ax = plt.subplots()
sns.**scatterplot**(data=dtf, x="x", y="y", hue="cluster", ax=ax)ax.legend().texts[0].set_text(None)
ax.set(xlabel=None, ylabel=None, xticks=[], xticklabels=[], 
       yticks=[], yticklabels=[])for i in range(len(dtf)):
    ax.annotate(dtf.index[i], 
               xy=(dtf["x"].iloc[i],dtf["y"].iloc[i]), 
               xytext=(5,2), textcoords='offset points', 
               ha='right', va='bottom')*
```

*![](img/bfaca6b1348a9924f8d1e707439e337d.png)*

*ä½œè€…å›¾ç‰‡*

*é…·ï¼Œä»–ä»¬çœ‹èµ·æ¥å·²ç»è¶³å¤Ÿå­¤ç«‹äº†ã€‚å¨±ä¹é›†ç¾¤æ¯”æ”¿æ²»é›†ç¾¤æ›´æ¥è¿‘ç§‘æŠ€é›†ç¾¤ï¼Œè¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºåƒâ€œ*è‹¹æœ*â€å’Œâ€œ *youtube* â€è¿™æ ·çš„è¯å¯ä»¥åŒæ—¶å‡ºç°åœ¨ç§‘æŠ€å’Œå¨±ä¹æ–°é—»ä¸­ã€‚*

## *ç‰¹å¾å·¥ç¨‹*

*æ˜¯æ—¶å€™å°†æˆ‘ä»¬é¢„å¤„ç†çš„è¯­æ–™åº“å’Œæˆ‘ä»¬åˆ›å»ºçš„ç›®æ ‡èšç±»åµŒå…¥åˆ°åŒä¸€ä¸ªå‘é‡ç©ºé—´ä¸­äº†ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬æ˜¯è¿™æ ·åšçš„:*

*![](img/be98d23800c95e2897b91a88dc10c7f1.png)*

*ä½œè€…å›¾ç‰‡*

*æ˜¯çš„ï¼Œæˆ‘åœ¨ç”¨ [**BERT**](https://en.wikipedia.org/wiki/BERT_(language_model)) åšè¿™ä¸ªã€‚çš„ç¡®ï¼Œæ‚¨å¯ä»¥åˆ©ç”¨ä»»ä½•å•è¯åµŒå…¥æ¨¡å‹(å³ Word2Vecã€Glove ç­‰)ï¼Œç”šè‡³æ˜¯æˆ‘ä»¬å·²ç»åŠ è½½çš„å®šä¹‰å…³é”®å­—çš„æ¨¡å‹ï¼Œæ‰€ä»¥ä¸ºä»€ä¹ˆè¦è´¹å¿ƒä½¿ç”¨å¦‚æ­¤æ²‰é‡å’Œå¤æ‚çš„è¯­è¨€æ¨¡å‹å‘¢ï¼Ÿè¿™æ˜¯å› ä¸º BERT æ²¡æœ‰åº”ç”¨å›ºå®šçš„åµŒå…¥ï¼Œè€Œæ˜¯æŸ¥çœ‹æ•´ä¸ªå¥å­ï¼Œç„¶åç»™æ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ªåµŒå…¥ã€‚å› æ­¤ï¼ŒBERT åˆ†é…ç»™ä¸€ä¸ªå•è¯çš„å‘é‡æ˜¯æ•´ä¸ªå¥å­çš„å‡½æ•°ï¼Œå› æ­¤ä¸€ä¸ªå•è¯å¯ä»¥åŸºäºä¸Šä¸‹æ–‡å…·æœ‰ä¸åŒçš„å‘é‡ã€‚*

*æˆ‘å°†ä½¿ç”¨åŒ… *transformers* åŠ è½½åŸå§‹é¢„è®­ç»ƒç‰ˆæœ¬çš„ BERTï¼Œå¹¶ç»™å‡ºä¸€ä¸ªåŠ¨æ€åµŒå…¥çš„ç¤ºä¾‹:*

```
*tokenizer = transformers.**BertTokenizer**.from_pretrained('**bert-base-
            uncased'**, do_lower_case=True)nlp = transformers.**TFBertModel**.from_pretrained(**'bert-base-uncased'**)*
```

*è®©æˆ‘ä»¬ä½¿ç”¨è¯¥æ¨¡å‹å°†å­—ç¬¦ä¸²" *river bank* "è½¬æ¢æˆå‘é‡ï¼Œå¹¶æ‰“å°åˆ†é…ç»™å•è¯" *bank* "çš„å‘é‡:*

```
*txt = **"river bank"****## tokenize**
idx = tokenizer.encode(txt)
print("tokens:", tokenizer.convert_ids_to_tokens(idx))
print("ids   :", tokenizer.encode(txt))**## word embedding**
idx = np.array(idx)[None,:]
embedding = nlp(idx)
print("shape:", embedding[0][0].shape)**## vector of the second input word**
embedding[0][0][2]*
```

*![](img/e7cf0221859bf1584f5848bd69426d28.png)*

*ä½œè€…å›¾ç‰‡*

*å¦‚æœæ‚¨å¯¹å­—ç¬¦ä¸²â€œ*é‡‘èé“¶è¡Œ*â€åšåŒæ ·çš„å¤„ç†ï¼Œæ‚¨ä¼šå‘ç°åˆ†é…ç»™å•è¯â€œ*é“¶è¡Œ*çš„å‘é‡å› ä¸Šä¸‹æ–‡è€Œå¼‚ã€‚è¯·æ³¨æ„ï¼ŒBERT è®°å·èµ‹äºˆå™¨åœ¨å¥å­çš„å¼€å¤´å’Œç»“å°¾æ’å…¥ç‰¹æ®Šè®°å·ï¼Œå…¶å‘é‡ç©ºé—´çš„ç»´æ•°ä¸º 768(ä¸ºäº†æ›´å¥½åœ°ç†è§£ BERT å¦‚ä½•å¤„ç†æ–‡æœ¬ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹[è¿™ç¯‡æ–‡ç« ](/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794))ã€‚*

*![](img/35c538412e537c1f5190d4a7cfe6df08.png)*

*ä½œè€…å›¾ç‰‡*

*è¯´äº†è¿™ä¹ˆå¤šï¼Œè®¡åˆ’æ˜¯ç”¨ BERT Word Embedding ç”¨ä¸€ä¸ªæ•°ç»„(shape: number of tokens x 768)è¡¨ç¤ºæ¯ç¯‡æ–‡æœ¬ï¼Œç„¶åæŠŠæ¯ç¯‡æ–‡ç« æ±‡æ€»æˆä¸€ä¸ªå‡å€¼å‘é‡ã€‚*

*![](img/3e242f0f37c1bfd473f7c28960d83b26.png)*

*ä½œè€…å›¾ç‰‡*

*å› æ­¤ï¼Œæœ€ç»ˆçš„ç‰¹å¾çŸ©é˜µå°†æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º:æ–‡æ¡£æ•°(æˆ–å‡å€¼å‘é‡)x 768 çš„æ•°ç»„ã€‚*

```
***## function to apply** def **utils_bert_embedding**(txt, tokenizer, nlp):
    idx = tokenizer.encode(txt)
    idx = np.array(idx)[None,:]  
    embedding = nlp(idx)
    X = np.array(embedding[0][0][1:-1])
    return X**## create list of news vector**
lst_mean_vecs = [**utils_bert_embedding**(txt, tokenizer, nlp)**.mean(0)** 
                 for txt in dtf["**text_clean**"]]**## create the feature matrix (n news x 768)**
X = np.array(lst_mean_vecs)*
```

*æˆ‘ä»¬å¯ä»¥å¯¹ç›®æ ‡é›†ç¾¤ä¸­çš„å…³é”®å­—åšåŒæ ·çš„äº‹æƒ…ã€‚äº‹å®ä¸Šï¼Œæ¯ä¸ªæ ‡ç­¾éƒ½ç”±ä¸€ä¸ªå•è¯åˆ—è¡¨æ¥æ ‡è¯†ï¼Œå¸®åŠ© BERT ç†è§£é›†ç¾¤ä¸­çš„ä¸Šä¸‹æ–‡ã€‚å› æ­¤ï¼Œæˆ‘å°†åˆ›å»ºä¸€ä¸ªå­—å…¸æ ‡ç­¾:èšç±»å‡å€¼å‘é‡ã€‚*

```
*dic_y = {k:**utils_bert_embedding**(v, tokenizer, nlp)**.mean(0)** for k,v
         in dic_clusters.items()}*
```

*æˆ‘ä»¬å¼€å§‹æ—¶åªæœ‰ä¸€äº›æ–‡æœ¬æ•°æ®å’Œ 3 ä¸ªå­—ç¬¦ä¸²(*â€œå¨±ä¹â€ã€â€œæ”¿æ²»â€ã€â€œæŠ€æœ¯â€*)ï¼Œç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªç‰¹å¾çŸ©é˜µå’Œä¸€ä¸ªç›®æ ‡å˜é‡â€¦ ishã€‚*

## *æ¨¡å‹è®¾è®¡å’Œæµ‹è¯•*

*æœ€åï¼Œæ˜¯æ—¶å€™å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œæ ¹æ®ä¸æ¯ä¸ªç›®æ ‡èšç±»çš„ç›¸ä¼¼æ€§å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»äº†ã€‚*

*![](img/9b2527fd377c64fb6aaafa95bc37a29e.png)*

*ä½œè€…å›¾ç‰‡*

*æˆ‘å°†ä½¿ç”¨ [**ä½™å¼¦ç›¸ä¼¼åº¦**](https://en.wikipedia.org/wiki/Cosine_similarity) ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸¤ä¸ªéé›¶å‘é‡ä¹‹é—´çš„è§’åº¦ä½™å¼¦çš„ç›¸ä¼¼æ€§åº¦é‡ï¼Œå®ƒç­‰äºå½’ä¸€åŒ–ä¸ºé•¿åº¦éƒ½ä¸º 1 çš„ç›¸åŒå‘é‡çš„å†…ç§¯ã€‚æ‚¨å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ [*scikit çš„ä½™å¼¦ç›¸ä¼¼æ€§å®ç°-learn*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) *ï¼Œ*å®ƒé‡‡ç”¨ 2 ä¸ªæ•°ç»„(æˆ–å‘é‡)å¹¶è¿”å›ä¸€ä¸ªåˆ†æ•°æ•°ç»„(æˆ–å•ä¸ªåˆ†æ•°)ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¾“å‡ºå°†æ˜¯ä¸€ä¸ªå…·æœ‰å½¢çŠ¶çš„çŸ©é˜µ:æ–°é—»æ•°é‡ x æ ‡ç­¾æ•°é‡(3ï¼Œå¨±ä¹/æ”¿æ²»/æŠ€æœ¯)ã€‚æ¢å¥è¯è¯´ï¼Œæ¯è¡Œå°†ä»£è¡¨ä¸€ç¯‡æ–‡ç« ï¼Œå¹¶åŒ…å«æ¯ä¸ªç›®æ ‡èšç±»çš„ä¸€ä¸ªç›¸ä¼¼æ€§å¾—åˆ†ã€‚*

*ä¸ºäº†è¿è¡Œé€šå¸¸çš„è¯„ä¼°æŒ‡æ ‡(å‡†ç¡®æ€§ã€AUCã€ç²¾ç¡®åº¦ã€å¬å›ç‡ç­‰)ï¼Œæˆ‘ä»¬å¿…é¡»é‡æ–°è°ƒæ•´æ¯ä¸€è¡Œçš„åˆ†æ•°ï¼Œä½¿å®ƒä»¬çš„æ€»å’Œä¸º 1ï¼Œå¹¶å†³å®šæ–‡ç« çš„ç±»åˆ«ã€‚æˆ‘å°†é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ä¸€ä¸ªï¼Œä½†è®¾ç½®ä¸€äº›æœ€ä½é˜ˆå€¼å¹¶å¿½ç•¥å¾—åˆ†éå¸¸ä½çš„é¢„æµ‹å¯èƒ½æ˜¯æ˜æ™ºçš„ã€‚*

*![](img/c4d42a0cf2a7fa9cfd342a01bc77f84e.png)*

*ä½œè€…å›¾ç‰‡*

```
***#--- Model Algorithm ---#****## compute cosine similarities**
similarities = np.array(
            [metrics.pairwise.**cosine_similarity**(X, y).T.tolist()[0] 
             for y in dic_y.values()]
            ).T**## adjust and rescale**
labels = list(dic_y.keys())
for i in range(len(similarities)): **### assign randomly if there is no similarity**
    if sum(similarities[i]) == 0:
       similarities[i] = [0]*len(labels)
       similarities[i][np.random.choice(range(len(labels)))] = 1 **### rescale so they sum = 1**
    similarities[i] = similarities[i] / sum(similarities[i]) **## classify the label with highest similarity score** predicted_prob = similarities
predicted = [labels[np.argmax(pred)] for pred in predicted_prob]*
```

*å°±åƒåœ¨ç»å…¸çš„ç›‘ç£ç”¨ä¾‹ä¸­ä¸€æ ·ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå…·æœ‰é¢„æµ‹æ¦‚ç‡çš„å¯¹è±¡(è¿™é‡Œå®ƒä»¬æ˜¯è°ƒæ•´åçš„ç›¸ä¼¼æ€§åˆ†æ•°),å¦ä¸€ä¸ªå…·æœ‰é¢„æµ‹æ ‡ç­¾ã€‚è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æˆ‘ä»¬åšå¾—å¦‚ä½•:*

```
*y_test = dtf[**"y"**].values
classes = np.unique(y_test)
y_test_array = pd.get_dummies(y_test, drop_first=False).values **## Accuracy, Precision, Recall**
accuracy = metrics.accuracy_score(y_test, predicted)
auc = metrics.roc_auc_score(y_test, predicted_prob, 
                            multi_class="ovr")
print("Accuracy:",  round(accuracy,2))
print("Auc:", round(auc,2))
print("Detail:")
print(metrics.classification_report(y_test, predicted)) **## Plot confusion matrix**
cm = metrics.confusion_matrix(y_test, predicted)
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, 
            cbar=False)
ax.set(xlabel="Pred", ylabel="True", xticklabels=classes, 
       yticklabels=classes, title="Confusion matrix")
plt.yticks(rotation=0)
fig, ax = plt.subplots(nrows=1, ncols=2) **## Plot roc**
for i in range(len(classes)):
    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  
                           predicted_prob[:,i])
    ax[0].plot(fpr, tpr, lw=3, 
              label='{0} (area={1:0.2f})'.format(classes[i], 
                              metrics.auc(fpr, tpr))
               )
ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')
ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], 
          xlabel='False Positive Rate', 
          ylabel="True Positive Rate (Recall)", 
          title="Receiver operating characteristic")
ax[0].legend(loc="lower right")
ax[0].grid(True) **## Plot precision-recall curve** for i in range(len(classes)):
    precision, recall, thresholds = metrics.precision_recall_curve(
                 y_test_array[:,i], predicted_prob[:,i])
    ax[1].plot(recall, precision, lw=3, 
               label='{0} (area={1:0.2f})'.format(classes[i], 
                                  metrics.auc(recall, precision))
              )
ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', 
          ylabel="Precision", title="Precision-Recall curve")
ax[1].legend(loc="best")
ax[1].grid(True)
plt.show()*
```

*![](img/57cb3c6d73895c5a2b0fa48f6d1f89c8.png)**![](img/7d46d5866808ab0572f9c6e30cb8ceaf.png)*

*ä½œè€…å›¾ç‰‡*

*å¥½å§ï¼Œæˆ‘ç¬¬ä¸€ä¸ªè¯´è¿™ä¸æ˜¯æˆ‘è§è¿‡çš„æœ€å¥½çš„å‡†ç¡®åº¦ã€‚å¦ä¸€æ–¹é¢ï¼Œè€ƒè™‘åˆ°æˆ‘ä»¬æ²¡æœ‰è®­ç»ƒä»»ä½•æ¨¡å‹ï¼Œæˆ‘ä»¬ç”šè‡³è™šæ„äº†ç›®æ ‡å˜é‡ï¼Œè¿™ä¸€ç‚¹ä¹Ÿä¸å·®ã€‚ä¸»è¦é—®é¢˜æ˜¯åˆ†ç±»ä¸ºå¨±ä¹çš„ 4k ä»¥ä¸Šçš„æ”¿æ²»è§‚å¯Ÿï¼Œä½†è¿™äº›æ€§èƒ½å¯ä»¥é€šè¿‡å¾®è°ƒè¿™ä¸¤ä¸ªç±»åˆ«çš„å…³é”®å­—æ¥è½»æ¾æ”¹å–„ã€‚*

## *å¯è§£é‡Šæ€§*

*è®©æˆ‘ä»¬è¯•ç€ç†è§£æ˜¯ä»€ä¹ˆè®©æˆ‘ä»¬çš„ç®—æ³•ç”¨ä¸€ä¸ªç±»åˆ«è€Œä¸æ˜¯å…¶ä»–ç±»åˆ«å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»ã€‚è®©æˆ‘ä»¬ä»è¯­æ–™åº“ä¸­éšæœºè§‚å¯Ÿä¸€ä¸‹:*

```
*i = 7**txt_instance** = dtf[**"text_clean"**].iloc[i]print("True:", y_test[i], "--> Pred:", predicted[i], "| 
      Similarity:", round(np.max(predicted_prob[i]),2))
print(txt_instance)*
```

*![](img/a74963cd1b7f2b9f5d176dc5218f7007.png)*

*ä½œè€…å›¾ç‰‡*

*è¿™æ˜¯ä¸€ä¸ªæ­£ç¡®åˆ†ç±»çš„æ”¿æ²»è§‚å¯Ÿã€‚å¤§æ¦‚ï¼Œâ€œ*å…±å’Œå…š*â€å’Œâ€œ*å…‹æ—é¡¿*â€è¿™ä¸¤ä¸ªè¯ç»™äº†ä¼¯ç‰¹æ­£ç¡®çš„æš—ç¤ºã€‚æˆ‘å°†åœ¨ 2D ç©ºé—´ä¸­å¯è§†åŒ–æ–‡ç« çš„å¹³å‡å‘é‡ï¼Œå¹¶ç»˜åˆ¶ä¸ç›®æ ‡èšç±»çš„æœ€é«˜ç›¸ä¼¼åº¦ã€‚*

```
***## create embedding Matrix** y = np.concatenate([embedding_bert(v, tokenizer, nlp) for v in 
                    dic_clusters.values()])
X = embedding_bert(txt_instance, tokenizer,
                   nlp).mean(0).reshape(1,-1)
M = np.concatenate([y,X]) **## pca**
pca = manifold.**TSNE**(perplexity=40, n_components=2, init='pca')
M = pca.fit_transform(M)
y, X = M[:len(y)], M[len(y):] **## create dtf clusters**
dtf = pd.DataFrame()
for k,v in dic_clusters.items():
    size = len(dtf) + len(v)
    dtf_group = pd.DataFrame(y[len(dtf):size], columns=["x","y"], 
                             index=v)
    dtf_group["cluster"] = k
    dtf = dtf.append(dtf_group) **## plot clusters**
fig, ax = plt.subplots()
sns.**scatterplot**(data=dtf, x="x", y="y", hue="cluster", ax=ax)
ax.legend().texts[0].set_text(None)
ax.set(xlabel=None, ylabel=None, xticks=[], xticklabels=[], 
       yticks=[], yticklabels=[])
for i in range(len(dtf)):
    ax.annotate(dtf.index[i], 
               xy=(dtf["x"].iloc[i],dtf["y"].iloc[i]), 
               xytext=(5,2), textcoords='offset points', 
               ha='right', va='bottom') **## add txt_instance** ax.scatter(x=X[0][0], y=X[0][1], c="red", linewidth=10)
           ax.annotate("x", xy=(X[0][0],X[0][1]), 
           ha='center', va='center', fontsize=25) **## calculate similarity** sim_matrix = metrics.pairwise.**cosine_similarity**(X, y) **## add top similarity**
for row in range(sim_matrix.shape[0]): **### sorted {keyword:score}**
    dic_sim = {n:sim_matrix[row][n] for n in 
               range(sim_matrix.shape[1])}
    dic_sim = {k:v for k,v in sorted(dic_sim.items(), 
                key=lambda item:item[1], reverse=True)} **### plot lines**
    for k in dict(list(dic_sim.items())[0:5]).keys():
        p1 = [X[row][0], X[row][1]]
        p2 = [y[k][0], y[k][1]]
        ax.plot([p1[0],p2[0]], [p1[1],p2[1]], c="red", alpha=0.5)plt.show()*
```

*![](img/15f719e0c5bc98205bf7958226f78fad.png)*

*ä½œè€…å›¾ç‰‡*

*è®©æˆ‘ä»¬æ”¾å¤§ä¸€ä¸‹æ„Ÿå…´è¶£çš„é›†ç¾¤:*

*![](img/f94b673ab86710650fa386a4a26df041.png)*

*ä½œè€…å›¾ç‰‡*

*æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥è¯´å‡å€¼å‘é‡éå¸¸ç±»ä¼¼äºæ”¿æ²»èšç±»ã€‚è®©æˆ‘ä»¬å°†æ–‡ç« åˆ†è§£æˆä»¤ç‰Œï¼Œçœ‹çœ‹å“ªäº›ä»¤ç‰Œâ€œæ¿€æ´»â€äº†æ­£ç¡®çš„é›†ç¾¤ã€‚*

```
***## create embedding Matrix** y = np.concatenate([embedding_bert(v, tokenizer, nlp) for v in 
                    dic_clusters.values()])
X = embedding_bert(txt_instance, tokenizer,
                   nlp).mean(0).reshape(1,-1)
M = np.concatenate([y,X]) **## pca**
pca = manifold.**TSNE**(perplexity=40, n_components=2, init='pca')
M = pca.fit_transform(M)
y, X = M[:len(y)], M[len(y):] **## create dtf clusters**
dtf = pd.DataFrame()
for k,v in dic_clusters.items():
    size = len(dtf) + len(v)
    dtf_group = pd.DataFrame(y[len(dtf):size], columns=["x","y"], 
                             index=v)
    dtf_group["cluster"] = k
    dtf = dtf.append(dtf_group) **## add txt_instance** tokens = tokenizer.convert_ids_to_tokens(
               tokenizer.encode(txt_instance))[1:-1]
dtf = pd.DataFrame(X, columns=["x","y"], index=tokens)
dtf = dtf[~dtf.index.str.contains("#")]
dtf = dtf[dtf.index.str.len() > 1]
X = dtf.values
ax.scatter(x=dtf["x"], y=dtf["y"], c="red")
for i in range(len(dtf)):
     ax.annotate(dtf.index[i], 
                 xy=(dtf["x"].iloc[i],dtf["y"].iloc[i]), 
                 xytext=(5,2), textcoords='offset points', 
                 ha='right', va='bottom') **## calculate similarity** sim_matrix = metrics.pairwise.**cosine_similarity**(X, y) **## add top similarity**
for row in range(sim_matrix.shape[0]): **### sorted {keyword:score}**
    dic_sim = {n:sim_matrix[row][n] for n in 
               range(sim_matrix.shape[1])}
    dic_sim = {k:v for k,v in sorted(dic_sim.items(), 
                key=lambda item:item[1], reverse=True)} **### plot lines**
    for k in dict(list(dic_sim.items())[0:5]).keys():
        p1 = [X[row][0], X[row][1]]
        p2 = [y[k][0], y[k][1]]
        ax.plot([p1[0],p2[0]], [p1[1],p2[1]], c="red", alpha=0.5)plt.show()*
```

*![](img/046e2828cd122e7d39ac5589ec5e7024.png)*

*ä½œè€…å›¾ç‰‡*

*æ­£å¦‚æˆ‘ä»¬æ‰€æƒ³ï¼Œæ–‡æœ¬ä¸­æœ‰ä¸€äº›è¯æ˜æ˜¾ä¸æ”¿æ²»ç›¸å…³ï¼Œä½†å…¶ä»–ä¸€äº›è¯æ›´ç±»ä¼¼äºå¨±ä¹çš„ä¸€èˆ¬ä¸Šä¸‹æ–‡ã€‚*

*![](img/d2af8ce7492b1f31ec3820089cf7df9b.png)**![](img/e3926aa4ef7091b7d9a7c86abfc4931b.png)*

*ä½œè€…å›¾ç‰‡*

## *ç»“è®º*

*è¿™ç¯‡æ–‡ç« æ˜¯ä¸€ä¸ªæ•™ç¨‹ï¼Œæ¼”ç¤ºäº†å½“ä¸€ä¸ªå¸¦æ ‡ç­¾çš„è®­ç»ƒé›†ä¸å¯ç”¨æ—¶å¦‚ä½•æ‰§è¡Œæ–‡æœ¬åˆ†ç±»ã€‚*

*æˆ‘ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„å•è¯åµŒå…¥æ¨¡å‹æ¥æ„å»ºä¸€ç»„å…³é”®å­—ï¼Œä»¥å°†ç›®æ ‡å˜é‡ç½®äºä¸Šä¸‹æ–‡ä¸­ã€‚ç„¶åæˆ‘ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„ BERT è¯­è¨€æ¨¡å‹æŠŠé‚£äº›è¯å’Œè¯­æ–™åº“è½¬æ¢åˆ°åŒä¸€ä¸ªå‘é‡ç©ºé—´ã€‚æœ€åï¼Œæˆ‘è®¡ç®—æ–‡æœ¬å’Œå…³é”®è¯ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä»¥ç¡®å®šæ¯ç¯‡æ–‡ç« çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ä½¿ç”¨è¯¥ä¿¡æ¯æ¥æ ‡è®°æ–°é—»ã€‚*

*è¿™ç§ç­–ç•¥ä¸æ˜¯æœ€æœ‰æ•ˆçš„ï¼Œä½†å®ƒè‚¯å®šæ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸ºå®ƒèƒ½è®©ä½ è¿…é€Ÿè·å¾—å¥½çš„ç»“æœã€‚æ­¤å¤–ï¼Œä¸€æ—¦è·å¾—æ ‡è®°æ•°æ®é›†ï¼Œè¯¥ç®—æ³•å¯ä»¥ç”¨ä½œç›‘ç£æ¨¡å‹çš„åŸºçº¿ã€‚*

*æˆ‘å¸Œæœ›ä½ å–œæ¬¢å®ƒï¼å¦‚æœ‰é—®é¢˜å’Œåé¦ˆï¼Œæˆ–è€…åªæ˜¯åˆ†äº«æ‚¨æ„Ÿå…´è¶£çš„é¡¹ç›®ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚*

> *ğŸ‘‰[æˆ‘ä»¬æ¥è¿çº¿](https://linktr.ee/maurodp)ğŸ‘ˆ*

> *æœ¬æ–‡æ˜¯ä½¿ç”¨ Python çš„**NLP**ç³»åˆ—çš„ä¸€éƒ¨åˆ†ï¼Œå‚è§:*

*[](/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09) [## ä½¿ç”¨ NLP çš„æ–‡æœ¬æ‘˜è¦:TextRank vs Seq2Seq vs BART

### ä½¿ç”¨ Pythonã€Gensimã€Tensorflowã€Transformers è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†

towardsdatascience.com](/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09) [](/text-analysis-feature-engineering-with-nlp-502d6ea9225d) [## ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡æœ¬åˆ†æå’Œç‰¹å¾å·¥ç¨‹

### è¯­è¨€æ£€æµ‹ï¼Œæ–‡æœ¬æ¸…ç†ï¼Œé•¿åº¦ï¼Œæƒ…æ„Ÿï¼Œå‘½åå®ä½“è¯†åˆ«ï¼ŒN-grams é¢‘ç‡ï¼Œè¯å‘é‡ï¼Œä¸»é¢˜â€¦

towardsdatascience.com](/text-analysis-feature-engineering-with-nlp-502d6ea9225d) [](/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) [## åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡æœ¬åˆ†ç±»:Tf-Idf vs Word2Vec vs BERT

### é¢„å¤„ç†ã€æ¨¡å‹è®¾è®¡ã€è¯„ä¼°ã€è¯è¢‹çš„å¯è§£é‡Šæ€§ã€è¯åµŒå…¥ã€è¯­è¨€æ¨¡å‹

towardsdatascience.com](/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) [](/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9) [## å¸¦ NLP çš„ AI èŠå¤©æœºå™¨äºº:è¯­éŸ³è¯†åˆ«+å˜å½¢é‡‘åˆš

### ç”¨ Python æ„å»ºä¸€ä¸ªä¼šè¯´è¯çš„èŠå¤©æœºå™¨äººï¼Œä¸ä½ çš„äººå·¥æ™ºèƒ½è¿›è¡Œå¯¹è¯

towardsdatascience.com](/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9)*