<html>
<head>
<title>Volumetric Medical Image Segmentation with Vox2Vox</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Vox2Vox的体积医学图像分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/volumetric-medical-image-segmentation-with-vox2vox-f5350ed2094f?source=collection_archive---------37-----------------------#2020-06-12">https://towardsdatascience.com/volumetric-medical-image-segmentation-with-vox2vox-f5350ed2094f?source=collection_archive---------37-----------------------#2020-06-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6266" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何实现用于CT/ MRI分割的3D体积生成对抗网络</h2></div><p id="1b24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你熟悉生成对抗网络(GANs)及其流行变体，Pix2Pix这个术语应该一点也不陌生。Pix2Pix是一种执行图像到图像转换的条件GAN (cGAN)。在医学领域，它们通常用于执行模态翻译，在某些情况下用于器官分割。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/8dc91c3b490d58054f893b7e469c60ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QWOTXLvsQdvhgv75"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">“我脑海中的体素”——唐·巴克斯</p></figure></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="6c02" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">Pix2Pix，基础知识</h1><p id="fc17" class="pw-post-body-paragraph kf kg iq kh b ki mq jr kk kl mr ju kn ko ms kq kr ks mt ku kv kw mu ky kz la ij bi translated">与大多数gan类似，Pix2Pix由单个发生器网络和单个鉴别器网络组成。生成器网络只不过是一个U-Net，这是一种最初提出用于执行生物医学图像分割的深度卷积神经网络。U-Net具有以下架构:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mv"><img src="../Images/342f2c88bc2f14bea027a44026df8a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvXoKMHoPJMKpKK7keZMEA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">弗赖堡大学计算机科学系</p></figure><p id="e717" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">U-Net包含三个主要组件:编码器、瓶颈和解码器。Pix2Pix的生成器U-Net的技术细节包括输入/输出通道的数量、内核大小、步长和填充可以在其<a class="ae mw" href="https://arxiv.org/pdf/1611.07004.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>中找到。简而言之，编码器提供了一种压缩路径，该路径卷积并减少了给定2D图像的维度。瓶颈块包含具有跳跃连接的卷积块，并且最终解码器提供扩展路径来升级编码表示。共享相同大小的编码器和解码器层沿着它们的通道连接。如果你有兴趣了解更多关于U-Net以及它如何执行图像分割的信息，<a class="ae mw" href="https://towardsdatascience.com/@heetsankesara3?source=post_page-----b229b32b4a71----------------------" rel="noopener" target="_blank"> Heet Sankesara </a>有一篇关于它的很棒的<a class="ae mw" rel="noopener" target="_blank" href="/u-net-b229b32b4a71">文章</a>。</p><p id="f28e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Pix2Pix的鉴别器只不过是一个标准的卷积网络，它“鉴别”给定图像是真实的(原始训练数据)还是伪造的(由U-Net生成器生成)。Pix2Pix的训练目标是生成图像和真实图像之间的L₂/ MSE损失(对抗损失)和L₁损失(重建损失)的简单最小最大公式。</p><p id="9a4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Pix2Pix最早的应用之一就是从图纸生成猫的图片(给各位酷猫和小猫)。然而，它还被扩展到医学成像领域，以执行磁共振(MR)、正电子发射断层扫描(PET)和计算机断层扫描(CT)图像之间的畴转移。</p><div class="lc ld le lf gt ab cb"><figure class="mx lg my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><img src="../Images/0048462e267e50b40213d0d37bfb6d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/0*m6bAXC0c4pAf6VW0.jpg"/></div></figure><figure class="mx lg nd mz na nb nc paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><img src="../Images/614b9eeb5945f718687a5b50fd29c30e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/0*wTFID2xR4HkgHSCl.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk ne di nf ng translated">(左)Christopher Hesse的Pix2Pix demo(右)杨等的MRI跨通道翻译。</p></figure></div><p id="094e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对Pix2Pix的主要批评之一是它不能在3D级别上执行图像到图像的转换。这对许多医学人工智能研究人员来说是一个巨大的障碍，因为医学图像通常是自然的体积。随着图形处理单元(GPU)和深度神经网络设计的进步，研究人员近年来取得了巨大的突破，使他们能够执行体积分割。V-网(U-网的简单3D扩展)和密集V-网是执行3D单/多器官分割的常见架构。来自瑞典林雪平大学的M. Cirillo、D. Abramian和A. Eklund提出了Pix2Pix的一种变体，其中他们调用Vox2Vox网络以对抗的方式执行分割。这里是<a class="ae mw" href="https://arxiv.org/pdf/2003.13653.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>到论文。</p><p id="2840" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为在这篇博文发表时，目前还没有Vox2Vox的开源PyTorch实现，所以我决定尝试一下。全网实现可以在我的<a class="ae mw" href="https://github.com/enochkan/vox2vox" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> github repo </strong> </a>找到。还链接到<a class="ae mw" href="https://paperswithcode.com/paper/vox2vox-3d-gan-for-brain-tumour-segmentation" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> PapersWithCode </strong> </a>以获得更多曝光和反馈。请随意叉，修改我的代码，甚至提交错误修复，只要你给我信用。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="8c74" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">实现Vox2Vox</h1><p id="2197" class="pw-post-body-paragraph kf kg iq kh b ki mq jr kk kl mr ju kn ko ms kq kr ks mt ku kv kw mu ky kz la ij bi translated">对于这个项目，您将需要以下依赖关系:</p><ul class=""><li id="8fbc" class="nh ni iq kh b ki kj kl km ko nj ks nk kw nl la nm nn no np bi translated">Python 3.7</li><li id="20ce" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">PyTorch&gt;=0.4.0</li><li id="c341" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">火炬视觉</li><li id="77bc" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">Matplotlib</li><li id="57de" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">笨笨，笨笨</li><li id="882c" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">枕头</li><li id="e6f6" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">sci kit-图像</li><li id="f244" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">Pyvista</li><li id="a641" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">h5py</li></ul><p id="41f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">克隆存储库后，您可以通过运行以下命令来安装所有需要的库:</p><pre class="lc ld le lf gt nv nw nx ny aw nz bi"><span id="e8bc" class="oa lz iq nw b gy ob oc l od oe">pip <strong class="nw ir">install</strong> -r <strong class="nw ir">requirements</strong>.<strong class="nw ir">txt</strong></span></pre><p id="b877" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">U-Net生成器的实现可能有点棘手，主要是因为论文的作者没有指定卷积块是如何连接的。我花了好几个小时才得到正确的体积尺寸。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi of"><img src="../Images/bfc8d14e581f31b29ff9ad991fcece1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*jptw46x5ELf-7Hw9n6iYqw.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">“具有体积卷积和跳跃连接瓶颈块的3D U-Net生成器”</p></figure><p id="31a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，以下是体积U网生成器的三个主要构建模块。每个块包含一个卷积层、一个归一化层和一个激活层。三个模块:编码器模块、瓶颈模块和解码器模块实现如下:</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="b878" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">瓶颈模块和解码器模块都使用级联。然而，瓶颈模块的连接与解码器模块略有不同。如该论文所述，典型瓶颈块的输入是其前一个块的输入和输出的串联。尽管输入和输出的连接是恒定的，但是瓶颈块的输出尺寸应该保持恒定(8x8x8x512)。另一方面，解码器块的输出被连接到它们各自的编码器块的输出。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/15f5c379045d0b46a6054f1f05caaaa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*vOnO1uMfC9uMWXdGjfTm5w.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">“Vox2Vox的模型架构”——m . ci rillo、D. Abramian和A. Eklund</p></figure><p id="b7fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了模型架构描述，用我们之前创建的三个基本块来实现U-Net生成器就非常简单了:</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="c57e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">鉴别器的实现非常简单，因为它包含与发生器相同的体积编码器模块。事实上，你可以用任何你想要的鉴别器架构来训练。只要确保你在训练中注意发电机和鉴别器之间的平衡。通过调整初始学习率和设置鉴别器的精度阈值，您可以通过试错法轻松平衡两个网络。3DGAN的作者在他们最初的3DGAN论文中提供了许多关于容量GAN的平衡和稳定训练的有用建议。</p><p id="444b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Vox2Vox实现的另一个重要部分是它的损失函数。与Pix2Pix类似，Vox2Vox的损失可以分解为对抗性损失(MSE损失)和重构损失(广义骰子损失)。在PyTorch中，<em class="oj">标准</em>类似于损失函数。因此，<em class="oj"> criterion_GAN </em>表示对抗损失，而<em class="oj"> criterion_voxelwise </em>表示重建损失。发电机损耗的计算包含一个可调参数<em class="oj">λ</em>，它控制对抗损耗和重建损耗之间的比率。以下是发生器和鉴别器训练期间损失函数的实现:</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="og oh l"/></div></figure></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="f5e4" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">广义骰子损失</h1><p id="12c2" class="pw-post-body-paragraph kf kg iq kh b ki mq jr kk kl mr ju kn ko ms kq kr ks mt ku kv kw mu ky kz la ij bi translated">在医学图像分割中，一种常用的损失函数是广义dice损失。索伦森Dice系数通常用于评估两个样本之间的相似性，其公式如下:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ok"><img src="../Images/22209ba8cd30b1c267410b5c268eec96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SG-le9KHRXgU98rl.png"/></div></div></figure><p id="31f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TP代表真阳性，FP代表假阳性，FN代表假阴性。骰子系数通常在0到1之间，1代表两个给定样本之间的完美匹配。广义骰子损失是骰子分数的简单修改，以提供深度学习训练期间最小化的损失函数。下面是我的PyTorch实现的广义骰子损失:</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="9c7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然我有可能一行一行地检查我的代码的其余部分，但是我将把它留给您来通读和理解我的实现。通读原文肯定是有帮助的。我对创建Vox2Vox的作者充满敬意，所以如果你决定自己实现Vox2Vox，也请给予他们信任。如果你有任何问题，请在下面评论，告诉我你想在我的下一篇博文中看到什么！保持安全，保持好奇:)</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ol"><img src="../Images/3bf182ca2a37dc339e7616fe9b205836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/0*9JWykHkVvezf1oJM.JPG"/></div></div></figure></div></div>    
</body>
</html>