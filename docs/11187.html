<html>
<head>
<title>Reinforcement Learning — Part 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习—第 4 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-part-4-3c51edd8c4bf?source=collection_archive---------52-----------------------#2020-08-03">https://towardsdatascience.com/reinforcement-learning-part-4-3c51edd8c4bf?source=collection_archive---------52-----------------------#2020-08-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e460" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/fau-lecture-notes" rel="noopener" target="_blank"> FAU 讲座笔记</a>关于深度学习</h2><div class=""/><div class=""><h2 id="0c89" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">替代方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/eb0ef95adf3913a792d0f9acf2a7bd8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QK8JvYtNMMQphS3l.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">FAU 大学的深度学习。下图<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a></p></figure><p id="8907" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">这些是 FAU 的 YouTube 讲座</strong> <a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">深度学习</strong> </a> <strong class="lk jd">的讲义。这是与幻灯片匹配的讲座视频&amp;的完整抄本。我们希望，你喜欢这个视频一样多。当然，这份抄本是用深度学习技术在很大程度上自动创建的，只进行了少量的手动修改。</strong> <a class="ae lh" href="http://autoblog.tf.fau.de/" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">自己试试吧！如果您发现错误，请告诉我们！</strong></a></p><h1 id="7386" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">航行</h1><p id="b4b7" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/reinforcement-learning-part-3-711e31967398"> <strong class="lk jd">上一讲</strong> </a> <strong class="lk jd"> / </strong> <a class="ae lh" href="https://youtu.be/XbW-pUEs6PI" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">观看本视频</strong> </a> <strong class="lk jd"> / </strong> <a class="ae lh" rel="noopener" target="_blank" href="/all-you-want-to-know-about-deep-learning-8d68dcffc258"> <strong class="lk jd">顶级</strong></a>/<a class="ae lh" rel="noopener" target="_blank" href="/reinforcement-learning-part-5-70d10e0ca3d9"><strong class="lk jd">下一讲</strong> </a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/01eaeb3476951c7f29fa389a6ca33acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*RBgvHjVrw4L5KyJJ.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">刺猬索尼克也被看作是关于强化学习的。使用<a class="ae lh" href="https://github.com/vvo/gifify" rel="noopener ugc nofollow" target="_blank"> gifify </a>创建的图像。来源:<a class="ae lh" href="https://youtu.be/M0HTgnwPtmU" rel="noopener ugc nofollow" target="_blank"> YouTube </a>。</p></figure><p id="737d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">欢迎回到深度学习！今天，除了您在之前的视频中看到的策略迭代概念，我们还想讨论一些其他的强化学习方法。让我们看看今天我为你们带来了什么。我们将研究其他解决方法。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fc8a641881bf3d7967d345013d7ae8f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WKjyxKvFPj0u59le.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">政策迭代的局限性。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="5fae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以看到，在我们之前讨论的策略和值迭代中，它们需要在学习期间更新策略，以获得我们的最佳状态值函数的更好近似。这些被称为策略算法，因为你需要 n 个策略。该政策正在更新。此外，我们假设状态转换和奖励是已知的。因此，产生新状态和新奖励的概率密度函数是已知的。如果他们不是，那么你不能应用以前的概念。所以，这很重要，当然有方法可以让你放松。因此，这些方法的主要区别在于它们如何执行策略评估。那么，让我们来看几个替代方案。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/5cd2750b30ec175ce72991a70c7a33ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GDMqHStih2K9ArUa.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">蒙特卡洛技术。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>下的图片。</p></figure><p id="fded" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我想给你们看的第一个是基于蒙特卡罗技术的。这仅适用于偶发任务。在这里，这个想法是不符合政策的。因此，您可以通过遵循任意策略来了解最佳状态值。你用什么策略并不重要。所以这是一个武断的政策。可能是多份保单。当然，你仍然有探索/开发的困境。所以你要选择真正覆盖所有州的政策。您不需要关于环境动态的信息，因为您可以简单地运行许多临时任务。你试图到达所有可能的状态。如果您这样做，那么您可以使用一些策略来生成这些剧集。然后，你反向循环一集，积累预期的未来回报。因为你一直玩游戏到最后，所以你可以在这一集的时间上倒退，累积已经获得的不同奖励。如果一个州还没有被访问过，你可以把它添加到一个列表中，然后使用这个列表来计算状态值函数的更新。所以，你可以看到这只是特定状态下这些列表的总和。这将允许您更新您的状态值，这样您就可以迭代，以实现最佳的状态值函数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3131e23a4d090b93bc07c51c7e43b9a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cNHkTMLNx3hQADzv.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">时间差异学习。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="d208" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，另一个概念是时间差异学习。这是一种符合政策的方法。同样，它不需要关于环境动态的信息。这里的方案是，你循环并遵循一定的策略。然后，您使用策略中的一个动作来观察奖励和新状态。您可以使用先前的状态值函数加上α来更新您的状态值函数，α用于加权新观察的影响乘以新奖励加上新状态的旧状态值函数的折扣版本，然后减去旧状态的值。这样，你可以生成更新，这实际上会收敛到最优解。这种方法的一个变体实际上估计了动作值函数，然后被称为 SARSA。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/5de813047359628bdaddaa7b9aa3ad59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dDyQq8t1pZuznMsz.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">q 学习。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>下的图片。</p></figure><p id="417c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">q 学习是一种脱离政策的方法。这是一种时间差分类型的方法，但它不需要关于环境动态的信息。这里的想法是，你循环并遵循从你的动作值函数中导出的策略。例如，您可以使用ε-贪婪型方法。然后，你使用策略中的动作来观察你的奖励和你的新状态。接下来，您使用前一个行动值加上某个加权因子，再乘以观察到的奖励，更新您的行动值函数，再乘以贴现行动，该贴现行动将从生成的状态减去前一个状态的行动值函数，得出您已经知道的最大行动值。所以这又是一种时间差异，你在这里用它来更新你的行动值函数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/eced0dcb416ba1e086e425792ca917e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5I26GkKaLoGfveB2.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用通用函数逼近器的 q 学习。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="5b8b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好吧，如果你有通用函数逼近器，那么用权重和一些损失函数来参数化你的策略怎么样？这就是所谓的政策梯度。这种情况称为加强。所以，你用你的策略和权重生成一集。然后，你在你的剧集中从时间 0 前进到时间 t-1。如果你这样做，你实际上可以计算相对于重量的梯度。您使用这个渐变来更新您的权重。与我们之前在学习方法中看到的非常相似。你可以看到，这个在策略上使用梯度的想法，给了你一个如何更新权重的想法，也是一个学习率。我们现在离我们早期的机器学习想法已经很近了。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/272927bb97579cf890c687dc96939db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Nql1uX8HyyVoYAim.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在这个深度学习讲座中，更多令人兴奋的事情即将到来。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>下的图片。</p></figure><p id="9aa7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是为什么我们在下一个视频中谈论深度 Q 学习，这是一种强化学习的深度学习版本。所以，希望你喜欢这个视频。现在，您已经看到了如何实际确定最佳状态值和动作值函数的其他选项。这样，我们已经看到，有许多不同的想法不再需要关于如何产生未来状态和如何产生未来回报的确切知识。有了这些想法，你也可以进行强化学习，尤其是政策梯度的想法。我们已经看到，这与我们在这门课早些时候看到的机器学习和深度学习方法非常一致。我们将在下一个视频中讨论这个想法。非常感谢大家的收听，下期视频再见。拜拜。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f92f655bf4b570dbfbcfa9a4cc748c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*A023HlWSUiyAOPya.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">对于今天的强化学习方法来说，Sonic 仍然是一个挑战。使用<a class="ae lh" href="https://github.com/vvo/gifify" rel="noopener ugc nofollow" target="_blank"> gifify </a>创建的图像。来源:<a class="ae lh" href="https://youtu.be/M0HTgnwPtmU" rel="noopener ugc nofollow" target="_blank"> YouTube </a></p></figure><p id="0e5c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你喜欢这篇文章，你可以在这里找到更多的文章，或者看看我们的讲座。如果你想在未来了解更多的文章、视频和研究，我也会很感激关注<a class="ae lh" href="https://www.youtube.com/c/AndreasMaierTV" rel="noopener ugc nofollow" target="_blank"> YouTube </a>、<a class="ae lh" href="https://twitter.com/maier_ak" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae lh" href="https://www.facebook.com/andreas.maier.31337" rel="noopener ugc nofollow" target="_blank">脸书</a>或<a class="ae lh" href="https://www.linkedin.com/in/andreas-maier-a6870b1a6/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>。本文以<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/deed.de" rel="noopener ugc nofollow" target="_blank"> Creative Commons 4.0 归属许可</a>发布，如果引用，可以转载和修改。如果你有兴趣从视频讲座中获得文字记录，试试<a class="ae lh" href="http://autoblog.tf.fau.de/" rel="noopener ugc nofollow" target="_blank">自动博客</a>。</p><h1 id="e147" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">链接</h1><p id="9ba5" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><a class="ae lh" href="http://incompleteideas.net/book/bookdraft2018jan1.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>到萨顿 2018 年草案中的强化学习，包括深度 Q 学习和 Alpha Go 细节</p><h1 id="94b5" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">参考</h1><p id="7283" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">[1]大卫·西尔弗、阿贾·黄、克里斯·J·马迪森等，“用深度神经网络和树搜索掌握围棋”。载于:自然 529.7587 (2016)，第 484–489 页。<br/>【2】大卫·西尔弗、朱利安·施利特维泽、卡伦·西蒙扬等人《在没有人类知识的情况下掌握围棋游戏》。载于:自然 550.7676 (2017)，第 354 页。<br/>【3】David Silver，Thomas Hubert，Julian Schrittwieser，等《用通用强化学习算法通过自玩掌握国际象棋和松木》。载于:arXiv 预印本 arXiv:1712.01815 (2017)。<br/> [4] Volodymyr Mnih，Koray Kavukcuoglu，David Silver 等，“通过深度强化学习实现人类水平的控制”。载于:自然杂志 518.7540 (2015)，第 529-533 页。<br/>【5】马丁·穆勒。《电脑围棋》。摘自:人工智能 134.1 (2002)，第 145-179 页。<br/> [6]理查德·萨顿和安德鲁·g·巴尔托。强化学习导论。第一名。美国麻省剑桥:麻省理工学院出版社，1998 年。</p></div></div>    
</body>
</html>