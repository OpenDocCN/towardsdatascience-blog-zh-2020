<html>
<head>
<title>Bayesian Decision Theory</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯决策理论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-decision-theory-81103a68978e?source=collection_archive---------3-----------------------#2020-07-31">https://towardsdatascience.com/bayesian-decision-theory-81103a68978e?source=collection_archive---------3-----------------------#2020-07-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/8ec8fcead0c3318495c0babe8384df56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1bp-KW26z4HGJLnY_oN8NA.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">在<a class="ae kc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kc" href="https://unsplash.com/@snapsbyclark" rel="noopener ugc nofollow" target="_blank"> Clark Van </a>拍摄的照片</p></figure><blockquote class="kd"><p id="63d8" class="ke kf iq bd kg kh ki kj kk kl km kn dk translated">哦！如果不是模式，我们是什么？</p></blockquote><p id="48dc" class="pw-post-body-paragraph ko kp iq kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk kn ij bi translated">令人惊讶的是，我们如何轻松地完成如此多复杂的任务，却又渴望教机器如何去做。对我们来说，区分苹果和橘子是很简单的事情，但是把这个教给一个只懂“0”和“1”的人是很麻烦的。现在，使用一个非常熟悉的数学公式，看似艰巨的任务可以变得容易(或者至少可行)。但问题是:<strong class="kq ir"> <em class="ll">公式直观吗？</em> </strong></p><p id="eb6d" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">贝叶斯定理就像概率论的<strong class="kq ir"> <em class="ll"> E=mc </em> </strong>。每个人都看过了，但只有少数人理解它。如果我不得不为它选择一个形容词，那就是革命性的！它改变了我们以前摆弄的许多应用程序的进程。但是在深入算法的细节之前，我们需要先理清我们的基本原理。</p><p id="36e4" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">附注:为了解释这些概念，为了简单起见，但不失一般性，我举了二进制分类的例子。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h2 id="27ae" class="ly lz iq bd ma mb mc dn md me mf dp mg kz mh mi mj ld mk ml mm lh mn mo mp mq bi translated">边际概率、条件概率和联合概率</h2><blockquote class="mr ms mt"><p id="dac1" class="ko kp ll kq b kr lm kt ku kv ln kx ky mu lo lb lc mv lp lf lg mw lq lj lk kn ij bi translated">边缘概率</p></blockquote><p id="e9ff" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">当我们通常谈论事件的概率时，我们关心的是边际概率。换句话说，它是不考虑任何其他因素/事件/环境的事件发生的概率。基本上，你“边缘化”了其他事件，因此得名。用<strong class="kq ir"> <em class="ll"> P(A) </em> </strong>表示，读作“A 的概率”。</p><blockquote class="mr ms mt"><p id="c663" class="ko kp ll kq b kr lm kt ku kv ln kx ky mu lo lb lc mv lp lf lg mw lq lj lk kn ij bi translated">条件概率</p></blockquote><p id="10d4" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">条件概率是指一个事件的发生完全或部分受到其他事件的影响。换句话说，它是当一个事件 B 已经发生时，另一个事件 A 发生的概率。用<strong class="kq ir"> <em class="ll"> P(A|B) </em> </strong>表示，读作“给定 B 的概率”。</p><blockquote class="mr ms mt"><p id="7c59" class="ko kp ll kq b kr lm kt ku kv ln kx ky mu lo lb lc mv lp lf lg mw lq lj lk kn ij bi translated">联合概率</p></blockquote><p id="fa21" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">当我们对两个不同事件的同时发生感兴趣时，计算联合概率。它也经常被称为两个事件相交的概率。用<strong class="kq ir"> <em class="ll"> P(A，B) </em> </strong>表示，读作“A 和 B 的概率”。</p><h2 id="eda9" class="ly lz iq bd ma mb mc dn md me mf dp mg kz mh mi mj ld mk ml mm lh mn mo mp mq bi translated">概率和可能性</h2><p id="1df4" class="pw-post-body-paragraph ko kp iq kq b kr mx kt ku kv my kx ky kz mz lb lc ld na lf lg lh nb lj lk kn ij bi translated">可能性和概率之间有一个非常微妙的区别，也许这就是为什么人们经常认为它们相似，如果不是相同的话。</p><p id="669c" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">要了解它们之间的区别，我们首先需要了解什么是模型，更具体地说，是统计模型。</p><blockquote class="mr ms mt"><p id="ac60" class="ko kp ll kq b kr lm kt ku kv ln kx ky mu lo lb lc mv lp lf lg mw lq lj lk kn ij bi translated">模型可以被看作是理解和描述数据的过程、关系、等式或近似值中的任何一个。</p></blockquote><p id="8a88" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">考虑下图:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/786a4fbbcead3ecaf876fff8a5b88e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rKyK6lD6ufWay1wzi0wd3A.png"/></div></div></figure><p id="e3a6" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">这可能是一个模型，因为它为我们提供了关于我们的数据看起来如何的“描述”。我们可以在上图中看到特征(x 和 y)之间的<em class="ll">关系</em>，即特征之间的变化。</p><p id="780e" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">现在，如果我试图用一个数学公式<em class="ll">来近似</em>T2，我会得到:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="5965" class="ly lz iq ni b gy nm nn l no np">y = 1.3055 * x - 5.703</span></pre><p id="4c01" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">绘制时给出:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/c1a95b8956f0058234ce301adb5e96a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4jZAbqUl9zi3t3BvWhKZEg.png"/></div></div></figure><p id="a527" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">这个等式也是一个模型，因为它给出了数据的更具体的描述(更具体地说是特征之间的关系)。</p><blockquote class="mr ms mt"><p id="51a8" class="ko kp ll kq b kr lm kt ku kv ln kx ky mu lo lb lc mv lp lf lg mw lq lj lk kn ij bi translated">许多统计学致力于确定一个模型是数据的好的还是坏的近似。</p></blockquote><p id="342c" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">既然我们有了统计模型背后的直觉，我们就可以解决可能性和概率之间的差异。</p><p id="9221" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">每当我们计算随机过程中某一事件的概率时，它取决于我们用来描述我们的过程的<strong class="kq ir"> <em class="ll">模型</em> </strong> <em class="ll"> </em>的参数。也就是说，<strong class="kq ir"><em class="ll"/></strong><em class="ll"/>是过程的观察结果，<strong class="kq ir"> <em class="ll"> θ </em> </strong> <em class="ll"> </em>是描述底层模型的参数。那么我们感兴趣计算的<strong class="kq ir"> <em class="ll">概率</em> </strong>用<strong class="kq ir"> <em class="ll"> P(O|θ) </em> </strong>表示，即“给定用于描述过程的模型的参数，特定结果的概率<strong class="kq ir">”。</strong></p><p id="dad6" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">但是我们很少会事先知道<strong class="kq ir"> <em class="ll"> θ </em> </strong>的值。我们简单地观察<strong class="kq ir"> <em class="ll"> O </em> </strong>，然后目标是得出<strong class="kq ir"> <em class="ll"> θ </em> </strong>的估计，这将是给定观察结果<strong class="kq ir"> <em class="ll"> O </em> </strong>的一个合理选择。我们对参数的最佳估计是给出发生结果<strong class="kq ir"> <em class="ll">或</em> </strong>的最大概率的值。然后我们可以将<strong class="kq ir"><em class="ll"/></strong><strong class="kq ir"><em class="ll">函数</em> </strong>定义为<strong class="kq ir"> <em class="ll"> L(θ|O) </em> </strong>即它是<strong class="kq ir"> <em class="ll"> θ </em> </strong>函数对于<strong class="kq ir">给定的</strong>集合的结果，然后用它来寻找参数<strong class="kq ir"> <em class="ll">的最优值</em></strong></p><p id="7dfb" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">图形解释:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/ac084ca20a6952a77d670bacf29b672b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ifL4bv7P3xyA7ZQV5kMHLQ.png"/></div></div></figure><p id="fddd" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">考虑如上图所示的高斯分布。设<strong class="kq ir"> X </strong>为相关过程的随机变量。然后，</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="96fd" class="ly lz iq ni b gy nm nn l no np">Probability of the random variable equals x given the underlying model is Gaussian:<br/>P(X = x | <em class="ll">N</em>(μ, σ)) = 0 # For continous random variable, but can be closely approximated to the dark pink area</span><span id="f494" class="ly lz iq ni b gy nr nn l no np">Probability of the random variable to be greater than x given the underlying model is Gaussian::<br/>P(X &gt; x | <em class="ll">N</em>(μ, σ)) = (Pink + Dark Pink) area</span><span id="ad75" class="ly lz iq ni b gy nr nn l no np">Likelihood of the random variable at x:<br/>L(<em class="ll">N</em>(μ, σ) | X = x) = y</span></pre><blockquote class="mr ms mt"><p id="1df6" class="ko kp ll kq b kr lm kt ku kv ln kx ky mu lo lb lc mv lp lf lg mw lq lj lk kn ij bi translated">简单来说，可能性是模型(在这种情况下，高斯分布)的参数(在这种情况下，<strong class="kq ir"> μ </strong>和<strong class="kq ir"> σ </strong>)描述结果的程度(在这种情况下，<strong class="kq ir"> X </strong>)。具有最优参数的模型最大化了概率。另一方面，概率是对于所用模型的给定参数，事件或观察的可能性有多大。</p></blockquote><p id="9de5" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">既然我们已经学习了基本原理，我们终于可以深入到贝叶斯决策理论的工作中了。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="1698" class="ns lz iq bd ma nt nu nv md nw nx ny mg nz oa ob mj oc od oe mm of og oh mp oi bi translated">贝叶斯决策理论</h1><p id="5683" class="pw-post-body-paragraph ko kp iq kq b kr mx kt ku kv my kx ky kz mz lb lc ld na lf lg lh nb lj lk kn ij bi translated">贝叶斯决策理论是解决模式分类问题的基本统计方法。它被认为是理想的模式分类器，并经常被用作其他算法的基准，因为它的决策规则自动最小化其损失函数。现在可能没什么意义，所以等一下，我们会解开的。</p><blockquote class="mr ms mt"><p id="7cba" class="ko kp ll kq b kr lm kt ku kv ln kx ky mu lo lb lc mv lp lf lg mw lq lj lk kn ij bi translated">它假设决策问题是以概率的形式提出的，并且所有相关的概率值都是已知的。</p></blockquote><h2 id="9fc5" class="ly lz iq bd ma mb mc dn md me mf dp mg kz mh mi mj ld mk ml mm lh mn mo mp mq bi translated">贝叶斯定理</h2><p id="7187" class="pw-post-body-paragraph ko kp iq kq b kr mx kt ku kv my kx ky kz mz lb lc ld na lf lg lh nb lj lk kn ij bi translated">贝叶斯定理的推导；</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="9e14" class="ly lz iq ni b gy nm nn l no np">We know from the conditional probability:</span><span id="e006" class="ly lz iq ni b gy nr nn l no np">P(A|B) = P(A, B) / P(B)<br/>=&gt; P(A, B) = P(A|B) * P(B) ... (i)</span><span id="ad09" class="ly lz iq ni b gy nr nn l no np">Similarly,<br/>P(A, B) = P(B|A) * P(A) ... (ii)</span><span id="73ce" class="ly lz iq ni b gy nr nn l no np">From equation (i) and (ii):</span><span id="101f" class="ly lz iq ni b gy nr nn l no np">P(A|B) * P(B) = P(B|A) * P(A)<br/>=&gt; P(A|B) = [P(B|A) * P(A)] / P(B)</span></pre><p id="f664" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">对于分类的情况，假设:</p><ul class=""><li id="7dc6" class="oj ok iq kq b kr lm kv ln kz ol ld om lh on kn oo op oq or bi translated">A ≡ ω(条目的性质或类别的状态)</li><li id="1de0" class="oj ok iq kq b kr os kv ot kz ou ld ov lh ow kn oo op oq or bi translated">b≦<strong class="kq ir">x</strong>(输入特征向量)</li></ul><p id="21f0" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">代入后我们得到:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="f94e" class="ly lz iq ni b gy nm nn l no np">P(ω|<strong class="ni ir">x</strong>) = [P(<strong class="ni ir">x</strong>|ω) * P(ω)] / P(<strong class="ni ir">x</strong>)</span><span id="3983" class="ly lz iq ni b gy nr nn l no np">which becomes:</span><span id="8cab" class="ly lz iq ni b gy nr nn l no np">P(ω|<strong class="ni ir">x</strong>) = [p(<strong class="ni ir">x</strong>|ω) * P(ω)] / p(<strong class="ni ir">x</strong>)</span><span id="ea58" class="ly lz iq ni b gy nr nn l no np">because,</span><span id="bf29" class="ly lz iq ni b gy nr nn l no np">* P(ω|<strong class="ni ir">x</strong>) ≡ called the <strong class="ni ir">posterior</strong>, it is the probability of the predicted class to be ω for a given entry of feature (x). Analogous to P(O|<em class="ll">θ</em>), because the class<strong class="ni ir"> </strong>is the desired outcome to be predicted according to the data<strong class="ni ir"> </strong>distribution (model). Capital 'P' because <strong class="ni ir">ω</strong> is a discrete random variable.</span><span id="64a5" class="ly lz iq ni b gy nr nn l no np">* p(<strong class="ni ir">x</strong>|ω) ≡ class-conditional probability density function for the feature. We call it <strong class="ni ir"><em class="ll">likelihood</em></strong><em class="ll"> of </em>ω with respect to x, a term chosen to indicate that, other things being equal, the category (or class) for which it is large is more "likely" to be the true category. It is a function of parameters within the parameteric space that describes the probability of obtaining the observed data (x). Small 'P' because <strong class="ni ir">x</strong> is a continous random variable. We usually assume it to be following Gaussian Distribution.</span><span id="cbb1" class="ly lz iq ni b gy nr nn l no np">* P(ω)<strong class="ni ir"> </strong>≡ a <strong class="ni ir">priori</strong> <strong class="ni ir">probability</strong> (or simply prior) of class ω. It is usually pre-determined and depends on the external factors. It means how probable the occurence of class ω out of all the classes.</span><span id="b94e" class="ly lz iq ni b gy nr nn l no np">* p(<strong class="ni ir">x</strong>) ≡ called the <strong class="ni ir">evidence</strong>, it is merely a scaling factor that guarantees that the posterior probabilities sum to one. p(x) = sum(p(x|ω)*P(ω)) over all the classes.</span></pre><p id="05c6" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">因此，最终我们得到以下等式来构建我们的决策规则:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ox"><img src="../Images/62831ae42f50f421a62cb83083818cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mKEUxPsBtHeAoaNnCfrXQw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">贝叶斯分类公式</p></figure><h2 id="0597" class="ly lz iq bd ma mb mc dn md me mf dp mg kz mh mi mj ld mk ml mm lh mn mo mp mq bi translated">决策规则</h2><p id="822e" class="pw-post-body-paragraph ko kp iq kq b kr mx kt ku kv my kx ky kz mz lb lc ld na lf lg lh nb lj lk kn ij bi translated">上面的等式是我们决策理论的支配公式。规则如下:</p><blockquote class="kd"><p id="f0ab" class="ke kf iq bd kg kh oy oz pa pb pc kn dk translated">对于每个样本输入，它计算其后验概率，并将其分配给与后验概率的最大值对应的类别。</p></blockquote><p id="e144" class="pw-post-body-paragraph ko kp iq kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk kn ij bi translated">数学上它可以写成:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pd"><img src="../Images/f286d28f4c4e4f9243e3b3e4a1bbddaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y4xyfhJiM-wZNi3PFpue8A.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">贝叶斯决策规则</p></figure><p id="613f" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">现在你知道贝叶斯规则是什么，它是如何工作的，你可能想知道这里有什么特别的？这个算法非常出色，因为它的优雅和接近理想的结果。贝叶斯决策理论被认为是其他分类算法的基准。</p><blockquote class="kd"><p id="ecee" class="ke kf iq bd kg kh oy oz pa pb pc kn dk translated">根据贝叶斯定理，没有理论是完美的。相反，它是一项正在进行的工作，总是要接受进一步的完善和测试。</p></blockquote><p id="3ae5" class="pw-post-body-paragraph ko kp iq kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk kn ij bi translated">让我们试着理解为什么贝叶斯的分类器是最好的分类器！**</p><p id="e88a" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">**(假设贝叶斯决策理论要求的所有假设都为真)</p><h2 id="d78b" class="ly lz iq bd ma mb mc dn md me mf dp mg kz mh mi mj ld mk ml mm lh mn mo mp mq bi translated">损失函数</h2><p id="5115" class="pw-post-body-paragraph ko kp iq kq b kr mx kt ku kv my kx ky kz mz lb lc ld na lf lg lh nb lj lk kn ij bi translated">设<em class="ll"> { </em> <strong class="kq ir"> <em class="ll"> ω </em> </strong> <em class="ll"> 1、</em> <strong class="kq ir"> <em class="ll"> ω </em> </strong> <em class="ll"> 2、</em><strong class="kq ir"><em class="ll">ω</em></strong><em class="ll">3、…、</em><strong class="kq ir"><em class="ll">ω</em></strong><em class="ll">c }</em>为<strong class="kq ir"> <em class="ll"> c </em> </strong>范畴和<em class="ll"> {通过一个动作，我指的是所有可能决策中的一个特定决策，例如，<em class="ll">一个动作可以是将一个输入特征向量分配给类别(以下称为类别 3)，另一个动作可以是将一个输入分配给类别 7 </em>。</em></p><p id="9e8c" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">我们可以将损失函数(<strong class="kq ir"> λ </strong>)概括如下:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/d88ab5ab36a19c7d664453571f465371.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*QqQvaiGb6Mplpmcvq-T4lQ.png"/></div></figure><p id="915e" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">可以这样理解:</p><blockquote class="mr ms mt"><p id="f17a" class="ko kp ll kq b kr lm kt ku kv ln kx ky mu lo lb lc mv lp lf lg mw lq lj lk kn ij bi translated"><strong class="kq ir"> λ </strong>是当其真实类别为<em class="iq"> ω </em> <strong class="kq ir"> <em class="iq"> j </em> </strong>时，将特征向量分配给类别<em class="iq"> ω </em> <strong class="kq ir"> <em class="iq"> i </em> </strong>所产生的损失。换句话说，它是在采取行动<strong class="kq ir"> α </strong>将一个输入分配给类<em class="iq"> ω </em> <strong class="kq ir"> <em class="iq"> i </em> </strong>时遭受的损失，而该输入本应在类<em class="iq"> ω </em> <strong class="kq ir"> <em class="iq"> j </em> </strong>中。</p></blockquote><p id="fff7" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">例如，在二进制分类的情况下，如果我们采取行动将输入特征向量分类到类别 1 中，而它本应在类别 2 中，我们招致以下损失:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/31ecb66f2ace59e5fa2a51a4b59b1542.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/1*usXUe2R6MTFbu4Prtae-Og.png"/></div></figure><p id="5575" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">如果<strong class="kq ir"><em class="ll">I</em></strong>=<strong class="kq ir"><em class="ll">j</em></strong>，那么我们得到的损失值比备选情况下的小，因为它对应于正确的决策。</p><p id="6fd4" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">但是，如果接受这是我们为自己的损失寻求的全部功能，那就太天真了。可以看出，损失是<strong class="kq ir">有条件的</strong>，它取决于在任何特定时间所关注的类。为了更好地理解它，请回答这个问题:</p><blockquote class="mr ms mt"><p id="bdfd" class="ko kp ll kq b kr lm kt ku kv ln kx ky mu lo lb lc mv lp lf lg mw lq lj lk kn ij bi translated">损失是否与输入特征向量的分类概率无关？</p></blockquote><p id="ab57" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">如果你认为答案是肯定的，让我解释一下为什么答案是否定的！</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="2b72" class="ly lz iq ni b gy nm nn l no np">Say the posterior probability of class 2 is more than that of class 1 for a given feature vector, i.e.,</span></pre><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/c5c9f7cd671f3c823f5a194845cc2a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*cOSul94XGgjcJp8rnZuPGQ.png"/></div></figure><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="4928" class="ly lz iq ni b gy nm nn l no np">and let's consider an action of assigning x to class 1.</span><span id="6b22" class="ly lz iq ni b gy nr nn l no np">Since <strong class="ni ir">λ </strong>is the loss associated with one particular action (here, assigning x to class 1) we get its two values:</span></pre><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/95eb1e08f4a3ee2df6fbd9c90761377d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*zTJb2dyBus3CmavFMbM7PA.png"/></div></figure><blockquote class="kd"><p id="d960" class="ke kf iq bd kg kh ki kj kk kl km kn dk translated">现在这两个损失值相等吗？</p></blockquote><p id="b7b0" class="pw-post-body-paragraph ko kp iq kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk kn ij bi translated">没有后验概率的知识，你无法回答这个问题。您需要知道一个类成为真实类的可能性有多大，以便计算与之相关的损失。这将我们的损失修改如下:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/80187340dadac9feb2fe42ef8c067dcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*XKP57kGbImqgXAH3IUwlcA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">贝叶斯分类器的损失函数</p></figure><blockquote class="kd"><p id="3cf0" class="ke kf iq bd kg kh ki kj kk kl km kn dk translated">这就解释了为什么叫条件损失。</p></blockquote><p id="d294" class="pw-post-body-paragraph ko kp iq kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk kn ij bi translated"><strong class="kq ir">λ</strong><em class="ll">T3】可以是任何合适的函数。例如，它可以是<em class="ll">对称</em>或<em class="ll">零一</em>函数。</em></p><blockquote class="mr ms mt"><p id="fbae" class="ko kp ll kq b kr lm kt ku kv ln kx ky mu lo lb lc mv lp lf lg mw lq lj lk kn ij bi translated">决策的预期损失:</p></blockquote><p id="03eb" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">现在，我们可以在所有类别中定义特定决策(或行动)的预期损失。也就是说，我在采取特定行动后会招致的损失，以及它将如何受到所有类别而不仅仅是真实类别的“存在”的影响。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/03e56432ff95cb31c6dae9b19b59ba5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*dbMDnz0CLZbQf5ZFpQOuEg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">条件风险</p></figure><p id="fbc5" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">我们可以通过选择最小化条件风险的行动来最小化我们的预期损失。我们现在将展示这个<strong class="kq ir">贝叶斯决策过程实际上提供了最佳性能。</strong></p><p id="66e0" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">我们的问题是找到一个最小化整体风险的决策规则。一个通用的决策规则是一个函数<strong class="kq ir"> α(x) </strong>，它告诉我们对每一个可能的观察值(<strong class="kq ir"> x </strong>)采取哪一个动作。更具体地说，对于每一个<strong class="kq ir"> x </strong>决策函数<strong class="kq ir"> α(x) </strong>假设其中一个<strong class="kq ir"> <em class="ll"> a </em> </strong>值<strong class="kq ir"> <em class="ll"> α </em> </strong> <em class="ll"> 1、</em><strong class="kq ir"><em class="ll">α</em></strong><em class="ll">2、</em><strong class="kq ir"><em class="ll">α</em><em class="ll">3、…，</em> <strong class="kq ir">因为条件风险与动作<strong class="kq ir"> <em class="ll"> α </em> </strong> <em class="ll"> i </em>相关联，并且因为决策规则指定了动作，所以总体风险由下式给出:</strong></strong></p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/caa9a7939562a2b84cef65e58f8cc673.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*gcd0V3WMscg7tom9qMGGgA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">所有观察的行动的总体风险</p></figure><p id="41a4" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">如果选择<strong class="kq ir"> α(x) </strong>使得每个行动的条件风险最小化，那么整体风险也将最小化。这证明了贝叶斯决策规则。</p><p id="a474" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">也许举个例子会更好…</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="9984" class="ly lz iq ni b gy nm nn l no np">Let <strong class="ni ir">λ </strong>be symmetrical or zero-one loss function,</span></pre><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/e29c98d3afcd69b4fbe4acf7c5896393.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*wxzvaHAmsB1aLN-gOMFlfg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">零一损失</p></figure><p id="eb9c" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">那么条件风险就变成了:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/fdfc1f13e8abe612fdb3fdc1cec2b3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*i2mUWLm38kizPux7V13j1Q.png"/></div></figure><blockquote class="kd"><p id="c212" class="ke kf iq bd kg kh ki kj kk kl km kn dk translated">贝叶斯分类器的决策规则自动最小化与动作相关的条件风险。</p></blockquote><p id="d3a2" class="pw-post-body-paragraph ko kp iq kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk kn ij bi translated">这就是贝叶斯的决策理论！</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="17fd" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">我希望这篇文章能帮助你理解这个算法。请随意在下面留下您的建议和疑问。如果你喜欢，就砸了那个按钮！</p><p id="d9c3" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">如果你心中有任何主题，并希望在这里看到它，请在评论区留下它。这将是我的荣幸！</p><p id="c64a" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">一路平安！</p><p id="e607" class="pw-post-body-paragraph ko kp iq kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk kn ij bi translated">(除缩略图外，所有图片均由作者创作。)</p></div></div>    
</body>
</html>