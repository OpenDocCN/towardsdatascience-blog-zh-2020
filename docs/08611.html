<html>
<head>
<title>Anamoly Detection: Techniques to detect outliers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">异常检测:检测异常值的技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/anamoly-detection-techniques-to-detect-outliers-fea92047a222?source=collection_archive---------31-----------------------#2020-06-22">https://towardsdatascience.com/anamoly-detection-techniques-to-detect-outliers-fea92047a222?source=collection_archive---------31-----------------------#2020-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1861deb09d0b429f74049f2eb9080b4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pH6MFzWwNFDUOuq3"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://unsplash.com/@alisvisuals?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿里·哈坚</a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="8bad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">异常检测</strong>是对罕见项目、事件或观察结果的识别，这些项目、事件或观察结果通过与大多数数据显著不同而引起怀疑。通常，异常项目会转化为某种问题，如信用卡欺诈、网络入侵、医疗诊断、系统健康监控。</p><p id="78ed" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">异常检测基于两个基本前提</p><ul class=""><li id="83ca" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">数据中很少出现异常。</li><li id="ee38" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">他们的特征明显不同于正常情况。</li></ul><h1 id="9d7b" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">异常检测技术</h1><h1 id="500b" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">四分位数间距(IQR)</h1><p id="134d" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">识别数据中不规则性的最简单方法是标记偏离分布的常见统计属性的数据点，包括平均值、中值、众数和四分位数。</p><p id="686f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最流行的方法之一是<strong class="ki iu">四分位数间距(IQR)。</strong> IQR 是统计学中的一个概念，通过将数据集分成四分位数来衡量统计离差和数据可变性。</p><p id="4718" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简而言之，任何数据集或任何一组观察值都根据数据的值以及它们与整个数据集的比较情况被划分为四个定义的区间。四分位数将数据分为三个点和四个区间。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/4cd0bebb0ba998e092a6743c18cc4827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qShukKO06ey0Fs58.png"/></div></div></figure><p id="24ec" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图片来源:维基百科</p><p id="c7a9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">四分位距(IQR)很重要，因为它用于定义异常值。它是第三个四分位数和第一个四分位数的差值(IQR = Q3 -Q1)。在这种情况下，异常值被定义为低于(Q1 1.5 倍 IQR)或高于(Q3+1.5 倍 IQR)的观测值</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi na"><img src="../Images/2cc2b6c6e01e2c2721160d9151bde269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Amq1f59OybLuuyJ2.png"/></div></div></figure><p id="2db5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图片来源:维基百科</p><h2 id="3cf5" class="nb lt it bd lu nc nd dn ly ne nf dp mc kr ng nh mg kv ni nj mk kz nk nl mo nm bi translated">履行</h2><p id="0f43" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated"><code class="fe nn no np nq b">np.percentile</code>是 Python 中的烘焙功能</p><p id="099a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nn no np nq b">q75, q25 = np.percentile(x, [75 ,25])<br/>iqr = q75 - q25</code></p><h2 id="c123" class="nb lt it bd lu nc nd dn ly ne nf dp mc kr ng nh mg kv ni nj mk kz nk nl mo nm bi translated">缺点</h2><p id="1da9" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">IQR 技术在以下情况下不起作用</p><ol class=""><li id="14c7" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld nr lk ll lm bi translated">该模式基于季节性。这涉及到更复杂的方法，例如将数据分解成多个趋势，以确定季节性的变化。</li></ol><p id="2c9e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.随着恶意对手不断调整自己，异常或正常的定义可能会频繁改变</p><h1 id="6c2f" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">基于聚类的异常检测</h1><p id="48b7" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">聚类是无监督学习领域中最流行的概念之一。潜在的前提是相似的数据点倾向于属于相似的组或聚类，这是由它们与局部质心的距离决定的。</p><p id="8207" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">K-means 是一种广泛使用的聚类算法。它创建了“k”个相似的数据点聚类。不属于这些组的数据实例可能会被标记为异常。其他聚类算法，如层次聚类和数据库扫描，也可以用来检测异常值。</p><p id="08e6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">K-means 算法的工作方式如下:</p><ol class=""><li id="3179" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld nr lk ll lm bi translated">指定簇的数量<em class="ns"> K </em>。</li><li id="ab9b" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nr lk ll lm bi translated">通过首先改组数据集，然后为质心随机选择<em class="ns"> K </em>个数据点来初始化质心，而无需替换。</li><li id="b45c" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nr lk ll lm bi translated">计算质心和数据点之间的距离。</li><li id="9449" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nr lk ll lm bi translated">继续迭代，直到质心没有变化。也就是说，数据点到聚类的分配没有改变。</li></ol><h2 id="faf8" class="nb lt it bd lu nc nd dn ly ne nf dp mc kr ng nh mg kv ni nj mk kz nk nl mo nm bi translated">履行</h2><ol class=""><li id="0fd6" class="le lf it ki b kj mq kn mr kr nt kv nu kz nv ld nr lk ll lm bi translated"><strong class="ki iu">初始化随机质心</strong></li></ol><p id="2ba9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你从三个(我们决定 K 为 3)随机点(以(x，y)的形式)开始这个过程。这些点被称为<strong class="ki iu">质心</strong>，这只是一个用来表示<em class="ns">中心</em>的花哨名称。我们先把这三个点命名为<strong class="ki iu"> C1、</strong>和<strong class="ki iu"> C3 </strong>，这样你以后就可以参考了。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/998aeddc1b41afed67f21efbc9005549.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/0*7vnKR4Ss3JnfXSf2.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd nx">K-Means 中的步骤 1:随机质心</strong></p></figure><p id="1326" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 2。计算质心和数据点之间的距离</strong></p><p id="dfb1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，测量数据点与这三个随机选择的点之间的距离。一个非常流行的距离测量函数的选择，在本例中，是<a class="ae kf" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank"><strong class="ki iu"/></a><strong class="ki iu">。</strong></p><p id="6f19" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简而言之，如果 2D 空间上有 n 个点(如上图所示)，并且它们的坐标由(x_i，y_i)表示，那么该空间上任意两点(<strong class="ki iu"> (x1，y1) </strong>和<strong class="ki iu"> (x2，y2) </strong>)之间的欧几里德距离由下式给出:</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/a876b8f647709dc7f230cf02f856c9a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/0*upUwqPLX7TGqYCIF.png"/></div></figure><p id="39de" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设 C1、C2、C3 的坐标分别为— <strong class="ki iu"> (-1，4) </strong>、<strong class="ki iu"> (-0.2，1.5)</strong>、<strong class="ki iu"> (2，2.5) </strong>。现在让我们写几行 Python 代码，它将计算数据点和这些随机选择的质心之间的欧几里德距离。我们从初始化质心开始。</p><pre class="mw mx my mz gt nz nq oa ob aw oc bi"><span id="9abe" class="nb lt it nq b gy od oe l of og"># Initialize the centroids<br/>c1 = (-1, 4)<br/>c2 = (-0.2, 1.5)<br/>c3 = (2, 2.5)</span></pre><p id="8b16" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们编写一个小的辅助函数来计算数据点和质心之间的欧几里德距离。</p><pre class="mw mx my mz gt nz nq oa ob aw oc bi"><span id="f6bd" class="nb lt it nq b gy od oe l of og"># A helper function to calculate the Euclidean distance between the data points and the centroids</span><span id="df4f" class="nb lt it nq b gy oh oe l of og">def calculate_distance(centroid, X, Y):<br/>    distances = []<br/>        <br/>    # Unpack the x and y coordinates of the centroid<br/>    c_x, c_y = centroid<br/>        <br/>    # Iterate over the data points and calculate the distance using the           # given formula<br/>    for x, y in list(zip(X, Y)):<br/>        root_diff_x = (x - c_x) ** 2<br/>        root_diff_y = (y - c_y) ** 2<br/>        distance = np.sqrt(root_diff_x + root_diff_y)<br/>        distances.append(distance)<br/>        <br/>    return distances</span></pre><p id="1952" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在可以将该函数应用于数据点，并相应地分配结果。</p><pre class="mw mx my mz gt nz nq oa ob aw oc bi"><span id="2404" class="nb lt it nq b gy od oe l of og"># Calculate the distance and assign them to the DataFrame accordingly<br/>data['C1_Distance'] = calculate_distance(c1, data.X_value, data.Y_value)<br/>data['C2_Distance'] = calculate_distance(c2, data.X_value, data.Y_value)<br/>data['C3_Distance'] = calculate_distance(c3, data.X_value, data.Y_value)</span><span id="edd7" class="nb lt it nq b gy oh oe l of og"># Preview the data<br/>print(data.head())</span></pre><p id="e3a1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 3。比较、分配、平均和重复</strong></p><p id="8e31" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这基本上是 K-Means 聚类算法的最后一步。一旦你有了数据点和质心之间的距离，你就可以比较这些距离并取最小的一个。特定数据点到质心的距离最小，该质心被指定为该特定数据点的聚类。</p><p id="fd90" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们以编程的方式来做这件事。</p><pre class="mw mx my mz gt nz nq oa ob aw oc bi"><span id="6f88" class="nb lt it nq b gy od oe l of og"># Get the minimum distance centroids<br/>    data['Cluster'] = data[['C1_Distance', 'C2_Distance', 'C3_Distance']].apply(np.argmin, axis =1)<br/>    <br/># Map the centroids accordingly and rename them<br/>    data['Cluster'] = data['Cluster'].map({'C1_Distance': 'C1', 'C2_Distance': 'C2', 'C3_Distance': 'C3'})<br/>    <br/># Get a preview of the data<br/>    print(data.head(10))</span></pre><p id="7f74" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在最有趣的部分来了，<em class="ns">通过确定数据点坐标的<strong class="ki iu">平均值</strong>来更新质心</em>(这些数据点现在应该属于某个质心)。因此得名<strong class="ki iu">K-意为</strong>。平均值计算看起来是这样的:</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/946263f340e795385c0c28131badb27b.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/0*Vic_TmvsrIfX7w4Q.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd nx">K-Means 中的均值更新(n 表示属于一个聚类的数据点的数量)</strong></p></figure><p id="d7b1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面几行代码可以帮您做到这一点:</p><pre class="mw mx my mz gt nz nq oa ob aw oc bi"><span id="b4c8" class="nb lt it nq b gy od oe l of og"># Calculate the coordinates of the new centroid from cluster 1<br/>x_new_centroid1 = data[data['Cluster']=='C1']['X_value'].mean()<br/>y_new_centroid1 = data[data['Cluster']=='C1']['Y_value'].mean()</span><span id="cdc5" class="nb lt it nq b gy oh oe l of og"># Calculate the coordinates of the new centroid from cluster 2<br/>x_new_centroid2 = data[data['Cluster']=='C3']['X_value'].mean()<br/>y_new_centroid2 = data[data['Cluster']=='C3']['Y_value'].mean()</span><span id="9e84" class="nb lt it nq b gy oh oe l of og"># Print the coordinates of the new centroids<br/>print('Centroid 1 ({}, {})'.format(x_new_centroid1, y_new_centroid1))<br/>print('Centroid 2 ({}, {})'.format(x_new_centroid2, y_new_centroid2))</span></pre><p id="cf21" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">重复这个过程，直到质心的坐标不再更新。</p><h2 id="268b" class="nb lt it bd lu nc nd dn ly ne nf dp mc kr ng nh mg kv ni nj mk kz nk nl mo nm bi translated">缺点</h2><p id="629a" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">K-means 算法是一种流行的算法，被广泛应用于图像压缩、文档分类等领域。K-mean 的目标是将数据点分组到不同的非重叠子组中。当集群具有一种球形形状时，它做得非常好。然而，当团簇的几何形状偏离球形时，它会受到影响。此外，它也不会从数据中学习聚类数，而是需要预先定义。</p><h1 id="1b9c" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">隔离森林</h1><p id="ce88" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">隔离森林是一种无监督学习算法，属于集成决策树家族。这种方法不同于所有以前的方法。所有以前的方法都是试图找到数据的正常区域，然后将这个定义区域之外的任何东西识别为异常值或异常值。这种方法的工作原理不同。它通过给每个数据点分配一个分数来明确隔离异常，而不是描绘和构建正常点和区域。它利用了这样一个事实，即异常是少数数据点，并且它们具有与正常情况下非常不同的属性值。</p><p id="45ae" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">隔离森林通过随机选择一个特征，然后随机选择所选特征的最大值和最小值之间的分割值来“隔离”观察值。</p><p id="2183" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于递归分割可以用树结构表示，分离样本所需的分裂次数等于从根节点到终止节点的路径长度。</p><p id="e3c4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种路径长度在这种随机树的森林中平均，是常态的度量和我们的决策函数。</p><p id="0961" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机分区会为异常产生明显更短的路径。因此，当随机树的森林共同产生特定样本的较短路径长度时，它们极有可能是异常。</p><p id="78cc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种算法在处理非常高维的数据集时非常有效，并且被证明是一种非常有效的异常检测方法。</p><p id="9513" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇<a class="ae kf" href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>涵盖了隔离林如何工作的全部细节。</p><h2 id="34cc" class="nb lt it bd lu nc nd dn ly ne nf dp mc kr ng nh mg kv ni nj mk kz nk nl mo nm bi translated">履行</h2><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3f50b88997a9630ded4365c21b940718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*GP6YciONq7gFcnfVseY-YQ.png"/></div></figure><pre class="mw mx my mz gt nz nq oa ob aw oc bi"><span id="d9a2" class="nb lt it nq b gy od oe l of og"><strong class="nq iu">import</strong> <strong class="nq iu">numpy</strong> <strong class="nq iu">as</strong> <strong class="nq iu">np</strong><br/><strong class="nq iu">import</strong> <strong class="nq iu">matplotlib.pyplot</strong> <strong class="nq iu">as</strong> <strong class="nq iu">plt</strong><br/><strong class="nq iu">from</strong> <strong class="nq iu">sklearn.ensemble</strong> <strong class="nq iu">import</strong> <a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest" rel="noopener ugc nofollow" target="_blank">IsolationForest</a></span><span id="b62f" class="nb lt it nq b gy oh oe l of og">rng = np.random.RandomState(42)</span><span id="51cf" class="nb lt it nq b gy oh oe l of og"><em class="ns"># Generate train data</em><br/>X = 0.3 * rng.randn(100, 2)<br/>X_train = <a class="ae kf" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.r_.html#numpy.r_" rel="noopener ugc nofollow" target="_blank">np.r_</a>[X + 2, X - 2]<br/><em class="ns"># Generate some regular novel observations</em><br/>X = 0.3 * rng.randn(20, 2)<br/>X_test = <a class="ae kf" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.r_.html#numpy.r_" rel="noopener ugc nofollow" target="_blank">np.r_</a>[X + 2, X - 2]<br/><em class="ns"># Generate some abnormal novel observations</em><br/>X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))</span><span id="4739" class="nb lt it nq b gy oh oe l of og"><em class="ns"># fit the model</em><br/>clf = <a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest" rel="noopener ugc nofollow" target="_blank">IsolationForest</a>(max_samples=100, random_state=rng)<br/>clf.fit(X_train)<br/>y_pred_train = clf.predict(X_train)<br/>y_pred_test = clf.predict(X_test)<br/>y_pred_outliers = clf.predict(X_outliers)</span><span id="5f96" class="nb lt it nq b gy oh oe l of og"><em class="ns"># plot the line, the samples, and the nearest vectors to the plane</em><br/>xx, yy = <a class="ae kf" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html#numpy.meshgrid" rel="noopener ugc nofollow" target="_blank">np.meshgrid</a>(<a class="ae kf" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html#numpy.linspace" rel="noopener ugc nofollow" target="_blank">np.linspace</a>(-5, 5, 50), <a class="ae kf" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html#numpy.linspace" rel="noopener ugc nofollow" target="_blank">np.linspace</a>(-5, 5, 50))<br/>Z = clf.decision_function(<a class="ae kf" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html#numpy.c_" rel="noopener ugc nofollow" target="_blank">np.c_</a>[xx.ravel(), yy.ravel()])<br/>Z = Z.reshape(xx.shape)</span><span id="2c38" class="nb lt it nq b gy oh oe l of og"><a class="ae kf" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.title.html#matplotlib.pyplot.title" rel="noopener ugc nofollow" target="_blank">plt.title</a>("IsolationForest")<br/><a class="ae kf" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contourf.html#matplotlib.pyplot.contourf" rel="noopener ugc nofollow" target="_blank">plt.contourf</a>(xx, yy, Z, cmap=plt.cm.Blues_r)</span><span id="d9b0" class="nb lt it nq b gy oh oe l of og">b1 = <a class="ae kf" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html#matplotlib.pyplot.scatter" rel="noopener ugc nofollow" target="_blank">plt.scatter</a>(X_train[:, 0], X_train[:, 1], c='white',<br/>                 s=20, edgecolor='k')<br/>b2 = <a class="ae kf" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html#matplotlib.pyplot.scatter" rel="noopener ugc nofollow" target="_blank">plt.scatter</a>(X_test[:, 0], X_test[:, 1], c='green',<br/>                 s=20, edgecolor='k')<br/>c = <a class="ae kf" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html#matplotlib.pyplot.scatter" rel="noopener ugc nofollow" target="_blank">plt.scatter</a>(X_outliers[:, 0], X_outliers[:, 1], c='red',<br/>                s=20, edgecolor='k')<br/><a class="ae kf" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.axis.html#matplotlib.pyplot.axis" rel="noopener ugc nofollow" target="_blank">plt.axis</a>('tight')<br/><a class="ae kf" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.xlim.html#matplotlib.pyplot.xlim" rel="noopener ugc nofollow" target="_blank">plt.xlim</a>((-5, 5))<br/><a class="ae kf" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.ylim.html#matplotlib.pyplot.ylim" rel="noopener ugc nofollow" target="_blank">plt.ylim</a>((-5, 5))<br/><a class="ae kf" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html#matplotlib.pyplot.legend" rel="noopener ugc nofollow" target="_blank">plt.legend</a>([b1, b2, c],<br/>           ["training observations",<br/>            "new regular observations", "new abnormal observations"],<br/>           loc="upper left")<br/><a class="ae kf" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" rel="noopener ugc nofollow" target="_blank">plt.show</a>()</span></pre><p id="588a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">资料来源:scikit-learn.org</p><h1 id="f393" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">结论</h1><p id="1f8f" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">这篇文章概述了检测数据异常的不同技术。它的范围从使用简单的统计方法，如标准差，到无监督的学习算法，如隔离森林。每种方法都有其优点和缺点。例如，四分位间距(IQR)不适用于季节模式，K 均值聚类擅长将数据分组到不同的非重叠子组中。当集群具有一种球形形状时，它做得非常好。隔离森林提供了一种反向方法来检测异常。它利用了这样一个事实，即异常是少数数据点，并且它们具有与正常情况下非常不同的属性值。作为一个好的实践者，了解算法/方法背后的假设是很好的，这样你就会对每种方法的优缺点有一个很好的想法。这将有助于您决定何时以及在何种情况下使用每种方法。</p></div></div>    
</body>
</html>