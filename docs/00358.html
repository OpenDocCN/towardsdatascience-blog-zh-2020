<html>
<head>
<title>Machine Learning Resampling Techniques for Class Imbalances</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">针对类别不平衡的机器学习重采样技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-resampling-techniques-for-class-imbalances-30cbe2415867?source=collection_archive---------11-----------------------#2020-01-11">https://towardsdatascience.com/machine-learning-resampling-techniques-for-class-imbalances-30cbe2415867?source=collection_archive---------11-----------------------#2020-01-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6660" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何为一个更好的预测模型期待意想不到的事情</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6a8c2eee006be3299996f0fe010052ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AwYqZCQ-qLE3rC5-3alWEQ.jpeg"/></div></div></figure><p id="abb1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们面对现实吧。不平等糟透了。我甚至还没有谈到这样一个事实:尽管非洲裔美国人和西班牙裔美国人占美国总人口的 32%，但他们却占美国监狱人口的 56%；也没有提到 T2 比尔·盖茨、杰夫·贝索斯和沃伦·巴菲特的财富总和比美国最底层的 50%的人的财富总和还要多。</p><p id="6855" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">尽管我很想对这些事实感到愤怒，但现在我谈论的是机器学习分类模型背景下的阶级不平衡。根据维基百科的说法，<a class="ae ln" href="https://en.wikipedia.org/wiki/Algorithmic_bias" rel="noopener ugc nofollow" target="_blank">算法偏差</a>“可能由于许多因素而出现，包括但不限于算法的设计，或者与数据编码、收集、选择或用于训练算法的方式有关的无意或意外使用或决策。”使用高度不平衡的数据训练模型可能会偏向多数类，这可能会产生严重的影响。</p><p id="70ad" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">以 Kaggle 的宫颈癌风险分类数据集为例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/3020e8404ab6b932c658c33ba52afbe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*YhJmCVIR78v1xd4VDMZK9g.png"/></div></figure><p id="2fe8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在数据集中包含的所有诊断中，只有 2%被发现是癌症。在执行简单的随机森林分类模型后，该模型的准确率为 99%！</p><pre class="kg kh ki kj gt lq lr ls lt aw lu bi"><span id="b922" class="lv lw iq lr b gy lx ly l lz ma">from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.metrics import accuracy_score, recall_score, classification_report<br/>from sklearn.model_selection import train_test_split</span><span id="c447" class="lv lw iq lr b gy mb ly l lz ma">X = df.drop('Cancer', axis=1).dropna()<br/>y = df['Cancer'].dropna()</span><span id="6a23" class="lv lw iq lr b gy mb ly l lz ma">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)</span><span id="f117" class="lv lw iq lr b gy mb ly l lz ma">random_forest = RandomForestClassifier(n_estimators=500).fit(X_train, y_train)<br/>y_pred = random_forest.predict(X_test)<br/>print('Accuracy score: ' + str(accuracy_score(y_test, y_pred)))<br/>print('Recall score: ' + str(recall_score(y_test, y_pred)))<br/>print(classification_report(y_test, y_pred))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/832879231ab282f5c37138c10c7a05e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*Pr2RexTxTDulm_fBafQnPw.png"/></div></figure><p id="c6cd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">那不太可能。让我们再深入一点。让我们在模型中只包括 36 列中的两列。</p><pre class="kg kh ki kj gt lq lr ls lt aw lu bi"><span id="052a" class="lv lw iq lr b gy lx ly l lz ma">X = df.loc[:,['Smokes', 'Hormonal Contraceptives']]<br/>y = df['Cancer'].dropna()</span><span id="6dcc" class="lv lw iq lr b gy mb ly l lz ma">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)<br/>random_forest = RandomForestClassifier(n_estimators=500).fit(X_train, y_train)<br/>y_pred = random_forest.predict(X_test)<br/>print('Accuracy score: ' + str(accuracy_score(y_test, y_pred)))<br/>print('Recall score: ' + str(recall_score(y_test, y_pred)))<br/>print(classification_report(y_test, y_pred))</span></pre><p id="49e5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是准确率只降到了 97%！我们随机选择的两个特征能够以 97%的准确率预测癌症的可能性非常小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi md"><img src="../Images/1bf04d3f3210139c2f54089d19f8545b.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*Spu-tU7lV4_o46yTDXQ20Q.png"/></div></figure><p id="80d3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们已经知道 2%的患者被诊断患有癌症，但是我们的第二个模型预测没有患者会患有癌症。看到这怎么会成为一个问题了吗？</p><h1 id="e2fe" class="me lw iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">你应该用什么标准来评估？</h1><p id="e8d5" class="pw-post-body-paragraph kr ks iq kt b ku mv jr kw kx mw ju kz la mx lc ld le my lg lh li mz lk ll lm ij bi translated">在严肃的医疗诊断模型、惩罚性模型如<a class="ae ln" href="https://advances.sciencemag.org/content/4/1/eaao5580.full" rel="noopener ugc nofollow" target="_blank"> COMPAS </a>、累犯风险模型和欺诈检测中，都存在悬而未决的人命问题。在这些情况下，最好谨慎行事，保护尽可能多的人——无论这是否意味着降低误报或漏报的数量。</p><p id="9b47" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当考虑如何优化这些类型的模型时，我们可以解读一些指标。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/1d2aa9df3cb1f1232963136004fb91fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*SfgtkiMKpBOr4EizPIvD6Q.png"/></div></figure><p id="174a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">回想一下:</strong>以宫颈癌风险数据集为例，您可以问自己这个模型— <em class="lo">在所有实际被诊断患有癌症的患者中，我们的模型预测患有癌症的比例是多少？</em></p><p id="8af5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，回忆并不能让你了解全貌。假设你的模型将每个人都归类为患有癌症，包括那些没有患癌症的人，那么你的得分就是 100%。低召回分数表示高数量的假阴性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/6157f8f9a2a833f973edb5f7b0c95e2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*_O-jf2RyoLhEYEf8qmt9Iw.png"/></div></figure><p id="ad16" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> Precision: </strong> Precision 提出了相反的问题——<em class="lo">在所有被模型预测为患有癌症的患者中，有多少人确实患有癌症？</em></p><p id="676b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你的模型预测 10 名患者患有癌症，如果所有的预测都是正确的，我们的得分将是 100%，即使还有 1000 多名患者没有得到诊断。低精度分数表示高数量的假阳性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/29cde1acb27e8c45ce1055ff7101ba8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*DuQg4RGbrQH_kYX3De-frg.png"/></div></figure><p id="4451" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">准确性:</strong>准确性是一个更可靠的评估指标，因为它衡量真实预测的总数，包括正面和负面预测。这是分类任务中最常见的指标。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/a9d5287c1edfc8ab62190516e6bd2db7.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*LABFc5tzNN_ylAwbBBTVIg.png"/></div></figure><p id="bfa1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> F1 得分:</strong>F1 得分是另一个非常有用的指标。因为它衡量的是“精确度和召回率的调和平均值”，如果精确度和召回率都不高，它就不可能高，这表明模型整体表现良好。但是，F1 分数可以格式化，以解决二进制、多类和不平衡分类问题，在<a class="ae ln" href="http://sklearn.metrics" rel="noopener ugc nofollow" target="_blank">sk learn . metrics</a>. F1 _ score 方法中使用以下参数:</p><ul class=""><li id="a69e" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nj nk nl nm bi translated"><strong class="kt ir">二进制— </strong>用于二进制分类问题。</li><li id="f325" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated"><strong class="kt ir">微— </strong>统计总的真阳性、假阴性和假阳性。</li><li id="2725" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated"><strong class="kt ir">宏— </strong>计算所有类别的未加权平均值(对于多类别问题。)</li><li id="bbb6" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated"><strong class="kt ir">加权— </strong>通过对每个类别的真阳性进行加权并取平均分数来说明类别不平衡。</li><li id="c319" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated"><strong class="kt ir">样本— </strong>查找每个类的每个指标的平均分。</li></ul><h1 id="2af7" class="me lw iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">让我们对抗不平等，一次一个重采样方法！</h1><p id="23ea" class="pw-post-body-paragraph kr ks iq kt b ku mv jr kw kx mw ju kz la mx lc ld le my lg lh li mz lk ll lm ij bi translated">有了这些信息，我们将优化二进制 F1 分数，因为我们只有两个类。你可能会想“如果我们的数据如此严重不平衡，为什么不使用加权 F1 分数？”这就是重采样方法的用武之地！</p><p id="0fde" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将探讨三种简单而有用的方法(尽管还有更多)——对多数采样不足、对少数采样过采样和 SMOTE(合成少数采样过采样技术)。我们将使用的每种方法都旨在创建一个对半分布的训练集，因为我们正在处理一个二元分类问题。这些方法可用于为四个类别的多类别问题创建 25–25–25–25 分布，而不考虑类别的初始分布，或其他可能训练您的模型以获得更好结果的比率。</p><p id="c0dc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="lo">在重新采样之前，请确保将您的数据分成训练集和测试集！</em> </strong>如果你不这样做，你会因为数据泄露而损害模型的质量，导致过度拟合和泛化能力差。</p><pre class="kg kh ki kj gt lq lr ls lt aw lu bi"><span id="971a" class="lv lw iq lr b gy lx ly l lz ma"><em class="lo"># Import the resampling package</em><br/>from sklearn.utils import resample</span><span id="a5ca" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Split into training and test sets</em><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)</span><span id="559e" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Returning to one dataframe</em><br/>training_set = pd.concat([X_train, y_train], axis=1)</span><span id="2afa" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Separating classes</em><br/>cancer = training_set[training_set.Cancer == 1]<br/>not_cancer = training_set[training_set.Cancer == 0]</span></pre></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="e42c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">欠采样多数</strong></p><p id="692d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">欠采样可以定义为减少多数类的数量。这种技术最适用于有数千甚至数百万个数据点的数据。通常，您不希望减少正在处理的数据量，但是如果您可以牺牲一些训练数据，这种技术将非常有用。下面是它在宫颈癌数据集上的工作原理:</p><pre class="kg kh ki kj gt lq lr ls lt aw lu bi"><span id="e746" class="lv lw iq lr b gy lx ly l lz ma"><em class="lo"># Undersampling the majority</em><br/>undersample = resample(not_cancer, <br/>                       replace=True, <br/>                       n_samples=len(cancer), #set the number of samples to equal the number of the minority class<br/>                       random_state=42)</span><span id="dccb" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Returning to new training set</em><br/>undersample_train = pd.concat([cancer, undersample])</span><span id="c632" class="lv lw iq lr b gy mb ly l lz ma">undersample_train.Cancer.value_counts(normalize=True)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/27f545a015b5e396d00dbd5d0a66f397.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*EK_xPUTt0jZLFmuSQydozA.png"/></div></div></figure><p id="2aaf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们有均匀分布的班级！现在我们可以在随机森林分类器上测试它。</p><pre class="kg kh ki kj gt lq lr ls lt aw lu bi"><span id="2c74" class="lv lw iq lr b gy lx ly l lz ma"><em class="lo"># Separate undersampled data into X and y sets</em><br/>undersample_x_train = undersample_train.drop('Cancer', axis=1)<br/>undersample_y_train = undersample_train.Cancer</span><span id="ce30" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Fit model on undersampled data</em><br/>undersample_rf = RandomForestClassifier(n_estimators=500).fit(undersample_x_train, undersample_y_train)</span><span id="c680" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Make predictions on test sets</em><br/>y_pred = random_forest.predict(X_test)</span><span id="1bef" class="lv lw iq lr b gy mb ly l lz ma">print('Accuracy score: ' + str(accuracy_score(y_test, y_pred)))<br/>print('Average Recall score: ' + str(recall_score(y_test, y_pred, average='macro')))<br/>print(classification_report(y_test, y_pred))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/20d6917c015b9eefa6c829436f203941.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*51J0nUSR6av-sDyySPICRQ.png"/></div></div></figure><p id="1b59" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">不完全是最好的结果，但因为我们减少了多数类的数量，我们只在 28 个实例上训练我们的模型，这对于样本大小来说太小了。接下来，我们将尝试对少数进行过采样。</p><p id="a5e3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">过采样少数</strong></p><p id="c265" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对少数民族进行过采样将增加少数民族类中的数据点数量，同样旨在均匀分布训练集中的类。我们将重复和以前一样的过程。</p><pre class="kg kh ki kj gt lq lr ls lt aw lu bi"><span id="36af" class="lv lw iq lr b gy lx ly l lz ma"><em class="lo"># Oversampling the minority</em><br/>oversample = resample(cancer, <br/>                       replace=True, <br/>                       n_samples=len(not_cancer), #set the number of samples to equal the number of the majority class<br/>                       random_state=42)</span><span id="69fd" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Returning to new training set</em><br/>oversample_train = pd.concat([not_cancer, oversample])</span><span id="8bae" class="lv lw iq lr b gy mb ly l lz ma">oversample_train.Cancer.value_counts(normalize=True)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/27f545a015b5e396d00dbd5d0a66f397.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*EK_xPUTt0jZLFmuSQydozA.png"/></div></div></figure><pre class="kg kh ki kj gt lq lr ls lt aw lu bi"><span id="9281" class="lv lw iq lr b gy lx ly l lz ma"><em class="lo"># Separate oversampled data into X and y sets</em><br/>oversample_x_train = oversample_train.drop('Cancer', axis=1)<br/>oversample_y_train = oversample_train.Cancer</span><span id="6dfb" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Fit model on oversampled data</em><br/>oversample_rf = RandomForestClassifier(n_estimators=500).fit(oversample_x_train, oversample_y_train)</span><span id="fdc4" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Make predictions on test sets</em><br/>y_pred = oversample_rf.predict(X_test)<br/>print('Accuracy score: ' + str(accuracy_score(y_test, y_pred)))<br/>print('Average Recall score: ' + str(recall_score(y_test, y_pred, average='macro')))<br/>print(classification_report(y_test, y_pred))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/5f9a6dd8a37164348a6fc14dffeb67ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*vcQLX74302NEsFFA8qdiZQ.png"/></div></figure><p id="def1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">不幸的是，我们的结果只是稍微好一点。我们还有一项技术可以尝试。</p><p id="bbdb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> SMOTE(合成少数过采样技术)</strong></p><p id="f025" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">SMOTE 从现有的少数类池中合成数据点，并将它们添加到数据集中。这种技术通过为模型创建新的、不可见的数据点进行训练，确保了数据泄漏非常少。</p><pre class="kg kh ki kj gt lq lr ls lt aw lu bi"><span id="ebee" class="lv lw iq lr b gy lx ly l lz ma"><em class="lo"># Import the SMOTE package</em><br/>from imblearn.over_sampling import SMOTE</span><span id="f96a" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Synthesize minority class datapoints using SMOTE</em><br/>sm = SMOTE(random_state=42, sampling_strategy=’minority’)<br/>smote_x_train, smote_y_train = sm.fit_resample(X_train, y_train)</span><span id="bf15" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Separate into training and test sets</em><br/>smote_x_train = pd.DataFrame(smote_x_train, columns = X_train.columns)<br/>smote_y_train = pd.DataFrame(smote_y_train, columns = ['Cancer'])</span><span id="56e3" class="lv lw iq lr b gy mb ly l lz ma">smote = RandomForestClassifier(n_estimators=1000).fit(smote_x_train, smote_y_train)<br/> <br/><em class="lo"># Predict on training set</em><br/>smote_preds = smote.predict(X_test)</span><span id="7129" class="lv lw iq lr b gy mb ly l lz ma"><em class="lo"># Checking accuracy and recall</em><br/>print('Accuracy Score: ', accuracy_score(y_test, smote_preds),'\n\n')<br/>print('Averaged Recall Score: ', recall_score(y_test, smote_preds, average='macro'), '\n\n')<br/>    <br/>print(classification_report(y_test, smote_preds))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/045f0644802ce6425c9b9a74cb9a2a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*ScLbKYil5mfPVUtAnsUj0g.png"/></div></figure><p id="2a36" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">准确性和 f1 分数有所提高，但回忆分数略有下降。根据您的模型用例，此时您必须决定哪个模型保护的人最多。</p><h1 id="1af8" class="me lw iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">AOC 是美国下层的冠军，但是 ROC AUC 是模型评测的冠军</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/eb090d3c7de82ce9744adcfa42362c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*caTqmNnrt5v4iAK3FAz6tg.gif"/></div></figure><p id="61ae" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">可视化模型质量的最佳方法之一是检查 ROC 曲线。ROC(受试者工作特性)曲线绘出了真阳性率 TPR 与假阳性率 FPR 的关系。最佳预测将在没有假阳性(成本)和 100%真阳性(收益)的点(0，1)处找到。</p><p id="d199" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">包括在图中的是无描述线，它展示了随机猜测，类似于抛硬币的概率。线上的点是“好的”猜测，因为它们在点(0，1)上更接近完美的结果。反之亦然，线以下的点表示预测不佳。</p><p id="ebe1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">AUC，或曲线下面积，是可分离程度的定量度量。预测越接近理想结果，AUC 就越大。当 AOC 告诉我们不要退而求其次时，她是在告诉我们将 AUC 定在接近 1%或 100%的水平。</p><p id="d8fa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是一个用 SMOTE 重采样训练集训练的随机森林分类器的例子(阅读文档<a class="ae ln" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py" rel="noopener ugc nofollow" target="_blank">此处</a>):</p><pre class="kg kh ki kj gt lq lr ls lt aw lu bi"><span id="0cd6" class="lv lw iq lr b gy lx ly l lz ma">y_score = smote.fit(smote_x_train, smote_y_train).predict_proba(X_test)[:,1]</span><span id="59f1" class="lv lw iq lr b gy mb ly l lz ma">fpr, tpr, thresholds = roc_curve(y_test, y_score)<br/>print('AUC: {}'.format(auc(fpr, tpr)))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6c5adb4889f55058f672b77622ca8040.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*X8xl89IVhBvedj_cDwsDSg.png"/></div></figure><p id="0e77" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">不算太差！仍有很大的改进空间，但我们可以从图中看出这一点:</p><pre class="kg kh ki kj gt lq lr ls lt aw lu bi"><span id="cd83" class="lv lw iq lr b gy lx ly l lz ma">plt.figure(figsize=(10, 8))<br/>lw = 2<br/>plt.plot(fpr, tpr, color=’darkorange’,<br/> lw=lw, label=’ROC curve’)<br/>plt.plot([0, 1], [0, 1], color=’navy’, lw=lw, linestyle=’ — ‘)<br/>plt.xlim([0.0, 1.0])<br/>plt.ylim([0.0, 1.05])<br/>plt.yticks([i/20.0 for i in range(21)])<br/>plt.xticks([i/20.0 for i in range(21)])<br/>plt.xlabel(‘False Positive Rate’)<br/>plt.ylabel(‘True Positive Rate’)<br/>plt.title(‘Receiver operating characteristic (ROC) Curve’)<br/>plt.legend(loc=’lower right’)<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/5fb3f98c539b06f12bf9092262480cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*LLEUWEm_NlWZadLnDV2KRw.png"/></div></figure><p id="3d96" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">理想情况下，我们希望看到橙色线更加陡峭，直到它非常接近(0，1)时才会变平，但这是一个很好的起点。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="4139" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">与所有机器学习项目一样，这个过程是迭代的。在您决定采用哪种方法之前，还有其他方法可以对您的模型进行重新采样和验证。彻底考虑哪种性能指标符合您的目的，选择一种适合您的数据类型的算法，并根据部署后危害最小的算法来评估您的模型。</p><p id="e037" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请在下面评论，告诉我你重新采样数据的其他方法！</p></div></div>    
</body>
</html>