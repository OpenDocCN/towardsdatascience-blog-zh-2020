<html>
<head>
<title>Towards Explainable AI with Feature Space Exploration 🚀</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向可解释人工智能的特征空间探索🚀</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/towards-explainable-ai-with-feature-space-exploration-628930baf8ef?source=collection_archive---------70-----------------------#2020-06-18">https://towardsdatascience.com/towards-explainable-ai-with-feature-space-exploration-628930baf8ef?source=collection_archive---------70-----------------------#2020-06-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/00b5ea67364d939fe0b5c8efe4b88389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6bFxLYzuz36LRVUc"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">安迪·凯利在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="db60" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于大量数据训练的神经网络导致了难以置信的技术飞跃，几乎影响了我们生活的每一个部分。</p><p id="837d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些进步是有代价的——即数据模型的<em class="lb">可解释性</em>和<em class="lb">可解释性</em>。与操作的复杂性相对应，为输入“选择”给定输出的标准变得相当神秘，导致一些人将神经网络称为“黑盒”方法。</p><p id="c495" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度神经网络工作得如此神奇，因为它们学习数据的有效表示，并且它们被有意地限制为捕捉数据中复杂的非线性模式。识别非线性模式的代价相当于<strong class="kf ir">失去视觉，只是为了获得更微妙的声音感知</strong>。在学习这些表示的过程中，神经网络的每一层中的特征在训练期间改变，并且随着不同的网络架构/数据集以不同的方式改变。这让我们想到了几个与深度学习相关的研究问题:</p><ol class=""><li id="71ed" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">我们如何理解与这些变化相关的模型性能？</li><li id="86be" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">我们如何确定每层的最佳单元数量？</li><li id="520b" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">我们如何定量描述网络中的变化分布？</li></ol><p id="8606" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们试图在最近的论文“训练期间的特征空间饱和”中回答这些问题，该论文现已在<a class="ae kc" href="https://arxiv.org/abs/2006.08679" rel="noopener ugc nofollow" target="_blank"> arXiv </a>上发布。通过在训练期间将主成分分析(PCA)应用于每一层中学习到的表示，我们可以确定解释这种差异所需的层大小或维度数量，从而逼近内在维度。通过一种类似于信息瓶颈方法[1]、SVCCA [2]和彩票假设[3]的方法，我们试图通过训练来识别网络上的动态分布。</p><h1 id="dccf" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated"><strong class="ak">层特征空间的固有维度</strong></h1><p id="d6b3" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">为了回答需要多个维度来解释层特征差异的问题，我们看一下自动编码器。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/d6f57bab012369da2f3cf770a8b0c43f.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*Y_IdlUnm08v2jwN3R3n_8w.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">派进去，派出来。自动编码器学习将数据压缩成特征向量，并且对于理解所学习的表示的动态是有用的。</p></figure><p id="c473" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自动编码器学习数据集的紧凑表示。它们对于识别神经网络压缩的限制以及整个模型训练中特征/表示的动态非常有用。</p><p id="6f24" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">PCA的输出是对应于数据相关性的各种方向(特征向量)和特征值。在这种情况下，我们的输入是在整个训练过程中根据图层表示计算的特征协方差矩阵。该矩阵捕获与特征独立性和相关性程度相关的一些动态。换句话说，<strong class="kf ir">某些神经元响应与层</strong>中的其他神经元一致或独立的程度。我们称这个投影为<em class="lb">层特征空间。</em></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/26e1c751f7d25eab3f8179db0d20559a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IW_3Jac-Pj-SLncLAltjWg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">阈值处理仅解释了最终全连接层的方差投影。</p></figure><p id="94a3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过选择不同的阈值来投影特征(通过特征值的累积和计算)，我们能够比较图像的重建，在这种情况下，是一片饼。我们把这个阈值称为delta (δ <strong class="kf ir"> ) </strong>，它的范围是解释方差所需的方向的0到100%。因此，对于100%的增量，我们期望输入的近乎完美的重构——没有一个被排除。</p><p id="7f40" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的实验中，超过99%的阈值，特征在特征向量上的投影会留下几乎无法辨认的图像。大部分结构丢失，表明模型执行需要绝大多数特征子空间维度。这种方法允许我们比较由网络学习的特征空间，并且理解网络已经学习数据的最佳压缩的程度。</p><h1 id="bf66" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">饱和度为模型训练提供了一个窗口</h1><p id="7557" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">我们把解释层特征的方差所需的特征向量比例称为<em class="lb">饱和度</em>。每层具有介于0和1之间的饱和指数，指示层特征子空间的固有维度。这使我们能够在深度神经网络中比较各层的饱和度。此外，我们比较了Alain】和Ben gio【4】的探测分类器方法，显示了每层输出执行分类任务的相对能力。</p><p id="5939" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们观察到饱和度反映了推理过程是如何分布的。当饱和度较高时，图层的特征以复杂和非线性的方式变化，对应于探测分类器精度的相对较高的增益。接近最后一层时，层精度的边际增加会减少，大多数情况下饱和度也会减少。因此，饱和度是最佳网络深度的指标，因为冗余层会以<em class="lb">尾</em>模式使饱和度向零收敛，如本文所述。</p><h1 id="9e55" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">输入分辨力</h1><p id="2822" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">输入分辨率是神经网络架构中需要平衡的三个方面之一(包括深度和宽度)[5]。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/f008cc6c3548c97716d897daf2558125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*sBfA-ZKqDMylNBIFEyEFVg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">与网络深度收益递减相关的饱和“尾巴”。探头性能仅在高饱和部分有所提高。饱和度的计算速度比探头精度快几个数量级。</p></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/5f841a7e3a4c563090bcaca11a69b824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*jW8o_r2QcMu_k_Og0zm6Cw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">饱和度“驼峰”与层的重要性有关。高分辨率数据的分类需要更复杂、非线性的特征空间分离，因此需要更高的饱和度。</p></figure><p id="b1b1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae kc" href="https://arxiv.org/pdf/2006.08679.pdf" rel="noopener ugc nofollow" target="_blank"> arXiv文章</a>中阅读更多相关内容，或者在<a class="ae kc" href="https://github.com/delve-team/delve" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上下载用于生成图的代码(delve Python库)。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="na nb l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">“训练期间特征空间饱和”，【https://arxiv.org/pdf/2006.08679.pdf T4】</p></figure><div class="nc nd gp gr ne nf"><a href="https://github.com/delve-team/delve" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd ir gy z fp nk fr fs nl fu fw ip bi translated">delve-团队/delve</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">Delve是一个用于可视化深度学习模型训练的Python包。如果你需要一个轻量级的PyTorch或者…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">github.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt jw nf"/></div></div></a></div></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="1e0d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢合著者<a class="ob oc ep" href="https://medium.com/u/1f048937b1f4?source=post_page-----628930baf8ef--------------------------------" rel="noopener" target="_blank"> Mats L. Richter </a>，Wolf Byttner，<a class="ob oc ep" href="https://medium.com/u/8eda5a9ade80?source=post_page-----628930baf8ef--------------------------------" rel="noopener" target="_blank"> Anders Arpteg </a>，以及<a class="ob oc ep" href="https://medium.com/u/bf4c4151f086?source=post_page-----628930baf8ef--------------------------------" rel="noopener" target="_blank"> Mikael Huss </a>。非常感谢<a class="ob oc ep" href="https://medium.com/u/7de8f3513f2e?source=post_page-----628930baf8ef--------------------------------" rel="noopener" target="_blank">卡尔·托姆梅</a>、<a class="ob oc ep" href="https://medium.com/u/8452ea37bdd2?source=post_page-----628930baf8ef--------------------------------" rel="noopener" target="_blank">阿格林·希尔姆基尔</a>、拉斯穆斯·迪德里克森<a class="ob oc ep" href="https://medium.com/u/e2bf5583a?source=post_page-----628930baf8ef--------------------------------" rel="noopener" target="_blank">、理查德·锡格河</a>、<a class="ob oc ep" href="https://medium.com/u/622790aa1486?source=post_page-----628930baf8ef--------------------------------" rel="noopener" target="_blank">亚历克西斯·德拉科普洛斯</a>、<a class="ob oc ep" href="https://medium.com/u/90b775aab5a2?source=post_page-----628930baf8ef--------------------------------" rel="noopener" target="_blank">萨兰·N·苏布拉曼尼扬</a>、<a class="ob oc ep" href="https://medium.com/u/50fed9fd6145?source=post_page-----628930baf8ef--------------------------------" rel="noopener" target="_blank">皮奥特·米格达</a>和<a class="ob oc ep" href="https://medium.com/u/41ee6301db0b?source=post_page-----628930baf8ef--------------------------------" rel="noopener" target="_blank">乌尔夫·克鲁姆纳克</a>在撰写本文期间提供的宝贵反馈。</p><p id="08af" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您在研究中使用这项工作，请注明:</p><pre class="mu mv mw mx gt od oe of og aw oh bi"><span id="cf12" class="oi lr iq oe b gy oj ok l ol om"><a class="ae kc" href="http://twitter.com/misc" rel="noopener ugc nofollow" target="_blank">@misc</a>{shenk2020feature,<br/>  title={Feature Space Saturation during Training},<br/>  author={Justin Shenk and Mats L. Richter and Wolf Byttner and Anders Arpteg and Mikael Huss},<br/>  year={2020},<br/>  eprint={2006.08679},<br/>  archivePrefix={arXiv},<br/>  primaryClass={cs.LG}<br/>}</span></pre><h1 id="57fd" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">参考</h1><p id="0850" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">[1]信息瓶颈法，<a class="ae kc" href="https://en.wikipedia.org/wiki/Information_bottleneck_method" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Information_bottleneck_method</a></p><p id="07e8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] SVCCA，<a class="ae kc" href="https://medium.com/@maddyschiappa/svcca-summary-e83a53f7dd68" rel="noopener">https://medium . com/@ maddyschiappa/SVCCA-summary-e 83 a 53 f 7 DD 68</a></p><p id="7f57" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]彩票假说，<a class="ae kc" rel="noopener" target="_blank" href="/breaking-down-the-lottery-ticket-hypothesis-ca1c053b3e58">https://towards data science . com/breaking-down-the-Lottery-Ticket-Hypothesis-ca 1c 053 B3 e 58</a></p><p id="e42c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4]概率分类器，“使用线性概率分类器理解中间层”，<a class="ae kc" href="https://arxiv.org/abs/1610.01644" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1610.01644</a></p><p id="97a9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5] EfficientNet，<a class="ae kc" rel="noopener" target="_blank" href="/efficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff">https://towards data science . com/efficient net-scaling-of-convolutionary-neural-networks-done-right-3 FDE 32 AEF 8 ff</a></p></div></div>    
</body>
</html>