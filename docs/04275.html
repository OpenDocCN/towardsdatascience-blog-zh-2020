<html>
<head>
<title>Learning from Multimodal Target</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从多模态目标中学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-from-multimodal-target-5d3d2ea0d4c5?source=collection_archive---------15-----------------------#2020-04-18">https://towardsdatascience.com/learning-from-multimodal-target-5d3d2ea0d4c5?source=collection_archive---------15-----------------------#2020-04-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f760" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">混合密度神经网络——使用张量流违反假设、实施、分析和应用。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8d0c0e52279012c0f80020cf03f3521e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-jZA9f5l1RaEKvPoFMz5BQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:作者</p></figure><h1 id="e1de" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="4e79" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">出于统计和商业原因，为模型评估进行预测并测量其不确定性是非常重要的。基本上，对于监督模型，我们有预测值(x)和目标值(y ),我们尝试使用预测值预测目标值，并量化我们预测的效果。这通常通过最小化平方和或交叉熵误差函数来完成，在连续预测值或分类变量的后验概率的情况下，该函数输出每个预测值的近似平均值。</p><p id="bb7b" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">商业案例示例</strong></p><p id="10bb" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们考虑一家销售手表的电子商务公司(ABC ),该公司有一个典型的亚马逊类型的业务模型，其中卖家列出要销售的产品，买家访问这些产品。当另一个卖家试图列出类似产品时，ABC希望对产品列表价格进行建模，以提供建议价格。下面是一款数字式手表的标价分布。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/0fae0e14f9870abf18446e0831d53627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1EwUfVOpSa11Sj4R6YJcmg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数字型手表上市价格分布|来源:作者</p></figure><p id="72b2" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">从分布来看，一块数字手表可以卖到20美元到2500美元。有卖家在卖3种不同价位的手表。$20–$500, $700–$1500, $1600–$2500.这种类型的数据违反了线性回归的单峰正态假设之一。然而，在实际场景中，我们不知道基础分布，因为经验分布仍然是正态分布。</p><p id="e7b5" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">拟合模型和违反假设</strong></p><p id="1277" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">如果我们用手表的<strong class="lp ir">类型</strong>作为唯一的预测器来拟合一个线性回归模型，那么通过对第一类手表的过度预测和对第三类手表的低预测，它将为<strong class="lp ir">数字</strong>类型手表预测1133美元。标准偏差是预测中的一个误差，将是709美元，使这个模型彻底失败。如果我们用这个预测的平均值(mu)和标准差(sigma)生成随机样本，它也会生成空白区域之间的价格。负价格也一样！[注意:我们在拟合模型之前对价格进行对数变换，因为价格无论如何都不是正态分布的，但为了简单起见，我们假设它是正态分布的]。即使将神经网络拟合到这种类型的数据也是完全失败的模型，因为它仍然试图最小化平方误差函数的总和。不同类型的手表标准差不一样怎么办？让我们考虑下面的小提琴情节:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/cca95879924322557183ebca66d97446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hH_oAvigQ9pLvnfd6g18WA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">方差递增的双峰分布|来源:作者</p></figure><p id="92f9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">每种类型都是双峰分布模型，在不同的水平上有不同的方差。不同类别的手表类型变化的差异。如果我们将神经网络或线性回归拟合到此数据，并且仅使用手表类型作为预测因素，那么误差的预测和标准偏差会发生什么变化？这个数据是否遵循了回归的所有假设？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/90e4c7b35d6f578aeb40925b520afd18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eRB4VEB2oJJWywYb.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">正态分布方程|来源:维基百科</p></figure><p id="9bd6" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">基本上，当我们最小化线性函数或平方和时，我们最小化给定<strong class="lp ir"> x </strong>的线性函数μ的输出的平方误差项(μ( <strong class="lp ir"> x </strong>，<strong class="lp ir">θ</strong>-<em class="mr">y</em>)以及函数的参数(<strong class="lp ir">θ)</strong>。它是唯一一个依赖于<strong class="lp ir"> x </strong>的μ，并丢弃了其余的参数(标准差)，也称为同方差。</p><p id="b715" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">我们可以通过手表类型学习多模态分销参数吗？是，带MDN </strong></p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="481e" class="kv kw iq bd kx ky mz la lb lc na le lf jw nb jx lh jz nc ka lj kc nd kd ll lm bi translated">高斯混合模型和混合密度网络</h1><p id="2c90" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我知道很多人对GMM很熟悉，但我还是想描述一下它和MDN的区别，因为它们非常相似。GMM是一种期望最大化无监督学习算法，作为K-means，除了学习假设分布的参数。K-means在重叠聚类的情况下不起作用，而GMM可以通过学习底层分布的参数来执行重叠聚类分割。对于上面的例子，我们可以通过应用GMM来导出基本的三态分布的参数。理论上，GMM通过优化由下式给出的给定概率分布来计算n个分量(聚类)以及相关的mu、sigma和聚类成员概率:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/b8acb4e4cd15fd76955375d341f4c7b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/0*xiv7wBvEmtNayEo_.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">https://en.wikipedia.org/wiki/Mixture_model GMM似然函数|来源:<a class="ae nf" href="https://en.wikipedia.org/wiki/Mixture_model" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="435c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">其中K是聚类数，phi是聚类潜在概率。使用EM算法，我们可以学习φ、μ和σ。</p><p id="1058" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">对于我们的ABC示例，我们用1500 * 3[样本数*聚类数]<strong class="lp ir">φ</strong>计算3个<strong class="lp ir">μs</strong>和3个<strong class="lp ir">μs</strong>。为了便于理解，我在上述数据集上训练了GMM，并验证了集群mu和sigmas是否接近真实mu和信号。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/42a67a7b9dafa7c860ab028537b62e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ls9iKxCXtc3a1xq-.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输出【<a class="ae nf" href="https://github.com/dwipam/MDN/blob/master/gmm.py" rel="noopener ugc nofollow" target="_blank">https://github.com/dwipam/MDN/blob/master/gmm.py</a>】|来源:作者</p></figure><p id="7b8c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">这为混合密度网络(MDN)奠定了基础。我们可以学习依赖于预测器的模型核参数。为此，我们将把我们的核限制为高斯核。MDN优化了与GMM相同的似然函数，并为我们数据集中的每个样本输出<strong class="lp ir"> mu、sigma和phi </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/97fe763efde15afc9770a4c12986fdde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/0*VRRpIq7AT0KMvV8H.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">3集群MDN架构|来源:作者</p></figure><p id="c46c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在不深入数学细节的情况下，让我们跳到训练模型。克里斯托弗·贝肖普已经解释得比我所能解释的还要好！</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="fad8" class="kv kw iq bd kx ky mz la lb lc na le lf jw nb jx lh jz nc ka lj kc nd kd ll lm bi translated">构建和培训MDN</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/fbb6dc27ee69affc9c68e0e1290ba042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tc1YhojeQpPOmkGR.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">综合数据集平均分布在2个集群和x个相关参数(mu，sigma) |来源:作者</p></figure><p id="acbe" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">生成数据</strong></p><p id="f2cd" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">我们将尝试生成具有以下x和y关系的数据</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="16d8" class="no kw iq nk b gy np nq l nr ns">x_ = sin(0.5 * x) * 3.0+ x * 0.5)<br/>f1(x) = N(x_, square(x_)/15) + N(0,1) # Cluster 1 y<br/>f2(x) = N(f1(x) + 12, square(x)/100) + N(0,1) # Cluster 2 y<br/>This equation generates data with 2 distinct cluster and x dependent variance.</span></pre><p id="8681" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">试衣MLP </strong></p><p id="a186" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">一个完全连接的神经网络用平方和损失函数拟合数据，并从预测中产生随机样本。如前所述，预测值是平均值，标准偏差用以下公式计算</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f242c09c121688dccef179b71ef5f1e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/0*ZJGUxyNWpzBRnDq0.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型的标准差(适马)|来源:作者</p></figure><p id="3839" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">其中误差为(y模型预测)</p><p id="f780" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">正态随机样本由平均值和标准差生成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/464266c7f327a8fad6ba8ce57457b231.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FY2Gkk00yZuO5mTv.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MLP估计|来源:作者</p></figure><p id="a678" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">这是一个问题，模型通过对50%的数据预测不足，而对剩余的50%的数据预测过度，在空白区域内进行预测。因为means和sigmas是数据相关的，所以让我们尝试使用MDN</p><p id="e4e4" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">构建MDN </strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">全连接4层神经网络</p></figure><p id="7fd3" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">第一部分相当于某人会为任何其他问题创建一个神经网络。在这里，我们创建了4个隐藏层神经网络，每个分别具有(50，20，20，20)个节点。我使用的是Tanh激活，但这里任何其他激活都可以，因为我们正在尝试学习的函数是一个简单的函数，对任何其他激活函数都不会有问题。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">定义管理部门、西格玛、phi</p></figure><p id="6288" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">然后，我们为每个μ、σ和φ创建“k”大小向量。在我们的例子中，K是2</p><ul class=""><li id="ae64" class="nw nx iq lp b lq mj lt mk lw ny ma nz me oa mi ob oc od oe bi translated">我们的目标具有从-ve到+ve的值，因此我们不会使用激活函数来估计<strong class="lp ir">均值</strong> (mu)。</li><li id="83ea" class="nw nx iq lp b lq of lt og lw oh ma oi me oj mi ob oc od oe bi translated"><strong class="lp ir">标准差</strong> (sigma)不能为负，因此我们可以使用softplus、elu、relu或任何其他变量，只要它输出值&gt; 0，并且不像sigmoid那样限制输出。</li><li id="adba" class="nw nx iq lp b lq of lt og lw oh ma oi me oj mi ob oc od oe bi translated"><strong class="lp ir">在所有k上计算后验聚类分配概率</strong> (Pi)。由于它们是互斥事件，我们使用softmax激活函数。</li></ul><p id="1fb9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">可以使用张量流概率内置函数来定义损失函数(TFP . distributions .<strong class="lp ir">mixture same family</strong>)。然而，如果你真的想计算损失，我们仍然可以用下面的公式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/0d3d3b3b0768b8704d1841e5bb2714ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/0*HmUawdxC0gPmgcR7.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MDN负日志损失|来源:作者</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">无tfp的Tensorflow MDN损耗。mixture同一个家庭</p></figure><p id="3377" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">损失是使用上述相同的<strong class="lp ir"> GMM </strong>似然方程计算的。首先，计算每个分量的<strong class="lp ir">μ</strong>和<strong class="lp ir">σ</strong>，并计算后验概率。然后乘以关联的分量<strong class="lp ir">φ</strong>并对所有后验概率求和得到似然。然后记录可能性，并对所有样本求和，得到对数可能性。那么平均损失就是负对数似然的平均值，它将进一步进入优化器。如果您想节省一个步骤，让TensorFlow来处理，我们可以使用tensor flow-probability . distributions .<strong class="lp ir">mixture same family。张量流概率</strong>被<strong class="lp ir">数据科学家、ML研究人员和统计学家</strong>广泛用于概率建模。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">带tfp的Tensorflow MDN损耗。mixture同一个家庭</p></figure><p id="c247" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">TFP . distributions几何</strong>定义分类损失。由于我们局限于正态分布，我们可以用张量流概率来定义它。我们可以定义正态分布方程来代替！。MixtureSameFamily会将mus和sigma的向量转换为混合分布的协方差矩阵。然后使用log_prob返回log_likelihood，最后最小化负的log-likelihood。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">定义优化程序</p></figure><p id="1066" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">人们可以使用任何优化器，但我决定你RMSProp，因为它仍然比随机梯度下降优化器更好。</p><p id="0b65" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">最后，我们训练我们的网络，从混合分布中生成随机样本，并查看预测平均值(mu)、标准差(sigma)和聚类概率(phi)。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/db6b3f4fcb2683e32fcefd5b3b980f6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NGV-VuZK2wZEYhRQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MDN估计平均值|来源:作者</p></figure><p id="7e45" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在学习了包含2个组件的MDN之后，我们得到了我们所希望的。现在均值的估计不在空白区域，正确地估计了每x的双峰分布均值。</p><p id="2eb7" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">为了查看方差，我们可以生成随机样本，并确保这些样本属于各自的聚类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/ad02522b2d9320bcac39d700d3939b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hN4FsAt5kOSELv9y.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自MDN估计平均值和sigmas的随机样本|来源:作者</p></figure><p id="85cf" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">与MLP模型预测相比，我们可以看到两个不同的集群重叠的基本事实，从而准确估计x依赖的双模态分布参数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/38ba3538ebd6459f484bc7f4c9dd8642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*xcx_5jQIaBsaW4SU.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">估计聚类概率分布|来源:作者</p></figure><p id="8072" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">聚类概率(phi)非常接近50%(46%-54%是模型参数的方差，每次训练可能不同)，因为我为每个x的每个聚类生成了相同数量的样本，即P(cluster=1|x) = P(cluster=2|x)。如果不是这种情况，那么我们可以看到不同的集群分配概率的分布。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="60b9" class="kv kw iq bd kx ky mz la lb lc na le lf jw nb jx lh jz nc ka lj kc nd kd ll lm bi translated">结论</h1><p id="3990" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">通过我们之前的ABC示例，该公司可以拟合MDN并获得对价格分布的真实理解。如果我们不知道我们的目标是多模态分布，聚类概率(phi)可以很好地理解这一点。实际上，训练一个MDN是非常耗时和困难的，因为对分布的假设可能不成立。MDN可以用来学习不同家族的混合分布，但是你的模型和你的假设一样正确！</p><p id="3513" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">通过这篇简单的博文，我展示了平方和函数的缺点，以及应用MDN测量参数估计的不确定性的优点，以及它们对业务问题的重要性。鉴于其简单的架构和易于实现(感谢TF)，我很想听听读者的想法！</p><p id="cb7d" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">培训笔记本:</p><div class="om on gp gr oo op"><a href="https://github.com/dwipam/MDN/blob/master/MDN_NOTEBOOK.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd ir gy z fp ou fr fs ov fu fw ip bi translated">dwipam/MDN</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">permalink dissolve GitHub是4000多万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">github.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd kp op"/></div></div></a></div></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="f079" class="kv kw iq bd kx ky mz la lb lc na le lf jw nb jx lh jz nc ka lj kc nd kd ll lm bi translated"><strong class="ak">参考文献:</strong></h1><p id="14f9" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">MDN论文-【https://github.com/dwipam/MDN/blob/master/MDN.pdf】T2TF mixture same Family-<a class="ae nf" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/probability/API _ docs/python/TFP/distributions/mixture same Family</a>Shaked的博客-<a class="ae nf" href="https://engineering.taboola.com/predicting-probability-distributions/" rel="noopener ugc nofollow" target="_blank">https://engineering . taboo la . com/predicting-probability-distributions/</a><br/>GMM-<a class="ae nf" href="https://en.wikipedia.org/wiki/GMM" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/GMM</a></p></div></div>    
</body>
</html>