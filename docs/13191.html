<html>
<head>
<title>Understand the Logistic Regression from Scratch â€” Kaggle Notebook</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä»é›¶å¼€å§‹ç†è§£é€»è¾‘å›å½’â€” Kaggle ç¬”è®°æœ¬</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/understand-logistic-regression-from-scratch-430aedf5edb9?source=collection_archive---------28-----------------------#2020-09-10">https://towardsdatascience.com/understand-logistic-regression-from-scratch-430aedf5edb9?source=collection_archive---------28-----------------------#2020-09-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8c1a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">é€šè¿‡è‡ªå·±å®ç°æ¥å­¦ä¹ ç®—æ³•ã€‚</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/b878d7dc19be08af5fc211781fca42f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*hlJ4VjDAYT32-1qSgDPinA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">ç”±ä½œè€…åˆ›å»º</p></figure><h2 id="d30d" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">ç›®å½•</h2><p id="379f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">1.ç›®æ ‡</p><p id="65c5" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">2.åŠ è½½æ•°æ®</p><p id="0b59" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">3.ä»æ–‡æœ¬ä¸­æå–ç‰¹å¾</p><p id="bf14" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">4.å®æ–½é€»è¾‘å›å½’</p><ul class=""><li id="d3b8" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">4.1 æ¦‚è¿°</li><li id="bca7" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">4.2 ä¹™çŠ¶ç»“è‚ </li><li id="9e81" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">4.3 æˆæœ¬å‡½æ•°</li><li id="eb80" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">4.4 æ¢¯åº¦ä¸‹é™</li><li id="9181" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">4.5 æ­£è§„åŒ–</li></ul><p id="b3c1" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">5.ç«è½¦æ¨¡å‹</p><p id="0b64" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">6.æµ‹è¯•æˆ‘ä»¬çš„é€»è¾‘å›å½’</p><p id="52f2" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">7.ç”¨ Scikit å­¦ä¹ é€»è¾‘å›å½’æµ‹è¯•</p><p id="577d" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">è®©æˆ‘ä»¬ç”¨ Python å¯¼å…¥æ‰€æœ‰å¿…è¦çš„æ¨¡å—ã€‚</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="1767" class="ku kv it nd b gy nh ni l nj nk"># regular expression operations<br/>import re    <br/># string operation <br/>import string  <br/># shuffle the list<br/>from random import shuffle<br/><br/># linear algebra<br/>import numpy as np <br/># data processing<br/>import pandas as pd <br/><br/># NLP library<br/>import nltk<br/># download twitter dataset<br/>from nltk.corpus import twitter_samples                          <br/><br/># module for stop words that come with NLTK<br/>from nltk.corpus import stopwords          <br/># module for stemming<br/>from nltk.stem import PorterStemmer        <br/># module for tokenizing strings<br/>from nltk.tokenize import TweetTokenizer   <br/><br/># scikit model selection<br/>from sklearn.model_selection import train_test_split<br/><br/># smart progressor meter<br/>from tqdm import tqdm</span></pre><h2 id="d7c5" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">1.ç›®æ ‡</h2><p id="29e9" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">è¿™ä¸ªå†…æ ¸çš„ç›®æ ‡æ˜¯ä½¿ç”¨ twitter æ•°æ®é›†ä»é›¶å¼€å§‹å®ç°ç”¨äºæƒ…æ„Ÿåˆ†æçš„é€»è¾‘å›å½’ã€‚æˆ‘ä»¬å°†ä¸»è¦å…³æ³¨é€»è¾‘å›å½’çš„æ„å»ºæ¨¡å—ã€‚è¿™ä¸ªå†…æ ¸å¯ä»¥æä¾›å¯¹<strong class="ls iu"> <em class="nl">å†…éƒ¨å¦‚ä½•è¿›è¡Œé€»è¾‘å›å½’</em> </strong>çš„æ·±å…¥ç†è§£ã€‚ä½¿ç”¨<a class="ae nm" href="https://pypi.org/project/jupyter-to-medium/" rel="noopener ugc nofollow" target="_blank"> JupytertoMedium </a> python åº“å°†ç¬”è®°æœ¬è½¬æ¢æˆä¸­å‹æ–‡ç« ã€‚Kaggle ç¬”è®°æœ¬å¯ä»<a class="ae nm" href="https://www.kaggle.com/narendrageek/understand-the-logistic-regression-from-scratch" rel="noopener ugc nofollow" target="_blank">è¿™é‡Œ</a>è·å¾—ã€‚</p><p id="1f9f" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">ç»™å®šä¸€æ¡æ¨æ–‡ï¼Œå¦‚æœå®ƒæœ‰<strong class="ls iu">æ­£é¢æƒ…ç»ªï¼Œå®ƒå°†è¢«åˆ†ç±»ğŸ‘æˆ–è€…æ¶ˆææƒ…ç»ªğŸ‘</strong>ã€‚è¿™å¯¹åˆå­¦è€…å’Œå…¶ä»–äººéƒ½å¾ˆæœ‰ç”¨ã€‚</p><h2 id="2bf2" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">2.åŠ è½½æ•°æ®</h2><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="b312" class="ku kv it nd b gy nh ni l nj nk"># Download the twitter sample data from NLTK repository<br/>nltk.download('twitter_samples')</span></pre><ul class=""><li id="3d78" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated"><code class="fe nn no np nd b">twitter_samples</code>åŒ…å« 5000 æ¡æ­£é¢æ¨æ–‡å’Œ 5000 æ¡è´Ÿé¢æ¨æ–‡ã€‚æ€»å…±æœ‰ 10ï¼Œ000 æ¡æ¨æ–‡ã€‚</li><li id="827d" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">æˆ‘ä»¬æ¯ä¸ªç­éƒ½æœ‰ç›¸åŒæ•°é‡çš„æ•°æ®æ ·æœ¬ã€‚</li><li id="ee8e" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">è¿™æ˜¯ä¸€ä¸ªå¹³è¡¡çš„æ•°æ®é›†ã€‚</li></ul><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="bd4e" class="ku kv it nd b gy nh ni l nj nk"># read the positive and negative tweets<br/>pos_tweets = twitter_samples.strings('positive_tweets.json')<br/>neg_tweets = twitter_samples.strings('negative_tweets.json')<br/>print(f"positive sentiment ğŸ‘ total samples {len(pos_tweets)} \nnegative sentiment ğŸ‘ total samples {len(neg_tweets)}")</span><span id="1432" class="ku kv it nd b gy nq ni l nj nk">positive sentiment ğŸ‘ total samples 5000 <br/>negative sentiment ğŸ‘ total samples 5000</span><span id="9e58" class="ku kv it nd b gy nq ni l nj nk"># Let's have a look at the data<br/>no_of_tweets = 3<br/>print(f"Let's take a look at first {no_of_tweets} sample tweets:\n")<br/>print("Example of Positive tweets:")<br/>print('\n'.join(pos_tweets[:no_of_tweets]))<br/>print("\nExample of Negative tweets:")<br/>print('\n'.join(neg_tweets[:no_of_tweets]))</span><span id="fff9" class="ku kv it nd b gy nq ni l nj nk">Let's take a look at first 3 sample tweets:</span></pre><p id="088f" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">è¾“å‡º:</strong></p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="4e2c" class="ku kv it nd b gy nh ni l nj nk">Example of Positive tweets:<br/>#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)<br/>@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!<br/>@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!<br/><br/>Example of Negative tweets:<br/>hopeless for tmr :(<br/>Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(<br/>@Hegelbon That heart sliding into the waste basket. :(</span></pre><ul class=""><li id="26bf" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">æ¨æ–‡å¯èƒ½åŒ…å« URLã€æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ã€‚</li></ul><h2 id="e19b" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">é¢„å¤„ç†æ–‡æœ¬</h2><p id="ba77" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">é¢„å¤„ç†æ˜¯æµæ°´çº¿ä¸­çš„é‡è¦æ­¥éª¤ä¹‹ä¸€ã€‚å®ƒåŒ…æ‹¬åœ¨å»ºç«‹æœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹å‰æ¸…ç†å’Œåˆ é™¤ä¸å¿…è¦çš„æ•°æ®ã€‚</p><p id="ea0b" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">é¢„å¤„ç†æ­¥éª¤:</p><ol class=""><li id="cdce" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi nr mu mv mw bi translated">å¯¹å­—ç¬¦ä¸²è¿›è¡Œæ ‡è®°</li><li id="1e62" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi nr mu mv mw bi translated">å°† tweet è½¬æ¢æˆå°å†™ï¼Œå¹¶å°† tweet æ‹†åˆ†æˆä»¤ç‰Œ(å•è¯)</li><li id="c1c3" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi nr mu mv mw bi translated">åˆ é™¤åœç”¨å­—è¯å’Œæ ‡ç‚¹ç¬¦å·</li><li id="d792" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi nr mu mv mw bi translated">åˆ é™¤ twitter å¹³å°ä¸Šçš„å¸¸ç”¨è¯ï¼Œå¦‚æ ‡ç­¾ã€è½¬å‘æ ‡è®°ã€è¶…é“¾æ¥ã€æ•°å­—å’Œç”µå­é‚®ä»¶åœ°å€</li><li id="d23b" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi nr mu mv mw bi translated">å µå¡ç‰©</li></ol><ul class=""><li id="63ef" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">è¿™æ˜¯æŠŠä¸€ä¸ªå•è¯è½¬æ¢æˆå®ƒæœ€æ™®é€šå½¢å¼çš„è¿‡ç¨‹ã€‚å®ƒæœ‰åŠ©äºå‡å°‘æˆ‘ä»¬çš„è¯æ±‡é‡ã€‚ä¾‹å¦‚ï¼Œengage è¿™ä¸ªè¯æœ‰ä¸åŒçš„è¯å¹²ï¼Œ</li><li id="b227" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated"><strong class="ls iu">è®¢å©š</strong></li><li id="354f" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">è®¢å©šçš„</li><li id="0005" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">è®¢å©š</li></ul><p id="4962" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å®ç°è¿™ä¸€ç‚¹ã€‚</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="2624" class="ku kv it nd b gy nh ni l nj nk"># helper class for doing preprocessing<br/>class Twitter_Preprocess():<br/>    <br/>    def __init__(self):<br/>        # instantiate tokenizer class<br/>        self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,<br/>                                       reduce_len=True)<br/>        # get the english stopwords <br/>        self.stopwords_en = stopwords.words('english') <br/>        # get the english punctuation<br/>        self.punctuation_en = string.punctuation<br/>        # Instantiate stemmer object<br/>        self.stemmer = PorterStemmer() <br/>        <br/>    def __remove_unwanted_characters__(self, tweet):<br/>        <br/>        # remove retweet style text "RT"<br/>        tweet = re.sub(r'^RT[\s]+', '', tweet)<br/><br/>        # remove hyperlinks<br/>        tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)<br/>     <br/>        # remove hashtags<br/>        tweet = re.sub(r'#', '', tweet)<br/>        <br/>        #remove email address<br/>        tweet = re.sub('\S+@\S+', '', tweet)<br/>        <br/>        # remove numbers<br/>        tweet = re.sub(r'\d+', '', tweet)<br/>        <br/>        ## return removed text<br/>        return tweet<br/>    <br/>    def __tokenize_tweet__(self, tweet):        <br/>        # tokenize tweets<br/>        return self.tokenizer.tokenize(tweet)<br/>    <br/>    def __remove_stopwords__(self, tweet_tokens):<br/>        # remove stopwords<br/>        tweets_clean = []<br/><br/>        for word in tweet_tokens:<br/>            if (word not in self.stopwords_en and  # remove stopwords<br/>                word not in self.punctuation_en):  # remove punctuation<br/>                tweets_clean.append(word)<br/>        return tweets_clean<br/>    <br/>    def __text_stemming__(self,tweet_tokens):<br/>        # store the stemmed word<br/>        tweets_stem = [] <br/><br/>        for word in tweet_tokens:<br/>            # stemming word<br/>            stem_word = self.stemmer.stem(word)  <br/>            tweets_stem.append(stem_word)<br/>        return tweets_stem<br/>    <br/>    def preprocess(self, tweets):<br/>        tweets_processed = []<br/>        for _, tweet in tqdm(enumerate(tweets)):        <br/>            # apply removing unwated characters and remove style of retweet, URL<br/>            tweet = self.__remove_unwanted_characters__(tweet)            <br/>            # apply nltk tokenizer<br/>/            tweet_tokens = self.__tokenize_tweet__(tweet)            <br/>            # apply stop words removal<br/>            tweet_clean = self.__remove_stopwords__(tweet_tokens)<br/>            # apply stemmer <br/>            tweet_stems = self.__text_stemming__(tweet_clean)<br/>            tweets_processed.extend([tweet_stems])<br/>        return tweets_processed</span><span id="2a0b" class="ku kv it nd b gy nq ni l nj nk"># initilize the text preprocessor class object<br/>twitter_text_processor = Twitter_Preprocess()<br/><br/># process the positive and negative tweets<br/>processed_pos_tweets = twitter_text_processor.preprocess(pos_tweets)<br/>processed_neg_tweets = twitter_text_processor.preprocess(neg_tweets)</span><span id="50c7" class="ku kv it nd b gy nq ni l nj nk">5000it [00:02, 2276.81it/s]<br/>5000it [00:02, 2409.93it/s]</span></pre><p id="54e0" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">è®©æˆ‘ä»¬çœ‹çœ‹é¢„å¤„ç† tweets åå¾—åˆ°äº†ä»€ä¹ˆè¾“å‡ºã€‚æˆ‘ä»¬èƒ½å¤ŸæˆåŠŸå¤„ç†æ¨æ–‡ï¼Œè¿™å¾ˆå¥½ã€‚</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="06c1" class="ku kv it nd b gy nh ni l nj nk">pos_tweets[:no_of_tweets], processed_pos_tweets[:no_of_tweets]</span><span id="cc70" class="ku kv it nd b gy nq ni l nj nk">(['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',<br/>  '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',<br/>  '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!'],<br/> [['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)'],<br/>  ['hey',<br/>   'jame',<br/>   'odd',<br/>   ':/',<br/>   'pleas',<br/>   'call',<br/>   'contact',<br/>   'centr',<br/>   'abl',<br/>   'assist',<br/>   ':)',<br/>   'mani',<br/>   'thank'],<br/>  ['listen', 'last', 'night', ':)', 'bleed', 'amaz', 'track', 'scotland']])</span></pre><h2 id="c87f" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">3.ä»æ–‡æœ¬ä¸­æå–ç‰¹å¾</h2><ul class=""><li id="4c27" class="mo mp it ls b lt lu lw lx ld ns lh nt ll nu mi mt mu mv mw bi translated">ç»™å®šæ–‡æœ¬ï¼Œä»¥è¿™æ ·ä¸€ç§æ–¹å¼è¡¨ç¤º<code class="fe nn no np nd b">features (numeric values)</code>æ˜¯éå¸¸é‡è¦çš„ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚</li></ul><h2 id="01ea" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">3.1 åˆ›å»ºä¸€ä¸ªå•è¯åŒ…(BOW)è¡¨ç¤ºæ³•</h2><p id="dcdf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">BOW ä»£è¡¨å•è¯åŠå…¶åœ¨æ¯ä¸ªç±»ä¸­çš„å‡ºç°é¢‘ç‡ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª<code class="fe nn no np nd b">dict</code>æ¥å­˜å‚¨æ¯ä¸ªå•è¯çš„<code class="fe nn no np nd b">positive</code>å’Œ<code class="fe nn no np nd b">negative</code>ç±»çš„é¢‘ç‡ã€‚è®©æˆ‘ä»¬æŒ‡å‡ºä¸€æ¡<code class="fe nn no np nd b">positive</code>æ¨æ–‡æ˜¯<code class="fe nn no np nd b">1</code>ï¼Œè€Œ<code class="fe nn no np nd b">negative</code>æ¨æ–‡æ˜¯<code class="fe nn no np nd b">0</code>ã€‚<code class="fe nn no np nd b">dict</code>é”®æ˜¯ä¸€ä¸ªåŒ…å«<code class="fe nn no np nd b">(word, y)</code>å¯¹çš„å…ƒç»„ã€‚<code class="fe nn no np nd b">word</code>æ˜¯å¤„ç†è¿‡çš„å­—ï¼Œ<code class="fe nn no np nd b">y</code>è¡¨ç¤ºç±»çš„æ ‡ç­¾ã€‚dict å€¼ä»£è¡¨ç±»<code class="fe nn no np nd b">y</code>çš„<code class="fe nn no np nd b">frequency of the word</code>ã€‚</p><p id="be81" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">ç¤ºä¾‹:#å•è¯ bad åœ¨ 0(è´Ÿ)ç±»ä¸­å‡ºç° 45 æ¬¡{(â€œbadâ€ï¼Œ0) : 32}</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="e4bb" class="ku kv it nd b gy nh ni l nj nk"># word bad occurs 45 time in the 0 (negative) class <br/>{("bad", 0) : 45}</span><span id="cb3a" class="ku kv it nd b gy nq ni l nj nk"># BOW frequency represent the (word, y) and frequency of y class<br/>def build_bow_dict(tweets, labels):<br/>    freq = {}<br/>    ## create zip of tweets and labels<br/>    for tweet, label in list(zip(tweets, labels)):<br/>        for word in tweet:<br/>            freq[(word, label)] = freq.get((word, label), 0) + 1<br/>        <br/>    return freq</span><span id="5224" class="ku kv it nd b gy nq ni l nj nk"># create labels of the tweets<br/># 1 for positive labels and 0 for negative labels<br/>labels = [1 for i in range(len(processed_pos_tweets))]<br/>labels.extend([0 for i in range(len(processed_neg_tweets))])<br/><br/># combine the positive and negative tweets<br/>twitter_processed_corpus = processed_pos_tweets + processed_neg_tweets<br/><br/># build Bog of words frequency <br/>bow_word_frequency = build_bow_dict(twitter_processed_corpus, labels)</span></pre><p id="e1f8" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰å„ç§æ–¹æ³•æ¥è¡¨ç¤º twitter è¯­æ–™åº“çš„ç‰¹å¾ã€‚ä¸€äº›åŸºæœ¬è€Œå¼ºå¤§çš„æŠ€æœ¯æ˜¯ï¼Œ</p><ul class=""><li id="6bf5" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">è®¡æ•°çŸ¢é‡å™¨</li><li id="796a" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">TF-IDF åŠŸèƒ½</li></ul><h2 id="2fcf" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">1.è®¡æ•°çŸ¢é‡å™¨</h2><p id="dcb1" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">è®¡æ•°çŸ¢é‡å™¨æŒ‡ç¤ºç¨€ç–çŸ©é˜µï¼Œå¹¶ä¸”è¯¥å€¼å¯ä»¥æ˜¯å•è¯çš„é¢‘ç‡<strong class="ls iu">ã€‚åœ¨æˆ‘ä»¬çš„è¯­æ–™åº“ä¸­ï¼Œæ¯ä¸€åˆ—éƒ½æ˜¯å”¯ä¸€çš„æ ‡è®°ã€‚</strong></p><blockquote class="nv nw nx"><p id="8e79" class="lq lr nl ls b lt mj ju lv lw mk jx ly ny ml ma mb nz mm md me oa mn mg mh mi im bi translated">ç¨€ç–çŸ©é˜µçš„ç»´æ•°å°†æ˜¯<code class="fe nn no np nd b"><em class="it">no of unique tokens in the corpus * no of sample tweets</em></code>ã€‚</p></blockquote><p id="f747" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">ç¤ºä¾‹:<code class="fe nn no np nd b">corpus = [ 'This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?', ]</code>å¹¶ä¸” CountVectorizer è¡¨ç¤ºä¸º</p><p id="ea3a" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><code class="fe nn no np nd b">[[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]]</code></p><h2 id="836b" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">2.TF-IDF(æœ¯è¯­é¢‘ç‡-é€†æ–‡æ¡£é¢‘ç‡)</h2><p id="6181" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">TF-IDF ç»Ÿè®¡åº¦é‡ï¼Œç”¨äºè¯„ä¼°å•è¯ä¸æ–‡æ¡£é›†åˆä¸­çš„æ–‡æ¡£çš„ç›¸å…³ç¨‹åº¦ã€‚TF-IDF çš„è®¡ç®—å¦‚ä¸‹:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/68e4884c6d2e08c0027b49b284d0f7db.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*Q0HEaKlRsLqEztzxTBidCA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">TF-IDF æ–¹ç¨‹</p></figure><p id="af77" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">è¯é¢‘:</strong>è¯é¢‘<strong class="ls iu"> <em class="nl"> tf(tï¼Œd) </em> </strong>ï¼Œæœ€ç®€å•çš„é€‰æ‹©å°±æ˜¯ä½¿ç”¨ä¸€ä¸ªè¯(è¯)åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°é¢‘ç‡ã€‚<strong class="ls iu">é€†æ–‡æ¡£é¢‘ç‡:</strong> <strong class="ls iu"> <em class="nl"> idf(tï¼ŒD) </em> </strong>è¡¡é‡å•è¯æä¾›å¤šå°‘ä¿¡æ¯ï¼Œå³å®ƒåœ¨æ‰€æœ‰æ–‡æ¡£ä¸­æ˜¯å¸¸è§è¿˜æ˜¯ç½•è§ã€‚å®ƒæ˜¯åŒ…å«è¯¥å•è¯çš„æ–‡æ¡£çš„é€†åˆ†æ•°çš„<strong class="ls iu">å¯¹æ•°æ ‡åº¦</strong>ã€‚å®šä¹‰è§<a class="ae nm" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">ç»´åŸº</a>ã€‚</p><h2 id="dc3a" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">3.2.ä¸ºæˆ‘ä»¬çš„æ¨¡å‹æå–ç®€å•ç‰¹å¾</h2><ul class=""><li id="60ae" class="mo mp it ls b lt lu lw lx ld ns lh nt ll nu mi mt mu mv mw bi translated">ç»™å®šä¸€ä¸ªæ¨æ–‡åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†æå–ä¸¤ä¸ªç‰¹å¾ã€‚</li><li id="79f1" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯ä¸€æ¡æ¨æ–‡ä¸­æ­£é¢è¯çš„æ•°é‡ã€‚</li><li id="dcaa" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">ç¬¬äºŒä¸ªç‰¹å¾æ˜¯æ¨æ–‡ä¸­è´Ÿé¢è¯çš„æ•°é‡ã€‚</li></ul><p id="b361" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">è¿™çœ‹ä¼¼ç®€å•ï¼Œä¸æ˜¯å—ï¼Ÿä¹Ÿè®¸æ˜¯çš„ã€‚æˆ‘ä»¬æ²¡æœ‰å‘ç¨€ç–çŸ©é˜µè¡¨ç¤ºæˆ‘ä»¬çš„ç‰¹å¾ã€‚å°†ä½¿ç”¨æœ€ç®€å•çš„ç‰¹å¾è¿›è¡Œåˆ†æã€‚</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="b3c0" class="ku kv it nd b gy nh ni l nj nk"># extract feature for tweet<br/>def extract_features(processed_tweet, bow_word_frequency):<br/>    # feature array<br/>    features = np.zeros((1,3))<br/>    # bias term added in the 0th index<br/>    features[0,0] = 1<br/>    <br/>    # iterate processed_tweet<br/>    for word in processed_tweet:<br/>        # get the positive frequency of the word<br/>        features[0,1] = bow_word_frequency.get((word, 1), 0)<br/>        # get the negative frequency of the word<br/>        features[0,2] = bow_word_frequency.get((word, 0), 0)<br/>    <br/>    return features</span></pre><p id="ce81" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">æ‰“ä¹±è¯­æ–™åº“ï¼Œå°†è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¼€ã€‚</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="45f4" class="ku kv it nd b gy nh ni l nj nk"># shuffle the positive and negative tweets<br/>shuffle(processed_pos_tweets)<br/>shuffle(processed_neg_tweets)<br/><br/># create positive and negative labels<br/>positive_tweet_label = [1 for i in processed_pos_tweets]<br/>negative_tweet_label = [0 for i in processed_neg_tweets]<br/><br/># create dataframe<br/>tweet_df = pd.DataFrame(list(zip(twitter_processed_corpus, positive_tweet_label+negative_tweet_label)), columns=["processed_tweet", "label"])</span></pre><h2 id="5dac" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">3.3 è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²</h2><p id="3e02" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">è®©æˆ‘ä»¬ä¿ç•™ 80%çš„æ•°æ®ç”¨äºè®­ç»ƒï¼Œ20%çš„æ•°æ®æ ·æœ¬ç”¨äºæµ‹è¯•ã€‚</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="f58b" class="ku kv it nd b gy nh ni l nj nk"># train and test split<br/>train_X_tweet, test_X_tweet, train_Y, test_Y = train_test_split(tweet_df["processed_tweet"], tweet_df["label"], test_size = 0.20, stratify=tweet_df["label"])<br/>print(f"train_X_tweet {train_X_tweet.shape}, test_X_tweet {test_X_tweet.shape}, train_Y {train_Y.shape}, test_Y {test_Y.shape}")</span><span id="43f2" class="ku kv it nd b gy nq ni l nj nk">train_X_tweet (8000,), test_X_tweet (2000,), train_Y (8000,), test_Y (2000,)</span><span id="6fff" class="ku kv it nd b gy nq ni l nj nk"># train X feature dimension<br/>train_X = np.zeros((len(train_X_tweet), 3))<br/><br/>for index, tweet in enumerate(train_X_tweet):<br/>    train_X[index, :] = extract_features(tweet, bow_word_frequency)<br/><br/># test X feature dimension<br/>test_X = np.zeros((len(test_X_tweet), 3))<br/><br/>for index, tweet in enumerate(test_X_tweet):<br/>    test_X[index, :] = extract_features(tweet, bow_word_frequency)<br/><br/>print(f"train_X {train_X.shape}, test_X {test_X.shape}")</span><span id="ff39" class="ku kv it nd b gy nq ni l nj nk">train_X (8000, 3), test_X (2000, 3)</span></pre><p id="0582" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">è¾“å‡º:</strong></p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="67b0" class="ku kv it nd b gy nh ni l nj nk">train_X[0:5]</span><span id="2331" class="ku kv it nd b gy nq ni l nj nk">array([[1.000e+00, 6.300e+02, 0.000e+00],<br/>       [1.000e+00, 6.930e+02, 0.000e+00],<br/>       [1.000e+00, 1.000e+00, 4.570e+03],<br/>       [1.000e+00, 1.000e+00, 4.570e+03],<br/>       [1.000e+00, 3.561e+03, 2.000e+00]])</span></pre><p id="2921" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">çœ‹ä¸€çœ‹æ ·æœ¬è®­ç»ƒç‰¹å¾ã€‚</p><ul class=""><li id="9eff" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">ç¬¬ 0 ä¸ªç´¢å¼•æ˜¯æ·»åŠ çš„åå·®é¡¹ã€‚</li><li id="3464" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">ç¬¬ä¸€ä¸ªæŒ‡æ ‡ä»£è¡¨æ­£è¯é¢‘</li><li id="8817" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">ç¬¬äºŒä¸ªæŒ‡æ•°ä»£è¡¨è´Ÿè¯é¢‘</li></ul><h2 id="644b" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.å®æ–½é€»è¾‘å›å½’</h2><h2 id="075a" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.1 æ¦‚è¿°</h2><p id="35a3" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹é€»è¾‘å›å½’æ˜¯å¦‚ä½•å·¥ä½œå’Œå®ç°çš„ã€‚</p><p id="8f5d" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">å¾ˆå¤šæ—¶å€™ï¼Œå½“ä½ å¬åˆ°é€»è¾‘å›å½’æ—¶ï¼Œä½ å¯èƒ½ä¼šæƒ³ï¼Œè¿™æ˜¯ä¸€ä¸ªå›å½’é—®é¢˜ã€‚ä¸ï¼Œä¸æ˜¯ï¼Œ<strong class="ls iu"> Logistic å›å½’</strong>æ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œæ˜¯ä¸€ä¸ªéçº¿æ€§æ¨¡å‹ã€‚</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oc"><img src="../Images/c002a89ec693b84600d4afa6d86ea2fb.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Sc2sm6O6Uu_Xp8zayn3qDA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">ç”±ä½œè€…åˆ›å»º</p></figure><p id="0d24" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¤§å¤šæ•°æœ€å¤§ä¼¼ç„¶ç®—æ³•æœ‰ 4 ä¸ªé˜¶æ®µï¼Œ</p><p id="badb" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">ç¬¬ä¸€æ­¥ã€‚åˆå§‹åŒ–æƒé‡</p><ul class=""><li id="0d48" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">éšæœºæƒé‡å·²åˆå§‹åŒ–</li></ul><p id="5001" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">ç¬¬äºŒæ­¥ã€‚åº”ç”¨åŠŸèƒ½</p><ul class=""><li id="72c8" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">è®¡ç®—ä¹™çŠ¶ç»“è‚ </li></ul><p id="7f5e" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">ç¬¬ä¸‰æ­¥ã€‚è®¡ç®—æˆæœ¬(ç®—æ³•çš„ç›®æ ‡)</p><ul class=""><li id="735f" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">è®¡ç®—äºŒå…ƒåˆ†ç±»çš„å¯¹æ•°æŸå¤±</li></ul><p id="10ca" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">ç¬¬å››æ­¥ã€‚æ¢¯åº¦ä¸‹é™</p><ul class=""><li id="1428" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">è¿­ä»£æ›´æ–°æƒé‡ï¼Œç›´åˆ°æ‰¾åˆ°æœ€å°æˆæœ¬</li></ul><p id="ae5b" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">é€»è¾‘å›å½’é‡‡ç”¨çº¿æ€§å›å½’ï¼Œå¹¶å°†<strong class="ls iu"> sigmoid </strong>åº”ç”¨äºçº¿æ€§å›å½’çš„è¾“å‡ºã€‚å› æ­¤ï¼Œå®ƒäº§ç”Ÿäº†æ¯ä¸€ç±»çš„æ¦‚ç‡ï¼Œå…¶æ€»å’Œä¸º 1ã€‚</p><p id="e9ee" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">å›å½’:</strong>ä¸€å…ƒçº¿æ€§å›å½’æ–¹ç¨‹å¦‚ä¸‹:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/3c68572ec470c6a18f6bc68eb88b4aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*4ykq1Pgq2-jYleFmOcYCTw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">å•å˜é‡çº¿æ€§å›å½’å…¬å¼</p></figure><ul class=""><li id="6ae8" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">æ³¨æ„ï¼Œ<strong class="ls iu">Î¸</strong>å€¼æ˜¯<strong class="ls iu"> <em class="nl">æƒé‡</em> </strong></li><li id="8f99" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated"><strong class="ls iu"> x_0ï¼Œx_1ï¼Œx_2ï¼Œâ€¦ x_N </strong>æ˜¯è¾“å…¥ç‰¹å¾</li></ul><p id="97f4" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">ä½ å¯èƒ½ä¼šæƒ³åˆ°è¿™ä¸ªæ–¹ç¨‹æœ‰å¤šå¤æ‚ã€‚æˆ‘ä»¬éœ€è¦å°†åœ¨<code class="fe nn no np nd b">ith</code>ä½ç½®çš„æ¯ä¸ªç‰¹å¾çš„æ‰€æœ‰æƒé‡ç›¸ä¹˜ï¼Œç„¶åæ±‚å’Œã€‚</p><blockquote class="nv nw nx"><p id="62ce" class="lq lr nl ls b lt mj ju lv lw mk jx ly ny ml ma mb nz mm md me oa mn mg mh mi im bi translated">å¥½åœ¨<strong class="ls iu">çº¿æ€§ä»£æ•°</strong>å¸¦æ¥äº†è¿™ä¸ªæ˜“æ“ä½œçš„æ–¹ç¨‹ã€‚æ²¡é”™ï¼Œå°±æ˜¯çŸ©é˜µ<code class="fe nn no np nd b"><em class="it">dot</em></code>äº§å“ã€‚æ‚¨å¯ä»¥åº”ç”¨ç‰¹å¾å’Œæƒé‡çš„ç‚¹ç§¯æ¥æ‰¾åˆ°<strong class="ls iu"> z </strong>ã€‚</p></blockquote><h2 id="c5be" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.2 ä¹™çŠ¶ç»“è‚ </h2><ul class=""><li id="84c6" class="mo mp it ls b lt lu lw lx ld ns lh nt ll nu mi mt mu mv mw bi translated">sigmoid å‡½æ•°å®šä¹‰ä¸º:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/9f593c09ac0f3d74c4032153436bf378.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*r-MWZtm-hdhg4lKQlr06OQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae nm" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">ä¹™çŠ¶ç»“è‚ åŠŸèƒ½</a></p></figure><p id="d25f" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">å®ƒå°†è¾“å…¥â€œzâ€æ˜ å°„åˆ°ä¸€ä¸ªä»‹äº 0 å’Œ 1 ä¹‹é—´çš„å€¼ï¼Œå› æ­¤å®ƒå¯ä»¥è¢«è§†ä¸ºä¸€ä¸ª<strong class="ls iu">æ¦‚ç‡</strong>ã€‚</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="fafe" class="ku kv it nd b gy nh ni l nj nk">def sigmoid(z): <br/>    <br/>    # calculate the sigmoid of z<br/>    h = 1 / (1+ np.exp(-z))<br/>    <br/>    return h</span></pre><h2 id="e768" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.3 æˆæœ¬å‡½æ•°</h2><p id="65bf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">é€»è¾‘å›å½’ä¸­ä½¿ç”¨çš„æˆæœ¬å‡½æ•°æ˜¯:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/ba3e27988e6d279e37bc4f82cd5bdaea.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*daYALvNJcsso7I3XgdI79Q.png"/></div></figure><p id="b2fd" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">è¿™å°±æ˜¯äºŒè¿›åˆ¶åˆ†ç±»çš„<strong class="ls iu">æµ‹äº•æŸå¤±ã€‚</strong>åœ¨é€»è¾‘å›å½’ä¸­è®¡ç®—æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„å¯¹æ•°æŸå¤±çš„å¹³å‡å€¼ï¼Œå¯¹æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„ç­‰å¼<strong class="ls iu"> <em class="nl"> 3 </em> </strong>ä¿®æ”¹å¦‚ä¸‹:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/9d019eb4e9d17942b1ef9fa78acf4fd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*N1cdGpAxyfnge4Go-x9RVw.png"/></div></figure><ul class=""><li id="8eeb" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated"><strong class="ls iu"> <em class="nl"> m </em> </strong>æ˜¯è®­ç»ƒæ ·æœ¬çš„æ•°é‡</li><li id="c3d6" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated"><strong class="ls iu"><em class="nl"/></strong>æ˜¯<strong class="ls iu"> <em class="nl">ä¸</em> </strong>è®­ç»ƒå®ä¾‹çš„å®é™…æ ‡ç­¾ã€‚</li><li id="5671" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated"><strong class="ls iu"><em class="nl">ã€h(z(\theta)^{(i)}ã€‘</em></strong>ä¸º<strong class="ls iu"> <em class="nl">ä¸</em> </strong>è®­ç»ƒæ ·æœ¬çš„æ¨¡å‹é¢„æµ‹ã€‚</li></ul><p id="eaf9" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">å•ä¸ªè®­ç»ƒç¤ºä¾‹çš„æŸå¤±å‡½æ•°æ˜¯ï¼Œ</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7185f6f9710d6f7fd3afeb667b4c2b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*65aScoEzsRWLVXskfMObgA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">æŸå¤±å‡½æ•°</p></figure><ul class=""><li id="96c2" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">æ‰€æœ‰çš„<strong class="ls iu"> <em class="nl"> h </em> </strong> <em class="nl"> </em>çš„å€¼éƒ½åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œæ‰€ä»¥æ—¥å¿—ä¼šæ˜¯è´Ÿæ•°ã€‚è¿™å°±æ˜¯å°†ç³»æ•°-1 åº”ç”¨äºä¸¤ä¸ªæŸå¤±é¡¹ä¹‹å’Œçš„åŸå› ã€‚</li><li id="26ac" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">å½“æ¨¡å‹é¢„æµ‹ 1ï¼Œ(<em class="nl">h</em>(<em class="nl">z</em>(<em class="nl">Î¸</em>))= 1)ä¸”æ ‡ç­¾<strong class="ls iu"> <em class="nl"> y </em> </strong> <em class="nl"> </em>ä¹Ÿä¸º 1 æ—¶ï¼Œè¯¥è®­ç»ƒç¤ºä¾‹çš„æŸå¤±ä¸º 0ã€‚</li><li id="88f2" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">åŒæ ·ï¼Œå½“æ¨¡å‹é¢„æµ‹ä¸º 0ï¼Œ(<em class="nl">h</em>(<em class="nl">z</em>(<em class="nl">Î¸</em>))= 0ï¼Œè€Œå®é™…æ ‡ç­¾ä¹Ÿä¸º 0 æ—¶ï¼Œè¯¥è®­ç»ƒç¤ºä¾‹çš„æŸå¤±ä¸º 0ã€‚</li><li id="e89e" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">ä½†å½“æ¨¡å‹é¢„æµ‹æ¥è¿‘ 1(<em class="nl">h</em>(<em class="nl">z</em>(<em class="nl">Î¸</em>))= 0.9999)ä¸”æ ‡å·ä¸º 0 æ—¶ï¼Œå¯¹æ•°æŸå¤±çš„ç¬¬äºŒé¡¹å˜æˆä¸€ä¸ªå¾ˆå¤§çš„è´Ÿæ•°ï¼Œå†ä¹˜ä»¥-1 çš„æ€»å› å­ï¼Œè½¬æ¢æˆæ­£çš„æŸå¤±å€¼ã€‚1Ã—(1 0)Ã—<em class="nl">log</em>(1 0.9999)â‰ˆ9.2 æ¨¡å‹é¢„æµ‹è¶Šæ¥è¿‘ 1ï¼ŒæŸè€—è¶Šå¤§ã€‚</li></ul><h2 id="1b7f" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.4 æ¢¯åº¦ä¸‹é™</h2><p id="422d" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§ç”¨äº<strong class="ls iu">è¿­ä»£æ›´æ–°æƒé‡<em class="nl">Î¸</em>T51ã€‘ä»¥æœ€å°åŒ–ç›®æ ‡å‡½æ•°(æˆæœ¬)çš„ç®—æ³•ã€‚æˆ‘ä»¬éœ€è¦è¿­ä»£åœ°æ›´æ–°æƒé‡ï¼Œå› ä¸ºï¼Œ</strong></p><blockquote class="nv nw nx"><p id="8626" class="lq lr nl ls b lt mj ju lv lw mk jx ly ny ml ma mb nz mm md me oa mn mg mh mi im bi translated">åœ¨åˆå§‹éšæœºæƒé‡ä¸‹ï¼Œæ¨¡å‹ä¸ä¼šå­¦åˆ°å¤ªå¤šä¸œè¥¿ã€‚ä¸ºäº†æ”¹è¿›é¢„æµ‹ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡å¤šæ¬¡è¿­ä»£ä»æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´éšæœºæƒé‡ã€‚</p></blockquote><p id="9fe2" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">å¯¹äºæƒé‡ä¹‹ä¸€<strong class="ls iu"><em class="nl">Î¸_ J</em></strong>çš„æˆæœ¬å‡½æ•°<strong class="ls iu"> <em class="nl"> J </em> </strong>çš„æ¢¯åº¦æ˜¯:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/8e7b378da360008deb2a9c6bbf42323c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*4rGEAPDAEwAuqRIUoyYeBA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">æ¢¯åº¦å‡½æ•°</p></figure><h2 id="60bd" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.5 æ­£è§„åŒ–</h2><p id="ba73" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">æ­£åˆ™åŒ–æ˜¯ä¸€ç§é€šè¿‡æƒ©ç½šæˆæœ¬å‡½æ•°æ¥è§£å†³æœºå™¨å­¦ä¹ ç®—æ³•ä¸­è¿‡æ‹Ÿåˆé—®é¢˜çš„æŠ€æœ¯ã€‚åœ¨æˆæœ¬å‡½æ•°ä¸­ä¼šæœ‰ä¸€ä¸ªé™„åŠ çš„æƒ©ç½šé¡¹ã€‚æœ‰ä¸¤ç§ç±»å‹çš„æ­£åˆ™åŒ–æŠ€æœ¯:</p><ul class=""><li id="c746" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">æ‹‰ç´¢(L1 èŒƒæ•°)æ­£åˆ™åŒ–</li><li id="31ed" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">å²­(L2 èŒƒæ•°)æ­£åˆ™åŒ–</li></ul><p id="ce9d" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">æ‹‰ç´¢å›å½’(L1)</strong>L1-èŒƒæ•°æŸå¤±å‡½æ•°ä¹Ÿè¢«ç§°ä¸ºæœ€å°ç»å¯¹è¯¯å·®(LAE)ã€‚$Î»*âˆ‘ |w| $æ˜¯ä¸€ä¸ªæ­£åˆ™é¡¹ã€‚å®ƒæ˜¯$Î»$æ­£åˆ™åŒ–é¡¹ä¸æƒçš„ç»å¯¹å’Œçš„ä¹˜ç§¯ã€‚è¾ƒå°çš„å€¼è¡¨ç¤ºè¾ƒå¼ºçš„æ­£åˆ™åŒ–ã€‚</p><p id="b704" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">å²­å›å½’(L2)</strong>L2-èŒƒæ•°æŸå¤±å‡½æ•°ä¹Ÿç§°ä¸ºæœ€å°äºŒä¹˜è¯¯å·®(LSE)ã€‚$Î»*âˆ‘ (w) $æ˜¯ä¸€ä¸ªæ­£åˆ™é¡¹ã€‚å®ƒæ˜¯$Î»$æ­£åˆ™åŒ–é¡¹ä¸æƒçš„å¹³æ–¹å’Œçš„ä¹˜ç§¯ã€‚è¾ƒå°çš„å€¼è¡¨ç¤ºè¾ƒå¼ºçš„æ­£åˆ™åŒ–ã€‚</p><p id="d107" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">ä½ ä¼šæ³¨æ„åˆ°ï¼Œè¿™æœ‰å¾ˆå¤§çš„ä¸åŒã€‚æ˜¯çš„ï¼Œå®ƒåšå¾—å¾ˆå¥½ã€‚ä¸»è¦çš„åŒºåˆ«åœ¨äºä½ åœ¨æˆæœ¬å‡½æ•°ä¸­åŠ å…¥äº†ä»€ä¹ˆç±»å‹çš„æ­£åˆ™é¡¹æ¥æœ€å°åŒ–è¯¯å·®ã€‚</p><blockquote class="nv nw nx"><p id="8523" class="lq lr nl ls b lt mj ju lv lw mk jx ly ny ml ma mb nz mm md me oa mn mg mh mi im bi translated">L2(å²­)ç¼©å°æ‰€æœ‰ç³»æ•°ç›¸åŒçš„æ¯”ä¾‹ï¼Œä½†å®ƒä¸æ¶ˆé™¤ä»»ä½•ç‰¹å¾ï¼Œè€Œ L1(æ‹‰ç´¢)å¯ä»¥ç¼©å°ä¸€äº›ç³»æ•°ä¸ºé›¶ï¼Œä¹Ÿæ‰§è¡Œç‰¹å¾é€‰æ‹©ã€‚</p></blockquote><p id="9379" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">åœ¨ä¸‹é¢çš„ä»£ç ä¸­å°†æ·»åŠ  L2 æ­£åˆ™åŒ–</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="63c6" class="ku kv it nd b gy nh ni l nj nk"># implementation of gradient descent algorithm  </span><span id="9226" class="ku kv it nd b gy nq ni l nj nk">def gradientDescent(x, y, theta, alpha, num_iters, c):</span><span id="457f" class="ku kv it nd b gy nq ni l nj nk">    # get the number of samples in the training<br/>    m = x.shape[0]<br/>    <br/>    for i in range(0, num_iters):<br/>        <br/>        # find linear regression equation value, X and theta<br/>        z = np.dot(x, theta)<br/>        <br/>        # get the sigmoid of z<br/>        h = sigmoid(z)<br/> <br/>        # calculate the cost function, log loss<br/>        #J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1-h)))<br/>        <br/>        # let's add L2 regularization<br/>        # c is L2 regularizer term<br/>        J = (-1/m) * ((np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1-h))) + (c * np.sum(theta)))<br/>        <br/>        # update the weights theta<br/>        theta = theta - (alpha / m) * np.dot((x.T), (h - y))<br/>   <br/>    J = float(J)<br/>    return J, theta</span></pre><h2 id="ea55" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">5.ç«è½¦æ¨¡å‹</h2><p id="0cfd" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">è®©æˆ‘ä»¬è®­ç»ƒæ¢¯åº¦ä¸‹é™å‡½æ•°æ¥ä¼˜åŒ–éšæœºåˆå§‹åŒ–çš„æƒé‡ã€‚åœ¨ç¬¬ 4 èŠ‚ä¸­å·²ç»ç»™å‡ºäº†ç®€è¦çš„è§£é‡Šã€‚</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="3068" class="ku kv it nd b gy nh ni l nj nk"># set the seed in numpy<br/>np.random.seed(1)<br/># Apply gradient descent of logistic regression<br/># 0.1 as added L2 regularization term<br/>J, theta = gradientDescent(train_X, np.array(train_Y).reshape(-1,1), np.zeros((3, 1)), 1e-7, 1000, 0.1)<br/>print(f"The cost after training is {J:.8f}.")<br/>print(f"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}")</span><span id="4e19" class="ku kv it nd b gy nq ni l nj nk">The cost after training is 0.22154867.<br/>The resulting vector of weights is [2.18e-06, 0.00270863, -0.00177371]</span></pre><h2 id="d9fe" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">6.æµ‹è¯•æˆ‘ä»¬çš„é€»è¾‘å›å½’</h2><p id="9e8c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">æ˜¯æ—¶å€™åœ¨æ¨¡å‹ä¹‹å‰æ²¡æœ‰è§è¿‡çš„æµ‹è¯•æ•°æ®ä¸Šæµ‹è¯•æˆ‘ä»¬çš„é€»è¾‘å›å½’å‡½æ•°äº†ã€‚</p><p id="90b0" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">é¢„æµ‹ä¸€æ¡æ¨æ–‡æ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ã€‚</p><ul class=""><li id="ab7a" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">å°† sigmoid åº”ç”¨äº logits ä»¥è·å¾—é¢„æµ‹å€¼(ä»‹äº 0 å’Œ 1 ä¹‹é—´çš„å€¼)ã€‚</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/07d5dbb24323cf817ef18d3673875aef.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*U4CemTnkbcs526uYCcxT3A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">æ–°æ¨æ–‡é¢„æµ‹</p></figure><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="3d36" class="ku kv it nd b gy nh ni l nj nk"># predict for the features from learned theata values<br/>def predict_tweet(x, theta):<br/>    <br/>    # make the prediction for x with learned theta values<br/>    y_pred = sigmoid(np.dot(x, theta))<br/>    <br/>    return y_pred</span><span id="537f" class="ku kv it nd b gy nq ni l nj nk"># predict for the test sample with the learned weights for logistics regression<br/>predicted_probs = predict_tweet(test_X, theta)<br/># assign the probability threshold to class<br/>predicted_labels = np.where(predicted_probs &gt; 0.5, 1, 0)<br/># calculate the accuracy<br/>print(f"Own implementation of logistic regression accuracy is {len(predicted_labels[predicted_labels == np.array(test_Y).reshape(-1,1)]) / len(test_Y)*100:.2f}")</span><span id="415c" class="ku kv it nd b gy nq ni l nj nk">Own implementation of logistic regression accuracy is 93.45</span></pre><p id="6e10" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•è‡ªå·±å®ç°é€»è¾‘å›å½’ã€‚å¾—åˆ°äº†<strong class="ls iu"> 94.45 çš„ç²¾åº¦ã€‚</strong>è®©æˆ‘ä»¬çœ‹çœ‹æ¥è‡ªæµè¡Œçš„æœºå™¨å­¦ä¹ (ML) Python åº“çš„ç»“æœã€‚</p><h2 id="5e28" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">7.ç”¨ Scikit å­¦ä¹ é€»è¾‘å›å½’æµ‹è¯•</h2><p id="7f51" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">è¿™é‡Œï¼Œæˆ‘ä»¬å°†è®­ç»ƒå†…ç½® Python åº“ä¸­çš„é€»è¾‘å›å½’æ¥æ£€æŸ¥ç»“æœã€‚</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="3257" class="ku kv it nd b gy nh ni l nj nk"># scikit learn logiticsregression and accuracy score metric<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import accuracy_score<br/>clf = LogisticRegression(random_state=42, penalty='l2')<br/>clf.fit(train_X, np.array(train_Y).reshape(-1,1))<br/>y_pred = clf.predict(test_X)</span><span id="473b" class="ku kv it nd b gy nq ni l nj nk">/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().<br/>  return f(**kwargs)</span><span id="fbc6" class="ku kv it nd b gy nq ni l nj nk">print(f"Scikit learn logistic regression accuracy is {accuracy_score(test_Y , y_pred)*100:.2f}")</span><span id="4b46" class="ku kv it nd b gy nq ni l nj nk">Scikit learn logistic regression accuracy is 94.45</span></pre><p id="b3f9" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">å¤ªå¥½äº†ï¼ï¼ï¼ã€‚ç»“æœéå¸¸æ¥è¿‘ã€‚</p><p id="6e02" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">æœ€åï¼Œæˆ‘ä»¬è‡ªå·±å®ç°äº†é€»è¾‘å›å½’ï¼Œå¹¶å°è¯•ä½¿ç”¨å†…ç½®çš„ Scikit learn é€»è¾‘å›å½’æ¥è·å¾—ç±»ä¼¼çš„å‡†ç¡®æ€§ã€‚ä½†æ˜¯ï¼Œè¿™ç§ç‰¹å¾æå–çš„æ–¹æ³•éå¸¸ç®€å•å’Œç›´è§‚ã€‚</p><p id="e873" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu"> <em class="nl">æˆ‘æ˜¯è¾¹åšè¾¹å­¦ã€‚</em> </strong> <em class="nl">æ¬¢è¿åœ¨è¯„è®ºä¸­ç•™ä¸‹æ‚¨çš„æƒ³æ³•æˆ–ä»»ä½•å»ºè®®ã€‚éå¸¸æ„Ÿè°¢ä½ çš„åé¦ˆï¼Œå®ƒèƒ½å¢å¼ºæˆ‘çš„ä¿¡å¿ƒã€‚</em></p><p id="e9f7" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">ğŸ™æ„Ÿè°¢é˜…è¯»ï¼ä½ å¯ä»¥é€šè¿‡ LinkedIn è”ç³»æˆ‘ã€‚</p></div></div>    
</body>
</html>