<html>
<head>
<title>Understand the Logistic Regression from Scratch — Kaggle Notebook</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始理解逻辑回归— Kaggle 笔记本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-logistic-regression-from-scratch-430aedf5edb9?source=collection_archive---------28-----------------------#2020-09-10">https://towardsdatascience.com/understand-logistic-regression-from-scratch-430aedf5edb9?source=collection_archive---------28-----------------------#2020-09-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8c1a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过自己实现来学习算法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/b878d7dc19be08af5fc211781fca42f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*hlJ4VjDAYT32-1qSgDPinA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">由作者创建</p></figure><h2 id="d30d" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">目录</h2><p id="379f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">1.目标</p><p id="65c5" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">2.加载数据</p><p id="0b59" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">3.从文本中提取特征</p><p id="bf14" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">4.实施逻辑回归</p><ul class=""><li id="d3b8" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">4.1 概述</li><li id="bca7" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">4.2 乙状结肠</li><li id="9e81" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">4.3 成本函数</li><li id="eb80" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">4.4 梯度下降</li><li id="9181" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">4.5 正规化</li></ul><p id="b3c1" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">5.火车模型</p><p id="0b64" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">6.测试我们的逻辑回归</p><p id="52f2" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">7.用 Scikit 学习逻辑回归测试</p><p id="577d" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">让我们用 Python 导入所有必要的模块。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="1767" class="ku kv it nd b gy nh ni l nj nk"># regular expression operations<br/>import re    <br/># string operation <br/>import string  <br/># shuffle the list<br/>from random import shuffle<br/><br/># linear algebra<br/>import numpy as np <br/># data processing<br/>import pandas as pd <br/><br/># NLP library<br/>import nltk<br/># download twitter dataset<br/>from nltk.corpus import twitter_samples                          <br/><br/># module for stop words that come with NLTK<br/>from nltk.corpus import stopwords          <br/># module for stemming<br/>from nltk.stem import PorterStemmer        <br/># module for tokenizing strings<br/>from nltk.tokenize import TweetTokenizer   <br/><br/># scikit model selection<br/>from sklearn.model_selection import train_test_split<br/><br/># smart progressor meter<br/>from tqdm import tqdm</span></pre><h2 id="d7c5" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">1.目标</h2><p id="29e9" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">这个内核的目标是使用 twitter 数据集从零开始实现用于情感分析的逻辑回归。我们将主要关注逻辑回归的构建模块。这个内核可以提供对<strong class="ls iu"> <em class="nl">内部如何进行逻辑回归</em> </strong>的深入理解。使用<a class="ae nm" href="https://pypi.org/project/jupyter-to-medium/" rel="noopener ugc nofollow" target="_blank"> JupytertoMedium </a> python 库将笔记本转换成中型文章。Kaggle 笔记本可从<a class="ae nm" href="https://www.kaggle.com/narendrageek/understand-the-logistic-regression-from-scratch" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="1f9f" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">给定一条推文，如果它有<strong class="ls iu">正面情绪，它将被分类👍或者消极情绪👎</strong>。这对初学者和其他人都很有用。</p><h2 id="2bf2" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">2.加载数据</h2><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="b312" class="ku kv it nd b gy nh ni l nj nk"># Download the twitter sample data from NLTK repository<br/>nltk.download('twitter_samples')</span></pre><ul class=""><li id="3d78" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated"><code class="fe nn no np nd b">twitter_samples</code>包含 5000 条正面推文和 5000 条负面推文。总共有 10，000 条推文。</li><li id="827d" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">我们每个班都有相同数量的数据样本。</li><li id="ee8e" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">这是一个平衡的数据集。</li></ul><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="bd4e" class="ku kv it nd b gy nh ni l nj nk"># read the positive and negative tweets<br/>pos_tweets = twitter_samples.strings('positive_tweets.json')<br/>neg_tweets = twitter_samples.strings('negative_tweets.json')<br/>print(f"positive sentiment 👍 total samples {len(pos_tweets)} \nnegative sentiment 👎 total samples {len(neg_tweets)}")</span><span id="1432" class="ku kv it nd b gy nq ni l nj nk">positive sentiment 👍 total samples 5000 <br/>negative sentiment 👎 total samples 5000</span><span id="9e58" class="ku kv it nd b gy nq ni l nj nk"># Let's have a look at the data<br/>no_of_tweets = 3<br/>print(f"Let's take a look at first {no_of_tweets} sample tweets:\n")<br/>print("Example of Positive tweets:")<br/>print('\n'.join(pos_tweets[:no_of_tweets]))<br/>print("\nExample of Negative tweets:")<br/>print('\n'.join(neg_tweets[:no_of_tweets]))</span><span id="fff9" class="ku kv it nd b gy nq ni l nj nk">Let's take a look at first 3 sample tweets:</span></pre><p id="088f" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">输出:</strong></p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="4e2c" class="ku kv it nd b gy nh ni l nj nk">Example of Positive tweets:<br/>#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)<br/>@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!<br/>@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!<br/><br/>Example of Negative tweets:<br/>hopeless for tmr :(<br/>Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(<br/>@Hegelbon That heart sliding into the waste basket. :(</span></pre><ul class=""><li id="26bf" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">推文可能包含 URL、数字和特殊字符。因此，我们需要对文本进行预处理。</li></ul><h2 id="e19b" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">预处理文本</h2><p id="ba77" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">预处理是流水线中的重要步骤之一。它包括在建立机器学习模型之前清理和删除不必要的数据。</p><p id="ea0b" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">预处理步骤:</p><ol class=""><li id="cdce" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi nr mu mv mw bi translated">对字符串进行标记</li><li id="1e62" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi nr mu mv mw bi translated">将 tweet 转换成小写，并将 tweet 拆分成令牌(单词)</li><li id="c1c3" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi nr mu mv mw bi translated">删除停用字词和标点符号</li><li id="d792" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi nr mu mv mw bi translated">删除 twitter 平台上的常用词，如标签、转发标记、超链接、数字和电子邮件地址</li><li id="d23b" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi nr mu mv mw bi translated">堵塞物</li></ol><ul class=""><li id="63ef" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">这是把一个单词转换成它最普通形式的过程。它有助于减少我们的词汇量。例如，engage 这个词有不同的词干，</li><li id="b227" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated"><strong class="ls iu">订婚</strong></li><li id="354f" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">订婚的</li><li id="0005" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">订婚</li></ul><p id="4962" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">让我们看看如何实现这一点。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="2624" class="ku kv it nd b gy nh ni l nj nk"># helper class for doing preprocessing<br/>class Twitter_Preprocess():<br/>    <br/>    def __init__(self):<br/>        # instantiate tokenizer class<br/>        self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,<br/>                                       reduce_len=True)<br/>        # get the english stopwords <br/>        self.stopwords_en = stopwords.words('english') <br/>        # get the english punctuation<br/>        self.punctuation_en = string.punctuation<br/>        # Instantiate stemmer object<br/>        self.stemmer = PorterStemmer() <br/>        <br/>    def __remove_unwanted_characters__(self, tweet):<br/>        <br/>        # remove retweet style text "RT"<br/>        tweet = re.sub(r'^RT[\s]+', '', tweet)<br/><br/>        # remove hyperlinks<br/>        tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)<br/>     <br/>        # remove hashtags<br/>        tweet = re.sub(r'#', '', tweet)<br/>        <br/>        #remove email address<br/>        tweet = re.sub('\S+@\S+', '', tweet)<br/>        <br/>        # remove numbers<br/>        tweet = re.sub(r'\d+', '', tweet)<br/>        <br/>        ## return removed text<br/>        return tweet<br/>    <br/>    def __tokenize_tweet__(self, tweet):        <br/>        # tokenize tweets<br/>        return self.tokenizer.tokenize(tweet)<br/>    <br/>    def __remove_stopwords__(self, tweet_tokens):<br/>        # remove stopwords<br/>        tweets_clean = []<br/><br/>        for word in tweet_tokens:<br/>            if (word not in self.stopwords_en and  # remove stopwords<br/>                word not in self.punctuation_en):  # remove punctuation<br/>                tweets_clean.append(word)<br/>        return tweets_clean<br/>    <br/>    def __text_stemming__(self,tweet_tokens):<br/>        # store the stemmed word<br/>        tweets_stem = [] <br/><br/>        for word in tweet_tokens:<br/>            # stemming word<br/>            stem_word = self.stemmer.stem(word)  <br/>            tweets_stem.append(stem_word)<br/>        return tweets_stem<br/>    <br/>    def preprocess(self, tweets):<br/>        tweets_processed = []<br/>        for _, tweet in tqdm(enumerate(tweets)):        <br/>            # apply removing unwated characters and remove style of retweet, URL<br/>            tweet = self.__remove_unwanted_characters__(tweet)            <br/>            # apply nltk tokenizer<br/>/            tweet_tokens = self.__tokenize_tweet__(tweet)            <br/>            # apply stop words removal<br/>            tweet_clean = self.__remove_stopwords__(tweet_tokens)<br/>            # apply stemmer <br/>            tweet_stems = self.__text_stemming__(tweet_clean)<br/>            tweets_processed.extend([tweet_stems])<br/>        return tweets_processed</span><span id="2a0b" class="ku kv it nd b gy nq ni l nj nk"># initilize the text preprocessor class object<br/>twitter_text_processor = Twitter_Preprocess()<br/><br/># process the positive and negative tweets<br/>processed_pos_tweets = twitter_text_processor.preprocess(pos_tweets)<br/>processed_neg_tweets = twitter_text_processor.preprocess(neg_tweets)</span><span id="50c7" class="ku kv it nd b gy nq ni l nj nk">5000it [00:02, 2276.81it/s]<br/>5000it [00:02, 2409.93it/s]</span></pre><p id="54e0" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">让我们看看预处理 tweets 后得到了什么输出。我们能够成功处理推文，这很好。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="06c1" class="ku kv it nd b gy nh ni l nj nk">pos_tweets[:no_of_tweets], processed_pos_tweets[:no_of_tweets]</span><span id="cc70" class="ku kv it nd b gy nq ni l nj nk">(['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',<br/>  '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',<br/>  '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!'],<br/> [['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)'],<br/>  ['hey',<br/>   'jame',<br/>   'odd',<br/>   ':/',<br/>   'pleas',<br/>   'call',<br/>   'contact',<br/>   'centr',<br/>   'abl',<br/>   'assist',<br/>   ':)',<br/>   'mani',<br/>   'thank'],<br/>  ['listen', 'last', 'night', ':)', 'bleed', 'amaz', 'track', 'scotland']])</span></pre><h2 id="c87f" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">3.从文本中提取特征</h2><ul class=""><li id="4c27" class="mo mp it ls b lt lu lw lx ld ns lh nt ll nu mi mt mu mv mw bi translated">给定文本，以这样一种方式表示<code class="fe nn no np nd b">features (numeric values)</code>是非常重要的，这样我们就可以输入到模型中。</li></ul><h2 id="01ea" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">3.1 创建一个单词包(BOW)表示法</h2><p id="dcdf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">BOW 代表单词及其在每个类中的出现频率。我们将创建一个<code class="fe nn no np nd b">dict</code>来存储每个单词的<code class="fe nn no np nd b">positive</code>和<code class="fe nn no np nd b">negative</code>类的频率。让我们指出一条<code class="fe nn no np nd b">positive</code>推文是<code class="fe nn no np nd b">1</code>，而<code class="fe nn no np nd b">negative</code>推文是<code class="fe nn no np nd b">0</code>。<code class="fe nn no np nd b">dict</code>键是一个包含<code class="fe nn no np nd b">(word, y)</code>对的元组。<code class="fe nn no np nd b">word</code>是处理过的字，<code class="fe nn no np nd b">y</code>表示类的标签。dict 值代表类<code class="fe nn no np nd b">y</code>的<code class="fe nn no np nd b">frequency of the word</code>。</p><p id="be81" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">示例:#单词 bad 在 0(负)类中出现 45 次{(“bad”，0) : 32}</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="e4bb" class="ku kv it nd b gy nh ni l nj nk"># word bad occurs 45 time in the 0 (negative) class <br/>{("bad", 0) : 45}</span><span id="cb3a" class="ku kv it nd b gy nq ni l nj nk"># BOW frequency represent the (word, y) and frequency of y class<br/>def build_bow_dict(tweets, labels):<br/>    freq = {}<br/>    ## create zip of tweets and labels<br/>    for tweet, label in list(zip(tweets, labels)):<br/>        for word in tweet:<br/>            freq[(word, label)] = freq.get((word, label), 0) + 1<br/>        <br/>    return freq</span><span id="5224" class="ku kv it nd b gy nq ni l nj nk"># create labels of the tweets<br/># 1 for positive labels and 0 for negative labels<br/>labels = [1 for i in range(len(processed_pos_tweets))]<br/>labels.extend([0 for i in range(len(processed_neg_tweets))])<br/><br/># combine the positive and negative tweets<br/>twitter_processed_corpus = processed_pos_tweets + processed_neg_tweets<br/><br/># build Bog of words frequency <br/>bow_word_frequency = build_bow_dict(twitter_processed_corpus, labels)</span></pre><p id="e1f8" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">现在，我们有各种方法来表示 twitter 语料库的特征。一些基本而强大的技术是，</p><ul class=""><li id="6bf5" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">计数矢量器</li><li id="796a" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">TF-IDF 功能</li></ul><h2 id="2fcf" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">1.计数矢量器</h2><p id="dcb1" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">计数矢量器指示稀疏矩阵，并且该值可以是单词的频率<strong class="ls iu">。在我们的语料库中，每一列都是唯一的标记。</strong></p><blockquote class="nv nw nx"><p id="8e79" class="lq lr nl ls b lt mj ju lv lw mk jx ly ny ml ma mb nz mm md me oa mn mg mh mi im bi translated">稀疏矩阵的维数将是<code class="fe nn no np nd b"><em class="it">no of unique tokens in the corpus * no of sample tweets</em></code>。</p></blockquote><p id="f747" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">示例:<code class="fe nn no np nd b">corpus = [ 'This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?', ]</code>并且 CountVectorizer 表示为</p><p id="ea3a" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><code class="fe nn no np nd b">[[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]]</code></p><h2 id="836b" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">2.TF-IDF(术语频率-逆文档频率)</h2><p id="6181" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">TF-IDF 统计度量，用于评估单词与文档集合中的文档的相关程度。TF-IDF 的计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/68e4884c6d2e08c0027b49b284d0f7db.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*Q0HEaKlRsLqEztzxTBidCA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">TF-IDF 方程</p></figure><p id="af77" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">词频:</strong>词频<strong class="ls iu"> <em class="nl"> tf(t，d) </em> </strong>，最简单的选择就是使用一个词(词)在文档中的出现频率。<strong class="ls iu">逆文档频率:</strong> <strong class="ls iu"> <em class="nl"> idf(t，D) </em> </strong>衡量单词提供多少信息，即它在所有文档中是常见还是罕见。它是包含该单词的文档的逆分数的<strong class="ls iu">对数标度</strong>。定义见<a class="ae nm" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">维基</a>。</p><h2 id="dc3a" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">3.2.为我们的模型提取简单特征</h2><ul class=""><li id="60ae" class="mo mp it ls b lt lu lw lx ld ns lh nt ll nu mi mt mu mv mw bi translated">给定一个推文列表，我们将提取两个特征。</li><li id="79f1" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">第一个特征是一条推文中正面词的数量。</li><li id="dcaa" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">第二个特征是推文中负面词的数量。</li></ul><p id="b361" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">这看似简单，不是吗？也许是的。我们没有向稀疏矩阵表示我们的特征。将使用最简单的特征进行分析。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="b3c0" class="ku kv it nd b gy nh ni l nj nk"># extract feature for tweet<br/>def extract_features(processed_tweet, bow_word_frequency):<br/>    # feature array<br/>    features = np.zeros((1,3))<br/>    # bias term added in the 0th index<br/>    features[0,0] = 1<br/>    <br/>    # iterate processed_tweet<br/>    for word in processed_tweet:<br/>        # get the positive frequency of the word<br/>        features[0,1] = bow_word_frequency.get((word, 1), 0)<br/>        # get the negative frequency of the word<br/>        features[0,2] = bow_word_frequency.get((word, 0), 0)<br/>    <br/>    return features</span></pre><p id="ce81" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">打乱语料库，将训练集和测试集分开。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="45f4" class="ku kv it nd b gy nh ni l nj nk"># shuffle the positive and negative tweets<br/>shuffle(processed_pos_tweets)<br/>shuffle(processed_neg_tweets)<br/><br/># create positive and negative labels<br/>positive_tweet_label = [1 for i in processed_pos_tweets]<br/>negative_tweet_label = [0 for i in processed_neg_tweets]<br/><br/># create dataframe<br/>tweet_df = pd.DataFrame(list(zip(twitter_processed_corpus, positive_tweet_label+negative_tweet_label)), columns=["processed_tweet", "label"])</span></pre><h2 id="5dac" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">3.3 训练和测试分割</h2><p id="3e02" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">让我们保留 80%的数据用于训练，20%的数据样本用于测试。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="f58b" class="ku kv it nd b gy nh ni l nj nk"># train and test split<br/>train_X_tweet, test_X_tweet, train_Y, test_Y = train_test_split(tweet_df["processed_tweet"], tweet_df["label"], test_size = 0.20, stratify=tweet_df["label"])<br/>print(f"train_X_tweet {train_X_tweet.shape}, test_X_tweet {test_X_tweet.shape}, train_Y {train_Y.shape}, test_Y {test_Y.shape}")</span><span id="43f2" class="ku kv it nd b gy nq ni l nj nk">train_X_tweet (8000,), test_X_tweet (2000,), train_Y (8000,), test_Y (2000,)</span><span id="6fff" class="ku kv it nd b gy nq ni l nj nk"># train X feature dimension<br/>train_X = np.zeros((len(train_X_tweet), 3))<br/><br/>for index, tweet in enumerate(train_X_tweet):<br/>    train_X[index, :] = extract_features(tweet, bow_word_frequency)<br/><br/># test X feature dimension<br/>test_X = np.zeros((len(test_X_tweet), 3))<br/><br/>for index, tweet in enumerate(test_X_tweet):<br/>    test_X[index, :] = extract_features(tweet, bow_word_frequency)<br/><br/>print(f"train_X {train_X.shape}, test_X {test_X.shape}")</span><span id="ff39" class="ku kv it nd b gy nq ni l nj nk">train_X (8000, 3), test_X (2000, 3)</span></pre><p id="0582" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">输出:</strong></p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="67b0" class="ku kv it nd b gy nh ni l nj nk">train_X[0:5]</span><span id="2331" class="ku kv it nd b gy nq ni l nj nk">array([[1.000e+00, 6.300e+02, 0.000e+00],<br/>       [1.000e+00, 6.930e+02, 0.000e+00],<br/>       [1.000e+00, 1.000e+00, 4.570e+03],<br/>       [1.000e+00, 1.000e+00, 4.570e+03],<br/>       [1.000e+00, 3.561e+03, 2.000e+00]])</span></pre><p id="2921" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">看一看样本训练特征。</p><ul class=""><li id="9eff" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">第 0 个索引是添加的偏差项。</li><li id="3464" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">第一个指标代表正词频</li><li id="8817" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">第二个指数代表负词频</li></ul><h2 id="644b" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.实施逻辑回归</h2><h2 id="075a" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.1 概述</h2><p id="35a3" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">现在，让我们看看逻辑回归是如何工作和实现的。</p><p id="8f5d" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">很多时候，当你听到逻辑回归时，你可能会想，这是一个回归问题。不，不是，<strong class="ls iu"> Logistic 回归</strong>是一个分类问题，是一个非线性模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oc"><img src="../Images/c002a89ec693b84600d4afa6d86ea2fb.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Sc2sm6O6Uu_Xp8zayn3qDA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">由作者创建</p></figure><p id="0d24" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">如上图所示，大多数最大似然算法有 4 个阶段，</p><p id="badb" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">第一步。初始化权重</p><ul class=""><li id="0d48" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">随机权重已初始化</li></ul><p id="5001" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">第二步。应用功能</p><ul class=""><li id="72c8" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">计算乙状结肠</li></ul><p id="7f5e" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">第三步。计算成本(算法的目标)</p><ul class=""><li id="735f" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">计算二元分类的对数损失</li></ul><p id="10ca" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">第四步。梯度下降</p><ul class=""><li id="1428" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">迭代更新权重，直到找到最小成本</li></ul><p id="ae5b" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">逻辑回归采用线性回归，并将<strong class="ls iu"> sigmoid </strong>应用于线性回归的输出。因此，它产生了每一类的概率，其总和为 1。</p><p id="e9ee" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">回归:</strong>一元线性回归方程如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/3c68572ec470c6a18f6bc68eb88b4aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*4ykq1Pgq2-jYleFmOcYCTw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">单变量线性回归公式</p></figure><ul class=""><li id="6ae8" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">注意，<strong class="ls iu">θ</strong>值是<strong class="ls iu"> <em class="nl">权重</em> </strong></li><li id="8f99" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated"><strong class="ls iu"> x_0，x_1，x_2，… x_N </strong>是输入特征</li></ul><p id="97f4" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">你可能会想到这个方程有多复杂。我们需要将在<code class="fe nn no np nd b">ith</code>位置的每个特征的所有权重相乘，然后求和。</p><blockquote class="nv nw nx"><p id="62ce" class="lq lr nl ls b lt mj ju lv lw mk jx ly ny ml ma mb nz mm md me oa mn mg mh mi im bi translated">好在<strong class="ls iu">线性代数</strong>带来了这个易操作的方程。没错，就是矩阵<code class="fe nn no np nd b"><em class="it">dot</em></code>产品。您可以应用特征和权重的点积来找到<strong class="ls iu"> z </strong>。</p></blockquote><h2 id="c5be" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.2 乙状结肠</h2><ul class=""><li id="84c6" class="mo mp it ls b lt lu lw lx ld ns lh nt ll nu mi mt mu mv mw bi translated">sigmoid 函数定义为:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/9f593c09ac0f3d74c4032153436bf378.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*r-MWZtm-hdhg4lKQlr06OQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae nm" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">乙状结肠功能</a></p></figure><p id="d25f" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">它将输入“z”映射到一个介于 0 和 1 之间的值，因此它可以被视为一个<strong class="ls iu">概率</strong>。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="fafe" class="ku kv it nd b gy nh ni l nj nk">def sigmoid(z): <br/>    <br/>    # calculate the sigmoid of z<br/>    h = 1 / (1+ np.exp(-z))<br/>    <br/>    return h</span></pre><h2 id="e768" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.3 成本函数</h2><p id="65bf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">逻辑回归中使用的成本函数是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/ba3e27988e6d279e37bc4f82cd5bdaea.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*daYALvNJcsso7I3XgdI79Q.png"/></div></figure><p id="b2fd" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">这就是二进制分类的<strong class="ls iu">测井损失。</strong>在逻辑回归中计算所有训练样本的对数损失的平均值，对所有训练样本的等式<strong class="ls iu"> <em class="nl"> 3 </em> </strong>修改如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/9d019eb4e9d17942b1ef9fa78acf4fd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*N1cdGpAxyfnge4Go-x9RVw.png"/></div></figure><ul class=""><li id="8eeb" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated"><strong class="ls iu"> <em class="nl"> m </em> </strong>是训练样本的数量</li><li id="c3d6" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated"><strong class="ls iu"><em class="nl"/></strong>是<strong class="ls iu"> <em class="nl">与</em> </strong>训练实例的实际标签。</li><li id="5671" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated"><strong class="ls iu"><em class="nl">【h(z(\theta)^{(i)}】</em></strong>为<strong class="ls iu"> <em class="nl">与</em> </strong>训练样本的模型预测。</li></ul><p id="eaf9" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">单个训练示例的损失函数是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7185f6f9710d6f7fd3afeb667b4c2b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*65aScoEzsRWLVXskfMObgA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">损失函数</p></figure><ul class=""><li id="96c2" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">所有的<strong class="ls iu"> <em class="nl"> h </em> </strong> <em class="nl"> </em>的值都在 0 到 1 之间，所以日志会是负数。这就是将系数-1 应用于两个损失项之和的原因。</li><li id="26ac" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">当模型预测 1，(<em class="nl">h</em>(<em class="nl">z</em>(<em class="nl">θ</em>))= 1)且标签<strong class="ls iu"> <em class="nl"> y </em> </strong> <em class="nl"> </em>也为 1 时，该训练示例的损失为 0。</li><li id="88f2" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">同样，当模型预测为 0，(<em class="nl">h</em>(<em class="nl">z</em>(<em class="nl">θ</em>))= 0，而实际标签也为 0 时，该训练示例的损失为 0。</li><li id="e89e" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">但当模型预测接近 1(<em class="nl">h</em>(<em class="nl">z</em>(<em class="nl">θ</em>))= 0.9999)且标号为 0 时，对数损失的第二项变成一个很大的负数，再乘以-1 的总因子，转换成正的损失值。1×(1 0)×<em class="nl">log</em>(1 0.9999)≈9.2 模型预测越接近 1，损耗越大。</li></ul><h2 id="1b7f" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.4 梯度下降</h2><p id="422d" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">梯度下降是一种用于<strong class="ls iu">迭代更新权重<em class="nl">θ</em>T51】以最小化目标函数(成本)的算法。我们需要迭代地更新权重，因为，</strong></p><blockquote class="nv nw nx"><p id="8626" class="lq lr nl ls b lt mj ju lv lw mk jx ly ny ml ma mb nz mm md me oa mn mg mh mi im bi translated">在初始随机权重下，模型不会学到太多东西。为了改进预测，我们需要通过多次迭代从数据中学习，并相应地调整随机权重。</p></blockquote><p id="9fe2" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">对于权重之一<strong class="ls iu"><em class="nl">θ_ J</em></strong>的成本函数<strong class="ls iu"> <em class="nl"> J </em> </strong>的梯度是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/8e7b378da360008deb2a9c6bbf42323c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*4rGEAPDAEwAuqRIUoyYeBA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">梯度函数</p></figure><h2 id="60bd" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.5 正规化</h2><p id="ba73" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">正则化是一种通过惩罚成本函数来解决机器学习算法中过拟合问题的技术。在成本函数中会有一个附加的惩罚项。有两种类型的正则化技术:</p><ul class=""><li id="c746" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">拉索(L1 范数)正则化</li><li id="31ed" class="mo mp it ls b lt mx lw my ld mz lh na ll nb mi mt mu mv mw bi translated">岭(L2 范数)正则化</li></ul><p id="ce9d" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">拉索回归(L1)</strong>L1-范数损失函数也被称为最小绝对误差(LAE)。$λ*∑ |w| $是一个正则项。它是$λ$正则化项与权的绝对和的乘积。较小的值表示较强的正则化。</p><p id="b704" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu">岭回归(L2)</strong>L2-范数损失函数也称为最小二乘误差(LSE)。$λ*∑ (w) $是一个正则项。它是$λ$正则化项与权的平方和的乘积。较小的值表示较强的正则化。</p><p id="d107" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">你会注意到，这有很大的不同。是的，它做得很好。主要的区别在于你在成本函数中加入了什么类型的正则项来最小化误差。</p><blockquote class="nv nw nx"><p id="8523" class="lq lr nl ls b lt mj ju lv lw mk jx ly ny ml ma mb nz mm md me oa mn mg mh mi im bi translated">L2(岭)缩小所有系数相同的比例，但它不消除任何特征，而 L1(拉索)可以缩小一些系数为零，也执行特征选择。</p></blockquote><p id="9379" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">在下面的代码中将添加 L2 正则化</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="63c6" class="ku kv it nd b gy nh ni l nj nk"># implementation of gradient descent algorithm  </span><span id="9226" class="ku kv it nd b gy nq ni l nj nk">def gradientDescent(x, y, theta, alpha, num_iters, c):</span><span id="457f" class="ku kv it nd b gy nq ni l nj nk">    # get the number of samples in the training<br/>    m = x.shape[0]<br/>    <br/>    for i in range(0, num_iters):<br/>        <br/>        # find linear regression equation value, X and theta<br/>        z = np.dot(x, theta)<br/>        <br/>        # get the sigmoid of z<br/>        h = sigmoid(z)<br/> <br/>        # calculate the cost function, log loss<br/>        #J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1-h)))<br/>        <br/>        # let's add L2 regularization<br/>        # c is L2 regularizer term<br/>        J = (-1/m) * ((np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1-h))) + (c * np.sum(theta)))<br/>        <br/>        # update the weights theta<br/>        theta = theta - (alpha / m) * np.dot((x.T), (h - y))<br/>   <br/>    J = float(J)<br/>    return J, theta</span></pre><h2 id="ea55" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">5.火车模型</h2><p id="0cfd" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">让我们训练梯度下降函数来优化随机初始化的权重。在第 4 节中已经给出了简要的解释。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="3068" class="ku kv it nd b gy nh ni l nj nk"># set the seed in numpy<br/>np.random.seed(1)<br/># Apply gradient descent of logistic regression<br/># 0.1 as added L2 regularization term<br/>J, theta = gradientDescent(train_X, np.array(train_Y).reshape(-1,1), np.zeros((3, 1)), 1e-7, 1000, 0.1)<br/>print(f"The cost after training is {J:.8f}.")<br/>print(f"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}")</span><span id="4e19" class="ku kv it nd b gy nq ni l nj nk">The cost after training is 0.22154867.<br/>The resulting vector of weights is [2.18e-06, 0.00270863, -0.00177371]</span></pre><h2 id="d9fe" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">6.测试我们的逻辑回归</h2><p id="9e8c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">是时候在模型之前没有见过的测试数据上测试我们的逻辑回归函数了。</p><p id="90b0" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">预测一条推文是正面的还是负面的。</p><ul class=""><li id="ab7a" class="mo mp it ls b lt mj lw mk ld mq lh mr ll ms mi mt mu mv mw bi translated">将 sigmoid 应用于 logits 以获得预测值(介于 0 和 1 之间的值)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/07d5dbb24323cf817ef18d3673875aef.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*U4CemTnkbcs526uYCcxT3A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">新推文预测</p></figure><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="3d36" class="ku kv it nd b gy nh ni l nj nk"># predict for the features from learned theata values<br/>def predict_tweet(x, theta):<br/>    <br/>    # make the prediction for x with learned theta values<br/>    y_pred = sigmoid(np.dot(x, theta))<br/>    <br/>    return y_pred</span><span id="537f" class="ku kv it nd b gy nq ni l nj nk"># predict for the test sample with the learned weights for logistics regression<br/>predicted_probs = predict_tweet(test_X, theta)<br/># assign the probability threshold to class<br/>predicted_labels = np.where(predicted_probs &gt; 0.5, 1, 0)<br/># calculate the accuracy<br/>print(f"Own implementation of logistic regression accuracy is {len(predicted_labels[predicted_labels == np.array(test_Y).reshape(-1,1)]) / len(test_Y)*100:.2f}")</span><span id="415c" class="ku kv it nd b gy nq ni l nj nk">Own implementation of logistic regression accuracy is 93.45</span></pre><p id="6e10" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">到目前为止，我们已经看到了如何自己实现逻辑回归。得到了<strong class="ls iu"> 94.45 的精度。</strong>让我们看看来自流行的机器学习(ML) Python 库的结果。</p><h2 id="5e28" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">7.用 Scikit 学习逻辑回归测试</h2><p id="7f51" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">这里，我们将训练内置 Python 库中的逻辑回归来检查结果。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="3257" class="ku kv it nd b gy nh ni l nj nk"># scikit learn logiticsregression and accuracy score metric<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import accuracy_score<br/>clf = LogisticRegression(random_state=42, penalty='l2')<br/>clf.fit(train_X, np.array(train_Y).reshape(-1,1))<br/>y_pred = clf.predict(test_X)</span><span id="473b" class="ku kv it nd b gy nq ni l nj nk">/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().<br/>  return f(**kwargs)</span><span id="fbc6" class="ku kv it nd b gy nq ni l nj nk">print(f"Scikit learn logistic regression accuracy is {accuracy_score(test_Y , y_pred)*100:.2f}")</span><span id="4b46" class="ku kv it nd b gy nq ni l nj nk">Scikit learn logistic regression accuracy is 94.45</span></pre><p id="b3f9" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">太好了！！！。结果非常接近。</p><p id="6e02" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">最后，我们自己实现了逻辑回归，并尝试使用内置的 Scikit learn 逻辑回归来获得类似的准确性。但是，这种特征提取的方法非常简单和直观。</p><p id="e873" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated"><strong class="ls iu"> <em class="nl">我是边做边学。</em> </strong> <em class="nl">欢迎在评论中留下您的想法或任何建议。非常感谢你的反馈，它能增强我的信心。</em></p><p id="e9f7" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">🙏感谢阅读！你可以通过 LinkedIn 联系我。</p></div></div>    
</body>
</html>