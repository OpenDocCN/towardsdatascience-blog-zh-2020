<html>
<head>
<title>Semi-supervised Intent Classification with GAN-BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 GAN-BERT 的半监督意图分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semi-supervised-intent-classification-with-gan-bert-934d8659bca2?source=collection_archive---------21-----------------------#2020-08-18">https://towardsdatascience.com/semi-supervised-intent-classification-with-gan-bert-934d8659bca2?source=collection_archive---------21-----------------------#2020-08-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1a3c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于 GAN-BERT 的半监督学习方法在 CLINC150 数据集上的意图分类</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/10292ec95e6d127eb4cd0916527c26fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*de1OIb8sJjEVnBRknV9gbg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">甘伯特建筑。来源:"<a class="ae kv" href="https://www.aclweb.org/anthology/2020.acl-main.191.pdf" rel="noopener ugc nofollow" target="_blank"> GAN-BERT:健壮文本分类的生成式对抗学习</a>"</p></figure><blockquote class="kw kx ky"><p id="d9b3" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">有没有可能对 150 个目标类别进行文本分类，每个类别只使用 10 个标记样本，但仍能获得良好的性能？</p></blockquote><p id="d927" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">从那个简单的问题开始，我开始做研究，以便回答那个问题。花了几个小时后，我终于和甘伯特在一起了。甘伯特是什么？我用甘博特做了什么实验？在本文中，我将尝试简要介绍 GAN-BERT，并使用<a class="ae kv" href="https://www.aclweb.org/anthology/D19-1131.pdf" rel="noopener ugc nofollow" target="_blank"> CLINC150 数据集</a>实现其意图分类。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><p id="579b" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">在自然语言处理(NLP)领域，<a class="ae kv" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank"> BERT 或来自 Transformers </a>的双向编码器表示是一种基于 Transformers 架构的众所周知的技术，用于执行广泛的任务，包括文本分类。然而，当有“足够”的标记训练数据要利用时，这种技术可以很好地执行，而获得标记数据是耗时且昂贵的过程。对此的潜在解决方案是使用半监督学习方法。</p><p id="defa" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">半监督学习是机器学习领域中的一种方法，它在训练过程中结合了标记数据和未标记数据。目标与监督学习方法相同，即在给定具有若干特征的数据的情况下预测目标变量。当我们没有这么多标记数据，而我们的模型需要大量训练数据才能表现良好时，这种方法是至关重要的。</p><p id="d582" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">最近在 2020 年 7 月，一篇名为<a class="ae kv" href="https://www.aclweb.org/anthology/2020.acl-main.191.pdf" rel="noopener ugc nofollow" target="_blank">“GAN-BERT:具有一堆标记示例的健壮文本分类的生成对抗学习”</a>的论文，试图在生成对抗设置中扩展具有未标记数据的 BERT 类架构的微调。在高层，他们试图从<a class="ae kv" href="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf" rel="noopener ugc nofollow" target="_blank"> SS-GAN </a>(半监督 GAN)的角度丰富 BERT 微调过程。</p><blockquote class="mg"><p id="2d07" class="mh mi iq bd mj mk ml mm mn mo mp lv dk translated">“在本文中，我们在生成性对抗设置中使用未标记数据来扩展 BERT 训练。特别是，我们在所谓的 GAN-BERT 模型中，从 SS-GAN 的角度丰富了 BERT 微调过程</p></blockquote></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="a37b" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">甘伯特</h1><p id="6c37" class="pw-post-body-paragraph kz la iq lc b ld ni jr lf lg nj ju li lw nk ll lm lx nl lp lq ly nm lt lu lv ij bi translated">这种体系结构结合了 BERT 和 SS-GAN 的能力来进行文本分类。生成器通过从高斯分布中提取 100 维噪声向量的输入来产生“假”示例。鉴别器是在 BERT 上的 MLP，它接收输入向量或者是由发生器产生的假向量或者是来自 BERT 产生的真实数据的向量。鉴别器的最后一层是 softmax 层，它输出 logits 的 k+1 维向量，其中 k 是数据集中类的数量。这里，真实数据被分成两部分，它们被标记为(L)和未标记的(U)数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/10292ec95e6d127eb4cd0916527c26fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*de1OIb8sJjEVnBRknV9gbg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">甘伯特建筑。来源:"<a class="ae kv" href="https://www.aclweb.org/anthology/2020.acl-main.191.pdf" rel="noopener ugc nofollow" target="_blank"> GAN-BERT:健壮文本分类的生成式对抗学习</a>"</p></figure><p id="3726" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">鉴别器旨在对输入是否为真实实例进行分类。如果它预测输入是真实的实例，那么它必须预测输入属于哪个类。</p><p id="5941" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">训练过程试图优化<strong class="lc ir">两个竞争损耗</strong>，它们是鉴别器损耗和发电机损耗。<strong class="lc ir">鉴别器损耗</strong>是另外两个损耗的总和:监督损耗和非监督损耗。监督损失测量将错误的类别分配给原始 k 个类别中的真实示例的误差，而非监督损失测量不正确地将真实(未标记的)示例识别为假的并且不识别假的示例的误差。<strong class="lc ir">发电机损耗</strong>也是另外两个损耗求和的结果:<strong class="lc ir"> </strong>特征匹配和无监督损耗。特征匹配损失旨在确保生成器应该产生其在输入到鉴别器时提供的中间表示与真实表示非常相似的示例，而无监督损失测量由鉴别器正确识别的虚假示例引起的误差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/021fcfcecdffb88985e71dd27aa4f8ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Id2rUNxFMLUr8lX5"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基思·约翰斯顿在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0a82" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">在训练期间，每一类中的样本以<em class="lb"> log </em> (2|U|/|L|)的因子被复制，以保证在每一批中存在一些标记的实例，从而避免由于对抗训练的无监督成分而导致的发散。在推理过程中，生成器被从体系结构中丢弃，而保留其余部分。</p><blockquote class="kw kx ky"><p id="43be" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这意味着相对于标准的 BERT 模型，在推断时没有额外的成本。</p></blockquote></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="b473" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">实验设置</h1><p id="33fa" class="pw-post-body-paragraph kz la iq lc b ld ni jr lf lg nj ju li lw nk ll lm lx nl lp lq ly nm lt lu lv ij bi translated">在本实验中，使用了<a class="ae kv" href="https://www.aclweb.org/anthology/D19-1131.pdf" rel="noopener ugc nofollow" target="_blank"> CLINC150 </a>数据集。该数据集由 10 个领域的 150 个意图类组成。提供的数据有 4 种变型:<em class="lb">满</em>、<em class="lb">小</em>、<em class="lb">不平衡</em>、<em class="lb"> OOS+、</em>这里用的是<em class="lb">满</em>变型<em class="lb">。</em>对于每个意图，有 100 个训练话语、20 个验证话语和 30 个测试话语。实际上，除了 150 个意向范围内类之外，该数据还提供了范围外类(OOS)。但是，在这个实验中，我只关注范围内的类预测。</p><p id="9853" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">由于实际的 CLINC150 数据集不是为半监督学习设置而构建的，因此在这里，我尝试用 6 种不同的训练数据进行实验。对于每个变体，训练数据被分成标记的和未标记的集合。第一个变体由 10%标记的和 90%未标记的数据集组成。因为训练数据中的话语总数是 100，所以对于第一种变型，有标记集合有 10 个话语，无标记集合有 90 个话语。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/2f3cee82f20726c44d798ea67318780a.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*w9EixKDdEpOPzSPmKXbuSQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练数据的变量。来源:作者的财产</p></figure><p id="b90e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我对所有训练数据变量使用完全相同的参数设置。唯一的区别是训练时期的数量。我使用<em class="lb"> 20、18、16、14、12、10 </em>个历元数分别表示第一个到最后一个变量。</p><h1 id="2148" class="mq mr iq bd ms mt np mv mw mx nq mz na jw nr jx nc jz ns ka ne kc nt kd ng nh bi translated">实验结果</h1><p id="432f" class="pw-post-body-paragraph kz la iq lc b ld ni jr lf lg nj ju li lw nk ll lm lx nl lp lq ly nm lt lu lv ij bi translated">CLINC150 论文上的结果表明，通过利用 BERT，他们在测试数据集上获得了<strong class="lc ir"> 96.2%的准确率</strong>。这里使用相同的测试数据集是跨越 6 个训练数据变化的 GAN-BERT 的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/de9bd10acb6b4c2a8ab387a82068224c.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*DEItlPk3gnwq_Vf7HZjB7g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">跨 6 个训练数据变量的测试数据集的准确性。来源:作者的财产</p></figure><p id="046e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我们可以看到，在训练过程中，即使在每个意图中仅使用 10 个标记的话语，GAN-BERT 也能够给出合理的性能。甘伯特的性能随着标记话语数量的增加而增加。第四、第五和第六变型的性能彼此相似。这表明，即使仅使用 60%的标记训练数据，GAN-BERT 的性能也类似于使用 80%甚至 90%的标记训练数据训练的模型。</p><h1 id="3d37" class="mq mr iq bd ms mt np mv mw mx nq mz na jw nr jx nc jz ns ka ne kc nt kd ng nh bi translated">最后的话</h1><p id="5fc3" class="pw-post-body-paragraph kz la iq lc b ld ni jr lf lg nj ju li lw nk ll lm lx nl lp lq ly nm lt lu lv ij bi translated">GAN-BERT 在多文本分类任务的半监督学习中具有很大的潜力。仅给定有限的标记训练数据，它表现良好。然而，这只是在文本分类任务中处理有限标记训练数据的方法之一。还有另一种方法叫做少镜头文本分类。如果你有更多的兴趣，你可以阅读这篇<a class="ae kv" href="https://arxiv.org/pdf/2003.04807.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，它也使用 CLINC150 作为他们的训练数据。</p><blockquote class="mg"><p id="c8d5" class="mh mi iq bd mj mk ml mm mn mo mp lv dk translated">您可以在这里找到本文<a class="ae kv" href="https://github.com/louisowen6/GAN_BERT_CLINC150" rel="noopener ugc nofollow" target="_blank">中使用的所有代码。</a></p></blockquote></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="e08b" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">关于作者</h1><p id="ea1c" class="pw-post-body-paragraph kz la iq lc b ld ni jr lf lg nj ju li lw nk ll lm lx nl lp lq ly nm lt lu lv ij bi translated">Louis Owen 是一名数据科学爱好者，他总是渴望获得新知识。他获得了最后一年的全额奖学金，在印尼顶尖大学<a class="ae kv" href="https://www.itb.ac.id/" rel="noopener ugc nofollow" target="_blank"> <em class="lb">的万隆技术学院</em> </a>攻读数学专业。</p><p id="d21b" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">Louis 曾在多个行业领域担任分析/机器学习实习生，包括 OTA ( <a class="ae kv" href="https://www.linkedin.com/company/traveloka-com/" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> Traveloka </em> </a>)、电子商务(<a class="ae kv" href="https://www.linkedin.com/company/pt--tokopedia/" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> Tokopedia </em> </a>)、fin tech(<a class="ae kv" href="https://www.linkedin.com/company/doitglotech/" rel="noopener ugc nofollow" target="_blank"><em class="lb">Do-it</em></a>)、智慧城市 App ( <a class="ae kv" href="https://www.linkedin.com/company/qluesmartcity/" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> Qlue 智慧城市</em> </a>)，目前在<a class="ae kv" href="https://www.linkedin.com/company/the-world-bank/" rel="noopener ugc nofollow" target="_blank"><em class="lb"/></a>世界银行担任数据科学顾问。</p><p id="339f" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">查看路易斯的网站，了解更多关于他的信息！最后，如果您有任何疑问或需要讨论的话题，请通过 LinkedIn 联系 Louis。</p></div></div>    
</body>
</html>