# 训练文本中的不稳定性——甘

> 原文：<https://towardsdatascience.com/instability-in-training-text-gan-20273d6a859a?source=collection_archive---------47----------------------->

## 大量的暗箱操作

![](img/107be7bc56d1c031389e7cd1e42b2589.png)

照片由[詹姆斯·庞德](https://unsplash.com/@jamesponddotco?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 拍摄

# 介绍

在文本生成中，传统上，最大似然估计用于训练模型以一次一个标记地生成文本。每个生成的令牌都将与真实数据进行比较。如果任何令牌与实际令牌不同，该信息将用于更新模型。然而，这样的训练可能导致生成是通用的或重复的。

生成对抗网络(GAN)通过引入两个模型——生成器和鉴别器来解决这个问题。鉴别器的目标是确定一个句子 x 是真的还是假的(假的是指由模型生成的)，而生成器试图生成一个可以欺骗鉴别器的句子。这两个模型相互竞争，这导致了两个网络的改进，直到生成器可以产生类似人类的句子。

尽管我们可能会在计算机视觉和文本生成社区中看到一些有希望的结果，但实际操作这种类型的建模是困难的。

# GANS 的问题

1.  **模式崩溃(缺乏多样性)**这是 GAN 训练的通病。当模型不关心输入的随机噪声时，就会发生模式崩溃，不管输入是什么，它都会不断地生成同一个句子。从这个意义上来说，模型现在试图欺骗鉴别器，找到一个单点就足够了。
2.  **不稳定训练。**最重要的问题是保证发生器和鉴别器并驾齐驱。如果任何一个胜过对方，整个训练就会变得不稳定，学不到任何有用的信息。例如，当发电机的损耗缓慢降低时，这意味着发电机开始寻找方法来欺骗鉴别器，即使该代仍然不成熟。另一方面，当鉴别器为 OP 时，没有新的信息供生成器学习。每一代都会被评价为假的；因此，生成器将不得不依靠随机改变单词来搜索可能欺骗 d 的句子。
3.  **直觉不够**。有时，您的预期建模是正确的，但它可能不会像您希望的那样工作。它可能需要更多的工作。通常，您需要通过调整学习率、尝试不同的损失函数、使用批量范数或尝试不同的激活函数来进行超参数调整。
4.  大量的训练时间。一些工作报道了高达 400 个纪元的训练。如果我们与 Seq2Seq 相比，这是巨大的，seq 2 seq 可能只需要大约 50 个代来获得结构良好的一代。导致它缓慢的原因是一代人的探索。g 没有接收到哪个令牌是坏的任何明确信号。相反，它是为整个一代人而接受的。为了能够生成一个自然的句子，G 需要探索各种单词组合才能到达那里。你觉得 G 有多经常会不小心凭空产生< eos >？如果我们使用 MLE，信号非常清楚，应该有< eos >和紧随其后的< pad >。

# 潜在的解决方案

已经尝试了许多方法来处理这种类型的训练。

1.  **使用亚当优化器**。有人建议使用 ADAM 作为发生器，SGD 作为鉴别器。但最重要的是，一些论文开始调整亚当的测试版。贝塔系数=(0.5，0.999)
2.  **瓦塞尔斯坦甘**。一些使用 WGAN 的工作报告可以极大地稳定训练。然而，从我们的实验来看，WGAN 甚至达不到常规 GAN 的质量。也许我们遗漏了什么。(看到了吗？挺难的)
3.  **GAN 变异**。有人建议尝试吉隆坡或 VAE 甘。这些可以使模型更容易训练。
4.  **鉴频器的输入噪声**。为了使鉴频器的学习与发生器的学习不相上下，发生器的学习通常比鉴频鉴相器更困难，我们在输入的同时输入一些噪声，并使用压差使事情变得更容易。
5.  **DCGAN(深度卷积 GAN)** 。这只适用于计算机视觉任务。然而，这种模型可以避免不稳定的训练。这个模型的关键是不使用 ReLU，使用 BatchNorm，使用步长卷积。
6.  **鉴频器组合**。不是只有一个鉴别器，而是用不同的批次训练多个鉴别器，以捕捉不同方面的尊重。因此，生成器不能只欺骗单个 D，而是要更一般化，以便它可以欺骗所有的 D。这也和中途辍学的甘(很多 D，训练时辍学一些)有关。
7.  **参数调谐**。学习率、辍学率、批量等等。很难确定一个模型比另一个模型好多少。因此，有些人会测试多个参数，看看哪个效果最好。一个瓶颈是没有针对 GAN 的评估指标，这导致需要大量手动检查来确定质量。
8.  **调度 G 和 D** 。据报道，在许多工作中，试图学习 G 5 次然后学习 D 1 次是没有用的。如果你想尝试计划，做一些更有意义的事情。

```
while generator_loss > 0.5:
    train_G()while discriminator_loss > 0.5:
    train_D()
```

# 结论

基于对抗的文本生成为如何训练模型开辟了新的途径。不是依赖于 MLE，而是使用鉴别器来指示生成是否正确。然而，这种训练也有不好的一面，那就是很难训练。许多研究提出了一些如何避免上述问题的建议；但是，您需要尝试各种设置(或参数),以确保您的创成式模型能够正确学习。

# 进一步阅读

[](https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/) [## 如何识别和诊断 GAN 故障模式—机器学习掌握

### 如何在训练生成对抗网络时识别不稳定模型？甘很难训练。原因是…

machinelearningmastery.com](https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/) [](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html) [## 从 GAN 到 WGAN

### 这篇文章解释了生成性对抗网络(GAN)模型背后的数学原理，以及为什么它很难被训练…

lilianweng.github.io](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html) [](https://github.com/soumith/ganhacks) [## soumith/ganhacks

### (该列表不再保留，我不确定它在 2020 年的相关性如何),而生殖研究…

github.com](https://github.com/soumith/ganhacks) [](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b) [## 甘——为什么生成性对抗网络这么难训练！

### 认出莫奈的画比画一幅容易。生成模型(创建数据)被认为是非常…

medium.com](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)