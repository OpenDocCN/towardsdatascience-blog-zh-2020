<html>
<head>
<title>Working with Hugging Face Transformers and TF 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用拥抱脸变形金刚和 TF 2.0</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a?source=collection_archive---------3-----------------------#2020-04-24">https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a?source=collection_archive---------3-----------------------#2020-04-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0076" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于变形金刚的模型是当前 NLP 世界的热点。拥抱脸的变形金刚库提供了所有的 SOTA 模型(像 BERT，GPT2，RoBERTa 等)用于 TF 2.0，这个博客旨在展示它的接口和 API</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/13d637d82a6587021ba8dfc5d93b991a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WTpgNpP5Ksr2o9e8z4XbOA.png"/></div></div></figure><h1 id="924f" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">0.放弃</h1><p id="df8b" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我假设你知道变形金刚和它的注意力机制。这个博客的主要目的是展示如何在 TF 2.0 中使用 Hugging Face 的 transformer 库，也就是说，它将是一个更加关注代码的博客。</p><h1 id="9694" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">1.介绍</h1><p id="b0f7" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">拥抱脸最初只支持 PyTorch，现在 TF 2.0 也很支持。在 PyTorch 上使用 transformer 库，你可以找到很多高质量的教程，但是在 TF 2.0 上就不一样了(这是这篇博客的主要动机)。</p><p id="28ab" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">使用 BERT 甚至 AlBERT 是相当容易的，TF 2.0 中的标准流程由 tensorflow_hub 提供，但 GPT2、RoBERTa、DistilBERT 等就不一样了。抱脸的变形金刚库来救援了。它们提供了直观的 API 来从头构建定制模型，或者为大量基于 transformer 的模型微调预先训练的模型。</p><p id="0782" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">它支持广泛的自然语言处理应用，如文本分类、问答系统、文本摘要、标记分类等。前往他们的<a class="ae mk" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">文档</a>了解更多细节。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><p id="223f" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">本教程将基于<a class="ae mk" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" rel="noopener ugc nofollow" target="_blank"> Kaggle 的有毒评论分类挑战</a>的多标签文本分类。</p><p id="41b6" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">以下是任何变压器型号的通用管道:</p><p id="37c3" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">标记器定义→文档的标记化→模型定义→模型训练→推理</p><p id="7448" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">现在让我们一个接一个地检查它们，我还将尝试涵盖多个可能的用例。</p><h1 id="b28c" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">2.拥抱面变压器总管道</h1><h2 id="5623" class="ms ks iq bd kt mt mu dn kx mv mw dp lb ls mx my ld lw mz na lf ma nb nc lh nd bi translated">2.1 标记器定义</h2><p id="7a77" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">每个基于 transformer 的模型都有一个独特的标记化技术，特殊标记的独特使用。transformer 库为我们解决了这个问题。它支持与它相关联的每个模型的标记化。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="595a" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→每个变压器型号都有类似的令牌定义 API</p><p id="91ce" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→这里我使用了一个来自预训练模型的记号化器。</p><p id="d990" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→这里，</p><ul class=""><li id="da1e" class="ng nh iq ll b lm mf lp mg ls ni lw nj ma nk me nl nm nn no bi translated">add_special_tokens:用于添加特殊字符，如<cls>、<sep>、<unk>等正在使用的 w.r.t 预训练模型。它应该永远保持真实</unk></sep></cls></li><li id="5814" class="ng nh iq ll b lm np lp nq ls nr lw ns ma nt me nl nm nn no bi translated">max_length:任何要标记的句子的最大长度，这是一个超参数。(最初 BERT 的最大长度为 512)</li><li id="faae" class="ng nh iq ll b lm np lp nq ls nr lw ns ma nt me nl nm nn no bi translated">pad_to_max_length:执行填充操作。</li></ul><h2 id="4887" class="ms ks iq bd kt mt mu dn kx mv mw dp lb ls mx my ld lw mz na lf ma nb nc lh nd bi translated">2.2 文档的标记化</h2><p id="4647" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">下一步是在文档上执行标记化。它可以通过 encode()或 encode_plus()方法来执行。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="e826" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→任何变压器模型通常需要三个输入:</p><ul class=""><li id="6b88" class="ng nh iq ll b lm mf lp mg ls ni lw nj ma nk me nl nm nn no bi translated">输入 id:与他们的词汇相关联的单词 id</li><li id="0556" class="ng nh iq ll b lm np lp nq ls nr lw ns ma nt me nl nm nn no bi translated">关注面膜:必须关注哪个 id；1 =注意。简单地说，它告诉模型哪些是原始单词，哪些是填充单词或特殊标记</li><li id="5ad7" class="ng nh iq ll b lm np lp nq ls nr lw ns ma nt me nl nm nn no bi translated">令牌类型 id:它与使用多句子的模型相关联，如问答模型。它告诉模型句子的顺序。</li></ul><p id="35fd" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→虽然不是必须提供所有这三个 id，只有输入 id 也可以，但是注意力屏蔽有助于模型只关注有效单词。因此，至少对于分类任务，这两者都应该提供。</p><h2 id="2af7" class="ms ks iq bd kt mt mu dn kx mv mw dp lb ls mx my ld lw mz na lf ma nb nc lh nd bi translated">2.3 培训和微调</h2><p id="a932" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">现在是最关键的部分，即“训练”。我将要讨论的方法绝不是训练的“唯一可能的方法”。虽然经过大量的实验，我发现这个方法是最可行的。我将讨论训练模型的三种可能方式:</p><ol class=""><li id="322c" class="ng nh iq ll b lm mf lp mg ls ni lw nj ma nk me nu nm nn no bi translated">直接使用预训练模型作为分类器</li><li id="8604" class="ng nh iq ll b lm np lp nq ls nr lw ns ma nt me nu nm nn no bi translated">提取嵌入并将其用作另一个分类器的输入的转换器模型。</li><li id="60e3" class="ng nh iq ll b lm np lp nq ls nr lw ns ma nt me nu nm nn no bi translated">在定制配置和数据集上微调预训练的转换器模型。</li></ol><p id="161d" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir"> 2.3.1 直接使用预训练模型作为分类器</strong></p><p id="a591" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">这是最简单的，但也是应用最少的。拥抱脸的变形金刚库提供了一些具有序列分类能力的模型。这些模型有两个头部，一个是作为基础的预训练模型架构&amp;一个分类器作为顶部头部。</p><p id="2a7c" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">标记器定义→文档的标记化→模型定义</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/e8d8d194e97ad1b535e937dcbf0f167b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cKSutDaJMvoshzmEB0ariQ.png"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">直接作为分类器的预训练模型的总结</p></figure><p id="8141" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→注:顺序分类的模型仅适用于此。</p><p id="4ab3" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→定义适当的配置在这里至关重要。正如您在第 6 行看到的，我正在定义配置。“num_labels”是当模型是分类模型时要使用的类的数量。它还支持各种配置，请继续查看他们的文档。</p><p id="e026" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→这里需要注意的一些关键事项是:</p><ul class=""><li id="c797" class="ng nh iq ll b lm mf lp mg ls ni lw nj ma nk me nl nm nn no bi translated">这里只有预训练模型的权重可以被更新，但是更新它们不是一个好主意，因为它将违背迁移学习的目的。所以，实际上这里没有什么要更新的。这是我最不喜欢这个的原因。</li><li id="392f" class="ng nh iq ll b lm np lp nq ls nr lw ns ma nt me nl nm nn no bi translated">它也是最不可定制的。</li><li id="db75" class="ng nh iq ll b lm np lp nq ls nr lw ns ma nt me nl nm nn no bi translated">你可以尝试的一个技巧是使用 no 更高的 num_labels，最后在末尾添加一个可以训练的密集层。</li></ul><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="a5e9" class="ms ks iq ob b gy of og l oh oi"># Hack<br/>config = DistilBertConfig(num_labels=64)<br/>config.output_hidden_states = False</span><span id="9aca" class="ms ks iq ob b gy oj og l oh oi">transformer_model=TFDistilBertForSequenceClassification.from_pretrained(distil_bert, config = config) </span><span id="6d69" class="ms ks iq ob b gy oj og l oh oi">input_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')<br/>input_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')<br/>X = transformer_model(input_ids, input_masks_ids)[0]<br/>X = tf.keras.layers.Dropout(0.2)(X)<br/>X = tf.keras.layers.Dense(6, activation='softmax')</span><span id="8443" class="ms ks iq ob b gy oj og l oh oi">model = tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = X)</span><span id="e15f" class="ms ks iq ob b gy oj og l oh oi">for layer in model.layer[:2]:<br/>    layer.trainable = False</span></pre><p id="f6e1" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir"> 2.3.2 提取嵌入的变压器模型，并将其用作另一个分类器的输入</strong></p><p id="86b4" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">这种方法需要两个层次或两个独立的模型。我们使用任何变换器模型来提取单词嵌入&amp;然后使用这个单词嵌入作为任何分类器(例如逻辑分类器、随机森林、神经网络等)的输入。</p><p id="49d7" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">我建议你阅读由<a class="ok ol ep" href="https://medium.com/u/34369d020458?source=post_page-----89bf35e3555a--------------------------------" rel="noopener" target="_blank">杰伊·阿拉姆马</a>撰写的这篇<a class="ae mk" href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" rel="noopener ugc nofollow" target="_blank">文章</a>，它非常详细清晰地讨论了这种方法。</p><p id="a8ff" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">因为这个博客是关于神经网络的，所以让我给你一个关于神经网络的例子。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/41605f40a2bf9eafa0a9676711c84562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HrSGWmTlaHt6Ajwc469TMQ.png"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">模型摘要</p></figure><p id="0615" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→11 号线是这里的关键。我们只对可以使用切片操作提取的模型的<cls>或分类令牌感兴趣。现在我们有了 2D 的数据，并按照我们的要求建立了网络。</cls></p><p id="c7f8" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→与 2.3.1 方法相比，这种方法通常每次都更有效。但是它也有一些缺点，比如:</p><ul class=""><li id="2db0" class="ng nh iq ll b lm mf lp mg ls ni lw nj ma nk me nl nm nn no bi translated">它不太适合生产，因为您必须使用 transformer 模型作为唯一的特征提取器，因此您现在必须维护两个模型，因为您的分类器头是不同的(像 XGBoost 或 cat saw)。</li><li id="3bce" class="ng nh iq ll b lm np lp nq ls nr lw ns ma nt me nl nm nn no bi translated">在将 3D 数据转换为 2D 时，我们可能会错过有价值的信息。</li></ul><p id="9f02" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">如果您只想提取单词嵌入，transformers 库提供了一个很好的工具。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="03e2" class="ms ks iq ob b gy of og l oh oi">import numpy as np<br/>from transformers import AutoTokenizer, pipeline, TFDistilBertModel<br/>model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')</span><span id="32ac" class="ms ks iq ob b gy oj og l oh oi">tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')</span><span id="2b2f" class="ms ks iq ob b gy oj og l oh oi">pipe = pipeline('feature-extraction', model=model, <br/>                tokenizer=tokenizer)</span><span id="9d4f" class="ms ks iq ob b gy oj og l oh oi">features = pipe('any text data or list of text data',<br/>                pad_to_max_length=True)</span><span id="03b3" class="ms ks iq ob b gy oj og l oh oi">features = np.squeeze(features)<br/>features = features[:,0,:]</span></pre><p id="2047" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir"> 2.3.3 微调预应变变压器型号</strong></p><p id="b3c9" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">这是我最喜欢的方法，因为这里我们充分利用了任何变压器模型的潜力。在这里，我们将使用预训练变压器模型的权重，然后对我们的数据进行微调，即迁移学习。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/a54ce066f4c9b7d190b1efe8dc0d35ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gx2XjRrDPQ3m-bYgrBWU-w.png"/></div></div></figure><p id="27a5" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→请看第 17 行，因为 3D 数据是在嵌入层之前生成的，我们可以使用 LSTM 提取更多细节。</p><p id="ab85" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→下一步是将 3D 数据转换为 2D，以便我们可以使用 FC 层。您可以使用任何池层来执行此操作。</p><p id="ba98" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→另外，注意第 18 和 19 行。我们应该总是冻结变压器模型的预训练权重&amp;永远不要更新它们，只更新剩余的权重。</p><p id="fb99" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir">一些临时演员</strong></p><p id="7eb3" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→每种方法都有两个共同点:</p><ol class=""><li id="06da" class="ng nh iq ll b lm mf lp mg ls ni lw nj ma nk me nu nm nn no bi translated">config . output _ hidden _ States = False；因为我们正在训练&amp;对输出状态不感兴趣。</li><li id="80da" class="ng nh iq ll b lm np lp nq ls nr lw ns ma nt me nu nm nn no bi translated">x = transformer _ model(…)[0]；这在 config.output_hidden_states 中是内联的，因为我们只需要顶部的头。</li></ol><p id="3169" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→config 是字典。因此，要查看所有可用配置，只需简单地打印它。</p><p id="430f" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">→小心选择基本型号，因为 TF 2.0 支持是新的，所以可能会有错误。</p><h2 id="5ef2" class="ms ks iq bd kt mt mu dn kx mv mw dp lb ls mx my ld lw mz na lf ma nb nc lh nd bi translated">2.4 推理</h2><p id="ce8f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">由于该模型基于 tf.keras 模型 API，我们可以使用 keras 常用的 model.predict()方法</p><p id="60b5" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">我们甚至可以使用 transformer 库的管道实用程序(请参考 2.3.2 中所示的示例)。这个实用程序非常有效，因为它在一个通用的简单 API 下统一了标记化和预测。</p><h1 id="c01e" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">3.结束注释</h1><p id="b45c" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">拥抱脸真的很容易使用他们的任何模型，现在与 tf.keras。它有很大的可能性。</p><p id="a63e" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">他们还使得在交叉库中使用他们的模型变得非常容易(从 PyTorch 到 TF，反之亦然)。</p><p id="533a" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">我建议访问他们的文档，因为他们有非常直观和中肯的文档。</p><p id="9156" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">你可以通过<a class="ae mk" href="https://www.linkedin.com/in/akashdesarda" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我</p></div></div>    
</body>
</html>