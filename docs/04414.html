<html>
<head>
<title>ML Privacy Meter — A comprehensive tool to quantify privacy risks of Machine Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML Privacy Meter——量化机器学习模型隐私风险的综合工具</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ml-privacy-meter-a-comprehensive-tool-for-privacy-attacks-on-your-ml-model-b4d3f2e05cb4?source=collection_archive---------52-----------------------#2020-04-20">https://towardsdatascience.com/ml-privacy-meter-a-comprehensive-tool-for-privacy-attacks-on-your-ml-model-b4d3f2e05cb4?source=collection_archive---------52-----------------------#2020-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d9927c0c91e708d0faaa84a3e803d565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D7o_zQUwI4yUItGv"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">马库斯·斯皮斯克在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="264c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于计算能力的提高，以及训练模型的大量数据的可用性，深度学习越来越多地用于解决跨领域的问题。随着深度学习的日益普及，这些模型变得更加健壮、安全和隐私变得至关重要。</p><p id="6e6e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，研究表明，深度学习和机器学习模型在训练不当时，往往容易出现各种类型的隐私漏洞。其中一种攻击是成员推断攻击[1]，攻击者试图推断某些数据是否是训练集的一部分。用于训练模型的数据通常取自真实世界的数据，例如用于图像分类问题的真实图像，或者用于医疗诊断应用的实际用户的病历。因此，泄漏此类数据的模型可能会威胁到数据集个体成员的隐私。</p><p id="61a8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://github.com/privacytrustlab/ml_privacy_meter" rel="noopener ugc nofollow" target="_blank"> ML隐私测量仪</a> [2]是一种分析机器学习模型如何容易受到成员推理攻击的工具。该工具在经过训练的目标模型上生成攻击，假设对模型进行黑盒或白盒访问，以获得攻击的推理准确性。白盒攻击可以利用目标模型参数的梯度、中间层输出或模型预测来推断输入的训练集成员资格，而黑盒攻击仅使用目标模型预测来识别成员资格[3]。通过生成推理模型来执行攻击，该推理模型使用可用于某些数据的目标模型组件，并返回该数据的训练集成员的概率。</p><p id="0885" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">安装工具</strong></p><p id="e512" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">安装ML Privacy Meter的先决条件是Python 3.6。和TensorFlow 2.1。首先安装工具的依赖项，然后安装工具。</p><p id="ab3a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb"> pip安装-r要求. txt <br/> pip安装-e . </em></p><p id="5412" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">攻击模型<br/> </strong>该工具提供了攻击给定训练模型的方法。用户需要提供模型权重文件、用于训练模型的数据以及完整的数据集。</p><p id="6d45" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该工具假设用户有一个已经训练好的Keras模型，以及它的数据集和用于训练的数据集子集。数据文件的格式可以在“数据集/自述”文件中找到。keras模型可以使用Keras的keras.load_model()函数通过传递模型文件直接加载，也可以按照下面代码片段中给出的方式进行初始化，使用Keras的load_weights()函数加载训练好的权重文件。</p><p id="b280" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们加载要分析的完整目标Keras模型及其参数和权重。</p><figure class="lc ld le lf gt jr"><div class="bz fp l di"><div class="lg lh l"/></div></figure><p id="f885" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们通过提供完整的数据集和用于训练的数据集来创建攻击处理程序。攻击处理程序提取数据，并为攻击模型的训练集和测试集创建批次。</p><figure class="lc ld le lf gt jr"><div class="bz fp l di"><div class="lg lh l"/></div></figure><p id="9229" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">attack_data()函数中使用的参数是:</p><ul class=""><li id="0ee9" class="li lj iq kf b kg kh kk kl ko lk ks ll kw lm la ln lo lp lq bi translated">dataset_path:指向。txt数据集文件。(项目的“数据集”目录中提供了下载和创建示例)</li><li id="1a2f" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">member_dataset_path:用于创建目标模型的numpy数据文件的路径。(项目的“数据集”目录中提供了下载和创建示例)</li><li id="473d" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">batch_size:训练推理模型时使用的批量大小</li><li id="4e37" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">attack_percentage:用于训练推理模型的训练数据的百分比。</li><li id="da08" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">normalization : Boolean，如果数据需要规范化，则设置为true。这将计算中值和标准偏差以标准化数据。这些值可能会被“tutorials/attack_alex.py”示例中给出的值覆盖。</li><li id="6696" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">input_shape:模型输入数据的形状</li></ul><p id="e718" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，初始化whitebox类以生成推理模型组件。对于白盒攻击，用户可以提供他们想要利用的层的梯度，以及白盒攻击者可以访问的神经网络的其他参数。对于黑盒攻击，只有最终层的输出可用于攻击参数。<br/>在下面的例子中，进行了白盒攻击，其利用了最后一层的梯度和模型的最后两层的输出。还使用了输出损耗值、实际标签的一键编码(参见下面的函数参数描述)。<br/>最后，调用train_attack()函数来执行实际的攻击，并生成结果。在此期间，根据给定的参数，要利用的模型组件用于训练攻击模型。</p><figure class="lc ld le lf gt jr"><div class="bz fp l di"><div class="lg lh l"/></div></figure><p id="9ead" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">initialize()函数中使用的参数有:</p><ul class=""><li id="f401" class="li lj iq kf b kg kh kk kl ko lk ks ll kw lm la ln lo lp lq bi translated">target_train_model:用于训练攻击模型的目标分类模型</li><li id="9144" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">target_attack_model:用于评估攻击模型的目标分类模型</li><li id="5da0" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">train _ datahandler:<code class="fe lw lx ly lz b">target_train_model</code>的data handler</li><li id="0a64" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">优化器:用于训练攻击模型的优化器操作。支持“sgd”、“adam”、“adadelta”、“adagrad”、“momentum”和“rmsprop”。</li><li id="9bde" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">layers_to_exploit:中间输出将被利用的层索引的列表。对于黑盒攻击，这个值应该包含最后一层的索引。</li><li id="a385" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">gradients_to_exploit:将利用其梯度的层索引的列表。层索引取自模型中具有可训练参数的层。</li><li id="9467" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">exploit_loss:用于指定是否将利用目标模型的损失值的布尔值。</li><li id="14aa" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">exploit_label:用于指定是否将利用独热编码标签的布尔值。</li><li id="d61d" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">learning_rate:优化器的学习速率</li><li id="d691" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">时期:训练攻击模型的时期数。</li></ul><p id="4e3c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">攻击执行后，可以在日志文件夹和控制台中查看攻击结果。</p><p id="6836" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">Alex net CIFAR-100攻击的端到端示例</strong></p><p id="b92b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了像Nasr等人[3]中那样执行攻击，使用了在CIFAR-100数据集上训练的Alexnet模型。可以在模型上执行白盒攻击，同时利用梯度、最终层输出、损失值和标签值。<br/>首先，从`<a class="ae kc" href="https://github.com/privacytrustlab/ml_privacy_meter/tree/master/tutorials" rel="noopener ugc nofollow" target="_blank">教程/模型</a>`目录下载预训练的模型，并放在项目的根目录下。<br/> <strong class="kf ir">解压tutorials/models/Alex net _ pre trained . zip-d .</strong></p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/cfebcc556d1ae2fd4f9935ba287eb7ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*wmm_fXG76Sz6JVZOgRljEA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">预训练的CIFAR-100 Alexnet模型(从Pytorch转换为Keras)</p></figure><p id="94c0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">注意:用户也可以训练自己的模型来攻击类似于` tutorials/alexnet.py` <br/> </em>中的例子，然后执行脚本来下载所需的数据文件。这将下载数据集文件和训练集文件，并将其转换为工具所需的格式。<br/> <strong class="kf ir"> cd数据集<br/>chmod+x download _ cifar 100 . sh<br/>。/download _ CIFS ar 100 . sh</strong></p><p id="dd1e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，执行主攻击代码以获得结果。<br/> <strong class="kf ir"> python教程/attack_alexnet.py </strong></p><p id="22ae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">文件中的“attackobj”初始化meminf类和攻击配置。以下是可在功能中更改的一些配置示例。<br/>注:根据CIFAR-100分布，代码明确设置了图像归一化的平均值和标准偏差。<br/> 1。白盒攻击-利用最终图层梯度、最终图层输出、损失值和标签值(默认)</p><blockquote class="mb mc md"><p id="6f6e" class="kd ke lb kf b kg kh ki kj kk kl km kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated">attack obj = ml _ privacy _ meter . attack . meminf . initialize(<br/>target _ train _ model = CMO dela，<br/>target _ attack _ model = CMO dela，<br/>train _ data handler = datahandlerA，<br/>attack _ data handler = datahandler，ew <br/> layers_to_exploit=[26]，<br/> gradients_to_exploit=[6]，<br/> device=None)</p></blockquote><p id="0616" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.白盒攻击—利用最后两个模型层输出、损失值和标签值</p><blockquote class="mb mc md"><p id="ce7b" class="kd ke lb kf b kg kh ki kj kk kl km kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated">attack obj = ml _ privacy _ meter . attack . meminf . initialize(<br/>target _ train _ model = CMO dela，<br/> target_attack_model=cmodelA，<br/>train _ data handler = data handlera，<br/>attack _ data handler = data handler，<br/> layers_to_exploit=[22，26]，<br/> device=None)</p></blockquote><p id="e61a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.黑盒攻击—利用最终层输出和标签值</p><blockquote class="mb mc md"><p id="d23d" class="kd ke lb kf b kg kh ki kj kk kl km kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated">attack obj = ml _ privacy _ meter . attack . meminf . initialize(<br/>target _ train _ model = CMO dela，<br/> target_attack_model=cmodelA，<br/>train _ data handler = datahandlerA，<br/>attack _ data handler = datahandler，<br/> layers_to_exploit=[26]，<br/> exploit_loss=False，<br/> device=None)</p></blockquote></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><blockquote class="mb mc md"><p id="10f8" class="kd ke lb kf b kg kh ki kj kk kl km kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated"><strong class="kf ir"> ML隐私测量仪</strong>，可从<a class="ae kc" href="https://github.com/privacytrustlab/ml_privacy_meter/tree/master/tutorials" rel="noopener ugc nofollow" target="_blank">https://github . com/privacytrustlab/ML _ Privacy _ Meter/tree/master/tutorials</a>获得</p></blockquote><p id="13ac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参考文献</strong></p><p id="8eb8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1] Shokri，Reza等，“针对机器学习模型的成员推理攻击”<em class="lb"> 2017年IEEE安全与隐私(SP)研讨会</em>。IEEE，2017。</p><p id="a50a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] ML隐私计:通过机器学习模型量化信息泄露的工具。在https://github.com/privacytrustlab/ml_privacy_meter<a class="ae kc" href="https://github.com/privacytrustlab/ml_privacy_meter" rel="noopener ugc nofollow" target="_blank">有售</a></p><p id="2ba0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]纳斯尔、肖克里和胡曼萨德。深度学习的综合隐私分析:2019年IEEE安全与隐私研讨会上被动和主动白盒推理攻击下的独立和联合学习。</p></div></div>    
</body>
</html>