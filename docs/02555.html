<html>
<head>
<title>A no-frills guide to most Natural Language Processing Models — The Transformer (XL) Era</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大多数自然语言处理模型的简明指南——Transformer(XL)时代</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f?source=collection_archive---------28-----------------------#2020-03-11">https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-the-transformer-xl-era-ff5035f04e0f?source=collection_archive---------28-----------------------#2020-03-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7260" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从通用语句编码器到Open-GPT2、(AL)BERT、XLNET和图灵-NLG</h2></div><p id="aa3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LSTMs非常受欢迎，但是它们也有很多限制。它们计算量很大，并且很难维持长期的依赖关系(尽管它们的名字如此)。2018年，谷歌发表了一篇论文<a class="ae le" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">“注意力是你所需要的一切”</a>介绍了变形金刚，表明我们可以克服递归神经网络的很多缺陷，并彻底改变语言模型领域。</p><p id="64c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">虽然这可能仍然包含一些无法解释的行话，但通过其他帖子应该很容易获得关于各种概念的信息。</em></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/19e195837afc900ecb134d37d335cd57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V0Xlh7EOosP2i9pOblqloA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">展示变形金刚时代主要型号的时间表</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="e978" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">通用句子编码器</h1><p id="5dda" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">谷歌发布了通用句子编码器，旨在提供一种句子嵌入，这种句子嵌入特别适合迁移学习，可以用于各种各样的任务(因此是“通用的”)。</p><p id="d1f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通用句子编码器最初发布了两个版本:一个利用深度平均网络(DANs)，另一个使用变压器。自最初实现以来，谷歌已经发布了许多基于<a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank"> DANs </a>或<a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder-large/5" rel="noopener ugc nofollow" target="_blank"> Transformers </a>的版本，并使它们在他们的<a class="ae le" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank"> Tensorflow Hub </a>平台上非常容易访问。所有模型都采用单词、句子或句子组，并输出512维的向量。</p><p id="e221" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了训练这个模型，谷歌使用了各种各样的数据源和任务，但主要的是围绕着在斯坦福自然语言推理语料库上识别相似的句子(所谓的“语义文本相似性”)。这种语言模型在这个任务上表现得特别好。</p><p id="5a25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优点:</strong> <br/> -该模型在句子相似度方面表现特别好<br/> -在各种各样的“开箱即用”任务方面表现相对较好<br/> -将您的输入转换为嵌入非常快(比大多数LSTM模型快得多)</p><p id="09e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:<br/> </strong> -模型可以嵌入单词或句子。但是，它是针对句子进行训练的，因此当输入不是正确的“句子”时应该更加小心使用<br/> -其特殊的训练(主要集中在句子相似性上)使其具有独特的地位，但也使其在各种任务(如文本生成)中的表现不如其他模型</p><p id="22c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Cer等人<a class="ae le" href="https://arxiv.org/abs/1803.11175" rel="noopener ugc nofollow" target="_blank">通用语句编码器</a> (2018)，ACL</p><h1 id="54ba" class="md me it bd mf mg na mi mj mk nb mm mn jz nc ka mp kc nd kd mr kf ne kg mt mu bi translated">Open-GPT2</h1><p id="c9db" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">最初，OpenAI在开发Open-GPT2时拒绝发布他们的模型，因为他们认为它太危险，并担心人们可能会恶意使用它。</p><p id="1fe5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型有大约15亿个参数，并在OpenAI的WebText数据集上进行训练，该数据集包含超过800万个文档和大约40 GB的文本数据(一些人试图复制和开源<a class="ae le" href="https://skylion007.github.io/OpenWebTextCorpus/" rel="noopener ugc nofollow" target="_blank">他们的版本</a>)。</p><p id="2b74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Open-GPT2由具有多头注意力的变压器解码器块组成，并接受语言建模训练，一次预测一个下一个令牌。由于这种训练，它特别适合文本生成。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nf"><img src="../Images/03e01088d07b0c770bd7a69fa8d393e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SbSqU8wNi5vF-Y3j.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">来自http://jalammar.github.io/illustrated-gpt2/<a class="ae le" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">的图表说明了Open-GPT2解码器的不同层次</a></p></figure><p id="fddb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">OpenAI最终向公众发布了这个模型，现在已经可以轻易获得了。</p><p id="399a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型需要大量的磁盘空间、计算和内存资源。</p><p id="b108" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优点:<br/> </strong> -由于数据集和从左到右的语言建模训练方法，在文本生成方面表现出色(明显优于BERT和其他从掩蔽语言模型学习的类似模型)</p><p id="956a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:<br/> </strong> -仅用于文本生成:该非营利组织看到了该模型在语法辅助、自动完成、创造性写作和游戏/聊天机器人创建方面的潜在用例，但这仍然有限<br/> - GPT-2占用超过5Gb，需要非常高的计算和内存要求(提取的模型有所帮助，但仍在进行中)</p><p id="6aaa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">拉德福德等人<a class="ae le" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是无监督的多任务学习者</a> (2019)</p><h1 id="6939" class="md me it bd mf mg na mi mj mk nb mm mn jz nc ka mp kc nd kd mr kf ne kg mt mu bi translated">伯特</h1><p id="5e8c" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">谷歌在2018年底发布了BERT，吸引了大量关注。该模型利用多层双向变压器架构(即带有一个编码器和一个解码器)，在广泛的任务中创下了新的记录。</p><p id="05c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">BERT主要在两个任务上接受训练:<br/> -掩蔽语言模型:该模型试图预测随机掩蔽的句子的大约15%的单词<br/> -识别下一个句子</p><p id="178a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">BERT占用大量内存，速度相对较慢。基础版和大版分别有1000万和3.4亿个参数。</p><p id="f9e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2019年，另一个研究团队发布了ALBERT，引入了参数缩减技术，允许降低内存要求和提高训练速度，同时取得了明显优于BERT的结果，在一些主要基准上达到了最先进的水平(SOTA)。ALBERT的参数比BERT-large少18倍，训练速度快1.7倍。</p><p id="a6cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">伯特和T2【艾伯特都很容易访问，你可以通过<a class="ae le" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank"> Tensorflow Hub </a>轻松创建你的嵌入。</p><p id="d016" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优势:<br/> </strong> -在多种任务(分类、句子相似度、名称实体识别、问题回答)上表现出色<br/> -在<a class="ae le" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank"> Tensorflow Hub上有超过100种不同语言的模型</a></p><p id="20a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:<br/></strong></p><p id="ddc2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Devlin等人<a class="ae le" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a> (2018)，ACL</p><p id="8b4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">兰等<a class="ae le" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank">阿尔伯特:一个用于语言表征自我监督学习的Lite BERT</a>(2019)</p><h1 id="e864" class="md me it bd mf mg na mi mj mk nb mm mn jz nc ka mp kc nd kd mr kf ne kg mt mu bi translated">XLNet</h1><p id="65f1" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">来自谷歌大脑和卡耐基梅隆大学的一组研究人员研究了BERT的主要缺陷，并利用新的<a class="ae le" href="https://arxiv.org/abs/1901.02860" rel="noopener ugc nofollow" target="_blank"> Transformer-XL </a>架构发布了XLNet，在18个NLP任务上实现了SOTA。</p><p id="6a1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">XLNet基于<a class="ae le" href="https://arxiv.org/abs/1901.02860" rel="noopener ugc nofollow" target="_blank"> Transformer-XL </a>。后者对Transformer架构进行了改进，速度快了很多(根据Google的说法是1800+),并且增加了段级递归以及相对位置编码。与标准的transformer架构相比，它能够处理更大的句子，并保留更长时间的依赖性。Transformer-XL学习依赖关系的时间分别比RNNs和标准Transformer长约80%和约450%。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/e601eb40dda4c7c8f3bd96233a9c16b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*61qlCAF4CHWbcbNA.gif"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">来自<a class="ae le" href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2019/01/Transformer-XL-unleashing-potential-of . html</a>解释Transformer-XL的段级递归机制的图形</p></figure><p id="a630" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">伯特展示了两个大问题。屏蔽的令牌出现在训练中，但不出现在微调中，导致训练测试偏斜<br/> 2。被屏蔽的令牌是并行预测的，因此它们之间的依赖关系通常不能被正确处理。</p><p id="51d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过使用“置换语言建模”，XLNet能够克服这一点。XLNet不是预测下一个令牌或前一个令牌，而是以随机顺序预测令牌。假设Transformer-XL允许它保持位置嵌入的一致性，并且该模型将定位与嵌入的剩余部分分开，并且在尝试预测屏蔽的记号时保持其可用，则该模型仍然能够以所需的顺序处理记号。</p><p id="993e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于Transformer-XL架构，以前的段被缓存和冻结，使得置换语言建模成为可能，而无需知道以前的置换顺序。</p><p id="d823" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过所有的改进，XLNet有效地取代了(AL)BERT，成为语言模型中的新参考。该模型可以很容易地在Tensorflow 2.0和PyTorch上使用<a class="ae le" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">拥抱脸的变形金刚库</a>进行使用和调整。</p><p id="7b68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优势:<br/> </strong> -在几乎所有任务上比(AL)BERT表现更好<br/> -比(AL)BERT <br/>训练和使用速度快得多-由于其“置换语言建模”和自回归方法，在文本生成方面相当不错</p><p id="00c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:<br/></strong></p><p id="7c3d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">杨等<a class="ae le" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet:面向语言理解的广义自回归预训练</a> (2019)</p><h1 id="c4b0" class="md me it bd mf mg na mi mj mk nb mm mn jz nc ka mp kc nd kd mr kf ne kg mt mu bi translated">图灵-NLG</h1><p id="5e5b" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">2020年2月，微软提出了一个有170亿个参数的生成语言模型。该模型有78个变形层和28个注意力头。</p><p id="50da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型的许多成就源于微软实现的并行化，以跨GPU训练图灵-NLG。作者充分利用了PyTorch和最近的微软库/出版物，如<a class="ae le" href="https://github.com/microsoft/DeepSpeed" rel="noopener ugc nofollow" target="_blank"> DeepSpeed </a>和<a class="ae le" href="https://www.microsoft.com/en-us/research/publication/zero-memory-optimization-towards-training-a-trillion-parameter-models/" rel="noopener ugc nofollow" target="_blank"> ZeRO </a>，这些库/出版物显著改善了具有超过10亿个参数的深度学习模型的并行训练。</p><p id="132d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目前，作者对模型的许多细节保持沉默，提到了不同的层类型，以及它是以多任务的方式训练的。尽管如此，这种规模的模型标志着大规模深度学习语言模型的重大突破，可能是未来趋势的一个标志。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nh"><img src="../Images/aa72247df88ecdd146dc0043c3232510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IuOmhZHUfG534rwF.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">图表突出显示了来自微软研究院博客的最新模型及其参数数量(<a class="ae le" href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" rel="noopener ugc nofollow" target="_blank">https://www . Microsoft . com/en-us/Research/Blog/turing-NLG-a-170亿-parameter-language-model-by-Microsoft/</a>)</p></figure><p id="0d2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个模型最近才发布，但是使用<a class="ae le" href="https://developer.msturing.org/" rel="noopener ugc nofollow" target="_blank">微软的API </a>来生成这个模型的嵌入是可能的。</p><p id="69a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优点:<br/> </strong> -在众多任务中实现SOTA，在文本生成方面非常高效</p><p id="4c83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:<br/></strong>——相对难以接近，不可能训练出自己的模型<br/>——目前还没有透露太多细节</p><p id="9bce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">罗赛特，科比。<a class="ae le" href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" rel="noopener ugc nofollow" target="_blank">图灵-NLG:微软的170亿参数语言模型</a> (2020)</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="43a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在过去的两年里，变形金刚彻底改变了NLP的世界。这个领域已经从静态嵌入(我在这里写了那些<a class="ae le" rel="noopener" target="_blank" href="/a-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c">的概述</a>)和第一个上下文化的表示(我在这里写了那些<a class="ae le" rel="noopener" target="_blank" href="/a-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687">的(其他)概述</a>)走了很长的路。Transformer、Transformer-XL和分布式计算的进步允许开发具有更多参数的模型，这些模型能够保留越来越多的长期依赖关系。一些最新的模型已经能够在许多NLP任务中击败人类，并且它们能够生成的文本越来越难与人类书写的文本区分开来。成为基准的新模型似乎每6个月就出现一次，在NLP中从未像现在这样令人兴奋。</p><p id="8c5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想从生成建模中获得乐趣并创建酷文本，我强烈推荐抱抱脸的<a class="ae le" href="https://transformer.huggingface.co/" rel="noopener ugc nofollow" target="_blank">“用变形金刚写”</a>。</p><p id="2fa0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf"> PS:我现在是伯克利的工程硕士，我还在学习这方面的知识。如果有什么需要改正或不清楚的地方，请告诉我。你也可以发邮件给我</em> <a class="ae le" href="mailto:ilias.miraoui@gmail.com" rel="noopener ugc nofollow" target="_blank"> <em class="lf">这里</em> </a> <em class="lf">。</em></p></div></div>    
</body>
</html>