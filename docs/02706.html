<html>
<head>
<title>Self-supervised Keypoint Learning — A Review</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自我监督关键点学习综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-supervised-keypoint-learning-aade18081fc3?source=collection_archive---------6-----------------------#2020-03-16">https://towardsdatascience.com/self-supervised-keypoint-learning-aade18081fc3?source=collection_archive---------6-----------------------#2020-03-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="4387" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">关键点或兴趣点检测是许多计算机视觉任务的一个重要组成部分，例如<a class="ae ko" href="https://youtu.be/ufvPS5wJAx0?t=40" rel="noopener ugc nofollow" target="_blank"> SLAM </a>(同时定位和地图绘制)<a class="ae ko" href="https://grail.cs.washington.edu/rome/" rel="noopener ugc nofollow" target="_blank">【SfM】</a>(从运动得到的结构)和<a class="ae ko" href="https://en.wikipedia.org/wiki/Chessboard_detection#Chessboard_feature_extraction" rel="noopener ugc nofollow" target="_blank">摄像机校准</a>。关键点检测在深度学习之前有很长的历史，许多广泛行业应用中的辉煌算法(如<a class="ae ko" href="https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test" rel="noopener ugc nofollow" target="_blank"> FAST </a>、<a class="ae ko" href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform" rel="noopener ugc nofollow" target="_blank"> SIFT </a>和<a class="ae ko" href="https://en.wikipedia.org/wiki/Oriented_FAST_and_rotated_BRIEF" rel="noopener ugc nofollow" target="_blank"> ORB </a>)都是基于手工制作的特征。正如在许多其他计算机视觉任务中一样，人们一直在探索如何使用深度学习来胜过手工制作的算法。在本帖中，我们将回顾这一领域的一些最新进展。</p><h2 id="76ba" class="kp kq it bd kr ks kt dn ku kv kw dp kx kb ky kz la kf lb lc ld kj le lf lg lh bi translated">语义关键点与兴趣点</h2><p id="d12d" class="pw-post-body-paragraph jq jr it js b jt li jv jw jx lj jz ka kb lk kd ke kf ll kh ki kj lm kl km kn im bi translated">在我们继续之前，让我们弄清楚一些概念。在计算机视觉中通常使用两种类型的关键点。<strong class="js iu">语义关键点</strong>是对图像中的物体具有语义意义的兴趣点，例如人脸的左眼角、人的右肩或汽车的左前轮胎轮毂。<strong class="js iu">兴趣点</strong>更多的是可能没有明确语义的低级点，比如线段的一个角点或者终点。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/27e12667dd14a6f4dd31263db8b73e4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*75sdh5nlqUCAdk2ffoNmQw.png"/></div></div></figure><p id="19dc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">深度学习方法主导了最先进的语义关键点检测。<a class="ae ko" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">Mask RCNN</strong></a>(ICCV 2017)和<a class="ae ko" href="https://arxiv.org/abs/1903.06593" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">pif PAF</strong></a>(CVPR 2019)是语义关键点检测的两种代表性方法。这些方法是监督学习，需要大量昂贵的人工注释。这使得它们难以容易地应用于兴趣点检测，因为兴趣点在语义上是不明确的，因此人类注释者不能可靠地和重复地识别同一组兴趣点。因此，不可能将兴趣点检测的任务公式化为监督学习问题。</p><p id="86c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在进入自我监督学习。自监督学习(或无监督学习，如果你关注它不需要明确的人类注释的事实)是2020年初重新出现的话题。这包括最近的进展，如FAIR的<a class="ae ko" href="https://arxiv.org/abs/1911.05722" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">、MoCo </strong> </a>和Geoffrey Hinton团队的<a class="ae ko" href="https://arxiv.org/abs/2002.05709" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> SimCLR </strong> </a>。(关于自我监督学习的更多一般趋势，我会推荐<a class="ae ko" href="https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html" rel="noopener ugc nofollow" target="_blank"> Lilian Weng的博客</a>。)与监督学习相比，自监督学习受益于每个训练样本多几个数量级的监督比特，并且不需要昂贵的和特定任务的人工注释。由于难以从人类获得可靠的注释，这非常适合于兴趣点检测。</p><p id="131a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">这篇文章的主题是将自我监督学习应用于兴趣点学习。</strong>下面我们将互换使用<em class="lz">兴趣点</em>和<em class="lz">关键点</em>。</p><p id="5fcd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面这篇博文是基于我第一次阅读这篇论文时的笔记。欢迎星/叉/评论！</p><div class="ma mb gp gr mc md"><a href="https://github.com/patrick-llgc/Learning-Deep-Learning" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd iu gy z fp mi fr fs mj fu fw is bi translated">帕特里克-llgc/学习-深度学习</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">这个知识库包含了我关于深度学习和机器学习的论文阅读笔记。它的灵感来自丹尼·布里兹…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">github.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr lx md"/></div></div></a></div><h1 id="c02c" class="ms kq it bd kr mt mu mv ku mw mx my kx mz na nb la nc nd ne ld nf ng nh lg ni bi translated">超级点</h1><p id="0a6b" class="pw-post-body-paragraph jq jr it js b jt li jv jw jx lj jz ka kb lk kd ke kf ll kh ki kj lm kl km kn im bi translated"><a class="ae ko" href="https://arxiv.org/abs/1712.07629" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> SuperPoint </strong>:自监督兴趣点检测与描述</a> (CVPR 2018)是利用自监督学习进行兴趣点检测与描述的开创性工作。总之，它首先在合成数据上预训练兴趣点检测器，然后通过使用已知的单应变换生成图像对来学习描述符。</p><p id="2393" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">寻找兴趣点的任务由<strong class="js iu">探测</strong>和<strong class="js iu">描述</strong>组成。检测是图像中感兴趣点(或特征点，或关键点，取决于文献)的定位，描述是用向量(即描述符)描述每个检测到的点。总体目标是有效和高效地找到有特征的和稳定的视觉特征。在下面的博文中，我们将看到兴趣点学习是如何处理检测和描述这两项任务的。</p><p id="bd61" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文仍然遵循许多经典算法的思路:先检测后描述。首先如何学习一个健壮的检测器？我们可以使用已知兴趣点的3D对象来绘制2D投影，例如长方体的角和线段的端点。作者称这种探测器为Magic point(Magic Leap的作者起的一个好听的名字。).</p><blockquote class="nj nk nl"><p id="4f24" class="jq jr lz js b jt ju jv jw jx jy jz ka nm kc kd ke nn kg kh ki no kk kl km kn im bi translated">现在你可能会说，这与兴趣点在语义上定义不清的事实相矛盾，但在实践中，这似乎工作得很好。当然，这就为以后的作品留下了一个需要改进的地方(比如下面讨论的<a class="ae ko" href="https://arxiv.org/abs/1907.04011" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"/></a>)。</p></blockquote><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi np"><img src="../Images/3749125296f8ae4bd3fee534fe040705.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71wsyra_GHT7UphGW7e88Q.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">MagicPoint:对合成数据进行预处理</p></figure><p id="6f6c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从合成图像到真实图像，为了弥补真实图像之间的差距，使用测试时间增强(TTA)来积累兴趣点特征。这种密集的TTA(约100次扩充)被称为“单应适应”。该步骤隐含地要求MagicPoints以低假阳性率产生高精度检测结果。聚合步骤是增加回忆和创造更多兴趣点。类似的技术也在最近的作品<a class="ae ko" href="https://arxiv.org/abs/2001.07252" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> UR2KiD </strong> </a> <strong class="js iu"> </strong>中使用(他们在一种称为<strong class="js iu">组-概念检测器-描述</strong>的技术中聚集了来自不同概念组的关键点)。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nu"><img src="../Images/3858187f8b8478f92e2791ea76b4debc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0dNdtiUgsv-qoTQ6KVWhQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">单应性适应:一种弥合模拟真实传输差距的TTA方案</p></figure><p id="fb01" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，从生成建模的角度来看，如果我们知道一幅图像的关键点，我们就可以将图像和关键点一起进行单应变换。这将生成大量的训练数据来学习描述符。作者使用<a class="ae ko" href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf" rel="noopener ugc nofollow" target="_blank">对比损失</a> (CVPR 2006，Yann LeCun的小组)来学习描述符，该描述符基本上包括成对点的拉动项和不成对点的推动项。注意这个损失中有很多项，O(N)其中N是每个图像的点数。这是将明确定义的数学问题的知识转移到神经网络的又一个例子。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nv"><img src="../Images/219daa3c579b4b40308b0abfdff71957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k0G0U99TegRAhQJ-z_am4A.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">SuperPoint对检测器和描述符使用相同的编码器，以便快速推断</p></figure><blockquote class="nj nk nl"><p id="ddc0" class="jq jr lz js b jt ju jv jw jx jy jz ka nm kc kd ke nn kg kh ki no kk kl km kn im bi translated">我发现一项特别有趣的技术是，检测器使用分类和<strong class="js iu">通道2图像</strong>技巧来实现高精度检测。总之，它将特征图中每个像素表示的输入图像中的每个8×8像素扭曲为64个通道，后跟一个垃圾箱通道。如果在8×8区域没有关键点，垃圾箱具有高激活度。否则，其他64个通道通过<em class="it"> softmax </em>在8×8区域找到关键点。</p></blockquote><p id="c553" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以上步骤很大程度上总结了SuperPoint的主要思路。</p><ul class=""><li id="b74f" class="nw nx it js b jt ju jx jy kb ny kf nz kj oa kn ob oc od oe bi translated">MagicPoint:对关键点检测器的合成数据进行预处理</li><li id="3531" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">同形适应:实像上的TTA</li><li id="7079" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">SuperPoint: MagicPoint，使用经过已知单应变换的图像对训练的描述符。描述符用于图像匹配任务。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ok"><img src="../Images/1dbd1d4f1c3d5597ad7fddd4a2f57125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xmsrr_o9cHPwqlXyADjqjA.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ko" href="https://arxiv.org/abs/1712.07629" rel="noopener ugc nofollow" target="_blank"> SuperPoint </a>的三个主要步骤</p></figure><p id="818e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在大多数基准测试中，SuperPoints优于手工算法。在配有GPU的机器上，480p (480x640)的运行速度为13毫秒或70 FPS，240p (320x240)的运行速度为167 FPS。</p><h1 id="d0c6" class="ms kq it bd kr mt mu mv ku mw mx my kx mz na nb la nc nd ne ld nf ng nh lg ni bi translated">未点</h1><p id="6dcb" class="pw-post-body-paragraph jq jr it js b jt li jv jw jx lj jz ka kb lk kd ke kf ll kh ki kj lm kl km kn im bi translated"><a class="ae ko" href="https://arxiv.org/abs/1907.04011" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"/>:端到端无监督兴趣点检测器和描述符</a>通过消除检测器预训练步骤，进一步采用了superPoint的概念。关键点在训练中自然出现。</p><p id="513c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该网络的架构在一个VGG风格的主干上有三个头。</p><ul class=""><li id="142a" class="nw nx it js b jt ju jx jy kb ny kf nz kj oa kn ob oc od oe bi translated">第一个输出是粗略评分热图(x8下采样)。它消除了NMS的需要，并鼓励关键点更均匀地分布在图像上。</li><li id="9c4b" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">子像素位置回归以预测8×8像素区域内的精确位置。</li><li id="ac69" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">描述符描述。它使用子像素位置进行插值。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ol"><img src="../Images/5da3d189e68efa4195271b3d703ba235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_PIJ6L5Hub5Pjj2kb_lZhQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ko" href="https://arxiv.org/abs/1907.04011" rel="noopener ugc nofollow" target="_blank">未点</a>的网络架构</p></figure><p id="8996" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">SuperPoint之间有几个区别/改进:</p><ul class=""><li id="4770" class="nw nx it js b jt ju jx jy kb ny kf nz kj oa kn ob oc od oe bi translated">没有对关键点位置的直接监督。没有对合成数据和单应自适应繁琐的预处理。</li><li id="899a" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">用于亚像素位置预测的直接回归而不是逐通道分类。</li><li id="75ca" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">插值发生在网络内部。SuperPoint在推理后做插值，占用宝贵的CPU周期。</li></ul><p id="4108" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">未点引入吨损失！平衡它们是一项艰巨的任务。为了计算这些损失，必须建立点对对应关系。UnsuperPoint使用了贪婪匹配算法。如果目标图像(pt)中的点在已知的单应变换(pt*)下变形到源图像后，在源图像中有一个关键点(ps)在阈值(4个像素)内，那么ps和pt称为<em class="lz">点对</em>。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi om"><img src="../Images/61c6d9431a82d64823b746249301e06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gTbSIgzz7tpAuRerUXM48Q.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">寻找点对(来源:<a class="ae ko" href="https://arxiv.org/pdf/1912.03426.pdf" rel="noopener ugc nofollow" target="_blank"> KP3D </a></p></figure><blockquote class="nj nk nl"><p id="5051" class="jq jr lz js b jt ju jv jw jx jy jz ka nm kc kd ke nn kg kh ki no kk kl km kn im bi translated">注意，这不是匈牙利匹配，因此不是内射的。换句话说，目标图像中的一个点在源图像中可能有多个匹配。在没有直接监督的其他数据协会论文中可以找到类似的匹配步骤，例如，<a class="ae ko" href="https://arxiv.org/abs/1706.07365" rel="noopener ugc nofollow" target="_blank">像素到图形</a> (NeurIPS 2017)。</p></blockquote><p id="30d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">利用定义的点对，定义了以下损失项。</p><ul class=""><li id="1f22" class="nw nx it js b jt ju jx jy kb ny kf nz kj oa kn ob oc od oe bi translated"><strong class="js iu">点对的位置损失</strong>:像素坐标间的欧几里德L2距离。</li><li id="d2ce" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">点对的分数损失:点对中的点应该有相似的分数。</li><li id="a354" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">匹配损失:它确保点分数实际上是关键点的置信度分数。它鼓励网络为网络认为在单应变换下可以可靠检索的点输出高分。具体地说，它是点对的平均得分和零均值距离的乘积。</li><li id="1e07" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">均匀分布损失:它鼓励[0，1]内分布均匀。这取决于排序函数是可微的这一事实。这是第一次(正如作者所声称的，据我所知)损失被应用于强制神经网络预测的特定分布。</li><li id="eff4" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">描述符损失:对比损失，同SuperPoint。</li><li id="848b" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">去相关损失:鼓励描述符之间的维度是独立的。具体是相关矩阵中非对角元素的L2范数(见<a class="ae ko" href="http://www.nlpr.ia.ac.cn/fanbin/pub/L2-Net_CVPR17.pdf" rel="noopener ugc nofollow" target="_blank">ICCV L2网2017 </a>)。</li></ul><p id="1ee7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过仔细平衡这些损失项，UnsuperPoint在准确性和速度方面都优于superPoint。UnsuperPoint对于480p图像以65 FPS运行，对于240p图像以119 FPS运行，比superPoint稍慢。</p><h1 id="ab24" class="ms kq it bd kr mt mu mv ku mw mx my kx mz na nb la nc nd ne ld nf ng nh lg ni bi translated">KP2D</h1><p id="6948" class="pw-post-body-paragraph jq jr it js b jt li jv jw jx lj jz ka kb lk kd ke kf ll kh ki kj lm kl km kn im bi translated"><a class="ae ko" href="https://arxiv.org/abs/1912.10615" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">【KP2D】</strong>:自监督关键点学习的神经离群点剔除</a> (ICLR 2020)是丰田研究所的后续工作之一。虽然工作看起来是渐进的，但是建议的技巧产生了具体的改进。</p><p id="5e4f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">相对于UnsuperPoint有三大改进。</p><ul class=""><li id="93a5" class="nw nx it js b jt ju jx jy kb ny kf nz kj oa kn ob oc od oe bi translated">一种更具表现力的关键点位置回归方法。UnsuperPoint将子像素回归的输出限制在[0，1]内，甚至使用了专门设计的损失来强制每个8×8像素区域中的均匀分布。KP2D没有鼓励[0，1]的分布是均匀的，而是深入研究了分布不均匀的原因，并将值放宽到[0，1]之外，允许每个8×8像素区域为其边界之外的关键点投票。</li></ul><blockquote class="nj nk nl"><p id="72ba" class="jq jr lz js b jt ju jv jw jx jy jz ka nm kc kd ke nn kg kh ki no kk kl km kn im bi translated">我认为这是因为点对匹配不是内射的。仅通过距离阈值就可以将源图像中的一个点匹配到目标图像中的多个点。</p></blockquote><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi on"><img src="../Images/120277de00e81b37e6464d8a4cada7e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YWMwbf-KjCJnYWkUuZCEfA.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">KP2D中的每个8×8像素区域可以为其相邻区域中的关键点投票</p></figure><ul class=""><li id="e45a" class="nw nx it js b jt ju jx jy kb ny kf nz kj oa kn ob oc od oe bi translated">快速和学习的上采样层，即亚像素ConvNet ( <a class="ae ko" href="https://arxiv.org/abs/1609.05158" rel="noopener ugc nofollow" target="_blank">使用高效亚像素卷积神经网络</a>，CVPR 2016)的实时单个图像和视频超分辨率)用于在执行插值之前将特征地图上采样2x2。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi oo"><img src="../Images/e831b73ac66e262bd4730c47e41b8e70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KTMYVvqDHmhrwXMXJMY5Ow.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">用于快速超分辨率的亚像素卷积(通道2空间)</p></figure><ul class=""><li id="0a62" class="nw nx it js b jt ju jx jy kb ny kf nz kj oa kn ob oc od oe bi translated">基于异常值拒绝的附加代理任务，仅在训练期间使用。所谓的InlierOutlierNet (IO-Net)的灵感来自于<a class="ae ko" href="https://arxiv.org/abs/1905.04132" rel="noopener ugc nofollow" target="_blank">神经引导的Ransac </a> (ICCV 2019)，其本身就是受PointNet的启发。IO-Net接受一个点对(2个点的坐标，带有一个数量的附加描述符距离),并预测该点对是否是用于下游任务(如单应性估计)的Ransac类鲁棒匹配方案中的内侧集。</li></ul><p id="2bd1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，在超点和非超点中使用的对比损失由三重损失代替。总体而言，KP2D始终比UnsuperPoint表现出色。它的运行速度与SuperPoint和UnsuperPoint差不多，在240p (320x240)下为175 FPS。</p><h1 id="ece5" class="ms kq it bd kr mt mu mv ku mw mx my kx mz na nb la nc nd ne ld nf ng nh lg ni bi translated">KP3D</h1><p id="d9d5" class="pw-post-body-paragraph jq jr it js b jt li jv jw jx lj jz ka kb lk kd ke kf ll kh ki kj lm kl km kn im bi translated"><a class="ae ko" href="https://arxiv.org/abs/1912.03426" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> KP3D </strong>:用于自我运动估计的自监督3D关键点学习</a> (ArXiv 12/2019)由来自丰田研究所的KP2D的相同作者完成，并将自监督关键点学习的思想进一步推进到SLAM的端到端视觉里程表前端。KP3D结合了近年来新兴的两个热门话题:<strong class="js iu">基于视频的关键点自监督学习</strong>(如KP2D)和<strong class="js iu">单目逐像素深度估计</strong>(如<a class="ae ko" href="https://github.com/patrick-llgc/Learning-Deep-Learning/blob/master/paper_notes/sfm_learner.md" rel="noopener ugc nofollow" target="_blank"> sfm学习器</a>、<a class="ae ko" href="https://github.com/patrick-llgc/Learning-Deep-Learning/blob/master/paper_notes/mono_depth_video_in_the_wild.md" rel="noopener ugc nofollow" target="_blank">野外深度</a>和<a class="ae ko" href="https://github.com/patrick-llgc/Learning-Deep-Learning/blob/master/paper_notes/sc_sfm_learner.md" rel="noopener ugc nofollow" target="_blank">尺度一致sfm学习器</a>)。</p><p id="fbff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">KP3D借鉴了KP2D (ICLR 2020)和<a class="ae ko" href="https://arxiv.org/abs/1908.10553" rel="noopener ugc nofollow" target="_blank">规模一致的sfm学习者</a> (NeurIPS 2019)在上述两个领域的SOTA表现。特别地，在尺度一致的sfm中，学习器增加了帧之间的深度一致性，这对于精确的自我运动估计是至关重要的。</p><p id="b4f9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">综合来看，KP3D有一个令人惊叹的SLAM演示视频。作为一种单目方法，KP3D击败了所有以前的SOTA单目和几乎所有的立体SLAM方法。它唯一没有胜过的立体SLAM(<a class="ae ko" href="https://arxiv.org/abs/1807.02570" rel="noopener ugc nofollow" target="_blank">DVSO:深度视觉立体里程计</a>，ECCV 2018)也是基于深度学习的方法。</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="op oq l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi or"><img src="../Images/bd062a9bec6a3ace5d41ec87a267d62d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xlETLGzNCn9NtNvuXMVGyA.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">2D关键点完全从无标签的单目视频中学习，匹配稀疏缓慢，并在KP3D中产生惊人的远程单目视觉里程计</p></figure><h2 id="6097" class="kp kq it bd kr ks kt dn ku kv kw dp kx kb ky kz la kf lb lc ld kj le lf lg lh bi translated">注释</h2><p id="7bec" class="pw-post-body-paragraph jq jr it js b jt li jv jw jx lj jz ka kb lk kd ke kf ll kh ki kj lm kl km kn im bi translated">乍一看，这篇论文数学很重。确切地说，它需要更多的SLAM领域知识来理解。为了更深入地研究算法，我们必须考虑一些符号约定:</p><ul class=""><li id="4057" class="nw nx it js b jt ju jx jy kb ny kf nz kj oa kn ob oc od oe bi translated">pt∈是目标图像中的关键点，PS∈是源图像中的关键点。</li><li id="8233" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">pt(MV)∈是基于描述子空间的源图像中pt的匹配关键点。(MV是多视图的简写)基于这对pt↔pt(MV，我们可以计算相关的自我运动Xt→s。描述符损失基于此。</li><li id="d47a" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">pt∑∈是源图像中的扭曲pt(类似于UnsuperPoint和KP2D中的pt*)。稀疏关键点位置损失介于pt(MV)和pt *之间。</li><li id="07e2" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">一旦Xt→s已知，密集光度损失和稀疏关键点位置损失被公式化。</li></ul><p id="5732" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在整个流水线中，计算pt* <em class="lz">是最难的。</em>在以前的2D关键点作品(例如，SuperPoint、UnsuperPoint、KP2D)中，pt*可以很容易地计算，但是在多视图适应中，这很难，并且需要首先用估计的深度投影到3D，然后用估计的自我运动投影到另一个视图。</p><blockquote class="nj nk nl"><p id="c4a9" class="jq jr lz js b jt ju jv jw jx jy jz ka nm kc kd ke nn kg kh ki no kk kl km kn im bi translated">我认为KP3D的作者误解了单应适应的原始命名，这是SuperPoint中第<strong class="js iu">第二</strong>阶段使用的TTA技术。他们似乎把这种技术误认为是第<strong class="js iu">第三</strong>阶段的超级要点，即利用已知的单应变换来产生训练信号。这个细节不会对我们理解这篇论文有太大影响。</p></blockquote><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi os"><img src="../Images/4ae376dc4f0b0cb7b81994ac730616e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YNkhEQ_qgsRpMmee9rtPiA.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">根据多视图适应变换的各种关键点</p></figure><h2 id="7aeb" class="kp kq it bd kr ks kt dn ku kv kw dp kx kb ky kz la kf lb lc ld kj le lf lg lh bi translated">可微分三维姿态估计</h2><p id="f743" class="pw-post-body-paragraph jq jr it js b jt li jv jw jx lj jz ka kb lk kd ke kf ll kh ki kj lm kl km kn im bi translated">KP3D不是直接使用CNN来进行姿态估计(在<a class="ae ko" href="https://github.com/patrick-llgc/Learning-Deep-Learning/blob/master/paper_notes/sfm_learner.md" rel="noopener ugc nofollow" target="_blank"> sfm学习器</a>中的PoseNet)，而是使用匹配的关键点来进行姿态估计，这可能是更好的性能的关键，因为上面回顾的上述2D关键点学习方法已知会产生非常好的HA或单应性准确性。</p><p id="6da8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文的一个主要贡献是如何以完全可微的方式从匹配的2D点执行姿态估计。快速入门经典算法基于<a class="ae ko" href="https://en.wikipedia.org/wiki/Epipolar_geometry" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">极线几何</strong> </a>从两幅图像中匹配的2D点估计自我运动。基本思想是估计图像对的基本矩阵或<a class="ae ko" href="https://en.wikipedia.org/wiki/Essential_matrix" rel="noopener ugc nofollow" target="_blank">本质矩阵</a>，然后从任一矩阵中恢复旋转和平移。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/1fde38d234c201bf1812360ddc1aab58.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/0*V4F6JKicB1wv-LT5.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ko" href="https://en.wikipedia.org/wiki/Epipolar_geometry" rel="noopener ugc nofollow" target="_blank">极线约束</a>:三维点及其在2D图像上的投影之间的几何关系</p></figure><p id="674c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为我们联合估计每个像素的密集深度，所以我们可以使用Epnp ( <a class="ae ko" href="https://en.wikipedia.org/wiki/Perspective-n-Point" rel="noopener ugc nofollow" target="_blank">透视n点</a>)来估计帧之间的相对姿态。然而，这是不可微的，因此只能作为一个初步的猜测。</p><p id="97b0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">KP3d提出的方法是，我们可以粗略地使用关键点的变换的3D位置(具有初始猜测)来获得点在新的相机坐标中的3D位置。那么本质上，该问题被简化为基于两组匹配的3D点来估计相对相机姿态变化。这个问题被称为正交Procrustes问题，具有诸如ICP(迭代闭合点，通常用于激光雷达点云配准)的良好解决方案。在我们知道3D点之间的匹配的假设下，ICP具有基于SVD(矩阵的奇异值分解)的封闭形式的公式，称为<a class="ae ko" href="https://en.wikipedia.org/wiki/Kabsch_algorithm" rel="noopener ugc nofollow" target="_blank"> Kabsch算法</a>。</p><h1 id="5b4b" class="ms kq it bd kr mt mu mv ku mw mx my kx mz na nb la nc nd ne ld nf ng nh lg ni bi translated">外卖食品</h1><p id="1a08" class="pw-post-body-paragraph jq jr it js b jt li jv jw jx lj jz ka kb lk kd ke kf ll kh ki kj lm kl km kn im bi translated">近年来，使用深度学习的关键点检测和描述领域受到了越来越多的关注。正如KP2D的公开评论中所指出的，“<a class="ae ko" href="https://openreview.net/forum?id=Skx82ySYPH&amp;noteId=rklyawTEYS" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">这个问题是老问题，但还没有完全解决，因为手工制作的SIFT仍然在基准测试中胜出。</strong> </a></p><ul class=""><li id="a23e" class="nw nx it js b jt ju jx jy kb ny kf nz kj oa kn ob oc od oe bi translated">SuperPoint提出使用已知单应变换来生成用于关键点检测和描述的监控信号。这是这篇文章中所有论文探索的开创性思想。</li><li id="8107" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">SuperPoint使用合成数据来预训练关键点探测器，并使用称为单应自适应的测试时间增加技术来转换到真实数据。</li><li id="0d6c" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">UnsuperPoint建议取消关键点检测器繁琐的预训练阶段。通过巧妙地平衡损失，关键点的检测和描述都将从训练中出现。</li><li id="6a3c" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">KP2D用几种技术改进了UnsuperPoint，最显著的是用代理损失来确定建议的点对是否匹配inlier。</li><li id="0212" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated">KP3D结合了KP2D的工作和单目深度估计的最新进展，以提供基于SOTA单目SLAM系统。</li></ul><h1 id="adf9" class="ms kq it bd kr mt mu mv ku mw mx my kx mz na nb la nc nd ne ld nf ng nh lg ni bi translated">参考</h1><ul class=""><li id="16df" class="nw nx it js b jt li jx lj kb ou kf ov kj ow kn ob oc od oe bi translated"><a class="ae ko" href="https://arxiv.org/abs/1712.07629" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">超级点</strong>:自监督兴趣点检测与描述</a>，CVPR 2018</li><li id="b5b8" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated"><a class="ae ko" href="https://arxiv.org/abs/1907.04011" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">非重叠点</strong>:端到端非监督兴趣点检测器和描述符</a>，ArXiv 07/2019</li><li id="864c" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated"><a class="ae ko" href="https://arxiv.org/abs/1912.10615" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> KP2D </strong>:自监督关键点学习的神经离群点剔除</a>，ICLR 2020</li><li id="e9f7" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated"><a class="ae ko" href="https://arxiv.org/abs/1912.03426" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> KP3D </strong>:用于自我运动估计的自监督3D关键点学习</a>，ArXiv 12/2019</li><li id="659c" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated"><a class="ae ko" href="https://arxiv.org/pdf/1807.02570.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> DVSO: </strong>深度虚拟立体里程计:利用深度预测进行单目直接稀疏里程计</a>，ECCV 2018</li><li id="7163" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated"><a class="ae ko" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">面具RCNN </strong> </a>，ICCV 2017</li><li id="b307" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated"><a class="ae ko" href="https://arxiv.org/abs/1903.06593" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"/>:人体姿态估计的复合场</a>，CVPR 2019</li><li id="0d99" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated"><strong class="js iu"> SC-SfM-Learner </strong> : <a class="ae ko" href="https://arxiv.org/abs/1908.10553" rel="noopener ugc nofollow" target="_blank">来自单目视频的无监督尺度一致深度和自我运动学习</a>，NeurIPS 2019</li><li id="9408" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated"><a class="ae ko" href="https://arxiv.org/abs/1905.04132" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">神经引导的RANSAC </strong>:学习从哪里采样模型假设</a> ICCV 2019</li><li id="aaaf" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated"><strong class="js iu">亚像素卷积神经网络</strong> : <a class="ae ko" href="https://arxiv.org/abs/1609.05158" rel="noopener ugc nofollow" target="_blank">使用高效亚像素卷积神经网络的实时单幅图像和视频超分辨率</a>，CVPR 2016</li><li id="0c56" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated"><a class="ae ko" href="https://arxiv.org/abs/1911.05722" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> MoCo: </strong>用于无监督视觉表征学习的动量对比</a>，ArXiv 11/2019</li><li id="334c" class="nw nx it js b jt of jx og kb oh kf oi kj oj kn ob oc od oe bi translated"><a class="ae ko" href="https://arxiv.org/abs/2002.05709" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"/>:视觉表征对比学习的简单框架</a>，ArXiv 02/2020</li></ul></div></div>    
</body>
</html>