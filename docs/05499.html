<html>
<head>
<title>PyTorch[Vision] — Multiclass Image Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">py torch[视觉] —多类图像分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-vision-multiclass-image-classification-531025193aa?source=collection_archive---------9-----------------------#2020-05-09">https://towardsdatascience.com/pytorch-vision-multiclass-image-classification-531025193aa?source=collection_archive---------9-----------------------#2020-05-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/840b4e62b667c78ce2f768c8b11e4488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Dsdw-L4qVhT1WkyLvtsPg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">如何训练你的神经网络[图片[0]]</p></figure><h2 id="e64f" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/akshaj-wields-pytorch" rel="noopener">如何训练你的神经网络</a></h2><div class=""/><p id="bf2a" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">本笔记本将带您使用PyTorch上的<a class="ae lj" href="http://www.laurencemoroney.com/rock-paper-scissors-dataset/" rel="noopener ugc nofollow" target="_blank">石头剪刀布</a>数据集，通过CNN实现多类图像分类。</p><h1 id="aedc" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">导入库</h1><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="9e9e" class="mr ll jf mn b gy ms mt l mu mv">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from tqdm.notebook import tqdm<br/>import matplotlib.pyplot as plt<br/>import torch<br/>import torchvision<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>import torch.nn.functional as F<br/>from torchvision import transforms, utils, datasets<br/>from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler<br/>from sklearn.metrics import classification_report, confusion_matrix</span></pre><p id="1336" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">设置随机种子。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="2078" class="mr ll jf mn b gy ms mt l mu mv">np.random.seed(0)<br/>torch.manual_seed(0)</span></pre><p id="490c" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">设置<code class="fe mw mx my mn b">Seaborn</code>样式。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="6882" class="mr ll jf mn b gy ms mt l mu mv">%matplotlib inline<br/>sns.set_style('darkgrid')</span></pre><h1 id="4c95" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">定义路径并设置GPU</h1><p id="560a" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">让我们定义数据的路径。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="f1a2" class="mr ll jf mn b gy ms mt l mu mv">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")</span><span id="c759" class="mr ll jf mn b gy ne mt l mu mv">print("We're using =&gt;", device)root_dir = "../../../data/computer_vision/image_classification/hot-dog-not-hot-dog/"<br/>print("The data lies here =&gt;", root_dir)</span><span id="c729" class="mr ll jf mn b gy ne mt l mu mv">###################### OUTPUT ######################</span><span id="d06a" class="mr ll jf mn b gy ne mt l mu mv">We're using =&gt; cuda<br/>The data lies here =&gt; ../../../data/computer_vision/image_classification/hot-dog-not-hot-dog/</span></pre><h1 id="18e6" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">定义转换</h1><p id="7643" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">让我们定义一个字典来保存训练/测试集的图像转换。我们将调整所有图像的大小为大小(224，224 ),并将图像转换为张量。</p><p id="3ecc" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">PyTorch中的<code class="fe mw mx my mn b">ToTensor</code>操作将所有张量转换到(0，1)之间。</p><blockquote class="nf ng nh"><p id="9e46" class="kl km ni kn b ko kp kq kr ks kt ku kv nj kx ky kz nk lb lc ld nl lf lg lh li ij bi translated"><code class="fe mw mx my mn b">ToTensor</code>将范围为[0，255]的PIL图像或<code class="fe mw mx my mn b">numpy.ndarray</code>(高x宽x高)转换为范围为[0.0，1.0]的形状的<code class="fe mw mx my mn b">torch.FloatTensor</code></p></blockquote><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="d720" class="mr ll jf mn b gy ms mt l mu mv">image_transforms = {<br/>    "train": transforms.Compose([<br/>        transforms.Resize((224, 224)),<br/>        transforms.ToTensor()<br/>    ]),<br/>    "test": transforms.Compose([<br/>        transforms.Resize((224, 224)),<br/>        transforms.ToTensor()<br/>}</span></pre><h1 id="646f" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">定义转换</h1><p id="2cdb" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">让我们定义一个字典来保存训练/测试集的图像转换。所有图像的尺寸都是<strong class="kn jp"> (300，300) </strong>。我们仍将调整所有图像的大小(以防止错误)为大小<strong class="kn jp"> (300，300) </strong>，并将图像转换为张量。PyTorch中的<code class="fe mw mx my mn b">ToTensor</code>操作将所有张量转换到(0，1)之间。</p><blockquote class="nf ng nh"><p id="34bc" class="kl km ni kn b ko kp kq kr ks kt ku kv nj kx ky kz nk lb lc ld nl lf lg lh li ij bi translated"><code class="fe mw mx my mn b"><em class="jf">ToTensor</em></code> <em class="jf">将范围[0，255]中的PIL图像或</em> <code class="fe mw mx my mn b"><em class="jf">numpy.ndarray</em></code> <em class="jf"> (H x W x C)转换为范围[0，1.0] </em>中的形状的 <code class="fe mw mx my mn b"><em class="jf">torch.FloatTensor</em></code> <em class="jf"/></p></blockquote><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="0874" class="mr ll jf mn b gy ms mt l mu mv">image_transforms = {<br/>    "train": transforms.Compose([<br/>        transforms.Resize((300, 300)),<br/>        transforms.ToTensor(),<br/>        transforms.Normalize([0.5, 0.5, 0.5],<br/>                             [0.5, 0.5, 0.5])<br/>    ]),<br/>    "test": transforms.Compose([<br/>        transforms.Resize((300, 300)),<br/>        transforms.ToTensor(),<br/>        transforms.Normalize([0.5, 0.5, 0.5],<br/>                             [0.5, 0.5, 0.5])<br/>    ])<br/>}</span></pre><h1 id="4be6" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">初始化数据集</h1><h2 id="914d" class="mr ll jf bd lm nm nn dn lq no np dp lu kw nq nr ly la ns nt mc le nu nv mg jl bi translated">训练+验证数据集</h2><p id="f671" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">我们带着2个数据集文件夹— <strong class="kn jp">训练</strong>和<strong class="kn jp">测试</strong>。</p><p id="0f9d" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们将进一步划分我们的<strong class="kn jp">列车</strong>集合为<strong class="kn jp">列车+ Val </strong>。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="c963" class="mr ll jf mn b gy ms mt l mu mv">rps_dataset = datasets.ImageFolder(root = root_dir + "train",<br/>                                   transform = image_transforms["train"]<br/>                                  )</span><span id="33f5" class="mr ll jf mn b gy ne mt l mu mv">rps_dataset</span><span id="e62b" class="mr ll jf mn b gy ne mt l mu mv"><br/>###################### OUTPUT ######################</span><span id="17b4" class="mr ll jf mn b gy ne mt l mu mv">Dataset ImageFolder<br/>    Number of datapoints: 2520<br/>    Root location: ../../../data/computer_vision/image_classification/rock-paper-scissor/train<br/>    StandardTransform<br/>Transform: Compose(<br/>               Resize(size=(300, 300), interpolation=PIL.Image.BILINEAR)<br/>               ToTensor()<br/>               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])<br/>           )</span></pre><h2 id="1d8c" class="mr ll jf bd lm nm nn dn lq no np dp lu kw nq nr ly la ns nt mc le nu nv mg jl bi translated">输出的类&lt;=&gt; ID映射</h2><p id="0257" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">PyTorch内置了<code class="fe mw mx my mn b">class_to_idx</code>函数。它返回数据集中的类ID。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="68b1" class="mr ll jf mn b gy ms mt l mu mv">rps_dataset.class_to_idx</span><span id="5827" class="mr ll jf mn b gy ne mt l mu mv"><br/>###################### OUTPUT ######################<br/>{'paper': 0, 'rock': 1, 'scissors': 2}</span></pre><p id="f488" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们现在将构建该字典的反向；ID到类的映射。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="f729" class="mr ll jf mn b gy ms mt l mu mv">idx2class = {v: k for k, v in rps_dataset.class_to_idx.items()}<br/>idx2class</span><span id="c51b" class="mr ll jf mn b gy ne mt l mu mv"><br/>###################### OUTPUT ######################</span><span id="e608" class="mr ll jf mn b gy ne mt l mu mv">{0: 'paper', 1: 'rock', 2: 'scissors'}</span></pre><p id="4bac" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">让我们还编写一个函数，它接受一个dataset对象并返回一个包含类样本计数的字典。我们将使用这个字典来构建图，并观察我们的数据中的类分布。</p><p id="34e4" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe mw mx my mn b">get_class_distribution()</code>接受一个名为<code class="fe mw mx my mn b">dataset_obj</code>的参数。</p><ul class=""><li id="9000" class="nw nx jf kn b ko kp ks kt kw ny la nz le oa li ob oc od oe bi translated">我们首先初始化一个<code class="fe mw mx my mn b">count_dict</code>字典，其中所有类的计数都被初始化为0。</li><li id="f94a" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">然后，让我们遍历数据集，并为循环中遇到的每个类标签将计数器加1。</li></ul><p id="162f" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe mw mx my mn b">plot_from_dict()</code>接受3个参数:一个名为<code class="fe mw mx my mn b">dict_obj</code>、<code class="fe mw mx my mn b">plot_title</code>和<code class="fe mw mx my mn b">**kwargs</code>的字典。我们传入<code class="fe mw mx my mn b">**kwargs</code>是因为稍后，我们将构建需要在seaborn中传递<code class="fe mw mx my mn b">ax</code>参数的支线剧情。</p><ul class=""><li id="3483" class="nw nx jf kn b ko kp ks kt kw ny la nz le oa li ob oc od oe bi translated">首先，将字典转换成数据帧。</li><li id="bc16" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">熔化数据框并绘图。</li></ul><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="20ce" class="mr ll jf mn b gy ms mt l mu mv">def get_class_distribution(dataset_obj):<br/>    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}</span><span id="f7e5" class="mr ll jf mn b gy ne mt l mu mv">    for _, label_id in dataset_obj:<br/>        label = idx2class[label_id]<br/>        count_dict[label] += 1<br/>    return count_dict<br/></span><span id="5447" class="mr ll jf mn b gy ne mt l mu mv">def plot_from_dict(dict_obj, plot_title, **kwargs):<br/>    return sns.barplot(data = pd.DataFrame.from_dict([dict_obj]).melt(), x = "variable", y="value", hue="variable", **kwargs).set_title(plot_title)</span><span id="a9e3" class="mr ll jf mn b gy ne mt l mu mv">plt.figure(figsize=(15,8))<br/>plot_from_dict(get_class_distribution(rps_dataset), plot_title="Entire Dataset (before train/val/test split)")</span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/88e3e231e693e1f00ace11d4f1abc147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h76rZXX9CFtUVKnNWYs1cA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">数据分布[图像[1]]</p></figure><h1 id="f8a6" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">获取训练和验证样本</h1><p id="8428" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">我们使用<code class="fe mw mx my mn b">SubsetRandomSampler</code>来制作我们的训练和验证加载器。<code class="fe mw mx my mn b">SubsetRandomSampler</code>用于使每批接收一个随机分布的类。</p><p id="f42c" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们也可以将数据集分成两部分——train和val，即。使2 <code class="fe mw mx my mn b">Subsets</code>。但是这更简单，因为我们的数据加载器现在几乎可以处理所有的事情。</p><p id="053d" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe mw mx my mn b">SubsetRandomSampler(indices)</code>将数据的索引作为输入。</p><p id="de72" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们首先创建我们的采样器，然后将它传递给我们的数据加载器。</p><ul class=""><li id="062e" class="nw nx jf kn b ko kp ks kt kw ny la nz le oa li ob oc od oe bi translated">创建索引列表。</li><li id="ab2a" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">打乱索引。</li><li id="5fa3" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">根据列车价值百分比拆分指数。</li><li id="d8a7" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">创建<code class="fe mw mx my mn b">SubsetRandomSampler</code>。</li></ul><p id="6ff6" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">创建从0到数据集长度的索引列表。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="e774" class="mr ll jf mn b gy ms mt l mu mv">rps_dataset_size = len(rps_dataset)<br/>rps_dataset_indices = list(range(rps_dataset_size))</span></pre><p id="65c5" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">使用np.shuffle打乱索引列表。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="3125" class="mr ll jf mn b gy ms mt l mu mv">np.random.shuffle(rps_dataset_indices)</span></pre><p id="bac4" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">创建拆分索引。我们选择拆分索引为数据集大小的20% (0.2)。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="aca9" class="mr ll jf mn b gy ms mt l mu mv">val_split_index = int(np.floor(0.2 * rps_dataset_size))</span></pre><p id="3ae5" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">将列表切片以获得2个索引列表，一个用于训练，另一个用于测试。</p><blockquote class="ol"><p id="b0d8" class="om on jf bd oo op oq or os ot ou li dk translated"><code class="fe mw mx my mn b">0-----------val_split_index------------------------------n</code></p><p id="bbee" class="om on jf bd oo op oq or os ot ou li dk translated">Train =&gt; val_split_index到n</p><p id="96ce" class="om on jf bd oo op oq or os ot ou li dk translated">Val =&gt; 0到val_split_index</p></blockquote><pre class="ov ow ox oy oz mm mn mo mp aw mq bi"><span id="af5a" class="mr ll jf mn b gy ms mt l mu mv">train_idx, val_idx = rps_dataset_indices[val_split_index:], rps_dataset_indices[:val_split_index]</span></pre><p id="3165" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">最后，创建采样器。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="2ff6" class="mr ll jf mn b gy ms mt l mu mv">train_sampler = SubsetRandomSampler(train_idx)<br/>val_sampler = SubsetRandomSampler(val_idx)</span></pre><h1 id="233c" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">试验</h1><p id="3b1c" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">既然我们已经完成了训练和赋值数据，让我们加载测试数据集。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="f316" class="mr ll jf mn b gy ms mt l mu mv">rps_dataset_test = datasets.ImageFolder(root = root_dir + "test",<br/>                                        transform = image_transforms["test"])</span><span id="50ab" class="mr ll jf mn b gy ne mt l mu mv">rps_dataset_test</span><span id="18ff" class="mr ll jf mn b gy ne mt l mu mv"><br/>###################### OUTPUT ######################</span><span id="0d16" class="mr ll jf mn b gy ne mt l mu mv">Dataset ImageFolder<br/>    Number of datapoints: 372<br/>    Root location: ../../../data/computer_vision/image_classification/rock-paper-scissor/test<br/>    StandardTransform<br/>Transform: Compose(<br/>               Resize(size=(300, 300), interpolation=PIL.Image.BILINEAR)<br/>               ToTensor()<br/>               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])<br/>           )</span></pre><h1 id="240a" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">培训、验证和测试数据加载器</h1><p id="02a2" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">现在，我们将把采样器传递给我们的数据加载器。请注意，当您使用<code class="fe mw mx my mn b">SubsetRandomSampler</code>时，不能使用<code class="fe mw mx my mn b">shuffle=True</code>。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="04b2" class="mr ll jf mn b gy ms mt l mu mv">train_loader = DataLoader(dataset=rps_dataset, shuffle=False, batch_size=8, sampler=train_sampler)</span><span id="c91f" class="mr ll jf mn b gy ne mt l mu mv">val_loader = DataLoader(dataset=rps_dataset, shuffle=False, batch_size=1, sampler=val_sampler)</span><span id="4067" class="mr ll jf mn b gy ne mt l mu mv">test_loader = DataLoader(dataset=rps_dataset_test, shuffle=False, batch_size=1)</span></pre><h1 id="c16c" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">探索数据</h1><p id="b57d" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">为了研究我们train和val数据加载器，让我们创建一个新函数，它接收一个数据加载器并返回一个包含类计数的字典。</p><ul class=""><li id="122b" class="nw nx jf kn b ko kp ks kt kw ny la nz le oa li ob oc od oe bi translated">将字典<code class="fe mw mx my mn b">count_dict</code>初始化为全0。</li><li id="8010" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">如果<code class="fe mw mx my mn b">dataloader_obj</code>的batch_size为1，则循环通过<code class="fe mw mx my mn b">dataloader_obj</code>并更新计数器。</li><li id="17bd" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">否则，如果<code class="fe mw mx my mn b">dataloader_obj</code>的batch_size是<strong class="kn jp">而不是</strong> 1，则循环通过<code class="fe mw mx my mn b">dataloader_obj</code>以获得批次。循环遍历批以获得单个张量。现在，相应地更新了计数器。</li></ul><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="a05c" class="mr ll jf mn b gy ms mt l mu mv">def get_class_distribution_loaders(dataloader_obj, dataset_obj):<br/>    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}</span><span id="7199" class="mr ll jf mn b gy ne mt l mu mv">    if dataloader_obj.batch_size == 1:    <br/>        for _,label_id in dataloader_obj:<br/>            y_idx = label_id.item()<br/>            y_lbl = idx2class[y_idx]<br/>            count_dict[str(y_lbl)] += 1<br/>    else: <br/>        for _,label_id in dataloader_obj:<br/>            for idx in label_id:<br/>                y_idx = idx.item()<br/>                y_lbl = idx2class[y_idx]<br/>                count_dict[str(y_lbl)] += 1<br/></span><span id="7fba" class="mr ll jf mn b gy ne mt l mu mv">    return count_dict</span></pre><p id="3872" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了绘制类分布，我们将使用前面用<code class="fe mw mx my mn b">ax</code>参数定义的<code class="fe mw mx my mn b">plot_from_dict()</code>函数。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="e467" class="mr ll jf mn b gy ms mt l mu mv">fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,7))</span><span id="16ac" class="mr ll jf mn b gy ne mt l mu mv">plot_from_dict(get_class_distribution_loaders(train_loader, rps_dataset), plot_title="Train Set", ax=axes[0])</span><span id="5a73" class="mr ll jf mn b gy ne mt l mu mv">plot_from_dict(get_class_distribution_loaders(val_loader, rps_dataset), plot_title="Val Set", ax=axes[1])</span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pa"><img src="../Images/1d5a57c026da5e868ddf2da1f6ed4d9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0o_jAwe7JlDJbT5iCAVdEA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">列车价值等级分布[图片[2]]</p></figure><p id="0a80" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">既然我们已经看了类分布，现在让我们看一个单一的图像。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="4c43" class="mr ll jf mn b gy ms mt l mu mv">single_batch = next(iter(train_loader))</span></pre><p id="f7d2" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe mw mx my mn b">single_batch</code>是两个元素的列表。第一个元素(第0个索引)包含图像张量，而第二个元素(第1个索引)包含输出标签。</p><p id="4691" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这是列表的第一个元素，它是一个张量。这个张量的形状是<code class="fe mw mx my mn b">(batch, channels, height, width)</code>。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="94cd" class="mr ll jf mn b gy ms mt l mu mv">single_batch[0].shape</span><span id="d291" class="mr ll jf mn b gy ne mt l mu mv"><br/>###################### OUTPUT ######################</span><span id="911f" class="mr ll jf mn b gy ne mt l mu mv">torch.Size([8, 3, 300, 300])</span></pre><p id="e094" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这是该批次的输出标签。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="e5a6" class="mr ll jf mn b gy ms mt l mu mv">print("Output label tensors: ", single_batch[1])<br/>print("\nOutput label tensor shape: ", single_batch[1].shape)</span><span id="53e9" class="mr ll jf mn b gy ne mt l mu mv"><br/>###################### OUTPUT ######################</span><span id="8447" class="mr ll jf mn b gy ne mt l mu mv">Output label tensors:  tensor([2, 0, 2, 2, 0, 1, 0, 0])</span><span id="73a0" class="mr ll jf mn b gy ne mt l mu mv">Output label tensor shape:  torch.Size([8])</span></pre><p id="1e51" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了绘制图像，我们将使用matloptlib中的<code class="fe mw mx my mn b">plt.imshow</code>。它期望图像尺寸为<code class="fe mw mx my mn b">(height, width, channels)</code>。我们将<code class="fe mw mx my mn b">.permute()</code>我们的单个图像张量来绘制它。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="5b7f" class="mr ll jf mn b gy ms mt l mu mv"># Selecting the first image tensor from the batch. <br/>single_image = single_batch[0][0]<br/>single_image.shape</span><span id="e31c" class="mr ll jf mn b gy ne mt l mu mv"><br/>###################### OUTPUT ######################</span><span id="26f3" class="mr ll jf mn b gy ne mt l mu mv">torch.Size([3, 300, 300])</span></pre><p id="a17c" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">让我们绘制图像。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="6a6e" class="mr ll jf mn b gy ms mt l mu mv">plt.imshow(single_image.permute(1, 2, 0))</span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/82299c32b8c8e61d71c38c930f61c44e.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*jeoEfOKsmiIKPYfd9EStcA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">单一图像样本[图像[3]]</p></figure><p id="1279" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">PyTorch使我们更容易直接从批次中绘制网格图像。</p><p id="5b6c" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们首先从列表中提取图像张量(由我们的数据加载器返回)并设置<code class="fe mw mx my mn b">nrow</code>。然后我们使用<code class="fe mw mx my mn b">plt.imshow()</code>函数来绘制网格。记住<code class="fe mw mx my mn b">.permute()</code>张量维度！</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="02db" class="mr ll jf mn b gy ms mt l mu mv"># We do single_batch[0] because each batch is a list <br/># where the 0th index is the image tensor and 1st index is the<br/># output label.</span><span id="6451" class="mr ll jf mn b gy ne mt l mu mv">single_batch_grid = utils.make_grid(single_batch[0], nrow=4)</span><span id="55d0" class="mr ll jf mn b gy ne mt l mu mv">plt.figure(figsize = (10,10))<br/>plt.imshow(single_batch_grid.permute(1, 2, 0))</span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/e17d796d611974b2736cad8e1a1d9146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lurOhxiZ1RiDSJr38ZYQcA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图像样本网格[图像[4]]</p></figure><h1 id="aa63" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">定义CNN架构</h1><p id="a82b" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">我们的建筑很简单。我们使用4块Conv层。每个区块由<code class="fe mw mx my mn b">Convolution</code> + <code class="fe mw mx my mn b">BatchNorm</code> + <code class="fe mw mx my mn b">ReLU</code> + <code class="fe mw mx my mn b">Dropout</code>层组成。</p><p id="5033" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们不会在最后使用一个<code class="fe mw mx my mn b">FC</code>层。我们将坚持使用<code class="fe mw mx my mn b">Conv</code>层。</p><blockquote class="nf ng nh"><p id="8b0a" class="kl km ni kn b ko kp kq kr ks kt ku kv nj kx ky kz nk lb lc ld nl lf lg lh li ij bi translated"><strong class="kn jp">将FC图层转换为CONV图层— </strong> <a class="ae lj" href="https://cs231n.github.io/convolutional-networks/#convert" rel="noopener ugc nofollow" target="_blank"> <strong class="kn jp">来源</strong> </a></p><p id="2bc3" class="kl km ni kn b ko kp kq kr ks kt ku kv nj kx ky kz nk lb lc ld nl lf lg lh li ij bi translated">值得注意的是，<code class="fe mw mx my mn b">FC</code>层和<code class="fe mw mx my mn b">CONV</code>层之间的唯一区别在于，<code class="fe mw mx my mn b">CONV</code>层中的神经元仅连接到输入中的局部区域，并且<code class="fe mw mx my mn b">CONV</code>体积中的许多神经元共享参数。然而，两层中的神经元仍然计算点积，因此它们的功能形式是相同的。因此，事实证明在<code class="fe mw mx my mn b">FC</code>和<code class="fe mw mx my mn b">CONV</code>图层之间转换是可能的。</p><p id="d743" class="kl km ni kn b ko kp kq kr ks kt ku kv nj kx ky kz nk lb lc ld nl lf lg lh li ij bi translated">对于任何<code class="fe mw mx my mn b">CONV</code>层，都有一个<code class="fe mw mx my mn b">FC</code>层实现相同的转发功能。权重矩阵将是一个大矩阵，除了在某些块(由于局部连通性)处，其中许多块中的权重是相等的(由于参数共享)，该大矩阵大部分为零。</p><p id="f383" class="kl km ni kn b ko kp kq kr ks kt ku kv nj kx ky kz nk lb lc ld nl lf lg lh li ij bi translated">相反，任何<code class="fe mw mx my mn b">FC</code>层都可以转换成<code class="fe mw mx my mn b">CONV</code>层。例如，具有<code class="fe mw mx my mn b">K=4096</code>的<code class="fe mw mx my mn b">FC</code>层正在查看大小为<code class="fe mw mx my mn b">7×7×512</code>的某个输入体积，可以等效地表示为具有<code class="fe mw mx my mn b">F=7,P=0,S=1,K=4096</code>的<code class="fe mw mx my mn b">CONV</code>层。</p><p id="9c4a" class="kl km ni kn b ko kp kq kr ks kt ku kv nj kx ky kz nk lb lc ld nl lf lg lh li ij bi translated">换句话说，我们将过滤器大小设置为输入体积的大小，因此输出将简单地为<code class="fe mw mx my mn b">1×1×4096</code>，因为只有一个深度列“适合”输入体积，给出与初始<code class="fe mw mx my mn b">FC</code>层相同的结果。</p></blockquote><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="414f" class="mr ll jf mn b gy ms mt l mu mv">class RpsClassifier(nn.Module):<br/>    def __init__(self):<br/>        super(RpsClassifier, self).__init__()<br/></span><span id="81af" class="mr ll jf mn b gy ne mt l mu mv">        self.block1 = self.conv_block(c_in=3, c_out=256, dropout=0.1, kernel_size=5, stride=1, padding=2)<br/>        self.block2 = self.conv_block(c_in=256, c_out=128, dropout=0.1, kernel_size=3, stride=1, padding=1)<br/>        self.block3 = self.conv_block(c_in=128, c_out=64, dropout=0.1, kernel_size=3, stride=1, padding=1)<br/>        self.lastcnn = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=75, stride=1, padding=0)</span><span id="84e3" class="mr ll jf mn b gy ne mt l mu mv">        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)<br/></span><span id="ed4b" class="mr ll jf mn b gy ne mt l mu mv">    def forward(self, x):<br/>        x = self.block1(x)<br/>        x = self.maxpool(x)</span><span id="80b2" class="mr ll jf mn b gy ne mt l mu mv">        x = self.block2(x)</span><span id="fc62" class="mr ll jf mn b gy ne mt l mu mv">        x = self.block3(x)<br/>        x = self.maxpool(x)</span><span id="7907" class="mr ll jf mn b gy ne mt l mu mv">        x = self.lastcnn(x)</span><span id="3ca5" class="mr ll jf mn b gy ne mt l mu mv">        return x<br/></span><span id="b8bc" class="mr ll jf mn b gy ne mt l mu mv">    def conv_block(self, c_in, c_out, dropout, **kwargs):<br/>        seq_block = nn.Sequential(<br/>            nn.Conv2d(in_channels=c_in, out_channels=c_out, **kwargs),<br/>            nn.BatchNorm2d(num_features=c_out),<br/>            nn.ReLU(),<br/>            nn.Dropout2d(p=dropout)<br/>        )</span><span id="c445" class="mr ll jf mn b gy ne mt l mu mv">        return seq_block</span></pre><p id="680a" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在我们将初始化模型、优化器和损失函数。</p><p id="4664" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">然后我们将模型传输到GPU。</p><p id="d13a" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">尽管这是一个二元分类问题，我们还是使用了<code class="fe mw mx my mn b">nn.CrossEntropyLoss</code>。这意味着，我们将处理<code class="fe mw mx my mn b">0 and 1</code>的返回2个值，而不是返回<code class="fe mw mx my mn b">1/0</code>的单个输出。更具体地说，产出的概率是<code class="fe mw mx my mn b">1</code>或<code class="fe mw mx my mn b">0</code>。</p><p id="19c5" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们不需要在最后一层之后手动添加一个<code class="fe mw mx my mn b">log_softmax</code>层，因为<code class="fe mw mx my mn b">nn.CrossEntropyLoss</code>已经为我们做了。</p><p id="6082" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">然而，我们需要应用<code class="fe mw mx my mn b">log_softmax</code>来进行验证和测试。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="9106" class="mr ll jf mn b gy ms mt l mu mv">model = RpsClassifier()<br/>model.to(device)<br/>print(model)</span><span id="d29d" class="mr ll jf mn b gy ne mt l mu mv">criterion = nn.CrossEntropyLoss()<br/>optimizer = optim.Adam(model.parameters(), lr=0.005)</span><span id="d05a" class="mr ll jf mn b gy ne mt l mu mv"><br/>###################### OUTPUT ######################</span><span id="e097" class="mr ll jf mn b gy ne mt l mu mv">RpsClassifier(<br/>  (block1): Sequential(<br/>    (0): Conv2d(3, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))<br/>    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (2): ReLU()<br/>    (3): Dropout2d(p=0.1, inplace=False)<br/>  )<br/>  (block2): Sequential(<br/>    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br/>    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (2): ReLU()<br/>    (3): Dropout2d(p=0.1, inplace=False)<br/>  )<br/>  (block3): Sequential(<br/>    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br/>    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (2): ReLU()<br/>    (3): Dropout2d(p=0.1, inplace=False)<br/>  )<br/>  (lastcnn): Conv2d(64, 3, kernel_size=(75, 75), stride=(1, 1))<br/>  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br/>)</span></pre><p id="1322" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在我们开始训练之前，让我们定义一个函数来计算每个历元的精度。</p><p id="7d06" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">该函数将<code class="fe mw mx my mn b">y_pred</code>和<code class="fe mw mx my mn b">y_test</code>作为输入参数。然后，我们将<code class="fe mw mx my mn b">softmax</code>应用于<code class="fe mw mx my mn b">y_pred</code>，并提取概率较高的类别。</p><p id="9a6f" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">之后，我们比较预测类别和实际类别来计算准确度。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="b815" class="mr ll jf mn b gy ms mt l mu mv">def multi_acc(y_pred, y_test):<br/>    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)<br/>    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    </span><span id="1cbb" class="mr ll jf mn b gy ne mt l mu mv">    correct_pred = (y_pred_tags == y_test).float()<br/>    acc = correct_pred.sum() / len(correct_pred)</span><span id="b23d" class="mr ll jf mn b gy ne mt l mu mv">    acc = torch.round(acc * 100)</span><span id="cb41" class="mr ll jf mn b gy ne mt l mu mv">    return acc</span></pre><p id="67f7" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们还将定义2个字典，用于存储训练集和验证集的准确度/时期和损失/时期。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="e6cc" class="mr ll jf mn b gy ms mt l mu mv">accuracy_stats = {<br/>    'train': [],<br/>    "val": []<br/>}</span><span id="9d78" class="mr ll jf mn b gy ne mt l mu mv">loss_stats = {<br/>    'train': [],<br/>    "val": []<br/>}</span></pre><p id="9087" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">让我们训练我们的模型！</p><p id="6a59" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">你可以看到我们在循环之前放了一个<code class="fe mw mx my mn b">model.train()</code>。<code class="fe mw mx my mn b">model.train()</code>告诉PyTorch你正处于训练模式。为什么我们需要这么做？如果您使用的是<code class="fe mw mx my mn b">Dropout</code>或<code class="fe mw mx my mn b">BatchNorm</code>等在训练和评估期间表现不同的层(例如；评估期间不使用<code class="fe mw mx my mn b">dropout</code>)，您需要告诉PyTorch采取相应的行动。而PyTorch中的默认模式是火车，因此，您不必显式地编写它。但这是很好的练习。</p><p id="1e11" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">类似地，当我们测试我们的模型时，我们将调用<code class="fe mw mx my mn b">model.eval()</code>。我们将在下面看到。回到训练；我们开始一个循环。在这个for循环的顶部，我们将每个历元的损失和精度初始化为0。在每个时期之后，我们将打印出损失/精度并将其重置回0。</p><p id="d3d5" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">然后我们有另一个for循环。这个for循环用于从<code class="fe mw mx my mn b">train_loader</code>中批量获取我们的数据。</p><p id="e761" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们在做任何预测之前都会做<code class="fe mw mx my mn b">optimizer.zero_grad()</code>。由于<code class="fe mw mx my mn b">.backward()</code>函数累加梯度，我们需要为每个小批量手动将其设置为0。从我们定义的模型中，我们获得一个预测，获得这个小批量的损失(和准确性)，使用loss.backward()和optimizer.step()执行反向传播。</p><p id="37ad" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">最后，我们将所有小批量损失(和精度)相加，以获得该时期的平均损失(和精度)。我们将每个小批量的所有损耗/精度相加，最后除以小批量的数量，即。获得每个历元的平均损失/精度的<code class="fe mw mx my mn b">train_loader</code>的长度。</p><p id="cef8" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们遵循的训练程序与验证程序完全相同，除了我们将其封装在<code class="fe mw mx my mn b">torch.no_grad</code>中，并且不执行任何反向传播。<code class="fe mw mx my mn b">torch.no_grad()</code>告诉PyTorch我们不想执行反向传播，这样可以减少内存使用并加快计算速度。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="620a" class="mr ll jf mn b gy ms mt l mu mv">print("Begin training.")</span><span id="273b" class="mr ll jf mn b gy ne mt l mu mv">for e in tqdm(range(1, 11)):</span><span id="82d4" class="mr ll jf mn b gy ne mt l mu mv">    # TRAINING</span><span id="b844" class="mr ll jf mn b gy ne mt l mu mv">    train_epoch_loss = 0<br/>    train_epoch_acc = 0</span><span id="5516" class="mr ll jf mn b gy ne mt l mu mv">    model.train()<br/>    for X_train_batch, y_train_batch in train_loader:<br/>        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)</span><span id="a010" class="mr ll jf mn b gy ne mt l mu mv">        optimizer.zero_grad()</span><span id="ccd5" class="mr ll jf mn b gy ne mt l mu mv">        y_train_pred = model(X_train_batch).squeeze()</span><span id="d680" class="mr ll jf mn b gy ne mt l mu mv">        train_loss = criterion(y_train_pred, y_train_batch)<br/>        train_acc = multi_acc(y_train_pred, y_train_batch)</span><span id="6de5" class="mr ll jf mn b gy ne mt l mu mv">        train_loss.backward()<br/>        optimizer.step()</span><span id="cf89" class="mr ll jf mn b gy ne mt l mu mv">        train_epoch_loss += train_loss.item()<br/>        train_epoch_acc += train_acc.item()<br/></span><span id="30b5" class="mr ll jf mn b gy ne mt l mu mv">    # VALIDATION<br/>    with torch.no_grad():<br/>        model.eval()<br/>        val_epoch_loss = 0<br/>        val_epoch_acc = 0<br/>        for X_val_batch, y_val_batch in val_loader:<br/>            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)</span><span id="3bca" class="mr ll jf mn b gy ne mt l mu mv">            y_val_pred = model(X_val_batch).squeeze()</span><span id="13a0" class="mr ll jf mn b gy ne mt l mu mv">            y_val_pred = torch.unsqueeze(y_val_pred, 0)</span><span id="daa3" class="mr ll jf mn b gy ne mt l mu mv">            val_loss = criterion(y_val_pred, y_val_batch)<br/>            val_acc = multi_acc(y_val_pred, y_val_batch)</span><span id="e5c7" class="mr ll jf mn b gy ne mt l mu mv">            val_epoch_loss += train_loss.item()<br/>            val_epoch_acc += train_acc.item()</span><span id="8202" class="mr ll jf mn b gy ne mt l mu mv">    loss_stats['train'].append(train_epoch_loss/len(train_loader))<br/>    loss_stats['val'].append(val_epoch_loss/len(val_loader))<br/>    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))<br/>    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))<br/></span><span id="9176" class="mr ll jf mn b gy ne mt l mu mv">    print(f'Epoch {e+0:02}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')</span><span id="d637" class="mr ll jf mn b gy ne mt l mu mv">###################### OUTPUT ######################</span><span id="6cf3" class="mr ll jf mn b gy ne mt l mu mv">Begin training.</span><span id="43fa" class="mr ll jf mn b gy ne mt l mu mv">Epoch 01: | Train Loss: 33.38733 | Val Loss: 10.19880 | Train Acc: 91.667| Val Acc: 100.000</span><span id="369c" class="mr ll jf mn b gy ne mt l mu mv">Epoch 02: | Train Loss: 6.49906 | Val Loss: 41.86950 | Train Acc: 99.603| Val Acc: 100.000</span><span id="5bff" class="mr ll jf mn b gy ne mt l mu mv">Epoch 03: | Train Loss: 3.15175 | Val Loss: 0.00000 | Train Acc: 100.000| Val Acc: 100.000</span><span id="b83f" class="mr ll jf mn b gy ne mt l mu mv">Epoch 04: | Train Loss: 0.40076 | Val Loss: 0.00000 | Train Acc: 100.000| Val Acc: 100.000</span><span id="3e6b" class="mr ll jf mn b gy ne mt l mu mv">Epoch 05: | Train Loss: 5.56540 | Val Loss: 0.00000 | Train Acc: 100.000| Val Acc: 100.000</span><span id="0cc5" class="mr ll jf mn b gy ne mt l mu mv">Epoch 06: | Train Loss: 1.56760 | Val Loss: 0.00000 | Train Acc: 100.000| Val Acc: 100.000</span><span id="6341" class="mr ll jf mn b gy ne mt l mu mv">Epoch 07: | Train Loss: 1.21176 | Val Loss: 0.00000 | Train Acc: 100.000| Val Acc: 100.000</span><span id="2244" class="mr ll jf mn b gy ne mt l mu mv">Epoch 08: | Train Loss: 0.84762 | Val Loss: 0.00000 | Train Acc: 100.000| Val Acc: 100.000</span><span id="b028" class="mr ll jf mn b gy ne mt l mu mv">Epoch 09: | Train Loss: 0.35811 | Val Loss: 0.00000 | Train Acc: 100.000| Val Acc: 100.000</span><span id="6146" class="mr ll jf mn b gy ne mt l mu mv">Epoch 10: | Train Loss: 0.01389 | Val Loss: 0.00000 | Train Acc: 100.000| Val Acc: 100.000</span></pre><h1 id="b190" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">可视化损失和准确性</h1><p id="a956" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">为了绘制损耗和精度线图，我们再次从<code class="fe mw mx my mn b">accuracy_stats</code>和<code class="fe mw mx my mn b">loss_stats</code>字典中创建一个数据帧。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="d77e" class="mr ll jf mn b gy ms mt l mu mv">train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})</span><span id="0795" class="mr ll jf mn b gy ne mt l mu mv">train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})</span><span id="d724" class="mr ll jf mn b gy ne mt l mu mv"><br/># Plot line charts<br/>fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))</span><span id="aebd" class="mr ll jf mn b gy ne mt l mu mv">sns.lineplot(data=train_val_acc_df, x = "epochs", y="value", hue="variable",  ax=axes[0]).set_title('Train-Val Accuracy/Epoch')</span><span id="d4a5" class="mr ll jf mn b gy ne mt l mu mv">sns.lineplot(data=train_val_loss_df, x = "epochs", y="value", hue="variable", ax=axes[1]).set_title('Train-Val Loss/Epoch')</span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pd"><img src="../Images/e38fffc786aa7577cefb8506d96e5786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HMQ-2I9_7iuiDWUZ89twFw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">train和val的精度损失曲线[图片[5]]</p></figure><h1 id="dbd1" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">试验</h1><p id="9e4a" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">训练完成后，我们需要测试我们的模型进展如何。注意，在运行测试代码之前，我们已经使用了<code class="fe mw mx my mn b">model.eval()</code>。为了告诉PyTorch我们不希望在推断过程中执行反向传播，我们使用了<code class="fe mw mx my mn b">torch.no_grad()</code>，就像我们对上面的验证循环所做的那样。</p><ul class=""><li id="69bb" class="nw nx jf kn b ko kp ks kt kw ny la nz le oa li ob oc od oe bi translated">我们首先定义一个包含我们预测的列表。然后我们使用<code class="fe mw mx my mn b">test_loader</code>遍历我们的批处理。对于每一批-</li><li id="43e4" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">我们将输入小批量数据转移到GPU。</li><li id="9130" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">我们使用训练好的模型进行预测。</li><li id="c952" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">将<code class="fe mw mx my mn b">log_softmax</code>激活应用于预测，并选择概率最高的指数。</li><li id="63f2" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">将批处理从CPU移动到GPU。</li><li id="5e78" class="nw nx jf kn b ko of ks og kw oh la oi le oj li ob oc od oe bi translated">将张量转换为numpy对象，并将其添加到我们的列表中。</li></ul><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="4b25" class="mr ll jf mn b gy ms mt l mu mv">y_pred_list = []<br/>y_true_list = []<br/>with torch.no_grad():<br/>    for x_batch, y_batch in tqdm(test_loader):<br/>        x_batch, y_batch = x_batch.to(device), y_batch.to(device)</span><span id="d737" class="mr ll jf mn b gy ne mt l mu mv">        y_test_pred = model(x_batch)<br/>        _, y_pred_tag = torch.max(y_test_pred, dim = 1)</span><span id="ad41" class="mr ll jf mn b gy ne mt l mu mv">        y_pred_list.append(y_pred_tag.cpu().numpy())<br/>        y_true_list.append(y_batch.cpu().numpy())</span></pre><p id="b305" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们将把这个列表展平，这样我们就可以把它作为<code class="fe mw mx my mn b">confusion_matrix</code>和<code class="fe mw mx my mn b">classification_report</code>的输入。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="2168" class="mr ll jf mn b gy ms mt l mu mv">y_pred_list = [i[0][0][0] for i in y_pred_list]<br/>y_true_list = [i[0] for i in y_true_list]</span></pre><h1 id="c807" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">分类报告</h1><p id="b117" class="pw-post-body-paragraph kl km jf kn b ko mz kq kr ks na ku kv kw nb ky kz la nc lc ld le nd lg lh li ij bi translated">最后，我们打印出包含精确度、召回率和F1分数的分类报告。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="82e7" class="mr ll jf mn b gy ms mt l mu mv">print(classification_report(y_true_list, y_pred_list))</span><span id="2205" class="mr ll jf mn b gy ne mt l mu mv"><br/>###################### OUTPUT ######################</span><span id="d681" class="mr ll jf mn b gy ne mt l mu mv">precision    recall  f1-score   support</span><span id="76da" class="mr ll jf mn b gy ne mt l mu mv">           0       0.71      0.85      0.77       124<br/>           1       0.70      0.65      0.67       124<br/>           2       0.82      0.73      0.77       124</span><span id="01c5" class="mr ll jf mn b gy ne mt l mu mv">    accuracy                           0.74       372<br/>   macro avg       0.74      0.74      0.74       372<br/>weighted avg       0.74      0.74      0.74       372</span></pre><h1 id="f642" class="lk ll jf bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">混淆矩阵</h1><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="47b4" class="mr ll jf mn b gy ms mt l mu mv">print(confusion_matrix(y_true_list, y_pred_list))</span><span id="ac09" class="mr ll jf mn b gy ne mt l mu mv"><br/>###################### OUTPUT ######################</span><span id="b964" class="mr ll jf mn b gy ne mt l mu mv">[[105  10   9]<br/> [ 33  80  11]<br/> [ 10  24  90]]</span></pre><p id="8f8a" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们从混淆矩阵中创建了一个数据框，并使用seaborn库将其绘制为热图。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="8996" class="mr ll jf mn b gy ms mt l mu mv">confusion_matrix_df = pd.DataFrame(confusion_matrix(y_true_list, y_pred_list)).rename(columns=idx2class, index=idx2class)</span><span id="8441" class="mr ll jf mn b gy ne mt l mu mv">fig, ax = plt.subplots(figsize=(7,5))         <br/>sns.heatmap(confusion_matrix_df, annot=True, ax=ax)</span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pe"><img src="../Images/c1cb82720a205db79f19bf80ad4ca306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aPVeLF9q2fLiJ14LyFwwow.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">混淆矩阵的热图[图片[6]]</p></figure></div><div class="ab cl pf pg hu ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="ij ik il im in"><p id="96c6" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">感谢您的阅读。欢迎提出建议和建设性的批评。:)</p><p id="f1c6" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这篇博文是专栏— <a class="ae lj" href="https://towardsdatascience.com/tagged/akshaj-wields-pytorch" rel="noopener" target="_blank"> <strong class="kn jp">如何训练你的神经网络</strong> </a>的一部分。</p><p id="2032" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">你可以在<a class="ae lj" href="https://www.linkedin.com/in/akshajverma7/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lj" href="https://twitter.com/theairbend3r" rel="noopener ugc nofollow" target="_blank"> Twitter </a>找到我。如果你喜欢这个，看看我的其他<a class="ae lj" href="https://medium.com/@theairbend3r" rel="noopener">博客</a>。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><a href="https://www.buymeacoffee.com/theairbend3r"><div class="gh gi pm"><img src="../Images/041a0c7464198414e6ce355f9235099e.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*SGCT6C60o4t58wRqeU2viQ.png"/></div></a></figure></div></div>    
</body>
</html>