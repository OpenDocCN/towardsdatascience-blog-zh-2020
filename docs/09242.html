<html>
<head>
<title>Improving the Performance of Machine Learning Model using Bagging</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Bagging 提高机器学习模型的性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improving-the-performance-of-machine-learning-model-using-bagging-534cf4a076a7?source=collection_archive---------24-----------------------#2020-07-02">https://towardsdatascience.com/improving-the-performance-of-machine-learning-model-using-bagging-534cf4a076a7?source=collection_archive---------24-----------------------#2020-07-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0bac" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解 Bootstrap Aggregation (Bagging)集成学习的工作原理，并使用 sklearn 库实现一个随机森林 Bagging 模型。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cf5630b41d3e379cdeb29140cd588aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wvgLhAuBBtfq1V7G"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@kmuza?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Carlos Muza </a>拍摄的照片</p></figure><p id="3802" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习模型的性能告诉我们，对于看不见的数据点，模型的表现如何。有各种各样的策略和技巧来提高 ML 模型的性能，其中一些是:</p><ul class=""><li id="4532" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">ML 模型的微调超参数</li><li id="ab6a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">使用集成学习。</li></ul><h2 id="20b8" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">什么是集成学习？</h2><p id="123b" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">集成学习是一种组合多个 ML 模型以形成单个模型的技术。也被称为<strong class="lb iu">基础模型或弱学习器</strong>的多个 ML 模型可以是不同的算法，也可以是超参数有变化的相同算法。</p><p id="187c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像分类任务一样，多个 ML 模型可以是逻辑回归、朴素贝叶斯、决策树、SVM 等。对于回归任务，多个 ML 模型可以是线性回归、Lasso 回归、决策树回归等。</p><p id="7ab9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">集成学习结合了基本模型的优点，以形成具有改进性能的单个鲁棒模型。各种类型的集成学习技术有:</p><ol class=""><li id="4c6e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nh mb mc md bi translated">引导聚集</li><li id="31a3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nh mb mc md bi translated">助推</li><li id="6b04" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nh mb mc md bi translated">投票</li><li id="e146" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nh mb mc md bi translated">级联</li><li id="d3d8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nh mb mc md bi translated">堆垛</li></ol><p id="4172" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有很多。本文将介绍 Bagging 集成技术的工作和实现。</p><h2 id="7313" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">Bagging(引导聚合)概述:</h2><p id="765a" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">Bagging ensemble 技术也称为 Bootstrap Aggregation，它使用随机化来提高性能。在 bagging 中，我们使用在部分数据集上训练的基础模型。在 bagging 中，我们使用<strong class="lb iu">弱学习者</strong>(或<strong class="lb iu">基础模型</strong>)模型作为积木，通过组合其中的几个来设计复杂的模型。</p><p id="e933" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数时候，这些基本模型表现不佳，因为它们要么过拟合，要么过拟合。模型的过拟合或欠拟合是由偏差-方差权衡决定的。</p><blockquote class="ni nj nk"><p id="0e7c" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated"><strong class="lb iu">什么是偏差-方差权衡？[1] </strong></p></blockquote><p id="aa4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型的总体误差取决于模型的偏差和方差，遵循以下等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c8191da146c24c0dfe0eb64177253c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*YvUTXyCYhPqPlNSeFxXE3g.png"/></div></figure><p id="6c1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于一个好的稳健模型，模型的误差尽可能小。为了最小化误差，偏差和方差需要最小，并且不可约误差保持恒定。下面的误差与模型灵活性(自由度)图描述了偏差和方差以及测试和训练误差的变化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/b5ac1237f6e9e6974459ac3ca363a610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3UOtAOJ90WdF2WlegV61FQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" rel="noopener" target="_blank" href="/the-bias-variance-tradeoff-8818f41e39e9">来源</a>，偏差-方差权衡误差图</p></figure><p id="384f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">从上图分析:</strong></p><ul class=""><li id="d5da" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">当模型处于训练的初始阶段时，训练和测试误差都非常高。</li><li id="808d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">当模型训练足够的时候，训练误差很低，测试误差很高。</li><li id="5572" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">训练和测试误差高的阶段是欠拟合阶段。</li><li id="6d09" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">训练误差低而测试误差高的阶段是过拟合阶段。</li><li id="ecab" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在训练和测试误差之间存在平衡的阶段是最合适的。</li><li id="8816" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">欠拟合模型具有低方差和高偏差。</li><li id="d37e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">过度拟合模型具有高方差和低偏差。</li></ul><p id="b80b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Bagging Ensemble 技术可用于具有低偏差和高方差的基础模型。Bagging ensemble 使用数据集的随机化(将在本文稍后讨论)来<strong class="lb iu">减少基础模型的方差，从而保持较低的偏差</strong>。</p><h2 id="7741" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">装袋工作[1]:</h2><p id="15f3" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">现在很清楚，装袋减少了基础模型的方差，保持了较低的偏差。通过结合 bootstrap 抽样和聚集策略来减少基本模型的方差。装袋的整个工作分为三个阶段:</p><ol class=""><li id="9748" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nh mb mc md bi translated">自助抽样</li><li id="8888" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nh mb mc md bi translated">基础建模</li><li id="9815" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nh mb mc md bi translated">聚合</li></ol><p id="06f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图描述了具有 n 行的样本数据集 D 的所有三个步骤:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/822d0f35b5e1a860a10d0290190de137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jnm58RnbcicWETrDJNLQzQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，打包的 3 个步骤——引导取样、建模、汇总</p></figure><h2 id="5af5" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">自举采样:</h2><p id="7e3e" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated"><strong class="lb iu">引导样本</strong>是一个较小的样本，是初始数据集的子集。引导样本是通过替换采样从初始数据集创建的。</p><p id="0bd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设一个数据集有<strong class="lb iu"> n </strong>行和<strong class="lb iu"> f </strong>个特征，我们做一个引导取样，它指的是取样替换成 k 个不同的较小数据集，每个数据集大小为 m，具有相同的<strong class="lb iu"> f </strong>个特征。D_i 形成的每个更小的数据集看到数据集的子集。在下图中，形状为(n，f)的初始数据集 D 被采样为形状为(m，f)的 k 个数据集，其中 m &lt; n</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/2cb1131b105b51574f345c68e0a7cc1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WPmVx-HPT-d3yCNsssqfRA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，数据集的自助抽样</p></figure><p id="eb30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图描述了如何引导样本。具有 10 行的数据集 D 被采样并替换成 k 个更小的数据集，每个数据集具有 5 行。上图中 n=10，m=5。</p><p id="b3f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">观察到通过自举形成的每个数据集仅看到原始数据集的一部分，并且所有数据集彼此独立。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/94e9a683f239bbb87d56aadb4b79f08f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9kI1oSM8aOndueJqS08jXg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，10 行样本数据集的 Bootstrap 采样。</p></figure><p id="a507" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是 bagging 集成技术的第一步，其中 k 个较小的数据集通过彼此独立的引导来创建。</p><h2 id="fc2f" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">建模:</h2><p id="d9d8" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">建模是装袋的第二步。在通过自举创建了 k 个较小的数据集之后，使用 ML 算法来训练 k 个数据集中的每一个。用于训练 k 数据集的算法在超参数改变或不改变的情况下可以是相同的，或者可以使用不同的算法。</p><p id="a5e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举个例子，</p><ul class=""><li id="9e41" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">决策树算法可以用作基础模型，并改变超参数，如“深度”。</li><li id="534b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">可以使用不同算法的组合，例如 SVM、朴素贝叶斯、逻辑回归。</li></ul><p id="78ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在每个引导数据集上训练的模型被称为基础模型或弱学习器。下图描述了独立模型的每个数据集的训练:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/83ac850ecbbb0cd017d5d43913e6b643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ltdA1EuvF90UR0ChBY54Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，引导数据集的建模</p></figure><h2 id="a4d5" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">聚合:</h2><p id="5368" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">通过组合 k 个不同的基础模型来创建最终强大的健壮模型。因为基本模型是在引导样本上训练的，所以每个模型可能有不同的预测。根据问题陈述的不同，聚合技术也不同。</p><ul class=""><li id="0f97" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">对于<strong class="lb iu">回归问题</strong>:聚合可以取每个基础模型预测的平均值。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d4cc0fb036199e1c729008c40147b8d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*fRz9e_xDXBQDFdbQ-P8WhQ.png"/></div></figure><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="5f9d" class="mj mk it nu b gy ny nz l oa ob">Notation,<br/><strong class="nu iu">prediction:</strong> Final Output of bagging ensemble<br/><strong class="nu iu">k:</strong> number of base models<br/><strong class="nu iu">pred_i:</strong> prediction of ith base model</span></pre><ul class=""><li id="1eff" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">对于一个<strong class="lb iu">分类问题</strong>:聚集可以使用多数投票，具有最多投票的类可以被声明为最终预测。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/de94e94f447565c4aab02c267ab72a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*Vo2EHZgqGoKy4oklIcRUaQ.png"/></div></figure><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="91eb" class="mj mk it nu b gy ny nz l oa ob">Notation,<br/><strong class="nu iu">prediction:</strong> Final Output of bagging ensemble<br/><strong class="nu iu">pred_i:</strong> prediction target class of ith base model<br/><strong class="nu iu">1,2,3...,c:</strong> c different target class<br/><strong class="nu iu">C:</strong> Target Class having maximum vote</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/62d0b6b9a89e75e769e3276738548f7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*trtNLsmNLQUzv58Wx3efPg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，k 基础模型的聚合</p></figure></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="cc42" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">随机森林:</h2><p id="13b3" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">随机森林是 bagging 集成学习的一个例子。在随机森林算法中，<strong class="lb iu">基学习器</strong>只是<strong class="lb iu">决策树。</strong>随机森林使用<strong class="lb iu">装袋</strong>和<strong class="lb iu">列抽样</strong>形成稳健模型。</p><p id="a70f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，RF 使用 bagging 集成学习的实现，除此之外，它在自举步骤期间使用替换进行列采样。引导第一步的变化如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/da882e15bc8c3f4b3e20762ae3515340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZVx0yPpdddZtXD3e1YFKkg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，自举采样和列采样</p></figure><p id="198c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">初始数据集 D 的形状是(<strong class="lb iu"> n </strong>行，<strong class="lb iu"> f </strong>特征)，在 bootstrap 采样+ column 采样的情况下，形成的 k 基数据集是行采样和列采样。基础数据集 D_i 的形状是(<strong class="lb iu"> m </strong>行，<strong class="lb iu"> d </strong>特征)其中<strong class="lb iu"> n &gt; m </strong>和<strong class="lb iu"> f &gt; d </strong>。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h2 id="2d4d" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">随机森林的实现:</h2><p id="c5bf" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">为来自 Kaggle 的<a class="ae ky" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank"> Pima Indians 糖尿病数据集实施决策树和随机森林分类器。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok ol l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)，决策树和随机森林分类器的实现</p></figure><p id="4971" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">决策树分类器的观察:</strong></p><p id="bf93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树分类器针对 X_train 进行训练(第 16–19 行)，DT 分类器模型的性能(第 21–23 行)在 X_test 数据上进行测试。对 DT 模型性能的观察是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/7a70000a2414c47db38cd48f1f155200.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*2StAm9mnHTMQwHRbqIBWig.png"/></div></figure><p id="28d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">随机森林分类器的观察:</strong></p><p id="f482" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">k=50 的随机森林分类器针对 X_train 进行训练(第 25–28 行)，RF 分类器模型的性能(第 30–32 行)在 X_test 数据上进行测试。对 RF 模型性能的观察是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e894a8c45b9b44e1de80109a7b486465.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*P-XEydK3VE4xBuVBqELA-Q.png"/></div></figure><p id="3528" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">性能改善观察:</strong></p><ul class=""><li id="798c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">模型的准确率从 71%(对于 DT 分类器)提高到 75%(对于 RF 分类器)。</li><li id="511a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">从两个混淆矩阵中可以观察到 FN 和 FP 值减少以及 TP 和 FN 值增加的变化。</li></ul></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><p id="7b49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献:</strong></p><p id="01cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1](2019 . 4 . 23)，系综方法:装袋、升压、堆叠:<a class="ae ky" rel="noopener" target="_blank" href="/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205">https://towardsdatascience . com/Ensemble-methods-bagging-boosting-and-stacking-c 9214 a 10 a 205</a></p><blockquote class="oo"><p id="e34a" class="op oq it bd or os ot ou ov ow ox lu dk translated">感谢您的阅读！</p></blockquote></div></div>    
</body>
</html>