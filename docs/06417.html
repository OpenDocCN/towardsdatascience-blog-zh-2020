<html>
<head>
<title>Interpretability in Deep Learning with W&amp;B — CAM and GradCAM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">W &amp; B——CAM 和 GradCAM 深度学习的可解释性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretability-in-deep-learning-with-w-b-cam-and-gradcam-45ba5296a58a?source=collection_archive---------26-----------------------#2020-05-22">https://towardsdatascience.com/interpretability-in-deep-learning-with-w-b-cam-and-gradcam-45ba5296a58a?source=collection_archive---------26-----------------------#2020-05-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2fc5" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/production-ml" rel="noopener" target="_blank">生产中的机器学习</a></h2><div class=""/><div class=""><h2 id="30d4" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">这份报告将回顾 Grad-CAM 如何反驳神经网络不可解释的常见批评。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c34b1f9c1911ad838660bfd535d72246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xHx8RnZBd5lv0X1Iag94oQ.png"/></div></div></figure><h2 id="f5bc" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">点击查看互动报道<a class="ae ly" href="https://app.wandb.ai/ayush-thakur/interpretability/reports/Interpretability-in-Deep-Learning-with-W%26B-CAM-and-GradCAM--Vmlldzo5MTIyNw" rel="noopener ugc nofollow" target="_blank">。所有的代码都可以在</a><a class="ae ly" href="https://github.com/ayulockin/interpretabilitycnn/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</h2><p id="28d4" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh lm mi mj mk lq ml mm mn lu mo mp mq mr im bi translated">训练一个分类模型很有趣，但是你有没有想过你的模型是如何进行预测的？你的模型是不是真的看了图像中的狗，才以 98%的准确率将其归类为狗？很有趣，不是吗。在今天的报告中，我们将探讨为什么深度学习模型需要可解释，以及一些有趣的方法来窥视深度学习模型的内幕。深度学习的可解释性是一个非常令人兴奋的研究领域，在这个方向上已经取得了很多进展。</p><p id="6945" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">那么，为什么要关心可解释性呢？毕竟，企业或项目的成功主要是由模型的准确性来判断的。但是为了在现实世界中部署我们的模型，我们还需要考虑其他因素。例如，有种族偏见？或者，如果它对人类的分类准确率为 97%,但对男性的分类准确率为 99%,而对女性的分类准确率仅为 95%,那该怎么办？</p><p id="4385" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">了解模型如何做出预测也可以帮助我们调试您的网络。[查看这篇关于使用 PyTorch 和 W &amp; B 使用渐变和可视化调试神经网络的博文，了解其他一些有帮助的技术]。</p><p id="b4a1" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">在这一点上，我们都熟悉这样一个概念，即深度学习模型根据用其他更简单的表示形式表达的学习表示进行预测。也就是说，深度学习允许我们从更简单的概念中构建复杂的概念。这里有一篇惊人的<a class="ae ly" href="https://distill.pub/2018/building-blocks/" rel="noopener ugc nofollow" target="_blank">distilt Pub</a>帖子，可以帮助你更好地理解这个概念。我们还知道，在一些监督学习任务(如图像分类)的情况下，这些表示是在我们用输入数据和标签训练模型时学习的。对这种方法的批评之一是神经网络中学习到的特征是不可解释的。</p><p id="df75" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">今天，我们将着眼于解决这一批评的两种技术，并揭示神经网络学习的“黑盒”性质。</p><ul class=""><li id="3092" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">类别激活图(CAM)</li><li id="3a0b" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">梯度凸轮</li></ul><h1 id="132e" class="nl le it bd lf nm nn no li np nq nr ll ki ns kj lp kl nt km lt ko nu kp lx nv bi translated">类别激活映射</h1><p id="3147" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh lm mi mj mk lq ml mm mn lu mo mp mq mr im bi translated">已经观察到，卷积神经网络的各层的卷积单元充当对象检测器，即使在为分类任务训练网络时没有提供关于对象位置的这种先验。尽管卷积具有这种显著的性质，但是当我们使用完全连接的层来执行分类任务时，它就失去了作用。为了避免使用完全连接的网络，一些架构，如网络中的网络(T2)和谷歌网络(T4)都是完全卷积的神经网络。</p><p id="32ba" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">全局平均池(GAP)是这种架构中非常常用的层。它主要用作调整剂，以防止训练时过度拟合。<a class="ae ly" href="https://arxiv.org/pdf/1512.04150.pdf" rel="noopener ugc nofollow" target="_blank">学习区分性定位的深度特征</a>的作者发现，通过调整这样的架构，他们可以扩展 GAP 的优势，并可以保留其定位能力，直到最后一层。让我们试着快速理解使用 GAP 生成 CAM 的过程。</p><p id="a300" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated"><strong class="mb jd">类别激活图简单地指示图像中的区别区域，CNN 使用该区域将该图像分类到特定类别中。</strong>对于这种技术，网络由 ConvNet 和 Softmax 层(用于多类分类)组成，在卷积特征图上执行全局平均池。该图层的输出用作全连接图层的要素，该图层可生成所需的分类输出。<strong class="mb jd">给定这个简单的连通性结构，我们可以通过将输出层的权重投射回卷积特征图上来识别图像区域的重要性。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/542bef7e2213b97485a2c1239d344dbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xdd9O4J8faFnsIGr.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated"><em class="ob">图 CAM 的理想网络架构(</em> <a class="ae ly" href="https://arxiv.org/pdf/1512.04150.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ob">来源</em> </a> <em class="ob"> ) </em></p></figure><p id="5741" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">让我们尝试实现这一点。😄</p><h2 id="0d84" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">步骤 1:修改你的模型</h2><p id="6d5c" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh lm mi mj mk lq ml mm mn lu mo mp mq mr im bi translated">假设您已经用 Conv 块和几个完全连接的层构建了深度分类器。我们将不得不修改这个架构，这样就没有任何完全连接的层。我们将在输出层(softmax/sigmoid)和最后一个卷积块之间使用<code class="fe oc od oe of b">GlobalAveragePooling2D</code>层。</p><p id="e5a1" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated"><code class="fe oc od oe of b">CAMmodel</code>对我们的猫狗分类器进行了必要的修改。<strong class="mb jd">这里我用预先训练好的</strong> <code class="fe oc od oe of b"><strong class="mb jd">VGG16</strong></code> <strong class="mb jd">模型来模拟我已经训练好的猫狗分类器。</strong></p><pre class="ks kt ku kv gt og of oh oi aw oj bi"><span id="4f43" class="ld le it of b gy ok ol l om on">def CAMmodel():<br/>     ## Simulating my pretrained dog and cat classifier. <br/>     vgg = VGG16(include_top=False, weights='imagenet')<br/>     vgg.trainable = False<br/>     ## Flatten the layer so that it's not nested in the sequential model.<br/>     vgg_flat = flatten_model(vgg)<br/>     ## Insert GAP<br/>     vgg_flat.append(keras.layers.GlobalAveragePooling2D())<br/>     vgg_flat.append(keras.layers.Dense(1, activation='sigmoid'))<br/>   <br/>     model = keras.models.Sequential(vgg_flat)<br/>     return model</span></pre><p id="71f1" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">一个简单的实用程序<code class="fe oc od oe of b">flatten_model</code>返回我的预训练模型中的图层列表。这样做是为了在使用<code class="fe oc od oe of b">Sequential</code>模型修改时各层不会嵌套，并且可以访问最后一个卷积层并将其用作输出。我在从<code class="fe oc od oe of b">flatten_model</code>返回的数组中追加了<code class="fe oc od oe of b">GlobalAveragePooling2D</code>和<code class="fe oc od oe of b">Dense</code>。最后，返回顺序模型。</p><pre class="ks kt ku kv gt og of oh oi aw oj bi"><span id="bbbf" class="ld le it of b gy ok ol l om on">def flatten_model(model_nested):<br/>    '''<br/>    Utility to flatten pretrained model<br/>    '''<br/>    layers_flat = []<br/>    for layer in model_nested.layers:<br/>       try:<br/>          layers_flat.extend(layer.layers)<br/>       except AttributeError:<br/>          layers_flat.append(layer)<br/>      <br/>    return layers_flat</span></pre><p id="8e3d" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">接下来，我们用适当的模型输入形状调用<code class="fe oc od oe of b">model.build()</code>。</p><pre class="ks kt ku kv gt og of oh oi aw oj bi"><span id="9c8b" class="ld le it of b gy ok ol l om on">keras.backend.clear_session()<br/>model = CAMmodel()<br/>model.build((None, None, None, 3)) # Note<br/>model.summary()</span></pre><h2 id="5100" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">第二步:用<code class="fe oc od oe of b">CAMLogger</code>回调重新训练你的模型</h2><p id="730f" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh lm mi mj mk lq ml mm mn lu mo mp mq mr im bi translated">由于引入了新的层，我们必须重新训练模型。但是我们不需要重新训练整个模型。我们可以通过使用<code class="fe oc od oe of b">vgg.trainable=False</code>来冻结卷积块。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/00aadc9f3f4b08d659a491722ff107fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*mVbxn4u8MF86GpxgaTJE6g.png"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图 2:再训练模型后的度量图。</p></figure><p id="cc9e" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated"><strong class="mb jd">观察:</strong></p><ul class=""><li id="9ee4" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">就训练和验证准确性而言，模型性能有所下降。我达到的最佳训练和验证精度分别是<code class="fe oc od oe of b">99.01%</code>和<code class="fe oc od oe of b">95.67%</code>。</li><li id="ab14" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated"><strong class="mb jd">因此，为了实施 CAM，我们必须修改我们的架构，从而降低模型性能。</strong></li></ul><h2 id="4f16" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">步骤 3:使用<code class="fe oc od oe of b">CAMLogger</code>查看类激活图</h2><p id="5037" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh lm mi mj mk lq ml mm mn lu mo mp mq mr im bi translated">在<code class="fe oc od oe of b">CAM</code>类的<code class="fe oc od oe of b">__init__</code>中，我们初始化<code class="fe oc od oe of b">cammodel</code>。注意这个<code class="fe oc od oe of b">cammodel</code>有两个输出:</p><ul class=""><li id="30f1" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">最后一个卷积层的输出(此处为<code class="fe oc od oe of b">block5_conv3</code></li><li id="db46" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">模型预测(softmax/sigmoid)。</li></ul><pre class="ks kt ku kv gt og of oh oi aw oj bi"><span id="47c5" class="ld le it of b gy ok ol l om on">class CAM:<br/>  def __init__(self, model, layerName):<br/>    self.model = model<br/>    self.layerName = layerName<br/>    ## Prepare cammodel<br/>    last_conv_layer = self.model.get_layer(self.layerName).output<br/>    self.cammodel = keras.models.Model(inputs=self.model.input,<br/>                                       outputs=[last_conv_layer, self.model.output])<br/>    <br/>  def compute_heatmap(self, image, classIdx):<br/>    ## Get the output of last conv layer and model prediction<br/>    [conv_outputs, predictions] = self.cammodel.predict(image)<br/>    conv_outputs = conv_outputs[0, :, :, :]<br/>    conv_outputs = np.rollaxis(conv_outputs, 2)<br/>    ## Get class weights between <br/>    class_weights = self.model.layers[-1].get_weights()[0]<br/>    ## Create the class activation map.<br/>    caml = np.zeros(shape = conv_outputs.shape[1:3], dtype=np.float32)<br/>    for i, w in enumerate(class_weights[:]):<br/>      caml += w * conv_outputs[i, :, :]</span><span id="3e15" class="ld le it of b gy op ol l om on">    caml /= np.max(caml)<br/>    caml = cv2.resize(caml, (image.shape[1], image.shape[2]))<br/>    ## Prepare heat map<br/>    heatmap = cv2.applyColorMap(np.uint8(255*caml), cv2.COLORMAP_JET)<br/>    heatmap[np.where(caml &lt; 0.2)] = 0</span><span id="417a" class="ld le it of b gy op ol l om on">    return heatmap<br/>  def overlay_heatmap(self, heatmap, image):<br/>    img = heatmap*0.5 + image<br/>    img = img*255<br/>    img = img.astype('uint8')<br/>    return (heatmap, img)</span></pre><p id="4ec9" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated"><code class="fe oc od oe of b">compute_heatmap</code>方法负责生成热图，这是 CNN 用来识别类别(图像类别)的区分区域。</p><ul class=""><li id="32fd" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">输入图像上的<code class="fe oc od oe of b">cammodel.predict()</code>将给出形状<code class="fe oc od oe of b">(1,7,7,512)</code>的最后一个卷积层的特征图。</li><li id="d564" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">我们还提取形状<code class="fe oc od oe of b">(512,1)</code>的输出层的权重。</li><li id="2abd" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">最后，计算从最终层提取的权重和特征图的点积，以产生类别激活图。</li></ul><p id="89f7" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">现在我们把所有东西都包装在回调中。<code class="fe oc od oe of b">CamLogger</code>回调集成了<code class="fe oc od oe of b">wandb.log()</code>方法，将生成的激活图记录到 W &amp; B 运行页面上。通过调用<code class="fe oc od oe of b">overlay_heatmap()</code>方法，从 CAM 返回的热图最终叠加在原始图像上。</p><h2 id="6208" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">步骤 4:从 CAM 中得出结论</h2><p id="6afa" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh lm mi mj mk lq ml mm mn lu mo mp mq mr im bi translated">从下图中我们可以得出很多结论。👇注意<code class="fe oc od oe of b">examples</code>图表包含验证图像及其预测分数。如果预测得分大于 0.5，则网络将图像分类为狗，否则分类为猫。而<code class="fe oc od oe of b">CAM</code>图表有其对应的类激活图。让我们回顾一些观察结果:</p><ul class=""><li id="89fc" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">该模型通过查看图像中的面部区域将图像分类为狗。对于一些图像，它能够看到整个身体，除了爪子。</li><li id="c37b" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">该模型通过观察耳朵、爪子和胡须将这些图像归类为猫。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/85668379b6cc26e69661ba35d1eb40fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*jrcRwqLRID6NunkBhZI0gg.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图 3:看着狗的脸和猫的胡须，爪子和耳朵。</p></figure><ul class=""><li id="8949" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">对于错误分类的图像，模型没有看到它应该看到的地方。因此，通过使用 CAM，我们能够解释这种错误分类背后的原因，这真的很酷。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/0f46bed73dbbd1e37abd9fcceefab5ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*vNPgqRWWQDHjT9goS_zNFw.png"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图 4: CNN 在看别的东西。</p></figure><p id="97e3" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">这是为什么呢？即使耳朵、爪子和胡须出现在图像中，为什么它会看着别的东西呢？我能想到的一个原因是，由于我们没有在我们的猫狗数据集上微调我们的预训练<code class="fe oc od oe of b">VGG16</code>，作为特征提取器的 CNN 并不完全熟悉我们数据集中出现的模式(分布)。</p><ul class=""><li id="2f18" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">当同一类的多个实例出现在图像中时，模型只查看其中的一个。但这没关系，因为我们不关心物体检测。请注意，由于这个原因，可信度很低。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/7bd3b9b6d58455013d4e0d9a3da42b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*dWDfYB3j5C-J4ciqXp2HKg.png"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图 5:只查看一个事件。</p></figure><p id="7674" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated"><strong class="mb jd">其他用例:</strong></p><p id="fac4" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">CAM 可用于弱监督的目标定位任务。链接论文的作者在 ILSVRC 2014 基准数据集上测试了 CAM 执行本地化任务的能力。该技术能够在该数据集上实现 37.1%的目标定位前 5 名误差，这接近于由完全监督的 CNN 方法实现的 34.2%的前 5 名误差。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/f9fdabfb36415d1a2a13dd2ec6953eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-3GOWfjh_qNPIbhe0DE1HQ.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图 6:更多的例子。点击<a class="ae ly" href="https://app.wandb.ai/ayush-thakur/interpretability/reports/Interpretability-in-Deep-Learning-with-W%26B-CAM-and-GradCAM--Vmlldzo5MTIyNw" rel="noopener ugc nofollow" target="_blank">此处</a>查看更多此类示例。</p></figure><h1 id="648c" class="nl le it bd lf nm nn no li np nq nr ll ki ns kj lp kl nt km lt ko nu kp lx nv bi translated">梯度加权类激活图</h1><p id="18d6" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh lm mi mj mk lq ml mm mn lu mo mp mq mr im bi translated"><strong class="mb jd">尽管 CAM 很棒，但它也有一些局限性:</strong></p><ul class=""><li id="4d0d" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">为了使用 CAM，需要修改模型。</li><li id="3ed1" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">修改后的模型需要重新训练，这在计算上是昂贵的。</li><li id="6ba7" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">因为完全连接的致密层被移除。模型性能肯定会受到影响。这意味着预测分数没有给出模型能力的实际情况。</li><li id="3b04" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">用例受到架构约束的限制，即在输出层之前执行卷积映射间隙的架构。</li></ul><p id="4ff4" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated"><strong class="mb jd">什么是好的视觉解释？:</strong></p><ul class=""><li id="c1ce" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">当然，该技术应该将图像中的类本地化。我们在 CAM 中看到了这一点，而且效果非常好。</li><li id="abc7" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">应该捕获更精细的细节，即激活图应该是高分辨率的。</li></ul><p id="8a84" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">因此<a class="ae ly" href="https://arxiv.org/pdf/1610.02391.pdf" rel="noopener ugc nofollow" target="_blank"> Grad-CAM:通过基于梯度的定位</a>从深度网络进行视觉解释，这是一篇非常惊人的论文，作者提出了对 CAM 和以前方法的修改。他们的方法使用流入最终卷积层的任何目标预测的梯度来产生粗略的定位图，该定位图突出显示图像中的重要区域，用于预测图像的类别。</p><p id="3c40" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">因此 Grad-CAM 是 CAM 的严格推广。除了克服 CAM 的局限性，它还适用于涉及 CNN 的不同深度学习任务。它适用于:</p><ul class=""><li id="a160" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">具有全连接层(例如 VGG)的 CNN，无需对网络进行任何修改。</li><li id="8f80" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">CNN 用于结构化输出，如图像字幕。</li><li id="450a" class="mx my it mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">CNN 用于具有多模态输入的任务，如视觉问答或强化学习，无需架构改变或重新训练。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/ae4eeb212a36916fba2f52b26200ac03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jeNioOxvury5V9Tn.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated"><em class="ob">图 7: Grad-CAM 概述(</em> <a class="ae ly" href="https://arxiv.org/pdf/1610.02391.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ob">来源</em> </a> <em class="ob"> ) </em></p></figure><p id="56a0" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">让我们实现它😄</p><h2 id="4b51" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">第一步:你的深度学习任务</h2><p id="bd9e" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh lm mi mj mk lq ml mm mn lu mo mp mq mr im bi translated">我们将关注图像分类任务。与 CAM 不同，我们不必为此任务修改模型并重新训练它。</p><p id="1be9" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">我使用了一个在<code class="fe oc od oe of b">ImageNet</code>上预先训练的<code class="fe oc od oe of b">VGG16</code>模型作为我的基础模型，我用它来模拟迁移学习。</p><p id="7a10" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">通过使用<code class="fe oc od oe of b">vgg.trainable = False</code>，基线模型的层变成不可训练的。请注意我是如何在模型中使用完全连接的层的。</p><pre class="ks kt ku kv gt og of oh oi aw oj bi"><span id="8a42" class="ld le it of b gy ok ol l om on">def catdogmodel():<br/>  inp = keras.layers.Input(shape=(224,224,3))<br/>  vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp,<br/>                                            input_shape=(224,224,3))<br/>  vgg.trainable = False<br/>  <br/>  x = vgg.get_layer('block5_pool').output<br/>  x = tf.keras.layers.GlobalAveragePooling2D()(x)<br/>  x = keras.layers.Dense(64, activation='relu')(x)<br/>  output = keras.layers.Dense(1, activation='sigmoid')(x)</span><span id="b0e0" class="ld le it of b gy op ol l om on">  model = tf.keras.models.Model(inputs = inp, outputs=output)<br/>  <br/>  return model</span></pre><p id="f1f5" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">你会在<a class="ae ly" href="https://github.com/ayulockin/interpretabilitycnn/" rel="noopener ugc nofollow" target="_blank">链接的笔记本</a>中找到班级<code class="fe oc od oe of b">GradCAM</code>。这是一个来自<a class="ae ly" href="https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/" rel="noopener ugc nofollow" target="_blank"> Grad-CAM 的修改实现:用 Keras、TensorFlow 和 Deep Learning </a>可视化类激活图，这是一篇令人惊叹的博文，作者是 PyImageSearch.com 的 Adrian Rosebrook。我强烈建议查看那篇博文中的<code class="fe oc od oe of b">GradCAM</code>类的逐步实现。</p><p id="c2dc" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">我对它做了两处修改:</p><ul class=""><li id="89bf" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">在进行迁移学习时，即如果你的目标(最后一层)卷积层不可训练，<code class="fe oc od oe of b">tape.gradient(loss, convOutputs)</code>会返回<code class="fe oc od oe of b">None</code>。这是因为<code class="fe oc od oe of b">tape.gradient()</code>默认不跟踪不可训练的变量/层。因此，要使用该层来计算你的渐变，你需要通过在目标层输出(张量)上调用<code class="fe oc od oe of b">tape.watch()</code>来允许<code class="fe oc od oe of b">GradientTape</code>到<code class="fe oc od oe of b">watch</code>。因此发生了变化，</li></ul><pre class="ks kt ku kv gt og of oh oi aw oj bi"><span id="52af" class="ld le it of b gy ok ol l om on">with tf.GradientTape() as tape:<br/>      tape.watch(self.gradModel.get_layer(self.layerName).output)<br/>      inputs = tf.cast(image, tf.float32)<br/>      (convOutputs, predictions) = self.gradModel(inputs)</span></pre><ul class=""><li id="d296" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">最初的实现没有考虑二进制分类。最初的作者也谈到了输出的软最大化。所以为了训练一个简单的猫狗分类器，我做了一个小小的修改。因此发生了变化，</li></ul><pre class="ks kt ku kv gt og of oh oi aw oj bi"><span id="5067" class="ld le it of b gy ok ol l om on">if len(predictions)==1:<br/>      # Binary Classification<br/>      loss = predictions[0]<br/>    else:<br/>      loss = predictions[:, classIdx]</span></pre><p id="7273" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated"><code class="fe oc od oe of b">GRADCAM</code>类可以在模型训练后使用，也可以作为回调使用。这是他博客文章的一小段摘录。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/73d124291774083964b3c63dbe5e774a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FoVXeEdOufHB5N46.png"/></div></div></figure><p id="4df3" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">第三点激励我从事这个项目。我围绕这个<code class="fe oc od oe of b">GRADCAM</code>实现构建了一个定制回调，并使用<code class="fe oc od oe of b">wandb.log()</code>来记录激活映射。因此，通过使用这个回调，您可以在训练时使用 GradCAM。</p><h2 id="60ec" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">第三步:使用<code class="fe oc od oe of b">GRADCamLogger</code>并训练</h2><p id="1bf5" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh lm mi mj mk lq ml mm mn lu mo mp mq mr im bi translated">鉴于我们正在处理一个简单的数据集，我只训练了几个时期，这个模型似乎工作得很好。</p><p id="e1c1" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated"><strong class="mb jd">下面是 GradCAM 自定义回调。</strong></p><pre class="ks kt ku kv gt og of oh oi aw oj bi"><span id="7c1a" class="ld le it of b gy ok ol l om on">class GRADCamLogger(tf.keras.callbacks.Callback):<br/>    def __init__(self, validation_data, layer_name):<br/>      super(GRADCamLogger, self).__init__()<br/>      self.validation_data = validation_data<br/>      self.layer_name = layer_name</span><span id="8fb0" class="ld le it of b gy op ol l om on">    def on_epoch_end(self, logs, epoch):<br/>      images = []<br/>      grad_cam = []</span><span id="518d" class="ld le it of b gy op ol l om on">      ## Initialize GRADCam Class<br/>      cam = GradCAM(model, self.layer_name)</span><span id="5975" class="ld le it of b gy op ol l om on">      for image in self.validation_data:<br/>        image = np.expand_dims(image, 0)<br/>        pred = model.predict(image)<br/>        classIDx = np.argmax(pred[0])<br/>  <br/>        ## Compute Heatmap<br/>        heatmap = cam.compute_heatmap(image, classIDx)<br/>        <br/>        image = image.reshape(image.shape[1:])<br/>        image = image*255<br/>        image = image.astype(np.uint8)</span><span id="b977" class="ld le it of b gy op ol l om on">        ## Overlay heatmap on original image<br/>        heatmap = cv2.resize(heatmap, (image.shape[0],image.shape[1]))<br/>        (heatmap, output) = cam.overlay_heatmap(heatmap, image, alpha=0.5)</span><span id="53e6" class="ld le it of b gy op ol l om on">        images.append(image)<br/>        grad_cam.append(output)</span><span id="02b1" class="ld le it of b gy op ol l om on">      wandb.log({"images": [wandb.Image(image)<br/>                            for image in images]})<br/>      wandb.log({"gradcam": [wandb.Image(cam)<br/>                            for cam in grad_cam]})</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/c7bdaead7a856f5c975b1b372e52a2ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*owrMTNburZCS9jklAIromw.png"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图 8:培训后的指标。</p></figure><h2 id="9e8c" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">步骤 4:从 GradCAM 中得出结论</h2><p id="b9e2" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh lm mi mj mk lq ml mm mn lu mo mp mq mr im bi translated">GradCAM 是 CAM 的严格概括，应优先于 CAM。为了理解这项技术的理论基础，我推荐阅读 Divyanshu Mishra 的<a class="ae ly" rel="noopener" target="_blank" href="/demystifying-convolutional-neural-networks-using-gradcam-554a85dd4e48">使用 GradCam </a>揭开卷积神经网络的神秘面纱，或者只是阅读链接的论文。我们可以得出一些有趣的结论，包括:</p><ul class=""><li id="11ae" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">模型看着狗的脸来正确分类，而我不确定猫。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/cce164b8d33c8f5852fde4093af13087.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/0*Nc1UlMSZtayYPgO8.png"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图 8:看狗狗的脸。</p></figure><ul class=""><li id="fa1e" class="mx my it mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">该模型能够在图像中定位该类的多个实例，即预测分数考虑了图像中的多个狗和猫。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/5853ae27f455b324ec5eef6925d49f38.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*iUJwQSU47cEUZ6OVSEkMTQ.png"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图 9:查看多次出现。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/c69348077623e2e40087c4ea0a5b1bb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5xaAFU4WwUeiDbg4nGCt1g.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">图 10:更多的例子。点击<a class="ae ly" href="https://app.wandb.ai/ayush-thakur/interpretability/reports/Interpretability-in-Deep-Learning-with-W%26B-CAM-and-GradCAM--Vmlldzo5MTIyNw" rel="noopener ugc nofollow" target="_blank">此处</a>查看更多此类示例。</p></figure><h1 id="076e" class="nl le it bd lf nm nn no li np nq nr ll ki ns kj lp kl nt km lt ko nu kp lx nv bi translated">结论</h1><p id="dafd" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh lm mi mj mk lq ml mm mn lu mo mp mq mr im bi translated"><strong class="mb jd">类激活图</strong>和<strong class="mb jd"> Grad-CAMs </strong>是将一些可解释性/可解释性引入深度学习模型的几种方法，并且被相当广泛地使用。这些技术最吸引人的是执行目标定位任务的能力，即使没有事先训练模型的位置。GradCAM 在用于图像字幕时，可以帮助我们理解图像中的哪个区域用于生成某个单词。当用于视觉 Q &amp; A 任务时，它可以帮助我们理解为什么模型会得出特定的答案。尽管 Grad-CAM 具有类别区分能力，并能定位相关图像区域，但它无法像像素空间梯度可视化方法(如引导反向传播和去卷积)那样突出精细细节。因此，作者将 Grad-CAM 与引导反向传播相结合。</p><p id="37f8" class="pw-post-body-paragraph lz ma it mb b mc ms kd me mf mt kg mh lm mu mj mk lq mv mm mn lu mw mp mq mr im bi translated">感谢阅读这篇报道直到最后。我希望你发现这里介绍的回调对你的深度学习魔法有帮助。请随时在 Twitter( <a class="ae ly" href="https://twitter.com/ayushthakur0" rel="noopener ugc nofollow" target="_blank"> @ayushthakur0 </a>)上与我联系，获取对本报告的任何反馈。谢谢你。</p></div></div>    
</body>
</html>