# 装袋、增压和梯度增压

> 原文：<https://towardsdatascience.com/bagging-boosting-and-gradient-boosting-1a8f135a5f4e?source=collection_archive---------31----------------------->

## 易于理解的解释

![](img/e11cc892b989c6234349e4b0a7b2eae4.png)

埃里克·穆尔在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

# 制袋材料

Bagging 是在 bootstrap 样本上训练的机器学习模型的聚合(Bootstrap AGGregatING)。什么是引导样本？这些是*几乎*独立同分布(iid)样本——因为样本之间的相关性很低，而且它们来自同一个分布。引导过程用于创建这些样本。这包括从数据集中抽取样本*并替换掉*，记住如果这些样本足够大，它们将代表抽取样本的数据集，假设抽取样本的数据集也足够大以代表总体。在我们的数据集中有大量的观察值也意味着 bootstrap 样本更有可能在样本之间具有低相关性。

![](img/4fa165e41c8413b0a2c10b8961fd1e3f.png)

图一。样本取自数据集，数据集也是取自总体的样本。

为什么我们不简单地从总体中抽取更多的样本，而不是从我们的数据集中抽取样本？我们可能没有资源或能力从人群中获取更多样本——通常，在研究和工业中，我们必须完全依赖我们已经获得的或以前收集的数据。在这种情况下，bootstrapping 已经被证明是一种准确且计算效率高的估计人口分布的方法。

例如，如果我们有关于一个人是否拖欠贷款的数据，带有伯努利分布响应变量(0 或 1 表示此人是否拖欠)，我们可以计算响应变量的平均值，以快速估计我们的数据集中类的表现程度。为了找到响应变量均值的分布(了解其可变性以及我们的数据集对总体的代表性)，我们必须从总体中重新取样几次，并计算每个样本的均值，以便更好地了解响应变量的真实分布。然而，这并不总是可能的，我们只限于一个样本(我们的数据集)。经验表明，通过 bootstrapping，仅使用我们通过从数据集本身中抽取替换样本*而获得的数据，有可能获得我们采样统计(响应变量的平均值)的真实分布的相当准确的估计值。*

![](img/f337cb225e967374a02f5f34bad3cd94.png)

图二。左图:样本统计量α估计值的直方图，该估计值是根据总体中的 1，000 个样本(本例中为数据集)计算得出的。中心:从单个数据集的 1000 个 bootstrap 样本中获得的α估计值的直方图。右图:显示在左图和中间图中的α估计值显示为箱线图。在每个面板中，粉色线表示α的真实值。(詹姆斯等人，未注明)

bagging 的想法自然来自 bootstrapping，因为独立、同质的机器学习模型是在几乎独立的 bootstrap 样本上训练的，并且它们的结果是通过加权平均值聚合的。在这些 bootstrap 样本上进行训练的原因是，样本具有低相关性，因此我们可以更好地代表总体。流行的 bagging 算法，random forest，在为每个引导样本拟合决策树时，也对一小部分特征进行子采样，从而进一步降低样本之间的相关性。Bagging 提供了真实总体的良好表示，因此最常用于具有高方差的模型(如基于树的模型)。

# 助推

在高层次上，所有提升算法都以相似的方式工作:

1.  数据集中的所有观察值最初被赋予相同的权重。
2.  在这个数据集上训练机器学习模型。
3.  来自步骤 2 的模型被添加到带有与其相关联的误差的集合中，该误差是误分类观测值的权重之和。
4.  错误分类的观察值被赋予更大的权重。
5.  步骤 2-4 会重复许多个时期。

这里的想法是使错误分类的观察在随后的学习迭代中显得更加重要。在这一点上应该注意的是，boosting 是一种非常强大的集合技术，但是由于其关注困难观测的性质，它容易过拟合。因此，它通常用于相对有偏差的基础模型，例如浅树。

最容易理解的升压算法是 AdaBoost，其工作原理如下:

*   初始化变量。

```
M = Number of models to fitN = Number of observations in datasetm = 1 # Iteration counter, m <= M.w_i = 1/N # For i = 1, 2, 3, ..., N.F_0 = 0 # Ensemble model.
```

*   使用最小化加权误差`e_m = sum(w_misclassified)`的观察权重`w_i`训练模型`f_m`。
*   最新合奏模型`F_m = F_m_minus_1 + (alpha_m * f_m)`哪里`alpha_m = log(1 — e_m) / e_m`。
*   以与`alpha_m`成比例的增量更新错误分类观测值的权重。
*   递增迭代计数器。

```
m += 1if m <= M:
    go to step 2
```

*   最终模型是各个模型的预测总和乘以各自的 alpha 系数。

`F = (alpha_1 * f_1) + (alpha_2 * f_2) + ... + (alpha_m * f_m)`

# 梯度推进

这种方法被称为梯度推进，因为它使用梯度下降算法来最小化向集合添加模型时的损失。后续模型被拟合到*伪残差*而不是调整权重。*随机梯度下降*(如 XGBoost 所用)通过对每个阶段的观察值和特征进行采样来增加随机性，这类似于随机森林算法，只是采样是在没有替换的情况下进行的。

梯度推进算法通过构建新的模型来改进每次迭代，该新的模型添加了估计器 *h* 以提供更好的模型。完美的 *h* 意味着:

![](img/dad3dee92eed2d56b8b2a381d68a7ac4.png)

其中 y 是目标。或者简单地说:

![](img/048028c8cd2384c67671b899cdefa785.png)

这是目标和前一次迭代的模型预测之间的残差。为了近似这个残差，我们使用前面提到的伪残差，这在下面的步骤中解释:

1.  最初，像往常一样在数据集上训练，并获得每个观察的预测。
2.  根据步骤 1 中的预测，将伪残差计算为损失函数偏导数的负值。
3.  使用伪残差作为下一个模型的新目标，并像以前一样获得每个观测值的预测。
4.  重复步骤 2-3，进行 *M* 次迭代。

![](img/0f4d8a66293530b61fced9d60b114795.png)

图三。按照步骤 2 的伪残差。该值用作在每个训练时期预测 F(x)和目标 y 之间的“距离”的度量。m 是每个基础模型，N 是之前数据集中的观察次数。

在进行上述操作时，我们有效地预测了每次迭代中残差的近似值，因此，我们得到了以下集合模型:

![](img/0d855fa6aba4330142a0ae91fefef48f.png)

带 *h* ，如中所述:

![](img/dad3dee92eed2d56b8b2a381d68a7ac4.png)![](img/048028c8cd2384c67671b899cdefa785.png)

取而代之的是我们的伪残值。

# 参考

詹姆斯·g·威滕·d·哈斯蒂·t·蒂布希拉尼·r(未注明)。*统计学习入门*。