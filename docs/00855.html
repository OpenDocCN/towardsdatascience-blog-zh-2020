<html>
<head>
<title>A Unique Approach to Short Text Clustering (Algorithmic Theory)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">短文本聚类的独特方法(算法理论)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-unique-approach-to-short-text-clustering-part-1-algorithmic-theory-4d4fad0882e1?source=collection_archive---------13-----------------------#2020-01-24">https://towardsdatascience.com/a-unique-approach-to-short-text-clustering-part-1-algorithmic-theory-4d4fad0882e1?source=collection_archive---------13-----------------------#2020-01-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="7b01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">短文本聚类中的高维数挑战和使用GSDMM对抗稀疏性。</em></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/96170164efc4a4c5d588724fb2d851fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*CWBSdKqHYTzYzlJcjBfw1g.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">高纤维(呃)？！当然了。</p></figure><h1 id="89e4" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">问题是</h1><p id="209c" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">最近我一直在读安·兰德的《阿特拉斯耸耸肩》(我知道……回到高中英语文学时代，那时我本来应该读它，但没有读)。安·兰德有很多东西，但简短肯定不是其中之一。在不少于1000页的篇幅里，兰德精心构思了这部杰作中的场景和人物。好像我必须知道窗外树叶下面的颜色，因为里尔登喜欢啜饮轻微掺水的威士忌。我花了太多时间阅读文章片段、新闻摘要和听优雅概括的播客，以至于我忘记了如何真正融入作者的大脑。在过去的1000页中，我对安·兰德的写作风格了如指掌，我甚至可以复制它。这让我想到我的大脑如何为这位作者构建了一个非常优秀的自然语言处理器，因为我向它提供了大量的数据。但是我看文章，文字，推文等等。尽管理解不同，但考虑到随着时间的推移对材料的吸收，我仍然能够得出一些结论。我对短文材料的理解与安·兰德长篇大论的独白有何不同？我的大脑处理许多小片段和一篇长文章的方式有什么不同吗？</p><p id="9c2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">鉴于许多机器学习模型是建立在我们大脑如何处理信息的假设基础上的，我认为在我们处理新问题时考虑这些问题是有用的。短文本聚类传统上是一个难题，原因如下:</p><ol class=""><li id="bd74" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated"><a class="ae mk" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>意义不大，因为与文章相比，句子中的字数较少。如果一个句子中所有的词频都是1(因为这个句子很短，没有重复的词)，那么我获得了什么价值？</li><li id="a28b" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">对高维数据使用向量空间会导致稀疏。这导致高计算和内存存储。</li><li id="a903" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">问题1和2的结果是，确定用于分离文本的聚类的数量越来越具有挑战性，因此我们失去了将数据解释成主题的能力。</li></ol><h1 id="a8e2" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">什么是GSDMM？</h1><p id="fc81" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">GSD mm(Gibbs Sampling Dirichlet Multinomial Mixture)是几年前由尹建华和王建勇在一篇论文中提出的短文本聚类模型。该模型声称解决了短文本聚类的稀疏性问题，同时还显示了像LDA这样的词主题。GSDMM本质上是一种改进的LDA(潜在狄利克雷分配),它假设一个文档(例如tweet或text)包含一个主题。这与LDA不同，LDA假设一个文档可以有多个主题。正如我在前面的TF-IDF陷阱中提到的，由于稀疏性，在一个文档中权衡多个主题对于短文本文档来说不是一个好方法。</p><p id="99b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用称为“电影组方法”的类比来描述GSDMM的基本原理。想象一下，一群学生(文档)，他们都有一个喜欢的电影(词)的列表。学生们被随机分配到K桌。在一位教授的指导下，学生们必须在脑海中为两个目标洗牌:1)找一张有更多学生的桌子2)选择一张你对电影感兴趣的桌子。冲洗并重复，直到达到集群数量不变的稳定水平。</p><p id="9d73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里<a class="ae mk" rel="noopener" target="_blank" href="/short-text-topic-modeling-70e50a57c883">可以找到对LDA和GSDMM之间差异的直观描述</a>，而在这里可以找到对LDA更深入的分析<a class="ae mk" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158">。在这篇文章中，我将重点讨论尹和王论文中描述的GSDMM模型的参数和推导。</a></p><h1 id="a12f" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">狄利克雷多项式混合物</h1><p id="ffbb" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">对于该模型的第一部分，理解什么是<strong class="jp ir">狄利克雷分布</strong>是很重要的。<a class="ae mk" href="https://en.wikipedia.org/wiki/Dirichlet_distribution" rel="noopener ugc nofollow" target="_blank">狄利克雷分布</a>本质上是多维度(文档)上的<a class="ae mk" href="https://en.wikipedia.org/wiki/Beta_distribution" rel="noopener ugc nofollow" target="_blank">贝塔分布</a>。贝塔分布是简单的概率分布，其表示文档加入聚类的先验状态可能性以及该文档与该聚类的相似性。两个参数(下面描述的α和β)控制β分布的形状。</p><h2 id="ffcc" class="mq kz iq bd la mr ms dn le mt mu dp li jy mv mw lm kc mx my lq kg mz na lu nb bi translated">数字万用表的组件</h2><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/0021e0a81fb1ec4a63d7299d5bda144a.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*pGYXG566T3npiPkBOoVa0A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">转自尹和王的论文</p></figure><p id="e420" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来看看构成该模型的关键参数:</p><p id="1eb6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所述，α是一个影响我们概率分布形状的参数。更重要的是，alpha是从文档被分组到一个簇中的概率中得出的。在电影例子中，这是学生选择桌子的概率。</p><p id="c0b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">贝塔:</strong>贝塔是我们分布的另一个形状参数。Beta来自一个文档中的单词与另一个文档中的单词的相似性。与电影组相关，beta是学生加入有相似电影选择的桌子的概率。例如，如果beta为0，学生将只连接有共同电影的表。这可能不是最佳策略。也许两个同学都喜欢惊悚电影，但是他们没有列出相同的电影。我们仍然希望学生们最终加入同一批惊悚片爱好者。</p><p id="9290" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> phi </strong>:使用k个聚类(混合)，phi是聚类在单词上的多项式分布，使得p(w|z = k) = phi，其中w =单词，z =聚类标签</p><p id="ea8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> theta: </strong>同样，theta是考虑了alpha的多项式分布，所以p(d|z=k) = theta其中d = document。</p><p id="381e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些参数的最终结果是文档(d)是由假设Dirichlet先验的聚类(k)生成的概率。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/b6867c6b54ebea3502c496138733ccca.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*S9HxBBuSh44CdrGKIn8nfQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">转自尹和王的论文</p></figure><p id="9990" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，重要的是要指出，论文假设<strong class="jp ir">对称</strong>狄利克雷先验。这意味着相同的阿尔法和贝塔在开始时被假定。Alpha表示相同的群集同等重要，而beta表示相同的单词同等重要。Yin和Wang指出，在该算法的未来迭代中，betas不应该具有对称的先验，而是更流行的词应该具有较低的重要性。如果一个单词出现在每个文档中，它就不是一个非常有价值的信号(再次参见TF-IDF的缺陷)。</p><h1 id="4209" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">吉布斯采样</h1><p id="bd7f" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">Gibbs抽样描述了基于条件分布遍历和重新分配聚类的方法。与<a class="ae mk" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯分类器</a>的工作方式相同，基于最高条件概率将文档分配到聚类中。</p><h1 id="01c7" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">其他短文本算法</h1><p id="0482" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">我关注GSDMM是因为我通过一个个人项目对它了解最多(见下一篇文章)。短文本聚类还有其他方法，但我注意到DMM是其中许多方法的基本原则(GPU-DMM和GPU-PDMM添加单词嵌入)，因此我相信理解这些参数和基本构建块将有助于开发更复杂的模型。</p><h1 id="83a1" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">参考</h1><p id="35ca" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">原论文由尹和王<a class="ae mk" rel="noopener" target="_blank" href="/short-text-topic-modeling-70e50a57c883">此处</a></p><p id="c6a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇<a class="ae mk" rel="noopener" target="_blank" href="/short-text-topic-modeling-70e50a57c883">文章</a>很好地总结了GSDMM与LDA和应用的关系。</p><p id="723c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我没有深入研究吉布斯采样。LDA和Gibbs的另一篇好文章是<a class="ae mk" href="https://medium.com/analytics-vidhya/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045" rel="noopener"/></p></div></div>    
</body>
</html>