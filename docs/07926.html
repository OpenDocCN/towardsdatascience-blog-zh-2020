<html>
<head>
<title>Accelerating Tensorflow Lite with XNNPACK</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 XNNPACK 加速 Tensorflow Lite</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/accelerating-tensorflow-lite-with-xnnpack-ece7dc8726d0?source=collection_archive---------20-----------------------#2020-06-12">https://towardsdatascience.com/accelerating-tensorflow-lite-with-xnnpack-ece7dc8726d0?source=collection_archive---------20-----------------------#2020-06-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4c5b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">全新 Tensorflow Lite XNNPACK delegate 可在 x86 和 ARM CPUs 上实现最佳性能</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/237630a6ee8090d7952d854fb8cd4b74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*urM2hWRDX8tRFGr0.jpg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">SpaceX 在<a class="ae lc" href="https://www.flickr.com/photos/spacex/49421604803https://www.flickr.com/photos/spacex/49421604803" rel="noopener ugc nofollow" target="_blank"> Flickr 上的照片</a></p></figure><p id="7a23" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">TL；DR:新的 Tensorflow Lite XNNPACK delegate 在 x86 和 ARM CPUs 上实现了最佳性能，在某些情况下比默认的 Tensorflow Lite 后端快 10 倍以上。</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><p id="4f31" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">Tensorflow Lite 是我最喜欢的软件包之一。它能够在一系列硬件上轻松快速地部署，现在提供了一系列代理来加速推理，例如 GPU、Core ML 和 Hexagon。然而，Tensorflow Lite 的一个缺点是，它的设计考虑了移动应用，因此没有针对英特尔和 AMD x86 处理器进行优化。更好的 x86 支持在 Tensorflow Lite 开发<a class="ae lc" href="https://www.tensorflow.org/lite/guide/roadmap" rel="noopener ugc nofollow" target="_blank">路线图</a>中，但目前 Tensorflow Lite 主要依赖于通过 Neon_2_SSE 桥将 ARM Neon 指令转换为 SSE。</p><p id="1f6e" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然而，有一个新的 Tensorflow Lite 委托用于基于 CPU 的浮点计算，XNNPACK，它具有 x86 AVX 和 AVX-512 优化功能。在这篇文章中，我将带您使用 XNNPACK 并展示一些基准测试。</p><h1 id="949f" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">安装和使用 XNNPACK</h1><p id="8c80" class="pw-post-body-paragraph ld le iq lf b lg mr jr li lj ms ju ll lm mt lo lp lq mu ls lt lu mv lw lx ly ij bi translated">XNNPACK 的使用说明可以在<a class="ae lc" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack" rel="noopener ugc nofollow" target="_blank">这里</a>找到。最值得注意的是，现在有一个默认情况下启用 XNNPACK 委托的构建标志。这很方便，因为直到现在还不可能在 Python 中加载 Tensorflow Lite 委托。从源代码构建张量流的命令如下所示:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="d986" class="nb ma iq mx b gy nc nd l ne nf">bazel build --define tflite_with_xnnpack=true \ <br/>  //tensorflow/tools/pip_package:build_pip_package</span></pre><p id="d8af" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">Tensorflow Lite <a class="ae lc" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#on-android" rel="noopener ugc nofollow" target="_blank">基准测试工具</a>现在也有一个标志来启用 XNNPACK 委托。例如，要在 x86 机器上进行分析，首先要构建分析器工具:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="4f72" class="nb ma iq mx b gy nc nd l ne nf">bazel build -c opt --verbose_failures \    <br/>  tensorflow/lite/tools/benchmark:benchmark_model</span></pre><p id="d9d5" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然后使用以下命令运行探查器:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="9c20" class="nb ma iq mx b gy nc nd l ne nf">bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model \<br/>  --graph=&lt;model path&gt; --warmup_runs=50 --num_runs=1000 \<br/>  --enable_op_profiling=true --use_xnnpack=true</span></pre><h1 id="6259" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">为 XNNPACK 优化</h1><p id="f237" class="pw-post-body-paragraph ld le iq lf b lg mr jr li lj ms ju ll lm mt lo lp lq mu ls lt lu mv lw lx ly ij bi translated">确保模型适合 XNNPACK 很重要，因为它只支持所有 Tensorflow Lite 运算符的子集。例如，标准的 Keras 实现通常使用显式填充层，并通过 mean 运算符实现顶层全局池层。当使用普通的 TFLite 后端时，这只增加了几个百分点的运行时间，但是 XNNPACK 不支持这些操作，导致了相当大的开销——在具有 8 个线程的 MobileNet V2 的情况下为 30%(见下文)！</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ng"><img src="../Images/e942b2de9072fb9977651ac2cb6f49d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d49SeJX0byxthP4FLltg9Q.png"/></div></div></figure><p id="4a9a" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">通过用卷积运算中内置的填充替换显式填充层，可以轻松修复填充:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="f507" class="nb ma iq mx b gy nc nd l ne nf"># Before<br/>x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(img_input)<br/>x = layers.Conv2D(64, 7, strides=2, use_bias=use_bias, name='conv1_conv', padding='valid')(x)</span><span id="d075" class="nb ma iq mx b gy nh nd l ne nf"># After<br/>x = layers.Conv2D(64, 7, strides=2, use_bias=use_bias, name='conv1_conv', padding='same')(img_input)</span></pre><p id="0600" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">全局平均池层可以由具有大内核的平均池层代替:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="d73b" class="nb ma iq mx b gy nc nd l ne nf"># Before<br/>x = layers.GlobalAveragePooling2D()(x)<br/>x = layers.Dense(classes, activation='softmax')(x)</span><span id="38fc" class="nb ma iq mx b gy nh nd l ne nf"># After<br/># Use XNNPACK compatible average pooling<br/>x = layers.AveragePooling2D(pool_size=(7, 7))(x)<br/>    <br/># Implement the top dense layer as a convolution, so we don't need to remove spatial dims<br/>x = layers.Conv2D(classes, kernel_size=1)(x)<br/>x = layers.Softmax()(x)</span></pre><p id="f10c" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">请注意，您必须重新训练模型。你可以在本文附带的回购协议中找到这些模型的修复版本<a class="ae lc" href="https://github.com/yaysummeriscoming/xnnpack_benchmarks" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="c7e0" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">基准</h1><h2 id="4c64" class="nb ma iq bd mb ni nj dn mf nk nl dp mj lm nm nn ml lq no np mn lu nq nr mp ns bi translated">手臂ˌ武器ˌ袖子ˌ装备</h2><p id="63c7" class="pw-post-body-paragraph ld le iq lf b lg mr jr li lj ms ju ll lm mt lo lp lq mu ls lt lu mv lw lx ly ij bi translated">好了，基准测试！首先，我决定在 Galaxy S8 上测试 MobileNet V3:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nt"><img src="../Images/8be34d6c6ebfca6fc1a7271ae24cbdc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABvU6g5jZR-PePxi4sQItA.png"/></div></div></figure><p id="250b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我用 1 个线程测试了 1000 次迭代，其中有 50 次预热迭代。</p><p id="25a6" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如您所见，XNNPACK 在标准 Tensorflow Lite CPU 后端的基础上提供了出色的性能。值得注意的是，XNNPACK 支持较新的 ARM v 8.2-A CPU(如 A55)中包含的 ARM Float 16 指令，可惜我手头没有。GPU 后端仍然更快，尤其是对于更大的模型。但是，它需要 OpenGL ES 3.1 或更高版本，仅在所有 Android 设备的约 2/3 上可用(见市场份额<a class="ae lc" href="https://developer.android.com/about/dashboards" rel="noopener ugc nofollow" target="_blank">此处</a>)。</p><h2 id="628b" class="nb ma iq bd mb ni nj dn mf nk nl dp mj lm nm nn ml lq no np mn lu nq nr mp ns bi translated">x86</h2><p id="57b0" class="pw-post-body-paragraph ld le iq lf b lg mr jr li lj ms ju ll lm mt lo lp lq mu ls lt lu mv lw lx ly ij bi translated">现在转到 x86。我决定使用 MobileNet V2 和 ResNet50 与英特尔的 OpenVino 软件包进行比较。为了测试，我使用了一个谷歌云 N2 瀑布湖实例，有 8 个虚拟 CPU。带 1 根线:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nu"><img src="../Images/444dc36fcac9a72adc3edaed48139f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rLBa_ZPnrmH0t_ZSjCX-fQ.png"/></div></div></figure><p id="a833" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">和 8 个线程:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nv"><img src="../Images/9f9000ce1b3e143603e4220f45f712f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*St-EGIFwQ38w6Pk1H8BMJQ.png"/></div></div></figure><p id="5625" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如您所见，使用 XNNPACK delegate 的 Tensorflow Lite 的性能令人钦佩，在某些情况下比默认的 Tensorflow Lite 后端快 10 倍以上。性能接近 V2 MobileNet 的 OpenVino，但不及 ResNet 50。不过，我不认为这是一个大问题，因为基于深度方向卷积的架构，如 MobileNet V2，更适合 CPU 部署。XNNPACK 还具有比标准后端更好的跨多个 CPU 内核的伸缩性。请注意，TFLite 基准测试工具对应于 OpenVINO 的延迟模式，因此如果针对吞吐量进行配置，看看 XNNPACK 能够提供什么将会很有意思。</p><h1 id="081c" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">摘要</h1><p id="b271" class="pw-post-body-paragraph ld le iq lf b lg mr jr li lj ms ju ll lm mt lo lp lq mu ls lt lu mv lw lx ly ij bi translated">Tensorflow Lite 现在可以通过新的 XNNPACK delegate 提供出色的 x86 性能，在某些情况下超过了英特尔的 OpenVino 包。XNNPACK 的主要缺点是它只为浮点计算而设计。8 位模型量化可以轻松实现 2 倍以上的性能提升，当部署在支持 AVX-512 VNNI 指令的全新英特尔 Cascade Lake CPUs 上时，性能提升甚至更高。对 x86 上 8 位量化的支持在 Tensorflow Lite 路线图中，甚至可能在<a class="ae lc" href="https://github.com/tensorflow/tensorflow/issues/34536" rel="noopener ugc nofollow" target="_blank">的下一个版本</a>中。</p><p id="bd23" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">同样，对于移动部署，XNNPACK 优于 Tensorflow Lite 默认后端。虽然 GPU delegate 仍然比 XNNPACK 快，但 XNNPACK 在不支持 GPU 计算的设备上很有用。</p><p id="5470" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我对 XNNPACK 感到特别兴奋，因为它允许用户留在 Tensorflow 生态系统中，从而简化了部署过程。现在可以使用 Tensorflow Lite 转换一次模型并部署到多个平台，从而减少所需的不同软件包的数量。同样值得注意的是，AMD 处理器越来越少见，OpenVino 是英特尔的产品。我试图在谷歌云 N2D EYPC 实例上测试，但不幸的是，我无法增加我的配额。</p><p id="cd16" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我希望这篇文章对你有所帮助。复制这些基准的代码位于<a class="ae lc" href="https://github.com/yaysummeriscoming/mobilenet_v3_tflite" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae lc" href="https://github.com/yaysummeriscoming/xnnpack_benchmarks" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div></div>    
</body>
</html>