<html>
<head>
<title>Accelerating Tensorflow Lite with XNNPACK</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用XNNPACK加速Tensorflow Lite</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/accelerating-tensorflow-lite-with-xnnpack-ece7dc8726d0?source=collection_archive---------20-----------------------#2020-06-12">https://towardsdatascience.com/accelerating-tensorflow-lite-with-xnnpack-ece7dc8726d0?source=collection_archive---------20-----------------------#2020-06-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4c5b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">全新Tensorflow Lite XNNPACK delegate可在x86和ARM CPUs上实现最佳性能</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/237630a6ee8090d7952d854fb8cd4b74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*urM2hWRDX8tRFGr0.jpg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">SpaceX在<a class="ae lc" href="https://www.flickr.com/photos/spacex/49421604803https://www.flickr.com/photos/spacex/49421604803" rel="noopener ugc nofollow" target="_blank"> Flickr上的照片</a></p></figure><p id="7a23" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">TL；DR:新的Tensorflow Lite XNNPACK delegate在x86和ARM CPUs上实现了最佳性能，在某些情况下比默认的Tensorflow Lite后端快10倍以上。</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><p id="4f31" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">Tensorflow Lite是我最喜欢的软件包之一。它能够在一系列硬件上轻松快速地部署，现在提供了一系列代理来加速推理，例如GPU、Core ML和Hexagon。然而，Tensorflow Lite的一个缺点是，它的设计考虑了移动应用，因此没有针对英特尔和AMD x86处理器进行优化。更好的x86支持在Tensorflow Lite开发<a class="ae lc" href="https://www.tensorflow.org/lite/guide/roadmap" rel="noopener ugc nofollow" target="_blank">路线图</a>中，但目前Tensorflow Lite主要依赖于通过Neon_2_SSE桥将ARM Neon指令转换为SSE。</p><p id="1f6e" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然而，有一个新的Tensorflow Lite委托用于基于CPU的浮点计算，XNNPACK，它具有x86 AVX和AVX-512优化功能。在这篇文章中，我将带您使用XNNPACK并展示一些基准测试。</p><h1 id="949f" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">安装和使用XNNPACK</h1><p id="8c80" class="pw-post-body-paragraph ld le iq lf b lg mr jr li lj ms ju ll lm mt lo lp lq mu ls lt lu mv lw lx ly ij bi translated">XNNPACK的使用说明可以在<a class="ae lc" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack" rel="noopener ugc nofollow" target="_blank">这里</a>找到。最值得注意的是，现在有一个默认情况下启用XNNPACK委托的构建标志。这很方便，因为直到现在还不可能在Python中加载Tensorflow Lite委托。从源代码构建张量流的命令如下所示:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="d986" class="nb ma iq mx b gy nc nd l ne nf">bazel build --define tflite_with_xnnpack=true \ <br/>  //tensorflow/tools/pip_package:build_pip_package</span></pre><p id="d8af" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">Tensorflow Lite <a class="ae lc" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#on-android" rel="noopener ugc nofollow" target="_blank">基准测试工具</a>现在也有一个标志来启用XNNPACK委托。例如，要在x86机器上进行分析，首先要构建分析器工具:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="4f72" class="nb ma iq mx b gy nc nd l ne nf">bazel build -c opt --verbose_failures \    <br/>  tensorflow/lite/tools/benchmark:benchmark_model</span></pre><p id="d9d5" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然后使用以下命令运行探查器:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="9c20" class="nb ma iq mx b gy nc nd l ne nf">bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model \<br/>  --graph=&lt;model path&gt; --warmup_runs=50 --num_runs=1000 \<br/>  --enable_op_profiling=true --use_xnnpack=true</span></pre><h1 id="6259" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">为XNNPACK优化</h1><p id="f237" class="pw-post-body-paragraph ld le iq lf b lg mr jr li lj ms ju ll lm mt lo lp lq mu ls lt lu mv lw lx ly ij bi translated">确保模型适合XNNPACK很重要，因为它只支持所有Tensorflow Lite运算符的子集。例如，标准的Keras实现通常使用显式填充层，并通过mean运算符实现顶层全局池层。当使用普通的TFLite后端时，这只增加了几个百分点的运行时间，但是XNNPACK不支持这些操作，导致了相当大的开销——在具有8个线程的MobileNet V2的情况下为30%(见下文)！</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ng"><img src="../Images/e942b2de9072fb9977651ac2cb6f49d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d49SeJX0byxthP4FLltg9Q.png"/></div></div></figure><p id="4a9a" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">通过用卷积运算中内置的填充替换显式填充层，可以轻松修复填充:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="f507" class="nb ma iq mx b gy nc nd l ne nf"># Before<br/>x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(img_input)<br/>x = layers.Conv2D(64, 7, strides=2, use_bias=use_bias, name='conv1_conv', padding='valid')(x)</span><span id="d075" class="nb ma iq mx b gy nh nd l ne nf"># After<br/>x = layers.Conv2D(64, 7, strides=2, use_bias=use_bias, name='conv1_conv', padding='same')(img_input)</span></pre><p id="0600" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">全局平均池层可以由具有大内核的平均池层代替:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="d73b" class="nb ma iq mx b gy nc nd l ne nf"># Before<br/>x = layers.GlobalAveragePooling2D()(x)<br/>x = layers.Dense(classes, activation='softmax')(x)</span><span id="38fc" class="nb ma iq mx b gy nh nd l ne nf"># After<br/># Use XNNPACK compatible average pooling<br/>x = layers.AveragePooling2D(pool_size=(7, 7))(x)<br/>    <br/># Implement the top dense layer as a convolution, so we don't need to remove spatial dims<br/>x = layers.Conv2D(classes, kernel_size=1)(x)<br/>x = layers.Softmax()(x)</span></pre><p id="f10c" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">请注意，您必须重新训练模型。你可以在本文附带的回购协议中找到这些模型的修复版本<a class="ae lc" href="https://github.com/yaysummeriscoming/xnnpack_benchmarks" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="c7e0" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">基准</h1><h2 id="4c64" class="nb ma iq bd mb ni nj dn mf nk nl dp mj lm nm nn ml lq no np mn lu nq nr mp ns bi translated">手臂ˌ武器ˌ袖子ˌ装备</h2><p id="63c7" class="pw-post-body-paragraph ld le iq lf b lg mr jr li lj ms ju ll lm mt lo lp lq mu ls lt lu mv lw lx ly ij bi translated">好了，基准测试！首先，我决定在Galaxy S8上测试MobileNet V3:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nt"><img src="../Images/8be34d6c6ebfca6fc1a7271ae24cbdc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABvU6g5jZR-PePxi4sQItA.png"/></div></div></figure><p id="250b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我用1个线程测试了1000次迭代，其中有50次预热迭代。</p><p id="25a6" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如您所见，XNNPACK在标准Tensorflow Lite CPU后端的基础上提供了出色的性能。值得注意的是，XNNPACK支持较新的ARM v 8.2-A CPU(如A55)中包含的ARM Float 16指令，可惜我手头没有。GPU后端仍然更快，尤其是对于更大的模型。但是，它需要OpenGL ES 3.1或更高版本，仅在所有Android设备的约2/3上可用(见市场份额<a class="ae lc" href="https://developer.android.com/about/dashboards" rel="noopener ugc nofollow" target="_blank">此处</a>)。</p><h2 id="628b" class="nb ma iq bd mb ni nj dn mf nk nl dp mj lm nm nn ml lq no np mn lu nq nr mp ns bi translated">x86</h2><p id="57b0" class="pw-post-body-paragraph ld le iq lf b lg mr jr li lj ms ju ll lm mt lo lp lq mu ls lt lu mv lw lx ly ij bi translated">现在转到x86。我决定使用MobileNet V2和ResNet50与英特尔的OpenVino软件包进行比较。为了测试，我使用了一个谷歌云N2瀑布湖实例，有8个虚拟CPU。带1根线:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nu"><img src="../Images/444dc36fcac9a72adc3edaed48139f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rLBa_ZPnrmH0t_ZSjCX-fQ.png"/></div></div></figure><p id="a833" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">和8个线程:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nv"><img src="../Images/9f9000ce1b3e143603e4220f45f712f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*St-EGIFwQ38w6Pk1H8BMJQ.png"/></div></div></figure><p id="5625" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如您所见，使用XNNPACK delegate的Tensorflow Lite的性能令人钦佩，在某些情况下比默认的Tensorflow Lite后端快10倍以上。性能接近V2 MobileNet的OpenVino，但不及ResNet 50。不过，我不认为这是一个大问题，因为基于深度方向卷积的架构，如MobileNet V2，更适合CPU部署。XNNPACK还具有比标准后端更好的跨多个CPU内核的伸缩性。请注意，TFLite基准测试工具对应于OpenVINO的延迟模式，因此如果针对吞吐量进行配置，看看XNNPACK能够提供什么将会很有意思。</p><h1 id="081c" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">摘要</h1><p id="b271" class="pw-post-body-paragraph ld le iq lf b lg mr jr li lj ms ju ll lm mt lo lp lq mu ls lt lu mv lw lx ly ij bi translated">Tensorflow Lite现在可以通过新的XNNPACK delegate提供出色的x86性能，在某些情况下超过了英特尔的OpenVino包。XNNPACK的主要缺点是它只为浮点计算而设计。8位模型量化可以轻松实现2倍以上的性能提升，当部署在支持AVX-512 VNNI指令的全新英特尔Cascade Lake CPUs上时，性能提升甚至更高。对x86上8位量化的支持在Tensorflow Lite路线图中，甚至可能在<a class="ae lc" href="https://github.com/tensorflow/tensorflow/issues/34536" rel="noopener ugc nofollow" target="_blank">的下一个版本</a>中。</p><p id="bd23" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">同样，对于移动部署，XNNPACK优于Tensorflow Lite默认后端。虽然GPU delegate仍然比XNNPACK快，但XNNPACK在不支持GPU计算的设备上很有用。</p><p id="5470" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我对XNNPACK感到特别兴奋，因为它允许用户留在Tensorflow生态系统中，从而简化了部署过程。现在可以使用Tensorflow Lite转换一次模型并部署到多个平台，从而减少所需的不同软件包的数量。同样值得注意的是，AMD处理器越来越少见，OpenVino是英特尔的产品。我试图在谷歌云N2D EYPC实例上测试，但不幸的是，我无法增加我的配额。</p><p id="cd16" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我希望这篇文章对你有所帮助。复制这些基准的代码位于<a class="ae lc" href="https://github.com/yaysummeriscoming/mobilenet_v3_tflite" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae lc" href="https://github.com/yaysummeriscoming/xnnpack_benchmarks" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div></div>    
</body>
</html>