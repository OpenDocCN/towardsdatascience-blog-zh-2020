<html>
<head>
<title>Mask Adaptivity Tracking Using Computer Vision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用计算机视觉的掩模适应性跟踪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mask-adaptivity-tracking-using-computer-vision-8d36de26f29?source=collection_archive---------45-----------------------#2020-05-31">https://towardsdatascience.com/mask-adaptivity-tracking-using-computer-vision-8d36de26f29?source=collection_archive---------45-----------------------#2020-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3ac0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用YOLOv3构建实时掩模适应性检测器</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/de542fa16ab0bcb566ed95b5edbf952e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fg_tireNBX_wkzjS0ABBww.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原始图片来源:Pixabay</p></figure><p id="8ced" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">口罩迅速成为新冠肺炎疫情的象征，超过50个国家强制规定。刚摆脱封锁的国家正强制要求在公共场合戴口罩，并接受一般服务。执行这一新规范也给全球各地的机构带来了新的挑战，即我们的安全摄像头无法自动跟踪社交距离和其他预防措施是否得到遵守。在本案例研究中，我们的目的是开发一种解决方案来跟踪人群的面具适应性，跟踪大多数人是否使用面具。</p><p id="52ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">问题陈述</strong></p><p id="ec6e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据佩戴口罩的人数，确定口罩适应性是差、中还是高。从技术上讲，我们可以细分如下:</p><ol class=""><li id="fbed" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">检测一帧中戴面具的人数(X)。</li><li id="b4a2" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">检测一帧中未戴口罩的人数(Y)。</li><li id="777a" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">计算度量(X/(X+Y))，基于某些阈值确定适应性的类别。</li></ol><p id="8961" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">我们的方法</strong></p><p id="d847" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将该问题转化为两类(掩模，无掩模)对象检测问题，并训练了一个微小的YOLO(基于卷积神经网络的最先进的对象检测框架)模型来服务于该目的。</p><p id="9575" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">有点YOLO(你只看一次)</strong></p><p id="5717" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">YOLO是由Joseph Redmon等人开发的实时对象识别算法。在此之前，对象检测主要由分类器分两个阶段执行:</p><ol class=""><li id="e432" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">将图像分成各种区域，并找到感兴趣的区域(具有物体的概率高的区域)</li><li id="f54a" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">使用CNN对这些区域进行分类</li></ol><p id="be16" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">整个过程很慢，因为我们需要在多个区域中运行预测。YOLO提出了另一种方法，不是从图像中选择ROI，而是将问题映射到回归问题。在YOLO，单个卷积神经网络被应用于整个图像，该网络在单次通过中预测每个区域的边界框和概率。这些预测的有界框然后被预测的概率加权。这种单次预测使它比其他算法快得多，如R-CNN，更快的R-CNN等。</p><p id="20d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">YOLO在COCO数据集上的地图准确率达到48.1，高于SSD500模型(46.1)。虽然它比两级检测器实现了更高的FPS，但我们仍然认为它不足以满足我们基础设施有限的用例(单个GPU)。我们选择了YOLO的一个更轻的版本，叫做小YOLO，它比完整版快了将近442%。虽然这是准确性和速度之间的权衡，我们仍然能够实现下降性能。</p><p id="ca72" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">如何使用YOLO构建屏蔽检测器？</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/b83606436eed90f802cd09ab427833bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/1*Qn6snuSGtCGp5Yvht1XxSQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">很酷的东西，原始视频来源:<a class="ae mj" href="https://www.youtube.com/channel/UChirEOpgFCupRAk5etXqPaA" rel="noopener ugc nofollow" target="_blank">彭博快拍</a></p></figure><p id="01a7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们讨论真正有趣的部分。我将带你一步步使用微小的YOLOv3自己构建一个掩膜检测器。</p><p id="02fb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤0: </strong>喝杯咖啡(完全可选)并下载<a class="ae mj" href="https://github.com/arpan65/Mask-Adaptivity-Detection-Using-Computer-Vision" rel="noopener ugc nofollow" target="_blank">这个包含所有必要文件的资源库</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mk"><img src="../Images/431a0011f30b4e730251ed6ebc0dc327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uxaTSXV0TchmuP9C9ZGAmg.png"/></div></div></figure><ol class=""><li id="cf46" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la iu">数据采集</strong></li></ol><p id="d921" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">收集高质量的数据是任何数据科学案例研究的关键部分，同样，谷歌是你的最佳选择。试着下载戴面具的人、不戴面具的人以及两者在不同背景、角度和尺度下的照片。更多高质量的数据使您的模型更加智能。我比较喜欢用一个叫<strong class="la iu"> <em class="ml">的chrome扩展来批量下载图片</em> </strong>。如果你想使用我的数据集，在这里找到它<a class="ae mj" href="https://drive.google.com/drive/folders/1T6N4qd4ep2YgxXdpCrkngOe_o1NszHH_?usp=sharing" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="aa41" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.<strong class="la iu">注释图像</strong></p><blockquote class="mm mn mo"><p id="68cd" class="ky kz ml la b lb lc ju ld le lf jx lg mp li lj lk mq lm ln lo mr lq lr ls lt im bi translated">如果您正在使用我的数据集，请跳过此步骤。如果您想使用自己的数据，我们建议使用<strong class="la iu"><em class="it">label mg</em></strong>工具创建注释文件。</p></blockquote><p id="4342" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于windows，你可以在下载的库的<strong class="la iu">注释工具</strong>文件夹中找到它。对于Linux，你可以从<a class="ae mj" href="https://tzutalin.github.io/labelImg/" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/e52ca7b424d4a3cef01a44531c6f7d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x-wkegVZYuB8jLui5-8RiQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标签工具</p></figure><p id="20e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它将创建一个txt文件，如下所示，包含边界框的标签和坐标。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/bb19c3c662e3103aeab08a7f236f50c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*p7iZ-XNJ9uovp_iFFHe9Vg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成的注释文件</p></figure><p id="6564" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.<strong class="la iu">准备培训文件</strong></p><p id="9713" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了让所有人都可以访问这个案例研究，并以最少的努力轻松再现它，我们将使用免费的GPU在colab中训练我们的模型。</p><p id="474f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">a.进入google drive，创建一个名为<strong class="la iu"> darknet </strong>的文件夹</p><p id="bdc5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">b.你会在<strong class="la iu">YOLO _自定义</strong>文件夹中找到培训所需的所有文件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/734c5f44b0c6641a8b1f5a5f755a01de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X7OQ-8zHEsBW5k4EBtPWpA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">YOLO _自定义文件夹</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/8b845b8bfe42640d6263f0ed8a064d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYYY3O_1fHbVchb00GMccA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文件描述</p></figure><p id="b834" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将您的图像和注释文件包含在<strong class="la iu">图像</strong>文件夹中，并将路径添加到<strong class="la iu"> train.txt </strong>文件中，如下所示(无需更改图像文件夹的路径)。同样，如果你想坚持我的数据集，跳过这一步。上传<strong class="la iu">YOLO _自定义</strong>文件夹到新建的<strong class="la iu">暗网</strong>文件夹下的驱动器</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/e45ebef1fcd5acc6873b948d1925a44b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*fr_3vj975b0yXbpjwW8V5w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">train.txt文件</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/c22101db352ea11d1753b3555409aea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*8hUzAkhrf7-Qd6SwxeMbzQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Google Drive文件夹结构</p></figure><p id="07b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.<strong class="la iu">残局</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/7364fc78041e26a40991abcc538e2da3.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*GjtcZTf9iSwDtCl4gFjUjA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原创图片鸣谢:www.syfy.com</p></figure><p id="83e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在去<a class="ae mj" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank">https://colab.research.google.com</a>，用你的谷歌id登录</p><p id="ca0c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从下载的资源库上传<strong class="la iu">YOLO _面具_适应性. ipynb </strong>笔记本(你会在笔记本文件夹下找到)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/107484625f3d1b850bffed1a8eb0ddc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FhS_nf44GHe7AN375nRUoA.png"/></div></div></figure><p id="68a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">确保从<strong class="la iu">运行时</strong>启用GPU</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/4a10793bcb47388d811465ec9214515a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QeRCRPHMDz21FxEZzOphkg.png"/></div></div></figure><p id="fd14" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">按照笔记本中提到的一步一步运行单元，准备darknet环境(相信我，它可以省去设置CUDA环境的许多麻烦)和培训。您还会发现检查图像、视频和网络摄像头预测所需的实用程序(不幸的是，colab无法检测本地硬件，无论如何，我已经添加了代码，以防您有自己的darknet配置箱)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/55eb2c957509b747904dd8204aac7c71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MszKYugwJDgIjinb6Lo51A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">笔记本中的步骤</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/23b971ddc3af85a882052fd69fe74771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRiXsvw-CYIOBN6FX0m-Sw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">留意平均损失，训练时会定期保存重量</p></figure><p id="929f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我需要训练多长时间？</p><p id="8ebd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练，直到你达到下降平均损失(0。XX)。您可以在<strong class="la iu">yolo v3-tiny _ obj _ train . CFG</strong>文件中配置最大批次数量。训练时会定期保存重量，您也可以保存这些重量，下次从那里继续训练。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/49f3f977c4913bdc56158b9b8b86c29f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5H-u7m5lqYSZXhhhQM-zXw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更换重量路径以恢复训练</p></figure><p id="1e21" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">冬天终于来了！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/955b3e99c8957019cf8197b3e7fdf546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zXMeuYn-3QXctRhXbLVpTg.png"/></div></div></figure><p id="2f91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">现实世界影响</strong></p><p id="035b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该模型具有244 FPS的帧速率，适用于实时人群监控应用，并且可以通过公共场所的摄像头轻松使用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/0f95d39f75bfecca35262fc34c92c7df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*APKSKOQJzIAVVXN5A_3F7Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">人群面具适应性监测，原始图像学分:Pixabay</p></figure><p id="8f50" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">尾注</strong></p><p id="62c5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个案例研究中，我们学习了如何使用colab中的微小YOLOv3轻松构建一个遮罩自适应检测器。为了简单起见，我们避免了详细的培训步骤。实际上，您可以直接下载存储库，在drive中创建文件夹结构，然后立即在colab中启动，不需要担心设置darknet、更改配置文件或文件路径。我们的目的是创建训练框架，以便您只需做最小的更改来训练掩模适应性模型或改进我们的案例研究。一个很好的练习将是使用您的家庭安全摄像机或网络摄像头部署模型。保持安全和愉快的学习！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/083b49d722cc95c6f867909ca0620bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5iOFNMDOxVhhj1Nbb-HA2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预测，原始图片来源:Pixabay</p></figure><p id="8891" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">附加链接</strong></p><p id="22cb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请参考以下文章，了解有关对象检测的更多信息</p><ol class=""><li id="0024" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">Neptune.ai博客—<a class="ae mj" href="https://neptune.ai/blog/object-detection-algorithms-and-libraries" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3tOjbSI</a></li><li id="1e08" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mj" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank">https://pjreddie.com/darknet/yolo/</a></li><li id="1468" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mj" href="https://drive.google.com/drive/folders/1T6N4qd4ep2YgxXdpCrkngOe_o1NszHH_?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://drive.google.com/drive/folders/1T6N4qd4ep2YgxXdpCrkngOe_o1NszHH</a></li><li id="5183" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mj" href="https://github.com/AlexeyAB/darknet" rel="noopener ugc nofollow" target="_blank">https://github.com/AlexeyAB/darknet </a></li></ol><p id="9b5b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">引用(T5)</strong></p><ol class=""><li id="0899" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><a class="ae mj" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1506.02640 </a></li></ol><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="c2ac" class="nl nm it nh b gy nn no l np nq">@misc{darknet13,<br/>  author =   {Joseph Redmon},<br/>  title =    {Darknet: Open Source Neural Networks in C},<br/>  howpublished = {\url{http://pjreddie.com/darknet/}},<br/>  year = {2013--2016}<br/>}</span></pre></div></div>    
</body>
</html>