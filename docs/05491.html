<html>
<head>
<title>Linear Discriminant Analysis, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性判别分析，已解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b?source=collection_archive---------1-----------------------#2020-05-09">https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b?source=collection_archive---------1-----------------------#2020-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1b91" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener"> <strong class="ak">里面的艾</strong> </a></h2><div class=""/><div class=""><h2 id="f30c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">直觉、插图和数学:它如何不仅仅是一个降维工具，以及为什么它对于现实世界的应用是健壮的。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f76ff7ba31111b60647b45251c91249c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GmQel2gpb48wj9YFacuD_w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">通过混合判别分析(MDA)学习的边界(蓝线)成功地分离了三个混合类。MDA是LDA的强大扩展之一。</p></figure><h2 id="71c4" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">关键要点</h2><ol class=""><li id="3fc2" class="mc md it me b mf mg mh mi lq mj lu mk ly ml mm mn mo mp mq bi translated">线性判别分析不仅是一种降维工具，也是一种稳健的分类方法。</li><li id="9d33" class="mc md it me b mf mr mh ms lq mt lu mu ly mv mm mn mo mp mq bi translated">无论有没有数据正态性假设，我们都可以得到相同的LDA特征，这解释了它的鲁棒性。</li></ol><p id="1642" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">线性判别分析被用作分类、降维和数据可视化的工具。它已经存在了一段时间了。尽管LDA很简单，但它经常产生健壮、体面且可解释的分类结果。在处理现实世界的分类问题时，LDA通常是在使用其他更复杂和更灵活的方法之前的基准方法。</p><p id="865a" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">使用LDA(及其变体)的两个突出例子包括:</p><ul class=""><li id="e656" class="mc md it me b mf my mh na lq nl lu nm ly nn mm no mo mp mq bi translated"><strong class="me jd">破产预测</strong> : Edward Altman的<a class="ae np" href="https://en.wikipedia.org/wiki/Altman_Z-score" rel="noopener ugc nofollow" target="_blank"> 1968年模型</a>使用训练好的LDA系数预测公司破产的概率。根据31年的数据评估，准确率在80%到90%之间。</li><li id="d055" class="mc md it me b mf mr mh ms lq mt lu mu ly mv mm no mo mp mq bi translated"><strong class="me jd">面部识别</strong>:虽然从主成分分析(PCA)中学习到的特征被称为特征脸，但从LDA中学习到的特征被称为<a class="ae np" href="http://www.scholarpedia.org/article/Fisherfaces" rel="noopener ugc nofollow" target="_blank">鱼脸</a>，以统计学家罗纳德·费雪爵士的名字命名。我们稍后解释这种联系。</li></ul><p id="c540" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">本文首先介绍了经典的LDA，以及为什么它作为一种分类方法根深蒂固。接下来，我们看到这种方法中固有的降维，以及它如何导致降秩LDA。在那之后，我们看到费希尔是如何熟练地得出同样的算法，而没有对数据做任何假设。一个手写数字分类问题被用来说明LDA的性能。最后总结了该方法的优缺点。</p><p id="aa4b" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">本文改编自我的一篇博文。如果你喜欢LaTex格式的数学和HTML风格的页面，你可以在我的博客上阅读这篇文章。此外，还有一组相应的<a class="ae np" href="https://yangxiaozhou.github.io/assets/2019-10-02/Discriminant_Analysis.pdf" rel="noopener ugc nofollow" target="_blank">幻灯片</a>，我在其中展示了大致相同的材料，但解释较少。</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h2 id="cdf4" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">判别分析分类</h2><p id="5ee3" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">让我们看看LDA作为一种监督分类方法是如何导出的。考虑一个一般的分类问题:一个随机变量<strong class="me jd"> X </strong>来自<em class="oa"> K </em>类中的一个，带有一些特定于类的概率密度<em class="oa"> f </em> ( <strong class="me jd"> x </strong>)。判别规则试图将数据空间划分为代表所有类别的<em class="oa"> K </em>个不相交区域(想象棋盘上的盒子)。对于这些区域，通过判别分析进行分类仅仅意味着如果<strong class="me jd"> x </strong>在区域<em class="oa"> j </em>中，我们将<strong class="me jd"> x </strong>分配到类别<em class="oa"> j </em>中。问题是，我们如何知道数据<strong class="me jd"> x </strong>属于哪个区域？自然，我们可以遵循两个分配规则:</p><ul class=""><li id="4a9b" class="mc md it me b mf my mh na lq nl lu nm ly nn mm no mo mp mq bi translated"><strong class="me jd">最大似然法则</strong>:如果我们假设每一类都以相等的概率发生，那么分配<strong class="me jd"> x </strong>给类<em class="oa"> j </em>如果</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/7d6389dec4c757c45525ce7f2f91e6fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*UwOLt4NYUoQnlDb_ZRrz3A.png"/></div></figure><ul class=""><li id="e39e" class="mc md it me b mf my mh na lq nl lu nm ly nn mm no mo mp mq bi translated"><strong class="me jd">贝叶斯法则</strong>:如果我们知道类先验概率<strong class="me jd"> π </strong>，那么分配<strong class="me jd"> x </strong>给类<em class="oa"> j </em>如果</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/238a24a78bac2c1d693f173840b1c9ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*JDPyKz96SvkRZh5jT0-IBA.png"/></div></figure><h2 id="21ae" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">线性和二次判别分析</h2><p id="afae" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">如果我们假设数据来自多元高斯分布，即<strong class="me jd"> X </strong>的分布可以用其均值(<strong class="me jd"><em class="oa">【μ</em></strong>)和协方差(<strong class="me jd">【σ</strong>)来表征，就可以得到上述分配规则的显式形式。遵循贝叶斯规则，如果对于<em class="oa"> i </em> = 1，…，<em class="oa"> K </em>，数据<strong class="me jd"> x </strong>在所有<em class="oa"> K </em>类中具有最高的可能性，我们将数据<em class="oa"> j </em>分类:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/0888fc00e310555a1b2f2d7cc06db54a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KDzs55rpvEj1YTeavjnp_A.png"/></div></div></figure><p id="40ec" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">上述函数称为判别函数。注意这里对数似然的使用。换句话说，判别函数告诉我们数据<strong class="me jd"> x </strong>来自每个类的可能性有多大。因此，分离任意两个类的判定边界<em class="oa"> k </em>和<em class="oa"> l </em>是两个判别函数具有相同值的<strong class="me jd"> x </strong>的集合。因此，任何落在决策边界上的数据都同样可能来自这两个类(我们无法决定)。</p><p id="20c9" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">LDA出现在我们假设在<em class="oa"> K </em>类中协方差相等的情况下。也就是说，不是每个类一个协方差矩阵，而是所有类都有相同的协方差矩阵。那么我们可以得到下面的判别函数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/6e4a2982aa3c67a8bc2efc9de61a62e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hWeHMYCxbCJnKG8UUmGdaQ.png"/></div></div></figure><p id="f1cc" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">注意这是<strong class="me jd"> x </strong>中的线性函数。由此可见，任意一对类之间的决策边界也是<strong class="me jd"> x </strong>中的线性函数，其得名原因:线性判别分析。在没有相等协方差假设的情况下，似然中的二次项不会抵消，因此得到的判别函数是<strong class="me jd"> x </strong>中的二次函数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/80f3230c66d45921b99473b5a7ba6748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sPEEihq5JD1lsjqa69eBaw.png"/></div></div></figure><p id="6fba" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">在这种情况下，判定边界在<strong class="me jd"> x </strong>中是二次的。这被称为二次判别分析(QDA)。</p><h2 id="c231" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">哪个更好？艾达还是QDA？</h2><p id="4caf" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">在实际问题中，总体参数通常是未知的，只能通过训练数据作为样本均值和样本协方差矩阵来估计。虽然与LDA相比，QDA适应更灵活的决策边界，但是需要估计的参数数量也比LDA增加得更快。对于LDA，需要<em class="oa"> (p+1) </em>个参数来构造(2)中的判别函数。对于具有<em class="oa"> K </em>个类的问题，我们将只需要<em class="oa"> (K-1) </em>这样的判别函数，通过任意选择一个类作为基类(从所有其他类中减去基类可能性)。因此，LDA的估计参数的总数是<strong class="me jd"> <em class="oa"> (K-1)(p+1) </em> </strong>。</p><p id="7e76" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">另一方面，对于(3)中的每个QDA判别函数，需要估计均值向量、协方差矩阵、类先验:<br/> -均值:<em class="oa"> p </em> <br/> -协方差:<em class="oa"> p(p+1)/2 </em> <br/> -类先验:<em class="oa"> 1 </em> <br/>同样，对于QDA，需要估计<strong class="me jd"><em class="oa">(K-1){ p(p+3)/2+1 }</em></strong>参数。</p><p id="154d" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">因此，LDA中估计的参数数量随<em class="oa"> p </em>线性增加，而QDA的参数数量随<em class="oa"> p </em>二次增加。当问题维数较大时，我们预计QDA的性能会比LDA差。</p><h2 id="8271" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">两全其美？LDA和QDA之间的妥协</h2><p id="31be" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">我们可以通过正则化各个类别的协方差矩阵来找到LDA和QDA之间的折衷。正则化意味着我们对估计的参数施加一定的限制。在这种情况下，我们要求各个协方差矩阵通过惩罚参数(例如α:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/2887a61e422a6ca83e8f49ffc72c3314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gKobn0I1fpInf0Cj5TJ_Q.png"/></div></div></figure><p id="a319" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">公共协方差矩阵也可以通过罚参数例如β朝着单位矩阵正则化:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/67f98b60a9a7b5ac4f55fc9448306a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EwQMThgi1m50zqyLz8mSYQ.png"/></div></div></figure><p id="5354" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">在输入变量的数量大大超过样本数量的情况下，协方差矩阵的估计可能很差。收缩有望提高估计和分类精度。下图说明了这一点。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/93f7ed444e56c348ae48cc4e0e35062d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ODXzsakBeC_uKfcALBbMyw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">有收缩和无收缩LDA的性能比较。归功于<a class="ae np" href="https://scikit-learn.org/stable/auto_examples/classification/plot_lda.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>。</p></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oi oj l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">生成上图的脚本。</p></figure><h2 id="d54c" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">LDA的计算</h2><p id="c7dd" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">从(2)和(3)可以看出，如果我们先对角化协方差矩阵，鉴别函数的计算可以简化。也就是说，数据被转换为具有相同的协方差矩阵(无相关性，方差为1)。对于LDA，我们是这样进行计算的:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/79911591adffead2e40d7224ff1b2c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HlPEC0rOIMnauAXv_i_ofw.png"/></div></div></figure><p id="035d" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">步骤2将数据球形化，以在变换的空间中产生单位协方差矩阵。步骤4是通过下面的(2)获得的。</p><p id="2703" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">我们举两个类的例子，看看LDA到底在做什么。假设有两个类，<em class="oa"> k </em>和<em class="oa"> l </em>。我们将<strong class="me jd"> x </strong>归类到<em class="oa"> k </em>类，如果</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/358c23007a2983956b0affdd89e2dfc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qoAEtVndyDQg1g3G7ee7Rw.png"/></div></div></figure><p id="2631" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">上述条件意味着类<em class="oa"> k </em>比类<em class="oa"> l </em>更有可能产生数据<strong class="me jd"> x </strong>。按照上面概述的四个步骤，我们编写</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/69652f10629938400b5c9830674d9980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-NAZj1_myEq-6_ds4q8VpQ.png"/></div></div></figure><p id="a572" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">也就是说，我们将数据<strong class="me jd"> x </strong>分类到类别<em class="oa"> k </em>如果</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/8ef8c21cd09940263ef2b581bbd29aeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hR8GZwzb0kGoHJZCqPnlVQ.png"/></div></div></figure><p id="3689" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">导出的分配规则揭示了LDA的工作原理。等式的左侧(l.h.s .)是<strong class="me jd"> x* </strong>在连接两类平均值的线段上的正交投影长度。右手边是由类别先验概率校正的片段的中心位置。<strong class="me jd">本质上，LDA将球形数据分类到最接近的类均值。这里我们可以做两个观察:</strong></p><ol class=""><li id="0f4a" class="mc md it me b mf my mh na lq nl lu nm ly nn mm mn mo mp mq bi translated">当类别先验概率不相同时，判定点偏离中间点，即，边界被推向具有较小先验概率的类别。</li><li id="9a42" class="mc md it me b mf mr mh ms lq mt lu mu ly mv mm mn mo mp mq bi translated">数据被投影到由类平均值(规则的l . h . s .<strong class="me jd">x *</strong>的乘法和平均值减法)跨越的空间上。然后在该空间中进行距离比较。</li></ol></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="5b1b" class="oo li it bd lj op oq or lm os ot ou lp ki ov kj lt kl ow km lx ko ox kp mb oy bi translated">降秩LDA</h1><p id="a884" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">我刚才描述的是用于分类的LDA。LDA还因其能够找到少量有意义的维度而闻名，使我们能够可视化并处理高维问题。我们所说的有意义是什么意思，LDA是如何找到这些维度的？我们将很快回答这些问题。</p><p id="1e6b" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">首先，看看下面的情节。对于具有三种不同类型的葡萄酒和13个输入变量的<a class="ae np" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine" rel="noopener ugc nofollow" target="_blank">葡萄酒分类</a>问题，该图将数据可视化在LDA找到的两个判别坐标中。在这个二维空间中，各个阶层可以很好地区分开来。相比之下，使用主成分分析发现的前两个主成分，类别没有被清楚地分开。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/f3768865d7d24e40e3fe7b551dc09096.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vliduHArNxywvZb6FrkRJw.png"/></div></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="8b08" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">LDA固有的降维</h2><p id="3695" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">在上面的葡萄酒例子中，一个13维的问题在2d空间中被可视化。为什么会这样？这是可能的，因为LDA有一个固有的降维。从上一节中我们已经观察到，LDA在不同类均值所跨越的空间中进行距离比较。两个不同的点位于一维直线上；三个不同的点位于2d平面上。同样，<em class="oa"> K </em>类意味着位于一个维数最多为<em class="oa"> (K-1) </em>的超平面上。特别地，该装置所跨越的子空间是</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/b6854a4934c3025414e8ee6951fe1f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lOa1Y566Ty1H69Acfye0oQ.png"/></div></div></figure><p id="27a6" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">当在该空间中进行距离比较时，与该子空间正交的距离不会添加任何信息，因为它们对每个类别的贡献是相等的。因此，通过将距离比较限制到这个子空间，不会丢失任何对LDA分类有用的信息。这意味着，通过将数据正交投影到这个子空间上，我们可以安全地将我们的任务从一个<em class="oa"> p </em>维问题转换为一个<em class="oa"> (K-1) </em>维问题。当<em class="oa"> p </em>远大于<em class="oa"> K </em>时，这是维度数量的一个相当大的下降。</p><p id="9d70" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">如果我们想进一步减少尺寸，从<em class="oa"> p </em>到<em class="oa"> L </em>，比如二维的<em class="oa"> L = 2 </em>怎么办？我们可以尝试从<em class="oa"> (K-1) </em>维空间构造一个<em class="oa"> L </em>维子空间，并且这个子空间在某种意义上对于LDA分类是最优的。</p><h2 id="f775" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">最佳子空间是什么？</h2><p id="13eb" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">Fisher提出，当空间中球形数据的类均值在方差方面具有最大分离时，小得多的<em class="oa"> L </em>维<em class="oa">T25】子空间是最佳的。根据这个定义，通过对球形类均值进行PCA，可以简单地找到最佳子空间坐标，因为PCA找到最大方差的方向。计算步骤总结如下:</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/3a80d269ca05ebaf756c5dff7666910f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NrYDn2Y0W2k79_XFkJO-Mw.png"/></div></div></figure><p id="aff5" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">通过这个过程，我们将数据从<strong class="me jd"> X </strong>替换到<strong class="me jd"> Z </strong>，并将问题维度从<em class="oa"> p </em>减少到<em class="oa"> L </em>。通过设置<em class="oa"> L = 2 </em>，通过该程序找到前一葡萄酒图中的判别坐标1和2。使用新数据重复先前的分类LDA过程被称为降秩LDA。</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="a14d" class="oo li it bd lj op oq or lm os ot ou lp ki ov kj lt kl ow km lx ko ox kp mb oy bi translated">费希尔的LDA</h1><p id="7575" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">如果之前降秩LDA的推导看起来与你之前知道的非常不同，你并不孤单！启示来了。Fisher根据他的最优性定义以不同的方式导出了计算步骤。他执行降秩LDA的步骤后来被称为Fisher判别分析。</p><p id="5d44" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">费希尔没有对数据的分布做任何假设。相反，他试图找到一个“合理的”规则，以便分类任务变得更容易。特别地，费希尔找到了原始数据的线性组合，其中类间方差<strong class="me jd"> B </strong> = cov( <strong class="me jd"> M </strong>)相对于类内方差<strong class="me jd"> W </strong>被最大化，如(6)中所定义的。</p><p id="b1f2" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">下面的图来自ESL，显示了为什么这个规则有直观的意义。该规则开始寻找一个方向，<strong class="me jd"> <em class="oa"> a </em> </strong>，其中，在将数据投影到该方向上之后，类意味着它们之间具有最大间隔，并且每个类在其内部具有最小方差。在这个规则下找到的投影方向，如右图所示，使得分类更加容易。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/25f900f86d89fe8fd4656a347963dc3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KZ4ZrKSAG8XIm7yndddp6A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在费雪的“合理法则”下找到的投影方向如右图所示。</p></figure><h2 id="12ac" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">寻找方向:费希尔的方式</h2><p id="df39" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">使用费希尔的合理规则，找到最佳投影方向相当于解决一个优化问题:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/1d114e4594c98cdacb69788bceea7e18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHhn84tSC_KYhCGhyEpF2A.png"/></div></div></figure><p id="f3a2" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">回想一下，我们希望找到一个方向，使类间方差最大化(分子)，类内方差最小化(分母)。这可以转化为广义特征值问题。对于那些感兴趣的人，你可以在我最初的<a class="ae np" href="https://yangxiaozhou.github.io/data/2019/10/02/linear-discriminant-analysis.html" rel="noopener ugc nofollow" target="_blank">博客文章</a>中找到这个问题是如何解决的。</p><p id="8bec" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">求解后得到最优子空间坐标，也称为判别坐标，作为inv(<strong class="me jd">W</strong>)∏<strong class="me jd">B</strong>的特征向量。<strong class="me jd"> </strong>可以看出，这些坐标与上述降秩LDA公式中从<strong class="me jd"> X </strong>到<strong class="me jd"> Z </strong>的坐标相同。</p><p id="915b" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">令人惊讶的是，与降秩LDA公式不同，Fisher在没有对总体进行任何高斯假设的情况下得出这些判别坐标。人们希望，有了这个合理的规则，即使数据不完全符合高斯分布，LDA也能表现良好。</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="f62a" class="oo li it bd lj op oq or lm os ot ou lp ki ov kj lt kl ow km lx ko ox kp mb oy bi translated">手写数字问题</h1><p id="9619" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">下面的例子展示了Fisher's LDA(简称LDA)的可视化和分类能力。我们需要使用64个变量(来自图像的像素值)来识别10个不同的数字，即0到9。数据集取自<a class="ae np" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits" rel="noopener ugc nofollow" target="_blank">这里的</a>。首先，我们可以将训练图像可视化，它们看起来像这样:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/a93432cd2fe0f0511e7a4681c8599944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AWsf2RU4tooIkq0gy3AKng.png"/></div></div></figure><p id="4497" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">接下来，我们在前半部分数据上训练LDA分类器。解决前面提到的广义特征值问题给了我们一个最佳投影方向的列表。在这个问题中，我们保留了前四个坐标，转换后的数据如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/a061f27e64713fa9e5ab5fe1f30c0e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qMSj2-CpNVehrPd4vinhQ.png"/></div></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="b38a" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">上面的图允许我们解释训练好的LDA分类器。例如，坐标1有助于对比4和2/3，而坐标2有助于对比0和1。随后，坐标3和4有助于区分坐标1和2中没有很好分开的数字。我们使用数据集的另一半来测试训练好的分类器。下面的报告总结了结果。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/b3423dda5718c6c74f2c2cbed640d2cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NEMbUIpHsXvuum0zjceIPA.png"/></div></div></figure><p id="0042" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">最高精度为99%，最低精度为77%，这是一个相当不错的结果，因为该方法是在大约70年前提出的。此外，我们没有做任何事情来改善这个具体问题的程序。例如，输入变量中存在共线性，收缩参数可能不是最佳的。</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="b4db" class="oo li it bd lj op oq or lm os ot ou lp ki ov kj lt kl ow km lx ko ox kp mb oy bi translated">LDA概述</h1><p id="4c4b" class="pw-post-body-paragraph mw mx it me b mf mg kd mz mh mi kg nb lq nx nd ne lu ny ng nh ly nz nj nk mm im bi translated">LDA的优点:</p><ol class=""><li id="a678" class="mc md it me b mf my mh na lq nl lu nm ly nn mm mn mo mp mq bi translated">简单的原型分类器:使用到类均值的距离，这很容易解释。</li><li id="a08a" class="mc md it me b mf mr mh ms lq mt lu mu ly mv mm mn mo mp mq bi translated">决策边界是线性的:实现简单，分类稳健。</li><li id="95a2" class="mc md it me b mf mr mh ms lq mt lu mu ly mv mm mn mo mp mq bi translated">降维:它提供了关于数据的信息性低维视图，这对可视化和特征工程都很有用。</li></ol><p id="a426" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">LDA的缺点:</p><ol class=""><li id="b92f" class="mc md it me b mf my mh na lq nl lu nm ly nn mm mn mo mp mq bi translated">线性决策边界可能无法充分区分类别。希望支持更通用的边界。</li><li id="7a67" class="mc md it me b mf mr mh ms lq mt lu mu ly mv mm mn mo mp mq bi translated">在高维设置中，LDA使用了太多的参数。LDA的正则化版本是期望的。</li><li id="0ce8" class="mc md it me b mf mr mh ms lq mt lu mu ly mv mm mn mo mp mq bi translated">希望支持更复杂的原型分类。</li></ol><p id="f22d" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated">谢谢你一直读到最后！在下一篇文章中，我们将介绍灵活的、惩罚的和混合判别分析来解决LDA的三个缺点。有了这些概括，LDA可以处理更加困难和复杂的问题，例如特征图像中显示的问题。</p><p id="e304" class="pw-post-body-paragraph mw mx it me b mf my kd mz mh na kg nb lq nc nd ne lu nf ng nh ly ni nj nk mm im bi translated"><strong class="me jd">如果你对更多统计学习的东西感兴趣，可以随意看看我的其他文章:</strong></p><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/a-math-free-introduction-to-convolutional-neural-network-ff38fbc4fc76"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd jd gy z fp po fr fs pp fu fw jc bi translated">卷积神经网络:它与其他网络有何不同？</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">CNN有什么独特之处，卷积到底是做什么的？这是一个无数学介绍的奇迹…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px lb pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/under-the-hood-what-links-ols-ridge-regression-and-pca-b64fcaf37b33"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd jd gy z fp po fr fs pp fu fw jc bi translated">引擎盖下:什么联系线性回归，岭回归，主成分分析？</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">从挽救病态回归问题，使快速计算的正则化路径，这是…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="py l pu pv pw ps px lb pj"/></div></div></a></div><h1 id="2653" class="oo li it bd lj op pz or lm os qa ou lp ki qb kj lt kl qc km lx ko qd kp mb oy bi translated">参考</h1><ol class=""><li id="c38d" class="mc md it me b mf mg mh mi lq mj lu mk ly ml mm mn mo mp mq bi translated">R.a .、费希尔、<a class="ae np" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x" rel="noopener ugc nofollow" target="_blank">、<em class="oa">分类问题中多重测量的使用</em>、</a> (1936)，《优生学年鉴》，7卷2期，179–188页。</li><li id="9259" class="mc md it me b mf mr mh ms lq mt lu mu ly mv mm mn mo mp mq bi translated">J.Friedman，T. Hastie和R. Tibshirani，<a class="ae np" href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="oa">统计学习的要素</em> </a> (2001)，统计中的斯普林格系列。</li><li id="ecc4" class="mc md it me b mf mr mh ms lq mt lu mu ly mv mm mn mo mp mq bi translated">K.V. Mardia，J. T. Kent和J. M. Bibby，<em class="oa">多元分析</em> (1979)，概率与数理统计，学术出版社。</li></ol></div></div>    
</body>
</html>