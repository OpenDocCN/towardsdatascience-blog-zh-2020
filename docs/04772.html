<html>
<head>
<title>Machine-learned Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习单词嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learned-word-embeddings-638c3fb5b916?source=collection_archive---------65-----------------------#2020-04-26">https://towardsdatascience.com/machine-learned-word-embeddings-638c3fb5b916?source=collection_archive---------65-----------------------#2020-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6454" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">没有数学的介绍</h2></div><p id="0215" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词是许多自然语言中的关键结构。它们被赋予了意义。往往因环境而大相径庭。拿<em class="le"> python </em>来说。这在计算机编程和生物学中意味着完全不同的东西。</p><p id="982b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要掌握一门语言，人和计算机都是从掌握关键词开始的。这篇文章试图解释计算机是如何掌握单词的。</p><p id="7cc4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用适合自然语言处理或信息检索(搜索)的形式来表示单词已经有很长的历史了。出现了两大类代表:<em class="le">本地</em>和<em class="le">分布式。</em></p><p id="afe1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章关注的是(I)分布式的和(ii)机器学习的表示。不熟悉本地或其他分布式表示的读者可能想先阅读自然语言处理中的单词和文本表示。</p><p id="c043" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这一点开始，我们转而使用术语<em class="le">单词嵌入</em>。这是通常用来表示“机器学习的分布式单词表示”的术语。</p><p id="1eba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">为什么<em class="le">机器学习</em>单词嵌入？</strong></p><p id="42ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">原因1 </strong>。可以仅从丰富的文档语料库中学习单词的准确和丰富的表示。举个例子。从维基百科语料库中，我们可以机器学习数百万不同单词的好单词嵌入。对它们运行算法就行了。呈现出来。</p><p id="7433" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">原因二</strong>。<em class="le">领域特定的</em>单词嵌入可以仅仅通过在领域特定的语料库上训练来学习。例如，从关于计算机编程的文档语料库中，嵌入单词的学习者可以了解python在计算机编程上下文中的含义。当然，这种含义与非编程环境中的含义完全不同。</p><p id="d34b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">想象一下，一个对某个领域一无所知的人，比如说计算机编程，要花多长时间才能理解它的词汇？即使是最基本的。</p><p id="7160" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">学习问题公式化</strong></p><p id="c31b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从高层次来看，很明显。我们想学习单词的“准确而丰富”的表达。不过有点太笼统了。我们如何知道算法是否(以及何时)学习得足够好？人们不断地与其他人互动，所以他们缺乏理解很快暴露出来。计算机还没有出现。因此，我们希望更具体地指定学习目标。</p><p id="c811" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个怎么样？我们想学习相似单词有相似表示法的表示法。嗯，这似乎也有点宽泛。事实上，我们已经获得了有用的特异性。</p><p id="6743" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">很容易构建成对单词的测试集，每对都标有“相似”或“不相似”。下面是一些例子</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="fff5" class="lp lq it ll b gy lr ls l lt lu">{lawyer, attorney} → similar<br/>{computer, cat} → not-similar</span></pre><p id="c319" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">标签可以被细化，例如，我们可以引入多个相似或相异的级别。</p><p id="c44e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">学习问题公式化:我们还没到那一步</strong></p><p id="1414" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">熟悉机器学习的读者可能会倾向于认为前面提到的测试集也说明了机器学习问题。并非如此。我们来解释一下。</p><p id="a66f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看，如果我们把测试集的形式作为学习问题的文字说明，我们会得到什么。我们得到:<em class="le">训练一个二元分类器，输入两个词，输出它们是否相似</em>。</p><p id="bebb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然这是一个值得解决的问题，但这不是我们的目标。我们希望输入是一个<em class="le">单个</em>字。我们希望算法输出这个单词的准确而丰富的表示。测试集只是作为一种间接的手段来评估学习模型对这项任务有多好。</p><p id="8e18" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">学习问题公式化:取两个</strong></p><p id="deb1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好吧，让我们重新开始</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="ec6f" class="lp lq it ll b gy lr ls l lt lu">P1: We’d like to learn representations of words in which similar words have similar representations.</span></pre><p id="f066" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑将此重新表述为</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="6989" class="lp lq it ll b gy lr ls l lt lu">P2: We’d like to learn representations of words that capture their context.</span></pre><p id="f9c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">什么是“语境”？这个词经常出现在其他词的近旁。<em class="le">猫</em>的上下文将包括<em class="le">胡须</em>。但不是<em class="le">电脑</em></p><p id="4a4a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有理由认为，语境表征相似的词本身也是相似的。因此，解决P2问题也是解决P1问题的好办法。</p><p id="ea4f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们取得了进展。我们之前的公式包括输入两个单词。这才<em class="le">一个</em>！</p><p id="6cc1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯。我们如何获得(<em class="le">单词</em>，<em class="le">上下文</em>)对的训练集？在这一点上，我们甚至不知道如何将上下文的概念塞进一个变量中，这个变量可以作为我们的预测目标！</p><p id="cc27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好吧，让我们先解决“将上下文填充到变量中”的问题。我们将一个单词的上下文建模为词典中单词的概率分布。以<em class="le">猫</em>为例。<em class="le">胡须</em>在<em class="le">猫</em>的上下文中的概率应该很高。<em class="le">计算机</em>在<em class="le">猫</em>的上下文中的概率应该很低。</p><p id="ee3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">上下文向量</strong></p><p id="4a1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在本帖中描述的学习方法最好在神经网络环境中描述。神经网络喜欢在向量空间上操作。也就是说，它们将输入向量映射到输出向量(或标量)，通常在两者之间会发生一些神奇的事情。</p><p id="2892" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经完成了输出。词典中单词的概率分布是空间中的向量，其维数是单词。(所以一百万单词词典的向量空间将有一百万个维度。)向量的一个分量的值就是相应单词的概率。</p><p id="924e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输入呢？这是一个特别的词。这也可以表示为同一空间中的概率向量。与单词相关联的维度的值为1，其余为0。</p><p id="b640" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们当前的架构可以描述为</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="cfa7" class="lp lq it ll b gy lr ls l lt lu">I ⇒ magic ⇒ O</span></pre><p id="b52e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">I和O是同一空间上的概率向量，而“魔术”是在它们之间发生的任何事情。</p><p id="21aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">训练集？</strong></p><p id="b2d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了学习，我们需要一个训练集(<em class="le">单词</em>、<em class="le">上下文</em>)对。不清楚如何建造。单词不会被贴上上下文的标签。</p><p id="a519" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以从具有代表性的文档语料库中估计单词的上下文向量。在这个词附近出现(足够频繁)的词是在它的上下文中，那些不在它的上下文中的词不是。</p><p id="a5d7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们所说的“代表”是什么意思？我们指的是与我们试图学习的内容相匹配的语料库。学习计算机编程术语，我们的语料库应该由计算机编程文档组成。</p><p id="62fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">训练集简化</strong></p><p id="7071" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">建立上述训练集包括从语料库中学习各种单词的上下文向量。我们可以简化这个。相反，我们可以用一组足够接近的成对单词来训练，例如在同一个句子中。表示这样的一对(<em class="le"> word1 </em>，<em class="le"> word2 </em>)。我们将此解释为单词2出现在单词1的上下文中的实例。</p><p id="c9cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一个例子。</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="bded" class="lp lq it ll b gy lr ls l lt lu">Sentence: the <strong class="ll iu">dog</strong> and <strong class="ll iu">cat</strong> are <strong class="ll iu">black<br/></strong>Training Set: dog → cat, cat → dog, dog → black, black → dog, cat → black, black → cat</span></pre><p id="5aae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练实例仅限于加粗的实例。(剩下的就当是停用词吧。)</p><p id="8a65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将来自我们语料库中所有文档的所有句子的这种对流传送到学习算法将有效地向算法揭示每个单词的上下文向量。</p><p id="bed4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种简化对我们有两方面的好处。</p><p id="dc6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，它简化了构建训练集的过程。我们可以独立处理每个句子。简单地发出由此产生的训练对。我们甚至可以在一个流动的，即连续学习的环境中做到这一点。想象一下新文档不断到达，就像web级搜索引擎的索引器一样。我们可以从他们那里即时流出新的训练实例。</p><p id="0e6c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二，有利于学习算法。与输入相关联的目标是概率向量，其质量集中在一个单词上。这个目标比一个任意的概率向量更清晰。</p><p id="c137" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，现在谈谈学习算法。我们只会看到一个。传达主要思想就足够了。也就是说，那些寻求从大型语料库中的大型词典上建立工业强度单词嵌入的人需要阅读更多。</p><p id="28fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">连续词袋模型</strong></p><p id="ae66" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个描述相对于最初的描述有所简化，尽管对于我们在这篇文章中要解决的问题来说已经很好了。</p><p id="a0bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个只有一个隐藏层的神经网络。</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="b63e" class="lp lq it ll b gy lr ls l lt lu">I ⇒ H ⇒ O</span></pre><p id="f303" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，我们用H代替了魔法。确实神奇！</p><p id="7cdc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这两个符号下面发生了什么？这在下面描述。</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="3a5d" class="lp lq it ll b gy lr ls l lt lu">H = W1*I O=probability_normalizer(W2*H)</span></pre><p id="80db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们解释一下数据是如何从输入流向输出的。我们从输入向量<em class="le"> I </em>开始。我们用矩阵W1乘以<em class="le"> I </em>。这给了我们一个隐藏向量<em class="le"> H </em>。接下来，我们用矩阵W2乘以<em class="le"> H </em>。得到的向量还不一定是概率向量。我们对它应用概率规格化器，把它转换成一个。这就变成了我们的输出向量<em class="le"> O </em>。</p><p id="0ca0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一些问题仍然存在。首先，学习发生在哪里？第二，这个概率规格化器的概念有点抽象。</p><p id="b5a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">对第一个问题</strong>的回应。实际上，我们到目前为止描述的是输入到输出的数据流。我们将在一两分钟后开始实际的学习。我们将学习的参数是矩阵W1和W2中的值。</p><p id="eed2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">对第二个问题</strong>的回应。下面是两个概率规格化器的具体例子。两者都假设输入是一个具有非负值的向量。</p><p id="834c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一个，姑且称之为<em class="le">简单</em>，只是取每个值，除以所有值的和。第二个称为<em class="le"> softmax </em>，首先对值取幂，然后应用简单的规格化器。softmax是在实践中使用的。为什么？在我们描述了实际的学习算法之后，我们将给出一些直觉。</p><p id="c874" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是两个规格化函数的例子。</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="90b2" class="lp lq it ll b gy lr ls l lt lu">simple([1,3,5]) → [1/9,3/9,5/9]<br/>softmax([1,3,5]) → simple([e¹,e³,e⁵])</span></pre><p id="9fa2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在神经网络术语中，概率归一化器是一个<em class="le">矢量激活函数</em>的实例。</p><p id="bc59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">更多要点</strong></p><p id="8651" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">鉴于学习的过程将包括学习从<em class="le"> I </em>预测<em class="le"> O </em>，一旦我们学习得足够好，我们将只提取权重矩阵W1。我们会扔掉W2。</p><p id="45a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们只关心从<em class="le"> I </em>到<em class="le"> H </em>的映射。<em class="le"> H </em>将作为由<em class="le"> I </em>表示的单词的表示。(也就是说，为了学习一个好的映射<em class="le"> I → H </em>，我们别无选择，只能解决完整的学习问题，因为我们没有关于<em class="le"> H </em>的目标，只有关于<em class="le"> O </em>的目标。)</p><p id="81dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个关键点是，我们会有意将<em class="le"> H </em>的维度限制为远小于<em class="le"> I </em>和<em class="le"> O </em>的维度。鉴于<em class="le"> I </em>和<em class="le"> O </em>可能有几百万个维度，我们将强制<em class="le"> H </em>有几百个，可能一千个，也可能更少。想法是迫使学习算法学习<em class="le">密集</em>单词表示。</p><p id="d660" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">理由是这样的。首先，这是否可行？是的。单词有结构，否则一开始就不会有任何单词相似的概念。所以我们应该能够将<em class="le"> I </em>空间压缩到一个维度更低的空间。事实上，很难想象有超过20个描述单词的正交潜在属性！也许我们只是想象力不够丰富。所以让我们把这个乘以五倍。甚至更好，百倍。这还是在低几千！</p><p id="5838" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好的，我们认为我们已经解决了可行性问题。接下来，我们为什么要这么做？(可行不代表是个好主意。)原因是密集表示更好地概括。直觉很简单:如果我们认为单词的潜在特征比它们的数量少得多，那么强迫学习者映射到低维空间将比试图学习映射到原始向量空间更好地概括。</p><p id="a5c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最后，学习！</strong></p><p id="4088" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将通过一个简单的例子来集中传达直觉。没有数学。</p><p id="b4dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的例子是在一个只有四个单词A，B，C，d的词典上。我们将H设为二维。我们将使用训练集</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="1436" class="lp lq it ll b gy lr ls l lt lu">(A,B), (B,A), (C,D), (D,C)</span></pre><p id="998c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将从空白状态开始学习，将W1和W2中的所有值随机初始化为较小的正值或负值。接下来，我们将呈现第一个训练实例的输入A，其编码为1000。所有可学习的权重都接近0；所以<em class="le"> H </em>会是(~0，~0)，W2* <em class="le"> H </em>会是(~0，~0，~0，~0)。我们假设probability_normalizer激活函数将这个<em class="le"> H </em>映射到(~、~、~、~)。目标是B，具体来说，是编码(0，1，0，0)。这个目标与预测大相径庭。所以我们需要学习。</p><p id="701c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来发生的事情最好用一张图来传达。</p><figure class="lg lh li lj gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lv"><img src="../Images/f554d9af62c6fa7ce5c09b00af5efd78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ErgYQDEsn204F4KrQp3k9A.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">学习范例</p></figure><p id="df3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在图中，<em class="le"> T </em>表示目标输出。从<em class="le"> H </em>到<em class="le"> O </em>的圆弧上的+s和-s表示根据O<em class="le">和<em class="le"> T </em>之间的差值改变重量的方向。我们看到B的产出的权重应该增加；其余减少。(这是因为B的产量太低；其余的产量太高了。)</em></p><p id="e016" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还没完。我们还需要计算从<em class="le"> I </em>到<em class="le"> H </em>的权重调整。为什么？因为这些权重值也可能导致我们在输出端看到的误差。</p><p id="9a2f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些调整是通过著名的反向传播算法计算的。正是这种算法，在这种隐藏的基于层的架构中利用适当选择的激活函数(在我们的情况下是概率归一化器)，提供了魔力。</p><p id="5b24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来发生的事情最容易用数字来描述。设<em class="le"> H </em> 1表示<em class="le"> H </em>的第一个分量。<em class="le"> H </em> 1的当前值为0.1。考虑B的输出。它看到的错误是~ 1-=。在这种情况下，如果我们能够以某种方式在<em class="le"> H </em> 1上设置一个合理的目标，我们就会知道将重量从<em class="le"> I </em>移动到<em class="le"> H </em> 1的方向。如果1的目标值大于0.1，我们会降低权重。如果1的目标值小于0.1，我们会增加权重。</p><p id="2d22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯，估计1的目标和估计1看到的误差是一样的。这一点我们可以估计如下。考虑一个特定的输出节点。它的误差是目标输出减去实际输出。我们首先通过probability_normalizer反向传播这个误差，然后通过从H1到这个输出节点的权重。这给了我们H1从这个输出节点看到<em class="le">的误差。输出节点上这些误差的总和。是H1认为的完全错误。</em></p><p id="faa2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们一步一步地解决这个问题。三个输出都有错误，其中一个(B)有错误。想象一下，这些误差通过概率归一化器成比例地反向传播。(我们只是要求你去想象，而不是假设。)加上回想一下，从<em class="le"> H </em> 1到<em class="le"> O </em>的权重是随机的，幅度很小。考虑到这两个因素，在<em class="le"> H </em> 1处对这些误差求和将大致抵消它们，即产量接近0。</p><p id="0718" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，H2看到的误差也将接近于0。这令人不安吗？不。在这一点上，<em class="le"> H </em> 1和<em class="le"> H </em> 2都不知道它想要成为什么。<em class="le"> H </em> 1和<em class="le"> H </em> 2误差虽然可能接近0，但也可能不同。因此，从T36到T37，H1和H2的权重可能会经历至少略有不同的调整。这将导致H1和H2在他们所学的东西上产生分歧。在许多训练实例中，在我们的例子中，这意味着在相同的4个训练实例中的多次迭代，<em class="le"> H </em> 1和<em class="le"> H </em> 2将学习从输出中挑选不同的东西。引发这一现象的现象被称为<em class="le">对称性破缺</em>。</p><p id="1f5d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">为什么选择Softmax？</strong></p><p id="20cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">哦，我们忘了。我们答应解释这个。直觉告诉我们，我们想要鼓励学习区分。这样想。我们在隐藏层中有一个神经元池。起初，他们不知道他们应该做什么。如果我们不鼓励他们开始歧视，他们可能会保持不承诺。我们都知道会发生什么。有没有在一个没有人想脱颖而出的委员会里？</p><p id="8db6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯，softmax功能就像一位高管，推动人们探索自己舒适区以外的地方。softmax中的幂运算具有放大差异的效果。于是轻微的赢家(输家)变成了强大的赢家(输家)。</p><p id="7e62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">延伸阅读</strong></p><p id="5482" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">【https://arxiv.org/pdf/1411.2738.pdf】T2—word 2 vec参数学习讲解</p><p id="e251" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Mikolov，Sutskever，I .，Chen，k .，Corrado，G. S .，和Dean，J. (2013b)。词和短语的分布式表示及其组合性。神经信息处理系统进展，3111-3119页。</p></div></div>    
</body>
</html>