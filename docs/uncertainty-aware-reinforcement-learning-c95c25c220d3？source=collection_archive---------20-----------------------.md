# 不确定性感知强化学习

> 原文：<https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3?source=collection_archive---------20----------------------->

## 窥探知道自己在做什么的建筑代理

[![](img/56bf769159fc4cc650025cf7aab45da0.png)](https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3?)

马赛舞蹈:[兰迪神父](https://unsplash.com/@randyfath)上 [Unsplash](https://unsplash.com/photos/osXl4JI3vcQ)

基于模型的强化学习(RL)最受样本效率的青睐。它对期望的输入量要求不高，并且对我们期望模型实现的目标有一个上限。

这个模型不太可能完美地再现环境。当通过训练有素的代理与现实世界互动时，我们可能会遇到与训练中看到的不同的状态和奖励。为了让基于模型的 RL 发挥作用，我们需要克服这个问题。这至关重要。这有助于我们的特工知道自己在做什么。

第一，无模型 RL 怎么样？无模型 RL 总是在训练和测试代理时使用*地面实况*环境转换。除非有我们引入的偏移，比如模拟到真实的转移，在这种情况下，我们不能责怪算法。因此，不确定性不是一个大问题。对于像 Q 函数 *Q(s，a)*这样优化动作的东西，我们可以尝试在动作选择上集成确定性意识。但是既然它工作得很好，就目前而言，闭上我们的眼睛，假装我们没有看到，这并没有什么坏处。

## 内容

1.基于模型的 RL
2 的不确定性来源。不确定性意识的好处
3。构建不确定性感知模型
-什么看起来可行
-什么可行
4。结论

## 不确定性的来源

模型不确定性是由模型在测试期间看到的数据和用于训练模型的数据之间的分布不匹配造成的。我们在不同于培训期间看到的分布上测试代理。

# **确切地说，不确定性意识会产生什么样的影响？**

在训练开始时，模型***p****(****s****ₜ₊₁|****s***ₜ*、* ***a*** 、ₜ*已经暴露了自己相当小的真实世界数据。我们希望函数不要过度适合这个小数量，因为我们需要它足够有表现力来捕捉后面时间步骤中的转换。然后，将积累真实数据来学习精确的模型。*

*这在基于模型的 RL 中很难实现。为什么？RL 的简单目标是最大化未来的累积回报。计划者以此为目标，试图遵循模型预测高回报的计划。因此，如果模型高估了特定行动序列将获得的回报，规划者将很高兴遵循这一闪闪发光但错误的估计。在现实世界中选择这样的行为会导致滑稽的行为。简而言之，计划者有动机利用模型的积极错误。*

*(我们可以把计划器看作是我们在给定世界状态的情况下选择最佳行动的方法)。*

*这还能更糟吗？在高维空间中——比如输入是一幅图像——由于[潜在变量](https://en.wikipedia.org/wiki/Latent_variable)，模型会犯更多的错误。在基于模型的 RL 中，通常通过使用策略上的数据收集来缓解分布不匹配，在现实世界中观察到的转换被添加到训练数据中，并用于重新规划和纠正模型中的偏差。然而，在这种情况下，错误太多，政策上的修正无法赶上失去的模式。大量的错误可能会导致我们每次重新计划时都改变策略，结果，模型可能永远不会收敛。*

*我们可以选择为模型可能犯的每一个错误收集数据，但如果我们能够检测到模型可能出错的地方，那么基于模型的规划者就可以避免可能导致严重后果的行动，这不是更好吗？*

# ***估计不确定性***

*首先，让我们用一个简单的故事来表达我们所知道的。*

*一对恩爱的夫妇得到了一个婴儿和一个机器人的祝福——不一定同时得到。作为一名保姆，机器人的目标是让小朱丽叶开心。虽然实现这一目标受到奖励的激励，但机器人避免任何损坏或可能伤害婴儿的事情也是可取的。*

*现在，婴儿变得喜欢在指着虫子的时候哭泣——因为[好孩子会这样做](https://raw.githubusercontent.com/mugoh/curious/master/20200628_191322-01.jpeg)——机器人的最佳奖励计划变成压扁虫子，让婴儿朱丽叶看着它把害虫喂给猫。*

*不过，为了改变，假设机器人在电视上指着一些可怕的东西时遇到了婴儿哭泣——这是一种不熟悉的状态，似乎接近婴儿朱丽叶的哭泣指行为。不确定动态，机器人的最佳计划可能是压扁电视，喂猫。我们不确定这是否会让宝宝开心，但这肯定会造成伤害。*

*然而，如果模特因为不自信而回避了这个动作，那么最好不要碰电视，以一个悲伤的朱丽叶为代价来避免伤害。*

*一个不确定性感知模型可以让代理人知道哪里出现不良结果的可能性高，哪里需要更加小心。但是如果模型对采取行动后的结果没有信心，那么利用它来达到目标可能是好的。*

*如果我们的机器人确信泡菜能让朱丽叶平静下来，而不会带来任何风险，那么它可能会考虑跑到厨房，让她咀嚼一个，因为这样一来，它就能达到让她开心的目的。*

*一个能够获得其不确定性的精确估计的模型，给予基于模型的计划者避免有不小可能导致不希望的结果的行动的能力。渐渐地，模型将学会做出更好的估计。不确定性意识也将告知模型它需要探索更多的状态。*

# *有什么解决方案吗？*

***利用熵***

*我们知道熵是随机性的一种度量，或者随机变量的概率分布的扩散程度。*

*![](img/4e074fd4806b3352eaa1b1042644e01b.png)*

**离散随机变量的熵* X*

*当每一个结果(***【xᵢ】***)以相等的概率发生时，熵将达到峰值，即最大不确定性，并且当存在具有高概率 *p* (几乎为 1)的单个结果时，熵将最小，而其余结果共享接近于零的概率( *1 — p* )，即最大确定性。*

*![](img/cff2fda4a6749dea6c03ff0e6fb11f6e.png)*

**这里，A 比 b 具有更低的不确定性，对于 A 来说* [*单峰*](https://en.wikipedia.org/wiki/Unimodality) [*高斯分布*](https://en.wikipedia.org/wiki/Gaussian_function) *，熵和方差(围绕均值扩散)倾向于在相同的相对方向上移动。**

*然而，问题就在这里——数据的不确定性会影响我们模型的不确定性吗？为了更好地表达这个问题，这里有两个图表:*

> *你将会看到手工制作的数据的可视化图表。把注意力放在我们要用它们传达的概念上，而不是它们代表什么，好吗？谢了。*

*![](img/69edbbae5f36a2d7436896c83baf2877.png)*

***左** : *模型预测为线性函数——模型拟合数据，但是“太好了”。——*——*——*——****右*** *:拟合多项式回归函数**

*这两个情节有助于我们区分两个相关的概念。在它们之间，你认为哪一个是不确定的？抓住一个轻松的时刻再次观察它们，并尝试想出一个答案。*

*你选好了吗？让我给你一些思考的空间。*

*![](img/b65c93b2587b104c780c2f886a0746d3.png)*

*在第一个图中，模型对几千个样本进行了线性回归(LR)。数据似乎是确定的。过度拟合显示确定性，对吗？当然，模型对数据很有信心。红线预测的方差将非常接近于零。但是，这个估计是否给出了关于观察结果的最佳解释呢？*

*为此，模型需要通过在数据和可能的协变量之间建立良好的线性关系来解释观察结果。但是不确定包括哪些和省略哪些，它包括了(几乎)所有的东西！这影响了模型的不确定性，因为我们对模型没有信心。*

*在多项式回归(PR)中，数据中存在噪声。它在观测值中产生了[不确定性。样本显示了与平均值的显著偏差，但模型相对来说拟合得很好——我们仍然可以在没有提供牛奶的情况下从心理学课上获得一些笑声，这似乎足够真实。与通过过度拟合线性函数接收的误差相比，这将给出更大的 MSE。](https://en.m.wikipedia.org/wiki/Measurement_uncertainty)*

*所以回到我们最初的问题，数据中的不确定性不会揭示我们的模型-不确定性。这使得熵，作为模型不确定性的度量，并不总是有效的，因为当数据的方差接近零时，即使模型仍然不确定，熵也是低的，如 LR 图所示。*

# ***不确定性感知模型——有效的解决方案***

***a)学习预测不良行为的函数***

*考虑无人机在雨林中学习飞行。我们希望它能学会在环境中导航，同时避免与树木相撞。在 RL 中，代理通过尝试一个动作来学习该动作的结果。所以要学会躲避树，它必须经历几次撞击。但是高速撞击肯定会造成破坏。*

*我们可以通过让无人机经历温和、低速的撞击来训练它，这样它就能学习森林环境。当它遇到训练分布中不存在的森林部分时，它需要了解其策略的不确定性，以便在收集新的训练数据时能够与该部分进行安全交互。一旦对该部分有信心，它就可以在未来高速飞行。这是一个安全探险的例子。*

*为了实现这一点，我们在 RL 成本函数 *c(st，at)* 中集成了击中树的成本，以使 *c(st，at) + C_bad。* *C_bad* 是分配给导致不良行为(冲突)的行为的新成本。它影响着无人机何时可以快速飞行，何时应该小心翼翼。*

*为了估计 *C_bad，*我们使用一个不良行为预测神经网络 ***P*** *，*带权重***θ****。*它将无人机的当前状态 *st* 作为输入，它是观测值*o*ₜ*t19】加上一系列动作*a*ₜ*，aₜ₊₁…ah*无人机计划执行 *e* 并估计碰撞发生的概率。**

*动作序列由[模型预测控制](https://en.wikipedia.org/wiki/Model_predictive_control) (MPC)在从当前时间步长 *t* 直到 *t + H 的后退时间范围内选择和优化。不良行为模型***P****θ*输出一个[伯努利分布](https://en.wikipedia.org/wiki/Bernoulli_distribution)(二进制 0 或 1)，指示在该时间范围内是否发生碰撞。**

*为每个水平 h 记录碰撞标签。这意味着对于标签 1，在时间步长 *t* 和*t+h*之间的子序列中发生了不良行为。利用以上述输入为条件的这个概率标签，不良行为模型可以简单地表示为:*

*![](img/3922107bfaa6f9899f835976bcf2a08e.png)*

*使用 P_theta 模型估计不良行为 C_bad 的成本*

*类似地，一个简单的实现应该是这样的:*

*然而，你可能已经注意到****P****θ*输出的是不良行为的概率分布，而不是该行为的实际费用。因此，实际的不良行为成本将乘以这个概率 *p* 得到 *pC_bad* 。最后，我们用一个标量 *λ* 来调整它，这个标量决定了与实现目标相比，代理避免风险结果的重要性。**

**![](img/fc7bd701fa57bb90c0e5b71cd61ed866.png)**

**RL 成本由**目标成本**和代理的不良行为加权成本**构成****

**值得注意的是，虽然我们想要一个预测不安全行为的函数，但一个接受输入并给出安全估计的[判别模型](https://en.wikipedia.org/wiki/Discriminative_model)可能并不总是让我们高兴——它的预测在不熟悉的状态下可能毫无意义。最好是，在其预测中加入模型不确定性是有益的。**

****b)贝叶斯网络****

**神经网络可以被称为条件模型 *P(y|x，w)，*，其给定输入 *x，*使用权重 *w，*将概率分配给每个可能的输出 *y* 。对于贝叶斯神经网络，不是每个神经元具有单个输出，而是将权重表示为可能值上的概率分布。**

**![](img/7b13e7ed3be77825c43dfb58de171453.png)**

***右图:贝叶斯神经网络。代替每个权重的单个固定标量，概率分布被分配给所有可能的值(* [*源*](https://arxiv.org/pdf/1505.05424v2.pdf) *)***

**这是如何工作的？**

**使用一组训练样本 *D* ，我们在模型权重上找到一个[后验](https://en.wikipedia.org/wiki/Posterior_probability)，以这些样本 *P (* **w** *|D)为条件。*为了预测特定标签*或*的分布，由后验分布缩放的权重的每个可行组合对相同的输入 *x* 进行预测。**

**如果一个单位对观测值不确定，这将在输出中表示出来，因为不确定性越高的权重在预测中引入的可变性越大。这在模型发现极少数据或没有数据的地区很常见，这将鼓励勘探。随着观察越来越多，模型做出的决策也越来越确定。**

**对权重*P(***w***| D)*的后验分布进行近似。这是通过试图找到权重 q(**w**|*θ*)上不同分布的参数*θ*来完成的，通过使其[尽可能接近真实的后验分布*P(***w**|*| D)。*这是](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)[变分推论](https://medium.com/@jonathan_hui/machine-learning-variational-inference-273d8e6480bb)；有点超出我们目前的范围:)。**

****c)辍学****

**在 RL 辍学是一个[坏主意](https://ai.stackexchange.com/a/8295)。不过，这不是我们要冒的风险。还记得我们说过，一个有区别的模型不会总是让我们快乐，除非它能在不良行为预测中包含不确定性？辍学是一个简单的方法。**

**丢弃是一种正则化技术，它以概率 *p、*随机丢弃神经网络中的一个单元，或者以概率 1 *— p* 保留它。它经常在训练中使用，以防止神经元*过度依赖彼此*。这在每次训练迭代期间创建了新的但相关的神经网络。**

**在实践中，已知仅在训练期间应用漏失，并在测试时移除，以实现高测试精度。然而，通过保留测试时的漏失，我们可以通过寻找不同前向传递的样本均值和方差来估计不确定性。这是估算不确定性的简单方法。**

**它的警告是，作为一种变分推理方法，[由于](http://jmlr.org/proceedings/papers/v48/gal16.pdf)[变分下限](https://en.wikipedia.org/wiki/Evidence_lower_bound)而严重低估了不确定性。**

**那是什么意思？**

> **为了理解这一点，我们需要引入 KL 散度——一种对同一随机变量的两个概率分布之间差异的度量。**

**有时，在大的实值分布上寻找真实概率是昂贵的。因此，取而代之的是使用该分布的近似值，并且两者之间的 KL 散度(差)被最小化。**

**![](img/d6c5fb3faf74ad3cd37bfbd5c3a23547.png)**

***p(x)与 q(x)之间 KL 发散的一个例证——****DKL(p | | q)****(*[*出处*](http://www.deeplearningbook.org/) *)***

**在上图中， *q(x)* 是精确分布 *p(x)* 的近似值。这种近似的目的是在 *p(x)* 具有高概率的情况下放置高发生概率。在插图上，注意 *q(x)* 是单高斯，而 *p(x)* 是两个高斯的混合？为了在 *p(x)* 的概率高的地方放置高概率， *q(x)* 使 *p* 中的两个高斯分布相等，以在两者上相等地放置高概率质量。**

**类似地，在输入 *x* 和标签 *y 的条件下，漏失在模型的权重 *w* 上具有真实的后验概率 *p(w| x，y)*。q(w)*被用作该后验概率的近似分布。然后我们降低 *q(w)* 和实际后验概率 *p(w| x，y)* 之间的 KL 散度，使它们尽可能接近*。*然而，这样做将对 *q(w)* 在 *p(w)* 没有概率质量的情况下放置概率质量进行惩罚，但是忽略 *q(w)* 在 *p(w)* 实际上具有高概率的情况下没有放置高概率质量。这就是低估模型不确定性的原因。**

***c)引导程序集合***

*训练多个独立的模型，并对它们的预测进行平均。如果这些模型近似一个几乎相似的输出，这将表明他们同意，表明他们的预测的确定性。*

*![](img/b39cecf7b585892afbe0b2870b99190f.png)*

*平均 N 个集合模型*

*为了使模型相互独立，每个模型的权重*θᵢ*用数据[的子集训练，数据](https://en.wikipedia.org/wiki/Sampling_%28statistics%29#Replacement_of_selected_units)[用来自训练集的替换](https://en.wikipedia.org/wiki/Sampling_%28statistics%29#Replacement_of_selected_units)采样。然而，已知训练期间权重的随机初始化*θᵢ*和随机梯度下降使它们足够独立。*

*作为不确定性的一种度量，漏失可以被假定为集合方法的廉价近似，其中每个采样漏失充当不同的模型。贝叶斯神经网络也有集成概念——通过对权重**(***w***| D)*取后验分布下的[期望](https://en.wikipedia.org/wiki/Expected_value)，贝叶斯网络变得相当于无限个集成——越多意味着越好。**

****d)好奇 iLQR****

**将好奇心视为解决代理环境中不确定性的灵感。让我们看看如何将好奇行为添加到代理的控制循环中。**

> **一些 LQR 的背景**

**在 RL 中，线性二次型调节器(LQR)输出一个用于开发模型的线性控制器。当处理非线性动力学时，我们使用线性回归在每个时间步拟合模型*(sₜ₊₁|sₜ，aₜ)。这种迭代控制过程被称为迭代 LQR (iLQR)，是差分动态规划(DDP)的一种形式。***

***系统动力学由以下等式表示:***

***![](img/e9e1d9a9ba13fade660616e3fc8fabd8.png)***

***系统动态学***

******f*** 代表学习到的动力学模型而 ***xₜ ₊₁*** 是下一时间步的状态，表示为当前状态*加上模型对当前状态的预测变化 ***x* ₜ** 当采取动作 *u* ₜ 时*。*例如，如果状态是机器人的速度， ***x* ₜ** 将是当前速度，而 ***f(x* ₜ *，u*ₜ*)***δ*t*将是选择 *u* ₜ 时的预测变化，从而产生新的速度****

> *****让人好奇*****

***对于上述系统动力学中不确定性的积分，它被写成一个[高斯分布](https://en.wikipedia.org/wiki/Normal_distribution)，由一个均值向量μ和一个协方差矩阵σ表示。***

***高斯策略具有将状态和动作对映射到状态平均变化的神经网络。这种状态的变化就是模型*μ(****f****)表示的均值向量。****

**我们可以通过从正态分布绘制模型 ***f*** 来将系统动力学实现为高斯过程(GP)，其中我们试图学习使成本函数最小化的最佳均值向量 *μ* 。GP 随后使用以下等式进行预测:**

**![](img/c201ca6d2a9c32d6de5ab0d5eacda1d0.png)**

**作为高斯过程的系统动力学**

**其中 *f(x* ₜ *，u* ₜ *)* 为可训练动力学函数表示的均值向量，*σₜ₊₁***为当前状态和动作下 GP 预测的协方差矩阵。****

****这个 GP 等同于一个普通的非好奇的 LQR 随机动力学方程。有什么不同？在非好奇的 iLQR 中，由于高斯的[对称性，我们将忽略方差参数*σₜ₊₁*。然而，**好奇 iLQR 需要预测分布的协方差来确定模型不确定性**。高模型不确定性等于高方差。*σₜ₊₁*代表当前状态和动作下模型对预测 *xₜ ₊₁* 的不确定性( *x* ₜ *，u* ₜ).](https://math.stackexchange.com/a/544268)****

****来自 GP 模型的这种不确定性随后被用于恳求代理采取行动来解决模型在这种状态下的未来不确定性。简而言之，鼓励代理人选择减少模型方差的行动。这是通过奖励代理人包含一定程度的不确定性的行为，同时仍然最大化特定目标的奖励来实现的。****

****![](img/5374c91795c0f198f34c63556bc8b282.png)****

****山地车实验:目标是到达山顶。由 ***开始寻求负回报*** *(向后移动到对面的山上)汽车获得足够的动力到达顶部。看到只有好奇的特工才知道* ***探究*** *这个。(* [*来源*](https://arxiv.org/pdf/1904.06786.pdf) *)*****

*****理解* [*基于模型的 RL 中的 LQR 优化*](https://medium.com/@jonathan_hui/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750) *可能有点杂耍，但对于掌握好奇号算法是如何导出的是必不可少的。不过，我们现在不要用这些方程来吓唬自己。*****

****奖励好奇的行为使代理比使用标准的 iLQR 更快地达到目标。防止模型陷入局部最优，在短时间内找到更好的解。****

## ****结论****

****不确定性意识可用于影响代理的学习策略，这取决于它如何添加到培训成本中。它可以鼓励探索或悲观行为，在这种情况下，代理人避免可能有风险的结果。后者是 AI 安全感兴趣的。为了更加稳健，增加不确定性意识的解决方案可以一起使用。****

******参考文献******

****[1] A. Nagabandi，G. Kahn，S. Fearing，S. Levine，[无模型微调的基于模型的深度强化学习的神经网络动力学](https://arxiv.org/pdf/1708.02596) (2018)，ICRA 2018。****

****[2] S. Daftry，S. Zeng，J. A. Bagnell，M. Hebert，[内省知觉:学习预测视觉系统中的故障](https://arxiv.org/pdf/1602.04450v1.pdf) (2016)，IROS，2016。****

****[3] C. Blundell，J. Cornebise，K. Kavukcuoglu，D. Wierstra，[神经网络中的权重不确定性](https://arxiv.org/pdf/1505.05424v2.pdf) (2015)，ICML 2015。****

****Y. Gal 和 Z. Ghahramani。[作为贝叶斯近似的辍学:表示深度学习中的模型不确定性](http://jmlr.org/proceedings/papers/v48/gal16.pdf)。ICML，2016。****

****[5]李彦宏和李彦宏。[具有α发散度的贝叶斯神经网络中的脱落推理](https://arxiv.org/pdf/1703.02914.pdf)，2017。****

****[6]基于模型的强化学习，[深度 RL 决策与控制](http://rail.eecs.berkeley.edu/deeprlcourse/) (2019)， *Berkley* 。****

****[7]卡恩、维拉弗洛、庞和莱文。[碰撞避免的不确定性感知强化学习](https://arxiv.org/pdf/1702.01182v1.pdf)，2017。****

****[8] S .贝克特尔，a .拉伊，y .林，l .里盖蒂，f .迈耶。[好奇 iLQR](https://arxiv.org/pdf/1904.06786.pdf) :解决基于模型的 RL 中的不确定性。ICML，2019。****