<html>
<head>
<title>Why is Batch Normalization useful in Deep Neural Networks?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么批量归一化在深度神经网络中有用？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/batch-normalisation-in-deep-neural-network-ce65dd9e8dbf?source=collection_archive---------8-----------------------#2020-07-29">https://towardsdatascience.com/batch-normalisation-in-deep-neural-network-ce65dd9e8dbf?source=collection_archive---------8-----------------------#2020-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f134" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">数据科学，深度学习</h2><div class=""/><div class=""><h2 id="2abf" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">批量规范化有助于深度神经网络的规范化</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/df5617b41434c91db0d5b9c59dabb6d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MQZPF02ak7kzvvo3"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">纳斯蒂亚·杜尔希尔在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="baa4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文中，您可以探索规范化与批量规范化，为什么深度学习需要批量规范化，批量规范化在深度神经网络中是如何执行的，以及批量规范化的优势是什么？</p><h1 id="1121" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">什么是规范化 vs 批量规范化？</strong></h1><p id="669e" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><strong class="lk jd">标准化</strong>是将数据集中数值变量的值更改为典型标度的过程，不会在值的范围内形成错误的对比。</p><p id="b7a5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">批量标准化</strong>是一种用于训练深度神经网络的技术，可对每个小批量的层贡献进行标准化。这有助于解决学习过程，并大幅减少训练深度神经网络所需的训练次数。</p><h1 id="d59d" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">深度学习为什么需要批量归一化？批处理规范化为什么有帮助？</strong></h1><p id="c54a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在<strong class="lk jd">深度学习</strong>中，准备具有许多层的<strong class="lk jd">深度神经网络</strong>,因为它们可能对底层初始随机权重和学习算法的设计很敏感。</p><p id="f88b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个问题背后的一个潜在目的是，当权重被刷新时，在每个小批量之后，网络中某处的层的输入分布可能改变。这可以使学习算法始终追求一个移动的目标。对网络中各层的输入分布的这种调整暗指了专门的名称<strong class="lk jd">内部协变量移位</strong>。</p><p id="99e9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">挑战在于，利用接受当前层之前的层中的权重是固定的误差估计，从输出到输入逐层反向刷新模型。</p><blockquote class="nb nc nd"><p id="a01d" class="li lj ne lk b ll lm kd ln lo lp kg lq nf ls lt lu ng lw lx ly nh ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">批量规格化</em> </strong> <em class="it">给出了一个丰富的参数化实际上任何深度神经网络的方法。重新参数化从根本上减少了跨多个层规划更新的问题。</em></p></blockquote><h1 id="ecf0" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">深度神经网络是如何进行批量归一化的？</strong></h1><p id="0ccf" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">它通过对每个小批量的每个输入变量的激活进行归一化，明确地缩放该层的输出，例如，来自最后一层的节点的制定。回顾一下，标准化意味着重新调整数据以使<strong class="lk jd">平均值</strong>为 0，而<strong class="lk jd">标准偏差</strong>为 1。</p><p id="57db" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过使每一层的输入变亮，它将朝着实现输入的<strong class="lk jd">固定分布迈出一大步，这将消除<strong class="lk jd">内部协变量移动</strong>的不利影响。</strong></p><p id="2e3a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">归一化早期层的<strong class="lk jd">激活意味着后续层在权重更新期间做出的关于输入的传播和分布的假设不会改变，无论如何不会显著改变。这具有稳定和加速深度神经网络的准备训练过程的影响。</strong></p><p id="8c4b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于不包含来自训练数据集的模型的代理分布的较小的<strong class="lk jd">小批量</strong>,训练和推理(利用训练后的模型)之间的标准化输入的差异可以在执行性能中带来可察觉的对比。这可以通过改变称为<strong class="lk jd">批量重正化</strong>的技术来实现，该技术使得小批量的变量平均值和标准偏差的评估越来越稳定。</p><p id="1f4a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种输入的标准化可以应用于第一个隐藏层的输入变量，或者应用于更深层的隐藏层的激活。</p><p id="1401" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="ne">倾向于与大多数深度网络类型一起使用，例如</em> <strong class="lk jd"> <em class="ne">卷积神经网络(CNN) </em> </strong> <em class="ne">和</em> <strong class="lk jd"> <em class="ne">递归神经网络(RNN) </em> </strong> <em class="ne">。</em></p><p id="d66f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它可以用在先前层的输入上，或者用在过去层的激活功能之后。</p><p id="b3eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="ne">如果对于像</em> <strong class="lk jd"> <em class="ne">双曲正切</em> </strong> <em class="ne">和</em> <strong class="lk jd"> <em class="ne">逻辑函数</em> </strong> <em class="ne">这样的 s 形容量，激活函数后可能会更加合适。</em></p><p id="ec56" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它可能在激活函数之前适合于可能在非高斯散射中上升<strong class="lk jd">的激活，如<strong class="lk jd">整流线性激活函数(ReLU) </strong>，这是大多数深度神经网络类型的前线默认值。</strong></p><p id="7e42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它提供了一些正则化影响，减少了<strong class="lk jd">泛化误差</strong>，可能需要利用<strong class="lk jd">丢失</strong>进行正则化。</p><p id="86c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在<strong class="lk jd">批量标准化网络</strong>中，平均值和方差在整个网络中保持适度稳定。对于一个<strong class="lk jd">非规范化的网络</strong>，他们似乎以指数<strong class="lk jd">发展</strong>有深度。</p><h1 id="8583" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">批量标准化的优势是什么？</strong></h1><ul class=""><li id="201b" class="ni nj it lk b ll mw lo mx lr nk lv nl lz nm md nn no np nq bi translated">该模型对<strong class="lk jd">超参数调整</strong>不太敏感。也就是说，尽管更大的<strong class="lk jd">学习率</strong>已经促进了无价值的模型，更大的 lr 在这一点上是令人满意的</li><li id="693d" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">收缩<strong class="lk jd">内部协变</strong>移位</li><li id="5d23" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">减少<strong class="lk jd">渐变</strong>对参数比例或其潜在值的依赖</li><li id="231c" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated"><strong class="lk jd">权重初始化</strong>在这一点上不太重要</li><li id="e625" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated"><strong class="lk jd">辍学者</strong>可被疏散进行调整</li></ul><h1 id="0e13" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">结论</strong></h1><p id="5226" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">这使我们达到了本文的最大限度，在这里我们发现了批处理规范化和利用它的优点。批处理规范化解决了一个叫做内部协变量移位的主要问题。</p><p id="f51c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它有助于使神经网络中间层之间的数据流动看起来，这意味着你可以使用更高的学习速率。它有一个正规化的效果，这意味着你可以经常消除辍学。</p></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><p id="7bcc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="ne">现在，把你对</em><strong class="lk jd"><em class="ne">Twitter</em></strong><em class="ne">，</em><strong class="lk jd"><em class="ne">Linkedin</em></strong><em class="ne">，以及</em><strong class="lk jd"><em class="ne">Github</em></strong><em class="ne">！！</em></p><p id="ec37" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="ne">同意</em> </strong> <em class="ne">还是</em> <strong class="lk jd"> <em class="ne">不同意</em> </strong> <em class="ne">与 Saurav Singla 的观点和例子？想告诉我们你的故事吗？</em></p><p id="8f00" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="ne">他乐于接受建设性的反馈——如果您对此分析有后续想法，请在下面的</em> <strong class="lk jd"> <em class="ne">评论</em> </strong> <em class="ne">或联系我们！！</em></p><p id="6672" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="ne">推文</em><a class="ae lh" href="https://twitter.com/SAURAVSINGLA_08" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd"><em class="ne">@ SauravSingla _ 08</em></strong></a><em class="ne">、评论</em><a class="ae lh" href="http://www.linkedin.com/in/saurav-singla-5b412320" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd"><em class="ne">Saurav _ Singla</em></strong></a><em class="ne">以及明星</em><a class="ae lh" href="https://github.com/sauravsingla" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd"><em class="ne">SauravSingla</em></strong></a><em class="ne">对</em></p></div></div>    
</body>
</html>