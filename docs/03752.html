<html>
<head>
<title>Ray and RLlib for Fast and Parallel Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于快速并行强化学习的 Ray 和 RLlib</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c?source=collection_archive---------11-----------------------#2020-04-08">https://towardsdatascience.com/ray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c?source=collection_archive---------11-----------------------#2020-04-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9b85" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Ray 进行 RL 训练的介绍教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d501b823eda02714793b7c125a17c336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EQdwvlvK7WdA_Xwz"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@the_gerbs1?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jean Gerber </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="32e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Ray 不仅仅是一个用于多处理的库；Ray 的真正能力来自 RLlib 和 Tune 库，它们利用这种能力进行强化学习。它使您能够将培训扩展到大规模分布式服务器，或者只是利用并行化属性，使用您自己的笔记本电脑更高效地进行培训。选择权在你。</p><h1 id="8be2" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="9789" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们展示了如何使用 Ray 和 RLlib 来训练一个定制的强化学习环境，这个环境是在 OpenAI Gym 的基础上构建的。</p><h1 id="de2c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">一个温和的 RLlib 教程</h1><p id="4733" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">一旦你用<code class="fe ms mt mu mv b">pip install ray[rllib]</code>安装了 Ray 和 RLlib，你就可以用命令行中的一个命令训练你的第一个 RL 代理了:</p><p id="6ff0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ms mt mu mv b">rllib train --run=A2C --env=CartPole-v0</code></p><p id="d066" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将告诉你的计算机使用<code class="fe ms mt mu mv b">CartPole</code>环境使用<a class="ae ky" href="https://www.datahubbs.com/policy-gradients-and-advantage-actor-critic/" rel="noopener ugc nofollow" target="_blank">优势演员评论家算法(A2C) </a>进行训练。A2C 和许多其他算法已经内置到库中，这意味着你不必担心自己实现这些算法的细节。</p><p id="439c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这真的很棒，特别是如果你想用标准的环境和算法来训练的话。然而，如果你想做得更多，你必须挖掘得更深一点。</p><h1 id="0f23" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">RLlib 代理</h1><p id="4176" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">您可以通过<code class="fe ms mt mu mv b">ray.rllib.agents</code>访问各种算法。在这里，您可以找到 PyTorch 和 Tensorflow 中不同实现的<a class="ae ky" href="https://github.com/ray-project/ray/tree/master/rllib/agents" rel="noopener ugc nofollow" target="_blank">长列表</a>,并开始使用。</p><p id="6d99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些都是使用算法的训练器方法访问的。例如，如果您想要使用如上所示的 A2C，您可以运行:</p><pre class="kj kk kl km gt mw mv mx my aw mz bi"><span id="259b" class="na lw it mv b gy nb nc l nd ne">import ray<br/>from ray.rllib import agents</span><span id="4a01" class="na lw it mv b gy nf nc l nd ne">ray.init()<br/>trainer = agents.a3c.A2CTrainer(env='CartPole-v0')</span></pre><p id="5fd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想尝试 DQN，您可以拨打:</p><pre class="kj kk kl km gt mw mv mx my aw mz bi"><span id="c2ad" class="na lw it mv b gy nb nc l nd ne">trainer = agents.dqn.DQNTrainer(env='CartPole-v0') # Deep Q Network</span></pre><p id="eb3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有算法都遵循相同的基本结构，从小写 algo 缩写到大写 algo 缩写，后跟“Trainer”</p><p id="b630" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更改超参数就像将配置字典传递给<code class="fe ms mt mu mv b">config</code>参数一样简单。查看可用内容的一个快速方法是调用<code class="fe ms mt mu mv b">trainer.config</code>来打印出适用于您选择的算法的选项。一些例子包括:</p><ul class=""><li id="1d02" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><code class="fe ms mt mu mv b">fcnet_hiddens</code>控制隐藏单元和隐藏层的数量(作为一个名为<code class="fe ms mt mu mv b">model</code>的字典传递给<code class="fe ms mt mu mv b">config</code>，然后是一个列表，下面我会给出一个例子)。</li><li id="ab4a" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><code class="fe ms mt mu mv b">vf_share_layers</code>确定您是否拥有<a class="ae ky" href="https://www.datahubbs.com/two-headed-a2c-network-in-pytorch/" rel="noopener ugc nofollow" target="_blank">一个具有多个输出头的神经网络</a>或独立的价值和策略网络。</li><li id="964b" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><code class="fe ms mt mu mv b">num_workers</code>设置并行化的处理器数量。</li><li id="e17c" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><code class="fe ms mt mu mv b">num_gpus</code>设置您将使用的 GPU 数量。</li></ul><p id="bbce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从网络(通常位于<code class="fe ms mt mu mv b">model</code>字典中)到各种回调和多代理设置，还有许多其他的需要设置和定制。</p><h1 id="d9bd" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">示例:为<code class="fe ms mt mu mv b">CartPole</code>培训 PPO</h1><p id="2064" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我想转而展示一个快速的例子来让你开始，并向你展示这是如何在一个标准的开放的健身房环境中工作的。</p><p id="70be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选择您的 IDE 或文本编辑器，并尝试以下操作:</p><pre class="kj kk kl km gt mw mv mx my aw mz bi"><span id="3f0a" class="na lw it mv b gy nb nc l nd ne">import ray<br/>from ray.rllib import agents<br/>ray.init() # Skip or set to ignore if already called<br/>config = {'gamma': 0.9,<br/>          'lr': 1e-2,<br/>          'num_workers': 4,<br/>          'train_batch_size': 1000,<br/>          'model': {<br/>              'fcnet_hiddens': [128, 128]<br/>          }}<br/>trainer = agents.ppo.PPOTrainer(env='CartPole-v0', config=config)<br/>results = trainer.train()</span></pre><p id="f5b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ms mt mu mv b">config</code>字典更改了上述值的默认值。您可以看到我们如何通过在<code class="fe ms mt mu mv b">config</code>字典中嵌套一个名为<code class="fe ms mt mu mv b">model</code>的字典来影响网络的层数和节点数。一旦我们指定了我们的配置，在我们的<code class="fe ms mt mu mv b">trainer</code>对象上调用<code class="fe ms mt mu mv b">train()</code>方法将会把环境发送给工人并开始收集数据。一旦收集到足够的数据(根据我们上面的设置，有 1000 个样本)，模型将更新并将输出发送到一个名为<code class="fe ms mt mu mv b">results</code>的新字典。</p><p id="8541" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想要运行多个更新，那么您可以设置一个训练循环来连续调用<code class="fe ms mt mu mv b">train()</code>方法，以达到给定的迭代次数，或者直到达到某个其他阈值。</p><h1 id="c2d7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">定制您的 RL 环境</h1><p id="7ee8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">OpenAI Gym 和它的所有扩展都很棒，但是如果你正在寻找 RL 的新颖应用或者在你的公司使用它，你将需要一个定制的环境。</p><p id="417d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，当前版本的 Ray (0.9) <a class="ae ky" href="https://ray.readthedocs.io/en/latest/rllib-env.html" rel="noopener ugc nofollow" target="_blank">明确声明</a>与健身房注册表不兼容。幸运的是，整合一个助手函数来让定制的健身房环境与 Ray 一起工作并不太困难。</p><p id="0390" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们假设您有一个名为<code class="fe ms mt mu mv b">MyEnv-v0</code>的环境，它已经被正确注册，这样您就可以像调用任何其他健身房环境一样使用<code class="fe ms mt mu mv b">gym.make('MyEnv-v0')</code>来调用它(如果您还没有，您可以在这里查看我关于设置环境的<a class="ae ky" href="https://www.datahubbs.com/building-custom-gym-environments-for-rl/" rel="noopener ugc nofollow" target="_blank">逐步过程</a>)。</p><p id="5961" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要从 Ray 调用定制环境，您需要将它封装在一个函数中，该函数将返回环境类，<em class="nu">而不是</em>一个实例化的对象。<a class="ae ky" href="https://stackoverflow.com/questions/58551029/rllib-use-custom-registered-environments/60792871#60792871" rel="noopener ugc nofollow" target="_blank">我发现做这件事的最好方法</a>是使用一个<code class="fe ms mt mu mv b">create_env()</code>助手函数:</p><pre class="kj kk kl km gt mw mv mx my aw mz bi"><span id="06c2" class="na lw it mv b gy nb nc l nd ne">def env_creator(env_name):<br/>    if env_name == 'MyEnv-v0':<br/>        from custom_gym.envs.custom_env import CustomEnv0 as env<br/>    elif env_name == 'MyEnv-v1':<br/>        from custom_gym.envs.custom_env import CustomEnv1 as env<br/>    else:<br/>        raise NotImplementedError<br/>    return env</span></pre><p id="eb2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这里，您可以设置您的代理，并在这个新环境中对它进行训练，只需对<code class="fe ms mt mu mv b">trainer</code>稍加修改。</p><pre class="kj kk kl km gt mw mv mx my aw mz bi"><span id="052a" class="na lw it mv b gy nb nc l nd ne">env_name = 'MyEnv-v0'<br/>config = {<br/>    # Whatever config settings you'd like...<br/>    }<br/>trainer = agents.ppo.PPOTrainer(<br/>    env=env_creator(env_name), <br/>    config=config)<br/>max_training_episodes = 10000<br/>while True:<br/>    results = trainer.train()<br/>    # Enter whatever stopping criterion you like<br/>    if results['episodes_total'] &gt;= max_training_episodes:<br/>        break<br/>print('Mean Rewards:\t{:.1f}'.format(results['episode_reward_mean']))</span></pre><p id="fc89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，在上面，我们用<code class="fe ms mt mu mv b">env_creator</code>来称呼环境，其他一切保持不变。</p><h1 id="c029" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">使用自定义环境的提示</h1><p id="175f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如果您习惯于从环境到网络和算法构建自己的模型，那么在使用 Ray 时，您需要了解一些特性。</p><p id="d7ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，Ray 遵循<a class="ae ky" href="http://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank"> OpenAI Gym API </a>，这意味着您的环境需要有<code class="fe ms mt mu mv b">step()</code>和<code class="fe ms mt mu mv b">reset()</code>方法，以及精心指定的<code class="fe ms mt mu mv b">observation_space</code>和<code class="fe ms mt mu mv b">action_space</code>属性。对于最后两个，我总是有点懒惰，因为我可以简单地定义我的网络输入和输出维度，而不必考虑输入值的范围，例如，<code class="fe ms mt mu mv b">gym.spaces</code>方法所要求的。Ray 检查所有的输入以确保它们都在指定的范围内(我花了太多时间调试运行，才意识到我的<code class="fe ms mt mu mv b">gym.spaces.Box</code>上的<code class="fe ms mt mu mv b">low</code>值被设置为 0，但是环境返回了-1e-17 数量级的值并导致它崩溃)。</p><p id="d89e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当建立你的行动和观察空间时，坚持<code class="fe ms mt mu mv b">Box</code>、<code class="fe ms mt mu mv b">Discrete</code>和<code class="fe ms mt mu mv b">Tuple</code>。<code class="fe ms mt mu mv b">MultiDiscrete</code>和<code class="fe ms mt mu mv b">MultiBinary</code>不工作(<a class="ae ky" href="https://github.com/ray-project/ray/issues/6372" rel="noopener ugc nofollow" target="_blank">目前为</a>)，将导致运行崩溃。相反，在<code class="fe ms mt mu mv b">Tuple</code>函数中换行<code class="fe ms mt mu mv b">Box</code>或<code class="fe ms mt mu mv b">Discrete</code>空格。</p><p id="8489" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽可能利用定制预处理。Ray 对您的状态输入进行假设，这通常很好，但它也使您能够定制预处理步骤，这可能有助于您的训练。</p><h1 id="2e0a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">超越 RLlib</h1><p id="a1cc" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Ray 可以大大加快训练速度，并使深度强化学习的开始变得容易得多。RLlib 并不是最终的结果(我们在这里只是触及了其功能的表面)，它有一个强大的表亲，称为 Tune，它使您能够调整模型的超参数，并为您管理所有重要的数据收集和后端工作。请务必回来查看如何将此库引入您的工作流程的更新。</p></div></div>    
</body>
</html>