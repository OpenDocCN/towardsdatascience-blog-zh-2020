# åŸºäº Keras å’Œ W&B çš„å›¾åƒåˆ†å‰²

> åŸæ–‡ï¼š<https://towardsdatascience.com/image-segmentation-using-keras-and-w-b-98223c38e4d4?source=collection_archive---------28----------------------->

## **è¯¥æŠ¥å‘Šåˆ©ç”¨ Keras ä¸­ç±»ä¼¼ UNET çš„æ¶æ„æ¢ç´¢è¯­ä¹‰åˆ†å‰²ï¼Œå¹¶äº¤äº’å¼å¯è§†åŒ–æ¨¡å‹å¯¹æƒé‡åå·®çš„é¢„æµ‹&ã€‚**

## ç‚¹å‡»æŸ¥çœ‹äº’åŠ¨æŠ¥é“[ã€‚è¿™é‡Œæœ‰](https://wandb.ai/ayush-thakur/image-segmentation/reports/Image-Segmentation-Using-Keras-and-W-B--VmlldzoyNTE1Njc)[çš„ Colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1rXV31gdyqEiXCtmSgff-H-VRuOSzv7IH?usp=sharing)ã€‚

# ä»‹ç»

æ‚¨æ˜¯å¦æœ‰å…´è¶£äº†è§£å›¾åƒä¸­æŸä¸ªå¯¹è±¡çš„ä½ç½®ï¼Ÿè¿™ä¸ªç‰©ä½“çš„å½¢çŠ¶æ˜¯ä»€ä¹ˆï¼Ÿå“ªäº›åƒç´ å±äºå¯¹è±¡ï¼Ÿä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦åˆ†å‰²å›¾åƒï¼Œå³å°†å›¾åƒçš„æ¯ä¸ªåƒç´ åˆ†ç±»åˆ°å®ƒæ‰€å±çš„å¯¹è±¡ï¼Œæˆ–è€…ç»™å›¾åƒçš„æ¯ä¸ªåƒç´ ä¸€ä¸ªæ ‡ç­¾ï¼Œè€Œä¸æ˜¯ç»™å›¾åƒä¸€ä¸ªæ ‡ç­¾ã€‚

**å› æ­¤ï¼Œå›¾åƒåˆ†å‰²æ˜¯ä¸ºå›¾åƒä¸­çš„æ¯ä¸ªå¯¹è±¡å­¦ä¹ é€åƒç´ æ©æ¨¡çš„ä»»åŠ¡ã€‚**ä¸ä¸ºå›¾åƒä¸­å‡ºç°çš„æ¯ä¸ªå¯¹è±¡ç»™å‡ºè¾¹ç•Œæ¡†åæ ‡çš„å¯¹è±¡æ£€æµ‹ä¸åŒï¼Œå›¾åƒåˆ†å‰²å¯¹å›¾åƒä¸­çš„å¯¹è±¡ç»™å‡ºäº†æ›´ç²¾ç»†çš„ç†è§£ã€‚

![](img/fdd859833edccfebe5adb18f3edb7077.png)

**å›¾ 1** :è¯­ä¹‰åˆ‡åˆ†å’Œå®ä¾‹åˆ‡åˆ†ã€‚([æ¥æº](https://www.researchgate.net/figure/Semantic-segmentation-left-and-Instance-segmentation-right-8_fig1_339616270))

å›¾åƒåˆ†å‰²å¯ä»¥å¤§è‡´åˆ†ä¸ºä¸¤ç§ç±»å‹:

*   **è¯­ä¹‰åˆ†å‰²:**è¿™é‡Œï¼Œæ¯ä¸ªåƒç´ å±äºä¸€ä¸ªç‰¹å®šçš„ç±»ã€‚å›¾ 1 ä¸­çš„å·¦å›¾æ˜¯è¯­ä¹‰åˆ†å‰²çš„ä¸€ä¸ªä¾‹å­ã€‚åƒç´ æˆ–è€…å±äºäºº(ä¸€ä¸ªç±»åˆ«)ï¼Œæˆ–è€…å±äºèƒŒæ™¯(å¦ä¸€ä¸ªç±»åˆ«)ã€‚
*   **å®ä¾‹åˆ†å‰²:**è¿™é‡Œï¼Œæ¯ä¸ªåƒç´ å±äºä¸€ä¸ªç‰¹å®šçš„ç±»ã€‚ä½†æ˜¯ï¼Œå±äºç¦»æ•£å¯¹è±¡çš„åƒç´ ç”¨ä¸åŒçš„é¢œè‰²(è’™ç‰ˆå€¼)æ ‡è®°ã€‚å›¾ 1 ä¸­çš„å³å›¾æ˜¯ä¸€ä¸ªå®ä¾‹åˆ†å‰²çš„ä¾‹å­ã€‚å±äºè¯¥äººç‰©ç±»åˆ«çš„åƒç´ è¢«ä¸åŒåœ°ç€è‰²ã€‚

è¯¥æŠ¥å‘Šå°†**å»ºç«‹ä¸€ä¸ªè¯­ä¹‰åˆ†å‰²æ¨¡å‹**ï¼Œå¹¶åœ¨[ç‰›æ´¥-IIIT Pet æ•°æ®é›†](https://www.robots.ox.ac.uk/%7Evgg/data/pets/)ä¸Šå¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å°†**äº¤äº’å¯è§†åŒ–æˆ‘ä»¬æ¨¡å‹çš„é¢„æµ‹**æƒé‡&åå·®ã€‚

# æ•°æ®é›†

æˆ‘ä»¬å°†ä½¿ç”¨[ç‰›æ´¥-IIIT Pet æ•°æ®é›†](https://www.robots.ox.ac.uk/%7Evgg/data/pets/)æ¥è®­ç»ƒæˆ‘ä»¬çš„ç±» UNET è¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚

æ•°æ®é›†ç”±å›¾åƒåŠå…¶åƒç´ å¼æ©è†œç»„æˆã€‚é€åƒç´ é®ç½©æ˜¯æ¯ä¸ªåƒç´ çš„æ ‡ç­¾ã€‚

*   ç¬¬ 1 ç±»:å±äºå® ç‰©çš„åƒç´ ã€‚
*   ç¬¬ 2 ç±»:å±äºå® ç‰©è½®å»“çš„åƒç´ ã€‚
*   ç¬¬ä¸‰ç±»:å±äºèƒŒæ™¯çš„åƒç´ ã€‚

![](img/fa2fa2cd655c9a0c05b2d01b850ec885.png)

**å›¾ 2** :å® ç‰©å’Œå®ƒä»¬çš„åƒç´ å¼é®ç½©ã€‚

## ä¸‹è½½æ•°æ®é›†

```
!curl -O [http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz](http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz)!curl -O [http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz](http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz)!tar -xf images.tar.gz
!tar -xf annotations.tar.gz
```

## æ•°æ®é›†å‡†å¤‡

`images/`å’Œ`annotations/trimaps`ç›®å½•åŒ…å«æå–çš„å›¾åƒåŠå…¶æ³¨é‡Š(æŒ‰åƒç´ çš„é®ç½©)ã€‚æ‰€éœ€å›¾åƒä¸º`.jpg`æ ¼å¼ï¼Œè€Œæ³¨é‡Šä¸º`.png`æ ¼å¼ã€‚ä½†æ˜¯ï¼Œåœ¨è¿™äº›ç›®å½•ä¸­æœ‰ä¸€äº›æˆ‘ä»¬ä¸éœ€è¦çš„æ–‡ä»¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å‡†å¤‡ä¸¤ä¸ªåˆ—è¡¨- `input_img_paths`å’Œ`annotation_img_paths`ï¼Œå…¶ä¸­åŒ…å«æ‰€éœ€å›¾åƒå’Œæ³¨é‡Šçš„è·¯å¾„ã€‚

```
IMG_PATH = 'images/'
ANNOTATION_PATH = 'annotations/trimaps/'

input_img_paths = sorted(
    [
        os.path.join(IMG_PATH, fname)
        for fname in os.listdir(IMG_PATH)
        if fname.endswith(".jpg")
    ]
)
annotation_img_paths = sorted(
    [
        os.path.join(ANNOTATION_PATH, fname)
        for fname in os.listdir(ANNOTATION_PATH)
        if fname.endswith(".png") and not fname.startswith(".")
    ]
)

print(len(input_img_paths), len(annotation_img_paths))
```

æ€»å…±æœ‰ 7390 å¼ å›¾ç‰‡å’Œæ³¨é‡Šã€‚æˆ‘ä»¬å°†ä½¿ç”¨ 1000 å¹…å›¾åƒåŠå…¶æ³¨é‡Šä½œä¸ºéªŒè¯é›†ã€‚

## ä½¿ç”¨`tf.data`çš„æ•°æ®åŠ è½½å™¨

æˆ‘ä»¬å°†ä½¿ç”¨`tf.data.Dataset`æ„å»ºæˆ‘ä»¬çš„è¾“å…¥ç®¡é“ã€‚

```
IMG_SHAPE = 128
AUTO = tf.data.experimental.AUTOTUNE
BATCH_SIZE = 32

def scale_down(image, mask):
  # apply scaling to image and mask
  image = tf.cast(image, tf.float32) / 255.0
  mask -= 1
  return image, mask

def load_and_preprocess(img_filepath, mask_filepath):
   # load the image and resize it
    img = tf.io.read_file(img_filepath)
    img = tf.io.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, [IMG_SHAPE, IMG_SHAPE])

    mask = tf.io.read_file(mask_filepath)
    mask = tf.io.decode_png(mask, channels=1)
    mask = tf.image.resize(mask, [IMG_SHAPE, IMG_SHAPE])

    img, mask = scale_down(img, mask)

    return img, mask

# shuffle the paths and prepare train-test split
input_img_paths, annotation_img_paths = shuffle(input_img_paths, annotation_img_paths, random_state=42)
input_img_paths_train, annotation_img_paths_train = input_img_paths[: -1000], annotation_img_paths[: -1000]
input_img_paths_test, annotation_img_paths_test = input_img_paths[-1000:], annotation_img_paths[-1000:]

trainloader = tf.data.Dataset.from_tensor_slices((input_img_paths_train, annotation_img_paths_train))
testloader = tf.data.Dataset.from_tensor_slices((input_img_paths_test, annotation_img_paths_test))

trainloader = (
    trainloader
    .shuffle(1024)
    .map(load_and_preprocess, num_parallel_calls=AUTO)
    .batch(BATCH_SIZE)
    .prefetch(AUTO)
)

testloader = (
    testloader
    .map(load_and_preprocess, num_parallel_calls=AUTO)
    .batch(BATCH_SIZE)
    .prefetch(AUTO)
)
```

# æ¨¡å‹

è¿™é‡Œä½¿ç”¨çš„æ¨¡å‹æ˜¯é¦™è‰ [UNET å»ºç­‘](https://arxiv.org/abs/1505.04597)ã€‚å®ƒç”±ç¼–ç å™¨å’Œè§£ç å™¨ç½‘ç»œç»„æˆã€‚è¿™ä¸ªæ¶æ„çš„è¾“å…¥æ˜¯å›¾åƒï¼Œè€Œè¾“å‡ºæ˜¯é€åƒç´ çš„è´´å›¾ã€‚æ‚¨å¯ä»¥é€šè¿‡ W & B æŠ¥å‘Šåœ¨[æ·±åº¦ç”Ÿæˆå»ºæ¨¡ä¸­äº†è§£æ›´å¤šå…³äºç¼–ç å™¨-è§£ç å™¨(Autoencoder)ç½‘ç»œçš„ä¿¡æ¯ã€‚](https://wandb.ai/ayush-thakur/keras-gan/reports/Towards-Deep-Generative-Modeling-with-W-B--Vmlldzo4MDI4Mw)

ç±»ä¼¼ UNET çš„æ¶æ„åœ¨è‡ªæˆ‘ç›‘ç£çš„æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­å¾ˆå¸¸è§ï¼Œå¦‚[å›¾åƒä¿®å¤](https://www.wandb.com/articles/introduction-to-image-inpainting-with-deep-learning)ã€‚

ä½ å¯ä»¥åœ¨è¿™ä¸ª[é€è¡Œè§£é‡Š](/unet-line-by-line-explanation-9b191c76baf5)ä¸­äº†è§£æ›´å¤šå…³äº UNET å»ºç­‘çš„ä¿¡æ¯ã€‚

![](img/4c18c4c37b8f56fb8c304351503518e1.png)

å›¾ 3 :å…¸å‹çš„ UNET å»ºç­‘ã€‚([æ¥æº](/unet-line-by-line-explanation-9b191c76baf5))

ä¸‹é¢æ˜¾ç¤ºçš„ä»£ç ç‰‡æ®µæ„å»ºäº†æˆ‘ä»¬çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹æ¶æ„ã€‚

```
class SegmentationModel:
  '''
  Build UNET like model for image inpaining task.
  '''
  def prepare_model(self, OUTPUT_CHANNEL, input_size=(IMG_SHAPE,IMG_SHAPE,3)):
    inputs = Input(input_size)

    # Encoder 
    conv1, pool1 = self.__ConvBlock(32, (3,3), (2,2), 'relu', 'same', inputs) 
    conv2, pool2 = self.__ConvBlock(64, (3,3), (2,2), 'relu', 'same', pool1)
    conv3, pool3 = self.__ConvBlock(128, (3,3), (2,2), 'relu', 'same', pool2) 
    conv4, pool4 = self.__ConvBlock(256, (3,3), (2,2), 'relu', 'same', pool3) 

    # Decoder
    conv5, up6 = self.__UpConvBlock(512, 256, (3,3), (2,2), (2,2), 'relu', 'same', pool4, conv4)
    conv6, up7 = self.__UpConvBlock(256, 128, (3,3), (2,2), (2,2), 'relu', 'same', up6, conv3)
    conv7, up8 = self.__UpConvBlock(128, 64, (3,3), (2,2), (2,2), 'relu', 'same', up7, conv2)
    conv8, up9 = self.__UpConvBlock(64, 32, (3,3), (2,2), (2,2), 'relu', 'same', up8, conv1)

    conv9 = self.__ConvBlock(32, (3,3), (2,2), 'relu', 'same', up9, False)

    # Notice OUTPUT_CHANNEL and activation
    outputs = Conv2D(OUTPUT_CHANNEL, (3, 3), activation='softmax', padding='same')(conv9)

    return Model(inputs=[inputs], outputs=[outputs])  

  def __ConvBlock(self, filters, kernel_size, pool_size, activation, padding, connecting_layer, pool_layer=True):
    conv = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(connecting_layer)
    conv = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(conv)
    if pool_layer:
      pool = MaxPooling2D(pool_size)(conv)
      return conv, pool
    else:
      return conv

  def __UpConvBlock(self, filters, up_filters, kernel_size, up_kernel, up_stride, activation, padding, connecting_layer, shared_layer):
    conv = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(connecting_layer)
    conv = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(conv)
    up = Conv2DTranspose(filters=up_filters, kernel_size=up_kernel, strides=up_stride, padding=padding)(conv)
    up = concatenate([up, shared_layer], axis=3)

    return conv, up
```

**æ³¨æ„**å¯¹äºæˆ‘ä»¬çš„æ•°æ®é›†æ¥è¯´`OUTPUT_CHANNEL`æ˜¯ 3ã€‚è¿™æ˜¯å› ä¸ºæœ‰ä¸‰ç±»åƒç´ ï¼Œå¦‚æ•°æ®é›†éƒ¨åˆ†æ‰€è¿°ã€‚è€ƒè™‘æˆ‘ä»¬æ­£åœ¨è¿›è¡Œå¤šç±»åˆ†ç±»ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ å¯ä»¥å±äºä¸‰ç±»ä¸­çš„ä»»ä½•ä¸€ç±»ã€‚

è¿˜æœ‰ï¼Œ**æ³¨æ„**ç”±äºæ˜¯æ¯åƒç´ å¤šç±»åˆ†ç±»é—®é¢˜ï¼Œæ‰€ä»¥è¾“å‡ºæ¿€æ´»å‡½æ•°æ˜¯`softmax`ã€‚

```
OUTPUT_CHANNEL = 3

model = SegmentationModel().prepare_model(OUTPUT_CHANNEL)
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
```

æœ€åç”¨`sparse_categorical_crossentropy`ç¼–è¯‘æ¨¡å‹ã€‚ç¨€ç–ï¼Œå› ä¸ºæŒ‰åƒç´ çš„é®ç½©/æ³¨é‡Šæ˜¯æ•´æ•°ã€‚

# `SemanticLogger`å›è°ƒ-é¢„æµ‹çš„äº¤äº’å¼å¯è§†åŒ–

åœ¨è¿›è¡Œè¯­ä¹‰åˆ†å‰²æ—¶ï¼Œæ‚¨å¯ä»¥åœ¨æƒé‡å’Œåå·®ä¸­äº¤äº’å¼åœ°å¯è§†åŒ–æ¨¡å‹çš„é¢„æµ‹ã€‚å¦‚æœæ‚¨çš„å›¾åƒå¸¦æœ‰ç”¨äºè¯­ä¹‰åˆ†æ®µçš„é®ç½©ï¼Œæ‚¨å¯ä»¥è®°å½•é®ç½©å¹¶åœ¨ UI ä¸­æ‰“å¼€å’Œå…³é—­å®ƒä»¬ã€‚ç‚¹å‡»æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£[ã€‚](https://docs.wandb.com/library/log#images-and-overlays)

Stacey Svetlichnaya[çš„æŠ¥å‘Š](https://wandb.ai/stacey)[è¯­ä¹‰åˆ†å‰²çš„å›¾åƒé®ç½©](https://wandb.ai/stacey/deep-drive/reports/Image-Masks-for-Semantic-Segmentation--Vmlldzo4MTUwMw)å°†å¸¦æ‚¨äº†è§£è¯¥å·¥å…·çš„äº¤äº’æ§ä»¶ã€‚å®ƒæ¶µç›–äº†æ—¥å¿—å›¾åƒå’Œé®ç½©çš„å„ç§éº»çƒ¦ã€‚

ä¸‹é¢æ˜¾ç¤ºçš„ä»£ç ç‰‡æ®µæ˜¯æˆ‘ä»¬çš„`SemanticLogger`å›è°ƒçš„åŠ©æ‰‹å‡½æ•°ã€‚å‡½æ•°`labels`è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­`key`æ˜¯ç±»å€¼ï¼Œ`value`æ˜¯æ ‡ç­¾ã€‚å‡½æ•°`wandb_mask`ä»¥æ‰€éœ€çš„æ ¼å¼è¿”å›å›¾åƒã€é¢„æµ‹æ©ç å’ŒåŸºæœ¬äº‹å®æ©ç ã€‚

```
segmentation_classes = ['pet', 'pet_outline', 'background']

# returns a dictionary of labels
def labels():
  l = {}
  for i, label in enumerate(segmentation_classes):
    l[i] = label
  return l

# util function for generating interactive image mask from components
def wandb_mask(bg_img, pred_mask, true_mask):
  return wandb.Image(bg_img, masks={
      "prediction" : {
          "mask_data" : pred_mask, 
          "class_labels" : labels()
      },
      "ground truth" : {
          "mask_data" : true_mask, 
          "class_labels" : labels()
      }
    }
  )
```

æˆ‘ä»¬çš„`SemanticLogger`æ˜¯ä¸€ä¸ªå®šåˆ¶çš„ Keras å›è°ƒå‡½æ•°ã€‚æˆ‘ä»¬å¯ä»¥å°†å®ƒä¼ é€’ç»™`model.fit`æ¥è®°å½•æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸€ä¸ªå°å‹éªŒè¯é›†ä¸Šçš„é¢„æµ‹ã€‚æƒé‡å’Œåå·®å°†è‡ªåŠ¨è¦†ç›–å›¾åƒä¸Šçš„è’™ç‰ˆã€‚

```
class SemanticLogger(tf.keras.callbacks.Callback):
    def __init__(self):
        super(SemanticLogger, self).__init__()
        self.val_images, self.val_masks = next(iter(testloader))

    def on_epoch_end(self, logs, epoch):
        pred_masks = self.model.predict(self.val_images)
        pred_masks = np.argmax(pred_masks, axis=-1)
        # pred_masks = np.expand_dims(pred_masks, axis=-1)

        val_images = tf.image.convert_image_dtype(self.val_images, tf.uint8)
        val_masks = tf.image.convert_image_dtype(self.val_masks, tf.uint8)
        val_masks = tf.squeeze(val_masks, axis=-1)

        pred_masks = tf.image.convert_image_dtype(pred_masks, tf.uint8)

        mask_list = []
        for i in range(len(self.val_images)):
          mask_list.append(wandb_mask(val_images[i].numpy(), 
                                      pred_masks[i].numpy(), 
                                      val_masks[i].numpy()))

        wandb.log({"predictions" : mask_list})
```

æˆ‘ä»¬å¾ˆå¿«å°±ä¼šçœ‹åˆ°ç»“æœã€‚

# ç»“æœ

ç°åœ¨åˆ°äº†æ¿€åŠ¨äººå¿ƒçš„éƒ¨åˆ†ã€‚æˆ‘å·²ç»è®­ç»ƒäº† 15 ä¸ªçºªå…ƒçš„æ¨¡å‹ã€‚æŸå¤±å’ŒéªŒè¯æŸå¤±æŒ‡æ ‡å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚*éšæ„è®­ç»ƒæ›´é•¿æ—¶æœŸçš„æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å…¶ä»–è¶…å‚æ•°ã€‚*

[](https://colab.research.google.com/drive/1rXV31gdyqEiXCtmSgff-H-VRuOSzv7IH?usp=sharing) [## è°·æ­Œè”åˆå®éªŒå®¤

### ç¼–è¾‘æè¿°

colab.research.google.com](https://colab.research.google.com/drive/1rXV31gdyqEiXCtmSgff-H-VRuOSzv7IH?usp=sharing) 

åŸ¹è®­å’ŒéªŒè¯æŸå¤±å¦‚å›¾**å›¾ 3** æ‰€ç¤ºã€‚ç»è¿‡ä¸€äº›æ—¶æœŸåï¼Œæ¨¡å‹å¼€å§‹è¿‡åº¦æ‹Ÿåˆã€‚

![](img/2f5e976ab5893530f80033e0c59cd473.png)

**å›¾ 4** :åŸ¹è®­å’ŒéªŒè¯æŸå¤±æŒ‡æ ‡ã€‚([ç‚¹å‡»è¿™é‡ŒæŸ¥çœ‹äº’åŠ¨æŠ¥é“ã€‚](https://wandb.ai/ayush-thakur/image-segmentation/reports/Image-Segmentation-Using-Keras-and-W-B--VmlldzoyNTE1Njc))

`SemanticLogger`çš„ç»“æœå¦‚ä¸‹æ‰€ç¤ºã€‚**ç‚¹å‡»ä¸‹é¢** [**åª’ä½“é¢æ¿**](https://wandb.ai/ayush-thakur/image-segmentation/reports/Image-Segmentation-Using-Keras-and-W-B--VmlldzoyNTE1Njc#Results-7) **ä¸­çš„âš™ï¸å›¾æ ‡(SemanticLogger çš„ç»“æœ)æŸ¥çœ‹äº¤äº’æ§ä»¶**ã€‚æ‚¨å¯ä»¥åˆ†åˆ«å¯è§†åŒ–å›¾åƒå’Œé®ç½©ï¼Œå¹¶å¯ä»¥é€‰æ‹©è¦å¯è§†åŒ–çš„è¯­ä¹‰ç±»ã€‚

# è§‚å¯Ÿ

*   è¯¥æ¨¡å‹å­¦ä¹ å¾ˆå¥½åœ°é¢„æµ‹`pet`å’Œ`background`ç±»ã€‚
*   æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¨¡å‹å¾ˆéš¾ç»†åˆ†`pet_outline`ç±»ã€‚è¿™æ˜¯å› ä¸ºé«˜ç­‰çº§çš„ä¸å¹³è¡¡ï¼Œå¹¶ä¸”æ¨¡å‹æ²¡æœ‰è¢«æ­£åˆ™åŒ–ä»¥å¯¹æŠ—è¿™ç§ä¸å¹³è¡¡ã€‚

![](img/6794a02795a377202382e239c603c97e.png)

**å›¾ 5** :è¯­ä¹‰è®°å½•å™¨å›è°ƒç»“æœã€‚([ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹äº’åŠ¨æŠ¥é“ã€‚](https://wandb.ai/ayush-thakur/image-segmentation/reports/Image-Segmentation-Using-Keras-and-W-B--VmlldzoyNTE1Njc))

# ç»“è®ºå’Œæœ€ç»ˆæƒ³æ³•

æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¿™ç¯‡å…³äºè¯­ä¹‰åˆ†å‰²çš„æŠ¥å‘Šã€‚è¿™ä»½æŠ¥å‘Šæœ‰ä¸¤ä¸ªç›®çš„:

*   è®©æ„Ÿå…´è¶£çš„äººæ›´å®¹æ˜“ä½¿ç”¨è¯­ä¹‰åˆ†å‰²æŠ€æœ¯ã€‚
*   å±•ç¤ºæƒé‡å’Œåå·®å¦‚ä½•å¸®åŠ©äº¤äº’å¼åœ°å¯è§†åŒ–æ¨¡å‹çš„é¢„æµ‹å’Œåº¦é‡ã€‚æ­¤å¤–ï¼Œå±•ç¤ºäººä»¬å¯ä»¥ä»è¿™äº›å¯è§†åŒ–ä¸­å¾—åˆ°çš„è§‚å¯Ÿç»“æœã€‚

æœ€åï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å€¼å¾—ä¸€è¯»çš„èµ„æº:

*   [è¯­ä¹‰å›¾åƒåˆ†å‰²æ¦‚è¿°](https://www.jeremyjordan.me/semantic-segmentation/)
*   [å›¾åƒåˆ†å‰²](https://www.tensorflow.org/tutorials/images/segmentation)
*   [é©¾é©¶åº§ä¸Šçš„è§†é‡](https://wandb.ai/stacey/deep-drive/reports/The-View-from-the-Driver-s-Seat--Vmlldzo1MTg5NQ)

æˆ‘å¾ˆæƒ³åœ¨è¯„è®ºåŒºå¾—åˆ°ä½ çš„åé¦ˆã€‚ğŸ˜„