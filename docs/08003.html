<html>
<head>
<title>Maximum Likelihood Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最大似然估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818?source=collection_archive---------28-----------------------#2020-06-13">https://towardsdatascience.com/ml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818?source=collection_archive---------28-----------------------#2020-06-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8987" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">高斯模型和线性判别分析</h2></div><h1 id="49ea" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">背景</h1><p id="4f68" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">最大似然估计(ML Estimation，MLE)是统计领域常用的一种强有力的参数估计方法。最大似然法的思想是估计模型的参数，其中给定的数据是可能获得的。在这一节中，我将从模式识别方法中介绍 MLE 的重要性。我们为什么需要 MLE？它在模式识别过程中是如何工作的？</p><h2 id="1818" class="lt kg iq bd kh lu lv dn kl lw lx dp kp lg ly lz kr lk ma mb kt lo mc md kv me bi translated">统计模式识别</h2><p id="edee" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">模式识别是机器学习的一个分支。它是识别给定<em class="mf">模式</em>对应的<em class="mf">类别</em>的过程。这里，模式是指可用于定义任何空间或序列可观测数据是否在同一组中的特征。范畴是指模式识别的结果，意为一组相同或相似的模式。在<strong class="kz ir">统计模式识别</strong>中，提取给定训练样本的统计特征并用于形成识别过程。</p><p id="2b2b" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">模式识别的目标相当于确定一个多类别的鉴别器函数。为了生成一个性能良好的鉴别器函数，需要几个标准，例如最大后验概率判定规则、最小鉴别器误差判定规则、贝叶斯判定规则。在本帖中，我们重点以最大后验概率决策规则为例。在下面的解释中，我们致力于使用最大后验概率决策规则来定义给定输入数据 x 的相应类别 y。</p><h2 id="c09f" class="lt kg iq bd kh lu lv dn kl lw lx dp kp lg ly lz kr lk ma mb kt lo mc md kv me bi translated">最大后验概率决策规则</h2><p id="b725" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了定义给定输入 x 的类别 y，自然要选择一个类别，在该类别中该输入属于它的可能性最高。这意味着选择具有后验概率 p(y|x)的最大值的类别。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/caf6ed8aa24296f750a07569a41b34c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*UzC8E7WALdlN2ZdwDZDBjQ.png"/></div></figure><p id="a7c2" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">这种决策规则称为最大后验概率规则。这与将决策区域设置为</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/a8ed86189a70aa97e88e7153a20ca25c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*x95UtgJ_CECU0SakuBm15Q.png"/></div></figure><p id="b72f" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">根据贝叶斯理论，后验概率可以写成以下形式。由此，最大后验概率规则等价于最大条件概率 p(x|y)和先验概率 p(y)的乘积。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/190b2c18652ad47641568ac35303ddbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*WR6dVe_k1WnITyIVlRHMyA.png"/></div></figure><p id="9212" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">因此，需要估计条件概率 p(x|y)和先验概率 p(y)来获得后验概率 p(y|x)。</p><h2 id="25ec" class="lt kg iq bd kh lu lv dn kl lw lx dp kp lg ly lz kr lk ma mb kt lo mc md kv me bi translated">为什么我们需要参数估计？</h2><p id="2868" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">由于类别 y 是一个离散型随机变量，我们可以简单地估计先验概率</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/d533f5168ab6994d4e08f0cb5841a9cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*nlHtRZtBWGyXqHB-tr_Ptw.png"/></div></figure><p id="bb11" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">然而，用这种方法估计连续型随机变量输入 x 是不可能的。例如，假设输入 x 具有高斯分布。为了定义 x 的条件概率，我们需要期望值和标准变化值作为参数。<em class="mf">这就是为什么需要 MLE 等参数估计方法的原因。</em></p><h1 id="6e00" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">MLE 的定义</h1><p id="cde2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">由有限数量的参数形成的一组概率密度函数被称为<strong class="kz ir">参数模型</strong>。我们称<strong class="kz ir">为 q(x；θ)</strong>参数模型，其中<strong class="kz ir">θ</strong>是参数。</p><p id="89ad" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">当逼近概率密度函数时，很自然地确定参数值，使得我们拥有的训练样本是最有可能出现 的<strong class="kz ir"> <em class="mf">。这里，在参数<strong class="kz ir"> theta </strong>下，我们考虑当前训练样本<strong class="kz ir"> x_i </strong> (i=1，…，n)发生的概率。作为参数θ的函数，这个概率被称为<strong class="kz ir">似然</strong>并被写为<strong class="kz ir">L(θ)</strong>。在独立同分布假设下，可能性为</em></strong></p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/fadba0ed48e2b8b1824bb95eb0cf51fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*j7e8zwQHi9hn91Z8WW9r4w.png"/></div></figure><p id="956c" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">在最大似然估计方法中，我们找到使似然值最大化的参数θ的值。用最大似然法得到的最佳参数θ写为</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/785c9d44e0bd21213ffbec74cd8ab8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*KzywOdFMcklLfltEnnbhog.png"/></div></figure><p id="c58a" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">当参数模型 q(x；θ)可由θ导出，下面的等式为真。这个方程被称为<strong class="kz ir">似然方程</strong>。这是最大似然估计答案的必要条件<em class="mf">而不是充分条件</em>。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/4c70945dba73c1da99a1aea964914a0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*nRTOK2L6OCEnZtSRDIhTaA.png"/></div></div></figure><p id="76b5" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">在大多数情况下，求解似然方程是复杂的。作为一种解决方案，使用对数似然法。因为对数函数是单调增加的，所以对数似然和似然中的最佳参数是相同的。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nd"><img src="../Images/cc74f2d2f45cac000f21cc00554026e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*eHA4zRAeqHySNOHVLrlFyg.png"/></div></div></figure><h1 id="018a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">高斯模型</h1><p id="e205" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">高斯模型是具有高斯分布的参数模型。d 维模式<strong class="kz ir"> x </strong>的高斯模型通常以下列形式给出。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ne"><img src="../Images/841a3312b03cdaff8091469522c871e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YnfeHZA9I7KAL10Nq5AxNA.png"/></div></div></figure><p id="ec71" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">\μ和\适马是高斯模型的参数。\适马是正定对称矩阵。高斯模型的一个重要特征是参数μ和适马分别是概率分布的期望值和方差-协方差矩阵。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/fef3f354440db0448053e22dd477245c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*AMBk76C6VMc5qq5DLcRubw.png"/></div></figure><h1 id="9fd8" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">高斯模型的极大似然估计</h1><p id="472e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在，我们以高斯模型为例。假设我们有 n 个样本数据{x_i} (i=1，…，n)。我们将使用这些输入来估计高斯模型的参数。</p><p id="73bd" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">首先，模型的可能性和对数可能性是</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ng"><img src="../Images/c7d1a02a2979639b2dea81304004766b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m3U68xw2zsZ9Bo3ug2_Tug.png"/></div></div></figure><p id="1b41" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">接下来，似然方程可以写成</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b04b068a10426b0d6d287b60123f9399.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*rz6F2MZrvx7Ob01vtP0xsw.png"/></div></figure><p id="2534" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">求解这些方程，我们最终获得估计的参数</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/dd9c345f1047ee2beefba3e9fcf3f007.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*5dLcVdlO7FE1V5xXb2s3tA.png"/></div></figure><p id="971b" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">这同样适用于样本均值和样本方差-协方差矩阵。</p><p id="8d6a" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">下图显示了使用 8000 个样本点对高斯模型应用 MLE 的结果。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nj"><img src="../Images/7802fca4a906c5c867bc23c55eb7358b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IWqZfu9v8-xZDFd8Dd443A.jpeg"/></div></div></figure><h1 id="38d3" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">类别的后验概率</h1><p id="f3b4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">回到我们在定义给定输入数据的相应类别时的问题。我们通过计算 p(y|x)来选择概率的最大值，使得给定的数据 x 属于类别 y，从而找到这个类别。正如上一节所讨论的，我们的问题是估计条件概率 p(x|y)。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/190b2c18652ad47641568ac35303ddbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*WR6dVe_k1WnITyIVlRHMyA.png"/></div></figure><p id="56ad" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">假设我们的观测数据用高斯模型表示。现在，利用之前提到的 MLE，我们可以通过下式估计每个类别 y，p(x|y)的条件概率</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/7a5e89e14a2d4d7c46324bc97c9ca294.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*RBYrL1yIN8ofiSGquG96bw.png"/></div></figure><p id="94e0" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">。{mu}_y 和{适马}_y 是属于 y 类模式的估计期望值和方差-协方差矩阵</p><p id="0715" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">考虑到上一节提到的这个估计的 p(x|y)和 p(y ),我们现在可以计算后验概率了。为了使计算更简单，我们使用对数后验概率 log p(y|x)。n_y 是类别 y 中的样本数，n 是样本总数。c 是与变量 y 无关的常数。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nl"><img src="../Images/07223a525061c8dec904c274933a8568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OzU5TiTwK4Tyk80ELdlZPQ.png"/></div></div></figure><p id="d0d8" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">最后，为了定义模式 x 的相应类别，我们为类别集中的所有 y 计算 log p(y|x ),并选择具有最大值的一个。</p><h1 id="52e3" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">线性判别分析</h1><p id="e164" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为避免复杂化，我们假设每一类的方差-协方差矩阵相等，常见的方差-协方差矩阵为\适马。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/1aa6a3abba2f9f373498bdeb759c093c.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*QBF4loW3Y02EeyeYqPPXoQ.png"/></div></figure><p id="737d" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">根据这一假设和上述讨论，公共方差-协方差矩阵的估计为</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/a94a520900a3a12d233c1a849507be63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*CJfKSJYLrNHj6KgJrb7zgg.png"/></div></figure><p id="d436" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">使用这种估计，对数后验概率现在可以写成</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ng"><img src="../Images/53b476d6ada2cb07346e9302bcca8111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3B43PAWW08j8V8UfPifMUg.png"/></div></div></figure><p id="dd3e" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">作为一个例子，现在假设类别的数量是 2。在这种情况下，决策边界是一组后验概率相等的点，意思是 p(y = 1 |<strong class="kz ir">x</strong>)= p(y = 2 |<strong class="kz ir">x</strong>)。决策边界可以写成</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nn"><img src="../Images/40eb94299932c755b2d756dd1c3c30ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LE6x2kWss3MJUNsVZHvVaw.png"/></div></div></figure><p id="d590" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">这可以简化为</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi no"><img src="../Images/5d368e63022658a9cdff8dbe2224dc8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k9p-YlsQyvRXvjEmBVDrkg.png"/></div></div></figure><p id="1dd3" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">可以说决策边界是样本<strong class="kz ir"> x </strong>的超平面。这种确定决策边界的方法称为 Fisher 线性判别分析。</p><h1 id="5f81" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">例子</h1><p id="c4b2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">假设样本数据 x 在 2d 空间中。这里我们将做实值的线性判别分析。</p><h2 id="81f0" class="lt kg iq bd kh lu lv dn kl lw lx dp kp lg ly lz kr lk ma mb kt lo mc md kv me bi translated">训练样本</h2><p id="0cd4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">训练样本数据如下图所示，其中“x”表示类别 1，“+”表示类别 2。这里，我们在每个类别中使用 n1(=200)和 n2(=200)个样本。这些训练样本是用总体均值=(2，2)和(-2，-2)的高斯分布，总体方差-协方差矩阵[1，0；0,9].</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5623f2e8e765ba3cc2c6a3b6e6cd22e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*JU1Gwaq6kmX7bB_VDNPFbQ.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">人口参数</p></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d6dd48ec93564dc47a3aa63c9700fb01.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*vj9M1rpAJihZrbwnxHEbSg.jpeg"/></div></figure><h2 id="0d2a" class="lt kg iq bd kh lu lv dn kl lw lx dp kp lg ly lz kr lk ma mb kt lo mc md kv me bi translated">模型</h2><p id="6c0e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">与前面的讨论相同，我们用高斯模型近似数据，其中两个类别的方差-协方差矩阵相等。</p><h2 id="ad16" class="lt kg iq bd kh lu lv dn kl lw lx dp kp lg ly lz kr lk ma mb kt lo mc md kv me bi translated">估计</h2><p id="d28e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用最大似然估计，每个类别的期望值和方差-协方差矩阵的估计为</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nv"><img src="../Images/26034c5d9d09074e1f69dce651331ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b6ZgM9dQJMJNWDWcL2ngdQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">估计参数(样本参数)</p></figure><p id="f9a5" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">根据线性判别分析部分的结果，给定数据的判定边界为</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nw"><img src="../Images/2d44ae2b75cd4ded19395dc676dc8b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wsi8p5mbkZCffkjaKEV_NA.png"/></div></div></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/3d401b7cc72451537b0c81f5085af364.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*HnaJMP1hja4IhCG1x2g2MA.jpeg"/></div></figure><h2 id="3902" class="lt kg iq bd kh lu lv dn kl lw lx dp kp lg ly lz kr lk ma mb kt lo mc md kv me bi translated">讨论</h2><p id="677f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在，让我们看看样本数量如何影响决策边界。<br/>我们在[10，5，1，1/5，1/10]中对 n1/n2 值进行测试。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c71c94785449c234ac7409b862cee224.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*FbeSKlTLHqYpU4aUuYS_0A.jpeg"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">n1/n2 = 10</p></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c8fff6e69254b88fd339af526cf094e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*c0O_KtAR1uZ99MzufDc1Fg.jpeg"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">n1/n2 = 5</p></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/9ee9fccc80e0d081adf4b140fac7204a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*awELxJwQPTtwLuAxAlTC2g.jpeg"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">n1/n2 = 1</p></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/01bc6994cb12dcd2162392fb7e734b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*f_-jw6Ybgf5A9sIR2lNLqQ.jpeg"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">n1/n2 = 1/5</p></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1e782316d091db6c5da7d8c95ee73f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*GfVQm_wFa-rWEmXfUXE9SA.jpeg"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">n1/n2 = 1/10</p></figure><p id="479d" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">从这些结果中我们可以看出，当 n1/n2 &gt;1 (n1&gt; n2)时，将类别 1 的模式归入类别 2 的错误比类别 2 的模式少。在 n1<n2/></p><h1 id="6b90" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">结论</h1><p id="d757" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">最大似然估计在基于生成模型的模式识别中起着关键作用。正如我们在将 ML 估计应用于高斯模型中所讨论的，参数的估计与样本期望值和方差-协方差矩阵相同。这在统计估计中直观上很容易理解。然而，作为讨论部分的结果，样本的数量影响估计的准确性，这导致对模式识别(在这种情况下是线性判别分析)的性能的影响。因此，使用平衡的样本数据来避免过度适应识别过程的任何类别是至关重要的。</p><p id="cc12" class="pw-post-body-paragraph kx ky iq kz b la mg jr lc ld mh ju lf lg mi li lj lk mj lm ln lo mk lq lr ls ij bi translated">在这篇文章中，我们只讨论了高斯模型。然而，在现实生活的数据分析中，我们需要根据数据的自然特征为数据定义一个特定的模型。在不久的将来，我将介绍“最大似然估计中的模型选择”。</p><h2 id="a5d6" class="lt kg iq bd kh lu lv dn kl lw lx dp kp lg ly lz kr lk ma mb kt lo mc md kv me bi translated">源代码</h2><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h1 id="ec3f" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">参考</h1><p id="3a54" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[1] Masashi Sugiyama，统计机器学习—基于生成模型的模式识别(2019)</p></div></div>    
</body>
</html>