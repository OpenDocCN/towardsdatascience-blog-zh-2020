<html>
<head>
<title>The Maths behind Back Propagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播背后的数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-maths-behind-back-propagation-cf6714736abf?source=collection_archive---------2-----------------------#2020-03-21">https://towardsdatascience.com/the-maths-behind-back-propagation-cf6714736abf?source=collection_archive---------2-----------------------#2020-03-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6a8c" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">全民数据科学</h2><div class=""/><div class=""><h2 id="9bc9" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">BP算法幕后的一瞥</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/08ad192421c6d96bb6240f63f95ffe35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XJ7ioX3mFycK5FwsLqVJ8w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">是的，一样。</p></figure><h1 id="d7e9" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">介绍</h1><p id="3272" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">对于大多数人来说，反向传播(BP)如何工作的高级解释在概念上是相当容易理解的。</p><ol class=""><li id="a045" class="mv mw it mb b mc mx mf my mi mz mm na mq nb mu nc nd ne nf bi translated">计算成本函数C(w)</li><li id="c59c" class="mv mw it mb b mc ng mf nh mi ni mm nj mq nk mu nc nd ne nf bi translated">计算C(w)相对于你的神经网络(NN)中所有权重、<strong class="mb jd"> w </strong>和偏差、<strong class="mb jd"> b、</strong>的梯度</li><li id="d236" class="mv mw it mb b mc ng mf nh mi ni mm nj mq nk mu nc nd ne nf bi translated">调节<strong class="mb jd"> w </strong>和<strong class="mb jd"> b </strong>与它们的梯度大小成比例。</li></ol><p id="7cc0" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">更可怕的是我们到底是如何计算这些梯度的。</p><h2 id="d638" class="no li it bd lj np nq dn ln nr ns dp lr mi nt nu lt mm nv nw lv mq nx ny lx iz bi translated">你为什么要关心BP是如何工作的？</h2><p id="0314" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">看看上面的图片就足以让我们当中除了最注重数量的人之外的所有人远离这个深度学习的领域。尤其是有了这么多的高级库(例如Keras ),人们很容易忽略BP实际上是如何工作的。然而，理解BP如何工作将有助于你理解许多用于提高普通神经网络性能的技术。此外，了解BP将为您提供一个很好的基础，来攻击那里更奇特的深度学习架构，因为它们几乎都在引擎盖下使用它。</p><p id="1987" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">我真的相信大多数人能够理解上面介绍的内容，因为它只不过是高中数学，用一些花哨的符号来记录神经网络中数百万个参数。我们只需要把它分解成它的组成部分，然后把它们翻译成英语，这样我们就能理解这些方程想要表达的意思。</p><p id="4d74" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">在本文中，我将阐明驱动BP的方程——驱动深度学习的奇迹算法。在继续之前，我假设读者已经知道神经网络是如何训练的。具体来说，假设在高水平上理解神经网络的正向和反向传播阶段。如果这些听起来很奇怪，那么我会把你引向我的<a class="ae nz" rel="noopener" target="_blank" href="/a-beginners-guide-to-neural-nets-5cf4050117cb"> <strong class="mb jd">上一篇文章</strong> </a>，我在其中解释了这一点。</p><p id="5ca0" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">在接下来的文章中，我将继续我上一篇文章中识别手写数字的例子。所以让我们继续吧。</p></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><h1 id="f784" class="lh li it bd lj lk oh lm ln lo oi lq lr ki oj kj lt kl ok km lv ko ol kp lx ly bi translated">简单的一对一网络</h1><p id="f843" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">与大多数问题一样，我们可以通过简化场景来取得很大进展。因此，让我们先考虑一个简单的1-1-1网络，而不是通常的神经元和层以各种方式连接到一切。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/16af0571d503d1fd1cb6719e9ef829f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3bu4nhqYki6zO9GzQKvFTg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图1:标有激活、权重和偏差的1–1–1网络。</p></figure><p id="9e0b" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">图1显示了这样一个网络，其中绿色、蓝色和红色神经元分别代表输入、隐藏和输出神经元。我们将最后一层神经元的激活称为a^(L，将前一层神经元的激活称为a^(l-1)，其中l是我们网络的层数(在这种情况下L = 3)。类似地，将层(L-1)和l之间的权重和偏差定义为w^(L-1)和b^(L-1).</p><p id="5b1b" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">想象一下，我们把一个9的1像素图像(只是逗逗我，显然这是不可能的)通过我们的神经网络的前向传播阶段，这个输出神经元对应于数字9。期望的输出是1。但是我们的神经网络给我们一个随机值，比如0.68。我们将使用均方误差作为我们的成本函数。一个训练示例的成本C就是(0.68–1)。</p><h2 id="aa11" class="no li it bd lj np nq dn ln nr ns dp lr mi nt nu lt mm nv nw lv mq nx ny lx iz bi translated">第一层</h2><p id="f1f6" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">所以现在我们想计算我们的成本C . w . r . t .的梯度，即连接L层到L-1层的神经元的权重。为了了解如何做到这一点，我们可以展开网络的最后一层。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/0cfd30eb4a5fce78e3c15208ce142475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2StFNIVfPq2reTu3Uwrjfg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图2:图1的最后一层展开以显示术语如何影响成本函数c。</p></figure><p id="603a" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">图2示出了对最终层的激活(a^(L)有贡献的每一项如何对成本c有贡献。为了得到最终层的加权和(z^(L ),我们将来自层(L-1)的激活乘以连接两层的权重w^(L-1).然后我们加上一个偏差项，b^(L-1).最后，我们将加权和通过非线性函数σ(z^(L)，来计算a^(L).</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ca"><img src="../Images/ba97de83bf173b9a6ae38b3a78167c7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nsH_4rWrzXfAwiJtxGK6VQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图3:最终层的加权和与激活的等式。</p></figure><p id="24a1" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">现在我们想知道如果我们改变w^(L-1).，c会改变多少换句话说，我们想计算dC/dw^(L-1).要做到这一点，我们可以使用我们所有人在中学时都学过的东西:<a class="ae nz" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd"> <em class="oo">链式法则</em> </strong> </a> <strong class="mb jd"> <em class="oo">。</em>T9】</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/35e6561caa46a5df3aa600ad52abf1d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nr-Ajn_VaAZfdYRK4ADEaw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图4:用于计算成本函数相对于最终层权重的梯度的链式法则。还显示了展开的网络，以显示权重w^(L-1如何间接影响c</p></figure><p id="c7c4" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">从图4可以看出重量w^(L-1)影响c的路径。w^(L-1)对加权和z^(L)有贡献，加权和)用于计算活化度a^(L)，活化度本身直接用于计算c。因此，通过对适当的项取导数，我们可以构建c对重量w^(L-1).的梯度方程</p><p id="faef" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">在链式法则中使用来自图3的z^(L和a^(L的方程给出</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/05c29266c2883e4a120d1eddafda6293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5GH2TkdOi5Juie1dq3MNWg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图5:图4的链式法则中每一项的解析表达式，得到w^(L-1).梯度的表达式</p></figure><p id="fe42" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">图5左侧三项中的第一项是a^(L).的梯度因为我们使用MSE(a^(l)-y)，作为我们的成本函数，梯度是2(a^(L)-y).</p><p id="40ac" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">第二项是最终层的激活梯度与最终层的加权和。这就是我们正在使用的非线性函数的导数，所以我们称之为σ’。例如，如果我们使用sigmoid作为我们的非线性函数(1/(1+e^-z)，那么导数将计算为<a class="ae nz" rel="noopener" target="_blank" href="/derivative-of-the-sigmoid-function-536880cf918e"> (e^-z/(1+e^-z) ) </a>。</p><p id="bed4" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated"><em class="oo">【注意:这就是为什么选择一个处处可微的非线性函数很重要，否则你不能传播(计算)你的梯度，尽管有类似</em> <a class="ae nz" href="https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/" rel="noopener ugc nofollow" target="_blank"> <em class="oo"> ReLU激活函数</em></a><em class="oo">】</em>的警告</p><p id="2596" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">最后一项是连接层(L-1)和L的加权和的导数。根据图3，这简单地等于层(L-1)的激活。</p><p id="edca" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">我们还想知道，如果我们改变偏置项，b^(L-1).，我们的成本函数会如何变化因此除了偏置项之外，可以构建与图4类似的展开图。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/65ea0399b0676dbbdf1bc948f8155247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-rrCc0qs4dGZLw3EMCAoTA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图6:用于计算成本函数相对于最终层偏差的梯度的链式法则。还显示了展开的网络，以显示权重b^(L-1如何间接影响c</p></figure><p id="b9ff" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">链式法则中唯一改变的项是第一项，现在最后一层的加权和的导数相对于偏差。</p><p id="4b17" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">像以前一样，我们仔细检查并计算链式法则所需的每一项的值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/62f2ef0feff6048cda43aaf9cebc65f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TY4xe9SS4YZ6adkHRCK7g.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图7:图6的链式法则中每一项的解析表达式，得到b^(L-1).梯度的表达式</p></figure><p id="14ea" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">很好，现在神经网络可以使用权重更新方程中的这些方程来计算出下一轮训练中w^(L-1和b(L-1)的值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/3cf94a355d2af8e40f25fae6f3b75f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sdFfGOmJt4sAyxdynuUxCw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图8:最终层权重和偏差项的权重更新方程。</p></figure><p id="3bf6" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">尽管神经网络对a^(L-1没有直接控制，但当我们考虑进一步进入神经网络时，很快就会发现我们需要它，所以让我们重复上述过程，但对a^(L-1).来说</p><p id="6701" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">再次，我们沿着回到a^(L-1的路径，并构建链接a^(L-1的变化如何影响c的链式规则</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/47fc3821ff10b5231c057e680370917c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oj4NrNWlxgXd2oFCFrwnfA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图9:用于计算成本函数相对于(L-1)层激活的梯度的链式法则。还显示了展开的网络，以显示激活a^(L-1如何间接影响c</p></figure><p id="41e9" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">和以前一样，我们只是替换了链式法则中的第一项。然后我们计算这三项以获得</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/01801cb0735fbefe8f6d9ddb868a459c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-rKBc4zvWKUD7wqjgxvxw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图10:图9的链式法则中每一项的解析表达式，得到a^(L-1).梯度的表达式</p></figure><p id="8a37" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">如果你一直坚持到这一点，你已经完成了大部分工作！再多一点点，你就能得到几乎所有的方程了。</p><h2 id="d698" class="no li it bd lj np nq dn ln nr ns dp lr mi nt nu lt mm nv nw lv mq nx ny lx iz bi translated">L-2层</h2><p id="57bb" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">现在让我们考虑一下，如果我们改变层(L-2)中的一个权重，C会发生什么变化。和前面一样，我们可以画一个展开图来说明w^(L-2)是如何间接影响c的</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/98c447e62e93189198de6a22d3d1898f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*242xbU1rZy8Yn38-6RIH7Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图11:展开图1的三层以显示术语如何影响成本函数c</p></figure><p id="5713" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">构建适当的链式规则表达式，其将w^(L-2的变化如何影响成本函数c联系起来</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/3ef21fe98904186a5c27b0cc7132464e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AWiKnT7t1OJqJYlwBo2lug.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图12:用于计算成本函数相对于(L-2)层权重的梯度的链式法则。还显示了展开的网络，以显示权重w^(L-2如何间接影响c</p></figure><p id="a9fd" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">从图12中，你可以看到连接w^(L-2和c的路径。因此，我们在链规则中使用路径中所有项的偏导数。但是我们刚刚算出了a^(L-1的变化如何影响c的表达式(见图10)。所以我们用一个偏导数代替最后三个偏导数，因为我们已经有了它的表达式。</p><p id="9391" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">前两项与之前非常相似，但现在我们只是替换了图5中上标的负1。这样，我们就可以得到重量w^(L-2).的梯度的下式</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/498afab6c1d430d713c52828882bc711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lOHZ6akjCJY6En2gQQpB8Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图13:图12的链式法则中每一项的分析表达式，得到w^(L-2).梯度的表达式</p></figure><p id="8ab8" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">你可以看到，如果我们有一个更深的网络，甚至更远的一层，这个表达式将调用梯度dC/da^(L-2)，前一个将调用dC/da^(L-3)，等等。因此，通过计算每层激活的C . w . r . t .的导数，我们可以递归地计算C . w . r . t .的梯度，而不考虑网络中的任何权重或偏差。这个过程被称为<strong class="mb jd">反向传播</strong>，因为<strong class="mb jd"> </strong>你是字面上传播梯度从最后一层回来。</p><p id="dca1" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">如果你已经做到了这一步，那么你会很高兴听到以上是90%的反向传播。我们刚刚针对1–1–1网络的简单情况执行了上述所有操作。将其推广到任意大小的网络，只意味着我们必须添加一些求和以及更多的指数，以跟踪每一项对应于层中的哪个神经元。但是我们刚刚推导出的方程具有完全相同的形式。</p></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><h1 id="ccfc" class="lh li it bd lj lk oh lm ln lo oi lq lr ki oj kj lt kl ok km lv ko ol kp lx ly bi translated">通用网络</h1><p id="3212" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">现在让我们考虑如何修改上面的方程，以应用于每层有许多神经元而不是只有一个神经元的一般网络。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/4fbb1923ce15471cbaba632177ec2ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LkeJHCHfPvAoKkFljs34JA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图14:标记了激活和权重的一般网络。与图1相反，现在每一项都有一个下标来表示它所指的是层中的哪一个神经元。</p></figure><p id="1219" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">图14示出了这样的网络，其中有L层，并且层L具有N _ 1个神经元。例如，如果N _ 1 = 5，则最后一层中有5个神经元，如果N_3 = 7，则意味着第3层有7个神经元。还显示了连接不同层中的神经元的权重之一。和以前一样，它有一个上标来表示它对应于哪一层，还有一个下标来表示它连接每一层中的哪一个神经元。例如，图中的权重w_21连接L-1层的神经元1和L层的神经元2。</p><p id="eb4c" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated"><em class="oo">【注意:我将只考虑权重方程如何变化，因为这与偏差方程的过程相同。] </em></p><h2 id="0af6" class="no li it bd lj np nq dn ln nr ns dp lr mi nt nu lt mm nv nw lv mq nx ny lx iz bi translated">调整正向传播方程</h2><p id="e167" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">让我们首先来看看前向传播的术语在1–1–1情况和最终层的成本、加权和以及激活的一般情况之间是如何变化的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/a8760503463f2af41a137f2a361d0df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yppV3Wx4xorWBHoIf2apzw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图15:简单1-1-1情况(左)和一般情况(右)的成本、加权和以及激活的等式。</p></figure><p id="f298" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">在1–1–1的情况下，成本函数只取决于输出层中单个神经元的值。但是现在在更一般的情况下，我们在最后一层有N_L个神经元。在我们的数字识别例子中，N _ L = 10——每个数字对应一个神经元。所以我们需要对最后一层中每个神经元的平方误差求和。最后，因为我们使用均方误差作为我们的成本函数，所以我们除以该层中神经元的数量。</p><p id="a877" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">对于一般情况下的加权和，现在有来自前一层(L-1)中所有神经元的贡献，这是k上的和，此外，由于一层中有多个神经元，我们必须确保指定我们指的是哪一个，因此我们为此添加下标j。与往常一样，这是最容易理解的图表和简化的例子，其中最后和倒数第二层都有3个神经元，如图16所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/73655a02538658a64d2c31fd35c4252a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fYo3EgKSuFSdkbj_LJwFRA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图16:每层有3个神经元的网络。示出了对最终层中的第二神经元的加权和有贡献的激活和权重。</p></figure><p id="a4b7" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">因此，如果我们考虑哪些项对最后一层中第二个神经元的加权和有贡献，我们会看到前一层中的每个激活都有贡献。它们被乘以一个权重，该权重将它们连接到最后一层中的第二个神经元，因此我们给权重一个下标，其中两个数字对应于这两个神经元。您还可以将单个偏差项添加到加权和中(未显示)。</p><p id="c6a8" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">最后，图15中的第三项没有改变，所以你将这个加权和通过一个非线性函数来获得神经元的激活。我们唯一要做的是添加一个下标，来显示我们所指的是哪一层中的神经元。例如，a₁^(L)是最后一层中第一个神经元(图16中右上神经元)的激活，同样，a₃^(L)是最后一层中第三个神经元(图16中右下)的激活。</p><p id="35e6" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">很好，我们已经调整了条款，以解决所有激活，并最终在正向传播中使用的成本。现在，我们需要对用于反向传播的梯度进行同样的操作。</p><h2 id="f8bd" class="no li it bd lj np nq dn ln nr ns dp lr mi nt nu lt mm nv nw lv mq nx ny lx iz bi translated">调整反向传播方程</h2><h2 id="0049" class="no li it bd lj np nq dn ln nr ns dp lr mi nt nu lt mm nv nw lv mq nx ny lx iz bi translated"><strong class="ak">L-1层</strong></h2><p id="2f35" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">和以前一样，我会先陈述结果，然后我们会看到调整的原因。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/b01e340ae80504ba0c81f8ad93167faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aSfFE1rttwfmI-RV9Ufk9A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图17:简单1-1-1情况(左)和一般情况(右)下成本函数梯度与(L-1)层活化和重量的关系式。</p></figure><p id="107a" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">C w.r.t .的梯度与(L-1)层的权重相比没有太大变化，除了我们增加了一个下标，正如我们在调整前向传播方程时所做的那样。</p><p id="80dd" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">真正改变的术语是C w.r.t上一层活化的梯度。当我们调整加权和方程时，推理与以前基本相同，通过图表和简化的3-3-3网络更清晰。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/5a073d308cee05ea1f12351836e76083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2kSpqLdwUGHzmnMOzRnYww.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图18:每层3个神经元的网络该图显示了(L-1)层中特定神经元的激活现在如何影响L层中的所有激活，从而通过这些N_L通道影响C。在这种情况下，它有3个不同的渠道来影响成本。</p></figure><p id="9d56" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">图18显示来自(L-1)层中单个神经元的激活现在如何影响最终层的所有激活。然后将所有这些激活相加以计算成本c。因此，在该示例中，改变a₁^(L-1)将影响下一层中的所有N_L神经元，这将影响成本。这就是为什么梯度现在是下一层神经元上的总和，以说明将受该激活值变化影响的所有通道。</p><p id="5d11" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">关键是它和以前的形式完全一样，我们只是引入了求和和下标来说明每层有多个神经元。</p><h2 id="0010" class="no li it bd lj np nq dn ln nr ns dp lr mi nt nu lt mm nv nw lv mq nx ny lx iz bi translated">L-2层</h2><p id="80fb" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">最后，我们进一步回到这个网络中，问，如果我改变层(L-2)中的权重会发生什么？</p><p id="9bd6" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">让我们首先考虑一下，通过改变将层(L-2)中的第一个神经元连接到层(L-1)中的第一个神经元的权重，我们预计会影响哪些项。另一幅救援图！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/754b5911a60b9985bfbadf25b49fbdd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DILZZU9QiADQwyKWKZKoxQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图19:每层3个神经元的网络该图显示了改变(L-2)和(L-1)层中第一个神经元之间的重量现在如何影响L层中的所有激活，从而通过这些N_L通道影响C。在这种情况下，它有3个不同的渠道来影响成本。</p></figure><p id="2141" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">改变(L-2)和(L-1)层第1个神经元之间的重量会改变(L-1)层第1个神经元的激活。但是，正如我们刚才看到的，这现在影响了通过N_L通道的成本，因为它连接到最后一层的所有神经元。因此，我们预计这个权重的梯度将包含这些N_L通道上的和。</p><p id="371b" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">图20示出了成本函数相对于层(L-2)中的权重的导数的调整方程。如果我们像以前一样，只调整1–1–1情况下的表达式，加上下标If，可能看起来我们没有包括这些N_L通道。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/cd6739d9fae87a2ed063d9c3885dd27d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h7zW8Bdd9dWYCksNcTGzPg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图20:简单1-1-1情况(左)和一般情况(右)下成本函数梯度与层(L-2)中权重的等式。</p></figure><p id="9307" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated"><em class="oo">但是，</em>如果我们现在输入上面图17的表达式，对于图20右侧的最后一个导数，我们确实看到N_L个通道已经包括在这个表达式中。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/507f2c962ef5b465c487c27067822a96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3n1bshVcC_DuGvSDJlcRxA.png"/></div></div></figure><p id="ebcf" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">我们现在遵循与之前完全相同的过程，由此计算网络中任何 权重的梯度，我们只是递归地找到成本函数w.r.t .的梯度，所有的激活都在成本和感兴趣的权重之间。换句话说，我们<strong class="mb jd"> <em class="oo">反向传播</em> </strong>梯度计算一直回到感兴趣的权重。</p><p id="c235" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">遵循相同的程序来获得网络中任何偏差的梯度。</p><p id="430d" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">就是这样！</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ph pi l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">【鸣谢:<a class="ae nz" href="https://giphy.com/gifs/reaction-spoilers-outlander-LSNqpYqGRqwrS" rel="noopener ugc nofollow" target="_blank">https://gi phy . com/gifs/reaction-扰流板-外域人-LSNqpYqGRqwrS </a>】</p></figure></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><h1 id="e9ba" class="lh li it bd lj lk oh lm ln lo oi lq lr ki oj kj lt kl ok km lv ko ol kp lx ly bi translated">摘要</h1><p id="e8f8" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">祝贺你，如果你能在一次阅读中理解所有的内容。如果这是你第一次看这些方程，它们肯定会因为所有的下标和上标而显得非常混乱。但是通过观察网络和跟踪一个项影响C的渠道，你可以推理出求和应该发生在哪里。这使得方程看起来不那么可怕了。</p><p id="ed2d" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">最好的事情是，因为网络中更后面的每个梯度都依赖于网络中它之后的梯度项，所以您不需要构建长得离谱的链式规则表达式，因为您已经从前面的层中获得了您需要的表达式。</p><p id="d11d" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">在本文中，我们首先使用简单的1–1–1网络来推导反向传播方程的一般形式。这是一个展开网络的问题，以明确地看到从网络中的术语到成本函数的路径，然后构建适当的链式规则表达式。一旦我们有了这个，我们就可以简单地计算出这些表达式是什么。然后，我们看到层(L-1)中的项如何依赖于层L中的项。因此，我们可以通过不断地将后面层中的项链接在一起，将我们的计算结果传播回网络。</p><p id="a508" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">最后，我们转向网络的一般情况，每层有多个数量可变的神经元。这个简单案例和这个案例的区别在于:</p><ol class=""><li id="a56f" class="mv mw it mb b mc mx mf my mi mz mm na mq nb mu nc nd ne nf bi translated">包含下标来跟踪我们所指的神经元</li><li id="6e49" class="mv mw it mb b mc ng mf nh mi ni mm nj mq nk mu nc nd ne nf bi translated">确保一个项的变化可能影响C的所有渠道都考虑在内的总和。</li></ol><p id="a22d" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">不要误解我，支持反向传播的方程乍一看并不容易理解(或者第二、第三、第四等)。但它们所代表的实际数学工具只是导数和加法。其余的只是奇特的簿记。</p><p id="f24b" class="pw-post-body-paragraph lz ma it mb b mc mx kd me mf my kg mh mi nl mk ml mm nm mo mp mq nn ms mt mu im bi translated">我希望你喜欢研究黑盒的方程式。下一篇文章再见。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pj pi l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">【鸣谢:<a class="ae nz" href="https://tenor.com/vemb.gif" rel="noopener ugc nofollow" target="_blank">https://tenor.com/vemb.gif</a></p></figure></div></div>    
</body>
</html>