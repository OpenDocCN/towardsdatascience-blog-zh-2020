<html>
<head>
<title>Multiclass Classification Using Logistic Regression from Scratch in Python: Step by Step Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中从头开始使用逻辑回归的多类分类:分步指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multiclass-classification-algorithm-from-scratch-with-a-project-in-python-step-by-step-guide-485a83c79992?source=collection_archive---------1-----------------------#2020-09-05">https://towardsdatascience.com/multiclass-classification-algorithm-from-scratch-with-a-project-in-python-step-by-step-guide-485a83c79992?source=collection_archive---------1-----------------------#2020-09-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/2d9241705a54d9f4470e4eeb563a70b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kKg_pAv6GLGUhk3W"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Tj·霍洛韦丘克在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="1734" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">逻辑回归的两种方法:梯度下降法和优化函数</h2></div><p id="cb9e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑回归是一种非常流行的机器学习技术。当因变量是分类变量时，我们使用逻辑回归。本文将集中讨论多类分类问题的逻辑回归的实现。我假设您已经知道如何用逻辑回归实现二元分类。</p><p id="2d3f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你还没有用逻辑回归研究过二元分类，我建议你在深入研究这篇文章之前先浏览一下这篇文章。</p><blockquote class="lu"><p id="5840" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">因为多类分类是建立在二类分类之上的。</p></blockquote><p id="18e7" class="pw-post-body-paragraph ky kz jj la b lb me kk ld le mf kn lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">在本文中，您将学习二进制分类的概念、公式和工作示例:</p><div class="is it gp gr iu mj"><a rel="noopener follow" target="_blank" href="/logistic-regression-in-python-to-detect-heart-disease-2892b138d0c0"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">Python 中用于检测心脏病的逻辑回归</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">发展逻辑回归演算法的重要方程式和如何发展逻辑回归演算法…</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx ja mj"/></div></div></a></div><h2 id="2fb1" class="my mz jj bd na nb nc dn nd ne nf dp ng lh nh ni nj ll nk nl nm lp nn no np nq bi translated">多类分类</h2><p id="3aa5" class="pw-post-body-paragraph ky kz jj la b lb nr kk ld le ns kn lg lh nt lj lk ll nu ln lo lp nv lr ls lt im bi translated">多类分类的实现遵循与二类分类相同的思想。如你所知，在二元分类中，我们解决一个是或否的问题。与上述文章中的示例一样，输出回答了一个人是否患有心脏病的问题。我们只有两节课:心脏病和非心脏病。</p><p id="2f0c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果输出为 1，则此人患有心脏病，如果输出为 0，则此人没有心脏病。</p><p id="5deb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在多类分类中，我们有两个以上的类。这里有一个例子。比方说，我们将汽车、卡车、自行车和船只的不同功能和特征作为输入功能。我们的工作是预测标签(汽车、卡车、自行车或船)。</p><blockquote class="nw nx ny"><p id="6447" class="ky kz nz la b lb lc kk ld le lf kn lg oa li lj lk ob lm ln lo oc lq lr ls lt im bi translated"><strong class="la jk">这怎么解决？</strong></p></blockquote><p id="7e32" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将把每一个类当作一个二元分类问题，就像我们解决一个心脏病或没有心脏病的问题一样。</p><blockquote class="lu"><p id="57c7" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">这种方法被称为一对一方法</p></blockquote><p id="d87d" class="pw-post-body-paragraph ky kz jj la b lb me kk ld le mf kn lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">在 one vs all 方法中，当我们处理一个类时，这个类用 1 表示，其余的类变成 0。</p><p id="96a2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，如果我们有四类:汽车、卡车、自行车和船。当我们处理汽车时，我们将汽车作为 1，其余的类作为 0。同样，当我们在卡车上工作时，卡车的元素将是 1，其余的类将是 0。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/b63222b2ea7cceaf385e4d82835f25db.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*NbEEq5qMi5cMMhDX.png"/></div></figure><p id="d39a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">等你落实了就更好理解了。我建议你在阅读的时候继续编码和运行代码。</p><blockquote class="nw nx ny"><p id="1e61" class="ky kz nz la b lb lc kk ld le lf kn lg oa li lj lk ob lm ln lo oc lq lr ls lt im bi translated"><strong class="la jk">在这里，我将用两种不同的方式实现这个算法:</strong></p></blockquote><ol class=""><li id="e064" class="oi oj jj la b lb lc le lf lh ok ll ol lp om lt on oo op oq bi translated">梯度下降法。</li></ol><p id="10f1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.优化函数方法。</p><blockquote class="nw nx ny"><p id="ddca" class="ky kz nz la b lb lc kk ld le lf kn lg oa li lj lk ob lm ln lo oc lq lr ls lt im bi translated"><strong class="la jk">重要方程式及其工作原理:</strong></p></blockquote><p id="cb5b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑回归使用 sigmoid 函数来预测输出。sigmoid 函数返回一个从 0 到 1 的值。一般我们取一个阈值，比如 0.5。如果 sigmoid 函数返回大于等于 0.5 的值，我们将其视为 1，如果 sigmoid 函数返回小于 0.5 的值，我们将其视为 0。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/513072060bb3a13a74c59574dae6f738.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/0*E7Fh2Y6ggl4auHfL.png"/></div></figure><p id="3256" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">z 是输入特征乘以表示为θ的随机初始值。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/b4ad11a3bf0f954520bdfcee8a2b7c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/format:webp/0*jTIfm0I36ngvCwyb.png"/></div></figure><p id="0325" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，X 是输入特征。在大多数情况下，有几个输入特征。所以，这个公式变大了:</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/d8c73fee3abccfcd951658175f288591.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/0*xvhpCGeRCcW_YWK0.png"/></div></figure><p id="8a65" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">X1、X2 和 X3 是输入要素，将为每个输入要素随机初始化一个θ。开始时的θ0 是偏置项。</p><blockquote class="lu"><p id="590d" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">该算法的目标是在每次迭代中更新该θ，以便能够建立输入要素和输出标注之间的关系。</p></blockquote><blockquote class="nw nx ny"><p id="4a60" class="ky kz nz la b lb me kk ld le mf kn lg oa mg lj lk ob mh ln lo oc mi lr ls lt im bi translated"><strong class="la jk">成本函数和梯度下降</strong></p></blockquote><p id="61e0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">成本函数给出了我们的预测与原始输出有多远的想法。这是它的公式:</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/6d08b8caca8201cb8376ab81087a98a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/0*3VgbEv47VZwzP8kR.png"/></div></div></figure><p id="c30f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，</p><p id="7749" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">m 是训练样本的数量或训练数据的数量，</p><p id="9d5e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">y 是原始输出标签，</p><p id="d984" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">h 是假设或预测输出。</p><p id="6573" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是梯度下降的方程式。使用这个公式，我们将在每次迭代中更新θ值:</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/2be00a39a60c05b0836609542403cf00.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/0*oO66Qg22zMmwiEZr.png"/></div></figure><h2 id="0c74" class="my mz jj bd na nb nc dn nd ne nf dp ng lh nh ni nj ll nk nl nm lp nn no np nq bi translated">用梯度下降法实现</h2><blockquote class="nw nx ny"><p id="00f9" class="ky kz nz la b lb lc kk ld le lf kn lg oa li lj lk ob lm ln lo oc lq lr ls lt im bi translated"><strong class="la jk">先决条件:</strong></p></blockquote><p id="aa9b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">a.您需要能够舒适地读写 python 代码。</p><p id="809d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">b.基本数字图书馆和熊猫图书馆。</p><blockquote class="nw nx ny"><p id="0e25" class="ky kz nz la b lb lc kk ld le lf kn lg oa li lj lk ob lm ln lo oc lq lr ls lt im bi translated">在这里，我将一步一步地展示实现过程。</p></blockquote><ol class=""><li id="2ac4" class="oi oj jj la b lb lc le lf lh ok ll ol lp om lt on oo op oq bi translated">导入必要的包和数据集。我从 Coursera 的<a class="ae jg" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">吴恩达的机器学习课程中获取了数据集。这是一个手写识别数据集。有从 1 到 10 的数字。</a></li></ol><p id="55a4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从像素数据集中，我们需要识别数字。在该数据集中，输入变量和输出变量被组织在 Excel 文件的不同表格中。请随意从本页末尾的链接下载数据集。</p><blockquote class="lu"><p id="573c" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">如果你正在阅读这篇文章来学习这个算法，请运行每一段代码。</p></blockquote><p id="24bf" class="pw-post-body-paragraph ky kz jj la b lb me kk ld le mf kn lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">让我们导入必要的包和数据集，</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="e30c" class="my mz jj ox b gy pb pc l pd pe">import pandas as pd<br/>import numpy as np<br/>xl = pd.ExcelFile('ex3d1.xlsx')<br/>df = pd.read_excel(xl, 'X', header=None)</span></pre><figure class="oe of og oh gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/d0a1e539d239364f4895325022e00bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/0*lKjFKiuUHo399UhZ.png"/></div></figure><p id="5516" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.导入 y，这是输出变量</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="9e00" class="my mz jj ox b gy pb pc l pd pe">y = pd.read_excel(xl, 'y', header = None)</span></pre><figure class="oe of og oh gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/ffdc169a437dfc54a75fb400a62902f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:116/format:webp/0*3KJazqYW0JRdDwpW.png"/></div></figure><p id="b391" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.定义采用输入变量和θ的假设。它返回计算出的输出变量。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="cb82" class="my mz jj ox b gy pb pc l pd pe">def hypothesis(theta, X):<br/>    return 1 / (1 + np.exp(-(np.dot(theta, X.T)))) - 0.0000001</span></pre><p id="dbb1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.构建采用输入变量、输出变量和θ的成本函数。它返回假设的成本。这意味着它给出了预测与原始输出之间的距离。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="8e6d" class="my mz jj ox b gy pb pc l pd pe">def cost(X, y, theta):<br/>    y1 = hypothesis(X, theta)<br/>    return -(1/len(X)) * np.sum(y*np.log(y1) + (1-y)*np.log(1-y1))</span></pre><p id="9743" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">5.现在，是数据预处理的时候了。</p><p id="990b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据是干净的。不需要太多的预处理。我们需要在输入变量中添加一个偏差列。请检查 df 和 y 的长度。如果长度不同，模型将不起作用。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="5fca" class="my mz jj ox b gy pb pc l pd pe">print(len(df))<br/>print(len(y))<br/>X = pd.concat([pd.Series(1, index=df.index, name='00'), df], axis=1)</span></pre><p id="7e94" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">6.y 列有从 1 到 10 的数字。这意味着我们有 10 节课。</p><p id="2ffd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">y 是一个不必要的数据帧。我将只把列作为包含值的序列。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="32ea" class="my mz jj ox b gy pb pc l pd pe">y = y.iloc[:, 0]</span></pre><p id="1d92" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将为每个类创建一个长度与 y 相同的列。当类为 5 时，为包含 5 和 0 的行创建一个包含 1 的列，否则为 0。</p><p id="7a9f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看，我们有多少节课，</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="0a79" class="my mz jj ox b gy pb pc l pd pe">y.unique()</span></pre><p id="2225" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="0317" class="my mz jj ox b gy pb pc l pd pe">array([10,  1,  2,  3,  4,  5,  6,  7,  8,  9], dtype=int64)</span></pre><p id="6382" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，我们有 10 节课。启动一个有 10 列和 df.shape[0]行的 DataFrame。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="6659" class="my mz jj ox b gy pb pc l pd pe">y1 = np.zeros([df.shape[0], len(y.unique())])<br/>y1 = pd.DataFrame(y1)</span></pre><p id="63a2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将用一些简单的代码以编程的方式实现它:</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="d9cf" class="my mz jj ox b gy pb pc l pd pe">for i in range(0, len(y.unique())):<br/>    for j in range(0, len(y1)):<br/>        if y[j] == y.unique()[i]:<br/>            y1.iloc[j, i] = 1<br/>        else: <br/>            y1.iloc[j, i] = 0<br/>y1.head()</span></pre><p id="3e8c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">7.现在定义函数‘gradient _ descent’。该函数将输入变量、输出变量、theta、alpha 和历元数作为参数。这里，α是学习率。</p><p id="f5cc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你应该根据你的要求来选择它。学习率太小或太大都会使你的算法变慢。我喜欢针对不同的学习速率运行算法，并获得正确学习速率的想法。可能需要几次迭代来选择正确的学习速率。</p><blockquote class="nw nx ny"><p id="a13a" class="ky kz nz la b lb lc kk ld le lf kn lg oa li lj lk ob lm ln lo oc lq lr ls lt im bi translated">对于 y1 中的每一列，我们将实现一个二元分类。</p></blockquote><p id="9ff9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，当我考虑数字 2 时，它应该为数字 2 返回 1，为其余的数字返回 0。因此，由于我们有 10 个类，我们已经运行了每个时期(迭代)10 次。这里有一个嵌套的 for 循环。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="4a45" class="my mz jj ox b gy pb pc l pd pe">def gradient_descent(X, y, theta, alpha, epochs):<br/>    m = len(X)<br/>    for i in range(0, epochs):<br/>        for j in range(0, 10):<br/>            theta = pd.DataFrame(theta)<br/>            h = hypothesis(theta.iloc[:,j], X)<br/>            for k in range(0, theta.shape[0]):<br/>                theta.iloc[k, j] -= (alpha/m) * np.sum((h-y.iloc[:, j])*X.iloc[:, k])<br/>            theta = pd.DataFrame(theta)<br/>    return theta, cost</span></pre><p id="f9ef" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">7.初始化θ。记住，我们将为每个类实现逻辑回归。每堂课也会有一系列的θ。</p><p id="923d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我已经运行了 1500 个纪元了。我相信随着时代的增加，准确率会更高。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="3b26" class="my mz jj ox b gy pb pc l pd pe">theta = np.zeros([df.shape[1]+1, y1.shape[1]])<br/>theta = gradient_descent(X, y1, theta, 0.02, 1500)</span></pre><p id="a858" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">8.使用更新后的θ，计算输出变量。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="50e7" class="my mz jj ox b gy pb pc l pd pe">output = []<br/>for i in range(0, 10):<br/>    theta1 = pd.DataFrame(theta)<br/>    h = hypothesis(theta1.iloc[:,i], X)<br/>    output.append(h)<br/>output=pd.DataFrame(output)</span></pre><p id="b6b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">9.将计算的输出与原始输出变量进行比较，以计算模型的准确性。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="753d" class="my mz jj ox b gy pb pc l pd pe">accuracy = 0<br/>for col in range(0, 10):<br/>    for row in range(len(y1)):<br/>        if y1.iloc[row, col] == 1 and output.iloc[col, row] &gt;= 0.5:<br/>            accuracy += 1<br/>accuracy = accuracy/len(X)</span></pre><p id="d0a2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">准确率为 72%。我确信，更多时代的准确性会更好。因为花了太多时间，我没有重新运行算法。</p><blockquote class="lu"><p id="07bf" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">如果你正在运行这个，请随意尝试更多的纪元，并在评论部分让我知道，你有多少准确性。</p></blockquote><blockquote class="nw nx ny"><p id="e66f" class="ky kz nz la b lb me kk ld le mf kn lg oa mg lj lk ob mh ln lo oc mi lr ls lt im bi translated"><strong class="la jk">除了梯度下降法，你还可以使用内置的优化函数</strong>。</p></blockquote><p id="ed0b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这种方法中，使用优化函数来优化算法的θ。这是一个更快的方法。</p><h2 id="f62a" class="my mz jj bd na nb nc dn nd ne nf dp ng lh nh ni nj ll nk nl nm lp nn no np nq bi translated">用优化函数实现</h2><p id="ffba" class="pw-post-body-paragraph ky kz jj la b lb nr kk ld le ns kn lg lh nt lj lk ll nu ln lo lp nv lr ls lt im bi translated">1.我们将使用与之前相同的数据集。如果使用相同的笔记本，请使用不同的名称导入数据集:</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="64ba" class="my mz jj ox b gy pb pc l pd pe">xls = pd.ExcelFile('ex3d1.xlsx')<br/>df = pd.read_excel(xls, 'X', header=None)</span></pre><figure class="oe of og oh gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/48367666b72b1b14fc623f9f126fb9e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/0*OBmflKYzjWRaMCf2.png"/></div></figure><p id="52bf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.我们仍然需要为 df 中的偏差项添加一列全 1。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="7129" class="my mz jj ox b gy pb pc l pd pe">X = np.c_[np.ones((df.shape[0], 1)), df]</span></pre><figure class="oe of og oh gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/61a14d8d084b477856b3513d2deb2e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*1F1VdHWVMehzXU5J.png"/></div></figure><p id="ffd1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.导入“y”的数据。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="ba2f" class="my mz jj ox b gy pb pc l pd pe">y = pd.read_excel(xls, 'y', header=None)</span></pre><figure class="oe of og oh gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/15b3fe2cb90dab4683aac671a43a3cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:116/format:webp/0*BqPwaYiFoNiEVW-r.png"/></div></figure><p id="8583" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于这是一个数据帧，只需将列 0 作为一个系列，并使其二维，以匹配 x 的维度。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="62de" class="my mz jj ox b gy pb pc l pd pe">y = y[0]<br/>y = y[:, np.newaxis]</span></pre><figure class="oe of og oh gt iv gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/bc1b96818dc4cececcf3239412931dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/0*EfZg58335XM_6zWM.png"/></div></figure><p id="d4b8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，“y”只有一列。10 节课 10 栏。每个专栏将处理一个类。例如，当我们将处理类 10 时，我们将把 10 保留在它的位置，并用零替换其余的值。下面是函数 y_change，它将接受 y 本身和一个类(比如 3)。然后，它会用 1 替换 3，用所有其他类替换 0。在后面的步骤中很快就会用到这个函数。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="6a88" class="my mz jj ox b gy pb pc l pd pe">def y_change(y, cl):<br/>    y_pr=[]<br/>    for i in range(0, len(y)):<br/>        if y[i] == cl:<br/>            y_pr.append(1)<br/>        else:<br/>            y_pr.append(0)<br/>    return y_pr</span></pre><p id="6de3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据准备完成。现在开发模型:</p><p id="bbfb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.定义假设函数。这与前面的方法相同。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="2ed1" class="my mz jj ox b gy pb pc l pd pe">def hypothesis(X, theta):<br/>    z = np.dot(X, theta)<br/>    return 1/(1+np.exp(-(z)))</span></pre><p id="12c6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">5.开发成本函数。这一个也和前面的方法一样:</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="812b" class="my mz jj ox b gy pb pc l pd pe">def cost_function(theta, X, y):<br/>    m = X.shape[0]<br/>    y1 = hypothesis(X, theta)<br/>    return -(1/len(X)) * np.sum(y*np.log(y1) + (1-y)*np.log(1-y1))</span></pre><p id="6922" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">6.定义渐变。这个不一样。该函数定义了如何更新θ。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="b45b" class="my mz jj ox b gy pb pc l pd pe">def gradient(theta, X, y):<br/>    m = X.shape[0]<br/>    y1 = hypothesis(X, theta)<br/>    return (1/m) * np.dot(X.T, y1 - y)</span></pre><p id="7942" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">7.现在，导入优化函数并初始化θ。我取零作为初始θ值。任何其他值应该也可以。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="178e" class="my mz jj ox b gy pb pc l pd pe">from scipy.optimize import minimize, fmin_tnc<br/>theta = np.zeros((X.shape[1], 1))</span></pre><p id="849f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">8.让我们创建一个拟合函数，它将 X，y 和θ作为输入。它将使用一个<a class="ae jg" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_tnc.html" rel="noopener ugc nofollow" target="_blank">优化函数</a>并为我们输出优化后的θ。</p><p id="a602" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它需要这三个参数:</p><p id="6a18" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">I .需要最小化的功能，</p><p id="2de8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">二。待优化的参数，</p><p id="2395" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">三。用于优化的参数。</p><p id="4d90" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本例中，成本函数应最小化，θ需要为此进行优化。输入和输出变量 X 和 y 是要使用的参数。</p><p id="cd11" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个优化函数采用另一个参数，即梯度。但这是可选的。这里，我们有一个关于梯度的公式或函数。所以我们正在通过它。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="2b65" class="my mz jj ox b gy pb pc l pd pe">def fit(X, y, theta):<br/>    opt_weigths = fmin_tnc(func=cost_function, x0=theta,<br/>                          fprime=gradient, args=(X, y.flatten()))<br/>    return opt_weigths[0]</span></pre><p id="71ba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">9.使用这种拟合方法找到优化的θ。我们必须分别优化每个类的θ。让我们开发一个函数，其中对于每个类，将使用步骤 3 中的 y_change 方法相应地修改“y”。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="ced8" class="my mz jj ox b gy pb pc l pd pe">def find_param(X, y, theta):<br/>    y_uniq = list(set(y.flatten()))<br/>    theta_list = []<br/>    for i in y_uniq:<br/>        y_tr = pd.Series(y_change(y, i))<br/>        y_tr = y_tr[:, np.newaxis]<br/>        theta1 = fit(X, y, theta)<br/>        theta_list.append(theta1)<br/>    return theta_list</span></pre><p id="bcb1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用这种方法找到最终的θ</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="287a" class="my mz jj ox b gy pb pc l pd pe">theta_list = find_param(X, y, theta)</span></pre><p id="6ccb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">10.是时候预测产量了。我们还必须单独预测各个类别。</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="2ac0" class="my mz jj ox b gy pb pc l pd pe">def predict(theta_list, x, y):<br/>    y_uniq = list(set(y.flatten()))<br/>    y_hat = [0]*len(y)<br/>    for i in range(0, len(y_uniq)):<br/>        y_tr = y_change(y, y_uniq[i])<br/>        y1 = hypothesis(X, theta_list[i])<br/>        for k in range(0, len(y)):<br/>            if y_tr[k] == 1 and y1[k] &gt;= 0.5:<br/>                y_hat[k] = y_uniq[i]<br/>    return y_hat</span></pre><p id="97d3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用上述预测方法，计算预测输出 y_hat:</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="4080" class="my mz jj ox b gy pb pc l pd pe">y_hat = predict(theta_list, X, y)</span></pre><p id="882d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">11.计算准确度</p><pre class="oe of og oh gt ow ox oy oz aw pa bi"><span id="c42b" class="my mz jj ox b gy pb pc l pd pe">accuracy=0<br/>for i in range(0, len(y)):<br/>    if y_hat[i] == y.flatten()[i]:<br/>        accuracy += 1<br/>print(accuracy/len(df)*100)</span></pre><p id="839c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">这个过程给出了 100%的准确率。</strong>现在。你可以自己决定，你想在你的项目中使用哪种逻辑回归方法。</p><p id="ed74" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，使用神经网络解决了同样的问题，展示了如何从头开始开发神经网络:</p><div class="is it gp gr iu mj"><a href="https://medium.com/towards-artificial-intelligence/build-a-neural-network-from-scratch-in-python-f23848b5a7c6" rel="noopener follow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">用 Python 从头开始构建神经网络</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">神经网络的详细说明和逐步实现</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">medium.com</p></div></div><div class="ms l"><div class="pi l mu mv mw ms mx ja mj"/></div></div></a></div><p id="5fea" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你有任何问题，请在评论区问我。查看 GitHub 页面获取数据集:</p><div class="is it gp gr iu mj"><a href="https://github.com/rashida048/Machine-Learning-With-Python/blob/master/ex3d1.xlsx" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">rashida 048/用 Python 进行机器学习</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">github.com</p></div></div><div class="ms l"><div class="pj l mu mv mw ms mx ja mj"/></div></div></a></div><p id="5d56" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是梯度下降法代码的链接</p><div class="is it gp gr iu mj"><a href="https://github.com/rashida048/Machine-Learning-With-Python/blob/master/multicall_classification_with_logistic_regression.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">rashida 048/用 Python 进行机器学习</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">github.com</p></div></div><div class="ms l"><div class="pk l mu mv mw ms mx ja mj"/></div></div></a></div><p id="7e62" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是优化函数方法 Github 链接的链接:</p><div class="is it gp gr iu mj"><a href="https://github.com/rashida048/Machine-Learning-With-Python/blob/master/multiclass_classification_with_fmin_tnc.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">rashida 048/用 Python 进行机器学习</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">github.com</p></div></div><div class="ms l"><div class="pl l mu mv mw ms mx ja mj"/></div></div></a></div><p id="c371" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">推荐阅读:</p><div class="is it gp gr iu mj"><a rel="noopener follow" target="_blank" href="/basic-linear-regression-algorithm-in-python-for-beginners-c519a808b5f8"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">Python 中的线性回归算法:一步一步</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">学习线性回归的概念，并使用 python 从头开始开发一个完整的线性回归算法</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="pm l mu mv mw ms mx ja mj"/></div></div></a></div><div class="is it gp gr iu mj"><a rel="noopener follow" target="_blank" href="/multivariate-linear-regression-in-python-step-by-step-128c2b127171"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">Python 中多元线性回归的逐步实现</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">学习用 Python 从头开始开发任意数量变量的多元线性回归。</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="pn l mu mv mw ms mx ja mj"/></div></div></a></div><div class="is it gp gr iu mj"><a rel="noopener follow" target="_blank" href="/polynomial-regression-from-scratch-in-python-1f34a3a5f373"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">Python 中从头开始的多项式回归</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">学习用一些简单的 python 代码从头开始实现多项式回归</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="po l mu mv mw ms mx ja mj"/></div></div></a></div><div class="is it gp gr iu mj"><a rel="noopener follow" target="_blank" href="/a-complete-anomaly-detection-algorithm-from-scratch-in-python-step-by-step-guide-e1daf870336e"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">Python 中从头开始的完整异常检测算法:分步指南</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">基于概率的异常检测算法</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="pp l mu mv mw ms mx ja mj"/></div></div></a></div><div class="is it gp gr iu mj"><a rel="noopener follow" target="_blank" href="/a-complete-guide-to-confidence-interval-and-examples-in-python-ff417c5cb593"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">置信区间的完整指南，以及 Python 中的示例</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">对统计学中一个非常流行的参数——置信区间及其计算的深入理解</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="pq l mu mv mw ms mx ja mj"/></div></div></a></div></div></div>    
</body>
</html>