<html>
<head>
<title>TensorFlow Semi-Supervised Object Detection Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流半监督对象检测体系结构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tensorflow-semi-supervised-object-detection-architecture-757b9c88f270?source=collection_archive---------41-----------------------#2020-07-24">https://towardsdatascience.com/tensorflow-semi-supervised-object-detection-architecture-757b9c88f270?source=collection_archive---------41-----------------------#2020-07-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c7ba" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">测试整体模型性能时自动标记图像的简单方法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4b60fe5c51459f759590ce20ce9cfce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EL_1p7SYkMJ4FeFy0C4iXg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">托拜厄斯·凯勒在<a class="ae ky" href="https://unsplash.com/s/photos/architecture?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1a19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目标检测是当今最流行和最常用的计算机视觉方法之一，其目的不仅在于以与最常见的分类问题相同的方式确定是否在图像中找到目标，还在于指出这些感兴趣的目标的位置，这是在多个目标可能同时出现在图像中的情况下的必要方法。</p><p id="b0bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法的挑战之一是创建数据集，一旦需要手动设置图像中所有对象的位置，在大量的观察中花费大量的时间来这样做。</p><blockquote class="lv lw lx"><p id="00a2" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">这一过程效率低、成本高、耗时长，主要表现在一些需要在每幅图像中标注几十个物体或者需要专门知识的问题上。</p></blockquote><p id="d254" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于此，我创建了 TensorFlow 半监督对象检测架构(TSODA)来交互式地训练对象检测模型，并使用它来基于置信度阈值水平自动标记新图像，将它们聚合到后期的训练过程中。</p><p id="de2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将向您展示在您的对象检测项目中重现这种方法的必要步骤。有了它，您将能够在测量模型性能的同时自动在图像中创建标签！</p><h1 id="15f7" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">目录:</h1><ol class=""><li id="2152" class="mu mv it lb b lc mw lf mx li my lm mz lq na lu nb nc nd ne bi translated"><strong class="lb iu">措达如何工作</strong></li><li id="f108" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb iu">示例应用</strong></li><li id="73ea" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb iu">实施</strong></li><li id="d816" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb iu">结果</strong></li><li id="a950" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated"><strong class="lb iu">结论</strong></li></ol></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><h1 id="fc43" class="mc md it bd me mf nr mh mi mj ns ml mm jz nt ka mo kc nu kd mq kf nv kg ms mt bi translated">TSODA 的工作原理</h1><p id="5d8b" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">工作类似于任何其他半监督方法，其中使用标记和未标记的数据进行训练，不像最常见的监督方法。</p><p id="29fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用手动完成的强标记数据训练初始模型，从这些数据中学习一些特征，然后在未标记数据中创建推断，以将这些新标记的图像聚集到新的训练过程中。</p><p id="61f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个想法可以用下图来说明:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b98ba66ec041b2cadf423909301f5e8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/0*sGfLkyIL29toBHdl"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(字体:作者)</p></figure><p id="9689" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个操作一直进行到达到停止标准，或者是执行的次数，或者是没有剩余的未标记数据。</p><p id="8a02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在模式中看到的，最初配置了 80%的置信度阈值。一旦新图像将被用于新的训练过程，这是一个重要的参数，并且如果不正确地标记，可能产生不期望的噪声，破坏模型性能。</p><p id="4133" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TSODA 的目的是介绍一种简单快速的方法，在你的目标检测项目中使用半监督学习。</p><h1 id="cf81" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">示例应用程序</h1><p id="b701" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">为了举例说明这种方法并测试是否一切正常，我们对 Asirra 数据集的 1，100 幅图像进行了随机抽样，每类样本占 50%。</p><p id="49e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些图像被手动标记，以便稍后进行比较，你可以在 Kaggle 上下载相同的数据<a class="ae ky" href="https://www.kaggle.com/alvarole/asirra-cats-vs-dogs-object-detection-dataset" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="b441" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我使用单次多盒探测器(SSD)作为对象检测架构，Inception 作为基础网络，而不是像在<a class="ae ky" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>中的 VGG 16。</p><p id="8377" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SSD 和 Inception 在训练速度和准确性之间有很好的权衡，所以我认为这是一个很好的起点，主要是因为在每次迭代中，TSODA 需要保存训练模型的检查点，推断新的图像并加载模型以再次训练它，所以更快的训练有利于更多地迭代并将这些图像聚合到学习中。</p><h2 id="5441" class="oa md it bd me ob oc dn mi od oe dp mm li of og mo lm oh oi mq lq oj ok ms ol bi translated">测试性能</h2><p id="2bb7" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">为了测试 TSODA 的性能，只提供了每个类别的 100 个标记图像来分成训练和测试，同时让 900 个图像作为未标记的，模拟只花很少时间来创建标记数据集的情况。将获得的结果与用所有手动标记的图像训练的模型进行比较。</p><p id="944b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据被随机分成 80%的图像用于训练，20%用于测试。</p><h1 id="44ec" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">履行</h1><p id="17d9" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">顾名思义，整个架构是使用 TensorFlow 环境完成的，在 2.x 版本中。</p><p id="6b63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个新的 TF 版本还不完全兼容对象检测，有些部分很难适应，但在接下来的几个月里，这将是所有项目中默认的和更多使用的 TF 版本，这就是为什么我认为调整代码以使用它是重要的。</p><p id="7c8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了创建 TSODA，新的脚本和文件夹被添加到 TF Model Garden repository 的一个分支中，这样你就可以很容易地克隆和运行你的半监督项目，除了为那些使用 TF 的人提供一个熟悉的结构之外。</p><p id="cd2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以克隆我的<a class="ae ky" href="https://github.com/AlvaroCavalcante/tf-models" rel="noopener ugc nofollow" target="_blank">库</a>来轻松地遵循这些步骤或者修改你的 TF 模型库。</p><p id="dc1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这项工作是在<em class="ly">models/research/object _ detection、</em>中完成的，在这里您会找到以下文件夹和文件:</p><ul class=""><li id="41a2" class="mu mv it lb b lc ld lf lg li om lm on lq oo lu op nc nd ne bi translated"><strong class="lb iu"><em class="ly">inference _ from _ model . py</em>:</strong>该文件将被执行以使用模型来推断新的图像。</li><li id="fddf" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu op nc nd ne bi translated"><strong class="lb iu"><em class="ly">generate _ XML . py</em></strong><em class="ly"/>和<em class="ly"/><strong class="lb iu"><em class="ly">generate _ TF record . py</em></strong>:都将用于创建训练和测试 TF 记录，用于对象检测模型的训练(这些脚本改编自<a class="ae ky" href="https://github.com/datitran/raccoon_dataset" rel="noopener ugc nofollow" target="_blank"> <em class="ly">浣熊数据集</em> </a>)。</li><li id="f63c" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu op nc nd ne bi translated"><strong class="lb iu"> <em class="ly"> test_images </em> </strong>和<strong class="lb iu"> <em class="ly"> train_images </em> </strong>文件夹:有将要使用的 JPG 图像和 XML 文件。</li><li id="305a" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu op nc nd ne bi translated"><strong class="lb iu"> <em class="ly">未标记 _ 图像</em> </strong>和<strong class="lb iu"> <em class="ly">标记 _ 图像</em> </strong>文件夹:分别包含所有未标记的图像和算法自动标记的图像，这些图像将被分为训练和测试文件夹以保持比例。</li></ul><p id="8873" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 utils 文件夹中，我们还有一些东西:</p><ul class=""><li id="45dc" class="mu mv it lb b lc ld lf lg li om lm on lq oo lu op nc nd ne bi translated"><strong class="lb iu"><em class="ly">generate _ xml . py</em></strong>:该脚本负责获取模型推理并生成一个新的 XML，该 XML 将存储在<em class="ly">label _ images</em>文件夹中。</li><li id="bc16" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu op nc nd ne bi translated"><strong class="lb iu"><em class="ly">visualization _ utils . py</em>:</strong>这个文件在代码上也有一些修改，用来捕捉模型推断，并传递给“generateXml”类。</li></ul><p id="e6ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样，这就是你需要在你的存储库中拥有的一切！</p><h2 id="f596" class="oa md it bd me ob oc dn mi od oe dp mm li of og mo lm oh oi mq lq oj ok ms ol bi translated">准备环境</h2><p id="5d55" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">运行这个项目你不需要<strong class="lb iu">任何东西！？</strong></p><blockquote class="lv lw lx"><p id="3e1a" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">训练过程是在一个谷歌 Colab 笔记本电脑，所以它的快速和简单的训练你的模型，你真的只需要用你的图像替换我的图像，并选择另一个基本模型，如果你和。</p></blockquote><p id="4263" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将<a class="ae ky" href="https://drive.google.com/file/d/1srnHOXFv779IGT0lKKGbTn8J-5dGSAX_/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">原始 Colab 笔记本</a>复制到您的 Google Drive 并执行它。</p><p id="86ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你真的想在你的机器上运行 TSODA，在 Jupiter notebook 的开始你会看到安装要求，只要按照它做，但不要忘记也安装 TF 2.x，我建议创建一个虚拟环境。</p><h2 id="fe35" class="oa md it bd me ob oc dn mi od oe dp mm li of og mo lm oh oi mq lq oj ok ms ol bi translated">理解代码</h2><p id="4e5c" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated"><strong class="lb iu"> <em class="ly">推理 _ 自 _ 模型. py </em> </strong>负责加载在训练中创建的<em class="ly">保存 _ 模型. pb </em>并使用它在未标记图像中进行新的推理。大部分代码是从<em class="ly"> colab_tutorials </em>文件夹中的<strong class="lb iu">object _ detection _ tutorial . ipynb</strong>获得的。</p><p id="0c41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您不想使用 Colab 进行训练，您需要替换文件开头的路径。</p><p id="44c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该文件中的另一个重要方法是<strong class="lb iu"> <em class="ly"> partition_data </em> </strong>，它负责将推断的图像(将在<em class="ly">标签 _ 图像</em>文件夹中)分割成训练和测试，以保持相同的比率。</p><p id="cf15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能想做的一个更改是分流比，在我的例子中，我选择了 80/20 的比例，但如果您想要不同的东西，您可以在方法参数中设置它。</p><p id="a735" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> visualization_utils.py </strong>是边界框被绘制到图像中的地方，因此我们使用它来获取框的位置、类名、文件名，并将其传递到我们的 XML 生成器中。下面的代码展示了这个过程的大部分:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="159f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果在图像中检测到一个比指定可信度更高的框，则生成 XML。</p><p id="35b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有信息到达<strong class="lb iu"> generate_xml.py </strong>中，xml 是使用 ElementTree 创建的。</p><p id="9089" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在代码中，有一些注释可以帮助你理解一切是如何工作的。</p><h1 id="4fad" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结果</h1><p id="dfda" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">为了评估模型性能，使用了平均精度(mAP)，如果您对其工作原理有疑问，请查看<a class="ae ky" href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener">。</a></p><p id="344e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个测试是通过 4000 个时期训练一个模型，使用所有强烈标记的图像。</p><p id="0393" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练持续大约 21 分钟，结果如表 1 所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/8dd04c7dd45641d93d686d1d6d26de70.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*vlIMOmeu9oykRyckiyiUPQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表 2:使用正确标记的所有图像进行训练和测试的地图。(字体:作者)</p></figure><p id="e22e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如预期的那样，该模型获得了高地图，主要是在较低的 UoI 率。</p><p id="fecc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二个测试是使用相同的配置完成的，但是 TSODA 只考虑了 100 个带标签的图像。在每次迭代中，该模型由 1000 个时期训练，然后用于推断和创建新的标记图像。结果如图 2 所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/10ad8b4a16c79e70b4f4cbc43f4c43a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UWQ_bkdVMoMxZLcT"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TSODA 中的模型收敛(font: Author)</p></figure><p id="4b79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个训练过程用了三十八分钟，比上一次多了十七分钟左右，模型达到了更差的最终图，如表 2 所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/bf23b539b03850b54956addc47df570c.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*LqCt-_cC3y-kaxekMBea_A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第一次测试的最终地图。(字体:作者)</p></figure><p id="0792" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如表 3 所示，大多数图像在第一次迭代中被成功注释，在训练中被聚集。这可能意味着最小置信度阈值不够高，因为在前一千次迭代中，模型还没有正确收敛，可能会创建错误的注释。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/ae26bf8d9bd8e283737e95075347e941.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*Jgka8p3eIx5_SEYyxOJK3g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">迭代后剩余的未标记图像的数量(font: Author)。</p></figure><p id="010c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TSODA 需要更多的时间和历元来提高模型性能并接近原始方法。发生这种情况的原因是，一旦模型需要学习如何概括新模式，在训练集中添加新图像会导致 mAP 丢失，如图 2 所示，其中 mAP 随着新图像的加入而减少，然后在模型学习新特征时再次开始增加。</p><p id="958e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在图 3 中，有一些图像自动注释的例子。值得注意的是，有些标签没有被很好地标记出来，但是这足以保证给模型提供更多的信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/4103c9cd7da2a6642a58278a4a2e2ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*a3baPhXRm8BjdAo0"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动注释图像的示例。正如所见，如果由人来做，标签可能更适合对象。</p></figure><p id="d28d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到不同的历元增量行为以及更高的置信度阈值，执行了一些新的实验。结果如表 4 所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/59c4e0d4a463f3bafab7f7bbdf314c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*JaC2QFD_SNiAKWl_F_67ug.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用第二种配置的结果！(字体:作者)</p></figure><p id="b866" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将置信度阈值设置为 90%可确保在预测中获得正确标签的几率更高，这是模型收敛的一个重要因素。虽然在初始迭代中对 2，500 个时期进行训练，而不是在第一次迭代中只对 1，000 个时期进行训练，因为第一次迭代是大多数图像被标记的地方，所以模型需要学习更多的特征，并且能够击败更高的置信度。在第一次迭代之后，随后的迭代增加 1，500 个历元，直到限制为 8，500 个。这些新配置改善了最终结果。</p><p id="ab8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据感兴趣对象的种类及其复杂性，TSODA 的性能可能会有所不同。如果通过更多的时期进行训练或者设置更高的置信度阈值，结果可以得到改善，缺点是增加了训练时间。此外，通过迭代的历元增量必须根据问题而改变，以基于未标记图像的数量和阈值来控制模型收敛。</p><blockquote class="lv lw lx"><p id="458f" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">然而，这是一个很好的选择，一旦训练时间比需要人的手动标记时间更便宜，TSODA 的构建方式是，只需少量修改，就可以从头开始训练一个全新的大规模模型。</p></blockquote><p id="af1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自动创建的标签也可以在一些图像中手动调整，这可以提高整体性能，并且比手动创建所有标签更快。</p><h1 id="dcf3" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="cd85" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">所提出的 TSODA 可以在为未标记图像创建新标记方面取得令人满意的结果，达到与强标记训练方法类似的结果，但需要相当少的人力。该解决方案还适用于任何其他 CNN 检测器架构，实施简单快速，有助于数据集创建过程，同时测量物体检测器的整体性能。</p><h1 id="1577" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考</h1><p id="3641" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">关于这个半监督项目的更多细节和背景，请看我的<a class="ae ky" href="https://www.researchgate.net/publication/343167804_Semi-supervised_approach_to_auto-label_images_in_object_detection" rel="noopener ugc nofollow" target="_blank">预印本</a>。</p></div></div>    
</body>
</html>