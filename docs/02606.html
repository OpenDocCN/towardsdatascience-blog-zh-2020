<html>
<head>
<title>StyleGAN v2: notes on training and latent space exploration</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">StyleGAN v2:训练和潜在空间探索笔记</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stylegan-v2-notes-on-training-and-latent-space-exploration-e51cf96584b3?source=collection_archive---------2-----------------------#2020-03-13">https://towardsdatascience.com/stylegan-v2-notes-on-training-and-latent-space-exploration-e51cf96584b3?source=collection_archive---------2-----------------------#2020-03-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/7dca6970809c4fe5d640f12e47573e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*N8yWQZonuy728fswi59bsA.gif"/></div></div></figure><p id="11a8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">内容:</strong>在这篇文章中，我将展示我在训练多个StyleGAN模型和探索学习的潜在空间时收集的笔记、想法和实验结果。</p><p id="c2a9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">为什么:</strong>这是一个想法/考虑的垃圾场，范围从显而易见的到“神圣的钼”，旨在为其他有类似兴趣和意图的人提供洞察力或讨论的起点。因此，可以浏览一下，看看是否有什么感兴趣的东西。</p><p id="9520" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我也在这里大量利用<a class="ae kz" href="https://en.wikipedia.org/wiki/Ward_Cunningham#Cunningham%27s_Law" rel="noopener ugc nofollow" target="_blank">坎宁安定律</a>。</p><p id="de1b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">许多考虑因素适用于StyleGAN v1和v2，但是所有生成的结果都来自v2模型，除非明确指定。</p><p id="455a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">就代码资源而言:</p><ul class=""><li id="6bbf" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated"><a class="ae kz" href="https://github.com/NVlabs/stylegan" rel="noopener ugc nofollow" target="_blank"> StyleGAN v1 </a>和<a class="ae kz" href="https://github.com/NVlabs/stylegan2" rel="noopener ugc nofollow" target="_blank"> v2 </a>官方回购</li><li id="bae8" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated"><a class="ae kz" href="https://github.com/Puzer/stylegan-encoder" rel="noopener ugc nofollow" target="_blank">v1用编码器(+方向学习</a>)和<a class="ae kz" href="https://github.com/rolux/stylegan2encoder" rel="noopener ugc nofollow" target="_blank">v2用编码器</a></li></ul><p id="cc1b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里的所有内容都是通过<a class="ae kz" href="https://github.com/5agado/data-science-learning/tree/master/deep%20learning/StyleGAN" rel="noopener ugc nofollow" target="_blank">我定制的Jupyter笔记本</a>在这些回复的基础上生成的。</p><p id="1cf6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">值得一提的是勤勤恳恳分享代码、实验和宝贵建议的人:<a class="ae kz" href="https://twitter.com/gwern" rel="noopener ugc nofollow" target="_blank"> Gwern </a> ( <a class="ae kz" href="https://www.gwern.net/Faces" rel="noopener ugc nofollow" target="_blank"> TWDNE </a>)，<a class="ae kz" href="https://twitter.com/pbaylies" rel="noopener ugc nofollow" target="_blank"> pbaylies，</a> <a class="ae kz" href="https://twitter.com/gpt2ent" rel="noopener ugc nofollow" target="_blank"> gpt2ent，</a> <a class="ae kz" href="https://twitter.com/xsteenbrugge" rel="noopener ugc nofollow" target="_blank"> xsteenbrugge，</a> <a class="ae kz" href="https://twitter.com/Veqtor" rel="noopener ugc nofollow" target="_blank"> Veqtor </a>，<a class="ae kz" href="https://twitter.com/Norod78" rel="noopener ugc nofollow" target="_blank"> Norod78 </a>，<a class="ae kz" href="https://twitter.com/ak92501" rel="noopener ugc nofollow" target="_blank"> roadrunner01 </a>。所有这些都非常值得关注。</p><h1 id="6a7f" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">培养</h1><h2 id="2aeb" class="mm lp it bd lq mn mo dn lu mp mq dp ly km mr ms mc kq mt mu mg ku mv mw mk mx bi translated">资料组</h2><p id="fa77" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">对于自定义数据集，需要预先创建一组图像，确保所有条目具有相同的方形和颜色空间。然后，您可以生成tf记录，如这里的<a class="ae kz" href="https://github.com/NVlabs/stylegan2#preparing-datasets" rel="noopener ugc nofollow" target="_blank">所述</a>。<br/>正如人们多次指出的那样，这个模型非常渴求数据。数据集大小要求因图像内容而异，但一般目标是4到5个数量级，比如有镜像时大约50k，没有镜像时加倍。这适用于从头开始的训练，而在微调的情况下，已经在非常小的数据集上进行了许多实验，提供了有趣的结果。</p><p id="c57e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我主要用两个“时尚”数据集进行实验</p><ul class=""><li id="92a2" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">服装(~30k包装照片)</li><li id="4cfb" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">鞋类(~ 140，000包照片)</li></ul><div class="nd ne nf ng gt ab cb"><figure class="nh ju ni nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/1a8420474413c402cdc70e7f9104d82b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*rT-4jCKqfJ-Q41EobH3FwA.png"/></div></figure><figure class="nh ju ni nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/7608df1713fc278a613d9a98d1dc595d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*SfmS6FSMnX0y4bEdCzV_xQ.png"/></div><p class="nn no gj gh gi np nq bd b be z dk nr di ns nt translated">来自鞋类和服装数据集的真实图像样本</p></figure></div><p id="bae6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但是也将展示一些在人工策划的艺术数据集上的实验(每个数据集大约1k张图片)。</p><div class="nd ne nf ng gt ab cb"><figure class="nh ju nu nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/58a2ca22db50e8614f4978ec79f79404.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*YZGfYOQX0AskLiQO7QW9PA.png"/></div></figure><figure class="nh ju nv nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/d782636348a79ab59bd17b7fbb4e69d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*IQ1PMlf7s7UqGVrrLM35GA.png"/></div></figure><figure class="nh ju nv nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/b78be0d15a741b83465592022b8d9332.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*UXL1FYdDGj6-FbsOrmA_oA.png"/></div><p class="nn no gj gh gi np nq bd b be z dk nw di nx nt translated">真实图像样本(弗兰克·弗兰泽塔，埃贡·席勒，阿尔丰斯·穆夏)</p></figure></div><p id="c3a6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请注意，时尚数据集的内容一致性更高(在位置、背景等方面。).与艺术作品相比。因此，无论数据集大小如何，我们都希望前者能够达到更高的生成精度。</p><p id="9604" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">服装图像是应该启用<strong class="kd iu">镜像增强</strong>的一个例子，因为我们可以免费加倍我们的训练集，学习服装的语义不变属性用于水平镜像。</p><p id="ad8f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">相反，鞋类是应该禁用镜像的情况，因为数据集中的所有图像都是对齐的，并且学习镜像版本对于网络来说只是不必要的负担。</p><p id="63c9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">关于大小，我选择512来首先验证结果。从之前其他网络的实验来看，我相信这是一个捕捉图案、材质和纽扣、拉链等小细节的好方法。我还计划在256上进行下一次迭代，并验证所学表述的质量以及与训练制度的比较。<br/>我建议，如果你有合适的硬件资源，并且已经用较小的分辨率验证了你的数据集，那么就转到全1024分辨率(或更高)。</p><h2 id="01cf" class="mm lp it bd lq mn mo dn lu mp mq dp ly km mr ms mc kq mt mu mg ku mv mw mk mx bi translated">火车</h2><p id="e31c" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">人们可以从头开始训练模型，或者利用以前训练的结果，将学习到的表示微调到新的数据集。</p><p id="79be" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">从头开始训练</strong>就像运行以下程序一样简单</p><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="0ec6" class="mm lp it nz b gy od oe l of og">python run_training.py -num-gpus=2 -data-dir=&lt;your_data_dir&gt; -config=config-f -dataset=&lt;your_dataset_name&gt; -mirror-augment=True -result-dir=&lt;your_results_dir&gt;</span></pre><p id="1548" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们已经讨论过镜像增强，GPU的数量取决于您的设置。</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/fa1bf528c21c141fde4478866a843625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3N363f_ZAqNCVcJeME8nJA.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">服装数据集多次训练的fid50k曲线比较</p></figure><p id="70b2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">配置</strong>用于控制模型的架构和属性，从<em class="oi"> config-a </em>(最初的SyleGAN v1)开始，逐步添加/更改组件(权重解调、惰性正则化、路径长度正则化)到<em class="oi">config-e</em>(v2的完整更新设置)，加上<em class="oi"> config-f </em>，一个更大的网络版本<em class="oi"> config-e </em>和本文中使用的默认设置。<br/>对于我的512大小的数据集，我选择了<em class="oi"> config-e </em>。在<em class="oi"> config-f </em>中增加的复杂性主要是由在训练时探索网络对更高分辨率的隐含关注时注意到的信息瓶颈引起的，因此我的基本原理是增加的复杂性只会对训练稳定性更加有害。</p><p id="fe4a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">微调</strong>用于节省时间，依赖于先前模型已经学习的结构，并试图使这样的结构适应新的目标数据集。我也见过<a class="ae kz" href="https://twitter.com/Norod78" rel="noopener ugc nofollow" target="_blank">人</a>使用与原始数据集不完全相关的图像进行微调，所以这很大程度上取决于你的目标。微调提供了关于训练质量的更快的视觉反馈，因此更容易运行多次迭代和实验，这是最初了解更多模型行为和超参数的好方法。</p><p id="77ce" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">要微调现有网络，需要指定要重新启动的快照。对于v2，这是在<a class="ae kz" href="https://github.com/NVlabs/stylegan2/blob/master/run_training.py" rel="noopener ugc nofollow" target="_blank"><em class="oi">run _ training . py</em></a>中完成的，通过添加和修改以下内容</p><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="cd95" class="mm lp it nz b gy od oe l of og">train.resume_pkl = "your_network_snapshot.pkl"  # Network pickle to resume training from, None = train from scratch.<br/>train.resume_kimg = 0.0  # Assumed training progress at the beginning. Affects reporting and training schedule.<br/>train.resume_time = 0.0  # Assumed wallclock time at the beginning. Affects reporting.</span></pre><p id="c0d2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请注意，根据您的快照指定<code class="fe oj ok ol nz b">resume_kimg</code>是多么重要，因为它用于控制训练计划(如学习率衰减)。</p><h2 id="a93c" class="mm lp it bd lq mn mo dn lu mp mq dp ly km mr ms mc kq mt mu mg ku mv mw mk mx bi translated">监测进展</h2><div class="nd ne nf ng gt ab cb"><figure class="nh ju om nj nk nl nm paragraph-image"><img src="../Images/fff855baf52e01b3cdb7c87351feb24c.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/1*GgvuMoiEjh5stXyJYLDqYw.gif"/></figure><figure class="nh ju on nj nk nl nm paragraph-image"><img src="../Images/5c24a2dd111f16719e21e4ae8eebe838.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*tuZo9umhWbfDOlA_IBRdZw.gif"/><p class="nn no gj gh gi np nq bd b be z dk oo di op nt translated">StyleGAN v1(左)和v2(右)的训练进度对比</p></figure></div><p id="3cfa" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在训练时，我看到的主要<strong class="kd iu">度量</strong>是<a class="ae kz" href="https://nealjean.com/ml/frechet-inception-distance/" rel="noopener ugc nofollow" target="_blank"> Frechet初始距离(FID) </a>，默认情况下，它是在50k图像上每50个训练刻度计算一次。如<a class="ae kz" href="https://github.com/NVlabs/stylegan2#evaluation-metrics" rel="noopener ugc nofollow" target="_blank"> repo自述文件</a>中所述，可以调用其他指标。注意，Tensorflow日志是在目标目录中生成的，可以通过运行tensorboard来访问</p><pre class="nd ne nf ng gt ny nz oa ob aw oc bi"><span id="7091" class="mm lp it nz b gy od oe l of og">tensorboard --logdir &lt;LOGS_TARGET_DIR&gt;</span></pre><p id="fca4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">除了FID之外，您还可以在这里检查分数/假和分数/真，这分别是假图像和真图像的鉴别器预测。由于标签最初可能会产生误导，因此值得再次指出，这些不是损失值，而是鉴别器的纯预测输出。</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oq"><img src="../Images/778633cf6ccfaf98d27c1fd90ddbc475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ppSRovYWGpfIHGr9eTc1Rg.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">张量板图</p></figure><p id="a0a0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我试图从这些图表中获得一些指导，但发现很难提取网络运行情况的精确信息。理想情况下，人们希望鉴别器不能区分真假，因此趋向于零，因为这只是随机猜测。其他次要指标也是基于损失选择。例如，默认情况下，鉴别器使用带有R1正则化的<a class="ae kz" href="https://arxiv.org/pdf/1801.04406.pdf" rel="noopener ugc nofollow" target="_blank">逻辑损失，</a>提供梯度罚分图。</p><p id="4c46" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于我的数据集，FID很好地映射到个人感知的图像质量，但由于度量是基于从inception v3模型中提取的特征之间的汇总统计比较来计算的，因此它可能对其他类型的数据集没有帮助，特别是度量值越趋于零。<br/>我仍然建议避免在短期训练期间进行视觉比较，因为我们的主观近似判断往往会因我们所观察的特定示例或甚至仅因噪声贡献而产生偏差，尽管假样本中没有可察觉的变化，但网络可能仍在改进。</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi or"><img src="../Images/fa7677da92f606d691ce6d442f98d033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IIN-LCZWtHLBbjx7XZsevg.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">约10，000公里后为鞋类数据集生成的图像</p></figure><p id="ceb6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">另一方面，通过视觉检查，人们可以很容易地发现像<strong class="kd iu">模式崩溃</strong>这样的问题，如以下为穆卡数据集生成的假样本所示。</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi os"><img src="../Images/3fc7aaf10b14617f1166c5c7bb88b607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LWED_6thcLdNL8dL7SCB7w.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">穆卡数据集的部分模式折叠示例</p></figure><p id="69ea" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是部分模式崩溃的一个例子，生成器被减少到只生成几个例子，在多样性方面有所损失。造成这种情况的一个主要原因可能是数据集的规模较小，再加上常见的重复条目。</p><p id="caf4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请参见下面的Franzetta art示例，尤其是与训练曲线相关的示例。虽然有些条目可能会给人模式崩溃的印象，但更有可能的是，对于该特定示例，训练数据集中的重复项太多了。</p><div class="nd ne nf ng gt ab cb"><figure class="nh ju ot nj nk nl nm paragraph-image"><img src="../Images/9a1cd2fa568a924ec5b3022dedcee115.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*7A5bk1SLvv_w3bbv1tTc4Q.png"/></figure><figure class="nh ju ou nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/853edb934de0a7bc3874b995c293bb79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*K6zFwBeGlFS7quIx16tuew.png"/></div><p class="nn no gj gh gi np nq bd b be z dk ov di ow nt translated">Franzetta数据集的FID50k曲线，带有在最后一步生成的假图像样本</p></figure></div><h2 id="2418" class="mm lp it bd lq mn mo dn lu mp mq dp ly km mr ms mc kq mt mu mg ku mv mw mk mx bi translated">调整和超参数调谐</h2><p id="e919" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">一个像样的数据集通常应该提供一个遵循<a class="ae kz" href="https://en.wikipedia.org/wiki/Pareto_distribution" rel="noopener ugc nofollow" target="_blank">帕累托分布</a>的良好的训练曲线。<a class="ae kz" href="https://github.com/NVlabs/stylegan2#training-networks" rel="noopener ugc nofollow" target="_blank">官方自述文件</a>指定了不同配置的训练时间，但是正如所说的，你将在训练的第一部分获得大部分的改进；棘手和繁琐的部分是试图达到更好的质量细节。</p><p id="e366" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">平台是一种常见的情况，模型很少或没有改进。另一种可能但不太常见的行为是发散或爆炸，在初始步骤改进后，模型开始恶化，并以指数形式退化为纯噪声结果。</p><p id="681d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">针对这种情况采取的行动是从先前的检查点重新训练(如先前的<em class="oi">微调</em>部分所述)，根据识别的问题合理地调整超参数。正如经常提到的，这本身就是一门艺术，强烈地基于直觉、经验和运气。<br/>根据目标、模型状态等，有多种可能性..以下是一些最常见的建议，并附有一些经验笔记。</p><ul class=""><li id="b291" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">像往常一样，如果你的数据集在大小或噪音方面有明显的缺陷，花些精力去修复它们，而不是冒险在模型调优上浪费时间</li><li id="d177" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated"><strong class="kd iu"> learning-rate (lr) </strong>，在这里您可以降低lr以获得潜在的更稳定(但缓慢)的训练进度。您还可以根据架构中可能存在的两个组件之间的不平衡，单独调整鉴频器和发生器的lr。对此，<a class="ae kz" href="https://twitter.com/theshawwn" rel="noopener ugc nofollow" target="_blank"> Shawn Presser </a>在推特上指出<a class="ae kz" href="https://twitter.com/theshawwn/status/1230022825538248704" rel="noopener ugc nofollow" target="_blank">v2其实有一个bug </a>，针对这个bug，生成器lr值被错误地用作鉴别器lr。在试验G/D lr平衡之前，一定要修补这个部分。我试着增加lr，只是为了搞笑，但总是以快速突然的发散结束。</li><li id="0992" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated"><strong class="kd iu">批量</strong>，增加批量也可以提供更稳定的训练或从局部最小值突破。这是基于这样的想法，即随着越来越多的预测用于计算梯度，权重更新和训练方向将越精确。我用更大的minibatch运行的多个实验迫使FID50k增加了一个很好的百分点，但其中一些很快变得不稳定并爆炸。此外，请注意您的设置的内存限制，v2更昂贵，1024分辨率的培训需要至少16 GB内存的GPU。<br/>值得指出的是，StyleGAN有两个不同的批量参数，<code class="fe oj ok ol nz b">minibatch_size_base</code>和<code class="fe oj ok ol nz b">minibatch_gpu_base</code>。前者是要使用的实际最大minibatch大小，而后者是单个GPU一次处理的大小。这意味着这两个参数需要根据您的设置的GPU数量进行调整，因为梯度会累积，直到达到基本迷你批次大小。</li></ul><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ox"><img src="../Images/bb7cf92082c5b41b5396838464582e44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pGM2A1M4mydn6_j0iHI3hg.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">训练曲线:用相同的数据集测试不同的超参数</p></figure><p id="33e7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">关于这些方面，我想再次建议<a class="ae kz" href="https://www.gwern.net/Faces" rel="noopener ugc nofollow" target="_blank"> Gwern StyleGAN v1 post </a>，因为它提供了深入的细节和参考，如果你想进一步探索GAN-training，还有三篇额外的优秀论文:<a class="ae kz" href="http://arxiv.org/abs/1910.00927" rel="noopener ugc nofollow" target="_blank">稳定生成性对抗网络训练:调查(2019) </a>，<a class="ae kz" href="http://arxiv.org/abs/1801.04406" rel="noopener ugc nofollow" target="_blank">GAN的哪些训练方法实际上是趋同的？(2018) </a>，<a class="ae kz" href="https://arxiv.org/abs/1606.03498" rel="noopener ugc nofollow" target="_blank">训练甘斯的改进技术(2016) </a>。</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oy"><img src="../Images/4b73b5c9be8d15077b52759bbaebe688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dIJvH8Ul1yEikgFpSXThBw.png"/></div></div></figure><p id="9bb2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一些行为仍然困扰着我，比如在我从之前的检查点(相同的配置)重新开始训练后，是什么导致了损失分数的变化。</p><h1 id="ea05" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">潜在空间探索</h1><div class="nd ne nf ng gt ab cb"><figure class="nh ju ni nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/1eec3c7dff7ed94e0ee44637220eab04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*PXxyXvfaKM2j8fpnm-riNQ.gif"/></div></figure><figure class="nh ju ni nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/042690db3f4cf4e9dafec0311217f834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*bxyzYhYaWbwER1fYV0Bazg.gif"/></div><p class="nn no gj gh gi np nq bd b be z dk nr di ns nt translated">服装和鞋类数据集的结果</p></figure></div><h2 id="d403" class="mm lp it bd lq mn mo dn lu mp mq dp ly km mr ms mc kq mt mu mg ku mv mw mk mx bi translated">漫步在林荫道上</h2><p id="b03f" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">以上是我的服装和鞋款潜在空间探索的一些例子。其思想只是生成N个样本向量(从高斯分布中提取)并使用任何优选的转换函数在它们之间顺序转换。在我的例子中，这个函数只是一个固定帧数的线性插值(相当于变形)。</p><p id="dd53" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请注意，我们依赖于非常初始的潜在向量<em class="oi"> z. </em>这意味着我们使用StyleGAN映射网络来首先生成潜在向量<em class="oi"> w </em>，然后使用<em class="oi"> w </em>来合成新图像。由于这个原因，我们可以依靠截断技巧，丢弃潜在空间中表现不佳的区域。我们想要指定所生成的中间向量<em class="oi"> w </em>必须接近平均值多少(基于映射网络的随机输入来计算)。ψ (psi)值衡量了<em class="oi"> w </em>与平均值的偏差，因此可以针对质量/品种的权衡进行调整。ψ=1相当于没有截断(原始的<em class="oi"> w </em>),而值越接近0，我们越接近平均值，质量有所提高，但多样性有所减少。</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/c99126f8c4b5078e2b478c92a5b0351f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*q_SMak7NjmslsTgQ3s425Q.gif"/></div><p class="nn no gj gh gi np nq bd b be z dk translated">截断技巧在起作用，这里线性间隔从-0.5(左上)到1.5(右下)</p></figure><h2 id="5eb9" class="mm lp it bd lq mn mo dn lu mp mq dp ly km mr ms mc kq mt mu mg ku mv mw mk mx bi translated">编码真实图像</h2><p id="7aa6" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">我们经常希望能够获得关于目标模型的真实图像的代码/潜在向量/嵌入，换句话说:我应该馈送给我的模型的输入值是什么，以生成我的图像的最佳近似。</p><p id="4a37" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一般来说，有两种方法:</p><ul class=""><li id="f5c3" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">通过网络的编码器组件传递图像</li><li id="6f49" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">优化潜在(使用梯度下降)</li></ul><p id="e776" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">前者提供了一个快速的解决方案，但在训练数据集之外推广时存在问题，不幸的是，对我们来说，它不是现成的标准样式。该架构根本不学习显式编码功能。</p><p id="4563" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们只剩下使用感知损失的<strong class="kd iu">潜在优化选项</strong>。我们为参考和生成的图像提取高级特征(例如，从像VGG这样的预训练模型中)，计算它们之间的距离，并优化潜在表示(我们的目标代码)。该目标代码的<strong class="kd iu">初始化对于效率和效果来说是一个非常重要的方面。最简单的方法是简单的随机初始化，但可以做很多事情来改进这一点，例如通过学习从图像到潜像的显式编码函数。想法是随机生成一组N个示例，并存储生成的图像和生成它的代码。然后，我们可以在这些数据上训练一个模型(例如ResNet ),并在实际的StyleGAN编码过程之前使用它来初始化我们的latent。参见<a class="ae kz" href="https://github.com/rolux/stylegan2encoder/issues/2" rel="noopener ugc nofollow" target="_blank">关于改进初始化的丰富讨论</a>。</strong></p><p id="3dc0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">用于v1的<a class="ae kz" href="https://github.com/Puzer/stylegan-encoder" rel="noopener ugc nofollow" target="_blank">编码器</a>和用于v2的<a class="ae kz" href="https://github.com/rolux/stylegan2encoder" rel="noopener ugc nofollow" target="_blank">编码器</a>为该操作提供代码和分步指南。我还推荐以下两篇文章:<a class="ae kz" href="http://arxiv.org/abs/1904.03189" rel="noopener ugc nofollow" target="_blank"> Image2StyleGAN </a>和<a class="ae kz" href="http://arxiv.org/abs/1911.11544" rel="noopener ugc nofollow" target="_blank"> Image2StyleGAN++和</a>，它们很好地概述了StyleGAN的图像编码，考虑了初始化选项和潜在空间质量，并分析了图像编辑操作，如变形和样式混合。</p><h2 id="f169" class="mm lp it bd lq mn mo dn lu mp mq dp ly km mr ms mc kq mt mu mg ku mv mw mk mx bi translated">w(1)对w(N)</h2><p id="5d3a" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">StyleGAN使用映射网络(八个完全连接的层)将输入噪声(<code class="fe oj ok ol nz b">z</code>)转换为中间潜在向量(<code class="fe oj ok ol nz b">w</code>)。两者的大小都是512，但是中间向量是为每个样式层复制的。对于在1024尺寸图像上训练的网络，该中间向量将是形状(512，18)，对于512尺寸，它将是(512，16)。</p><p id="7f35" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">编码过程通常在这个中间向量上完成，因此可以决定是优化<code class="fe oj ok ol nz b">w(1)</code>(意味着只有一个512层，然后根据需要平铺到每个样式层)还是整个<code class="fe oj ok ol nz b">w(N)</code>。官方放映员操作前者，而改编通常依赖于单独优化所有<code class="fe oj ok ol nz b">w</code>条目，以达到视觉保真度。关于这个话题，请参见<a class="ae kz" href="https://twitter.com/robertluxemburg/status/1216854802254254082" rel="noopener ugc nofollow" target="_blank">这个Twitter帖子</a>。</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pa"><img src="../Images/a4f43d56320d55eee0fee1a490ee2d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Qt5ljUolVks4MPrnoc1tRA.gif"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">使用Nvidia FFHQ模型投影到w(N)和w(1)的比较</p></figure><p id="e9ef" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当投影不适合模型训练分布的参考时，更引人注目，就像在下面的示例中为FFHQ模型投影服装图像。</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pa"><img src="../Images/ef186256b838026f4108aa293ddcfcfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*tkJM3WELngU3bdxBm5MxHQ.gif"/></div></div></figure><p id="0e29" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一般来说，人们总是会注意到，对于高分辨率，投影仪似乎无法匹配参考图片的精细细节，但这很可能是使用256x256分辨率的感知损失的结果，如<a class="ae kz" href="https://twitter.com/quasimondo/status/1226801679208718336" rel="noopener ugc nofollow" target="_blank">本主题</a>所示。</p><h2 id="4475" class="mm lp it bd lq mn mo dn lu mp mq dp ly km mr ms mc kq mt mu mg ku mv mw mk mx bi translated">学习方向</h2><p id="1da9" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">StyleGAN对潜在空间解缠的改进允许以令人愉快的正交方式探索数据集的单个属性(意味着不影响其他属性)。<br/>鉴别模型学习区分目标属性的界限(例如，男性/女性、微笑/不微笑、猫/狗)，而我们感兴趣的是跨越这些界限，垂直于它们移动。例如，如果我从一张悲伤的脸开始，我可以缓慢但稳定地移动到同一张脸的微笑版本。</p><p id="5cbd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这应该已经提供了如何学习新方向的提示。我们首先从我们的模型中收集多个样本(图像+潜像),并针对我们的目标属性对图像进行手动分类(例如，微笑与不微笑),试图保证适当的类别表示平衡。然后，我们训练一个模型来对我们的潜在客户和人工标签进行分类或回归。此时，我们可以使用这些支持模型的学习功能作为过渡方向。</p><p id="40d8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://twitter.com/robertluxemburg" rel="noopener ugc nofollow" target="_blank">罗伯特·卢森堡</a>分享了【FFHQ官方车型的学习方向。</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/b748272cb228903d46746fae73806327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/0*KKe7POwK2ZFTfFdd"/></div><p class="nn no gj gh gi np nq bd b be z dk translated">样从玩弄微笑潜向</p></figure><p id="f58e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://arxiv.org/abs/2003.03581" rel="noopener ugc nofollow" target="_blank">用于前馈图像处理的StyleGAN2蒸馏</a>是一篇非常新的论文，通过在StyleGAN生成的不成对数据集上训练的“学生”图像到图像网络来探索方向处理。该论文旨在克服编码性能瓶颈，并学习能够有效地应用于真实世界图像的变换函数。</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pc"><img src="../Images/032d030b173dfdeabb3e36310aaa00ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Jul2ZaG51UV9SVu_.jpg"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">来自论文“用于前馈图像处理的StyleGAN2蒸馏”的例子</p></figure><h1 id="e291" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">结论</h1><p id="59fd" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">我的很多实验最初都是为了评估StyleGAN模型学习的潜在空间有多好(<strong class="kd iu">表示学习</strong>)，以及获得的嵌入对于下游任务(例如，通过线性模型的图像分类)有多好。一方面，我将继续致力于这种评估，试图涵盖其他生成模型类型，如<a class="ae kz" href="http://www.offconvex.org/2018/07/27/approximating-recurrent/" rel="noopener ugc nofollow" target="_blank">自回归</a>和<a class="ae kz" href="https://openai.com/blog/glow/" rel="noopener ugc nofollow" target="_blank">基于流程的模型</a>。</p><p id="481f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我也有兴趣探索这种模型的纯图像合成能力，以及有效的语义混合和编辑的日益增长的潜力，特别是与我对<a class="ae kz" href="https://www.instagram.com/amartinelli1/" rel="noopener ugc nofollow" target="_blank">绘画、数字绘画和动画</a>的热情有关的方面。<a class="ae kz" href="https://github.com/lllyasviel/style2paints" rel="noopener ugc nofollow" target="_blank">自动线条画着色</a>、<a class="ae kz" href="https://www.ebsynth.com/" rel="noopener ugc nofollow" target="_blank">动画绘画</a>、<a class="ae kz" href="https://grisk.itch.io/dain-app" rel="noopener ugc nofollow" target="_blank">帧插值</a>已经有了一些很棒的免费工具，但是对于<a class="ae kz" href="https://medium.com/@samim/assisted-drawing-7b26c81daf2d" rel="noopener"> <strong class="kd iu">辅助绘图</strong> </a>还有很多可以做的，特别是从更语义的角度来看。实际改进的空间也很大:泛化能力、加快推理时间、训练优化和迁移学习。</p><p id="d655" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">接下来，我还想超越纯粹的二维画布，更多地了解在3D图形领域已经取得的惊人成就。<a class="ae kz" href="https://www.youtube.com/watch?v=fONxsfY9nO0" rel="noopener ugc nofollow" target="_blank">去噪</a>是我现在经常依赖的东西，<a class="ae kz" href="https://rgl.epfl.ch/publications/Loubet2019Reparameterizing" rel="noopener ugc nofollow" target="_blank">可微渲染</a>只是让我意乱情迷，为了闭环，又回到了<a class="ae kz" href="https://marian42.de/article/shapegan/" rel="noopener ugc nofollow" target="_blank"> GAN进行连续的3D形状生成</a>。</p><p id="c776" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">引用科学家卡罗里·索尔奈-费希尔</p><blockquote class="pd pe pf"><p id="6a2f" class="kb kc oi kd b ke kf kg kh ki kj kk kl pg kn ko kp ph kr ks kt pi kv kw kx ky im bi translated">“多么美好的活着的时光”</p></blockquote></div><div class="ab cl pj pk hx pl" role="separator"><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po"/></div><div class="im in io ip iq"><blockquote class="pq"><p id="e9cb" class="pr ps it bd pt pu pv pw px py pz ky dk translated">你可以在<a class="ae kz" href="https://github.com/5agado/data-science-learning" rel="noopener ugc nofollow" target="_blank"> Github </a>上查看我的代码，在<a class="ae kz" href="https://twitter.com/5agado" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我的更多实验和解释，并在<a class="ae kz" href="https://www.instagram.com/amartinelli1/" rel="noopener ugc nofollow" target="_blank"> Instagram </a>上查看我的图形结果。</p></blockquote></div><div class="ab cl pj pk hx pl" role="separator"><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po"/></div><div class="im in io ip iq"><p id="7037" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> <em class="oi">免责声明</em> </strong> <em class="oi">:目前我不能分享数据集和训练模型。抱歉。</em></p></div></div>    
</body>
</html>