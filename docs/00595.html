<html>
<head>
<title>TinyBERT for Search: 10x faster and 20x smaller than BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于搜索的TinyBERT:比BERT快10倍，小20倍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tinybert-for-search-10x-faster-and-20x-smaller-than-bert-74cd1b6b5aec?source=collection_archive---------19-----------------------#2020-01-17">https://towardsdatascience.com/tinybert-for-search-10x-faster-and-20x-smaller-than-bert-74cd1b6b5aec?source=collection_archive---------19-----------------------#2020-01-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d705" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">加速谷歌用来回答你的问题的算法，这样它就可以在标准的CPU上运行</h2></div><p id="7311" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">合著</em> <a class="ae lc" href="https://medium.com/@colethienes" rel="noopener"> <em class="lb">科尔梯恩梯</em> </a></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/043003eadc4fbc16110b1d5bfba58bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6NKI51E3v2Y8rd68XS-pAA.jpeg"/></div></div></figure><p id="a775" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最近，<a class="ae lc" href="https://www.blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">谷歌推出了一种理解搜索并决定你看到哪些结果的新方法。这种方法基于流行的开源转换器BERT，使用语言理解来理解搜索背后的含义，这是传统的关键字方法无法做到的。</a></p><p id="5466" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们构建了<a class="ae lc" href="https://github.com/koursaros-ai/nboost" rel="noopener ugc nofollow" target="_blank"> NBoost </a>来让非Google用户也能轻松使用高级搜索排名模型，并在此过程中开发了TinyBERT for search，我将在本文中介绍它。</p><blockquote class="lp lq lr"><p id="c372" class="kf kg lb kh b ki kj jr kk kl km ju kn ls kp kq kr lt kt ku kv lu kx ky kz la ij bi translated">特别是对于更长、更具对话性的查询，或者像“for”和“to”这样的介词对意义非常重要的搜索，<strong class="kh ir"> Search将能够理解您查询中单词的上下文。</strong>你可以用自己觉得自然的方式搜索。<br/> - Pandu Nayak，谷歌搜索副总裁</p></blockquote><h1 id="b0c2" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">让伯特变得更小更快</h1><p id="babe" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">BERT已经被证明可以改善搜索结果，但是有一个问题:运行这些查询理解模型需要大量的计算机。当速度很重要并且需要处理数百万次搜索时，这一点尤为重要。这一挑战如此艰巨，以至于谷歌甚至建立了自己的硬件来运行这些模型。而且他们用来在生产中运行这些TPU的代码是私有的，所以其他任何想运行它的人都要倒霉了。</p><p id="be32" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了在标准硬件上运行这些模型，我们使用<a class="ae lc" href="https://nervanasystems.github.io/distiller/knowledge_distillation.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">知识提炼</strong> </a>，一个更大的<strong class="kh ir">教师</strong>网络被用来训练一个更小的<strong class="kh ir">学生</strong>网络的过程，该网络保持了大部分的准确性，但使用了更少、通常更小的层，使其<em class="lb">更小</em>和<em class="lb">更快</em>。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ms"><img src="../Images/cee37326180b98152282930b5278e86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gr1YgHZHQK6eSOp9.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated"><a class="ae lc" href="https://nervanasystems.github.io/distiller/knowledge_distillation.html" rel="noopener ugc nofollow" target="_blank">https://nerv anasystems . github . io/distiller/knowledge _ distillation . html</a></p></figure></div><div class="ab cl mx my hu mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="ij ik il im in"><h1 id="0f82" class="lv lw iq bd lx ly ne ma mb mc nf me mf jw ng jx mh jz nh ka mj kc ni kd ml mm bi translated">TinyBERT建筑</h1><p id="8419" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">我们使用来自<a class="ae lc" href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT" rel="noopener ugc nofollow" target="_blank">这个报告</a>的代码进行知识提炼，并对其进行修改，用于在<a class="ae lc" href="http://www.msmarco.org/" rel="noopener ugc nofollow" target="_blank">女士马可</a>数据集上进行训练和评估。我们最初用MS Marco训练三元组在PyTorch中训练了一个教师bert-base-uncased网络。然后我们用它作为老师训练一个更小的学生BERT网络，只有<strong class="kh ir"> 4个隐含层</strong>而不是标准的<strong class="kh ir"> 12个</strong>。此外，这些层中的每一层都只有<strong class="kh ir"> 312 </strong>的尺寸，而不是<strong class="kh ir"> 768 </strong>的尺寸，使得模型更加轻便。我们在BERT的末尾使用前馈二进制分类层来产生搜索排名的分数。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nj"><img src="../Images/aff1ead4572163c1e95d0056dc6a759d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CmBtAKVB-kXFZhgN5QszYg.jpeg"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">BERT用于对(问题、答案)或(搜索、搜索结果)对进行搜索评分，然后根据这些评分对结果进行排名</p></figure><p id="6e7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是我们使用的tinyBERT架构的示例<em class="lb"> bert_config.json </em>，与标准bert_config的显著差异以粗体显示。</p><pre class="le lf lg lh gt nk nl nm nn aw no bi"><span id="e687" class="np lw iq nl b gy nq nr l ns nt">{<br/> “attention_probs_dropout_prob”: 0.1,<br/> “cell”: {},<br/> <strong class="nl ir">“emb_size”: 312,</strong><br/> “hidden_act”: “gelu”,<br/> “hidden_dropout_prob”: 0.1,<br/> “hidden_size”: 312,<br/> “initializer_range”: 0.02,<br/><strong class="nl ir"> “intermediate_size”: 1200,</strong><br/> “max_position_embeddings”: 512,<br/> “num_attention_heads”: 12,<br/><strong class="nl ir"> “num_hidden_layers”: 4,</strong><br/> “pre_trained”: “”,<br/> “structure”: [],<br/> “type_vocab_size”: 2,<br/> “vocab_size”: 30522<br/>}</span></pre></div><div class="ab cl mx my hu mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="ij ik il im in"><h1 id="94d7" class="lv lw iq bd lx ly ne ma mb mc nf me mf jw ng jx mh jz nh ka mj kc ni kd ml mm bi translated">评估模型</h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nu"><img src="../Images/a4f5b62c2cbc0167bedc20f5ed8403b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNARi07tZz6dE1Ats5ZRgQ.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">[1] MRR对BM25排名前50的结果。宾查询指的是马尔科女士。K80 GPU上的速度。</p></figure><p id="0afe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lc" href="http://msmarco.org" rel="noopener ugc nofollow" target="_blank">Marco女士</a>是真实世界搜索引擎使用数据的最大公开来源，使其成为评估搜索和问答模型的理想选择。<strong class="kh ir">它显示真实世界的必应搜索结果，以及用户最终点击了什么的信息</strong>。当<a class="ae lc" href="https://arxiv.org/abs/1904.07531" rel="noopener ugc nofollow" target="_blank">伯特基地第一次在MSMarco </a>上使用的时候，它以<strong class="kh ir"> 0.05 MRR </strong>的成绩击败了最先进水平(很多)。基于BERT的解决方案仍然位居排行榜首位。我们的目标是找到一种方法，从一个在现实世界中使用足够快的模型中实现这种提升。</p><p id="deb8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">进来吧，TinyBERT。虽然在重新排名方面不如BERT Base有效，但我们的实验表明，它保留了BERT Base 的90%的MRR分数(0.26比0.29，从BM25重新排名前50)，同时使模型<strong class="kh ir"> ~快10倍</strong>，<strong class="kh ir"> ~小20倍。然而，基于马尔科女士等学术基准的结果往往缺乏现实世界的普遍性，因此应该持保留态度。</strong></p></div></div>    
</body>
</html>