<html>
<head>
<title>The Evolution of AlphaGo to MuZero</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AlphaGo向MuZero的进化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-evolution-of-alphago-to-muzero-c2c37306bf9?source=collection_archive---------8-----------------------#2020-01-17">https://towardsdatascience.com/the-evolution-of-alphago-to-muzero-c2c37306bf9?source=collection_archive---------8-----------------------#2020-01-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ea66" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">DeepMind的MuZero算法在57款不同的Atari游戏中达到了超人的能力。这篇文章将解释导致它的背景！</h2></div><p id="c9b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DeepMind最近发布了他们的MuZero算法，在57款不同的Atari游戏中以超人的能力为标题。</p><p id="e9f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以玩雅达利游戏的强化学习代理很有趣，因为除了视觉上复杂的状态空间，玩雅达利游戏的代理没有一个<strong class="kk iu">完美的模拟器</strong>，它们可以像在国际象棋、日本象棋和围棋中那样用于规划。</p><p id="4910" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种“<strong class="kk iu">完美模拟器</strong>的想法是限制AlphaGo及其后续改进(如AlphaGo Zero和AlphaZero)的关键之一，仅限于国际象棋、Shogi和Go，而<strong class="kk iu">对某些现实世界应用(如机器人控制)毫无用处</strong>。</p><p id="ebc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">强化学习问题包含在下面描述的马尔可夫决策过程(MDP)中:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/afd47256187158ffd81169f95d671ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P8mOMhYz6lrfl3afRsIDNg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">国际象棋、围棋和松木带有一个知道如何操作的模拟器</p></figure><p id="6fa6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaGo、AlphaGo Zero、AlphaZero和MuZero的算法家族通过使用<strong class="kk iu">规划</strong>扩展了这一框架，如下图所示:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/106a97bac1738649d188cc7f5b57b27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*RBz39wnkO9eniB2_mzyfEA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">整合规划扩展了强化学习问题的框架</p></figure><p id="1456" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DeepMind的AlphaGo、AlphaGo Zero和AlphaZero利用了(动作、状态)→下一个状态的完美模型，以<strong class="kk iu">蒙特卡罗树搜索(MCTS) </strong>的形式进行前瞻规划。MCTS是使用深度神经网络进行策略映射和价值估计的完美补充，因为它平均了这些函数近似的误差。MCTS为AlphaZero在国际象棋、围棋和围棋方面提供了巨大的推动力，在那里你可以做<strong class="kk iu">完美的规划</strong>，因为你有一个完美的环境模型。</p><p id="486e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">穆泽罗通过学习下面描述的动力学模型，提供了一种挽救MCTS计划的方法:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/a6b89222ba1315ecfd35dbfa66ce4239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*m9vkY9HRpfCA9EJSLl2MGg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">穆泽罗的蒙特卡洛树搜索</p></figure><p id="917a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">MuZero的基于模型的强化学习方法，具有从(s，a)→(s’，r)的参数模型图，在于它<strong class="kk iu">没有精确地重建s’</strong>处的像素空间。与Ha和Schmidhuber的“世界模型”中的图片形成对比:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lw"><img src="../Images/0a57c44f764a24b93a577a34e11c0f13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ynAs03NRe-cz6oBC9l_H-w.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">基于模型的RL在模型中重建像素空间的例子。图片来自:<a class="ae lx" href="https://worldmodels.github.io/" rel="noopener ugc nofollow" target="_blank">https://worldmodels.github.io/</a></p></figure><p id="3680" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">MuZero的这种规划算法在Atari领域非常成功，在强化学习问题上有巨大的应用潜力。本文将解释AlphaGo、AlphaGoZero、AlphaZero和MuZero的发展，以便更好地理解MuZero的工作方式。如果你感兴趣，我还制作了一个视频来解释这一点:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ly lz l"/></div></figure><h1 id="1918" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">AlphaGo</h1><p id="a273" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">AlphaGo是该系列的第一篇论文，表明深度神经网络可以通过预测<strong class="kk iu">策略</strong>(从状态到行动的映射)和<strong class="kk iu">价值估计</strong>(从给定状态获胜的概率)来玩围棋。这些策略和值网络用于通过选择从给定状态采取哪些动作以及哪些状态值得进一步探索来增强基于树的前瞻搜索。</p><p id="14c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaGo使用4个深度卷积神经网络，3个策略网络和一个价值网络。策略网络中的2个使用专家移动的<strong class="kk iu">监督学习</strong>进行训练。</p><p id="6b68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">监督学习描述了由某种L(y '，y)组成的损失函数。在这种情况下，y’是策略网络从给定状态预测的动作，而y是专家玩家在该状态下已经采取的动作。</p><p id="eadc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首次展示策略是一个较小的神经网络，它也接受较小的输入状态表示。因此，与高容量网络相比，首次展示策略具有明显较低的专家移动建模准确度。然而，首次展示策略网络的推理时间(对给定状态的动作进行预测的时间)是2微秒，相比之下，大型网络的推理时间是3毫秒，这使得它对于蒙特卡罗树搜索模拟非常有用。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/aea31e732c84cc157e5a49e8781a94c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*sycPq0SfiUjEASNOENJXkQ.png"/></div></figure><p id="2121" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">S1策略网络用于初始化第三策略网络，该第三策略网络通过自播放和策略梯度来训练。与学习价值函数然后使策略相对于价值函数贪婪的其他RL算法相比，策略梯度描述了直接相对于结果奖励优化策略的思想。策略梯度训练的策略网络与其自身参数的先前迭代进行对抗，优化其参数以选择导致胜利的移动。<strong class="kk iu">自玩数据集</strong>随后用于训练价值网络，以从给定状态预测游戏的赢家。</p><p id="1420" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaGo最后的主力是MCTS政策和价值网络的结合，如下图所示:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi my"><img src="../Images/69764baa19736599d8746238cb641664.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YmmFO1seK7q-P0FrEZ_OrA.png"/></div></div></figure><p id="f353" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">MCTS的想法是执行前瞻搜索，以更好地估计要立即采取的行动。这是通过从一个根节点(棋盘的当前状态)开始，通过选择一个动作来扩展该节点，并对状态“动作转换”产生的后续状态重复这一操作来完成的。MCTS基于这个<strong class="kk iu"> Q + u(P) </strong>项来选择沿着树的哪个边，这个项是价值网络对状态的估计、策略网络给予该状态的原始概率密度、以及该节点被访问过多少次的负加权的加权组合，因为这被一次又一次地重复。AlphaGo的独特之处在于使用首次展示策略模拟来平均价值网络的贡献。首次展示策略将一直模拟，直到导致成功或失败的情节与带有额外参数lambda的该状态的价值函数估计相混合。</p><h1 id="5f91" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">AlphaGo Zero</h1><p id="2857" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">AlphaGo Zero通过使AlphaGo算法更加通用，并从<strong class="kk iu">“零”人类知识</strong>出发，显著改进了AlphaGo算法。AlphaGo Zero避免了专家走法初始化的监督学习，并将值和策略网络结合到单个神经网络中。与AlphaGo中更简单的卷积网络相比，这个神经网络也是按比例放大的，以利用一个<strong class="kk iu"> ResNet </strong>。执行值和策略映射的ResNet的贡献在下图中是显而易见的，该图比较了双重任务ResNet和独立任务CNN:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ee07b99ba964b1c9d0d1bce4b2fdab1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*bzd7rqTw-sfoauBxLSM20Q.png"/></div></figure><p id="751c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaGo Zero最有趣的特征之一是它使用MCTS发现的动作分布来训练其策略网络的方式，如下所示:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi na"><img src="../Images/ce9b95a1b2ab14aada65d9c194d659a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fb1zutdoVskZEPa7_y2sQQ.png"/></div></div></figure><p id="a4c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">MCTS通过将其用作监督来训练策略网络，以更新策略网络。这是一个聪明的想法，因为与政策网络从状态到行动的即时映射相比，MCTS通过前瞻搜索产生了更好的行动分布。</p><h1 id="3c44" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">阿尔法零</h1><p id="55f1" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">AlphaZero是将AlphaGo家族推广到围棋之外的第一步，着眼于下棋和下日本象棋所需的变化。这需要公式化残差神经网络的<strong class="kk iu">输入状态和输出动作</strong>表示。</p><p id="63f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在AlphaGo中，状态表示使用一些手工制作的特征平面，如下所示:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nb"><img src="../Images/c4b9ba534d5bc0f2bfee21b9ba7ea1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YZfdt1WHWB1kllQtvHuKBg.png"/></div></div></figure><p id="18a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaGo Zero使用一种更通用的表示法，简单地传入两个玩家的前8个棋子位置和一个二元特征平面，告诉代理它正在控制哪个玩家，如下所示:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c6e979c924e609f4c58c021c052c33d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*CyzF5KT0B-jQsEN79tGUOA.png"/></div></figure><p id="a98b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaZero使用类似的思想对Chess和Shogi的输入状态表示进行编码，如下所示:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nd"><img src="../Images/474c2cf6b85fefa19591b7cce8ea555c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rGuVf2bubbo5pXYsKDKtA.png"/></div></div></figure><p id="4ed7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaZero还对算法进行了一些更微妙的改变，例如自玩冠军的加冕方式，以及取消围棋棋盘游戏中的数据增强，如反射和旋转。</p><h1 id="9375" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">穆泽罗</h1><p id="03c0" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">这就把我们带到了这个系列目前最先进的，MuZero。MuZero对算法提出了一个非常强大的概括，允许它在没有完美模拟器的情况下学习<strong class="kk iu">。国际象棋、象棋和围棋都是带有完美模拟器的游戏，如果你将你的棋子向前移动2个位置，你就能确切地知道棋盘的最终状态。在OpenAI的魔方手这种复杂灵巧的操控任务中，对给定关节施加30 N的力就不能说是一回事了。</strong></p><p id="216a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图说明了MuZero的主要观点:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ne"><img src="../Images/d650ba11b0c3d02186817b43528fdead.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UfX-gcQkO8Y9e1l4alky0w.png"/></div></div></figure><p id="ecf4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图A示出了使用<strong class="kk iu">表示函数h </strong>将原始观察值映射到用于基于树的规划的隐藏状态s0的流水线。在MuZero中，组合的价值/政策网络在这个<strong class="kk iu">隐藏状态空间</strong>中推理，因此不是将原始观察映射到行动或价值估计，而是将这些隐藏状态作为输入。<strong class="kk iu">动态函数g </strong>学习从隐藏状态和动作映射到未来的隐藏状态。</p><p id="11fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图表B示出了如何通过模仿由MCTS产生的动作分布来类似地训练策略网络，该动作分布首先在AlphaGo Zero中引入。</p><p id="81a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图表C显示了这个系统是如何被训练的。三个神经网络中的每一个都在价值网络和实际回报之间的差异、动态模型经历和预测的中间回报之间的差异以及MCTS行动分布和政策映射之间的差异的<strong class="kk iu">联合优化</strong>中被训练。</p><p id="3334" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">如何在这个优化循环中训练表示函数h？</strong></p><p id="a05a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">表示函数h通过<strong class="kk iu">随时间的反向传播</strong>在该联合优化方程中发挥作用。比方说，您正在计算MCTS行动分布pi(s1)和策略分布p(s1)之间的差值。p(s1)的输出是p(g(s0，a1))的一个结果，p(g(h(raw_input)，a1))的一个结果。这也是时间反向传播将更新信号一路发送回隐藏表示函数的方式。</p><h1 id="48fd" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">alpha go→alpha go Zero→alpha Zero→MuZero</h1><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nf"><img src="../Images/d8a2cdd5c0befc5b68c092db905aa513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*efkx62AUP8UJ99RAa3Mx8Q.png"/></div></div></figure><p id="269d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望这篇文章有助于澄清MuZero如何在以前的算法AlphaGo、AlphaGo Zero和AlphaZero的上下文中工作！感谢阅读！</p><h1 id="b7af" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">纸质链接</h1><p id="265e" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">alpha go:<a class="ae lx" href="https://www.youtube.com/redirect?q=https%3A%2F%2Fwww.nature.com%2Farticles%2Fnature16961&amp;v=A0HX8BgckFI&amp;event=video_description&amp;redir_token=xWFupkVllJha70MV9mB5IiYg3Gt8MTU3OTM3NDk0NEAxNTc5Mjg4NTQ0" rel="noopener ugc nofollow" target="_blank">https://www.nature.com/articles/natur...</a></p><p id="595e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaGo零分:<a class="ae lx" href="https://www.youtube.com/redirect?q=https%3A%2F%2Fwww.nature.com%2Farticles%2Fnature16961&amp;v=A0HX8BgckFI&amp;event=video_description&amp;redir_token=xWFupkVllJha70MV9mB5IiYg3Gt8MTU3OTM3NDk0NEAxNTc5Mjg4NTQ0" rel="noopener ugc nofollow" target="_blank">https://www.nature.com/articles/natur...</a></p><p id="94f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">阿尔法零:https://arxiv.org/abs/1712.01815<a class="ae lx" href="https://www.youtube.com/redirect?q=https%3A%2F%2Farxiv.org%2Fabs%2F1712.01815&amp;v=A0HX8BgckFI&amp;event=video_description&amp;redir_token=xWFupkVllJha70MV9mB5IiYg3Gt8MTU3OTM3NDk0NEAxNTc5Mjg4NTQ0" rel="noopener ugc nofollow" target="_blank"/></p><p id="f34c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">穆泽罗:https://arxiv.org/abs/1911.08265<a class="ae lx" href="https://www.youtube.com/redirect?q=https%3A%2F%2Farxiv.org%2Fabs%2F1911.08265&amp;v=A0HX8BgckFI&amp;event=video_description&amp;redir_token=xWFupkVllJha70MV9mB5IiYg3Gt8MTU3OTM3NDk0NEAxNTc5Mjg4NTQ0" rel="noopener ugc nofollow" target="_blank"/></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ng"><img src="../Images/e9a07ad9ba8c1aaaacff588e01129e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhIGimngzFRsdDGY3v8_uw.png"/></div></div></figure></div></div>    
</body>
</html>