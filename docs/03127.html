<html>
<head>
<title>Bayesian regression with implementation in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯回归及其R语言实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-regression-with-implementation-in-r-fa71396dd59e?source=collection_archive---------16-----------------------#2020-03-25">https://towardsdatascience.com/bayesian-regression-with-implementation-in-r-fa71396dd59e?source=collection_archive---------16-----------------------#2020-03-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fd87" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从零开始的理论推导，R实现，以及贝叶斯观点的讨论</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e638d1631ceffb0cd5dbb0a438749393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FI1I22a0hVHT5dkltq2t_g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示回归中变量之间依赖关系的概率图形模型(Bishop 2006)</p></figure><p id="d449" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">可以从贝叶斯的角度建立和解释线性回归。第一部分从头开始讨论理论和假设，后面的部分包括R实现和备注。读者可以随意地将这两个代码块复制到R笔记本中，并对其进行研究。</p><h1 id="299a" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">从基础开始</h1><p id="e4ce" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">回想一下，在线性回归中，给我们目标值<strong class="kx ir"> y </strong>，数据<strong class="kx ir"> X，</strong>，我们使用模型</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/3990f1ac809753a40cae5c18b052d37d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2CnSnmME261SEaItR3n8PA.png"/></div></div></figure><p id="325d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中y为N*1向量，X为N*D矩阵，w为D*1向量，误差为N*1向量。我们有N个数据点。维数D是用特征来理解的，所以如果我们用一个x的列表，一个x的列表(和一个1的列表对应w_0)，我们说D=3。如果你不喜欢矩阵的形式，就把它想成是以下内容的浓缩形式，其中所有东西都是一个标量，而不是向量或矩阵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/aa0252bad8ef926b4eaddff7b79cd8a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Mj7_mI4yOMaLQNnKOLsIA.png"/></div></div></figure><p id="4738" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在经典的线性回归中，误差项被假设为正态分布，因此可以直接得出<strong class="kx ir"> y </strong>正态分布，均值<strong class="kx ir"> Xw </strong>，以及误差项具有的任何方差的方差(用σ表示，或具有σ项的对角矩阵)。正态假设在大多数情况下证明是正确的，这个正态模型也是我们在贝叶斯回归中使用的。</p><h1 id="c9e6" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">参数推断</h1><p id="c1cc" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">我们现在面临两个问题:对w的推断和对任何新x的y的预测。使用众所周知的贝叶斯规则和上述假设，我们不仅可以解决这两个问题，还可以给出任何新x的y的全概率分布。下面是使用我们的符号的贝叶斯规则，它表示给定数据的参数w的后验分布:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/e2d0fda80fc8f26bd861bcd5b689f086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ag5vo1q4rlRxeDT8Uy8plA.png"/></div></div></figure><p id="c121" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">π和f是概率密度函数。由于结果是w的函数，我们可以忽略分母，因为分子与左边的常数成正比。我们从假设中知道，似然函数f(y|w，x)遵循正态分布。另一项是w的先验分布，顾名思义，这反映了参数的先验知识。</p><p id="6702" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">先验分布。定义先验是贝叶斯工作流程中一个有趣的部分。为了方便起见，我们设w ~ N(m_o，S_o)，超参数m和S现在反映了w的先验知识。如果你对w知之甚少，或者发现m和S的任何赋值太主观，“无信息”先验是一种修正。在这种情况下，我们将m设置为0，更重要的是将S设置为具有非常大的值的对角矩阵。我们说w有很高的方差，所以我们不知道w会是什么。</strong></p><p id="d81b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在定义了所有这些概率函数之后，几行简单的代数运算(实际上相当多行)将给出N个数据点观察后的后验概率:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/11d49a03f9d1e9275f03009eb5d89bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gZ4f5bVgniXkY5lRYCyVbg.png"/></div></div></figure><p id="7b78" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它看起来像一堆符号，但它们都已经被定义了，一旦这个理论结果在代码中实现，你就可以计算这个分布。(N(m，S)表示具有均值m和协方差矩阵S的正态分布。)</p><h1 id="2c8a" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">预测分布</h1><p id="1d4b" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">完全贝叶斯方法不仅意味着获得单个预测(用y_o，x_o表示新的一对数据)，还意味着获得这个新点的分布。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/b7367da27214164e43b0f4a0a6ac46b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QMnNj60-T3XxHAd3k-_H4Q.png"/></div></div></figure><p id="34de" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们所做的是从第一行的联合边缘化得到边缘分布的反向操作，并在第二行的积分中使用贝叶斯规则，在那里我们也删除了不必要的相关性。注意，我们知道最后两个概率函数是什么。完全预测分布的结果是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/0e96ec1e5e660df139b18f16df633866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8VhX7mD8RhTa_tJs6ixz8A.png"/></div></div></figure><h1 id="b926" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">R中的实现</h1><p id="e508" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">用R实现相当方便。在上述理论结果的支持下，我们只需将矩阵乘法输入到代码中，即可获得预测和预测分布的结果。为了举例说明，我们使用了一个玩具问题:X是从-1到1，均匀分布，y被构造为以下具有正常噪声的正弦曲线的加法(见下图中y的图示)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/5b6de7a04e0fe00a8724c052abe33648.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HMgiA2msbet7lTtjRC9bNA.png"/></div></div></figure><p id="3ec6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的代码获取这些数据。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="9859" class="na ls iq mw b gy nb nc l nd ne">library(ggplot2)<br/> <br/><em class="nf"># — — — — — Get data — — — — — — — — — — — — — — — — — — — — —+</em></span><span id="1a01" class="na ls iq mw b gy ng nc l nd ne">X &lt;- (-30:30)/30 <br/>N &lt;- length(X) <br/>D &lt;- 10 <br/>var &lt;- 0.15*0.15 <br/>e &lt;- rnorm(N,0,var^0.5) <br/>EY &lt;- sin(2*pi*X)*(X&lt;=0) + 0.5*sin(4*pi*X)*(X&gt;0) <br/>Y &lt;- sin(2*pi*X)*(X&lt;=0) + 0.5*sin(4*pi*X)*(X&gt;0) + e <br/>data &lt;- data.frame(X,Y) <br/>g1 &lt;- ggplot(data=data) + geom_point(mapping=aes(x=X,y=Y))</span></pre><p id="5de2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的代码(在“推断”一节中)实现了上述理论结果。我们还扩展了X的特性(在代码中表示为phi_X，在构造基函数一节中)。就像我们将x展开成x，等等。，我们现在将其扩展为9个径向基函数，每个函数如下所示。请注意，尽管这些看起来像正态密度，但它们并不被解释为概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/c921977421fe8416ed5dadee2f5d5934.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ybf5h_JHzm9JKixB_xQjiQ.png"/></div></div></figure><p id="11ae" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">径向基函数的一个优点是径向基函数可以拟合各种曲线，包括多项式和正弦曲线。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="637c" class="na ls iq mw b gy nb nc l nd ne"><em class="nf"># — — — — — Construct basis functions — — — — — — — — — — — —+</em></span><span id="6534" class="na ls iq mw b gy ng nc l nd ne">phi_X &lt;- matrix(0, nrow=N, ncol=D)<br/>phi_X[,1] &lt;- X<br/>mu &lt;- seq(min(X),max(X),length.out=D+1)<br/>mu &lt;- mu[c(-1,-length(mu))]<br/>for(i in 2:D){<br/> phi_X[,i] &lt;- exp(-(X-mu[i-1])^2/(2*var))<br/>}</span><span id="d296" class="na ls iq mw b gy ng nc l nd ne"><em class="nf"># — — — — — Inference — — — — — — — — — — — — — — — — — — — —+</em></span><span id="d6b0" class="na ls iq mw b gy ng nc l nd ne"><em class="nf"># Commented out is general prior<br/># m0 &lt;- matrix(0,D,1)<br/># S0 &lt;- diag(x=1000,D,D) <br/># SN &lt;- inv(inv(S0)+t(phi_X)%*%phi_X/var)<br/># mN &lt;- SN%*%(inv(S0)%*%m0 + t(phi_X)%*%Y/var)<br/># Y_hat &lt;- t(mN) %*% t(phi_X)</em></span><span id="cf8c" class="na ls iq mw b gy ng nc l nd ne"><em class="nf"># We use non-informative prior for now</em><br/>m0 &lt;- matrix(0,D,1)<br/>SN &lt;- solve(t(phi_X)%*%phi_X/var)<br/>mN &lt;- SN%*%t(phi_X)%*%Y/var<br/>Y_hat &lt;- t(mN) %*% t(phi_X)<br/>var_hat &lt;- array(0, N)<br/>for(i in 1:N){<br/> var_hat[i] &lt;- var + phi_X[i,]%*%SN%*%phi_X[i,]<br/>}</span><span id="60c1" class="na ls iq mw b gy ng nc l nd ne">g_bayes &lt;- g1 + <br/>             geom_line(mapping=aes(x=X,y=Y_hat[1,]),color=’#0000FF’)<br/>g_bayes_full &lt;- g_bayes + geom_ribbon(mapping=aes(x=X,y=Y_hat[1,],<br/>                  ymin=Y_hat[1,]-1.96*var_hat^0.5,<br/>                  ymax=Y_hat[1,]+1.96*var_hat^0.5, alpha=0.1),<br/>                  fill=’#9999FF’)</span></pre><p id="335a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这些计算中需要注意的一个细节是，我们使用了非信息先验。注释掉的部分正是上面的理论结果，而对于非信息先验，我们使用对角元素接近无穷大的协方差矩阵，因此在此代码中，其倒数直接被视为0。如果您想使用此代码，请确保您安装了ggplot2包用于绘图。</p><p id="ec65" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下图旨在展示完整的预测分布，并给出数据拟合程度的感觉。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/80cad17100302f8e283fe492fea7f61e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rzuJmV1EWmr-MOl2Tlbrxw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">蓝线是每个点x的预测分布的期望值，浅蓝色区域是指两个标准差以内的区域。红线是y的真函数，点是从给定函数中随机产生的数据，带有正态噪声。</p></figure><h1 id="26a6" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">评论</h1><p id="2a6c" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">多元线性回归结果与使用具有无限协方差矩阵的不适当先验的贝叶斯回归的情况相同。一般来说，获得一些关于参数的经验知识，并使用信息丰富的先验知识是一种很好的做法。贝叶斯回归可以量化和显示不同的先验知识如何影响预测。在任何情况下，贝叶斯观点可以方便地将y预测的范围解释为概率，不同于从经典线性回归计算的置信区间。</p><p id="b047" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从这个角度来看，数据拟合也让你很容易“边走边学”。假设我首先观察了10000个数据点，并计算了参数w的后验概率。之后，我设法获得了1000个数据点，而不是再次运行整个回归，我可以使用之前计算的后验概率作为这1000个点的先验。这个顺序过程产生的结果与再次使用全部数据的结果相同。我喜欢这个想法，因为它非常直观，因为学到的观点与以前学到的观点加上新的观察结果成正比，而且学习还在继续。一个笑话说，一个贝叶斯谁梦见一匹马，观察到一头驴，会称之为骡子。但是如果他对它进行更多的观察，最终他会说这确实是一头驴。</p></div></div>    
</body>
</html>