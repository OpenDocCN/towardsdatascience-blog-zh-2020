<html>
<head>
<title>Mind the Jensen Gap</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">小心詹森差距</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mind-the-jensen-gap-c54e0eb9e1b7?source=collection_archive---------34-----------------------#2020-08-06">https://towardsdatascience.com/mind-the-jensen-gap-c54e0eb9e1b7?source=collection_archive---------34-----------------------#2020-08-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ec45" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当在预测之前转换数据时，学习如何在返回到原始尺度时发现偏差</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8d08eb39718db959ea72b93c23060c52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*s5J1I8RGPCbaEqak"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://commons.wikimedia.org/w/index.php?curid=4379199" rel="noopener ugc nofollow" target="_blank"> WillMcC </a>(已修改)</p></figure><h1 id="bf85" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">背景</h1><p id="1b00" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在 Kaggle 的<a class="ae ky" href="https://www.kaggle.com/c/m5-forecasting-accuracy" rel="noopener ugc nofollow" target="_blank"> M5 预测-准确性竞赛</a>中，平方根转换毁了我团队的许多预测，并导致在最后时刻进行选择性的修补工作。尽管<a class="ae ky" href="https://nousot.com/blog/how-we-won-gold/" rel="noopener ugc nofollow" target="_blank">的结果很好</a>，但我们被提醒“重构偏差”会困扰原始尺度的预测，即使是平方根这样的普通变换。</p><h1 id="4ea6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">平方根变换</h1><p id="fec1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对于<a class="ae ky" href="https://en.wikipedia.org/wiki/Poisson_distribution" rel="noopener ugc nofollow" target="_blank">泊松数据</a>，平方根的基本原理是它是方差稳定变换；理论上，值的平方根近似正态分布，具有恒定的方差和平均值，即原始平均值的平方根。这是一个近似值，正如<a class="ae ky" href="https://en.wikipedia.org/wiki/Poisson_distribution#Related_distributions" rel="noopener ugc nofollow" target="_blank">维基百科所说的</a>，其中“<em class="mn">收敛到常态(随着[原始均值]增加)的速度远远快于未转换的变量。</em></p><p id="1c16" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">想象一下，你决定在一个计数数据场景中求平方根，感觉很好，确信收敛到正态是“快速的”。然后，对平方根转换数据的平均值进行建模，然后获得平方根级别的预测。在某些时候，特别是在预测场景中，你必须回到最初的规模。这可能需要对模型估计的平均值求平方。M5 竞赛提醒人们，这种方法可能而且将会失败。</p><h1 id="e60c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">詹森差距</h1><p id="4912" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" rel="noopener ugc nofollow" target="_blank">詹森不等式</a>陈述了对于凸函数，在期望处求值的函数小于或等于该函数的期望，即 g(E[Y]) ≤ E[g(Y)]。对于凹函数，这个不等式是颠倒的。</p><p id="0f42" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">同样的，<strong class="lt iu">詹森间隙</strong>定义为差值 E[<em class="mn">g</em>(<em class="mn">Y</em>)]-<em class="mn">g</em>(E[<em class="mn">Y</em>])，对于凸函数<em class="mn"> g. ( </em>作为题外话，<em class="mn"> </em>注意当<em class="mn"> g </em> ( <em class="mn"> x </em>)是平方函数时，詹森间隙【T32)</p><p id="4ef3" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">当把<em class="mn"> g </em> ( <em class="mn"> x </em>)作为平方函数，把 <em class="mn"> Y </em>的<em class="mn">平方根作为随机变量时，詹森间隙就变成 E[<em class="mn">Y</em>]-E[sqrt(<em class="mn">Y</em>)]。因为数量是正，我们的重建平均值将向下偏移。为了进一步了解这个差距的大小，我们转向泰勒展开式。</em></p><h1 id="76e7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">泰勒展开至近似偏差</h1><p id="a433" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对于<a class="ae ky" href="https://math.stackexchange.com/questions/1536459/expected-value-of-square-root-of-poisson-random-variable" rel="noopener ugc nofollow" target="_blank">Mathematics stack exchange</a>提示“<em class="mn">泊松随机变量</em>平方根的期望值”，<a class="ae ky" href="https://math.stackexchange.com/users/312/leonbloy" rel="noopener ugc nofollow" target="_blank">投稿人 Hernan Gonzalez </a>解释了一个随机变量关于其均值的泰勒展开，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/3e30bd9495a1622aaf125510df1edbc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*a1oC7sDSyTgfWFlr"/></div></div></figure><p id="0ee9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">注意，展开至少需要原始分布的几个中心矩。对于泊松，前三个只是平均参数。</p><p id="693a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">忽略均值估计量也是一个随机变量，我们可以通过逆变换运行上面的期望，即平方它，以了解任何泊松均值在原始尺度上的偏差(代数不在这里，但它在<a class="ae ky" href="https://github.com/nousot/Public/blob/main/blog/simulate-transformation-bias.R" rel="noopener ugc nofollow" target="_blank">演示代码</a>的第 34 行中计算)。)同样，利用随机变量平方根的性质，可以用同样的方法直接分析<em class="mn">g</em>(x)=<em class="mn">x</em>^<em class="mn">2</em>。这开启了偏差修正的可能性，这是一个有趣的命题，尽管它有自己的假设和复杂性。</p><h2 id="d008" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">近似分解</h2><p id="ede0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在他的回答接近尾声时，Gonzalez 提到，只有当原始泊松的平均值比 1 大很多时，近似"<em class="mn">才有用，并在评论中澄清这是必要的，以便"<em class="mn">总和的项快速减少。</em>”这是从原始术语后的均值被提升到负幂得出的。</em></p><p id="0e11" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在 M5 竞赛中，许多商品的平均销售额都大大低于 1，因此使用平方根变换会导致糟糕的表现。为了了解这在实际样本中是如何发生的，下一节将通过模拟来研究这一现象。</p><h1 id="647f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">示范</h1><p id="e44d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本节中，我们使用黄土平滑器在原始比例和平方根比例上创建模型，并对后者的平均估计值进行平方。对于平均值为 20 和 0.2 的模拟泊松数据，我们绘制两组预测并检查偏差。代码不到 50 行，可以在 Nousot 的公共 Github 库中找到。</p><h1 id="ca65" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">当平均值为 20 时</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/84afa8ebc94119937dfa679b471d0715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bldYMGZ79sBQDI_V"/></div></div></figure><p id="1177" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">对于泊松随机变量的均值为 20 的情况，再转换偏差为负(如詹森不等式所说)，但也相对较小。在代码中，计算泰勒展开的前两项，并与平方根标度上的经验偏差进行比较。分别在-0.027 和-0.023，比较接近。</p><h1 id="6e3b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">当平均值为 0.20 时</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/0782dd3a5f806bc6f09f73f002b54ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*u1Nz8UoHwuwT0Ejm"/></div></div></figure><p id="9671" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">对于泊松随机变量的平均值为 0.20 的情况，情况就大不相同了。虽然延森不平等总是成立的，但相对而言，延森差距现在很大。此外，泰勒近似法已经完全失效，前两个偏差项总和为 0.419，而经验偏差为-.251(仍在平方根范围内)。</p><h1 id="57de" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">讨论</h1><p id="0e28" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">大卫·沃顿(David Warton)在 2018 年发表的论文“<a class="ae ky" href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.12728" rel="noopener ugc nofollow" target="_blank"> <em class="mn">为什么你不能为小计数</em> </a>摆脱困境”表明了对小均值计数数据进行标准假设的无望。对于 M5 稀疏的时间序列来说，求平方根没有什么好处，反而会损失很多。至少，我们应该区别对待这些系列。(关于我们对卡尔曼滤波器的使用，<a class="ae ky" href="https://medium.com/@oseiskar/hello-great-to-hear-you-found-my-kaggle-solution-inspiring-5ba651aee3e0" rel="noopener"> Otto Seiskari 的建议</a>当模型被错误指定时，通过交叉验证进行调整尤其引人注目)。</p><p id="7c7f" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">沃顿的论文对一般转换的用户有一些严厉的言辞。我仍然相信，如果一个转换让你更接近标准假设，你的代码运行得更快，你享受更好的属性，那么它是值得考虑的。但是需要对数据环境中的转换属性进行诚实的探索，而这不是免费的。</p><p id="4bf5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">通常，变换(及其逆变换)要么是凸的，要么是凹的，因此詹森不等式将保证以詹森间隙的形式出现偏差。如果你想知道为什么你从来没有听说过它，那是因为它经常被认为是近似误差。根据<a class="ae ky" href="https://arxiv.org/pdf/1712.05267.pdf" rel="noopener ugc nofollow" target="_blank">高等(2018) </a>，</p><p id="bbc4" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><em class="mn">“计算一个难以计算的[函数的期望]出现在从统计力学到机器学习理论的各种场景的理论估计中。解决这一问题的常用方法是……证明误差(即詹森差距)对于应用来说足够小。”</em></p><p id="97a8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">当使用转换时，理解逆转换的属性(在数据的上下文中)是值得的。外面很危险。注意脚下，小心延森沟！</p></div></div>    
</body>
</html>