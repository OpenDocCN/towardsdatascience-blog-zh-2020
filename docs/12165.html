<html>
<head>
<title>Machine Learning : Linear Regression using Pyspark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:使用 Pyspark 的线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-linear-regression-using-pyspark-9d5d5c772b42?source=collection_archive---------7-----------------------#2020-08-22">https://towardsdatascience.com/machine-learning-linear-regression-using-pyspark-9d5d5c772b42?source=collection_archive---------7-----------------------#2020-08-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/37190bf92629a4cdeca7449212ee531b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TMGDTm8fKsrSzdqY-3zabQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://unsplash.com" rel="noopener ugc nofollow" target="_blank">https://unsplash.com</a></p></figure><p id="a2ac" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">简介</strong>:</p><p id="723c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark 是用 Python 编写的 Python API，用于支持 Apache Spark。Apache Spark 是一个分布式框架，可以处理大数据分析。Spark 是用 Scala 写的，可以和 Python，Scala，Java，R，SQL 语言集成。Spark 基本上是一个计算引擎，通过并行和批处理系统处理大量数据。当你下载 spark 二进制文件时，会有单独的文件夹来支持上述语言。</p><p id="8b7b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基本上有两种主要类型的算法-转换器:转换使用输入数据集，并使用转换()将其修改为输出数据集。</p><p id="01bc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">估计器是采用输入数据集并使用 fit()生成经过训练的输出模型的算法。</p><p id="4256" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这一节，我将展示使用<a class="ae kf" href="https://spark.apache.org/docs/latest/api/python/index.html" rel="noopener ugc nofollow" target="_blank"> Spark </a>和 Python 的机器学习实现。我将在这里重点介绍在 Spark 环境中实现的基本 ML 算法线性回归。该程序已在独立服务器中执行。</p><p id="2282" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，如下所示导入库。最重要的是给出 Spark 二进制文件在系统中的路径。否则，您可能会在执行代码时遇到问题。</p><figure class="le lf lg lh gt ju"><div class="bz fp l di"><div class="li lj l"/></div></figure><p id="2608" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">火花时段</strong>:</p><p id="c6e1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是 Dataframe API &amp; dataset 编程火花的入口点。它允许您使用 spark 执行各种任务。spark 上下文、hive 上下文、SQL 上下文，现在所有这些都封装在会话中。在 spark 2.0 之前，sparkContext 用于访问所有 spark 功能。spark 驱动程序使用 sparkContext 通过资源管理器连接到集群。sparkConf 创建 sparkContext 对象，该对象存储配置参数，如 appName(用于标识 spark 驱动程序)、应用程序、内核数量以及在 worker 节点上运行的执行器的内存大小。从 spark 2.0 开始，这两个特性被封装在 spark 会话中。因此，每次您想使用 spark 执行任务时，您都需要创建一个会话，并且在执行之后，您必须结束该会话。</p><p id="7e75" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在使用 read.csv()读取数据集，您可以允许 spark 读取数据集并在需要时执行。这里我使用了一个 r<a class="ae kf" href="http://Real estate.csv" rel="noopener ugc nofollow" target="_blank">real estate dataset</a>used。</p><figure class="le lf lg lh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lk"><img src="../Images/e54adcc64bdc2990baa028f3ab11fe92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SCPFs21_ZKOcCYju3inMcQ.png"/></div></div></figure><p id="03d6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，您可以注意到诸如 No 和 X1 交易日期之类的列与房屋价格无关，并且在数据集中没有正确给出交易日期。因此，我们将删除这些列</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="378a" class="lq lr it lm b gy ls lt l lu lv">colm = ['No','X1 transaction date']<br/>df = dataset.select([column <strong class="lm iu">for</strong> column <strong class="lm iu">in</strong> dataset.columns <strong class="lm iu">if</strong> column <strong class="lm iu">not</strong> <strong class="lm iu">in</strong> colm])</span></pre><p id="6290" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有一个很酷的 spark 语法可以做到这一点。如果您在 select()中应用列表理解，您将获得所需的数据框。此数据框不同于熊猫数据框。嗯，和《星火》和《熊猫》里创造的物体有关。</p><blockquote class="lw"><p id="60d2" class="lx ly it bd lz ma mb mc md me mf ld dk translated">Spark 数据帧是分布式的，因此在处理大型数据集时，您将获得并行处理和处理速度的优势。</p><p id="771a" class="lx ly it bd lz ma mb mc md me mf ld dk translated">Spark 确保容错。因此，如果您的数据处理在处理之间中断/失败，spark 可以从沿袭中重新生成失败的结果集。</p></blockquote><pre class="mg mh mi mj mk ll lm ln lo aw lp bi"><span id="8adf" class="lq lr it lm b gy ls lt l lu lv">df.printSchema()</span><span id="d98b" class="lq lr it lm b gy ml lt l lu lv">#output</span><span id="828e" class="lq lr it lm b gy ml lt l lu lv">root<br/> |-- X2 house age: string (nullable = true)<br/> |-- X3 distance to the nearest MRT station: string (nullable = true)<br/> |-- X4 number of convenience stores: string (nullable = true)<br/> |-- X5 latitude: string (nullable = true)<br/> |-- X6 longitude: string (nullable = true)<br/> |-- Y house price of unit area: string (nullable = true)</span></pre><p id="d178" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您查看数据集的模式，它是字符串格式的。让类型转换浮动。</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="b3f1" class="lq lr it lm b gy ls lt l lu lv"><strong class="lm iu">from</strong> <strong class="lm iu">pyspark.sql.functions</strong> <strong class="lm iu">import</strong> col</span><span id="c200" class="lq lr it lm b gy ml lt l lu lv">df = df.select(*(col(c).cast('float').alias(c) <strong class="lm iu">for</strong> c <strong class="lm iu">in</strong> df.columns))</span></pre><p id="2715" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们检查空值。</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="758e" class="lq lr it lm b gy ls lt l lu lv">df.select([count(when(col(c).isNull(), c)).alias(c) <strong class="lm iu">for</strong> c <strong class="lm iu">in</strong> df.columns]).show()</span></pre><p id="253d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太好了！不存在空值。但是列名有点长。所以我们现在将用我们自己的自定义名称来替换它们。重命名列名有几种技术，我使用 reduce()来实现。你可以用另一种方式表演。</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="5904" class="lq lr it lm b gy ls lt l lu lv"><strong class="lm iu">from</strong> <strong class="lm iu">functools</strong> <strong class="lm iu">import</strong> reduce<br/><br/>oldColumns = df.schema.names<br/>newColumns = ['Age','Distance_2_MRT','Stores','Latitude','Longitude','Price']<br/><br/>df = reduce(<strong class="lm iu">lambda</strong> df, idx: df.withColumnRenamed(oldColumns[idx], newColumns[idx]),range(len(oldColumns)), df)</span></pre><blockquote class="lw"><p id="bffa" class="lx ly it bd lz ma mm mn mo mp mq ld dk translated">尝试不同的技术，也让我知道。</p><p id="8a2b" class="lx ly it bd lz ma mb mc md me mf ld dk translated">分享就是关爱: )</p></blockquote><p id="7e2d" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">现在，我们将进行分割以获得要素和标签列。</p><p id="2e6f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">向量汇编器:</strong></p><p id="3afd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://spark.apache.org/docs/latest/ml-features#vectorassembler" rel="noopener ugc nofollow" target="_blank"> VectorAssembler </a>是一个转换器，它将给定的列列表合并成一个向量列。这对于将原始特征和由不同特征转换器生成的特征组合成单个特征向量以训练 ML 模型是有用的。</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="5d43" class="lq lr it lm b gy ls lt l lu lv"><strong class="lm iu">from</strong> <strong class="lm iu">pyspark.ml.feature</strong> <strong class="lm iu">import</strong> VectorAssembler<br/><em class="mw">#let's assemble our features together using vectorAssembler</em><br/>assembler = VectorAssembler(<br/>    inputCols=features.columns,<br/>    outputCol="features")<br/><br/>output = assembler.transform(df).select('features','Price')</span></pre><p id="9167" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这将转换目标和功能列。现在我们将它分成训练和测试数据集。</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="5cf8" class="lq lr it lm b gy ls lt l lu lv">train,test = output.randomSplit([0.75, 0.25])</span></pre><p id="da07" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们应用一个线性回归模型</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="7f36" class="lq lr it lm b gy ls lt l lu lv"><strong class="lm iu">from</strong> <strong class="lm iu">pyspark.ml.regression</strong> <strong class="lm iu">import</strong> LinearRegression<br/>lin_reg = LinearRegression(featuresCol = 'features', labelCol='Price')<br/>linear_model = lin_reg.fit(train)</span><span id="fc19" class="lq lr it lm b gy ml lt l lu lv">print("Coefficients: " + str(linear_model.coefficients))<br/>print("<strong class="lm iu">\n</strong>Intercept: " + str(linear_model.intercept))</span><span id="3b8b" class="lq lr it lm b gy ml lt l lu lv">#Output <br/>Coefficients: </span><span id="6e41" class="lq lr it lm b gy ml lt l lu lv">[-0.2845380180805475,-0.004727311005402087,1.187968326885585,201.55230488460887,-43.50846789357342]<br/><br/>Intercept: 298.6774040798928</span></pre><p id="b311" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到的每一列和截距的系数。</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="2f73" class="lq lr it lm b gy ls lt l lu lv">trainSummary = linear_model.summary<br/>print("RMSE: <strong class="lm iu">%f</strong>" % trainSummary.rootMeanSquaredError)<br/>print("<strong class="lm iu">\n</strong>r2: <strong class="lm iu">%f</strong>" % trainSummary.r2)</span><span id="3b32" class="lq lr it lm b gy ml lt l lu lv">#Output<br/>RMSE: 9.110080<br/><br/>r2: 0.554706</span></pre><p id="a333" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用于测试数据集</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="18ca" class="lq lr it lm b gy ls lt l lu lv"><strong class="lm iu">from</strong>  <strong class="lm iu">pyspark.sql.functions</strong> <strong class="lm iu">import</strong> abs<br/>predictions = linear_model.transform(test)<br/>x =((predictions['Price']-predictions['prediction'])/predictions['Price'])*100<br/>predictions = predictions.withColumn('Accuracy',abs(x))<br/>predictions.select("prediction","Price","Accuracy","features").show()</span></pre><figure class="le lf lg lh gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/e4bec732c13125f2f5ba2acc60df47ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*aJivcdAq6XkrixiLaTt_SA.png"/></div></figure><p id="b6f2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">r-测试数据集的平方值</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="9d6d" class="lq lr it lm b gy ls lt l lu lv"><strong class="lm iu">from</strong> <strong class="lm iu">pyspark.ml.evaluation</strong> <strong class="lm iu">import</strong> RegressionEvaluator</span><span id="f853" class="lq lr it lm b gy ml lt l lu lv">pred_evaluator = RegressionEvaluator(predictionCol="prediction", \<br/>                 labelCol="Price",metricName="r2")<br/>print("R Squared (R2) on test data = <strong class="lm iu">%g</strong>" % pred_evaluator.evaluate(predictions))</span><span id="c84c" class="lq lr it lm b gy ml lt l lu lv">#output<br/>R Squared (R2) on test data = 0.610204</span></pre><p id="87aa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们检查调整后的 R 平方。</p><p id="0f9b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">调整后的 R-square:</strong></p><p id="29b4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">调整的 R 平方</strong>是<strong class="ki iu"> R 平方</strong>的修改版本，已经针对模型中预测器的数量<strong class="ki iu">进行了</strong>调整。只有当新项对模型的改进超过偶然预期时，<strong class="ki iu">调整后的 R 平方</strong>才会增加。当预测者对模型的改进小于预期时，它会减少。我们使用调整后的 R2 值来惩罚与输出数据不相关的此类特征的过度使用。</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="24b6" class="lq lr it lm b gy ls lt l lu lv"><br/>r2 = trainSummary.r2<br/>n = df.count()<br/>p = len(df.columns)<br/>adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)</span></pre><p id="67f3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到调整后的 r 平方值 0.54 用于训练和测试。</p><p id="8b43" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们进一步探索 spark 中的 LinearRegression()。</p><pre class="le lf lg lh gt ll lm ln lo aw lp bi"><span id="1823" class="lq lr it lm b gy ls lt l lu lv">lin_reg = LinearRegression(featuresCol = 'features', labelCol='Price',maxIter=50, regParam=0.12, elasticNetParam=0.2)<br/>linear_model = lin_reg.fit(train)</span></pre><p id="1056" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里你可以应用套索，山脊，弹性网正则化，阿尔法值你可以修改。有一篇关于这些概念的非常好的文章。这是一个用于<a class="ae kf" href="https://github.com/runawayhorse001/LearningApacheSpark" rel="noopener ugc nofollow" target="_blank">学习 Apache Spark Notes </a>的共享存储库。这个共享存储库主要包含文强在<a class="ae kf" href="https://www.ima.umn.edu/2016-2017/SW1.23-3.10.17#" rel="noopener ugc nofollow" target="_blank"> IMA 数据科学奖学金</a>期间的自学和自学笔记。感谢<a class="ae kf" href="https://www.linkedin.com/in/wenqiang-feng-ph-d-51a93742/" rel="noopener ugc nofollow" target="_blank">乔治·冯</a>，ML 实验室的高级数据科学家。</p><p id="92c4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将在未来的故事中分享其他 spark 实现的 ML 算法。</p><p id="1f90" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mw">对于建议，我将在</em><a class="ae kf" href="https://www.linkedin.com/in/somesh-kumar-routray/" rel="noopener ugc nofollow" target="_blank"><em class="mw">LinkedIn</em></a><em class="mw"/><a class="ae kf" href="http://somesh.routray11@gmail.com/" rel="noopener ugc nofollow" target="_blank"><em class="mw">Gmail</em></a><em class="mw"/><a class="ae kf" href="https://twitter.com/RoutraySomesh" rel="noopener ugc nofollow" target="_blank"><em class="mw">Twiiter</em></a><em class="mw">&amp;关注我在</em><a class="ae kf" href="https://github.com/someshkr" rel="noopener ugc nofollow" target="_blank"><em class="mw">GitHub</em></a><em class="mw">的工作。</em></p></div></div>    
</body>
</html>