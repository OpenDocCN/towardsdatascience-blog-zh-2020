<html>
<head>
<title>Using Skip Connections To Enhance Denoising Autoencoder Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用跳过连接来增强去噪自动编码器算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-skip-connections-to-enhance-denoising-autoencoder-algorithms-849e049c0ac9?source=collection_archive---------25-----------------------#2020-06-01">https://towardsdatascience.com/using-skip-connections-to-enhance-denoising-autoencoder-algorithms-849e049c0ac9?source=collection_archive---------25-----------------------#2020-06-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="97c9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在Flickr的一个RGB图像样本上，比较了具有跨越瓶颈的残差网络的自动编码器和没有残差网络的自动编码器的去噪性能。</h2></div><h1 id="2c25" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">自动编码器:</h1><p id="a22a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">官方的<a class="ae lw" href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener ugc nofollow" target="_blank"> Keras </a>博客，称自动编码器为“<strong class="lc iu">自我监督</strong>算法的一个例子，因为它们的<strong class="lc iu">目标是从输入数据</strong>生成<strong class="lc iu"/>。因此，它们被用于图像重建的任务。</p><p id="773b" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">自动编码器的主要部件有:<strong class="lc iu"> <em class="lx">编码器、瓶颈</em>和<em class="lx">解码器</em> </strong>。编码器在每一步提取图像特征，并在此过程中压缩输入数据。瓶颈将输入限制在其最低维度，即输入数据的压缩表示。解码器出现在这个瓶颈之后，用于重构输入数据。损失测量用于比较生成或重建的数据与输入数据的接近程度。</p><h1 id="a1a7" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">图像去噪:</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi md"><img src="../Images/3583bf0b7dc57eca82a0b668789fceae.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*hDDcgrg1NfL6aVxSjRIDoQ.jpeg"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">来源:<a class="ae lw" href="https://giphy.com/gifs/color-colour-FkUyGd7FDh1gk" rel="noopener ugc nofollow" target="_blank"> GIPHY </a></p></figure><p id="bc3c" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">数据去噪图像是自动编码器的一个常见应用。图像中的噪声可以理解为图像颜色或亮度的<em class="lx">随机变化</em>，降低图像质量。在图像数据的各种使用情况下，去除这种噪声通常是预处理步骤。卷积自动编码器可用于此目的。编码器学习提取将它们与图像中的噪声分开的特征。从而压缩图像。来自瓶颈的最终压缩表示被传递到解码器。解码器最终解压缩图像，使噪声最小化。</p><h2 id="1df4" class="mp kj it bd kk mq mr dn ko ms mt dp ks lj mu mv ku ln mw mx kw lr my mz ky na bi translated">跳过从编码器到解码器的连接</h2><p id="55ae" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们知道深度神经网络遭受<em class="lx"/><em class="lx"/><a class="ae lw" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"><em class="lx">退化问题</em> </a>。由于自动编码器具有多个卷积和去卷积层，因此当重建图像时，由于这种信息丢失，它们的性能也会受到影响。由跳跃连接组成的剩余网络是解决这个问题的已知方案。因此，为了提高自动编码器的性能，可以从编码器到解码器添加这样的“跳过连接”，即跨越瓶颈 的<strong class="lc iu"> <em class="lx">。这些附加连接可以直接将特征映射从编码器的较早层发送到解码器的较晚层。这有助于解码器形成输入图像的更清晰定义的解压缩。</em></strong></p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nb"><img src="../Images/58e46d736f114e09fa2f838c815d1646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lp_C2i-wq-wja5GMQdOBhA.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">跨越瓶颈的剩余网络，来源:作者</p></figure><p id="f427" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">为了了解这些残差网络的优势，让我们来看看卷积自动编码器模型不同阶段的激活输出。首先，我们训练了一个没有跨越瓶颈的跳跃连接的自动编码器模型。然后，我们添加它们，并在相同的图像样本上再次训练模型。</p><h1 id="320c" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">数据集</h1><p id="aa6c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">10K RGB图像的样本取自Kaggle上Flickr图像的<a class="ae lw" href="https://www.kaggle.com/hsankesara/flickr-image-dataset" rel="noopener ugc nofollow" target="_blank">数据集。所有的分析都是使用Google Colab的GPU完成的。</a></p><h1 id="c41e" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">模型</h1><p id="5f86" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们使用Keras的功能API来构建自动编码器模型。由于计算限制，输入图像被调整到较小的尺寸(128，128，3)。图像像素也以1/255的因子标准化。此外，通过应用高斯噪声矩阵，图像被故意破坏。这些损坏的图像形成了自动编码器的输入，而原始图像在训练模型时被用作目标。构建卷积网络时，重要的是要记住，随着深入，通道或滤波器的数量会增加，而输入的大小(高度和宽度)会减小。</p><pre class="me mf mg mh gt ng nh ni nj aw nk bi"><span id="dd57" class="mp kj it nh b gy nl nm l nn no"><strong class="nh iu">#Input </strong><br/>input_img = Input(shape=(128, 128, 3))</span><span id="8e4e" class="mp kj it nh b gy np nm l nn no"><strong class="nh iu">#Encoder </strong><br/>y = Conv2D(32, (3, 3), padding='same',strides =(2,2))(input_img)<br/>y = LeakyReLU()(y)<br/>y = Conv2D(64, (3, 3), padding='same',strides =(2,2))(y)<br/>y = LeakyReLU()(y)<br/>y1 = Conv2D(128, (3, 3), padding='same',strides =(2,2))(y) <strong class="nh iu"># skip-1</strong><br/>y = LeakyReLU()(y1)<br/>y = Conv2D(256, (3, 3), padding='same',strides =(2,2))(y)<br/>y = LeakyReLU()(y)<br/>y2 = Conv2D(256, (3, 3), padding='same',strides =(2,2))(y)<strong class="nh iu"># skip-2</strong><br/>y = LeakyReLU()(y2)<br/>y = Conv2D(512, (3, 3), padding='same',strides =(2,2))(y)<br/>y = LeakyReLU()(y)<br/>y = Conv2D(1024, (3, 3), padding='same',strides =(2,2))(y)<br/>y = LeakyReLU()(y)</span><span id="c76e" class="mp kj it nh b gy np nm l nn no"><strong class="nh iu">#Flattening for the bottleneck</strong><br/>vol = y.shape<br/>x = Flatten()(y)<br/>latent = Dense(128, activation='relu')(x) </span></pre><p id="274d" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">下面是解码器的代码。转置卷积或反卷积层用于构建该解码器。<a class="ae lw" href="https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank">去卷积层</a>的工作方式大致类似于卷积层和上采样层的组合。最初，我们在没有第一个和第二个跳过连接的情况下训练模型。然后，使用Keras中layers API的Add()将这些连接从编码器的早期层添加到解码器的后期层。lrelu_bn()辅助函数用于将激活应用于这些添加，并将结果传递给批处理规范化层。</p><pre class="me mf mg mh gt ng nh ni nj aw nk bi"><span id="678e" class="mp kj it nh b gy nl nm l nn no"><strong class="nh iu"># Helper function to apply activation and batch normalization to the # output added with output of residual connection from the encoder</strong></span><span id="a97d" class="mp kj it nh b gy np nm l nn no">def lrelu_bn(inputs):<br/>   lrelu = LeakyReLU()(inputs)<br/>   bn = BatchNormalization()(lrelu)<br/>   return bn</span><span id="9053" class="mp kj it nh b gy np nm l nn no"><strong class="nh iu">#Decoder</strong><br/>y = Dense(np.prod(vol[1:]), activation='relu')(latent)<br/>y = Reshape((vol[1], vol[2], vol[3]))(y)<br/>y = Conv2DTranspose(1024, (3,3), padding='same')(y)<br/>y = LeakyReLU()(y)<br/>y = Conv2DTranspose(512, (3,3), padding='same',strides=(2,2))(y)<br/>y = LeakyReLU()(y)<br/>y = Conv2DTranspose(256, (3,3), padding='same',strides=(2,2))(y)<br/>y= Add()([y2, y]) <strong class="nh iu"># second skip connection added here</strong><br/>y = lrelu_bn(y)<br/>y = Conv2DTranspose(256, (3,3), padding='same',strides=(2,2))(y)<br/>y = LeakyReLU()(y)<br/>y = Conv2DTranspose(128, (3,3), padding='same',strides=(2,2))(y)<br/>y= Add()([y1, y]) <strong class="nh iu"># first skip connection added here</strong><br/>y = lrelu_bn(y)<br/>y = Conv2DTranspose(64, (3,3), padding='same',strides=(2,2))(y)<br/>y = LeakyReLU()(y)<br/>y = Conv2DTranspose(32, (3,3), padding='same',strides=(2,2))(y)<br/>y = LeakyReLU()(y)<br/>y = Conv2DTranspose(3, (3,3), activation='sigmoid', padding='same',strides=(2,2))(y)</span></pre><p id="fede" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">因为输入图像是标准化的，所以它们的结果像素值在0和1之间。为了获得可比较的重建图像，在最终层中使用了“sigmoid”激活。在训练集之外，1k个图像被用作验证集。使用二元交叉熵作为损失函数。最后，两个模型都被训练了200个时期，最小批量为32。学习率为0.001的Adam优化器在收敛时给出最小的训练和验证损失。</p><p id="caa3" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated"><strong class="lc iu"> <em class="lx">让我们来看看测试图像模型各层的激活输出。这将有助于我们清楚地看到编码器-解码器的作用！</em> </strong></p><p id="e1f2" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">激活输出是使用<a class="ae lw" href="https://github.com/philipperemy/keract" rel="noopener ugc nofollow" target="_blank"> keract </a>包创建的。</p><h2 id="8c65" class="mp kj it bd kk mq mr dn ko ms mt dp ks lj mu mv ku ln mw mx kw lr my mz ky na bi translated">测试图像:</h2><p id="55d0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们拍摄一张哥伦比亚大学校园的测试图片。测试图像根据模型调整了大小，并添加了随机噪声。“有噪声的”图像被用作模型的输入。</p><div class="me mf mg mh gt ab cb"><figure class="nq mi nr ns nt nu nv paragraph-image"><img src="../Images/551de5f30cf5ee697e8553b05eb96b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/0*fRE4U7lplBGyy82H"/></figure><figure class="nq mi nr ns nt nu nv paragraph-image"><img src="../Images/e1b22fcabba8836c53c897280f5e25c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/0*6NaPTmWi83ISEwKO"/><p class="ml mm gj gh gi mn mo bd b be z dk nw di nx ny translated">原始调整大小的图像(左)，添加了随机噪声的图像用作模型的输入。(右)</p></figure></div><h2 id="8814" class="mp kj it bd kk mq mr dn ko ms mt dp ks lj mu mv ku ln mw mx kw lr my mz ky na bi translated">编码器的激活输出</h2><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nz"><img src="../Images/c6e2b82a779e0781f9f646a2b681f8f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XoMMB3hczjX_mbyr"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">第一卷积层的激活输出</p></figure><p id="6d8a" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">由于两种型号的编码器相同，因此该部件的激活输出也相同。在检查前三个卷积层的激活输出时，可以看到大部分图像信息被保留。这也可能意味着图像中的噪声也保留在模型的这个阶段。然而，当我们深入模型时(参见第三个卷积层之后的层的激活输出)，保留的信息相当抽象。该模型开始提取更高层次的特征，例如边界、拐角和角度。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi oa"><img src="../Images/0d5a2831950e1c15d8d64669daab6fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e6OGwCCL4kS7Rk4jVOZNrA.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">第二和第三卷积层的激活输出</p></figure><div class="me mf mg mh gt ab cb"><figure class="nq mi ob ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/18bacc668b1ea81a0390677266810b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/0*IiB9P5xI5RIkZoxm"/></div></figure><figure class="nq mi oc ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/91dadb0d9b4bae97f936ce77ff760905.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/0*j94rVNQMWySOM8q9"/></div></figure><figure class="nq mi od ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/a37f3e36a00ae79d9d29cab1d9611f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/0*p9M7HQ0acp1okLV6"/></div><p class="ml mm gj gh gi mn mo bd b be z dk oe di of ny translated">“摘要”第四、第五和第六卷积层的激活输出</p></figure></div><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi og"><img src="../Images/b0779b388d9bd59ad61654d55b12376e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yu5pvyArsROQZBBh3Z6qcw.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">第七卷积层的激活输出和输入图像的压缩表示</p></figure><h2 id="c814" class="mp kj it bd kk mq mr dn ko ms mt dp ks lj mu mv ku ln mw mx kw lr my mz ky na bi translated">解码器的激活输出(没有跨越瓶颈的跳跃连接的模型)</h2><p id="8240" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">有趣的是，解码器的激活输出与上述编码器的序列完全相反。</p><div class="me mf mg mh gt ab cb"><figure class="nq mi oh ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/e809ac97cc451e54df6203a11761ee35.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*NMf8GwF22qu72rUvUwNOBQ.png"/></div></figure><figure class="nq mi oi ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/1b27f06d02ec77104a992c786228023a.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*l5KkSGnVnW3IiU1TexGybA.png"/></div></figure><figure class="nq mi oj ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/c468c4bd77f1564ddc975ec5dd46c7cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*vM_x1M3AHvFKOdcP8MhfMQ.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk ok di ol ny translated">第一、第二和第三去卷积层的激活输出</p></figure></div><p id="8312" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">在第三个去卷积层之后，我们仍然看不到图像的任何边缘被再次形成。</p><div class="me mf mg mh gt ab cb"><figure class="nq mi om ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/c76b6a8bbe075da011d354f1127977d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*bJXu7IXG505xUbfp1lmLKw.png"/></div></figure><figure class="nq mi on ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/6265675a06dec5da4bc72bff652fe277.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*pYCZt41dc67Fd1-H0YWlAA.png"/></div></figure><figure class="nq mi oo ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/b48ecf4e236edfc39ffcb49b20d0c5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*MliILpXD38Bo_DdGRp5EQQ.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk op di oq ny translated">第四、第五和第六解卷积层的激活输出</p></figure></div><p id="e57d" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">如下图所示，在解码器的最后一层，没有明确定义图像的解压缩方式！</p><div class="me mf mg mh gt ab cb"><figure class="nq mi or ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/b18975418eba27ad888c2bb3a6d936e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*bVSCHhB5O75Bb-5bf3udTQ.png"/></div></figure><figure class="nq mi os ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/4f93d0d3b1a02c7045d0337873199f7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*Sk3fDmUYSSZYeB0Mf4jNPQ.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk ot di ou ny translated">第七反卷积层和最后一层的激活输出</p></figure></div><h2 id="f5be" class="mp kj it bd kk mq mr dn ko ms mt dp ks lj mu mv ku ln mw mx kw lr my mz ky na bi translated">解码器的激活输出(添加了跳过瓶颈连接的模型)</h2><p id="270d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">正如在编码器的激活输出中所看到的，其早期层保留了噪声，但是后期层提取了图像的更高表示。因此，从编码器的第三和第五卷积层到解码器的第三和第五去卷积层进行跳跃连接。该解码器的第一、第二和第三解卷积层的激活输出与先前版本的解码器相同。然而，在第三层之后，我们看到激活输出与先前解码器的不同。下面我们可以清楚地看到，在第六个反卷积层的激活输出中，图像再次形成。</p><div class="me mf mg mh gt ab cb"><figure class="nq mi ov ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/04f22c032f55f262b73211dbb7006cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*EShc9qKbu88Kj-GlXOxGLA.png"/></div></figure><figure class="nq mi ow ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/107db2962e27f4d7c1e810484ba1642d.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*5Ryq3IxGjo5Qs_bggCqdgA.png"/></div></figure><figure class="nq mi oh ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/7840b784f18328465f3350aa8751ab83.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*mpV1eT6uAzGFblwY9I-Ovg.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk op di oq ny translated">第四、第五和第六解卷积层的激活输出</p></figure></div><p id="7a9b" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">该解码器的最后一层给出了测试输入图像的清晰定义的解压缩版本。</p><div class="me mf mg mh gt ab cb"><figure class="nq mi ox ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/8175612e8a47ae8c43d7753f4df4b0bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*CQLW5qK9Lz0XJWbLDhmvWA.png"/></div></figure><figure class="nq mi oy ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><img src="../Images/a949afe264d8defddfefc3044fac7611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*vkqmwX2ar-lDSBjLy5Jjmg.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk ot di ou ny translated">第七反卷积层和最后一层的激活输出</p></figure></div><h1 id="dd73" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结果的比较</h1><p id="28e9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">第一个模型的去噪图像输出:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/f384e8a65a2832d38b5045688cfea9c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/0*rkl0l4AE5ih59gO8"/></div></figure><p id="0d16" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">具有跨越瓶颈的跳跃连接的第二模型的去噪图像输出:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/ea2772244a2b5d636baeb7aad9e37a2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/0*Upux_1b3NPR_QKbv"/></div></figure><p id="6ef5" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj ma ll lm ln mb lp lq lr mc lt lu lv im bi translated">我们可以清楚地看到，跨越瓶颈连接的autoencoder模型性能更好！</p><h1 id="d969" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">前方的路…</h1><p id="adeb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">除了对图像去噪之外，这种方法可以用于自动编码器的任何图像重建应用。虽然跳过连接提高了自动编码器的性能，但是可以试验这些连接的位置和数量。该模型也可以用不同级别的噪声因子来训练，以便更好地概括结果。完整的代码可以访问<a class="ae lw" href="https://github.com/MS1997/Autoencoders-with-skip-connections" rel="noopener ugc nofollow" target="_blank">这里</a>！</p></div><div class="ab cl pa pb hx pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="im in io ip iq"><h2 id="1865" class="mp kj it bd kk mq mr dn ko ms mt dp ks lj mu mv ku ln mw mx kw lr my mz ky na bi translated">参考资料:</h2><p id="15ea" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">J.董，人。毛，沈振华。杨，【2017】利用卷积自动编码器和对称跳跃连接学习深度表示</p></div></div>    
</body>
</html>