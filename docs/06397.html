<html>
<head>
<title>Gradient Descent Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c?source=collection_archive---------6-----------------------#2020-05-22">https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c?source=collection_archive---------6-----------------------#2020-05-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f7c8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">梯度下降综合指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7b91d857963da788898d84d2299ebc05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MmugXR31tAWDev6ve5vMsg.png"/></div></div></figure><p id="d1bc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">优化是指最小化/最大化由 x 参数化的目标函数 f(x)的任务。在机器/深度学习术语中，它是最小化由模型参数 w ∈ R^d. <br/>参数化的成本/损失函数 J(w)的任务。优化算法(在最小化的情况下)具有以下目标之一:</p><ol class=""><li id="2a1a" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">找到目标函数的全局最小值。如果目标函数是凸的，即任何局部最小值都是全局最小值，这是可行的。</li><li id="e761" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">找到目标函数在其邻域内的最低可能值。如果目标函数不像大多数深度学习问题那样是凸的，通常就是这种情况。</li></ol><h1 id="21d5" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">梯度下降</h1><p id="074c" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">梯度下降是机器/深度学习算法中使用的优化算法。梯度下降的目标是使用迭代最小化目标凸函数 f(x)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/54e4bc684baad37bd1427e00dfb0a7a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1p-XXGCA2d2UAKUlvh4BCg.jpeg"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">凸函数 v/s 不是凸函数</p></figure><p id="4fd0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">成本函数的梯度下降。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/fc066453055f40f7321ddf147326ab2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SHb4QPkl02FzT8QtHFRVlQ.jpeg"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">梯度下降背后的直觉</p></figure><p id="6678" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为方便起见，我们来看一个简单的线性模型。</p><blockquote class="ne"><p id="3365" class="nf ng iq bd nh ni nj nk nl nm nn lm dk translated">误差= Y(预测)-Y(实际)</p></blockquote><p id="74ec" class="pw-post-body-paragraph kr ks iq kt b ku no jr kw kx np ju kz la nq lc ld le nr lg lh li ns lk ll lm ij bi translated">机器学习模型总是希望最大精度的低误差，为了减少误差，我们将直觉地告诉我们的算法，你正在做一些需要纠正的错误，这将通过梯度下降来完成。</p><p id="bca6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们需要最小化我们的误差，为了得到指向最小值的指针，我们需要走一些被称为 alpha(学习率)的步骤。</p><h2 id="d7d5" class="nt mc iq bd md nu nv dn mh nw nx dp ml la ny nz mn le oa ob mp li oc od mr oe bi translated">实施梯度下降的步骤</h2><ol class=""><li id="8a31" class="ln lo iq kt b ku mt kx mu la of le og li oh lm ls lt lu lv bi translated">随机初始化值。</li><li id="94f0" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">更新值。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/786d51f01b443b070989eb8e2817789b.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/0*mNbip4ggc_Sr5zXv.png"/></div></figure><p id="f9aa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">3.重复直到斜率=0</p><p id="2863" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">导数是一个来自微积分的术语，计算为图形在特定点的斜率。斜率是通过在该点画一条切线来描述的。所以，如果我们能计算出这条切线，我们就能计算出达到最小值的方向。</p><p id="e3e4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">学习率一定要明智的选择为:<br/> 1。如果太小，那么模型需要一些时间来学习。<br/> 2。如果它太大，模型会收敛，因为我们的指针会射出，我们将无法达到最小值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/60057ffd460246369776f92b2cf21871.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oqzNRIFF5k75j33G.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">大学习率 v/s 小学习率，<a class="ae ok" href="https://algorithmia.com/blog/introduction-to-optimizers" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/8f0fd8bf95bca296d3f92f9872f4be5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*svH5eeB6JalWU7YQ.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">不同学习率的梯度下降，<a class="ae ok" href="https://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="7a67" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，普通梯度下降法不能保证良好的收敛性，原因如下:</p><ul class=""><li id="5641" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm om lt lu lv bi translated">选择一个合适的学习速度可能会很麻烦。学习率过低会导致训练缓慢，而学习率过高会导致斜率超调。</li><li id="79cd" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm om lt lu lv bi translated">香草梯度下降面临的另一个关键障碍是它避免陷入局部最小值；这些局部极小值被同样误差的小山包围，这使得香草梯度下降很难逃脱它。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/6e2182efec03c0224f834b4a467343b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RgzVRfhuqs2SyY7N4uVaTg.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">显示曲线平缓和陡峭区域的等高线图，<a class="ae ok" href="https://padhai.onefourthlabs.in/courses/dl-feb-2019" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="223f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">简而言之，我们朝着最小值迈出的每一步都会降低我们的斜率，现在如果我们想象，在曲线的陡峭区域，导数将会很大，因此我们的模型采取的步骤也会很大，但是当我们进入斜率平缓的区域时，导数将会降低，达到最小值的时间也会减少。</p><h2 id="d98c" class="nt mc iq bd md nu nv dn mh nw nx dp ml la ny nz mn le oa ob mp li oc od mr oe bi translated">基于动量的梯度下降</h2><p id="a2c2" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">如果我们考虑，简单梯度下降完全仅依赖于计算，即，如果有 10000 个步骤，那么我们的模型将尝试实施简单梯度下降 10000 次，这显然太费时且计算昂贵。</p><p id="52ee" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在门外汉的语言中，假设一个人正在走向他的家，但他不知道路，所以他向路人问路，现在我们希望他走一段距离，然后问路，但人在他走的每一步都在问路，这显然更耗时，现在比较简单梯度下降的人和他的目标与最小值。</p><p id="bb40" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了避免普通梯度下降的缺点，我们引入了基于动量的梯度下降，其目标是降低计算时间，这可以在我们引入经验的概念时实现，即使用先前步骤的置信度。</p><p id="bc1c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">基于动量梯度下降的伪代码；</p><pre class="kg kh ki kj gt oo op oq or aw os bi"><span id="e00c" class="nt mc iq op b gy ot ou l ov ow">update = learning_rate * gradient<br/>velocity = previous_update * momentum<br/>parameter = parameter + velocity – update</span></pre><p id="3cee" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">以这种方式，我们不是一次又一次地计算新的步骤，而是对衰减进行平均，并且随着衰减增加，其对决策的影响减小，因此越老的步骤对决策的影响越小。<br/>历史越悠久，就要迈出越大的步伐。</p><p id="6fe5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">即使在平缓区域，基于动量的梯度下降也由于其承受的动量而迈出了大步。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/217bc8445b4a58abcfd96172e844a034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TKxSMrG2xPLtcRVy.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">香草梯度下降 v/s 梯度下降带动量，<a class="ae ok" href="https://nhannguyen95.github.io/coursera-deep-learning-course-2-week-2/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="45fc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是由于较大的步数，当它由于陡峭的坡度而在最小值附近振荡时，它以更长的距离超过了它的目标，但是尽管有这些障碍，它还是比普通的梯度下降快。</p><p id="a685" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">简而言之，假设一个人想要到达 1200 米远的目的地，但他不知道路径，所以他决定每走 250 米就问路，现在如果他问路 5 次，他就已经走了 1250 米，这说明他已经超过了目标，要实现目标，他需要往回走。基于动量的 GD 也是类似的情况，由于经验丰富，我们的模型采取了较大的步骤，这导致了超调，因此错过了目标，但为了达到最小值，模型必须回溯其步骤。</p><h2 id="533e" class="nt mc iq bd md nu nv dn mh nw nx dp ml la ny nz mn le oa ob mp li oc od mr oe bi translated">内斯特罗夫加速梯度下降法</h2><p id="48ad" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">为了克服基于动量的梯度下降的问题，我们使用 NAG，在这种情况下，我们首先移动，然后计算梯度，这样，如果我们的振荡过冲，那么与基于动量的梯度下降相比，它一定是不重要的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/6a8fc885dd4c067223f33ba8d12dd82b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*l_diU8353yrN3sIi.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">NAG 背后的直觉，<a class="ae ok" href="https://stats.stackexchange.com/questions/179915/whats-the-difference-between-momentum-based-gradient-descent-and-nesterovs-acc" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="624b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">内斯特罗夫加速梯度(NAG)是一种为我们的动量提供历史的方法。我们现在可以通过计算相对于我们现在的参数θ的角度来充分地向前看。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/1455542036c2e8305e7f31627eca0097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/0*hdinlGYWaUNT4Hnk.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/368c4822cb1cbd4bd333fb7ac4e63ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sQMM2D5VuIPoDN0m.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">基于动量梯度下降 v/s 内斯特罗夫加速梯度下降，<a class="ae ok" href="https://stats.stackexchange.com/questions/179915/whats-the-difference-between-momentum-based-gradient-descent-and-nesterovs-acc" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="f9ec" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">梯度下降策略。</h1><h2 id="7b53" class="nt mc iq bd md nu nv dn mh nw nx dp ml la ny nz mn le oa ob mp li oc od mr oe bi translated">随机梯度下降</h2><p id="309a" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">在这里，学习发生在每个例子上:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/16a459311f06aac8468fa8ab513f35a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/0*5AALDVnR_fE-TA-o.png"/></div></figure><ul class=""><li id="f52a" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm om lt lu lv bi translated">打乱训练数据集以避免预先存在的示例顺序。</li><li id="e9af" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm om lt lu lv bi translated">将训练数据集划分成<em class="pc"> m </em>个例子。</li></ul><p id="8887" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">优点:——</strong></p><p id="8986" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">a.易于安装在内存中<br/> b .计算速度快<br/> c .对大型数据集高效</p><p id="eae9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">缺点:- </strong></p><p id="80fc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">a.由于频繁更新，朝着最小值采取的步骤非常嘈杂。<br/> b .噪音会使等待时间变长。<br/> c .频繁更新的计算开销很大。</p><h2 id="3238" class="nt mc iq bd md nu nv dn mh nw nx dp ml la ny nz mn le oa ob mp li oc od mr oe bi translated">批量梯度下降</h2><p id="ced1" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">这是一种贪婪的方法，我们必须对每次更新的所有例子求和。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/41e94e6f06a3f4059c9c14cf609dbfcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/0*vKSwrVt0jNA9tKSL.png"/></div></figure><p id="273d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">优点:- </strong></p><p id="b07b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">a.噪音较小的步骤<br/> b .产生稳定的 GD 收敛。c .计算效率高，因为所有资源不是用于单个样本，而是用于所有训练样本</p><p id="2acc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">缺点:- </strong></p><p id="2aaa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">a.可能需要额外的内存。处理大型数据库可能需要很长时间。<br/> c .近似梯度</p><h2 id="7823" class="nt mc iq bd md nu nv dn mh nw nx dp ml la ny nz mn le oa ob mp li oc od mr oe bi translated">小批量梯度下降</h2><p id="caee" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">小批量梯度下降法不是遍历所有样品，而是根据批量大小对少量样品进行汇总。</p><p id="0ef6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">它是批梯度下降和随机梯度下降的和。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/f059714262404dcc5555c01ae3f5c45a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*c0Ie-FcWDPJJPKX8.png"/></div></div></figure><p id="7554" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">优点:- </strong></p><p id="e5f7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">a.易于记忆。b .计算效率高。<br/> c .稳定误差 go 和收敛。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/6086b997c6e794bee3d7e57ef73935c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9vUWG5x2Df-oZJoJ.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">批量 v/s 随机 v/s 微型批量梯度下降，<a class="ae ok" href="https://datascience.stackexchange.com/questions/52884/possible-for-batch-size-of-neural-network-to-be-too-small" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/4ac4d3536368f53ba31eb11322b0c086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nekwm7XWZI15QFh6gM7ltg.jpeg"/></div></div></figure><h1 id="6b99" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">如果我们用梯度下降来处理稀疏数据会怎么样？</h1><p id="69f1" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">在稀疏数据的情况下，我们将经历稀疏的 ON(1)特征和更频繁的 OFF(0)特征，现在，大部分时间梯度更新将为零，因为在大多数情况下导数为零，当导数为零时，步长将太小而无法达到最小值。</p><p id="029a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于频繁特征，我们要求低学习率，但是对于高特征，我们要求高学习率。</p><p id="99ae" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，为了提高我们的稀疏自然数据模型，我们需要选择自适应学习率。</p><p id="c461" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">如果你喜欢这篇文章，请考虑订阅我的简讯:</strong> <a class="ae ok" href="https://mailchi.mp/b535943b5fff/daksh-trehan-weekly-newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">达克什·特雷汉每周简讯</strong> </a> <strong class="kt ir">。</strong></p><h1 id="c9d0" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">结论</h1><p id="92d3" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">希望这篇文章不仅增加了你对梯度下降的理解，还让你意识到机器学习并不难，并且已经在你的日常生活中发生了。</p><p id="93a8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">一如既往，非常感谢您的阅读，如果您觉得这篇文章有用，请分享！:)</p><p id="a526" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要了解更多参数优化技术，请访问:-</p><div class="ph pi gp gr pj pk"><a rel="noopener follow" target="_blank" href="/parameters-optimization-explained-876561853de0"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd ir gy z fp pp fr fs pq fu fw ip bi translated">解释参数优化</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">梯度下降的简要描述指南，ADAM，ADAGRAD，RMSProp</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">towardsdatascience.com</p></div></div><div class="pt l"><div class="pu l pv pw px pt py kp pk"/></div></div></a></div></div><div class="ab cl pz qa hu qb" role="separator"><span class="qc bw bk qd qe qf"/><span class="qc bw bk qd qe qf"/><span class="qc bw bk qd qe"/></div><div class="ij ik il im in"><h2 id="d86f" class="nt mc iq bd md nu nv dn mh nw nx dp ml la ny nz mn le oa ob mp li oc od mr oe bi translated">参考资料:</h2><p id="1eae" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">【1】<a class="ae ok" rel="noopener" target="_blank" href="/gradient-descent-algorithm-and-its-variants-10f652806a3">梯度下降算法及其变体</a>由<a class="ae ok" href="https://towardsdatascience.com/@ImadPhd?source=post_page-----10f652806a3----------------------" rel="noopener" target="_blank"> Imad Dabbura </a></p><p id="fa59" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[2] <a class="ae ok" rel="noopener" target="_blank" href="/learning-parameters-part-2-a190bef2d12">学习参数，第 2 部分:基于动量的&amp;内斯特罗夫加速梯度下降</a>作者阿克谢·L·钱德拉</p><p id="3668" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[3]<a class="ae ok" href="https://ruder.io/optimizing-gradient-descent/index.html#momentum" rel="noopener ugc nofollow" target="_blank">Sebastian Ruder 的梯度下降优化算法概述</a></p><p id="cd30" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[4] <a class="ae ok" rel="noopener" target="_blank" href="/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e">理解梯度下降背后的数学原理</a>Parul Pandey 著。</p><p id="bfe6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[5] <a class="ae ok" href="https://padhai.onefourthlabs.in/courses/dl-feb-2019" rel="noopener ugc nofollow" target="_blank">深度学习</a> (padhAI)作者 Mitesh Khapra 博士和 Pratyush Kumar 博士</p><p id="4c5d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">封面模板是我在 canva.com 上设计的，来源在每张图片上都有提及，未提及的图片来自我的笔记本。</p></div><div class="ab cl pz qa hu qb" role="separator"><span class="qc bw bk qd qe qf"/><span class="qc bw bk qd qe qf"/><span class="qc bw bk qd qe"/></div><div class="ij ik il im in"><h2 id="ab40" class="nt mc iq bd md nu nv dn mh nw nx dp ml la ny nz mn le oa ob mp li oc od mr oe bi translated">请随意连接:</h2><blockquote class="qg qh qi"><p id="3f1e" class="kr ks pc kt b ku kv jr kw kx ky ju kz qj lb lc ld qk lf lg lh ql lj lk ll lm ij bi translated"><em class="iq">加入我在【www.dakshtrehan.com】<a class="ae ok" href="http://www.dakshtrehan.com" rel="noopener ugc nofollow" target="_blank"><em class="iq">的</em> </a></em></p><p id="5882" class="kr ks pc kt b ku kv jr kw kx ky ju kz qj lb lc ld qk lf lg lh ql lj lk ll lm ij bi translated"><em class="iq">LinkedIN ~</em><a class="ae ok" href="https://www.linkedin.com/in/dakshtrehan/" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://www.linkedin.com/in/dakshtrehan/</em></a></p><p id="fc36" class="kr ks pc kt b ku kv jr kw kx ky ju kz qj lb lc ld qk lf lg lh ql lj lk ll lm ij bi translated"><em class="iq">Instagram ~</em><a class="ae ok" href="https://www.instagram.com/_daksh_trehan_/" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://www.instagram.com/_daksh_trehan_/</em></a></p><p id="39c6" class="kr ks pc kt b ku kv jr kw kx ky ju kz qj lb lc ld qk lf lg lh ql lj lk ll lm ij bi translated"><em class="iq">Github ~</em><a class="ae ok" href="https://github.com/dakshtrehan" rel="noopener ugc nofollow" target="_blank">T3】https://github.com/dakshtrehanT5】</a></p></blockquote><p id="a1ab" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">查看我的其他文章:-</p><blockquote class="qg qh qi"><p id="8a37" class="kr ks pc kt b ku kv jr kw kx ky ju kz qj lb lc ld qk lf lg lh ql lj lk ll lm ij bi translated"><a class="ae ok" rel="noopener" target="_blank" href="/detecting-covid-19-using-deep-learning-262956b6f981">利用深度学习检测新冠肺炎。</a></p><p id="4f4c" class="kr ks pc kt b ku kv jr kw kx ky ju kz qj lb lc ld qk lf lg lh ql lj lk ll lm ij bi translated"><a class="ae ok" rel="noopener" target="_blank" href="/logistic-regression-explained-ef1d816ea85a">逻辑回归解释</a></p><p id="b20c" class="kr ks pc kt b ku kv jr kw kx ky ju kz qj lb lc ld qk lf lg lh ql lj lk ll lm ij bi translated"><a class="ae ok" href="https://medium.com/towards-artificial-intelligence/linear-regression-explained-f5cc85ae2c5c" rel="noopener"> <em class="iq">线性回归解释</em> </a></p><p id="69ef" class="kr ks pc kt b ku kv jr kw kx ky ju kz qj lb lc ld qk lf lg lh ql lj lk ll lm ij bi translated"><a class="ae ok" href="https://medium.com/datadriveninvestor/determining-perfect-fit-for-your-ml-model-339459eef670" rel="noopener">确定最适合您的 ML 模型。</a></p><p id="1a6c" class="kr ks pc kt b ku kv jr kw kx ky ju kz qj lb lc ld qk lf lg lh ql lj lk ll lm ij bi translated"><a class="ae ok" href="https://medium.com/towards-artificial-intelligence/serving-data-science-to-a-rookie-b03af9ea99a2" rel="noopener">为菜鸟服务数据科学。</a></p><p id="8dc4" class="kr ks pc kt b ku kv jr kw kx ky ju kz qj lb lc ld qk lf lg lh ql lj lk ll lm ij bi translated"><a class="ae ok" href="https://levelup.gitconnected.com/relating-machine-learning-techniques-to-real-life-4dafd626fdff" rel="noopener ugc nofollow" target="_blank">将机器学习技术与现实生活联系起来。</a></p></blockquote><p id="baab" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">关注更多机器学习/深度学习博客。</p><blockquote class="qg qh qi"><p id="eef1" class="kr ks pc kt b ku kv jr kw kx ky ju kz qj lb lc ld qk lf lg lh ql lj lk ll lm ij bi translated"><em class="iq">干杯。</em></p></blockquote></div></div>    
</body>
</html>