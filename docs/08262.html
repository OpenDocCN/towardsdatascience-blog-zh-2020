<html>
<head>
<title>What is Group Normalization?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是群体常态化？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-group-normalization-45fe27307be7?source=collection_archive---------10-----------------------#2020-06-17">https://towardsdatascience.com/what-is-group-normalization-45fe27307be7?source=collection_archive---------10-----------------------#2020-06-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="16b1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">批处理规范化的替代方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/484f8f281cf9c59357a63a95fc30829d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*N-aGkj2hTabdQ1D2"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@carissaweiser?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">假虎刺属·魏泽</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="b1b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">批处理规范化(BN)已经成为许多最新深度学习模型的重要组成部分，特别是在计算机视觉中。它通过批次内计算的平均值和方差对层输入进行归一化，因此得名。为了使 BN 起作用，批量需要足够大，通常至少为 32。然而，有些情况下我们不得不接受小批量:</p><ul class=""><li id="36f0" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">当每个数据样本都非常消耗内存时，例如视频或高分辨率图像</li><li id="f16b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">当我们训练一个非常大的神经网络时，它只留下很少的 GPU 内存来处理数据</li></ul><p id="04ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们需要 BN 的替代品，它能很好地用于小批量生产。分组归一化(GN)是最新的归一化方法之一，它避免了利用批维数，因此与批大小无关。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="ce41" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">不同的标准化方法</h2><p id="0a86" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">为了推动 GN 的形成，我们将首先看看以前的一些归一化方法。</p><p id="a4e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下所有标准化方法都执行计算</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="e8c1" class="mq mr it np b gy nt nu l nv nw"><em class="nx">xᵢ</em> ← (<em class="nx">xᵢ</em> - 𝜇<em class="nx">ᵢ</em>) / √(𝜎<em class="nx">ᵢ</em>² + 𝜀)</span></pre><p id="6564" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于输入特征<em class="nx"> x </em>的每个系数<em class="nx"> xᵢ </em>。𝜇 <em class="nx"> ᵢ </em>和𝜎 <em class="nx"> ᵢ </em>是对一组系数<em class="nx"> Sᵢ </em>计算的平均值和方差，𝜀是一个小常数，为了数值稳定和避免被零除而添加。唯一的区别是如何选择 set <em class="nx"> Sᵢ </em>。</p><p id="92f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了说明归一化方法的计算，我们考虑一批大小为<em class="nx"> N </em> = 3，具有输入特征<em class="nx"> a </em>、<em class="nx"> b </em>和<em class="nx"> c. </em>它们具有通道<em class="nx"> C </em> = 4，高度<em class="nx"> H </em> = 1，宽度<em class="nx"> W </em> = 2:</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="3d42" class="mq mr it np b gy nt nu l nv nw"><em class="nx">a</em> = [ [[2, 3]], [[5, 7]], [[11, 13]], [[17, 19]] ]<br/><em class="nx">b</em> = [ [[0, 1]], [[1, 2]], [[3, 5]], [[8, 13]] ]<br/>c = [ [[1, 2]], [[3, 4]], [[5, 6]], [[7, 8]] ]</span></pre><p id="d8ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，该批次将具有形状(<em class="nx"> N </em>、<em class="nx"> C </em>、<em class="nx"> H </em>、<em class="nx"> W </em> ) = (3，4，1，2)。我们取𝜀 = 0.00001。</p><h2 id="a68e" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">批量标准化</h2><p id="f2aa" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">BN 对通道进行归一化，并沿(<em class="nx"> N </em>、<em class="nx"> H </em>、<em class="nx"> W </em>)轴计算𝜇 <em class="nx"> ᵢ </em>和𝜎 <em class="nx"> ᵢ </em>。<em class="nx"> Sᵢ </em>被定义为与<em class="nx"> xᵢ </em>在同一通道中的一组系数。</p><p id="6f48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<em class="nx"> a </em>的第一个系数<em class="nx"> aᵢ </em> = 2，其中<em class="nx"> i </em> = (0，0，0)，对应的𝜇 <em class="nx"> ᵢ </em>和𝜎 <em class="nx"> ᵢ </em>是在第一个通道的<em class="nx"> a </em>、<em class="nx"> b </em>和<em class="nx"> c </em>的系数上计算的:</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="7e89" class="mq mr it np b gy nt nu l nv nw">𝜇<em class="nx">ᵢ = mean</em>(2, 3, 0, 1, 1, 2) = 1.5<br/>𝜎<em class="nx">ᵢ</em>² = var(2, 3, 0, 1, 1, 2) = 0.917</span></pre><p id="569e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将这些代入标准化公式，</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="18f7" class="mq mr it np b gy nt nu l nv nw"><em class="nx">aᵢ</em> ← (2 - 1.5) / √(0.917 + 0.00001) = 0.522</span></pre><p id="f419" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算一个给出的<em class="nx">的所有系数</em></p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="17e9" class="mq mr it np b gy nt nu l nv nw"><em class="nx">a</em> ← [ [[0.522, 1.567]], [[0.676, 1.690]], [[1.071, 1.630]], [[1.066, 1.492]] ]</span></pre><h2 id="770c" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">图层规范化</h2><p id="8702" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">层标准化(LN)旨在克服 BN 的缺点，包括其对批量大小的限制。它沿着(<em class="nx"> C </em>、<em class="nx"> H </em>、<em class="nx"> W </em>)轴计算𝜇 <em class="nx">、ᵢ </em>和𝜎 <em class="nx">、ᵢ </em>，其中<em class="nx"> Sᵢ </em>被定义为与<em class="nx"> xᵢ </em>属于同一输入特征的所有系数。因此，输入要素的计算完全独立于批处理中的其他输入要素。</p><p id="16dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nx"> a </em>的所有系数都被同一个𝜇 <em class="nx"> ᵢ </em>和𝜎 <em class="nx"> ᵢ </em>归一化</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="4655" class="mq mr it np b gy nt nu l nv nw">𝜇<em class="nx">ᵢ = mean</em>(2, 3, 5, 7, 11, 13, 17, 19) = 9.625<br/>𝜎<em class="nx">ᵢ</em>² = var(2, 3, 5, 7, 11, 13, 17, 19) = 35.734</span></pre><p id="22b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此将 LN 应用于<em class="nx"> a </em>给出</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="9d1c" class="mq mr it np b gy nt nu l nv nw"><em class="nx">a</em> ← [ [[-1.276, -1.108]], [[-0.773, -0.439]], [[0.230, 0.565]], [[1.234, 1.568]] ]</span></pre><h2 id="ca89" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">实例规范化</h2><p id="7ce8" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">实例规范化(IN)可视为将 BN 公式单独应用于每个输入要素(也称为实例),就好像它是批处理中的唯一成员一样。更准确地说，在沿着(<em class="nx"> H </em>、<em class="nx"> W </em>)轴计算𝜇 <em class="nx">、ᵢ </em>和𝜎 <em class="nx">、ᵢ </em>时，<em class="nx"> Sᵢ </em>被定义为与<em class="nx"> xᵢ </em>处于相同输入特征并且也在相同通道中的一组系数。</p><p id="2f03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于 IN 的计算与批大小= 1 的 BN 的计算相同，因此在大多数情况下，IN 实际上会使情况变得更糟。然而，对于风格转换任务，IN 更善于丢弃图像的对比度信息，并且具有比 BN 更好的性能。</p><p id="aed3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于第一个系数<em class="nx"> aᵢ </em> = 2 的<em class="nx"> a </em>，<em class="nx">t39】其中<em class="nx"> i </em> = (0，0，0)，对应的𝜇 <em class="nx"> ᵢ </em>和𝜎 <em class="nx"> ᵢ </em>简单来说就是</em></p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="e2d4" class="mq mr it np b gy nt nu l nv nw">𝜇<em class="nx">ᵢ = mean</em>(2, 3) = 2.5<br/>𝜎<em class="nx">ᵢ</em>² = var(2, 3) = 0.25</span></pre><p id="e859" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这给了</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="b6de" class="mq mr it np b gy nt nu l nv nw"><em class="nx">aᵢ</em> ← (2 - 2.5) / √(0.25 + 0.00001) = -1.000</span></pre><p id="d5a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们向<em class="nx"> a </em>申请时，我们得到</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="462c" class="mq mr it np b gy nt nu l nv nw"><em class="nx">a</em> ← [ [[-1.000, 1.000]], [[-1.000, 1.000]], [[-1.000, 1.000]], [[-1.000, 1.000]] ]</span></pre><h2 id="e53f" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">群体规范化</h2><p id="4413" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">之前，我们在中介绍了将 BN 单独应用于每个输入要素，就好像批量大小= 1 一样。请注意，IN 也可以视为将 LN 单独应用于每个通道，就好像通道数= 1 一样。</p><p id="3634" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">群规范化(GN)是 IN 和 LN 之间的中间地带。它将通道组织成不同的组，并沿(<em class="nx"> H </em>、<em class="nx"> W </em>)轴和一组通道计算𝜇 <em class="nx"> ᵢ </em>和𝜎 <em class="nx"> ᵢ </em>。<em class="nx"> Sᵢ </em>则是与<em class="nx"> xᵢ </em>处于相同输入特征和相同通道组的一组系数。</p><p id="eb1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">组数<em class="nx"> G </em>是一个预定义的超参数，通常需要除以<em class="nx"> C </em>。为简单起见，我们按顺序对通道进行分组。因此，通道 1、…、<em class="nx"> C </em> / <em class="nx"> G </em>属于第一组，通道<em class="nx"> C </em> / <em class="nx"> G </em> + 1、…、2 <em class="nx"> C </em> / <em class="nx"> G </em>属于第二组，依此类推。当<em class="nx"> G </em> = <em class="nx"> C </em>时，意味着每组只有一个通道，GN 变为 IN。另一方面，当<em class="nx"> G </em> = 1 时，GN 变为 LN。因此<em class="nx"> G </em>控制 IN 和 LN 之间的插值。</p><p id="07f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们的例子，考虑 G = 2。为了归一化第一个系数<em class="nx"> aᵢ </em> = 2 的<em class="nx"> a </em>其中<em class="nx"> i </em> = (0，0，0)，我们在前 4 / 2 = 2 个通道中使用<em class="nx"> a </em>的系数</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="a214" class="mq mr it np b gy nt nu l nv nw">𝜇<em class="nx">ᵢ = mean</em>(2, 3, 5, 7) = 4.25<br/>𝜎<em class="nx">ᵢ</em>² = var(2, 3, 5, 7) = 3.687</span></pre><p id="96b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将这些代入标准化公式，</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="9a1a" class="mq mr it np b gy nt nu l nv nw"><em class="nx">aᵢ</em> ← (2 - 4.25) / √(3.687 + 0.00001) = -1.172</span></pre><p id="6302" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<em class="nx"> a </em>的其他系数，计算类似:</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="06d7" class="mq mr it np b gy nt nu l nv nw"><em class="nx">a</em> ← [ [[-1.172, -0.651]], [[0.391, 1.432]], [[-1.265, -0.633]], [[0.633, 1.265]] ]</span></pre></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="0ced" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">归一化方法的比较</h2><p id="8c8c" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">下图显示了 BN、LN、IN 和 GN 之间的关系。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/38f08f37915afc4a1677ae2dd287b007.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O2kJUz0MfZ754kLlcHtTBQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二之四</p></figure><p id="74e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">蓝色区域对应于用于计算𝜇 <em class="nx"> ᵢ </em>和𝜎 <em class="nx"> ᵢ </em>的集合<em class="nx"> Sᵢ </em>，然后这些集合用于归一化蓝色区域中的任何系数。</p><p id="f968" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这个图我们可以看出 GN 是如何在 IN 和 LN 之间插值的。GN 比 IN 好，因为 GN 可以利用跨信道的依赖性。它也比 LN 好，因为它允许为每组通道学习不同的分布。</p><p id="186d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当批量较小时，GN 始终优于 BN。但是，当批量非常大时，GN 的扩展性不如 BN，并且可能无法与 BN 的性能相匹配。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="2548" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">履行</h2><ul class=""><li id="2bec" class="lv lw it lb b lc nj lf nk li nz lm oa lq ob lu ma mb mc md bi translated">TensorFlow:可通过 TensorFlow 插件<a class="ae ky" href="https://www.tensorflow.org/addons/api_docs/python/tfa/layers/GroupNormalization" rel="noopener ugc nofollow" target="_blank">TFA . layers . group normalization</a>获得</li><li id="b603" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">py torch:<a class="ae ky" href="https://pytorch.org/docs/stable/nn.html#groupnorm" rel="noopener ugc nofollow" target="_blank">torch . nn . group norm</a></li></ul><p id="427b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，GN 的两种实现都有一个可学习的、按通道的线性变换，遵循故障归一化。这类似于 BN、LN 和 IN 的实现。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="369c" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">进一步阅读</h2><ol class=""><li id="e6e1" class="lv lw it lb b lc nj lf nk li nz lm oa lq ob lu oc mb mc md bi translated">GN [4]的原始论文是关于 GN 的技术细节以及不同归一化方法的比较的极好参考。</li><li id="9f83" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oc mb mc md bi translated">即使在批量较大时 GN 不匹配 BN，GN +重量标准化[2]也能够匹配甚至优于 BN。我们参考了文献[1]和[2]的一些实验结果。</li><li id="f188" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oc mb mc md bi translated">[3]展示了 BN 通过使优化前景更加平滑所做的工作。这推动了重量标准化的形成。</li></ol></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="92f4" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">参考</h2><ol class=""><li id="783a" class="lv lw it lb b lc nj lf nk li nz lm oa lq ob lu oc mb mc md bi translated">A.、L. Beyer、X. Zhai、J. Puigcerver、J. Yung、S. Gelly 和 N. Houlsby。<a class="ae ky" href="https://arxiv.org/abs/1912.11370" rel="noopener ugc nofollow" target="_blank">大迁移(BiT):一般视觉表征学习</a> (2019)，arXiv 预印本。</li><li id="6595" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oc mb mc md bi translated">南乔，王，刘，沈文伟，尤耶。<a class="ae ky" href="https://arxiv.org/abs/1903.10520" rel="noopener ugc nofollow" target="_blank">重量标准化</a> (2019)，arXiv 预印本。</li><li id="5496" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oc mb mc md bi translated">南桑图尔卡、齐普拉斯、易勒雅斯和马德瑞。<a class="ae ky" href="https://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf" rel="noopener ugc nofollow" target="_blank">批处理规范化如何帮助优化？</a> (2018)，NIPS 2018。</li><li id="9dfb" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oc mb mc md bi translated">Y.吴和何国梁。<a class="ae ky" href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank">分组归一化</a> (2018)，ECCV 2018。</li></ol></div></div>    
</body>
</html>