# 技术和偏见

> 原文：<https://towardsdatascience.com/tech-and-bias-6e19b69bda4e?source=collection_archive---------38----------------------->

## 不仅算法偏向人类，人类也偏向算法

![](img/e3b2982e6ba0a459159e87ce3d089458.png)

安迪·凯利在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

几周前，我写了一篇关于有偏见的算法的文章([如果你想回去读的话，这里有一篇文章](https://medium.com/@katec125/dont-blame-me-blame-the-machine-2f290957120c))。在这篇文章中，我提到了一些事件，比如[一个女性社交网络](https://www.indy100.com/article/gender-biometric-trans-inclusive-app-giggle-9323956)使用面部识别来确定你是否是女性，那一次[亚马逊使用算法](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)来确定最佳求职者，使用算法[来决定谁能获得贷款，谁不能获得贷款，以及](https://www.techtimes.com/articles/240769/20190402/ai-perpetuating-human-bias-in-the-lending-space.htm)[AirBnb](https://www.standard.co.uk/tech/airbnb-software-scan-online-life-suitable-guest-a4325551.html)使用算法来禁止它认为不受欢迎的人。在所有这些情况下，算法在某种程度上都有偏差，因为输入算法的数据有偏差，或者算法做出的假设有偏差，或者两者都有。考虑到这一切，为什么有相当一部分人认为算法不会也不可能有偏差？

在这一点上，我想我们都意识到每个人都有这样或那样的偏见。我们知道自己的一些偏见，但有些偏见我们可能甚至没有意识到。即使我们努力减少我们的偏见，它仍然会存在。然而，这并不是说我们一出生就有种族歧视、性别歧视或同性恋恐惧症。随着时间的推移，随着我们的成长，我们从周围的成年人那里学到了偏见。机器也是如此，但人们倾向于认为这不是真的原因是因为一个有趣的东西叫做[自动化偏见](https://en.wikipedia.org/wiki/Automation_bias)。

自动化偏差被定义为一种偏差，“当人类决策者忽视或不根据被接受为正确的计算机生成的解决方案来搜索矛盾的信息时，就会出现这种偏差”(Cummings 2004，pg。2).换句话说，人类假设计算机是自动正确的，不会质疑它的决定，即使他们应该质疑。这是一个记录良好的现象，在飞行飞机(Mosier 1998)、开药(Lyell 2017)、军事指挥和控制行动(Parasuraman 2010)等许多领域产生了深远的影响。

当人们成为自动化偏见的受害者时，会出现两种类型的错误:

1.  遗漏错误:当操作员“在 aid 没有明确提示的情况下遗漏了事件”(Skitka 1999)
2.  失误:当操作员“做了自动化辅助设备推荐的事情，即使这与他们的训练和其他 100%有效和可用的指标相矛盾”(Skitka 1999)

换句话说，遗漏的错误将是“我没有发现，因为计算机没有发现，”而委托的错误将是“我做了一些与我的训练和/或可用数据相矛盾的事情，因为计算机告诉我这样做。”在亚马逊有偏见的招聘算法的情况下，遗漏的错误可能是这样的，“我没有意识到这个人会非常适合这个职位，因为算法认为这个人不太适合，”而佣金的错误可能是，“我认为女性不太适合我们的空缺职位，因为算法认为女性不太适合，尽管我知道女性的属性不会使你不太适合某份工作。”显然，这两个错误会开始相互融合，有时人类基于算法误用而犯的错误是这里描述的两个错误的组合。

算法和 AI 越来越成为我们生活中不可或缺的一部分。这就是为什么我们需要不断地重新评估我们与技术的关系，以及我们对所述算法的使用的假设。特别是，我们需要评估我们自己对技术的偏见，并确定我们可能拥有的自动化偏见。显然，我们都想避免犯错误和遗漏，并拥有尽可能少的偏见。问题是，我们怎么做？同样的方式，我们处理每一个其他的偏见，我们努力减少:意识和责任。我们需要意识到我们的偏见，因为很明显，你无法将你没有意识到的事情最小化。我们还需要负责任，因为我们需要在一天结束时对我们的决定负责，即使那个决定是由机器帮助的。就飞行员而言。艾尔通发现:

> 与那些不认同这种看法的飞行员相比，那些报告对自己的表现和与自动化交互的策略“负责任”有内在看法的飞行员更有可能根据其他线索仔细检查自动化功能，并且更不容易犯错误。(Mosier 1998 年)

换句话说，如果我们想与我们使用的技术保持健康的关系，我们需要对我们的偏见、我们的行动和决定负责，这些可能会也可能不会得到某种技术的帮助。

[1]玛丽·卡明斯。"智能时间关键决策支持系统中的自动化偏差."在 *AIAA 第一届智能系统技术会议上*，第 6313 页。2004.

[2] Mosier，Kathleen L .，Linda J. Skitka，Susan Heers 和 Mark Burdick。《自动化偏差:高科技驾驶舱中的决策和绩效》*国际航空心理学杂志* 8，第 1 期(1998):47–63。

[3]莱尔、戴维、法拉·马格拉比、马格达莱纳·拉班、L. G .庞特、梅丽莎·贝萨里、理查德·戴和恩里科·科埃拉。"电子处方中的自动化偏差." *BMC 医学信息学与决策* 17，第 1 期(2017): 28。

[4]帕拉苏拉曼、拉贾和迪特里希·h·曼泽伊。"人类使用自动化的自满和偏见:注意力整合."*人为因素* 52，3 号(2010):381–410。

[5] Skitka，Linda J .，Kathleen L. Mosier 和 Mark Burdick。“自动化是否会影响决策？."《国际人机研究杂志》 51，第 5 期(1999 年):991–1006。