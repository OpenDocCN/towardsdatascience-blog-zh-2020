<html>
<head>
<title>Understanding and Implementing Neural Networks in Java from Scratch 💻</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始用Java理解和实现神经网络💻</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-and-implementing-neural-networks-in-java-from-scratch-61421bb6352c?source=collection_archive---------2-----------------------#2020-06-21">https://towardsdatascience.com/understanding-and-implementing-neural-networks-in-java-from-scratch-61421bb6352c?source=collection_archive---------2-----------------------#2020-06-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="92f6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学习最流行的概念💪有史以来的强类型语言。</h2></div><p id="91c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然听起来有点吓人，但我们将从头开始用JAVA创建一个人工智能程序，我会解释所有的概念，你也可以边阅读边编码，建议编写程序而不是Ctrl+C和Ctrl+V。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi li"><img src="../Images/86fa182129f66d952eb8bc2cd089f50e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*uSKKL19eFimHkj1fuO6MSQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">简单神经网络(图片由作者提供)</p></figure><p id="1808" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">神经网络是一种以较小的方式代表人脑的计算系统。黄色的点叫做<strong class="kh ir"> " </strong> <a class="ae lu" href="https://en.wikipedia.org/wiki/Neuron" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">神经元</strong> </a>，连接它们的线叫做<strong class="kh ir"> " </strong> <a class="ae lu" href="https://en.wikipedia.org/wiki/Synapse" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">突触</strong> </a>，这些概念取自🧠.系统通过<strong class="kh ir"/><a class="ae lu" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">反向传播</strong> </a>的过程调整突触的<strong class="kh ir">【权值】来模拟大脑的学习过程。</strong></p><p id="083e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">创建这个系统的第一步是制作我们自己的矩阵库，因为矩阵是神经网络的基本数据结构。</p><p id="4f43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我已经创建了一个开源库，与本文中解释的概念相同，如果感兴趣，你可以贡献出来🙌</p><div class="lv lw gp gr lx ly"><a href="https://github.com/SuyashSonawane/JavaNet" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd ir gy z fp md fr fs me fu fw ip bi translated">SuyashSonawane/JavaNet</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">从头开始用Java理解和实现神经网络💻学习最流行的概念…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">github.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm lo ly"/></div></div></a></div><h1 id="fb34" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">1.矩阵库</h1><p id="1fb2" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">我们将创建一个矩阵类，并为其添加必要的功能，如矩阵的加法、减法、转置和乘法</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="6650" class="np mo iq nl b gy nq nr l ns nt">class Matrix<br/>{<br/>   double [][]data;<br/>   int rows,cols;<br/>}</span></pre><p id="7388" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的矩阵类包含3个对象变量。数据是一个2d双数组来保存矩阵<br/> 2的内容。保存矩阵行数的行<br/> 3。您可能已经注意到，这些列包含列！！</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="b9e5" class="np mo iq nl b gy nq nr l ns nt">public Matrix(int rows,int cols)<br/> {<br/>        data= new double[rows][cols];<br/>        this.rows=rows;<br/>        this.cols=cols;<br/>        for(int i=0;i&lt;rows;i++)<br/>        {<br/>            for(int j=0;j&lt;cols;j++)<br/>            {<br/>                data[i][j]=Math.random()*2-1;<br/>            }<br/>        }<br/>    }</span></pre><p id="13f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们创建了一个构造函数，通过将行和列作为参数传递，用-1和1之间的随机值初始化矩阵对象。</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="2684" class="np mo iq nl b gy nq nr l ns nt">public void add(double scaler)<br/>{<br/>    for(int i=0;i&lt;rows;i++)<br/>    {<br/>        for(int j=0;j&lt;cols;j++)<br/>        {<br/>            this.data[i][j]+=scaler;<br/>        }<br/>        <br/>    }<br/>}</span><span id="75ca" class="np mo iq nl b gy nu nr l ns nt">public void add(Matrix m)<br/>{<br/>    if(cols!=m.cols || rows!=m.rows) {<br/>        System.out.println("Shape Mismatch");<br/>        return;<br/>    }<br/>    <br/>    for(int i=0;i&lt;rows;i++)<br/>    {<br/>        for(int j=0;j&lt;cols;j++)<br/>        {<br/>            this.data[i][j]+=m.data[i][j];<br/>        }<br/>    }<br/>}</span></pre><p id="88ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们创建了一个“add”函数，它重载了两个参数，一个double和一个矩阵对象，该对象执行元素相加。</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="5037" class="np mo iq nl b gy nq nr l ns nt">public static Matrix subtract(Matrix a, Matrix b) {<br/>        Matrix temp=new Matrix(a.rows,a.cols);<br/>        for(int i=0;i&lt;a.rows;i++)<br/>        {<br/>            for(int j=0;j&lt;a.cols;j++)<br/>            {<br/>                temp.data[i][j]=a.data[i][j]-b.data[i][j];<br/>            }<br/>        }<br/>        return temp;<br/>    }</span><span id="a36f" class="np mo iq nl b gy nu nr l ns nt">public static Matrix transpose(Matrix a) {<br/>        Matrix temp=new Matrix(a.cols,a.rows);<br/>        for(int i=0;i&lt;a.rows;i++)<br/>        {<br/>            for(int j=0;j&lt;a.cols;j++)<br/>            {<br/>                temp.data[j][i]=a.data[i][j];<br/>            }<br/>        }<br/>        return temp;<br/>    }</span></pre><p id="a554" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是两个函数，用于计算矩阵的减法和转置，作为参数发送给类静态函数。这些函数返回新的矩阵对象。</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="6980" class="np mo iq nl b gy nq nr l ns nt">public static Matrix multiply(Matrix a, Matrix b) {<br/>        Matrix temp=new Matrix(a.rows,b.cols);<br/>        for(int i=0;i&lt;temp.rows;i++)<br/>        {<br/>            for(int j=0;j&lt;temp.cols;j++)<br/>            {<br/>                double sum=0;<br/>                for(int k=0;k&lt;a.cols;k++)<br/>                {<br/>                    sum+=a.data[i][k]*b.data[k][j];<br/>                }<br/>                temp.data[i][j]=sum;<br/>            }<br/>        }<br/>        return temp;<br/>    }<br/>    <br/>    public void multiply(Matrix a) {<br/>        for(int i=0;i&lt;a.rows;i++)<br/>        {<br/>            for(int j=0;j&lt;a.cols;j++)<br/>            {<br/>                this.data[i][j]*=a.data[i][j];<br/>            }<br/>        }<br/>        <br/>    }<br/>    <br/>    public void multiply(double a) {<br/>        for(int i=0;i&lt;rows;i++)<br/>        {<br/>            for(int j=0;j&lt;cols;j++)<br/>            {<br/>                this.data[i][j]*=a;<br/>            }<br/>        }<br/>        <br/>    }</span></pre><p id="ba34" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好吧！这里发生了很多事情🤯第一个乘法函数接受2个矩阵对象，并对各个矩阵执行点积运算，然后返回一个新的矩阵对象，第二个函数执行矩阵的逐元素乘法，最后一个函数通过缩放器对整个矩阵进行缩放或相乘。</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="f802" class="np mo iq nl b gy nq nr l ns nt">public void sigmoid() {<br/>        for(int i=0;i&lt;rows;i++)<br/>        {<br/>            for(int j=0;j&lt;cols;j++)<br/>                this.data[i][j] = 1/(1+Math.exp(-this.data[i][j])); <br/>        }<br/>        <br/>    }<br/>    <br/>    public Matrix dsigmoid() {<br/>        Matrix temp=new Matrix(rows,cols);<br/>        for(int i=0;i&lt;rows;i++)<br/>        {<br/>            for(int j=0;j&lt;cols;j++)<br/>                temp.data[i][j] = this.data[i][j] * (1-this.data[i][j]);<br/>        }<br/>        return temp;<br/>        <br/>    }</span></pre><p id="3f6d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这个例子，我们为我们的人工智能🧠.使用了<a class="ae lu" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> Sigmoid </a>激活函数这两个函数将sigmoid和sigmoid的导数应用于矩阵的元素。计算反向传播的梯度时需要Sigmoid的导数。</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="d11c" class="np mo iq nl b gy nq nr l ns nt">public static Matrix fromArray(double[]x)<br/>    {<br/>        Matrix temp = new Matrix(x.length,1);<br/>        for(int i =0;i&lt;x.length;i++)<br/>            temp.data[i][0]=x[i];<br/>        return temp;<br/>        <br/>    }<br/>    <br/>    public List&lt;Double&gt; toArray() {<br/>        List&lt;Double&gt; temp= new ArrayList&lt;Double&gt;()  ;<br/>        <br/>        for(int i=0;i&lt;rows;i++)<br/>        {<br/>            for(int j=0;j&lt;cols;j++)<br/>            {<br/>                temp.add(data[i][j]);<br/>            }<br/>        }<br/>        return temp;<br/>   }</span></pre><p id="e569" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些是在矩阵对象和数组之间进行转换的辅助函数。</p><p id="0eb0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，✌，我们有了一个运行良好且易于调试的，我们自己的——矩阵库！！</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi nv"><img src="../Images/3f104ef2bb1092bd1592ff948e7f86da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*M66l5i_d4dDxyHXB"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">由<a class="ae lu" href="https://unsplash.com/@brookecagle?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布鲁克·卡吉尔</a>在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="4b6e" class="mn mo iq bd mp mq oa ms mt mu ob mw mx jw oc jx mz jz od ka nb kc oe kd nd ne bi translated">2.构建神经网络</h1><p id="d48f" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">现在，神经网络的真正有趣和令人生畏的概念来了，但不要害怕，我们将通过编码来学习这些概念，你肯定会发现它们非常直观。</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="c841" class="np mo iq nl b gy nq nr l ns nt">public class NeuralNetwork {    <br/>    Matrix weights_ih , weights_ho , bias_h , bias_o;    <br/>    double l_rate=0.01;<br/>}</span></pre><p id="5d25" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们有一个包含许多变量的NeuralNetwork类，让我们逐个定义它们。<br/> 1。重量_ih👉输入和隐藏层的权重矩阵。<br/> 2。权重_ho👉隐藏层和输出层的权重矩阵。<br/> 3。bias_h👉隐藏层的偏差矩阵。<br/> 4。bias_o👉输出图层的偏差矩阵。<br/> 5。低利率👉学习率是一个超参数，用于控制权重优化过程中的学习步骤。</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="f699" class="np mo iq nl b gy nq nr l ns nt">public NeuralNetwork(int i,int h,int o) {<br/>        weights_ih = new Matrix(h,i);<br/>        weights_ho = new Matrix(o,h);<br/>        <br/>        bias_h= new Matrix(h,1);<br/>        bias_o= new Matrix(o,1);<br/>        <br/>    }</span></pre><p id="46b0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们让构造函数在传递给构造函数的输入、隐藏和输出形状的帮助下初始化所有变量。<br/>检查矩阵的形状很重要，因为点积仅适用于兼容矩阵。有关点积的更多信息，请遵循<a class="ae lu" href="https://en.wikipedia.org/wiki/Dot_product" rel="noopener ugc nofollow" target="_blank">此</a></p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="ea23" class="np mo iq nl b gy nq nr l ns nt">public List&lt;Double&gt; predict(double[] X)<br/>    {<br/>        Matrix input = Matrix.fromArray(X);<br/>        Matrix hidden = Matrix.multiply(weights_ih, input);<br/>        hidden.add(bias_h);<br/>        hidden.sigmoid();<br/>        <br/>        Matrix output = Matrix.multiply(weights_ho,hidden);<br/>        output.add(bias_o);<br/>        output.sigmoid();<br/>        <br/>        return output.toArray();<br/>    }</span></pre><p id="9783" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们有预测函数，它在神经网络上进行前向传递或前向传播</p><blockquote class="of og oh"><p id="d7bc" class="kf kg oi kh b ki kj jr kk kl km ju kn oj kp kq kr ok kt ku kv ol kx ky kz la ij bi translated">前向传播是神经网络中的一个计算步骤，其中输入(矩阵)与隐藏层的权重(矩阵)相乘，然后偏差(列矩阵)与前一步骤的点积相加。最后，结果由激活函数按元素处理。这是在每一层上执行的，前一层的结果作为下一层的输入。</p><p id="5c95" class="kf kg oi kh b ki kj jr kk kl km ju kn oj kp kq kr ok kt ku kv ol kx ky kz la ij bi translated">这一步也称为前向传递，用于从网络生成预测。</p></blockquote><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi om"><img src="../Images/47ed0b2798033884542bf8c026f8cc80.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*QB-4PS0HqdBiZTDbgE0KTA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">单个神经元的正向传递(图片由作者提供)</p></figure><p id="453c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该函数接受输入的双数组，然后通过我们的帮助函数将它们转换为列矩阵。然后在两个层上计算向前传递，然后通过另一个帮助函数将输出展平到一个列表中。</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="2b87" class="np mo iq nl b gy nq nr l ns nt">public void train(double [] X,double [] Y)<br/>    {<br/>        Matrix input = Matrix.fromArray(X);<br/>        Matrix hidden = Matrix.multiply(weights_ih, input);<br/>        hidden.add(bias_h);<br/>        hidden.sigmoid();<br/>        <br/>        Matrix output = Matrix.multiply(weights_ho,hidden);<br/>        output.add(bias_o);<br/>        output.sigmoid();<br/>        <br/>        Matrix target = Matrix.fromArray(Y);<br/>        <br/>        Matrix error = Matrix.subtract(target, output);<br/>        Matrix gradient = output.dsigmoid();<br/>        gradient.multiply(error);<br/>        gradient.multiply(l_rate);<br/>        <br/>        Matrix hidden_T = Matrix.transpose(hidden);<br/>        Matrix who_delta =  Matrix.multiply(gradient, hidden_T);<br/>        <br/>        weights_ho.add(who_delta);<br/>        bias_o.add(gradient);<br/>        <br/>        Matrix who_T = Matrix.transpose(weights_ho);<br/>        Matrix hidden_errors = Matrix.multiply(who_T, error);<br/>        <br/>        Matrix h_gradient = hidden.dsigmoid();<br/>        h_gradient.multiply(hidden_errors);<br/>        h_gradient.multiply(l_rate);<br/>        <br/>        Matrix i_T = Matrix.transpose(input);<br/>        Matrix wih_delta = Matrix.multiply(h_gradient, i_T);<br/>        <br/>        weights_ih.add(wih_delta);<br/>        bias_h.add(h_gradient);<br/>        <br/>    }</span></pre><p id="1ecd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，在上面的训练函数中，我们将X和Y作为双数组，我们将它们转换为矩阵，从矩阵形式的目标Y中减去前向传递的输出。相减的结果是当前传递的样本的误差。该误差用于计算反向传播的梯度。sigmoid函数的导数按元素应用于输出矩阵，输出矩阵返回梯度矩阵，然后乘以输出误差和决定学习步骤的学习速率。</p><blockquote class="of og oh"><p id="c964" class="kf kg oi kh b ki kj jr kk kl km ju kn oj kp kq kr ok kt ku kv ol kx ky kz la ij bi translated">反向传播正好与正向传递相反，在正向传递中，我们对权重矩阵进行转置，然后将它们乘以根据误差计算的梯度，这又返回用于调整当前层中权重的增量。使用梯度更新偏差。</p></blockquote><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi on"><img src="../Images/7740a1497edcffacdcba6a249a549859.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*-ONJMSJxDAP2j2qLA3NuVQ.jpeg"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">神经网络中的反向传播(图片由<a class="ae lu" href="http://sebastianraschka.com" rel="noopener ugc nofollow" target="_blank">sebastianraschka.com</a>提供)</p></figure><p id="23c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当反向支持从输出层运行到输入层时，我们对网络中的前几层重复相同的步骤。<br/>最后，我们已经更新了当前样本的所有层的权重，这完成了数据集上的1个步骤。</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="b4f2" class="np mo iq nl b gy nq nr l ns nt">public void fit(double[][]X,double[][]Y,int epochs)<br/>    {<br/>        for(int i=0;i&lt;epochs;i++)<br/>        {    <br/>            int sampleN =  (int)(Math.random() * X.length );<br/>            this.train(X[sampleN], Y[sampleN]);<br/>        }<br/>    }</span></pre><p id="a374" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">fit函数接受2个2d数组，即X和Y，以及历元数，即我们需要在数据集上迭代多少次。在这里，我们使用数据集中的随机数据点重复调用train函数，以便我们可以在数据集上概化网络。<br/>这完成了我们的神经网络类🥳，现在我们可以在一个简单的数据集上测试我们的程序。</p><div class="lv lw gp gr lx ly"><a href="https://suyashsonawane.me/" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd ir gy z fp md fr fs me fu fw ip bi translated">作品集| Suyash Sonawane</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">嗨，我是苏亚士·索纳瓦尼👋这是我的网络作品集</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">suyashsonawane.me</p></div></div><div class="mh l"><div class="oo l mj mk ml mh mm lo ly"/></div></div></a></div><h1 id="cd65" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">3.测试我们的程序</h1><p id="afc1" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">我们将利用XOR逻辑门的数据制作一个简单的2d双数组。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi op"><img src="../Images/61ea4c40650d3a3156871c69717388b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*p2CrBzSr2Py2wQUY8QHADA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">XOR逻辑门(图片由作者提供)</p></figure><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="f262" class="np mo iq nl b gy nq nr l ns nt">static double [][] X= {<br/>            {0,0},<br/>            {1,0},<br/>            {0,1},<br/>            {1,1}<br/>    };<br/>static double [][] Y= {<br/>            {0},{1},{1},{0}<br/>    };</span></pre><p id="bbeb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们将创建一个神经网络对象，并在数据集上进行训练</p><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="07c3" class="np mo iq nl b gy nq nr l ns nt">NeuralNetwork nn = new NeuralNetwork(2,10,1);<br/>nn.fit(X, Y, 50000);</span></pre><p id="30ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用50000个历元，因为数据集非常小，只包含4个样本。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi oq"><img src="../Images/c91dd75e07ce9fb3821ba556436e4d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BPY_Q9pb3sIoL897NSJeYQ.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">跨时代的损失(作者图片)</p></figure><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="9cbb" class="np mo iq nl b gy nq nr l ns nt">double [][] input ={{0,0},{0,1},{1,0},{1,1}};<br/>for(double d[]:input)<br/>{<br/>    output = nn.predict(d);<br/>    System.out.println(output.toString());<br/>}</span><span id="7bef" class="np mo iq nl b gy nu nr l ns nt">//Output<br/>[0.09822298990353093]<br/>[0.8757877124658147]<br/>[0.8621529792837699]<br/>[0.16860984858200806]</span></pre><p id="9f49" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们用双数组形式的输入来测试神经网络。输出看起来很棒，因为大于0.5的值类似于1输出，而另一个类似于0输出。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi or"><img src="../Images/30cdafe29bf6ca74db0c44827b752cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dTCKqshV4LnYGpKL"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">照片由<a class="ae lu" href="https://unsplash.com/@awcreativeut?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Aw创意</a>在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4cd9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面给出了所有的源代码</p><figure class="lj lk ll lm gt ln"><div class="bz fp l di"><div class="os ot l"/></div></figure><figure class="lj lk ll lm gt ln"><div class="bz fp l di"><div class="os ot l"/></div></figure><figure class="lj lk ll lm gt ln"><div class="bz fp l di"><div class="os ot l"/></div></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="6419" class="mn mo iq bd mp mq oa ms mt mu ob mw mx jw oc jx mz jz od ka nb kc oe kd nd ne bi translated">结论</h1><p id="ebc0" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">因此，我们已经成功地从零开始创建了一个神经网络，我们还创建了自己的矩阵库，可以无缝地处理所有的矩阵运算。我们还在一个小的XOR数据集上测试了它，得到了非常好的结果。</p><p id="f567" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为一名Python程序员，我对Java编程不是很熟悉，因为它需要太多的注意力在数据类型和类型转换上。😓分号；如果你发现了任何不必要或多余的代码，请在评论中分享，如果你有任何建议，请在下面评论。🤗</p><h1 id="c1d3" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated"><strong class="ak">这就是所有的乡亲们</strong></h1><p id="93c6" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">跟我来:</p><p id="68f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作品集:<a class="ae lu" href="https://suyashsonawane.me/" rel="noopener ugc nofollow" target="_blank">https://suyashsonawane.me/</a><br/>推特:<a class="ae lu" href="https://twitter.com/SuyashYSonawane" rel="noopener ugc nofollow" target="_blank">苏亚什·索纳万(@ SuyashSonawane)/推特</a> <br/> LinkedIn: <a class="ae lu" href="https://www.linkedin.com/in/suyash-sonawane-44661417b/" rel="noopener ugc nofollow" target="_blank">苏亚什·索纳万| LinkedIn </a> <br/> Github: <a class="ae lu" href="https://github.com/SuyashSonawane" rel="noopener ugc nofollow" target="_blank">苏亚什·索纳万(Suyash Sonawane) </a></p></div></div>    
</body>
</html>