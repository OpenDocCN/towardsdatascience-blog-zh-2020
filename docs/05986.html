<html>
<head>
<title>Image Super-Resolution using Convolution Neural Networks and Auto-encoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于卷积神经网络和自动编码器的图像超分辨率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-super-resolution-using-convolution-neural-networks-and-auto-encoders-28c9eceadf90?source=collection_archive---------8-----------------------#2020-05-16">https://towardsdatascience.com/image-super-resolution-using-convolution-neural-networks-and-auto-encoders-28c9eceadf90?source=collection_archive---------8-----------------------#2020-05-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3546" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深度学习提升图像质量指南！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ba086526d53e7f36a8b96c4a0732ce92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TFC2WK68_lA-JzJf"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@jeroenbosch?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">(来源)</a></p></figure><p id="eb07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">问题陈述挺熟悉的。你们可能都在某个时候遇到过图像失真的问题，因此会试图提高图像质量。嗯，由于深度学习技术的进步，我们将尝试通过训练卷积神经网络和使用自动编码器来提高图像的分辨率！</p><h2 id="806e" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated"><strong class="ak">先决条件</strong></h2><ol class=""><li id="fc78" class="ml mm iq ky b kz mn lc mo lf mp lj mq ln mr lr ms mt mu mv bi translated">基本了解<a class="ae kv" rel="noopener" target="_blank" href="/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">卷积神经网络</a>(CNN)</li><li id="24b8" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">TensorFlow、Keras和其他一些强制性python库的工作。</li></ol><h2 id="25d5" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">什么是自动编码器？</h2><p id="3dc1" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">自动编码器是一种用于无监督学习的生成模型。</p><blockquote class="ne nf ng"><p id="d72f" class="kw kx nh ky b kz la jr lb lc ld ju le ni lg lh li nj lk ll lm nk lo lp lq lr ij bi translated">通俗地说，可以说这些模型获取一些输入<strong class="ky ir"> <em class="iq"> x </em> </strong>，试图学习一些潜在特征，然后在这些学习到的特征的帮助下重构输入<strong class="ky ir"> <em class="iq"> x </em> </strong>以给出一些期望的输出<strong class="ky ir"> <em class="iq"> X </em> </strong></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/10d068cccc4ca9cd7afb17e57b7bcebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s4XXGeLQtoMnuf8ma-OobQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在这里，图像输入被重建<a class="ae kv" href="https://ars.els-cdn.com/content/image/1-s2.0-S235291481830176X-egi10PTKWBT6NG.jpg" rel="noopener ugc nofollow" target="_blank">(源)</a></p></figure><p id="8b90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用自动编码器模型的概念来增加图像的分辨率。关于自动编码器的详细了解，点击<a class="ae kv" rel="noopener" target="_blank" href="/applied-deep-learning-part-3-autoencoders-1c083af4d798"> <em class="nh">这里</em> </a>。</p><h2 id="7371" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">实施:</h2><p id="22bf" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated"><strong class="ky ir">库导入</strong></p><p id="9e4a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们打开Jupyter笔记本，导入一些需要的库。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="57a2" class="ls lt iq nn b gy nr ns l nt nu">import numpy as np<br/>import cv2<br/>import glob<br/>import tensorflow as tf<br/>from tensorflow.keras import Model, Input, regularizers<br/>from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Add, Dropout<br/>from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint<br/>from keras.preprocessing import image<br/>import matplotlib.pyplot as plt<br/>from sklearn.model_selection import train_test_split <br/>import pickle</span></pre></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="483b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">下载数据集</strong></p><p id="bf35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在野生家园 数据集中对<strong class="ky ir"> <em class="nh">标记的人脸进行处理。该数据集包含标记人脸的数据库，通常用于人脸识别和检测。然而，我们的目的不是检测人脸，而是建立一个模型来提高图像分辨率。</em></strong></p><blockquote class="ne nf ng"><p id="3552" class="kw kx nh ky b kz la jr lb lc ld ju le ni lg lh li nj lk ll lm nk lo lp lq lr ij bi translated">点击此处  <em class="iq">下载数据集。</em></p></blockquote><p id="3159" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据集由多个子目录组成，这些子目录包含该人的各种图像。因此，从这些目录中捕获图像路径非常重要。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="c9e1" class="ls lt iq nn b gy nr ns l nt nu">face_images = glob.glob('lfw/lfw/**/*.jpg') #returns path of images<br/>print(len(face_images)) #contains 13243 images</span></pre></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="4600" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">加载并预处理图像</strong></p><p id="601a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原始图像的尺寸为250 x 250像素。然而，在普通计算机上处理这些图像需要很大的计算能力。因此，我们将把所有图像的尺寸缩小到80 x 80像素。</p><blockquote class="ne nf ng"><p id="b6c2" class="kw kx nh ky b kz la jr lb lc ld ju le ni lg lh li nj lk ll lm nk lo lp lq lr ij bi translated">由于大约有13，000张图像，如果我们单独处理它们，将会花费很多时间。因此，我们利用python中提供的<a class="ae kv" href="https://docs.python.org/2/library/multiprocessing.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">多处理</strong> </a>库来简化执行。</p><p id="18a3" class="kw kx nh ky b kz la jr lb lc ld ju le ni lg lh li nj lk ll lm nk lo lp lq lr ij bi translated"><a class="ae kv" href="https://pypi.org/project/tqdm/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> tqdm </strong> </a>是一个进度库，我们用它来获得工作完成的进度条。</p></blockquote><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="3301" class="ls lt iq nn b gy nr ns l nt nu">from tqdm import tqdm<br/>from multiprocessing import Pool</span><span id="c6f1" class="ls lt iq nn b gy oc ns l nt nu">progress = tqdm(total= len(face_images), position=0)<br/>def read(path):<br/>  img = image.load_img(path, target_size=(80,80,3))<br/>  img = image.img_to_array(img)<br/>  img = img/255.<br/>  progress.update(1)<br/>  return img</span><span id="89ad" class="ls lt iq nn b gy oc ns l nt nu">p = Pool(10)<br/>img_array = p.map(read, face_images)</span></pre><p id="61d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了将来节省时间，让我们在pickle库的帮助下存储我们的<em class="nh"> img_array </em>(包含图像):</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="4ce7" class="ls lt iq nn b gy nr ns l nt nu">with open('img_array.pickle','wb') as f:<br/>  pickle.dump(img_array, f)</span><span id="38a1" class="ls lt iq nn b gy oc ns l nt nu">print(len(img_array))</span></pre></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="9cdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">模型训练的数据准备</strong></p><p id="ae2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将把数据集分成训练集和验证集。我们将使用训练数据来训练我们的模型，验证数据将用于评估模型。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="c6ef" class="ls lt iq nn b gy nr ns l nt nu">all_images = np.array(img_array)</span><span id="818a" class="ls lt iq nn b gy oc ns l nt nu">#Split test and train data. all_images will be our output images<br/>train_x, val_x = train_test_split(all_images, random_state = 32, test_size=0.2)</span></pre><p id="7a54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于这是一个图像分辨率增强任务，我们将扭曲我们的图像，并把它作为一个输入图像。原始图像将被添加为我们的输出图像。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="ff9c" class="ls lt iq nn b gy nr ns l nt nu">#now we will make input images by lowering resolution without changing the size<br/>def pixalate_image(image, scale_percent = 40):<br/>  width = int(image.shape[1] * scale_percent / 100)<br/>  height = int(image.shape[0] * scale_percent / 100)<br/>  dim = (width, height)</span><span id="61e0" class="ls lt iq nn b gy oc ns l nt nu">  small_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)<br/>  <br/>  # scale back to original size<br/>  width = int(small_image.shape[1] * 100 / scale_percent)<br/>  height = int(small_image.shape[0] * 100 / scale_percent)<br/>  dim = (width, height)</span><span id="c6c4" class="ls lt iq nn b gy oc ns l nt nu">  low_res_image = cv2.resize(small_image, dim, interpolation =  cv2.INTER_AREA)</span><span id="a796" class="ls lt iq nn b gy oc ns l nt nu"> return low_res_image</span></pre><blockquote class="ne nf ng"><p id="6b47" class="kw kx nh ky b kz la jr lb lc ld ju le ni lg lh li nj lk ll lm nk lo lp lq lr ij bi translated">我们的想法是将这些扭曲的图像输入到我们的模型中，让模型学会恢复原始图像。</p></blockquote><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="d265" class="ls lt iq nn b gy nr ns l nt nu">train_x_px = []</span><span id="1b52" class="ls lt iq nn b gy oc ns l nt nu">for i in range(train_x.shape[0]):<br/>  temp = pixalate_image(train_x[i,:,:,:])<br/>  train_x_px.append(temp)</span><span id="1691" class="ls lt iq nn b gy oc ns l nt nu">train_x_px = np.array(train_x_px)   #Distorted images</span><span id="1af5" class="ls lt iq nn b gy oc ns l nt nu"># get low resolution images for the validation set<br/>val_x_px = []</span><span id="34a9" class="ls lt iq nn b gy oc ns l nt nu">for i in range(val_x.shape[0]):<br/>  temp = pixalate_image(val_x[i,:,:,:])<br/>  val_x_px.append(temp)</span><span id="923d" class="ls lt iq nn b gy oc ns l nt nu">val_x_px = np.array(val_x_px)     #Distorted images</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/294ffd8f372c99af6f0d8764cf632bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*6vollQxPsSu07FZg1X9_2g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输入图像</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/61b121ead7df5c1bfbf3cc80633bbf5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*R-thvW_0gVh9Y79lBlYEZw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">原象</p></figure></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="6584" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">模型构建</strong></p><p id="f621" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们定义模型的结构。此外，为了克服过度拟合的可能性，我们在卷积层中使用了<strong class="ky ir"> l1正则化</strong>技术。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="1e2f" class="ls lt iq nn b gy nr ns l nt nu">Input_img = Input(shape=(80, 80, 3))  <br/>    <br/>#encoding architecture<br/>x1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(Input_img)<br/>x2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x1)<br/>x3 = MaxPool2D(padding='same')(x2)</span><span id="810b" class="ls lt iq nn b gy oc ns l nt nu">x4 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x3)<br/>x5 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x4)<br/>x6 = MaxPool2D(padding='same')(x5)</span><span id="9969" class="ls lt iq nn b gy oc ns l nt nu">encoded = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x6)<br/>#encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(x2)</span><span id="1301" class="ls lt iq nn b gy oc ns l nt nu"># decoding architecture<br/>x7 = UpSampling2D()(encoded)<br/>x8 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x7)<br/>x9 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x8)<br/>x10 = Add()([x5, x9])</span><span id="e89f" class="ls lt iq nn b gy oc ns l nt nu">x11 = UpSampling2D()(x10)<br/>x12 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x11)<br/>x13 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x12)<br/>x14 = Add()([x2, x13])</span><span id="4ef6" class="ls lt iq nn b gy oc ns l nt nu"># x3 = UpSampling2D((2, 2))(x3)<br/># x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x3)<br/># x1 = Conv2D(256, (3, 3), activation='relu', padding='same')(x2)<br/>decoded = Conv2D(3, (3, 3), padding='same',activation='relu', kernel_regularizer=regularizers.l1(10e-10))(x14)</span><span id="c51a" class="ls lt iq nn b gy oc ns l nt nu">autoencoder = Model(Input_img, decoded)<br/>autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])</span></pre><p id="c98c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以根据自己的选择和要求修改该模型，以获得更好的结果。你也可以改变层数、单位数或一些正则化技术。暂时先往前走，看看我们的模型是什么样子的！</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="1cba" class="ls lt iq nn b gy nr ns l nt nu">autoencoder.summary()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/dfa988be0d60aaef4cdb67f42c4f70fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l0LGxbhH5QBo-hcA-PNzYQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型摘要的屏幕截图</p></figure></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="8570" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">模特培训</strong></p><p id="f4c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将首先定义一些回调，以便将来模型可视化和评估变得容易。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="bf20" class="ls lt iq nn b gy nr ns l nt nu">early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=50, verbose=1, mode='min')</span><span id="f423" class="ls lt iq nn b gy oc ns l nt nu">model_checkpoint =  ModelCheckpoint('superResolution_checkpoint3.h5', save_best_only = True)</span></pre><p id="7c8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们训练我们的模型:</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="a445" class="ls lt iq nn b gy nr ns l nt nu">history = autoencoder.fit(train_x_px,train_x,<br/>            epochs=500,<br/>            validation_data=(val_x_px, val_x),<br/>            callbacks=[early_stopper, model_checkpoint])</span></pre><blockquote class="ne nf ng"><p id="3677" class="kw kx nh ky b kz la jr lb lc ld ju le ni lg lh li nj lk ll lm nk lo lp lq lr ij bi translated">在12GB<strong class="ky ir">NVIDIA</strong>Tesla K80<strong class="ky ir">GPU上，每个纪元的执行时间约为21秒。</strong>65世达到早期停止。</p></blockquote><p id="15c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们在测试数据集上评估我们的模型:</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="e220" class="ls lt iq nn b gy nr ns l nt nu">results = autoencoder.evaluate(val_x_px, val_x)<br/>print('val_loss, val_accuracy', results)</span></pre><blockquote class="ne nf ng"><p id="906e" class="kw kx nh ky b kz la jr lb lc ld ju le ni lg lh li nj lk ll lm nk lo lp lq lr ij bi translated">val_loss，val _ accuracy[0.002111185426451，0.265356767</p></blockquote><p id="61ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从我们的模型中得到了一些非常好的结果，大约93%的验证准确性和0.0021的验证损失。</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="873b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">做出预测</strong></p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="c8f4" class="ls lt iq nn b gy nr ns l nt nu">predictions = autoencoder.predict(val_x_px)</span><span id="1789" class="ls lt iq nn b gy oc ns l nt nu">n = 4<br/>plt.figure(figsize= (20,10))</span><span id="1117" class="ls lt iq nn b gy oc ns l nt nu">for i in range(n):<br/>  ax = plt.subplot(3, n, i+1)<br/>  plt.imshow(val_x_px[i+20])<br/>  ax.get_xaxis().set_visible(False)<br/>  ax.get_yaxis().set_visible(False)</span><span id="1e6e" class="ls lt iq nn b gy oc ns l nt nu">  ax = plt.subplot(3, n, i+1+n)<br/>  plt.imshow(predictions[i+20])<br/>  ax.get_xaxis().set_visible(False)<br/>  ax.get_yaxis().set_visible(False)</span><span id="db50" class="ls lt iq nn b gy oc ns l nt nu">plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/9606a0287138f93506883828f24fbe7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wzZbWyKt9v_vWVmdHmBxA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">第一行—输入图像，第二行—输出图像</p></figure></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><h2 id="f77d" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">结尾注释</h2><p id="2996" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在这个故事中，我们学习了自动编码器的基本功能，并实现了一个图像超分辨率增强任务。这项任务在日常生活中可能有多个用例。例如，我们也可以使用这种技术来提高低分辨率视频的质量。因此，即使没有标签，我们也可以利用图像数据解决一些现实世界的问题。</p><p id="6b0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您有任何其他使用案例或技术来处理图像数据，并且如果您发现了更多改进的图像增强模型，请在下面的响应块中分享！</p><p id="65bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章的全部代码可以在<a class="ae kv" href="https://github.com/harshilpatel99/image_superResolution" rel="noopener ugc nofollow" target="_blank"> <em class="nh">这里</em> </a>找到。</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><h2 id="a59b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">参考</h2><ol class=""><li id="061d" class="ml mm iq ky b kz mn lc mo lf mp lj mq ln mr lr ms mt mu mv bi translated">Snehan Kekre在<a class="ae kv" href="http://www.coursera.org/projects/image-super-resolution-autoencoders-keras?edocomorp=freegpmay2020" rel="noopener ugc nofollow" target="_blank"> <em class="nh"> Coursera </em> </a>上的一门课程。</li></ol></div></div>    
</body>
</html>