<html>
<head>
<title>Building a Convolutional Neural Network to Recognize Shaved vs UnShaved Faces</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">建立一个卷积神经网络来识别刮胡子和未刮胡子的人脸</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-convolutional-neural-network-to-recognize-shaved-vs-unshaved-faces-cb96ea5bc0f0?source=collection_archive---------40-----------------------#2020-06-21">https://towardsdatascience.com/building-a-convolutional-neural-network-to-recognize-shaved-vs-unshaved-faces-cb96ea5bc0f0?source=collection_archive---------40-----------------------#2020-06-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="59c1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用计算机视觉和Keras深度学习API构建CNN模型的代码指南。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/db0353ef2b34f844b9a48225c61c5e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_0_hF6n0Qhhz2p5cTPZzdw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乔希·里纳拍摄的图片</p></figure><p id="6179" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我们将使用由<a class="ae kv" href="https://medium.com/@jacob.tadesse/web-scraping-stock-images-using-google-selenium-and-python-8b825ba649b9" rel="noopener">报废免费库存照片网站</a>创建的图像数据集。该图像集包含大约2000张被标记为“刮胡子”或“未刮胡子”的个人图像。我们将结合计算机视觉和机器学习，使用卷积神经网络(CNN)对图像进行分类。</p><p id="291a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本教程结束时，您将能够:</p><ol class=""><li id="6996" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">从头开始构建CNN模型并使用迁移学习</li><li id="d1ea" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">可视化模型结构、隐藏层和评估指标</li><li id="0865" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">保存您的CNN模型，以便重复使用和/或部署</li></ol><h1 id="95a8" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">导入库和加载图像数据帧</h1><p id="be06" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">要继续编码，请从Google Drive下载我的<a class="ae kv" href="https://drive.google.com/file/d/15FIs0RUgC_UQ7_aSLlH84B4IX0VYditS/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">图像数据集</a>。所有图像数据都存储在Pandas dataframe中，包括原始图像数组，以及大小调整为700x700、300x300、150x150和50x50的图像数组。</p><p id="698a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用pickle来“解钩”和加载熊猫数据帧。我们使用Keras从头开始构建CNN，并利用VGG16预训练模型。我们将使用Pyplot来可视化图像数据，并可视化CNN隐藏层中的中间激活。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="6455" class="ni mh iq ne b gy nj nk l nl nm">import pickle<br/>import pandas as pd<br/>import numpy as np   <br/>from numpy import asarray<br/>from matplotlib import image<br/>from matplotlib import pyplot<br/>from keras.utils import to_categorical<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout<br/>from keras.callbacks import EarlyStopping</span><span id="3592" class="ni mh iq ne b gy nn nk l nl nm"># loading the data frame using pickle<br/>data = pickle.load( open( "data_pickle_final.p", "rb" ) )</span><span id="d6be" class="ni mh iq ne b gy nn nk l nl nm"># creating a list of image arrays to preview<br/>size_list = data.iloc(0)[0][-5:]</span><span id="fbc2" class="ni mh iq ne b gy nn nk l nl nm"># iterating through list to view images<br/>for i in size_list:<br/>    pyplot.imshow(i)<br/>    pyplot.show();</span></pre><h1 id="3576" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">预览数据帧</h1><p id="5a8a" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在这里，我们可以看到保存在图像集中的图像数据。为了确保图像不是堆叠的，我们将在执行训练、测试、评估分割之前打乱图像的顺序。这可以使用内置的pandas方法<em class="no">来实现。样本()</em>。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="9a6a" class="ni mh iq ne b gy nj nk l nl nm"># preview dataframe information<br/>display(data.info())</span><span id="37c1" class="ni mh iq ne b gy nn nk l nl nm"># preview first 5 rows in dataframe<br/>display(data.head())</span><span id="dd66" class="ni mh iq ne b gy nn nk l nl nm"># shuffle order of images and preview first 5 images<br/>resampled_data = data.sample(frac=1)<br/>resampled_data.head()</span></pre><p id="b075" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此时，我们将选择我们将要使用的图像形状。在你自己的项目中，你可能需要根据自己的需要来抓取照片，所以请查看我的文章<a class="ae kv" href="https://medium.com/@jacob.tadesse/web-scraping-stock-images-using-google-selenium-and-python-8b825ba649b9" rel="noopener">使用Google Selenium和Python </a>来抓取股票图片。好的，我们将选择尺寸为300 x 300的图像，然后继续前进。</p><h1 id="5a2e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">创建张量和定义变量</h1><p id="6bb4" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">最常见的像素格式是字节图像，其中该数字存储为8位整数，给出了从0到255的可能值范围。通常，0表示黑色，255表示白色。介于两者之间的值构成了不同的灰度。在我们的例子中，我们将使用共享红色、绿色和蓝色比例的彩色照片，即:“rbg”；其中0表示没有光线，255表示最大光线。我们将通过将每个值除以该范围中的最大数来归一化这些值，并且因为我们将每个图像存储为数组，所以我们将特征变量重新格式化为神经网络的张量。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="1937" class="ni mh iq ne b gy nj nk l nl nm"># Normalizing the image arrays<br/>X = resampled_data.img_data_300<br/>X = X/255</span><span id="c6e1" class="ni mh iq ne b gy nn nk l nl nm"># Creating an empty tensor to store image arrays<br/>tensor = np.zeros((X.shape[0],300,300,3))</span><span id="99f3" class="ni mh iq ne b gy nn nk l nl nm"># Iterating through image arrays to update tensor<br/>for idx, i in enumerate(X):<br/>    tensor[idx] = i</span><span id="eff5" class="ni mh iq ne b gy nn nk l nl nm"># Checking the tensor shape<br/>tensor.shape</span></pre><p id="6f05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们将定义我们的目标变量。我们将首先把我们的标签转换成神经网络的二进制目标，也称为一键编码。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="5622" class="ni mh iq ne b gy nj nk l nl nm"># creating list to store target codes<br/>target = []</span><span id="65f6" class="ni mh iq ne b gy nn nk l nl nm"># iterating through labels to change strings to numbersfor i in resampled_data['Labels']:<br/>  if i == 'shaved':<br/>    target.append(0)<br/>  elif i == 'unshaved':<br/>    target.append(1)</span></pre><p id="d4b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将把新的二进制目标添加到我们的数据帧中，并将目标变量定义为“y”。根据您下载数据帧的时间，这可能已经包含在标记为“目标”的数据帧中。如果没有，不用担心。就照着这个剧本。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="a261" class="ni mh iq ne b gy nj nk l nl nm"># updating dataframe to include target<br/>resampled_data['target'] = target</span><span id="4502" class="ni mh iq ne b gy nn nk l nl nm"># defining target variable<br/>y = resampled_data.target</span><span id="e8a3" class="ni mh iq ne b gy nn nk l nl nm"># checking the shape<br/>y.shape</span></pre><p id="e8c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经改组了我们的图像数据集，定义了我们的目标和特征变量；现在是我们进行训练-测试-评估分离的时候了。我们将使用Sklearn的<strong class="ky ir"> <em class="no"> train_test_split </em> </strong>模块来执行此操作。但是执行训练-测试-分割，我们将使用最后100个图像创建一个保留集，用于模型评估。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="54b3" class="ni mh iq ne b gy nj nk l nl nm"># importing train-test-split module from sklearn<br/>from sklearn.model_selection import train_test_split</span><span id="74cd" class="ni mh iq ne b gy nn nk l nl nm"><br/># Performing train-test split for model training<br/>X_train, X_test, y_train, y_test = train_test_split(tensor[:-100], <br/>                                                    y[:-100], <br/>                                                    test_size=0.20, <br/>                                                    random_state=42)</span><span id="ffdc" class="ni mh iq ne b gy nn nk l nl nm"># check the size of the train/test split<br/>for i in [X_train, X_test, y_train, y_test]:<br/>    print(len(i))</span><span id="507b" class="ni mh iq ne b gy nn nk l nl nm"># Defining hold out data for evaluation <br/>evals_tensors = tensor[-100:]<br/>evals_targets = y[-100:]</span></pre><h1 id="7e20" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">从头开始构建CNN模型</h1><p id="7bfe" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">卷积神经网络(CNN)是一种专门用于图像的深度学习算法，其中该算法识别图像内的模式，为图像中的不同模式/对象分配重要性，使用权重和偏差来识别输入图像之间的差异和相似性。这些CNN基于标记的图像进行训练。CNN的工作是将图像缩减成一种更容易处理的结构，而不损失权重/偏差，这是获得最佳结果的必要条件。</p><p id="675e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我们将建立我们的卷积神经网络层。我们将从添加一个连续层开始；其次是我们的2D卷积层，最大池层，密集层，和辍学层。</p><p id="d137" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代码如下:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="4ec0" class="ni mh iq ne b gy nj nk l nl nm">model = Sequential()<br/>model.add(Conv2D(32,(4,4),activation='relu',input_shape=(300,300,3)))<br/>model.add(Conv2D(32,(4,4),activation='relu', padding='same'))<br/>model.add(Conv2D(128,(4,4),activation='relu', padding='same'))<br/>model.add(Conv2D(128,(4,4),activation='relu', padding='same'))<br/>model.add(MaxPooling2D((2, 2)))<br/>model.add(Dropout(.3))<br/>model.add(Conv2D(256, (4,4), activation='relu', padding='same'))<br/>model.add(Conv2D(256, (4,4), activation='relu', padding='same'))<br/>model.add(Conv2D(128, (4,4), activation='relu', padding='same'))<br/>model.add(Conv2D(128, (4,4), activation='relu', padding='same'))<br/>model.add(MaxPooling2D((2, 2)))<br/>model.add(Dropout(.3))<br/>model.add(MaxPooling2D((2, 2)))<br/>model.add(Dense(1024, activation='relu'))<br/>model.add(Dense(512, activation='relu'))<br/>model.add(Dropout(.3))<br/>model.add(Flatten())<br/>model.add(Dense(32, activation='relu'))<br/>model.add(Dense(1, activation='sigmoid'))<br/>model.summary()</span></pre><p id="91e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2D卷积层使得CNN在图像预测方面如此强大。该层使用内核/过滤器(4x4)来检查每个通道的所有像素；从左到右、从上到下、从前到后开始；卷积所有像素(灰度、RGB等)。该滤波器将使用特征内的像素数据执行点积乘法，以便从输入图像中提取特征，例如边缘。</p><p id="404d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一个Conv2D层捕捉基本特征，如边缘、颜色、大小等。随着更多的图层添加到网络中，该算法会从输入图像中识别复杂的要素。我们使用ReLu或修正线性激活函数来训练我们的深度网络，因为ReLU往往比用其他激活函数训练收敛得更快更可靠。</p><p id="680c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最大池层减小了卷积特征的大小，降低了降维所需的计算能力，提取了主要特征，还抑制了噪声。</p><p id="d3ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将对输出图层使用sigmoid激活函数，因为它存在于(0到1)之间，并且我们希望预测输出为剃齿或未剃齿的概率。</p><h1 id="5344" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">编译、拟合和评估模型</h1><p id="7e7d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">现在我们已经建立了CNN模型，我们需要编译、拟合和评估这个模型。我们将使用早期停止来帮助我们确定二元交叉熵损失函数的全局最小值。我们将存储每个时期的准确性和损失度量，以便可视化模型随时间的性能。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="e139" class="ni mh iq ne b gy nj nk l nl nm"># timing the training<br/>import time<br/>start_time = time.time()</span><span id="818f" class="ni mh iq ne b gy nn nk l nl nm"># compiling the model<br/>model.compile(loss='binary_crossentropy',<br/>              optimizer='adam',<br/>              metrics=['accuracy'])</span><span id="fd57" class="ni mh iq ne b gy nn nk l nl nm"># setting upearly stopping<br/>es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)</span><span id="3b1f" class="ni mh iq ne b gy nn nk l nl nm"># fitting the model to train/test data<br/>history = model.fit(X_train, y_train,<br/>                      batch_size=128, epochs=1000, <br/>                      validation_data=(X_test, y_test),<br/>                      callbacks=[es], verbose=1)</span><span id="02bf" class="ni mh iq ne b gy nn nk l nl nm"># printing training time<br/>print("--- %s seconds ---" % (time.time() - start_time))</span><span id="4762" class="ni mh iq ne b gy nn nk l nl nm"># evaluating the model<br/>train_loss, train_acc = model.evaluate(X_train, y_train)<br/>test_loss, test_acc = model.evaluate(X_test, y_test)</span><span id="ee1e" class="ni mh iq ne b gy nn nk l nl nm"># checking model performance by accuracy metrics<br/>print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))</span></pre><h1 id="8b03" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">评估培训结果</h1><p id="1470" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在这里，我们将使用PyPlot来可视化我们的模型在训练和测试期间的性能。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="f313" class="ni mh iq ne b gy nj nk l nl nm"># plotting the loss during training<br/>plt.plot(history.history['loss'], label='train')<br/>ply.plot(history.history['val_loss'], label='test')<br/>plt.title('Training vs Test Loss')<br/>plt.xlabel('Epoch')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show()</span><span id="8f3a" class="ni mh iq ne b gy nn nk l nl nm"># plotting the accuracy during training<br/>plt.plot(history.history['accuracy'], label='train')<br/>plt.plot(history.history['val_accuracy'], label='test')<br/>plt.title('Training vs Test Accuracy')<br/>plt.xlabel('Epoch')<br/>plt.ylabel('Accuracy')<br/>plt.legend()<br/>plt.show()</span></pre><p id="0bbe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经有了相当精确的模型，我们将使用它来对我们坚持的评估集进行预测。</p><h1 id="7c00" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">评估模型预测</h1><p id="a2f9" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">模型没有看到这个数据，应该可以做出准确的预测。让我们看看评估目标vs预测，也看看评估图像集中的图像。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="6218" class="ni mh iq ne b gy nj nk l nl nm"># making predictions using evaluation features<br/>res = model.predict(evals_tensors)</span><span id="a2e7" class="ni mh iq ne b gy nn nk l nl nm"># iterating through prediction results, and evaluation features<br/>for i, j, k in zip(res, evals_tensors, evals_targets):<br/>    if i &gt;= .5:<br/>      _ = 'Unshaved'<br/>      print('Prediction:', round(i[0],2), '-', _) # printing test<br/>    else:<br/>      _ = 'Shaved'<br/>      print('Prediction:', round(i[0],2), '-',_) # printing test<br/>    print('Actual:', k)<br/>    pyplot.imshow(j)<br/>    pyplot.show();</span></pre><p id="bb44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还可以利用Sklearn的指标来查看分类报告，并查看将预测与实际结果进行比较的混淆矩阵。我们将在下面创建测试集和评估集。</p><p id="5cbc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代码如下:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="9e48" class="ni mh iq ne b gy nj nk l nl nm">from sklearn.metrics import classification_report, confusion_matrix</span><span id="40cc" class="ni mh iq ne b gy nn nk l nl nm"># making predictions<br/>Y_pred = model.predict(X_test)<br/>Y_pred_train = model.predict(evals_tensors)</span><span id="bf66" class="ni mh iq ne b gy nn nk l nl nm"># creating lists to store predictions<br/>y_preds = []<br/>y_preds_train = []</span><span id="ff0e" class="ni mh iq ne b gy nn nk l nl nm"># iterating through results to update Test prediction lists<br/>for i in [round(i[0],0) for i in Y_pred]:<br/>    if i &gt; 0.5:<br/>        y_preds.append(1)<br/>    else:<br/>        y_preds.append(0)<br/>        <br/># iterating through results to update Evaluation prediction lists<br/>for i in [round(i[0],0) for i in Y_pred_train]:<br/>    if i &gt; 0.5:<br/>        y_preds_train.append(1)<br/>    else:<br/>        y_preds_train.append(0)</span></pre><p id="6655" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是打印混淆矩阵和分类报告的代码。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="7ddf" class="ni mh iq ne b gy nj nk l nl nm"># printing the confusion matrix<br/>print('Confusion Matrix - Test Set')<br/>print(confusion_matrix(y_test.values.tolist(), y_preds))<br/>print('Confusion Matrix - Evaluation Set')<br/>print(confusion_matrix(evals_targets.values.tolist(), y_preds_train))</span><span id="227b" class="ni mh iq ne b gy nn nk l nl nm"># printing the classification report<br/>print('Classification Report - Test Set')<br/>print(classification_report(y_test.values.tolist(), y_preds, target_names=['Shaved','Unshaved']))<br/>print('Classification Report - Evaluation Set')<br/>print(classification_report(evals_targets.values.tolist(), y_preds_train, target_names=['Shaved','Unshaved']))</span></pre><h1 id="fcc0" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">绘制模型结构并可视化中间激活</h1><p id="a5b5" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">有这么多隐藏层，很难理解模型在引擎盖下做什么。虽然不是模型执行所必需的，但这是可视化模型结构和查看中间激活的好方法。</p><p id="59e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此代码将把您的模型结构保存为PDF文件:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="1229" class="ni mh iq ne b gy nj nk l nl nm"># Saving the model structure<br/>import pydot<br/>pydot.find_graphviz = lambda: True<br/>from keras.utils import plot_model<br/>plot_model(model, show_shapes=True, to_file='model_pdf/{}.pdf'.format('Model_Structure'))</span></pre><p id="fd08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这段代码将为中间激活层保存一个图像:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="146f" class="ni mh iq ne b gy nj nk l nl nm"># Saving intermediate activations<br/>from keras import models<br/>import math</span><span id="e03a" class="ni mh iq ne b gy nn nk l nl nm"># Creating list of layer outputs<br/>layer_outputs = [layer.output for layer in model.layers[:9]]<br/>activation_model = models.Model(inputs=model.input, outputs=layer_outputs)<br/>activations = activation_model.predict(img_tensor)</span><span id="e17d" class="ni mh iq ne b gy nn nk l nl nm"># Extract Layer Names for Labelling<br/>layer_names = []<br/>for layer in activation_model.layers[:9]:<br/>    layer_names.append(layer.name)</span><span id="3700" class="ni mh iq ne b gy nn nk l nl nm"># Defining number of images per row<br/>images_per_row = 16</span><span id="31df" class="ni mh iq ne b gy nn nk l nl nm"># iterating through layer names and activations<br/>for layer_name, layer_activation in zip(layer_names, activations): <br/>    n_features = layer_activation.shape[-1] <br/>    size = layer_activation.shape[1] <br/>    n_cols = n_features // images_per_row <br/>    display_grid = np.zeros((size * n_cols, images_per_row * size))<br/>    for col in range(n_cols): # Tiles each filter into a big horizontal grid<br/>        for row in range(images_per_row):<br/>            channel_image = layer_activation[0,<br/>                                             :, :,<br/>                                             col * images_per_row + row]<br/>            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable<br/>            channel_image /= channel_image.std()<br/>            channel_image *= 64<br/>            channel_image += 128<br/>            channel_image = np.clip(channel_image, 0, 255).astype('uint8')<br/>            display_grid[col * size : (col + 1) * size, # Displays the grid<br/>                         row * size : (row + 1) * size] = channel_image<br/>    scale = 1. / size<br/>    plt.figure(figsize=(scale * display_grid.shape[1],<br/>                        scale * display_grid.shape[0]))<br/>    plt.title(layer_name)<br/>    plt.grid(False)<br/>    plt.imshow(display_grid, aspect='auto', cmap='viridis')<br/>plt.savefig(f'intermediate_activation_visualizations/unshaved/{layer_name}')</span></pre><h1 id="1371" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">使用迁移学习构建CNN</h1><p id="f26c" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">这里，我们将利用来自Kera预训练模型之一的imagenet权重，<strong class="ky ir"> VGG16 </strong>。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="1a69" class="ni mh iq ne b gy nj nk l nl nm">from keras.applications import vgg16<br/>from keras.models import Model<br/>import keras</span><span id="302e" class="ni mh iq ne b gy nn nk l nl nm">input_shape = (300, 300, 3)<br/>vgg = vgg16.VGG16(include_top=False, weights='imagenet', <br/>                                     input_shape=input_shape)</span><span id="75d7" class="ni mh iq ne b gy nn nk l nl nm">output = vgg.layers[-1].output<br/>output = keras.layers.Flatten()(output)<br/>vgg_model = Model(vgg.input, output)</span><span id="a495" class="ni mh iq ne b gy nn nk l nl nm">vgg_model.trainable = False<br/>for layer in vgg_model.layers:<br/>    layer.trainable = False<br/>    <br/>import pandas as pd<br/>pd.set_option('max_colwidth', -1)<br/>layers = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]<br/>pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])</span><span id="cd09" class="ni mh iq ne b gy nn nk l nl nm">vgg.summary()</span></pre><p id="dea8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们将使用预训练的模型层重新构建我们的模型，并添加类似于上面所做的其他层。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="6d01" class="ni mh iq ne b gy nj nk l nl nm">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer<br/>from keras.models import Sequential<br/>from keras import optimizers</span><span id="ee23" class="ni mh iq ne b gy nn nk l nl nm">model = Sequential()<br/>model.add(vgg_model)<br/>model.add(Dense(512, activation='relu', input_dim=input_shape))<br/>model.add(Dropout(0.3))<br/>model.add(Dense(32, activation='relu'))<br/>model.add(Dense(1, activation='sigmoid'))</span><span id="0c01" class="ni mh iq ne b gy nn nk l nl nm">model.summary()</span></pre><p id="0d18" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将编译、拟合和评估我们的新模型。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="f1f4" class="ni mh iq ne b gy nj nk l nl nm">from keras.callbacks import EarlyStopping</span><span id="5d5f" class="ni mh iq ne b gy nn nk l nl nm">model.compile(loss='binary_crossentropy',<br/>              optimizer='adam',<br/>              metrics=['accuracy'])</span><span id="c315" class="ni mh iq ne b gy nn nk l nl nm"># simple early stopping<br/>es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10) ## patience=200</span><span id="e7d2" class="ni mh iq ne b gy nn nk l nl nm">history = model.fit(X_train, y_train,<br/>                      batch_size=128, epochs=1000, <br/>                      validation_data=(X_test, y_test),<br/>                      callbacks=[es], verbose=1)</span><span id="b527" class="ni mh iq ne b gy nn nk l nl nm">train_loss, train_acc = model.evaluate(X_train, y_train)<br/>test_loss, test_acc = model.evaluate(X_test, y_test)</span><span id="7f20" class="ni mh iq ne b gy nn nk l nl nm">print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))</span></pre><p id="391b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以评估来自培训的指标。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="0a3b" class="ni mh iq ne b gy nj nk l nl nm">import matplotlib.pyplot as plt</span><span id="3feb" class="ni mh iq ne b gy nn nk l nl nm">plt.plot(history.history['loss'], label='train')<br/>plt.plot(history.history['val_loss'], label='test')<br/>plt.title('Loss')<br/>plt.legend()<br/>plt.show()</span><span id="ebfb" class="ni mh iq ne b gy nn nk l nl nm">plt.plot(history.history['accuracy'], label='train')<br/>plt.plot(history.history['val_accuracy'], label='test')<br/>plt.title('Accuracy')<br/>plt.legend()<br/>plt.show()</span></pre><p id="9bc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还可以评估混淆矩阵和分类报告的结果，看看我们的模型是否比我们从头构建的模型表现得更好。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="f688" class="ni mh iq ne b gy nj nk l nl nm">from sklearn.metrics import classification_report, confusion_matrix</span><span id="9a92" class="ni mh iq ne b gy nn nk l nl nm">#Confution Matrix and Classification Report<br/>Y_pred = model.predict(X_test)<br/>Y_pred_train = model.predict(evals_tensors)</span><span id="b9b2" class="ni mh iq ne b gy nn nk l nl nm">y_preds = []<br/>y_preds_train = []<br/>for i in [round(i[0],0) for i in Y_pred]:<br/>    if i &gt; 0.5:<br/>        y_preds.append(1)<br/>    else:<br/>        y_preds.append(0)<br/>        <br/>for i in [round(i[0],0) for i in Y_pred_train]:<br/>    if i &gt; 0.5:<br/>        y_preds_train.append(1)<br/>    else:<br/>        y_preds_train.append(0)</span><span id="0d5e" class="ni mh iq ne b gy nn nk l nl nm">print('Confusion Matrix - Test Set')<br/>print(confusion_matrix(y_test.values.tolist(), y_preds))<br/>print('Confusion Matrix - Evaluation Set')<br/>print(confusion_matrix(evals_targets.values.tolist(), y_preds_train))</span><span id="a8d4" class="ni mh iq ne b gy nn nk l nl nm">print('Classification Report - Test Set')<br/>print(classification_report(y_test.values.tolist(), y_preds, target_names=['Shaved','Unshaved']))<br/>print('Classification Report - Evaluation Set')<br/>print(classification_report(evals_targets.values.tolist(), y_preds_train, target_names=['Shaved','Unshaved']))</span></pre><p id="93e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在可以看到，这个模型表现得更加准确。</p><h1 id="3316" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">保存最终模型</h1><p id="052d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">现在我们有了一个在看不见的图像上表现良好的实体模型，我们可以保存这个模型和权重。这将允许我们部署这个模型，或者甚至与对类似项目感兴趣的其他人共享这个文件。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="7ac7" class="ni mh iq ne b gy nj nk l nl nm"># Saving whole model<br/>model.save('/content/drive/My Drive/Model_Files/Final_Model.h5')</span><span id="b7db" class="ni mh iq ne b gy nn nk l nl nm"># Loading the whole model<br/>from keras.models import load_model<br/>loaded_model = load_model('/content/drive/My Drive/Final_Model.h5')<br/>loaded_model.compile(loss='binary_crossentropy', <br/>                     optimizer='adam', metrics=['accuracy'])</span><span id="6a29" class="ni mh iq ne b gy nn nk l nl nm"># Testing predictions using loaded model<br/>res2 = loaded_model.predict(evals_tensors)<br/>results2= []<br/>for i in res2:<br/>    results2.append(round(i[0],2))<br/>compare = zip(results2, evals_targets)</span><span id="3b78" class="ni mh iq ne b gy nn nk l nl nm"># View the comparison<br/>for i in compare:<br/>  print(i)</span></pre><p id="cef0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们从pickle文件开始，如果不将我们的模型保存为pickle文件，本教程将是不完整的。要将您的模型保存为pickle文件，只需运行以下代码:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="ed9c" class="ni mh iq ne b gy nj nk l nl nm">filename = 'finalized_model.p'<br/>pickle.dump(loaded_model, open(filename, 'wb'))<br/> <br/># some time later...<br/> <br/># load the model from disk<br/>loaded_model = pickle.load(open(filename, 'rb'))<br/>result = loaded_model.score(X_test, Y_test)</span></pre><h1 id="a6af" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="74f6" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">现在我们来复习一下。首先，我们导入我们的库和映像集。然后，我们对数据进行重新采样，定义和缩放我们的特征，并将图像存储在张量中。我们一次性编码了我们的目标变量；然后将我们的数据分成训练/测试/评估子集。我们从零开始建立了一个CNN模型，还使用预训练的VGG16和迁移学习建立了一个模型；将每个模型拟合到我们的图像集，并使用adam优化器编译每个模型，使用binary_crossentropy损失函数来提高我们在向前和向后传播期间的准确性度量。</p><p id="0e2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">培训完成后，我们能够评估指标，并可视化培训结果、模型结构和中间激活。最后，我们能够将整个模型保存为一个. h5文件，也可以保存为一个pickle文件。我们流程的下一步是部署我们的模型供公众使用。我将使用<a class="ae kv" href="http://jtadesse.pythonanywhere.com/" rel="noopener ugc nofollow" target="_blank"> flask构建一个python应用程序</a>，并将其托管在<a class="ae kv" href="https://www.pythonanywhere.com/?affiliate_id=0077ae73" rel="noopener ugc nofollow" target="_blank"> PythonAnywhere </a>上，请继续关注本系列的下一篇文章。</p><p id="5c2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里是最终产品:【http://jtadesse.pythonanywhere.com/】T4</p><p id="77b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在这里找到我的GitHub库<a class="ae kv" href="https://github.com/cousinskeeta/machine-learning-using-computer-vision" rel="noopener ugc nofollow" target="_blank"/>。谢谢大家！</p></div></div>    
</body>
</html>