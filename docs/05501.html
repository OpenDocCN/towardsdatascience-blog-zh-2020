<html>
<head>
<title>Battle of the Transformers: ELECTRA, BERT, RoBERTa, or XLNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚之战:伊莱克特拉，伯特，罗伯塔，还是 XLNet</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/battle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3?source=collection_archive---------11-----------------------#2020-05-09">https://towardsdatascience.com/battle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3?source=collection_archive---------11-----------------------#2020-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3811" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">伊莱克特拉是新来的。让我们来看看它是如何对抗老守卫的！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/158eef0e612f46737908e0a14e30c080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O322S3aigsTRzDi1Q1teKw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/272447-272447/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=569288" rel="noopener ugc nofollow" target="_blank"> 272447 </a>来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=569288" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="c312" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Transformer 模型成功背后的“秘密”之一是迁移学习技术。在迁移学习中，一个模型(在我们的例子中是一个 Transformer 模型)使用一个无监督的<em class="lv">预训练目标在一个巨大的数据集上<em class="lv">预训练</em>。</em>同样的模型然后<em class="lv">在手头的实际任务中进行微调</em>(通常是监督训练)。这种方法的美妙之处在于<em class="lv">微调</em>数据集可以小到 500–1000 个训练样本！如果称之为深度学习，这个数字小到可能会被嗤之以鼻。这也意味着昂贵且耗时的流水线部分<em class="lv">预训练</em>只需进行一次，并且<em class="lv">预训练</em>模型可在此后的任意数量的任务中重复使用。因为<em class="lv">预先训练的</em>模型通常是公开的🙏，我们可以获取相关模型，<em class="lv">在自定义数据集上对其进行微调</em>，几个小时后就有一个最先进的模型可以使用了！</p><p id="c0d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有兴趣了解预训练如何工作，以及如何在单个 GPU 上训练一个全新的语言模型，请查看下面链接的我的文章！</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/understanding-electra-and-training-an-electra-language-model-3d33e3a9660d"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">理解 ELECTRA 并训练一个 ELECTRA 语言模型</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">变形金刚模型如何学习语言？伊莱克特拉有什么新消息？你如何在一个…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn ks lz"/></div></div></a></div><p id="4f91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ELECTRA 是谷歌发布的<em class="lv">预训练</em>变形金刚模型的最新类别之一，与大多数其他版本相比，它有所改变。在很大程度上，Transformer 模型遵循了人们熟知的深度学习路径，更大的模型、更多的训练和更大的数据集等于更好的性能。然而，ELECTRA 通过使用更少的计算能力、更小的数据集、<em class="lv">和</em>更少的训练时间，超越了早期的模型，如 BERT，从而扭转了这种趋势。<em class="lv">(如果你想知道，伊莱克特拉和伯特的“尺寸”是一样的)。</em></p><p id="d9ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将看看如何使用一个<em class="lv">预训练的</em> ELECTRA 模型进行文本分类，并在此过程中与其他标准模型进行比较。具体来说，我们将比较下面列出的每个模型的最终性能(马修斯相关系数<em class="lv"> (MCC) </em>)和训练时间。</p><ul class=""><li id="889c" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">electra-小型</li><li id="49a2" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">电子基础</li><li id="cc0c" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">伯特基础案例</li><li id="e3c5" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">蒸馏底壳的</li><li id="b210" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">蒸馏贝塔碱</li><li id="319c" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">罗伯塔基地</li><li id="6caa" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">xlnet-base-cased</li></ul><p id="f290" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和往常一样，我们将使用<a class="ae ky" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank">简单变形金刚</a>库(基于拥抱脸<a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>库)来完成这项工作，并且我们将使用<a class="ae ky" href="https://www.wandb.com/" rel="noopener ugc nofollow" target="_blank">权重&amp;偏差</a>进行可视化。</p><p id="2590" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以在库的<a class="ae ky" href="https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/text_classification/yelp_reviews_polarity" rel="noopener ugc nofollow" target="_blank">示例目录</a>中找到这里使用的所有代码。</p><h1 id="d3b5" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">装置</h1><ol class=""><li id="21aa" class="mo mp it lb b lc nu lf nv li nw lm nx lq ny lu nz mu mv mw bi translated">从<a class="ae ky" href="https://www.anaconda.com/distribution/" rel="noopener ugc nofollow" target="_blank">这里</a>安装 Anaconda 或 Miniconda 包管理器。</li><li id="5530" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu nz mu mv mw bi translated">创建新的虚拟环境并安装软件包。<br/><code class="fe oa ob oc od b">conda create -n simpletransformers python pandas tqdm</code><br/><code class="fe oa ob oc od b">conda activate simpletransformers</code><br/>T2】</li><li id="a5ed" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu nz mu mv mw bi translated">如果您使用 fp16 培训，请安装 Apex。请遵循此处的说明<a class="ae ky" href="https://github.com/NVIDIA/apex" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="aba6" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu nz mu mv mw bi translated">安装简单变压器。<br/>T3】</li></ol><h1 id="8677" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">数据准备</h1><p id="0ae3" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">我们将使用 Yelp 评论极性数据集，这是一个二元分类数据集。下面的脚本将下载它并存储在<code class="fe oa ob oc od b">data</code>目录中。或者，您可以从<a class="ae ky" href="https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz" rel="noopener ugc nofollow" target="_blank"> FastAI </a>手动下载数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h1 id="39e6" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">超参数</h1><p id="884f" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">一旦数据在<code class="fe oa ob oc od b">data</code>目录中，我们就可以开始训练我们的模型。</p><p id="baf8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单的变压器模型可以进行广泛的配置(见<a class="ae ky" href="https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model" rel="noopener ugc nofollow" target="_blank">文档</a>)，但是我们将只进行一些基本的、“足够好的”超参数设置。这是因为我们更感兴趣的是在平等的基础上相互比较模型，而不是试图优化每个模型的绝对最佳超参数。</p><p id="00ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到这一点，我们将把<code class="fe oa ob oc od b">train_batch_size</code>增加到<code class="fe oa ob oc od b">128</code>，并且我们将把<code class="fe oa ob oc od b">num_train_epochs</code>增加到<code class="fe oa ob oc od b">3</code>，这样所有的模型都将有足够的训练来收敛。</p><p id="e3d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里需要注意的是，XLNet 的<code class="fe oa ob oc od b">train_batch_size</code>减少到了<code class="fe oa ob oc od b">64</code>，因为它不能在 RTX 泰坦 GPU 上用<code class="fe oa ob oc od b">train_batch_size=128</code>进行训练。但是，通过将<code class="fe oa ob oc od b">gradient_accumulation_steps</code>设置为<code class="fe oa ob oc od b">2</code>，将有效批量更改为<code class="fe oa ob oc od b">128</code>，可以将这种差异的影响降至最低。(每两步只计算一次梯度并更新一次模型权重)</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="2826" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">影响训练的所有其他设置都保持默认值不变。</p><h1 id="8bef" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">训练模型</h1><p id="1845" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">设置培训流程非常简单。我们只需要将数据加载到数据帧中并定义超参数，然后我们就可以开始比赛了！</p><p id="df75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了方便起见，我使用相同的脚本来训练所有模型，因为我们只需要在每次运行之间更改模型名称。模型名称由 shell 脚本提供，该脚本还会自动为每个模型运行训练脚本。</p><p id="d181" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">培训脚本如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="7b31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">注意，Yelp 评论极性数据集使用标签</em> <code class="fe oa ob oc od b"><em class="lv">[1, 2]</em></code> <em class="lv">分别表示正极和负极。我把这个改成</em> <code class="fe oa ob oc od b"><em class="lv">[0, 1]</em></code> <em class="lv">分别表示否定和肯定。简单变形金刚要求标签从</em> <code class="fe oa ob oc od b"><em class="lv">0</em></code> <em class="lv">(咄！)而负面情绪的一个标签</em> <code class="fe oa ob oc od b"><em class="lv">0</em></code> <em class="lv">就直观了很多(在我看来)。</em></p><p id="547f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以自动化整个过程的 bash 脚本:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="083c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">注意，您可以通过向 bash 脚本添加</em> <code class="fe oa ob oc od b"><em class="lv">rm -r outputs</em></code> <em class="lv">来删除每个阶段保存的模型。如果你没有足够的磁盘空间，这可能是个好主意。</em></p><p id="1a1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练脚本还将记录权重和偏差的评估分数，让我们可以轻松地比较模型。</p><p id="d95b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关训练分类模型的更多信息，请查看简单转换程序文档。</p><h1 id="59de" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">结果</h1><p id="1fd4" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">你可以在这里找到我所有的结果<a class="ae ky" href="https://app.wandb.ai/thilina/Classification%20Model%20Comparison?workspace=user-thilina" rel="noopener ugc nofollow" target="_blank">。尝试使用不同的图表和信息！</a></p><p id="957a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们回顾一下重要的结果。</p><h2 id="5d30" class="oj nd it bd ne ok ol dn ni om on dp nm li oo op no lm oq or nq lq os ot ns ou bi translated">最终分数</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/06dc24c285d307eab718755b80bb21f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cX_ImDK_NDRZ1J_ic40apw.png"/></div></figure><p id="449c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些是每个模型获得的最终 MCC 分数。如你所见，所有型号的分数都非常接近。</p><p id="ebea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更好地了解差异，下面的图表放大到 X 轴，只显示范围 0.88-0.94。</p><p id="1477" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">请注意，放大视图虽然有助于发现差异，但会扭曲对结果的感知。因此，下表仅用于说明目的。</em> <strong class="lb iu"> <em class="lv">当心隐藏了零的图！</em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/03f6e4fd484717df1044c8e566db6f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fwabG8DcYfx_AucD1ybYTg.png"/></div></figure><p id="ef14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe oa ob oc od b">roberta-base</code>型号领先，<code class="fe oa ob oc od b">xlnet-base</code>紧随其后。接下来是<code class="fe oa ob oc od b">distilroberta-base</code>和<code class="fe oa ob oc od b">electra-base</code>车型，它们之间几乎没有任何区别。老实说，在这种情况下，两者之间的差异可能更多地是由于随机机会而不是其他任何东西。最后，我们分别有<code class="fe oa ob oc od b">bert-base-cased</code>、<code class="fe oa ob oc od b">distilbert-base-cased</code>和<code class="fe oa ob oc od b">electra-small</code>。</p><p id="9942" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看实际值会发现它们非常接近。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="ba63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这次实验中，罗伯塔似乎比其他模特表现得更好。然而，我敢打赌，通过一些技巧，如超参数调整和集成，ELECTRA 模型能够弥补这一差异。这一点得到了当前 GLUE 基准<a class="ae ky" href="https://gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank">排行榜</a>的证实，在该排行榜中，伊莱克特拉位于罗伯塔之上。</p><p id="043d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重要的是要记住，与 RoBERTa 相比，ELECTRA 模型所需的<em class="lv">预培训资源要少得多(大约四分之一)。对于<code class="fe oa ob oc od b">distilroberta-base</code>也是如此。尽管<code class="fe oa ob oc od b">distilroberta-base</code>模型相对较小，但在<em class="lv">将</em>提取到<code class="fe oa ob oc od b">distilroberta-base</code>之前，您需要原始的<code class="fe oa ob oc od b">roberta-base</code>模型。</em></p><p id="b0c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XLNet 模型几乎与 RoBERTa 模型并驾齐驱，但它比这里显示的所有其他模型需要更多的计算资源(参见训练时间图)。</p><p id="2349" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">古老的(虽然不到两年)BERT 模型开始显示出它的年龄，并且被除了<code class="fe oa ob oc od b">electra-small</code>模型之外的所有模型超越。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/8e7559d0ccdaa98382b3ee054ad242fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*rLbsjW3C7LuzcmGvLsvHkw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://imgflip.com/i/40s1sv" rel="noopener ugc nofollow" target="_blank">https://imgflip.com/i/40s1sv</a></p></figure><p id="ea61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然不完全符合其他型号的标准，但<code class="fe oa ob oc od b">electra-small</code>型号的性能仍然令人钦佩。正如所料，它训练速度最快，内存需求最小，推理速度最快。</p><h2 id="8884" class="oj nd it bd ne ok ol dn ni om on dp nm li oo op no lm oq or nq lq os ot ns ou bi translated">说到训练时间…</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/ace90fe71a943bc509cf15c7bb927481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZwN8rnn7fQaH-U-ne7S-w.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/f292e240c1923eef9bca12a95bf4e798.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*ZB5lvw0r9DOfgOmaaalOHQ.png"/></div></figure><p id="8028" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练的速度主要由模型的大小(参数的数量)决定，XLNet 的情况除外。与 XLNet 一起使用的训练算法使得它比比较的 BERT、RoBERTa 和 ELECTRA 模型慢得多，尽管具有大致相同数量的参数。与这里测试的其他模型相比，XLNet 的 GPU 内存需求也更高，因此需要使用更小的训练批量(如前所述，64 比 128)。</p><p id="bb02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">推断时间(此处未测试)也应遵循这一总体趋势。</p><p id="d26b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，另一个重要的考虑因素是每个模型收敛的速度。<em class="lv">所有这些模型都训练了 3 个完整时期，没有使用早期停止。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/7a4d8bf55bdfe6e7c63d3cb4c7943e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KIpbeLfTn_mWsbFjrSi9XQ.png"/></div></figure><p id="ea28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，在收敛需要多少训练步骤方面，模型之间没有可辨别的差异。所有的模型似乎都集中在 9000 步左右。当然，收敛所需的<em class="lv">时间</em>会因训练速度的不同而不同。</p><h1 id="3c46" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">结论</h1><p id="bb53" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">很难在不同的变压器型号之间做出选择。然而，我们仍然可以从我们看到的实验中获得一些有价值的见解。</p><ul class=""><li id="ce07" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">根据具体情况，旧型号可能会超过 ELECTRA 型号。但是，它的优势在于它能够以明显更少的用于<em class="lv">预训练</em>的计算资源达到有竞争力的性能水平。</li><li id="9f70" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">ELECTRA <a class="ae ky" href="https://openreview.net/pdf?id=r1xMH1BtvB" rel="noopener ugc nofollow" target="_blank">论文</a>指出<code class="fe oa ob oc od b">electra-small</code>模型明显优于类似规模的 BERT 模型。</li><li id="985a" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">为了更快的训练和推断，Transformer 模型的精华版本牺牲了一些精度点。在某些情况下，这可能是一种理想的交换。</li><li id="c3e6" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">XLNet 牺牲了训练和推理的速度，以换取在复杂任务上可能更好的性能。</li></ul><p id="d059" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于这些见解，我可以提供以下建议(尽管应该有所保留，因为不同数据集之间的结果可能会有所不同)。</p><ul class=""><li id="9119" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">对于大多数任务来说，这似乎仍然是一个很好的起点。根据结果，您可以决定超参数调整，变大，或变小！</li><li id="0c4a" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">如果你正在训练一个新的语言模型，或者如果你想微调一个语言模型，我推荐使用 ELECTRA 方法。尤其是在计算资源和/或可用数据有限的情况下。</li></ul><p id="9492" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看看<em class="lv">大型</em>车型是否也追随这一趋势会很有趣。我希望在以后的文章中测试这一点(T5 也可能会加入进来)！</p><p id="53fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想看到一些关于不同模型的训练和推理速度的更深入的分析，请查看下面链接的我的早期文章(很遗憾，没有 ELECTRA)。</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">蒸馏还是不蒸馏:伯特、罗伯塔和 XLNet</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">变形金刚是自然语言处理中无可争议的王者。但是有这么多不同的模型，它可以…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="oy l mk ml mm mi mn ks lz"/></div></div></a></div></div></div>    
</body>
</html>