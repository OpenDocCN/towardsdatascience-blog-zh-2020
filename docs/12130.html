<html>
<head>
<title>LawBERT: Towards a Legal Domain-Specific BERT?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">走向一个特定法律领域的伯特？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lawbert-towards-a-legal-domain-specific-bert-716886522b49?source=collection_archive---------21-----------------------#2020-08-21">https://towardsdatascience.com/lawbert-towards-a-legal-domain-specific-bert-716886522b49?source=collection_archive---------21-----------------------#2020-08-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dab1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">法律行业的特定领域 BERT</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/735dfefe351f99f67c8bfbe9e7e24f8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fc7IRPT8FNMzeP8TCpVVWw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://www.bl.uk/collection-guides/modern-law-reports-and-legal-cases" rel="noopener ugc nofollow" target="_blank">大英图书馆</a></p></figure><p id="59d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谷歌来自变形金刚的双向编码器表示(BERT)是 2018 年开发的大规模预训练自动编码语言模型。它的发展被描述为 NLP 社区的“ImageNet 时刻”，主要是因为 BERT 在执行下游 NLP 语言理解任务时非常熟练，只需要很少的反向传播和微调(通常只有 2-4 个时期)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/602b6f3434088ea9dc9738f7ef7b4080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O42cqkWhFW1n2IzOGeWj7Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">德夫林等人(2019 年)</a></p></figure><p id="3f8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于上下文，传统的单词嵌入(例如 word2vec 和 GloVe)是<em class="lt">非上下文的。t</em>hey<em class="lt">T7】用单个静态向量表示每个单词令牌，通过单词共现而不是单词的顺序上下文来学习。当一词多义时(即同一个词有多个不同的意思)，这可能会有问题，这在法律中是很常见的。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lu"><img src="../Images/23bef0d77133fda4c4c6776e9d37d4b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SyY1GT1GvKifzpyLc18YEg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space" rel="noopener ugc nofollow" target="_blank">谷歌开发者</a></p></figure><p id="fade" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，单个静态上下文无关向量将用于表示句子“柠檬是酸的”和“那辆车是柠檬”中的单词“柠檬”(即，对柠檬法则的有趣引用，该法则在新车被证明有缺陷时保护消费者)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lv"><img src="../Images/e929c2f8e3b31f3af5966244e3c78a07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uySqg87Ay9vAyaJspgbHMA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://www.aclweb.org/anthology/D19-1006.pdf" rel="noopener ugc nofollow" target="_blank"> Ethayarajh (2019) </a></p></figure><p id="b54a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相比之下，BERT 是一个<em class="lt">上下文</em>模型，它基于每个单词周围的单词生成特定于上下文的表示。有趣的是，Ethayarajh (2019)表明，BERT 不会为每个词义创建一个“柠檬”的表示(左选项)，而是会创建无限多个“柠檬”的表示，每个表示都高度特定于其上下文(右选项)。这意味着不是生成单个密集向量来表示令牌“lemon”，而是将令牌动态地表示为“______ 是酸的”和“那辆车是 _____”。</p><p id="42ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">伯特捕捉语言多义性的能力尤其适用于多义性大量存在的法律领域。例如，法律中的“对价”一词代表契约关系中的互惠理念，与该词通常的含义“考虑周到”有着不同的含义。此外，同一术语在成文法和判例法中可以有多种定义。例如,“工人”一词在欧盟法律中有四种不同的定义，甚至在同一份文件中也可以用来指不同种类的工人。因此，语境语言模型的发展对法律人工智能领域具有重要意义。</p><p id="29ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BERT 体系结构的技术细节超出了本文的范围(已经有大量文章讨论过)。然而，值得注意的是，BERT 的关键创新之一是它的双向性，即它克服了顺序文本解析的传统问题。值得注意的是，这个上下文中的双向并不意味着在<em class="lt">顺序</em>意义上的双向(即同时从左到右和从右到左进行解析)，而是在<em class="lt">同时</em>意义上的双向(即它同时从所有层中的令牌的左右上下文中学习信息)。这是通过使用像掩蔽 LMs (MLMs)和下一句预测(NSP)这样的方法来实现的，以获得更好的上下文单词嵌入结果。</p><h1 id="f5fa" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated"><strong class="ak"> <em class="mo">特定领域的优势</em> </strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/b9875ac3548e77b15aaf9cb1dc314a1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GqM1xRkoLYCyGn4zx2GPjA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://www.britannica.com/story/what-is-the-difference-between-criminal-law-and-civil-law" rel="noopener ugc nofollow" target="_blank">大英百科全书</a></p></figure><p id="8211" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然 BERT 在执行一般语言表示任务方面非常有效，但问题是——在未标记的通用维基百科和开源文章上接受训练——它缺乏特定领域的知识。这是一个重要的警告，因为语言模型只能和它的语料库一样好。打个比方，将一个普通的伯特模型应用于特定法律领域的问题，可能相当于让一个文科生去解决一个法律问题，而不是让一个学了多年法律知识的法律学生去解决。这是有问题的，因为在一般开源语料库(例如维基百科和新闻文章)中发现的语言和法律语言之间有很多脱节，这些语言可能是深奥的和基于拉丁语的。</p><p id="debc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然在撰写本文时尚未开发出法律领域特定的 BERT 模型，但已经开发的其他领域特定的 BERT 的示例包括 BioBERT(生物医学科学)、SciBERT(科学出版物)、FinBERT(金融通讯)和 ClinicalBERT(临床笔记)。尽管它们都具有特定于领域的 BERTs 的相似性，但是它们的体系结构表现出许多关键的不同。</p><h1 id="6f04" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">培训特定法律领域的专家</h1><p id="9aa3" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ms lh li lj mt ll lm ln mu lp lq lr ij bi translated">我将探索一些培训特定领域业务技术人员的方法:</p><h2 id="1e4b" class="mv lx iq bd ly mw mx dn mc my mz dp mg lf na nb mi lj nc nd mk ln ne nf mm ng bi translated"><em class="mo">完全预先训练伯特</em></h2><p id="a5d2" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ms lh li lj mt ll lm ln mu lp lq lr ij bi translated">这包括用大规模未标记的法律语料库(例如，法令、判例)完全重做 BERT 的预训练过程。从这个意义上说，特定领域的知识将在预培训过程中注入。这是 SCIVOCAB SciBERT 采用的方法。</p><p id="4646" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然这确实大大提高了 BERT 在特定领域任务上的性能，但关键问题是完成重新训练所需的时间、成本和数据量可能太大而不可行。BERT-Base 本身是一个具有 12 个堆叠层和大约 1.1 亿个权重/参数的神经网络，其预训练语料库需要 33 亿个令牌。重新训练特定领域的 BERT 将需要类似数量的训练数据(例如，SciBERT 使用了 31.7 亿个令牌)。训练该模型需要一周或更长时间(例如，SCIVOCAB SciBERTs 在 v3 TPU 上训练需要一周时间),并且成本极高，这对于一般企业来说根本不可行。</p><h2 id="c070" class="mv lx iq bd ly mw mx dn mc my mz dp mg lf na nb mi lj nc nd mk ln ne nf mm ng bi translated">进一步预培训 BERT</h2><p id="7ba6" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ms lh li lj mt ll lm ln mu lp lq lr ij bi translated">因此，一种折衷方案可能是不完全重新训练 BERT，而是从 BERT 初始化权重，并用合法的特定领域数据进一步对其进行预训练。这已被证明可以提高 BERT 的性能，Sun (2019)表明，经过进一步预训练的模型在所有七个数据集上的表现都优于原始的 BERT-Base 模型。</p><p id="868d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这似乎是最受欢迎的方法，用于 BioBERT、BASEVOCAB SciBERT 和 FinBERT。研究人员没有完全从头开始重新训练，而是用从 BERT-Base 学习到的权重初始化新的 BERT 模型，然后在特定领域的文本上训练它(例如 PubMed abstracts 和 BioBERT 的 PMC 全文文章)。Lee 等人(2020)报告说，BioBERT 在所有数据集上取得了比 BERT 更高的 F1、精度和召回分数。同样，Beltagy 等人(2019)报告称，SciBert 在生物医学、计算机科学和多领域任务上表现出色。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/26dac84818d76995adfdabfb137c1ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uM0E_cqklqPrUJVfP14IPw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf" rel="noopener ugc nofollow" target="_blank">李等人(2019) </a></p></figure><h2 id="81ad" class="mv lx iq bd ly mw mx dn mc my mz dp mg lf na nb mi lj nc nd mk ln ne nf mm ng bi translated"><em class="mo">微调伯特</em></h2><p id="1630" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ms lh li lj mt ll lm ln mu lp lq lr ij bi translated">另一个更简单的选择是使用预先训练的 BERT，但在下游 NLP 任务中使用特定于法律领域的语料库对其进行微调。微调的概念来自迁移学习的领域，简单地说，这意味着采用一个旨在解决任务<em class="lt"> x </em>的模型，并将其重新用于解决任务<em class="lt"> y </em>。在实践中，它通常意味着采用预训练的 BERT 模型，并在最后添加一个未训练神经元的额外输出层，然后使用带标签的法律语料库进行训练(微调通常是一项监督任务)。在对特定于领域的数据进行微调之后，得到的模型将具有更新的权重，更接近于目标领域的特征和词汇</p><p id="96af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法的优点是，它需要的数据少得多，而且训练起来明显更快、更便宜。鉴于 BERT-Base 的预训练已经编码了一般领域中大多数单词的知识，它只需要一些调整就可以适应法律上下文的功能。微调通常只需要 2-4 个时期(几分钟的事情)，而不是几周的时间跨度。这种方法还需要更少的数据(尽管它通常需要带标签的数据)，这使它更可行。</p><p id="7182" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，如果需要，微调可以结合特定领域的预训练来完成，即它们不是互斥的方法。例如，BioBERT 首先接受生物医学领域特定语料库的预训练，然后在生物医学文本挖掘任务上进行微调，如命名实体识别、关系提取和 QA。</p><h1 id="f3e9" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated"><strong class="ak">结论</strong></h1><p id="157b" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ms lh li lj mt ll lm ln mu lp lq lr ij bi translated">随着最近对像 OpenAI 的 GPT-3 这样的超大规模模型的大肆宣传，最初围绕 BERT 的兴奋似乎有些消退。然而，值得记住的是，BERT 仍然是一个非常强大和敏捷的模型，可以说它有更多的实际应用。从特定领域的 BERT(如 SciBERT 和 BioBERT)的经验来看，在特定领域的语料库上微调和/或预训练 BERT 可能会提高法律 NLP 任务的性能。为此，根据研究人员的时间和金钱资源，有各种方法来改善和调整伯特的表现。</p></div></div>    
</body>
</html>