<html>
<head>
<title>All deep learning is statistical model building</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">所有的深度学习都是建立统计模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-deep-learning-is-statistical-model-building-fc310328f07?source=collection_archive---------19-----------------------#2020-08-03">https://towardsdatascience.com/all-deep-learning-is-statistical-model-building-fc310328f07?source=collection_archive---------19-----------------------#2020-08-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/a52d98411abbcdb3d5d767a2a356375e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*62JNtKp1yLIKKksF-PuHzQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><p id="64b0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">深度学习通常用于为数据驱动的分析进行预测。但是这些预测的意义是什么呢？</p><p id="0223" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这篇文章解释了深度学习中使用的神经网络如何提供描述事件发生概率的统计模型的参数。</p><h1 id="c95c" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">事件的发生和任意的不确定性</h1><p id="8801" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">数据、可观察的事物、事件或任何其他描述我们可以看到和/或收集的事物的方式都是绝对的:我们在一对六面骰子上掷出两个六，或者我们得到一些其他结果的组合；我们掷硬币 10 次，每次都是正面，或者是正面和反面的混合。我们的宇宙以某种方式进化，我们观察它，或者它没有——我们没有。我们不知道，<em class="mg">，</em>我们是否会在每次投掷硬币时掷骰子或掷人头得到两个六，或者有什么可能的宇宙存在，让我们能够形成并观察它。我们把这种由于缺乏知识而产生的不确定性描述为<em class="mg">任意性。</em>这是由于关于这种<em class="mg">数据生成的基本信息缺失——</em>我们永远无法确切知道我们将获得什么结果。我们可以认为随机不确定性是无法知道某个随机数生成过程的随机种子。</p><p id="6f7d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们用函数描述事件发生的概率，<em class="mg">p</em>‏‏‎‖<em class="mg">:</em>‏‏‎‖<em class="mg">d</em>‏‏‎‎∈‏‏‎‖<em class="mg">e</em>‏‏‎‖<em class="mg">↦</em>‏‏‎‖<em class="mg">p(d)</em>‏‏‎‎∈‏‏‎‖【0，1】<em class="mg">，即</em>概率分布函数，<em class="mg">p<em class="mg"> <em class="mg"> E. </em>如果一个事件不可能发生，那么<em class="mg">p(d)</em>‏‏‎;<em class="mg">=</em>‏‏‎0，而某个结果有概率<em class="mg">p(d)</em>‏‏‎;<em class="mg">=</em>‏‏‎1。 这个概率是可加的，使得所有可能事件的联合<em class="mg">d</em>‏‏‎‎∈‏‏‎<em class="mg">e</em>是确定的，<em class="mg">即 p(e)</em>‏‏‎<em class="mg">=</em>‏‏‎1。</em></em></p><p id="396d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">稍微滥用一下符号，我们可以写成<em class="mg">d</em>‏‏‎‖<em class="mg">~</em>‏‏‎‖<em class="mg">p</em>，这意味着某个事件<em class="mg"> d </em>是从所有可能事件<em class="mg"> E </em>的空间中抽取出来的，概率为<em class="mg"> P(d) </em>。这意味着观察到事件<em class="mg"> d </em>的概率为 100× <em class="mg"> P(d) </em> %。<em class="mg"> d </em>可以是一个过程的任何观察、事件或结果，例如，当掷出<em class="mg"> n = </em> 2 个六面骰子，两个都得到一个六时，<em class="mg"> d = (d </em> <em class="mg"> = 0，d </em> <em class="mg"> = 0，d </em> <em class="mg"> = 0，d </em> ⁴ <em class="mg"> = 0，d </em> ⁵ <em class="mg"> = 0，d </em> ⁶ <em class="mg"> = 2 我们事先并不知道通过掷骰子会得到什么结果，但是我们知道有一定的可能性会得到任何特定的结果。在多次重复掷骰子实验(骰子完全平衡且条件相同)后，我们应该看到<em class="mg"> d </em>发生的概率是<em class="mg">p(d)</em>‏‏‎‎≈‏‏‎/₃₆.即使不重复多次掷骰子，我们也可以提供我们<em class="mg">相信的</em>对我们看到特定结果的可能性分布的估计。</em></p><h1 id="9082" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">统计模型</h1><p id="2604" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">为了进行统计预测，我们使用参数化分布<em class="mg"> Pₐ </em>对数据分布进行建模。我们可以认为<em class="mg"> a </em>定义了一个统计模型，它包含了数据分布的描述<em class="mg">和</em>模型的任何可能的不可观察参数<em class="mg"> v </em> ∈ <em class="mg"> Eᵥ </em>。然后，分布函数将概率值归因于可观察/不可观察事件的发生<em class="mg"> Pₐ : (d，v) </em> ∈ <em class="mg"> (E，Eᵥ </em> ) ↦ <em class="mg"> Pₐ(d，v)</em>∈【0，1】。需要注意的是，我们可以把这个联合概率分布写成一个条件语句，<em class="mg">pₐ=lₐ</em><em class="mg">pₐ=ρₐ</em><em class="mg">eₐ</em>。这些概率分布函数是:</p><ul class=""><li id="6dc4" class="mh mi it kh b ki kj km kn kq mj ku mk ky ml lc mm mn mo mp bi translated"><em class="mg">可能性——lₐ:(d，v) </em> ∈ <em class="mg"> (E，eᵥ</em>)↦<em class="mg">lₐ(d|v)</em>∈【0，1】</li><li id="d6bb" class="mh mi it kh b ki mq km mr kq ms ku mt ky mu lc mm mn mo mp bi translated"><em class="mg">先验——pₐ:v</em>∈<em class="mg">eᵥ</em>↦<em class="mg">pₐ(v</em>∈【0，1】</li><li id="716f" class="mh mi it kh b ki mq km mr kq ms ku mt ky mu lc mm mn mo mp bi translated"><em class="mg">后方——ρₐ:(d，v) </em> ∈ <em class="mg"> (E，eᵥ</em>)↦<em class="mg">ρₐ(v|d)</em>∈【0，1】</li><li id="f729" class="mh mi it kh b ki mq km mr kq ms ku mt ky mu lc mm mn mo mp bi translated"><em class="mg">证据——eₐ:d</em>∈<em class="mg">e</em>↦<em class="mg">eₐ(d)</em>∈【0，1】</li></ul><p id="cb9b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这些函数的引入允许我们将观察到<em class="mg"> d </em>和<em class="mg"> v </em>的概率解释为，在给定模型参数值<em class="mg"> v、</em>的情况下，观察到<em class="mg"> d </em>的概率乘以这些模型参数值的可能性——同样，它也等于给定观察到<em class="mg"> d </em>的情况下，模型参数值<em class="mg"> v、</em>的概率乘以在中观察到<em class="mg"> d </em>的可能性</p><p id="4295" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于掷骰子实验，我们可以(也确实)使用多项式分布来模拟数据的分布，<em class="mg"> Pₐ = ∏ᵢ n！/d </em> ⁱ！<em class="mg"> pᵢ </em> ᵈⁱ其中多项式模型的固定参数为<em class="mg"> v </em> = <em class="mg"> {p₁、p₂、p₃、p₄、p₅、p₆、n } = { p \，n | I</em>∈【1，6】<em class="mg">}</em>其中<em class="mg">p \</em>为从骰子中获得数值<em class="mg">I</em>∈【1，6】的概率如果我们考虑完全无偏的骰子，那么<em class="mg">p₁=p₂=p₃=p₄=p₅=p₆=</em>/<em class="mg">₆.</em>观察到两个六的概率，<em class="mg"> d = (d </em> <em class="mg"> = 0，d </em> <em class="mg"> = 0，d </em> <em class="mg"> = 0，d </em> ⁴ <em class="mg"> = 0，d </em> ⁵ <em class="mg"> = 0，d </em> ⁶ <em class="mg"> = 2)，我们用<em class="mg"> n = </em> 2 掷骰子的多项式模型中的</em>因此可以估计为<em class="mg"> Pₐ(d)由于模型参数<em class="mg"> v </em>是固定的，这相当于为<em class="mg">I</em>∈【1，6】设置了<em class="mg">pₐ=δ(pᵢ</em>/<em class="mg">₆，n</em>2)的先验值，使得<em class="mg">lₐ=∏ᵢ</em>2(/<em class="mg">₆</em>)ᵈⁱ<em class="mg">/d</em>ⁱ！或者 0。</em></p><p id="58e6" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">当然，我们可以建立一个更复杂的模型，其中<em class="mg"> pᵢ </em>的值取决于其他因素，例如骰子可以弹开的表面数量，或者投掷的力度，或者在骰子离开我们手的那一刻击中骰子的每个空气分子的速度，或者无数种不同的效果。在这种情况下，分布<em class="mg"> Pₐ </em>将根据描述这种物理效应的不可观察参数<em class="mg"> v </em>、<em class="mg">即</em>之间的相互作用，为数据<em class="mg"> d ~ P </em>的出现分配概率，在多项式模型中，模型参数的值<em class="mg"> v、</em>将<em class="mg"> </em>改变描述<em class="mg"> d </em>有多可能<em class="mg">的<em class="mg"> pᵢ </em>的值然而，我们可能不知道这些不可观测参数的确切值。因此<em class="mg"> Pₐ </em>不仅描述了对数据真实分布的估计，还描述了其对不可观测模型参数的依赖。我们把描述来自不可观测参数的观测数据的概率的条件分布函数称为<em class="mg">似然、</em> <em class="mg"> Lₐ </em>。由于模型<em class="mg"> a </em>描述了整个统计模型，模型参数<em class="mg"> v </em>的先验概率分布<em class="mg"> pₐ </em>是模型的固有属性。</em></p><p id="2298" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们不知道模型<em class="mg"> a </em>中参数的值<em class="mg"> v </em>(甚至缺乏关于模型本身选择的知识)这一事实引入了不确定性的来源，我们称之为<em class="mg">认知</em>——这种不确定性是由于我们<em class="mg">可以通过观察事件的支持</em>了解到的事情。因此，尽管由于来自数据分布<em class="mg"> P </em>的事件<em class="mg"> d </em>发生的真实随机性质而存在随机不确定性，但也存在认知不确定性，这种认知不确定性来自使用<em class="mg"> Pₐ </em>对该分布进行建模。然而，先验分布<em class="mg"> pₐ </em>不应该与认知不确定性混淆，因为先验是对特定模型<em class="mg">和</em>的选择。可能使用不明智的先验分布选择(由于统计模型的定义),这不允许模型得到数据的支持。</p><p id="ef32" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">例如，对于掷骰子问题，当我们建立一个模型时，我们可以决定我们的模型是确定的，并且模型参数的可能值的先验分布是<em class="mg">pₐ=δ(pᵢ</em>/<em class="mg">₆，n</em>-2)对于<em class="mg">I</em>∈【1，6】。在这种情况下，认知的不确定性将不被考虑在内，因为假设没有什么我们可以了解的。然而，如果骰子被加权，使得<em class="mg"> p₁ = 1，p₂ = p₃ = p₄ = p₅ = p₆ = 0 </em>，那么我们将永远得不到两个 6，我们的模型也不会得到数据的支持。相反，如果我们选择一个不同的模型<em class="mg">a</em>’，它是一个多项分布，但是其中关于<em class="mg"> pᵢ </em>的可能值的先验分布是这样的，在∑ <em class="mg"> ᵢ pᵢ = </em> 1 的条件下，它们可以取从 0 到 1 的任何值，那么没有假定的知识(在这个特定的模型内)。因此，由于我们缺乏知识，存在着很大的认知不确定性，但这种不确定性可以通过在观察可用数据时推断可能的参数值来减少。</p><h2 id="80cc" class="mv le it bd lf mw mx dn lj my mz dp ln kq na nb lr ku nc nd lv ky ne nf lz ng bi translated">主观推断</h2><p id="d572" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">我们可以使用主观推断(通常称为贝叶斯推断)了解观察到的数据支持哪些模型参数值，从而减少我们选择模型时的认知不确定性。使用联合分布的条件展开的两个等式，<em class="mg"> Pₐ，</em>我们可以计算后验概率，在模型<em class="mg"> a </em>中，当某个<em class="mg"> d ~ P </em>被观察到时，参数具有值<em class="mg"> v </em></p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/00f88b836d6da165db68785b9a11ea18.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*jNgQYh9N4ShDJ0HbBkygYw@2x.png"/></div></figure><p id="10f6" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这个后验分布现在可以用作新模型的基础，<em class="mg">a′</em>，具有联合概率分布<em class="mg">pₐ</em>′，其中<em class="mg">pₐ</em>′=<em class="mg">ρₐ，即 pₐ</em>′=<em class="mg">lₐ</em><em class="mg">pₐ</em>′。请注意，模型的形式没有改变，只是由于数据的支持，我们对模型参数的确定性有所改变——我们可以使用这个新模型对数据的分布做出更明智的预测。</p><p id="ee40" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在众多方法中，我们可以使用 MCMC 技术来描述后验分布的特征，从而减少这一假设模型中的认知不确定性。然而，无论对错，人们通常只对数据的最佳拟合分布感兴趣，<em class="mg">即</em>找到<em class="mg"> v </em>的集合，其中<em class="mg"> Pₐ </em>与<em class="mg">p</em>最相似</p><h2 id="f4cf" class="mv le it bd lf mw mx dn lj my mz dp ln kq na nb lr ku nc nd lv ky ne nf lz ng bi translated">最大似然和最大后验估计</h2><p id="0878" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">为了使模型符合数据的真实分布，我们需要测量两个分布之间的距离。最常用的度量是相对熵(也称为 Kullback-Leibler (KL)散度)</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/65f03c34fae92934cbe01ab949e7ee4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*UOTJLSvKKbmaFXbVmwT9HA@2x.png"/></div></figure><p id="be3d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">相对熵描述了由于用<em class="mg"> Pₐ(d，v) </em>逼近<em class="mg"> P(d) </em>而丢失的信息。相对熵有一些有趣的性质，这使得它不能作为理想的距离度量。首先，它不是对称的，<em class="mg">d(p</em>∩<em class="mg">pₐ)</em>≦<em class="mg">d(pₐ</em>∩<em class="mg">p)</em>，因此它不能被用作度量。我们可以采用<em class="mg">d(pₐ</em>∩<em class="mg">p)</em>和<em class="mg">d(p</em>∩<em class="mg">pₐ)</em>的对称组合，但问题仍然存在，例如<em class="mg"> P </em>和<em class="mg"> Pₐ </em>必须定义在同一个域上，<em class="mg"> E </em>。其他测量，例如推土机距离，在这里可能具有优势，因为它是对称的，并且可以在不同的坐标系上定义(并且当用作任意函数而不是用于预测模型参数时，现在可以使用神经网络很好地近似)。然而，我们仍然经常考虑相对熵。重写相对熵我们看到，我们可以用两个术语来表示相似性的度量</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/3e75fce845527a5d760a30a59f8e3830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HeA5AR8oHBvzCgUQnldpdA@2x.png"/></div></div></figure><p id="94c3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">第一项是数据分布的负熵，<em class="mg">即</em>通过观察一个结果可以获得的预期信息量，直接类比于统计热力学中的熵。第二项是交叉熵，<em class="mg"> H(P，pₐ)</em>，它量化了将一个分布<em class="mg"> P </em>与另一个分布<em class="mg"> P </em> ₐ <em class="mg">区分开所需的信息量，即</em>需要多少次<em class="mg"> d ~ P </em>的抽取才能判断出<em class="mg"> d </em>是从<em class="mg"> P </em>中抽取的，而不是从<em class="mg"> P </em>中抽取的注意，在这种相对熵的形式中，只有一组自由参数<em class="mg"> v </em>，然后我们可以尝试通过最小化相对于这些参数的相对熵，使<em class="mg"> Pₐ </em>尽可能接近<em class="mg"> P </em></p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/373246983aa743446d5ecb156ee8146e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*KGTMxgtQv7SVNmJaKQwQpQ@2x.png"/></div></figure><p id="0595" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">但是我们实际上是如何做到的呢？我们可能无法访问所有可能的数据分布来进行积分。相反，考虑一个采样分布，<em class="mg">s:d</em>∈<em class="mg">s</em>↦<em class="mg">s(d)</em>∈【0，1】，其中<em class="mg"> s(d) </em>是来自采样空间的事件的归一化频率，其中<em class="mg"> N </em>的条件独立值为<em class="mg"> d </em>，<em class="mg"> S </em> ⊆ <em class="mg"> E </em>。在这种情况下，积分变成一个和<em class="mg"> H(P，pₐ)</em>≊∑s<em class="mg">(d)</em>log<em class="mg">pₐ(d，v)。</em>使用条件关系，我们然后如前写下<em class="mg"> Pₐ = Lₐ </em> <em class="mg"> pₐ </em>，这样交叉熵是<em class="mg"> H(P，pₐ)</em>≊∑s<em class="mg">(d)</em>log<em class="mg">lₐ(d| v)</em>∑s<em class="mg">(d)</em>log<em class="mg">pₐ(v).</em>由于先验独立于数据，它只是向交叉熵添加了一个附加常数。</p><p id="9af9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">换句话说，我们可以把可能性写成概率的乘积，给出抽样分布中数据出现的频率</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/21252ae378f7807e6f364823c45bab5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*p7MHwt8ct_-E21l5WtLfZA@2x.png"/></div></figure><p id="64fd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">所以，除了由于先验的加性常数，交叉熵与模型中数据的似然负对数成正比。这意味着最大化数据相对于模型参数的可能性的对数，假设所有<em class="mg"> v </em>的均匀先验(或忽略先验)等同于最小化交叉熵，这可以解释为最小化相对熵，从而使<em class="mg"> Pₐ </em>尽可能接近<em class="mg"> P </em>。忽略交叉熵中的第二项提供了参数值的非主观最大似然估计(非主观意味着我们忽略了参数值的任何先验知识)。然而，如果我们考虑先验，我们恢复最简单形式的主观推断，<em class="mg">最大后验概率(MAP) </em>估计</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/bc445f941e53c4b712543fbc309ac883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*OPBW5nRIqVA_NCyuJL91uw@2x.png"/></div></figure><p id="2c7c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">其描述了一组参数值<em class="mg"> v </em>，其提供尽可能接近<em class="mg"> P </em>的<em class="mg"> Pₐ </em>。这里必须强调一点，因为我们使用抽样分布，<em class="mg"> s，</em>最大化可能性(或后验概率)实际上为我们提供了最接近抽样分布的分布<em class="mg"> Pₐ </em>，<em class="mg"> s </em>。如果<em class="mg"> s </em>不代表<em class="mg"> P，</em>则该模型不一定是<em class="mg"> P </em>的合适模型。第二点需要注意的是，尽管<em class="mg"> Pₐ </em>可能尽可能接近<em class="mg"> P </em>(或实际上<em class="mg"> s </em> ) <em class="mg"> </em>与这组<em class="mg"> v </em>，但可能性(或后验概率)的模式实际上可能远离分布的高密度区域，因此根本不能代表更可能的模型参数值。当使用 MCMC 技术或类似技术考虑整个后验分布时，这是可以避免的。本质上，使用最大似然或最大后验估计，认知误差将被大大低估，而没有考虑大部分先验(或后验)概率密度。</p><h2 id="11f4" class="mv le it bd lf mw mx dn lj my mz dp ln kq na nb lr ku nc nd lv ky ne nf lz ng bi translated">模型比较</h2><p id="e64f" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">请注意，到目前为止还没有声明说一个型号<em class="mg">一个</em>实际上有任何好处。我们可以通过计算证据来测量模型与数据<em class="mg"> d </em>的拟合程度，这相当于对模型参数<em class="mg">即</em>的所有可能值进行积分</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/018a27edf6409103295e72fcb0188670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*8n1cRn7uVUxooDtDWY1D4g@2x.png"/></div></figure><p id="147c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">通过选择一个不同的模型<em class="mg">a</em>’和它自己的一组参数<em class="mg"> u </em> ∈ <em class="mg"> Eᵤ </em>，我们可以得出一个标准来描述是模型<em class="mg"> a </em>还是<em class="mg">a’</em>更符合数据<em class="mg"> d </em>。请注意，该标准不一定定义明确<strong class="kh iu">。</strong>我们是更喜欢与数据完全吻合但具有半无限数量参数的模型，还是更喜欢参数很少但拟合度不太好的优雅模型？在神经网络出现之前，我们通常选择参数最少的模型，这些模型能够很好地拟合数据，并进行归纳，以对未来事件做出一致的预测——但这仍有待讨论。</p><p id="f451" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">到目前为止所描述的一切都是科学方法。我们观察一些数据，并希望对未来观察的可能性进行建模。因此，我们建立一个参数化的模型来描述观察结果和它发生的可能性，了解该模型参数的可能值，然后根据某些标准(如奥卡姆剃刀或其他标准)来改进该模型。</p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h1 id="082b" class="ld le it bd lf lg nz li lj lk oa lm ln lo ob lq lr ls oc lu lv lw od ly lz ma bi translated">神经网络作为统计模型</h1><h2 id="ee2d" class="mv le it bd lf mw mx dn lj my mz dp ln kq na nb lr ku nc nd lv ky ne nf lz ng bi translated">深度学习是一种建立数据分布模型的方法</h2><p id="47ce" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">不管目标是什么——监督学习、分类、回归、生成等等——深度学习只是为数据的分布建立模型。对于监督学习和其他预测方法，我们认为我们的数据，<em class="mg"> d，</em>是一对输入和目标，<em class="mg"> d = (x，y) </em>。例如，我们的输入可以是猫和狗的图片，<em class="mg"> x </em>，附带标签，<em class="mg"> y </em>。然后，我们可能想要对先前未看到的图像<em class="mg">x</em>’进行标签<em class="mg">y</em>’<em class="mg">’</em>的预测——这相当于对一对相应的输入和目标<em class="mg"> d、</em>进行预测，假定部分<em class="mg"> d </em>是已知的。</p><p id="ca93" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">因此，我们希望使用神经网络对分布<em class="mg"> P </em>进行建模，<em class="mg"> f: (x，v) </em> ∈ ( <em class="mg"> E，Eᵥ) </em> ↦ <em class="mg"> g= f(x，v) </em> ∈ <em class="mg"> G </em>，其中<em class="mg"> f </em>是一个由权重、<em class="mg"> v、</em>参数化的函数，它接受输入<em class="mg"> x </em>函数的形式<em class="mg"> f </em>由超参数化<em class="mg">a</em>描述，包括架构、初始化、优化例程，以及最重要的损失或成本函数<em class="mg">λₐ:(d，v) </em> ∈ <em class="mg"> (E，eᵥ)</em>↦<em class="mg">λₐ(y|x，v) </em> ∈ K [0，1】。损失函数描述了数据出现概率的非标准化度量，<em class="mg"> d = (x，y)，</em>带有不可观察参数，<em class="mg"> v </em>。也就是说，当给定<em class="mg"> x </em>时，使用神经网络对<em class="mg"> y </em>进行预测等同于对数据<em class="mg"> d </em>出现的概率<em class="mg"> P </em>进行建模，其中分布的形状由网络<em class="mg"> a </em>的形式和属性以及 it 参数<em class="mg"> v </em>的值来定义。我们通常将经典神经网络(预测目标)与神经密度估计器(估计输入的概率，<em class="mg">即</em>空间<em class="mg"> G = </em> [0，1】)区分开来——然而，它们执行相同的工作，但是来自经典神经网络的分布只能使用损失函数来评估(并且通常不像真实概率那样归一化为 1)。这阐明了经典神经网络的输出或预测的意义-它们是控制我们模型中数据概率分布形状的参数值(由超参数的选择定义)。</p><p id="1319" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">例如，当使用均方误差作为损失函数来执行回归时，神经网络的输出<em class="mg"> g = f(x，v) </em>相当于具有单位方差的广义正态分布的平均值。这意味着，当馈入一些输入<em class="mg"> x </em>时，网络通过参数的值<em class="mg"> v、</em>提供对<em class="mg"> y </em>的可能值的平均值的估计，其中<em class="mg"> y </em>的可能值是从具有单位方差<em class="mg">即 y ~ N(g，</em><strong class="kh iu"><em class="mg">I</em></strong><em class="mg">)</em>的广义正态中提取的。请注意，无论如何，这种针对<em class="mg"> y </em>值的模型可能都不是一个好的选择。另一个例子是当使用 softmax 输出执行分类时，我们可以将输出直接解释为多项式分布的<em class="mg"> pᵢ </em>，其中模型的不可观察参数<em class="mg"> v </em>以类似于物理模型中的参数影响不同数据出现概率的方式影响这些输出的值。</p><p id="69f4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">有了这些知识，我们就可以将网络参数的优化(称为训练)理解为模拟数据的分布，<em class="mg"> P </em>。通常，在经典训练一个网络时，我们选择的模型允许<em class="mg"> v ~ pₐ = </em>均匀[-∞，∞](虽然我们倾向于从某种正态分布中得出它们的初始值)的任意值。本质上，我们忽略了关于权重值的任何先验信息，因为我们没有任何先验知识。在这种情况下，所有关于数据分布的信息都来自于可能性，<em class="mg"> Lₐ </em>。因此，为了训练，我们执行网络参数的最大似然估计，这最小化了数据分布和来自神经网络的估计分布之间的交叉熵，因此最小化了相对熵。为了实际评估具有参数值<em class="mg"> v </em>的经典神经网络的似然对数，我们可以将一些观察数据<em class="mg"> d </em>的似然展开为<em class="mg">lₐ(d| v)</em>≊<em class="mg">λₐ(y| x，v)s(x)</em>其中<em class="mg"> s(x) </em>是<em class="mg"> x </em>的采样分布，相当于指定<em class="mg"> x </em>在<em class="mg">中出现的次数的归一化频率当给定相应的<em class="mg"> x，</em>时，在抽样分布中的每个<em class="mg"> y </em>处评估<em class="mg">λₐ(y| x，v) </em>，取该概率的对数并对结果求和，从而给出抽样分布的可能性的对数。相对于网络参数最大化这一点，<em class="mg"> v </em>，因此给出了分布，<em class="mg"> Pₐ </em>，其最接近<em class="mg"> s </em>(其应该有希望接近<em class="mg"> P </em>)。</em></p><p id="7755" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">仅此而已。一旦经过训练，神经网络提供了统计模型的参数，可以对这些参数进行评估以找到最可能的预测值。如果损失函数给出一个非标准化的似然性，像 MCMC 这样的方法可以用来获得表征数据分布的样本。</p><p id="1105" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">必须考虑一些注意事项。首先，损失函数的选择定义了统计模型—如果损失函数无法描述数据的分布，那么数据分布的统计模型将是错误的。避免分布假设的一种方法是考虑均方差、分类交叉熵或绝对误差之外的损失函数——一个理想的选择是推土机距离，它可以通过特定类型的神经网络很好地近似，并提供一个目标，模拟数据分布到统计模型之间的最佳运输计划，从而为<em class="mg"> Pₐ </em>提供一个未假设的形式。另一件需要注意的事情是，使用神经网络<em class="mg">的统计模型</em>被过度参数化了。宇宙演化的模型只需要 6 个参数(在天气好的时候)——而神经网络会使用数百万个无法识别的参数来完成更简单的任务。在追求模型优雅的情况下进行模型选择时，神经网络几乎总是会失败。最后，基于数据，使网络适合数据——如果数据中存在任何偏差，<em class="mg"> d </em> ∈ <em class="mg"> S </em>，即<em class="mg"> s </em>与<em class="mg"> P、</em>不相似，则偏差会很明显。物理模型可以通过建立直觉来避免这些偏见。事实上，神经网络也可以做到这一点，但代价是要比盲目挑选东西花费更多的脑力和更多的时间来编写代码。</p><h2 id="51b6" class="mv le it bd lf mw mx dn lj my mz dp ln kq na nb lr ku nc nd lv ky ne nf lz ng bi translated">所以深度学习使用神经网络和损失函数，相当于建立一个描述数据分布的参数化统计模型。</h2></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="5920" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">汤姆·查诺克是统计学和机器学习方面的专家。他目前在巴黎工作，致力于解决机器学习和人工智能模型的统计可解释性方面的突出问题。作为一名国际自由职业顾问，他为与复杂数据分析、数据建模和计算机科学的下一代方法相关的问题提供实用的解决方案。他的职责包括一对一的支持、全球协作以及通过讲座、研讨会、教程和文章进行推广。</p></div></div>    
</body>
</html>