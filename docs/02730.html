<html>
<head>
<title>The real reason why BatchNorm works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BatchNorm 有效的真正原因</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-batchnorm-works-518bb004bc58?source=collection_archive---------30-----------------------#2020-03-16">https://towardsdatascience.com/why-batchnorm-works-518bb004bc58?source=collection_archive---------30-----------------------#2020-03-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ad19" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解“损失情况”如何随着批量标准化而变化</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/77e769c3e446682a3ff7c0a783e32426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-hfkwu7EUHAs88aTBeNug.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://pixabay.com/photos/schrecksee-bergsee-allg%C3%A4u-2534484/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="3e4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">归一化技术是我们在分析任何形式的数据时拥有的一些伟大的工具，调整分布的均值和方差的简单操作导致深度神经网络中各种归一化技术的灾难性成功，其中之一是著名的批量归一化 Ioffe 等人。</p><p id="3e71" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个人都听说过:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/0bc08eb700b951734cafbe360e71966b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XKr2e_wMF_HLX6mz4doMLg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Ioffe 等人的算法-1。</p></figure><p id="abf7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">目前，对 BatchNorm 成功的最广泛接受的解释，以及其最初的动机，与所谓的内部协变量转移(ICS)有关。非正式地，ICS 指的是由对先前层的更新引起的层输入分布的变化。据推测，这种持续的变化会对训练产生负面影响。BatchNorm 的目标是减少 ICS，从而补救这种影响。</p><p id="c308" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在易勒雅斯等人的文章中，作者提出了一个观点，即批处理范式的性能增益和内部协变量移位的减少之间似乎没有任何联系。或者说这种联系充其量也是脆弱的。事实上，他们发现在某种意义上，BatchNorm 甚至可能不会减少内部协变量偏移，并证明在自然条件下，损失和梯度的 Lipschitzness(基本上是损失表面及其梯度的平滑度)在具有 BatchNorm 的模型中得到改善，因此证明 BatchNorm 以一种基本的方式影响网络训练:它使相应的优化问题的前景明显更加平滑。</p><p id="2c2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，我们将分析误差空间权重为二次的模型的误差表面的二阶属性，然后重新对表面进行参数化，以便根据输入协方差矩阵的特征谱进行分析，并由此使用结果来推断为什么批处理范数可能会提高收敛速度，以及为什么它会使网络在初始化时保持不变。</p><p id="c2fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于在权重空间<em class="lt"> {W} </em>中搜索误差函数<em class="lt"> E(W) </em>最小的最优值<em class="lt"> W* </em>的各种学习算法，基于梯度下降，它们的性质由<em class="lt"> E(W) </em>曲面的二阶性质控制。</p><p id="bf13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这种分析，我将集中于神经网络的单层，其中<em class="lt"> X(N </em> x <em class="lt"> 1) </em>作为这一层的输入向量，并且<em class="lt"> W(1 </em> x <em class="lt"> N) </em>是这一层的权重，并且<em class="lt"> Y(1 </em> x <em class="lt"> 1) </em>是这一层的输出，为了方便考虑均方误差:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/3911e72428c07601e9e70d17dce11f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/1*s-p9RJGgfH4q_nHSC29r3g.gif"/></div></figure><p id="4a21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lt"> p 是数据集的大小</em></p><p id="89d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很明显，误差曲面在权重上是二次的，因此将其重写为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/1e5cac4e199297d9954244fb805d5b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/1*zecR4rofg8Qh7HGusxokdg.gif"/></div></figure><p id="86b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中 R 定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lw"><img src="../Images/f3ab8d85299838210a894e3735640bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/1*SxRLwbqiFUx43t0EMVJM0w.gif"/></div></div></figure><p id="2da3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lt">其中 Xui 是第 u 个输入向量(Nx1)的第 I 个分量，因此 R 显然是第 N 个</em> <strong class="ky ir"> x </strong> <em class="lt"> N 个输入协方差矩阵。</em></p><p id="7fa9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lt"> Q 是一个 N 维向量，</em>定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/4079149957da3c582698560edfbfd247.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/1*FewnZeWtNPga9yBt26o2kw.gif"/></div></figure><p id="0e2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">c 只是一个常数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/d0df13fabdcf3f7bd8d2f3142e09e902.png" data-original-src="https://miro.medium.com/v2/resize:fit:194/1*G2RDH3-QFJy6P9e0hYu5yg.gif"/></div></figure><p id="a188" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在<em class="lt"> E(W) </em> w.r.t. <em class="lt"> W </em>的梯度是<em class="lt"> J(E) = RW-Q </em>而二阶导数的海森矩阵显然是<strong class="ky ir"> <em class="lt"> H = R </em> </strong></p><p id="c351" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最小化<em class="lt"> E(W) </em>的解空间<strong class="ky ir"> <em class="lt"> W* </em> </strong>显然包含梯度为零的条目，因此该解空间是线性方程解的<strong class="ky ir">子空间</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/a456f18d05b981ab3fca255f10ee5034.png" data-original-src="https://miro.medium.com/v2/resize:fit:142/1*lUP12TZl-f3HX3d_CLRSsg.gif"/></div></figure><p id="f99f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(来自<em class="lt"> J(E) </em> =0)</p><p id="1866" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，这种<em class="lt"> RW=Q </em>是线性代数(<em class="lt"> Ax=b </em>)中常见的一类问题，工程应用中的很多问题都可以用这种格式公式化，然后分析<em class="lt"> A </em>的本征谱来解决。</p><p id="f8ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于解空间<em class="lt"> W* </em>是<em class="lt"> RW=Q </em>的解的子空间，现在如果<em class="lt"> R </em>的所有列都是独立的，因此<em class="lt"> R </em>具有满秩，很明显，这个子空间塌陷到一个点(唯一解)。</p><p id="c95e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在这一切都很好，但是<strong class="ky ir">我们为什么要这样做？</strong></p><p id="ef5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">当损失曲面的权重为二次时</strong><em class="lt">r</em>的特征向量定义了<em class="lt"> E(W) </em>的主轴(这可以从“噪声和振动的主动控制(Colin Hansen，Scott Snyder)】section-6.5.2)⁵<br/>中研究，我们可以通过下式计算曲面在任一单位向量<em class="lt"> u </em>方向的二阶导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/0733fba176173492afe3adcd82d7379a.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/1*M8_GhmzIIrbtqKELecqVJA.gif"/></div></figure><p id="4959" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">选择<strong class="ky ir"><em class="lt"/></strong>为<strong class="ky ir"> H </strong> (=R)的特征向量，我们可以得到<em class="lt"> E(W) </em>在主轴方向的二阶导数，从矩阵对角化可知，<strong class="ky ir">将是<em class="lt"> H </em> (= <em class="lt"> R </em> ) </strong>的特征值。</p><p id="19aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经用<em class="lt"> R </em>和我们的 Hessian = <em class="lt"> R </em>表示了我们的梯度，并且由于我们可以用<em class="lt"> R </em>定义<em class="lt"> E(W) </em>的主轴，我们将在包含我们的误差表面的空间上进行 2 个变换:</p><ul class=""><li id="cd44" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><strong class="ky ir">通过基本平移，以解决点</strong>为中心</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/706d114596a974cd9725c6ee288c2be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/1*J9ZCu_vm92SWT2R04MWhsQ.gif"/></div></figure><ul class=""><li id="cebd" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><strong class="ky ir">旋转</strong>，我们要把这个<em class="lt"> V` </em>旋转到<em class="lt"> E(W) </em>的主轴上，这样就可以分析主轴方向上的黑森。现在，如果我们将一个矩阵乘以一个向量，就会产生一个空间变换(从旋转、平移到将空间压缩成一个点)。为了实现旋转，<strong class="ky ir">我们将把<em class="lt"> V` </em>乘以特征向量矩阵<em class="lt"> R </em> </strong>，包含相互正交的归一化特征向量<em class="lt"> R </em>，将这个矩阵命名为<strong class="ky ir"><em class="lt"/></strong>(通用约定)，显然这个矩阵将是正交的，并且列的维数将与<em class="lt"> W </em>相同，所以:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/989e2c1254421c18d6590318b52365e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:142/1*fAREqTp4R5xojzqj2JuJAQ.gif"/></div></figure><p id="cfc4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完整的转变将是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/1a1cf48011f5fd5eb07a80d66efb0ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/1*-VazpOmvLtO7_zHgS0a3OQ.gif"/></div></figure><p id="0e16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于此，我们的方程<em class="lt"> E(W) </em>被重新参数化为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/809aec5882bfa841a81fcb2093613104.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/1*wuFgHz3HKKKvG4L0anVxhQ.gif"/></div></figure><p id="7c74" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<strong class="ky ir"> <em class="lt"> D </em> </strong>正好是对角线上含有<strong class="ky ir"> <em class="lt"> R </em> </strong>特征值的对角矩阵(也叫矩阵对角化)<strong class="ky ir"> <em class="lt"> E_o </em> </strong>正好是<em class="lt"> E(W*)。</em></p><p id="1eb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">记住<em class="lt"> V </em>还是一个矢量，但是<strong class="ky ir">V<em class="lt">V</em>的每一个分量都代表了误差面对应的主轴。</strong></p><p id="3fb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/ba8bac082f0046502a5499457f24ac63.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/1*_kFap35kSJz1Nn4Yae1ufg.gif"/></div></figure><p id="d049" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">λ_j 是<em class="lt"> D 的第 j 个对角元素，</em>也是 R(=H)的第 j 个特征向量的特征值</p><p id="ab82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">和</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/c39c025386bbb3695dd833ae64aed7c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/1*FxlRm6Ck1Hfv_okOSTAL4w.gif"/></div></figure><p id="990b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">𝛿(j,k)=1 代表 j=k，其他地方代表 0</p><p id="1080" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">显然，输入协方差矩阵的特征值给出了误差表面的二阶导数。</p><p id="6faa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将这两个结果放在一个矩阵中给出了作为对角特征矩阵的 Hessian<em class="lt">D</em>，(<strong class="ky ir"><em class="lt">H = D</em></strong><em class="lt">)</em>，因此</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/f8dc39f0fc4716c6bc409041ed094abc.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/1*suaoPIxmy4AwMkXRa58NYw.gif"/></div></figure><p id="d2e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于梯度下降的基本权重更新规则是:<br/><em class="lt">= V(k+1)——</em>η*<em class="lt">J(E(V))</em><br/><em class="lt">V(k+1)= V(k)——</em>η<em class="lt">* D * V(k)</em></p><p id="971f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lt"> V(k) </em>是第 k 个时间步长<em class="lt"> V </em>的值。</p><p id="8883" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lt"> V </em>是<em class="lt"> N </em> x <em class="lt"> 1 </em>向量，这导致<em class="lt"> N </em>解耦方程(由于<em class="lt"> V </em>中的每个分量都是正交的)，并且由于<em class="lt"> V=U(W-W*) </em>，对于最优解<strong class="ky ir"> <em class="lt"> V </em>衰减到零</strong>，因此每个分量沿着主方向 as 演化</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/5919d3813e74cb800e605900908d46a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/1*Ftmoq-w3xqyAzpS0GfOc_w.gif"/></div></figure><p id="b781" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lt"> Vj(0) </em>是初始化时<em class="lt"> V </em>的第 j 个分量</p><p id="23f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，为了使 V <em class="lt"> j </em>在多个时间步长(k &gt; 0)上收敛到零，具有正幂的项的幅度必须小于 1，因此</p><p id="4310" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">| 1-<em class="lt">η</em>*<em class="lt">λ_ j</em>|&lt;1<br/>0&lt;<em class="lt">η</em>&lt;2/<em class="lt">λ_ j</em></p><p id="e749" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于这只是指数衰减的公式，V <strong class="ky ir"> <em class="lt"> j </em>在特征时间</strong>内衰减到零</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/fa366459cb51d0b4fb38e1018e6d327f.png" data-original-src="https://miro.medium.com/v2/resize:fit:132/1*5holuPLtYDsNdlL81N5Ctg.gif"/></div></figure><ul class=""><li id="3175" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">对于<em class="lt"> η </em>在(1/ <em class="lt"> λ_j </em>，2/<em class="lt">λ_ j</em>)<br/>1-<em class="lt">η</em>*<em class="lt">λ_ j</em>范围内为负，因此收敛为振荡行为，步长较大</li><li id="915d" class="mb mc iq ky b kz ms lc mt lf mu lj mv ln mw lr mg mh mi mj bi translated">对于范围(0，1/ <em class="lt"> λ_j </em>)内的<em class="lt"> η </em>，<br/>步长较小，收敛需要较高的 k 值(时间步长)</li><li id="4bc2" class="mb mc iq ky b kz ms lc mt lf mu lj mv ln mw lr mg mh mi mj bi translated">那么，如果<em class="lt"> η = </em> 1/ <em class="lt"> λ_j </em>，如果这样的选择是可能的，那么在单次迭代中就达到了收敛。</li></ul><p id="5d16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果所有特征值相等，<strong class="ky ir">λ</strong><em class="lt">_ j =</em><strong class="ky ir">λ</strong>，对于所有<em class="lt"> 1 &lt; =j &lt; =N </em>，由于<em class="lt"> H=D </em>，<strong class="ky ir">收敛在单个步骤</strong>中达到，其中:</p><p id="6322" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lt">η</em>= 1/<strong class="ky ir">T39】λT41】</strong></p><p id="57d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">但是这种高度对称的情况很少发生</strong>。从几何学上讲，如果误差曲面<em class="lt"> E(W) </em>的横截面是 N 维空间<em class="lt"> {W} </em>中的超球(由环绕主分量形成的超球，这些特征值相等，它们被同等地拉伸或压扁，因此是超球)，就会发生这种情况，但这是非常罕见的。<em class="lt"> E(W) </em>截面为椭圆形，沿不同主方向有不同的特征值。</p><p id="bfb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据上述等式，η<em class="lt">必须在(0，1/ <em class="lt"> λ_mx </em>和<em class="lt"> λ_mx </em>之间选择，λ_mx </em>是输入协方差矩阵的最大特征值，最慢的时间常数为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/876f04671c26ab3e11f37befb554052b.png" data-original-src="https://miro.medium.com/v2/resize:fit:184/1*U7qwlQ33zPHiVPQHAu5abw.gif"/></div></figure><p id="2cb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最小步长<em class="lt">η</em>= 1/<strong class="ky ir"><em class="lt">λ</em></strong><em class="lt">_ max</em>给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/3fbc5350d5bc48c89ed5fc33a9c09955.png" data-original-src="https://miro.medium.com/v2/resize:fit:166/1*2YYIOE_pojn2do8SGp_Iuw.gif"/></div></figure><p id="9fdd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">沿非零曲率最小的主方向衰减</strong>(基本上是非零特征值最小的特征向量，即<em class="lt"> λ_mn </em>)。</p><p id="d028" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这清楚地表明<strong class="ky ir">对于权重为二次的误差曲面，学习动态由 Hessian </strong>的特征值分布控制。</p><p id="db7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然我们知道这个，为什么停在这里，让我们在这里检查特征谱。</p><p id="5f0c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设输入分量，即<strong class="ky ir"><em class="lt">【Xi】</em></strong>对于所有<em class="lt"> 1 &lt; =i &lt; =N </em>是独立的，并且每个分量是从均值<strong class="ky ir"><em class="lt">【m</em></strong>和方差<strong class="ky ir"> <em class="lt"> v </em> </strong>中提取的(为了方便起见，考虑一个输入的所有分量在数值上具有相同的均值和方差)。我们将<strong class="ky ir"> <em class="lt"> p </em> </strong>作为数据大小，将<strong class="ky ir"> N </strong>作为向量维数，假设<strong class="ky ir">α</strong><strong class="ky ir"><em class="lt">= p/N</em></strong>(量化训练集的比率)，得到本征谱如下(本征谱的完整推导可以在 I. Kanter、Yann Lecun 和 A. Solla 1991 的联合著作中找到)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/113aa45f4d99887bd450ee0852722fe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LXHF3LxRFH38wZmNUjEbwg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自 LeCun 等人的等式-13。</p></figure><p id="252c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在哪里</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/7f959171ae2b651e27e0220b2b8b33f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/1*x4NMCKsdoAM3IIFimZzJhg.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/3a3d0d33185a0036d59fafebdabb4d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/1*_21-z8iyz5t2R-bx9JVK-Q.gif"/></div></figure><p id="5efd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<strong class="ky ir"> λ </strong> ∈ ( <strong class="ky ir"> λ </strong> -，<strong class="ky ir"> λ </strong> +)，谱是连续的，那么在<strong class="ky ir"> p </strong>和<strong class="ky ir"> α </strong>趋于无穷大的极限内，<strong class="ky ir"> λ- =λ+ = <em class="lt"> v </em> </strong></p><ul class=""><li id="de81" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">现在应该清楚的是，如果<em class="lt"> p &lt; N </em>，则<em class="lt"> R </em>的独立列的数量将等于<em class="lt">p .</em>T46】R 不会是满秩的，因此<em class="lt"> N、</em>T52】中总共有(<em class="lt"> N-p </em>个特征值将为零，从而导致对于所有零特征值(基本上是谱)的权重为 1-α的δ函数贡献</li><li id="b692" class="mb mc iq ky b kz ms lc mt lf mu lj mv ln mw lr mg mh mi mj bi translated"><strong class="ky ir">当输入有偏置，即<strong class="ky ir"> <em class="lt"> m≠0 </em> </strong>时，会出现一个大的<em class="lt"> N </em> </strong>阶孤立特征值，称为<strong class="ky ir"> λ </strong> _ <em class="lt"> N </em>。这一点可以清楚地理解，考虑<br/>的结构<strong class="ky ir"> <em class="lt"> R </em> </strong>为<strong class="ky ir"/><em class="lt">p→</em><strong class="ky ir">∞</strong>极限，<br/> 1。所有非对角元素都等于<strong class="ky ir"> <em class="lt"> m </em> </strong>(两者均值的乘积)<br/> 2。所有对角线元素都等于<strong class="ky ir"> <em class="lt"> v+m </em> </strong></li></ul><p id="9de0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，特征向量<em class="lt">U _ N =(1…1)</em>对应于特征值</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/c25db733252a9927250b09105b3a6d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/1*V5TXv2vyecA_g4bvCvzP6w.gif"/></div></figure><p id="af0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">∫<em class="lt">R * U _ N =(Nm+v)* U _ N</em></p><p id="91dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">和其他所有<em class="lt"> N-1 </em>特征值等于<em class="lt"> v </em> </strong>，(怎么？满足<em class="lt"> trace(R) =特征值之和= N(m + v) </em>的事实)，此外，谱的连续部分在λ- = λ+= <em class="lt"> v </em>处折叠成δ函数，作为<em class="lt"> p </em> - &gt; inf，并且只有一个值(= <strong class="ky ir"> λ_ </strong> <em class="lt"> N </em>)大于<strong class="ky ir"> λ </strong> +。</p><p id="5a51" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果<em class="lt"> m </em> =0，即无偏置的居中输入，显著降低<strong class="ky ir"> λ </strong> _ <em class="lt"> N </em>，则<strong class="ky ir"> λ </strong> _ <em class="lt"> N </em>的最大部分被消除，并且由于<strong class="ky ir"> λ </strong> _ <em class="lt"> N </em>最大(<strong class="ky ir"><em class="lt">λ</em></strong><em class="lt">_ max</em>，降低其值<strong class="ky ir">会影响<em class="lt"> t_mx</em></strong></p><p id="5036" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，很明显，有偏输入会产生较大的特征值，并导致收敛缓慢，为了消除这种情况，可以将输入居中，<br/>或者，根据我们之前的等式，另一种处理方法是使用与<em class="lt"> N </em>(被观察神经元的输入数量)成反比的个体学习率。</p><p id="431b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有这一切清楚地证明了以下几点</p><ul class=""><li id="bcfc" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">对于损失面，梯度依赖于 i/p 协方差矩阵和权重，但 Hessian 在给定主轴方向(这里是第 I 列<em class="lt"> U </em>)是常数，只依赖于 i/p 协方差矩阵</li><li id="5cd6" class="mb mc iq ky b kz ms lc mt lf mu lj mv ln mw lr mg mh mi mj bi translated">偏置输入减缓收敛</li><li id="d0fc" class="mb mc iq ky b kz ms lc mt lf mu lj mv ln mw lr mg mh mi mj bi translated">输入协方差矩阵的大特征值(与其他特征值相比很大，但不是绝对大)也会减缓收敛</li><li id="d34d" class="mb mc iq ky b kz ms lc mt lf mu lj mv ln mw lr mg mh mi mj bi translated">误差曲面的 Hessian 特征值谱受输入分量分布方差的控制</li><li id="33d8" class="mb mc iq ky b kz ms lc mt lf mu lj mv ln mw lr mg mh mi mj bi translated">通过很好地控制协方差矩阵的特征值，可以使用大的学习率</li></ul><p id="30df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">就批次而言:</strong></p><ul class=""><li id="f5a4" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">我们知道“相对”大的特征值(基本上，外围特征值)损害了收敛性，并且批处理范数抑制了这些异常值(基于戈尔巴尼和 al.⁴的特征谱图)，指出了 BN 工作的原因之一</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/d59aa4478ae0075322ebb78206298987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MxgBvkXmhRqytswgmGVIvQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nd">图 6 </strong>来自古尔巴尼等人 al.⁴，y 轴显示<strong class="bd nd"> λ_mx </strong> / <strong class="bd nd"> λ_mn </strong>其中<strong class="bd nd"> λ_mx </strong>和<strong class="bd nd"> λ_mn </strong>分别是 Hessian 的最大和最小特征值。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/9e0bd5d1b49921e142bfcd11a21e538c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fmPYahvK5kFNWyrbP9hwkA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nd">戈尔巴尼和 al.⁴的图 7 </strong>显示了批次规范和无批次规范的损失表面的海森特征谱</p></figure><ul class=""><li id="0538" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">在这个博客的第一部分，我们有一个分析的原因，为什么“相对”大的特征值伤害，戈尔巴尼和 al.⁴显示在梯度能量方面。通过分析(图表和附录 E)，他们表明，如果没有批次范数，在时间步长<em class="lt"> t </em>计算的“随机”梯度的方向与朝向最优值的方向不一致，相反，它主要位于主特征向量的子空间中，并且主要与朝向最优值的方向正交。但是在批量范数之后，梯度方向几乎与主特征向量<br/>正交，并且更好地对准最优点的方向。因为，当梯度在主特征向量的方向上对齐时，只有那些分量得到训练，而其他分量(具有低特征值)需要时间来训练(因为梯度主要朝向其他方向，主特征向量的方向)，并且梯度在具有低特征值的方向上的投影很小，因此收敛很慢。即使在上面这里，正如我们计算的<strong class="ky ir">t =(1/<em class="lt">η</em>*<em class="lt">λ</em>)</strong>，对于给定的η，low <strong class="ky ir"> λ </strong>需要更多的时间，现在这个<strong class="ky ir"> λ </strong>基本上是在给定特征向量方向上的二阶导数，小意味着更平坦的区域。并且，<strong class="ky ir">这些具有小λ的方向贡献了几乎 50%的 Hessian 特征值的 L1 能量</strong></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/f99918815e1ae74e8092cd0895d5b177.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/1*3uC2ihWRDPhKImK9kOP3sQ.gif"/></div></figure><p id="8582" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基本上，这些小特征值对损耗很重要，最佳路径由这些方向组成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/9adc588f0847b9f20919c4a111a40fce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tiVW2HiWE2VseAhhj-zlgA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nd">图 11 </strong>来自戈尔巴尼和 al.⁴，y 轴是损失梯度在最主要特征值方向上的投影的范数与损失梯度的范数之比，显然，在非 batchnorm 模型中，损失梯度主要在最主要特征值的方向上</p></figure><ul class=""><li id="7d39" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">上一点是关于 Hessian 特征值的 L1 能量(此处为<strong class="ky ir"> λ </strong> _i)，现在讨论梯度的能量(梯度能量基本上由梯度向量的协方差的特征谱描述)，因为小的特征值大约包括 Hessian 特征值的 50%的 L1 能量，<strong class="ky ir">几乎所有的梯度能量都集中在异常值</strong>所跨越的子空间中。所以基本上，特征值小的那些组成了梯度能量的一小部分，但是根据第二点，它们很重要，所以这使得整个收敛过程很慢。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/b90ab2ddbd9eedaa4dc8ec3a9b9eeb9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ELxyVXFOiZtw8JR5NsOxSw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">戈尔巴尼和 al.⁴的图 12，y 轴是损耗梯度向量与损耗向量本身的内积，很明显，在非批处理网络中，“随机”损耗梯度与实际损耗方向正交</p></figure><ul class=""><li id="61ab" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">如前所述，收敛完全取决于这些特征值，相对异常值的缺失允许使用基于<strong class="ky ir"><em class="lt">【λ_ MX】</em></strong>的微调学习速率，因此<strong class="ky ir">使得网络对于初始化</strong>不变。</li><li id="877f" class="mb mc iq ky b kz ms lc mt lf mu lj mv ln mw lr mg mh mi mj bi translated">降低协方差矩阵的特征值(通过保持协方差矩阵接近单位矩阵，例如在 BatchNorm 中)允许更高的学习速率，如上所示</li><li id="da43" class="mb mc iq ky b kz ms lc mt lf mu lj mv ln mw lr mg mh mi mj bi translated">批处理范数据说使所有权重的训练以相等的速率发生，它的发生是因为那些特征值不具有任何离群值，如以上所述，所有参数的相同学习速率((=1/ <strong class="ky ir"> <em class="lt"> λ_mx </em> </strong>))将给出相同的收敛速率。</li><li id="939f" class="mb mc iq ky b kz ms lc mt lf mu lj mv ln mw lr mg mh mi mj bi translated">批范数也可以被视为重新参数化误差表面，使其在每个方向上更平滑</li></ul></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><p id="06ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]:“批量归一化:通过减少内部协变量偏移加速深度网络训练”(<a class="ae kv" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1502.03167.pdf</a>)</p><p id="5170" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]:“批处理规范化如何帮助优化？”(<a class="ae kv" href="https://arxiv.org/pdf/1805.11604.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1805.11604.pdf</a>)</p><p id="6d0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]:“协方差矩阵的特征值:在神经网络学习中的应用”(<a class="ae kv" href="http://yann.lecun.com/exdb/publis/pdf/lecun-kanter-solla-91.pdf" rel="noopener ugc nofollow" target="_blank">http://yann . le Cun . com/exdb/publis/pdf/le Cun-kanter-solla-91 . pdf</a>)</p><p id="f26e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]:“基于 Hessian 特征值密度的神经网络优化研究”(<a class="ae kv" href="https://arxiv.org/pdf/1901.10159.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1901.10159.pdf</a>)</p><p id="eef8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5]:“噪声和振动的主动控制(柯林·汉森，斯科特·斯奈德)(<a class="ae kv" href="https://books.google.co.in/books?id=COfqBgAAQBAJ&amp;pg=PA403&amp;lpg=PA403&amp;dq=The+eigenvectors+of+the+input+correlation+matrix+define+the+principal+axes+of+the+error+surface&amp;source=bl&amp;ots=jKSC91Oc_1&amp;sig=ACfU3U2XD6E2ar4WqziymyQYxSGzWLHMDQ&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwjk4Kfb_MflAhVl6nMBHZPFDBkQ6AEwAnoECAYQAQ#v=onepage&amp;q&amp;f=false" rel="noopener ugc nofollow" target="_blank">https://books.google.co.in/books?id=COfqBgAAQBAJ&amp;pg = pa 403&amp;lpg = pa 403&amp;dq = of+特征向量+of+输入+相关性+矩阵+定义+of+误差+曲面&amp;source = bl&amp;ots = jksc 91 oc _ 1&amp;SIG = acfu 3u 2 xd 6 e 2 ar 4 qziymyqyxsgzwlhmd</a></p></div></div>    
</body>
</html>