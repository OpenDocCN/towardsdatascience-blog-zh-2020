<html>
<head>
<title>I built a DIY license plate reader with a Raspberry Pi and machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我用树莓派和机器学习做了一个 DIY 车牌阅读器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/i-built-a-diy-license-plate-reader-with-a-raspberry-pi-and-machine-learning-7e428d3c7401?source=collection_archive---------0-----------------------#2020-02-22">https://towardsdatascience.com/i-built-a-diy-license-plate-reader-with-a-raspberry-pi-and-machine-learning-7e428d3c7401?source=collection_archive---------0-----------------------#2020-02-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3f39" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">机器学习终于变得可行了</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/a13bfcc4a7246b0eb207d1be00e9ac66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*CHrXvdHE24xjDgv0.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">GIF 来自预测<a class="ae ku" href="https://www.youtube.com/watch?v=gsYEZtecXlA" rel="noopener ugc nofollow" target="_blank">视频</a>。检查结果部分。</p></figure><p id="1e12" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">几个月前，我开始考虑让我的汽车具有探测和识别物体的能力。我很喜欢这个想法，因为我已经看到了特斯拉的能力，虽然我不想马上买一辆特斯拉(<em class="lr"> Model 3 看起来越来越多，我不得不说</em>)，但我想我会尝试满足我的梦想。</p><p id="a14c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">所以，我做到了。</p><p id="6c81" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">下面，我记录了项目中的每一步。如果你只是想看探测器运行的视频 GitHub 链接，跳到底部。</p><h1 id="c663" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">第一步。确定项目范围</h1><p id="fd10" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">我开始思考这样一个系统应该有什么能力。如果说我一生中学到了什么的话，那就是从小处着手永远是最好的策略:循序渐进。所以，除了明显的车道保持任务(每个人都已经做过了)，我想到的只是在开车时简单地识别车牌。该识别过程包括 2 个步骤:</p><ol class=""><li id="14c8" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">检测车牌。</li><li id="11d5" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">识别每个牌照的边界框内的文本。</li></ol><p id="a40b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我认为，如果我能做到这一点，那么转移到其他任务应该是相当容易的(如确定碰撞风险、距离等)。甚至可能创建一个环境的向量空间表示——这将是 dope。</p><p id="b124" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在过多担心细节之前，我知道我需要:</p><ul class=""><li id="935c" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq nd mv mw mx bi translated">一种机器学习模型，将未标记的图像作为输入，并检测车牌。</li><li id="8745" class="mp mq it kx b ky my lb mz le na li nb lm nc lq nd mv mw mx bi translated">某种硬件。粗略地说，我需要一个连接到一个或多个摄像机的计算机系统来查询我的模型。</li></ul><p id="f279" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">首先，我着手建立正确的对象检测模型。</p><h1 id="9ee5" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">第二步。选择正确的型号</h1><p id="1377" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">经过仔细研究，我决定采用以下机器学习模型:</p><ol class=""><li id="db88" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated"><em class="lr">yolov 3</em>——这是迄今为止最快的车型，可与其他最先进的车型相媲美<code class="fe ne nf ng nh b">mAP</code>。该模型用于检测物体。</li><li id="ace6" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated"><em class="lr"> CRAFT </em>文本检测器——用于检测图像中的文本。</li><li id="0eda" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated"><em class="lr"> CRNN </em> —基本上是递归的<em class="lr"> CNN </em>(卷积神经网络)模型。它必须是递归的，因为它需要能够将检测到的字符以正确的顺序组成单词。</li></ol><p id="2dcc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这三种模式如何协同工作？嗯，操作流程是这样的:</p><ol class=""><li id="ebd7" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">首先，<em class="lr"> YOLOv3 </em>模型检测从摄像机接收的每一帧中的每个牌照的边界框。建议预测的边界框不要太精确，包含比检测到的对象更多的内容是个好主意。如果太窄，那么后续流程的性能可能会受到影响。这与下面的模型是相辅相成的。</li><li id="2152" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated"><em class="lr"> CRAFT </em>文字检测器从<em class="lr"> YOLOv3 </em>处接收裁剪下来的车牌。现在，如果被裁剪的帧过于狭窄，那么很有可能会遗漏部分车牌文本，这样预测就会失败。但是当边界框更大时，我们可以让<em class="lr"> CRAFT </em>模型检测字母在哪里被发现。这给了我们每个字母非常精确的位置。</li><li id="c8d4" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">最后，我们可以将每个单词的边界框从<em class="lr"> CRAFT </em>传递给我们的<em class="lr"> CRNN </em>模型来预测实际的单词。</li></ol><p id="bed9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">有了我的基本模型架构草图，我可以转移到硬件上。</p><h1 id="e7e1" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">第三步。设计硬件</h1><p id="43a0" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">知道我需要低功率的东西让我想起了我的旧爱:树莓派。它有足够的计算能力预处理帧在一个体面的帧速率，它有 Pi 相机。Pi 相机是树莓 Pi 的实际相机系统。它有一个非常棒的图书馆，非常成熟。</p><p id="063b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">至于互联网接入，我可以只扔在一个 EC25-E 的 4G 接入，也有一个 GPS 模块嵌入从我以前的一个项目。下面是第<a class="ae ku" href="https://www.robertlucian.com/2018/08/29/mobile-network-access-rpi/" rel="noopener ugc nofollow" target="_blank">篇</a>讲的就是这个盾。</p><p id="916c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我决定从圈地开始。把它挂在汽车的后视镜上应该效果不错，所以我最终设计了一个双组件支撑结构:</p><ol class=""><li id="b489" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">在后视镜的一侧，树莓 Pi + GPS 模块+ 4G 模块会留下。查看我在 EC25-E 模块上的文章，了解我对 GPS 和 4G 天线的选择。</li><li id="7e52" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">另一方面，我有一个 Pi 摄像机，通过一个带有球形接头的臂来支撑。</li></ol><p id="78b5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这些支架/外壳将由我信赖的<em class="lr"> Prusa i3 MK3S 3D 打印机</em>打印。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/a7257dd350ad4ea3236dcd3afa5984fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XAIFi1S2rwpkPsxW.jpg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 1—Raspberry Pi+4G/GPS 屏蔽外壳</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/70013ec81eb749cad842eb34f627a696.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5ULVM0Kuarh1LunF.jpg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 2 — Pi 摄像机支架，带有用于定向的球形接头</p></figure><p id="fae3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="lr">图 1 </em>和<em class="lr">图 2 </em>显示了渲染时的结构。请注意，<em class="lr"> C 型支架</em>是可插拔的，因此 Raspberry Pi 的外壳和 Pi 摄像头的支架不附带已经印刷好的支架。它们有一个插座，灯座插在上面。如果我的一个读者决定复制这个项目，这将非常有用。他们只需调整支架，就能在汽车后视镜上工作。目前，持有人在我的车上工作得很好:这是一辆路虎神行者。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nn"><img src="../Images/196807e23faed99485cf2abb68a4901b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NvBy9NfnsmEkynW4.jpg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 3-Pi 摄像机支撑结构的侧视图</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nn"><img src="../Images/691ea544392f8f614b69e6d5df40d100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1poSi6qCLX0cR8Tn.jpg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 4—Pi 摄像机支撑结构和 RPi 支架的前视图</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi no"><img src="../Images/ed11934cbe02322ea703effeb4e6412e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YsRoiRW4xDbn2cRV0FSGLA.jpeg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 5——摄像机视野的想象图</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nn"><img src="../Images/724ffe97334e5976544410a50db4b90f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*C8PDZ_BSyGQRup8H.jpg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图 6 —包含 4G/GPS 模块、Pi 摄像头和 Raspberry Pi 的嵌入式系统特写照片</p></figure><p id="b065" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">显然，这些需要一些时间来建模——我需要几次迭代来使结构坚固。我在 200 微米的图层高度使用了<code class="fe ne nf ng nh b">PETG</code>材质。<code class="fe ne nf ng nh b">PETG</code>在 80-90 度(摄氏度)下工作良好，抗紫外线辐射能力很强——虽然没有<code class="fe ne nf ng nh b">ASA</code>好，但也很强。</p><p id="809b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这是在 SolidWorks 中设计的，所以我所有的<code class="fe ne nf ng nh b">SLDPRT</code> / <code class="fe ne nf ng nh b">SLDASM</code>文件以及所有的<code class="fe ne nf ng nh b">STLs</code>和<code class="fe ne nf ng nh b">gcodes</code>都可以在这里<a class="ae ku" href="https://www.dropbox.com/sh/fw16vy1okrp606y/AAAwkoWXODmoaOP4yR-z4T8Va?dl=0" rel="noopener ugc nofollow" target="_blank">找到</a>。也可以使用它们来打印您的版本。</p><h1 id="c71b" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">第四步。训练模型</h1><p id="ced6" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">一旦我有了硬件，我就开始训练模型。</p><p id="3662" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">不出所料，最好不要多此一举，尽可能重用别人的作品。这就是迁移学习的意义所在——利用来自其他超大型数据集的见解。一个非常恰当的迁移学习的例子是几天前我在这篇文章中读到的。在中间的某个地方，它谈到了一个隶属于哈佛医学院的团队，该团队能够微调一个模型来预测<em class="lr">“从胸片中预测长期死亡率，包括非癌症死亡”</em>。他们只有一个只有 50，000 张标记图像的小数据集，但他们使用的预训练模型(<em class="lr"> Inception-v4 </em>)在大约 1400 万张图像上进行训练。他们花的时间不到原始模型训练时间的一小部分(时间和金钱)，但他们已经达到了相当高的精度。</p><p id="7bba" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">那也是我打算做的。</p><h1 id="0a06" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">YOLOv3</h1><p id="49a4" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">我在网上寻找预训练的车牌模型，并没有我最初预期的那么多，但我找到了一个对大约 3600 张车牌图像进行训练的模型。虽然不多，但也比什么都没有强，除此之外，它还在 Darknet 的预训练模型上进行了训练。我可以利用这点。这是那家伙的<a class="ae ku" href="https://github.com/ThorPham/License-plate-detection" rel="noopener ugc nofollow" target="_blank">型号</a>。</p><p id="a8cf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">由于我已经有了一个可以记录的硬件系统，我决定用我的在镇上开几个小时，收集帧来微调上面那个家伙的模型。</p><p id="abfc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我用<a class="ae ku" href="https://github.com/microsoft/VoTT" rel="noopener ugc nofollow" target="_blank"> VOTT </a>给收集到的帧做了注解(当然还有车牌)。我最终创建了一个由 534 张图片组成的小型数据集，并为车牌添加了带标签的边框。这里是<a class="ae ku" href="https://github.com/RobertLucian/license-plate-dataset" rel="noopener ugc nofollow" target="_blank">数据集</a>。</p><p id="aead" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然后我找到了 YOLOv3 net 的这个 Keras 实现。我用它来训练我的数据集，然后将我的模型预测到这个 repo 中，这样其他人也可以使用它。我在测试集上得到的<em class="lr">图</em>是<em class="lr"> 90% </em>，这确实很好地给出了我的数据集有多小。</p><h1 id="9b51" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">克拉夫特和 CRNN</h1><p id="86cc" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">在无数次试图找到一种好的网络来识别文本之后，我偶然发现了<a class="ae ku" href="https://github.com/faustomorales/keras-ocr" rel="noopener ugc nofollow" target="_blank"> keras-ocr </a>，这是 CRAFT 和 CRNN 的打包和灵活版本。而且还附带了他们预先训练好的模型。太棒了。我决定不对模型进行微调，让它们保持原样。</p><p id="ab02" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最重要的是，用<code class="fe ne nf ng nh b">keras-ocr</code>预测文本非常简单。基本上只有几行代码。查看他们的主页，看看是怎么做的。</p><h1 id="f7ac" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">第五步。部署我的车牌检测器模型</h1><p id="cbe6" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">我可以采用两种主要的模型部署方法:</p><ol class=""><li id="c6af" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">在本地进行所有的推理。</li><li id="6dd9" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">在云中进行推理。</li></ol><p id="39ef" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这两种方法都面临着挑战。第一个意味着有一个大的“大脑”计算机系统，这是复杂和昂贵的。第二个挑战是延迟和基础设施，特别是使用 GPU 进行推理。</p><p id="4e41" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在我的研究中，我偶然发现了一个叫做<code class="fe ne nf ng nh b"><a class="ae ku" href="https://github.com/cortexlabs/cortex" rel="noopener ugc nofollow" target="_blank">cortex</a></code>的开源项目。这对于游戏来说是相当新的，但它肯定是人工智能开发工具进化的下一步。</p><p id="6a7a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">基本上，<code class="fe ne nf ng nh b">cortex</code>是一个只需轻轻一按开关，就可以将机器学习模型部署为生产 web 服务的平台。这意味着我可以专注于我的应用程序，剩下的交给<code class="fe ne nf ng nh b">cortex</code>来管理。在这种情况下，它在 AWS 上做所有的准备工作，我唯一要做的就是使用模板模型来编写我的预测器。更牛逼的是，每个型号我只需要写几十行。</p><p id="7f37" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这是从他们的<a class="ae ku" href="https://github.com/cortexlabs/cortex" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中截取的<code class="fe ne nf ng nh b">cortex</code>在终端中的动作。如果这不是美丽和简单的，那么我不知道该叫它什么:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/b6aeadaf73f6d04cae9688f8b1b066ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_Mpqni6tztmoDWDk"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来源:<a class="ae ku" href="https://github.com/cortexlabs/cortex" rel="noopener ugc nofollow" target="_blank"> Cortex GitHub </a></p></figure><p id="9fa1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">由于这个计算机视觉系统不是由自动驾驶仪使用的，所以延迟对我来说并不重要，为此我可以选择<code class="fe ne nf ng nh b">cortex</code>。如果它是自动驾驶系统的一部分，那么使用通过云提供商提供的服务就不是一个好主意，至少现在不是。</p><p id="d420" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">使用<code class="fe ne nf ng nh b">cortex</code>部署 ML 模型只是一个问题:</p><ol class=""><li id="e91a" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">定义<code class="fe ne nf ng nh b">cortex.yaml</code>文件，这是我们的 API 的配置文件。每个 API 将处理一种类型的任务。我分配的<code class="fe ne nf ng nh b">yolov3</code> API 用于检测给定帧上的车牌边框，<code class="fe ne nf ng nh b">crnn</code> API 用于使用<em class="lr">工艺</em>文本检测器和<em class="lr"> CRNN </em>预测车牌号码。</li><li id="37b1" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">定义每个 API 的预测值。基本上，在<code class="fe ne nf ng nh b">cortex</code>中定义一个特定类的<code class="fe ne nf ng nh b">predict</code>方法来接收一个有效载荷(所有的<em class="lr"> servy </em>部分都已经被平台处理了)，使用有效载荷预测结果，然后返回预测结果。就这么简单！</li></ol><p id="23cb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在不深入我是如何做到这一点的具体细节的情况下(为了让文章保持一个适当的长度)，这里有一个经典虹膜数据集的预测器的例子。cortex 实现这两个 API 的链接可以在<a class="ae ku" href="https://github.com/cortexlabs/cortex/tree/master/examples/tensorflow/license-plate-reader" rel="noopener ugc nofollow" target="_blank">上找到，他们的资源库在这里</a>——这个项目的所有其他资源都在本文的末尾。</p><pre class="kj kk kl km gt np nh nq nr aw ns bi"><span id="4432" class="nt lt it nh b gy nu nv l nw nx"># predictor.pyimport boto3<br/>import picklelabels = ["setosa", "versicolor", "virginica"]<br/>class PythonPredictor:<br/>    def __init__(self, config):<br/>        s3 = boto3.client("s3")<br/>        s3.download_file(config["bucket"], config["key"], "model.pkl")<br/>        self.model = pickle.load(open("model.pkl", "rb"))    def predict(self, payload):<br/>        measurements = [<br/>            payload["sepal_length"],<br/>            payload["sepal_width"],<br/>            payload["petal_length"],<br/>            payload["petal_width"],<br/>        ]        label_id = self.model.predict([measurements])[0]<br/>        return labels[label_id]</span></pre><p id="e728" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然后做一个预测，你就像这样用<code class="fe ne nf ng nh b">curl</code></p><pre class="kj kk kl km gt np nh nq nr aw ns bi"><span id="0cb2" class="nt lt it nh b gy nu nv l nw nx">curl <a class="ae ku" href="http://***.amazonaws.com/iris-classifier" rel="noopener ugc nofollow" target="_blank">http://***.amazonaws.com/iris-classifier</a> \<br/>    -X POST -H "Content-Type: application/json" \<br/>    -d '{"sepal_length": 5.2, "sepal_width": 3.6, "petal_length": 1.4, "petal_width": 0.3}'</span></pre><p id="807a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">预测响应看起来像这样<code class="fe ne nf ng nh b">"setosa"</code>。很简单！</p><h1 id="fb13" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">第六步。开发客户端</h1><p id="1cbb" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">有了<code class="fe ne nf ng nh b">cortex</code>来处理我的部署，我可以继续设计客户端——这是棘手的部分。</p><p id="5b7a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我想到了以下架构:</p><ol class=""><li id="7ece" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">从 Pi 摄像机中以 30 FPS 的速度采集分辨率合适的帧(<em class="lr"> 800x450 </em>或<em class="lr"> 480x270 </em>)，并将每一帧推入一个公共队列。</li><li id="257f" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">在一个单独的进程中，我将从队列中取出帧，并将它们分发给不同线程上的许多工作线程。</li><li id="e1f5" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">每个工作线程(我称之为推理线程)都会向我的<code class="fe ne nf ng nh b">cortex</code>API 发出 API 请求。首先，向我的<code class="fe ne nf ng nh b">yolov3</code> API 发出一个请求，然后，如果检测到任何牌照，向我的<code class="fe ne nf ng nh b">crnn</code> API 发出另一个请求，请求一批被裁剪的牌照。响应将包含文本格式的预测车牌号码。</li><li id="19e3" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">将每个检测到的牌照(有或没有识别的文本)推送到另一个队列，最终将其广播到浏览器页面。同时，还将板号预测推送到另一个队列，以便稍后保存到磁盘(以<code class="fe ne nf ng nh b">csv</code>格式)。</li><li id="b654" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">广播队列将接收一堆无序的帧。其消费者的任务是在每次向客户端广播新的帧时，通过将它们放置在非常小的缓冲区(几个帧大小)中来对它们进行重新排序。该使用者正在单独运行另一个进程。这个消费者还必须设法将队列的大小保持在一个指定的值，以便帧能够以一致的帧速率显示，即 30 FPS。显然，如果队列大小下降，那么帧速率的下降是成比例的，反之亦然，当队列大小增加时，帧速率也成比例地增加。最初，我想实现一个滞后函数，但我意识到这会给流一种非常不稳定的感觉。</li><li id="d660" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">与此同时，在主进程中会有另一个线程运行，从另一个队列中提取预测和 GPS 数据。当客户端收到终止信号时，预测、GPS 数据和时间也被转储到一个<code class="fe ne nf ng nh b">csv</code>文件中。</li></ol><p id="a8be" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里是与 AWS 上的云 API 相关的客户端的流程图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ny"><img src="../Images/3bbcfec0ebe8ba33d245fa0f3b649496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bqGTKL3vgkdIN3xfQCwr-Q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><em class="nz">图 7 —客户端流程图以及配备 cortex 的云 APIs】</em></p></figure><p id="cbbf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在我们的例子中，客户端是 Raspberry Pi，推理请求发送到的云 API 是由 AWS 上的<code class="fe ne nf ng nh b">cortex</code>提供的(<em class="lr"> Amazon Web Services </em>)。</p><p id="7c06" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">客户端的源代码也可以在其<a class="ae ku" href="https://github.com/RobertLucian/cortex-license-plate-reader-client" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>上查看。</p><p id="2f9e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我必须克服的一个特殊挑战是 4G 网络的带宽。最好降低该应用程序所需的带宽，以减少可能的挂起或对可用数据的过度使用。我决定在 Pi 摄像头上使用非常低的分辨率:<code class="fe ne nf ng nh b">480x270</code>(我们可以使用小分辨率，因为 Pi 摄像头的视野非常窄，所以我们仍然可以轻松识别车牌)。尽管如此，即使在这个分辨率下，一帧的 JPEG 大小在 10 兆比特时也有 100KB 左右。乘以每秒 30 帧得到 3000KB，大约是 24 Mb/s，这还不包括 HTTP 开销——这是一个很大的数目。</p><p id="8e5e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">相反，我做了以下几招:</p><ul class=""><li id="2b77" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq nd mv mw mx bi translated">将宽度缩小到 416 像素，这正是 YOLOv3 模型调整图像的大小。规模显然保持完整。</li><li id="9803" class="mp mq it kx b ky my lb mz le na li nb lm nc lq nd mv mw mx bi translated">已将图像转换为灰度。</li><li id="e347" class="mp mq it kx b ky my lb mz le na li nb lm nc lq nd mv mw mx bi translated">移除图像顶部 45%的部分。这种想法是，车牌不会出现在画面的顶部，因为汽车不会飞，对吗？据我所见，剪切掉 45%的图像不会影响预测器的性能。</li><li id="5e4e" class="mp mq it kx b ky my lb mz le na li nb lm nc lq nd mv mw mx bi translated">再次将图像转换为 JPEG，但质量较低。</li></ul><p id="ce3c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最终得到的帧大小约为 7–10KB，非常好。这相当于 2.8Mb/s。但是加上所有开销，大约是 3.5Mb/s(包括响应)。</p><p id="a061" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">对于<code class="fe ne nf ng nh b">crnn</code> API，即使没有应用压缩技巧，裁剪后的车牌也不会占用太多时间。他们坐在大约 2-3KB 的一块。</p><p id="1b3e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">总而言之，要以 30FPS 的速度运行，推理 API 所需的带宽大约是 6Mb/s。这个数字我可以接受。</p><h1 id="1aaf" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">结果</h1><p id="2012" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">有用！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="283d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">上面的例子是通过<code class="fe ne nf ng nh b">cortex</code>运行推理的实时例子。我需要大约 20 个配备 GPU 的实例才能顺利运行。根据集群的延迟，您可能需要更多或更少的实例。从一帧被捕获到广播到浏览器窗口的平均延迟大约为<em class="lr"> 0.9 秒</em>，考虑到推断发生在很远的某个地方，这真是太不可思议了——这仍然让我感到惊讶。</p><p id="728e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">文本识别部分可能不是最好的，但它至少证明了这一点——通过提高视频的分辨率、缩小摄像头的视野或进行微调，它可以更加准确。</p><p id="6174" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">至于高 GPU 数量，这可以通过优化来减少。例如，转换模型以使用混合/全半精度(<em class="lr"> FP16/BFP16 </em>)。一般来说，让模型使用混合精度将对精度产生最小的影响，所以我们不会有太大的损失。</p><p id="6423" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">T4 和 V100 GPUs 具有特殊的张量核，在半精度类型的矩阵乘法上速度超快。与单精度运算相比，半精度运算在 T4 上的加速比约为 8 倍，在 V100 上为 10 倍。这是一个数量级的差异。这意味着，已经转换为使用单精度/混合精度的模型进行推理所需的时间可以减少 8 倍，而在 V100 上只需十分之一的时间。</p><p id="0014" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我没有将模型转换为使用单精度/混合精度，因为这超出了本项目的范围。在我看来，这只是一个优化问题。我最有可能在<code class="fe ne nf ng nh b">cortex</code>的版本<code class="fe ne nf ng nh b">0.14</code>发布时这样做(具有真正的多进程支持和基于队列的自动伸缩)，这样我也可以利用多进程 web 服务器。</p><p id="e84c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">总而言之，如果所有优化都到位，将集群的大小从 20 个配备 GPU 的实例减少到只有一个实例实际上是可行的。如果适当优化，甚至不可能最大化一个配备 GPU 的实例。</p><p id="9112" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了让成本更容易接受，在 AWS 上使用<a class="ae ku" href="https://aws.amazon.com/machine-learning/elastic-inference" rel="noopener ugc nofollow" target="_blank">弹性推理</a>可以减少高达 75%的成本，这是一个很大的数目！打个比方，你可以花一毛钱买一个管道来实时处理一个流。不幸的是，目前<code class="fe ne nf ng nh b">cortex</code>还不支持弹性推理，但我可以预见在不久的将来它会得到支持，因为它已经引起了他们的注意。见票<a class="ae ku" href="https://github.com/cortexlabs/cortex/issues/618" rel="noopener ugc nofollow" target="_blank">cortexlabs/cortex/issues/618</a>。</p><p id="ec56" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="lr">注意:YOLOv3 和 CRNN 模型，可以通过在更大的数据集(大约 50-100k 个样本)上对它们进行微调来大大改进。在这一点上，即使帧的大小可以进一步减少，以减少数据的使用，而不会失去太多的准确性:“补偿某处，以便能够从其他地方”。这与将所有这些模型转换为使用半精度类型(以及可能的弹性推理)相结合，可以构成一个非常高效/划算的推理机。</em></p><h2 id="28ce" class="nt lt it bd lu oc od dn ly oe of dp mc le og oh me li oi oj mg lm ok ol mi om bi translated">更新一</h2><p id="c447" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">有了支持 web 服务器多进程工作器的<code class="fe ne nf ng nh b">cortex</code>版本<code class="fe ne nf ng nh b">0.14</code>，我能够将<code class="fe ne nf ng nh b">yolov3</code> API 的 GPU 实例数量从 8 个减少到 2 个，将<code class="fe ne nf ng nh b">crnn</code> API(在<em class="lr"> CRNN </em>和<em class="lr"> CRAFT </em>模型上运行推理)的 GPU 实例数量从 12 个减少到 10 个。这实际上意味着实例总数减少了 40%，这是一个非常好的收获。所有这些实例都配备了单个 T4 GPU 和 4 个虚拟 CPU。</p><p id="a575" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我发现计算最密集的模型是<em class="lr"> CRAFT </em>模型，它是建立在大约 138 米重的<em class="lr"> VGG-16 </em>模型之上的。请记住，每一帧通常需要多个推断，因为在一个镜头中可能会有多个检测到的车牌。这大大增加了 it 的计算需求。从理论上讲，<em class="lr"> CRAFT </em>模型应该被淘汰，取而代之的是改进(微调)CRNN 模型，以更好地识别车牌。通过这种方式，<code class="fe ne nf ng nh b">crnn</code> API 可以缩小很多——减少到只有 1 或 2 个实例。</p><h2 id="bd9e" class="nt lt it bd lu oc od dn ly oe of dp mc le og oh me li oi oj mg lm ok ol mi om bi translated">更新二</h2><p id="8573" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">其他一些可以显著降低计算成本的因素包括:</p><ol class=""><li id="2748" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">使用<em class="lr"> TinyYOLOv3 </em>代替<em class="lr"> YOLOv3 </em>。运行一个推理大约需要 12 倍的时间。运行不太复杂的模型时丢失的<em class="lr">映射</em>可以通过在更全面的数据集上训练模型来恢复。</li><li id="262c" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">使用对象跟踪来避免必须检测每一帧上的牌照。这大大减少了两个 API 所需的推理总数。目标跟踪应该在本地完成，因为它在计算上是廉价的。</li><li id="5380" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">此外，我们不一定需要所有类型的锚盒为<em class="lr">约洛夫 3 </em>。我们只需要那些符合车牌形状的。减少数字可以使推断更快。</li><li id="c853" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">为输入源选择更高的帧速率，并在处理时只选取第<em class="lr">个</em>帧。例如，如果摄像机以 120 FPS 的速度运行，只需每隔 4 帧选取一帧，仍然可以得到 30 FPS。这允许更清晰/清晰的帧，从而导致更准确的预测。快速行驶的汽车是文本识别的罪魁祸首。</li><li id="a853" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">用<a class="ae ku" href="https://github.com/openalpr/openalpr" rel="noopener ugc nofollow" target="_blank"> OpenALPR </a>代替<em class="lr"> CRAFT + CRNN。我不完全确定这是不是一个完全好的主意，但是从我从别人那里听到的来看，这非常快。</em></li><li id="b48f" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">知道发送到文本识别 API 的每一批帧都是车牌，我可以看到如何使用<em class="lr">自动编码器</em>来大幅度压缩图像以进行数据传输。或者如果不是这样，那么至少它们可以用来从这些帧中去除噪声。由于自动编码器运行起来通常非常便宜，我可以看到如何在 Raspberry Pi 上使用它们来预处理帧。然后它们会在 API 级别被解码。</li></ol><p id="a19a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里的<a class="ae ku" href="https://news.ycombinator.com/item?id=22398872" rel="noopener ugc nofollow" target="_blank">这里的</a>、这里的<a class="ae ku" href="https://hackaday.com/2020/02/26/now-you-can-be-big-brother-too-with-a-raspberry-pi-license-plate-reader/" rel="noopener ugc nofollow" target="_blank"/>、这里的<a class="ae ku" href="https://www.reddit.com/r/learnmachinelearning/comments/f8tww9/i_built_a_diy_license_plate_reader_with_a/" rel="noopener ugc nofollow" target="_blank"/>或者这里的<a class="ae ku" href="https://www.reddit.com/r/raspberry_pi/comments/f8tut7/i_built_a_diy_license_plate_reader_with_a/" rel="noopener ugc nofollow" target="_blank"/>讨论了这些改进。</p><p id="f8d5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">至于产生的 AWS 集群成本，我有以下想法:</p><ul class=""><li id="97fb" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq nd mv mw mx bi translated">在当前配置下，运行此操作的成本约为 4.00 美元/小时。只有使用 spot 实例才有可能。</li><li id="3d7d" class="mp mq it kx b ky my lb mz le na li nb lm nc lq nd mv mw mx bi translated">假设所有的改进都到位了，成本可能会下降到<em class="lr"> ~0.2 美分/小时</em>。实际上，只需要大约 1%的 T4 GPU 的能力来运行这些。这也是一个非常粗略的数字。这当然可以通过运行 spot 实例来实现。</li></ul><h1 id="d42e" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">结论(以及 5G 如何融入所有这些)</h1><p id="43cf" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">我看到设备开始越来越依赖云计算，尤其是计算能力有限的边缘设备。由于 5G 目前正在部署，理论上它应该使云更接近这些计算受限的设备。因此，云的影响力应该随之增长。5G 网络越可靠，越普及，人们就越有信心将所谓的关键任务的计算转移到云上，比如无人驾驶汽车。</p><p id="30c3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我从这个项目中学到的另一件事是，随着在云中部署机器学习模型的流线型平台的出现，事情变得多么容易。5 年前，这可能是一个相当大的挑战:但现在，一个人可以在相对较短的时间内做这么多。</p><h1 id="fbf3" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">资源</h1><ul class=""><li id="1df7" class="mp mq it kx b ky mk lb ml le on li oo lm op lq nd mv mw mx bi translated">3D 打印支架的所有<code class="fe ne nf ng nh b">SLDPRTs</code> / <code class="fe ne nf ng nh b">SLDASMs</code> / <code class="fe ne nf ng nh b">STLs</code> / <code class="fe ne nf ng nh b">gcodes</code>都可以在<a class="ae ku" href="https://www.dropbox.com/sh/fw16vy1okrp606y/AAAwkoWXODmoaOP4yR-z4T8Va?dl=0" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</li><li id="3346" class="mp mq it kx b ky my lb mz le na li nb lm nc lq nd mv mw mx bi translated">这个项目的客户端实现可以在找到<a class="ae ku" href="https://github.com/RobertLucian/cortex-license-plate-reader-client" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="7790" class="mp mq it kx b ky my lb mz le na li nb lm nc lq nd mv mw mx bi translated">这个项目的<code class="fe ne nf ng nh b">cortex</code>实现在这里<a class="ae ku" href="https://github.com/cortexlabs/cortex/tree/master/examples/tensorflow/license-plate-reader" rel="noopener ugc nofollow" target="_blank">找到。</a></li><li id="cab7" class="mp mq it kx b ky my lb mz le na li nb lm nc lq nd mv mw mx bi translated"><em class="lr"> Keras </em>中<em class="lr"> YOLOv3 </em>模型的库在<a class="ae ku" href="https://github.com/experiencor/keras-yolo3" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</li><li id="1c5d" class="mp mq it kx b ky my lb mz le na li nb lm nc lq nd mv mw mx bi translated">用于<em class="lr">工艺的库</em>文本检测器+ <em class="lr"> CRNN </em>文本识别器在<a class="ae ku" href="https://github.com/faustomorales/keras-ocr" rel="noopener ugc nofollow" target="_blank">这里找到</a>。</li><li id="21a1" class="mp mq it kx b ky my lb mz le na li nb lm nc lq nd mv mw mx bi translated">欧洲车牌的数据集(由我的 Pi 相机拍摄的 534 个样本组成)在这里找到<a class="ae ku" href="https://github.com/RobertLucian/license-plate-dataset" rel="noopener ugc nofollow" target="_blank"/>。</li><li id="80e7" class="mp mq it kx b ky my lb mz le na li nb lm nc lq nd mv mw mx bi translated"><em class="lr">Keras</em>(<em class="lr">license _ plate . H5</em>)和<em class="lr">saved model</em>(<em class="lr">yolov 3</em>folder/zip)格式的<em class="lr"> YOLOv3 </em>模型在这里找到<a class="ae ku" href="https://www.dropbox.com/sh/4ltffycnzfeul01/AACe85GoIzlmjEnIhuh5JQPma?dl=0" rel="noopener ugc nofollow" target="_blank">。</a></li></ul></div></div>    
</body>
</html>