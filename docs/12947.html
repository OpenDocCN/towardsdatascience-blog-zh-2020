<html>
<head>
<title>Fine-tune a non-English GPT-2 Model with Huggingface</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调一个非英语 GPT-2 模型与拥抱脸</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b?source=collection_archive---------13-----------------------#2020-09-06">https://towardsdatascience.com/fine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b?source=collection_archive---------13-----------------------#2020-09-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a530" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">微调非英语，德国 GPT-2 模型与德国食谱拥抱脸。使用它们的训练器类和管道对象</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0cb361717fb1634b058961cbd49febe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dLmEH9nptxjfdFBvZfqPIQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@fifthlane?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">彼得道恩</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="98af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">原载于 2020 年 9 月 6 日</em><a class="ae ky" href="https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface" rel="noopener ugc nofollow" target="_blank"><em class="lv">https://www . philschmid . de</em></a><em class="lv">。</em></p><h1 id="7f48" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">介绍</h1><p id="feb4" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">除非你生活在岩石下，否则你可能听说过 OpenAI 的 GPT-3 语言模型。你可能也看过所有疯狂的演示，其中模型编写了<code class="fe mt mu mv mw b">JSX</code>、<code class="fe mt mu mv mw b">HTML</code>代码，或者它在零/少量学习领域的能力。西蒙·奥里甘写了一篇<a class="ae ky" rel="noopener" target="_blank" href="/gpt-3-demos-use-cases-implications-77f86e540dc1">文章，里面有基于 GPT 3 的优秀演示和项目。</a></p><p id="c0ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GPT-3 的一个缺点是它有 1750 亿个参数，这导致模型大小约为 350GB。相比之下，GPT-2 迭代的最大实现有 15 亿个参数。这小于 1/116 的大小。</p><p id="8119" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，由于有接近 175B 的可训练参数，GPT-3 在尺寸上比其他任何型号都要大得多。这里是最近流行的 NLP 模型的参数数量的比较，GPT-3 明显突出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/71bc1922a6ad742fb15b9f495bf48283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jBl-cX-CmFliPxR3AabBFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface" rel="noopener ugc nofollow" target="_blank">作者创作</a></p></figure><p id="b57a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一切都很壮观，但是你不需要 1750 亿个参数就能在<code class="fe mt mu mv mw b">text-generation</code>中获得好的结果。</p><p id="399e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">已经有关于如何微调 GPT-2 的教程了。但是很多都过时了。在本教程中，我们将在最新版本(3.1.0)中使用<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>的<code class="fe mt mu mv mw b">transformers</code>库。我们将使用新的<code class="fe mt mu mv mw b">Trainer</code>级，并用来自<a class="ae ky" href="http://chefkoch.de" rel="noopener ugc nofollow" target="_blank"> chefkoch.de </a>的德国配方对我们的 GPT-2 模型进行微调。</p><p id="ee91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这个<a class="ae ky" href="https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb" rel="noopener ugc nofollow" target="_blank"> colab 笔记本上找到我们正在做的一切。</a></p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="b54c" class="lw lx it bd ly lz nf mb mc md ng mf mg jz nh ka mi kc ni kd mk kf nj kg mm mn bi translated">变形金刚库由<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> Huggingface </a></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/000e618952a4826452427f422b675fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/0*CwRmpyRxUq8Z2s20.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚标志</a></p></figure><p id="fe4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚库</a>为自然语言理解(NLU)和自然语言生成(NLG)提供最先进的机器学习架构，如 BERT、GPT-2、罗伯塔、XLM、DistilBert、XLNet、T5。它还提供了 100 多种不同语言的数千个预训练模型，并可在 PyTorch &amp; TensorFlow 2.0 之间深度互操作。它使开发人员能够针对不同的 NLP 任务(如文本分类、情感分析、问答或文本生成)微调机器学习模型。</p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="86f2" class="lw lx it bd ly lz nf mb mc md ng mf mg jz nh ka mi kc ni kd mk kf nj kg mm mn bi translated">辅导的</h1><p id="97a6" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在教程中，我们从<a class="ae ky" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank"> Huggingface 模型中枢</a>微调一辆德国 GPT-2。作为数据，我们使用<a class="ae ky" href="https://www.kaggle.com/sterby/german-recipes-dataset" rel="noopener ugc nofollow" target="_blank">德国食谱数据集</a>，它由 12190 个德国食谱组成，元数据从<a class="ae ky" href="http://chefkoch.de/" rel="noopener ugc nofollow" target="_blank"> chefkoch.de </a>中抓取。</p><p id="f130" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用食谱指导来微调我们的 GPT-2 模型，并让我们在事后编写我们可以烹饪的食谱。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/38c93e9efaf983de7cefe096ca16b34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HZXvYEY__f5pV79-.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface" rel="noopener ugc nofollow" target="_blank">作者创作</a></p></figure><p id="3fcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我们使用带有 GPU 运行时的 Google Colab。如果你不确定如何使用 GPU 运行时，看看这里的<a class="ae ky" href="https://www.philschmid.de/google-colab-the-free-gpu-tpu-jupyter-notebook-service" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="afd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">我们要做什么:</strong></p><ul class=""><li id="4aff" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated">从 Kaggle 加载数据集</li><li id="368e" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">准备数据集并构建<code class="fe mt mu mv mw b">TextDataset</code></li><li id="9134" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">用<code class="fe mt mu mv mw b">TrainingArguments</code>和 GPT-2 模型初始化<code class="fe mt mu mv mw b">Trainer</code></li><li id="d5ea" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">训练并保存模型</li><li id="9ef5" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">测试模型</li></ul><p id="6628" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这个<a class="ae ky" href="https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb" rel="noopener ugc nofollow" target="_blank"> colab 笔记本</a>里找到我们做的一切。</p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="b4a5" class="lw lx it bd ly lz nf mb mc md ng mf mg jz nh ka mi kc ni kd mk kf nj kg mm mn bi translated">从 Kaggle 加载数据集</h1><p id="14ad" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">正如在教程介绍中已经提到的，我们使用 Kaggle 的"<a class="ae ky" href="https://www.kaggle.com/sterby/german-recipes-dataset" rel="noopener ugc nofollow" target="_blank">德国食谱数据集</a>"数据集。该数据集由 12190 个德国食谱组成，元数据从<a class="ae ky" href="http://chefkoch.de/" rel="noopener ugc nofollow" target="_blank"> chefkoch.de </a>抓取而来。在这个例子中，我们只使用食谱的说明。我们使用“下载”按钮下载数据集，并将其上传到我们的 colab 笔记本，因为它只有 4,7MB 的压缩大小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/7c513304b523198f39780a4423a30650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qqrduS9MZ1ODMsah.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.kaggle.com/sterby/german-recipes-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle 比赛截图</a></p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="3669" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上传文件后，我们使用<code class="fe mt mu mv mw b">unzip</code>提取<code class="fe mt mu mv mw b">recipes.json</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="fecd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">您也可以使用</em> <code class="fe mt mu mv mw b"><em class="lv">kaggle</em></code> <em class="lv"> CLI 来下载数据集，但请注意，您需要在 colab 笔记本中保存您的 Kaggle 凭据。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="f8b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个食谱的例子。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="1d12" class="lw lx it bd ly lz nf mb mc md ng mf mg jz nh ka mi kc ni kd mk kf nj kg mm mn bi translated">准备数据集并构建一个<code class="fe mt mu mv mw b">TextDataset</code></h1><p id="b456" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">下一步是从所有食谱中提取说明，并构建一个<code class="fe mt mu mv mw b">TextDataset</code>。<code class="fe mt mu mv mw b">TextDataset</code>是变形金刚库实现的<a class="ae ky" href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class" rel="noopener ugc nofollow" target="_blank"> Pytroch </a> <code class="fe mt mu mv mw b"><a class="ae ky" href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class" rel="noopener ugc nofollow" target="_blank">Dataset</a></code> <a class="ae ky" href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class" rel="noopener ugc nofollow" target="_blank">类</a>的自定义实现。如果你想在 Pytorch 中了解更多关于<code class="fe mt mu mv mw b">Dataset</code>的信息，你可以看看这个<a class="ae ky" href="https://www.youtube.com/watch?v=PXOzkkB5eH0&amp;ab_channel=PythonEngineer" rel="noopener ugc nofollow" target="_blank"> youtube 视频</a>。</p><p id="37c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将<code class="fe mt mu mv mw b">recipes.json</code>分成一个<code class="fe mt mu mv mw b">train</code>和<code class="fe mt mu mv mw b">test</code>部分。然后我们从食谱中提取出<code class="fe mt mu mv mw b">Instructions</code>并将它们写入<code class="fe mt mu mv mw b">train_dataset.txt</code>和<code class="fe mt mu mv mw b">test_dataset.txt</code></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="2c83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步是下载标记器。我们使用来自<code class="fe mt mu mv mw b">german-gpt2</code>模型的标记器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="46b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以建造我们的<code class="fe mt mu mv mw b">TextDataset</code>。因此，我们用<code class="fe mt mu mv mw b">tokenizer</code>和数据集的路径创建了一个<code class="fe mt mu mv mw b">TextDataset</code>实例。我们还创建了我们的<code class="fe mt mu mv mw b">data_collator</code>，它用于训练从我们的数据集形成一个批处理。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="eaac" class="lw lx it bd ly lz nf mb mc md ng mf mg jz nh ka mi kc ni kd mk kf nj kg mm mn bi translated">用<code class="fe mt mu mv mw b">TrainingArguments</code>和 GPT-2 模型初始化<code class="fe mt mu mv mw b">Trainer</code></h1><p id="dc51" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><a class="ae ky" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer" rel="noopener ugc nofollow" target="_blank">训练器</a>类为全功能训练提供了一个 API。Huggingface 的大多数<a class="ae ky" href="https://huggingface.co/transformers/examples.html" rel="noopener ugc nofollow" target="_blank">示例脚本</a>中都使用了它。在实例化我们的<code class="fe mt mu mv mw b">Trainer</code>之前，我们需要下载我们的 GPT-2 模型并创建<a class="ae ky" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments" rel="noopener ugc nofollow" target="_blank">训练参数</a>。<code class="fe mt mu mv mw b">TrainingArguments</code>用于定义超参数，我们在<code class="fe mt mu mv mw b">learning_rate</code>、<code class="fe mt mu mv mw b">num_train_epochs</code>或<code class="fe mt mu mv mw b">per_device_train_batch_size</code>等训练过程中使用这些超参数。你可以在这里找到完整的列表<a class="ae ky" href="https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="7ab3" class="lw lx it bd ly lz nf mb mc md ng mf mg jz nh ka mi kc ni kd mk kf nj kg mm mn bi translated">训练并保存模型</h1><p id="ccaa" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">为了训练模型，我们可以简单地运行<code class="fe mt mu mv mw b">trainer.train()</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="f2d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练完成后，您可以通过调用<code class="fe mt mu mv mw b">save_model()</code>保存模型。这将把训练好的模型从我们的<code class="fe mt mu mv mw b">TrainingArguments</code>保存到我们的<code class="fe mt mu mv mw b">output_dir</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><h1 id="ed93" class="lw lx it bd ly lz nf mb mc md ng mf mg jz nh ka mi kc ni kd mk kf nj kg mm mn bi translated">测试模型</h1><p id="9bfd" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">为了测试这个模型，我们使用了变形金刚库的另一个<a class="ae ky" href="https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines" rel="noopener ugc nofollow" target="_blank">亮点<code class="fe mt mu mv mw b">pipeline</code>。</a><a class="ae ky" href="https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines" rel="noopener ugc nofollow" target="_blank">管道</a>是提供简单 API 的对象，专用于几个任务，<code class="fe mt mu mv mw b">text-generation</code>等等。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="76d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果:</p><p id="d462" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">第一次做大足，2 分钟做大足。森林在河边倒下了。去死吧。黄油三明治。他的头发也是这样，比你的头发还长。”</em></p><p id="4b97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嗯，就是这样💫。我们做到了👨🏻‍🍳。我们已经成功地微调了我们的 gpt-2 模型来为我们编写食谱。</p><p id="fb79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了改善我们的结果，我们可以训练它更长时间，并调整我们的<code class="fe mt mu mv mw b">TrainingArguments</code>或扩大数据集。</p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><p id="8461" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这本<a class="ae ky" href="https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb" rel="noopener ugc nofollow" target="_blank"> colab 笔记本</a>里找到一切。</p><p id="a21f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读。如果你有任何问题，随时联系我或评论这篇文章。你也可以通过<a class="ae ky" href="https://twitter.com/_philschmid" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或<a class="ae ky" href="https://www.linkedin.com/in/philipp-schmid-a6a2bb196/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。</p></div></div>    
</body>
</html>