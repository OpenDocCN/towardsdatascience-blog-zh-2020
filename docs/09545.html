<html>
<head>
<title>XLNet: Autoregressive Pre-Training for Language Understanding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XLNet:用于语言理解的自回归预训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/xlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710?source=collection_archive---------42-----------------------#2020-07-07">https://towardsdatascience.com/xlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710?source=collection_archive---------42-----------------------#2020-07-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b355" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解基于变压器的自监督架构</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1e1242ecb7481ac63fdb0e4e8f4c0fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d3JZwytCVW65DxFK"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">蒂姆·莫斯霍尔德在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="3210" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像 BERT，OpenAI GPT 这样的艺术语言模型在最近的自然语言处理中已经成为明星。这些模型基于<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">转换器</a>架构，该架构将基于 RNN 和基于卷积的模型挤出了市场。</p><p id="17bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论 XLNET 模型，它是在最近的一篇论文中提出的:<a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet:用于语言理解的广义自回归预训练</a>。该模型解决了 BERT 的某些缺点，并通过<strong class="lb iu">在 20 项任务中超越 BERT</strong>成功克服了这些缺点。</p><p id="3d25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣了解 BERT 或 Transformers 背后的概念，可以考虑阅读一下<a class="ae ky" href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener"> this (BERT) </a>和<a class="ae ky" rel="noopener" target="_blank" href="/transformers-explained-65454c0f3fa7"> this (Transformer) </a>。</p><h1 id="dfbb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">伯特怎么了？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/802f899a4e27bfb279203d85608b4bb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EzCWMxsy73jlzOpM6PK1TQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://demo.allennlp.org/masked-lm?text=The%20doctor%20ran%20to%20the%20emergency%20room%20to%20see%20%5BMASK%5D%20patient." rel="noopener ugc nofollow" target="_blank"> AllenNLP </a>进行的屏蔽 LM 演示</p></figure><h2 id="0136" class="mo lw it bd lx mp mq dn mb mr ms dp mf li mt mu mh lm mv mw mj lq mx my ml mz bi translated">输入噪声</h2><p id="c5b1" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">BERT 的一个主要问题本质上是它对屏蔽序列的预训练目标，即<strong class="lb iu">去噪自动编码目标</strong>。屏蔽序列非常有助于理解语言语料库中的趋势，然而，在微调时，序列不会被屏蔽。</p><blockquote class="nf ng nh"><p id="d56e" class="kz la ni lb b lc ld ju le lf lg jx lh nj lj lk ll nk ln lo lp nl lr ls lt lu im bi translated">然而，<strong class="lb iu">BERT 在预训练期间使用的【MASK】等人工符号在微调时间</strong>的真实数据中不存在，从而导致预训练-微调差异。</p><p id="3d05" class="kz la ni lb b lc ld ju le lf lg jx lh nj lj lk ll nk ln lo lp nl lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> — XLNet 纸</a></p></blockquote><h2 id="802f" class="mo lw it bd lx mp mq dn mb mr ms dp mf li mt mu mh lm mv mw mj lq mx my ml mz bi translated">独立性假设</h2><p id="5f4d" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">BERT 最大化联合条件概率<strong class="lb iu"> <em class="ni"> p(x_t | x_hat) </em> </strong>，其中<strong class="lb iu"> <em class="ni"> x_t </em> </strong>为掩码项<strong class="lb iu"> <em class="ni"> x_hat </em> </strong>为记号序列。它读作，在给定序列<strong class="lb iu"> <em class="ni"> x_hat </em> </strong>中所有记号的情况下，屏蔽记号<strong class="lb iu"> <em class="ni"> x_t </em> </strong>出现在第‘<strong class="lb iu"><em class="ni">t</em></strong>位置的概率。</p><p id="4a3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这给出了独立性<strong class="lb iu">假设的直觉，即每个被屏蔽的记号被单独重建</strong>。我们将在后面的部分中清除这一点。</p><h1 id="dfcb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">XLNET</h1><p id="d617" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">与 BERT 相反，<strong class="lb iu"> XLNet 是一个自回归模型</strong>。这实质上消除了对输入去噪的依赖。</p><p id="6f73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，自回归模型大多因其单向性而受到批评。因此，为了克服这一点，XLNet 提出了一个新的<strong class="lb iu">置换语言建模</strong>目标来克服这种单向性。</p><h2 id="8523" class="mo lw it bd lx mp mq dn mb mr ms dp mf li mt mu mh lm mv mw mj lq mx my ml mz bi translated">置换语言建模</h2><p id="e168" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">如前所述，XLNet 提出了一种从两个世界(即自动编码和自回归)中吸取精华的机制。它没有自动编码目标中的输入去噪，并消除了传统自回归目标的单向性。</p><p id="f96c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了实现这一点，在对联合概率<strong class="lb iu"><em class="ni">p(x _ t | x _(I&lt;t))</em></strong>，<strong class="lb iu">进行因式分解时，XLNet 不像在传统的自回归模型中那样使用固定的向前或向后因式分解顺序</strong>，而是最大化一个序列的对数似然性 w.r.t <strong class="lb iu">因式分解顺序</strong>的所有可能排列。</p><blockquote class="nf ng nh"><p id="4107" class="kz la ni lb b lc ld ju le lf lg jx lh nj lj lk ll nk ln lo lp nl lr ls lt lu im bi translated">具体来说，对于长度为<strong class="lb iu"> T </strong>的序列<strong class="lb iu"> x </strong>，有<strong class="lb iu"> T！</strong>不同的订单执行一个有效的自回归因子分解。直观地说，如果<strong class="lb iu">模型参数在所有分解订单</strong>中共享，那么<strong class="lb iu">模型将学习从两侧</strong>的所有位置收集信息。</p><p id="9ccb" class="kz la ni lb b lc ld ju le lf lg jx lh nj lj lk ll nk ln lo lp nl lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> — XLNet 纸</a></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/e0299156d8e1ec45cb08b80d82cbe26e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8Q7-9wklX73LA4lZUPCjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">置换语言建模来自<a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet 论文</a></p></figure><p id="aff2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更详细地说明这个目标，让我们举一个例子。考虑上图，一个序列<strong class="lb iu"> <em class="ni"> x </em> </strong>有 4 个令牌。为简单起见，我们只考虑<strong class="lb iu"><em class="ni">x3</em></strong>的注意力计算。观察上面每个图下面的排列顺序。</p><ul class=""><li id="cdf1" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">而取顺序<strong class="lb iu"><em class="ni">3-&gt;2-&gt;4-&gt;1</em></strong>，<strong class="lb iu"> <em class="ni"> 3 </em> </strong>恰好是序列中的第一个令牌。因此，没有其他标记对其注意力计算有贡献。<strong class="lb iu">因为它们在当前排列</strong>中不先于 3。</li><li id="a76e" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">按照<strong class="lb iu"><em class="ni">2-&gt;4-&gt;3-&gt;1</em></strong>，<strong class="lb iu"> <em class="ni"> 3 </em> </strong>的顺序，前面是<strong class="lb iu"><em class="ni"/></strong>和<strong class="lb iu"> <em class="ni"> 4 </em> </strong>，因此它们有助于其注意力计算。</li><li id="9acf" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">同理，对于<strong class="lb iu">1-&gt;4-&gt;2-&gt;3T29】和<strong class="lb iu">T31】4-&gt;3-&gt;1-&gt;2-</strong>，对应的<strong class="lb iu">T35】x _(I&lt;t)T37】贡献给<strong class="lb iu">T39】x _ t .T41 的注意力计算</strong></strong></strong></li></ul><p id="79a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更正式地说:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/2af392c6ddfdc8b3234564549fc804f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RrNXd5i6XWXQBfy42GBaKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标函数来自<a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet 论文</a></p></figure><blockquote class="nf ng nh"><p id="69aa" class="kz la ni lb b lc ld ju le lf lg jx lh nj lj lk ll nk ln lo lp nl lr ls lt lu im bi translated"><strong class="lb iu">注意</strong>在训练时，实际获得序列的排列是不正确的，因为在下游任务的微调或推理过程中，序列不能被排列。因此，<strong class="lb iu">变压器中的注意屏蔽被适当地操纵以获得正确的排列</strong>；这也是有意义的，因为所提出的架构讨论的是因子分解顺序上的置换，而不是序列顺序。</p></blockquote><h2 id="6e95" class="mo lw it bd lx mp mq dn mb mr ms dp mf li mt mu mh lm mv mw mj lq mx my ml mz bi translated">目标感知表征的双流自我注意</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/989e48da654f7d7619a287bf54539a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Ib5e9mVwlSH4BfmNeGrZfw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet 论文</a>关于目标感知表征的双流自我关注</p></figure><p id="89f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">常规转换器参数化可能不适用于置换语言模型。为了理解这一点，让我们考虑使用<strong class="lb iu"> <em class="ni"> softmax </em> </strong>的分布的标准公式，其由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c94621c0be67ec5bd82479a1e5f5556e.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*HPKLuUPdFKYzrluPDWEADw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有标准变压器参数化的置换 LM</p></figure><p id="49ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的<strong class="lb iu"><em class="ni">h _θ(x _(z _(&lt;t))</em></strong>，是变压器的隐藏状态为<strong class="lb iu"><em class="ni">x _(&lt;t)。</em> </strong>这个术语，绝不依赖于它所预测的位置，即<strong class="lb iu"><em class="ni">【z _(&lt;</em></strong>。这意味着，无论预测的位置是什么，这种分布都是相同的；从而导致无法了解有用的趋势。</p><p id="29a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，为了克服这一点，XLNet 论文提出了一个<strong class="lb iu">重新参数化，用于下一个令牌分发，使其成为目标感知的:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/2e032e1ea1473813f6a35bf24c5e3f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*onuSVBMc0HKadgDF0eA1lg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有重新参数化表示的置换 LM</p></figure><p id="0e63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用修改的表示<strong class="lb iu"> <em class="ni"> g_θ </em> </strong>，其另外将目标位置<strong class="lb iu"> <em class="ni"> z_t </em> </strong>作为输入。因此，使用了两个隐藏状态，而不是一个:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/4aa29315c2763c6046feed87761fe1d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*guAYcVzqhZYCPROdXDyw9g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">内容流关注</p></figure><ul class=""><li id="02e4" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated"><strong class="lb iu">内容</strong>表示，本质上与标准变压器隐藏状态相同。这种表示对<strong class="lb iu">和</strong>都进行了编码；上下文<strong class="lb iu"><em class="ni">x _(z _(&lt;t))</em></strong>以及原令牌<strong class="lb iu"> <em class="ni"> x_(z_t)。</em>T19】</strong></li></ul><p id="a736" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数学上:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/665485780a3746d80d19349b41076b6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*okJzGqU0WlZvUf_i8sr4Lg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">内容表示</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/5cc1272c10dce2a452dc21af8c215458.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*qQZ5hrgb_RWmNf9LIKhUfQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">查询流注意</p></figure><ul class=""><li id="ae10" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated"><strong class="lb iu">查询</strong>表示，即只能访问上下文信息<strong class="lb iu"><em class="ni">【z _(&lt;t))</em></strong>和目标位置<strong class="lb iu"><em class="ni"/></strong>【z _ t】。</li></ul><p id="e51b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数学上:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/47fee006b3c1b8ae7127a13eec85179f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*UMIeLjubJzGD7jWOZrFzbQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">查询表示</p></figure><blockquote class="nf ng nh"><p id="60b2" class="kz la ni lb b lc ld ju le lf lg jx lh nj lj lk ll nk ln lo lp nl lr ls lt lu im bi translated"><strong class="lb iu">注意</strong>最初<strong class="lb iu">内容流(<em class="it"> h_i </em>)本质上是对应的嵌入向量</strong> ( <strong class="lb iu"> <em class="it"> e_x_i </em> </strong>)，而<strong class="lb iu">查询流(<em class="it"> g_i </em>)最初是可训练向量(<em class="it"> w </em> ) </strong>。使用上面的表达式在每一层上更新它们。</p></blockquote><h2 id="7bff" class="mo lw it bd lx mp mq dn mb mr ms dp mf li mt mu mh lm mv mw mj lq mx my ml mz bi translated">部分预测</h2><p id="cbe1" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">抛开置换 LM 的所有优点不谈，我们必须承认它很贵。由于置换，这是一个具有挑战性的优化问题。</p><p id="14b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，为了解决这个问题，在给定的序列 z 中，只有一个<strong class="lb iu">子序列<em class="ni"> z_( &gt; c) </em>被选择用于预测</strong>，其中<strong class="lb iu"> <em class="ni"> c </em> </strong>被称为切割点。我们只考虑<strong class="lb iu"> <em class="ni"> z_( &gt; c) </em> </strong>，因为它在该序列中具有最长的上下文。</p><p id="5bd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，使用另一个超参数<strong class="lb iu"> <em class="ni"> K </em> </strong>，使得<strong class="lb iu"><em class="ni">K ~ | z |/(| z | c)</em></strong>。并且我们<strong class="lb iu">只选择<em class="ni"> 1/K </em>个令牌用于预测</strong>。对于未选择的令牌，不计算它们的查询表示，这样可以节省速度和内存。</p><p id="0c4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将这个部分预测与 BERT 的部分预测进行比较。BERT 使用部分预测，因为屏蔽所有记号没有任何意义。XLNet 做部分预测是因为优化难度大。比如:来个序列:<strong class="lb iu">【深，学，是，伟大】</strong>。假设 BERT 和 XLNet 都选择<strong class="lb iu">预测令牌【深度，学习】</strong>。又假设<strong class="lb iu"> XLNet 将样本因式分解为【是，伟大，深度，学习】</strong>。在这种情况下，</p><p id="8ad7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">伯特最大化:</p><ul class=""><li id="8df7" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">L(BERT) = log p(深度|很棒)+ log p(学习|很棒)</li></ul><p id="79c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XLNet 最大化:</p><ul class=""><li id="b9f8" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">L(XLNet) = log p(深度|很棒)+ log p( <strong class="lb iu">深度</strong> |学习很棒)</li></ul><p id="5114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这清楚地解释了<strong class="lb iu"> XLNet 如何捕捉更多的依赖性，即深度和学习之间的依赖性</strong>。毫无疑问，伯特学会了大部分的依赖性；但是 XLNet 了解更多。另外，这是上一节中提到的 BERT 中的<strong class="lb iu">独立性假设</strong>的一个例子。</p><h2 id="fddc" class="mo lw it bd lx mp mq dn mb mr ms dp mf li mt mu mh lm mv mw mj lq mx my ml mz bi translated">从《变形金刚 XL》中汲取灵感</h2><p id="953f" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">最后，提到<a class="ae ky" href="https://arxiv.org/abs/1901.02860" rel="noopener ugc nofollow" target="_blank"> Transformer XL 模型</a>，XLNet 从这里借用了<strong class="lb iu">关系编码</strong>和<strong class="lb iu">段递归机制</strong>的概念，这使得 Transformer XL 能够在很长的序列上操作。</p><blockquote class="nf ng nh"><p id="d8a7" class="kz la ni lb b lc ld ju le lf lg jx lh nj lj lk ll nk ln lo lp nl lr ls lt lu im bi translated"><strong class="lb iu">有趣的事实:Transformer XL 可以参与比 RNNs 长 80%和比 vanilla Transformer 长 450%的序列，并且在评估期间比 vanilla Transformers 快 1800 多倍。</strong></p></blockquote><h1 id="a0c8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="ada6" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">我们已经介绍了另一个最新的模型 XLNet，并讨论了它背后的概念。</p><p id="1b40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XLNet 的代码是作者开源的，你可以在这里找到它<a class="ae ky" href="https://github.com/zihangdai/xlnet" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="0bd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以通过<a class="ae ky" href="https://huggingface.co/transformers/model_doc/xlnet.html" rel="noopener ugc nofollow" target="_blank">拥抱面部变形金刚</a>找到预先训练好的权重和一个易于使用的模型架构 API。</p><p id="7c76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">新:</strong>我已经为 XLNet 写了一篇论文总结和评论。有兴趣的可以看看:<a class="ae ky" href="https://docs.google.com/document/d/1nePIW67OqW1HPrIkoXUB-N8hK2A07-3pnmtRVMQMKTA/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://docs . Google . com/document/d/1 nepiw 67 oqw 1 hprikosub-n8hk 2 a 07-3 pnmtrvmqmkta/edit？usp =分享</a></p><h1 id="9a09" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><div class="oj ok gp gr ol om"><a href="https://arxiv.org/abs/1906.08237" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd iu gy z fp or fr fs os fu fw is bi translated">XLNet:用于语言理解的广义自回归预训练</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">有了双向上下文建模的能力，基于去噪自动编码的预训练如 BERT 实现了更好的性能</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="oj ok gp gr ol om"><a href="https://arxiv.org/abs/1901.02860" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd iu gy z fp or fr fs os fu fw is bi translated">Transformer-XL:超越固定长度上下文的注意力语言模型</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">变形金刚有学习长期依赖性的潜力，但是受限于…</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="oj ok gp gr ol om"><a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener follow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd iu gy z fp or fr fs os fu fw is bi translated">伯特:语言理解变形金刚的前期训练</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">了解基于变压器的自监督架构</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">medium.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa ks om"/></div></div></a></div><div class="oj ok gp gr ol om"><a rel="noopener follow" target="_blank" href="/transformers-explained-65454c0f3fa7"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd iu gy z fp or fr fs os fu fw is bi translated">变形金刚解释</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">对谷歌 Transformer 模型的详尽解释；从理论到实施</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="pb l ox oy oz ov pa ks om"/></div></div></a></div></div></div>    
</body>
</html>