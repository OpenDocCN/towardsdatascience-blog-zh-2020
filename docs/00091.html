<html>
<head>
<title>Building a simple Auto Encoder via Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过决策树构建一个简单的自动编码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-simple-auto-encoder-via-decision-trees-28ba9342a349?source=collection_archive---------10-----------------------#2020-01-03">https://towardsdatascience.com/building-a-simple-auto-encoder-via-decision-trees-28ba9342a349?source=collection_archive---------10-----------------------#2020-01-03</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="f69d" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/autoencoder" rel="noopener">自动编码器</a></h2><div class=""/><div class=""><h2 id="d457" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">如何使用随机决策树构建自动编码器</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/74fbdab1a07b648eddf75fcf109698a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5_lewXP4K8irbSFCOCdAIw.jpeg"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">我可爱的妻子<a class="ae li" href="https://dribbble.com/tinati" rel="noopener ugc nofollow" target="_blank">蒂娜蒂·库伯勒</a></p></figure><p id="9b5e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">决策树是非常通用的结构。您可以使用它们进行分类和回归(<em class="mf"> CART、随机森林、… </em>)，用于异常检测(<em class="mf">隔离森林、… </em>)，正如我们将看到的，还可以用于构建<em class="mf">自动编码器、</em>以及其他构造。</p><blockquote class="mg mh mi"><p id="38fb" class="lj lk mf ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated">声明:我没有想出这个主意。这种新的基于树的自动编码器是由吉峰和周志华[1]提出的，被称为“<em class="iu">forest</em>”。他们在AAAI 18上展示了他们的作品。</p></blockquote><p id="1a2f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然而，我认为这篇论文在如何从潜在空间中解压元素方面缺乏一些清晰度，我将在本文中解决这个问题。我还将简单介绍用于制作森林的所有材料。</p><p id="4e94" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">所以，在我们开始之前，让我们快速回顾一下什么是决策树和自动编码器。</p><h1 id="041a" class="mm mn iu bd mo mp mq mr ms mt mu mv mw kj mx kk my km mz kn na kp nb kq nc nd bi translated">重要概念的高度概括</h1><h2 id="01a6" class="ne mn iu bd mo nf ng dn ms nh ni dp mw ls nj nk my lw nl nm na ma nn no nc ja bi translated">决策树</h2><p id="11b7" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">决策树是一种将来自一个<em class="mf"> n </em>维特征空间的一组样本聚类到不同箱中的方法。这是通过检查样本的<em class="mf"> n </em>特征上的某些约束来完成的。</p><p id="ee9b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">以特征空间<em class="mf"> X = ℝ×ℝ×{0，1}×{A，b，C}，</em>以及生活在<em class="mf"> X </em>内的样本<em class="mf"> (1.7，4.3，0，a)，(2.2，3.6，1，b)，(3.5，2.6，0，C) </em>和<em class="mf"> (4.1，1.9，1，A) </em>为例。我构建了一个示例决策树来将这些样本分类到四个箱中。也许下面这张图甚至是不言自明的，即使你以前从未听说过决策树:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj nu"><img src="../Images/05ebf564b7c61a8272e5f7b5adddfd59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMngNJEHsjpcS8SaYT-8eA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">决策树。在每个节点(蓝色菱形)中，样本集被分成两个不相交的子集，这取决于某个特征是否满足特殊要求。例如，在bin 4中，我们可以找到第一个坐标严格大于4，最后一个坐标为A或C (=非B)的所有样本。</p></figure><p id="9cd3" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">最后，四个样本被分类到四个容器中。由于单个样本不能同时采用两个路径(一个特征不能小于或等于4 <strong class="ll je">且</strong>严格大于4)，每个样本被准确分类到一个箱中。</p><h2 id="ee72" class="ne mn iu bd mo nf ng dn ms nh ni dp mw ls nj nk my lw nl nm na ma nn no nc ja bi translated">自动编码器</h2><p id="eb57" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">自动编码器基本上是有损压缩算法。它们允许用更少的位来表示数据，从而节省了存储空间。它还使用这些压缩样本来加速(机器学习)算法，因为较短的输入大小通常会导致较短的运行时间。</p><p id="f1c2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">自动编码器由两种算法组成:</p><ul class=""><li id="02e7" class="nv nw iu ll b lm ln lp lq ls nx lw ny ma nz me oa ob oc od bi translated"><strong class="ll je">编码器:</strong>通过去除冗余和噪声来压缩数据。</li><li id="1f7f" class="nv nw iu ll b lm oe lp of ls og lw oh ma oi me oa ob oc od bi translated"><strong class="ll je">解码器:</strong>尝试从压缩形式恢复原始数据。</li></ul><p id="9cba" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">更准确地说，编码器将样本从<em class="mf"> n </em>维特征空间投影到其在<em class="mf"> m </em>维所谓的<em class="mf">潜在空间</em>、<strong class="ll je">中的压缩形式，其中<em class="mf"> m &lt; n. </em> </strong></p><p id="bf3d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">解码器从第<em class="mf"> m </em>维潜在空间中提取样本，并将其再次解压缩到第<em class="mf"> n </em>维特征空间中。</p></div><div class="ab cl oj ok hy ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="in io ip iq ir"><p id="18d0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">一个<strong class="ll je">好的自动编码器</strong>应该能够</p><ol class=""><li id="79c5" class="nv nw iu ll b lm ln lp lq ls nx lw ny ma nz me oq ob oc od bi translated">取样<em class="mf"> x </em></li><li id="05d5" class="nv nw iu ll b lm oe lp of ls og lw oh ma oi me oq ob oc od bi translated">将其压缩为<em class="mf"> x' = e(x) </em></li><li id="eaa2" class="nv nw iu ll b lm oe lp of ls og lw oh ma oi me oq ob oc od bi translated">并将其再次解压缩到性质为<em class="mf">x≈x″= d(e(x))的<em class="mf">x″= d(x′)</em>。</em></li></ol><p id="e24f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这意味着包装和打开样品<em class="mf"> x </em>不会对其造成太大的改变。当然，这应该适用于特征空间的任何<em class="mf"> x </em>。</p><p id="d2d3" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">通常，我们不能期望得到<em class="mf"> x. </em>的完美重建，例如<em class="mf"> n=1 </em>和<em class="mf"> m=0 </em>(即潜在空间仅由单个点<em class="mf">x’</em>组成)。两个不同的样品<em class="mf"> x₁ </em>和<em class="mf"> x₂ </em>将被任何编码器送到潜在空间的同一点<em class="mf">x’</em>。所以，在最好的情况下，任何解码器只能恢复<em class="mf">x’</em>到<em class="mf"> x₁ </em>或<em class="mf"> x₂ </em>，因此得到一个解码错误。如果<em class="mf">x’</em>被解压缩到某个其他的<em class="mf"> x </em>，解码器甚至会将两个样本都弄错。</p><p id="240e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">自动编码器的一个受欢迎的代表是<em class="mf">主成分分析</em> ( <em class="mf"> PCA </em>)，它将数据从<em class="mf"> n </em>线性映射到<em class="mf"> m </em>维度，然后反向映射，尽管人们通常只使用编码器来降低数据的维度。</p><h1 id="2531" class="mm mn iu bd mo mp mq mr ms mt mu mv mw kj mx kk my km mz kn na kp nb kq nc nd bi translated">一个玩具例子:eTree</h1><p id="6342" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">现在让我们看看如何用决策树构建一个自动编码器。我声称<strong class="ll je">我们已经在本文中看到了一个eForest映射</strong>到一个1维的潜在空间！姑且称之为<em class="mf"> eTree </em>。</p><h2 id="e4e1" class="ne mn iu bd mo nf ng dn ms nh ni dp mw ls nj nk my lw nl nm na ma nn no nc ja bi translated">编码</h2><p id="c43d" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">滚动回决策树示例图片，或者让我来帮你做:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj nu"><img src="../Images/05ebf564b7c61a8272e5f7b5adddfd59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMngNJEHsjpcS8SaYT-8eA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">上图的快捷方式。真的，又是同一个决策树。</p></figure><p id="0470" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们看到四个样本中的每一个都被分类到四个箱中的一个。</p><blockquote class="or"><p id="6be6" class="os ot iu bd ou ov ow ox oy oz pa me dk translated">核心思想是使用样本映射到的bin的编号作为编码。</p></blockquote><p id="cb40" class="pw-post-body-paragraph lj lk iu ll b lm pb ke lo lp pc kh lr ls pd lu lv lw pe ly lz ma pf mc md me in bi translated">所以<em class="mf"> (1.7，4.3，0，A)</em><em class="mf">(2.2，3.6，1，B) </em>被编码为1，<em class="mf"> (3.5，2.6，0，C) </em>被编码为2，<em class="mf"> (4.1，1.9，1，A) </em>被编码为4，或者简而言之:<br/> <em class="mf"> e((1</em></p><p id="0a0c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">很简单，对吧？</p><p id="215a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">当我们想再次解码这个数字时，问题就出现了。</p><h2 id="2584" class="ne mn iu bd mo nf ng dn ms nh ni dp mw ls nj nk my lw nl nm na ma nn no nc ja bi translated">解码</h2><p id="3fc2" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">在[1]中，作者引入了<em class="mf">规则</em>和所谓的<em class="mf"> MCRs </em>来进行解码<em class="mf">。</em>概念不难，但一开始我发现从纸上很难理解。所以，我来换个角度解释一下。</p><p id="686d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">让我们假设我们得到编码1，即从特征空间有一个<em class="mf"> x </em>，其中e是eTree编码器。沿着通向bin 1的唯一路径，我们获得关于输入<em class="mf"> x. </em>的我称之为<em class="mf">的线索</em></p><p id="a54f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在1号仓着陆需要</p><ol class=""><li id="321d" class="nv nw iu ll b lm ln lp lq ls nx lw ny ma nz me oq ob oc od bi translated"><strong class="ll je">线索1: </strong> <em class="mf"> X₁ </em>小于等于4且</li><li id="581b" class="nv nw iu ll b lm oe lp of ls og lw oh ma oi me oq ob oc od bi translated"><strong class="ll je">线索二:</strong> <em class="mf"> X₂ </em>要大于等于3，</li></ol><p id="e30f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">也就是说，我们知道<em class="mf"> x </em>看起来像这样:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pg"><img src="../Images/94c03665d6597170077d09b4cf216860.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*PrFH04LSRzpTJ_fDKdlHRQ.png"/></div></figure><p id="1226" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">或者，更准确地说，我们可以推断</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ph"><img src="../Images/601d3e1d3a970e918f617d838420015a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J3fYFkk0pIUq25zF-awhAA.png"/></div></div></figure><p id="2013" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然后，我们必须从这个潜在候选集中挑选<em class="mf">任何</em>元素，并将其用作解码。我们可以用确定性或概率性的方法来做这件事，这并不重要。</p><p id="822b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[1]中也使用的一种方法是<em class="mf">最小选择</em>。只取每个集合的最小值，如果集合是没有最小值的区间，就取它的右界。使用这个确定性规则，我们将解码1到<em class="mf"> (4，3，0，A) </em>(假设<em class="mf"> A &lt; B &lt; C </em>)。其他简单的方法包括选择最大值或平均值，或者从集合中随机抽样。</p></div><div class="ab cl oj ok hy ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="in io ip iq ir"><p id="fe0a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在我们的例子中，解码1的潜在候选集合相当大，因此我们可以预期重构相当糟糕。解决这个问题的一个方法是让树变得更深，也就是使用更多的分裂。更多的分裂产生更多的线索，进而产生更小的候选集。让我们在玩具示例中再使用一个拆分:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pi"><img src="../Images/96549d19ed1a969072644e508e7473e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NqJN1EXe4kj7y0cLr9kJQA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">多一层的玩具树。编码现在可以采用从1到5的值。</p></figure><p id="7b39" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这里，编码1将被解码成</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pj"><img src="../Images/093e1a5d73e8d27ac58f22cd40e93e2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*SyiGXIe87i6wh-_TYRGo4Q.png"/></div></figure><p id="44b8" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这为现在搞乱第一特征留下了更少的空间，给出了更低的重建误差。改善解码的其他方法是嵌入关于特征空间的知识，例如，有时特征总是大于零或小于一些界限，如1或255(图片数据)，因此我们可以移除无限或负无限的界限。</p><p id="9460" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">但是让决策树越来越深不是可持续的解决方案，因为我们会遇到过度拟合的问题，就像在回归和分类任务中一样。然而，到目前为止，我们仍然只使用一维！那么，我们能做什么呢？</p><h1 id="2e7a" class="mm mn iu bd mo mp mq mr ms mt mu mv mw kj mx kk my km mz kn na kp nb kq nc nd bi translated">埃弗斯特</h1><p id="b493" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">让我们并行连接更多的决策树进行编码！那么每一棵树给我们一个潜在空间的维度，每棵树的箱数就是特征值。考虑以下3棵树的例子:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pk"><img src="../Images/4f3e1bd2a63e95f4d4837d587dc303e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yvdKw0HT7WN6jZ3CZ1MVEA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">一个由三棵决策树组成的Forest。加粗的箭头表示x选择的路径。</p></figure><p id="9b1e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">三个决策树中的每一个都给了我们一个潜在空间的坐标，在这个例子中是3。在我们的例子中，潜在的每个特征可以是1、2、3或4。但是一般来说，如果每个决策树有不同数量的箱也没问题。</p><p id="fa1c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">所以，让我们想象我们的输入<em class="mf"> x </em>在潜在空间中被编码为<em class="mf"> (2，1，1) </em>，我们现在希望解码它。我们只需要再次收集所有的线索，并把它们放在一起，以创建一个潜在的候选人集。</p><p id="cf15" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">线索是:</p><ul class=""><li id="9845" class="nv nw iu ll b lm ln lp lq ls nx lw ny ma nz me oa ob oc od bi translated">从树1，bin 2: <br/> - <em class="mf"> X₁ </em>小于或等于4 <em class="mf"> <br/> - X </em> ₂小于3</li><li id="68d4" class="nv nw iu ll b lm oe lp of ls og lw oh ma oi me oa ob oc od bi translated">从树2，bin 1: <br/> - <em class="mf"> X₁ </em>大于或等于2<em class="mf">-<br/>x</em>₃等于0</li><li id="6ca2" class="nv nw iu ll b lm oe lp of ls og lw oh ma oi me oa ob oc od bi translated">从树3，bin 1: <br/> - <em class="mf"> X₄ </em>等于C <br/> - <em class="mf"> X </em> ₂大于1</li></ul><p id="1d6f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">把所有东西放在一起给了我们</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pl"><img src="../Images/c1aea6685f05e78b7ee2b9bc5038aeb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*8PkgEeozGrwi_k-kzrZx0w.png"/></div></figure><p id="ef58" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">所以最小解码会给我们<em class="mf"> d(x)=(2，1，0，C)。</em></p></div><div class="ab cl oj ok hy ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="in io ip iq ir"><p id="255d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这几乎是理解森林的全部内容！现在仍然缺少的是如何实际训练eForest ，但这是一个简短的问题:</p><blockquote class="or"><p id="15a6" class="os ot iu bd ou ov ow ox oy oz pa me dk translated">使用随机决策树，即使用随机特征和随机分界点(来自合理范围)进行分裂的树。</p></blockquote><p id="5e5c" class="pw-post-body-paragraph lj lk iu ll b lm pb ke lo lp pc kh lr ls pd lu lv lw pe ly lz ma pf mc md me in bi translated">我也是这样编造例子的。没有复杂的算法。</p><p id="aa6b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，让我们转到一些实验，看看它们在性能方面是否有任何优势。</p><h1 id="7a31" class="mm mn iu bd mo mp mq mr ms mt mu mv mw kj mx kk my km mz kn na kp nb kq nc nd bi translated">实验</h1><p id="fc7a" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">[1]的作者进行了几项实验，其中包括CNN自动编码器大放异彩的图像编码。我将直接粘贴[1]中的结果、表格和图像。</p><blockquote class="or"><p id="ca56" class="os ot iu bd ou ov ow ox oy oz pa me dk translated">你可以在<a class="ae li" href="https://github.com/kingfengji/eForest" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到用于实验的代码。</p></blockquote><p id="e210" class="pw-post-body-paragraph lj lk iu ll b lm pb ke lo lp pc kh lr ls pd lu lv lw pe ly lz ma pf mc md me in bi translated">数据集是具有<em class="mf"> 28x28x1=784 </em>特征(1个颜色通道)的经典MNIST和具有<em class="mf"> 32x32x3=3072 </em>特征(3个颜色通道)的CIFAR-10。</p><h2 id="26de" class="ne mn iu bd mo nf ng dn ms nh ni dp mw ls nj nk my lw nl nm na ma nn no nc ja bi translated"><strong class="ak">关于实验中使用的自动编码器</strong></h2><p id="abca" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">MLP是使用几个线性层的“正常”自动编码器。确切的规格可以从[1]中得到，但是MLP₁使用500维的潜在空间和1000维的MLP₂。<br/> CNN-AE是一个<em class="mf">卷积自动编码器</em>，即它在内部使用卷积。它遵循<a class="ae li" href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener ugc nofollow" target="_blank">本规范</a>。SWW AE是一个更好的CNN-AE。eForestˢ₅₀₀也是一家使用样品标签的林业公司，我们在这里没有涉及。它将输入数据压缩到500个特征(下标1000将其压缩到1000个维度)。最后，eForestᵘ₅₀₀是本文所讨论的森林。下标数字再次表示潜在空间的维度。</p><h2 id="c5b1" class="ne mn iu bd mo nf ng dn ms nh ni dp mw ls nj nk my lw nl nm na ma nn no nc ja bi translated">重建误差</h2><p id="4b9b" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">结果看起来很有希望！MNISt和CIFAR-10数据集的重建误差非常低。来自[1]:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ph"><img src="../Images/2c1a2bf0bd3df63a6dd0d27a5057bd7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zrhfl1xoWdtO6kGRLe99Gg.png"/></div></div></figure><div class="kt ku kv kw gu ab cb"><figure class="pm kx pn po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/67103830968c5acb951133697f49484e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*riQpzE8Vr1ZXVpcst1GA7g.png"/></div></figure><figure class="pm kx ps po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/ce55795f71fd960cba3e2dd9778cfdbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*2dOzUQ_hoDPKxFlqidyvaQ.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk pt di pu pv translated">原始测试样本(第一行)和重构样本。</p></figure></div><p id="98bb" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">但是，人们必须进行进一步的实验，以检查eForests是否过度拟合。至少作者复用了的<em class="mf">模型，并最终得到了好的结果，这是一个很好的指标，表明eForest推广得很好，而不仅仅是用心学习训练样本。模型重用意味着在一个训练集上训练一个模型，并在<strong class="ll je">不同的训练集</strong>上评估它。</em></p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pw"><img src="../Images/1617c329060864faf49b4af2a38fb7d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fNFCHpAenYfY5oNPCh8vYg.png"/></div></div></figure><div class="kt ku kv kw gu ab cb"><figure class="pm kx px po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/f55fedefec3168858b2b9ccad16dfd97.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*Kk6YLdzmEa8ibJ79TfzAcA.png"/></div></figure><figure class="pm kx py po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/5d7ffaa7a4513eac113aff1136c1470f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*93w42b73WmPlh9DnGCV4yg.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk pz di qa pv translated">原始测试样本(第一行)和重构样本。所有模型都在一些训练集上进行了训练，并在不同的训练集上进行了测试！使用eForests时，结果看起来不错。</p></figure></div><blockquote class="mg mh mi"><p id="a126" class="lj lk mf ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated">我只是不明白，当原始数据只有28*28=784维时，作者为什么要为MNIST数据集使用1000维的潜在空间。在我看来，使用500维会更有意义。但是，即使我们使用这种人为的设置:eForest比其他方法执行得更好，neark实现了无损压缩，这也是我所期望的。</p></blockquote><h2 id="4609" class="ne mn iu bd mo nf ng dn ms nh ni dp mw ls nj nk my lw nl nm na ma nn no nc ja bi translated">计算效率</h2><p id="33ea" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">与其他方法相比，训练时间非常短。然而，根据作者的说法，编码和解码需要更多的时间，这仍然有优化的空间。来自[1]:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qb"><img src="../Images/0261762bb7973e338797ac2bc9af7f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8KElG2oUEo22n8p8VtEo5Q.png"/></div></div></figure><h1 id="49b6" class="mm mn iu bd mo mp mq mr ms mt mu mv mw kj mx kk my km mz kn na kp nb kq nc nd bi translated">我的概念验证实施</h1><p id="1c30" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">由于算法相当简单，我尝试自己实现。我甚至不用从头开始，因为编码器已经自带了<a class="ae li" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a> <em class="mf">类型的</em>！</p><p id="cd30" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">类<a class="ae li" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html" rel="noopener ugc nofollow" target="_blank">randomtreesemleding</a>也创建了许多随机决策树，并根据输入的位置对它们进行编码<em class="mf"> x </em>。然而，它们使用二进制编码，这增加了潜在的空间维度。使用我的具有三棵树和每棵树4个箱的例子，编码将是<em class="mf"> (0，1，0，0 | 1，0，0，0 | 1，0，0) </em>而不是<em class="mf"> (2，1，1)</em>，即潜在空间将具有12的<strong class="ll je">维度，而不是只有3个</strong>。但是一旦知道了每棵树的叶子数量，将二进制表示转换成每棵树的一个数字就很容易了。幸运的是，scikit-learn为我们提供了一个方法。</p><p id="6a38" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">因此，实现编码部分很简单！也可以从我代码中的短<code class="fe qc qd qe qf b">encode</code>方法看出来。这仅仅是</p><pre class="kt ku kv kw gu qg qf qh bn qi qj bi"><span id="b7af" class="qk mn iu qf b be ql qm l qn qo">output = self.random_trees_embedding.transform(X)<br/>indices = []<br/>start = 0<br/>for estimator in self.random_trees_embedding.estimators_:<br/>    n_leaves = estimator.get_n_leaves()<br/>    indices.append(output[:, start:start + n_leaves].argmax(axis=1))<br/>    start = start + n_leaves<br/>return np.array(indices).transpose()</span></pre></div><div class="ab cl oj ok hy ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="in io ip iq ir"><p id="559e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">解码花了我更多的时间，这也是我效率低下的方式，请原谅我！:D，我只是需要一个快速的概念验证。</p><p id="a86a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">你可以在这里找到我的代码<a class="ae li" href="https://github.com/Garve/TreeAutoEncoder/blob/master/rted.py" rel="noopener ugc nofollow" target="_blank"/>。请随意改进！</p><p id="1ffa" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我的图形结果:</p><div class="kt ku kv kw gu ab cb"><figure class="pm kx qp po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/3cb160cb5f66427228e95a339ed7d2a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*zzpPr1q9EoBSgdFu97G46g.png"/></div></figure><figure class="pm kx qp po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/d4c9ee282e43f286830c638d9b37667d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*wu2vRYE0sJ9aJCfdZIr_5g.png"/></div></figure></div><div class="ab cb"><figure class="pm kx qq po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/ade2f0469713d82132fdefa40860a070.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*aw3Wqct8TQz9f2Vuy2zmMA.png"/></div></figure><figure class="pm kx qq po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/82c458f643b06ceda081747cfc649e50.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*6-dKEcCRxX3qNHLyfMbczg.png"/></div></figure><figure class="pm kx qq po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/2b2b455ab00ca1bfe6c350e5fe764307.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*a1raRx2U-XXRLdMihuplbw.png"/></div></figure></div><div class="ab cb"><figure class="pm kx qp po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/f9a5c9c832f4c4cebc123fde95dbb62c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*9P6U1fU0aF4LfFELiJIFvw.png"/></div></figure><figure class="pm kx qp po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/63e784c480c968eb797ae96c5d86e14f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*qVo7uk6VI85HjJAVsxL_3Q.png"/></div></figure></div><div class="ab cb"><figure class="pm kx qq po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/f57ef29e6a954f5d09c72416c7f06f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ZD42h76PFabNQudLPLfEYg.png"/></div></figure><figure class="pm kx qq po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/072fb7b90d51854de44538de850b2759.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*bkvG9F7He3aDyUaV-pO5ww.png"/></div></figure><figure class="pm kx qq po pp pq pr paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/ad7fff78206c29ef9631ec4da4091751.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ulabSu9Hpy7iScaXxbScNA.png"/></div></figure></div><p id="67a5" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我认为重建看起来很好，所以可能我的解码工作。但是，请在使用这些代码进行任何有成效的工作之前，先检查一下这些代码。</p><p id="7d3a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果你想要真正的交易，去官方克隆的GitHub上工作。但是要准备好创建一个虚拟环境并手动修补一些scikit-learn文件。</p><h1 id="fdf3" class="mm mn iu bd mo mp mq mr ms mt mu mv mw kj mx kk my km mz kn na kp nb kq nc nd bi translated">结论</h1><p id="96ec" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">我们已经看到了如何用决策树构建一个自动编码器。这是一个自动编码器，具有独特的，但易于理解的设计，可以训练得非常快。此外，重构精度可以与成熟的卷积自动编码器相媲美。</p><p id="44c0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">另一方面，编码和解码速度仍然很低，并且必须在野外的真实数据集上测试eForest。也许你可以写一篇关于它的文章。:)</p><h1 id="d1ce" class="mm mn iu bd mo mp mq mr ms mt mu mv mw kj mx kk my km mz kn na kp nb kq nc nd bi translated">参考</h1><p id="c294" class="pw-post-body-paragraph lj lk iu ll b lm np ke lo lp nq kh lr ls nr lu lv lw ns ly lz ma nt mc md me in bi translated">[1]冯军，周志军，<a class="ae li" href="https://arxiv.org/pdf/1709.09018.pdf" rel="noopener ugc nofollow" target="_blank">森林自动编码器</a> (2017)，第32届人工智能大会</p></div><div class="ab cl oj ok hy ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="in io ip iq ir"><p id="e665" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我希望你今天学到了新的、有趣的、有用的东西。感谢阅读！</p><p id="502e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">作为最后一点，如果你</strong></p><ol class=""><li id="2c80" class="nv nw iu ll b lm ln lp lq ls nx lw ny ma nz me oq ob oc od bi translated"><strong class="ll je">想支持我多写点机器学习和</strong></li><li id="167d" class="nv nw iu ll b lm oe lp of ls og lw oh ma oi me oq ob oc od bi translated"><strong class="ll je">无论如何，计划获得一个中等订阅，</strong></li></ol><p id="a442" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">为什么不通过此链接</strong><a class="ae li" href="https://dr-robert-kuebler.medium.com/membership" rel="noopener"><strong class="ll je"/></a><strong class="ll je">？这将对我帮助很大！😊</strong></p><p id="c87f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><em class="mf">说白了，给你的价格不变，但大约一半的订阅费直接归我。</em></p><p id="0521" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">非常感谢，如果你考虑支持我的话！</p><blockquote class="or"><p id="1f39" class="os ot iu bd ou ov ow ox oy oz pa me dk translated">如有任何问题，请在<a class="ae li" href="https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上给我写信！</p></blockquote></div></div>    
</body>
</html>