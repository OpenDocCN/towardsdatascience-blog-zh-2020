<html>
<head>
<title>My first journey into Learning Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我学习单词嵌入的第一次旅程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-first-journey-into-learning-word-embeddings-3b88d218fe6?source=collection_archive---------26-----------------------#2020-01-26">https://towardsdatascience.com/my-first-journey-into-learning-word-embeddings-3b88d218fe6?source=collection_archive---------26-----------------------#2020-01-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="c43b" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">我的第一个帖子在这里！</h1><p id="8fac" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><a class="ae lm" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu">单词嵌入</strong> </a>和NLP，更一般地说，以一种特殊的方式引起了我的注意。我很惊讶能从推文、评论、评论中提取多少价值；最奇怪的是，我们是一秒又一秒产生的相同数据的用户。</p><p id="ee70" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">过去，我们已经学会了如何通过<a class="ae lm" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu">线性回归</strong>，</a>等方法找到最符合给定数据的线的参数。我们开发了经典模型，如<a class="ae lm" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank"><strong class="kq iu">【ARIMA】</strong></a>来拟合时间序列，以及如何构建帮助我们做出决策的树。</p><p id="a1fd" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">然而，当我考虑这些模型时(也许这是我的一种偏见)，我总是倾向于将建模的数据看作是严格的和结构化的，是在某种测量或底层过程之后收集的数据。<br/>在我们逐渐不再使用口语的时候，书面语也越来越少，自然语言处理以某种方式让它们恢复了价值，让这个词再次变得有价值，这可能会带来很多积极的结果。</p><h1 id="20fc" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">当我爱上NLP时</h1><p id="3364" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我在研究机器学习时发现的第一个文本表示是<strong class="kq iu"> </strong> <a class="ae lm" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu">单词包</strong> </a> (BoW)表示，其中文档被表示为其单词的集合，并且完全不考虑词序。此外，我遇到了<a class="ae lm" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu"> n-gram </strong> </a>模型，其中考虑了n个单词的序列，以便在转换过程中保留空间信息。<br/>一把弓可以看成1克的模型。</p><p id="f86f" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">然后，您可以使用<a class="ae lm" href="https://en.wikipedia.org/wiki/One-hot" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu">一键编码</strong> </a>或借助于<a class="ae lm" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu"> TF-IDF </strong> </a> <strong class="kq iu"> </strong>表示对单词或单词序列进行编码，并且您可以使用这些生成的特征来训练机器学习分类器，例如<a class="ae lm" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu">朴素贝叶斯模型</strong> </a> <strong class="kq iu"> </strong>或<a class="ae lm" href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu">支持向量机</strong> </a>，以便执行正面与负面文本分类。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/7e555025d9493a701521cd4e9833a831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EH0fCTv-KrBRWbYcDCj2yw.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">字云！</p></figure><h1 id="5f4a" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">深度学习的力量</h1><p id="805d" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在机器学习之后，我为我的论文投入了深度学习，我发现了深度神经网络的迷人世界。<br/>深度神经网络是数学模型，在给定足够数量的数据的情况下，由于<a class="ae lm" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu">反向传播</strong> </a>算法的形式化，深度神经网络有望通过试错法发现输入和输出数据之间的关系，其中网络参数相对于给定损失函数的梯度进行优化，该损失函数根据必须解决的问题进行选择。</p><p id="822a" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">跳过剩下的深度学习之旅，我第一次见到了<a class="ae lm" href="https://en.wikipedia.org/wiki/Autoencoder" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu">自动编码器</strong> </a>，最终我对<a class="ae lm" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu">递归神经网络</strong> </a>有了更深入的了解，这还要感谢Andrew NG在Coursera上的<strong class="kq iu"> Deeplearning.ai课程</strong>。</p><p id="82fe" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">递归神经网络是具有内部循环的网络，允许信息持续存在。这使得它们非常适合于建模序列数据，例如时间序列，以及文本和语音。</p><h1 id="56fd" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">单词嵌入，我来了</h1><p id="c7fb" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我使用递归神经网络建立的第一个模型是一个字符级模型，它通过9000个意大利名字进行训练。具体来说，这是一个序列到序列模型，其中给定第<em class="mi">个i- </em>个字符作为输入，该模型被训练来预测第<em class="mi">个i+1- </em>个字符。每一个意大利名字都是用一键编码的字符序列处理的。该模型表现良好，因为生成的名字几乎都以<em class="mi"> o </em>或<em class="mi"> a </em>结尾(这是意大利名字的标准)。后来，我继续进行单词级模型，使用固定长度的句子，其中单词是一个热点编码的，但是我不满意这样的事实，即对于所有的向量对，一个热点向量的余弦相似性是0，即使是那些表示相似主题的向量。</p><p id="8fdb" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">下一步，我想看看单词嵌入实际上是如何工作的。</p><h1 id="3b4a" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">来自评论的训练嵌入</h1><p id="8b8a" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我从Kaggle下载了<a class="ae lm" href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu">亚马逊美食评论</strong> </a> <strong class="kq iu"> </strong>数据集，由~ 500k评论组成，我下载了SQLite数据库文件。</p><p id="50fa" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">首先，我导入了<a class="ae lm" href="https://docs.python.org/2/library/sqlite3.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu"> sqlite3 </strong> </a>并构建了一个查询来检索reviews表，并查看行数和包含的列数，例如Reviews以及我在此阶段没有考虑的其他数据:</p><pre class="lt lu lv lw gt mj mk ml mm aw mn bi"><span id="90b7" class="mo jr it mk b gy mp mq l mr ms"><strong class="mk iu">import sqlite3<br/>connection = sqlite3.connect('/content/gdrive/My Drive/Colab Notebooks/database.sqlite')<br/># Number of reviews<br/>c.execute("SELECT COUNT(*) FROM Reviews")<br/>c.fetchall()</strong></span></pre><p id="d27d" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">我找到了568000条评论。然后，我构建了helper函数来获取评论并对其进行处理，以便首先通过利用<a class="ae lm" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu"> NLTK </strong> </a>包将它们拆分成一系列句子，然后将每个句子拆分成一系列单词:</p><pre class="lt lu lv lw gt mj mk ml mm aw mn bi"><span id="952c" class="mo jr it mk b gy mp mq l mr ms"><strong class="mk iu">def fetch(limit):<br/>    c.execute(f"SELECT Text FROM Reviews LIMIT {limit};" )<br/>    records = c.fetchall()<br/>    print(f"The number of fetched records is {len(records)}")<br/>    return records</strong></span><span id="4c98" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">def tokenize(chunk):<br/>    from nltk.tokenize import sent_tokenize, word_tokenize<br/>    from tqdm import tqdm</strong></span><span id="e049" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">    list_of_tokens = []<br/>    print("Iterating over all the retrieved rows...")<br/>    <br/>    for row in tqdm(chunk):<br/>    sentences_from_reviews = sent_tokenize(row[0])<br/>    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences_from_reviews]<br/>    list_of_tokens.extend(tokenized_sentences)<br/>    print("Completed.")</strong></span><span id="a078" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">    return list_of_tokens</strong></span></pre><p id="aa2b" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">正如你所看到的，我没有过滤停用词或网址或其他任何东西，我想直接有一个最小的单词嵌入的例子。</p><p id="990d" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">我验证了数据的结构是一个列表的列表，其中每个内部列表只包含字符串:</p><pre class="lt lu lv lw gt mj mk ml mm aw mn bi"><span id="45d5" class="mo jr it mk b gy mp mq l mr ms"><strong class="mk iu">all(isinstance(elem, list) and isinstance(word, str) for elem in test for word in elem)</strong> <em class="mi">#returns True! </em></span></pre><p id="0882" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">我提取了数据并进行处理:</p><pre class="lt lu lv lw gt mj mk ml mm aw mn bi"><span id="d421" class="mo jr it mk b gy mp mq l mr ms"><strong class="mk iu">data = fetch(limit=-1)<br/>token_data = tokenize(data)</strong></span></pre><p id="63d6" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">现在训练数据已经准备好适合一个<a class="ae lm" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu"> Word2Vec </strong> </a>模型。我不会详细介绍Word2Vec，但是网上有很多资源。</p><pre class="lt lu lv lw gt mj mk ml mm aw mn bi"><span id="b887" class="mo jr it mk b gy mp mq l mr ms"><strong class="mk iu">import multiprocessing # for number of workers<br/>from gensim.models import Word2Vec</strong></span><span id="02bc" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">model = Word2Vec(token_data, size=300, min_count = 10, workers=multiprocessing.cpu_count(), iter=30, window=5)</strong></span></pre><p id="6259" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">过了一会儿，我准备好了我的模型，我腌制并下载了它(我用的是Google Colab笔记本，可以在这里<a class="ae lm" href="https://colab.research.google.com/drive/1Nsj2kHT4mxX1GYbVy-MrT7bG7ZXh9gfx" rel="noopener ugc nofollow" target="_blank"><strong class="kq iu"/></a>查阅):</p><pre class="lt lu lv lw gt mj mk ml mm aw mn bi"><span id="950f" class="mo jr it mk b gy mp mq l mr ms"><strong class="mk iu">import pickle<br/>from google.colab import files</strong></span><span id="f30b" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">with open('token_data.pkl','wb') as f:<br/>     pickle.dump(token_data,f)</strong></span><span id="4285" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">files.download('token_data.pkl')</strong></span></pre><p id="c0fc" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">然后，我将它导入我的本地Jupyter环境:</p><pre class="lt lu lv lw gt mj mk ml mm aw mn bi"><span id="7124" class="mo jr it mk b gy mp mq l mr ms"><strong class="mk iu">import pickle</strong></span><span id="c637" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">with open('token_data.pkl','rb') as f:<br/>    <br/>    token_data=pickle.load(f)</strong></span></pre><p id="eb83" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">并检查了与<em class="mi">【橙子】</em>和<em class="mi">【意大利面】</em>的相似性:</p><pre class="lt lu lv lw gt mj mk ml mm aw mn bi"><span id="3445" class="mo jr it mk b gy mp mq l mr ms"><strong class="mk iu">model.wv.most_similar('orange')</strong></span><span id="f4f8" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">Out[16]:</strong></span><span id="d319" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">[('lemon', 0.6406726241111755),<br/> ('grapefruit', 0.6343103647232056),<br/> ('pomegranate', 0.6024238467216492),<br/> ('citrus', 0.5851069688796997),<br/> ('blackberry', 0.5831919312477112),<br/> ('cherry', 0.5785644054412842),<br/> ('strawberry', 0.5701724290847778),<br/> ('grape', 0.5649375319480896),<br/> ('elderberry', 0.5609756708145142),<br/> ('guava', 0.5601833462715149)]</strong></span><span id="d4c3" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">model.wv.most_similar('pasta')</strong></span><span id="9320" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">Out[17]:</strong></span><span id="ad0e" class="mo jr it mk b gy mt mq l mr ms"><strong class="mk iu">[('spaghetti', 0.7852049469947815),<br/> ('couscous', 0.6740388870239258),<br/> ('pastas', 0.6413708329200745),<br/> ('noodles', 0.6318570971488953),<br/> ('linguine', 0.6257054805755615),<br/> ('vermicelli', 0.6104354858398438),<br/> ('penne', 0.6094731092453003),<br/> ('lasagna', 0.5939062833786011),<br/> ('macaroni', 0.5886696577072144),<br/> ('bread', 0.5816901922225952)]</strong></span></pre><p id="0e6c" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">所以它似乎抓住了食物的相似之处！接下来，我将使用<a class="ae lm" href="https://it.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu"> t-SNE </strong> </a>来可视化嵌入，但是我将在下一篇文章中保留它。</p></div></div>    
</body>
</html>