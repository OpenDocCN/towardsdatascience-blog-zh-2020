<html>
<head>
<title>Introduction to Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对象检测简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-object-detection-943f21e26063?source=collection_archive---------38-----------------------#2020-05-12">https://towardsdatascience.com/introduction-to-object-detection-943f21e26063?source=collection_archive---------38-----------------------#2020-05-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9b92" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从图像分类到目标检测</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bad737ba1c4910918d12575f5b90c3d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o9VKo_jD3w7T8OVEFnTZww.jpeg"/></div></div></figure><h1 id="71f1" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">入门指南</h1><p id="27a6" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">基本的图像分类模型相对简单。给定一幅图像作为输入，目标是将它归类为许多可能的输出类别之一。该架构通常(但不一定)是一系列卷积层和汇集层，其末端是一个或多个线性层，最终输出层具有与类数量相同的节点/神经元数量。输出层中的每个节点代表一个类，并且将为该最终层训练模型，以给出给定输入图像属于相应类的概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/a9a961cfc64dc80e06d7cf5601a5c5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GnxHJrtbjp-r3baYhiyzDg.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">说明性图像分类</p></figure><p id="7909" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">上图是一个多类影像分类示例，其中输出相互排斥，即所有输出概率总和为 1。另一种变化是多标签分类，其中每个概率都相互独立，一个图像可以被标记为多个类别，就像将类型与电影海报相关联一样。</p><h1 id="e382" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">目标</h1><p id="c87a" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">虽然这种图像分类能力具有许多应用，但是大多数真实世界的应用需要的不仅仅是单个图像的这种分类。如果您将自动驾驶汽车视为一个例子(注意:真正的自动驾驶解决方案可能更复杂，但出于说明目的，请使用这个例子)，它要求我们:</p><ol class=""><li id="9ab5" class="ms mt it lo b lp mn ls mo lv mu lz mv md mw mh mx my mz na bi translated">确定所识别的物体在图像中的位置。例如:如果识别的行人在正前方或旁边</li><li id="2059" class="ms mt it lo b lp nb ls nc lv nd lz ne md nf mh mx my mz na bi translated">识别多个对象。例如:一个单一的图像可以有多辆汽车，许多行人，交通灯等</li><li id="6dce" class="ms mt it lo b lp nb ls nc lv nd lz ne md nf mh mx my mz na bi translated">识别对象的方向。例如:汽车的前部面向，后部背向(例如，汽车向我们驶来或面向我们停放)</li></ol><p id="68e5" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">完成所有这些需要做的比图像分类模型多一点。在本帖中，我们将着眼于实现前两个目标。</p><h1 id="734c" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">思维练习</h1><p id="3ba1" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">那么，除了分类之外，我们如何确定由边界框坐标定义的对象的位置呢？当存在多个物体时，我们如何确定它们？</p><p id="4e55" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">我们想要确定位置的方法是通过识别对象的边界框的坐标。我们的目标是获得边界框的坐标，通常是通过识别角或中心的坐标以及边界框的高度和宽度。</p><blockquote class="ng nh ni"><p id="fdb0" class="lm ln nj lo b lp mn ju lr ls mo jx lu nk mp lx ly nl mq mb mc nm mr mf mg mh im bi translated"><strong class="lo iu">附注:</strong>另一种方法是获取已识别对象实例的掩码。对于已识别的对象，遮罩为我们提供了作为已识别实例一部分的所有像素。图 1 示出了这两种输出选项(为了便于理解，使用了多对象检测图示)。虚线表示边界框输出，而彩色对象表示我们希望在给定输入图像的情况下得到的遮罩输出。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/a37503d4f9f76619bf66d3d18af758f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kPYSKugvuiIRWLm73stzJA.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated"><strong class="bd no">图 1 </strong>:使用包围盒的对象检测与使用遮罩的实例分割</p></figure><p id="3412" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">让我们做一个简单的思考练习，从几个约束开始，这将简化思考，稍后再回来移除它们。第一，假设所有的物体都是固定的宽度和高度(比如 20px * 20px)。第二，让我们假设这些对象从 0 或者 20 的倍数开始。即它们的左上坐标将是 0 或 20 的倍数(即左上坐标将是 0，0 或 0，20 或 20，40 或 40，40 等)。</p><p id="c8fb" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">有了这两个约束，确定对象精确位置的一种方法是在图像上想象一个网格，每个单元的大小为 20*20。现在，我们要做的就是像图像分类一样，对网格中的每个单元评估分类概率。所有类别概率高于阈值的单元格都是对象所在的位置！</p><p id="d333" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">现在，当我们去除预先确定的尺寸和位置的两个约束时，很明显，我们必须有一个网格系统来帮助我们确定各种尺寸、长宽比和位置的盒子。这些箱子有时被称为锚箱。有不同的方法来解决产生不同大小/位置的锚盒的问题，我们将看看一些众所周知的方法。</p><p id="047c" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">同样重要的是要注意，在这个简单的思考练习中，我们直接在图像上想象网格系统和锚定框，但从技术上讲，我们将在表示图像的特征图上这样做，就像图像通过的基本网络的最后一层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/2011847b36a58c14623ec9f85a1cbda7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MpTgjjs4u_sufkxJeGY5mQ.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated"><strong class="bd no">图 2 </strong>:检测包围盒需要评估不同大小、长宽比和位置的盒子</p></figure><h2 id="6075" class="np kv it bd kw nq nr dn la ns nt dp le lv nu nv lg lz nw nx li md ny nz lk oa bi translated">YOLO</h2><p id="5295" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">例如，Yolo ( <a class="ae ob" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank"> paper link </a>)将输入图像分成一个 S × S 的网格。正如我们上面所做的，每个网格单元不仅被评估类别概率，而且一组“B”边界框和这些框的置信度分数也被一起预测。</p><p id="d2cf" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">换句话说，盒子不是像我们简单的思考练习中那样预先确定的，而是随着单元的类别概率一起预测的。每个边界框由 5 个预测组成:x，y，w，h 和置信度。前四个与坐标有关，最后一个，置信度反映了模型对盒子包含对象的置信度以及盒子坐标的精确度。</p><p id="1385" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">此外，确定对象的框坐标的责任属于对象中心所在的网格单元。这有助于防止多个单元格确定同一对象周围的框。但是每个单元仍然预测多个边界框。这些框中的一个被认为“负责”预测对象，基于该预测，在训练期间，哪个预测具有最高的当前值<a class="ae ob" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank"> IOU </a>。在训练过程中，这导致每个单元中不同的边界框专门用于预测物体的特定大小、长宽比或类别，从而提高整体回忆。</p><p id="b9a2" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">这些预测被编码为 S × S×(B∫5+C)张量(S×S 是网格维度，B 是网格中每个单元将确定的框，5 是每个框做出的预测，即 x、y、w、h 和置信度，C 是模型可以识别的 C 类对象的概率)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/b92a0b7eb3006c64a1745fdf50ecdb5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_bAiWOqL9Wvw6y-m7ei8tQ.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated"><strong class="bd no">图 3 </strong> : Yolo 插图，此处摘自<a class="ae ob" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">原图的第二部分。左边是生成两个边界框和 7 个类别概率的 4 x 4 网格的图示。</a></p></figure><h2 id="dce4" class="np kv it bd kw nq nr dn la ns nt dp le lv nu nv lg lz nw nx li md ny nz lk oa bi translated">（同 solid-statedisk）固态（磁）盘</h2><p id="60c9" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">单次多盒检测器(SSD) ( <a class="ae ob" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank">纸链接</a>)不会凭空预测盒子，而是从一组默认盒子开始。它使用几个不同比例的特征地图(即几个不同大小的网格，如图 4 中的<strong class="lo iu">所示的 4 x 4、8 x 8 等)以及每个网格/特征地图中每个单元不同纵横比的一组固定默认框。对于每个默认框，该模型随后计算“偏移量”以及类别概率。偏移量由 4 个数字 cx、cy、w 和 h 组成，给出了实际框相对于默认框的中心坐标、宽度和高度的偏移量。</strong></p><p id="201d" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">SSD 在将对象基础事实框与默认框相匹配的策略上也有所不同。没有一个单独的默认框负责并匹配一个对象。相反，默认框与任何高于阈值(0.5)的 IOU<a class="ae ob" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank">匹配。这意味着，对于与对象重叠的多个默认框，将会预测到高分，而不是只要求其中一个框负责。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/a8fedc91650d89075a646a1f49b38b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eWFun1llza75oBm5FC67Cw.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated"><strong class="bd no">图 4 </strong> : SSD 框架来自<a class="ae ob" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank">原文此处</a>。说明了多个(两个)匹配猫的蓝框和一个匹配狗的红框。匹配框也来自不同的特征地图，即网格大小。</p></figure><h2 id="7e11" class="np kv it bd kw nq nr dn la ns nt dp le lv nu nv lg lz nw nx li md ny nz lk oa bi translated"><strong class="ak">更快的 RCNN </strong></h2><p id="9dda" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">与 Yolo 和 SSD 不同，更快的 RCNN ( <a class="ae ob" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank"> paper link </a>)及其前身采用了两步方法。快速 RCNN 部署一个单独的区域建议网络，专门用于首先确定锚盒。接下来是一个快速的 R-CNN 检测器，它使用了提出的区域。</p><p id="daab" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">在区域建议网络(RPN)中，在基本网络的输出上应用小滑动窗口(卷积)。如果基础网络的输出是 n*m *个通道，则 n*m 就相当于我们的网格，对 n * m 个特征图中的每个位置(即细胞)进行评估。根据不同尺寸和长宽比的“k”个锚箱对每个位置进行评估。对于每个锚盒，确定 2 个类别预测和 4 个盒坐标。这两类预测表明盒子中的对象是背景还是前景(即有或没有对象)。4 个框坐标是典型的中心 x、y 以及宽度和高度。对于训练 rpn，我们给每个锚点分配一个二进制类标签(是否为对象)。保留具有最高<a class="ae ob" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank"> IOU </a>的锚和具有高于 0.7 的<a class="ae ob" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank"> IOU </a>重叠的锚。</p><p id="3051" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">特征图中落在方框内的部分是感兴趣区域，在 ROI 汇集层之后，这些感兴趣区域被提供给分类器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/b1f7fec7d2bdc6a8dc67b2cc7154a2f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uumgp33NDosIwfhwInICnw.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated"><strong class="bd no">图 5 </strong>:此处<a class="ae ob" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">原纸更快的 RCNN</a>。左侧是完整网络的示意图，包括区域提案网络。右边是如何使用锚定框的滑动窗口到达区域提议(框提议)的图示。</p></figure><h1 id="6a23" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">应用</h1><p id="daf1" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我们已经研究了这些流行的模型是如何解决目标检测问题的。对于每一个模型，都有一些我们在这里没有涉及到的细微差别，但是希望这篇文章仍然给出了一个关于这个问题是如何解决的一般概念。虽然知道这很好，但如果你更倾向于在应用程序中使用对象检测，那么检查一些可用的选项:<a class="ae ob" href="https://pytorch.org/docs/stable/torchvision/models.html#object-detection-instance-segmentation-and-person-keypoint-detection" rel="noopener ugc nofollow" target="_blank"> torchvision 模型</a>、<a class="ae ob" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank"> Tensorflow 对象检测 API </a>和 py torch powered<a class="ae ob" href="https://github.com/facebookresearch/detectron2" rel="noopener ugc nofollow" target="_blank">Detectron 2</a>。</p></div></div>    
</body>
</html>