# “黑匣子”。没有办法确定算法是如何得出你的决定的。

> 原文：<https://towardsdatascience.com/black-box-theres-no-way-to-determine-how-the-algorithm-came-to-your-decision-19c9ee185a8?source=collection_archive---------46----------------------->

## 人工智能黑匣子问题是基于无法完全理解为什么人工智能背后的算法会以这样的方式工作。

我们依赖机器学习，因为机器能够从经验和数据中学习，并在学习过程中不断改进。

但是如果我们不得不依赖它的一个模型， ***我们怎么能信任一个像它的数据一样不精确的模型呢？***

# 这一切都归结于透明度。

这是 AI 仍然可以为我们提供很多东西的地方。

虽然这在游戏或零售等行业可能是允许的，但对于在金融服务或医疗保健等高度监管行业运营的公司来说，这肯定是不可接受的。

![](img/90acd6c7723b22b51d159db412d224d7.png)

照片由 [Eugene Lim](https://unsplash.com/@overide) 在 [Unsplash](https://unsplash.com) 上拍摄

*如果你想了解更多，请访问*[***oscargarciaramos.com***](https://oscargarciaramos.com)

在高层次上，机器学习基本上是基于向机器提供大量数据，以便它们学习，并允许开发复杂的算法，这些算法可以推广和扩展到机器以前从未见过的其他数据。

然而，不管我们使用什么算法，有一个事实**:模型和它的数据一样好。 ***坏数据→坏模型*** 。**

**我举个例子:如果我想自动识别猫的照片，模型识别狗，我们就有问题了！**

**现在是“借口”来了:**

1.  **输入数据不好。**
2.  **之前没有进行过数据清理。**
3.  **设置和超参数配置不正确。**
4.  **数据有偏差，这种偏差会转移到我们训练过的数据集。**
5.  **模型训练不足，或者存在过拟合或欠拟合问题。**

**“喂，我们说的是猫狗！”。好吧，没什么事。 **

**现在让我们来谈谈面部识别模型，我们想把它卖给一家安全公司，这样他们就可以检测潜在的“小偷”。模型失败了，我们该怪谁？我发誓每个人都会逃走，没有人会告诉你一个字。**

**那么回到最初的问题， ***我们怎么能相信一个可以像它的数据一样不精确的模型呢？*****

# **我们需要透明度**

**太好了，我们想要透明。但是，我们在讨论机器学习，对吗？如果我们知道学习是如何工作的，我想就没有必要用数据来训练它了。我们只是从头开始编写模型代码，然后以 50K 的价格出售。我们有生意了！**

**抱歉，我的朋友，但事情不是这样的！**

**我们将模型的功能建立在数据的基础上，通过使用不同的算法，我们试图建立最准确的模型，以便我们可以推广到其他新数据。这是第一步，我们开始用四肢爬行。但是没有人会知道如何完美地行走。我们需要不断采取措施，用更好的数据，更好的超参数配置，更好的算法，更好的机器和资源迭代，尤里卡！**

**从开发人员或数据科学家的角度来看，这是完美的。让我们设身处地为消费者着想。我们无法控制模型，当它的表现不如预期时，我们该指责谁？数据、数据科学家、配置？**

****没有人知道这些问题的答案。****

**作为消费者，只能用模型，或者扔掉。没别的了。要么接受它，要么建立你的，恐怕我很清楚你的决定会是什么。但随着市场的进步，这种反应越来越让人无法接受。**

## **但是我们不能回答这个问题的原因是什么呢？**

# **1.无法解释的算法**

**用于建立模型的算法有多大的解释力？**

**当图像识别模型将狗识别为猫时，为什么会出现这种情况？也就是说，当模型得出诸如分类或回归之类的结论时，很难理解模型是如何得出该结论的。甚至像神经网络这样的技术也有这个问题。然而，并不是所有的机器学习算法都有相同的解释问题。决策树，由于其本质，是可以解释的，然而，当我们谈论随机森林时，我们失去了那些元素。**

# **2.无形的训练数据**

**正如我们所说，模型的性能直接取决于它的数据。但是，拥有完美的、干净的、标记良好的数据并不总是能确保一个好的模型。为什么？如果训练数据不代表真实世界的数据，我们就会有问题。我们将无法推断出任何东西！**

# **3.数据选择**

**“我想让你给我访问你正在使用的全部数据集”。**

**够了吗？号码**

**你真的知道这些数据中的哪些被用于训练模型吗？如果我们用你无法访问的其他数据来丰富它们，会怎么样？完全透明还意味着知道如何从可用的训练数据中选择数据，并且理想情况下能够在训练数据中使用与模型消费者相同的选择方法来查看哪些数据被包括在内，哪些数据被排除在外。**

# **4.训练数据集中的偏差**

**“偏差”一词可以指三个方面:在模型中建立的权重和“偏差”的意义上，在导致过度拟合或不足拟合的方差的未补偿意义上，或者在最广泛理解的意义上，作为信息“偏差”，由我们基于自己的先入为主的概念直接引起。**

# **5.模型版本控制**

**我们必须明白，模型不是在一次迭代中创建的，而是连续的。过去正常工作的东西现在可能会降低性能。新的数据，新的设置，你必须发展它。但是，所有这些变化都应该主动宣布，而不是被动地宣布，以避免出现问题，并确保在新型号表现不佳时，始终能够使用以前的版本。**

## **那么，下一步是什么？**

***“我们需要让行业透明化”***

**企业需要他们可以信任的模型。随着我们的采用增加，实现人工智能系统的透明度至关重要。**

****人工智能在用于现实世界的推理时必须是可靠的。****

***欢迎发表评论或分享这篇文章。关注* [*me*](/@ogarciaramos) *以后的帖子。***

***如果你想了解更多，你可以在*[***oscargarciaramos.com***](https://oscargarciaramos.com)找到我**