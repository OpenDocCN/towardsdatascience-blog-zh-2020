<html>
<head>
<title>PCA in a single line of code.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一行代码中的 PCA。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pca-in-a-single-line-of-code-ed79ae42059b?source=collection_archive---------32-----------------------#2020-05-16">https://towardsdatascience.com/pca-in-a-single-line-of-code-ed79ae42059b?source=collection_archive---------32-----------------------#2020-05-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/69b99c4d5340db276905d17ebd5acf79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3UPIaF7o3X3pnuo8o4Bttw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">复杂问题的简单解决方案！图片来自<a class="ae jd" href="https://pixabay.com/photos/hiking-trail-trail-single-trail-336603/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><div class=""/><p id="4279" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">利用主成分分析进行维数约简</p><p id="7096" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在真实世界的数据中，有大量的要素，很难分析或可视化这些海量的数据。因此，我们在数据预处理阶段使用降维来丢弃冗余特征。</p><p id="4e17" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> <em class="lb">降维</em> </strong>是将数据投影到一个更低维度的空间，这样更容易对数据进行分析和可视化。然而，维度的减少需要在准确性(高维)和可解释性(低维)之间进行权衡。</p><p id="eaa8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但关键是保留最大方差特征，减少冗余特征。</p><p id="d476" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">PCA 代表<strong class="kf jh"> <em class="lb">主成分分析</em> </strong>。以下是维基百科对五氯苯甲醚的描述。</p><blockquote class="lc ld le"><p id="6a36" class="kd ke lb kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">给定<a class="ae jd" href="https://en.wikipedia.org/wiki/Real_coordinate_space" rel="noopener ugc nofollow" target="_blank">二维、三维或更高维空间</a>中的点的集合，可以将“最佳拟合”线定义为最小化从点到该线的平均平方距离的线。可以类似地从垂直于第一条线的方向中选择下一条最佳拟合线。重复这个过程产生一个<a class="ae jd" href="https://en.wikipedia.org/wiki/Orthogonal" rel="noopener ugc nofollow" target="_blank">正交</a> <a class="ae jd" href="https://en.wikipedia.org/wiki/Basis_(linear_algebra)" rel="noopener ugc nofollow" target="_blank">基</a>，其中数据的不同维度是不相关的。这些基向量称为<strong class="kf jh">主成分</strong>，几个相关的程序<strong class="kf jh">主成分分析</strong> ( <strong class="kf jh"> PCA </strong>)。</p></blockquote><p id="fe9c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从广义上讲，PCA 就是找出有效的成分(特征)，丢弃多余的特征。于是，我们计算<strong class="kf jh">主成分</strong>来实现降维。</p><p id="d1fb" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">说到这里，首先，让我们理解 PCA 背后的数学原理，然后进入那行神奇的代码。</p><p id="0b4b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们有一个数据集 X，有“n”个数据点和“d”个列/特征。我们想把这个 d 维转换成 d `，比 d 小很多。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="534c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"><em class="lb">PCA 降维背后的数学:</em> </strong></p><p id="83a3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> 1。标准化数据:</strong></p><ul class=""><li id="e8ee" class="lp lq jg kf b kg kh kk kl ko lr ks ls kw lt la lu lv lw lx bi translated">数据标准化意味着使数据的均值为 0，标准差或方差为 1。</li><li id="969e" class="lp lq jg kf b kg ly kk lz ko ma ks mb kw mc la lu lv lw lx bi translated">标准化有助于数据归入相同的范围，并且不受单位/度量的限制。</li><li id="2c6e" class="lp lq jg kf b kg ly kk lz ko ma ks mb kw mc la lu lv lw lx bi translated">这可以使用 sklearn.preprocessing 轻松完成</li></ul><p id="2684" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> 2。计算数据的协方差矩阵:</strong></p><ul class=""><li id="ea6e" class="lp lq jg kf b kg kh kk kl ko lr ks ls kw lt la lu lv lw lx bi translated">协方差矩阵是通过基于每列对每个元素应用方差来形成的。</li><li id="3b8a" class="lp lq jg kf b kg ly kk lz ko ma ks mb kw mc la lu lv lw lx bi translated">协方差矩阵是列数大小的正方形对称矩阵。</li></ul><p id="85b6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> 3。矩阵的特征分解:</strong></p><ul class=""><li id="d465" class="lp lq jg kf b kg kh kk kl ko lr ks ls kw lt la lu lv lw lx bi translated"><em class="lb">特征向量:</em>特征向量是当对其应用线性变换时，其方向保持不变的向量。</li><li id="b79e" class="lp lq jg kf b kg ly kk lz ko ma ks mb kw mc la lu lv lw lx bi translated"><em class="lb">特征值:</em>特征值是变换过程中使用的比例因子。</li><li id="82d5" class="lp lq jg kf b kg ly kk lz ko ma ks mb kw mc la lu lv lw lx bi translated">求协方差矩阵 s 的特征值和对应的特征向量。</li><li id="76bf" class="lp lq jg kf b kg ly kk lz ko ma ks mb kw mc la lu lv lw lx bi translated">对于大小为 dxd 的协方差矩阵，我们得到‘d’个特征值和‘d’个特征向量。</li></ul><p id="3115" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> 4。对协方差矩阵进行特征值分解</strong></p><ul class=""><li id="f573" class="lp lq jg kf b kg kh kk kl ko lr ks ls kw lt la lu lv lw lx bi translated">将 d '个特征向量中的每一个排列成向量 v 中的列。现在，向量 v 的形状是，dxd '</li><li id="6314" class="lp lq jg kf b kg ly kk lz ko ma ks mb kw mc la lu lv lw lx bi translated">数据集 X(nxd)和 v (dxd `)的点积产生大小为 nxd `的降维数据集。让我们称之为 x`。</li></ul><p id="e5ea" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，数据集，X 个 n 个数据点和 d 个特征被简化为 X’个 n 个数据点和 d’个特征的数据集。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="a8ed" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，降维对数据的保留几乎没有什么好处。</p><p id="5c16" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了计算保留百分比，我们使用特征值。这里有 d 个特征向量，在这 d 个特征向量中，我们使用了 d 个特征向量。</p><p id="4eae" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个简单但非常有用的公式，用于在降维后保留数据。</p><figure class="me mf mg mh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi md"><img src="../Images/d8197f7e9c7e36b1156e96498f5b0f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1TXM-gkRhzBaBrtHDh4oow.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">保留值的数学公式</p></figure></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="2528" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">到了实现部分，我们可以使用 python 来执行这些步骤。然而，sckit-learn 提供了一个以更简单的方式执行 PCA 的库。</p><p id="f49d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文使用的数据集是 Kaggle 著名的<a class="ae jd" href="https://www.kaggle.com/c/digit-recognizer" rel="noopener ugc nofollow" target="_blank"> <strong class="kf jh"> MNIST 数字识别器</strong> </a>数据集。</p><p id="5a7b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> <em class="lb">数据描述:</em> </strong></p><ul class=""><li id="06cf" class="lp lq jg kf b kg kh kk kl ko lr ks ls kw lt la lu lv lw lx bi translated">数据文件包含手绘数字的灰度图像，从 0 到 9。</li><li id="4e01" class="lp lq jg kf b kg ly kk lz ko ma ks mb kw mc la lu lv lw lx bi translated">每幅图像高 28 像素，宽 28 像素，总共 784 像素。每个像素都有一个与之关联的像素值，表示该像素的亮度或暗度，数字越大表示越暗。该像素值是 0 到 255 之间的整数，包括 0 和 255。</li><li id="52b7" class="lp lq jg kf b kg ly kk lz ko ma ks mb kw mc la lu lv lw lx bi translated">训练数据集(train.csv)有 785 列。第一列称为“标签”，是用户绘制的数字。其余的列包含相关图像的像素值。<br/>数据集中的每个像素列都有一个类似 pixelx 的名称，其中 x 是 0 到 783 之间的整数，包括 0 和 783。为了在图像上定位这个像素，假设我们将 x 分解为 x = i * 28 + j，其中 I 和 j 是 0 到 27 之间的整数，包括 0 和 27。那么 pixelx 位于 28×28 矩阵的第 I 行和第 j 列(由零索引)。</li></ul><p id="0965" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> <em class="lb">加载数据:</em> </strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4b00" class="mn mo jg mj b gy mp mq l mr ms">import pandas as pd<br/>mnist_data = pd.read_csv("mnist.csv")</span></pre><p id="ca1e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用 Pandas 将 CSV 格式(逗号分隔值)的数据文件加载到数据框中。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="ba92" class="mn mo jg mj b gy mp mq l mr ms">mnist_data.shape</span></pre><p id="c80b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出:(42000，785)</p><p id="fcb2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">查看数据，我们发现有 42，000 个数据点和 785 个特征。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9e27" class="mn mo jg mj b gy mp mq l mr ms">mnist_data.head()</span></pre><p id="8d2e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出:</p><figure class="me mf mg mh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mt"><img src="../Images/63fa02c5c3ecaa31d0db2092cb909d0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hn_Sc-jEim4j1t2rwPYKwA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">head()对数据的输出</p></figure><p id="3f74" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Head()给出了数据的前 5 个数据点。</p><p id="f9e3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一列标签是目标变量。剩下的 784 列是特征。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6fcc" class="mn mo jg mj b gy mp mq l mr ms">target_variable = mnist_data["label"]<br/>features_variable=mnist_data.drop("label",axis=1)</span></pre><p id="6d38" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将标签列分配给 target_variable，其余列分配给 features_variable。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0441" class="mn mo jg mj b gy mp mq l mr ms">print(target_variable.shape)<br/>print(features_variable.shape)</span></pre><p id="38e1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出:(42000，)<br/> (42000，784)</p><p id="4756" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们看看如何使用 PCA 来减少这个维数为 42000 个数据点和 784 个特征的特征变量。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="945a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> <em class="lb">利用 PCA 实现降维的分步实现:</em> </strong></p><ol class=""><li id="5a47" class="lp lq jg kf b kg kh kk kl ko lr ks ls kw lt la mu lv lw lx bi translated"><strong class="kf jh"> <em class="lb">数据标准化:</em> </strong></li></ol><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="7ea9" class="mn mo jg mj b gy mp mq l mr ms">from sklearn.preprocessing import StandardScaler<br/>standarized_data = StandardScaler().fit_transform(features_variable)</span></pre><p id="f99e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">fit()执行数学运算，transform()将这些值设置为数据，并将数据转换为标准形式。</p><p id="f146" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">②<em class="lb">。使用 sckit 的 PCA-learn:</em></strong></p><p id="7691" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Sckit-learn 提供了分解，通过它可以处理 PCA 的所有数学运算。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="c9a5" class="mn mo jg mj b gy mp mq l mr ms">from sklearn import decomposition<br/>pca = decomposition.PCA(n_components=2)</span></pre><p id="765f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从 sckit-learn 导入分解并为 PCA()创建一个对象。这里 n_components 是我们期望降维后的特征数。</p><p id="a57e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是降低数据维度的一行代码:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="523a" class="mn mo jg mj b gy mp mq l mr ms">pca_data = pca.fit_transform(standarized_data)</span></pre><p id="52f0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输入是标准化数据，输出应该是降维数据。</p><p id="6399" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们打印输出并查看减少的数据，</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="8d04" class="mn mo jg mj b gy mp mq l mr ms">print(pca_data.shape)<br/>print(pca_data)</span></pre><p id="bfa5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出:</p><figure class="me mf mg mh gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/4aa1c383f8294bd256c818d358922561.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*XYTY_eqDlHJRyNhTcuknZA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">使用 PCA 降维后的 MNIST 数据</p></figure><p id="fc7b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">观察:</strong></p><ul class=""><li id="27a6" class="lp lq jg kf b kg kh kk kl ko lr ks ls kw lt la lu lv lw lx bi translated">pca_data 中的每一列都是我们感兴趣的主要成分之一。</li><li id="d7a0" class="lp lq jg kf b kg ly kk lz ko ma ks mb kw mc la lu lv lw lx bi translated">对于 42，000 个数据点，使用一行代码可以将特征的数量从 784 个减少到 2 个。</li></ul></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="0cf6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">剩下的问题是，如何确定要保留的特征的数量。Scree 阴谋来拯救。</p><p id="82da" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">scree 图用于确定探索性因子分析(FA)中要保留的因子数或主成分分析(PCA)中要保留的主成分数。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d4b2" class="mn mo jg mj b gy mp mq l mr ms">import numpy as np<br/>pca.n_components = 784<br/>pca_data = pca.fit_transform(standarized_data)</span><span id="76bb" class="mn mo jg mj b gy mw mq l mr ms">retention_value = pca.explained_variance_ / np.sum(pca.explained_variance_);<br/>cumulative_retention = np.cumsum(retention_value)<br/></span></pre><p id="70f8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们将 n_components 设置为 784，即数据点的总数，以便查看用于约束每个值的保留值。</p><p id="c528" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> <em class="lb">标图</em>标图</strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="48d5" class="mn mo jg mj b gy mp mq l mr ms">import matplotlib.pyplot as plt<br/>plt.plot(cumulative_retention)<br/>plt.grid()</span><span id="905c" class="mn mo jg mj b gy mw mq l mr ms">plt.xlabel('n_components')<br/>plt.ylabel('Retention value on scale of 1')</span></pre><figure class="me mf mg mh gt is gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/be427a14f6042d43ebf824fb98f02daa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*-wrB-jxee1JJhSlCmcmT2w.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">碎石图</p></figure><p id="4d56" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">观察:</strong></p><p id="1775" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从 Scree 图中，我们观察到，对于约 90%的保留率(这对数据分析非常有用),需要限制 200 个特征。<br/>然而，为了数据可视化，建议减少到二维。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="4776" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">谢谢你的阅读。我也将在未来写更多初学者友好的帖子。请在<a class="ae jd" href="https://medium.com/@ramyavidiyala" rel="noopener">媒体</a>上关注我，以便了解他们。我欢迎反馈，可以通过 Twitter <a class="ae jd" href="https://twitter.com/ramya_vidiyala" rel="noopener ugc nofollow" target="_blank"> ramya_vidiyala </a>和 LinkedIn <a class="ae jd" href="https://www.linkedin.com/in/ramya-vidiyala-308ba6139/" rel="noopener ugc nofollow" target="_blank"> RamyaVidiyala </a>联系我。快乐学习！</p></div></div>    
</body>
</html>