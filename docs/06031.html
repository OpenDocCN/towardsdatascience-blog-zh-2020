<html>
<head>
<title>YOLO v4: Optimal Speed &amp; Accuracy for object detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLO v4:物体检测的最佳速度和精确度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/yolo-v4-optimal-speed-accuracy-for-object-detection-79896ed47b50?source=collection_archive---------1-----------------------#2020-05-17">https://towardsdatascience.com/yolo-v4-optimal-speed-accuracy-for-object-detection-79896ed47b50?source=collection_archive---------1-----------------------#2020-05-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ee90" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">实时目标探测模型综述</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/41a836fbc7064419e146eefe33db3eaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*RpTe5TQOA0x_ch-WGLJPgQ.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">YOLO v3 演示，摘自<a class="ae ku" href="https://www.youtube.com/watch?v=MPU2HistivI" rel="noopener ugc nofollow" target="_blank">视频</a></p></figure><p id="f5d9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">你只看一次(<a class="ae ku" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank"> YOLO </a>)是一系列快速准确的一级物体探测器。最近，<a class="ae ku" href="https://arxiv.org/abs/2004.10934" rel="noopener ugc nofollow" target="_blank"> YOLO v4 </a>论文发布，显示了与其他物体探测器相比非常好的结果。</p><p id="8496" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> <em class="lr">更新 1 </em> </strong> <em class="lr">:增加了一个</em> <a class="ae ku" href="#0260" rel="noopener ugc nofollow"> <em class="lr"> colab 演示</em> </a></p><h1 id="67da" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">目录</h1><ol class=""><li id="736d" class="mk ml it kx b ky mm lb mn le mo li mp lm mq lq mr ms mt mu bi translated"><a class="ae ku" href="#6199" rel="noopener ugc nofollow"> <strong class="kx iu">简介</strong> </a></li><li id="d938" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><a class="ae ku" href="#ed40" rel="noopener ugc nofollow"> <strong class="kx iu">物体探测器的一般架构</strong> </a></li><li id="88cf" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><a class="ae ku" href="#f096" rel="noopener ugc nofollow"> <strong class="kx iu">袋赠品&amp;袋特价</strong> </a></li><li id="4483" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><a class="ae ku" href="#56ff" rel="noopener ugc nofollow"> <strong class="kx iu"> YOLO v4 设计</strong> </a></li><li id="5828" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><a class="ae ku" href="#a112" rel="noopener ugc nofollow"> <strong class="kx iu">汇总 BoF 和 BoS 使用的</strong> </a></li><li id="6620" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><a class="ae ku" href="#03c8" rel="noopener ugc nofollow"> <strong class="kx iu">附加改进</strong> </a></li><li id="7448" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><a class="ae ku" href="#0260" rel="noopener ugc nofollow"> <strong class="kx iu"> Colab 演示</strong> </a></li><li id="8852" class="mk ml it kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><a class="ae ku" href="#9e04" rel="noopener ugc nofollow"> <strong class="kx iu">最后一句话</strong> </a></li></ol></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="6199" class="ls lt it bd lu lv nh lx ly lz ni mb mc jz nj ka me kc nk kd mg kf nl kg mi mj bi translated">介绍</h1><p id="5e81" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">大多数现代精确模型需要许多 GPU 来进行大规模小批量的训练，而用一个 GPU 来进行训练会使训练变得非常缓慢和不切实际。YOLO v4 通过制作一个对象检测器来解决这个问题，该对象检测器可以在单个 GPU 上以较小的小批量进行训练。这使得用单个 1080 Ti 或 2080 Ti GPU 训练超快速和精确的物体检测器成为可能。</p><p id="cb83" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">YOLO v4 在 MS COCO 数据集上以实时速度获得了最先进的结果，在 Tesla V100 上以 65 FPS 运行的 AP 为 43.5 %。相当有趣的结果！为了实现这些结果，他们结合了一些特征，如加权剩余连接(WRC)、跨阶段部分连接(CSP)、交叉小批量归一化(CmBN)、自我对抗训练(SAT)和 Mish 激活、镶嵌数据增强、DropBlock 正则化和 CIoU 损失。这些被称为通用特征，因为它们应该独立于计算机视觉任务、数据集和模型工作良好。我们将在后面讨论这些特性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ec5b39c58536b870924b101b78d0a629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*dyX7F7rF28Y-qKkA34KywQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">分别提高约洛夫 3 的 AP 和 FPS 10%和 12%[<a class="ae ku" href="#6ac9" rel="noopener ugc nofollow">5</a>]</p></figure><p id="7019" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="lr">注意:落在浅蓝色区域的模型被认为是实时物体探测器(+30 FPS) </em></p><p id="f198" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们可以看到，EfficientDet D4-D3 比 YOLO v4 模型实现了更好的 AP，但它们以&lt; 30 FPS on a V100 GPU. On the other hand, YOLO is able to run at a much higher speed (&gt; 60 FPS 的速度运行，具有非常好的准确性。</p><h1 id="ed40" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">物体检测器的一般结构</h1><p id="656b" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">虽然 YOLO 是一级检测器，但也有像 R-CNN、快速 R-CNN 和更快 R-CNN 这样的两级检测器，它们准确但速度慢。我们将把重点放在前者上。让我们来看看现代一级物体探测器的主要组成部分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/8cfdc714a5117e492b8f6c772b302051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*jLUJU34dSbrRWdspJZbLXA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">摘自 YOLO v4 论文，<a class="ae ku" href="https://arxiv.org/abs/2004.10934" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="49b5" class="nr lt it bd lu ns nt dn ly nu nv dp mc le nw nx me li ny nz mg lm oa ob mi oc bi translated">毅力</h2><p id="dcab" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">使用<a class="ae ku" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet </a>、<a class="ae ku" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNet </a>、<a class="ae ku" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"> VGG </a>等模型作为特征提取器。它们在 ImageNet 等图像分类数据集上进行了预训练，然后在检测数据集上进行了微调。事实证明，随着网络变得更深(更多层)，这些网络产生具有更高语义的不同级别的特征，对于物体检测网络的后面部分是有用的。</p><h2 id="1fae" class="nr lt it bd lu ns nt dn ly nu nv dp mc le nw nx me li ny nz mg lm oa ob mi oc bi translated">脖子</h2><p id="b24e" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">这些额外的层位于脊椎和头部之间。它们用于提取脊柱不同阶段的不同特征图。颈部可以是例如 FPN[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 1 </a> ]、帕内特[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 2 </a> ]、双 FPN[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 3 </a> ]等。例如，YOLOv3 使用 FPN 从主干中提取不同尺度的特征。</p><p id="661e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">金字塔网络(FPN)有什么特点？</p><blockquote class="od oe of"><p id="cf0a" class="kv kw lr kx b ky kz ju la lb lc jx ld og lf lg lh oh lj lk ll oi ln lo lp lq im bi translated">使用自上而下的路径和横向连接增强标准卷积网络，以便网络从单分辨率输入图像中高效构建丰富的多尺度特征金字塔[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 4 </a> ]</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/ab806a2deca086aea800c2dba6d4d8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*GpaVvUhlw6p8sBLg9lSemw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">用于目标检测的特征金字塔网络[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 1 </a></p></figure><p id="f178" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">每个横向连接将从自下而上路径到自上而下路径的特征图合并，产生不同的金字塔等级。在合并特征地图之前，先前的金字塔等级在 FPN[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 1 </a> ]中被上采样 2 倍，因此它们具有相同的空间大小。然后将分类/回归网络(头部)应用于金字塔的每一层，以便帮助检测不同大小的对象。</p><p id="77ed" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这种特征金字塔网络的思想可以应用于不同的主干模型，作为一个例子，最初的 FPN[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 1 </a> ]论文使用了 ResNets。还有很多以不同方式整合 FPN 的模块，比如 SFAM [ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 7 </a>、ASFF [ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 9 </a>、比 FPN[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 3 </a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/2919d088d854945080879e57220a2b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*dKmSNrxJrxmgt_Xo4ZNSIg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">四种类型的要素金字塔。SFAM[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 7 </a> ]模块是(d)</p></figure><p id="7ca3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">图像<em class="lr"> (a) </em>显示了如何在单触发检测器架构(SSD)中从主干提取特征。上图还显示了其他三种不同类型的金字塔网络，但它们背后的理念是相同的，因为它们有助于:</p><blockquote class="od oe of"><p id="4ece" class="kv kw lr kx b ky kz ju la lb lc jx ld og lf lg lh oh lj lk ll oi ln lo lp lq im bi translated">缓解由对象实例之间的比例变化引起的问题[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 3 </a> ]。</p></blockquote><p id="7432" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">ASFF[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 9 </a> ]和比 FPN[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 3 </a> ]也是有趣的 FPN 类型，并显示出有趣的结果，但我们在这里将跳过它们。</p><h2 id="f302" class="nr lt it bd lu ns nt dn ly nu nv dp mc le nw nx me li ny nz mg lm oa ob mi oc bi translated">头</h2><p id="f9aa" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">这是一个负责实际执行包围盒检测部分(分类和回归)的网络。单个输出可能看起来像(取决于实现):4 个描述预测边界框的值(<em class="lr"> x，y，h，w </em>)和<em class="lr"> k </em>类的概率+ 1(额外一个用于背景)。对象检测器<em class="lr">基于锚的</em>，如 YOLO，将头部网络应用于每个锚盒。其他流行的基于<em class="lr">锚的</em>一级探测器有:单炮探测器<a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 6 </a>和 retina net<a class="ae ku" href="#6ac9" rel="noopener ugc nofollow">4</a>。</p><p id="7567" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">下图结合了上述三个模块。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi ol"><img src="../Images/65f2c435051d5e6ad923b6cc7fa5b019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6SNJiyXJe0nQ9_L4vD3Ycw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">原始图像来自 RetinaNet[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 4 </a> ]纸张</p></figure></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="f096" class="ls lt it bd lu lv nh lx ly lz ni mb mc jz nj ka me kc nk kd mg kf nl kg mi mj bi translated"><strong class="ak">一袋赠品</strong> <strong class="ak"> &amp;一袋特价商品</strong></h1><p id="49b7" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">YOLO v4 论文的作者[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 5 </a> ]区分了两类用于提高物体检测器精度的方法。他们分析了两个类别中的不同方法，以实现具有良好准确性的快速运算速度的神经网络。这两个类别是:</p><h2 id="4640" class="nr lt it bd lu ns nt dn ly nu nv dp mc le nw nx me li ny nz mg lm oa ob mi oc bi translated"><strong class="ak">一袋赠品</strong> (BoF):</h2><blockquote class="od oe of"><p id="f38e" class="kv kw lr kx b ky kz ju la lb lc jx ld og lf lg lh oh lj lk ll oi ln lo lp lq im bi translated">在不增加推理代价的情况下，使目标检测器获得更高精度的方法。这些方法只是改变了培训策略或者只是增加了培训成本。[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 5 </a></p></blockquote><p id="8a8f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">BoF 的一个例子是<strong class="kx iu">数据扩充</strong>，增加了模型的泛化能力。要做到这一点，我们可以做照片度量失真，如:改变亮度，饱和度，对比度和噪声，或者我们可以做图像的几何失真，如旋转，裁剪等。这些技术是 BoF 的一个明显的例子，它们有助于检测机的准确性！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/3be6b071458f15d1295fcb2eef50d4be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*cqegHkmX7rAq2FhsK_Oa2w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">几何扭曲的例子，<a class="ae ku" href="https://developers.google.com/machine-learning/practica/image-classification/preventing-overfitting" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="bdde" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="lr">注意:对于物体检测任务，边界框也应该应用相同的变换</em></p><p id="dc78" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">还有其他一些增强图像的有趣技术，比如<strong class="kx iu">剪切</strong> [ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 8 </a> ]，它在训练过程中随机屏蔽输入的方形区域。这表明改进了 CNN 的鲁棒性和性能。同样，<strong class="kx iu">随机擦除</strong> [ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 10 </a> ]选择图像中的矩形区域，用随机值擦除其像素。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/837533d710b663a1e593e517081add02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*VZjvRfyTBgKj31zbQDvpWQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">对象检测的随机擦除示例，<a class="ae ku" href="https://arxiv.org/pdf/1708.04896.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="9da5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">免费赠品的其他背面是用于避免过拟合的正则化技术，如:<strong class="kx iu"> DropOut </strong>、<strong class="kx iu"> DropConnect </strong>和<strong class="kx iu"> DropBlock </strong>、<a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 13 </a>。这最后一个实际上在 CNN 中显示了非常好的结果，并且在 YOLO v4 主干中使用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/3a122eca98833287c80e878490d4f46b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*ZZM9t_UX5T3X4-Zp7ami3Q.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来自 DropBlock paper[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 13 </a></p></figure><p id="e439" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">随机删除激活(b)不利于删除语义信息，因为附近的激活包含密切相关的信息。相反，通过丢弃连续区域，它可以移除某些语义信息(例如，头或脚)并强制剩余单元学习用于分类输入图像的其他特征。</p><p id="3e30" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">回归网络的成本函数也适用于该类别。传统的做法是应用均方误差对坐标进行回归。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/31901fc0ee86f125840716137459329d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*zGL8lj2XLUnO9mosLdkLQw.png"/></div></figure><p id="091d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如论文中所述，这将这些点视为独立变量，但不考虑对象本身的完整性。为了改善这一点，已经提出了 IoU[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 12 </a> ]损失，其考虑了预测边界框(BBox)和地面真实边界框的面积。这个想法被<strong class="kx iu"> GIoU loss </strong> [ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 11 </a>进一步改进，除了覆盖区域之外还包括了一个物体的形状和方位。另一方面，还引入了<strong class="kx iu"> CIoU loss </strong>，它考虑了重叠面积、中心点之间的距离和纵横比。YOLO v4 使用 CIoU 损失作为边界框的损失，主要是因为与提到的其他方法相比，它导致更快的收敛和更好的性能。</p><p id="8cf6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="lr">注意:可能引起混淆的一件事是，尽管许多模型使用 MSE 来计算 BBox 回归损失，但它们使用 IoU 作为度量，而不是如上所述的损失函数。</em></p><p id="4818" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">下图比较了具有不同 IoU 损耗的同一型号:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi ou"><img src="../Images/0ef0afd4c4c3f4e7d0c28e73fb90d936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qkY4UWFjR8prY210yPzNiA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">损失比较，<a class="ae ku" href="https://arxiv.org/abs/1911.08287" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="ca73" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们可以注意到 CIoU 比 GIoU 表现得更好。这些检测来自更快的 R-CNN (Ren 等人，2015 年)，其在相同的 MS COCO 数据集上训练，具有 GIoU 和 CIoU 损失。</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h2 id="dbad" class="nr lt it bd lu ns nt dn ly nu nv dp mc le nw nx me li ny nz mg lm oa ob mi oc bi translated"><strong class="ak">特价包</strong> (BoS) <strong class="ak"> : </strong></h2><blockquote class="od oe of"><p id="d6a6" class="kv kw lr kx b ky kz ju la lb lc jx ld og lf lg lh oh lj lk ll oi ln lo lp lq im bi translated">那些插件模块和后处理方法，只是少量增加了推理成本，却能显著提高物体检测的准确性[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 5 </a> ]</p></blockquote><p id="6a35" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如论文中所述，这类模块/方法通常涉及:引入注意机制(<em class="lr">挤压-激发</em>和<em class="lr">空间</em> <em class="lr">注意模块</em>)，扩大模型感受野，增强特征整合能力等。</p><p id="1660" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">用于改善感受野的常见模块有<a class="ae ku" href="https://arxiv.org/abs/1406.4729" rel="noopener ugc nofollow" target="_blank"> SPP </a>、<a class="ae ku" href="https://arxiv.org/abs/1606.00915" rel="noopener ugc nofollow" target="_blank"> ASPP </a>和<a class="ae ku" href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"> RFB </a> (YOLO v4 使用<strong class="kx iu"> SPP </strong>)。</p><p id="d50e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">此外，细胞神经网络的注意模块主要分为通道注意，如挤压和激发(SE)[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 15 </a> ]，和空间注意，如空间注意模块(SAM)[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 16 </a>。后者有时更受青睐的一个原因是，se 在 GPU 上增加了 10%的推理速度，这是不可取的。事实上，YOLO v4 考虑了 SAM[16]模块，但并不完全像最初在这篇<a class="ae ku" href="https://arxiv.org/abs/1807.06521" rel="noopener ugc nofollow" target="_blank">论文</a>中发表的那样。请注意以下几点:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/58b08137b73759ccd1faaf9658e22c8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*xD-AZRCPm6f2jVORmzhbjg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">原始空间注意模块[16]</p></figure><p id="4f8b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">给定特征映射 F’，最初的实现沿着通道轴执行平均池和最大池操作，然后将它们连接起来。然后应用卷积层(以 sigmoid 为激活函数)生成注意图(<strong class="kx iu"> Ms </strong>)，应用于原<strong class="kx iu">F’</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/7159b7f28e629d1957541a379898e1f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*lCz3pvszf1R4BEDI5HS_jw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">YOLO v4 修改的空间注意模块，来源[5]</p></figure><p id="ab6c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">另一方面，YOLO v4 修改的<strong class="kx iu"> SAM </strong>不应用最大池和平均池，而是<strong class="kx iu">F’</strong>通过一个 conv。层(使用 sigmoid 激活),然后乘以原始特征图(F’)。</p><p id="17c7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们之前讨论过的 SFAM[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 7 </a> ]、ASFF[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 9 </a> ]和比 FPN[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 3 </a> ]等特征金字塔也属于这一类 BoS，激活功能也是如此。自从 ReLU 问世以来，它的变种很多，像 LReLU，PReLU，ReLU6。像 ReLU6 和 hard-Swish 这样的激活是专门为用于在嵌入式设备上进行推理的量子化网络设计的，就像在谷歌<a class="ae ku" href="https://www.coral.ai/" rel="noopener ugc nofollow" target="_blank">珊瑚边缘 TPU </a>中一样。</p><p id="60e1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">另一方面，YOLO v4 在主干中使用了大量的<strong class="kx iu"> Mish </strong> [ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 14 </a> ]激活函数。看一看图表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi ow"><img src="../Images/f6407ed8d7bcbed3c6b95f78f8f3324d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o1LfiOXiCmfLRadkUMpTJQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">米什公式</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/03cab5de82b24f0b36e6816ade1795c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*xmKdHmfmcirR5bTPRmiT3Q.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来源[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 14 </a> ]</p></figure><p id="0358" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这个激活函数显示了非常有希望的结果。例如，与带有 Swish 和 ReLU 的相同网络相比，使用带有 Mish(在 CIFAR-100 数据集上)的挤压激励网络[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 15 </a> ]导致 Top-1 测试精度分别增加<strong class="kx iu">0.494%</strong>和<strong class="kx iu"> 1.671% </strong>。[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 14 </a> ]</p><p id="5a67" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">你可以检查这个<a class="ae ku" href="https://www.desmos.com/calculator/rhx5tl8ygi" rel="noopener ugc nofollow" target="_blank"> desmos </a>，它包含了一些其他的激活功能。</p><h1 id="56ff" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">YOLO v4 设计</h1><p id="b709" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">到目前为止，我们已经讨论了用于提高模型精度的方法和对象检测器的不同部分(脊柱、颈部、头部)。现在让我们谈谈在新 YOLO 使用什么。</p><ul class=""><li id="5bdc" class="mk ml it kx b ky kz lb lc le oy li oz lm pa lq pb ms mt mu bi translated"><strong class="kx iu">主干</strong>:使用<strong class="kx iu"> CSPDarknet53 </strong>作为 GPU 版本的特征提取器模型。对于 VPU(视觉处理单元)，他们考虑使用 efficient net-lite—mix net—ghost net 或 MobileNetV3。我们现在将重点放在 GPU 版本上。</li></ul><p id="e36b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">下表显示了 GPU 版本的不同主干</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi pc"><img src="../Images/83cd5a7509bade02e2bf78e6d2167140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2O2yR0Ma_jyenJHP-c6cEA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来源[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 5 </a></p></figure><p id="6260" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">某些主干更适合于分类而不是检测。例如，CSPDarknet53 在检测对象方面比 CSPResNext50 更好，CSPResNext50 在图像分类方面比 CSPDarknet53 更好。如论文中所述，用于对象检测的主干模型需要<strong class="kx iu">更大的输入网络尺寸</strong>，以更好地检测小对象，并需要<strong class="kx iu">更多的层</strong>，以获得更高的感受域。</p><ul class=""><li id="d9f8" class="mk ml it kx b ky kz lb lc le oy li oz lm pa lq pb ms mt mu bi translated"><strong class="kx iu">瓶颈</strong>:他们使用空间金字塔池(SPP)和路径聚合网络(PAN)。后者与最初的 PAN 不同，而是一个修改版本，用一个 concat 代替了加法。插图显示了这一点:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/58179839e598b96bb4ab255ff33097a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*ZModSr_qNZTJZYfbonGCoQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来源[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 5 </a> ]</p></figure><p id="f82c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最初在 PAN 纸中，在将 N4 的尺寸减小到与 P5 具有相同的空间尺寸之后，他们<strong class="kx iu">将这个新的减小尺寸的 N4 与 P5 相加</strong>。这种情况在𝑃𝑖+1 和𝑁𝑖的各个层面上重复出现，从而产生了𝑁𝑖+1.在 YOLO v4 中不是用<em class="lr"> </em>每个𝑃𝑖+1 加上𝑁𝑖，而是用<strong class="kx iu">串联<em class="lr"> </em> </strong>它们(如上图所示)<em class="lr">。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi pe"><img src="../Images/36f009eb1f8427fbfeaf0da5f2971d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*quxQFrRlwqxuR0tQGFdc0w.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">路径聚合网络(PAN)源[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 2 </a></p></figure><p id="10f9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">查看<strong class="kx iu"> SPP </strong>模块，它基本上在 19*19*512 的特征图上执行最大池化，具有不同的内核大小<em class="lr"> s k = </em> { 5，9，13}和“相同的”填充(以保持相同的空间大小)。四个相应的特征地图然后被连接以形成 19*19*2048 的体积。这增加了颈部感受野，从而提高了模型的准确性，而推理时间的增加可以忽略不计。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/cd21237ecd726ce3586a14421182b72f.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*9rS4jWjF82imeX8PdIR3kQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">在 yolov4.cfg 中观察到的 SPP</p></figure><p id="20e0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果你想<strong class="kx iu">可视化</strong>yolo 中使用的不同图层，如上图所示，我推荐使用这个<a class="ae ku" href="https://github.com/lutzroeder/netron" rel="noopener ugc nofollow" target="_blank">工具</a>(网络/桌面版本都可以)，然后用它打开<a class="ae ku" href="https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov4.cfg" rel="noopener ugc nofollow" target="_blank"> yolov4.cfg </a>。</p><ul class=""><li id="419a" class="mk ml it kx b ky kz lb lc le oy li oz lm pa lq pb ms mt mu bi translated"><strong class="kx iu">头</strong>:和 YOLO v3 用的一样。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi pg"><img src="../Images/042f0eeb4924300bd13ed23730284a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A2U1GJD2s5om4lkcF6oQRw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">YOLO 头适用于不同的规模</p></figure><p id="79bd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这些探头应用于网络的不同规模，用于检测不同大小的物体。因为(80 个<em class="lr">类</em> + 1 个<em class="lr">对象</em> + 4 个<em class="lr">坐标</em> ) * 3 个<em class="lr">锚，所以通道数为 255。</em></p><h1 id="a112" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">使用的 BoF 和 BoS 汇总</h1><p id="7c5e" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">YOLO v4 的主干网和检测器中使用的 BoF 和 BoS 的不同模块/方法可以总结如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi ph"><img src="../Images/7fcc4049a400be24c46f0c4b8b807dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RAg1JQ_AbL-tni5u6ESm1g.jpeg"/></div></div></figure><h1 id="03c8" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">其他改进</h1><p id="1738" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">该论文的作者介绍了一种新的数据增强方法，称为“马赛克”。基本上，这将训练数据集的 4 幅图像组合在一幅图像中。通过现在这样做:</p><blockquote class="od oe of"><p id="438d" class="kv kw lr kx b ky kz ju la lb lc jx ld og lf lg lh oh lj lk ll oi ln lo lp lq im bi translated">批量标准化从每层上的 4 个不同图像计算激活统计[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 5 </a></p></blockquote><p id="639e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">因此，它大大减少了选择一个大的小批量训练的需要。检查下图，显示新的增强方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi pi"><img src="../Images/e9865da21d3014dc148c2fd3f472d959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8GANcQ6E4B-3gE0fGqRu5w.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来自[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 5 </a> ]的镶嵌数据增强</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/be5008ae6cc1684e4c1bc61ca2bc6904.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*lZ27jvUVgkaZYPisPGjiVQ.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">示例:用于车牌检测的马赛克增强</p></figure><p id="df61" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">他们也使用自我对抗训练 (SAT)，分两个前进后退阶段进行。在第一阶段，神经网络改变原始图像而不是网络权重。通过这种方式，神经网络对自身进行对抗性攻击，改变原始图像以制造图像上没有所需对象的假象。在第二阶段，训练神经网络以正常方式检测该修改图像上的对象。[ <a class="ae ku" href="#6ac9" rel="noopener ugc nofollow"> 5 </a></p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="0260" class="ls lt it bd lu lv nh lx ly lz ni mb mc jz nj ka me kc nk kd mg kf nl kg mi mj bi translated">Colab 演示</h1><p id="4d0d" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">我做了一个 Colab 来测试 YOLO v4 &amp;你自己视频的小版本。这使用了在可可小姐身上训练的模型。你可以看一看<a class="ae ku" href="https://colab.research.google.com/drive/1PuI9bYeM8O1OA82pI12oGopRJJrLWfs9?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a></p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="9e04" class="ls lt it bd lu lv nh lx ly lz ni mb mc jz nj ka me kc nk kd mg kf nl kg mi mj bi translated">结论</h1><p id="c232" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">这篇文章中提到了许多有趣的想法，可以更详细地解释，但我希望主要概念是清楚的。</p><p id="1ef3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">更多细节可在<a class="ae ku" href="https://arxiv.org/abs/2004.10934" rel="noopener ugc nofollow" target="_blank">文件</a>中找到。如果你想在自己的数据集上训练它，可以查看一下<a class="ae ku" href="https://github.com/AlexeyAB/darknet" rel="noopener ugc nofollow" target="_blank">官方回购</a>。</p><p id="31f4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">YOLO v4 在实时对象检测方面取得了最先进的结果(43.5% AP)，并且能够在 V100 GPU 上以 65 FPS 的速度运行。如果你想要更低的精度，但更高的 FPS，请在官方回购处查看新的 Yolo v4 微型版本。</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h2 id="6ac9" class="nr lt it bd lu ns nt dn ly nu nv dp mc le nw nx me li ny nz mg lm oa ob mi oc bi translated">参考</h2><p id="15ab" class="pw-post-body-paragraph kv kw it kx b ky mm ju la lb mn jx ld le nm lg lh li nn lk ll lm no lo lp lq im bi translated">[1] <a class="ae ku" href="https://arxiv.org/abs/1612.03144" rel="noopener ugc nofollow" target="_blank">用于目标检测的特征金字塔网络</a></p><p id="a208" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[2] <a class="ae ku" href="https://arxiv.org/abs/1803.01534" rel="noopener ugc nofollow" target="_blank">路径聚合网络实例分割</a></p><p id="7629" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[3] <a class="ae ku" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank"> EfficientDet:可扩展且高效的对象检测</a></p><p id="bdf9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[4] <a class="ae ku" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank">密集物体检测的焦点损失</a></p><p id="7bfa" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[5] <a class="ae ku" href="https://arxiv.org/abs/2004.10934" rel="noopener ugc nofollow" target="_blank"> YOLOv4:物体检测的最佳速度和精度</a></p><p id="c50c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[6] <a class="ae ku" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank">单发多盒探测器(SSD) </a></p><p id="76fc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">【7】<a class="ae ku" href="https://arxiv.org/abs/1811.04533" rel="noopener ugc nofollow" target="_blank">基于多级特征金字塔网络的单镜头目标检测器</a></p><p id="e913" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[8] <a class="ae ku" href="https://arxiv.org/abs/1708.04552" rel="noopener ugc nofollow" target="_blank">改进了带有断流器的卷积神经网络的正则化</a></p><p id="ba82" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[9] <a class="ae ku" href="https://arxiv.org/abs/1911.09516" rel="noopener ugc nofollow" target="_blank">学习空间融合用于单次拍摄对象检测</a></p><p id="6da9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[10] <a class="ae ku" href="https://arxiv.org/abs/1708.04896" rel="noopener ugc nofollow" target="_blank">随机擦除数据增强</a></p><p id="8d4b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[11] <a class="ae ku" href="https://arxiv.org/pdf/1902.09630v2.pdf" rel="noopener ugc nofollow" target="_blank">并集上的广义交集:包围盒回归的度量和损失</a></p><p id="979f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[12] <a class="ae ku" href="https://arxiv.org/abs/1608.01471" rel="noopener ugc nofollow" target="_blank"> UnitBox:一个先进的物体探测网络</a></p><p id="da6d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">【13】<a class="ae ku" href="https://arxiv.org/abs/1810.12890" rel="noopener ugc nofollow" target="_blank">drop block:卷积网络的正则化方法</a></p><p id="1f26" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[14] <a class="ae ku" href="https://arxiv.org/abs/1908.08681" rel="noopener ugc nofollow" target="_blank"> Mish:一个自正则化的非单调神经激活函数</a></p><p id="30aa" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[15] <a class="ae ku" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank">压缩和激励网络</a></p><p id="0a0e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[16] <a class="ae ku" href="https://arxiv.org/abs/1807.06521" rel="noopener ugc nofollow" target="_blank"> CBAM:卷积块注意模块</a></p></div></div>    
</body>
</html>