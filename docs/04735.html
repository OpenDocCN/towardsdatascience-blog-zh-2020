<html>
<head>
<title>Feature Engineering for Numerical Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数字数据的特征工程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-engineering-for-numerical-data-e20167ec18?source=collection_archive---------28-----------------------#2020-04-26">https://towardsdatascience.com/feature-engineering-for-numerical-data-e20167ec18?source=collection_archive---------28-----------------------#2020-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="db7d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">工程数值的技巧</h2></div><p id="2375" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数字数据几乎是一种福气。为什么差不多？因为它已经是一种机器学习模型可以接受的格式。然而，如果我们把它翻译成与人类相关的术语，仅仅因为一个博士级别的教科书是用英语写的——我用英语说、读和写——并不意味着我能够很好地理解教科书以获得有用的见解。让教科书对我有用的是，它以一种考虑到我的心理模型假设的方式概括了最重要的信息，比如“数学是一个神话”(顺便说一下，这不再是我的观点，因为我真的开始喜欢它了)。同样，一个好的特征应该代表数据的突出方面，以及机器学习模型所做的假设的形状。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/77337907692ce77bd2182975efbef734.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DOEKjmOU4I2WoNWf"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图 1:一名士兵在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="ac68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">特征工程是从原始数据中提取特征并将其转换为机器学习模型可以接受的格式的过程。通常需要转换来降低建模的难度并提高模型的结果。因此，设计数字数据类型的技术是数据科学家(机器学习工程师等)的基本工具。</p><blockquote class="lv"><p id="f8a1" class="lw lx it bd ly lz ma mb mc md me ld dk translated">“数据就像机器学习中的<em class="mf">原油</em>，这意味着它必须被提炼为<em class="mf">特征</em>——预测变量——才能对训练模型有用。”- <a class="mg mh ep" href="https://medium.com/u/e2f299e30cb9?source=post_page-----e20167ec18--------------------------------" rel="noopener" target="_blank">威尔·科尔森</a></p></blockquote><p id="b858" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">当我们努力掌握时，重要的是要注意，仅仅知道一个机制为什么工作以及它能做什么是远远不够的。精通知道事情是如何完成的，对潜在的原则有直觉，并有神经连接，这使得在面临挑战时绘制正确的工具成为一个无缝的过程。这不会来自于阅读这篇文章，而是来自于有意的实践，这篇文章将通过提供技术背后的直觉为你打开大门，这样你可以理解如何以及何时应用它们。</p><blockquote class="lv"><p id="68ad" class="lw lx it bd ly lz ma mb mc md me ld dk translated">数据中的特征将直接影响您使用的预测模型以及您可以实现的结果。”——<a class="mg mh ep" href="https://medium.com/u/f374d0159316?source=post_page-----e20167ec18--------------------------------" rel="noopener" target="_blank">杰森·布朗利</a></p></blockquote><blockquote class="mn mo mp"><p id="bb11" class="ki kj mq kk b kl mi ju kn ko mj jx kq mr mk kt ku ms ml kx ky mt mm lb lc ld im bi translated">注意:您可以在我的 Github 页面上找到本文使用的代码</p></blockquote><div class="mu mv gp gr mw mx"><a href="https://github.com/kurtispykes/demo/tree/master" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">kurtispykes/演示</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">与中等博客文章相关的演示代码。-路径/到/文件；链接到文章有效的数据可视化…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl lo mx"/></div></div></a></div></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="a1f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可能会有这样的情况，即在累积的要素上收集数据，从而具有无限的上限。这种类型的连续数据的例子可能是一个跟踪系统，该系统监视我的所有媒体帖子每天接收的访问量。这种类型的数据很容易吸引离群值，因为可能会有一些不可预测的事件影响我的文章累积的总流量，例如有一天，人们可能会决定他们希望能够进行数据分析，所以我关于<a class="ae lu" rel="noopener" target="_blank" href="/effective-data-visualization-ef30ae560961">有效数据可视化</a>的文章可能会在那一天出现峰值。换句话说，当数据可以被快速和大量地收集时，那么它很可能包含一些需要工程的极端值。</p><p id="25b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">处理此实例的一些方法有:</p><p id="a38d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">量子化</strong></p><p id="89e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此方法通过将值分组到箱中来包含数据的比例。因此，量化将连续值映射为离散值，从概念上讲，这可以被认为是有序的二进制序列。为了实现这一点，我们必须考虑我们创建的箱的宽度，其解决方案分为两类，固定宽度箱或自适应箱。</p><blockquote class="mn mo mp"><p id="7328" class="ki kj mq kk b kl km ju kn ko kp jx kq mr ks kt ku ms kw kx ky mt la lb lc ld im bi translated">注意:这对于线性模型特别有用，在基于树的模型中这是没有用的(基于树的模型进行它们自己的分割)。</p></blockquote><p id="36a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在固定宽度的情况下，该值是自动或自定义设计的，用于将数据分割到离散的条块中，它们也可以线性缩放或指数缩放。一个流行的例子是以十年为间隔将人的年龄分成几个部分，例如容器 1 包含 0-9 岁，容器 2 包含 10-19 岁，等等。</p><p id="bab2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，如果这些值跨越大量的数字，那么更好的方法可能是将这些值分组为常数的幂，例如 10 的幂:0–9、10–99、100–999、1000–9999。请注意，仓位宽度呈指数增长，因此在 1000–9999 的情况下，仓位宽度为 O(10000)，而 0–9 为 O(10)。获取计数的日志，以从计数映射到数据的 bin。</p><pre class="lf lg lh li gt nt nu nv nw aw nx bi"><span id="6a69" class="ny nz it nu b gy oa ob l oc od">import numpy as np </span><span id="2d6c" class="ny nz it nu b gy oe ob l oc od">#15 random integers from the "discrete uniform" distribution<br/>ages = np.random.randint(0, 100, 15)</span><span id="5924" class="ny nz it nu b gy oe ob l oc od">#evenly spaced bins<br/>ages_binned = np.floor_divide(ages, 10)</span><span id="bd60" class="ny nz it nu b gy oe ob l oc od">print(f"Ages: {ages} \nAges Binned: {ages_binned} \n")<br/>&gt;&gt;&gt; Ages: [97 56 43 73 89 68 67 15 18 36  4 97 72 20 35]<br/>Ages Binned: [9 5 4 7 8 6 6 1 1 3 0 9 7 2 3]</span><span id="4bdc" class="ny nz it nu b gy oe ob l oc od">#numbers spanning several magnitudes<br/>views = [300, 5936, 2, 350, 10000, 743, 2854, 9113, 25, 20000, 160, 683, 7245, 224]</span><span id="b321" class="ny nz it nu b gy oe ob l oc od">#map count -&gt; exponential width bins<br/>views_exponential_bins = np.floor(np.log10(views))</span><span id="5006" class="ny nz it nu b gy oe ob l oc od">print(f"Views: {views} \nViews Binned: {views_exponential_bins}")<br/>&gt;&gt;&gt; Views: [300, 5936, 2, 350, 10000, 743, 2854, 9113, 25, 20000, 160, 683, 7245, 224]<br/>Views Binned: [2. 3. 0. 2. 4. 2. 3. 3. 1. 4. 2. 2. 3. 2.]</span></pre><p id="826d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当计数中有大的间隙时，自适应箱是更好的选择。当计数值之间有大的余量时，一些固定宽度的仓将是空的。</p><p id="f01d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了进行自适应宁滨，我们可以利用数据的分位数，即像中位数一样将数据分成相等部分的值。</p><pre class="lf lg lh li gt nt nu nv nw aw nx bi"><span id="68e4" class="ny nz it nu b gy oa ob l oc od">import pandas as pd</span><span id="cd4a" class="ny nz it nu b gy oe ob l oc od">#map the counts to quantiles (adaptive binning)<br/>views_adaptive_bin = pd.qcut(views, 5, labels=False)</span><span id="33e1" class="ny nz it nu b gy oe ob l oc od">print(f"Adaptive bins: {views_adaptive_bin}")<br/>&gt;&gt;&gt; Adaptive bins: [1 3 0 1 4 2 3 4 0 4 0 2 3 1]</span></pre></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="2a76" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">电力变换</strong></p><p id="7b39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经看到了一个这样的例子…对数变换是方差稳定变换系列的一部分，称为幂变换。维基百科将幂变换描述为一种<em class="mq">“用于稳定方差、使数据更像正态分布、提高关联测量的有效性(如变量之间的皮尔逊相关性)以及用于其他数据稳定程序的技术。”</em></p><p id="3c58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为什么我们要转换数据以符合正态分布？很棒的问题！您可能希望使用参数模型——对数据进行假设的模型——而不是非参数模型。当数据呈正态分布时，参数模型非常强大。然而，在某些情况下，我们拥有的数据可能需要帮助来呈现正态分布的美丽的钟形曲线，例如，数据可能是倾斜的，因此我们应用幂变换来帮助我们的特征看起来更像高斯。</p><p id="6dcd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的代码利用数据科学框架(如 pandas、scipy 和 numpy)来演示电源转换，并使用 Plotly.py 框架将它们可视化以进行交互式绘图。使用的数据集是 Kaggle 的<a class="ae lu" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" rel="noopener ugc nofollow" target="_blank"> <em class="mq">房价:高级回归技术</em> </a>，可以轻松下载(<a class="ae lu" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">点击此处获取数据</strong> </a>)。</p><pre class="lf lg lh li gt nt nu nv nw aw nx bi"><span id="f210" class="ny nz it nu b gy oa ob l oc od">import numpy as np<br/>import pandas as pd<br/>from scipy import stats<br/>import plotly.graph_objects as go<br/>from plotly.subplots import make_subplots</span><span id="f4aa" class="ny nz it nu b gy oe ob l oc od">df = pd.read_csv("../data/raw/train.csv")</span><span id="d548" class="ny nz it nu b gy oe ob l oc od"># applying various transformations<br/>x_log = np.log(df["GrLivArea"].copy()) # log <br/>x_square_root = np.sqrt(df["GrLivArea"].copy()) # square root x_boxcox, _ = stats.boxcox(df["GrLivArea"].copy()) # boxcox<br/>x = df["GrLivArea"].copy() # original data</span><span id="fbaf" class="ny nz it nu b gy oe ob l oc od"># creating the figures<br/>fig = make_subplots(rows=2, cols=2,<br/>                    horizontal_spacing=0.125,<br/>                    vertical_spacing=0.125,<br/>                    subplot_titles=("Original Data",<br/>                                    "Log Transformation",<br/>                                    "Square root transformation",<br/>                                    "Boxcox Transformation")<br/>                    )</span><span id="4dd0" class="ny nz it nu b gy oe ob l oc od"># drawing the plots<br/>fig.add_traces([<br/>                go.Histogram(x=x,<br/>                             hoverinfo="x",<br/>                             showlegend=False),<br/>                go.Histogram(x=x_log,<br/>                             hoverinfo="x",<br/>                             showlegend=False),</span><span id="8bc2" class="ny nz it nu b gy oe ob l oc od">                go.Histogram(x=x_square_root,<br/>                             hoverinfo="x",<br/>                             showlegend=False),</span><span id="0e2e" class="ny nz it nu b gy oe ob l oc od">                go.Histogram(x=x_boxcox,<br/>                             hoverinfo="x",<br/>                             showlegend=False),<br/>               ],<br/>               rows=[1, 1, 2, 2],<br/>               cols=[1, 2, 1, 2]<br/>)</span><span id="b409" class="ny nz it nu b gy oe ob l oc od">fig.update_layout(<br/>    title=dict(<br/>               text="GrLivArea with various Power Transforms",<br/>               font=dict(<br/>                         family="Arial",<br/>                         size=20)),<br/>    showlegend=False,<br/>    width=800,<br/>    height=500)</span><span id="0f8d" class="ny nz it nu b gy oe ob l oc od">fig.show() # display figure</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi of"><img src="../Images/56146e2310f90211af2d244e414b57cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HeCloiMiQxrEG-HZuKnggg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图 2:可视化原始数据和各种电力转换电力转换。</p></figure><blockquote class="mn mo mp"><p id="00ae" class="ki kj mq kk b kl km ju kn ko kp jx kq mr ks kt ku ms kw kx ky mt la lb lc ld im bi translated">注意:Box-cox 变换仅在数据为非负时有效</p></blockquote><blockquote class="lv"><p id="9aae" class="lw lx it bd ly lz og oh oi oj ok ld dk translated">“这些中哪一个最好？你无法事先知道。你必须尝试它们，并评估结果，以实现你的算法和性能指标。”- <a class="mg mh ep" href="https://medium.com/u/f374d0159316?source=post_page-----e20167ec18--------------------------------" rel="noopener" target="_blank">杰森·布朗利</a></p></blockquote></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="7d12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">特征缩放</strong></p><p id="673b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">顾名思义，特征缩放(也称为特征归一化)与改变特征的比例有关。当数据集的要素在比例上有很大差异时，对输入要素比例敏感的模型(即线性回归、逻辑回归、神经网络)会受到影响。确保特征在相似的范围内是必要的。然而，诸如基于树的模型(即决策树、随机森林、梯度增强)之类的模型不关心规模。</p><p id="eeed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">缩放要素的常用方法包括最小-最大缩放、标准化和 L 归一化，接下来是 python 中的简单介绍和实现。</p><p id="aae0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最小-最大缩放</strong> -特征被缩放到一个固定的范围(通常在 0-1 之间)，这意味着我们将减少标准偏差，从而抑制异常值对特征的影响。其中 x 是实例的单个值(即人 1，特征 2)，max(x)，min(x)是特征的最大值和最小值，见图 3。关于这方面的更多信息，请参见<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" rel="noopener ugc nofollow" target="_blank"> sklearn 文档。</a></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ol"><img src="../Images/aaddefbdd48b7ecde7d67b58b0e59bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hrlR_IqI13XjrjlcslbDSw.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图 3:最小-最大缩放的公式</p></figure><p id="55cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">标准化</strong> -将重新调整特征值，使其符合均值为 0、标准差为 1 的正态分布的属性。为此，我们从特征实例值中减去特征的平均值(所有实例的平均值),然后除以方差，见图 4。<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank"> Sklearn 文档</a>用于标准化。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi om"><img src="../Images/577b16c010f311029834dad56162c9c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H47v7SPAuEbXkowfI_QSOA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图 4:标准化公式</p></figure><p id="8b5d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> L 归一化</strong> -这种技术将原始特征值除以 L 范数(也称为欧氏距离)，这是图 5 中的第二个等式。l 范数取所有实例的特征集中的值的平方和。L 范数的 Sklearn <a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer" rel="noopener ugc nofollow" target="_blank">文档</a>(注意，也可以通过将<code class="fe on oo op nu b">norm</code>参数设置为<code class="fe on oo op nu b">"l1"</code>来进行 L 归一化)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oq"><img src="../Images/9d12c6cc5d04ca473812c5eec128e057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fRRWqzBlDcTJnGMALqkYWQ.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图 5:L 归一化的公式</p></figure><blockquote class="mn mo mp"><p id="a8f4" class="ki kj mq kk b kl km ju kn ko kp jx kq mr ks kt ku ms kw kx ky mt la lb lc ld im bi translated">特征缩放效果的可视化将给出正在发生的事情的更好的图像。为此，我使用可以从 sklearn 数据集导入的葡萄酒数据集。</p></blockquote><pre class="lf lg lh li gt nt nu nv nw aw nx bi"><span id="300f" class="ny nz it nu b gy oa ob l oc od">import pandas as pd<br/>from sklearn.datasets import load_wine<br/>from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer<br/>import plotly.graph_objects as go</span><span id="c9f1" class="ny nz it nu b gy oe ob l oc od">wine_json= load_wine() # load in dataset</span><span id="42b6" class="ny nz it nu b gy oe ob l oc od">df = pd.DataFrame(data=wine_json["data"], columns=wine_json["feature_names"]) # create pandas dataframe</span><span id="e4d1" class="ny nz it nu b gy oe ob l oc od">df["Target"] = wine_json["target"] # created new column and added target labels</span><span id="4e41" class="ny nz it nu b gy oe ob l oc od"># standardization<br/>std_scaler = StandardScaler().fit(df[["alcohol", "malic_acid"]])<br/>df_std = std_scaler.transform(df[["alcohol", "malic_acid"]])</span><span id="6914" class="ny nz it nu b gy oe ob l oc od"># minmax scaling<br/>minmax_scaler = MinMaxScaler().fit(df[["alcohol", "malic_acid"]])<br/>df_minmax = minmax_scaler.transform(df[["alcohol", "malic_acid"]])</span><span id="5641" class="ny nz it nu b gy oe ob l oc od"># l2 normalization<br/>l2norm = Normalizer().fit(df[["alcohol", "malic_acid"]])<br/>df_l2norm = l2norm.transform(df[["alcohol", "malic_acid"]])</span><span id="ef4b" class="ny nz it nu b gy oe ob l oc od"># creating traces</span><span id="9600" class="ny nz it nu b gy oe ob l oc od">trace1 = go.Scatter(x= df_std[:, 0],<br/>                    y= df_std[:, 1],<br/>                    mode= "markers",<br/>                    name= "Standardized Scale")</span><span id="d0c0" class="ny nz it nu b gy oe ob l oc od">trace2 = go.Scatter(x= df_minmax[:, 0],<br/>                    y= df_minmax[:, 1],<br/>                    mode= "markers",<br/>                    name= "MinMax Scale")</span><span id="8fff" class="ny nz it nu b gy oe ob l oc od">trace3 = go.Scatter(x= df_l2norm[:, 0],<br/>                    y= df_l2norm[:, 1],<br/>                    mode= "markers",<br/>                    name= "L2 Norm Scale")</span><span id="7833" class="ny nz it nu b gy oe ob l oc od">trace4 = go.Scatter(x= df["alcohol"],<br/>                    y= df["malic_acid"],<br/>                    mode= "markers",<br/>                    name= "Original Scale")</span><span id="f19b" class="ny nz it nu b gy oe ob l oc od">layout = go.Layout(<br/>         title= "Effects of Feature scaling",<br/>         xaxis=dict(title= "Alcohol"),<br/>         yaxis=dict(title= "Malic Acid")<br/>         )</span><span id="b351" class="ny nz it nu b gy oe ob l oc od">data = [trace1, trace2, trace3, trace4]<br/>fig = go.Figure(data=data, layout=layout)<br/>fig.show()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi or"><img src="../Images/2541cd52e2098d09b7fed5b2cbc6dae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kWQgQtwiANDHijvXsk-s8w.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图 6:原始特性和各种缩放实现的曲线图。</p></figure><p id="b1bd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">功能交互</strong></p><p id="84e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过使用特征之间成对交互的乘积来创建逻辑 AND 函数。在基于树的模型中，这些交互是隐式发生的，但是在假设特征独立的模型中，我们可以显式地声明特征之间的交互以改进模型的输出。</p><p id="4a79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑一个简单的线性模型，该模型使用输入要素的线性组合来预测输出 y:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi os"><img src="../Images/21e41afbdc3e3035722ae671a024bf39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LpGyRwI4s8Kla5n7bUPWGA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图 7:线性模型的公式</p></figure><p id="dd00" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以扩展线性模型来捕捉特征之间发生的交互。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ot"><img src="../Images/5962c4c621be1fb3246c8939e25ba58d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZO7RIvVdMp6EXCLoESBJDQ.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图 8:扩展线性模型</p></figure><blockquote class="mn mo mp"><p id="8f04" class="ki kj mq kk b kl km ju kn ko kp jx kq mr ks kt ku ms kw kx ky mt la lb lc ld im bi translated">注意:使用线性函数是昂贵的，并且具有成对交互的线性模型的评分和训练将从 O(n)到 O(n)。但是，您可以执行特征提取来克服这个问题(特征提取超出了本文的范围，但是我将在以后的文章中讨论)。</p></blockquote><p id="35f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们用 python 来编码这个，我将利用 scitkit-learn <code class="fe on oo op nu b">PolynomialFeatures</code>类，你可以在<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html" rel="noopener ugc nofollow" target="_blank">文档</a>中读到更多关于它的内容:</p><pre class="lf lg lh li gt nt nu nv nw aw nx bi"><span id="2fde" class="ny nz it nu b gy oa ob l oc od">import numpy as np<br/>from sklearn.preprocessing import PolynomialFeatures</span><span id="0c0b" class="ny nz it nu b gy oe ob l oc od"># creating dummy dataset<br/>X = np.arange(10).reshape(5, 2)<br/>X.shape<br/>&gt;&gt;&gt; (5, 2)</span><span id="431b" class="ny nz it nu b gy oe ob l oc od"># interactions between features only<br/>interactions = PolynomialFeatures(interaction_only=True)<br/>X_interactions= interactions.fit_transform(X)<br/>X_interactions.shape<br/>&gt;&gt;&gt; (5, 4)</span><span id="ac90" class="ny nz it nu b gy oe ob l oc od"># polynomial features <br/>polynomial = PolynomialFeatures(5)<br/>X_poly = polynomial.fit_transform(X)<br/>X_poly.shape<br/>&gt;&gt;&gt; (5, 6)</span></pre><blockquote class="mn mo mp"><p id="41f4" class="ki kj mq kk b kl km ju kn ko kp jx kq mr ks kt ku ms kw kx ky mt la lb lc ld im bi translated">这篇文章很大程度上受到了《机器学习的特征工程:数据科学家的原则和技术》这本书的启发，我强烈推荐阅读这本书。虽然它是在 2016 年出版的，但它仍然信息量很大，解释清楚，即使对那些没有数学背景的人来说也是如此。</p></blockquote><p id="51d0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结论</strong></p><p id="3746" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们讨论了处理数字特征的技术，如量化、幂变换、特征缩放和交互特征(可应用于各种数据类型)。这绝不是特性工程的全部，每天都有更多的东西需要学习。特征工程是一门艺术，需要实践，所以现在你有了直觉，你就可以开始实践了。您可以在我的 Github 上找到本文中使用的代码(链接如下)。非常感谢您的宝贵时间！</p><div class="mu mv gp gr mw mx"><a href="https://github.com/kurtispykes/demo/tree/master" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">kurtispykes/演示</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">与中等博客文章相关的演示代码。-路径/到/文件；链接到文章有效的数据可视化…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl lo mx"/></div></div></a></div><p id="9c73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">其他资源……</strong></p><p id="b59e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="mg mh ep" href="https://medium.com/u/f374d0159316?source=post_page-----e20167ec18--------------------------------" rel="noopener" target="_blank"> Jason Brownlee </a> - <a class="ae lu" href="https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/" rel="noopener ugc nofollow" target="_blank">探索特性工程，如何设计特性以及如何做好它</a></p><p id="db0d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="mg mh ep" href="https://medium.com/u/f374d0159316?source=post_page-----e20167ec18--------------------------------" rel="noopener" target="_blank">杰森·布朗利</a> - <a class="ae lu" href="https://machinelearningmastery.com/how-to-transform-data-to-fit-the-normal-distribution/" rel="noopener ugc nofollow" target="_blank">如何将数据更好地拟合正态分布</a></p><p id="23af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="mg mh ep" href="https://medium.com/u/c6b0b560a0c4?source=post_page-----e20167ec18--------------------------------" rel="noopener" target="_blank">Emre ren bero Lu</a>-<a class="ae lu" rel="noopener" target="_blank" href="/feature-engineering-for-machine-learning-3a5e293a5114">机器学习的特征工程基础技术</a></p><p id="b793" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">黛博拉·拉姆齐- <a class="ae lu" href="https://www.dummies.com/education/math/statistics/types-of-statistical-data-numerical-categorical-and-ordinal/" rel="noopener ugc nofollow" target="_blank">统计数据的类型:数字、分类和顺序</a></p><p id="da30" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Alice Zheng &amp; Amanda Casari - <a class="ae lu" href="https://www.amazon.co.uk/Feature-Engineering-Machine-Learning-Principles-ebook/dp/B07BNX4MWC" rel="noopener ugc nofollow" target="_blank">机器学习的特征工程:数据科学家的原则和技术</a></p></div></div>    
</body>
</html>