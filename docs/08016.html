<html>
<head>
<title>Chat Images to Textual Conversation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聊天图像到文本对话</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/chat-images-to-textual-conversation-c44aadef59fe?source=collection_archive---------41-----------------------#2020-06-13">https://towardsdatascience.com/chat-images-to-textual-conversation-c44aadef59fe?source=collection_archive---------41-----------------------#2020-06-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2160" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用基于深度学习的光学字符识别(Tesseract 4)从聊天图像中提取JSON格式的消息</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/baee01a84b6a5858ffdfbc3373e8ff70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lDfVSDqLCS8KxL7k"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae kv" href="https://unsplash.com/@rachitank?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Rachit坦克</a>拍摄</p></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="764f" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最近，社交媒体上流传着许多聊天图像，可能是与客户服务的聊天图像，或者只是来自WhatsApp或脸书的有趣聊天图像，也可能是任何重要的群体对话。这些数据在互联网上随处可见，却没有得到很好的利用。</p><p id="7035" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">Messagink向我介绍了这个问题的一个有趣的用例。任务是将聊天图像中的信息提取成JSON格式。所以，我想为什么不探索这个领域，看看是否能在这个特定的领域有所成就。</p><p id="c980" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我开始的任务是接收聊天图像并从中提取JSON对象。这个看似简单的任务包含了许多错综复杂的问题。每一个都要求我迭代我的方法，并使用机器学习概念来解决它们。让我们详细研究一下这些问题。</p><p id="39d8" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">首先，我必须找到一个好的库，能够以非常高的精度检测图像中的文本。我搜索了很多，通读了很多库的利弊，最终敲定了谷歌的tesseract库(从2006年开始由谷歌赞助，在此之前由惠普实验室开发)。传统宇宙魔方是一个多阶段的过程。首先使用阈值对图像进行二值化，随后确定连接的部分和它们之间的连接，字符分类和聚集字符以形成单词、行、段落。更多细节可以在这篇<a class="ae kv" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/33418.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中读到。</p><p id="3364" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">2018年10月，他们推出了tesseract 4，该产品在底层使用双向LSTMs来完成OCR任务。此后，他们进行了多次改进，最新版本于2019年12月26日发布。要了解更多关于宇宙魔方4如何使用深度神经网络的信息，请阅读这里的<a class="ae kv" href="https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/6ModernizationEfforts.pdf" rel="noopener ugc nofollow" target="_blank"/>。我们将在所有任务中使用宇宙魔方4。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/0bc944af0d8cc0b27c0c2b9d9ae4ed45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*_2nsdmjpH6VsGp1imcJJqQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">宇宙魔方(谷歌照片)</p></figure><p id="5066" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最初，我只是输入一些聊天图像来吐槽文字。我用“宇宙魔方”包装宇宙魔方，因为使用它的结果是精确和准确的。我从图像中获取文本，但最难的部分是从这些文本中获取一些意义。例如，如果是两个人之间的聊天，我们如何分离文本信息，我们如何从图像中删除不想要的文本，我们如何检测信息的时间。所以这是接下来需要关注的几个挑战。</p><p id="8007" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了能够解决所有这些问题，主要问题是检测文本行周围的边界框。下图显示了文本周围的红色边框</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/f7370b33f4d8dd3c2b9b55e498b43ab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*TyBXLYB9khbZ8zo9OwTO6Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">红色边框(作者照片)</p></figure><p id="7b98" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">理想情况下，我想检测每个消息周围的边界框，但很难找到这样的工具。而“pytesseract”正在检测字符周围的单个边界框，这不符合我的目的。我在tesseract周围找到了另一个包装器“tesserocr ”,它展示了检测每行文本周围的边界框的功能，但是“tesserocr”的文本识别不如pytesseract准确，因此我决定使用tesserocr进行边界框检测，并使用pytesseract的image_to_string函数来识别该边界框中的文本。下面的代码片段显示了tesserocr和pytesseract的基本功能</p><pre class="kg kh ki kj gt mb mc md me aw mf bi"><span id="6534" class="mg mh iq mc b gy mi mj l mk ml">final = cv2.imread(img, cv2.IMREAD_UNCHANGED)<br/># since tesserocr accepts PIL images, converting opencv image to pil<br/>pil_img = Image.fromarray(cv2.cvtColor(final, cv2.COLOR_BGR2RGB))<br/>#initialize api<br/>api = tesserocr.PyTessBaseAPI()</span><span id="57aa" class="mg mh iq mc b gy mm mj l mk ml">try:</span><span id="1b46" class="mg mh iq mc b gy mm mj l mk ml"> # set pil image for ocr<br/> api.SetImage(pil_img)</span><span id="11e8" class="mg mh iq mc b gy mm mj l mk ml"> boxes = api.GetComponentImages(tesserocr.RIL.TEXTLINE,True)</span><span id="30f1" class="mg mh iq mc b gy mm mj l mk ml"> height, width, c = final.shape<br/> print(height, width)<br/> inc = int(0.01*width)</span><span id="4416" class="mg mh iq mc b gy mm mj l mk ml"> for i, (im,box,_,_) in enumerate(boxes):</span><span id="fcf3" class="mg mh iq mc b gy mm mj l mk ml">   x,y,w,h = box['x'],box['y'],box['w'],box['h']<br/>  <br/>   crop_img = final[max(0, y-inc):min(y+h+inc,height), max(0, x-inc):min(x+w+inc, width)]<br/>  <br/>   cv2_imshow(crop_img)<br/>   cv2.waitKey(0)<br/>   extractedInformation = pytesseract.image_to_string(crop_img)<br/>  <strong class="mc ir"> </strong>print(extractedInformation)</span></pre><p id="755b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最后，我们有了边界框和文本。现在必须好好利用这些边界框。考虑两个人聊天，第一个主要任务是准确分离他们之间的消息。我只是使用了每个边界框的起始边缘的X坐标，并将其与阈值进行了比较。阈值根据图像的宽度决定，以处理不同分辨率的图像。所以我希望threshold是这样一个值，即左边消息的边界框起始边缘的X坐标小于threshold，而右边消息的边界框起始边缘的X坐标大于threshold。在对whatsapp图像进行大量实验后，发现将阈值保持为图像宽度的10%比较合适。这有助于隔离两个人之间的信息。下图显示了阈值如何映射到图像上。<strong class="lf ir">蓝点表示阈值，而红框是文本上的边界框。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/3fbc96811b941eff35fbc1a404def4b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*c7Mx8PioxFlQvvz5BvqNAQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">隔离邮件的阈值(蓝色)(作者提供图片)</p></figure><p id="fc65" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">因为“tesserocr”每行检测一个边界框，所以长于一行的消息被分成多行。我不得不把属于同一信息的所有行整理在一起。文本周围的边框在这里也扮演了重要的角色。所以我注意到，同一封邮件中的边框的Y坐标之间的差异小于不同邮件中的边框。因此，我们可以再次使用一个阈值来决定是否对文本行进行排序。对不同分辨率的whatsapp图像进行实验，结果表明使用阈值作为图像宽度的1%是最佳选择。在下图中，你可以看到同一封邮件中的行是如何足够近，而不同邮件中的行又是如何的远。<strong class="lf ir">绿色注释显示两个不同消息边界框之间的Y坐标差异，蓝色注释显示同一消息各行之间的Y坐标差异。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/9ce6d535e3b57cb2b71c167e24772610.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*kks5_hZmISf40-Uq_oUbkw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">将文本行整理成消息的逻辑(作者提供照片)</p></figure><p id="712e" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">不错！所以现在我们把文本行整理成信息，并在两个人之间分开。输出的下一个问题是在图像的页眉或页脚检测到了不必要的噪声。例如，在whatsapp聊天图像中，如果全屏拍摄，会在图像的顶部或底部检测到一些东西，如下所示。现在困难的部分是我们如何知道图像是裁剪过的还是完整的图像。我试着从字体大小中找出标题，或者试着把名字映射到头像图片附近，但是都没有给出有希望的结果。然后，我突然想到，来自同一来源的图像会有完全相同的模板。也就是说，如果截图来自特定人的IOS设备，它们将具有相同的标题和底部模板。我肯定可以使用图像减法来检测差异，并只取包含所有差异的最大边界框。但是实现比这个想法更难，我使用opencv的功能来提取这样一个边界框。我将使用来自同一来源的2个聊天截图来解释以下过程:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/9710e63e0dc552c124b446ac29de56f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*oa0j2wVcl1EOLG5fjGVUWQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">左:图片1，右:图片2(作者照片)</p></figure><ul class=""><li id="b0f7" class="mq mr iq lf b lg lh lj lk lm ms lq mt lu mu ly mv mw mx my bi translated">首先，我们逐像素地减去两幅图像。这有助于突出两幅图像的差异。然后，我们将这些图像的大小调整为1/4，以加快处理速度。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/9761bae26955c9f77e868cf687f925d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*PTTXx_GGZJ0ppOvF_ZRecw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">两幅图像的差异(作者提供照片)</p></figure><ul class=""><li id="e3aa" class="mq mr iq lf b lg lh lj lk lm ms lq mt lu mu ly mv mw mx my bi translated">然后我们执行形态学闭运算。它在关闭前景对象内的小孔或对象上的小黑点时很有用。从而允许将附近的差异聚集在一起。我们执行20次闭合操作迭代两次，以完全聚集差异。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/8f2999d2a47f782de370684ef2562332.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*6yQ2xY4YIGrdK_i-SeRP6Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">形态学封闭操作:先扩张后侵蚀(作者摄影)</p></figure><ul class=""><li id="1474" class="mq mr iq lf b lg lh lj lk lm ms lq mt lu mu ly mv mw mx my bi translated">然后，我们对从步骤2获得的结果应用阈值来加强差异。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/7629a18f81b001a1170701f0a861b942.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*p3EFw4Hf6LVrPF6_5w4G0A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用阈值像素值20进行阈值处理(图片由作者提供)</p></figure><ul class=""><li id="2925" class="mq mr iq lf b lg lh lj lk lm ms lq mt lu mu ly mv mw mx my bi translated">之后，我们从结果图像中提取连接的组件，并在每个连接的组件周围绘制实体边界框。这会使图像中被主要差异包围的整个区域变亮。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/e439346ab71b31b97ac220e7c71dfc79.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*xSVWw3EH3oXCVQRLsHbf0A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">连接组件分析后的输出(作者提供照片)</p></figure><ul class=""><li id="c9de" class="mq mr iq lf b lg lh lj lk lm ms lq mt lu mu ly mv mw mx my bi translated">在此之后，我们运行最终的连接组件分析，以提取每个区域的边界框，在我们的情况下，我们希望提取中间的白色大框，因为这将有实际的聊天内容。</li><li id="a5e4" class="mq mr iq lf b lg na lj nb lm nc lq nd lu ne ly mv mw mx my bi translated">我们裁剪从图像中获得的第一个区域，因为它将包含人们之间的实际聊天，消除图像顶部或底部的任何噪声。在我们的例子中，结果如下:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/8833226accb7323db078169d3ec862cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*3MNPJOnWqZTbPzVEzWnyhg.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">左:裁剪后的图像1，右:裁剪后的图像2(作者提供的照片)</p></figure><p id="f46b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">因此，最后我们能够裁剪图像，如果任何页眉或页脚存在。下一步是消除从边框中识别文本时遇到的一些噪声。在某些行中,“pytesseract”也探测到了时间，它被附加在文本的末尾。我们不得不把这段时间从文本中删除。<strong class="lf ir"> <em class="nf">我们使用12小时和24小时格式的时间正则表达式来检测文本中的时间，并相应地将其去除。</em>T3】</strong></p><p id="8a15" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在执行了所有这些步骤之后，算法就准备好了，并且对测试图像给出了良好的结果。我们在messagink.com部署了它，你可以在下面的网站上看到它正在工作。(您必须创建一个故事并选择导入选项。)</p><div class="ng nh gp gr ni nj"><a href="https://messagink.com/" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">Messagink -讲述你的故事</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">发现有趣的故事。使用我们的编辑器创建您自己的文本故事。免费发布。</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">messagink.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx kp nj"/></div></div></a></div><p id="c589" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然而，当我们开始在这上面扔随机图像时，出现了几个问题。我们的目标是尽快解决下面列出的所有这些问题:</p><ol class=""><li id="4db4" class="mq mr iq lf b lg lh lj lk lm ms lq mt lu mu ly ny mw mx my bi translated">脸书/电报图像—除了消息之外，他们还有头像，所以我们必须相应地改变我们的X轴阈值。</li><li id="1418" class="mq mr iq lf b lg na lj nb lm nc lq nd lu ne ly ny mw mx my bi translated">启用黑暗模式的截图效果不佳。</li><li id="9573" class="mq mr iq lf b lg na lj nb lm nc lq nd lu ne ly ny mw mx my bi translated">表情符号检测是一个巨大的挑战，这是我们接下来要做的。</li><li id="da43" class="mq mr iq lf b lg na lj nb lm nc lq nd lu ne ly ny mw mx my bi translated">需要处理来自连续图片的消息之间的重叠。</li><li id="ac22" class="mq mr iq lf b lg na lj nb lm nc lq nd lu ne ly ny mw mx my bi translated">WhatsApp图片需要去除蓝色勾号。</li></ol><p id="f629" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我将在下一篇博客中公布我找到的解决这些问题的方法。让我开始黑这些，回头见…</p><p id="0a50" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">感谢<a class="nz oa ep" href="https://medium.com/u/bd8d094402aa?source=post_page-----c44aadef59fe--------------------------------" rel="noopener" target="_blank"> Akshat Sharma </a>在这项工作中的合作:)！</p><p id="3fb4" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">更新:我也发表了博客的第二部分，请在这里阅读:</p><div class="ng nh gp gr ni nj"><a rel="noopener follow" target="_blank" href="/chat-images-to-textual-conversation-part-2-8260c09a032e"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">聊天图像到文本对话:第2部分</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">所以我又来了，第二部分是从聊天图像到文本对话。如果您还没有阅读第1部分，我…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">towardsdatascience.com</p></div></div><div class="ns l"><div class="ob l nu nv nw ns nx kp nj"/></div></div></a></div></div></div>    
</body>
</html>