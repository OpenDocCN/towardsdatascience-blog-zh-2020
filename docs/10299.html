<html>
<head>
<title>Measuring social distance in the time of Covid-19</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">新冠肺炎时代的社会距离测量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/measuring-social-distance-in-the-time-of-covid-19-da0503717a62?source=collection_archive---------27-----------------------#2020-07-20">https://towardsdatascience.com/measuring-social-distance-in-the-time-of-covid-19-da0503717a62?source=collection_archive---------27-----------------------#2020-07-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6c95" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 TensorFlow 对象检测 API 来检测行人并计算他们之间的“社交距离”。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3a9de065efb5d158abddd75cd898d6e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4TbSdKWejM2r4WSlZNfI8g.png"/></div></div></figure><p id="0147" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">不幸的是，今天每个人都很熟悉这个术语<em class="lq">、【社会距离】、</em>。在一切恢复正常之前，我们将不得不忍受一段时间。在<a class="ae lr" href="https://immune.institute/en/?utm_campaign=IMMUNE&amp;utm_source=Embajador" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">免疫技术研究所</strong> </a>我们尝试使用<strong class="kw iu"> TensorFlow 对象检测 API </strong>开发一个应用程序，用于识别和测量行人之间的社交距离。</p><h2 id="07fe" class="ls lt it bd lu lv lw dn lx ly lz dp ma ld mb mc md lh me mf mg ll mh mi mj mk bi translated">等等…什么是 TensorFlow 物体检测 API？</h2><p id="90f3" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated"><strong class="kw iu"> TensorFlow 对象检测 API </strong>是用于创建深度学习网络的框架，该网络解决<strong class="kw iu">对象检测</strong>问题。它包含一些在不同数据集上训练的预训练模型，可用于推理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/f42b74788f4d6fccf274b7c65b05933f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*X6Lm9b8_SRsCzFkV.jpeg"/></div></div></figure><p id="5e14" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，我们可以使用该框架将<strong class="kw iu">迁移学习</strong>应用于之前在大型数据集上训练过的预训练模型，这使我们能够为特定任务定制这些模型。例如，我们可以应用迁移学习来训练一个识别一个人是否戴着面具的模型。</p><p id="42bb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">用于图像分类的迁移学习背后的直觉<a class="ae lr" href="https://www.tensorflow.org/tutorials/images/transfer_learning" rel="noopener ugc nofollow" target="_blank">是，如果一个模型是在一个足够大且通用的数据集上训练的，那么这个模型将有效地充当视觉世界的通用模型。然后，您可以利用这些学习到的要素地图，而不必通过在大型数据集上训练大型模型来从头开始。</a></p><p id="f1c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这种情况下，我们不需要应用迁移学习，因为我们想要识别行人，并且已经有一些模型被训练来推断这一点。我们使用的模型<strong class="kw iu">SSD _ mobilenet _ v2 _ coco _ 2018 _ 03 _ 29</strong>已经针对这些对象进行了培训:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">型号 ssd 的类别 _mobilenet_v2_coco_2018_03_29</p></figure><p id="f1af" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于这个用例，我们只需要识别和显示<strong class="kw iu">行人</strong>，因此我们将创建一个按标签过滤预测的函数，我们将只显示<code class="fe mx my mz na b">person (id=1)</code>标签。</p><h2 id="2fa0" class="ls lt it bd lu lv lw dn lx ly lz dp ma ld mb mc md lh me mf mg ll mh mi mj mk bi translated">走吧…一些代码！</h2><p id="8422" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">当我开发这段代码的时候，<strong class="kw iu"> TensorFlow 对象检测 API </strong>还没有完全支持<strong class="kw iu"> TensorFlow 2 </strong>，但是 7 月 10 日 Google 发布了一个新版本，开发了对一些新功能的支持。在这种情况下，我使用了<strong class="kw iu"> TensorFlow 1 </strong>和<strong class="kw iu"> TF 对象检测 API </strong>的<strong class="kw iu"> r1.13.0 </strong>版本以及 Google Colab 的所有容量来进行这个实验。</p><p id="2652" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可以在我的<a class="ae lr" href="https://github.com/alejandrods/Social-Distance-Using-TensorFlow-API-Object" rel="noopener ugc nofollow" target="_blank"> GitHub </a>里找到代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb ms l"/></div></figure><p id="79de" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，我们需要在我们的<strong class="kw iu">驱动器</strong>中创建我们的工作目录<code class="fe mx my mz na b">Projects/Pedestrian_Detection</code>，在那里我们将克隆<strong class="kw iu"> TensorFlow 对象检测 API </strong>库。然后，我们将能够用 Google Colab 启动一个笔记本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/0a06f0ed03b1fe152bd6bb56b228fcb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0YsPZGrOb14vQAeP.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">在项目/行人检测中创建一个 Google Colab</p></figure><p id="33c5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，我们需要将我们的驱动器安装到我们的笔记本电脑中，我们只需要按照说明进行操作。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">在 Google Colab 中挂载驱动器映像</p></figure><p id="a82d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，我们将能够把<code class="fe mx my mz na b">path</code>改变到我们的根文件夹<code class="fe mx my mz na b">Projects/Pedestrian_Detection</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">更改我们根文件夹的路径</p></figure><p id="fd27" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们可以克隆<strong class="kw iu"> TF Object Detection </strong>的库，在本例中是 release<a class="ae lr" href="https://github.com/tensorflow/models/tree/r1.13.0" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu">r 1 . 13 . 0</strong></a><strong class="kw iu">。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="8069" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">我为什么要使用这个版本？</strong>😄基本上，因为我已经测试了一些其他版本，但我发现了一些错误，这是我测试的第一个版本，它工作得很好。</p><p id="0a71" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦你克隆了这个库，你会在目录<code class="fe mx my mz na b">Projects/Pedestrian_Detection</code>中看到一个名为<code class="fe mx my mz na b">Models</code>的文件夹。</p><h2 id="37e3" class="ls lt it bd lu lv lw dn lx ly lz dp ma ld mb mc md lh me mf mg ll mh mi mj mk bi translated">现在有趣的部分开始了！！</h2><p id="5f4d" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">来自 TensorFlow 的家伙都是非常好的人，他们为我们提供了一个笔记本，让我们带您了解使用预先训练的模型来检测图像中的对象的过程(您可以在这里找到:<code class="fe mx my mz na b">Projects/Pedestrian_Detection/models/research/object_detection/colab_tutorials)</code>)。但是，我们将创建自己的笔记本，因为我们将学习如何实现新的可视化功能。</p><p id="8573" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">转到 Drive 中的<code class="fe mx my mz na b">Projects/Pedestrian_Detection/models/research/object_detection</code>,在这个目录中创建一个新的 Google Colab。</p><p id="ec2b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先是将<strong class="kw iu"> TensorFlow 版本</strong>设置为<strong class="kw iu"> 1.x. </strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="6407" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们之前所做的，我们应该将我们的驱动器映像装载到 Google Colab 中，并转到<code class="fe mx my mz na b">Projects/Pedestrian_Detection/models/research/</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">安装驱动</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">正在将路径更改为。/模型/研究</p></figure><p id="2e4c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将需要安装一些库，如<code class="fe mx my mz na b">Cython, contextlib2, pillow, lxml, matplotlib, pycocotools</code>和<code class="fe mx my mz na b">protocbuf compiler</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">安装库</p></figure><p id="e01c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们应该运行<code class="fe mx my mz na b">protoc compiler</code>，这是一个用于序列化结构数据的 Google 库，由<strong class="kw iu"> TensorFlow </strong>使用，可以想象为 XML，但是更小、更快、更简单。</p><p id="e5cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">协议</strong>已经安装在 Google Colab 中，但是如果你正在使用你自己的机器，你必须遵循<a class="ae lr" href="https://github.com/protocolbuffers/protobuf" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> <em class="lq">这些说明。</em> </strong> </a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">运行协议库</p></figure><p id="b176" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">等等……协议做了什么？基本上，<code class="fe mx my mz na b">protoc</code>已经为<code class="fe mx my mz na b">/models/research/object_detection/protos</code>中的每个文件生成了一个 python 脚本。</p><p id="dec4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">太好了！！我们离✌️.更近了一点下一步非常简单，因为我们需要设置<code class="fe mx my mz na b">PYTHONPATH</code>并使用<code class="fe mx my mz na b">setup.py.</code>安装包</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">设置环境变量和测试 TensorFlow</p></figure><p id="45bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，您应该会看到类似这样的内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/230d60e13a723410774bbc8494f195f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHGz4G5X3ZjVrOM1KLuK8Q.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ne ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">当您意识到一切都已正确安装时…</p></figure><p id="a3da" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">等一下……我们需要一个模特，对吗？ <strong class="kw iu"> Yeep！</strong>我们需要下载一个预先训练好的模型，在这种情况下我选择了这个:<strong class="kw iu">SSD _ mobilenet _ v2 _ coco _ 2018 _ 03 _ 29</strong>因为它速度快(在一个实时应用中很重要)，而且准确性很高。但是，您可以探索其他型号<a class="ae lr" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md" rel="noopener ugc nofollow" target="_blank">的性能，并为您的应用选择正确的型号。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="776b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果一切正常，您将能够使用<strong class="kw iu"> TensorFlow 对象检测 API。</strong>下一步是导入我们将要使用的库。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">导入库</p></figure><p id="1fbc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">目前，我们能够加载我们预训练的模型，我们只需要定义<code class="fe mx my mz na b">PATH_TO_FROZEN_GRAPH</code>，它是我们模型的路径，以及<code class="fe mx my mz na b">PATH_TO_LABELS</code>，它是标签的路径。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">装载图</p></figure><p id="d251" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">标签索引映射到类别名称</strong>，这样当我们的卷积网络预测<code class="fe mx my mz na b">1</code>时，我们就知道这对应的是<code class="fe mx my mz na b">person</code>。这里我们使用内部实用函数，但是任何返回将整数映射到适当字符串标签的字典的函数都可以。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div></figure><h2 id="089d" class="ls lt it bd lu lv lw dn lx ly lz dp ma ld mb mc md lh me mf mg ll mh mi mj mk bi translated">沉住气…最重要的功能！</h2><p id="b6db" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">函数<code class="fe mx my mz na b">run_inference_for_single_image</code>将图像和我们的模型作为参数，为我们的图像运行推理。该函数返回一个字典<code class="fe mx my mz na b">output_dict</code>，其中包含图像中检测到的每个对象的坐标和标签。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">为单个图像运行我们的模型</p></figure><p id="8236" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们之前说过的，我们的模型将要预测任何包含在<code class="fe mx my mz na b">category_index</code>中的物体。但是，我们只想特别显示一个类别:<code class="fe mx my mz na b">person</code>。因此，我们创建了一个函数，根据一个<strong class="kw iu"> <em class="lq">、</em> </strong> <code class="fe mx my mz na b">min_score</code>和<strong class="kw iu"> <em class="lq">标签 id </em>、</strong>、<code class="fe mx my mz na b">categories</code>来过滤我们的预测。在这种情况下，<strong class="kw iu">人员标签</strong>的<strong class="kw iu"> id </strong>为<code class="fe mx my mz na b">1</code>。(这个我们可以在<code class="fe mx my mz na b">category_index</code>查一下)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">用于过滤预测的函数</p></figure><h2 id="f919" class="ls lt it bd lu lv lw dn lx ly lz dp ma ld mb mc md lh me mf mg ll mh mi mj mk bi translated">测量盒子之间的距离！🖼️</h2><p id="f1d1" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">我们已经有一个运行推理的模型，它返回一个带有预测的字典<code class="fe mx my mz na b">output_dict</code>。我们想要测量这些预测对象之间的距离，但这不是一个简单的解决方案。因此，我们创建了一些函数来计算每个预测盒的<strong class="kw iu">质心</strong>之间的距离。这些是步骤:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/31148696de1116d8520e39e9a71f0dec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nV749oNHAeKWowUrSBsO3w.jpeg"/></div></div></figure><ul class=""><li id="142c" class="ng nh it kw b kx ky la lb ld ni lh nj ll nk lp nl nm nn no bi translated">使用函数<code class="fe mx my mz na b">calculate_coord</code>获取每个对象的坐标。</li><li id="da2e" class="ng nh it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">计算每个盒子的质心— <code class="fe mx my mz na b">calculate_centr</code>。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/dd38f6fba62170240116badcf92c74a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ofphZZWhUjTYOAoQG88DTA.jpeg"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">质心之间的排列</p></figure><ul class=""><li id="d5ba" class="ng nh it kw b kx ky la lb ld ni lh nj ll nk lp nl nm nn no bi translated">我们应该使用<code class="fe mx my mz na b">calculate_perm</code>计算质心之间所有可能的排列。</li><li id="aaf7" class="ng nh it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">用此函数<code class="fe mx my mz na b">calculate_centr_distance</code>计算每个质心之间的距离(如<code class="fe mx my mz na b">person A</code>和<code class="fe mx my mz na b">person B</code>)。</li><li id="2e3f" class="ng nh it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">最后，我们计算每个线段的中点，以便在图像中以文本形式显示距离— <code class="fe mx my mz na b">midpoint</code>和<code class="fe mx my mz na b">calculate_slope</code>。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">显示和计算社交距离的功能</p></figure><h2 id="7fc5" class="ls lt it bd lu lv lw dn lx ly lz dp ma ld mb mc md lh me mf mg ll mh mi mj mk bi translated">让我们把所有的东西放在一起！</h2><p id="de61" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">现在我们已经定义了这些函数，我们可以创建名为<code class="fe mx my mz na b">show_inference</code>的主函数，它将运行图像预测并显示行人之间的方框和距离。</p><p id="05b1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该函数的第一部分负责获取图像和运行推理。我们将得到一个字典<code class="fe mx my mz na b">output_dict</code>,里面有模型已经预测到的盒子。</p><pre class="kj kk kl km gt nv na nw nx aw ny bi"><span id="b316" class="ls lt it na b gy nz oa l ob oc">image = Image.open(image_path)</span><span id="ba3b" class="ls lt it na b gy od oa l ob oc"># the array based representation of the image will be used later<br/>image_np = load_image_into_numpy_array(image)</span><span id="0ca0" class="ls lt it na b gy od oa l ob oc"># Expanding dimensions <br/># Since the model expects images to have shape: [1, None, None, 3]<br/>image_np_expanded = np.expand_dims(image_np, axis=0)</span><span id="a4f3" class="ls lt it na b gy od oa l ob oc"># Actual detection.<br/>output_dict = run_inference_for_single_image(image_np, detection_graph)</span></pre><p id="7ae0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，我们将设置<code class="fe mx my mz na b">confidence_cutoff=0.5</code>以避免显示低精度预测。同时，我们将得到我们的图像的大小，我们需要设置一个关系<strong class="kw iu">“像素-米”</strong>来正确计算距离。我检查了一些图像，我认为<code class="fe mx my mz na b">width — 150px = 7 meters</code>是一个很好的关系。这一部分很复杂，因为我们没有考虑视角或相机角度，这是一个很难解决的问题，我鼓励你改进它，并与我们分享你的解决方案😄。</p><pre class="kj kk kl km gt nv na nw nx aw ny bi"><span id="0213" class="ls lt it na b gy nz oa l ob oc"># Get boxes only for person<br/>confidence_cutoff = 0.5</span><span id="c5ae" class="ls lt it na b gy od oa l ob oc">boxes, scores, classes = filter_boxes(confidence_cutoff, output_dict['detection_boxes'], <br/>output_dict['detection_scores'], <br/>output_dict['detection_classes'], [1])</span><span id="ab2a" class="ls lt it na b gy od oa l ob oc"># Get width and heigth<br/>im = Image.fromarray(image_np)<br/>width, height = im.size</span><span id="0b2f" class="ls lt it na b gy od oa l ob oc"># Pixel per meters - THIS IS A REFERENCE, YOU HAVE TO ADAPT THIS FOR EACH IMAGE<br/># In this case, we are considering that (width - 150) approximately is 7 meters<br/>average_px_meter = (width-150) / 7</span></pre><p id="0cc6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们可以为我们的预测计算所有的质心，并生成排列。然后，我们将能够创建连接质心和显示距离的线<code class="fe mx my mz na b">dx,dy</code>。</p><pre class="kj kk kl km gt nv na nw nx aw ny bi"><span id="5046" class="ls lt it na b gy nz oa l ob oc"># Calculate normalized coordinates for boxes<br/>centroids = []<br/>coordinates = []<br/>for box in boxes:<br/>    coord = calculate_coord(box, width, height)<br/>    centr = calculate_centr(coord)<br/>    centroids.append(centr)<br/>    coordinates.append(coord)</span><span id="5cce" class="ls lt it na b gy od oa l ob oc"># Calculate all permutations<br/>permutations = calculate_perm(centroids)</span><span id="9161" class="ls lt it na b gy od oa l ob oc"># Display boxes and centroids<br/>fig, ax = plt.subplots(figsize = (20,12), dpi = 90)<br/>ax.imshow(image, interpolation='nearest')</span><span id="bc41" class="ls lt it na b gy od oa l ob oc">for coord, centr in zip(coordinates, centroids):<br/>    ax.add_patch(patches.Rectangle((coord[0], coord[1]), coord[2],       coord[3], linewidth=2, edgecolor='y', facecolor='none', zorder=10))<br/>    ax.add_patch(patches.Circle((centr[0], centr[1]), 3, color='yellow', zorder=20))</span><span id="2417" class="ls lt it na b gy od oa l ob oc"># Display lines between centroids<br/>for perm in permutations:<br/>    dist = calculate_centr_distances(perm[0], perm[1])<br/>    dist_m = dist/average_px_meter    print("M meters: ", dist_m)<br/>    <br/>    middle = midpoint(perm[0], perm[1])<br/>    print("Middle point", middle)    x1 = perm[0][0]<br/>    <br/>    x2 = perm[1][0]<br/>    y1 = perm[0][1]<br/>    y2 = perm[1][1]    </span><span id="f1ec" class="ls lt it na b gy od oa l ob oc">    slope = calculate_slope(x1, y1, x2, y2)<br/>    dy = math.sqrt(3**2/(slope**2+1))<br/>    dx = -slope*dy</span></pre><p id="f2c0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">综上所述，这就是这个功能的最终状态。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">显示推理和距离的功能</p></figure><h2 id="7699" class="ls lt it bd lu lv lw dn lx ly lz dp ma ld mb mc md lh me mf mg ll mh mi mj mk bi translated">加油！！…让我们运行一些示例！</h2><p id="0b58" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">完成所有这些工作后，我们能够生成一些示例图像。我们只需要把我们的图片添加到这个文件夹:<code class="fe mx my mz na b">../Projects/Pedestrian_Detection/models/research/test_images</code></p><div class="kj kk kl km gt ab cb"><figure class="oe kn of og oh oi oj paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/a4748793e69f0257f461f1846035acc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*7coQNS7_14BXFUfCMphJFw.png"/></div></figure><figure class="oe kn ok og oh oi oj paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/9c91f1b2c6a69d1043a5a8ccb8b6da5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*p7zZ6TwaPH9omVDW0ustww.png"/></div></figure></div><p id="8777" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">哇！它看起来很好，但如果我们能在视频中运行它，它会更酷。所以，我们走吧！</p><h2 id="f4c4" class="ls lt it bd lu lv lw dn lx ly lz dp ma ld mb mc md lh me mf mg ll mh mi mj mk bi translated">视频中的推理</h2><p id="59e2" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">在视频中运行模型的代码与图像相同，因为我们使用<code class="fe mx my mz na b">openCV</code>将视频分割成帧，并将每一帧作为单独的图像进行处理。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">视频功能</p></figure><p id="7062" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们之前所做的，我们只需要将我们的视频复制到<code class="fe mx my mz na b">../Projects/Pedestrian_Detection/models/research/test_images</code>中，并更新<code class="fe mx my mz na b">cap = cv2.VideoCapture(…)</code>中的路径。此外，我们可以为<code class="fe mx my mz na b">FILE_OUTPUT</code>设置一个名称。</p><div class="kj kk kl km gt ab cb"><figure class="oe kn ol og oh oi oj paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/1a524639bd41f2252ebdc03c0e620963.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*0KncNJxBCKfwes9aSQKSGA.gif"/></div></figure><figure class="oe kn ol og oh oi oj paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/580846be95d36dd77edd76685fd489f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*ihFAwaWxHeePO8JUJ1hKiw.gif"/></div></figure></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/fd2cde9ba04ac5099dfb08c67cee2987.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*GsfCkil_h2pme0xzCk5KAA.gif"/></div></figure><p id="a99f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">干得好！！</strong>代码现在已经可以预测和测量社交距离了。</p><p id="81b0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们开发了这个应用程序，因为在<a class="ae lr" href="https://immune.institute/en/?utm_campaign=IMMUNE&amp;utm_source=Embajador" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">免疫技术研究所</strong> </a>我们尝试应用最先进的技术。我们喜欢分享知识，因为我们认为这是它变得强大的时候。</p><p id="b55f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你想学习如何开发真实世界的应用程序，比如这个用于测量<em class="lq">、</em>、的<strong class="kw iu">应用程序，你可能会对我们的<a class="ae lr" href="https://immune.institute/en/data-science?utm_campaign=MDS2021_2&amp;utm_source=Embajador" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">数据科学硕士</strong> </a> <strong class="kw iu"> e. </strong>感兴趣。这是一个针对寻求专攻数据科学、了解主要<strong class="kw iu">人工智能</strong>技术以及如何将它们应用到不同行业的专业人士的计划。</strong></p><p id="63b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果您对我们的工作和工作方式感兴趣，请加入我们的<a class="ae lr" href="https://tech.immune.institute/sesion-informativa-data-science?utm_campaign=MDS2021_2&amp;utm_source=Embajador" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">实时会议</strong> </a>。请随意使用我们的代码并对其进行改进。与我们分享成果！😄</p></div><div class="ab cl on oo hx op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="im in io ip iq"><p id="1caa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本文及代码由:<a class="ou ov ep" href="https://medium.com/u/3b43171da13b?source=post_page-----da0503717a62--------------------------------" rel="noopener" target="_blank">亚历杭德罗·迪亚斯·桑多斯</a>——(<a class="ae lr" href="https://www.linkedin.com/in/alex-diaz-santos-8aab812a/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>，<a class="ae lr" href="https://github.com/alejandrods" rel="noopener ugc nofollow" target="_blank"> GitHub </a>)为免疫技术研究所撰写。</p></div></div>    
</body>
</html>