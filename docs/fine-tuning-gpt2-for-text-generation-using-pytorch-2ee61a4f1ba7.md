# ä½¿ç”¨ Pytorch å¾®è°ƒç”¨äºæ–‡æœ¬ç”Ÿæˆçš„ GPT2

> åŸæ–‡ï¼š<https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7?source=collection_archive---------5----------------------->

## ä½¿ç”¨ Huggingface åº“æä¾›çš„ GPT2 ç”Ÿæˆä»»ä½•æ•…äº‹

![](img/eac7d5a2a2e7417a14e671e4b4d8188a.png)

[äºšå†å±±å¤§Â·å¥ˆç‰¹](https://unsplash.com/@agkdesign?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹ç…§

# ä»‹ç»

åœ¨è¿‡å»çš„å‡ å¹´é‡Œï¼ŒNLP çš„ä¸–ç•Œç‰¹åˆ«ç¹è£ã€‚è¿™ä¸»è¦å¾—ç›Šäº NLP åœ¨ç°ä»£åå¹´æœ€é‡è¦çš„çªç ´ä¹‹ä¸€â€”â€”[**å˜å½¢é‡‘åˆš**](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) ã€‚å¦‚æœä½ æ²¡æœ‰çœ‹è¿‡æˆ‘ä¹‹å‰å…³äº [**BERT è¿›è¡Œæ–‡æœ¬åˆ†ç±»**](/bert-text-classification-using-pytorch-723dfb8b6b5b) çš„æ–‡ç« ï¼Œé‚£å°±å»çœ‹çœ‹å§ï¼æˆ‘ä»¬ä»Šå¤©è¦è¯´çš„å¦ä¸€æ¬¾çƒ­é—¨å˜å‹å™¨æ˜¯ [**GPT2**](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ã€‚GPT2 ç”± OpenAI å¼€å‘ï¼Œæ˜¯ä¸€ä¸ªåŸºäº transformer çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªå¤§å‹æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒ:800 ä¸‡ä¸ªé«˜è´¨é‡ç½‘é¡µã€‚å®ƒåªä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„çŸ¥è¯†ï¼Œè€Œæ²¡æœ‰å¯¹å®ƒä»¬è¿›è¡Œæ˜ç¡®çš„è®­ç»ƒï¼Œä»è€Œåœ¨å¤šç§è¯­è¨€ä»»åŠ¡ä¸Šäº§ç”Ÿç«äº‰æ€§çš„è¡¨ç°ã€‚GPT2 å¯¹äºè¯­è¨€ç”Ÿæˆä»»åŠ¡éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªè‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚

åœ¨ä»Šå¤©çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨å¦‚ä½•å®ç°å¦ä¸€ä¸ªæµè¡Œçš„è½¬æ¢å™¨ GPT2ï¼Œä»¥ç¼–å†™æœ‰è¶£å’Œæœ‰åˆ›æ„çš„æ•…äº‹ï¼å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [CMU å›¾ä¹¦æ‘˜è¦æ•°æ®é›†](http://www.cs.cmu.edu/~dbamman/booksummaries.html)æµ‹è¯• GPT2 æ’°å†™æœ‰åˆ›æ„çš„å›¾ä¹¦æ‘˜è¦çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ [Huggingface](https://huggingface.co/) åº“æ¥æ„å»ºæˆ‘ä»¬çš„æ¨¡å‹å¹¶ç”Ÿæˆæ–‡æœ¬ã€‚

æœ¬æ–‡çš„**å®Œæ•´ä»£ç åº“**å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/itsuncheng/fine-tuning-GPT2)æŸ¥çœ‹ã€‚

# æ­¥éª¤ 1:å‡†å¤‡æ•°æ®é›†

åœ¨æ„å»ºæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆä¸‹è½½å¹¶é¢„å¤„ç†æ•°æ®é›†ã€‚

æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ CMU å›¾ä¹¦æ‘˜è¦æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä»ç»´åŸºç™¾ç§‘ä¸­æå–çš„ 16ï¼Œ559 æœ¬å›¾ä¹¦ï¼Œä»¥åŠå…ƒæ•°æ®ï¼ŒåŒ…æ‹¬ä¹¦åã€ä½œè€…ã€å‡ºç‰ˆæ—¥æœŸã€æµæ´¾å’Œæƒ…èŠ‚æ‘˜è¦ã€‚ç‚¹å‡»ä¸‹è½½æ•°æ®é›†[ã€‚ä»¥ä¸‹æ˜¯æ•°æ®é›†çš„å¤–è§‚:](http://www.cs.cmu.edu/~dbamman/booksummaries.html)

![](img/c2792e37a4b43da5899b593ca63c1adc.png)

ä½œè€…å›¾ç‰‡

å¯¹äºæ•°æ®é¢„å¤„ç†ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ•´ä¸ªæ•°æ®é›†åˆ†æˆè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†ï¼Œè®­ç»ƒæœ‰æ•ˆæµ‹è¯•æ¯”ç‡ä¸º 70â€“20â€“10ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ªæ‘˜è¦çš„å¼€å¤´æ·»åŠ äº†ä¸€ä¸ª bos ä»¤ç‰Œ<bos>ï¼Œåœ¨æ¯ä¸ªæ‘˜è¦çš„ç»“å°¾æ·»åŠ äº†ä¸€ä¸ª eos ä»¤ç‰Œ<eos>ï¼Œä»¥ä¾›ä»¥ååŸ¹è®­ä½¿ç”¨ã€‚æˆ‘ä»¬æœ€ç»ˆå°†æ‘˜è¦ä¿å­˜åˆ°ã€‚txt æ–‡ä»¶ï¼Œè·å– train.txtï¼Œvalid.txtï¼Œtest.txtã€‚</eos></bos>

ä½ å¯ä»¥åœ¨è¿™é‡Œè·å¾—é¢„å¤„ç†ç¬”è®°æœ¬[ã€‚](https://github.com/itsuncheng/fine-tuning-GPT2/blob/master/preprocessing.ipynb)

# æ­¥éª¤ 2:ä¸‹è½½åº“

ä¸ºäº†æ„å»ºå’Œè®­ç»ƒ GPT2ï¼Œæˆ‘ä»¬éœ€è¦å®‰è£… Huggingface åº“ï¼Œä»¥åŠå®ƒçš„å­˜å‚¨åº“ã€‚

å®‰è£… Huggingface åº“:

```
pip install transformers
```

å…‹éš†æ‹¥æŠ±è„¸å›è´­:

```
git clone github.com/huggingface/transformers
```

å¦‚æœæ‚¨æƒ³åœ¨è®­ç»ƒæœŸé—´çœ‹åˆ°æ¨¡å‹å’Œè¶…å‚æ•°çš„å¯è§†åŒ–æ•ˆæœï¼Œä¹Ÿå¯ä»¥é€‰æ‹©å®‰è£… tensorboard æˆ– wandb:

```
pip install tensorboardpip install wandb; wandb login
```

# ç¬¬ä¸‰æ­¥:å¾®è°ƒ GPT2

åœ¨è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥æŒ‰ç…§ä¹‹å‰åœ¨æ•°æ®é›†ä¸­å®šä¹‰çš„é‚£æ ·è®¾ç½® bos ä»¤ç‰Œå’Œ eos ä»¤ç‰Œã€‚

æˆ‘ä»¬è¿˜åº”è¯¥è®¾ç½® pad ä»¤ç‰Œï¼Œå› ä¸ºæˆ‘ä»¬å°†ä½¿ç”¨ *LineByLineDataset* ï¼Œå®ƒå°†æŠŠæ•°æ®é›†ä¸­çš„æ¯ä¸€è¡Œéƒ½è§†ä¸ºä¸åŒçš„ç¤ºä¾‹ã€‚åœ¨*transformers/example/language-modeling/run-language-modeling . py*ä¸­ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨è®­ç»ƒä¹‹å‰ä¸ºæ¨¡å‹è¿½åŠ ä»¥ä¸‹ä»£ç :

```
special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)model.resize_token_embeddings(len(tokenizer))
```

è¿è¡Œè¿™æ®µä»£ç åï¼Œç‰¹æ®Šçš„æ ‡è®°å°†è¢«æ·»åŠ åˆ°æ ‡è®°å™¨ä¸­ï¼Œæ¨¡å‹å°†è°ƒæ•´å…¶åµŒå…¥çš„å¤§å°ï¼Œä»¥é€‚åº”ä¿®æ”¹åçš„æ ‡è®°å™¨ã€‚

å¯¹äºè®­ç»ƒï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€äº›å‚æ•°ï¼Œç„¶åè¿è¡Œè¯­è¨€å»ºæ¨¡è„šæœ¬:

```
cd transformers/example/language-modelingN=gpu_numOUTPUT_DIR=/path/to/modelTRAIN_FILE=/path/to/dataset/train.txtVALID_FILE=/path/to/dataset/valid.txtCUDA_VISIBLE_DEVICES=$N python run_language_modeling.py \--output_dir=$OUTPUT_DIR \--model_type=gpt2 \--model_name_or_path=gpt2 \--do_train \--train_data_file=$TRAIN_FILE \--do_eval \--eval_data_file=$VALID_FILE \--per_device_train_batch_size=2 \--per_device_eval_batch_size=2 \--line_by_line \--evaluate_during_training \--learning_rate 5e-5 \--num_train_epochs=5
```

ç”±äº GPU çš„é™åˆ¶ï¼Œæˆ‘ä»¬è®¾ç½® per_device_train_batch_size=2ï¼Œper_device_eval_batch_size=2ã€‚è¯·éšæ„ä½¿ç”¨é€‚åˆæ‚¨çš„ GPU çš„æ‰¹é‡å¤§å°ã€‚æˆ‘ä»¬ä½¿ç”¨ line_by_lineï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬çš„æ¨¡å‹å°†æ•°æ®é›†ä¸­çš„æ¯ä¸€è¡Œéƒ½è§†ä¸ºä¸€ä¸ªå•ç‹¬çš„ç¤ºä¾‹ï¼Œå¦‚å‰æ‰€è¿°ã€‚Evaluate_during_training åœ¨æ¯ä¸ª`logging_steps`ä¹‹åå¯¹è¯„ä¼°æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œé»˜è®¤ä¸º 500ã€‚

å¦‚æœæ‚¨æƒ³ä»æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒï¼Œæ‚¨å¯ä»¥è¿è¡Œ:

```
CUDA_VISIBLE_DEVICES=$N python run_language_modeling.py \--output_dir=$OUTPUT_DIR \--model_type=gpt2 \--model_name_or_path=$OUTPUT_DIR \--do_train \--train_data_file=$TRAIN_FILE \--do_eval \--eval_data_file=$VALID_FILE \--per_device_train_batch_size=2 \--per_device_eval_batch_size=2 \--line_by_line \--evaluate_during_training \--learning_rate 5e-5 \--num_train_epochs=5 \--overwrite_output_dir
```

# (å¯é€‰)æ­¥éª¤ 4:è¯„ä¼°æµ‹è¯•æ•°æ®é›†çš„å›°æƒ‘

è¿™ä¸€æ­¥æ˜¯å¯é€‰çš„ï¼Œå–å†³äºä½ æ˜¯å¦æƒ³è¯„ä¼°ä½ è®­ç»ƒè¿‡çš„ GPT2 çš„è¡¨ç°ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¯„ä¼°å›°æƒ‘æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚

```
TEST_FILE=/path/to/dataset/test.txtCUDA_VISIBLE_DEVICES=$N python run_language_modeling.py \--output_dir=$OUTPUT_DIR \--model_type=gpt2 \--model_name_or_path=$OUTPUT_DIR \--do_eval \--eval_data_file=$TEST_FILE \--per_device_eval_batch_size=2 \--line_by_line
```

è¿™é‡Œï¼Œåœ¨æˆ‘çš„ä¾‹å­ä¸­ï¼Œåœ¨è®­ç»ƒ 5 ä¸ªæ—¶æœŸåï¼Œæˆ‘ä»¬è·å¾—äº† 2.46 çš„æŸå¤±å’Œ 11.70 çš„å›°æƒ‘åº¦:

![](img/43aacc97c40f8468428f717054f71b2d.png)

ä½œè€…å›¾ç‰‡

# æ­¥éª¤ 5:ç”Ÿæˆæ–‡æœ¬

åœ¨ä½¿ç”¨æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡åœ¨*transformers/examples/text-generation/run _ generation . py*ä¸­è®¾ç½®`add_special_tokens=True`æ¥å¯ç”¨æç¤ºä¸­çš„ç‰¹æ®Šæ ‡è®°:

```
encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=True, return_tensors=â€ptâ€)
```

ç„¶åï¼Œæˆ‘ä»¬å‡†å¤‡ç”Ÿæˆä¸€äº›æ–‡æœ¬ï¼å¼€å§‹ç”Ÿæˆæ–¹å¼:

```
cd transformers/examples/text-generationK=k_for_top-k_sampling_decoderCUDA_VISIBLE_DEVICES=$N python run_generation.py \--model_type gpt2 \--model_name_or_path $OUTPUT_DIR \--length 300 \--prompt "<BOS>" \--stop_token "<EOS>" \--k $K \--num_return_sequences 5
```

æˆ‘ä»¬è¾“å…¥æç¤ºâ€œ<bos>â€ä½œä¸ºè¾“å…¥ï¼Œå®ƒä»£è¡¨æ¯ä¸ªä¾‹å­çš„å¼€å§‹ï¼Œä¸€æ—¦ç”Ÿæˆäº†â€œ<eos>â€æ ‡è®°ï¼Œå°±åœæ­¢æ¨¡å‹çš„ç”Ÿæˆã€‚è¿™æ ·ï¼Œæˆ‘ä»¬çš„ GPT2 å°†å­¦ä¹ ä»å¤´åˆ°å°¾ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æ‘˜è¦ç¤ºä¾‹ï¼Œåˆ©ç”¨å®ƒåœ¨åŸ¹è®­æœŸé—´ä» bos ä»¤ç‰Œå’Œ eos ä»¤ç‰Œä¸­å­¦åˆ°çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ top-k é‡‡æ ·è§£ç å™¨ï¼Œè¯¥è§£ç å™¨å·²è¢«è¯æ˜åœ¨ç”Ÿæˆéç«äº‰æ€§å’Œæ›´å¥½çš„æ–‡æœ¬æ–¹é¢éå¸¸æœ‰æ•ˆã€‚k=50 æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¼€å§‹å€¼ã€‚Huggingface è¿˜æ”¯æŒå…¶ä»–è§£ç æ–¹æ³•ï¼ŒåŒ…æ‹¬è´ªå©ªæœç´¢ã€æ³¢æŸæœç´¢å’Œ top-p é‡‡æ ·è§£ç å™¨ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹`model.generate`çš„[æ–‡æ¡£ä¸²](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate)ã€‚</eos></bos>

ä¸‹é¢æ˜¯å‡ ä¸ª k=50 çš„ç”Ÿæˆæ–‡æœ¬çš„ä¾‹å­ã€‚

> ä¸»è§’æ˜¯è‹±å›½äººå¨å»‰Â·æ‹‰å…‹ï¼Œä»–è¢«è‹±å›½æ”¿åºœæ´¾å¾€åŒ—ææ‰§è¡Œä»»åŠ¡ï¼Œå¼€å§‹äº†ä¸€æ¬¡å†’é™©ä¹‹æ—…ã€‚è¿™éƒ¨å°è¯´è®²è¿°äº†ä»–çš„æœ‹å‹å’Œå®¶äººå¦‚ä½•è¢«å–åˆ°æŒªå¨å°é•‡è‚–å…‹å½“å¥´éš¶çš„æ•…äº‹â€¦
> 
> ä¸€ä¸ªæ–°çš„ä¸–ç•Œæ­£åœ¨è§‰é†’ï¼Œæ²ƒå°”å¡”æ˜Ÿçƒçš„äººç±»å¿…é¡»é½å¿ƒååŠ›æ‹¯æ•‘å®ƒå…äºæ¯ç­ã€‚æ–°åœ°çƒç°åœ¨å±…ä½ç€ä¸‰ä¸ªç‰©ç§ã€‚ç¬¬ä¸€ä¸ªæ˜¯å¹´é¾„ç¨å¤§çš„äººç±»ï¼Œç¬¬äºŒä¸ªæ˜¯æ²ƒå°”å¡”äººï¼Œç¬¬ä¸‰ä¸ªæ˜¯æœ‰ç€æ·±è“è‰²çœ¼ç›çš„äººç±»â€¦
> 
> è¿™éƒ¨å°è¯´å¼€å§‹äº 2143 å¹´ï¼Œä¸€ç¾¤â€œåœ°ç‰¢â€æˆ–å¥³å·«å†³å®šé€šè¿‡æ¶ˆè€—æ­»è€…çš„çµé­‚æ¥æ‰“ç ´é˜»æ­¢æ­»è€…åŠ›é‡çš„å’’è¯­ã€‚ä»–ä»¬ç”¨å°¸ä½“æ¥å¸®åŠ©å‚æ­»çš„äººï¼Œä¹Ÿç”¨å°¸ä½“æ¥å¤æ´»æ­»è€…â€¦

ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ›´å¤šç”Ÿæˆçš„ä¾‹å­[ã€‚](https://github.com/itsuncheng/fine-tuning-GPT2/blob/master/generated_summaries.txt)

# ç»“è®º

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å®ç°æœ€æµè¡Œçš„ transformer æ¨¡å‹ä¹‹ä¸€ GPT2 æ¥åˆ›å»ºæœ‰è¶£çš„æ–‡æœ¬ã€‚GPT2 çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†å’Œæ¶æ„å…è®¸å®ƒäº§ç”Ÿè¿è´¯å’Œæµç•…çš„å†™ä½œç‰‡æ®µã€‚è™½ç„¶ GPT2 çš„æ–‡æœ¬ä»ç„¶å¯ä»¥ä¸äººç±»ä¹¦å†™çš„æ–‡æœ¬åŒºåˆ†å¼€æ¥ï¼Œä½†è¿™è¯æ˜äº†æœºå™¨çš„åˆ›é€ åŠ›åªæ˜¯ä»ç°åœ¨å¼€å§‹ä¸Šå‡ã€‚æƒ³äº†è§£æ›´å¤šä¿¡æ¯ï¼Œä½ å¯ä»¥çœ‹çœ‹ GPT2 ä¸Šçš„[å®˜æ–¹è®ºæ–‡](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)æˆ–è€… OpenAI çš„[åšå®¢](https://openai.com/blog/better-language-models/)ã€‚

æœ¬æ–‡åªå±•ç¤ºäº†å¦‚ä½•ç”Ÿæˆç”±äººå·¥æ™ºèƒ½å†³å®šçš„æ–‡æœ¬ã€‚å¦‚æœæ‚¨æƒ³çŸ¥é“æ˜¯å¦æœ‰å¯èƒ½æ§åˆ¶æ­£åœ¨ç”Ÿæˆçš„æ–‡æœ¬(è¿™æ˜¯å¯èƒ½çš„ï¼)ï¼Œçœ‹çœ‹æˆ‘å†™çš„ä¸‹é¢è¿™ç¯‡æ–‡ç« ğŸ˜Šã€‚

[](/controlling-text-generation-from-language-models-6334935e80cf) [## æ§åˆ¶è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆ

### æ§åˆ¶æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„æ ·å¼å’Œå†…å®¹çš„å®é™…æ“ä½œæ–¹æ³•

towardsdatascience.com](/controlling-text-generation-from-language-models-6334935e80cf) 

# å‚è€ƒ

[1] A .ç“¦æ–¯ç“¦å°¼ï¼Œn .æ²™æ³½å°”ï¼Œn .å¸•å°”é©¬ç­‰ã€‚ï¼Œ[æ³¨æ„åŠ›æ˜¯ä½ éœ€è¦çš„å…¨éƒ¨](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) (2017)ï¼Œç¬¬ 31 å±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®

[2] A .ã€j .å´ã€r .æŸ´å°”å¾·ç­‰ã€‚ï¼Œ[è¯­è¨€æ¨¡å‹æ˜¯æ— ç›‘ç£çš„å¤šä»»åŠ¡å­¦ä¹ å™¨](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2019)ï¼ŒOpenAI