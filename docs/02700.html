<html>
<head>
<title>The Ultimate Guide to AdaBoost, random forests and XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AdaBoost、随机森林和 XGBoost 的终极指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-ultimate-guide-to-adaboost-random-forests-and-xgboost-7f9327061c4f?source=collection_archive---------0-----------------------#2020-03-16">https://towardsdatascience.com/the-ultimate-guide-to-adaboost-random-forests-and-xgboost-7f9327061c4f?source=collection_archive---------0-----------------------#2020-03-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e1c2c9d96ee4001f6d4ea6924101879b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wpVgt07J_TeH3jEdc3A50g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">资料来源:朱莉娅·尼库尔斯基。</p></figure><div class=""/><div class=""><h2 id="d280" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">它们是如何工作的，有何不同，何时应该使用？</h2></div><p id="9e4d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><a class="ae lt" href="http://kaggle.com" rel="noopener ugc nofollow" target="_blank"> kaggle </a>上的很多内核都使用基于树的集成算法来解决有监督的机器学习问题，比如<a class="ae lt" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html" rel="noopener ugc nofollow" target="_blank"> AdaBoost </a>、<a class="ae lt" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" rel="noopener ugc nofollow" target="_blank">随机森林</a>、<a class="ae lt" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>、<a class="ae lt" href="https://xgboost.readthedocs.io/en/latest/#" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>或者<a class="ae lt" href="https://catboost.ai/docs/" rel="noopener ugc nofollow" target="_blank"> CatBoost </a>。也许你以前也使用过它们，但是你能解释一下它们是如何工作的，为什么要选择它们而不是其他算法吗？以下段落将概述基于树的集成算法的一般好处，描述打包和增强的概念，并解释和对比集成算法 AdaBoost、随机森林和 XGBoost。</p><h1 id="3a51" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">基于树的集成算法</h1><p id="8595" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">集成算法，尤其是那些利用决策树作为弱学习器的算法，与其他算法相比具有多种优势(基于<a class="ae lt" href="https://www.sciencedirect.com/science/article/pii/S1383762109000605" rel="noopener ugc nofollow" target="_blank">本文</a>、<a class="ae lt" href="https://link.springer.com/chapter/10.1007/3-540-45014-9_1" rel="noopener ugc nofollow" target="_blank">这一</a>和<a class="ae lt" href="https://www.researchgate.net/publication/275641579_COMPARISON_OF_MACHINE_LEARNING_ALGORITHMS_RANDOM_FOREST_ARTIFICIAL_NEURAL_NETWORK_AND_SUPPORT_VECTOR_MACHINE_TO_MAXIMUM_LIKELIHOOD_FOR_SUPERVISED_CROP_TYPE_CLASSIFICATION" rel="noopener ugc nofollow" target="_blank">这一</a>):</p><ol class=""><li id="6aa3" class="mr ms ji kz b la lb ld le lg mt lk mu lo mv ls mw mx my mz bi translated">他们的算法很容易理解和可视化:描述和绘制一个决策树可以说比向你的祖母描述支持向量机更容易</li><li id="9615" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">它们是<strong class="kz jj">非参数</strong>，不假设或要求数据遵循特定的分布:这将节省您将数据转换为正态分布的时间</li><li id="6d92" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">它们可以<strong class="kz jj">处理混合数据类型</strong>:分类变量不一定必须是热编码的</li><li id="0124" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated"><strong class="kz jj">特征<strong class="kz jj">的多重共线性</strong>不影响模型的准确性</strong>和预测性能:不需要移除特征或以其他方式设计以减少它们之间的相关性和相互作用</li><li id="4c44" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">它们对过拟合 : <strong class="kz jj"> </strong>具有<strong class="kz jj">鲁棒性，因为它们使用许多欠拟合(高偏差)的弱学习器，并将这些预测组合成一个更强的学习器，它们减少了模型的过拟合(方差)</strong></li><li id="e097" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">它们对异常值和噪声相对<strong class="kz jj">稳健</strong> : <strong class="kz jj"> </strong>一般来说，它们会很好地处理噪声数据(例如对目标没有影响的特征)或异常值(例如极值)，对整体性能几乎没有影响(这一点对于 AdaBoost 是有争议的；更多信息请见下文)</li><li id="8c26" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">输入<strong class="kz jj">不需要缩放</strong>:不需要使用最小最大缩放器或标准缩放器对特征进行预处理和转换</li><li id="435d" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">它们在计算上相对便宜:与支持向量机或神经网络等算法相比，它们速度更快</li><li id="3691" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">他们通常比他们的弱学习者表现得更好<strong class="kz jj">:与 boosting 和 bagging 算法相比，决策树由于其高方差/过拟合而不太准确</strong></li></ol><h1 id="6553" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">增压和装袋</h1><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e1c2c9d96ee4001f6d4ea6924101879b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wpVgt07J_TeH3jEdc3A50g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">单次迭代、打包和提升算法概述。资料来源:朱莉娅·尼库尔斯基。</p></figure><p id="868d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">boosting 和 bagging 的概念是理解这些基于树的集合模型的核心。<a class="ae lt" href="https://pdfs.semanticscholar.org/5fb5/f7b545a5320f2a50b30af599a9d9a92a8216.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kz jj"> Boosting </strong> </a>描述了将许多弱学习者组合成一个非常精确的预测算法。弱学习者指的是只比随机预测略好的学习算法。例如，当查看基于树的集成算法时，单个<em class="nj">决策树</em>将是<em class="nj">弱学习器</em>，并且多个弱学习器的<em class="nj">组合</em>将产生<em class="nj"> AdaBoost </em>算法。boosting 方法是一种顺序算法，它对整个训练样本的<em class="nj"> T </em>轮进行预测，并使用来自前一轮预测准确性的信息迭代地提高 boosting 算法的性能(有关更多详细信息，请参见<a class="ae lt" href="https://pdfs.semanticscholar.org/5fb5/f7b545a5320f2a50b30af599a9d9a92a8216.pdf" rel="noopener ugc nofollow" target="_blank">本文</a>和<a class="ae lt" href="https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5" rel="noopener">这篇中型博客文章</a>)。</p><p id="d3c7" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><a class="ae lt" href="https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kz jj">装袋</strong> </a> <strong class="kz jj"> </strong>另一方面是指无顺序学习。对于<em class="nj"> T </em>轮，从训练样本中随机抽取样本子集(替换)。这些抽签中的每一次都独立于前一轮的抽签，但具有相同的分布。这些随机选择的样本然后被用于生长决策树(弱学习者)。然后选择最受欢迎的类别(或回归问题中的平均预测值)作为最终预测值。装袋方法也被称为<em class="nj">自举</em>(更多细节参见<a class="ae lt" href="https://link.springer.com/content/pdf/10.1023/A:1007607513941.pdf" rel="noopener ugc nofollow" target="_blank">本</a>和<a class="ae lt" href="https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf" rel="noopener ugc nofollow" target="_blank">本</a>论文)。</p><h1 id="352b" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">adaboost 算法</h1><p id="1bdb" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">AdaBoost 算法是 boosting 算法家族的一部分，由<a class="ae lt" href="https://pdfs.semanticscholar.org/5fb5/f7b545a5320f2a50b30af599a9d9a92a8216.pdf" rel="noopener ugc nofollow" target="_blank"> Freund &amp; Schapire 于 1996 年</a>首次推出。它将决策树作为弱学习器连续生长，并通过在每一轮预测后给错误预测的样本分配较大的权重来惩罚它们。这样，算法正在从以前的错误中学习。最终的预测是加权多数票(或者在回归问题的情况下是加权中值)。</p><p id="7ee6" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">用于分类问题的 AdaBoost 算法的<strong class="kz jj">伪代码</strong>如下所示，改编自 1996 年<a class="ae lt" href="https://pdfs.semanticscholar.org/5fb5/f7b545a5320f2a50b30af599a9d9a92a8216.pdf" rel="noopener ugc nofollow" target="_blank">Freund&amp;Schapire</a>(关于回归问题，请参考基础论文):</p><p id="6f9c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">对于<em class="nj"> T </em>回合中的<em class="nj"> t </em>:</p><ol class=""><li id="9490" class="mr ms ji kz b la lb ld le lg mt lk mu lo mv ls mw mx my mz bi translated">通过归一化权重向量<em class="nj"> w 来计算分布<em class="nj">p</em>(</em>第一轮的<em class="nj"> w </em>中的初始权重为<em class="nj"> 1/N </em>，其中<em class="nj"> N </em>表示标记的样本数)</li><li id="d23c" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">使用分布<em class="nj"> p </em>生成弱学习器(决策树)；返回假设<em class="nj"> h </em>以及每个示例的预测值</li><li id="9bb0" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">计算<em class="nj"> h </em>的误差项<em class="nj"> ε </em></li><li id="11ec" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">将<em class="nj"> β </em>与<em class="nj">ε/(1-</em>T46】ε)赋值</li><li id="75ef" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">将权重向量更新为<em class="nj"> w = w*β </em>，以便性能差的预测将具有较高的权重，而性能好的预测将具有较低的权重</li></ol><p id="8759" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">输出:最终假设是所有<em class="nj"> T </em>弱学习者加权多数投票的结果</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/2239960f9d4d755274509678d8b75c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*kPazgmGTBHDMb8kVwesxbg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于分类问题的 AdaBoost 算法综述及可视化。较大的点表示这些样本先前被错误分类，并被赋予较高的权重。资料来源:朱莉娅·尼库尔斯基。</p></figure><p id="ff27" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">AdaBoost 算法固有某些<strong class="kz jj">优点</strong> <strong class="kz jj">和缺点</strong>。AdaBoost 对低噪声数据集中的过拟合相对鲁棒(参见<a class="ae lt" href="https://link.springer.com/content/pdf/10.1023/A:1007618119488.pdf" rel="noopener ugc nofollow" target="_blank">rt sch 等人(2001) </a>)。AdaBoost 只有<a class="ae lt" href="https://arxiv.org/pdf/1710.04725.pdf" rel="noopener ugc nofollow" target="_blank">几个超参数</a>需要调整以提高模型性能。此外，该算法易于理解和可视化。然而，对于有噪声的数据，AdaBoost 的性能存在争议<a class="ae lt" href="https://pdfs.semanticscholar.org/5fb5/f7b545a5320f2a50b30af599a9d9a92a8216.pdf" rel="noopener ugc nofollow" target="_blank">一些人认为</a>它概括得很好，而<a class="ae lt" href="http://parnec.nuaa.edu.cn/%5C/pubs/songcan%20chen/journal/2016/SBKBS-2016.pdf" rel="noopener ugc nofollow" target="_blank">其他人则表明</a>有噪声的数据导致性能不佳，因为算法在学习极端情况和扭曲结果上花费了太多时间。与 random forests 和 XGBoost 相比，AdaBoost 在模型中包含不相关的功能时表现更差，正如我对自行车共享需求的<a class="ae lt" rel="noopener" target="_blank" href="/go-highly-accurate-or-go-home-61828afb0b13">时间序列分析</a>所示。此外，AdaBoost 没有针对速度进行优化，因此比 XGBoost 慢得多。</p><p id="f2b0" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">要调整的<a class="ae lt" href="https://arxiv.org/pdf/1710.04725.pdf" rel="noopener ugc nofollow" target="_blank">相关<strong class="kz jj">超参数</strong> </a>限于弱学习器/决策树的<em class="nj">最大深度</em>、<em class="nj">学习速率</em>和<em class="nj">迭代/回合数</em>。学习率平衡了每个决策树对整个算法的影响，而最大深度确保了样本不会被记住，但模型会用新数据很好地概括。</p><p id="65fc" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">以上信息表明，AdaBoost 最适合<strong class="kz jj">在低噪声数据集</strong>中使用，此时计算复杂性或结果的及时性不是主要问题，并且由于用户缺乏时间和知识，没有足够的资源进行更广泛的超参数调整。</p><h1 id="67f5" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">随机森林</h1><p id="5ee8" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">随机森林算法由 Breiman 于 2001 年开发，基于 bagging 方法。该算法通过为生长树的每次迭代随机选择子样本来引导数据。增长是并行发生的，这是 AdaBoost 和随机森林之间的<a class="ae lt" href="https://www.cse.wustl.edu/~ychen/public/OAE.pdf" rel="noopener ugc nofollow" target="_blank">关键区别</a>。随机森林通过组合许多欠适应的弱学习者来实现过适应的减少，因为它们仅利用所有训练样本的子集。AdaBoost 和随机森林的另一个区别是，后者只选择随机的特征子集包含在每棵树中，而前者包含所有树的所有特征。</p><p id="4622" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">根据<a class="ae lt" href="https://www.researchgate.net/profile/Hitesh_Parmar6/publication/268509189_Sentiment_Mining_of_Movie_Reviews_using_Random_Forest_with_Tuned_Hyperparameters/links/546db7d50cf2a7492c55d7eb/Sentiment-Mining-of-Movie-Reviews-using-Random-Forest-with-Tuned-Hyperparameters.pdf" rel="noopener ugc nofollow" target="_blank"> Parmer et al. (2014) </a>，随机森林的<strong class="kz jj">伪码</strong>如下所示:</p><p id="10a6" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">对于第<em class="nj"> T </em>轮中的 T(其中<em class="nj"> T </em>是生长的树的数量):</p><ol class=""><li id="9f2e" class="mr ms ji kz b la lb ld le lg mt lk mu lo mv ls mw mx my mz bi translated">从训练集中随机抽取一个替换样本<em class="nj"> s </em></li><li id="abb0" class="mr ms ji kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">递归重复以下步骤，直到树的预测不再提高:</li></ol><p id="4563" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">2.1.从所有可用功能中随机选择<em class="nj"> f </em>个功能<em class="nj"> F </em></p><p id="715d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">2.2.选择信息增益最大的特征</p><p id="0abe" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">2.3.此功能用于拆分树的当前节点</p><p id="ad4e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">输出:所有<em class="nj"> T </em>树的多数投票决定最终的预测结果</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/241a7eeb6d6823cb576b7021b5961fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qPCOcn6jdOETLlBlK2zhjA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">随机森林算法的可视化。训练样本和特征的子样本用于生长多个弱学习器(决策树)。资料来源:朱莉娅·尼库尔斯基。</p></figure><p id="7e3d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">随机森林是一种非常受欢迎的算法，因为它非常准确，对噪声和异常值相对鲁棒，速度快，可以进行隐式特征选择，并且易于实现、理解和可视化(更多详细信息<a class="ae lt" href="https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>)。与 AdaBoost 相比，随机森林的主要<strong class="kz jj">优势</strong>是它受噪声的影响更小，并且它可以更好地概括减少方差，因为随着树木数量的增加，概括误差达到极限(根据中心极限定理)。然而，随机森林的一个<strong class="kz jj">缺点</strong>是因为相关参数的数量更多，所以需要<a class="ae lt" href="https://arxiv.org/pdf/1706.09865.pdf" rel="noopener ugc nofollow" target="_blank">更多的超参数调整</a>。此外，随机森林将随机性引入到训练和测试数据中，这并不适合所有数据集(更多细节见下文)。</p><p id="e8ea" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">要考虑的<strong class="kz jj">超参数</strong>包括<em class="nj">特征数量</em>、<em class="nj">树数量</em>、<em class="nj">树的最大深度</em>、是否<em class="nj">引导</em>样本、分割前节点中剩余的<em class="nj">最小样本数量以及<em class="nj">最后叶节点中剩余的</em>最小样本数量(基于<a class="ae lt" href="https://arxiv.org/pdf/1706.09865.pdf" rel="noopener ugc nofollow" target="_blank">此</a>、<a class="ae lt" href="https://www.researchgate.net/profile/Hitesh_Parmar6/publication/268509189_Sentiment_Mining_of_Movie_Reviews_using_Random_Forest_with_Tuned_Hyperparameters/links/546db7d50cf2a7492c55d7eb/Sentiment-Mining-of-Movie-Reviews-using-Random-Forest-with-Tuned-Hyperparameters.pdf" rel="noopener ugc nofollow" target="_blank">此</a><a class="ae lt" href="https://www.researchgate.net/profile/Hitesh_Parmar6/publication/268509189_Sentiment_Mining_of_Movie_Reviews_using_Random_Forest_with_Tuned_Hyperparameters/links/546db7d50cf2a7492c55d7eb/Sentiment-Mining-of-Movie-Reviews-using-Random-Forest-with-Tuned-Hyperparameters.pdf" rel="noopener ugc nofollow" target="_blank">一般来说，训练阶段过于复杂会导致过度拟合。因此，应选择较少数量的特征(大约三分之一)。较大数量的树倾向于产生较好的性能，而分裂前每个叶片的最大深度和最小样本数量应该相对较低。</a></em></p><p id="c03c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在处理时间序列数据或任何其他应避免前瞻偏差且需要确保样本顺序和连续性的数据时，不应使用随机森林<strong class="kz jj"/>(参考我的<a class="ae lt" rel="noopener" target="_blank" href="/go-highly-accurate-or-go-home-61828afb0b13"> TDS 帖子</a>关于 AdaBoost、随机森林和 XGBoost 的时间序列分析)。这种算法可以相对较好地处理噪声，但与 AdaBoost 相比，需要用户掌握更多知识才能充分调整算法。</p><h1 id="a8e0" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">XGBoost</h1><p id="0a54" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">XGBoost(e<strong class="kz jj">X</strong>treme<strong class="kz jj">G</strong>radient<strong class="kz jj">Boost</strong>ing)是一个相对较新的算法，由<a class="ae lt" href="https://dl.acm.org/doi/abs/10.1145/2939672.2939785" rel="noopener ugc nofollow" target="_blank">陈&amp; Guestrin 在 2016 </a>年推出，利用了梯度树 boosting 的概念。开发 XGBoost 是为了提高速度和性能，同时引入正则化参数来减少过拟合。梯度增强树在顺序学习过程中使用回归树(或<a class="ae lt" href="https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/" rel="noopener ugc nofollow" target="_blank"> CART </a>)作为弱学习器。这些回归树类似于决策树，然而，它们使用分配给每个叶子(即，树完成生长后的最后一个节点)的连续分数，该分数被累加并提供最终预测。对于生成树<em class="nj"> t </em>的每次迭代<em class="nj"> i </em>，计算预测某个结果<em class="nj"> y </em>的得分<em class="nj"> w </em>。学习过程的目标是<a class="ae lt" href="https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5" rel="noopener">最小化总得分</a>，总得分由<em class="nj"> i-1 </em>处的损失函数和<em class="nj"> t </em>的新树结构组成。这允许算法顺序地生长树并从先前的迭代中学习。然后使用梯度下降来计算每片叶子的最优值和树<em class="nj"> t </em>的总得分。分数也被称为树的预测的杂质。</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/786a938c32e1cad2cee1527c316f93b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vE0zsu-5aF4u3-5fPnHCcQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">使用连续分数提供最终预测的树集合模型的可视化。来源:茱莉亚·尼库尔斯基根据<a class="ae lt" href="https://dl.acm.org/doi/abs/10.1145/2939672.2939785" rel="noopener ugc nofollow" target="_blank">陈&amp;guest rin(2016)</a>；由<a class="ae lt" href="https://www.flaticon.com/authors/freepik" rel="noopener ugc nofollow" target="_blank"> Freepik </a>从<a class="ae lt" href="https://www.flaticon.com" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>制作的图标。</p></figure><p id="beba" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">上述算法中的损失函数包含正则化或惩罚项ω，其目标是降低回归树函数的复杂性。该参数可以调整，可以取等于或大于 0 的值。如果设置为 0，那么梯度增强树和 XGBoost 的预测结果没有区别。此外，Chen &amp; Guestrin 将收缩(即学习率)和列子采样(随机选择特征子集)引入到这种梯度树提升算法中，这允许进一步减少过拟合。因此，它将 AdaBoost(学习率)和随机森林(列或特征子采样)中引入的处理过拟合的方法添加到随机梯度下降模型中的正则化参数中。</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/3d5e39965cdb7058e5a69aa478c58980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*0E_zTQ8Nefnx2Xf1ASfI5A.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">XGBoost 算法的最相关功能概述。资料来源:朱莉娅·尼库尔斯基。</p></figure><p id="af94" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">与 AdaBoost 等其他算法相比，XGBoost 的主要<strong class="kz jj">优势</strong>是其闪电般的速度，以及其成功减少方差的正则化参数。但即使不考虑正则化参数，该算法也利用了学习率(收缩)和来自随机森林等特征的子样本，这增加了它进一步推广的能力。<strong class="kz jj">然而</strong>，与 AdaBoost 和 random forests 相比，XGBoost 更难理解、可视化和调优。有许多超参数可以通过调整来提高性能。</p><p id="e61d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">举几个相关的<strong class="kz jj">超参数</strong>:已经提到了<em class="nj">学习率</em>、<em class="nj">列子采样</em>和<em class="nj">正则化率</em>。此外，<em class="nj">子样本</em>(引导训练样本)<em class="nj">树的最大深度</em>、用于分裂的<em class="nj">子注释中的最小权重</em>以及<em class="nj">估计器数量</em>(树)<em class="nj"> </em>也经常用于解决偏差-方差-权衡。虽然子音符中的估计器、正则化和权重的数量的较高值与减少的过拟合相关联，但是学习速率、最大深度、子采样和列子采样需要具有较低值以实现减少的过拟合。然而，极值将导致<a class="ae lt" href="https://arxiv.org/ftp/arxiv/papers/1901/1901.08433.pdf" rel="noopener ugc nofollow" target="_blank">模型</a>的欠拟合。</p><p id="f3ca" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">当速度和高精度至关重要时，XGBoost 是一个特别有趣的算法。然而，需要更多的资源来训练模型，因为模型调整需要用户更多的时间和专业知识来实现有意义的结果。</p><h1 id="6718" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated"><strong class="ak">结论</strong></h1><p id="57a0" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">何时使用哪种算法取决于您的数据集、资源和知识。我希望这篇概述能更清楚地说明基于树的集成算法的一般优势，AdaBoost、随机森林和 XGBoost 之间的区别，以及何时实现它们中的每一个。</p><p id="a09c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">注意:这篇博文是基于我写的一篇未发表的关于基于树的集成算法的研究论文的一部分。</p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="3ea0" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">你想在媒体上阅读更多高质量的故事吗？考虑注册一个支持我和其他媒体作者的会员。</p><div class="is it gp gr iu nv"><a href="https://medium.com/@julia.nikulski/membership" rel="noopener follow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd jj gy z fp oa fr fs ob fu fw jh bi translated">通过我的推荐链接加入 Medium-Julia Nikulski</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">medium.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj ja nv"/></div></div></a></div></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="e07b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">你有没有想过是什么决定了一部电影的成功？看看我用 AdaBoost、XGBoost 和 LightGBM 预测电影收入的项目<strong class="kz jj">。</strong></p><div class="is it gp gr iu nv"><a rel="noopener follow" target="_blank" href="/predicting-movie-revenue-with-adaboost-xgboost-and-lightgbm-262eadee6daa"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd jj gy z fp oa fr fs ob fu fw jh bi translated">使用 AdaBoost、XGBoost 和 LightGBM 预测电影收入</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">哪些指标决定了电影的成功？</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">towardsdatascience.com</p></div></div><div class="oe l"><div class="ok l og oh oi oe oj ja nv"/></div></div></a></div></div></div>    
</body>
</html>