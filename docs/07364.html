<html>
<head>
<title>Support Vector Machines (SVM) clearly explained: A python tutorial for classification problems with 3D plots</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机(SVM)清楚地解释:一个python教程的分类问题与三维绘图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8?source=collection_archive---------2-----------------------#2020-06-04">https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8?source=collection_archive---------2-----------------------#2020-06-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6e4d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这篇文章中，我解释了支持向量机的核心，为什么以及如何使用它们。此外，我还展示了如何在2D和3D中绘制支持向量和决策边界。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4d0280965b50e27f9d21e66068e3bf64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z_B0o4JbD0C6gpmcenUc4w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">手工制作</strong>草图<strong class="bd ky">作者</strong>。SVM插图。</p></figure><h1 id="fad1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="7c76" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">每个人都听说过著名且广泛使用的<strong class="lt iu">支持向量机</strong> (SVMs)。最初的SVM算法是由Vladimir N. Vapnik和Alexey Ya发明的。1963年的切尔沃嫩基斯。</p><p id="94ac" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">支持向量机</strong>是<strong class="lt iu">监督的</strong>机器学习模型，通常用于<strong class="lt iu">分类</strong> ( <strong class="lt iu"> SVC </strong> —支持向量分类)或<strong class="lt iu">回归</strong> ( <strong class="lt iu"> SVR </strong> —支持向量回归)问题。根据目标变量(我们希望预测的)的特征，如果我们有一个<strong class="lt iu">离散目标变量</strong>(例如分类标签)，我们的问题将是一个分类任务，或者如果我们有一个<strong class="lt iu">连续目标变量</strong>(例如房价)，我们的问题将是一个回归任务。</p><p id="a8f9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">支持向量机更常用于分类问题，因此，在本文中，我将只关注支持向量机模型。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="b085" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">方法的核心</h1><p id="15ee" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我不打算详细介绍算法的每一步(因为有大量的在线资源),而是解释关于支持向量机的最重要的概念和术语。</p><h2 id="b86b" class="ne la it bd lb nf ng dn lf nh ni dp lj ma nj nk ll me nl nm ln mi nn no lp np bi translated">1.决策边界(分隔<strong class="ak">超平面)</strong></h2><p id="ea25" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">SVCs的目标是找到<strong class="lt iu">最佳</strong> <strong class="lt iu">超平面</strong>(也称为<strong class="lt iu">决策</strong> <strong class="lt iu">边界</strong>)使得<strong class="lt iu">最佳</strong> <strong class="lt iu">将</strong>(分裂)一个数据集分成两个类/组(二元分类问题)。</p><p id="defe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">根据输入特征</strong>/变量的数量，<strong class="lt iu">决策</strong> <strong class="lt iu">边界</strong>可以是一条<strong class="lt iu">线</strong>(如果我们只有2个特征)或一个<strong class="lt iu">超平面</strong>(如果我们的数据集中有2个以上的特征)。</p><p id="7668" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了获得主要思想，考虑如下:每个观察(或样本/数据点)被绘制在一个N维空间<strong class="lt iu">中，其中<code class="fe nq nr ns nt b"><strong class="lt iu">N</strong></code>是我们数据集中特征/变量的数量。在那个空间中，分离</strong> <strong class="lt iu">超平面</strong>的<strong class="lt iu">是一个<strong class="lt iu"> (N-1)维子空间</strong>。</strong></p><blockquote class="nu nv nw"><p id="6124" class="lr ls nx lt b lu mn ju lw lx mo jx lz ny mp mc md nz mq mg mh oa mr mk ml mm im bi translated"><strong class="lt iu">超平面</strong>是<strong class="lt iu"> N </strong> - <strong class="lt iu">维</strong>-<strong class="lt iu">空间</strong>的一个(<strong class="lt iu"> N-1 </strong> )- <strong class="lt iu">维</strong>-<strong class="lt iu">子空间</strong>。</p></blockquote><p id="97e7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所以，如前所述，对于一个<strong class="lt iu">二维空间</strong>，<strong class="lt iu">决策</strong>，<strong class="lt iu">边界</strong>将只是一条<strong class="lt iu">线</strong>，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/b49574f09b10035a9401a37dc518eae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVMFi27XsG3Z3-TshhnWFQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">手工制作</strong>草图由<strong class="bd ky">作者</strong>制作。使用只有两个特征(即x1和x2)的数据集来说明SVM分类模型(SVC)的决策边界。决策边界是一条线。</p></figure><p id="16de" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">数学上</strong>，我们可以定义<strong class="lt iu">决定</strong>边界<strong class="lt iu">为:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/7b5c36f3b2440ec775998ae3cd7cac94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*05BDbj8UjUD_he1LVY1lWQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者写的渲染latex代码。</p></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="37e2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你想在交互式路线图和活跃的学习社区的支持下自学数据科学，看看这个资源:<a class="ae od" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h2 id="4b4a" class="ne la it bd lb nf ng dn lf nh ni dp lj ma nj nk ll me nl nm ln mi nn no lp np bi translated">2.支持向量</h2><p id="0d29" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">支持</strong> <strong class="lt iu">向量</strong>就是距离<strong class="lt iu">分离</strong> <strong class="lt iu">超平面</strong>最近<strong class="lt iu">的样本(数据点)。如果这些样本被移除，它们将改变分离超平面的位置。由此可见，这些就是<strong class="lt iu">最重要的</strong> <strong class="lt iu">最重要的</strong> <strong class="lt iu">样本</strong>即<strong class="lt iu">定义</strong><strong class="lt iu"/>位置和方位的<strong class="lt iu">最佳</strong> <strong class="lt iu">决定</strong> <strong class="lt iu">边界</strong>。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4d0280965b50e27f9d21e66068e3bf64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z_B0o4JbD0C6gpmcenUc4w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">手工制作</strong>草图<strong class="bd ky">作者</strong>。在这个玩具二维SVM问题中，用紫色圈起来的点代表支持向量。</p></figure><h2 id="e391" class="ne la it bd lb nf ng dn lf nh ni dp lj ma nj nk ll me nl nm ln mi nn no lp np bi translated">3.硬边缘:SVM如何找到最佳超平面？</h2><p id="e86e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">几条不同的线(或者一般来说，不同的决策边界)可以分隔我们的类。但是哪一个是最好的呢？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/d730ff51ebc19b09d496442776c7411c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KHoBdr_TE7TdgItE0JuUVA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">手工制作</strong>草图由<strong class="bd ky">作者</strong>制作。此图显示了分隔两个类别的3个候选决策边界。</p></figure><p id="7fb3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">超平面与最近的数据点(样本)之间的<strong class="lt iu">距离</strong>称为<strong class="lt iu"/><strong class="lt iu"/><strong class="lt iu">余量</strong>。目标是在<strong class="lt iu">超平面</strong>和任何<strong class="lt iu">支持</strong> <strong class="lt iu">向量之间选择一个具有最大<strong class="lt iu">可能</strong> <strong class="lt iu">余量</strong>的超平面。</strong> SVM算法寻找最佳决策边界，如边际最大化。这里最好的线是黄线，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/c4b45328aba4a5111a6582a093027c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_yOWAJaNw9rSITj08M345Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">手工制作</strong>草图<strong class="bd ky">作者</strong>。最佳分隔线是最大化边距(绿色距离)的黄色分隔线。</p></figure><p id="c01e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在<strong class="lt iu">摘要</strong>中，支持向量机选择<strong class="lt iu">最大化</strong>到<strong class="lt iu">支持向量</strong>的<strong class="lt iu">距离</strong>的决策边界。以最大化<strong class="lt iu">到支持向量的距离</strong>的方式绘制<strong class="lt iu">决策</strong> <strong class="lt iu">边界</strong>。如果决策边界太靠近支持向量，那么它将对噪声敏感并且不能很好地概括。</p><h2 id="8d44" class="ne la it bd lb nf ng dn lf nh ni dp lj ma nj nk ll me nl nm ln mi nn no lp np bi translated">4.关于软边界和C参数的一个注记</h2><p id="a07a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">有时，我们可能想要(故意)允许一些<strong class="lt iu">误差</strong>(错误分类)。这就是“<strong class="lt iu">软</strong> <strong class="lt iu">边距</strong>”背后的主旨。软边界实现允许一些样本被错误分类或者位于决策边界的错误一侧，从而允许高度一般化的模型。</p><p id="d362" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一个<strong class="lt iu">软</strong>余量<strong class="lt iu">SVM</strong>解决了下面的优化问题:</p><ul class=""><li id="6505" class="og oh it lt b lu mn lx mo ma oi me oj mi ok mm ol om on oo bi translated"><strong class="lt iu">增加</strong>决策边界的<strong class="lt iu">距离</strong>到<strong class="lt iu">支持向量</strong> s(即余量)并且</li><li id="fee1" class="og oh it lt b lu op lx oq ma or me os mi ot mm ol om on oo bi translated"><strong class="lt iu">最大化</strong>训练集中<strong class="lt iu">正确</strong> <strong class="lt iu">分类</strong>的点数。</li></ul><p id="b7e1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">很明显，这两个优化目标之间有一个<strong class="lt iu">的权衡。这种权衡由著名的<strong class="lt iu"> C </strong>参数控制。简而言之，如果<strong class="lt iu"> C </strong>为<strong class="lt iu">小</strong>，则误分类数据点的罚分为<strong class="lt iu">低</strong>，因此选择具有<strong class="lt iu">大</strong> <strong class="lt iu">裕量</strong>的<strong class="lt iu">决策</strong> <strong class="lt iu">边界</strong>，代价是更大数量的误分类。如果<strong class="lt iu"> C </strong>是<strong class="lt iu">大</strong>，<strong class="lt iu"> SVM </strong>试图<strong class="lt iu">最小化<strong class="lt iu">误分类</strong>样本的数量，并导致<strong class="lt iu">判定</strong> <strong class="lt iu">边界</strong>具有<strong class="lt iu">较小</strong> <strong class="lt iu">余量</strong>。</strong></strong></p><h2 id="c35c" class="ne la it bd lb nf ng dn lf nh ni dp lj ma nj nk ll me nl nm ln mi nn no lp np bi translated">5.当没有清晰的分离超平面(核SVM)时会发生什么？</h2><p id="d65e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果我们有一个数据集是线性可分的，那么支持向量机的工作通常很容易。然而，在现实生活中，在大多数情况下，我们手头都有一个线性不可分的数据集，这正是<strong class="lt iu">内核技巧</strong>提供一些魔力的时候。</p><p id="f5f8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">内核</strong> <strong class="lt iu">技巧</strong>将原始数据点投影到一个更高维的空间中，以便使它们可以线性分离(在那个更高维的空间中)。</p><blockquote class="nu nv nw"><p id="9acd" class="lr ls nx lt b lu mn ju lw lx mo jx lz ny mp mc md nz mq mg mh oa mr mk ml mm im bi translated">因此，通过使用核技巧，我们可以使我们的非线性可分的数据，在一个更高维的空间中线性可分。</p></blockquote><p id="e467" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">内核技巧是基于一些<strong class="lt iu">内核</strong> <strong class="lt iu">函数</strong>来测量样本的相似性。这个技巧实际上并没有将数据点转换到一个新的高维特征空间，显式地<strong class="lt iu"/>。核SVM根据高维特征空间中的相似性度量来计算决策边界，而不实际进行投影。一些著名的核函数包括线性、多项式、径向基函数(RBF)和sigmoid核。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/83d334b40d0f5eea40d0cb02262cd42a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k42usMPNYivRTrN6iKc-rQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">手工制作</strong>草图<strong class="bd ky">作者</strong>。内核技巧。在原始空间中，数据不是线性可分的，但是在投影到更高维空间后，它们是线性可分的。</p></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="d7eb" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">在scikit-learn中使用Iris数据集和线性SVC模型的Python工作示例</h1><p id="e409" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">提醒:虹膜数据集由150个花样本组成，每个样本具有4个特征/变量(即萼片宽度/长度和花瓣宽度/长度)。</p><h1 id="9ddb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">2D</h1><p id="237a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">让我们在2D绘制决策边界(我们将只使用数据集的2个特征):</strong></p><pre class="kj kk kl km gt ov nt ow ox aw oy bi"><span id="5983" class="ne la it nt b gy oz pa l pb pc">from sklearn.svm import SVC<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn import svm, datasets</span><span id="8779" class="ne la it nt b gy pd pa l pb pc">iris = datasets.load_iris()# Select 2 features / variables<br/>X = iris.data[:, :2] # we only take the first two features.<br/>y = iris.target<br/>feature_names = iris.feature_names[:2]<br/>classes = iris.target_names</span><span id="d1e7" class="ne la it nt b gy pd pa l pb pc">def make_meshgrid(x, y, h=.02):<br/>    x_min, x_max = x.min() — 1, x.max() + 1<br/>    y_min, y_max = y.min() — 1, y.max() + 1<br/>    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))<br/>    return xx, yy</span><span id="9b3b" class="ne la it nt b gy pd pa l pb pc">def plot_contours(ax, clf, xx, yy, **params):<br/>    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])<br/>    Z = Z.reshape(xx.shape)<br/>    out = ax.contourf(xx, yy, Z, **params)<br/>    return out</span><span id="44ae" class="ne la it nt b gy pd pa l pb pc"># The classification SVC model<br/>model = svm.SVC(kernel="linear")<br/>clf = model.fit(X, y)</span><span id="47d9" class="ne la it nt b gy pd pa l pb pc">fig, ax = plt.subplots()</span><span id="9957" class="ne la it nt b gy pd pa l pb pc"># title for the plots<br/>title = (‘Decision surface of linear SVC ‘)<br/># Set-up grid for plotting.<br/>X0, X1 = X[:, 0], X[:, 1]<br/>xx, yy = make_meshgrid(X0, X1)</span><span id="cc9a" class="ne la it nt b gy pd pa l pb pc">plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)<br/>ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors="k")<br/>ax.set_ylabel("{}".format(feature_names[0]))<br/>ax.set_xlabel("{}".format(feature_names[1]))<br/>ax.set_xticks(())<br/>ax.set_yticks(())<br/>ax.set_title(title)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/5373f1574a47379f0133d66662c2bc50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qmNy2D4ZCheIpcUvbhgemQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">以上python代码的输出。作者创作的赋格曲。</p></figure><p id="09ed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在虹膜数据集中，我们有3类花和4个特征。这里，我们仅使用2个特征(因此我们有一个2维特征空间)T1，并且我们绘制了线性SVC模型的决策边界。点的颜色对应于类别/组。</p><h1 id="bfac" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">三维（three dimension的缩写）</h1><p id="a0aa" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">让我们在3D中绘制决策边界(我们将只使用数据集的3个特征):</strong></p><pre class="kj kk kl km gt ov nt ow ox aw oy bi"><span id="9844" class="ne la it nt b gy oz pa l pb pc">from sklearn.svm import SVC<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn import svm, datasets<br/>from mpl_toolkits.mplot3d import Axes3D</span><span id="ebad" class="ne la it nt b gy pd pa l pb pc">iris = datasets.load_iris()<br/>X = iris.data[:, :3] # we only take the first three features.<br/>Y = iris.target</span><span id="b4e1" class="ne la it nt b gy pd pa l pb pc">#make it binary classification problem<br/>X = X[np.logical_or(Y==0,Y==1)]<br/>Y = Y[np.logical_or(Y==0,Y==1)]</span><span id="2711" class="ne la it nt b gy pd pa l pb pc">model = svm.SVC(kernel='linear')<br/>clf = model.fit(X, Y)</span><span id="7bd1" class="ne la it nt b gy pd pa l pb pc"># The equation of the separating plane is given by all x so that np.dot(svc.coef_[0], x) + b = 0.</span><span id="a8f7" class="ne la it nt b gy pd pa l pb pc"># Solve for w3 (z)<br/>z = lambda x,y: (-clf.intercept_[0]-clf.coef_[0][0]*x -clf.coef_[0][1]*y) / clf.coef_[0][2]<br/>tmp = np.linspace(-5,5,30)<br/>x,y = np.meshgrid(tmp,tmp)</span><span id="f2df" class="ne la it nt b gy pd pa l pb pc">fig = plt.figure()<br/>ax = fig.add_subplot(111, projection='3d')<br/>ax.plot3D(X[Y==0,0], X[Y==0,1], X[Y==0,2],'ob')<br/>ax.plot3D(X[Y==1,0], X[Y==1,1], X[Y==1,2],'sr')<br/>ax.plot_surface(x, y, z(x,y))<br/>ax.view_init(30, 60)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/225545e95aff0fa333c1bffec992e341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l1JnixlQTZiZfZPW2YY3sw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">以上python代码的输出。图由作者生成。</p></figure><p id="8785" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在虹膜数据集中，我们有3类花和4个特征。这里我们只使用了3个特征(所以我们有一个<strong class="lt iu">三维特征空间</strong>)和<strong class="lt iu">只有2个类</strong>(二进制分类问题)。然后，我们绘制了线性SVC模型的决策边界。点的颜色对应于2个类别/组。</p><h1 id="f769" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">绘制支持向量</strong></h1><pre class="kj kk kl km gt ov nt ow ox aw oy bi"><span id="5a3a" class="ne la it nt b gy oz pa l pb pc">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn import svm<br/>np.random.seed(2)</span><span id="6097" class="ne la it nt b gy pd pa l pb pc"># we create 40 linearly separable points<br/>X = np.r_[np.random.randn(20, 2) — [2, 2], np.random.randn(20, 2) + [2, 2]]<br/>Y = [0] * 20 + [1] * 20</span><span id="31be" class="ne la it nt b gy pd pa l pb pc"># fit the model<br/>clf = svm.SVC(kernel=’linear’, C=1)<br/>clf.fit(X, Y)</span><span id="5f2b" class="ne la it nt b gy pd pa l pb pc"># get the separating hyperplane<br/>w = clf.coef_[0]<br/>a = -w[0] / w[1]<br/>xx = np.linspace(-5, 5)<br/>yy = a * xx — (clf.intercept_[0]) / w[1]</span><span id="bdb7" class="ne la it nt b gy pd pa l pb pc">margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))<br/>yy_down = yy — np.sqrt(1 + a ** 2) * margin<br/>yy_up = yy + np.sqrt(1 + a ** 2) * margin</span><span id="19cc" class="ne la it nt b gy pd pa l pb pc">plt.figure(1, figsize=(4, 3))<br/>plt.clf()<br/>plt.plot(xx, yy, "k-")<br/>plt.plot(xx, yy_down, "k-")<br/>plt.plot(xx, yy_up, "k-")</span><span id="2ca9" class="ne la it nt b gy pd pa l pb pc">plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,<br/> facecolors="none", zorder=10, edgecolors="k")<br/>plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,<br/> edgecolors="k")<br/>plt.xlabel("x1")<br/>plt.ylabel("x2")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/777bd3bb9e925e13bf9f568eb46d6043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QgILzwi9ngc9MRWFwC03iA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">以上python代码的输出。图由作者生成。</p></figure><p id="61d4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">双</strong> - <strong class="lt iu">圆圈</strong> <strong class="lt iu">点</strong>代表<strong class="lt iu">支持</strong> <strong class="lt iu">向量</strong>。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="09f5" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">最新帖子</h1><div class="ph pi gp gr pj pk"><a rel="noopener follow" target="_blank" href="/time-series-forecasting-predicting-stock-prices-using-facebooks-prophet-model-9ee1657132b5"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd iu gy z fp pp fr fs pq fu fw is bi translated">时间序列预测:用脸书的先知模型预测股票价格</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">使用可从《先知脸书》公开获得的预测模型预测股票价格</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">towardsdatascience.com</p></div></div><div class="pt l"><div class="pu l pv pw px pt py ks pk"/></div></div></a></div><div class="ph pi gp gr pj pk"><a rel="noopener follow" target="_blank" href="/roc-curve-explained-using-a-covid-19-hypothetical-example-binary-multi-class-classification-bab188ea869c"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd iu gy z fp pp fr fs pq fu fw is bi translated">用新冠肺炎假设的例子解释ROC曲线:二分类和多分类…</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">在这篇文章中，我清楚地解释了什么是ROC曲线以及如何阅读它。我用一个新冠肺炎的例子来说明我的观点，我…</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">towardsdatascience.com</p></div></div><div class="pt l"><div class="pz l pv pw px pt py ks pk"/></div></div></a></div><div class="ph pi gp gr pj pk"><a rel="noopener follow" target="_blank" href="/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd iu gy z fp pp fr fs pq fu fw is bi translated">PCA清楚地解释了——如何、何时、为什么使用它以及特性的重要性:Python指南</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">在这篇文章中，我解释了什么是PCA，何时以及为什么使用它，以及如何使用scikit-learn在Python中实现它。还有…</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">towardsdatascience.com</p></div></div><div class="pt l"><div class="qa l pv pw px pt py ks pk"/></div></div></a></div><div class="ph pi gp gr pj pk"><a rel="noopener follow" target="_blank" href="/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd iu gy z fp pp fr fs pq fu fw is bi translated">关于Python中的最小-最大规范化，您需要知道的一切</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">在这篇文章中，我将解释什么是最小-最大缩放，什么时候使用它，以及如何使用scikit在Python中实现它</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">towardsdatascience.com</p></div></div><div class="pt l"><div class="qb l pv pw px pt py ks pk"/></div></div></a></div><div class="ph pi gp gr pj pk"><a rel="noopener follow" target="_blank" href="/how-and-why-to-standardize-your-data-996926c2c832"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd iu gy z fp pp fr fs pq fu fw is bi translated">Scikit-Learn的标准定标器如何工作</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">在这篇文章中，我将解释为什么以及如何使用scikit-learn应用标准化</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">towardsdatascience.com</p></div></div><div class="pt l"><div class="qc l pv pw px pt py ks pk"/></div></div></a></div></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="3ae0" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">敬请关注并支持这一努力</h1><p id="05c2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果你喜欢并发现这篇文章有用，请关注我！</p><p id="878b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有问题吗？把它们作为评论贴出来，我会尽快回复。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="f003" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">参考</h1><p id="3d49" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1]<a class="ae od" href="https://www.nature.com/articles/nbt1206-1565" rel="noopener ugc nofollow" target="_blank">https://www.nature.com/articles/nbt1206-1565</a></p><p id="bb63" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae od" href="https://en.wikipedia.org/wiki/Support_vector_machine" rel="noopener ugc nofollow" target="_blank"/><a class="ae od" href="https://en.wikipedia.org/wiki/Support_vector_machine" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Support_vector_machine</a></p><p id="cf37" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[2]<a class="ae od" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . SVM . SVC . html</a></p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="0b24" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">和我联系</h1><ul class=""><li id="39e4" class="og oh it lt b lu lv lx ly ma qd me qe mi qf mm ol om on oo bi translated"><strong class="lt iu">领英</strong>:【https://www.linkedin.com/in/serafeim-loukas/】T4</li><li id="eb3f" class="og oh it lt b lu op lx oq ma or me os mi ot mm ol om on oo bi translated"><strong class="lt iu">研究之门</strong>:<a class="ae od" href="https://www.researchgate.net/profile/Serafeim_Loukas" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/profile/Serafeim_Loukas</a></li><li id="c108" class="og oh it lt b lu op lx oq ma or me os mi ot mm ol om on oo bi translated"><strong class="lt iu">https://people.epfl.ch/serafeim.loukas</strong>EPFL<strong class="lt iu">简介</strong> : <a class="ae od" href="https://people.epfl.ch/serafeim.loukas" rel="noopener ugc nofollow" target="_blank">美国</a></li><li id="06d4" class="og oh it lt b lu op lx oq ma or me os mi ot mm ol om on oo bi translated"><strong class="lt iu">堆栈</strong> <strong class="lt iu">溢出</strong>:<a class="ae od" href="https://stackoverflow.com/users/5025009/seralouk" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/users/5025009/seralouk</a></li></ul></div></div>    
</body>
</html>