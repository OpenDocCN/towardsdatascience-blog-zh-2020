<html>
<head>
<title>Think twice before you use Principal Component Analysis in supervised learning tasks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在监督学习任务中使用主成分分析之前，请三思</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/think-twice-before-you-use-principal-component-analysis-in-supervised-learning-tasks-70fbb68ebd0c?source=collection_archive---------13-----------------------#2020-04-07">https://towardsdatascience.com/think-twice-before-you-use-principal-component-analysis-in-supervised-learning-tasks-70fbb68ebd0c?source=collection_archive---------13-----------------------#2020-04-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3020" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">因为它通常弊大于利</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6b9a799f32955e6094c53cc6d8bcbd58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*W-BylksSkmhdFu8O"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">布鲁斯·马斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="3e5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主成分分析(PCA)是最流行的机器学习技术之一。它降低了给定数据集的维度，使数据集更易于处理，计算成本更低，同时保留了大多数模式和趋势。这使得 PCA 成为探索性数据分析的优秀工具。然而，我们声称在监督学习任务中使用 PCA，特别是在流水线中，例如</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a4a6" class="ma mb it lw b gy mc md l me mf"><em class="mg">data</em> → <em class="mg">PCA</em> → <em class="mg">supervised learning algorithm</em></span></pre><p id="5bc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常弊大于利。我们将在本文中解释主要问题。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="2b86" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">PCA 不考虑标签</h2><p id="3f57" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们简单回顾一下 PCA 是如何工作的。粗略地说，主成分分析将数据集几何投影到更少的维度上，其中新的变量称为主成分。这是以这样的方式完成的，即主分量是正交的，并且具有最大可能的方差。下图显示了一个玩具示例，其中二维数据被投影到一维空间，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/5732b8cbe42e29ea67d073585950da8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eWbUkU2etLz6vmAqsHX1Dg.png"/></div></div></figure><p id="6f47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中水平虚线对应于最大变化的方向。请注意，PCA 是一种不受监督的方法，这意味着它在计算中不使用任何标签。</p><p id="655e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们进一步假设上面的玩具例子属于二进制分类任务，并且两个类被着色为红色和黑色。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/d76089af49f3c6bd059ec8043fdb238a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9jEYN5okmKNxjsodQgIow.png"/></div></div></figure><p id="f7ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以很容易地看到，纵坐标将是一个强大的特征，在分类红色类。不幸的是，如果我们应用 PCA，那么这样的特征将会消失。发生这种现象是因为标注可能与特征的方差不相关。即使真实世界数据集不会如此极端，PCA 仍然很容易丢弃强分类信号。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="f898" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">主成分通常是不可解释的</h2><p id="f6f9" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">大多数监督学习算法，从逻辑回归、基于树的算法到神经网络，都能够评估输入特征的重要性。这些重要特性非常有价值，因为它们</p><ol class=""><li id="6a8c" class="nl nm it lb b lc ld lf lg li nn lm no lq np lu nq nr ns nt bi translated">帮助用户发现漏洞和问题，如数据泄漏</li><li id="007c" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">指出进一步探索和特征工程可能有益的领域</li><li id="df58" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">指明哪种数据更值得获取</li></ol><p id="76c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">诸如此类。然而，这是基于功能是可解释的假设，允许用户采取相关的行动。</p><p id="9985" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，假设我们知道卧室的数量是我们当前模型在预测房价时最重要的特征。然后我们可以尝试获得更多关于卧室大小、卧室窗户数量等数据。</p><p id="71b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，考虑我们的模型的输入特征是主成分的情况，主成分是原始特征的线性组合。即使我们知道最重要的特征是由</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4500" class="ma mb it lw b gy mc md l me mf">0.18 * number_of_bedroom + 0.15 * house_size + ......</span></pre><p id="c906" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们很难决定后续行动。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="1cae" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">PCA 不是尺度不变的</h2><p id="4d5d" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">基于树的算法，如随机森林和 XGBoost，可以说是最好的现成监督学习算法，部分原因是它们对特征的缩放不变。事实上，它们甚至对于特征的单调变换是不变的。当我们的数据集包含不同单位的要素时，这尤其有用。例如，我们可能在人口普查数据集中有年龄、身高、体重和工资。我们不需要担心我们应该使用千克还是磅作为重量，如何标准化工资，因为我们基于树的算法将简单地以相同的方式工作。</p><p id="1866" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，如果我们有一个 PCA 管道，后面跟着一个监督学习算法，那么我们需要再次担心特征缩放和归一化。这是因为五氯苯甲醚对比例敏感。即使我们使用需要特征缩放和归一化的监督学习算法，也需要额外的小心。盲目标准化所有特征可能会扭曲数据，并由于噪声看起来很大而产生变化，扭曲主成分的计算。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="5fec" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">常见问题</h2><p id="24df" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">问:在监督学习任务中，应该使用什么来降低数据集的维度？</p><p id="3a31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">答:我们可以首先计算例如每个特征和标签之间的皮尔逊相关系数或斯皮尔曼相关系数。然后我们丢弃那些与标签不相关的特征。</p><p id="c969" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以训练一个像 Lasso 这样的简单模型，只保留重要度高的特征。<a class="ae ky" href="https://scikit-learn.org/stable/modules/feature_selection.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn 文档</a>有一个全面的特性选择方法列表。</p><p id="41a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问:在有监督的学习中，我们可以使用 PCA 吗？</p><p id="e6b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">答:PCA 对于探索和理解数据集非常有用。对于 PCA 之后是监督学习算法的管道，由于上面列出的原因，它们不适合模型迭代。然而，对于快速构建模型性能基准这样的任务，它们是很方便的。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="2fd8" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">进一步阅读</h2><ol class=""><li id="0b9e" class="nl nm it lb b lc nf lf ng li nz lm oa lq ob lu nq nr ns nt bi translated">[1]提供了五氯苯甲醚的简短总结。关于 PCA 局限性的数字很有启发性。</li><li id="6d72" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">[2]是 PCA 的综合教程。它包含数学细节以及对算法各方面的直观理解，并有许多很好的例子的帮助。</li></ol></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="13c7" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">参考</h2><ol class=""><li id="296c" class="nl nm it lb b lc nf lf ng li nz lm oa lq ob lu nq nr ns nt bi translated">J.Lever，M. Krzywinski 和 N. Altman。<a class="ae ky" href="https://www.nature.com/articles/nmeth.4346" rel="noopener ugc nofollow" target="_blank">主成分分析</a> (2017)，自然方法 14 <strong class="lb iu">，</strong>641–642</li><li id="2f89" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">J.史伦斯。<a class="ae ky" href="https://arxiv.org/abs/1404.1100" rel="noopener ugc nofollow" target="_blank">主成分分析教程</a> (2014)，arXiv</li></ol></div></div>    
</body>
</html>