<html>
<head>
<title>Logistic Regression using Gradient Descent Optimizer in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中使用梯度下降优化器的逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-using-gradient-descent-optimizer-in-python-485148bd3ff2?source=collection_archive---------9-----------------------#2020-01-21">https://towardsdatascience.com/logistic-regression-using-gradient-descent-optimizer-in-python-485148bd3ff2?source=collection_archive---------9-----------------------#2020-01-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/ea257da38cbaa406de054e0ae2b23e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xoD1Gv4Zmq07YKx3"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">由<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kf" href="https://unsplash.com/@chuttersnap?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> chuttersnap </a>拍摄</p></figure><p id="d9b6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将对逻辑回归进行硬编码，并将使用梯度下降优化器。如果你需要梯度下降的复习，可以看看我之前的<a class="ae kf" href="https://link.medium.com/fyYd61gnU1" rel="noopener">文章</a>。在这里，我将使用著名的<a class="ae kf" href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" rel="noopener ugc nofollow" target="_blank"> Iris数据集</a>来预测使用逻辑回归的类，而不使用scikit-learn库中的逻辑回归模块。开始吧！</p><h2 id="75a8" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">导入库</h2><p id="1a00" class="pw-post-body-paragraph kg kh it ki b kj lx kl km kn ly kp kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">让我们从导入所有需要的库和数据集开始。这个数据集有3个类。但是为了让你更容易理解，我将用两个类来演示梯度下降法。数据集有150个实例，3个类中的每一个都有50个实例。因此，让我们只考虑前100个实例中的两个类:</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/7cbd4e91a4756d9b5bfbf3107aedc5ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*py-Kv02OPguyUPc9ZiTxzQ.png"/></div></figure><h2 id="15d7" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">标签编码</h2><p id="a445" class="pw-post-body-paragraph kg kh it ki b kj lx kl km kn ly kp kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">这里的类必须<a class="ae kf" href="https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621" rel="noopener">标签编码</a>算法才能工作。让我们通过scikit-learn的LabelEncoder模块来完成它。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/bf7def9e84070d5ae195d9ede7d6db02.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*V89xEFT1cHt31nPbCCfj0Q.png"/></div></figure><p id="3aff" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个虹膜数据集是一个相当简单和直接的数据集。如你所见，只有4节课。没有空值和差异，因为在标签编码后它是纯数字的。</p><h2 id="759e" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">洗牌和分割数据帧</h2><p id="3435" class="pw-post-body-paragraph kg kh it ki b kj lx kl km kn ly kp kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">这些类在数据帧中被排序，因此它需要被打乱并分成两部分——训练和测试。使用scikit-learn中的<a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank"> train_test_split </a>只需一行即可完成。但是让我们在没有它的情况下做，看看在引擎盖下会发生什么。这个方法可能不是train_test_split的实际工作方式，但这肯定是一种实现方式。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/cffdc4b185b5f067a6bc2e6c45080d02.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*vgjKzjona2yA-7P0LfLpWQ.png"/></div></figure><p id="bd66" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，让我们使用scikit-learn的StandardScaler对数据进行标准化，即使它的规模几乎相同:</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="b479" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们的数据看起来像这样:</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/1c182da7c9d52c4b95a0ebe69a5de3a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*N7QynfiQpCsILTMuJgraUw.png"/></div></figure><p id="befc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将训练和测试数据70:30分开，</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="b61f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据准备好应用<strong class="ki iu">梯度下降优化器</strong>。下一步将应用GD来寻找损失最小的权重的最佳值。由于有4个参数影响类别，因此使用的公式为:</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/e06de56a3cf2d51d17ccdd3d9eaf3823.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*gXT5JZkCvxv2TeVZnVmOag.png"/></div></figure><p id="6387" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中，ŷ是预测值，w是权重，x是输入数据。但是我们需要将类预测为0和1，因此我们需要修改上面的<em class="mn">回归方程</em>，以便输出成为<em class="mn">默认类</em>的概率，该概率将在0和1之间。如果高于0.5，我们将其分配到1类，反之，如果低于0.5，我们将其分配到0类。</p><p id="df78" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，为了修改回归方程，我们将它与<a class="ae kf" href="https://en.wikipedia.org/wiki/Sigmoid_function#targetText=A%20sigmoid%20function%20is%20a,given%20in%20the%20Examples%20section" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> sigmoid </strong> </a>函数、<strong class="ki iu"> σ </strong>相乘，其输出如下:</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/4d8291ee2d2842489d2e91bc35027d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jSCPkJo0ZpBRA5H3JqFhQg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Sigmoid_function#targetText=A%20sigmoid%20function%20is%20a,given%20in%20the%20Examples%20section" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="544d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">sigmoid函数定义为:</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/cf5ccfcee532bd82a96390f613f96d58.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*oX0vGkMzqENKPr5lSHmd7Q.png"/></div></figure><p id="752f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">逻辑函数和大多数二元分类器中使用的损失函数是二元交叉熵损失函数，由下式给出:</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/2d1b36ccb0310de6c894693bbccd4fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*mrPYf6UAx7E8ac_t3Gbhng.png"/></div></figure><p id="a7f7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">求解上述方程，我们得到误差为(P - Y ),其中P是属于默认类别的预测概率。因此，权重的更新规则如下:</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mr"><img src="../Images/58db894265cae32c8c97b686ccebf671.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*R08eSSxVeRCChCL4yXgfNg.png"/></div></div></figure><p id="8613" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们在准备好的训练数据集上实现它，看看结果。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="ed69" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">完整的代码可以在我的<a class="ae kf" href="https://github.com/chayankathuria/LogReg01" rel="noopener ugc nofollow" target="_blank"> Git </a>上找到。预测概率后，实例被分成两类。如果概率是&gt; 0.5，则分配给等级1，否则为0。权重被更新，并且运行70次迭代。现在权重已经达到它们的新值，这应该是最佳值。</p><p id="9b1f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们必须评估模型的表现。让我们检查一下模型是否对任何实例进行了错误分类。此外，让我们来看看算法最后一次迭代后更新的权重:</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ms"><img src="../Images/e5f079eeb6adfd79cd8da5546d9e198e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mjmkvnwCg6Ta4W0WcE4sTA.png"/></div></div></figure><p id="3a1e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在为了评估，我们将构建一个<a class="ae kf" rel="noopener" target="_blank" href="/understanding-confusion-matrix-a9ad42dcfd62"> <strong class="ki iu">混淆矩阵</strong> </a>。混淆矩阵包含4个值:真阳性，假阳性，真阴性，假阴性。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="399d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到以下值——TP:34，FP: 0，TN: 36，FN: 0，混淆矩阵为:</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/a59edc34db913a612481f6a60e6340d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*g5zpskPaxO8uSl0OWT4NTQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" rel="noopener" target="_blank" href="/understanding-confusion-matrix-a9ad42dcfd62">来源</a></p></figure><p id="a0c8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">酷毙了。现在，让我们最后将学习到的权重应用到我们的测试数据中，并检查它的表现如何。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="2f8e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">完成了。现在，让我们通过构建混淆矩阵来检查它的表现如何:</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="eb71" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好吧！我们得到了TP: 16，FP: 0，TN: 14，FN: 0。这意味着100%的精确度和100%的召回率！你也可以继续检查F1的分数。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="251a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个非常小的玩具数据集，因此结果是准确的。当您处理稍大的数据集时，情况就不一样了。但是本文的目的是向您展示使用梯度下降的逻辑回归到底是如何工作的。</p></div></div>    
</body>
</html>