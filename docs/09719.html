<html>
<head>
<title>Dueling Double Deep Q Learning using Tensorflow 2.x</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Tensorflow 2.x 决斗双深度 Q 学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dueling-double-deep-q-learning-using-tensorflow-2-x-7bbbcec06a2a?source=collection_archive---------21-----------------------#2020-07-10">https://towardsdatascience.com/dueling-double-deep-q-learning-using-tensorflow-2-x-7bbbcec06a2a?source=collection_archive---------21-----------------------#2020-07-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7250" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用 TensorFlow 2.x 实现决斗式双深度 q 学习</h2></div><p id="0851" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将了解决斗双深度 q 学习的概念和代码。这涉及到一些应用于深度 Q 学习方法的改进技术。让我们首先理解这个方法背后的概念，然后我们将研究代码。</p><h2 id="74de" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">双 DQN</h2><p id="644d" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">这种方法解决了 DQN 高估的问题。这种高估是由于在 Q 学习更新等式中存在下一个状态的最大 Q 值。Q 值上的最大值算子导致最大化偏差，这可能导致代理在某些环境中的不良性能，在这些环境中，真实值的最大值为零，但是代理的最大估计值为正。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/fb716dede0fccbf5298eda61b43e534b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*28Kww5G48kzRdu1WcQo3YA.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">双 Q 学习更新，图像通过强化学习:<br/>理查德·萨顿和安德鲁·g·巴尔托介绍</p></figure><p id="e52e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将在代码中使用上述等式的深度 RL 版本。</p><h2 id="223b" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">决斗 DQN</h2><p id="7a31" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">在决斗 DQN，有两种不同的估计如下:</p><ol class=""><li id="68e9" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated">估计给定状态的值:这估计代理处于该状态有多好。</li><li id="09f4" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">对一个状态中每个动作的优势进行估计。</li></ol><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ca15cc5f23bf8d8765e34564c7308f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*n8UyR2HxQPudoBbZ6z4MjA.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">我们的 DDDQN</p></figure><p id="3f52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这两个估计值可以汇总为[Q = V+A-所有 A 的平均值]</p><h1 id="35d2" class="na lc iq bd ld nb nc nd lg ne nf ng lj jw nh jx lm jz ni ka lp kc nj kd ls nk bi translated">使用 Tensorflow 2.x 的代码</h1><h2 id="eef4" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">神经网络:</h2><ol class=""><li id="1797" class="ml mm iq kh b ki lu kl lv ko nl ks nm kw nn la mq mr ms mt bi translated">我们从使用 TensorFlow 子类 API 定义 DDDQN 类开始，注意我们使用了两个密集层，因为我们有数字输入。您不需要指定任何输入形状。</li><li id="f82c" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">请注意，我们的调用函数的状态值为“v ”, advantage 为“and Q 是聚合。</li><li id="6157" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">我们还为动作选择定义了一个优势方法。</li></ol><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="no np l"/></div></figure><h2 id="1ee6" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">体验回放:</h2><ol class=""><li id="5bfc" class="ml mm iq kh b ki lu kl lv ko nl ks nm kw nn la mq mr ms mt bi translated">我们的经验重放类包含两个方法，一个用于存储经验，另一个用于获取训练样本。</li><li id="f565" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">我们首先将内存定义为全零初始化的 numpy 数组。请注意，数据类型应该根据环境而定。</li><li id="8760" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">我们有一个指针，从零开始，每次存储经验时递增。请注意，索引是通过取模来定义的，因为索引永远不会超过缓冲区的大小。</li><li id="9cd7" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">在采样方法中，我们首先检查缓冲区是否已满，并且仅从可用的经验中进行采样。这是因为我们不想采样零值进行训练。</li></ol><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="no np l"/></div></figure><h2 id="8b07" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">代理人:</h2><p id="64c2" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">我们的代理类包含以下方法:</p><p id="5b07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">初始值设定项:</p><ol class=""><li id="2fb8" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated">我们首先定义所有需要的变量，并编译我们的目标网络和 q 网络。</li><li id="563d" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">self.trainstep 用零初始化，它记录我们的 q 模型被训练的次数。</li><li id="f55d" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">self.replace 用于在目标网络中复制 q 个网络权重。我们将在培训方法中使用这一点。</li></ol><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="d463" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">行动选择方法:</p><ol class=""><li id="1142" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated">我们希望我们的代理选择当时的最佳行动，但我们也希望我们的代理探索其他行动的优化。为此，我们采取随机行动，直到随机值小于或等于ε。</li><li id="e65a" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">如果随机值大于ε，我们通过使用模型中的优势方法采取最佳行动。</li></ol><p id="0384" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更新 _ε，更新 _ 记忆，更新 _ 目标:</p><ol class=""><li id="d322" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated">update_epsilon 用于每次训练后更新 epsilon 值。请注意，epsilon 的最小值可以是 0.01，就像前面在 __init__ 方法中指定的那样。</li><li id="6b75" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">Update_mem 充当连接器 b/w 代理，并在体验重放中存储体验方法。</li><li id="a268" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">Update_target 用于用 q 个网络权重更新我们的目标模型。</li></ol><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="e95c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DDDQN 训练方法:</p><ol class=""><li id="eee1" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated">训练方法用于为 q 值训练我们的 q 网络。</li><li id="f934" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">该方法以一个条件开始，即直到经验缓冲区具有至少批量大小的足够数量的经验，训练才会开始。</li><li id="0aa2" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">下一个条件在 q 网络被训练自替换次数后，每次更新目标网络。</li><li id="1409" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">然后我们抽样一批经验。这些样本批次是状态、动作、奖励、下一个状态和完成变量 numpy 数组。</li><li id="e8dc" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">然后由 Q 网络和目标网络分别预测当前状态和下一状态的 Q 值。用于执行此任务的代码将一组状态作为输入，并返回一组预测。</li><li id="3bfd" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">然后我们用 q 网络计算下一个状态的 q 值最大的动作。<strong class="kh ir">注意，我们没有使用优势法。</strong></li><li id="146a" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">然后，我们计算批索引并执行更新操作。<strong class="kh ir">注意</strong> <strong class="kh ir">本次更新使用双 DQN 更新，即我们根据下一状态下的 q 网络，为具有最大 q 值的动作放置由目标网络预测的 q 值，而不是下一状态的最大 q 值。</strong></li><li id="0c57" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">更新等式乘以 done 变量，因为对于终端状态，q 值总是零。</li><li id="f5fb" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">然后，我们训练模型，更新ε，并增加训练步长。</li></ol><blockquote class="nq nr ns"><p id="8260" class="kf kg nt kh b ki kj jr kk kl km ju kn nu kp kq kr nv kt ku kv nw kx ky kz la ij bi translated">重要提示:这里使用了 numpy 数组，不需要循环。</p><p id="57c7" class="kf kg nt kh b ki kj jr kk kl km ju kn nu kp kq kr nv kt ku kv nw kx ky kz la ij bi translated">重要提示:我们没有在训练方法中使用优势方法。我们只对动作选择而不是 q 值使用优势方法。</p></blockquote><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="no np l"/></div></figure><h2 id="61b2" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">代理培训:</h2><ol class=""><li id="99fa" class="ml mm iq kh b ki lu kl lv ko nl ks nm kw nn la mq mr ms mt bi translated">在下面的代码中，代理与环境交互并进行更新。</li></ol><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="2165" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以在这里找到这篇文章的代码。为了更好地理解，您还可以在同一个存储库中查找 DQN 文件。</p><p id="98a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢你阅读我的文章，希望你喜欢并且能够理解我想要解释的东西。希望你阅读我即将发表的文章。哈里奥姆…🙏</p><h1 id="712c" class="na lc iq bd ld nb nc nd lg ne nf ng lj jw nh jx lm jz ni ka lp kc nj kd ls nk bi translated">参考资料:</h1><div class="ny nz gp gr oa ob"><a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">强化学习，第二版</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">显着扩大和更新的广泛使用的文本强化学习的新版本，最…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">mitpress.mit.edu</p></div></div><div class="ok l"><div class="ol l om on oo ok op mf ob"/></div></div></a></div><div class="ny nz gp gr oa ob"><a href="https://www.youtube.com/channel/UC58v9cLitc8VaCjrcKyAbrw" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">菲尔的机器学习</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">你好。在 Neuralnet.ai，我们涵盖了各种主题的人工智能教程，从强化…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">www.youtube.com</p></div></div><div class="ok l"><div class="oq l om on oo ok op mf ob"/></div></div></a></div><div class="ny nz gp gr oa ob"><a href="https://arxiv.org/abs/1511.06581" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">用于深度强化学习的决斗网络架构</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">近年来，在强化学习中使用深度表征取得了许多成功。尽管如此，许多…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">arxiv.org</p></div></div></div></a></div></div></div>    
</body>
</html>