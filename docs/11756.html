<html>
<head>
<title>Model Selection with Large Neural Networks and Small Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大神经网络和小数据的模型选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-selection-with-large-neural-networks-and-small-data-955b4d929d55?source=collection_archive---------30-----------------------#2020-08-14">https://towardsdatascience.com/model-selection-with-large-neural-networks-and-small-data-955b4d929d55?source=collection_archive---------30-----------------------#2020-08-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="26b8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">高度超参数化的神经网络可以表现出很强的泛化性能，即使是在小数据集上</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ecab12ca1c3f1079f68755455de950ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AUdjSxyieJhLtN7UfMt7Rg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@danielkcheung?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">张家瑜</a>在<a class="ae kv" href="https://unsplash.com/s/photos/small?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="64c1" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir">标题声明</strong>当然是一个大胆的主张，我怀疑你们中的许多人现在正在摇头。</p><blockquote class="lz ma mb"><p id="7596" class="ld le mc lf b lg lh jr li lj lk ju ll md ln lo lp me lr ls lt mf lv lw lx ly ij bi translated">根据统计学习的经典教导，这与众所周知的偏差-方差权衡相矛盾。这个理论定义了一个最佳点，如果你进一步增加模型的复杂性，泛化误差会增加(典型的 U 型测试误差曲线)。</p></blockquote><p id="c641" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">您可能会认为这种影响对于参数数量<em class="mc"> p </em>大于观察数量<em class="mc"> n </em>的<strong class="lf ir">小数据集</strong>更为明显，但<strong class="lf ir">不一定如此。</strong></p><p id="84a5" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在最近由<a class="ae kv" href="https://proceedings.icml.cc/static/paper_files/icml/2020/6899-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> Deepmind </a> (Bornschein，2020)发表的一篇 ICML 2020 论文中，人们可以<strong class="lf ir">在训练数据的更小子集</strong>上进行训练，同时保持可概括的结果，即使对于大型的过度参数化模型也是如此。如果这是真的，我们可以<strong class="lf ir">显著降低模型选择和超参数调整的计算开销</strong>。</p><p id="4f19" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">思考一下这件事的<strong class="lf ir">含义</strong>。这可能会极大地改变我们选择最佳模型或调整超参数的方式(例如在 Kaggle 比赛中)，因为我们可以在网格搜索中包括更多的模型(等等)。</p><blockquote class="lz ma mb"><p id="e23a" class="ld le mc lf b lg lh jr li lj lk ju ll md ln lo lp me lr ls lt mf lv lw lx ly ij bi translated">这好得令人难以置信吗？我们如何证明这一点？</p></blockquote><p id="5d67" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在我们开始之前，这里有一些要点:</p><ul class=""><li id="7b68" class="mg mh iq lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated"><strong class="lf ir">模型选择</strong>可能只使用训练数据的<strong class="lf ir">子集，从而<strong class="lf ir">节省计算资源</strong> ( <em class="mc">相对排序假设</em>)</strong></li><li id="40c4" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">大型<strong class="lf ir">过度参数化的神经网络</strong>能够<strong class="lf ir">惊人地推广</strong> ( <em class="mc">双重下降</em>)</li><li id="7479" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">达到最小值后，<strong class="lf ir">测试交叉熵倾向于随时间逐渐增加，同时测试准确度提高</strong> ( <em class="mc">过度自信</em>)。这可以通过使用<strong class="lf ir">温度刻度</strong>来避免。</li></ul><p id="687f" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们开始吧。</p><h1 id="0635" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">1.偏倚-方差权衡的经典理论述评</h1><p id="cf05" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated">在我们开始之前，我会给你两个选择。如果你已经厌倦了第 100 次听到偏差-方差权衡，请阅读第 1 部分末尾的<strong class="lf ir"> TLDR </strong>，然后继续阅读第 2 部分。否则，我将简要介绍理解基础知识所需的最低要求，然后再继续讨论实际的论文。</p><p id="02c2" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">所有监督学习算法的预测误差可以分为三个(理论上的)部分，这对于理解偏差-方差权衡是必不可少的。这些是；1)偏差 2)方差 3)不可约误差(或噪声项)</p><p id="28e4" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir">不可约误差</strong>(有时称为噪声)是一个与所选模型无关的术语，永远无法减少。这是由于问题的框架不完善而产生的数据的一个方面，这意味着我们永远无法捕捉数据的真实关系——无论我们的模型有多好。</p><p id="a861" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir">偏差</strong>一词通常是人们在提到模型(预测)误差时想到的。简而言之，它衡量“平均”模型预测和实际情况之间的差异。在这种情况下，平均可能看起来很奇怪，因为我们通常只训练一个模型。这么想吧。由于我们数据中的小扰动(随机性),即使使用相同的模型，我们也可以得到稍微不同的预测。通过平均由于这些扰动而得到的预测范围，我们得到了偏差项。<strong class="lf ir">高偏差</strong>是模型拟合差(拟合不足)的标志，因为它在训练集和测试集上都有很大的预测误差。</p><p id="85f7" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最后，<strong class="lf ir">方差</strong>项是指给定数据点的模型预测的可变性。这听起来可能很相似，但关键区别在于“平均值”和“数据点”。<strong class="lf ir">高方差</strong>意味着高泛化误差。例如，虽然模型在训练集上可能相对准确，但它在测试集上的拟合度却相当差。当训练过度参数化的神经网络时，后一种情况(高方差、低偏差)通常是最有可能的，即我们所说的<strong class="lf ir">过度拟合</strong>。</p><p id="e92f" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这些术语的实际含义意味着平衡偏差和方差(因此得名权衡)，通常通过模型复杂性来控制。最终目标是获得低偏差和低方差。这是你以前可能见过的典型的 U 型测试误差曲线。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5d4ff5895aa18d7d36ebf91a2c19f026.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/0*s0Zal9SmQYvXnEyD.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae kv" href="https://www.digitalvidya.com/blog/bias-variance-tradeoff/" rel="noopener ugc nofollow" target="_blank">https://www.digitalvidya.com/blog/bias-variance-tradeoff/</a></p></figure><p id="8b8d" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">好吧，我假设你对偏差-方差权衡有足够的了解，可以理解为什么最初声称<strong class="lf ir">过度参数化的神经网络不一定意味着高方差</strong>确实令人困惑。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><blockquote class="lz ma mb"><p id="1e6a" class="ld le mc lf b lg lh jr li lj lk ju ll md ln lo lp me lr ls lt mf lv lw lx ly ij bi translated">TLDR；高方差、低偏差是过度拟合的标志。当模型在训练集上达到高精度，但在测试集上达到低精度时，就会发生过度拟合。这通常发生在过度参数化的神经网络中。</p></blockquote><h1 id="274f" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">2.现代政体——型号越大越好！</h1><p id="8552" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated">在实践中，我们通常使用带有(例如)提前停止的验证集来优化偏差-方差权衡。有趣的是，这种<strong class="lf ir">方法可能是完全错误的</strong>。在过去的几年中，研究人员发现，如果你继续拟合越来越灵活的模型，你会得到所谓的<em class="mc">双下降</em>，即泛化误差在达到中间峰值后将再次开始下降。这一发现在<a class="ae kv" href="https://arxiv.org/pdf/1912.02292.pdf" rel="noopener ugc nofollow" target="_blank"> Nakkiran et al. (2019) </a>关于现代神经网络架构的已建立和挑战性数据集上得到了经验验证。见下图来自 OpenAI，展示了这个场景；</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/f770f5144bf9f0e4714699ad75f7dd33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IadsVQqODaEfpo9AWfhz5A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae kv" href="https://openai.com/blog/deep-double-descent/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/deep-double-descent/</a></p></figure><ul class=""><li id="6871" class="mg mh iq lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated">测试误差最初下降，直到它达到(局部)最小值，然后随着复杂性的增加再次开始增加。在临界状态下，我们不断增加模型的复杂性是很重要的，因为测试误差将开始再次下降，最终达到(全局)最小值。</li></ul><p id="41c7" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这些发现表明，由于双下降现象，较大的模型通常更好，这挑战了长期以来关于过度参数化神经网络的过度拟合的观点。</p><h1 id="230d" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">3.相对排序假设</h1><p id="6971" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated">已经确定大型超参数化神经网络可以很好地推广，我们想更进一步。进入<strong class="lf ir">相对排名假设</strong>。在我们解释假设之前，我们注意到，如果被证明为真，那么<strong class="lf ir">您</strong>可以为您的下一个实验对您的训练数据集的一个小子集执行<strong class="lf ir">模型选择和超参数调整，这样做可以节省计算资源和宝贵的训练时间。</strong></p><p id="16b6" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们将简要介绍这个假设，然后用几个实验来验证这个说法。作为文献中未包括的额外实验(据我们所知)，我们将调查一个可能使相对排名假设无效的设置，即<strong class="lf ir">不平衡数据集</strong>。</p><h2 id="acfa" class="nt mv iq bd mw nu nv dn na nw nx dp ne lm ny nz ng lq oa ob ni lu oc od nk oe bi translated">a)理论</h2><p id="4d29" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated">Bornschein (2020)的一个关键假设是；</p><blockquote class="lz ma mb"><p id="2600" class="ld le mc lf b lg lh jr li lj lk ju ll md ln lo lp me lr ls lt mf lv lw lx ly ij bi translated"><em class="iq">“当在训练集的任意小的子集上训练时，过度参数化的模型架构似乎保持了它们在泛化性能方面的相对排名”</em>。</p></blockquote><p id="0a9c" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">他们称这一观察为相对排名假说。</p><p id="2183" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">用<strong class="lf ir">外行人的话说</strong>；假设我们有 10 个型号可供选择，编号从 1 到 10。我们在训练数据的 10%子集上训练我们的模型，并且发现模型 6 是最好的，其次是模型 4，然后是模型 3，等等..</p><p id="aa4d" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir">排序假设假定，当我们逐渐将子集百分比从 10%增加到 100%时，我们应该获得最优模型的完全相同的排序。</strong></p><p id="afd7" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果这个假设是真的，我们可以在原始数据的一个小的子集上执行模型选择，从而获得更快收敛的额外好处。如果这还不够有争议的话，作者甚至更进一步，因为他们发现了一些实验，在这些实验中，对小数据集的训练导致了更鲁棒的模型选择(更少的方差)，这当然似乎是违反直觉的，因为我们预计较小的数据集会有相对更多的噪声。</p><h2 id="f423" class="nt mv iq bd mw nu nv dn na nw nx dp ne lm ny nz ng lq oa ob ni lu oc od nk oe bi translated">b)温度校准</h2><p id="2ced" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated">训练神经网络分类器时的一个奇怪现象是，<strong class="lf ir">交叉熵误差有增大的趋势，而分类误差</strong>减小。这种<strong class="lf ir">看似违反直觉的</strong>，其实只是因为模型在预测中变得<strong class="lf ir">过于自信</strong>(<a class="ae kv" href="https://arxiv.org/pdf/1706.04599.pdf" rel="noopener ugc nofollow" target="_blank">郭等(2017) </a>)。我们可以使用一种叫做<strong class="lf ir">温度缩放</strong>的东西，它可以在一个小的保留数据集上校准交叉熵估计。与经典的交叉熵相比，这产生了更一般化和性能更好的结果，特别是与过度参数化的神经网络相关。作为一个粗略的类比，你可以认为这提供了更少的关于过度拟合情况的“假阴性”。</p><p id="d376" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">虽然 Bornschein (2020 年)没有提供他们论文中使用的 softmax 温度校准程序的明确细节，但我们在实验中使用了以下程序:</p><ul class=""><li id="fe7c" class="mg mh iq lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated">我们定义了一个保持校准数据集 C，相当于训练数据的 10%。</li><li id="44dc" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">我们将温度标量初始化为 1.5(如郭等人(2017 年)所述)</li></ul><h2 id="766f" class="nt mv iq bd mw nu nv dn na nw nx dp ne lm ny nz ng lq oa ob ni lu oc od nk oe bi translated">对于每个时期；</h2><ul class=""><li id="d765" class="mg mh iq lf b lg nm lj nn lm of lq og lu oh ly ml mm mn mo bi translated">1)在我们的校准集 C 上计算交叉熵损失</li><li id="cc48" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">2)在校准集上使用梯度下降优化温度标量(<a class="ae kv" href="https://github.com/gpleiss/temperature_scaling" rel="noopener ugc nofollow" target="_blank">参见郭等人的 Github 报告(2017) </a>)</li><li id="1570" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">3)在梯度下降期间，使用更新的温度标量来校准常规交叉熵</li><li id="1885" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">在训练 50 个时期后，我们计算校准的测试误差，该误差应该不再表现出过度自信的迹象。</li></ul><p id="124e" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">现在让我们转向实验设置。</p><h1 id="fcdb" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">4.实验</h1><p id="eae0" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated">在这篇文章中，我们将进行两个实验。一个用于验证 MNIST 数据集上的相对排名假设，一个用于评估如果我们综合地使 MNIST 不平衡，我们的结论会如何变化。Bornschein (2020)论文中的没有包括后一个实验<strong class="lf ir">，并且可能会使不平衡数据集的相对排序假设无效。</strong></p><h2 id="207a" class="nt mv iq bd mw nu nv dn na nw nx dp ne lm ny nz ng lq oa ob ni lu oc od nk oe bi translated">MNIST</h2><p id="1072" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated">我们从复制 Bornschein (2020)对 MNIST 的研究开始，然后继续进行不平衡数据集实验。这并不意味着否定论文中的任何说法，而只是为了确保我们尽可能地复制了他们的实验设置(做了一些修改)。</p><ul class=""><li id="daac" class="mg mh iq lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated">分别对训练集和校准集进行 90%/10%的拆分</li><li id="97af" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">随机抽样(根据论文，平衡子集抽样没有提供任何额外的好处)</li><li id="34d5" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">50 个时代</li><li id="21b4" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">具有固定学习率的 Adam[10e-4]</li><li id="946a" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">批量= 256</li><li id="1841" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">具有 3 个隐藏层和 2048 个单元的全连接 MLPs</li><li id="2c84" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">没有辍学(使我们的结果太不稳定，无法纳入)</li><li id="bab1" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">具有 4 层、5×5 空间核、跨距 1 和 256 个信道的简单卷积网络</li><li id="1a20" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">逻辑回归</li><li id="1c2b" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">10 种不同的种子来显现不确定性带(原始论文中有 30 种)</li></ul><p id="45bf" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">作者还提到了用 tanh、batch-norm、layer-norm 等替换 ReLU 的实验。，但不清楚这些测试是否包括在他们的最终结果中。因此，我们只考虑使用上述设置的实验。</p><h2 id="0a83" class="nt mv iq bd mw nu nv dn na nw nx dp ne lm ny nz ng lq oa ob ni lu oc od nk oe bi translated">实验一:梯度下降过程中的温度标度如何影响泛化？</h2><p id="24ac" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated">作为初始实验，我们希望验证为什么需要温度缩放。为此，我们分别使用 ReLU 和 3 个 2048 个单位的隐藏层来训练 MLP。我们不包括辍学者，我们培训 50 个纪元。</p><p id="4d7b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir">我们的假设是:</strong>随着时间的推移，测试交叉熵应该逐渐增加，而测试精度会逐渐降低(首先是温度缩放的动机，即模型过度自信)。</p><p id="044d" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir">下面是这个初步实验的结果:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/0b0bf40d7836f262e6fa68b8e05253b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0whH0tAcEsKdBsIW.png"/></div></div></figure><p id="57ce" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">显然，测试熵确实在开始时下降，然后随着时间逐渐增加，同时测试精度不断提高。这是支持假设 1 的证据。Guo 等人(2017 年)的图 3 展示了对 CIFAR-100 的完全相同的影响。<br/> <em class="mc">注:</em>我们对结果做了一点平滑处理(5 窗口滚动平均)使效果更明显。</p><p id="8774" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir">实验 1 的结论:</strong></p><ul class=""><li id="c61d" class="mg mh iq lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated">如果我们持续训练大型神经网络足够长的时间，我们会开始看到过于自信的概率预测，使它们在样本外变得不那么有用。</li></ul><p id="fb3b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了补救这种影响，我们可以<strong class="lf ir">结合温度缩放</strong></p><ul class=""><li id="53ce" class="mg mh iq lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated"><strong class="lf ir"> a) </strong>确保样本外的概率预测更加稳定和可靠，以及</li><li id="ca22" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated"><strong class="lf ir"> b) </strong>在梯度下降过程中，通过缩放训练交叉熵来提高泛化能力。</li></ul><h2 id="7798" class="nt mv iq bd mw nu nv dn na nw nx dp ne lm ny nz ng lq oa ob ni lu oc od nk oe bi translated">平衡数据集</h2><p id="af5a" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated">已经表明需要温度缩放，我们现在转向主要实验，即测试交叉熵如何作为我们训练数据集大小的函数而变化。我们的结果如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/d65e7ef632d4e51efdbeb13283edbea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yU5aF4KGyeyYHsNrymFLHg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试交叉熵作为 MNIST 训练集大小的函数</p></figure><p id="1594" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">注意，我们没有获得与 Bornschein (2020)完全相同的“平滑”结果。这很可能是因为我们没有完全复制他们的实验，因为他们包括了更多不同的种子。尽管如此，我们可以得出以下结论:</p><ul class=""><li id="559b" class="mg mh iq lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated">有趣的是，相对较大的 ResNet-18 模型在训练过程中的任何时候都不会比逻辑回归拟合得更好！</li><li id="bece" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">相对排序假设得到了证实</li><li id="ff53" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">超过 25000 次观察(大约是 MNIST 训练数据集的一半)，明显更大的 ResNet 模型仅比相对更快的 MLP 模型略好。</li></ul><h1 id="6001" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">不平衡数据集</h1><p id="31a7" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated">我们现在将针对不平衡数据集的情况进行一个实验，该实验并不包括在实际的论文中，因为它可能是一个测试假设无效的设置。</p><p id="7ba6" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们采样一个类似于<a class="ae kv" href="https://arxiv.org/pdf/1706.04599.pdf" rel="noopener ugc nofollow" target="_blank">郭等(2019) </a>的人为失衡版本的。程序如下。对于数据集中的每个类，我们对原始训练和测试数据集进行 0%到 100%的二次抽样。我们使用下面的<a class="ae kv" href="https://github.com/ufoym/imbalanced-dataset-sampler/blob/master/examples/mnist.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>进行采样。</p><p id="3bec" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然后，我们选择类似于先前实验的校准数据集，即，在训练和校准之间随机 90/10%分割。</p><p id="5372" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们包括了<strong class="lf ir">原始 MNIST 训练数据集</strong>的类分布的可视化</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/e93c043d0c6d7043428eb76a90b661ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*13hgb112Wrm9Vq3M.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MNIST 每个等级的频率计数</p></figure><p id="8fa5" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">还有<strong class="lf ir">不平衡版</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/0ba8b3c70e08d34a1da345e9f041b084.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*M8kJUaSMD6W0Iurb.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不平衡 MNIST 中每个类的频率计数</p></figure><p id="6f73" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">鉴于这种频率分布的巨大差异，你可以清楚地看到这个版本比原来的 MNIST 更加不平衡。</p><p id="6fe2" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">虽然存在大量不同的方法来克服不平衡数据集的问题(参见下面的<a class="ae kv" href="https://arxiv.org/pdf/1710.05381.pdf" rel="noopener ugc nofollow" target="_blank">综述论文</a>)，但我们希望研究和隔离不平衡数据集对相对排序假设的影响，即相对排序假设在不平衡数据设置中是否仍然成立？</p><p id="6aa7" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们使用这个综合不平衡的 MNIST 数据集再次运行我们的所有模型，并获得以下结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/2818ae2b12d2b0a55b70127ea8129aee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LCvE6Ulh9DmyRfD-dVc7_g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试作为不平衡 MNIST 的训练集大小的函数的交叉熵</p></figure><p id="1121" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">那么结论改变了吗？</p><p id="44fd" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir">不是真的</strong>！</p><p id="df5b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这是一个非常乐观的结果，因为我们现在更有信心，在不平衡数据集的情况下，相对排序假设大多是正确的。我们认为这也可能是引用 Bornschein (2020 年)论文中关于抽样策略的原因；</p><blockquote class="lz ma mb"><p id="2a4f" class="ld le mc lf b lg lh jr li lj lk ju ll md ln lo lp me lr ls lt mf lv lw lx ly ij bi translated"><em class="iq">“我们试验了平衡子集抽样，即确保所有子集在每类中都包含相同数量的样本。但是我们没有观察到这样做有任何可靠的改进，因此我们又回到了简单的 i.i.d .采样策略。”</em></p></blockquote><p id="2d52" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">平衡版本和不平衡版本之间的主要区别是更多的"<strong class="lf ir">跳动的</strong>"结果，这是有意义的，因为测试集中可能有在所选模型的训练期间看不到的类。</p><h1 id="984a" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">5.摘要</h1><p id="acfb" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated"><strong class="lf ir">总结我们的发现:</strong></p><ul class=""><li id="a2f1" class="mg mh iq lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated">由于<em class="mc">相对排序假设</em>，对于平衡和不平衡数据集，我们可以仅使用我们的训练数据的<strong class="lf ir">子集来执行<strong class="lf ir">模型选择</strong>，从而<strong class="lf ir">节省计算资源</strong></strong></li><li id="2ec4" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">大型<strong class="lf ir">过度参数化的神经网络</strong>能够<strong class="lf ir">惊人地推广</strong>，甚至在小型数据集上(<em class="mc">双重下降</em>)</li><li id="034c" class="mg mh iq lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">我们可以通过应用<strong class="lf ir">温度标度</strong>来<strong class="lf ir">避免过度自信</strong></li></ul></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="a1cc" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我希望你能够在下一次机器学习实验中应用这些发现，并且记住，越大(几乎)总是越好。</p><p id="e9dc" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir">感谢您的阅读！</strong></p><h1 id="af74" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">6.参考</h1><p id="4d1a" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm no lo lp lq np ls lt lu nq lw lx ly ij bi translated">[1] J. Bornschein，F. Visin 和 S. Osindero,《小数据，大决策:小数据体制中的模型选择》( 2020 年)，载于机器学习国际会议(ICML)。</p><p id="c0e2" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">[2] P. Nakkiran，G. Kaplun，Y. Yang，B. T. Barak 和 I. Sutskever，深度双重下降:更大的模型和更多数据的伤害(2019)，arXiv 预印本 arXiv:1912.02292。</p><p id="6ab3" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">[3]郭、普莱斯、孙和温伯格(2017 年)。关于现代神经网络的校准(2017)，arXiv 预印本 arXiv:1706.04599。</p><p id="1076" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">[4] T. Guo，X. Zhu，Y. Wang，和 F. Chen，用于深度不平衡学习的判别样本生成(2019)，载于人工智能组织国际联合会议(IJCAI)(第 2406-2412 页)。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="17e9" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><em class="mc">原载于 2020 年 8 月 14 日</em><a class="ae kv" href="https://holmdk.github.io/2020/08/14/deep_learning_small_data.html" rel="noopener ugc nofollow" target="_blank"><em class="mc">https://holm dk . github . io</em></a><em class="mc">。</em></p></div></div>    
</body>
</html>