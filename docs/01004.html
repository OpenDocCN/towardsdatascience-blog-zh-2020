<html>
<head>
<title>5 Ways to add a new column in a PySpark Dataframe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 PySpark 数据框架中添加新列的 5 种方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-ways-to-add-a-new-column-in-a-pyspark-dataframe-4e75c2fd8c08?source=collection_archive---------0-----------------------#2020-01-29">https://towardsdatascience.com/5-ways-to-add-a-new-column-in-a-pyspark-dataframe-4e75c2fd8c08?source=collection_archive---------0-----------------------#2020-01-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/07c71b326bd01ad84a3eac1fc51fccc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQipbMexsS3kj7rW0TQung.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1953253" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae jg" href="https://pixabay.com/users/nickgesell-3554748/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1953253" rel="noopener ugc nofollow" target="_blank"> nickgesell </a>提供</p></figure><div class=""/><div class=""><h2 id="040c" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">并且，它们都是有用的</h2></div><p id="196c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">每天都在生成太多的数据。</em> </strong></p><p id="1203" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">虽然有时我们可以使用类似于</em></strong><a class="ae jg" rel="noopener" target="_blank" href="/minimal-pandas-subset-for-data-scientist-on-gpu-d9a6c7759c7f?source=---------5------------------"><strong class="la jk"><em class="lu">Rapids</em></strong></a><strong class="la jk"><em class="lu">或</em> </strong> <a class="ae jg" rel="noopener" target="_blank" href="/add-this-single-word-to-make-your-pandas-apply-faster-90ee2fffe9e8?source=---------11------------------"> <strong class="la jk"> <em class="lu">并行化</em> </strong> </a> <strong class="la jk"> <em class="lu">，</em> </strong> Spark 这样的工具来管理我们的大数据，但如果您正在处理数 TB 的数据，这是一款非常棒的工具。</p><p id="7742" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我关于 Spark 的上一篇文章中，我解释了如何使用 PySpark RDDs 和 Dataframes。</p><p id="dccd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然这篇文章解释了很多关于如何使用 rdd 和基本数据帧操作的内容，但是我错过了很多关于使用 PySpark 数据帧的内容。</p><p id="72a9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">只有当我需要更多的功能时，我才会仔细阅读，想出多种解决方案来做一件事情。</p><p id="d29d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">如何在 spark 中创建新栏目？</em> </strong></p><p id="4318" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，这可能听起来微不足道，但相信我，它不是。由于您可能想对数据做这么多事情，我很确定您最终会在工作流中使用这些列创建过程中的大部分。有时利用 Pandas 的功能，有时使用基于 RDDs 的分区，有时利用成熟的 python 生态系统。</p><p id="6376" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">这篇文章的主题是“在 Pyspark 数据框架中创建新列的多种方法”</strong></p><p id="8fc1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您安装了 PySpark，可以跳过下面的入门部分。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="b16d" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">Spark 入门</h1><p id="70e5" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我知道你们很多人不会在系统中安装 spark 来尝试和学习。但是安装 Spark 本身就很头疼。</p><p id="211c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">既然我们想了解它是如何工作的，我建议你在社区版中使用 Spark on data bricks<a class="ae jg" href="https://databricks.com/try-databricks?utm_source=databricks&amp;utm_medium=homev2tiletest" rel="noopener ugc nofollow" target="_blank"><strong class="la jk">here</strong></a>。别担心，它是免费的，尽管资源更少，但对于我们现在的学习目的来说是有用的。</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/84e7b859cf2bbde5a8058ba48dc262cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Gh0z5yI_t4qtJfPy.png"/></div></div></figure><p id="66a4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦您注册并登录，将出现以下屏幕。</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/1a50fc631965fed1d52d02a58505fc67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Wdtq3kBiF7x7wnEU.png"/></div></div></figure><p id="9bca" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在这里开始一个新的笔记本。</p><p id="56bb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">选择 Python 笔记本，并为其命名。</p><p id="f004" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦您启动一个新的笔记本并尝试执行任何命令，笔记本会询问您是否要启动一个新的集群。动手吧。</p><p id="feca" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一步将检查 sparkcontext 是否存在。要检查 sparkcontext 是否存在，您必须运行以下命令:</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="80a0" class="nj md jj nf b gy nk nl l nm nn">sc</span></pre><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/bfe9fd15f7e457bd6c3283f9b8729099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FmAvG2iJNYitgzyvECUBiQ.png"/></div></div></figure><p id="79be" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这意味着我们可以在笔记本上运行 Spark。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="e611" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">数据</h1><p id="3351" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">在这里，我将处理 movie lens<a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/tree/master/spark_post" rel="noopener ugc nofollow" target="_blank"><strong class="la jk">ml-100k . zip</strong></a>数据集。1000 个用户对 1700 部电影的 100，000 次评分。在这个压缩文件夹中，我们将专门处理的文件是分级文件。该文件名保存为“u.data”</p><p id="54ff" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您想要上传此数据或任何数据，您可以单击左侧的 data 选项卡，然后使用提供的 GUI 添加数据。</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/5c41c0fb71686fdb724bb9733b71c3e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ItjLSOOOiY3KcM_WI20Pfw.png"/></div></div></figure><p id="0532" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们可以使用以下命令加载数据:</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="0df5" class="nj md jj nf b gy nk nl l nm nn">ratings = spark.read.load("/FileStore/tables/u.data",format="csv", sep="\t", inferSchema="true", header="false")</span><span id="782e" class="nj md jj nf b gy nq nl l nm nn">ratings = ratings.toDF(*['user_id', 'movie_id', 'rating', 'unix_timestamp'])</span></pre><p id="b399" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是它的样子:</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="5ebc" class="nj md jj nf b gy nk nl l nm nn">ratings.show()</span></pre><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/806389919bc1c55046349e9751f87711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hQ0rAwgVKWArFXCM2f4sEg.png"/></div></div></figure><p id="ce8d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好了，现在我们终于可以开始我们感兴趣的部分了。如何在 PySpark Dataframe 中创建新列？</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="83bb" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">1.使用 Spark 本地函数</h1><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/c91c0fdd76d2636d91f4952286f90b02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vCw7m5TXQiM4gYg2"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">安德鲁·詹姆斯在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="7f1c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 PySpark 数据帧中创建新列的最简单的方法是使用内置函数。这是创建新列的最有效的编程方式，所以每当我想做一些列操作时，这是我第一个去的地方。</p><p id="94e4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以使用<code class="fe nt nu nv nf b">.withcolumn</code>和 PySpark SQL 函数来创建一个新列。本质上，您可以找到已经使用 Spark 函数实现的字符串函数、日期函数和数学函数。我们可以将 spark 函数导入为:</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="44b7" class="nj md jj nf b gy nk nl l nm nn">import pyspark.sql.functions as F</span></pre><p id="323c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的第一个函数是<code class="fe nt nu nv nf b">F.col</code>函数，它让我们可以访问列。因此，如果我们想将一列乘以 2，我们可以使用<code class="fe nt nu nv nf b">F.col</code>作为:</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="c79a" class="nj md jj nf b gy nk nl l nm nn">ratings_with_scale10 = ratings.withColumn("ScaledRating", 2*F.col("rating"))</span><span id="99f6" class="nj md jj nf b gy nq nl l nm nn">ratings_with_scale10.show()</span></pre><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/557a73be3ef63ce14f8f47f8880e5143.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AiMfee3shzeWQEdREs-Oug.png"/></div></div></figure><p id="00b2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们也可以使用数学函数，如<code class="fe nt nu nv nf b">F.exp</code>函数:</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="24c9" class="nj md jj nf b gy nk nl l nm nn">ratings_with_exp = ratings.withColumn("expRating", 2*F.exp("rating"))</span><span id="01b2" class="nj md jj nf b gy nq nl l nm nn">ratings_with_exp.show()</span></pre><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/165fcd9f701aaaab24650c8cb325685c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fo-O-wV_BwYv_QsAr2FUog.png"/></div></div></figure><p id="1592" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个模块还提供了很多其他的功能，对于大多数简单的用例来说已经足够了。你可以点击查看功能列表<a class="ae jg" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ba38" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">2.Spark UDFs</h1><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/ac8f9647375ec9e1e403ca293fc150ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vQ_I_dKiVWsRjqSB"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae jg" href="https://unsplash.com/@divide_by_zero?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">除以零</a></p></figure><p id="ebd9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有时我们想对一列或多列做复杂的事情。这可以被认为是将 PySpark 数据帧映射到一列或多列的操作。虽然 Spark SQL 函数确实解决了许多列创建的用例，但每当我想使用更成熟的 Python 功能时，我都会使用 Spark UDF。</p><p id="ce80" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要使用 Spark UDF，我们需要使用<code class="fe nt nu nv nf b">F.udf</code>函数将常规 python 函数转换成 Spark UDF。我们还需要指定函数的返回类型。在这个例子中，返回类型是<code class="fe nt nu nv nf b">StringType()</code></p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="0dad" class="nj md jj nf b gy nk nl l nm nn">import pyspark.sql.functions as F<br/>from pyspark.sql.types import *</span><span id="0671" class="nj md jj nf b gy nq nl l nm nn">def somefunc(value):<br/>  if   value &lt; 3: <br/>      return 'low'<br/>  else:<br/>      return 'high'</span><span id="33ca" class="nj md jj nf b gy nq nl l nm nn">#convert to a UDF Function by passing in the function and return type of function</span><span id="ef97" class="nj md jj nf b gy nq nl l nm nn">udfsomefunc = F.udf(somefunc, StringType())</span><span id="40cf" class="nj md jj nf b gy nq nl l nm nn">ratings_with_high_low = ratings.withColumn("high_low", udfsomefunc("rating"))</span><span id="1b22" class="nj md jj nf b gy nq nl l nm nn">ratings_with_high_low.show()</span></pre><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/55b4f7a949cbb46da72f72d74bf72e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tb29wIQDDsvcYx-5VBwLLQ.png"/></div></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="c748" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">3.使用 rdd</h1><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/c7433c43853e6ab0d7412d863ea78438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SvCUDMk_SCL33ucc"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">瑞安·昆塔尔在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="c9ce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有时，spark UDFs 和 SQL 函数对于特定的用例来说是不够的。您可能希望利用 spark RDDs 带来的更好的分区。或者您可能想在 Spark RDDs 中使用组函数。您可以使用这个，主要是当您需要访问 python 函数内 spark 数据框中的所有列时。</p><p id="2403" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不管是哪种情况，我发现这种使用 RDD 创建新列的方式对于有使用 rdd 经验的人来说非常有用，rdd 是 Spark 生态系统中的基本构建块。</p><p id="9a98" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面的过程利用了在<code class="fe nt nu nv nf b">Row</code>和<code class="fe nt nu nv nf b">pythondict</code>对象之间转换的功能。我们将一个行对象转换成一个字典。像我们习惯的那样使用字典，并将字典转换回 row。</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="ba0b" class="nj md jj nf b gy nk nl l nm nn">import math<br/>from pyspark.sql import Row</span><span id="3587" class="nj md jj nf b gy nq nl l nm nn">def rowwise_function(row):<br/>  # convert row to dict:<br/>  row_dict = row.asDict()<br/>  # Add a new key in the dictionary with the new column name and value. <br/>  row_dict['Newcol'] = math.exp(row_dict['rating'])<br/>  # convert dict to row:<br/>  newrow = Row(**row_dict)<br/>  # return new row<br/>  return newrow</span><span id="d6a8" class="nj md jj nf b gy nq nl l nm nn"># convert ratings dataframe to RDD<br/>ratings_rdd = ratings.rdd<br/># apply our function to RDD<br/>ratings_rdd_new = ratings_rdd.map(lambda row: rowwise_function(row))<br/># Convert RDD Back to DataFrame<br/>ratings_new_df = sqlContext.createDataFrame(ratings_rdd_new)</span><span id="2f0b" class="nj md jj nf b gy nq nl l nm nn">ratings_new_df.show()</span></pre><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/32690e1fdf503d59748d5bc8869e6572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_gKxIQEIYGpXsU_BDUOWw.png"/></div></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ddb2" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">4.熊猫 UDF</h1><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/07006ee71f5f649ad63f52151af7a46c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*59LUo7y1XcffPUak"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">帕斯卡尔·贝纳登在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4b33" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Spark 版本 2.3.1 中引入了这一功能。这就允许你在 Spark 上使用 pandas 的功能。当我必须在 Spark 数据帧上运行 groupby 操作时，或者当我需要创建滚动特征并希望使用 Pandas 滚动函数/窗口函数时，我通常会使用它。</p><p id="650f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们使用它的方式是通过使用<code class="fe nt nu nv nf b">F.pandas_udf</code>装饰器。这里我们假设函数的输入是一个熊猫数据帧。我们需要从这个函数中依次返回一个熊猫数据帧。</p><p id="6b43" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里唯一的复杂性是我们必须为输出数据帧提供一个模式。我们可以使用下面的格式。</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="c133" class="nj md jj nf b gy nk nl l nm nn"># Declare the schema for the output of our function<br/>outSchema = StructType([StructField('user_id',IntegerType(),True),StructField('movie_id',IntegerType(),True),StructField('rating',IntegerType(),True),StructField('unix_timestamp',IntegerType(),True),StructField('normalized_rating',DoubleType(),True)])</span><span id="ed77" class="nj md jj nf b gy nq nl l nm nn"># decorate our function with pandas_udf decorator<br/><a class="ae jg" href="http://twitter.com/F" rel="noopener ugc nofollow" target="_blank">@F</a>.pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP)<br/>def subtract_mean(pdf):<br/>    # pdf is a pandas.DataFrame<br/>    v = pdf.rating<br/>    v = v - v.mean()<br/>    pdf['normalized_rating'] =v<br/>    return pdf</span><span id="77c3" class="nj md jj nf b gy nq nl l nm nn">rating_groupwise_normalization = ratings.groupby("movie_id").apply(subtract_mean)</span><span id="2f23" class="nj md jj nf b gy nq nl l nm nn">rating_groupwise_normalization.show()</span></pre><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/fdae2a123803025bebbc64e0279f914a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lW5p0EC65ULpU1AhcPvApA.png"/></div></div></figure><p id="57ab" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们也可以利用这个来<strong class="la jk"> <em class="lu">在每个 spark 节点上训练多个单独的模型。</em> </strong>为此，我们复制我们的数据，并给每个副本一个键和一些训练参数，如 max_depth 等。然后，我们的函数获取 pandas 数据帧，运行所需的模型，并返回结果。该结构如下所示。</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="aa6f" class="nj md jj nf b gy nk nl l nm nn"># 0. Declare the schema for the output of our function<br/>outSchema = StructType([StructField('replication_id',IntegerType(),True),StructField('RMSE',DoubleType(),True)])</span><span id="118b" class="nj md jj nf b gy nq nl l nm nn"># decorate our function with pandas_udf decorator<br/><a class="ae jg" href="http://twitter.com/F" rel="noopener ugc nofollow" target="_blank">@F</a>.pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP)<br/>def run_model(pdf):<br/>    # 1. Get hyperparam values<br/>    num_trees = pdf.num_trees.values[0]<br/>    depth = pdf.depth.values[0]<br/>    replication_id = pdf.replication_id.values[0]<br/>    # 2. Train test split<br/>    Xtrain,Xcv,ytrain,ycv = train_test_split.....<br/>    # 3. Create model using the pandas dataframe<br/>    clf = RandomForestRegressor(max_depth = depth, num_trees=num_trees,....)<br/>    clf.fit(Xtrain,ytrain)<br/>    # 4. Evaluate the model<br/>    rmse = RMSE(clf.predict(Xcv,ycv)<br/>    # 5. return results as pandas DF<br/>    res =pd.DataFrame({'replication_id':replication_id,'RMSE':rmse})<br/>    return res<br/>                <br/>results = replicated_data.groupby("replication_id").apply(run_model)</span></pre><p id="61b7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以上只是一个想法，而不是工作代码。尽管稍加修改就能工作。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="4e1d" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">5.使用 SQL</h1><p id="dd7a" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">对于喜欢 SQL 的人来说，甚至有一种方法可以使用 SQL 创建列。为此，我们需要注册一个临时 SQL 表，然后使用带有附加列的简单选择查询。人们也可以用它来连接。</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="48b8" class="nj md jj nf b gy nk nl l nm nn">ratings.registerTempTable('ratings_table')<br/>newDF = sqlContext.sql('select *, 2*rating as newCol from ratings_table')<br/>newDF.show()</span></pre><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oe"><img src="../Images/72c919ac29304f7a98232b51624addd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t9IQhgqHjwPhFsGZKjzB_A.png"/></div></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="565f" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">结论</h1><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/ccc2a2f85d268a1b648383344012a05e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oM9kztYgfNxdYmRc"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由<a class="ae jg" href="https://unsplash.com/@kellysikkema?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯利·西克玛</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b26d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是本专栏的结尾(双关语)</p><p id="74e1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">希望我已经很好地介绍了专栏创建过程，可以帮助您解决 Spark 问题。如果您需要了解更多 spark 基础知识，请查看:</p><div class="is it gp gr iu og"><a rel="noopener follow" target="_blank" href="/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jk gy z fp ol fr fs om fu fw ji bi translated">使用 Spark 处理大数据的指南</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">不仅仅是介绍</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou ja og"/></div></div></a></div><p id="9479" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">你可以在 databricks 上的</em> </strong> <a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/blob/master/spark_columns/Columns.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk"> <em class="lu"> GitHub 资源库</em> </strong> </a> <strong class="la jk"> <em class="lu">或</em> </strong> <a class="ae jg" href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7664398068420572/312750581110937/3797400441762013/latest.html" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk"> <em class="lu">发布的笔记本</em> </strong> </a> <strong class="la jk"> <em class="lu">中找到这篇文章的所有代码。</em> </strong></p><p id="04a1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有，如果你想了解更多关于 Spark 和 Spark DataFrames 的知识，我想调出 Yandex 提供的<a class="ae jg" href="https://coursera.pxf.io/coursera" rel="noopener ugc nofollow" target="_blank"><strong class="la jk"/></a><strong class="la jk"/>大数据精要<a class="ae jg" href="https://coursera.pxf.io/coursera" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">大数据专业化</strong> </a>的一个精品课程。</p><p id="5f2a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" rel="noopener"> <strong class="la jk">媒体</strong> </a>关注我或者订阅我的<a class="ae jg" href="https://mlwhiz.ck.page/a9b8bda70c" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter<a class="ae jg" href="https://twitter.com/MLWhiz?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"><strong class="la jk">@ mlwhiz</strong></a>联系</p><p id="10d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，一个小小的免责声明——这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p></div></div>    
</body>
</html>