<html>
<head>
<title>One Potential Cause of Overfitting That I Never Noticed Before</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我以前从未注意到的过度拟合的一个潜在原因</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/one-potential-cause-of-overfitting-that-i-never-noticed-before-a57904c8c89d?source=collection_archive---------31-----------------------#2020-03-19">https://towardsdatascience.com/one-potential-cause-of-overfitting-that-i-never-noticed-before-a57904c8c89d?source=collection_archive---------31-----------------------#2020-03-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0b9f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="737f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">当训练数据中的性能比测试数据中的性能好得多时，就会发生过度拟合。机器学习包中的默认超参数可能会让你陷入过度拟合的问题。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ba5796017a189deb2bdef5028739f104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eHRyRuD4n53HL5fw"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@joaosilas?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔·塞拉斯</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="li lj lk"><p id="aa08" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">“为什么我调好的模型还是过拟合？”</p><p id="740e" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">"你使用交叉验证了吗？"</p><p id="fd27" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">“当然可以。”</p><p id="1f0e" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">"你用的是什么模型，你调的超参数是什么？"</p><p id="f7c0" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">"随机森林回归器和我调整了每棵树的树数和最大特征数."</p><p id="d00e" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">“我需要检查你的代码。”</p></blockquote><p id="5944" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这是我和妻子在做一个迷你项目时的对话。</p><p id="0225" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">老实说，根据她的描述，我想不出程序中有什么错误。然而，在我一行一行地检查她的代码后，我发现了导致过度拟合的问题，这是我以前从未想到过的。</p><p id="69ba" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我们一起来解剖一下她的代码。</p><p id="6ba5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这是在训练数据集中调整模型的代码块，也是问题发生的<em class="ln"/>。</p><pre class="ks kt ku kv gt ml mm mn mo aw mp bi"><span id="19a3" class="mq mr it mm b gy ms mt l mu mv">def train_pipeline_rf(X,y):<br/>    # X are factors<br/>    # y is output<br/>    # impute missing X by median<br/>    X_prepared = pre_pipeline.fit_transform(X)<br/>    # set cross-validation<br/>    tscv = TimeSeriesSplit(n_splits=10)<br/>    data_split = tscv.split(X_prepared)<br/>    # hyper-parameter space<br/>    param_grid_RF = {<br/>        'n_estimators' : [10,20,50,100,200,500,1000],<br/>        'max_features' : [0.6,0.8,"auto","sqrt"]<br/>    }<br/>    # build random forest model<br/>    rf_model = RandomForestRegressor(random_state=42,n_jobs=-1)<br/>    # gridsearch for the best hyper-parameter<br/>    gs_rf = GridSearchCV(rf_model, param_grid=param_grid_RF, cv=data_split, scoring='neg_mean_squared_error', n_jobs=-1)<br/>    # fit dataset<br/>    gs_rf.fit(X_prepared, y)<br/>    return gs_rf</span></pre><h2 id="7136" class="mq mr it bd mw mx my dn mz na nb dp nc mi nd ne nf mj ng nh ni mk nj nk nl iz bi translated">预处理</h2><p id="ce25" class="pw-post-body-paragraph ll lm it lo b lp nm kd lr ls nn kg lu mi no lx ly mj np mb mc mk nq mf mg mh im bi translated">代码中的<strong class="lo jd"> <em class="ln"> pre_pipeline </em> </strong>是用于缺失值插补和特征缩放的<em class="ln">流水线</em>，两者都是数据<em class="ln">预处理</em>中必不可少的步骤。它看起来是这样的:</p><pre class="ks kt ku kv gt ml mm mn mo aw mp bi"><span id="6a56" class="mq mr it mm b gy ms mt l mu mv">pre_pipeline = Pipeline([<br/>        ('imputer', SimpleImputer(strategy="median")),<br/>        ('std_scaler', StandardScaler()),<br/>    ])</span></pre><p id="fd9e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">她在这里使用的<a class="ae lh" href="https://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values" rel="noopener ugc nofollow" target="_blank"> <em class="ln">估算器</em> </a>是用列中值替换<strong class="lo jd"> <em class="ln"> NA </em> </strong>值，而<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" rel="noopener ugc nofollow" target="_blank"> <em class="ln">定标器</em> </a>是标准定标器，它将列归一化为:</p><blockquote class="nr"><p id="cce6" class="ns nt it bd nu nv nw nx ny nz oa mh dk translated"><em class="ob"> z = (x — u) / s </em></p></blockquote><p id="643c" class="pw-post-body-paragraph ll lm it lo b lp oc kd lr ls od kg lu mi oe lx ly mj of mb mc mk og mf mg mh im bi translated">其中<strong class="lo jd"> <em class="ln"> u </em> </strong>为均值，<strong class="lo jd"> <em class="ln"> s </em> </strong>为列[1]的标准差。也可以使用<a class="ae lh" href="https://scikit-learn.org/stable/modules/preprocessing.html" rel="noopener ugc nofollow" target="_blank"><strong class="lo jd"><em class="ln">sk learn</em></strong></a>包中的其他一些<strong class="lo jd"> <em class="ln">估算器</em> </strong>和<strong class="lo jd"> <em class="ln">缩放器</em> </strong>。但这部分与过拟合问题无关。</p><h2 id="5783" class="mq mr it bd mw mx my dn mz na nb dp nc mi nd ne nf mj ng nh ni mk nj nk nl iz bi translated">在交叉验证中设置数据拆分</h2><pre class="ks kt ku kv gt ml mm mn mo aw mp bi"><span id="26fa" class="mq mr it mm b gy ms mt l mu mv">    # set cross-validation<br/>    tscv = TimeSeriesSplit(n_splits=10)<br/>    data_split = tscv.split(X_prepared)</span></pre><p id="c212" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这里她使用了一种专门为时序数据设计的方法，<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html" rel="noopener ugc nofollow" target="_blank"><strong class="lo jd"><em class="ln"/></strong></a><strong class="lo jd"><em class="ln">。</em>T9】</strong></p><p id="5756" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">通常，人们使用<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html" rel="noopener ugc nofollow" target="_blank"> k-fold </a>交叉验证来随机分割训练数据，或者使用<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html" rel="noopener ugc nofollow" target="_blank">分层k-fold </a>交叉验证来保留每个类别的样本百分比[2]。但是这两种方法不适合时间序列数据，因为它们没有保持数据点的原始顺序。</p><p id="8a09" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这一步非常标准，与过度拟合问题无关。</p><h2 id="e0ce" class="mq mr it bd mw mx my dn mz na nb dp nc mi nd ne nf mj ng nh ni mk nj nk nl iz bi translated">超参数空间和定义模型</h2><pre class="ks kt ku kv gt ml mm mn mo aw mp bi"><span id="ef66" class="mq mr it mm b gy ms mt l mu mv">    # hyper-parameter space<br/>    param_grid_RF = {<br/>        'n_estimators' : [10,20,50,100,200,500,1000],<br/>        'max_features' : [0.6,0.8,"auto","sqrt"]<br/>    }<br/>    # build random forest model<br/>    rf_model = RandomForestRegressor(random_state=42,n_jobs=-1)</span></pre><p id="60eb" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">关于超参数调优，要给出一组候选，通常在<a class="ae lh" href="https://scikit-learn.org/stable/modules/preprocessing.html" rel="noopener ugc nofollow" target="_blank"><strong class="lo jd"><em class="ln">sk learn</em></strong></a>包中定义为字典格式。超参数的名称是从模型中建立的，例如，我妻子的代码中的<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" rel="noopener ugc nofollow" target="_blank"><strong class="lo jd"><em class="ln">【RandomForestRegressor】</em></strong></a>。</p><p id="afe9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">所谓的<em class="ln">“tuning-hyperparameter”</em>步骤是在交叉验证过程中，从你给定的候选项中选择出优于其他超参数组合的最佳超参数组合。</p><p id="ac8b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">这是导致过度拟合问题的零件。</strong></p><h2 id="0eb1" class="mq mr it bd mw mx my dn mz na nb dp nc mi nd ne nf mj ng nh ni mk nj nk nl iz bi translated">解决过拟合问题。</h2><p id="1d08" class="pw-post-body-paragraph ll lm it lo b lp nm kd lr ls nn kg lu mi no lx ly mj np mb mc mk nq mf mg mh im bi translated">如果参考<strong class="lo jd"> <em class="ln"> sklearn </em> </strong>中随机森林回归器的手册页，可以看到函数<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" rel="noopener ugc nofollow" target="_blank"><strong class="lo jd"><em class="ln">RandomForestRegressor</em></strong></a><strong class="lo jd"><em class="ln"/></strong>(此处未列出，有兴趣请参考网页)<strong class="lo jd"> <em class="ln">。</em>T47】</strong></p><p id="7451" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">然而，在上面的代码中，在调优过程中，只有<strong class="lo jd"> <em class="ln"> n_estimators </em> </strong>和<strong class="lo jd"> <em class="ln"> max_features </em> </strong>被传递给函数，这导致所有其他参数都采用默认的<strong class="lo jd">值。</strong></p><p id="d5ca" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">函数中有一个参数叫做<strong class="lo jd"> <em class="ln"> max_depth </em> </strong>，是你的随机森林模型中<a class="ae lh" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">树的最大深度。它的缺省值是“无”，这意味着决策树将深入到每个叶子是纯的或者所有叶子最多有m个样本的深度</a><a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" rel="noopener ugc nofollow" target="_blank">，其中m由<strong class="lo jd"> <em class="ln"> min_samples_split(缺省值= 2) </em> </strong>定义。</a></p><p id="fc0c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">该设置显著增加了训练模型的复杂性。并且基于<a class="ae lh" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" rel="noopener ugc nofollow" target="_blank"> <em class="ln">偏倚和方差</em> </a>之间的权衡，在上面的代码中训练的模型具有低偏倚和高方差。当然最后导致了过拟合问题。</p><p id="3fc1" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我通过添加<strong class="lo jd"> <em class="ln"> max_depth </em> </strong>到超参数空间解决了这个问题。</p><pre class="ks kt ku kv gt ml mm mn mo aw mp bi"><span id="f5b5" class="mq mr it mm b gy ms mt l mu mv">    # hyper-parameter space<br/>    param_grid_RF = {<br/>        'n_estimators' : [10,20,50,100,200,500,1000],<br/>        'max_features' : [0.6,0.8,"auto","sqrt"],<br/>        'max_depth' : [4,5,6]<br/>    }</span></pre><p id="62a8" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">其实<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" rel="noopener ugc nofollow" target="_blank"><strong class="lo jd"><em class="ln">RandomForestRegressor</em></strong></a>中还有一些其他参数可以影响模型的复杂度，但不建议全部放入超参数空间。</p><p id="c05f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">上面代码中额外的超参数已经将计算成本增加了三倍。因此，仅仅调整更多参数而成倍增加运行时间<strong class="lo jd">是不值得的。</strong></p><p id="77f5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">相反，您可以像这样在模型初始化中设置它们中的一些。</p><pre class="ks kt ku kv gt ml mm mn mo aw mp bi"><span id="e3a2" class="mq mr it mm b gy ms mt l mu mv"># hyper-parameter space<br/>    param_grid_RF = {<br/>        'n_estimators' : [10,20,50,100,200,500,1000]<br/>    }<br/># build random forest model<br/>    rf_model = RandomForestRegressor(random_state=42,n_jobs=-1,max_features=0.6,max_depth=5)</span></pre><p id="f7a6" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在上面的代码中，唯一调整过的超参数是<strong class="lo jd"><em class="ln"/></strong><strong class="lo jd"><em class="ln">max _ features、</em> </strong>以及<strong class="lo jd"> <em class="ln"> max_depth、</em> </strong>在训练过程中是固定的。调谐的或固定的参数不必如此。</p><p id="db76" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">当<strong class="lo jd"> <em class="ln">已经</em> </strong>对一些超参数的选择有所了解时，设置的基本原理是减少计算成本。</p></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><p id="2dbf" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">是的，我妻子的问题已经解决了。然而，老实说，我不可能找到它<strong class="lo jd">，直到我检查了软件包中所有的默认参数。</strong>这就是为什么我想与你分享这个经验，以便你可以在你的机器学习模型训练过程中更加小心默认参数。</p><p id="b6eb" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><em class="ln">过拟合</em>是数据科学中最常见的问题之一，主要来源于模型的<strong class="lo jd"> <em class="ln">高复杂度</em> </strong>和数据点的<strong class="lo jd"><em class="ln"/></strong>缺失。</p><p id="586b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">为了避免它，最好完全控制你使用的软件包。</p><p id="25ca" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">希望这条建议能帮到你。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/f82b5765fefe03d8a5c78205fa47aee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*x2ertXEh2d6sP2SV"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Joshua Sortino 在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><h2 id="b91e" class="mq mr it bd mw mx my dn mz na nb dp nc mi nd ne nf mj ng nh ni mk nj nk nl iz bi translated">参考资料:</h2><ol class=""><li id="4a02" class="op oq it lo b lp nm ls nn mi or mj os mk ot mh ou ov ow ox bi translated"><a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . preprocessing . standard scaler . html # sk learn . preprocessing . standard scaler</a></li><li id="0623" class="op oq it lo b lp oy ls oz mi pa mj pb mk pc mh ou ov ow ox bi translated"><a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . model _ selection。StratifiedKFold.html</a></li><li id="e2c2" class="op oq it lo b lp oy ls oz mi pa mj pb mk pc mh ou ov ow ox bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Random_forest</a></li></ol></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><h2 id="923b" class="mq mr it bd mw mx my dn mz na nb dp nc mi nd ne nf mj ng nh ni mk nj nk nl iz bi translated">更正:</h2><p id="d2f5" class="pw-post-body-paragraph ll lm it lo b lp nm kd lr ls nn kg lu mi no lx ly mj np mb mc mk nq mf mg mh im bi translated">我要感谢Antonio Carlos 在我最初的帖子中指出了这个问题。他还建议了一个不错的帖子“<a class="ae lh" rel="noopener" target="_blank" href="/pre-process-data-with-pipeline-to-prevent-data-leakage-during-cross-validation-e3442cca7fdc">用管道预处理数据，以防止交叉验证过程中的数据泄漏</a>”到题。我真的很感激。</p><p id="8243" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">它与<strong class="lo jd"> <em class="ln">预处理</em> </strong>部分的流水线有关。它不应该在整个训练数据集上进行，而应该在交叉验证步骤的每次迭代中进行，因为训练数据的整体归一化将导致CV步骤中训练和验证数据集之间的泄漏。</p><p id="f357" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">因此，我将相应的修正代码放在下面:</p><pre class="ks kt ku kv gt ml mm mn mo aw mp bi"><span id="c106" class="mq mr it mm b gy ms mt l mu mv">def train_pipeline_rf(X,y):<br/>    # X are factors<br/>    # y is output<br/>    # set cross-validation<br/>    tscv = TimeSeriesSplit(n_splits=10)<br/>    data_split = tscv.split(X)<br/>    <br/>    # build a pipeline of pre-processing and random forest model<br/>    my_pipe = Pipeline([<br/>        ('imputer', SimpleImputer(strategy="median")), <br/>        ('std_scaler', StandardScaler()),<br/>        ('rf_model', RandomForestRegressor(random_state=42,n_jobs=-1))<br/>    ])</span><span id="0e8f" class="mq mr it mm b gy pd mt l mu mv">    # hyper-parameter space<br/>    param_grid_RF = {<br/>        'rf_model__n_estimators' : [10,20,50,100,200,500,1000],<br/>        'rf_model__max_features' : [0.6,0.8,"auto","sqrt"],<br/>        'rf_model__max_depth' : [4,5,6]<br/>    }<br/>    # gridsearch for the best hyper-parameter within the pipeline.<br/>    gs_rf = GridSearchCV(my_pipe, param_grid=param_grid_RF, cv=data_split, scoring='neg_mean_squared_error', n_jobs=-1)<br/>    # fit dataset<br/>    gs_rf.fit(X, y)<br/>    return gs_rf</span></pre></div></div>    
</body>
</html>