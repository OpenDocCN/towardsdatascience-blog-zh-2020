<html>
<head>
<title>Detecting Fake News With Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用深度学习检测假新闻</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/detecting-fake-news-with-deep-learning-7505874d6ac5?source=collection_archive---------37-----------------------#2020-03-30">https://towardsdatascience.com/detecting-fake-news-with-deep-learning-7505874d6ac5?source=collection_archive---------37-----------------------#2020-03-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1193" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用Keras实现简单的LSTM</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1105a7fcf837fb147a1775586edccbb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9zw6JrJvOia3fANCflkjjA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4881486" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae kv" href="https://pixabay.com/users/memyselfaneye-331664/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4881486" rel="noopener ugc nofollow" target="_blank"> memyselfaneye </a></p></figure><p id="472f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我一直想做一个涉及文本分类的小项目，并决定尝试一种我以前没有使用过的架构:长短期记忆(LSTM)。简而言之:LSTM氏症是一种循环神经网络(RNN)，能够长时间记忆信息(这是一种优于香草RNN的优势)。如果你想了解更多的细节:<a class="ae kv" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">这里有一个关于LSTM建筑</a>的精彩而详尽的解释。</p><p id="15fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好吧，让我们开始吧！</p><p id="9fb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我在Kaggle上找到了一个真假新闻的数据集:<a class="ae kv" href="https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset#Fake.csv" rel="noopener ugc nofollow" target="_blank">真假新闻数据集</a>。我试着在Jupyter笔记本上本地做这件事，但是一旦我到了训练部分，我的电脑几乎爆炸了——一个时期的ETA至少是2个小时。我把东西转移到一个GPU加速的Google Colab实例中，事情变得更加顺利。<a class="ae kv" href="https://colab.research.google.com/drive/15SmOCOnL8IktcMVDn0Lxhe9U_aq_ppqN" rel="noopener ugc nofollow" target="_blank">这里是查看笔记本</a>的链接。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="a494" class="ly lz iq lu b gy ma mb l mc md"># load datasets into a panda's dataframe<br/>real = pd.read_csv('data/True.csv')<br/>fake = pd.read_csv('data/Fake.csv')</span></pre><p id="89b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们看看数据是什么样子的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi me"><img src="../Images/f8eee14aae260ce17ef65b053630a40c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qEw9dKWTBDDiHDunlJqmoQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">“真实”数据集</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mf"><img src="../Images/9654c5244c7e4d30360ae5691d9b8f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0uS5QisVtCmg2E2ofnNk6A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">“假新闻”数据集</p></figure><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="543a" class="ly lz iq lu b gy ma mb l mc md">real.head()</span><span id="a881" class="ly lz iq lu b gy mg mb l mc md">fake.head()</span></pre><p id="f71f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我立即注意到的一件事是真实新闻文章中的“(路透社)”标签。事实证明，几乎所有的真实故事都来自路透社，几乎没有任何假新闻包含这个词。我想最终比较有单词和没有单词的模型。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="e5ec" class="ly lz iq lu b gy ma mb l mc md">real.loc[real.text.str.contains('Reuters')].count()/real.count()<br/>&gt; title      0.998179<br/>&gt; text       0.998179<br/>&gt; subject    0.998179<br/>&gt; date       0.998179</span><span id="896e" class="ly lz iq lu b gy mg mb l mc md">fake.loc[fake.text.str.contains('Reuters')].count()/fake.count()<br/>&gt; title      0.013247<br/>&gt; text       0.013247<br/>&gt; subject    0.013247<br/>&gt; date       0.013247</span></pre><p id="3dce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们给出数据标签，并将它们组合成一个数据集进行训练，然后对它们进行训练/测试拆分。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="1486" class="ly lz iq lu b gy ma mb l mc md"># Give labels to data before combining<br/>fake['fake'] = 1<br/>real['fake'] = 0<br/>combined = pd.concat([fake, real])</span><span id="3aa5" class="ly lz iq lu b gy mg mb l mc md">## train/test split the text data and labels<br/>features = combined['text']<br/>labels = combined['fake']</span><span id="a7e7" class="ly lz iq lu b gy mg mb l mc md">X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state = 42)</span></pre><p id="9dde" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们用Keras中的Tokenizer对象处理文本数据。我们不会删除停用词，因为每个词的上下文以及句子和段落的构成都很重要。我认为这两个班级的写作质量存在潜在的差异。路透社的记者居然有文字编辑！</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="61a8" class="ly lz iq lu b gy ma mb l mc md"># the model will remember only the top 2000 most common words<br/>max_words = 2000<br/>max_len = 400</span><span id="2f03" class="ly lz iq lu b gy mg mb l mc md">token = Tokenizer(num_words=max_words, lower=True, split=' ')<br/>token.fit_on_texts(X_train.values)<br/>sequences = token.texts_to_sequences(X_train.values)<br/>train_sequences_padded = pad_sequences(sequences, maxlen=max_len)</span></pre><p id="93f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们建立模型！</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="8812" class="ly lz iq lu b gy ma mb l mc md">embed_dim = 50<br/>lstm_out = 64<br/>batch_size = 32</span><span id="3f3c" class="ly lz iq lu b gy mg mb l mc md">model = Sequential()<br/>model.add(Embedding(max_words, embed_dim, input_length = max_len))<br/>model.add(LSTM(lstm_out))<br/>model.add(Dense(256))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(1, name='out_layer'))<br/>model.add(Activation('sigmoid'))</span><span id="216c" class="ly lz iq lu b gy mg mb l mc md">model.compile(loss = 'binary_crossentropy', optimizer='adam',\<br/>               metrics = ['accuracy'])</span><span id="f600" class="ly lz iq lu b gy mg mb l mc md">print(model.summary())</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mh"><img src="../Images/7650c8555582874be889bb5a9cecc3d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*crDmesZ1WFidj2uwH-pJUg.png"/></div></div></figure><p id="5815" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们来训练模型。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="60af" class="ly lz iq lu b gy ma mb l mc md">model.fit(train_sequences_padded, y_train, batch_size=batch_size, epochs = 5, validation_split=0.2)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mi"><img src="../Images/88d7f270db1c7aa8adf79fba929ce083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mlcJp7EyRQz1Se6inKN9vg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基于训练数据的LSTM模型训练进展</p></figure><p id="2734" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们评估一下测试/维持集。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="7b30" class="ly lz iq lu b gy ma mb l mc md">test_sequences = token.texts_to_sequences(X_test)</span><span id="7af3" class="ly lz iq lu b gy mg mb l mc md">test_sequences_padded = pad_sequences(test_sequences,\ <br/>                                       maxlen=max_len)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mj"><img src="../Images/b05da0ea1bb5bf52daf9ec0831777757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cOmPzgTiHhZgh5S5x9WbsA.png"/></div></div></figure><p id="efe2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">非常非常好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/7421d3d9453ffd0b71a1ab541bf5ab2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*OBTEsqWNS8_42vX1Ud_CDA.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/e0385d0f0b133e6fdab9c46b3c88d2bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*t-VS65kd53N5oDMC5YdHAg.png"/></div></figure><p id="be01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">绘制模型的准确性和损失表明，它可能仍然需要更多的训练，因为没有过度拟合的证据。</p><p id="5fed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">99%是一个很好的结果，但是，记住所有的真实新闻都有“路透社”在里面？假设它只是一个单词，我想看看从文本中删除它会如何影响模型的性能(如果有的话)。我认为在单词选择和编辑方面一定有很多其他潜在的模式，可能会使模型的分类变得容易。</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><p id="bd1b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从所有新闻文本中删除“Reuters”后，最终模型的测试集评估准确率为98.5%。因此，其预测能力略有下降(0.6%的差异)。我还以为会更多。</p><p id="24da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">获得这样一个几乎现成的好结果应该会让您更多地考虑底层数据。如果它好得令人难以置信，它很可能就是真的！路透社的新闻故事依赖于风格指南，并经过严格编辑，我不能说假新闻也是如此。这些潜在的模式可能允许模型从这个特定的数据集学习，<em class="ls">但是它如何推广到从不同来源在野外发现的新闻呢</em>？</p><p id="f72d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我在2019年脸书上查看最多的假新闻文章上找到了一篇<a class="ae kv" href="https://www.businessinsider.com/most-viewed-fake-news-stories-shared-on-facebook-2019-2019-11" rel="noopener ugc nofollow" target="_blank">商业内幕文章</a>。很难找到其中一些例子的全文。</p><p id="0068" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于第五个名为<em class="ls">“奥马尔与恐怖组织伊斯兰团体举行秘密筹款活动”的分享故事</em>，该模型预测其为<strong class="ky ir">真实</strong>。</p><p id="df94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于标题为“<em class="ls">的#1分享故事，特朗普的祖父是一名皮条客和逃税者；他的父亲是三k党成员，</em>模型预测它为<strong class="ky ir">假新闻</strong> <em class="ls">。</em></p><p id="1aa7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我还在CNN上抢到了当前的头条新闻，“<a class="ae kv" href="https://www.cnn.com/2020/03/29/politics/trump-coronavirus-press-conference/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="ls">特朗普将联邦社交距离准则延长至4月30日</em> </a>”，模型预测为<strong class="ky ir">真实</strong>。</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h2 id="958e" class="ly lz iq bd mt mu mv dn mw mx my dp mz lf na nb nc lj nd ne nf ln ng nh ni nj bi translated">结论</h2><p id="e1ac" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">该模型似乎是训练和测试数据集上的一个非常强大的预测器，但是它可能不会在此之外进行很好的推广。当出现一个不符合设定的假新闻故事时，模型是1对2。这个样本规模很小，我想尝试追踪更多数据集之外的假新闻，看看它的表现如何。我也想尝试更多的艺术模式(埃尔莫/伯特)。希望你喜欢阅读！</p></div></div>    
</body>
</html>