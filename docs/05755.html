<html>
<head>
<title>Word2Vec Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec 实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28?source=collection_archive---------2-----------------------#2020-05-13">https://towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28?source=collection_archive---------2-----------------------#2020-05-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2e6f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何使用 numpy 和 python 实现 Word2Vec</h2></div><p id="37f9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文是关于一种非常流行的单词嵌入技术 Word2Vec 的实现。它是由<a class="ae lb" href="https://en.wikipedia.org/wiki/Google" rel="noopener ugc nofollow" target="_blank">谷歌</a>的<a class="ae lb" href="https://en.wikipedia.org/wiki/Tomas_Mikolov" rel="noopener ugc nofollow" target="_blank">托马斯·米科洛夫</a>实现的。</p><h1 id="09eb" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">内容:</h1><h1 id="f3ad" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">概念</h1><ul class=""><li id="9c06" class="lu lv iq kh b ki lw kl lx ko ly ks lz kw ma la mb mc md me bi translated">目标</li><li id="ce09" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">介绍</li><li id="0040" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">核心理念</li><li id="e70c" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">体系结构</li></ul><h1 id="1132" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">B —实施</h1><ul class=""><li id="de12" class="lu lv iq kh b ki lw kl lx ko ly ks lz kw ma la mb mc md me bi translated">数据准备</li><li id="e6c4" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">模特培训</li><li id="73cc" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">模型推理和分析</li></ul></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h2 id="d9b9" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated">目标</h2><p id="fc47" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">本文的目的是使用 numpy 展示 python 中 Word2Vec 的内部工作方式。我不会为此使用任何其他库。这个实现不是一个有效的实现，因为这里的目的是理解它背后的机制。你可以在这里找到官方报纸<a class="ae lb" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank"/>。</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h2 id="c90f" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated">介绍</h2><p id="3b22" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">计算机只理解数字语言。我们将文本数据编码成数字的方式对结果影响很大。一般来说，有 3 种技术可以用来完成这项任务。</p><ul class=""><li id="c4e8" class="lu lv iq kh b ki kj kl km ko ng ks nh kw ni la mb mc md me bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">文字袋</a></li><li id="41a1" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> Tf-Idf </a></li><li id="d1c1" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">Word2Vec</li></ul><p id="b6c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中，word2vec 在 NLP 任务中表现得非常好。这个概念背后的核心思想非常简单，但却能产生惊人的效果。</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h2 id="8748" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated">核心理念</h2><blockquote class="nj nk nl"><p id="0619" class="kf kg nm kh b ki kj jr kk kl km ju kn nn kp kq kr no kt ku kv np kx ky kz la ij bi translated">“看一个人交的朋友就知道他是谁”</p><p id="cad3" class="kf kg nm kh b ki kj jr kk kl km ju kn nn kp kq kr no kt ku kv np kx ky kz la ij bi translated">― <strong class="kh ir">伊索</strong></p></blockquote><p id="d909" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一句众所周知的谚语。word2vec 也主要基于这个想法。一言既出，驷马难追。这听起来如此奇怪和有趣，但它给出了惊人的结果。让我们试着多理解一点。</p><p id="4863" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一些例句:</p><ul class=""><li id="71bc" class="lu lv iq kh b ki kj kl km ko ng ks nh kw ni la mb mc md me bi translated">你在花园里辛勤劳动的回报显而易见。</li><li id="e0e6" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">保持我的房间整洁是一件困难的工作。</li><li id="91b2" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">艰苦的工作开始对他产生影响。</li></ul><p id="3f58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们可以很容易地看到，“努力”和“工作”这两个词出现的位置非常接近。作为人类来说，这似乎很容易观察到，但对于计算机来说，这是一项非常困难的任务。因此，当我们将这些单词矢量化(将单词转化为数字)时，它们作为数字的表示应该是相似或接近的，这似乎是显而易见的。这正是 word2vec 所要实现的，并且取得了非常好的结果。说够了，现在是我们动手的时候了！</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h2 id="3bbc" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated">体系结构</h2><p id="ad70" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">因此，在理解了核心思想之后，我们知道该算法是基于识别彼此邻近的单词。换句话说，我们可以说，如果计算机试图学习单词“hard”和“work”在彼此附近出现，那么它将据此学习向量。</p><p id="201e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们说我们的“目标”单词是“硬的”,我们需要学习一个好的向量，我们向计算机提供它的邻近单词或“上下文”单词，在这种情况下是“工作”,在“the，began，is 等”中。</p><p id="0915" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有两种主要的架构试图了解上述内容。跳过 gram 和 CBOW</p><p id="6909" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"><em class="nm"/></strong>:这样我们就了解了目标词和语境词的概念。该模型试图学习每个目标单词的上下文单词。</p><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/e9991c7ad4d89afb809de5eba1792ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*hEuvSxlRBVwPJbVFsKNlCg.png"/></div><p class="ny nz gj gh gi oa ob bd b be z dk translated">图 1:跳过 gram 架构。<a class="ae lb" href="https://www.researchgate.net/figure/The-architecture-of-Skip-gram-model-20_fig1_322905432" rel="noopener ugc nofollow" target="_blank">演职员表</a></p></figure><p id="4bad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">直觉:</p><p id="c36d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正文:['成功的最佳途径是努力工作和坚持']</p><p id="4cc1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，对于该模型，我们的输入如下:</p><ul class=""><li id="702f" class="lu lv iq kh b ki kj kl km ko ng ks nh kw ni la mb mc md me bi translated">目标词:<strong class="kh ir">最好。</strong>上下文词:<strong class="kh ir">(方式)</strong>。</li><li id="5de9" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">目标词:<strong class="kh ir">方式</strong>。现在我们有两个词，一个在前面，一个在后面。所以在这个场景中，我们的上下文单词将是:<strong class="kh ir"> (Best，to) </strong>。</li><li id="35f6" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">目标词:<strong class="kh ir">到</strong>。现在我们也有两个词，一个在 I 之前，一个在 I 之后，即(成功之路)。但如果我们再想一想，那么“to”和“is”可以在彼此相邻的句子中找到。比如“他要去市场”。因此，如果我们将单词“is”包含在上下文单词列表中，这是一个好主意。但现在我们可以争论“最好”或“通过”。于是就有了“<strong class="kh ir">窗口大小</strong>的概念。窗口大小是我们决定要考虑多少邻近单词的数字。因此，如果窗口大小为 1，那么我们的上下文单词列表就变成了<strong class="kh ir"> (way，success) </strong>。并且如果窗口大小是 2，那么我们的上下文单词列表变成<strong class="kh ir">(最佳，方式，成功，是)</strong>。</li><li id="1dc3" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">类似地，我们可以列出其余的单词</li></ul><p id="e977" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们看到这里有一个输入层、一个隐藏层和一个输出层。我们还可以看到有两组权重(W，W `)。</p><p id="7fd0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nm"> CBOW: </em> </strong>语境包词。从概念上来说，这与 skip-gram 正好相反。这里我们试图从上下文单词列表中预测目标单词。因此，对于我们的示例，我们将输入为<strong class="kh ir">(最佳、方式、成功、是)</strong>，我们需要从中预测<strong class="kh ir">到</strong>。</p><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/14c24fc8216e950371934f5eec0f1e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*8bp_c0bxWCpaCOFN2HAYoA.png"/></div><p class="ny nz gj gh gi oa ob bd b be z dk translated">图 2: Cbow 模型架构</p></figure><p id="706d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到，这与跳格模型正好相反。</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h1 id="9c9d" class="lc ld iq bd le lf od lh li lj oe ll lm jw of jx lo jz og ka lq kc oh kd ls lt bi translated">实现进程</h1><p id="7a38" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">在本文中，我将实现跳格模型。</p><h2 id="79f5" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated">数据准备</h2><p id="3fb8" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">为了训练一个模型来学习单词的良好向量，我们需要大量的数据。但是在本文中，我将尝试展示一个非常小的数据集的工作原理。数据集由取自维基百科的杰弗里·阿彻各种故事的情节组成。</p><p id="7083" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm">步骤 1:从文件中读取数据</em></p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="d659" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解释:</p><ul class=""><li id="254a" class="lu lv iq kh b ki kj kl km ko ng ks nh kw ni la mb mc md me bi translated">第 2–4 行:将文本文件的内容读取到列表中</li><li id="f5de" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">第 7–8 行:只保留字母，删除每行中的其他内容</li><li id="54df" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">第 9–17 行:重复句子中的每个单词，如果指定了停用词，则删除停用词</li></ul><p id="27b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm">第二步:生成变量</em></p><p id="862e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将需要一些变量，这些变量将在后面的章节中派上用场。</p><pre class="nr ns nt nu gt ok ol om on aw oo bi"><span id="18f8" class="mr ld iq ol b gy op oq l or os">word_to_index : A dictionary mapping each word to an integer value {‘modern’: 0, ‘humans’: 1} </span><span id="c745" class="mr ld iq ol b gy ot oq l or os">index_to_word : A dictionary mapping each integer value to a word {0: ‘modern’, 1: ‘humans’}</span><span id="a3cc" class="mr ld iq ol b gy ot oq l or os">corpus : The entire data consisting of all the words </span><span id="df66" class="mr ld iq ol b gy ot oq l or os">vocab_size : Number of unique words in the corpus</span></pre><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="b484" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解释:</p><ul class=""><li id="a214" class="lu lv iq kh b ki kj kl km ko ng ks nh kw ni la mb mc md me bi translated">第 10 行:将每个单词转换成小写</li><li id="b288" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">第 12–15 行:用这个单词更新字典，如果这个单词还没有出现在字典中，就进行计数</li></ul><p id="0747" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该代码的输出将给出:</p><pre class="nr ns nt nu gt ok ol om on aw oo bi"><span id="f7f8" class="mr ld iq ol b gy op oq l or os">text = ['Best way to success is through hardwork and persistence']</span><span id="2913" class="mr ld iq ol b gy ot oq l or os">Number of unique words: 9</span><span id="8184" class="mr ld iq ol b gy ot oq l or os">word_to_index :  {'best': 0, 'way': 1, 'to': 2, 'success': 3, 'is': 4, 'through': 5, 'hardwork': 6, 'and': 7, 'persistence': 8}</span><span id="d26c" class="mr ld iq ol b gy ot oq l or os">index_to_word :  {0: 'best', 1: 'way', 2: 'to', 3: 'success', 4: 'is', 5: 'through', 6: 'hardwork', 7: 'and', 8: 'persistence'}</span><span id="cac8" class="mr ld iq ol b gy ot oq l or os">corpus: ['best', 'way', 'to', 'success', 'is', 'through', 'hardwork', 'and', 'persistence']</span><span id="0d59" class="mr ld iq ol b gy ot oq l or os">Length of corpus : 9</span></pre><p id="8713" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm">第三步:生成训练数据</em></p><p id="555e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在看代码之前，让我们先了解一些概念。</p><ul class=""><li id="a128" class="lu lv iq kh b ki kj kl km ko ng ks nh kw ni la mb mc md me bi translated">One-hot-vector:基本上这是一种用 0 和 1 对数据进行编码的方法。因此，我们的 one-hot-vector 的大小将为 2，因为我们有两个单词，我们将有两个单独的向量，一个用于 hi，一个用于 john。例如:(word: hi，one-hot-vector: [1，0])，(word: john，one-hot-vector: [0，1]</li></ul><p id="9f12" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下代码为我们的数据生成一个热向量:</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="a6e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解释:</p><pre class="nr ns nt nu gt ok ol om on aw oo bi"><span id="0bdb" class="mr ld iq ol b gy op oq l or os">text = ['Best way to success is through hardwork and persistence']</span><span id="a4d6" class="mr ld iq ol b gy ot oq l or os">Window size = 2, Vocab size = 9<br/></span><span id="2510" class="mr ld iq ol b gy ot oq l or os">We will set the indices as 1 according to the word_to_index dict i.e best : 0,  so we set the 0th index as 1 to denote natural</span><span id="d9c4" class="mr ld iq ol b gy ot oq l or os">Target word = best    <br/>Context words = (way,to)<br/>Target_word_one_hot_vector = [1, 0, 0, 0, 0, 0, 0, 0, 0]<br/>Context_word_one_hot_vector = [0, 1, 1, 0, 0, 0, 0, 0, 0]</span><span id="de21" class="mr ld iq ol b gy ot oq l or os">Target word = way    <br/>Context words = (best,to,success)<br/>Target_word_one_hot_vector = [0, 1, 0, 0, 0, 0, 0, 0, 0]<br/>Context_word_one_hot_vector= [1, 0, 1, 1, 0, 0, 0, 0, 0]</span></pre><p id="fca1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有一种替代方法来生成上下文 _ 单词 _ 一个 _ 热 _ 向量。</p><pre class="nr ns nt nu gt ok ol om on aw oo bi"><span id="ca6d" class="mr ld iq ol b gy op oq l or os">Target word = best    <br/>Context words = (way,to)<br/>Target_word_one_hot_vector = [1, 0, 0, 0, 0, 0, 0, 0, 0]<br/>Context_word_one_hot_vector = [0, 1, 0, 0, 0, 0, 0, 0, 0],[0, 0, 1, 0, 0, 0, 0, 0, 0]</span></pre><p id="cff5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们有两个不同的列表，而不是在一个列表中显示索引。但这种方法的问题是，如果我们的数据量增加，它将占用大量空间。我已经用 python 脚本说明了这一点。参考<a class="ae lb" href="https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Memory_comparison.ipynb" rel="noopener ugc nofollow" target="_blank">代码</a>查看不同之处。</p><p id="cec4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们有了为目标单词和上下文单词生成的向量。为了训练模型，我们需要(X，Y)形式的数据，即(目标单词，上下文单词)。</p><p id="61c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是通过以下代码实现的:</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="c9fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解释:</p><pre class="nr ns nt nu gt ok ol om on aw oo bi"><span id="e0a8" class="mr ld iq ol b gy op oq l or os">text = ['Best way to success is through hardwork and persistence']</span></pre><ul class=""><li id="b1d5" class="lu lv iq kh b ki kj kl km ko ng ks nh kw ni la mb mc md me bi translated">第 7 行:迭代语料库</li><li id="fdfd" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">第 9 行:将第 I 个单词设置为目标单词</li><li id="8412" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">第 14，21，27 行:检查第 9 行的第 I 个单词是(first :Best)，(middle : way)还是(last : persistence)单词的条件。</li><li id="c1d4" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">第 17 行:如果是第一个单词，获取接下来的 2 个(window_size =2)单词，并将它们设置为上下文单词</li><li id="c0d5" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">第 21 行:如果是最后一个单词，获取前面的 2 个(window_size =2)单词，并将其设置为上下文单词</li><li id="5a34" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">第 30，37 行:如果我们的第 I 个单词是中间单词，那么我们需要在第 I 个单词之前获取 2 个(window_size =2)单词，在第 I 个单词之后获取 2 个(window_size =2)单词，并将所有 4 个单词都设置为上下文单词。如果在第 I 个单词之前或之后只有 1 个单词，我们只能得到 1 个单词。</li></ul><p id="366a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">示例:</p><pre class="nr ns nt nu gt ok ol om on aw oo bi"><span id="1d6a" class="mr ld iq ol b gy op oq l or os">**************************************************<br/>Target word:best . Target vector: [1. 0. 0. 0. 0. 0. 0. 0. 0.] <br/>Context word:['way', 'to'] .<br/>Context  vector: [0. 1. 1. 0. 0. 0. 0. 0. 0.] <br/>**************************************************<br/>Target word:way . Target vector: [0. 1. 0. 0. 0. 0. 0. 0. 0.] <br/>Context word:['best', 'to', 'success'] .<br/>Context  vector: [1. 0. 1. 1. 0. 0. 0. 0. 0.] <br/>**************************************************<br/>Target word:hardwork . Target vector: [0. 0. 0. 0. 0. 0. 1. 0. 0.] <br/>Context word:['through', 'is', 'and', 'persistence'] .<br/>Context  vector: [0. 0. 0. 0. 1. 1. 0. 1. 1.] <br/>**************************************************<br/>Target word:and . Target vector: [0. 0. 0. 0. 0. 0. 0. 1. 0.] <br/>Context word:['hardwork', 'through', 'persistence'] . <br/>Context  vector: [0. 0. 0. 0. 0. 1. 1. 0. 1.] <br/>**************************************************<br/>Target word:persistence . Target vector: [0. 0. 0. 0. 0. 0. 0. 0. 1.] <br/>Context word:['and', 'hardwork'] .<br/>Context  vector: [0. 0. 0. 0. 0. 0. 1. 1. 0.]</span></pre><h2 id="babd" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated">模特培训</h2><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi ou"><img src="../Images/c712f155c6802a06e76b1934009debfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*raP5OO6T7T92THIz6_SUFQ.png"/></div></div><p class="ny nz gj gh gi oa ob bd b be z dk translated">图 3:跳跃图的训练</p></figure><p id="2996" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上图显示了一个<a class="ae lb" href="https://en.wikipedia.org/wiki/Neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a>是如何被训练的，在这个例子中是一个 skip-gram 模型。这里我们可以看到只有一个隐藏层。当我们在深度学习中训练一个神经网络时，我们往往会有几个隐藏层。这就是这个跳过程序工作得如此好的地方。尽管只有一个隐藏层，但这是一个最先进的算法。</p><p id="b920" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nm">正向传播</em> </strong></p><p id="0750" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们有以下参数:</p><ul class=""><li id="8a4a" class="lu lv iq kh b ki kj kl km ko ng ks nh kw ni la mb mc md me bi translated">输入:我们给模型的输入。我们场景中的目标词</li><li id="1028" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">W_1 或 weight_inp_hidden:第一组权重与输入相乘得到隐藏层。</li><li id="20df" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">W_2 或 weight_hidden_output:第二组权重乘以隐藏层。</li><li id="e20d" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">Softmax 层:这是在 0 和 1 之间挤压输出概率的最后一层。关于 softmax 函数的一个很好的解释可以在<a class="ae lb" rel="noopener" target="_blank" href="/the-softmax-function-neural-net-outputs-as-probabilities-and-ensemble-classifiers-9bd94d75932">这里</a>找到。</li></ul><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="897b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nm">训练错误:</em> </strong></p><p id="622e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，一旦我们完成了一轮前向传播，我们就会得到一些输出值。所以很明显，和初始值相比，我们的预测会有一些误差。</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="56f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解释:这是反向传播，以更新下一次迭代的权重</p><pre class="nr ns nt nu gt ok ol om on aw oo bi"><span id="2c73" class="mr ld iq ol b gy op oq l or os">Example: below if we have 2 context words. These are not actual values.</span><span id="5c98" class="mr ld iq ol b gy ot oq l or os">These are just for showing how the error is calculated</span><span id="481b" class="mr ld iq ol b gy ot oq l or os">context_words = [1 0 0 1 0]</span><span id="c20d" class="mr ld iq ol b gy ot oq l or os">y_pred = [9 6 5 4 2]</span><span id="1d84" class="mr ld iq ol b gy ot oq l or os">So if we break the context_word vector : [1 0 0 0 0] and [0 0 0 1 0] . 1 at index 0 and 3</span><span id="437f" class="mr ld iq ol b gy ot oq l or os">The error should be calculated as :</span><span id="092e" class="mr ld iq ol b gy ot oq l or os">diff_1 = y_pred - context_word_vector_1 =<br/> [9 6 5 4 2] - [1 0 0 0 0] = [8 6 5 4 2]</span><span id="7002" class="mr ld iq ol b gy ot oq l or os">diff_2 = y_pred - context_word_vector_2 = <br/>[9 6 5 4 2] - [0 0 0 1 0] = [9 6 5 3 2]</span><span id="6512" class="mr ld iq ol b gy ot oq l or os">Total_error = diff_1 + diff_2(column_wise) = [17 12 10 7 4]</span><span id="ce0d" class="mr ld iq ol b gy ot oq l or os">Since our context vector has only 1 array , we implement the above as:</span><span id="0483" class="mr ld iq ol b gy ot oq l or os">index_of_1_in_context_words -&gt; <strong class="ol ir">Line (6,7)</strong><br/>A dictionary which has the index of 1's in the context_word_vector -&gt; {0: 'yes', 3: 'yes'}</span><span id="3755" class="mr ld iq ol b gy ot oq l or os">number_of_1_in_context_vector -&gt; A count for the above -&gt; 2</span><span id="4540" class="mr ld iq ol b gy ot oq l or os">We loop the y_pred array and do the calculations as:</span><span id="4d05" class="mr ld iq ol b gy ot oq l or os">for i,value in enumerate(y_p):</span><span id="4d8b" class="mr ld iq ol b gy ot oq l or os"><strong class="ol ir">Line(13,14)</strong></span><span id="847b" class="mr ld iq ol b gy ot oq l or os"> if the ith index of y_pred has a 1 in context_word_vector:<br/>    total_error[i]  -&gt; i:0 . y_pred[i]:9.  -&gt; (9-1) + (1*9) <br/>    -&gt;error_calculated: 17<br/>    total_error[i]  -&gt; i:3 . y_pred[i]:4.  -&gt; (4-1) + (1*4)<br/>    -&gt;error_calculated: 7</span><span id="4e02" class="mr ld iq ol b gy ot oq l or os"><strong class="ol ir">Line(15,16)</strong></span><span id="955e" class="mr ld iq ol b gy ot oq l or os"> else:<br/>    total_error[i]  -&gt; i:1 . y_pred[i]:6.  -&gt; 6*2 <br/>     -&gt;error_calculated: 12<br/>    total_error[i]  -&gt; i:2 . y_pred[i]:5.  -&gt; 5*2<br/>     -&gt;error_calculated: 10<br/>    total_error[i]  -&gt; i:4 . y_pred[i]:2.  -&gt; 2*2<br/>     -&gt; error_calculated: 4</span><span id="0c76" class="mr ld iq ol b gy ot oq l or os">total_error  -&gt;  [17 12 10 7 4]</span></pre><p id="7816" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nm">反向传播</em> </strong></p><p id="961a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">利用上面计算的误差，我们需要更新权重(w1 和 w2 ),以便我们的网络尝试纠正该误差。</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="51f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="http://www.claudiobellei.com/2018/01/06/backprop-word2vec/#skipgram" rel="noopener ugc nofollow" target="_blank">这里的</a>是一个很好的链接，解释了反向传播的导数方程。</p><p id="9335" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nm">最后损失计算</em> </strong></p><p id="fac4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">损失计算如下:</p><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/c88f94b5a89916030ba5d98849579c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*OuHAONh5KrjNwy4xyxVlLQ.png"/></div><p class="ny nz gj gh gi oa ob bd b be z dk translated">图 4:损失函数。<a class="ae lb" href="https://arxiv.org/pdf/1411.2738.pdf" rel="noopener ugc nofollow" target="_blank">学分</a></p></figure><p id="2caf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们稍微深入地观察损失函数 E，我们会发现，在给定 WI(输入单词)的情况下，我们正试图优化找到正确上下文单词 p(WO，1，WO，2，，WO，C)的概率。因此，当我们更接近为每个给定的目标单词找到正确的上下文单词的分布时，损失将会减少。</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="c385" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解释:</p><p id="d406" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">损失函数由两部分组成。</p><ol class=""><li id="98cf" class="lu lv iq kh b ki kj kl km ko ng ks nh kw ni la pa mc md me bi translated">第一部分:我们取一个负值，其中上下文单词的值为 1</li><li id="64bd" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la pa mc md me bi translated">第二部分:我们取一个 u 的 exp，这是我们用隐藏层乘以第二组权重后得到的输出。</li></ol><pre class="nr ns nt nu gt ok ol om on aw oo bi"><span id="6b5c" class="mr ld iq ol b gy op oq l or os">u : [ 0.3831286   0.89608496  2.69426738 -1.60230182  0.45482701  0.73644591 1.10365796  1.1675781  -0.78555069]</span><span id="91b0" class="mr ld iq ol b gy ot oq l or os">context: [0, 1, 1, 0, 0, 0, 0, 0, 0]</span><span id="de90" class="mr ld iq ol b gy ot oq l or os">sum_1 = -(0.89608496 + 2.69426738)</span><span id="5aca" class="mr ld iq ol b gy ot oq l or os">sum_2 = number_of_context_words * np.log(np.sum(np.exp(u)))</span><span id="89d4" class="mr ld iq ol b gy ot oq l or os">total_loss = sum_1 + sum_2</span></pre><p id="5d7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nm">超参数</em> </strong></p><p id="e269" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">到目前为止，我们可以看到在这个过程中涉及到许多变量。所以训练的一个很大的部分是找到给出最佳结果的正确的变量集。我们将逐一讨论这些变量。</p><ol class=""><li id="349f" class="lu lv iq kh b ki kj kl km ko ng ks nh kw ni la pa mc md me bi translated">窗口大小:这是我们为每个目标单词准备的上下文单词的数量。</li><li id="0c77" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la pa mc md me bi translated">学习率:如果我们看到反向传播代码，我们会看到权重与学习率相乘。这是称为<a class="ae lb" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>的优化过程的一部分。一个好的学习率定义了我们的模型多快达到它的最优值。</li><li id="e561" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la pa mc md me bi translated">纪元:假设我们有 100 个训练例子。因此，当我们对每 100 个记录执行上述整个过程时，这被计为 1 个时期。</li><li id="65b6" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la pa mc md me bi translated">维度:当我们的最终模型准备好了，我们得到了每个单词的向量。该向量可以具有范围从 50 到 300 各种维数。</li><li id="5c08" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la pa mc md me bi translated">停用词:这些词像 a，an，the。它们本身没有任何意义，因此我们可以检查模型在有和没有它们的情况下是如何工作的。</li></ol><p id="feb4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在保持学习率为 0.01 的情况下，我做了一些有趣的实验，包括窗口大小、时期、尺寸、停用词</p><h2 id="bd09" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated">模型推理和分析</h2><p id="15d0" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">为了获得每个单词的单词向量，在最终训练之后，使用第一组权重，即 weight_1 来检索每个单词的向量。</p><p id="7765" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了找到相似的单词集，使用余弦相似度。这是一种度量标准，用来衡量多维空间中两个向量的相似程度。基本上，它测量两个向量之间的角度，以确定它们有多相似。很好的解释可以在<a class="ae lb" href="https://www.machinelearningplus.com/nlp/cosine-similarity/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="8b91" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我使用了两组不同的数据，一组是单行文本，另一组是文本语料库。</p><p id="2090" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nm">用单行文本作为输入进行推理</em> </strong></p><pre class="nr ns nt nu gt ok ol om on aw oo bi"><span id="f1db" class="mr ld iq ol b gy op oq l or os">Text : [‘best way to success is through hardwork and persistence’]</span></pre><p id="cce5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面显示的散点图是二维的。这是通过降维实现的。我已经为它使用了<a class="ae lb" href="https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/" rel="noopener ugc nofollow" target="_blank"> T-SNE </a>。下图并没有给出一个确切的图像，因为我们把几个维度压缩成了 2 个。</p><p id="24dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不同维度的散点图:</p><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi pb"><img src="../Images/8d03d69a0eb13f029dd1f867b59639f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3LocoLyRiRjvfW8qMSnwVA.png"/></div></div><p class="ny nz gj gh gi oa ob bd b be z dk translated">图 5:变暗</p></figure><p id="a634" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以在这里看到“最佳”和“方式”是多么接近。即使一行文字对于一个模型来说是非常稀缺的，但是它可以学习很好的嵌入。</p><p id="32e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不同窗口大小的散点图:</p><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi pb"><img src="../Images/a730a87255fa297e3c5d4ed8b974e279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DzAsQSprT82039yqBu8ShQ.png"/></div></div><p class="ny nz gj gh gi oa ob bd b be z dk translated">图 6:不同的窗口大小</p></figure><p id="e5e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nm">用相对较大的语料库进行推理</em> </strong></p><p id="5888" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了了解这个模型的效果，我打印了一个相似度矩阵，用于一些拥有更大语料库的单词</p><p id="b355" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">变化尺寸:</p><div class="nr ns nt nu gt ab cb"><figure class="pc nv pd pe pf pg ph paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><img src="../Images/90d2ade2bd9b8ea8ad78b9c73244d19c.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*TUGn3-uUlbmWNzi8pUhRNQ.png"/></div></figure><figure class="pc nv pi pe pf pg ph paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><img src="../Images/2f64292f2616a09003523839bbd7afc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*NenQodRX8MQ_7aXXXzs3UA.png"/></div><p class="ny nz gj gh gi oa ob bd b be z dk pj di pk pl translated">图 7:变维相似矩阵</p></figure></div><p id="7e0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里要注意的一件事是，对于更高的维度，相似性的数字很低。这背后的原因是语料库非常小，只有大约 700 个独特的单词。所以要学习更高维度的嵌入，需要一个庞大的语料库。word2vec 算法是在数百万规模的语料库上训练的。</p><p id="4c01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">停用词:</p><div class="nr ns nt nu gt ab cb"><figure class="pc nv pm pe pf pg ph paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><img src="../Images/6968407b8ea4fe1b32d289603e62d812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*rXSuBupiX7y4oRIkcK33uA.png"/></div></figure><figure class="pc nv pn pe pf pg ph paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><img src="../Images/5a37712c96378d8f77d327e27ed44d02.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*MlG04CLaKZWIs7ydLqrt9Q.png"/></div><p class="ny nz gj gh gi oa ob bd b be z dk po di pp pl translated">图 8:停用词效应</p></figure></div><p id="969b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型在没有停用词的情况下似乎表现得更好，因为每个时期的损失曲线都更好。为了更好地理解它，我们需要在更大的语料库上进行尝试。</p><p id="23db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的散点图可以在我的 github 链接<a class="ae lb" href="https://github.com/rahul1728jha/Word2Vec_Implementation/tree/master/output_images_bigger_data" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="eebc" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">进一步改进:</h1><p id="1c23" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">word2vec 的训练是一个计算量非常大的过程。由于有数百万字，训练可能要花很多时间。对抗这种情况的一些方法是<a class="ae lb" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" rel="noopener ugc nofollow" target="_blank">负采样</a>和分级 softmax。理解这两者的好链接可以在<a class="ae lb" rel="noopener" target="_blank" href="/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08">这里</a>找到。</p><p id="8f76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的代码可以在我的 github 资源库中找到:</p><p id="f9a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Word_2_Vec.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Rahul 1728 jha/Word 2 vec _ Implementation/blob/master/Word _ 2 _ vec . ipynb</a></p><p id="e60a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请留下任何澄清或问题的评论。</p><p id="84f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">快乐学习😃</p></div></div>    
</body>
</html>