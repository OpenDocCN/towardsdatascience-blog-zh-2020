# 解决人工智能的卡珊德拉问题

> 原文：<https://towardsdatascience.com/solving-ais-cassandra-problem-8edb1bc29876?source=collection_archive---------48----------------------->

## 使用可解释的人工智能来说服你的用户

![](img/6b3de83665fd06455f438ffdfc6d3278.png)

燃烧的特洛伊城前的卡珊德拉。伊芙琳·德·摩根(1898 年，伦敦)

人工智能社区有一个严重的问题:[我们 87%的项目从未投入生产](https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/)。如果另一个领域有这样的失败率，我们可能根本称不上科学。我们到底遗漏了什么？

我们不缺乏数据:我们在 2020 年每人每秒产生 [1.7 MB](https://hostingtribunal.com/blog/big-data-stats/#:~:text=The%20amount%20of%20data%20created,that's%2044%20trillion%20gigabytes)!)

我们并不缺乏理论上的进展:新的人工智能论文就在今天发表在 Arxiv 上

我们并不缺乏投资:预计到 2025 年投资将超过 1500 亿美元

我们不缺乏性能:人工智能计算性能每 3.4 个月翻一番

我们缺少的是信任。现代 AI/ML 技术，如深度学习和基于树的方法，可以提供很高的准确性，但也比基于回归的旧方法更难解释。我们的模型可能会非常准确地预测未来，但如果我们服务的专家将它视为一个黑匣子，它将无法走出实验室。如果最终用户不信任我们的模型，他们会忽略它，就像古希腊人忽略了卡珊德拉的准确预言一样。

[可解释的人工智能](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence) (XAI)是一套寻求让复杂模型变得可理解和可信的技术。在过去的两年里，人们的兴趣迅速增长:

![](img/2d327542967fa2ab622874c158480894.png)

来源:谷歌趋势。作者图片

你可以找到很多介绍 XAI 的文章。我在下面的脚注中加入了一些我最喜欢的。然而，他们大多关注于*如何* [而忽略了*为什么*](https://www.amazon.com/Start-Why-Leaders-Inspire-Everyone/dp/1591846447) ，而轻于现实数据和领域专业知识。

本文将通过一个案例研究来说明 XAI 是如何融入人工智能/人工智能工作流程的:一个我作为 Kaggle 家庭信贷竞赛的一部分开发的信贷模型。训练数据集包括来自潜在借款人的 397，511 份贷款申请。该模型试图预测哪些借款人将会违约。它是用基于[树的](https://blog.dataiku.com/tree-based-models-how-they-work-in-plain-english#:~:text=Tree%2Dbased%20models%20use%20a,classification%20(predicting%20categorical%20values).)算法 LightGBM 构建的。

在我们在 [Genpact](https://www.genpact.com/digital-transformation/artificial-intelligence-ai) 的咨询实践中，我们看到了三个主要的 XAI 用例。让我们逐一查看。

![](img/ebee0e99cc48226c664f163d40cb2392.png)

作者图片

# 1.开发人员:提高模型性能

![](img/3cfdb8394d7b728adc44abdc4b9e89e7.png)

照片由 [SpaceX](https://unsplash.com/@spacex?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

模型开发人员希望使用 XAI 来

消除没有增加多少价值的特征，使模型更轻，训练更快，不太可能过度拟合

确保模型依赖于从领域角度来看有意义的特性。例如，我们不希望模型使用借款人的名字来预测贷款违约

大多数基于树的库包括一个基本的特征重要性报告，可以帮助完成这些任务。这是我们的信用模型:

![](img/844f79a557e692b78b9f054e0b158a94.png)

我们应该如何阅读这个图表？

最重要的功能在最上面

特征重要性由模型树使用它的频率(通过分割)或它添加了多少信号(通过增益)来定义

它告诉我们什么？

最重要的特性是 NEW_EXT_SOURCES_MEAN。这代表借款人的信用评分，类似于美国的 FICO 评分。从技术上讲，它是外部信用机构三次评分的平均值

平均信用分数比任何个人分数都重要(EXT_SOURCE_1，2，3)

第二个最重要的特征是申请人的年龄、出生日期等。

**那又怎样？**

我们可以利用这一点来提高模型性能。例如，我们看到所有三个信用评分都很重要(EXT_SOURCE 1、2 和 3)，但平均值是最重要的。因此，模型试图学习的分数之间存在一些差异。我们可以通过工程特征来明确这种差异，例如 EXT_SOURCE_1 — EXT_SOURCE_2。

我们可以利用这一点在有限的程度上验证模型逻辑。例如，我们对借款人的年龄如此依赖会舒服吗？这很难回答，因为我们仍然不知道模型是如何使用借款人年龄的。

将此与 XAI 一个名为 [SHAP](https://github.com/slundberg/shap) 的软件包生成的类似报告进行比较:

![](img/3999bc969fb7203c50a93694a0c77600.png)

两个图表都在顶部显示了模型最“重要”的特征，尽管它们在顺序上并不完全一致。许多人认为 [SHAP 订单更准确。无论如何，SHAP 图表提供了更多的细节。](https://www.actuaries.digital/2019/06/18/analytics-snippet-feature-importance-and-the-shap-approach-to-machine-learning-models/)

**我们应该如何解读这张图表？**

让我们放大顶部的功能:

![](img/693127ae89197fec350c42e725196bc3.png)

**自上而下位置:**这是模型输出最“动针”的特征。是最有冲击力的。

**颜色:**在特性标签的右边，我们看到一个[小提琴](https://en.wikipedia.org/wiki/Violin_plot#:~:text=A%20violin%20plot%20is%20a,by%20a%20kernel%20density%20estimator.)形状的点状图。在我们的数据集中，每个点代表一个借款人。蓝点代表信用评分低(有风险)的借款人。红点是高信用分(安全)。同样的蓝红标度适用于所有的数量特征。

**左右位置:**中心线左侧的点代表其信用评分降低模型输出的借款人，即降低预测的违约概率。右边的点是那些信用评分增加模型输出的人。

它告诉我们什么？

左边的点大多是红色的，这意味着高信用分数(红色)往往会降低违约概率(左)。

**那又怎样？**

我们可以用它来调试模型。例如，如果一个信用机构报告了一个分数，其中高值是有风险的，低值是安全的，或者如果我们混淆了违约概率和还款概率，这个图表将显示问题并允许我们解决它。这种情况比人们想象的更经常发生。

让我们进一步关注最后一点。这张图表显示了信用评分如何影响违约的预测概率。

![](img/7d5f825ae97d9090910f0f6d1ef65421.png)

我们应该如何阅读这个图表？

和以前一样，每个点代表一个借款人。

横轴是信用评分，从 0 到 1。

纵轴是*而不是*违约的预测概率，而是信用评分对该概率的*贡献*。它是以对数比而不是概率单位来衡量的，与我们的逻辑模型的原始输出相同。请参阅脚注，了解将对数几率转换为概率的公式。

**它告诉我们什么？**

这个散点图从左上向右下倾斜，因为低信用评分(左)会增加违约的预测概率(上)，高信用评分(右)会降低违约概率(下)。

**那又怎样？**

如果我们的模型是线性的，那么这些点会以完美的直线向下倾斜。

相反，我们会看到一个更厚的散点图，因为我们的模型包括了特征交互。同样的信用评分(垂直线)可以对两个借款人产生不同的影响，这取决于另一个特征的价值。

让我们向图表中添加一些交互数据:

![](img/78826a1174c06f2ebecf417d956c1f8f.png)

我们应该如何阅读这个图表？

请注意，坐标轴没有改变，圆点也没有移动。

我们只添加了一个基于第二个(交互)特征的色标，即 NEW_CREDIT_TO_ANNUITY_RATIO。这是申请贷款额和每年贷款支付额之间的比率。比率越低(蓝色)，贷款被认为越安全，因为它应该用更少的时间还清。

它在告诉我们什么？

不幸的是，颜色并没有告诉我们太多:蓝色和红色的点似乎是随机分散的。这意味着我们绘制的两个特征之间没有明显的相互作用。然而，考虑下面的交互图:

![](img/e9b5435e8280ac8bf5d2c99414c2f8e1.png)

我们应该如何阅读这个图表？

色标代表与之前相同的特征，但现在我们将横轴从信用评分切换到借款人的年龄，表示为从今天算起到出生的天数，因此是负数。

年龄范围从右边的 21 岁(7500/365 = 20.5)到左边的 68 岁(25000/365 = 68.5)。

纵轴显示了借款人年龄对预测违约概率的*影响*，以对数比表示。

这张图表告诉我们什么？

总体情况是，年长的借款人通常比年轻的借款人更安全(低)。这种情况一直持续到你 65 岁左右(-23000 天)。过了那个年龄，就有很大的分散。老年人信贷质量到处都是，但新的信贷与年金比率起着很大的作用，越低越好。

更微妙的是:有趣的事情发生在 45 岁左右(-16250 天)。对于 45 岁以下的借款人(右图)，蓝点一般高于红点。在信贷方面，要求短期贷款的年轻借款人(低信贷对年度付款=蓝色)风险更大。对于年纪较大的借款人(左)，这种关系正好相反，长期贷款的风险越来越大。

**那又怎样？**

我不确定为什么家庭信用数据显示出这种模式。这可能是贷款政策、产品特性的结果，也可能是偶然的。如果这是一个真实的信用模型，我会和一个领域专家(在这种情况下是一个承保人)讨论这个模式。

如果我们确定该模式基于信用基本面，我会设计一些特征，以便模型可以利用它。例如，一个这样的特征可能是 abs(年龄-45 岁)。

# 2.最终用户:了解产出并建立信任

![](img/97bbeb57749ca0013633c0490f133d38.png)

Joshua Hoehne 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

如前所述，ML 模型没有走出实验室进入生产的最大原因之一是最终用户不信任它们。假设我们的最终用户，一个信贷员，正在对一个借款人(数据集中的借款人#4)运行我们的模型。该模型预测该借款人违约的可能性为 94.9%。非常冒险。在拒绝申请之前，信贷员想知道为什么模型如此不喜欢这个借款人。使用 XAI，我们可以将预测分解成它的组成部分。有几种方法可以将这些组件可视化。我最喜欢的是决策情节，也来自 SHAP 包:

![](img/e2b28c5ce414885df545eaa1506e9a1f.png)

**我们应该如何解读这张图表？**

在该图中，我们显示了前 20 个最重要的特征对借款人#4 的模型预测的影响。

预测是红线。模型的输出是线条与图表顶部的色带相交的地方，94.9%的可能性是默认的。

将借款人标记为风险高于平均水平的特征会将线向右移动。将借款人标记为比平均水平更安全的特征将线向左移动。

借款人#4 的特征值在线旁边显示为灰色。

尽管从图表中可能很难看出，但并非所有的特征都以相同的程度移动线条。顶部的功能使它更加动感。

它告诉我们什么？

借款人的信用评分在我们这一批人中处于平均水平，但是

关于这个借款人的几乎所有其他事情都是一个危险信号。这不是一件事:这是一切。

**那又怎样？**

这应该给信贷员一些信心的预测。即使模型在一两个特征上是错误的，结论也不会有太大变化。

如果这是一个生产模型，功能标签将更加用户友好，贷款官员将能够点击它们以获得附加信息，例如借款人#4 在任何给定功能上与其他人相比如何，或者前面显示的功能依赖图。

唯一的问题是，这个图没有显示我们的完整模型，只有前 20 个特征。下面是基本数据:

![](img/8da2752fd8cac93d8e9d9f4b3068bdbd.png)

我们应该如何阅读这张图表？

每行代表一个模型特征，总共 704 个。

中间一列显示了借款人#4 的特性值。

右栏显示了特征的*对模型预测借款人#4 的贡献*，以对数优势单位表示。数据按该列的绝对值排序。这将最重要的功能放在最上面，就像以前的情节一样。

如果我们将右边的列加到模型对所有借款人的平均预测上，我们得到模型对借款人#4 的预测。数学是这样的:

![](img/0850ff8629c76a636db96210323acee6.png)

**它告诉我们什么？**

从右栏往下看，这个借款人的信用评分很重要，但几乎所有其他特征都对风险标签有所贡献。很多风险因素，没有缓解措施。

看看列表的底部，我们的 704 个特征中的 336 个没有贡献，这意味着它们根本不影响我们的模型对借款人#4 的预测。

**那又怎样？**

如果这是所有借款人的共同模式(不仅仅是借款人#4)，我们会希望从模型中删除这些零影响特征。

# 3.审计师:管理风险、公平和偏见

![](img/993df6c3c0e3cd9ba93d9eba368b19d1.png)

[廷杰伤害律师事务所](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

在模型投入生产之前，通常需要由一个单独的团队进行验证。组织用不同的名字称呼这些团队:模型验证、模型风险、人工智能治理或类似的东西。为简单起见，我们称他们为审计员。

审计师不关心上述任何一个预测。他们关心模型的整体。例如，他们可能想确认我们的(候选)模型对受保护群体没有偏见，这将违反《平等信贷机会法》(如果这一模型服务于美国借款人)。以性别为例:

![](img/9b3d1feba39706cbfdeb736dd238c87f.png)

我们的数据集包括的女性和男性一样多，显示女性违约的频率更低。我们希望确保我们的模型不会因为预测女性比男性更有可能违约而不公平地对待女性。这很容易做到:我们简单地比较男性和女性的预测，并使用 t 检验来看差异是否显著。

![](img/cd8a5867cd0b001b11e9f53688d923cd.png)

该图表显示，该模型倾向于预测女性违约的频率更低，并且组间差异显著。这与上面总结的我们的训练数据是一致的。我们可以更进一步，分别测试每个特性:

![](img/bd98286f200b49ef06d463e467f026e6.png)

我们应该如何阅读这个图表？

我们再一次看看前 10 个最重要的特性。

延伸到垂直线左侧的特征“偏爱”女性，延伸到垂直线右侧的特征“偏爱”男性。

该图还显示了置信区间。

这张图表告诉我们什么？

大多数特征有利于女性，与潜在分布一致。这很好，因为我们的模型没有扭曲事实。

一些特征仍然有利于男性，例如每年的贷款支付和收入之间的比率。这可能是因为我们数据集中的男性收入更高。

我们根据样本制作了这张图表。如果我们使用更大的样本，我们可能会得到更好(更紧)的置信区间。

**那又怎样？**

在这一点上，审核员将考虑这些证据来决定一个模型是否应该进入生产阶段。不同的贷方会以不同的方式对证据做出反应。有些人可能会剔除有利于男性的特征，尽管这可能会降低模型的准确性。其他人将接受所有特征，只要它们不超过某个显著性阈值。还有一些人可能只关注总体预测。

同样的分析将对其他受保护的群体重复进行——按种族、年龄、性取向、残疾状况等。

# 结论

![](img/9232a955f6e4ee7489d61eda3333af99.png)

照片由[乔舒亚·内斯](https://unsplash.com/@theexplorerdad?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 拍摄

可解释人工智能(XAI)是人工智能中最令人兴奋的发展领域之一，因为它可以促进建模者和领域专家之间的对话，否则这种对话不会发生。这些对话非常有价值，因为它们让技术和业务利益相关者达成了共识。一起，我们可以确保模型“出于正确的原因是正确的”，甚至可能解决我们的卡珊德拉问题。

# 脚注:

我们只是触及了 XAI 的皮毛。我希望我激起了你的食欲。以下是一些附加资源

[XAI 概述视频](https://www.youtube.com/watch?v=rPSiEDYcXr4&feature=emb_logo)

[克里斯·莫尔纳尔(Chris Molnar)的可解释 ML 图书](https://christophm.github.io/interpretable-ml-book/)

为 XAI 设计用户体验(UX)，来自 [UXAI](https://www.uxai.design/design-strategy) 和[微软](https://www.microsoft.com/en-us/research/project/guidelines-for-human-ai-interaction/)

[XAI 方法的分类](https://www.researchgate.net/publication/338184751_Explainable_Artificial_Intelligence_XAI_Concepts_Taxonomies_Opportunities_and_Challenges_toward_Responsible_AI)

[来自谷歌的 XAI 产品](https://cloud.google.com/explainable-ai)

[确保信用违约风险建模的公平性和可解释性](https://medium.com/@tonyxu_71807/ensuring-fairness-and-explainability-in-credit-default-risk-modeling-shap-ibm-tool-kit-aix-360-bfc519c191bf)

下面是将对数赔率转换为概率的公式:

![](img/4f3740be3a31f3adec80a0a25c3c7c23.png)