<html>
<head>
<title>Monte Carlo Markov Chain</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">蒙特卡罗马尔可夫链</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/monte-carlo-markov-chain-89cb7e844c75?source=collection_archive---------16-----------------------#2020-08-24">https://towardsdatascience.com/monte-carlo-markov-chain-89cb7e844c75?source=collection_archive---------16-----------------------#2020-08-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/4106ef0500a2dc07fa11fb585e3da2d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OBUri8ukZ1zaxy0C"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">希瑟·吉尔在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><p id="ce5f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个<strong class="ki jk">蒙特卡罗马尔可夫链</strong> ( <strong class="ki jk"> MCMC </strong>)是一个描述一系列可能事件的模型，其中每个事件的概率只取决于前一个事件达到的状态。MCMC 有广泛的应用，其中最常见的是概率分布的近似。</p><p id="c9d6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个蒙特卡罗马尔可夫链的例子。假设我们想确定晴天和雨天的概率。</p><p id="f360" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到了以下条件概率:</p><ul class=""><li id="3bb6" class="le lf jj ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">如果今天下雨，明天有 50%的可能是晴天。</li><li id="ad06" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">如果今天下雨，明天有 50%的可能会下雨。</li><li id="333d" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">如果今天是晴天，明天有 90%的可能是晴天。</li><li id="e7f3" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">如果今天是晴天，明天有 10%的可能会下雨。</li></ul><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/759a27bfdcfdaba4d4d02d91643bdfbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*ZfuYDvHL9A90xeF8m3BZeg.png"/></div></figure><p id="d63e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们从阳光充足的州开始。然后我们进行蒙特卡罗模拟。也就是说，我们生成一个 0 到 1 之间的随机数，如果刚好在 0.9 以下，明天就是晴天，否则就是阴雨。我们又做了一次蒙特卡洛模拟，这一次，明天会下雨。我们重复该过程进行<strong class="ki jk"> <em class="lx"> n </em> </strong>次迭代。</p><p id="59c0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的序列被称为马尔可夫链。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ly"><img src="../Images/63d54ef7b874a8f527b04bea6f4d9f2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ubs1u9HLzSdJGS5Sm88nyQ.png"/></div></div></figure><p id="37f9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们数出晴天的天数，除以总天数，来确定天气晴朗的概率。如果马尔可夫链足够长，即使初始状态可能不同，我们也会得到相同的边际概率。这种情况下，天晴的概率是 83.3%，下雨的概率是 16.7%。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/15b7c0bdde340df6380ccad7cde1b04d.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*heVPk_wXMwWydFautTfq9A.png"/></div></figure><p id="5369" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看如何用 Python 实现蒙特卡罗马尔可夫链。</p><p id="703d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们从导入以下库开始:</p><pre class="lt lu lv lw gt ma mb mc md aw me bi"><span id="eac3" class="mf mg jj mb b gy mh mi l mj mk">import numpy as np<br/>from matplotlib import pyplot as plt<br/>import seaborn as sns<br/>sns.set()</span></pre><p id="017a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用状态机和相应的矩阵来表达上一个例子中的条件概率。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ml"><img src="../Images/24e6f9909223219335ebb8b037117a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C9-Ff2LXkNs9u6jEJ8gYvw.png"/></div></div></figure><p id="4769" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们做一些线性代数。</p><pre class="lt lu lv lw gt ma mb mc md aw me bi"><span id="65de" class="mf mg jj mb b gy mh mi l mj mk">T = np.array([[0.9, 0.1],[0.5, 0.5]])</span><span id="da56" class="mf mg jj mb b gy mm mi l mj mk">p = np.random.uniform(low=0, high=1, size=2)<br/>p = p/np.sum(p)<br/>q=np.zeros((100,2))</span><span id="9d0e" class="mf mg jj mb b gy mm mi l mj mk">for i in np.arange(0,100):<br/>    q[i, :] = np.dot(p,np.linalg.matrix_power(T, i))</span></pre><p id="f74a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们绘制结果。</p><pre class="lt lu lv lw gt ma mb mc md aw me bi"><span id="9a10" class="mf mg jj mb b gy mh mi l mj mk">plt.plot(q)<br/>plt.xlabel('i')<br/>plt.legend(('S', 'R'))</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/2f3d20963cb17e139a540324e89763bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*939bMBB0eu2WtvohJ-brdA.png"/></div></figure><p id="3edc" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所见，天气晴朗的概率约为 0.833。同样，多雨的概率也趋向于 0.167。</p><h1 id="5be1" class="mo mg jj bd mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk bi translated">贝叶斯公式</h1><p id="f0a6" class="pw-post-body-paragraph kg kh jj ki b kj nl kl km kn nm kp kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">很多时候，我们想知道某个事件发生的概率，假设另一个事件已经发生。这可以象征性地表示为<strong class="ki jk"> <em class="lx"> p(B|A) </em> </strong>。如果两个事件不是独立的，那么这两个事件发生的概率由下面的公式表示。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/160c71fd85705d861acfa19857c33a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*o4j3kfvNIXmZQ74rWyQfVA.png"/></div></figure><p id="821b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，假设我们从一副标准的 52 张牌中抽取两张牌。这副牌中一半是红色的，一半是黑色的。这些事件不是独立的，因为第二次抽签的概率取决于第一次。</p><p id="0b16" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">P(A) = P(第一次抽黑牌)= 25/52 = 0.5</p><p id="4e1e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">P(B|A) = P(第二次抽黑牌|第一次抽黑牌)= 25/51 = 0.49</p><p id="59f2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用这些信息，我们可以计算连续抽两张黑牌的概率，如下所示:</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/a30b3ad2a7f684dc1503f8a6cf75fdaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Pp-R3GevgBryMPMXYWJXA.png"/></div></div></figure><p id="62b8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们假设，我们想要开发一个垃圾邮件过滤器，它将根据某个单词的出现与否来将一封电子邮件分类为垃圾邮件。例如，如果一封电子邮件包含单词<strong class="ki jk"> <em class="lx">【伟哥】</em> </strong>，我们将其归类为垃圾邮件。另一方面，如果一封电子邮件包含单词 money，那么它有 80%的可能是垃圾邮件。</p><p id="f809" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据贝叶斯定理，给定包含给定单词的电子邮件是垃圾邮件的概率为:</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e81f79a3f3895221ca3725cd68971962.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*hTEon51za2mRsEI5Y1eGyw.png"/></div></figure><p id="a81a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可能知道一封电子邮件是垃圾邮件的概率，以及一个单词包含在被分类为垃圾邮件的电子邮件中的概率。<strong class="ki jk">然而，我们不知道给定的单词在电子邮件中出现的概率。这就是 Metropolis-Hastings 算法发挥作用的地方。</strong></p><h1 id="9bb4" class="mo mg jj bd mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk bi translated">大都会黑斯廷斯算法</h1><p id="6ec3" class="pw-post-body-paragraph kg kh jj ki b kj nl kl km kn nm kp kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">Metropolis-Hasting 算法使我们能够在不知道归一化常数的情况下确定后验概率。在高层次上，Metropolis-Hasting 算法的工作方式如下:</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/3af1362d17626f9cac599c4be0e68034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DbDrJttuEaaKd5FgRo1u2g.png"/></div></div></figure><p id="3db8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">验收标准只考虑目标分布的比率，因此分母抵消。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/cd6b0fadd0c657528aaf68dfb8a43344.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*shGxyulW4ddlmp7-YPkSrw.png"/></div></figure><p id="0b42" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于视觉学习者来说，让我们用一个例子来说明这个算法是如何工作的。</p><p id="fafd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先为θ选择一个随机的初始值。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/e01b66baf4dc903721bf04ca0d5a2b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zRCFEMy9KmDjv7Nc-eZw2w.png"/></div></div></figure><p id="4cbd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们提出一个新的θ值。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/51541cab753e84dd96be91575b90d92b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IRKSiADWcGOknsH4o91C5g.png"/></div></div></figure><p id="ddf6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们计算当前θ值的 PDF 和建议θ值的 PDF 之比。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/c5e09d02a7dbfaa535430f2f744bcb17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHIyt-dqa910JYFprJs-Cg.png"/></div></div></figure><p id="8a82" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果 rho 小于 1，那么我们以概率<strong class="ki jk"> <em class="lx"> p </em> </strong> <em class="lx">将 theta 设置为新值。</em>我们通过将 rho 与从均匀分布中抽取的样本<strong class="ki jk"><em class="lx"/></strong>进行比较。如果 rho 大于<strong class="ki jk"> <em class="lx"> u </em> </strong>则我们接受建议值，否则，我们拒绝它。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/103860139ba661e4bc28a886e51e311c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0UuiitvKB1hOQNJFzWWxmg.png"/></div></div></figure><p id="23e6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们尝试不同的θ值。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/08e1d57b4518e92e554c6d68a43fe2b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sFOr7Tv10DIN1qUr84T7SQ.png"/></div></div></figure><p id="c82d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果ρ大于 1，它将总是大于或等于从均匀分布中抽取的样本。因此，我们接受θ新值的建议。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/8d907bd3d528b17ac861eec0e1c2f160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IMrtuzMIMQJV_0sbNLTC5g.png"/></div></div></figure><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/251b5aa35d39c1763e5780eeda2d240c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J_4xMihPpjs5yoDXhL58Gg.png"/></div></div></figure><p id="4c94" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们重复这个过程<strong class="ki jk"> <em class="lx"> n 次</em> </strong>次迭代。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/f50cd5972515ba2f663425206786912a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*afI6nSqDDfxFPiA3F0OOGA.png"/></div></div></figure><p id="0e88" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为当目标分布大于当前位置时，我们自动接受提议的移动，所以θ将倾向于在目标分布更密集的地方。然而，如果我们只接受比当前位置大的值，我们会卡在其中一个峰值上。因此，我们偶尔会接受向低密度区域转移。这样，θ将被期望以这样的方式反弹，以接近后验分布的密度。</p><p id="76e3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些步骤实际上是一个马尔可夫链。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/172cff689ec3b39fd7f349a7f0cba731.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*isomi-P0kYXcc0jZJLOCfw.png"/></div></div></figure><p id="8c58" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一下如何用 Python 实现 Metropolis-Hasting 算法，但是首先，这里有一个不同类型的发行版的快速复习。</p><h2 id="bcb8" class="mf mg jj bd mp oe of dn mt og oh dp mx kr oi oj nb kv ok ol nf kz om on nj oo bi translated">正态分布</h2><p id="49b4" class="pw-post-body-paragraph kg kh jj ki b kj nl kl km kn nm kp kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">在自然界中，随机现象(如智商、身高)往往遵循正态分布。正态分布有两个参数μ和适马。改变μ会移动钟形曲线，而改变适马会改变钟形曲线的宽度。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi op"><img src="../Images/9c192394f33bea37d3f64616b8301188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rPhJcfII6aaG2nJp.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Normal_Distribution_PDF.svg" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:Normal _ Distribution _ pdf . SVG</a></p></figure><h2 id="d3fe" class="mf mg jj bd mp oe of dn mt og oh dp mx kr oi oj nb kv ok ol nf kz om on nj oo bi translated">贝塔分布</h2><p id="8c83" class="pw-post-body-paragraph kg kh jj ki b kj nl kl km kn nm kp kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">像正态分布一样，贝塔分布有两个参数。然而，与正态分布不同，β分布的形状会根据其参数α和β的值而显著变化。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/88f3ece31500db275a2ee3cf7858dd42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NMn1fh3abqHqbCvO.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Beta_distribution_pdf.svg" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:Beta _ distribution _ pdf . SVG</a></p></figure><h2 id="d922" class="mf mg jj bd mp oe of dn mt og oh dp mx kr oi oj nb kv ok ol nf kz om on nj oo bi translated">二项分布</h2><p id="6b4d" class="pw-post-body-paragraph kg kh jj ki b kj nl kl km kn nm kp kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">与以高度为定义域的正态分布不同，二项式分布的定义域始终是离散事件的数量。</p><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/5edeef1057fad442a561a93af369e37e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1wgqlDJZfgbZVI4H.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Binomial_distribution_pmf_sl.svg" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:Binomial _ distribution _ PMF _ sl . SVG</a></p></figure><p id="b7c8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经熟悉了这些概念，我们准备深入研究代码。我们从初始化超参数开始。</p><pre class="lt lu lv lw gt ma mb mc md aw me bi"><span id="df11" class="mf mg jj mb b gy mh mi l mj mk">n = 100<br/>h = 59<br/>a = 10<br/>b = 10<br/>sigma = 0.3<br/>theta = 0.1<br/>niters = 10000<br/>thetas = np.linspace(0, 1, 200)<br/>samples = np.zeros(niters+1)<br/>samples[0] = theta</span></pre><p id="4f6a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们定义一个函数，对于给定的θ值，该函数将返回似然和先验的乘积。</p><pre class="lt lu lv lw gt ma mb mc md aw me bi"><span id="66a6" class="mf mg jj mb b gy mh mi l mj mk">def prob(theta):<br/>    if theta &lt; 0 or theta &gt; 1:<br/>        return 0<br/>    else:<br/>        prior = st.beta(a, b).pdf(theta)<br/>        likelihood = st.binom(n, theta).pmf(h)<br/>        return likelihood * prior</span></pre><p id="3daf" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们逐步执行算法，根据前面描述的条件更新θ值。</p><pre class="lt lu lv lw gt ma mb mc md aw me bi"><span id="9676" class="mf mg jj mb b gy mh mi l mj mk">for i in range(niters):<br/>    theta_p = theta + st.norm(0, sigma).rvs()<br/>    rho = min(1, prob(theta_p) / prob(theta))<br/>    u = np.random.uniform()<br/>    if u &lt; rho:<br/>        # Accept proposal<br/>        theta = theta_p<br/>    else:<br/>        # Reject proposal<br/>        pass<br/>    samples[i+1] = theta</span></pre><p id="f916" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们定义了可能性，以及先验和后验概率分布。</p><pre class="lt lu lv lw gt ma mb mc md aw me bi"><span id="6b9f" class="mf mg jj mb b gy mh mi l mj mk">prior = st.beta(a, b).pdf(thetas)<br/>post = st.beta(h+a, n-h+b).pdf(thetas)<br/>likelihood = st.binom(n, thetas).pmf(h)</span></pre><p id="552a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用 Metropolis-Hastings 算法获得的后验分布可视化。</p><pre class="lt lu lv lw gt ma mb mc md aw me bi"><span id="8e10" class="mf mg jj mb b gy mh mi l mj mk">plt.figure(figsize=(12, 9))<br/>plt.hist(samples[len(samples)//2:], 40, histtype='step', normed=True, linewidth=1, label='Predicted Posterior');<br/>plt.plot(thetas, n*likelihood, label='Likelihood', c='green')<br/>plt.plot(thetas, prior, label='Prior', c='blue')<br/>plt.plot(thetas, post, c='red', linestyle='--', alpha=0.5, label='True Posterior')<br/>plt.xlim([0,1]);<br/>plt.legend(loc='best');</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/53e02dac2391d13b2d76fffd2e72c128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1keBvvIlsCAzKViB-O5wvA.png"/></div></div></figure><p id="5706" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，Metropolis Hasting 方法在逼近实际后验分布方面做得很好。</p><h1 id="2d4f" class="mo mg jj bd mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk bi translated">结论</h1><p id="2770" class="pw-post-body-paragraph kg kh jj ki b kj nl kl km kn nm kp kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">蒙特卡洛马尔可夫链是从一组概率分布中提取的一系列事件，这些概率分布可以用来近似另一个分布。Metropolis-Hasting 算法在我们知道似然和先验，但不知道归一化常数的情况下，利用蒙特卡罗马尔可夫链来近似后验分布。</p></div></div>    
</body>
</html>