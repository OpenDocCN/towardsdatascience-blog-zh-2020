<html>
<head>
<title>Day 125 of #NLP365: NLP Papers Summary — A2N: Attending to Neighbors for Knowledge Graph Inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365的第125天:NLP论文摘要— A2N:关注知识图推理的邻居</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-125-of-nlp365-nlp-papers-summary-a2n-attending-to-neighbors-for-knowledge-graph-inference-87305c3aebe2?source=collection_archive---------73-----------------------#2020-05-04">https://towardsdatascience.com/day-125-of-nlp365-nlp-papers-summary-a2n-attending-to-neighbors-for-knowledge-graph-inference-87305c3aebe2?source=collection_archive---------73-----------------------#2020-05-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="dd51" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="3d5f" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">NLP论文摘要是我总结NLP研究论文要点的系列文章</h2></div><p id="d61d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我在2020年每天记录我的NLP学习旅程的地方。在这里，你可以随意查看我在过去的270天里学到了什么。在本文的最后，你可以找到以前的论文摘要，按自然语言处理领域分类:)</p><p id="633d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的NLP论文是<strong class="lf jp"> <em class="lz"> A2N:参加邻居进行知识图推理</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="c37d" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="7f54" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">提出了一种新的基于注意力的方法A2N来处理知识图(KG)完成任务，该方法结合实体的相关图邻域来计算依赖于查询的实体嵌入。所提出的方法在两个评估数据集上表现出竞争性或优于当前的SOTA模型，并且通过定性探测，我们能够探索模型如何围绕知识图跳跃以导出其最终推理。</p><h1 id="6a7f" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">KG完成任务</h1><p id="88ca" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">KG完成的任务包括从KG中填充和推断缺失的实体关系。这通常被公式化为目标实体预测任务，由此给定源实体和关系，目标实体是什么？因此，给定一个KG，它由(s，r，t)的许多元组组成，其中s是源实体，r是关系，t是目标实体，我们的目标是在给定s和r的情况下预测目标实体，使得预测的元组在图中不存在。</p><p id="93c5" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">大多数基于嵌入的KG完成方法涉及为KG中的每个元组定义一个评分函数。评分函数可以不同，但它接受源实体、关系和目标实体的嵌入。在本文中，我们使用DistMult评分函数。</p><h1 id="1e96" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">A2N模型</h1><p id="7cf7" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们提出的A2N接受查询，并使用对实体的图邻域的双线性关注来生成依赖于查询的实体嵌入。这种特殊的嵌入随后被用于为查询的目标实体评分。下图展示了在给定两个不同查询的情况下，模型如何对同一节点的相邻节点进行不同评分的示例。</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nj"><img src="../Images/bfbbf4eb6949e9065cc1be2b6cf27d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DQt4UXaJK4HSeq_C.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">A2N模型如何生成答案[1]</p></figure><p id="7609" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">以下是A2N中每个步骤的分解:</p><ol class=""><li id="5ded" class="ns nt jf lf b lg lh lj lk lm nu lq nv lu nw ly nx ny nz oa bi translated">每个图形实体有一个初始嵌入\(\tilde{e}⁰\，每个关系r有一个嵌入</li><li id="9350" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly nx ny nz oa bi translated">给定实体和关系的嵌入，我们现在可以将相邻的实体和关系编码到嵌入中。实体s的邻居(\(\tilde{n}_i\))的嵌入通过a)连接初始实体嵌入和关系嵌入以及b)对其应用线性变换来计算</li><li id="b821" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly nx ny nz oa bi translated">该模型使用评分函数计算每个相邻嵌入的关注度得分(a_i ),并将其归一化以获得概率(p_i)</li><li id="a9e1" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly nx ny nz oa bi translated">步骤3给出了每个相邻嵌入在回答查询时的相关程度的概率。我们聚集这些加权的相邻嵌入来生成实体s的查询相关嵌入，\(\hat{s}\)</li><li id="73c6" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly nx ny nz oa bi translated">最后，我们将依赖于查询的嵌入与初始的源嵌入连接起来，以创建最终的源嵌入\(\tilde{s}\)</li></ol><p id="b712" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在我们已经获得了最终的源嵌入，我们可以使用最终的源嵌入、关系嵌入和评分函数来对KG中所有可能的目标实体进行评分。这将为我们提供特定查询的潜在实体的排序列表。</p><h1 id="e83e" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">实验设置和结果</h1><p id="4330" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">有两个KG完井评价数据集:FB15k-237和WN18RR。评估度量是正确实体的平均倒数排名(MRR)和Hits@N，Hits @ N衡量前N个预测的准确性。</p><h2 id="8df1" class="og mi jf bd mj oh oi dn mn oj ok dp mr lm ol om mt lq on oo mv lu op oq mx jl bi translated">结果</h2><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi or"><img src="../Images/5d19a2bc645b68838af90b4d2bdc9f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eQk4cO0CkNRNI9I0.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">FB15k-237和WN18RR数据集的总体结果[1]</p></figure><p id="2860" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于仅针对目标的预测(表1)，我们的A2N模型在两个数据集的所有评估指标上都显著优于之前的SOTA性能。对于源和目标预测(表2)，我们得到了混合的结果。除了Hits@10，A2N模型在所有指标上都优于WN18RR数据集中的所有模型。然而，在FB15k-237数据集上，我们的模型表现不如ConvE，然而，它仍然实现了接近SOTA的竞争性能。</p><p id="2787" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如上图所示，该模型能够根据查询加入同一实体的不同相邻节点，并执行多跳推理。例如，使用相邻的“places _ lived”，实体被映射到相关的嵌入子空间中，并且使用评分函数和关系“nationality”，我们能够为目标实体US获得高分，这是我们的模型的最终预测。给定这个例子，我们有一个两跳推理，首先是关于居住的地方，然后是关于这些地方的国家。更多示例见下图。</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi os"><img src="../Images/bf2f7ddef97ab121dd685647de8cc043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TvRROhKuSOWLSo-j.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">查询和顶级预测的示例[1]</p></figure><h1 id="acf4" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">结论和未来工作</h1><p id="3005" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">所提出的A2N模型是可解释的，并且其大小不依赖于实体邻域的数量。潜在的未来工作可能涉及应用这些方法来关注除了图之外的实体的文本提及，以联合推理文本和知识图。</p><h2 id="ab54" class="og mi jf bd mj oh oi dn mn oj ok dp mr lm ol om mt lq on oo mv lu op oq mx jl bi translated">来源:</h2><p id="066d" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">[1]班萨尔，t .，胡安，D.C .，拉维，s .和麦卡勒姆，a .，2019年7月。A2N:关注邻居进行知识图推理。在<em class="lz">计算语言学协会第57届年会的会议录</em>(第4387–4392页)。</p><p id="a428" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于2020年5月4日</em><a class="ae jc" href="https://ryanong.co.uk/2020/05/04/day-125-nlp-papers-summary-a2n-attending-to-neighbors-for-knowledge-graph-inference/" rel="noopener ugc nofollow" target="_blank"><em class="lz">【https://ryanong.co.uk】</em></a><em class="lz">。</em></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="0bba" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="f57c" class="ns nt jf lf b lg mz lj na lm ot lq ou lu ov ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="200d" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sense-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="dd8e" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitive-analysis-f 24 a2 EC 1 ca 32</a></li><li id="9fa3" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="470d" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towards data science . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li><li id="d857" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towardsdatascience . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li><li id="2264" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5">https://towards data science . com/day-112-of-NLP 365-NLP-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b 7 a5 e 245 b5</a></li><li id="ceb0" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-123-of-nlp365-nlp-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f998d1131">https://towardsdatascience . com/day-123-of-NLP 365-NLP-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f 998d 1131</a></li></ul><h1 id="6dad" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">总结</h1><ul class=""><li id="a791" class="ns nt jf lf b lg mz lj na lm ot lq ou lu ov ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="e1c1" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1 b 27</a></li><li id="a37e" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-113-of-nlp365-nlp-papers-summary-on-extractive-and-abstractive-neural-document-87168b7e90bc">https://towards data science . com/day-113-of-NLP 365-NLP-papers-summary-on-extractive-and-abstract-neural-document-87168 b 7 e 90 BC</a></li><li id="8084" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-116-of-nlp365-nlp-papers-summary-data-driven-summarization-of-scientific-articles-3fba016c733b">https://towards data science . com/day-116-of-NLP 365-NLP-papers-summary-data-driven-summary-of-scientific-articles-3 FBA 016 c 733 b</a></li><li id="f547" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-117-of-nlp365-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge-61ae6cdf32f">https://towards data science . com/day-117-of-NLP 365-NLP-papers-summary-abstract-text-summary-a-low-resource-challenge-61a E6 CDF 32 f</a></li><li id="415e" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-118-of-nlp365-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-aea118a5eb3f">https://towards data science . com/day-118-of-NLP 365-NLP-papers-summary-extractive-summary-of-long-documents-by-combining-AEA 118 a5 eb3f</a></li><li id="f576" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-120-of-nlp365-nlp-papers-summary-a-simple-theoretical-model-of-importance-for-summarization-843ddbbcb9b">https://towards data science . com/day-120-of-NLP 365-NLP-papers-summary-a-simple-theory-model-of-importance-for-summary-843 ddbcb 9b</a></li><li id="b5ae" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-121-of-nlp365-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization-cd55e577f6de">https://towards data science . com/day-121-of-NLP 365-NLP-papers-summary-concept-pointer-network-for-abstract-summary-CD 55 e 577 f 6 de</a></li><li id="f900" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-124-nlp-papers-summary-tldr-extreme-summarization-of-scientific-documents-106cd915f9a3">https://towards data science . com/day-124-NLP-papers-summary-tldr-extreme-summary-of-scientific-documents-106 CD 915 F9 a 3</a></li></ul><h1 id="4c1c" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">其他人</h1><ul class=""><li id="e958" class="ns nt jf lf b lg mz lj na lm ot lq ou lu ov ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li><li id="28b5" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-111-of-nlp365-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection-bff7f5f20ce5">https://towards data science . com/day-111-of-NLP 365-NLP-papers-summary-the-risk-of-race-of-bias-in-hate-speech-detection-BFF 7 F5 f 20 ce 5</a></li><li id="d463" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-115-of-nlp365-nlp-papers-summary-scibert-a-pretrained-language-model-for-scientific-text-185785598e33">https://towards data science . com/day-115-of-NLP 365-NLP-papers-summary-scibert-a-pre trained-language-model-for-scientific-text-185785598 e33</a></li><li id="8bb4" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-119-nlp-papers-summary-an-argument-annotated-corpus-of-scientific-publications-d7b9e2ea1097">https://towards data science . com/day-119-NLP-papers-summary-an-argument-annoted-corpus-of-scientific-publications-d 7 b 9 e 2e ea 1097</a></li><li id="14ae" class="ns nt jf lf b lg ob lj oc lm od lq oe lu of ly ow ny nz oa bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-122-of-nlp365-nlp-papers-summary-applying-bert-to-document-retrieval-with-birch-766eaeac17ab">https://towards data science . com/day-122-of-NLP 365-NLP-papers-summary-applying-Bert-to-document-retrieval-with-birch-766 EAC 17 ab</a></li></ul></div></div>    
</body>
</html>