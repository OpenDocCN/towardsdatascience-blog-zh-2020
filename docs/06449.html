<html>
<head>
<title>Determining a Quote’s Source Using Scikit-Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Scikit-Learn确定报价来源</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/determining-a-quotes-source-using-scikit-learn-9d3e2af894f4?source=collection_archive---------58-----------------------#2020-05-22">https://towardsdatascience.com/determining-a-quotes-source-using-scikit-learn-9d3e2af894f4?source=collection_archive---------58-----------------------#2020-05-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dd8c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用逻辑回归和朴素贝叶斯进行整体平均，以确定一条消息更有可能是伯尼·桑德斯或唐纳德·川普发的。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ac665bad2971df7020586257a488ebe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5g2PvK9w808IWgEXq_4DPA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用于文本分类的机器学习管道。图片作者。</p></figure><p id="3fbb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人们可以利用视觉和听觉等感官来学习。机器学习遵循类似的概念，但有一个关键的区别。电脑不能像我们一样看或听。它只懂一件事——数字。因此，在机器能够学习之前，它首先需要将人类能够感知的信息转换为数字。数字图像以像素的形式存储数字信息就是一个例子。</p><p id="6f30" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文<strong class="kx ir">中，我们将开发一个机器学习模型，它可以接收打字信息，并确定伯尼·桑德斯或唐纳德·川普更有可能发微博</strong>。在我的<a class="ae lr" href="https://github.com/Shawn-Chahal/who-said-it" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上可以找到包含数据集、Python代码和将要讨论的Jupyter笔记本的存储库。现在让我们开始吧！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="5a38" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">最小化数据集偏差</h1><p id="8f72" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">为了最大限度地减少偏见，我们需要根据我们两位政治家在类似情况下使用的文本来训练我们的模型。例如，如果我们只从一个政治家对国家悲剧的回应中提取文本，从另一个政治家对积极事件的回应中提取文本，那么我们的模型将学习区分积极和消极的文本，而不一定是他们的写作风格。因此，我们将在自2015年<em class="mw">(即</em>，他们为2016年美国总统大选发起竞选活动)以来的两位政治家的推特上训练模型。我们将不包括转发，因为尽管它们可能反映了政治家的社会经济观点，但它们不一定反映他们的语言模式。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="e052" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">特征抽出</h1><h2 id="a4ee" class="mx ma iq bd mb my mz dn mf na nb dp mj le nc nd ml li ne nf mn lm ng nh mp ni bi translated">清理推文</h2><p id="d817" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">推文可以包含指向其他网站、图像或视频的链接，这些链接将带有自己的URL，包括诸如(https://…)之类的术语。标签(#)和提及(@)也包含一些符号，这些符号不一定是这些政客文章的一部分，但却是使用某些twitter功能的必需品。因此，我们将删除这些项目和任何其他非字母数字字符，并将剩下的内容转换为小写。本质上，<strong class="kx ir">我们将只剩下包含小写字母、数字和空格的文本</strong>。例如:</p><ul class=""><li id="aa49" class="nj nk iq kx b ky kz lb lc le nl li nm lm nn lq no np nq nr bi translated">她打算买电视机。它以500美元出售。</li></ul><p id="88f5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">会变成:</p><ul class=""><li id="beae" class="nj nk iq kx b ky kz lb lc le nl li nm lm nn lq no np nq nr bi translated">她打算买这台减价500英镑的电视机</li></ul><p id="a781" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该消息在阅读时仍然有意义，并且现在将更容易在我们的模型中实现。</p><h2 id="5bd8" class="mx ma iq bd mb my mz dn mf na nb dp mj le nc nd ml li ne nf mn lm ng nh mp ni bi translated">词袋模型</h2><p id="7dc1" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">让我们考虑以下三个<strong class="kx ir"> <em class="mw">文件</em> </strong>:</p><ul class=""><li id="814f" class="nj nk iq kx b ky kz lb lc le nl li nm lm nn lq no np nq nr bi translated">我们正在学习机器学习。</li><li id="1cd9" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">具体来说，我们正在学习文本分类。</li><li id="c10b" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">你考虑过学习Python吗？</li></ul><p id="c94d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">清理这些文件给了我们:</p><ul class=""><li id="a3a4" class="nj nk iq kx b ky kz lb lc le nl li nm lm nn lq no np nq nr bi translated">我们正在学习机器学习</li><li id="0dbb" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">具体来说，我们正在学习文本分类</li><li id="2ef1" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">你考虑过学习python吗</li></ul><p id="f449" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们将把每个文档标记成<strong class="kx ir"> <em class="mw">个术语</em> </strong>:</p><ul class=""><li id="9598" class="nj nk iq kx b ky kz lb lc le nl li nm lm nn lq no np nq nr bi translated">我们|正在|学习|关于|机器|学习</li><li id="017b" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">具体来说，我们正在|学习|关于|文本|分类</li><li id="5191" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">你考虑过学习python吗</li></ul><p id="ad75" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意“|”符号在我们的模型中实际上并不存在，它只是在这里用来帮助可视化标记。每个文档现在可以被认为是一个包含每个单词的术语的袋子。因此得名<em class="mw"/>。让我们看看标记化对文档的影响:</p><ul class=""><li id="34ac" class="nj nk iq kx b ky kz lb lc le nl li nm lm nn lq no np nq nr bi translated">我们是→我们|是</li><li id="313e" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">我们→曾经是</li><li id="859d" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">曾经→曾经</li></ul><p id="70c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">前两个术语(<em class="mw">即</em>“我们是”和“我们是”)具有相同的含义，但将被注册为不同的术语。而且“我们”和“是”这两个词已经分离开了。这可能看起来不可取，但是请记住<strong class="kx ir">某人是否使用缩写是他们如何书写</strong>的一个显著特征。相反，“we 're”和“was”有不同的意思，但现在已经合并为同一个术语。</p><h2 id="d015" class="mx ma iq bd mb my mz dn mf na nb dp mj le nc nd ml li ne nf mn lm ng nh mp ni bi translated">n-gram模型</h2><p id="5083" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">在前面的例子中，我们将文档标记为一个单词术语，称为unigrams ( <em class="mw">，即</em>，1-grams)。为了改进我们的模型，我们还可以提取由包含两个单词的术语组成的二元模型(<em class="mw">即</em>，2-grams):</p><ul class=""><li id="651d" class="nj nk iq kx b ky kz lb lc le nl li nm lm nn lq no np nq nr bi translated">我们正在学习→我们|正在|学习|我们正在|正在学习</li><li id="bdd6" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">我们在学习→我们在学习</li><li id="f3e7" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">你是吗？你是吗</li></ul><p id="5943" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用二元模型，我们现在可以包括新的术语，如“我们是”。我们也有更多的信息来区分“我们是”和“曾经是”，因为“曾经是你”可能比“我们是你”更常见。我们还可以提取像“机器学习”这样的新术语，以前只能作为“机器”和“学习”使用。</p><p id="f8ac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将二元模型添加到我们的模型中会增加每个文档中的术语数量，使我们的模型更加复杂，但是我们必须小心不要忘乎所以。<strong class="kx ir">包括比必要特征更多的特征(<em class="mw">，例如</em>，3-grams，4-grams，5-grams，<em class="mw">等。</em> ) <em class="mw"> </em>来描述我们的模型会使它容易过拟合，以及增加它的规模和计算时间。</strong>虽然在本文中我们不会对<a class="ae lr" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过度拟合</a>进行详细的讨论，但总的想法是，我们希望尽可能用最少的必要特征制作最精确的模型。</p><h2 id="993e" class="mx ma iq bd mb my mz dn mf na nb dp mj le nc nd ml li ne nf mn lm ng nh mp ni bi translated">计数矢量化</h2><p id="8030" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">考虑下面的<strong class="kx ir"> <em class="mw">文集</em> </strong> ( <em class="mw">即</em>，文献集):</p><ul class=""><li id="33f9" class="nj nk iq kx b ky kz lb lc le nl li nm lm nn lq no np nq nr bi translated">这是语料库中的第一个例子。</li><li id="6447" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">这是第二个。</li><li id="3acd" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">每一个例子都在增加。</li></ul><p id="485d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，我们的模型不知道任何术语。因此，我们需要创建一个字典，包含我们希望模型知道的所有术语。这些术语组成了模型的<strong class="kx ir"> <em class="mw">词汇</em> </strong>，并且可以使用Python中的scikit-learn的<code class="fe nx ny nz oa b"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">CountVectorizer</a></code>从语料库中提取:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="e8c8" class="mx ma iq oa b gy of og l oh oi">from sklearn.feature_extraction.text import CountVectorizer<br/>import pandas as pd<br/><br/>corpus = ['This is the first example in the corpus.',<br/>          'This is the second.',<br/>          'The corpus is growing with each example.']<br/><br/>count = CountVectorizer(ngram_range=(1, 2))<br/>count.fit(corpus)<br/>df = pd.DataFrame.from_dict(count.vocabulary_,<br/>                            orient='index', columns=['INDEX'])<br/>df.sort_values(by=['INDEX'], inplace=True)<br/><br/>X_count = count.transform(corpus)<br/>df['doc1_tf'] = X_count.toarray().T[df['INDEX'], 0]<br/>df['doc2_tf'] = X_count.toarray().T[df['INDEX'], 1]<br/>df['doc3_tf'] = X_count.toarray().T[df['INDEX'], 2]<br/>print(df)</span></pre><p id="d6f5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输出是我们的词汇表，其中包含我们的语料库中所有唯一的术语(<em class="mw">即</em>，单词和双词)和<strong class="kx ir"> <em class="mw">术语频率</em> </strong> ( <em class="mw"> tf </em> ) ( <em class="mw">即</em>，每个术语在文档中出现的次数)表。</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="7e34" class="mx ma iq oa b gy of og l oh oi">               INDEX  doc1_tf  doc2_tf  doc3_tf<br/>corpus             0        1        0        1<br/>corpus is          1        0        0        1<br/>each               2        0        0        1<br/>each example       3        0        0        1<br/>example            4        1        0        1<br/>example in         5        1        0        0<br/>first              6        1        0        0<br/>first example      7        1        0        0<br/>growing            8        0        0        1<br/>growing with       9        0        0        1<br/>in                10        1        0        0<br/>in the            11        1        0        0<br/>is                12        1        1        1<br/>is growing        13        0        0        1<br/>is the            14        1        1        0<br/>second            15        0        1        0<br/>the               16        2        1        1<br/>the corpus        17        1        0        1<br/>the first         18        1        0        0<br/>the second        19        0        1        0<br/>this              20        1        1        0<br/>this is           21        1        1        0<br/>with              22        0        0        1<br/>with each         23        0        0        1</span></pre><h2 id="89c7" class="mx ma iq bd mb my mz dn mf na nb dp mj le nc nd ml li ne nf mn lm ng nh mp ni bi translated">术语频率-逆文档频率(tf-idf)</h2><p id="6fe2" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">有很多大家都用的常用词，在某人的写作中不会成为显著特征。例如，“is”和“the”都有一个<strong class="kx ir"> <em class="mw">文档频率</em> </strong> ( <em class="mw"> df </em>)等于3 ( <em class="mw">即</em>，它们出现在我们语料库的所有三个文档中)，因此，它们不会像具有<em class="mw"> df </em> = 1 ( <em class="mw">即</em>，它只出现在一个文档中)的术语“growing”那样给我们提供那么多有区别的信息。我们希望对文档出现频率低的术语赋予更高的权重。注意，在计算<em class="mw"> df </em>时，一个术语在文档中出现多少次并不重要，只要它至少出现一次。例如，术语“the”仍然有<em class="mw"> df </em> = 3，尽管在语料库中总共出现了4次。我们可以通过将术语频率乘以<strong class="kx ir"> <em class="mw">逆文档频率</em> </strong> ( <em class="mw"> idf </em>)来应用这个权重。这给了我们<strong class="kx ir"> <em class="mw">术语频率—逆文档频率</em> </strong> (tf-idf或<em class="mw"> tfidf </em>):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/db91bd104e7e9d1278cfcbb78c04ebfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*RuAY3oFvPkTcRigxoZb1Bg@2x.png"/></div></figure><p id="ae5f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中<em class="mw"> n </em>是语料库中的文档总数(在我们最后的例子中，<em class="mw"> n </em> =3)。请注意，等式中“+ 1”的用法因实施方式而异。它们的目的是避免产生零，尤其是在分母中。这里显示的等式是我们即将使用的<code class="fe nx ny nz oa b"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html" rel="noopener ugc nofollow" target="_blank">TfidfTransformer</a></code>的默认scikit-learn实现。</p><p id="e045" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我们希望确保文档的长度不会影响我们的模型。例如，如果一个政客通常比另一个政客写得更短，我们不希望用户能够通过简单地写较长的消息来欺骗我们的模型。因此，我们将标准化每个文档向量，使其平方和等于1。换句话说，<strong class="kx ir">在计算了<em class="mw"> tfidf </em>值之后，我们将把每个文档缩放到一个单位向量</strong>。</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="dd21" class="mx ma iq oa b gy of og l oh oi">from sklearn.feature_extraction.text import TfidfTransformer<br/><br/>tfidf = TfidfTransformer()<br/>X_tfidf = tfidf.fit_transform(X_count)<br/>df['doc1_tfidf'] = X_tfidf.toarray().T[df['INDEX'], 0]<br/>df['doc2_tfidf'] = X_tfidf.toarray().T[df['INDEX'], 1]<br/>df['doc3_tfidf'] = X_tfidf.toarray().T[df['INDEX'], 2]<br/>print(df[['INDEX', 'doc1_tfidf', 'doc2_tfidf', 'doc3_tfidf']])</span></pre><p id="d8e6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这为我们提供了下面的<em class="mw"> tfidf </em>表:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="9f65" class="mx ma iq oa b gy of og l oh oi">               INDEX  doc1_tfidf  doc2_tfidf  doc3_tfidf<br/>corpus             0    0.227103    0.000000    0.235457<br/>corpus is          1    0.000000    0.000000    0.309598<br/>each               2    0.000000    0.000000    0.309598<br/>each example       3    0.000000    0.000000    0.309598<br/>example            4    0.227103    0.000000    0.235457<br/>example in         5    0.298613    0.000000    0.000000<br/>first              6    0.298613    0.000000    0.000000<br/>first example      7    0.298613    0.000000    0.000000<br/>growing            8    0.000000    0.000000    0.309598<br/>growing with       9    0.000000    0.000000    0.309598<br/>in                10    0.298613    0.000000    0.000000<br/>in the            11    0.298613    0.000000    0.000000<br/>is                12    0.176366    0.280520    0.182854<br/>is growing        13    0.000000    0.000000    0.309598<br/>is the            14    0.227103    0.361220    0.000000<br/>second            15    0.000000    0.474961    0.000000<br/>the               16    0.352732    0.280520    0.182854<br/>the corpus        17    0.227103    0.000000    0.235457<br/>the first         18    0.298613    0.000000    0.000000<br/>the second        19    0.000000    0.474961    0.000000<br/>this              20    0.227103    0.361220    0.000000<br/>this is           21    0.227103    0.361220    0.000000<br/>with              22    0.000000    0.000000    0.309598<br/>with each         23    0.000000    0.000000    0.309598</span></pre><p id="4297" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意，即使一些术语在不同的文档中有相同的<em class="mw"> tf </em>，它们也可以有不同的<em class="mw"> tfidf </em>。</p><p id="3d14" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">既然我们知道了如何从语料库中提取特征并转换成模型可以理解的格式，我们就可以继续前进，开始训练实际的模型。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="699f" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">训练模型</h1><p id="5f7d" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">在前面的例子中，我们查看了一个包含3个文档和24个词汇的小型语料库。在实践中，如果我们想要开发一个模型来确定一个引用是属于伯尼·桑德斯还是唐纳德·川普，我们将需要更多的文档和更大的词汇表。如前所述，我们将使用他们自2015年以来的所有推文。</p><p id="eff0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用新的Jupyter笔记本，让我们从使用Python标准库中的<code class="fe nx ny nz oa b"><a class="ae lr" href="https://docs.python.org/3.7/library/re.html" rel="noopener ugc nofollow" target="_blank">re</a></code>清理推文开始:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="c2ac" class="mx ma iq oa b gy of og l oh oi">import os<br/>import re<br/>import pandas as pd<br/><br/><br/>def clean_text(text):<br/>    text = re.sub(r"'", '', text)<br/>    text = re.sub(r'http\S+', '', text)<br/>    text = re.sub(r'pic.twitter\S+', '', text)<br/>    text = re.sub(r'\W+', ' ', text.lower())<br/><br/>    return text<br/><br/><br/>df = pd.read_csv(os.path.join('tweets', 'tweets.csv'),<br/>                 low_memory=False)<br/>df.drop_duplicates(inplace=True)<br/>df['tweet-clean'] = df['tweet'].apply(clean_text)<br/>drop_index = []<br/><br/>for i in range(len(df)):<br/>    if df['tweet-clean'].iloc[i] in ('', ' '):<br/>        drop_index.append(i)<br/><br/>df.drop(drop_index, inplace=True)</span></pre><p id="9fdc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们定义了<code class="fe nx ny nz oa b">clean_text</code>,它获取一个字符串，并根据我们之前讨论的需求对其进行清理。然后，我们将数据集加载到一个<code class="fe nx ny nz oa b"><a class="ae lr" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" rel="noopener ugc nofollow" target="_blank">pandas.DataFrame</a></code>中，并将<code class="fe nx ny nz oa b">clean_text</code>应用到每条推文中。最后，在我们的清理过程之后，我们会删除任何现在是空白的推文。在我们训练模型之前，让我们定义两个常数:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="4e42" class="mx ma iq oa b gy of og l oh oi">random_state = 0<br/>n_jobs = -1</span></pre><p id="0962" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意，通过设置<code class="fe nx ny nz oa b">random_state = 0</code>，我们实际上并没有初始化我们的随机种子，而是将一个整数传递给我们的各种scikit-learn实现，这将允许我们在每次运行代码时保持相同的随机状态。<strong class="kx ir">在调整超参数时，我们希望确保每次测试我们的模型时都生成相同的随机序列</strong>，这样我们就可以确定我们看到的任何性能改进都是由于我们所做的更改，而不是由于随机数生成器产生的随机性。</p><p id="f940" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> Scikit-learn支持多线程。今天大多数电脑都使用多核CPU。我们可以设置<code class="fe nx ny nz oa b"><a class="ae lr" href="https://scikit-learn.org/stable/glossary.html#term-n-jobs" rel="noopener ugc nofollow" target="_blank">n_jobs</a> = -1</code>使用一个CPU上的所有线程来加快模型训练。</strong></p><p id="4a08" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们将计算推文的矢量化。使用<code class="fe nx ny nz oa b"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank">TfidifVectorizer</a></code>可以将语料库直接转换为scikit-learn中的<em class="mw"> tfidf </em>向量，这实际上是一个<code class="fe nx ny nz oa b">CountVectorizer</code>后跟<code class="fe nx ny nz oa b">TfidfTransformer</code>的管道，两者我们之前都使用过。</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="1550" class="mx ma iq oa b gy of og l oh oi">from sklearn.feature_extraction.text import TfidfVectorizer<br/><br/>tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)<br/>X = tfidf.fit_transform(df['tweet-clean'])<br/><br/>print(f'Number of documents: {X.shape[0]}')<br/>print(f'Size of vocabulary:  {X.shape[1]}')</span></pre><p id="cbac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它为我们的数据集输出一些统计数据:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="9a89" class="mx ma iq oa b gy of og l oh oi">Number of documents: 34648<br/>Size of vocabulary:  86092</span></pre><p id="9b9d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了在整篇文章中保持一致，我们将它们指定为:</p><ul class=""><li id="6cda" class="nj nk iq kx b ky kz lb lc le nl li nm lm nn lq no np nq nr bi translated">文档数量:<em class="mw"> n </em> = 34 648条推文</li><li id="05fd" class="nj nk iq kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">词汇量:<em class="mw"> v </em> = 86 092个词汇</li></ul><p id="2241" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们设置了<code class="fe nx ny nz oa b">ngram_range=(1, 2)</code>和<code class="fe nx ny nz oa b">min_df=2</code>，这表明我们的词汇表将由单词和双词组成，文档频率至少为2。通过消除文档频率仅为1的术语，我们可以减少模型中的特征数量，并减少过度拟合。注意，这种方法在这里工作得很好，因为我们的语料库包含成千上万的文档。这可能不适合较小的语料库，这些参数需要针对每个项目进行专门调整。</p><p id="627f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">就像我们的模型需要以数字的形式接收tweets一样，它也需要以数字的形式接收政客的名字。这可以使用scikit-learn的<code class="fe nx ny nz oa b"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" rel="noopener ugc nofollow" target="_blank">LabelEncoder</a></code>来完成:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="62c9" class="mx ma iq oa b gy of og l oh oi">from sklearn.preprocessing import LabelEncoder<br/><br/>le = LabelEncoder()<br/>y = le.fit_transform(df['name'])<br/><br/>for i in range(len(le.classes_)):<br/>    print(f'{le.classes_[i]:&lt;15} = {i}')</span></pre><p id="18b3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它分配以下标签:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="5565" class="mx ma iq oa b gy of og l oh oi">Bernie Sanders  = 0<br/>Donald J. Trump = 1</span></pre><p id="b7cd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在训练我们的模型之前，<strong class="kx ir">将我们的数据集分成训练和测试集</strong>是极其重要的。训练集将用于训练我们的模型，但重要的是要有一个单独的测试数据集，以真正衡量我们的模型在以前从未见过的数据上有多准确。使用scikit-learn的<code class="fe nx ny nz oa b"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank">train_test_split</a></code>功能:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="5e00" class="mx ma iq oa b gy of og l oh oi">from sklearn.model_selection import train_test_split<br/><br/>X_train, X_test, y_train, y_test = train_test_split(X, y,<br/>                                    test_size=0.5,<br/>                                    random_state=random_state,<br/>                                    stratify=y)</span></pre><p id="0fcf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这里，我们用<code class="fe nx ny nz oa b">test_size=0.5</code>指定我们将保留50%的数据集作为我们的测试集。我们还设置了<code class="fe nx ny nz oa b"><a class="ae lr" href="https://en.wikipedia.org/wiki/Stratified_sampling" rel="noopener ugc nofollow" target="_blank">stratify</a>=y</code>,以确保我们的测试和训练数据集具有相同的伯尼·桑德斯和唐纳德·川普推文比率。</p><p id="84cd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们已经建立了数据，让我们看看我们的第一个算法。</p><h2 id="dcac" class="mx ma iq bd mb my mz dn mf na nb dp mj le nc nd ml li ne nf mn lm ng nh mp ni bi translated">逻辑回归</h2><p id="6b40" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">逻辑回归是最著名的机器学习模型之一，通常是大多数人在开始机器学习时首先学习的算法之一。标准逻辑函数如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/892dea3364e9279d5e19d40e721fc2c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*J_opvoqP7B6uusPZQfhvHw@2x.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/4a210c2431b4ee40433c5388a4b15450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KKcK5DgyxvAj2M8ySWYdVg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">标准逻辑函数。图片作者。</p></figure><p id="aab3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">标准逻辑函数接收值<em class="mw"> z </em>并输出0和1之间的值。我们正在处理一个二元分类问题，这意味着结果必须是伯尼·桑德斯或唐纳德·特朗普，没有其他选择。这就是为什么我们之前给每个政治家分配0或1的值。</p><p id="35c1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在让我们看看如何将一条tweet转换成一个值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3c785a980691a7b94a5f0563664d36d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Vkem_VnPVOeTtR0JaLHkMQ@2x.png"/></div></figure><p id="3204" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从这个等式中，我们看到<em class="mw"> z </em>等于我们的<em class="mw"> tfidf </em>向量(<strong class="kx ir"> x </strong>)与添加了偏置项(<em class="mw"> b_0 </em>)的权重向量(<strong class="kx ir"> w </strong>)的点积。注意，虽然Python列表从0开始索引，但更常见的是在描述数学模型时从1开始索引，并为我们的偏差项保留索引0(有时也会出现为<em class="mw"> w_0 </em>)。对于每条推文，我们可以计算一个值<em class="mw"> z </em>，通过逻辑函数传递它，并四舍五入到最接近的整数，得到0 =伯尼·桑德斯或1 =川普。为了做到这一点，我们的机器学习算法需要计算出权重向量(<strong class="kx ir"> w </strong>)的哪些值将导致最高比例的推文被归因于正确的政治家。</p><p id="7c0e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们不会探究解决w的各种方法，因为这本身就是一篇完整的文章。Scikit-learn实现了各种求解器来训练一个<code class="fe nx ny nz oa b"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">LogisticRegression</a></code>分类器。在这个应用中，我们将<code class="fe nx ny nz oa b">solver='<a class="ae lr" href="https://arxiv.org/abs/1407.0202" rel="noopener ugc nofollow" target="_blank">saga</a>'</code>设置为我们的优化方法，将<code class="fe nx ny nz oa b">C=20</code>设置为我们的逆正则项，以减少过度拟合，并使我们的模型更容易训练:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="ee8c" class="mx ma iq oa b gy of og l oh oi">from sklearn.linear_model import LogisticRegression<br/><br/>clf_log = LogisticRegression(C=20, solver='saga',<br/>                             random_state=random_state,<br/>                             n_jobs=n_jobs)<br/><br/>clf_log.fit(X_train, y_train)<br/>log_score = clf_log.score(X_test, y_test)<br/>print(f'Logistic Regression accuracy: {log_score:.1%}')</span></pre><p id="366b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这返回了我们在测试数据集上的准确性:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="7c95" class="mx ma iq oa b gy of og l oh oi">Logistic Regression accuracy: 95.8%</span></pre><h2 id="b061" class="mx ma iq bd mb my mz dn mf na nb dp mj le nc nd ml li ne nf mn lm ng nh mp ni bi translated">伯努利朴素贝叶斯</h2><p id="1f7f" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">朴素贝叶斯分类器在文本分类任务中相对流行。我们将实现一个称为伯努利朴素贝叶斯的特定事件模型。每个政治家写的特定推文的<a class="ae lr" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">可能性</a>可以计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/31adb9ff20a7996dc992d3c9a875b957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*qL7N5MyJmcAn7nGEsItaFw@2x.png"/></div></figure><p id="13ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中<em class="mw"> p_ki </em>，是政客<em class="mw"> C_k </em>在推文中使用术语<em class="mw"> i </em>的概率。让我们使用scikit-learn的<code class="fe nx ny nz oa b"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html" rel="noopener ugc nofollow" target="_blank">BernoulliNB</a></code>分类器来训练我们的模型，然后看看这些参数代表了什么:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="e67c" class="mx ma iq oa b gy of og l oh oi">from sklearn.naive_bayes import BernoulliNB<br/><br/>clf_bnb = BernoulliNB(alpha=0.01, binarize=0.09)<br/>clf_bnb.fit(X_train, y_train)</span></pre><p id="a02f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在让我们来看看下面伯尼·桑德斯的名言:</p><blockquote class="ol om on"><p id="61fa" class="kv kw mw kx b ky kz jr la lb lc ju ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">“全民医疗保险”<em class="iq"> -伯尼·桑德斯</em></p></blockquote><p id="fad7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此报价将包含术语“medicare for”。让我们看看这如何转化为上述等式中的参数:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="6839" class="mx ma iq oa b gy of og l oh oi">import numpy as np<br/><br/>C_k = 'Bernie Sanders'<br/>k = le.transform([C_k])[0]<br/>i = tfidf.vocabulary_['medicare for']<br/>p_ki = np.exp(clf_bnb.feature_log_prob_[k, i])<br/>print(f'k = {k}')  # class label<br/>print(f'i = {i}')  # index for term 'medicare for'<br/>print(f'C_k = {C_k}')<br/>print(f'p_ki = {p_ki:.3}')</span></pre><p id="f0cc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它返回:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="0d9a" class="mx ma iq oa b gy of og l oh oi">k = 0  <br/>i = 44798  <br/>C_k = Bernie Sanders<br/>p_ki = 0.0289</span></pre><p id="5f46" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里，<em class="mw"> p_ki </em> = 0.0289可以解释为在我们的训练集中，来自伯尼·桑德斯的推文有2.89%包含术语“医疗保险”。关于伯努利朴素贝叶斯有趣的是<strong class="kx ir">可以直接求解<em class="mw"> p_ki </em>值</strong>。它简单地等于政治家<em class="mw"> C_k </em>语料库中术语<em class="mw"> i </em>的文档频率除以政治家<em class="mw"> C_k </em>语料库中的文档总数:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="2bf8" class="mx ma iq oa b gy of og l oh oi">df_ki = clf_bnb.feature_count_[k, i]<br/>n_k = clf_bnb.class_count_[k]<br/>p_ki_manual = df_ki / n_k<br/>print(f'{p_ki:.5}')<br/>print(f'{p_ki_manual:.5}')</span></pre><p id="7e54" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这两者都为我们提供了相同的值，p_ki有4个有效数字:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="5e70" class="mx ma iq oa b gy of og l oh oi">0.028924<br/>0.028922</span></pre><p id="5aeb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在逻辑回归中，<strong class="kx ir"> x </strong>包含<em class="mw"> tfidf </em>值。在伯努利朴素贝叶斯中，<em class="mw"> x_i </em>必须等于0或1。最简单的形式是，如果术语<em class="mw"> i </em>出现在tweet中，<em class="mw"> x_i </em>等于1，如果没有出现，则等于0。然而，我们可以通过在我们的<em class="mw"> tfidf </em>值上设置一个阈值来提取更多的信息。最佳阈值通常通过试错法或穷举搜索法找到，例如scikit-learn中的<code class="fe nx ny nz oa b"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">GridSearchCV</a></code>。对于这个应用程序，一个最佳的阈值是<code class="fe nx ny nz oa b">binarize=0.09</code>。因此，高于0.09的任何<em class="mw"> tfidf </em>值被设置为1，低于0的任何值被设置为0。我们还设置了<code class="fe nx ny nz oa b">alpha=0.01</code>，这是一个平滑参数。</p><p id="4b75" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">回到我们的等式，我们可以看到，如果我们从用户那里接收到一个文档，其中<em class="mw"> x_i </em> = 1，那么我们正在乘以术语<em class="mw"> i </em>出现在政治家<em class="mw"> C_k </em>的推文中的概率(<em class="mw"> p_ki </em>)。相反，如果<em class="mw"> x_i </em> = 0，那么我们就在成倍增加这个词<em class="mw"> i </em>不会<em class="mw">出现在政客<em class="mw"> C_k </em>的推特上的概率。对词汇表中的每个术语和每个政治家都进行这种乘法运算。然后输出具有最高联合对数似然的政治家作为结果。记住<strong class="kx ir">当逻辑函数输出概率时，伯努利朴素贝叶斯输出可能性</strong>。这两个术语是相关的，但并不意味着同一件事。这里将不讨论将可能性转换成概率的方法，但是下面的<a class="ae lr" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Discussion" rel="noopener ugc nofollow" target="_blank">维基百科条目</a>包含了对该主题及其与逻辑回归的关系的简明讨论。此外，<code class="fe nx ny nz oa b">BernoulliNB</code>可以使用它的<code class="fe nx ny nz oa b"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB.predict_proba" rel="noopener ugc nofollow" target="_blank">predict_proba</a></code>方法将可能性转换成概率。现在让我们计算一下测试数据集的准确度:</em></p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="9089" class="mx ma iq oa b gy of og l oh oi">bnb_score = clf_bnb.score(X_test, y_test)<br/>print(f'Bernoulli Naive Bayes accuracy: {bnb_score:.1%}')</span></pre><p id="3147" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它返回:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="7bce" class="mx ma iq oa b gy of og l oh oi">Bernoulli Naive Bayes accuracy: 96.2%</span></pre><h2 id="6cd9" class="mx ma iq bd mb my mz dn mf na nb dp mj le nc nd ml li ne nf mn lm ng nh mp ni bi translated">整体平均</h2><p id="da94" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">我们训练了两个模型，逻辑回归和伯努利朴素贝叶斯，这两个模型在测试数据集上都具有相对较高的准确性。使用scikit-learn的<code class="fe nx ny nz oa b"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html" rel="noopener ugc nofollow" target="_blank">VotingClassifier</a></code>，我们可以通过使用<code class="fe nx ny nz oa b">estimators=[('log', clf_log), ('bnb', clf_bnb)]</code>选择我们的分类器并设置<code class="fe nx ny nz oa b">voting='soft'</code>来获得一个更准确的结果，从而获得它们各自计算的概率的加权平均值。通过设置<code class="fe nx ny nz oa b">weights=(0.6, 0.4)</code>将60%的逻辑回归概率与40%的伯努利朴素贝叶斯概率相加具有最高的准确性。一个有趣的结果，考虑到我们的伯努利朴素贝叶斯有更高的准确性，但我们赋予它更小的权重。这可能是因为来自我们的伯努利朴素贝叶斯分类器的潜在概率估计不一定像来自我们的逻辑回归分类器的概率计算那样可靠。现在让我们训练我们的集合模型:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="1a72" class="mx ma iq oa b gy of og l oh oi">from sklearn.ensemble import VotingClassifier<br/><br/>clf_vot = VotingClassifier(<br/>    estimators=[('log', clf_log), ('bnb', clf_bnb)],<br/>    voting='soft', weights=(0.6, 0.4), n_jobs=n_jobs)<br/><br/>clf_vot.fit(X_train, y_train)<br/>vot_score = clf_vot.score(X_test, y_test)<br/>print(f'Ensemble Averaging accuracy: {vot_score:.1%}')</span></pre><p id="37ac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这返回了我们在测试数据集上的准确性:</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="8163" class="mx ma iq oa b gy of og l oh oi">Ensemble Averaging accuracy: 96.4%</span></pre><p id="70ef" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然这只是准确性的微小提高，但我们应该记住<strong class="kx ir">人们可能会输入不一定是任何一个候选人可能会说的信息</strong>。在这些情况下，整体平均可能是有益的，因为这相当于在决定哪位政治家更有可能在推特上发布用户所写的内容之前获得第二种意见。</p><h2 id="5789" class="mx ma iq bd mb my mz dn mf na nb dp mj le nc nd ml li ne nf mn lm ng nh mp ni bi translated">最终培训</h2><p id="1b87" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">一旦我们确定不再改变任何超参数(<em class="mw">，例如</em>，朴素贝叶斯的阈值，投票的加权平均值，<em class="mw">等)。</em>)然后，我们可以在整个数据集上重新训练我们的模型，以获得两倍的训练样本。</p><pre class="kg kh ki kj gt ob oa oc od aw oe bi"><span id="b127" class="mx ma iq oa b gy of og l oh oi">clf_vot.fit(X, y)</span></pre></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="991c" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">使用应用程序</h1><p id="d4d8" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">我希望你现在对scikit-learn如何应用于文本分类有了更好的理解，如果你是机器学习的新手，你现在有兴趣学习更多关于它的知识。</p><p id="90fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在您可以亲自试用<a class="ae lr" href="https://www.shawnchahal.com/who-said-it" rel="noopener ugc nofollow" target="_blank">应用</a>来看看我们实际操作过的模型！</p><div class="or os gp gr ot ou"><a href="https://www.shawnchahal.com/who-said-it" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd ir gy z fp oz fr fs pa fu fw ip bi translated">谁说的？</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">写一条信息，看看伯尼·桑德斯和唐纳德·川普谁更有可能说出来。</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">www.shawnchahal.com</p></div></div></div></a></div></div></div>    
</body>
</html>