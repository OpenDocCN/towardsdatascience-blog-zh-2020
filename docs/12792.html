<html>
<head>
<title>A Visual Guide to Random Forests</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林视觉指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-visual-guide-to-random-forests-b3965f453135?source=collection_archive---------45-----------------------#2020-09-02">https://towardsdatascience.com/a-visual-guide-to-random-forests-b3965f453135?source=collection_archive---------45-----------------------#2020-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2718" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">直观的视觉指南和视频解释了强大的组装方法</h2></div><p id="d7b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习中最明显的问题之一是“更多的模型比更少的模型更好吗？”回答这个问题的科学叫做<strong class="kk iu">模型组装</strong>。模型集成要求如何构建模型的集合，以提高测试准确性，同时降低与存储、训练和从多个模型中获取推理相关的成本。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/003206a51478696a6780d26eb63ab328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UdAuaKsRzmBpCxeoBsxeuA.png"/></div></div></figure><p id="1b47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将探索一个流行的应用于决策树的集合方法:<strong class="kk iu">随机森林。</strong></p><p id="2f9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了说明这一点，让我们举一个例子。想象一下，根据野火的大小、位置和日期，我们正试图预测是什么导致了这场野火。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lq"><img src="../Images/b6ba7176cb601402d9917070f287f870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LG6YiQYX7DUsJxfKy3YOdQ.png"/></div></div></figure><p id="2cec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机森林模型的基本构建模块是决策树，所以如果你想了解它们是如何工作的，我建议看看我在<a class="ae lr" href="https://medium.com/swlh/a-visual-guide-to-decision-trees-26606e456cbe" rel="noopener">之前的帖子</a>。作为快速复习，决策树通过递归地询问简单的真或假问题来执行分类或回归任务，这些问题将数据分成最纯粹的可能子组。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ls"><img src="../Images/cf07a88da78b9f5006275edfd8deae11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fX18drpXVsK3k6qcj5XvQA.png"/></div></div></figure><p id="8af5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在回到随机森林。在这种集成方法中，我们训练一组决策树(因此得名“森林”)，然后在不同的树中进行投票。一棵树一票。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lq"><img src="../Images/d1d542b9be4802c55f4e45d8414c06c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MHqZXIGWdCIdVuqZL1OALQ.png"/></div></div></figure><p id="bb65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在分类的情况下，每棵树吐出一个类别预测，然后拥有最多投票的类别成为随机森林的输出。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lt"><img src="../Images/4ce133fd3bdf5b70e0b75672c7f251a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_cZ_nFYe8T3vEvUyRa2YQ.png"/></div></div></figure><p id="7ee3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在回归的情况下，每棵树的预测的简单平均值成为随机森林的输出。</p><p id="77f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机森林背后的关键思想是群体中有智慧。从一大群模型中获得的洞察力可能比单独从任何一个模型中获得的预测更准确。</p><p id="774e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简单吧？当然，但是为什么这个有用呢？如果我们所有的模型都学习完全相同的东西，并投票给同一个答案会怎么样？这难道不等同于只有一个模型做出预测吗？</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lu"><img src="../Images/6bc6cf65e542f04421cf0e732e2a22b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*86APynEB5NDcINCa7eej_g.png"/></div></div></figure><p id="39d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是的，但是有一个办法可以解决。</p><p id="3291" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是首先，我们需要定义一个词来帮助解释:<strong class="kk iu">不相关</strong>。我们需要我们的决策树彼此不同。我们希望他们不同意分裂是什么和预测是什么。不相关对随机森林很重要。一大群不相关的树在一个系综中一起工作将胜过任何一个组成树。换句话说，森林不会受到个别树木错误的影响。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lw"><img src="../Images/aaa2678ef9a59fa755bb101117b6e10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zLu0xZMyajKQl5GvbcjRHw.png"/></div></div></figure><p id="3c8d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有几种不同的方法来确保我们的树是不相关的:</p><p id="03eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一种方法叫做<strong class="kk iu">“自举”</strong>。Bootstrapping 是通过采样从我们的训练集中创建更小的数据集。现在，使用普通决策树，我们将整个训练集输入到树中，并允许它生成预测。然而，通过自举，我们允许每棵树随机采样训练数据<em class="lv">和替换，</em>产生不同的树。当我们允许替换时，一些观察结果可能会在样本中重复。通常，bootstrap 的样本大小与原始数据集的大小相同，但为了提高计算效率，可以对数据集的子集进行采样。使用自举来创建不相关的模型，然后聚合它们的结果被称为<strong class="kk iu">自举聚合</strong>，或简称为<strong class="kk iu"> bagging </strong>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lx"><img src="../Images/b5c213400ddff27eaf68f4dc7de6137e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sC0XVMG6p48f6--XJwpsDQ.png"/></div></div></figure><p id="5afa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的树中引入变化的第二种方法是通过改变每棵树可以分裂的特征。这种方法叫做<strong class="kk iu">特征随机性。</strong>请记住，使用基本决策树，当需要在一个节点上分割数据时，该树会考虑每一个可能的特征，并选择一个导致最纯粹子群的特征。然而，对于随机森林，我们限制了每棵树可以考虑分割的特征的数量。一些库在分割级别而不是树级别随机化特征。如果我们假设树是决策树桩，这并不重要，这意味着只有 1 个分裂，或者最大深度= 1。在这两种情况下，目标都是限制可能特征的数量，以便去相关各个树。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ly"><img src="../Images/912e7c80aafe05c1c44ebe5aa97a3fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_myzeCNHNvQf1mUm0YVYHQ.png"/></div></div></figure><p id="40a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为单个树非常简单，并且它们只在训练数据和特征集的子集上被训练，所以训练时间非常短，所以我们可以训练成千上万的树。随机森林广泛应用于学术界和工业界。现在您已经理解了这个概念，您几乎已经准备好实现一个随机森林模型来用于您自己的项目了！请继续关注随机森林编码教程和另一个集合方法的新帖子——梯度增强树！</p><p id="d265" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看看下面的视频，看看你在行动中学到的一切！</p><p id="e9d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lr" href="https://youtu.be/cIbj0WuK41w" rel="noopener ugc nofollow" target="_blank">https://youtu.be/cIbj0WuK41w</a></p></div></div>    
</body>
</html>