<html>
<head>
<title>Building and deploying end-to-end fake news classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建和部署端到端假新闻分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-and-deploying-end-to-end-fake-news-classifier-caebe45bd30?source=collection_archive---------59-----------------------#2020-05-22">https://towardsdatascience.com/building-and-deploying-end-to-end-fake-news-classifier-caebe45bd30?source=collection_archive---------59-----------------------#2020-05-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8600" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/data-for-change" rel="noopener" target="_blank">变更数据</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/c287c4dd7578e4bae96962130b74714e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DyvhQfy5rFLZn8eG"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">罗马克拉夫特在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="cd0d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这个智能手机和互联网的数字时代，假新闻像野火一样传播，它看起来就像真新闻，对社会造成了很大的损害。因此，在本教程中，我们将构建一个假新闻分类器，并将其作为一个web应用程序部署在云上，以便任何人都可以访问它。它不会像谷歌或facebook的假新闻分类器那样好，但根据从Kaggle获得的数据集，它会相当不错。</p><blockquote class="ln lo lp"><p id="78bc" class="kp kq lq kr b ks kt ku kv kw kx ky kz lr lb lc ld ls lf lg lh lt lj lk ll lm im bi translated">在我们开始之前，为了让你有动力，让我向你展示一下在本教程结束时你将能够构建的web应用程序<a class="ae ko" href="http://real-fake-news-classifier.herokuapp.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kr jd">假新闻分类器</strong> </a> <strong class="kr jd">。</strong>现在你已经看到了最终产品，让我们开始吧。</p></blockquote><p id="dff1" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd"> <em class="lq">注</em> </strong> <em class="lq">:我假设你熟悉基本的机器学习技术、算法和软件包。</em></p><p id="8afd" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我将本教程分为三个部分:</p><ol class=""><li id="b037" class="lu lv it kr b ks kt kw kx la lw le lx li ly lm lz ma mb mc bi translated">探索性数据分析</li><li id="1605" class="lu lv it kr b ks md kw me la mf le mg li mh lm lz ma mb mc bi translated">预处理和模型训练</li><li id="7d83" class="lu lv it kr b ks md kw me la mf le mg li mh lm lz ma mb mc bi translated">在Heroku上构建和部署Web应用程序</li></ol><p id="8595" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，如果您是初学者，我建议您安装Anaconda发行版，因为它附带了数据科学所需的所有软件包，并设置了一个虚拟环境。</p><p id="159e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你想跟随这个教程，这里是我的GitHub上的源代码链接:<a class="ae ko" href="https://github.com/eaofficial/fake-news-classifier" rel="noopener ugc nofollow" target="_blank">https://github.com/eaofficial/fake-news-classifier</a>。</p><p id="6799" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可以在这里获得数据集<a class="ae ko" href="https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset" rel="noopener ugc nofollow" target="_blank"><strong class="kr jd"><em class="lq"/></strong></a>或者你可以克隆我的GitHub库。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="a500" class="mp mq it bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">1.探索性数据分析</h1><figure class="no np nq nr gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nn"><img src="../Images/342d839189efd4cbd1d54a1dd299ccbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2PJnxynnYOhOEB9R"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">照片由<a class="ae ko" href="https://unsplash.com/@element5digital?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">元素5数码</a>在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ca03" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在项目目录中创建一个名为eda.ipynb或eda.py的文件。</p><p id="8f0a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将首先导入所有需要的包。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="0886" class="nx mq it nt b gy ny nz l oa ob"><em class="lq">#Importing all the libraries</em><br/><strong class="nt jd">import</strong> <strong class="nt jd">warnings</strong><br/>warnings.filterwarnings('ignore')<br/><strong class="nt jd">import</strong> <strong class="nt jd">numpy</strong> <strong class="nt jd">as</strong> <strong class="nt jd">np</strong><br/><strong class="nt jd">import</strong> <strong class="nt jd">pandas</strong> <strong class="nt jd">as</strong> <strong class="nt jd">pd</strong><br/><strong class="nt jd">import</strong> <strong class="nt jd">matplotlib.pyplot</strong> <strong class="nt jd">as</strong> <strong class="nt jd">plt</strong><br/><strong class="nt jd">import</strong> <strong class="nt jd">seaborn</strong> <strong class="nt jd">as</strong> <strong class="nt jd">sns</strong><br/><strong class="nt jd">import</strong> <strong class="nt jd">nltk</strong><br/><strong class="nt jd">import</strong> <strong class="nt jd">re</strong><br/><strong class="nt jd">from</strong> <strong class="nt jd">wordcloud</strong> <strong class="nt jd">import</strong> WordCloud<br/><strong class="nt jd">import</strong> <strong class="nt jd">os</strong></span></pre><p id="4e33" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我们将首先使用<code class="fe oc od oe nt b">pd.read_csv()</code>读取假新闻数据集，然后我们将探索该数据集。</p><p id="1b52" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在上述笔记本的单元格4中，我们统计了每个主题中的样本假新闻的数量。我们还将使用seaborn count plot <code class="fe oc od oe nt b">sns.coountplot()</code>绘制其分布图。</p><p id="4fbf" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们现在将绘制一个词云，首先将所有新闻连接成一个字符串，然后生成标记并删除停用词。文字云是一种非常好的可视化文本数据的方式。</p><p id="cc58" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如您在下一个单元格中看到的，现在我们将true.csv作为真实新闻数据集导入，并执行与我们在fake.csv上执行的步骤相同的步骤。您会注意到真实新闻数据集中的一个不同之处是，在<strong class="kr jd"><em class="lq"/></strong>列中，有一个出版物名称，如<em class="lq"> WASHINGTON (Reuters) </em>，由连字符(-)分隔。</p><p id="fc22" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">看起来真实的新闻是可信的，因为它来自一家出版社，所以我们将从新闻部分中分离出出版物，以使本教程的预处理部分中的数据集一致。现在，我们将只探索数据集。</p><p id="034c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果您继续下去，可以看到新闻主题列在真实和虚假新闻数据集中的分布是不均匀的，所以我们稍后将删除该列。我们的<strong class="kr jd"> <em class="lq"> EDA到此结束。</em> </strong></p><blockquote class="ln lo lp"><p id="d82e" class="kp kq lq kr b ks kt ku kv kw kx ky kz lr lb lc ld ls lf lg lh lt lj lk ll lm im bi translated">现在我们可以用你们期待已久的东西弄脏我们的手了。我知道这部分令人沮丧，但EDA和预处理是任何数据科学生命周期中最重要的部分</p></blockquote><h1 id="a491" class="mp mq it bd mr ms of mu mv mw og my mz na oh nc nd ne oi ng nh ni oj nk nl nm bi translated">2.预处理和模型训练</h1><figure class="no np nq nr gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ok"><img src="../Images/a0e1d5cc99ca682134f251be2e8a1490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fYxlQRIhLhLcZ6fu"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://unsplash.com/@kmuza?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卡洛斯·穆扎</a>在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="f9da" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这一部分中，我们将对我们的数据执行一些预处理步骤，并使用之前从EDA中获得的见解来训练我们的模型。</p><h2 id="cfe3" class="nx mq it bd mr ol om dn mv on oo dp mz la op oq nd le or os nh li ot ou nl iz bi translated">预处理</h2><p id="b768" class="pw-post-body-paragraph kp kq it kr b ks ov ku kv kw ow ky kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">要按照本部分中的代码操作，请打开train ipynb文件。所以，不再多说，让我们开始吧。</p><p id="2be4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">像往常一样导入所有的包并读取数据。我们将首先从真实数据<em class="lq">文本</em>栏中删除路透社。由于有些行没有路透社，所以我们将首先获得这些指数。</p><p id="89bc" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">从文本中删除路透社或推特信息</p><ul class=""><li id="e6dd" class="lu lv it kr b ks kt kw kx la lw le lx li ly lm pa ma mb mc bi translated">文本只能在“—”处拆分一次，它总是出现在提及出版物来源之后，这给了我们出版物部分和文本部分</li><li id="bc86" class="lu lv it kr b ks md kw me la mf le mg li mh lm pa ma mb mc bi translated">如果我们没有得到文本部分，这意味着没有给出该记录的出版细节</li><li id="9b9e" class="lu lv it kr b ks md kw me la mf le mg li mh lm pa ma mb mc bi translated">Twitter上的推文总是有相同的来源，一个最长259个字符的长文本</li></ul><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="07dd" class="nx mq it nt b gy ny nz l oa ob"><em class="lq">#First Creating list of index that do not have publication part</em><br/>unknown_publishers = []<br/><strong class="nt jd">for</strong> index,row <strong class="nt jd">in</strong> enumerate(real.text.values):<br/>    <strong class="nt jd">try</strong>:<br/>        record = row.split(" -", maxsplit=1)<br/>        <em class="lq">#if no text part is present, following will give error</em><br/>        record[1]<br/>        <em class="lq">#if len of publication part is greater than 260</em><br/>        <em class="lq">#following will give error, ensuring no text having "-" in between is counted</em><br/>        <strong class="nt jd">assert</strong>(len(record[0]) &lt; 260)<br/>    <strong class="nt jd">except</strong>:<br/>        unknown_publishers.append(index)</span></pre><p id="6ddf" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">用一行代码总结一下，上面的代码所做的是获取真实数据集中不存在发布者的<strong class="kr jd"> <em class="lq"> text </em> </strong>列的索引。</p><p id="b8e0" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们将把路透社从文本栏中分离出来。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="15fb" class="nx mq it nt b gy ny nz l oa ob"><em class="lq"># separating publishers from the news text</em><br/>publisher = []<br/>tmp_text = []<br/>for index,row <strong class="nt jd">in</strong> enumerate(real.text.values):<br/>    if index <strong class="nt jd">in</strong> unknown_publishers:<br/>        <em class="lq">#add text to tmp_text and "unknown" to publisher</em><br/>        tmp_text.append(row)<br/>        <br/>        publisher.append("Unknown")<br/>        continue<br/>    record = row.split(" -", maxsplit=1)<br/>    publisher.append(record[0])<br/>    tmp_text.append(record[1])</span></pre><p id="32b5" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在上面的代码中，我们遍历text列并检查index是否属于，如果是，那么我们将文本添加到publishers列表中。否则，我们将文本分为出版商和新闻文本，并添加到各自的列表中。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="42f2" class="nx mq it nt b gy ny nz l oa ob"><em class="lq">#Replace existing text column with new text</em><br/><em class="lq">#add seperate column for publication info</em><br/>real["publisher"] = publisher<br/>real["text"] = tmp_text</span></pre><p id="97e4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">上面的代码非常简单明了，我们添加了一个新的publisher列，并用不带Reuter的新闻文本替换了text列。</p><p id="954c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们现在将检查真实和虚假新闻数据集中的文本列中是否有任何缺失值，并删除该行。</p><p id="2d5d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们检查假新闻数据集，我们会看到有许多行缺少文本值，整个新闻都出现在<code class="fe oc od oe nt b">title</code>列中，因此我们将合并<code class="fe oc od oe nt b">title</code>和<code class="fe oc od oe nt b">text</code>列。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="2eab" class="nx mq it nt b gy ny nz l oa ob">real['text'] = real['text'] + " " + real['title']<br/>fake['text'] = fake['text'] + " " + fake['title']</span></pre><p id="d330" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">接下来，我们将向数据集添加类，删除不必要的列，并将数据合并为一个。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="0aff" class="nx mq it nt b gy ny nz l oa ob"><em class="lq"># Adding class info</em> <br/>real['class'] = 1 <br/>fake['class'] = 0</span><span id="3e83" class="nx mq it nt b gy pb nz l oa ob"><em class="lq"># Subject is diffrent for real and fake thus dropping it</em> <em class="lq"># Also dropping Date, title and Publication</em> real.drop(["subject", "date","title",  "publisher"], axis=1, inplace=<strong class="nt jd">True</strong>) fake.drop(["subject", "date", "title"], axis=1, inplace=<strong class="nt jd">True</strong>)</span><span id="68de" class="nx mq it nt b gy pb nz l oa ob"><em class="lq">#Combining both into new dataframe</em> data = real.append(fake, ignore_index=<strong class="nt jd">True</strong>)</span></pre><p id="7ac9" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">删除停用字词、标点符号和单字符字词。(在任何NLP项目中非常常见和基本的任务)。</p><h2 id="a145" class="nx mq it bd mr ol om dn mv on oo dp mz la op oq nd le or os nh li ot ou nl iz bi translated">模特培训</h2><h2 id="b41f" class="nx mq it bd mr ol om dn mv on oo dp mz la op oq nd le or os nh li ot ou nl iz bi translated">矢量化:Word2Vec</h2><figure class="no np nq nr gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi pc"><img src="../Images/dbf4175958b7ae5ee0eac2757b509a7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ISIPE5ltif1jfLQA"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">尼克·莫瑞森在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="c801" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Word2Vec是使用浅层神经网络学习单词嵌入的最流行的技术之一。它是由托马斯·米科洛夫于2013年在谷歌开发的。单词嵌入是最流行的文档词汇表示。它能够捕捉文档中单词的上下文、语义和句法相似性、与其他单词的关系等。</p><p id="3c4a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你想了解更多，点击<a class="ae ko" rel="noopener" target="_blank" href="/introduction-to-word-embedding-and-word2vec-652d0c2060fa"> <strong class="kr jd"> <em class="lq">这里</em> </strong> </a></p><p id="78b3" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们创建我们的Word2Vec模型。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="d227" class="nx mq it nt b gy ny nz l oa ob">#install gensim if you haven't already<br/>#!pip install gensim<br/>import gensim</span><span id="3d02" class="nx mq it nt b gy pb nz l oa ob"><em class="lq">#Dimension of vectors we are generating</em><br/>EMBEDDING_DIM = 100<br/><em class="lq">#Creating Word Vectors by Word2Vec Method</em><br/>w2v_model = gensim.models.Word2Vec(sentences=X, size=EMBEDDING_DIM, window=5, min_count=1)</span><span id="e473" class="nx mq it nt b gy pb nz l oa ob"><em class="lq">#vocab size</em><br/>len(w2v_model.wv.vocab)<br/><em class="lq">#We have now represented each of 122248 words by a 100dim vector.</em></span></pre><p id="ca98" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些向量将被传递给LSTM/GRU，而不是单词。1D-CNN可以进一步用于从向量中提取特征。</p><p id="1984" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Keras有一个名为“<strong class="kr jd">嵌入层</strong>的实现，它将创建单词嵌入(向量)。因为我们是用gensim的word2vec做的，所以我们会将这些向量加载到嵌入层中，并使该层不可训练。</p><p id="d50f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们不能将字符串传递给嵌入层，因此需要用数字来表示每个单词。</p><p id="e5d9" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">记号赋予器可以用数字来表示每个单词</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="2f33" class="nx mq it nt b gy ny nz l oa ob"><em class="lq"># Tokenizing Text -&gt; Repsesenting each word by a number</em><br/><em class="lq"># Mapping of orginal word to number is preserved in word_index property of tokenizer</em></span><span id="4d3c" class="nx mq it nt b gy pb nz l oa ob"><em class="lq">#Tokenized applies basic processing like changing it yo lower case, explicitely setting that as False</em><br/>tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(X)</span><span id="99af" class="nx mq it nt b gy pb nz l oa ob">X = tokenizer.texts_to_sequences(X)</span></pre><p id="22d8" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们创建了单词索引和向量之间的映射矩阵。我们用它作为嵌入层的权重。嵌入层接受单词的数字符号，并向内层输出相应的向量。它向下一层发送一个零向量，用于将被标记为0的未知单词。嵌入层的输入长度是每个新闻的长度(由于填充和截断，现在是700)。</p><p id="8cc3" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我们将创建一个序列神经网络模型，并在嵌入层中添加从w2v生成的权重，还添加一个LSTM层。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="e91b" class="nx mq it nt b gy ny nz l oa ob"><em class="lq">#Defining Neural Network</em><br/>model = Sequential()<br/><em class="lq">#Non-trainable embeddidng layer</em><br/>model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))<br/><em class="lq">#LSTM </em><br/>model.add(LSTM(units=128))<br/>model.add(Dense(1, activation='sigmoid'))<br/>model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])</span></pre><p id="9abd" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在让我们使用<code class="fe oc od oe nt b">sklearn train_test_split</code>方法将数据集分成训练集和测试集。</p><p id="c52d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们使用<code class="fe oc od oe nt b">model.fit(X_train, y_train, validation_split=0.3, epochs=6)</code>来训练模型。这需要一些时间，在我的机器上花了大约40分钟，所以坐下来喝点咖啡，放松一下。</p><p id="b654" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">训练完成后，我们将在<code class="fe oc od oe nt b">test</code>数据集上进行测试，并使用<code class="fe oc od oe nt b">classification_report()</code>方法生成报告。</p><p id="39cf" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">哇，我们获得了99%的准确性，具有良好的精确度和召回率，因此我们的模型看起来很好，现在让我们将它保存在磁盘上，以便我们可以在我们的web应用程序中使用它。</p><h1 id="c4b8" class="mp mq it bd mr ms of mu mv mw og my mz na oh nc nd ne oi ng nh ni oj nk nl nm bi translated">3.构建和部署web应用程序</h1><figure class="no np nq nr gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi pd"><img src="../Images/2419e1bf9cc5e5e8c3b6f5338f82f44c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TACpyjz3mWgYUfxo"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">照片由<a class="ae ko" href="https://unsplash.com/@hishahadat?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">沙哈达特·拉赫曼</a>在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="05fe" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这一部分，我不会讲太多的细节，我建议你仔细阅读我的代码，它非常容易理解。如果你一直坚持到现在，你必须有相同的目录结构，如果没有，那么只需改变<code class="fe oc od oe nt b">app.py</code>文件中的路径变量。</p><p id="9dc2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在将整个目录上传到GitHub存储库中。</p><p id="ede2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将在Heroku上托管我们的web应用程序。如果你还没有，在Heroku上创建一个免费帐户，然后:</p><ol class=""><li id="9277" class="lu lv it kr b ks kt kw kx la lw le lx li ly lm lz ma mb mc bi translated">点击创建新应用程序</li><li id="b0ff" class="lu lv it kr b ks md kw me la mf le mg li mh lm lz ma mb mc bi translated">然后选择一个名称</li><li id="39f4" class="lu lv it kr b ks md kw me la mf le mg li mh lm lz ma mb mc bi translated">选择GitHub，然后选择您想要保留的存储库</li><li id="7049" class="lu lv it kr b ks md kw me la mf le mg li mh lm lz ma mb mc bi translated">点击部署。</li></ol><p id="1cd6" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">嘣，它完成了，你的假新闻分类器现在是活的。</p><h1 id="5591" class="mp mq it bd mr ms of mu mv mw og my mz na oh nc nd ne oi ng nh ni oj nk nl nm bi translated">结论…</h1><p id="9f20" class="pw-post-body-paragraph kp kq it kr b ks ov ku kv kw ow ky kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">如果您已经完成了，那么恭喜您，现在您可以构建和部署一个复杂的机器学习应用程序了。</p><p id="6cd2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我知道这很难理解，但你能走到这一步还是值得称赞的。</p><blockquote class="ln lo lp"><p id="6405" class="kp kq lq kr b ks kt ku kv kw kx ky kz lr lb lc ld ls lf lg lh lt lj lk ll lm im bi translated"><strong class="kr jd">注意:</strong>该应用程序适用于大多数新闻，只需记住粘贴整段新闻，最好是美国新闻，因为数据集被限制为美国新闻。</p></blockquote><p id="9475" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们还没有见过面，我是<strong class="kr jd"> <em class="lq"> Eish Kumar </em> </strong>你可以在Linkedin上关注我:<a class="ae ko" href="https://www.linkedin.com/in/eish-kumar/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/eish-kumar/</a>。</p><p id="beba" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">关注我更多这样的文章。</p></div></div>    
</body>
</html>