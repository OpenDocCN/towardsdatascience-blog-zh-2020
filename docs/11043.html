<html>
<head>
<title>Interpretable-AI: Use-Case with Health Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释人工智能:健康数据用例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-ai-use-case-1-bc2abd7b197a?source=collection_archive---------54-----------------------#2020-07-31">https://towardsdatascience.com/explainable-ai-use-case-1-bc2abd7b197a?source=collection_archive---------54-----------------------#2020-07-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div class="gh gi io"><img src="../Images/b341f75b07fe2339c3447d2fba0810e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/0*coEQTHMrgAqN2Vva.jpg"/></div><p class="iv iw gj gh gi ix iy bd b be z dk translated">图片来源:Research @ Infosys Labs (Infosys 博客)</p></figure><div class=""/><p id="80c3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">免责声明:为了更好地理解这篇文章的背景，阅读我之前关于可解释人工智能的文章</em><a class="ae kx" href="https://medium.com/swlh/explainable-ai-where-supervised-learning-can-falter-edf2b562845a" rel="noopener"><em class="kw"/></a><em class="kw">会很有帮助。如果您以前用 Python 编写过逻辑回归代码，这也会很有帮助，因为我在代码中提到过它。</em></p><p id="ba69" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我之前关于可解释人工智能的文章中，我提到过高级监督学习算法，如逻辑回归和神经网络(及其变体，如卷积神经网络和递归神经网络)，很难解释。特别是神经网络，深度学习的一部分，可以产生惊人的准确性，但是程序员和用户对算法如何产生分类知之甚少。部分原因是神经网络和逻辑回归努力创建与其预测的直接关系，但这是有代价的:<em class="kw">可解释性</em>。</p><p id="a46f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我之前提到过，理解分类问题的另一种方式是使用贝叶斯定理来推断我们的两个假设——H1(类别标签 1)和 H2(类别标签 0)。作为参考，下面是等式:</p><figure class="kz la lb lc gt is gh gi paragraph-image"><div class="gh gi ky"><img src="../Images/083be4ad198ccc33d0a2a35ff5b70cea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*Em9og6sBA6hJ5qPq-_5Uzw.jpeg"/></div><p class="iv iw gj gh gi ix iy bd b be z dk translated">两个假设的贝叶斯推断</p></figure><p id="5667" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在今天的用例中，我将把这个推理框架应用于加州大学欧文分校的机器学习知识库中的肝细胞癌数据集。该数据集包括在马德里大学接受治疗的 165 名患者，以及每个患者的近 50 个个体特征/测量值。类别标签表明患者是否从癌症中存活(1)或是否不幸不能存活(0)。</p><p id="dfec" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们开始吧！首先，我们需要导入相关的包。随着代码的进展，我们将导入更多的库。由于数据集和代码较长，这是<a class="ae kx" href="https://github.com/galaxyenby/bayesian-inference/blob/master/Carcinoma%20Prediction%20with%20Generative%20Modeling.ipynb" rel="noopener ugc nofollow" target="_blank">我的 GitHub repo，完整代码</a>。我将在这里张贴片段。</p><pre class="kz la lb lc gt ld le lf lg aw lh bi"><span id="8b7f" class="li lj jb le b gy lk ll l lm ln"># First, we read in the file and import packages<br/>import numpy as np<br/>import pandas as pd<br/>from sklearn.linear_model import LogisticRegression<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn.metrics import confusion_matrix as cmatrix </span><span id="15d6" class="li lj jb le b gy lo ll l lm ln">hcc_data = pd.read_csv('/Users/rabeya/Carcinoma_Classification_data/hcc.csv')</span></pre><figure class="kz la lb lc gt is"><div class="bz fp l di"><div class="lp lq l"/></div><p class="iv iw gj gh gi ix iy bd b be z dk translated">加载到 HCC 数据集中。</p></figure><p id="34a8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，在我们加载数据后，我们使用 Miriam Santos 在 HCC 数据集上的原始论文中的算法，利用“HEOM-distance”填充所有标有“？”的缺失值。这种“HEOM-distance”算法采用每个患者记录，并根据与其“最接近”的患者记录来填充缺失值；正如你所猜测的，“近”的概念是基于超远距离的。因为这个算法有很多行，所以我把你引向我的 Github repo。填充的数据集被命名为<em class="kw"> new_df </em>。</p><p id="28a0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦我填充完缺失的值，我需要平衡标签。标签不平衡是机器学习中的一个大问题，由于收集中的偏差或缺失值，标签 1 与标签 2 的记录比率可能不是 50%:50%。事实上，在 HCC 数据集中，患者生命记录与患者死亡记录的比率是 62%:38%，因此我们需要平衡这些记录。一种方法被称为“人工过采样”，我们为更小的类人工创建新的数据记录，这些记录与原始记录接近。这种过采样方法叫做“SMOTE”，代码如下。</p><pre class="kz la lb lc gt ld le lf lg aw lh bi"><span id="c428" class="li lj jb le b gy lk ll l lm ln"><strong class="le jc">from</strong> <strong class="le jc">imblearn.over_sampling</strong> <strong class="le jc">import</strong> SMOTE<br/>sme = SMOTE(random_state=10)<br/><br/><em class="kw">#features</em><br/>features = new_df.columns<br/><em class="kw"># transform the dataset</em><br/>X_matrix = new_df[features[:-1]].values<br/>y_matrix = new_df[features[-1]].values<br/>X_trans, y_trans = sme.fit_resample(X_matrix, y_matrix)</span><span id="150a" class="li lj jb le b gy lo ll l lm ln">transformed_df = pd.DataFrame(X_trans, columns=features[:-1])<br/>transformed_df['Class'] = y_trans</span></pre><p id="fa4d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们已经转换了数据集并平衡了类(有 102 个病人-生命和病人-死亡样本)，我们准备好构建我们的生成分类器了！(我们的平衡 HCC 数据集在接下来的部分中被称为<em class="kw">转换 _df </em>。)</p><p id="9d0c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在创建任何模型之前，我们减少了数据集中的要素数量。最初的一个有 49 个单独的特征/对于每个病人，那是太多要考虑的了！因此，相反，我们使用统计学中所谓的<a class="ae kx" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#r50b872b699c4-1" rel="noopener ugc nofollow" target="_blank"> <em class="kw">互信息</em> </a>将它削减到“最佳”10 个特征。</p><p id="7f7c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我将逻辑回归作为测试模型案例应用于训练和测试数据。我将训练集和测试集分成:65%给训练集，35%给测试集。这不是标准的，但是我这样做了，因为我们的整个集合只有 200 条记录，所以必须节约使用数据。</p><pre class="kz la lb lc gt ld le lf lg aw lh bi"><span id="bafa" class="li lj jb le b gy lk ll l lm ln"><strong class="le jc">from</strong> <strong class="le jc">sklearn.feature_selection</strong> <strong class="le jc">import</strong> SelectKBest<br/><strong class="le jc">from</strong> <strong class="le jc">sklearn.feature_selection</strong> <strong class="le jc">import</strong> mutual_info_classif</span><span id="e40a" class="li lj jb le b gy lo ll l lm ln">K = 10<br/>selector = SelectKBest(mutual_info_classif, k=K)<br/>X = transformed_df.iloc[:, :-1]<br/>y = transformed_df.iloc[:, -1]<br/>X_reduced = selector.fit_transform(X,y)</span><span id="9590" class="li lj jb le b gy lo ll l lm ln"><em class="kw"># use logistic regression as a test case model</em><br/>logreg = LogisticRegression(C=1.5, max_iter=1000, solver='lbfgs')<br/>logreg.fit(X_train, y_train)</span></pre><p id="0652" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对看不见的数据使用逻辑回归，预测准确度是:真阳性-36%；真阴性——30%；假阳性——22%；假阴性——12%。逻辑回归在预测如此小的数据集方面做得不太好，22%的 FPR 用于未来预测是危险的，因为五分之一的未来新 HCC 患者将被<em class="kw">预测</em>存活，而他们的<em class="kw">实际</em>存活机会将很低。这种糟糕的预测能力并不令人惊讶，因为我们只给出了大约 130 条记录供模型训练。</p><p id="0c24" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里有许多解决方案:1)收集更多的数据并将其转储，但 HCC 的数据来之不易。2)使用更多的参数/权重，例如用于小数据的神经网络模型。但是正如我们之前讨论的，神经网络很难解释。我采取的解决方案是完全改变预测的视角，并使用生成模型来代替。正如我提到的，生成模型可以帮助我们计算<em class="kw">可能的</em>数据在属于不同类别的假设下如何指向，这种<em class="kw">概率视角</em>对于小数据尤其有用。</p><figure class="kz la lb lc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/b03411ec3e800fb66e7594655e5ca984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F0bFBS-ZFmusicoE.png"/></div></div><p class="iv iw gj gh gi ix iy bd b be z dk translated">密度估计模型。图片来源:<a class="ae kx" href="https://machinelearningmastery.com/probability-density-estimation/" rel="noopener ugc nofollow" target="_blank">machinelearningmastery.com</a></p></figure><p id="baa6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，我们可以创建我们的生成模型。第一种类型称为<a class="ae kx" href="https://www.youtube.com/watch?v=W0XECm4-3LI" rel="noopener ugc nofollow" target="_blank"> <em class="kw">高斯混合模型</em> </a> <em class="kw"> </em> (GMM)，其中一堆单独的高斯模型(称为“组件”)被捆绑在一起，以创建一个大的生成模型。我们根据患者是死于癌症还是幸存来分离我们的训练数据，然后拟合模型。为了适应模型，我们不能只选择一个#的组件。相反，我训练了 64 个模型(计算机可以比你想象的更快地执行这个操作)，并使用<em class="kw">期望最大化</em> (EM)算法优化了这些模型。EM 算法已有几十年的历史，但经常用于寻找高斯混合。</p><figure class="kz la lb lc gt is gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/06a091198969efbf060225b74c128865.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/0*Sj3oa4MiTls_SF41.gif"/></div><p class="iv iw gj gh gi ix iy bd b be z dk translated">高斯混合(EM)在工作。图片来源:<a class="ae kx" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="a857" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，我根据一种叫做<a class="ae kx" href="https://en.wikipedia.org/wiki/Akaike_information_criterion" rel="noopener ugc nofollow" target="_blank"> <em class="kw">阿凯克信息标准</em> (AIC) </a>的统计方法，挑选出了最佳数字 22 和 21。它基本上比较了密度模型的精确程度和复杂程度，并试图在模型过拟合和欠拟合之间寻找平衡。</p><pre class="kz la lb lc gt ld le lf lg aw lh bi"><span id="371a" class="li lj jb le b gy lk ll l lm ln"><strong class="le jc">from</strong> <strong class="le jc">sklearn.mixture</strong> <strong class="le jc">import</strong> GaussianMixture <strong class="le jc">as</strong> GMM<br/>M = 64<br/><em class="kw"># so M models to train</em><br/>n_components = np.arange(2, M, 1)<br/><em class="kw"> </em><br/>life_models = [GMM(n, covariance_type='full', random_state=5) <strong class="le jc">for</strong> n <strong class="le jc">in</strong> n_components]</span><span id="693b" class="li lj jb le b gy lo ll l lm ln">death_models = [GMM(n, covariance_type='full', random_state=7) <strong class="le jc">for</strong> n <strong class="le jc">in</strong> n_components]</span><span id="766a" class="li lj jb le b gy lo ll l lm ln">life_aics = [model.fit(X_train_life).aic(X_train_life) <strong class="le jc">for</strong> model <strong class="le jc">in</strong> life_models]</span><span id="2e92" class="li lj jb le b gy lo ll l lm ln">death_aics = [model.fit(X_train_death).aic(X_train_death) <strong class="le jc">for</strong> model <strong class="le jc">in</strong> death_models]</span><span id="69bd" class="li lj jb le b gy lo ll l lm ln">gmm_life = GMM(22, covariance_type='tied',tol=1e-4, random_state=5)<br/>gmm_life.fit(X_train_life)</span><span id="1d74" class="li lj jb le b gy lo ll l lm ln">gmm_death = GMM(21, covariance_type='tied',tol=1e-4, random_state=7)<br/>gmm_death.fit(X_train_death)</span></pre><p id="586f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对看不见的数据使用高斯混合模型的准确度是:35%真阴性、12%假阳性、15%假阴性和 37%真阳性。与逻辑回归相比，假阳性率有了多大的提高！我们实际上可以通过使用类固醇上的高斯混合模型的变体在假阳性率上做得更好:<a class="ae kx" href="https://www.youtube.com/watch?v=W0XECm4-3LI" rel="noopener ugc nofollow" target="_blank"> <em class="kw">核密度估计</em> (KDE) </a>。这听起来肯定像是人工智能科幻小说中的东西，但它的工作原理是一样的！</p><pre class="kz la lb lc gt ld le lf lg aw lh bi"><span id="2dc4" class="li lj jb le b gy lk ll l lm ln"><strong class="le jc">from</strong> <strong class="le jc">sklearn.neighbors</strong> <strong class="le jc">import</strong> KernelDensity</span><span id="0e8d" class="li lj jb le b gy lo ll l lm ln">kde_life = KernelDensity(bandwidth=0.1).fit(X_train_life)<br/>kde_death = KernelDensity(bandwidth=0.1).fit(X_train_death)<br/><br/>log_density_life = kde_life.score_samples(X_test)<br/>log_density_death = kde_death.score_samples(X_test)</span></pre><p id="eae7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我们对看不见的数据使用 KDE 方法时，我们的预测精度是:44%真阴性，8%假阳性，2%假阴性和 46%真阳性。只在 130 张唱片上接受过训练的 KDE 在预测能力上做得非常出色。(顺便说一句，GMM 和 KDE 需要记住的是，让密度模型过度适应训练数据。我将在以后的文章中探讨这个问题。)</p><p id="00e8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好的，这一切都很好，但是我们如何<em class="kw">解释</em>密度模型呢？可解释性很重要。好吧，如果来自测试集的一个新的数据点病人 Y 进来，机器可以这样解释它的预测:“因为类别标签(1 =生存，0 =死亡)是平衡的，我的先验比率等于 1。因此，我的决定只基于我根据之前的 130 条训练记录计算出的密度似然模型。根据我的生存可能性模型，如果我假设 Y 属于 1 类，它的对数可能性是 p1。根据我的死亡可能性模型，如果我假设 Y 属于 0 类，它的对数可能性是 p2。因为我计算出，比如说，(p1-p2) &gt; 0，这意味着病人 Y 的数据记录被生成的概率——假设它属于类 1——大于类 0。因此，我根据他们的 10 个‘最佳’计算特征对癌症存活患者 Y 进行分类。”这是代码的实际样子:</p><pre class="kz la lb lc gt ld le lf lg aw lh bi"><span id="c1dd" class="li lj jb le b gy lk ll l lm ln">delta = np.subtract(log_density_life, log_density_death)<br/># this is the difference in likelihoods under the hypothesis  assumptions (H1: label=1, H2: label=0)</span><span id="c3a1" class="li lj jb le b gy lo ll l lm ln">kde_predictions = np.asarray([1 <strong class="le jc">if</strong> x &gt; 0 <strong class="le jc">else</strong> 0 <strong class="le jc">for</strong> x <strong class="le jc">in</strong> delta])</span></pre><p id="1423" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当然，机器不能和我们说话，但我相信这就是我们如何理解机器学习模型的决策。看看这比使用神经网络或逻辑回归更有解释力？</p><p id="d171" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但不仅仅如此。假设我们有一个更大的像这样的不平衡的数据集。我们可以在贝叶斯推理公式的<em class="kw">先验比</em>中考虑生存和死亡标签的比例。如果我们计算 sam 似然密度模型，并发现对新测试点的预测是错误的，我们可以<em class="kw">潜在地</em> <em class="kw">修改模型</em>。要么我们可以修改我们的类标签的先验概率，要么我们可以修改似然模型。怎么会？通过在<em class="kw">可能性模型本身上使用贝叶斯框架。例如，这可以用概率编程语言如 PYMC3 来动态完成。</em></p><p id="7bfe" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我希望这些用于医疗保健数据的结果提供了一种途径，不仅可以将密度估计用于聚类客户模式，还可以像我们在这个项目中所做的那样，将它们用于预测能力。我将指出，实验者没有考虑的一个特征是种族/民族。我可以理解，因为数据集是在西班牙收集的，所以种族会相当单一。但是在数据收集中忽略种族，不批判性地思考其他因素，如社会经济阶层和获得医疗保健的机会，是一件非常冒险和危险的事情。但是希望我能找到更多的 HCC 数据集(或类似的),这些数据集也考虑了这些因素。</p><p id="f14f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一会儿见！:)</p></div></div>    
</body>
</html>