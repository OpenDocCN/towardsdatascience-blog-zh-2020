<html>
<head>
<title>A reading guide about Deep Learning with CNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CNN深度学习阅读指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-reading-guide-about-deep-learning-with-cnns-71768f4d87e7?source=collection_archive---------58-----------------------#2020-06-08">https://towardsdatascience.com/a-reading-guide-about-deep-learning-with-cnns-71768f4d87e7?source=collection_archive---------58-----------------------#2020-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bb56" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第二部分:图像分割</h2></div><p id="61b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">欢迎回到本系列的第二部分。如果你错过了第一部分，看看这里:<strong class="kk iu"> </strong> <a class="ae le" rel="noopener" target="_blank" href="/a-reading-guide-about-deep-learning-with-cnns-3a0e0fc99b78"> <strong class="kk iu">第一部分:图像识别与卷积骨干</strong> </a> <strong class="kk iu">。</strong></p><p id="4db7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这一部分中，您将通过有关卷积神经网络(CNN)图像分割的文献找到指南，直到2019年。它在<a class="ae le" href="https://www.mdpi.com/2072-4292/12/10/1667" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">这篇开放存取综述论文</strong> </a>中增加了非科学来源，以进一步增加对CNN进化的直观理解。</p><p id="e162" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与第一部分相同，您可以在github资源库中找到源代码表:</p><div class="lf lg gp gr lh li"><a href="https://github.com/thho/CNN_reading_guide" rel="noopener  ugc nofollow" target="_blank"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">thho/CNN _阅读_指南</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">这份阅读指南发表在媒体故事系列中，是一份关于CNN深度学习的阅读指南。它补充道…</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">github.com</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw lx li"/></div></div></a></div><p id="4c4f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们和CNN一起进入深度学习冒险的下一个篇章。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="9ad4" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">基于细胞神经网络的图像分割概述</h1><p id="857b" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">在图像分割过程中，对每个像素预测一个类别，如下所示:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nc"><img src="../Images/803ddbb123687db9874394fbd3f80b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cz9OSWUK80OYQqjw6_xiCA.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">图像分割示例。修改依据:<a class="ae le" href="https://www.mdpi.com/2072-4292/12/10/1667" rel="noopener ugc nofollow" target="_blank"> Hoeser and Kuenzer 2020第8页【1】</a></p></figure><p id="52b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们在第一部分中讨论的CNN<a class="ae le" rel="noopener" target="_blank" href="/a-reading-guide-about-deep-learning-with-cnns-3a0e0fc99b78">变得更加流行时，它们首先被用于所谓的<em class="nr">基于小块的图像分割。</em>因此，CNN以移动窗口方式在输入图像上移动，并预测<em class="nr">小块</em>(整个图像的一小部分)或整个小块的中心像素的类别。</a></p><p id="9808" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着Long等人2014年[2]的工作，所谓的全卷积网络(FCNs)被引入，使用CNN的图像分割变得更加复杂。总的来说，FCNs中的处理看起来是这样的:首先通过使用卷积骨干从输入图像中提取特征(<strong class="kk iu">编码器</strong>，<a class="ae le" rel="noopener" target="_blank" href="/a-reading-guide-about-deep-learning-with-cnns-3a0e0fc99b78">参见第一部分</a>)。因此，分辨率越来越小，而特征深度越来越大。这样提取的特征图具有高语义，但是没有精确的定位。由于我们需要对图像分割进行逐像素预测，因此该特征图会被向上采样回输入分辨率<strong class="kk iu">(解码器)</strong>。与输入图像的不同之处在于，每个像素持有一个离散的类别标签，因此图像被分割成语义上有意义的类别。</p><p id="8da5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">存在两种主要的不同概念，即如何在解码器中进行上采样:</p><ul class=""><li id="fe81" class="ns nt it kk b kl km ko kp kr nu kv nv kz nw ld nx ny nz oa bi translated"><strong class="kk iu">朴素解码器</strong>(该术语例如在Chen等人2018 [3]中使用):通过应用例如双线性插值来完成上采样</li><li id="25fe" class="ns nt it kk b kl ob ko oc kr od kv oe kz of ld nx ny nz oa bi translated"><strong class="kk iu">编码器-解码器:</strong>上采样是通过可训练的去卷积操作和/或通过在上采样期间将来自编码器部分的特征与更高的定位信息合并来完成的，参见那些例子:</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi og"><img src="../Images/4d1f0c9682f39bab4d7d787e309d9a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BWfmQ1MjWqurWOVXSuOnKA.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated"><a class="ae le" href="https://www.mdpi.com/2072-4292/12/10/1667" rel="noopener ugc nofollow" target="_blank">资料来源:Hoeser和Kuenzer 2020年第17页【1】</a></p></figure><p id="3200" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了利用深度学习深入研究图像分割，下表中的来源是很好的起点。请注意，除了CNN之外，还有其他执行图像分割的深度学习模型类型，如生成对抗网络(GANs)或长短期记忆(LSTM)方法；本指南主要关注CNN。此外，有时从图像分割的角度讨论R-CNN家族的模型。本指南将在下一部分讨论物体检测时讨论它们。所以，当你在别的地方读到它们(比如在评论文章中)而这里没有提到它们时，不要感到困惑。</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h1 id="84bc" class="mf mg it bd mh mi oj mk ml mm ok mo mp jz ol ka mr kc om kd mt kf on kg mv mw bi translated">模糊神经网络在图像分割中的发展</h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oo"><img src="../Images/270a79dc30baa0fbdbb7b8eb248f2988.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*spHssA1DCYwQMIjr79O9Qg.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">PASCAL-VOC 2012基准数据集上不同FCN启发的里程碑式架构的性能演变概述。*这些模型在其他数据集上进行了测试。资料来源:Hoeser和Kuenzer，2020年，第17页[1]</p></figure><p id="ba3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DeepLab家族的发展是FCN启发的图像分割模型发展的特征。DeepLab变体可以在naive-decoder和encoder-decoder模型中找到。因此，本指南首先着眼于简单的解码器，然后转向编码器-解码器模型，从而对这一系列进行定位。</p><h2 id="ed83" class="op mg it bd mh oq or dn ml os ot dp mp kr ou ov mr kv ow ox mt kz oy oz mv pa bi translated">朴素解码器模型</h2><p id="4d26" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">朴素解码器模型的最重要的见解主要是建立所谓的atrous卷积和用于像素级预测的长距离图像上下文开发。阿特鲁卷积是普通卷积的一种变体，它允许在不损失图像分辨率的情况下增加感受野。DeepLab-V2 [4]中著名的阿特鲁空间金字塔池模块(<strong class="kk iu"> ASPP模块</strong>)以及后来的组合:Atrous卷积和长距离图像上下文开发。阅读以下文献时，请关注这些功能的发展——阿特鲁卷积、ASPP模块和远程图像上下文开发/解析。</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h2 id="b962" class="op mg it bd mh oq or dn ml os ot dp mp kr ou ov mr kv ow ox mt kz oy oz mv pa bi translated">编码器-解码器模型</h2><p id="2fac" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">今天，最著名的编码器-解码器可能是U-Net [5]。为分析医学图像而开发的CNN。其清晰的结构吸引了许多研究人员进行试验和采用，并以其跳跃连接而闻名，这允许编码器和解码器路径之间共享功能。编码器-解码器模型关注于在解码器中的上采样期间，利用来自编码器的更局部精确的特征映射来增强语义丰富的特征映射。</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="006b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了手头的文献，你将能够思考现代图像分割论文和CNN的实现。让我们在第三部分再次见面，我们将讨论对象检测。</p><h1 id="85e2" class="mf mg it bd mh mi oj mk ml mm ok mo mp jz ol ka mr kc om kd mt kf on kg mv mw bi translated">参考</h1><p id="87d9" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">[1]赫泽，T；利用对地观测数据的深度学习进行目标探测和图像分割:综述-第一部分:发展和最近趋势。遥感2020，12(10)，1667。DOI: 10.3390/rs12101667。</p><p id="7cf9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]龙，j；谢尔哈默，e；语义分割的完全卷积网络。IEEE Trans。肛门模式。马赫。智能。2014, 39, 640–651.</p><p id="a2ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]陈；朱；帕潘德里欧；施若夫，f；语义图像分割的阿特鲁可分卷积编码器-解码器。在计算机视觉领域——ECCV 2018；法拉利v，赫伯特m，斯明奇塞斯库c；韦斯，y，编辑。；施普林格国际出版公司:瑞士查姆，2018年；第833-851页</p><p id="5fb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4]陈；帕潘德里欧；科基诺斯岛；墨菲，k。用深度卷积网、阿特鲁卷积和全连接条件随机场进行语义图像分割。IEEE Trans。肛门模式。<br/>马赫。智能。2016, 40, 834–848.</p><p id="dbee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[5] Ronneberger，o；菲舍尔，p。生物医学图像分割的卷积网络。医学图像计算和计算机辅助介入——MICCAI 2015；Navab，Hornegger，j .，<br/> Wells，W.M .，Frangi，A.F .编辑；施普林格国际出版公司:瑞士查姆，2015年；第234-241页。</p></div></div>    
</body>
</html>