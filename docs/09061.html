<html>
<head>
<title>Machine Learning Explainability Introduction via eli5</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过 eli5 的机器学习可解释性介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-explainability-introduction-via-eli5-99c767f017e2?source=collection_archive---------38-----------------------#2020-06-29">https://towardsdatascience.com/machine-learning-explainability-introduction-via-eli5-99c767f017e2?source=collection_archive---------38-----------------------#2020-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5efc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从您的机器学习模型中提取洞察力</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ea67a70bb25d2176c2b1a3b40134264f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sZ1sQtg8PtFT2g56"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@heyerlein?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> h heyerlein </a>拍摄</p></figure><h1 id="80ba" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="bec4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">根据我作为数据科学家的工作经验，大多数时候，你需要解释为什么你的模型有效，以及你的模型给出了什么样的洞察力。就洞察力而言，我指的不是模型准确性或任何指标，而是机器学习模型本身。这就是我们所说的机器学习的可解释性。</p><p id="82d2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">机器学习可解释性最直接的例子是带有<a class="ae ky" href="https://en.wikipedia.org/wiki/Ordinary_least_squares" rel="noopener ugc nofollow" target="_blank">普通最小二乘</a>估计方法的<a class="ae ky" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">线性回归模型</a>。让我用一个数据集给你举个例子。请注意，我违反了一些普通的最小二乘假设，但我的观点不是创建最佳模型；我只是想有一个模型，可以提供一个洞察力。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="93a6" class="mx la it mt b gy my mz l na nb">#Importing the necessary package<br/>import pandas as pd<br/>import seaborn as sns<br/>import statsmodels.api as sm<br/>from statsmodels.api import OLS</span><span id="04fa" class="mx la it mt b gy nc mz l na nb">#Load the dataset and preparing the data<br/>mpg = sns.load_dataset('mpg')<br/>mpg.drop(['origin', 'name'], axis =1, inplace = True)<br/>mpg.dropna(inplace = True)</span><span id="7844" class="mx la it mt b gy nc mz l na nb">#Ordinary Least Square Linear Regression model Training<br/>sm_lm = OLS(mpg['mpg'], sm.add_constant(mpg.drop('mpg', axis = 1)))<br/>result = sm_lm.fit()</span><span id="c9c1" class="mx la it mt b gy nc mz l na nb">result.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/68f9271055a0aec56ce6399c11cd636d.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*ByCKDsF0FhLCJznkO06GNA.png"/></div></figure><p id="72fc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我不会详细解释该模型，但线性模型假设在<strong class="lt iu">自变量</strong>(要预测的特征)和<strong class="lt iu">因变量</strong>(要预测的内容)之间存在<strong class="lt iu">线性</strong>。等式如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/274c067ab391a7eab961e10038e0507f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lUfDyVscKTeIDJtVwRcq8g.png"/></div></figure><p id="eca7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里重要的是，每个自变量(x)都要乘以系数(m)。这意味着系数说明了自变量和因变量之间的关系。从上面的结果中，我们可以看到“model_year”变量的系数(coef)为 0.7534。这意味着‘model _ year’每增加 1，因变量‘mpg’值就会增加 0.7534。</p><p id="ea50" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">线性回归模型及其系数是机器学习可解释性的一个例子。模型本身用于解释我们的数据发生了什么，并且洞察的提取是可能的。然而，并不是所有模型都能做到这一点。</p></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="c2f8" class="kz la it bd lb lc nm le lf lg nn li lj jz no ka ll kc np kd ln kf nq kg lp lq bi translated">黑盒模型的机器学习可解释性</h1><h2 id="9601" class="mx la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated"><strong class="ak">基于树的特征重要性</strong></h2><p id="0393" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">随机森林等机器学习模型通常被视为黑盒。为什么？一个森林由大量的深层树组成，其中每棵树都使用随机选择的特征对袋装数据进行训练。通过检查每一棵树来获得全面的理解几乎是不可能的。</p><p id="70f0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">例如，xgboost 包中著名的 XGBoost 分类器几乎是一个利用随机森林过程的黑盒模型。这个模型被认为是一个黑盒模型，因为我们不知道在模型学习过程中发生了什么。让我们以同一个数据集为例来尝试一下。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="da3f" class="mx la it mt b gy my mz l na nb">#Preparing the model and the dataset</span><span id="a51f" class="mx la it mt b gy nc mz l na nb">from xgboost import XGBClassifier<br/>from sklearn.model_selection import train_test_split<br/>mpg = sns.load_dataset('mpg')<br/>mpg.drop('name', axis =1 , inplace = True)</span><span id="f080" class="mx la it mt b gy nc mz l na nb">#Data splitting for xgboost<br/>X_train, X_test, y_train, y_test = train_test_split(mpg.drop('origin', axis = 1), mpg['origin'], test_size = 0.2, random_state = 121)</span><span id="d300" class="mx la it mt b gy nc mz l na nb">#Model Training<br/>xgb_clf = XGBClassifier()<br/>xgb_clf.fit(X_train, y_train)</span></pre><p id="23ad" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">就这样，我们有了模型，但是我们从数据中得到了什么启示吗？或者我们能知道依赖者和独立者之间的关系吗？。</p><p id="c50f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你可能会说分类器拥有一个特征重要性方法，这是一个树模型，专门用来衡量特征的重要性。准确地说，它测量特征对模型的平均杂质减少的贡献。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="5f41" class="mx la it mt b gy my mz l na nb">tree_feature =  pd.Series(xgb_clf.feature_importances_, X_train.columns).sort_values(ascending = True)</span><span id="5e45" class="mx la it mt b gy nc mz l na nb">plt.barh(X_train.columns, tree_feature)<br/>plt.xlabel('Mean Impurity Reduction', fontsize = 12)<br/>plt.ylabel('Features', fontsize = 12)<br/>plt.yticks(fontsize = 12)<br/>plt.title('Feature Importances', fontsize = 20)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/93008e3bb9a707d8dbdf4865c07688fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*eAqw_wwoSaFoa5ax3R4f8A.png"/></div></figure><p id="97d8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在一定程度上，这是一个机器学习可解释的例子。但是，您需要记住 xgboost 依赖于引导过程来创建模型。也就是说，这个特征的重要性可能会因为随机过程而发生。此外，该贡献仅表明该特征可以减少多高的总杂质(总的来说是所有产生的树的平均值)。</p></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h2 id="9115" class="mx la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">通过 eli5 的排列重要性</h2><p id="b680" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">还有另一种方法可以从基于树的模型中获得洞察力，方法是逐个置换(改变位置)每个特征的值，并检查它如何改变模型性能。这就是我们所说的<strong class="lt iu">排列重要性</strong>方法。我们可以尝试使用<em class="od"> eli5 </em>包将这种方法应用到我们的 xgboost 分类器中。首先，我们需要使用下面的代码来安装这个包。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="45e2" class="mx la it mt b gy my mz l na nb">#installing eli5</span><span id="24dc" class="mx la it mt b gy nc mz l na nb">pip install eli5<br/>#or<br/>conda install -c conda-forge eli5</span></pre><p id="b256" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">安装后，我们将从现在开始使用<em class="od"> eli5 </em>包进行机器学习解释。让我们从排列重要性开始。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="21fb" class="mx la it mt b gy my mz l na nb">#Importing the module<br/>from eli5 import show_weights<br/>from eli5.sklearn import PermutationImportance</span><span id="afa5" class="mx la it mt b gy nc mz l na nb">#Permutation Importance<br/>perm = PermutationImportance(xgb_clf, scoring = 'accuracy' ,random_state=101).fit(X_test, y_test)<br/>show_weights(perm, feature_names = list(X_test.columns))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/00660a1bf986faf96db334a4d72aac02.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*GTnaslD9MrRf8rUSnwegwg.png"/></div></figure><p id="b2cd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">排列重要性背后的思想是如何评分(准确度、精确度、召回率等。)shift 随特征存在与否，在上面的结果中我们可以看到，displacement 得分最高，为 0.3797。这意味着，当我们置换位移特征时，它将改变模型的精度高达 0.3797。正负符号后的值是不确定度值。排列重要性方法本质上是一个随机过程；这就是为什么我们有不确定值。</p><p id="73cc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">位置越高，这些特征对得分的影响越大。底部的一些特征显示了一个负值，这很有趣，因为这意味着当我们改变特征时，这个特征增加了得分。发生这种情况是因为偶然的特征排列实际上提高了分数。</p><p id="3203" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过测量杂质减少和排列重要性，我们已经知道了这两种方法。它们是有用的，但在某种意义上说，它们是粗糙的和静态的，很难理解个人对实际数据的决策。这就是为什么我们会使用基于<a class="ae ky" href="http://blog.datadive.net/interpreting-random-forests/" rel="noopener ugc nofollow" target="_blank">树决策路径</a>的 eli5 权重特征重要性计算。</p></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h2 id="bbc2" class="mx la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">通过 eli5 确定树特征的重要性</h2><p id="45b3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们使用 eli5 来检查 XGBoost 分类器特性的重要性。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="b765" class="mx la it mt b gy my mz l na nb">show_weights(xgb_clf, importance_type = 'gain')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/9d1fff752fbf719f7e70d9f390c39b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*i7eMgFWp_bmp9F5xghIV6Q.png"/></div></figure><p id="923b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以看到“位移”特征是最重要的特征，但我们还不明白我们是如何得到重量的。因此，让我们看看分类器如何尝试预测单个数据。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="ddb3" class="mx la it mt b gy my mz l na nb">#Taking an example of test data</span><span id="68c9" class="mx la it mt b gy nc mz l na nb">from eli5 import show_prediction<br/>show_prediction(xgb_clf, X_test.iloc[1], show_feature_values=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/8f4a55334c84ee386137fce01b34cc61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIooDw4zZhvJx1o632FgIw.png"/></div></div></figure><p id="09d3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">上表显示了我们的分类器如何根据给定的数据预测我们的数据。每个类别都有自己的概率，以及每个特征对概率和分数的贡献(分数计算基于<a class="ae ky" href="http://blog.datadive.net/interpreting-random-forests/" rel="noopener ugc nofollow" target="_blank">决策路径</a>)。分类器还引入了<code class="fe oh oi oj mt b">&lt;BIAS&gt;</code>特征，它是模型基于训练集的分布输出的期望平均得分。如果你想了解更多，你可以查看这里的<a class="ae ky" href="https://medium.com/applied-data-science/new-r-package-the-xgboost-explainer-51dd7d1aa211" rel="noopener"/>。</p><p id="2f96" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">上表中最重要的是每个特征都对预测结果有贡献，因此特征贡献影响权重结果。权重是所有树中对最终预测有贡献的每个特征的百分比(如果将权重相加，它将接近 1)。</p><p id="28d4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有了这个软件包，我们不仅可以根据功能性能得分，还可以根据每个功能本身对决策过程的贡献来衡量功能的重要性。</p></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="c40d" class="kz la it bd lb lc nm le lf lg nn li lj jz no ka ll kc np kd ln kf nq kg lp lq bi translated">结论</h1><p id="e583" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">有了 eli5，我们能够将黑盒分类器转变为更易解释的模型。我们仍然有很多方法可以用于机器学习的解释目的，你可以在<a class="ae ky" href="https://eli5.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> eli5 </a>主页上查看。</p></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="b781" class="kz la it bd lb lc nm le lf lg nn li lj jz no ka ll kc np kd ln kf nq kg lp lq bi translated">如果您喜欢我的内容，并希望获得更多关于数据或数据科学家日常生活的深入知识，请考虑在此订阅我的<a class="ae ky" href="https://cornellius.substack.com/welcome" rel="noopener ugc nofollow" target="_blank">简讯。</a></h1><blockquote class="ok"><p id="8d42" class="ol om it bd on oo op oq or os ot mm dk translated">如果您没有订阅为中等会员，请考虑通过<a class="ae ky" href="https://cornelliusyudhawijaya.medium.com/membership" rel="noopener">我的推荐</a>订阅。</p></blockquote></div></div>    
</body>
</html>