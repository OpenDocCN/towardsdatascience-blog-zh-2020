<html>
<head>
<title>Model parallelism in one line of code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在一行代码中模拟并行性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-parallelism-in-one-line-of-code-352b7de5645a?source=collection_archive---------26-----------------------#2020-04-10">https://towardsdatascience.com/model-parallelism-in-one-line-of-code-352b7de5645a?source=collection_archive---------26-----------------------#2020-04-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="7725" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">利用多个GPU对大型图像进行训练</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/22ec28727cb9728ba7ecbab6eecdac53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kaGkjliSfhqPEV_P"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">保罗·德拉古纳斯在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="cb48" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">TL；DR；</h1><p id="8143" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">当由于内存限制而无法在单个GPU上运行训练时，应该使用模型并行。这种技术将模型分割到多个GPU上，并在不同的加速器上执行每个部分。通过这种方式，巨大的模型可以被拆分，以适应内存。</p><p id="9718" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种技术在过去实现起来相当复杂。模型的公平分割、对GPU的正确分配和高效执行是不容易实现的功能。</p><p id="2504" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lb" href="http://eisen.ai" rel="noopener ugc nofollow" target="_blank">艾森</a> 0.0.5在一行代码中实现了模型并行，因此对于任何使用不止一个GPU并苦于内存问题的人来说，模型并行都是显而易见的。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="e9d2" class="lc ld iq bd le lf mm lh li lj mn ll lm ln mo lp lq lr mp lt lu lv mq lx ly lz bi translated">更新！</h1><p id="02d3" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">主分支中的<a class="ae lb" href="https://github.com/eisen-ai/eisen-core" rel="noopener ugc nofollow" target="_blank">当前Eisen代码库</a>(以及 eisen==0.1.6和eisen-core==0.0.5之后的未来版本<strong class="jp ir">)已经更新，以进一步简化模型并行的实现方式。现在可以在没有Eisen工作流的情况下使用模型并行性(不要担心，您仍然可以像使用任何其他模型一样在工作流中使用它),其方式几乎与PyTorch中的数据并行性相同！</strong></p><p id="d8c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在它看起来真的像一行程序！</p><pre class="km kn ko kp gt mr ms mt mu aw mv bi"><span id="0c9a" class="mw ld iq ms b gy mx my l mz na">model = eisen.utils.ModelParallel(model, split_size=2)</span></pre></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><p id="8637" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以前处理过体积医学数据(CT、MRI等)。)我的研究一直受到单个GPU可用内存量的限制。回到V-Net时代，我们必须在GTX 1080显卡上的8GB显存上运行我们的3D卷积模型。后来，由于我可以使用高达32GB视频内存的更大的GPU，我们可以提高数据分辨率并改善我们的结果。</p><p id="423c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦我在一台机器上有了多个GPU，我就可以限制我的数据的分辨率，以便单个图像能够适合一个GPU，并且我将求助于数据并行性来运行更大批量的训练。数据并行会将计算分布在多个GPU上，但整个模型仍需要放在一个GPU中。</p><p id="0049" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我仍然相信，当您至少可以在单个GPU上运行每批单个图像的训练时，数据并行是首选。但是，如果您需要在非常高分辨率的数据上训练一个复杂的模型，在这种情况下，只有少数几个层而不是整个模型可以放在单个GPU上，该怎么办？在这种情况下，您需要模型并行性！</p><p id="06e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Tensorflow (1.x)中，启动数据并行或模型并行纯粹是一场噩梦。</p><p id="5d10" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在PyTorch中，我看到人们在一行代码中很好地实现了梯度<strong class="jp ir">检查点</strong>(另一种以牺牲性能为代价处理大型模型的技术)，以及<strong class="jp ir">数据</strong> <strong class="jp ir">并行性</strong>。这行代码:</p><pre class="km kn ko kp gt mr ms mt mu aw mv bi"><span id="627c" class="mw ld iq ms b gy mx my l mz na">model = torch.nn.DataParallel(model)</span></pre><p id="5750" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是对于模型并行性<strong class="jp ir">来说，没有只包含一行代码的解决方案。PyTorch团队有一本<a class="ae lb" href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html#speed-up-by-pipelining-inputs" rel="noopener ugc nofollow" target="_blank">优秀指南</a>，解释了如何在PyTorch中获得模型并行性，以及如何通过流水线执行获得出色的性能，但仅此而已。你必须重新实现。</strong></p><p id="e651" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我在<a class="ae lb" href="https://medium.com/@faustomilletari/reinventing-the-wheel-for-the-last-time-cba43860f8cf" rel="noopener">之前的帖子</a>中解释的那样，任何人都不应该浪费时间通过重新实现已经理解的东西来制造bug和问题。这就是为什么<a class="ae lb" href="http://eisen.ai" rel="noopener ugc nofollow" target="_blank"> Eisen </a> 0.0.5以遵循PyTorch建议的方式，在一行代码中为任何模型、任何应用和任何架构*实现了模型并行性。</p><p id="e6a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nb">(*只要架构有多层)</em></p><p id="875c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">【Eisen的模型并行在一行代码中完成。这行代码:</p><pre class="km kn ko kp gt mr ms mt mu aw mv bi"><span id="a79e" class="mw ld iq ms b gy mx my l mz na">model = EisenAutoModelParallelModuleWrapper(<br/>    module=YourModule,  # Python module<br/>    number_gpus=8,  # Desired parallelism<br/>    split_size=2, # How to split batch for pipelining <br/>    input_names=['inputs'],  # input names for Eisen workflow<br/>    output_names=['labels'],  # output names for Eisen workflow<br/>)</span><span id="7b35" class="mw ld iq ms b gy nc my l mz na"># please note that this has been changed as of <br/># eisen &gt; 1.6.0 and eisen-core &gt; 0.0.5</span></pre><p id="d051" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这行代码的作用是:</p><ul class=""><li id="2757" class="nd ne iq jp b jq jr ju jv jy nf kc ng kg nh kk ni nj nk nl bi translated">将模型实例化为“YourModule”的实例</li><li id="5dcc" class="nd ne iq jp b jq nm ju nn jy no kc np kg nq kk ni nj nk nl bi translated">当forward方法被调用时(第一次)，它对模型本身占用的GPU内存进行惰性评估</li><li id="aa60" class="nd ne iq jp b jq nm ju nn jy no kc np kg nq kk ni nj nk nl bi translated">跨GPU分割图层，并将其分配给设备</li><li id="f1f4" class="nd ne iq jp b jq nm ju nn jy no kc np kg nq kk ni nj nk nl bi translated">根据需要实现在GPU之间移动张量的自动方式</li><li id="d838" class="nd ne iq jp b jq nm ju nn jy no kc np kg nq kk ni nj nk nl bi translated">自动为模型定义流水线执行策略，以便尽可能多地使用所有的GPU</li></ul><p id="4e86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为这种策略的结果，我们获得了(1)模型在指定GPU上的平衡分割，以及(2)与单个GPU模型相比更快的执行。实现这个特性的源代码可以在这里找到。</p><p id="1924" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我整理了一个python笔记本，在模型并行性和单GPU训练之间做了一些比较。你可以在这里找到笔记本。这是来源于官方<a class="ae lb" href="http://docs.eisen.ai/eisen/tutorials.html" rel="noopener ugc nofollow" target="_blank">艾森教程</a>之一。</p><p id="ab08" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我决定在配备32GB Tesla V100的8 GPU AWS实例p3dn.24xlarge上运行以下实验:</p><ul class=""><li id="b708" class="nd ne iq jp b jq jr ju jv jy nf kc ng kg nh kk ni nj nk nl bi translated">对于固定模型、固定数据集、固定批量:<br/> -使用分割大小为2的1、2、4和8个GPU运行训练<br/> -使用分割大小为4的1、2、4和8个GPU运行训练</li><li id="5419" class="nd ne iq jp b jq nm ju nn jy no kc np kg nq kk ni nj nk nl bi translated">对于固定的模型、固定的数据集，尝试将整个数据集<strong class="jp ir">放入一个批处理</strong> : <br/> -使用8个GPU和模型并行性运行训练</li></ul><p id="ed30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一个实验是使用<a class="ae lb" href="http://medicaldecathlon.com" rel="noopener ugc nofollow" target="_blank"> MSD </a>任务04数据集运行的，批量大小为32，分辨率为64x64x64体素，以1毫米立方重采样数据。你可以在笔记本里找到更多<a class="ae lb" href="https://colab.research.google.com/drive/1x26OJPFtRWwG_sNtneYRCAP2YIA-BUSc" rel="noopener ugc nofollow" target="_blank">。批量大小为32时，1个GPU上的内存使用量约为18GB。</a></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nr"><img src="../Images/3aec20c2469f4b2658b9b14299420e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5KVQG5bLy5lIGUfaB88YqA.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">用一个GPU一个有感觉这个美好的p3dn.24xlarge浪费的差不多了。</p></figure><p id="07a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我想澄清的是，在这个特定的场景中，由于我们能够在一个GPU上一批安装32个图像，因此应该使用<strong class="jp ir">数据并行而不是</strong>模型并行。与数据并行相比，模型并行有一些缺点。其中一些问题与内存传输开销和高效的流水线执行有关。</p><p id="e743" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个玩具示例中，我有目的地在<strong class="jp ir">错误类型的工作负载</strong>上运行模型并行。事实上，只有在最大的GPU无法容纳批量1时，才应该使用模型并行。在这里，我们可以在单个GPU上安装32个图像，如果我们想一次处理260个(见下文)图像，我们可以求助于数据并行(这要简单得多)，而不去管整个模型并行的事情。</p><p id="06d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是为了<strong class="jp ir">公平比较</strong>，我们希望能够展示在一个GPU上运行所有东西和跨2、4和8个GPU运行所有东西之间的性能差异。</p><p id="ca48" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我通过Eisen打开模型并行时，我在所有GPU上获得了更低的内存消耗。真的管用！！</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nr"><img src="../Images/539623d3121fb60738624f6ae207848f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QB1QuSMoSzjMQcHHCTnl9g.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">模型并行可以利用系统中的所有GPU，由于流水线执行，所有GPU都可以并行运行。现在我们的p3dn.24xlarge实例运行正常。</p></figure><p id="a5ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在获得了看到所有GPU并行运行的快感并享受了8 Tesla V100和32GB内存同时运行的感觉后，我们准备看看一些性能。这些是在260个图像的数据集上获得的，批次大小为32，50个时期(450个批次)，分割大小为2:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f7b8c2a35d0722d474da308fbf429ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*Qi6TwAENe7ZaGxRyhA3WRQ.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">第一个实验:在固定数据集上运行，固定批量大小，分割大小为2(用于流水线)。</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/54f9782d46f827f61d060dc3008818e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*UeXU8SVQ4uW2qZaVXUDWFQ.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">分割大小为2的单次训练的总时间(用于流水线)。</p></figure><p id="6309" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我将分割大小增加到4时，为了获得更有效的流水线操作并最大化GPU的使用开销，情况得到了改善:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/83f57c2a54b9c6ba80f604ace81c18ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*KnBx2gq2T9sXFOPZrBvmEA.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">当使用更大的分割大小来“填满”GPU的计算资源时，所有场景的情况都会得到改善。</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f6543754f872172f2b6b542f5914fedf.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*nVqqWfijcHSJB_wUXqXTWA.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">单次训练的时间也相应增加。</p></figure><p id="9362" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此时，我想看看由于模型并行性，我还能在内存中容纳多少数据。</p><p id="9233" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">批量大小为32时，我在单个GPU上获得了18GB的内存使用量。现在，我想一次运行数据集中的所有260幅图像。这不一定是我们在真实用例中想要做的事情，无论如何，如上所述，在这种情况下运行数据并行比运行模型并行要好得多。但我想我想看看GPU和艾森如何处理批量大小为260、分割大小为20和50的时期。下面是对比。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/9bfd6c6d7530980e11e8dc0fb2c3fd52.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*b6ZnhMrfjyT8iad2GX6PLw.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">当将整个数据集作为一批处理并使用分割大小20时，运行训练的时间进一步减少。显然，通信开销和流水线仍然是一个问题。</p></figure><p id="8215" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总之，当由于图像分辨率、维度和模型架构而无法在GPU中容纳哪怕一个训练示例时，应该使用模型并行。然后，模型并行将在不同的GPU之间分割模型梯度和激活，以便进行训练。</p><p id="9844" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">艾森似乎能够自动处理模型并行性<strong class="jp ir"/>，因此提供了类似于拥有一个巨大的GPU(本例中为256 GB)进行训练的体验。</p></div></div>    
</body>
</html>