# TensorFlow 2 ä¸­çš„åŠ¨æ‰‹ NLP æ·±åº¦å­¦ä¹ æ¨¡åž‹å‡†å¤‡ã€‚X

> åŽŸæ–‡ï¼š<https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633?source=collection_archive---------22----------------------->

## è¿™æ˜¯ä¸€ä¸ªé€æ­¥å®Œæˆ NLP æ¨¡åž‹å‡†å¤‡æµç¨‹çš„æ•™ç¨‹:æ ‡è®°åŒ–ã€åºåˆ—å¡«å……ã€å•è¯åµŒå…¥å’ŒåµŒå…¥å±‚è®¾ç½®ã€‚

![](img/284b3fe9cd2f4d56d73d7dbd2bb693fe.png)

NLP æ¨¡åž‹å‡†å¤‡æ­¥éª¤ï¼Œ(ä½œè€…åˆ›å»º)

# ç®€ä»‹:æˆ‘ä¸ºä»€ä¹ˆå†™è¿™ç¯‡æ–‡ç« 

NLP é—®é¢˜ä¸­çš„è®¸å¤šæœ€å…ˆè¿›çš„ç»“æžœæ˜¯é€šè¿‡ä½¿ç”¨ DL(æ·±åº¦å­¦ä¹ )å®žçŽ°çš„ï¼Œå¹¶ä¸”å¾ˆå¯èƒ½æ‚¨ä¹Ÿæƒ³ä½¿ç”¨æ·±åº¦å­¦ä¹ é£Žæ ¼æ¥è§£å†³ NLP é—®é¢˜ã€‚è™½ç„¶æœ‰å¾ˆå¤šèµ„æ–™è®¨è®ºå¦‚ä½•é€‰æ‹©å’Œè®­ç»ƒâ€œæœ€ä½³â€ç¥žç»ç½‘ç»œæž¶æž„ï¼Œå¦‚ RNNï¼Œä½†é€‰æ‹©å’Œé…ç½®åˆé€‚çš„ç¥žç»ç½‘ç»œåªæ˜¯è§£å†³å®žé™… NLP é—®é¢˜çš„ä¸€éƒ¨åˆ†ã€‚å¦ä¸€ä¸ªé‡è¦çš„éƒ¨åˆ†ï¼Œä½†ç»å¸¸è¢«ä½Žä¼°ï¼Œæ˜¯æ¨¡åž‹å‡†å¤‡ã€‚NLP ä»»åŠ¡é€šå¸¸éœ€è¦åœ¨æ¨¡åž‹å‡†å¤‡é˜¶æ®µè¿›è¡Œç‰¹æ®Šçš„æ•°æ®å¤„ç†ã€‚æ¢å¥è¯è¯´ï¼Œåœ¨æˆ‘ä»¬å°†æ•°æ®è¾“å…¥ç¥žç»ç½‘ç»œè¿›è¡Œè®­ç»ƒä¹‹å‰ï¼Œè¿˜æœ‰å¾ˆå¤šäº‹æƒ…è¦åšã€‚é—æ†¾çš„æ˜¯ï¼Œæ²¡æœ‰å¤šå°‘æ•™ç¨‹ç»™å‡ºæ¨¡åž‹å‡†å¤‡çš„è¯¦ç»†æŒ‡å¯¼ã€‚

æ­¤å¤–ï¼Œæ”¯æŒæœ€æ–°çš„ NLP ç†è®ºå’Œç®—æ³•çš„è½¯ä»¶åŒ…æˆ– API é€šå¸¸æ˜¯æœ€è¿‘æ‰å‘å¸ƒçš„ï¼Œå¹¶ä¸”æ›´æ–°é€Ÿåº¦å¾ˆå¿«ã€‚(å¦‚ 2015 å¹´é¦–æ¬¡å‘å¸ƒ TensorFlowï¼Œ2016 å¹´å‘å¸ƒ PyTorchï¼Œ2015 å¹´å‘å¸ƒ spaCyã€‚)ä¸ºäº†èŽ·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œå¾ˆå¤šæ—¶å€™ï¼Œä½ å¯èƒ½å¿…é¡»åœ¨ä½ çš„æ·±åº¦å­¦ä¹ ç®¡é“ä¸­é›†æˆå‡ ä¸ªåŒ…ï¼ŒåŒæ—¶é˜²æ­¢å®ƒä»¬å½¼æ­¤å´©æºƒã€‚

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘å†³å®šå†™è¿™ç¯‡æ–‡ç« ç»™ä½ ä¸€ä¸ªè¯¦ç»†çš„æ•™ç¨‹ã€‚

*   æˆ‘å°†å‘æ‚¨ä»‹ç»ä»Žæ ‡è®°åŽŸå§‹æ•°æ®åˆ°é…ç½® Tensorflow åµŒå…¥çš„æ¨¡åž‹å‡†å¤‡æµç¨‹ï¼Œä»¥ä¾¿æ‚¨çš„ç¥žç»ç½‘ç»œä¸ºè®­ç»ƒåšå¥½å‡†å¤‡ã€‚
*   ç¤ºä¾‹ä»£ç å°†å¸®åŠ©æ‚¨å¯¹æ¨¡åž‹å‡†å¤‡æ­¥éª¤æœ‰ä¸€ä¸ªåšå®žçš„ç†è§£ã€‚
*   åœ¨æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†é€‰æ‹©ä¸“é—¨ç ”ç©¶ NLP çš„æµè¡ŒåŒ…å’Œ APIï¼Œå¹¶å»ºè®®å‚æ•°é»˜è®¤è®¾ç½®ï¼Œä»¥ç¡®ä¿æ‚¨åœ¨ NLP æ·±åº¦å­¦ä¹ ä¹‹æ—…ä¸­æœ‰ä¸€ä¸ªè‰¯å¥½çš„å¼€ç«¯ã€‚

# åœ¨è¿™ç¯‡æ–‡ç« ä¸­å¯ä»¥æœŸå¾…ä»€ä¹ˆ

*   æˆ‘ä»¬å°†ä½¿ç”¨ TensorFlow 2 å®Œæˆ NLP æ¨¡åž‹å‡†å¤‡æµç¨‹ã€‚x å’Œç©ºé—´ã€‚æµæ°´çº¿ä¸­çš„å››ä¸ªä¸»è¦æ­¥éª¤æ˜¯æ ‡è®°åŒ–ã€å¡«å……ã€å•è¯åµŒå…¥ã€åµŒå…¥å±‚è®¾ç½®ã€‚
*   å°†ä»‹ç»åŠ¨æœº(ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦è¿™ä¸ª)å’Œç›´è§‰(å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„)ï¼Œæ‰€ä»¥å¦‚æžœä½ æ˜¯ NLP æˆ–æ·±åº¦å­¦ä¹ çš„æ–°æ‰‹ï¼Œä¸è¦æ‹…å¿ƒã€‚
*   æˆ‘å°†æåˆ°æ¨¡åž‹å‡†å¤‡æœŸé—´çš„ä¸€äº›å¸¸è§é—®é¢˜å’Œæ½œåœ¨çš„è§£å†³æ–¹æ¡ˆã€‚
*   åœ¨ [Colab](https://drive.google.com/file/d/1E7l_XJ1HnhaGXwUo4jpYv5WZwmULqU92/view?usp=sharing) å’Œ [Github](https://github.com/kefeimo/DataScienceBlog/blob/master/08_06_2020_tf_embedding/tf_nlp_tokenizer_embedding.ipynb) ä¸Šæœ‰ä½ å¯ä»¥çŽ©çš„ç¬”è®°æœ¬ã€‚è™½ç„¶æˆ‘ä»¬åœ¨ç¤ºä¾‹ä¸­ä½¿ç”¨äº†ä¸€ä¸ªçŽ©å…·æ•°æ®é›†(å–è‡ª [IMDB ç”µå½±è¯„è®ºæ•°æ®é›†](https://ai.stanford.edu/~amaas/data/sentiment/))ï¼Œä½†ä»£ç å¯ä»¥åº”ç”¨äºŽæ›´å¤§ã€æ›´å®žç”¨çš„æ•°æ®é›†ã€‚

äº‹ä¸å®œè¿Ÿï¼Œæˆ‘ä»¬ä»Žç¬¬ä¸€æ­¥å¼€å§‹ã€‚

# æ ‡è®°åŒ–

# ä»€ä¹ˆæ˜¯æ ‡è®°åŒ–ï¼Ÿ

åœ¨ NLP ä¸­ï¼Œ[è®°å·åŒ–](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)æ„å‘³ç€å°†åŽŸå§‹æ–‡æœ¬åˆ†å‰²æˆå”¯ä¸€çš„å•å…ƒ(ä¹Ÿç§°ä¸ºè®°å·)ã€‚ä»¤ç‰Œå¯ä»¥æ˜¯å¥å­ã€çŸ­è¯­æˆ–å•è¯ã€‚æ¯ä¸ªä»¤ç‰Œéƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„ä»¤ç‰Œ idã€‚æ ‡è®°åŒ–çš„ç›®çš„æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æ ‡è®°(æˆ–æ ‡è®° id)æ¥è¡¨ç¤ºåŽŸå§‹æ–‡æœ¬ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªä¾‹å­ã€‚

![](img/c14fb72a32e094bd6a5128ab07eb47b4.png)

ç¬¦å·åŒ–æ’å›¾ï¼Œ(ä½œè€…åˆ›ä½œ)

æ ‡è®°åŒ–é€šå¸¸åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µ:

**é˜¶æ®µ 1:åˆ›å»ºä¸€ä¸ªä»¤ç‰Œå­—å…¸ï¼Œåœ¨è¿™ä¸ªé˜¶æ®µï¼Œ**

*   é€šè¿‡é¦–å…ˆå°†åŽŸå§‹æ–‡æœ¬åˆ†æˆå¥å­ï¼Œç„¶åŽå°†å¥å­åˆ†è§£æˆå•è¯ï¼Œæ¥é€‰æ‹©å€™é€‰æ ‡è®°(é€šå¸¸æ˜¯å•è¯)ã€‚
*   åº”è¯¥åŒ…æ‹¬æŸäº›é¢„å¤„ç†ï¼Œä¾‹å¦‚ï¼Œå°å†™ï¼Œæ ‡ç‚¹ç¬¦å·åˆ é™¤ç­‰ã€‚
*   è¯·æ³¨æ„ï¼Œä»¤ç‰Œåº”è¯¥æ˜¯å”¯ä¸€çš„ï¼Œå¹¶è¢«åˆ†é…ç»™ä¸åŒçš„ä»¤ç‰Œ idï¼Œä¾‹å¦‚ï¼Œâ€œæ±½è½¦â€å’Œâ€œæ±½è½¦â€æ˜¯ä¸åŒçš„ä»¤ç‰Œï¼Œâ€œæ±½è½¦â€å’Œâ€œæ±½è½¦â€ä¹Ÿæ˜¯ä¸åŒçš„ä»¤ç‰Œã€‚æ‰€é€‰æ‹©çš„ä»¤ç‰Œå’Œç›¸å…³çš„ä»¤ç‰Œ id å°†åˆ›å»ºä¸€ä¸ªä»¤ç‰Œå­—å…¸ã€‚

**é˜¶æ®µ 2:æ–‡æœ¬è¡¨ç¤ºï¼Œåœ¨æ­¤é˜¶æ®µï¼Œ**

*   é€šè¿‡å‚è€ƒæ ‡è®°å­—å…¸ï¼Œç”¨æ ‡è®°(æˆ–ç›¸å…³çš„æ ‡è®° id)è¡¨ç¤ºåŽŸå§‹æ–‡æœ¬ã€‚
*   æœ‰æ—¶æ ‡è®°è¢«éƒ¨åˆ†é€‰æ‹©ç”¨äºŽæ–‡æœ¬è¡¨ç¤º(ä¾‹å¦‚ï¼Œä»…é€‰æ‹©æœ€é¢‘ç¹çš„æ ‡è®°ã€‚);å› æ­¤ï¼Œæœ€ç»ˆçš„ä»¤ç‰ŒåŒ–åºåˆ—å°†åªåŒ…æ‹¬è¿™æ ·é€‰æ‹©çš„ä»¤ç‰Œã€‚

# åœ¨å¼ é‡æµä¸­

æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ª [IMDB ç”µå½±è¯„è®ºæ•°æ®é›†](https://ai.stanford.edu/~amaas/data/sentiment/)æ¥æ¼”ç¤ºç®¡é“ã€‚

```
from tensorflow.keras.preprocessing.text import Tokenizertokenizer = Tokenizer()
tokenizer.fit_on_texts(raw_text)
train_sequences = tokenizer.texts_to_sequences(raw_text) #Converting text to a vector of word indexes
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
print('1st token-id sequnce', train_sequences[0])>>Found 212 unique tokens.
>>1st token-id sequnce [21, 4, 2, 12, 22, 23, 50, 51, 13, 2, 52, 53, 54, 24, 6, 2, 55, 56, 57, 7, 2, 58, 59, 4, 25, 60]
```

çŽ°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬ä»Žæ ‡è®°åŒ–æ­¥éª¤ä¸­å¾—åˆ°äº†ä»€ä¹ˆã€‚

**ä»¤ç‰Œå­—å…¸**

```
# display the token dictionary (from most freqent to rarest)
# these are the 2 useful attributes, (get_config will show the rest)
print(tokenizer.word_index)
print(tokenizer.word_counts)
# tokenizer.get_config()>>{'the': 1, 'a': 2, 'and': 3, 'of': 4, 'to': 5, 'with': 6, 'is': 7, 'this': 8, 'by': 9, 'his': 10, 'movie': 11, 'man': 12, 'for': 13, ...
>>OrderedDict([('story', 2), ('of', 8), ('a', 11), ('man', 3), ('who', 2), ('has', 2), ('unnatural', 1), ('feelings', 1), ('for', 3), ('pig', 1), ('br', 1), ('starts', 1), ('out', 2), ('with', 6), ...
```

**è¯´æ˜Ž:**

*   è®°å·èµ‹äºˆå™¨å¯¹æ¯ä¸ªå•è¯(è®°å·)çš„æ•°é‡è¿›è¡Œè®¡æ•°ï¼Œå¹¶æ ¹æ®è®¡æ•°å¯¹è®°å·è¿›è¡ŒæŽ’åºã€‚ä¾‹å¦‚,â€œtheâ€æ˜¯è¯­æ–™åº“ä¸­æœ€é¢‘ç¹å‡ºçŽ°çš„æ ‡è®°ï¼Œå› æ­¤æŽ’åœ¨ç¬¬ä¸€ä½ï¼Œå°†æ ‡è®° id å…³è”ä¸ºâ€œ1â€ã€‚è¿™ä¸ªæŽ’ååœ¨ä¸€æœ¬å­—å…¸é‡Œæœ‰æè¿°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`tokenizer.word_index`å±žæ€§æ¥æ£€æŸ¥å¤±çœŸã€‚
*   æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`tokenizer.word_counts`æ¥æ£€æŸ¥ä¸Žæ¯ä¸ªä»¤ç‰Œç›¸å…³çš„è®¡æ•°ã€‚

**é‡è¦æç¤º:**ä½¿ç”¨ TensorFlow Tokenizer æ—¶ï¼Œ0-token-id ä¿ç•™ç»™ empty-tokenï¼Œå³ token-id ä»Ž 1 å¼€å§‹ï¼Œ

**ä»¤ç‰Œæ ‡è¯†åºåˆ—**

```
# compare the number of tokens and tokens after cut-off
train_sequences = tokenizer.texts_to_sequences(raw_text) #Converting text to a vector of word indexes
# print(len(text_to_word_sequence(raw_text[0])), len(train_sequences[0]))
print(raw_text[0])
print(text_to_word_sequence(raw_text[0]))
print()
tokenizer.num_words = None # take all the tokens
print(tokenizer.texts_to_sequences(raw_text)[0])
tokenizer.num_words = 50 # take the top 50-1 tokens
print(tokenizer.texts_to_sequences(raw_text)[0])>>Story of a man who has unnatural feelings for a pig.  <br>  Starts out with a opening scene that is a terrific example of absurd comedy. 
>>['story', 'of', 'a', 'man', 'who', 'has', 'unnatural', 'feelings', 'for', 'a', 'pig', 'br', 'starts', 'out', 'with', 'a', 'opening', 'scene', 'that', 'is', 'a', 'terrific', 'example', 'of', 'absurd', 'comedy']>>[21, 4, 2, 12, 22, 23, 50, 51, 13, 2, 52, 53, 54, 24, 6, 2, 55, 56, 57, 7, 2, 58, 59, 4, 25, 60]
>>[21, 4, 2, 12, 22, 23, 13, 2, 24, 6, 2, 7, 2, 4, 25]
```

**è¯´æ˜Ž:**

*   æˆ‘ä»¬ä½¿ç”¨`train_sequences = tokenizer.texts_to_sequences(raw_text)`å°†æ–‡æœ¬è½¬æ¢æˆå•è¯ç´¢å¼•/id çš„å‘é‡ã€‚è½¬æ¢åŽçš„åºåˆ—å°†é€‚åˆç®¡é“ä¸­çš„ä¸‹ä¸€æ­¥ã€‚
*   å½“æœ‰å¤ªå¤šä»¤ç‰Œæ—¶ï¼Œå­˜å‚¨å’Œè®¡ç®—å¯èƒ½æ˜¯æ˜‚è´µçš„ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`num_words`å‚æ•°æ¥å†³å®šä½¿ç”¨å¤šå°‘æ ‡è®°æ¥è¡¨ç¤ºæ–‡æœ¬ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œç”±äºŽæˆ‘ä»¬è®¾ç½®äº†å‚æ•°`num_words=50`ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å°†èŽ·å–å‰ 50-1=49 ä¸ªä»¤ç‰Œã€‚æ¢å¥è¯è¯´ï¼Œåƒâ€œä¸è‡ªç„¶:50â€ï¼Œâ€œæ„Ÿæƒ…:51â€è¿™æ ·çš„è®°å·ä¸ä¼šå‡ºçŽ°åœ¨æœ€ç»ˆçš„è®°å·åŒ–åºåˆ—ä¸­ã€‚
*   é»˜è®¤æƒ…å†µä¸‹ï¼Œ`num_words=None`ï¼Œè¿™æ„å‘³ç€å®ƒå°†å¸¦èµ°æ‰€æœ‰çš„ä»¤ç‰Œã€‚
*   æç¤º:æ‚¨å¯ä»¥éšæ—¶è®¾ç½® num_wordsï¼Œè€Œæ— éœ€é‡æ–°å®‰è£…åˆ†è¯å™¨ã€‚

> **æ³¨æ„:**num _ words å€¼åº”è¯¥æ˜¯ä»€ä¹ˆï¼Œæ²¡æœ‰ç®€å•çš„ç­”æ¡ˆã€‚ä½†è¿™é‡Œæ˜¯æˆ‘çš„å»ºè®®:æž„å»ºä¸€ä¸ªç®¡é“ï¼Œå¯ä»¥å…ˆä»Žä¸€ä¸ªæ¯”è¾ƒå°çš„æ•°å­—å¼€å§‹ï¼Œæ¯”å¦‚è¯´ num _ words = 10,000ï¼Œè¿›ä¸€æ­¥åˆ†æžåŽå†å›žæ¥ä¿®æ”¹ã€‚(æˆ‘å‘çŽ°è¿™ä¸ªå †æ ˆæº¢å‡º[å¸–å­](https://stackoverflow.com/questions/61760508/how-to-choose-num-words-parameter-for-keras-tokenizer)åˆ†äº«äº†ä¸€äº›å…³äºŽå¦‚ä½•é€‰æ‹© num_words å€¼çš„è§è§£ã€‚å¦å¤–ï¼ŒæŸ¥çœ‹æ ‡è®°å™¨çš„[æ–‡æ¡£äº†è§£å…¶ä»–å‚æ•°è®¾ç½®ã€‚)](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)

# ä¸€ä¸ªé—®é¢˜:OOV

è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¯¹æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿ MLs éƒ½éžå¸¸æœ‰å®³çš„æ ‡è®°åŒ–ä¸­çš„ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•å¤„ç†å®ƒã€‚è€ƒè™‘ä¸‹é¢çš„ä¾‹å­ï¼Œæ¥æ ‡è®°è¿™ä¸ªåºåˆ—[ã€Šä¸€ä¸ªå¥³äººçš„æ•…äº‹â€¦â€¦']ã€‚

```
test_sequence = ['Storys of a woman...'] 
print(test_sequence)
print(text_to_word_sequence(test_sequence[0]))
print(tokenizer.texts_to_sequences(test_sequence))>>['Storys of a woman...']
>>['storys', 'of', 'a', 'woman']
>>[[4, 2]]
```

ç”±äºŽç”¨äºŽè®­ç»ƒçš„è¯­æ–™åº“ä¸åŒ…æ‹¬å•è¯â€œstorysâ€æˆ–â€œwoman â€,è¿™äº›å•è¯ä¹Ÿä¸åŒ…æ‹¬åœ¨ä»¤ç‰Œå­—å…¸ä¸­ã€‚è¿™æ˜¯ OOV è¯æ±‡ä¹‹å¤–çš„é—®é¢˜ã€‚è™½ç„¶ OOV æ˜¯éš¾ä»¥é¿å…çš„ï¼Œä½†æœ‰ä¸€äº›è§£å†³æ–¹æ¡ˆå¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜:

*   ä¸€ä¸ªç»éªŒæ³•åˆ™æ˜¯åœ¨ä¸€ä¸ªç›¸å¯¹å¤§çš„è¯­æ–™åº“ä¸Šè®­ç»ƒï¼Œä»¥ä¾¿åˆ›å»ºçš„å­—å…¸å¯ä»¥è¦†ç›–æ›´å¤šçš„å•è¯ï¼Œå› æ­¤ä¸è¦è®¤ä¸ºå®ƒä»¬æ˜¯å¯ä»¥ä¸¢å¼ƒçš„æ–°å•è¯ã€‚
*   è®¾ç½®å‚æ•°`oov_token=`æ•æ‰ OOV çŽ°è±¡ã€‚æ³¨æ„ï¼Œè¿™ä¸ªæ–¹æ³•åªé€šçŸ¥ä½  OOV åœ¨æŸä¸ªåœ°æ–¹å‘ç”Ÿäº†ï¼Œä½†å®ƒä¸èƒ½è§£å†³ OOV é—®é¢˜ã€‚æŸ¥çœ‹ [Kerras æ–‡æ¡£](https://keras.io/api/preprocessing/text/)äº†è§£æ›´å¤šè¯¦æƒ…ã€‚
*   åœ¨æ ‡è®°åŒ–ä¹‹å‰æ‰§è¡Œæ–‡æœ¬é¢„å¤„ç†ã€‚ä¾‹å¦‚,â€œstorysâ€å¯ä»¥è¢«æ‹¼å†™çº æ­£æˆ–ç”¨ä¿¡å·è¡¨ç¤ºä¸ºâ€œstory â€,å…¶è¢«åŒ…æ‹¬åœ¨ä»¤ç‰Œå­—å…¸ä¸­ã€‚æœ‰ä¸€äº› NLP åŒ…ä¸ºæ ‡è®°åŒ–å’Œé¢„å¤„ç†æä¾›äº†æ›´å¥å£®çš„ç®—æ³•ã€‚ä¸€äº›å¥½çš„è®°å·åŒ–é€‰é¡¹æœ‰ [spaCy](https://spacy.io/usage/linguistic-features#native-tokenizers) å’Œ [Gensim](https://tedboy.github.io/nlps/generated/generated/gensim.utils.tokenize.html) ã€‚
*   é‡‡ç”¨(å¹¶å¾®è°ƒ)ä¸€ä¸ªé¢„è®­ç»ƒçš„è®°å·èµ‹äºˆå™¨(æˆ–å˜åŽ‹å™¨)ï¼Œä¾‹å¦‚ Huggingface çš„[é¢„è®­ç»ƒè®°å·èµ‹äºˆå™¨](https://huggingface.co/transformers/main_classes/tokenizer.html)ã€‚

# ç®€çŸ­è®¨è®º:è‰°éš¾çš„å¼€å§‹ï¼Ÿ

è®°å·åŒ–çš„æƒ³æ³•å¯èƒ½çœ‹èµ·æ¥éžå¸¸ç®€å•ï¼Œä½†æ˜¯è¿Ÿæ—©ï¼Œä½ ä¼šæ„è¯†åˆ°è®°å·åŒ–å¯èƒ½æ¯”è¿™ä¸ªä¾‹å­ä¸­çœ‹èµ·æ¥è¦å¤æ‚å¾—å¤šã€‚å¤æ‚åº¦ä¸»è¦æ¥æºäºŽå„ç§é¢„å¤„ç†æ–¹æ³•ã€‚é¢„å¤„ç†çš„ä¸€äº›å¸¸è§åšæ³•æ˜¯å°å†™ã€åˆ é™¤æ ‡ç‚¹ã€å•è¯å•æ•°åŒ–ã€[è¯å¹²åŒ–](https://en.wikipedia.org/wiki/Stemming)å’Œ[è¯æ¡åŒ–](https://en.wikipedia.org/wiki/Lemmatisation)ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰å¯é€‰çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå¦‚[æµ‹è¯•æ ‡å‡†åŒ–](https://en.wikipedia.org/wiki/Text_normalization)(ä¾‹å¦‚ï¼Œæ•°å­—åˆ°æ–‡æœ¬ï¼Œæ‰©å±•ç¼©å†™)[ã€è¯­è¨€è¯†åˆ«](https://en.wikipedia.org/wiki/Language_identification)å’Œ[ä»£ç æ··åˆå’Œç¿»è¯‘](https://en.wikipedia.org/wiki/Code-mixing)ï¼›ä»¥åŠé«˜çº§é¢„å¤„ç†ï¼Œå¦‚[è¯æ€§æ ‡æ³¨](è¯æ€§æ ‡æ³¨)(ä¹Ÿç§°ä¸ºè¯æ€§æ ‡æ³¨)ã€[è§£æž](https://en.wikipedia.org/wiki/Parsing)å’Œ[å…±æŒ‡æ¶ˆè§£](https://en.wikipedia.org/wiki/Coreference)ã€‚å–å†³äºŽé‡‡å–ä»€ä¹ˆé¢„å¤„ç†æ­¥éª¤ï¼Œæ ‡è®°å¯ä»¥ä¸åŒï¼Œå› æ­¤æ ‡è®°åŒ–çš„æ–‡æœ¬ä¹Ÿä¸åŒã€‚

å¦‚æžœä½ ä¸çŸ¥é“ä¸Šé¢è¿™äº›ä»¤äººå›°æƒ‘çš„åå­—ï¼Œä¹Ÿä¸ç”¨æ‹…å¿ƒã€‚äº‹å®žä¸Šï¼Œç¡®å®šåœ¨ NLP ç®¡é“ä¸­åŒ…å«å“ªç§é¢„å¤„ç†æ–¹æ³•æ˜¯éžå¸¸å›°éš¾çš„ã€‚ä¾‹å¦‚ï¼Œå†³å®šåœ¨æ–‡æœ¬è¡¨ç¤ºä¸­åŒ…å«å“ªäº›æ ‡è®°å¹¶ä¸å®¹æ˜“ã€‚æ•´åˆå¤§é‡çš„å€™é€‰è®°å·æ˜¯å­˜å‚¨å’Œè®¡ç®—æ˜‚è´µçš„ã€‚è¿˜ä¸æ¸…æ¥šå“ªä¸ªæ ‡è®°æ›´é‡è¦:æœ€å¸¸å‡ºçŽ°çš„å•è¯å¦‚â€œtheâ€ã€â€œaâ€å¯¹äºŽæ–‡æœ¬è¡¨ç¤ºæ¥è¯´ä¿¡æ¯ä¸å¤šï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦åœ¨é¢„å¤„ç†ä¸­å¤„ç†[åœç”¨è¯](https://en.wikipedia.org/wiki/Stop_words)ã€‚

å°½ç®¡å¯ä»¥è¯´ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæœ‰ä¸€ä¸ªå¥½æ¶ˆæ¯:æ·±åº¦å­¦ä¹ æ¯”ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ç®—æ³•éœ€è¦ç›¸å¯¹è¾ƒå°‘çš„é¢„å¤„ç†ã€‚åŽŸå› æ˜¯æ·±åº¦å­¦ä¹ å¯ä»¥åˆ©ç”¨ä¼ ç»Ÿ ML æ¨¡åž‹åœ¨é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹é˜¶æ®µæ‰§è¡Œçš„ç‰¹å¾æå–çš„ç¥žç»ç½‘ç»œæž¶æž„ã€‚å› æ­¤ï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥ä¿æŒæ ‡è®°åŒ–æ­¥éª¤ç®€å•ï¼Œå¦‚æžœéœ€è¦æ›´å¤šçš„é¢„å¤„ç†å’Œ/æˆ–åŽå¤„ç†ï¼Œç¨åŽå†å›žæ¥ã€‚

# æ ‡è®°åŒ–ç¿˜æ›²

è™½ç„¶å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ•™ç¨‹ä»ç„¶ä½¿ç”¨ list æˆ– np.array æ¥å­˜å‚¨æ•°æ®ï¼Œä½†æˆ‘å‘çŽ°ä½¿ç”¨ DataFrame(ä¾‹å¦‚ï¼ŒPandas æˆ– PySpark)æ¥å®Œæˆå·¥ä½œæ›´åŠ å¯æŽ§å’Œå¯ä¼¸ç¼©ã€‚è¿™ä¸€æ­¥æ˜¯å¯é€‰çš„ï¼Œä½†æˆ‘å»ºè®®æ‚¨è¿™æ ·åšã€‚ä¸‹é¢æ˜¯ç¤ºä¾‹ä»£ç ã€‚

```
# store in dataframe
df_text = pd.DataFrame({'raw_text': raw_text})
df_text.head()# updata df_text
df_text['train_sequence'] = df_text.raw_text.apply(lambda x: tokenizer.texts_to_sequences([x])[0])
df_text.head()>>																				raw_text	               train_sequence
0	Story of a man who has unnatural feelings for ...	[21, 4, 2, 12, 22, 23, 13, 2, 24, 6, 2, 7, 2, ...
1	A formal orchestra audience is turned into an ...	[2, 26, 7, 27, 14, 9, 1, 4, 28]
2	Unfortunately it stays absurd the WHOLE time w...	[15, 25, 1, 29, 6, 15, 30]
3	Even those from the era should be turned off. ...	[1, 16, 17, 27, 30, 1, 5, 2]
4	On a technical level it's better than you migh...	[31, 2, 28, 6, 32, 9, 33]
```

è¿™å°±æ˜¯ä½ éœ€è¦äº†è§£çš„è®°å·åŒ–ã€‚è®©æˆ‘ä»¬è¿›å…¥ä¸‹ä¸€æ­¥:å¡«å……ã€‚

# å¡«æ–™

å¤§å¤šæ•°(å¦‚æžœä¸æ˜¯å…¨éƒ¨)ç¥žç»ç½‘ç»œè¦æ±‚è¾“å…¥åºåˆ—æ•°æ®å…·æœ‰ç›¸åŒçš„é•¿åº¦ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å¡«å……:å°†åºåˆ—æˆªæ–­æˆ–å¡«å……(é€šå¸¸ç”¨ 0 å¡«å……)æˆç›¸åŒçš„é•¿åº¦ã€‚è¿™æ˜¯ä¸€ä¸ªå¡«å……çš„ä¾‹å­ã€‚

![](img/41f61d027fa3983ad5476a295a807805.png)

å¡«å……æ’å›¾ï¼Œ(ä½œè€…åˆ›ä½œ)

è®©æˆ‘ä»¬çœ‹çœ‹ä¸‹é¢çš„ç¤ºä¾‹ä»£ç ï¼Œåœ¨ TensorFlow ä¸­æ‰§è¡Œå¡«å……ã€‚

```
from tensorflow.keras.preprocessing.sequence import pad_sequences# before padding
print(type(train_sequences))
train_sequences
>> <class 'list'>
>>  [[21, 4, 2, 12, 22, 23, 13, 2, 24, 6, 2, 7, 2, 4, 25],
		 [2, 26, 7, 27, 14, 9, 1, 4, 28],
		 [15, 25, 1, 29, 6, 15, 30],
		 [1, 16, 17, 27, 30, 1, 5, 2],
		 [31, 2, 28, 6, 32, 9, 33],
		...MAX_SEQUENCE_LENGTH = 10 # length of the sequence
trainvalid_data_pre = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, 
                                padding='pre',
                                truncating='pre',)
trainvalid_data_pre>>array([[23, 13,  2, 24,  6,  2,  7,  2,  4, 25],
	       [ 0,  2, 26,  7, 27, 14,  9,  1,  4, 28],
	       [ 0,  0,  0, 15, 25,  1, 29,  6, 15, 30],
	       [ 0,  0,  1, 16, 17, 27, 30,  1,  5,  2],
	       [ 0,  0,  0, 31,  2, 28,  6, 32,  9, 33],
	   ...
```

**è§£è¯´:**

*   åœ¨å¡«å……ä¹‹å‰ï¼Œä»¤ç‰Œè¡¨ç¤ºçš„åºåˆ—å…·æœ‰ä¸åŒçš„é•¿åº¦ï¼›å¡«å……åŽéƒ½æ˜¯ä¸€æ ·é•¿çš„ã€‚
*   å‚æ•°â€œmaxlenâ€å®šä¹‰äº†å¡«å……åºåˆ—çš„é•¿åº¦ã€‚å½“æ ‡è®°åŒ–åºåˆ—çš„é•¿åº¦å¤§äºŽâ€œmaxlenâ€æ—¶ï¼Œâ€œmaxlenâ€ä¹‹åŽçš„åºåˆ—çš„æ ‡è®°å°†è¢«æˆªæ–­ï¼›å½“æ ‡è®°åŒ–åºåˆ—çš„é•¿åº¦å°äºŽâ€œmaxlenâ€æ—¶ï¼Œå°†ç”¨â€œ0â€å¡«å……ã€‚
*   æˆªæ–­å’Œå¡«å……åºåˆ—çš„ä½ç½®åˆ†åˆ«ç”±â€œpadding=â€å’Œâ€œtruncating=â€ç¡®å®šã€‚

# è®¨è®ºå’Œæç¤º

**å‰ç½®è¿˜æ˜¯åŽç½®ï¼Ÿ**

é»˜è®¤æƒ…å†µä¸‹ï¼Œpad_sequences å‚æ•°è®¾ç½®ä¸º padding='pre 'ï¼Œtruncating='pre 'ã€‚ç„¶è€Œï¼Œæ ¹æ® [TensorFlow æ–‡æ¡£](https://www.tensorflow.org/guide/keras/masking_and_padding)ï¼Œå»ºè®®â€œåœ¨å¤„ç† RNN å›¾å±‚æ—¶ä½¿ç”¨â€˜åŽâ€™å¡«å……â€ã€‚(å»ºè®®åœ¨è‹±æ–‡ä¸­ï¼Œæœ€é‡è¦çš„ä¿¡æ¯å‡ºçŽ°åœ¨å¼€å¤´ã€‚æ‰€ä»¥æˆªæ–­æˆ–å¡«å……åŽçš„åºåˆ—èƒ½æ›´å¥½åœ°ä»£è¡¨åŽŸæ–‡ã€‚)ä¸‹é¢æ˜¯ç¤ºä¾‹ä»£ç ã€‚

```
MAX_SEQUENCE_LENGTH = 10
trainvalid_data_post = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, 
                                padding='post',
                                truncating='post',)
trainvalid_data_post
>>array([[21,  4,  2, 12, 22, 23, 13,  2, 24,  6],
	       [ 2, 26,  7, 27, 14,  9,  1,  4, 28,  0],
	       [15, 25,  1, 29,  6, 15, 30,  0,  0,  0],
	       [ 1, 16, 17, 27, 30,  1,  5,  2,  0,  0],
	       [31,  2, 28,  6, 32,  9, 33,  0,  0,  0],
	     ...
```

**å…³äºŽé©¬å…‹ä¼¦ã€‚**

å¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œmaxlen å€¼åº”è¯¥æ˜¯å¤šå°‘ã€‚è¿™é‡Œçš„æŠ˜è¡·æ˜¯ï¼Œè¾ƒå¤§çš„ maxlen å€¼ä¼šå¯¼è‡´åºåˆ—ä¿ç•™æ›´å¤šçš„ä¿¡æ¯ï¼Œä½†ä¼šå ç”¨æ›´å¤šçš„å­˜å‚¨ç©ºé—´å’Œæ›´å¤§çš„è®¡ç®—å¼€é”€ï¼Œè€Œè¾ƒå°çš„ maxlen å€¼å¯ä»¥èŠ‚çœå­˜å‚¨ç©ºé—´ï¼Œä½†ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚

*   åœ¨ç®¡é“æž„å»ºé˜¶æ®µï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©å‡å€¼æˆ–ä¸­å€¼ä½œä¸º maxlenã€‚å½“åºåˆ—çš„é•¿åº¦å˜åŒ–ä¸å¤§æ—¶ï¼Œå®ƒå·¥ä½œå¾—å¾ˆå¥½ã€‚
*   å¦‚æžœåºåˆ—çš„é•¿åº¦åœ¨å¾ˆå¤§çš„èŒƒå›´å†…å˜åŒ–ï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸€ä¸ªä¸ªæ¡ˆçš„å†³å®šï¼Œä¸€äº›è¯•é”™æ³•æ˜¯å¯å–çš„ã€‚ä¾‹å¦‚ï¼Œå¯¹äºŽ RNN ä½“ç³»ç»“æž„ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©æ›´é«˜ç«¯çš„ maxlen å€¼(å³ï¼Œå¤§ maxlen ),å¹¶åˆ©ç”¨å±è”½(æˆ‘ä»¬å°†åœ¨åŽé¢çœ‹åˆ°å±è”½)æ¥å‡å°‘å­˜å‚¨å’Œè®¡ç®—æµªè´¹ã€‚è¯·æ³¨æ„ï¼Œå¦‚æžœå¤„ç†ä¸å½“ï¼Œç”¨ 0 å¡«å……åºåˆ—ä¼šç»™æ¨¡åž‹å¸¦æ¥å™ªå£°ã€‚ä½¿ç”¨éžå¸¸å¤§çš„ maxlen å€¼ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚å¦‚æžœä½ ä¸ç¡®å®šä½¿ç”¨ä»€ä¹ˆæ ·çš„ç¥žç»ç½‘ç»œç»“æž„ï¼Œæœ€å¥½åšæŒä½¿ç”¨æ— å¡«å……åºåˆ—çš„å¹³å‡å€¼æˆ–ä¸­å€¼ã€‚

ç”±äºŽæˆ‘ä»¬å°†ä»¤ç‰Œåºåˆ—æ•°æ®å­˜å‚¨åœ¨æ•°æ®å¸§ä¸­ï¼Œå› æ­¤èŽ·å–åºåˆ—é•¿åº¦ç»Ÿè®¡æ•°æ®éžå¸¸ç®€å•ï¼Œä¸‹é¢æ˜¯ç¤ºä¾‹ä»£ç :

```
# ckeck sequence_length stats
df_text.train_sequence.apply(lambda x: len(x))
print('sequence_length mean: ', df_text.train_sequence.apply(lambda x: len(x)).mean())
print('sequence_length median: ', df_text.train_sequence.apply(lambda x: len(x)).median())>> sequence_length mean:  9.222222222222221
>> sequence_length median:  8.5
```

åºåˆ—å¡«å……åº”è¯¥å¾ˆå®¹æ˜“ã€‚è®©æˆ‘ä»¬è¿›å…¥ä¸‹ä¸€æ­¥ï¼Œå‡†å¤‡ word2vec å•è¯åµŒå…¥ã€‚

# Word2vec å•è¯åµŒå…¥

# ç›´è§‰

å•è¯åµŒå…¥åœ¨äººç±»å¯¹è¯­è¨€çš„ç†è§£å’Œæœºå™¨ä¹‹é—´æž¶èµ·äº†ä¸€åº§æ¡¥æ¢ã€‚è¿™å¯¹äºŽè®¸å¤š NLP é—®é¢˜æ˜¯å¿…ä¸å¯å°‘çš„ã€‚ä½ å¯èƒ½å¬è¯´è¿‡â€œ [word2vec](https://en.wikipedia.org/wiki/Word2vec) â€ã€â€œ [GloVe](https://nlp.stanford.edu/projects/glove/) â€å’Œâ€œ [FastText](https://fasttext.cc/) â€è¿™äº›åå­—ã€‚

å¦‚æžœæ‚¨ä¸ç†Ÿæ‚‰å•è¯åµŒå…¥ï¼Œä¹Ÿä¸ç”¨æ‹…å¿ƒã€‚æˆ‘ç®€å•ä»‹ç»ä¸€ä¸‹å•è¯åµŒå…¥åº”è¯¥èƒ½æä¾›è¶³å¤Ÿçš„ç›´è§‰ï¼Œåœ¨ TensorFlow ä¸­åº”ç”¨å•è¯åµŒå…¥ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬äº†è§£ä¸€äº›å…³é”®æ¦‚å¿µ:

**åµŒå…¥**:å¯¹äºŽè¯­æ–™åº“ä¸­çš„è¯é›†ï¼ŒåµŒå…¥æ˜¯æ¥è‡ªåˆ†å¸ƒå¼è¡¨ç¤ºçš„å‘é‡ç©ºé—´åˆ°æ¥è‡ªåˆ†å¸ƒå¼è¡¨ç¤ºçš„å‘é‡ç©ºé—´ä¹‹é—´çš„æ˜ å°„ã€‚

**å‘é‡è¯­ä¹‰:**è¿™æ˜¯æŒ‡ä¸€ç»„ NLP æ–¹æ³•ï¼Œæ—¨åœ¨åŸºäºŽå¤§åž‹è¯­æ–™åº“ä¸­å•è¯çš„åˆ†å¸ƒå±žæ€§æ¥å­¦ä¹ å•è¯è¡¨ç¤ºã€‚

è®©æˆ‘ä»¬çœ‹çœ‹ä¸€äº›ä½¿ç”¨ spaCy é¢„è®­ç»ƒåµŒå…¥æ¨¡åž‹çš„å¯é ç¤ºä¾‹ã€‚

```
import spacy
# if first use, download en_core_web_sm
nlp_sm = spacy.load("en_core_web_sm")
nlp_md = spacy.load("en_core_web_md")
# nlp_lg = spacy.load("en_core_web_lg")doc = nlp_sm("elephant")
print(doc.vector.size)
doc.vector
>>96
>>array([ 1.5506991 , -1.0745661 ,  1.9747349 , -1.0160941 ,  0.90996253,
	       -0.73704714,  1.465313  ,  0.806101  , -4.716807  ,  3.5754416 ,
	        1.0041305 , -0.86196965, -1.4205945 , -0.9292773 ,  2.1418033 ,
	        0.84281194,  1.4268254 ,  2.9627366 , -0.9015219 ,  2.846716  ,
	        1.1348789 , -0.1520077 , -0.15381837, -0.6398335 ,  0.36527258,
	...
```

**è§£è¯´:**

*   ä½¿ç”¨ spaCy(ä¸€ä¸ªè‘—åçš„ NLP åŒ…)å°†å•è¯â€œelephantâ€åµŒå…¥åˆ°ä¸€ä¸ª 96 ç»´å‘é‡ä¸­ã€‚
*   åŸºäºŽè¦åŠ è½½çš„æ¨¡åž‹ï¼Œå‘é‡å°†å…·æœ‰ä¸åŒçš„ç»´åº¦ã€‚(ä¾‹å¦‚â€œen_core_web_smâ€ã€â€œen_core_web_mdâ€å’Œâ€œen_core_web_lgâ€çš„ç»´æ•°åˆ†åˆ«ä¸º 96ã€300 å’Œ 300ã€‚)

çŽ°åœ¨â€œå¤§è±¡â€è¿™ä¸ªè¯å·²ç»ç”¨å‘é‡æ¥è¡¨ç¤ºäº†ï¼Œé‚£åˆå¦‚ä½•ï¼Ÿä¸è¦çœ‹åˆ«å¤„ã€‚ä¸€äº›å¥‡è¿¹å³å°†å‘ç”Ÿã€‚ðŸ§™ðŸ¼â€â™‚ï¸

å› ä¸ºæˆ‘ä»¬å¯ä»¥ç”¨å‘é‡æ¥è¡¨ç¤ºå•è¯ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è®¡ç®—å•è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦(æˆ–è·ç¦»)ã€‚è€ƒè™‘ä¸‹é¢çš„ä»£ç ã€‚

```
# demo1
word1 = "elephant"; word2 = "big"
print("similariy {}-{}: {}".format(word1, word2, nlp_md(word1).similarity(nlp_md(word2))) )
word1 = "mouse"; word2 = "big"
print("similariy {}-{}: {}".format(word1, word2, nlp_md(word1).similarity(nlp_md(word2))) )
word1 = "mouse"; word2 = "small"
print("similariy {}-{}: {}".format(word1, word2, nlp_md(word1).similarity(nlp_md(word2))) )>>similariy elephant-big: 0.3589780131997766
>>similariy mouse-big: 0.17815787869074504
>>similariy mouse-small: 0.32656001719452826# demo2
word1 = "elephant"; word2 = "rock"
print("similariy {}-{}: {}".format(word1, word2, nlp_md(word1).similarity(nlp_md(word2))) )
word1 = "mouse"; word2 = "elephant"
print("similariy {}-{}: {}".format(word1, word2, nlp_md(word1).similarity(nlp_md(word2))) )
word1 = "mouse"; word2 = "rock"
print("similariy {}-{}: {}".format(word1, word2, nlp_md(word1).similarity(nlp_md(word2))) )
word1 = "mouse"; word2 = "pebble"
print("similariy {}-{}: {}".format(word1, word2, nlp_md(word1).similarity(nlp_md(word2))) )>>similariy elephant-rock: 0.23465476998562218
>>similariy mouse-elephant: 0.3079661539409069
>>similariy mouse-rock: 0.11835070985447328
>>similariy mouse-pebble: 0.18301520085660278
```

**è¯„è®º:**

*   åœ¨æµ‹è¯• 1 ä¸­:â€œå¤§è±¡â€æ¯”â€œè€é¼ â€æ›´åƒâ€œå¤§â€ï¼›è€Œâ€œè€é¼ â€æ›´ç±»ä¼¼äºŽâ€œå°â€ï¼Œè€Œä¸æ˜¯â€œå¤§è±¡â€æ›´ç±»ä¼¼äºŽâ€œå°â€ã€‚å½“æåˆ°å¤§è±¡å’Œè€é¼ çš„é€šå¸¸å°ºå¯¸æ—¶ï¼Œè¿™ç¬¦åˆæˆ‘ä»¬çš„å¸¸è¯†ã€‚
*   åœ¨æµ‹è¯• 2 ä¸­:â€œå¤§è±¡â€ä¸Žâ€œå²©çŸ³â€çš„ç›¸ä¼¼åº¦ä½ŽäºŽâ€œå¤§è±¡â€æœ¬èº«ä¸Žâ€œè€é¼ â€çš„ç›¸ä¼¼åº¦ï¼›åŒæ ·ï¼Œâ€œè€é¼ â€ä¸Žâ€œå²©çŸ³â€çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œä¸å¦‚â€œè€é¼ â€æœ¬èº«ä¸Žâ€œå¤§è±¡â€çš„ç›¸ä¼¼ç¨‹åº¦ã€‚è¿™å¤§æ¦‚å¯ä»¥è§£é‡Šä¸ºâ€œå¤§è±¡â€å’Œâ€œè€é¼ â€éƒ½æ˜¯åŠ¨ç‰©ï¼Œè€Œâ€œçŸ³å¤´â€æ²¡æœ‰ç”Ÿå‘½ã€‚
*   test2 ä¸­çš„å‘é‡ä¸ä»…è¡¨ç¤ºæ´»æ€§çš„æ¦‚å¿µï¼Œè¿˜è¡¨ç¤ºå¤§å°çš„æ¦‚å¿µ:å•è¯â€œrockâ€é€šå¸¸ç”¨äºŽæè¿°å¤§å°æ›´æŽ¥è¿‘äºŽå¤§è±¡å’Œè€é¼ çš„ç‰©ä½“ï¼Œå› æ­¤â€œrockâ€æ›´ç±»ä¼¼äºŽâ€œå¤§è±¡â€è€Œä¸æ˜¯â€œè€é¼ â€ã€‚åŒæ ·ï¼Œâ€œpebbleâ€é€šå¸¸ç”¨æ¥å½¢å®¹æ¯”â€œrockâ€å°çš„ä¸œè¥¿ï¼›å› æ­¤ï¼Œâ€œåµçŸ³â€å’Œâ€œè€é¼ â€ä¹‹é—´çš„ç›¸ä¼¼æ€§å¤§äºŽâ€œå²©çŸ³â€å’Œâ€œè€é¼ â€ã€‚
*   è¯·æ³¨æ„ï¼Œå•è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§å¯èƒ½å¹¶ä¸æ€»æ˜¯ä¸Žä½ å¤´è„‘ä¸­çš„ç›¸ä¼¼æ€§ç›¸åŒ¹é…ã€‚ä¸€ä¸ªåŽŸå› æ˜¯ç›¸ä¼¼æ€§åªæ˜¯ä¸€ä¸ªåº¦é‡(å³æ ‡é‡)æ¥è¡¨ç¤ºä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å…³ç³»ï¼›å½“ç›¸ä¼¼æ€§å°†é«˜ç»´å‘é‡åŽ‹ç¼©æˆæ ‡é‡æ—¶ï¼Œé‚£ä¹ˆå¤šä¿¡æ¯éƒ½ä¸¢å¤±äº†ã€‚åŒæ ·ï¼Œä¸€ä¸ªè¯å¯ä»¥æœ‰å‡ ä¸ªæ„æ€ã€‚ä¾‹å¦‚ï¼Œå•è¯ bank å¯ä»¥ä¸Žé‡‘èžæˆ–æ²³æµç›¸å…³ï¼›æ²¡æœ‰èƒŒæ™¯ï¼Œå¾ˆéš¾è¯´æˆ‘ä»¬åœ¨è°ˆè®ºä»€ä¹ˆæ ·çš„é“¶è¡Œã€‚æ¯•ç«Ÿï¼Œè¯­è¨€æ˜¯ä¸€ä¸ªå¯ä»¥è§£é‡Šçš„æ¦‚å¿µã€‚

# ä¸è¦æŽ‰è¿›å…”å­æ´ž

Word2Vec éžå¸¸å¼ºå¤§ï¼Œæ˜¯ä¸€ä¸ªç›¸å½“æ–°çš„æ¦‚å¿µ(Word2vec æ˜¯ 2013 å¹´åˆ›å»ºå¹¶å‘å¸ƒçš„)ã€‚æœ‰å¤ªå¤šçš„ä¸œè¥¿è¦è°ˆï¼Œæ¯”å¦‚

*   ä½ å¯èƒ½æƒ³çŸ¥é“å‘é‡ä¸­çš„å€¼æ˜¯å¦‚ä½•åˆ†é…çš„ã€‚ä»€ä¹ˆæ˜¯è·³æ ¼ï¼ŸCBOW æ˜¯ä»€ä¹ˆï¼Ÿ
*   è¿˜æœ‰å…¶ä»–çš„å•è¯åµŒå…¥æ¨¡åž‹ï¼Œåƒâ€œ [GloVe](https://nlp.stanford.edu/projects/glove/) â€å’Œâ€œ [FastText](https://fasttext.cc/) â€ã€‚æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿæˆ‘ä»¬åº”è¯¥ä½¿ç”¨å“ªä¸€ä¸ªï¼Ÿ

å•è¯åµŒå…¥æ˜¯ä¸€ä¸ªéžå¸¸æ¿€åŠ¨äººå¿ƒçš„è¯é¢˜ï¼Œä½†æ˜¯ä¸è¦å¡åœ¨è¿™é‡Œã€‚å¯¹äºŽä¸ç†Ÿæ‚‰å•è¯åµŒå…¥çš„è¯»è€…æ¥è¯´ï¼Œæœ€é‡è¦çš„æ˜¯ç†è§£

*   å•è¯åµŒå…¥çš„ä½œç”¨:å°†å•è¯è½¬æ¢æˆå‘é‡ã€‚
*   ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦è¿™äº›åµŒå…¥å‘é‡:è¿™æ ·ä¸€å°æœºå™¨å°±å¯ä»¥åšä»¤äººæƒŠå¥‡çš„äº‹æƒ…ï¼›è®¡ç®—å•è¯é—´çš„ç›¸ä¼¼åº¦æ˜¯å…¶ä¸­ä¹‹ä¸€ï¼Œä½†ç»å¯¹ä¸æ­¢è¿™äº›ã€‚
*   OOV ä»ç„¶æ˜¯å•è¯åµŒå…¥çš„ä¸€ä¸ªé—®é¢˜ã€‚è€ƒè™‘ä»¥ä¸‹ä»£ç :

```
print(nlp_md("elephan")[0].has_vector)
print(nlp_md("elephan")[0].is_oov)
nlp_md("elephan").vector>>False
>>True
>>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
	       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
....
```

ç”±äºŽå•è¯â€œelephanâ€åœ¨æˆ‘ä»¬ä¹‹å‰åŠ è½½çš„ spaCyâ€œen _ core _ web _ MDâ€æ¨¡åž‹ä¸­ä¸å­˜åœ¨ï¼ŒspaCy è¿”å›žä¸€ä¸ª 0 å‘é‡ã€‚å†è¯´ä¸€æ¬¡ï¼Œæ²»ç–— OOV ä¸æ˜¯ä¸€é¡¹ç®€å•çš„ä»»åŠ¡ã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`.has_vector`æˆ–`.is_oov`æ¥æ•æ‰ OOV çŽ°è±¡ã€‚

å¸Œæœ›æ‚¨çŽ°åœ¨å¯¹å•è¯åµŒå…¥æœ‰äº†å¾ˆå¥½çš„ç†è§£ã€‚è®©æˆ‘ä»¬å›žåˆ°ä¸»è½¨é“ï¼Œçœ‹çœ‹æˆ‘ä»¬å¦‚ä½•åœ¨ç®¡é“ä¸­åº”ç”¨å•è¯åµŒå…¥ã€‚

# é‡‡ç”¨é¢„å…ˆè®­ç»ƒçš„å•è¯åµŒå…¥æ¨¡åž‹

é¢„è®­ç»ƒçš„å•è¯åµŒå…¥æ˜¯åœ¨ä¸€ä¸ªä»»åŠ¡ä¸­å­¦ä¹ çš„åµŒå…¥ï¼Œç”¨äºŽè§£å†³å¦ä¸€ä¸ªç±»ä¼¼çš„ä»»åŠ¡ã€‚é¢„å…ˆè®­ç»ƒçš„å•è¯åµŒå…¥æ¨¡åž‹ä¹Ÿç§°ä¸ºå˜æ¢å™¨ã€‚ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„å•è¯åµŒå…¥æ¨¡åž‹å¯ä»¥çœåŽ»æˆ‘ä»¬ä»Žå¤´è®­ç»ƒä¸€ä¸ªçš„éº»çƒ¦ã€‚æ­¤å¤–ï¼Œé¢„å…ˆè®­ç»ƒçš„åµŒå…¥å‘é‡ä»Žå¤§æ•°æ®é›†ç”Ÿæˆçš„äº‹å®žé€šå¸¸å¯¼è‡´æ›´å¼ºçš„ç”Ÿæˆèƒ½åŠ›ã€‚

åº”ç”¨é¢„å…ˆè®­ç»ƒçš„å•è¯åµŒå…¥æ¨¡åž‹æœ‰ç‚¹åƒåœ¨å­—å…¸ä¸­æœç´¢ï¼Œæˆ‘ä»¬å·²ç»åœ¨å‰é¢ä½¿ç”¨ spaCy çœ‹åˆ°äº†è¿™æ ·çš„è¿‡ç¨‹ã€‚(ä¾‹å¦‚ï¼Œè¾“å…¥å•è¯â€œå¤§è±¡â€, spaCy è¿”å›žä¸€ä¸ªåµŒå…¥å‘é‡ã€‚)åœ¨è¿™ä¸€æ­¥çš„æœ€åŽï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªâ€œåµŒå…¥çŸ©é˜µâ€,å…¶ä¸­åµŒå…¥å‘é‡ä¸Žæ¯ä¸ªä»¤ç‰Œç›¸å…³è”ã€‚(åµŒå…¥çŸ©é˜µæ˜¯ TensorFlow å°†ç”¨æ¥è¿žæŽ¥ä»¤ç‰Œåºåˆ—å’Œå•è¯åµŒå…¥è¡¨ç¤ºçš„çŸ©é˜µã€‚)

è¿™æ˜¯ä»£ç ã€‚

```
# import pandas as pd
# nlp_sm = spacy.load("en_core_web_sm")
df_index_word = pd.Series(tokenizer.index_word)
# df_index_word
df_index_word_valid = df_index_word[:MAX_NUM_TOKENS-1]
df_index_word_valid = pd.Series(["place_holder"]).append(df_index_word_valid)
df_index_word_valid = df_index_word_valid.reset_index()
# df_index_word_valid.head()
df_index_word_valid.columns = ['token_id', 'token']
# df_index_word_valid.head()
df_index_word_valid['word2vec'] = df_index_word_valid.token.apply(lambda x: nlp_sm(x).vector)
df_index_word_valid['is_oov'] = df_index_word_valid.token.apply(lambda x: nlp_sm(x)[0].is_oov)
df_index_word_valid.at[0, "word2vec"] = np.zeros_like(df_index_word_valid.at[0, "word2vec"])
print(df_index_word_valid.head())>>
		token_id token                                           word2vec  is_oov
0         0   NAN  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    True
1         1   the  [-1.3546131, -1.7212939, 1.7044731, -2.8054314...    True
2         2     a  [-1.9769197, -1.5778058, 0.116705105, -2.46210...    True
3         3   and  [-2.8375597, 0.8632377, -0.69991976, -0.508865...    True
4         4    of  [-2.7217283, -2.1163979, -0.88265955, -0.72048...    True# Embbeding matrix
embedding_matrix = np.array([vec for vec in df_index_word_valid.word2vec.values])
embedding_matrix[1:3]print(embedding_matrix.shape)
>>(50, 96)
```

è§£é‡Š:

*   æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ spaCy æ¥å¯»æ‰¾ä¸Žæ¯ä¸ªä»¤ç‰Œç›¸å…³è”çš„åµŒå…¥å‘é‡(å­˜å‚¨åœ¨æ•°æ®å¸§ä¸­)ã€‚ç»è¿‡ä¸€äº›æ•°æ®è§’åŠ›ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåµŒå…¥çŸ©é˜µ(é‡‡ç”¨ TensorFlow çº¦å®šï¼Œè¿™æ¬¡å­˜å‚¨åœ¨ np.array ä¸­)ã€‚
*   åµŒå…¥çŸ©é˜µçš„è¡Œæ•°:æ€»è¡Œæ•°ä¸º 50ï¼Œç¬¬ä¸€è¡ŒåŒ…å«ä¸€ä¸ªè¡¨ç¤ºç©ºæ ‡è®°çš„é›¶å‘é‡ï¼Œå…¶ä½™ 50â€“1 ä¸ªæ ‡è®°æ˜¯åœ¨æ ‡è®°åŒ–æ­¥éª¤ä¸­é€‰æ‹©çš„æ ‡è®°ã€‚(å¾ˆå¿«ï¼Œä½ å°±ä¼šæ˜Žç™½ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨ä¸‹ä¸€èŠ‚è¯¾è®¾ç½®åµŒå…¥å±‚çš„æ—¶å€™è¦åœ¨ç¬¬ä¸€è¡Œæ”¾ä¸€ä¸ªé›¶å‘é‡ã€‚)
*   åµŒå…¥çŸ©é˜µçš„åˆ—æ•°:word2vec ç»´æ•°ä¸º 96(ä½¿ç”¨â€œen_core_web_smâ€æ—¶)ï¼Œæ‰€ä»¥åˆ—æ•°ä¸º 96ã€‚

è¿™é‡Œæˆ‘ä»¬æœ‰å½¢çŠ¶ä¸º(50ï¼Œ96)çš„åµŒå…¥çŸ©é˜µ(å³ 2-d æ•°ç»„)ã€‚è¯¥åµŒå…¥çŸ©é˜µå°†åœ¨è¯¥ NLP æ¨¡åž‹å‡†å¤‡æµæ°´çº¿çš„æœ€åŽä¸€æ­¥ä¸­è¢«é¦ˆå…¥ TensorFlow åµŒå…¥å±‚ã€‚

> æ³¨æ„:æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°æ‰€æœ‰çš„ is_oov å€¼éƒ½æ˜¯ Trueã€‚ä½†æ˜¯ä½ ä»ç„¶ä¼šå¾—åˆ°éžé›¶çš„åµŒå…¥å‘é‡ã€‚ä½¿ç”¨ spaCyâ€œen _ core _ web _ smâ€æ¨¡åž‹ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚

# å°è´´å£«:å¦‚ä½•çœ‹å¾…å•è¯åµŒå…¥ä¸­çš„ OOV

ä¸Žâ€œen_core_web_mdâ€åœ¨ä»¤ç‰Œä¸åœ¨åµŒå…¥æ¨¡åž‹ä¸­æ—¶è¿”å›žé›¶å‘é‡ä¸åŒï¼Œâ€œen_core_web_smâ€çš„å·¥ä½œæ–¹å¼å°†ä½¿å…¶æ€»æ˜¯è¿”å›žä¸€äº›éžé›¶å‘é‡ã€‚ç„¶è€Œï¼Œæ ¹æ® [spaCy çºªå½•ç‰‡](https://spacy.io/models/en)ï¼Œâ€œen_core_web_smâ€è¿”å›žçš„å‘é‡å¹¶ä¸åƒâ€œen_core_web_mdâ€æˆ–â€œen_core_web_lgâ€é‚£æ ·â€œç²¾ç¡®â€ã€‚

å–å†³äºŽåº”ç”¨ï¼Œæ‚¨å¯ä»¥å†³å®šé€‰æ‹©â€œä¸å¤ªç²¾ç¡®â€çš„åµŒå…¥æ¨¡åž‹ï¼Œä½†æ€»æ˜¯ç»™å‡ºéžé›¶å‘é‡æˆ–æ¨¡åž‹è¿”å›žâ€œæ›´ç²¾ç¡®â€çš„å‘é‡ï¼Œä½†æœ‰æ—¶åœ¨çœ‹åˆ° oov æ—¶æ˜¯é›¶å‘é‡ã€‚

åœ¨æ¼”ç¤ºä¸­ï¼Œæˆ‘é€‰æ‹©äº†â€œen_core_web_smâ€æ¨¡åž‹ï¼Œå®ƒæ€»æ˜¯ç»™æˆ‘ä¸€äº›éžé›¶çš„åµŒå…¥å‘é‡ã€‚ä¸€ç§ç­–ç•¥å¯ä»¥æ˜¯åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨ä¸ºå­è¯ç‰‡æ®µå­¦ä¹ çš„å‘é‡ï¼Œç±»ä¼¼äºŽäººä»¬é€šå¸¸å¯ä»¥ä»Žç†Ÿæ‚‰çš„è¯æ ¹ä¸­æ‰¾å‡ºå•è¯çš„è¦ç‚¹ã€‚æœ‰äº›äººæŠŠè¿™ç§ç­–ç•¥ç§°ä¸ºâ€œä¸ç²¾ç¡®æ€»æ¯”ä»€ä¹ˆéƒ½æ²¡æœ‰å¥½â€ã€‚(è™½ç„¶æˆ‘ä¸ç¡®å®š spaCy å¦‚ä½•ç»™ OOVs åˆ†é…éžé›¶å€¼ã€‚)

# æœ€åŽï¼ŒåµŒå…¥å±‚è®¾ç½®

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æœ‰å¡«å……çš„æ ‡è®°åºåˆ—æ¥è¡¨ç¤ºåŽŸå§‹æ–‡æœ¬æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªåµŒå…¥çŸ©é˜µï¼Œæ¯è¡Œéƒ½ä¸Žä»¤ç‰Œç›¸å…³è”ã€‚çŽ°åœ¨æ˜¯æ—¶å€™è®¾ç½® TensorFlow åµŒå…¥å±‚äº†ã€‚

ä¸‹å›¾æ€»ç»“äº†åµŒå…¥å±‚æœºåˆ¶ã€‚

![](img/5d3acc42b31f177e674829de54b1f6db.png)

åµŒå…¥å±‚æ’å›¾ï¼Œ(ä½œè€…åˆ›ä½œ)

**è§£è¯´:**

åµŒå…¥å±‚é€šè¿‡åµŒå…¥çŸ©é˜µ(ä½œä¸ºæƒé‡)åœ¨æ ‡è®°åºåˆ—(ä½œä¸ºè¾“å…¥)å’Œå•è¯åµŒå…¥è¡¨ç¤º(ä½œä¸ºè¾“å‡º)ä¹‹é—´å»ºç«‹æ¡¥æ¢ã€‚

*   **åµŒå…¥å±‚çš„è¾“å…¥**:å¡«å……åŽçš„åºåˆ—ä½œä¸ºåµŒå…¥å±‚çš„è¾“å…¥é€å…¥ï¼›å¡«å……åºåˆ—çš„æ¯ä¸ªä½ç½®è¢«æŒ‡å®šç»™ä¸€ä¸ªä»¤ç‰Œ idã€‚
*   **åµŒå…¥å±‚çš„æƒé‡**:é€šè¿‡æŸ¥æ‰¾åµŒå…¥çŸ©é˜µï¼ŒåµŒå…¥å±‚å¯ä»¥æ‰¾åˆ°ä¸Ž token-id ç›¸å…³è”çš„å•è¯(ä»¤ç‰Œ)çš„ word2vec è¡¨ç¤ºã€‚æ³¨æ„ï¼Œå¡«å……åºåˆ—ä½¿ç”¨é›¶æ¥æŒ‡ç¤ºç©ºçš„ä»¤ç‰Œï¼Œä»Žè€Œäº§ç”Ÿé›¶åµŒå…¥å‘é‡ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨åµŒå…¥çŸ©é˜µä¸­ä¸ºç©ºä»¤ç‰Œä¿å­˜äº†ç¬¬ä¸€è¡Œã€‚
*   **åµŒå…¥å±‚çš„è¾“å‡º**:åœ¨éåŽ†è¾“å…¥çš„å¡«å……åºåˆ—åŽï¼ŒåµŒå…¥å±‚ç”¨ä»£è¡¨å‘é‡(word2vec)â€œæ›¿æ¢â€token-idï¼Œå¹¶è¾“å‡ºåµŒå…¥åºåˆ—ã€‚

> **æ³¨æ„:**çŽ°ä»£ NLP ç‰¹å¾æå–çš„å…³é”®:**å¦‚æžœä¸€åˆ‡æ­£å¸¸ï¼ŒåµŒå…¥å±‚çš„è¾“å‡ºåº”è¯¥å¾ˆå¥½åœ°ä»£è¡¨åŽŸå§‹æ–‡æœ¬ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½å­˜å‚¨åœ¨å•è¯åµŒå…¥æƒé‡ä¸­ï¼›è¿™æ˜¯çŽ°ä»£è‡ªç„¶è¯­è¨€å¤„ç†ç‰¹å¾æå–çš„æ ¸å¿ƒæ€æƒ³ã€‚**ä½ å¾ˆå¿«å°±ä¼šçœ‹åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸ºåµŒå…¥å±‚è®¾ç½® trainable = True æ¥å¾®è°ƒè¿™ä¸ªæƒé‡ã€‚

è¿˜è¦æ³¨æ„ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ç©ºä»¤ç‰Œçš„ word2vec æ˜¾å¼æŒ‡å®šä¸ºé›¶ï¼Œåªæ˜¯ä¸ºäº†è¿›è¡Œæ¼”ç¤ºã€‚äº‹å®žä¸Šï¼Œä¸€æ—¦åµŒå…¥å±‚çœ‹åˆ° 0-token-idï¼Œå®ƒå°†ç«‹å³ä¸ºè¯¥ä½ç½®åˆ†é…ä¸€ä¸ªé›¶å‘é‡ï¼Œè€Œæ— éœ€æŸ¥çœ‹åµŒå…¥çŸ©é˜µã€‚

# åœ¨å¼ é‡æµä¸­

ä»¥ä¸‹ç¤ºä¾‹ä»£ç æ˜¾ç¤ºäº†å¦‚ä½•åœ¨ TensorFlow ä¸­å®ŒæˆåµŒå…¥ï¼Œ

```
from tensorflow.keras.initializers import Constant
from tensorflow.keras.layers import Embedding# MAX_NUM_TOKENS = 50
EMBEDDING_DIM = embedding_matrix.shape[1]
# MAX_SEQUENCE_LENGTH = 10
embedding_layer = Embedding(input_dim=MAX_NUM_TOKENS,
                            output_dim=EMBEDDING_DIM,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=MAX_SEQUENCE_LENGTH,
														mask_zero=True,
                            trainable=False)
```

**è§£è¯´:**

*   ä¸Žç»´åº¦ç›¸å…³çš„å‚æ•°æ˜¯â€œè¾“å…¥å°ºå¯¸â€ã€â€œè¾“å‡ºå°ºå¯¸â€å’Œâ€œè¾“å…¥é•¿åº¦â€ã€‚é€šè¿‡å‚è€ƒå›¾ç¤ºï¼Œæ‚¨åº”è¯¥å¯¹å¦‚ä½•è®¾ç½®è¿™äº›å‚æ•°æœ‰å¾ˆå¥½çš„ç›´è§‰ã€‚
*   åœ¨ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„å•è¯åµŒå…¥æ¨¡åž‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨[tensor flow . keras . initializer . constant](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/Constant)å°†åµŒå…¥çŸ©é˜µé¦ˆå…¥ä¸€ä¸ªåµŒå…¥å±‚ã€‚å¦åˆ™ï¼ŒåµŒå…¥å±‚çš„æƒé‡å°†ç”¨ä¸€äº›éšæœºæ•°åˆå§‹åŒ–ï¼Œç§°ä¸ºâ€œä»Žé›¶å¼€å§‹è®­ç»ƒå•è¯åµŒå…¥â€ã€‚
*   `trainable=`åœ¨æœ¬ä¾‹ä¸­è®¾ç½®ä¸º Falseï¼Œè¿™æ ·åœ¨ç¥žç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­ word2vec çš„æƒé‡ä¸ä¼šæ”¹å˜ã€‚è¿™æœ‰åŠ©äºŽé˜²æ­¢è¿‡åº¦æ‹Ÿåˆï¼Œå°¤å…¶æ˜¯åœ¨ç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶ã€‚ä½†å¦‚æžœä½ æƒ³å¾®è°ƒæƒé‡ï¼Œä½ çŸ¥é“è¯¥æ€Žä¹ˆåšã€‚(è®¾ç½®`trainable=True`
*   `mask_zero=`æ˜¯ä½ åº”è¯¥æ³¨æ„çš„å¦ä¸€ä¸ªè®ºç‚¹ã€‚æŽ©è†œæ˜¯ä¸€ç§å‘ŠçŸ¥åºåˆ—å¤„ç†å±‚è¾“å…¥ä¸­çš„æŸäº›ä½ç½®ç¼ºå¤±çš„æ–¹å¼ï¼Œå› æ­¤åœ¨å¤„ç†æ•°æ®æ—¶åº”è¯¥è·³è¿‡ã€‚é€šè¿‡è®¾ç½®å‚æ•°`mask_zero=True`ï¼Œä¸ä»…åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ï¼Œè€Œä¸”æ›´å¥½åœ°å†çŽ°äº†åŽŸæ–‡ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹æ¥æ£€æŸ¥åµŒå…¥å±‚çš„è¾“å‡ºã€‚åµŒå…¥å±‚çš„è¾“å‡ºå¼ é‡çš„å½¢çŠ¶åº”è¯¥æ˜¯[num_sequenceï¼Œpadded_sequence_lengthï¼Œembedding_vector_dim]ã€‚

```
# output
embedding_output = embedding_layer(trainvalid_data_post)
# result = embedding_layer(inputs=trainvalid_data_post[0])
embedding_output.shape>>TensorShape([18, 10, 96])# check if tokens and embedding vectors mathch
print(trainvalid_data_post[1])
embedding_output[1]>>[21  4  2 12 22 23 13  2 24  6]
>><tf.Tensor: shape=(10, 96), dtype=float32, numpy=
array([[-1.97691965e+00, -1.57780576e+00,  1.16705105e-01,
        -2.46210432e+00,  1.27643692e+00,  4.56989884e-01,
         ...
			[ 2.83787537e+00,  1.16508913e+00,  1.27923262e+00,
        -1.44432998e+00, -7.07145482e-02, -1.63411784e+00,
				 ...
```

ä»…æ­¤è€Œå·²ã€‚æ‚¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒæ‚¨çš„æ–‡æœ¬æ•°æ®ã€‚(æ‚¨å¯ä»¥å‚è€ƒç¬”è®°æœ¬ï¼ŒæŸ¥çœ‹ä½¿ç”¨ RNN å’Œ CNN çš„åŸ¹è®­ã€‚)

# æ‘˜è¦

ä¸º NLP æ·±åº¦å­¦ä¹ å‡†å¤‡æ•°æ®ï¼Œæˆ‘ä»¬ç»åŽ†äº†å¾ˆé•¿çš„è·¯ã€‚ä½¿ç”¨ä»¥ä¸‹æ¸…å•æ¥æµ‹è¯•æ‚¨çš„ç†è§£ç¨‹åº¦:

**è®°å·åŒ–:**åœ¨è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåˆ›å»ºè®°å·å­—å…¸ï¼Œå¹¶å‚ç…§åˆ›å»ºçš„è®°å·å­—å…¸ï¼Œç”¨è®°å·(æˆ–è®°å·-id)è¡¨ç¤ºåŽŸæ–‡ã€‚åœ¨ TensorFlow ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Tokenizer è¿›è¡Œæ ‡è®°åŒ–ã€‚

*   åœ¨æ ‡è®°åŒ–è¿‡ç¨‹ä¸­é€šå¸¸éœ€è¦é¢„å¤„ç†ã€‚è™½ç„¶ä½¿ç”¨ Tensorflow çš„ä»¤ç‰ŒåŒ–å™¨åŠå…¶é»˜è®¤è®¾ç½®æœ‰åŠ©äºŽå¯åŠ¨ç®¡é“ï¼Œä½†å‡ ä¹Žæ€»æ˜¯å»ºè®®åœ¨ä»¤ç‰ŒåŒ–æœŸé—´æ‰§è¡Œé«˜çº§é¢„å¤„ç†å’Œ/æˆ–åŽå¤„ç†ã€‚
*   è¯æ±‡å¤–(OOV)æ˜¯æ ‡è®°åŒ–çš„ä¸€ä¸ªå¸¸è§é—®é¢˜ã€‚æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬åœ¨æ›´å¤§çš„è¯­æ–™åº“ä¸Šè®­ç»ƒæˆ–ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ ‡è®°å™¨ã€‚
*   åœ¨ TensorFlow çº¦å®šä¸­ï¼Œ0-token-id ä¿ç•™ç»™ç©ºä»¤ç‰Œï¼Œè€Œå…¶ä»– NLP åŒ…å¯èƒ½ä¼šå°†ä»¤ç‰Œåˆ†é…ç»™ 0-token-idã€‚å°å¿ƒè¿™ç§å†²çªï¼Œå¦‚æžœéœ€è¦çš„è¯ï¼Œè°ƒæ•´ä»¤ç‰Œ id å‘½åã€‚

**å¡«å……:**å°†åºåˆ—å¡«å……æˆ–æˆªæ–­è‡³ç›¸åŒé•¿åº¦ï¼Œå³å¡«å……åŽçš„åºåˆ—å…·æœ‰ç›¸åŒæ•°é‡çš„æ ‡è®°(åŒ…æ‹¬ç©ºæ ‡è®°)ã€‚åœ¨ TensorFlow ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ pad_sequences è¿›è¡Œå¡«å……ã€‚

*   å¯¹äºŽ RNN ä½“ç³»ç»“æž„ï¼Œå»ºè®®åœ¨(è®¾ç½®ä¸ºâ€œpostâ€)ä¹‹åŽå¡«å……å’Œæˆªæ–­åºåˆ—ã€‚
*   å¡«å……åºåˆ—é•¿åº¦å¯ä»¥è®¾ç½®ä¸ºå¡«å……(æˆ–æˆªæ–­)å‰åºåˆ—çš„å¹³å‡å€¼æˆ–ä¸­å€¼ã€‚

**å•è¯åµŒå…¥:**å¯ä»¥é€šè¿‡å‚è€ƒåµŒå…¥æ¨¡åž‹ï¼Œä¾‹å¦‚ word2vecï¼Œå°†è®°å·æ˜ å°„åˆ°å‘é‡ã€‚åµŒå…¥å‘é‡æ‹¥æœ‰äººç±»å’Œæœºå™¨éƒ½èƒ½ç†è§£çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ spaCyâ€œen _ core _ web _ smâ€ã€â€œen_core_web_mdâ€æˆ–â€œen_core_web_lgâ€è¿›è¡Œå•è¯åµŒå…¥ã€‚

*   ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„å•è¯åµŒå…¥æ¨¡åž‹æ˜¯ä¸€ä¸ªå¥½çš„å¼€å§‹ã€‚ä¸éœ€è¦æ‰¾åˆ°â€œå®Œç¾Žçš„â€é¢„è®­ç»ƒå•è¯åµŒå…¥æ¨¡åž‹ï¼›é¦–å…ˆï¼Œå°±æ‹¿ä¸€ä¸ªã€‚ç”±äºŽ Tensorflow è¿˜æ²¡æœ‰ word embeddings APIï¼Œæ‰€ä»¥é€‰æ‹©ä¸€ä¸ªå¯ä»¥åœ¨æ·±åº¦å­¦ä¹ ç®¡é“ä¸­è½»æ¾åº”ç”¨çš„åŒ…ã€‚åœ¨æ­¤é˜¶æ®µï¼Œæž„å»ºç®¡é“æ¯”å®žçŽ°æ›´å¥½çš„æ€§èƒ½æ›´é‡è¦ã€‚
*   å¯¹äºŽä½¿ç”¨é¢„è®­ç»ƒæ¨¡åž‹çš„å•è¯åµŒå…¥ï¼ŒOOV ä¹Ÿæ˜¯ä¸€ä¸ªé—®é¢˜ã€‚å¤„ç† OOV çš„ä¸€ä¸ªæ½œåœ¨è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡ä½¿ç”¨åœ¨è®­ç»ƒæœŸé—´ä¸ºå­è¯ç‰‡æ®µå­¦ä¹ çš„å‘é‡ã€‚å¦‚æžœå¯èƒ½çš„è¯ï¼Œè¿™ç§â€œçŒœæµ‹â€é€šå¸¸æ¯”ä½¿ç”¨é›¶å‘é‡æ¥é¢„æµ‹ oov ç»™å‡ºæ›´å¥½çš„ç»“æžœï¼Œå› ä¸ºé›¶å‘é‡ä¼šç»™æ¨¡åž‹å¸¦æ¥å™ªå£°ã€‚

**TensorFlow ä¸­çš„åµŒå…¥å±‚:**ä¸ºäº†åˆ©ç”¨é¢„è®­ç»ƒçš„å•è¯åµŒå…¥ï¼Œtensor flow ä¸­çš„åµŒå…¥å±‚çš„è¾“å…¥åŒ…æ‹¬ç”± token-id è¡¨ç¤ºçš„å¡«å……åºåˆ—ï¼Œä»¥åŠå­˜å‚¨ä¸Žå¡«å……åºåˆ—å†…çš„ä»¤ç‰Œç›¸å…³è”çš„åµŒå…¥å‘é‡çš„åµŒå…¥çŸ©é˜µã€‚è¾“å‡ºæ˜¯ä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼Œå…¶å½¢çŠ¶ä¸º[æ•°å­—åºåˆ—ï¼Œå¡«å……åºåˆ—é•¿åº¦ï¼ŒåµŒå…¥å‘é‡å°ºå¯¸]ã€‚

*   åµŒå…¥å±‚æœ‰è®¸å¤šå‚æ•°è®¾ç½®ã€‚ä½¿ç”¨çŽ©å…·æ•°æ®é›†æ¥ç¡®ä¿åµŒå…¥å›¾å±‚çš„è¡Œä¸ºç¬¦åˆæ‚¨çš„ç†è§£ã€‚åº”è¯¥ç‰¹åˆ«æ³¨æ„è¾“å…¥å’Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶ã€‚
*   æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½® trainable = True æ¥å¾®è°ƒåµŒå…¥çŸ©é˜µã€‚
*   é€šè¿‡è®¾ç½® mask_zero=Trueï¼Œå¯ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚æ­¤å¤–ï¼Œå®ƒæ›´å¥½åœ°å†çŽ°äº†åŽŸæ–‡ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨ RNN å¼å»ºç­‘çš„æ—¶å€™ã€‚ä¾‹å¦‚ï¼Œæœºå™¨å°†è·³è¿‡é›¶æ•°æ®ï¼Œå¹¶ä¸”æ— è®ºå¦‚ä½•éƒ½å°†ç›¸å…³é‡é‡ä¿æŒä¸º 0ï¼Œå³ä½¿ trainable = Trueã€‚

å¦‚æžœä½ è¿˜æ²¡æœ‰æ£€æŸ¥è¿‡ç¬”è®°æœ¬ï¼Œè¿™é‡Œæœ‰é“¾æŽ¥:

[](https://drive.google.com/file/d/1E7l_XJ1HnhaGXwUo4jpYv5WZwmULqU92/view?usp=sharing) [## TF _ NLP _ tokenizer _ embedding . ipynb

### åˆä½œç¬”è®°æœ¬

drive.google.com](https://drive.google.com/file/d/1E7l_XJ1HnhaGXwUo4jpYv5WZwmULqU92/view?usp=sharing) 

æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ã€‚ä¸‹æ¬¡è§ã€‚

## å‚è€ƒ:

*   å®žç”¨è‡ªç„¶è¯­è¨€å¤„ç†:æž„å»ºçœŸå®žä¸–ç•Œè‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿçš„ç»¼åˆæŒ‡å—-Oreilly&Associates Inc(2020)
*   [è‡ªç„¶è¯­è¨€å¤„ç†å®žè·µ:ä½¿ç”¨ Python-Manning å‡ºç‰ˆç‰©ç†è§£ã€åˆ†æžå’Œç”Ÿæˆæ–‡æœ¬(2019)](https://www.manning.com/books/natural-language-processing-in-action)
*   [æ·±åº¦å­¦ä¹ ä¸Ž Python-Manning å‡ºç‰ˆç‰©(2018)](https://www.manning.com/books/deep-learning-with-python)