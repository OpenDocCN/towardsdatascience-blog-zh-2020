<html>
<head>
<title>From Linear Regression to Ridge Regression, the Lasso, and the Elastic Net</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从线性回归到岭回归，套索和弹性网</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-linear-regression-to-ridge-regression-the-lasso-and-the-elastic-net-4eaecaf5f7e6?source=collection_archive---------2-----------------------#2020-11-06">https://towardsdatascience.com/from-linear-regression-to-ridge-regression-the-lasso-and-the-elastic-net-4eaecaf5f7e6?source=collection_archive---------2-----------------------#2020-11-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/5db46f7ce6e38a69af062c8bd713e863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nrWncnoJ4V_BkzEf1pd4MA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图1:普通回归与套索回归、山脊回归和弹性网回归相比的图像。图片引用:邹，h .，&amp;哈斯蒂，T. (2005)。通过弹性网的正则化和变量选择。</p></figure><div class=""/><div class=""><h2 id="27d7" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">以及为什么您应该学习替代回归技术</h2></div><h1 id="c10b" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">简介:</h1><p id="3aa1" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">普通最小二乘法((<em class="mi">)OLS(</em>)是最古老、最简单的回归算法之一。然而，现在有几个变种被发明出来，以解决使用常规最小二乘回归时遇到的一些弱点。</p><p id="1767" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">尽管是最古老的算法之一，线性模型仍然非常有用。事实上，它们往往能胜过花哨复杂的模型。当没有大量的观察值时，或者当输入可靠地预测响应时(低信噪比)，它们特别有用。</p><p id="240c" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">在本文中，我们将首先回顾使用线性回归的基本公式，讨论如何使用梯度下降法求解参数(权重)，然后介绍岭回归。然后我们将讨论套索，最后是弹性网。这篇文章也将属于我的从零开始构建机器学习算法系列(大部分)。到目前为止，我已经从头开始讨论了逻辑回归，从奇异值分解和遗传算法中导出主成分。</p><p id="62dc" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">我们将使用来自1989年研究的真实世界癌症数据集来了解其他类型的回归、收缩以及为什么有时线性回归是不够的。</p><h1 id="3a65" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">癌症数据:</h1><p id="e7fe" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">这个<a class="ae mo" href="https://github.com/Robby955/CancerData/blob/master/prostate.txt" rel="noopener ugc nofollow" target="_blank">数据集</a>由1989年完成的一项真正的科学<a class="ae mo" href="https://pubmed.ncbi.nlm.nih.gov/2468795/" rel="noopener ugc nofollow" target="_blank">研究的97个观察数据组成。</a>数据包括8个预测因子，感兴趣的结果是lpsa(对数前列腺特异性抗原)。</p><p id="244e" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">这个数据集在<a class="ae mo" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank"> <em class="mi">统计学习的要素</em> </a> <em class="mi">中有所详细讨论。</em></p><p id="be5e" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">首先，我们加载将要使用的库，然后读入数据集。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="6ffb" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">下面是最初的一些观察结果:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mv"><img src="../Images/033c283944588aabd5a64d47b2e35a1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6YBZWE7ZRFllgkumyY7afg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图2:前列腺癌数据集的最初几个观察结果。预测因子包括log cavol、log weight、age、lbph、sci、lcp、gleason、pgg45。我们还有一个指示器，告诉我们观察值是属于训练集还是测试集。图片来自作者。</p></figure><p id="fbb7" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">在97个观察值中，67个被指示为属于训练，而剩余的30个被保存用于在算法训练结束时进行测试。请注意，我们不需要“Id”列或“train”列，因此我们将其删除。在回归之前，我们还经常建议缩放和居中我们的列。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="dc0c" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">我们首先将97个观察结果分成初始训练集和测试集。初始训练集的大小为67，剩余的30个观察值在测试集中。(x_train，y_train)和(x_test，y_test)。在本文的后面，我们将进一步把我们的训练集分解成一个训练/验证集。请注意，我们的模型将根据测试数据进行评估，因此我们在拟合模型时不使用任何测试数据。</p><h1 id="f43a" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">回归设置:</h1><p id="92f0" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">首先，考虑一个简单的回归问题，有<strong class="lo jg"> N </strong>个观察值(行)和p个预测值(列)，包括:</p><ul class=""><li id="b8d0" class="mw mx jf lo b lp mj ls mk lv my lz mz md na mh nb nc nd ne bi translated">N x 1个结果向量，<strong class="lo jg"> Y. </strong></li></ul><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/90394578f93d4f47932b298d6a4dc578.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/1*hDWSTPqKfPM5Zg8ShQkNNg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图3:我们N次观察的结果向量。图来自作者。</p></figure><ul class=""><li id="755f" class="mw mx jf lo b lp mj ls mk lv my lz mz md na mh nb nc nd ne bi translated">观察值的N x (p+1)矩阵，<strong class="lo jg"> X </strong></li></ul><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/02c6763472b2398a95852066eace8460.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*LTA1scIjvC_OfgWak7igtw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图4:N个观察值中的每一个都表示在一行中。考虑到截距或“偏差”项，我们还在每个观察值上加1。图来自作者。</p></figure><ul class=""><li id="800a" class="mw mx jf lo b lp mj ls mk lv my lz mz md na mh nb nc nd ne bi translated">(p+1) x 1个权重向量，<strong class="lo jg"> W </strong>。</li></ul><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/0761f28688e3cceda7862ed199fe2ed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*hGnv9oiBng_Vptz_Dw6n7A.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图5:我们的权重向量，w。图来自作者。</p></figure><p id="e305" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">为了获得我们的预测，我们将我们的权重W乘以我们的观测值x。因此，残差或真实结果与我们的预测之间的差异可以用N x 1矩阵表示:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/8ae13ef98b0cac8e699705cf6b2382e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*ybKXyomF01sToERTFMkHew.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图6:我们的预测减去我们的估计。请注意，估计值是通过将权重乘以我们的观察值获得的。我们的预测越接近真实值，该行就越接近零。图来自作者。</p></figure><p id="ca90" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">“完美”的情况是图6中的矩阵充满了零，因为这将代表对训练数据的完美拟合。但这种情况几乎从来没有发生过，这也可能是“过度拟合”模型的一种情况。</p><h1 id="c6e1" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated"><strong class="ak">成本函数:</strong></h1><p id="002a" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">为了确定一个模型有多好，我们需要一些“好”的定义。在线性回归中，这几乎总是均方误差(MSE)。这只是我们的估计和真实观察之间的误差平方和。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/63dd8c4d6c7b68fd06b96806d9c8c18b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*r4FWWup276clusG9keIJyA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图7:红点代表实际观察，表面代表我们在任一点(X1，X2)的预测。这些线条表示我们的预测与实际观测数据之间的距离。这些距离的平方和定义了我们的最小二乘成本。图片引用:该图片经许可使用，如图3.1所示，出现在《统计学习要素》第二版中。</p></figure><p id="8764" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">通常，这在一个例子中被定义为损失函数。对于整个训练数据，我们使用成本函数，即每个训练示例的平均损失。</p><p id="2fbc" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">为了找到成本曲面的最小值，我们使用<em class="mi">梯度下降</em>，这涉及到对每个参数进行求导。</p><p id="7a66" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">当只有两个参数时，成本表面实际上可以被可视化为等高线图。在更高维度中，我们不能直接看到表面，但是寻找最小值的过程是一样的。梯度下降依赖于<em class="mi">学习率</em>α，它控制我们采取的步长。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/9aa3892653c701972acb045b008a1365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*DHootdFfDVov_7BDt35-gA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图8:梯度下降就是采取步骤寻找损失面最小值的过程。图片引用:<a class="ae mo" href="https://www.researchgate.net/figure/Non-convex-optimization-We-utilize-stochastic-gradient-descent-to-find-a-local-optimum_fig1_325142728" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/figure/Non-convex-optimization-We-utilize-random-gradient-descent-to-find-a-local-optimum _ fig 1 _ 325142728</a></p></figure><p id="8010" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">对于每个时期或迭代，我们计算每个参数相对于成本函数的导数，并在方向(最陡)方向上迈出一步。这确保我们(最终)达到最小值。实际上，这并不简单，因为学习率可能太大或太小，导致陷入局部最优。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/574f146919ba276ef073cdd925c61955.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*fYlMDpJGJxk1-pwMUFL8vg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图9:梯度下降。训练我们的重量(参数)的步骤。这包括通过减去成本函数相对于权重的导数，再乘以α(学习率)来更新每个权重。图来自作者。</p></figure><p id="3e36" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">现在是定义我们稍后将使用的一些助手函数的好时机:</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><h1 id="7786" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">线性回归:</h1><p id="b981" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">线性回归是最简单的回归算法，于1875年首次被描述。“回归”这个名字来源于弗朗西斯·高尔顿注意到的<em class="mi">向平均值</em>回归的现象。这指的是这样一个事实，当非常高的父母或非常矮的父母的孩子通常仍然更高或更矮时，他们倾向于接近平均身高。这被称为“回归均值”。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/1e56e5ad9267e6f8be48e623ab7b9cc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*X99Dc36ZaRbuTzm3jgi06w.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><em class="nn">图10:高尔顿在遗传地位上回归平庸。图片引用:</em><a class="ae mo" href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2011.00509.x" rel="noopener ugc nofollow" target="_blank">https://RSS . online library . Wiley . com/doi/full/10.1111/j . 1740-9713.2011 . 00509 . x</a></p></figure><p id="6008" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><em class="mi">最小二乘回归</em>的工作原理是简单地拟合一条线(或二维以上的超曲面)并计算从估计值到实际观察点的距离。最小二乘模型是最小化模型和观测数据之间的平方距离的模型。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi no"><img src="../Images/2900d96c304ba4232372519face82ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*v6EnnH1WkXJ01YLcK--qhA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图11:线性回归的成本函数。成本是单个损失函数的归一化总和。这和均方误差乘以一个标量是一样的(最后的结果是等价的)。图来自作者。</p></figure><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/4d6eb17fd8d4056a57b91c5925f8329b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*ynsxwjenPPBDSC-fNaKeSA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图12:线性回归的成本函数的导数。图来自作者。</p></figure><p id="1f07" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">您可能会注意到，这可能会使我们的算法容易受到离群值的影响，其中一个孤立的观察值可能会极大地影响我们的估计。这是真的。换句话说，线性回归对异常值并不稳健。</p><p id="3048" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">另一个问题是，我们可能对训练数据拟合得太好了。假设我们有许多训练数据和许多预测器，有些具有共线性。我们可能会获得非常适合训练数据的线，但它可能不会在测试数据上表现得那么好。这就是替代线性回归方法的优势所在。因为我们在最小二乘法中考虑了所有的预测值，这使得它容易过度拟合，因为添加额外的预测值没有损失。</p><p id="afb8" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">因为线性回归不需要我们调整任何超参数，所以我们可以使用训练数据集来拟合我们的模型。然后，我们在测试数据集上评估线性模型，并获得我们的均方误差。</p><h1 id="700c" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">从零开始渐变下降:</h1><p id="5697" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">下面的代码从头开始实现梯度下降，我们提供了添加正则化参数的选项。默认情况下，“reg”设置为零，因此这将等同于与简单最小二乘法相关联的成本函数的梯度下降。当reg大于零时，算法将产生岭回归的结果。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="fb12" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">由于我们现在使用自定义函数，我们需要向矩阵x_train_scaled添加一列1，这将考虑截距项(将乘以权重W0的项)。我们还将对象转换成numpy数组，以便进行更简单的矩阵计算。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="fb4a" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">让我们来看看梯度下降是如何进行的:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/65e41e76db5615332576bb5bef92aae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*94eOLEFDAqh1EmiGqKog7Q.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图14:随着我们不断制造越来越好的砝码，成本下降得相当快。图来自作者。图来自作者。</p></figure><p id="105d" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">现在让我们使用通过梯度下降获得的权重来对我们的测试数据进行预测。我们内置的MSE函数将使用<em class="mi">wllinear</em>来计算预测，并将返回测试MSE。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="d32d" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">使用梯度下降来获得我们的权重，我们在我们的测试数据上获得0.547的MSE。</p><h1 id="a8f9" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">岭回归:</h1><p id="552d" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">与最小二乘成本函数相比，岭回归使用增强的成本函数。岭回归引入了一个附加的“正则化”参数，而不是简单的平方和，该参数会对权重的大小产生不利影响。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/161e9a723d4af1dbf1986899fa8da887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*KVEOIIEIC34rr8gNElpaBg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图15:岭回归的成本函数。成本是单个损失函数的归一化总和。该成本函数通过正参数λ来惩罚权重。图来自作者。</p></figure><p id="2c78" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">幸运的是，这个成本函数的导数仍然很容易计算，因此我们仍然可以使用梯度下降。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/95692dceb48119c95a1ed19269819b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*f1riSeYQXAGmzaoa0v3pvA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图16:岭回归的成本函数的导数。图来自作者。</p></figure><p id="e1ae" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg">快速事实:</strong></p><ul class=""><li id="0a39" class="mw mx jf lo b lp mj ls mk lv my lz mz md na mh nb nc nd ne bi translated">岭回归是<strong class="lo jg">吉洪诺夫正则化</strong>的特例</li><li id="17c8" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated">存在封闭形式的解，因为在矩阵上添加对角元素确保了它是可逆的。</li><li id="9880" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated">允许可容忍的额外偏置量，以换取效率的大幅提高。</li><li id="3fe1" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated">在神经网络中使用，这里称为<em class="mi">权重衰减</em>。</li><li id="0f8d" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated">当您有太多预测值，或预测值之间有高度多重共线性时使用。</li><li id="f618" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated">等效于lambda为0时的普通最小二乘。</li><li id="9512" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated">又名<strong class="lo jg"> L2 </strong>正规化。</li><li id="4ae5" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated">在应用岭之前，必须缩放预测值。</li></ul><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/4fac37f8b4e76a0884ac26e73e91948f.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*CZDf_v6U_TEDdHlF6JErRQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图17:二维情况下OLS估计和岭回归估计的比较。请注意，在成本函数中正则化项的原点处，岭估计被限制在一个圆内。岭估计可视为线性回归系数等高线与B1+B2≤λ定义的圆相交的点。图像引用:统计学习的要素，第2版。</p></figure><p id="2836" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">因为我们在岭回归中有一个超参数lambda，所以我们形成了一个额外的维持集，称为<em class="mi">验证集</em>。这与测试集是分开的，允许我们调整理想的超参数。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><h1 id="bea0" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">选择λ:</h1><p id="8bce" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">为了找到理想的λ，我们使用一系列可能的λ值来计算验证集的MSE。函数<em class="mi"> getRidgeLambda </em>在维持训练集上尝试一系列Lambda值，并在验证集上检查MSE。它返回理想的参数lambda，然后我们将使用它来拟合整个训练数据。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="2fdf" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">理想的λ是8.8，因为它导致验证数据的最低MSE。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="c5e8" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">使用交叉验证，我们获得了λ= 8.8的理想“reg”参数，因此我们使用该参数通过梯度下降来获得我们的岭估计。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="22e6" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">使用岭回归，我们在0.511的测试数据上得到甚至更好的MSE。请注意，与最小二乘法估计的系数相比，我们的系数被“缩小”了。</p><h1 id="344b" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">套索回归:</h1><p id="abdc" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">套索回归或(<em class="mi">最小绝对收缩和选择算子’</em>)也适用于替代成本函数；</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/640ea60326ba9bbc1d3c0b7cdfc7f5ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*MzVvLRrsHNX6txA27U9k-Q.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图Lasso回归的成本函数。我们仍然正则化，但使用L1正则化，而不是在山脊L2。这个成本函数的导数没有封闭形式。图来自作者。</p></figure><p id="cbac" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">然而，成本函数的导数没有封闭形式(由于<strong class="lo jg"> L1 </strong>在权重上的损失),这意味着我们不能简单地应用梯度下降。Lasso允许系数实际上被强制为零的可能性(见图19)，<strong class="lo jg">本质上使Lasso成为一种模型选择方法以及一种回归技术。</strong></p><p id="86fc" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg">快速事实:</strong></p><ul class=""><li id="7ca9" class="mw mx jf lo b lp mj ls mk lv my lz mz md na mh nb nc nd ne bi translated">被称为“诱导稀疏”的方法。</li><li id="a455" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated">有时被称为<em class="mi">基础追踪。</em></li></ul><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/5e89655f123ca24256cb6157b9b8d2f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*T2N2pp5BE26nshsH3XkpmQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图19:OLS估计和Lasso回归估计的比较。请注意，Lasso估计值被限制在成本函数中正则化项的原点处的一个框中。椭圆与边界框相交的点给出了套索估计。注意，在上面的例子中，我们在一个拐角处相交，这导致上面例子中的系数(B1)被设置为零。统计学习的要素，第二版。</p></figure><p id="8086" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">由于我们不能应用梯度下降，我们使用scikit-learn的内置函数来计算理想的权重。然而，这仍然需要我们选择理想的收缩参数(就像我们对山脊所做的那样)。我们采用与岭回归中相同的方法来搜索验证数据上的理想正则化参数。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="5628" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">Lasso在测试数据上提供了0.482的MSE，甚至小于岭和线性回归！而且，Lasso还将一些系数设置为零，完全排除在考虑范围之外。</p><h1 id="cfe6" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">弹性网:</h1><p id="6c79" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">最后，我们来到弹性网。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/ef54e8d7062d3c0b834efb71914fdfcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5qda942X54E4i9exXF3XmQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图20:弹性网的成本函数。它包含了L1和L2的损失。图来自作者。</p></figure><p id="4d86" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">弹性网有两个参数，因此，我们需要搜索组合网格，而不是搜索单个理想参数。因此训练可能会有点慢。不是直接搜索λ1和λ2，通常最好是搜索这两个参数之间的理想比率，以及λ1和λ2之和的α参数。</p><p id="e05c" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg">快速事实:</strong></p><ul class=""><li id="bb82" class="mw mx jf lo b lp mj ls mk lv my lz mz md na mh nb nc nd ne bi translated">线形、脊形和套索都可以看作是弹性网的特例。</li><li id="539c" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated">2014年证明了弹性网可以化简为线性支持向量机。</li><li id="8c78" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated">损失函数是强凸的，因此存在唯一的最小值。</li></ul><p id="7e71" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">这个<em class="mi">弹性网</em>是套索的延伸，它结合了<strong class="lo jg"> L1 </strong>和<strong class="lo jg"> L2 </strong>的正规化。所以L1需要一辆lambda1，L2需要一辆lambda2。类似于套索，导数没有封闭的形式，所以我们需要使用python的内置功能。我们还需要找到我们的两个参数之间的理想比率，以及作为λ1和λ2之和的附加α参数。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/edf18278c1a3ec39540607a136c4d1d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*L-1m7UrIeKJNEqBBk0xQAg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图21:弹性网(红色)是脊回归(绿色)和套索(蓝色)的组合。图片引用:<a class="ae mo" href="https://www.researchgate.net/figure/Visualization-of-the-elastic-net-regularization-red-combining-the-L2-norm-green-of_fig6_330380054" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/figure/Visualization-of-the-elastic-net-regularity-red-combining-the-L2-norm-green-of _ fig 6 _ 330380054</a></p></figure><p id="25bd" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">我们不会从头开始编码弹性网，scikit-learn提供了它。</p><p id="2381" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">然而，我们执行交叉验证来选择两个参数，alpha和l1_ratio。一旦我们有了理想的参数，我们就使用选择的参数在完全训练上训练我们的算法。</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="d2e8" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">哇！弹性网提供了比所有其他模型更小的MSE (0.450)。</p><h1 id="0b72" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">综合起来看:</h1><p id="b22b" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">最后，我们计算了最小二乘法、脊线法、套索法和弹性网格法的结果。我们已经获得了每种方法的权重，还获得了原始测试数据集的MSE。我们可以在表格中总结这些方法的表现。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/9cf3d44c5e59afbd0b4713cd85b97336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZSbdRxhiOVi6tyqe4X_WEw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图22:我们每个模型的最终比较。此表总结了测试集的最终估计系数和均方误差。图来自作者。</p></figure><p id="da5e" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">与所有其他模型相比，简单最小二乘法在我们的测试数据中表现最差。岭回归提供了与最小二乘法相似的结果，但它在测试数据上做得更好，并缩减了大多数参数。最终，Elastic Net在测试数据集上提供了最好的MSE，而且差距很大。弹性网去掉了lcp，格里森，年龄，缩水了其他参数。拉索也去除了年龄、lcp和格里森的考虑，但表现略差于弹性网。</p><h1 id="63c6" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">总结:</h1><p id="b1b8" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">了解基本的最小二乘回归仍然非常有用，但也应该考虑其他改进的方法。常规最小二乘法的一个问题是它没有考虑过拟合的可能性。岭回归通过缩小某些参数来解决这个问题。Lasso更进一步，允许将某些系数直接强制为零，从模型中消除它们。最后，弹性网结合了套索和脊的优点。</p><p id="4339" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">在某些情况下，我们可以推导出最小二乘的精确解，只要λ&gt; 0，我们总是可以推导出岭的解。选择lambda是困难的部分(你应该在训练数据集上使用交叉验证来选择理想的lambda)。在本指南中，我们没有显示封闭形式的解决方案，因为最好从头开始了解解决方案是如何解决的，并且因为封闭形式的解决方案通常不存在于高维空间中。</p><p id="24c1" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">感谢您的阅读，如果您有任何问题或意见，请发送给我！</p><p id="9139" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg">想了解更多？</strong></p><p id="aee3" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">如果您喜欢这些主题，并想学习更高级的回归技术，请查看以下主题:</p><ul class=""><li id="f34d" class="mw mx jf lo b lp mj ls mk lv my lz mz md na mh nb nc nd ne bi translated"><a class="ae mo" href="http://statweb.stanford.edu/~tibs/ftp/lars.pdf" rel="noopener ugc nofollow" target="_blank">拉斯(最小角度回归)。</a></li><li id="4f89" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated"><a class="ae mo" href="https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Principal_Components_Regression.pdf" rel="noopener ugc nofollow" target="_blank">主成分回归</a>。</li><li id="40c5" class="mw mx jf lo b lp nt ls nu lv nv lz nw md nx mh nb nc nd ne bi translated"><a class="ae mo" href="https://personal.utdallas.edu/~herve/Abdi-PLS-pretty.pdf" rel="noopener ugc nofollow" target="_blank">偏最小二乘法。</a></li></ul><h1 id="4fae" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">来源:</h1><p id="1924" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">来自统计学习的<em class="mi">元素的图像被允许使用。<em class="mi"> </em> <a class="ae mo" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank"> <em class="mi">“作者(Hastie)保留所有这些数字的版权。它们可以用在学术报告中。”</em> </a></em></p><p id="3051" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">GitHub上的代码:</p><div class="ip iq gp gr ir oe"><a href="https://github.com/Robby955/CancerData/blob/master/pythonRegression.py" rel="noopener  ugc nofollow" target="_blank"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd jg gy z fp oj fr fs ok fu fw je bi translated">Robby955/CancerData</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">github.com</p></div></div><div class="on l"><div class="oo l op oq or on os ix oe"/></div></div></a></div><p id="b802" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">[1]邹，h .，，哈斯蒂，T. (2005).通过弹性网的正则化和变量选择。皇家统计学会杂志:B辑(统计方法学)。</p><p id="a1e0" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">[2]阿米尼，亚力山大&amp;索莱马尼，艾娃&amp;卡拉曼，塞尔塔克&amp;鲁斯，达尼埃拉。(2018).<em class="mi"> </em> <a class="ae mo" href="https://www.researchgate.net/figure/Non-convex-optimization-We-utilize-stochastic-gradient-descent-to-find-a-local-optimum_fig1_325142728" rel="noopener ugc nofollow" target="_blank"> <em class="mi">用于端到端控制的空间不确定性采样</em> </a> <em class="mi">。神经信息处理系统。</em></p><p id="056a" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">[3]斯塔梅、T. A .、卡巴林、J. N .、麦克尼尔、J. E .、约翰斯通、I. M .、弗赖哈、f .、雷德温、E. A .、杨(1989)。前列腺特异性抗原在前列腺癌诊断和治疗中的应用。二。根治性前列腺切除术治疗的患者。<em class="mi">《泌尿外科杂志》</em>，<em class="mi"> 141 </em> (5)，1076–1083。<a class="ae mo" href="https://doi.org/10.1016/s0022-5347(17)41175-x" rel="noopener ugc nofollow" target="_blank">https://doi . org/10.1016/s 0022-5347(17)41175-x</a></p><p id="a2df" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><em class="mi">【4】</em>哈斯蒂，t .【哈斯蒂，t .】蒂布希拉尼，r .&amp;弗里德曼，J. H. <em class="mi"> (2001)。统计学习的</em> <strong class="lo jg"> <em class="mi">要素</em> </strong> <em class="mi">:数据挖掘、推断、预测。纽约:斯普林格。</em></p><p id="d6c2" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">[5]霍尔，A. E .，&amp;肯纳德，R. W. (1970年)。岭回归:非正交问题的有偏估计。<em class="mi">技术指标</em>。</p><p id="7df8" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">6 TiB shirani，R. (1996年)。通过套索的回归收缩和选择。<em class="mi">英国皇家统计学会杂志:B辑(方法论)</em>，<em class="mi"> 58 </em> (1)，267–288。</p></div></div>    
</body>
</html>