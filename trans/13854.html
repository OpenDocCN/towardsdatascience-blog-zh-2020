<html>
<head>
<title>All the Way from Information Theory to Log Loss in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从信息论到机器学习中的日志丢失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15?source=collection_archive---------27-----------------------#2020-09-23">https://towardsdatascience.com/all-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15?source=collection_archive---------27-----------------------#2020-09-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bba1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">熵、交叉熵、对数损失和背后的直觉</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e5d9c392c4d0dd1873a14e51418d7419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dl0F36mQqB_LNtA_dKd3ug.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·斯皮斯克在<a class="ae ky" href="https://unsplash.com/s/photos/information?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2bbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1948年，Claude Shannon在他长达55页的论文中介绍了信息论，这篇论文名为“<a class="ae ky" href="http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf" rel="noopener ugc nofollow" target="_blank">一种沟通的数学理论</a>”。信息论是我们开始讨论的地方，这将把我们带到log loss，这是机器学习和深度学习模型中广泛使用的成本函数。</p><p id="32ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">信息论的目标是有效地将信息从发送者传递到接收者。在数字时代，信息用比特0和1来表示。根据香农的说法，发送给接收者的一位信息意味着将接收者的不确定性降低两倍。因此，信息与不确定性的减少成正比。</p><p id="5d76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑抛一枚公平硬币的情况。正面朝上的概率P(正面)是0.5。在你(接收者)被告知正面朝上后，P(正面)变成1。因此，1比特的信息被发送给你，不确定性被降低了两倍。我们获得的信息量是不确定性的减少，不确定性是事件概率的倒数。</p><p id="5a98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">信息的位数可以通过不确定性减少的对数(以2为底)很容易地计算出来。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/4c789aa3e1c7ca7d7e732196d6e29e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*L6F7MmZEOUplVOZJ_hlqDA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="ac05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看一个稍微复杂一点的案例。你的两个朋友去商店买一件t恤，有4种不同的颜色可供选择。</p><p id="4b44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你的朋友朱莉娅有点犹豫不决，她告诉你她可以选择任何颜色。你的另一个朋友约翰告诉你，他喜欢蓝色，他很可能会买一件蓝色的t恤。</p><p id="4cc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你对朱莉娅的决定肯定比对约翰的更不确定。<strong class="lb iu">熵</strong>是一种量化不确定性的度量。更准确地说，它是从概率分布内的样本接收的平均信息量。</p><p id="4222" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下表显示了Julia和John购买t恤衫事件的概率分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/55327fec46b5394091291f2586d783d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*wLyn0zyhG3DRIBq5r3vVbQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">朱莉娅和约翰购买t恤衫的概率分布(图片由作者提供)</p></figure><p id="6daa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从朱莉娅开始。如果朱莉娅选蓝色，不确定性减少4 (1/0.25)。在以2为底的对数中等于2位(熵的基本单位是位)。因此，在蓝色的情况下，我们获得的信息量是2位。由于熵是样本的平均信息量，我们对其他颜色重复相同的计算。因为概率相同，所以它们的位数相同。对于Julia，熵的计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/199942c8da86f65a9da7fc7c3084bd15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P_pElJ93iAbHgZ2HCRbtBg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="161f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对约翰来说，步骤是一样的，但结果是不同的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ly"><img src="../Images/225cf9d5fd22623c062a7e746b490ff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CWQdPt0kaLa4k6bIGiy6PQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="bee5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熵更多的是在朱莉娅的情况下，所以我们对朱莉娅的决定有更多的不确定性，这是我们开始时预期的。</p><p id="eeec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经计算了熵。是时候引入公式了:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/e0c3d9f8a07cb6e526e90bf539e2d8d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*dOiEXJXDrXZwDxQ05bmnCA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">熵的公式(图片作者提供)</p></figure><p id="6bd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意</strong>:我们在计算中没有包括负号，因为它已经通过取概率的倒数(1 / p)而被消除了。</p><p id="87a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有两个事件，有四个结果。第一个事件是朱莉娅买了一件t恤衫，第二个事件是约翰买了一件t恤衫。熵分别是2比特和1.19比特。换句话说，平均来说，我们接收到关于第一个事件的2比特信息和关于第二个事件的1.19比特信息。</p><p id="41b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们正在朝着机器学习中使用的概念前进。下一个主题是<strong class="lb iu">交叉熵</strong>，它是平均消息长度。</p><p id="6364" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你的朋友选择的颜色是以数字方式(即以比特为单位)传输给你的。下表表示用于传输有关John选择的信息的两种不同编码。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/51324fee19e78a44e1814a529ad779e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*cgUkGNMktNpcY2339d_hQA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="5a4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在情况1中，每种颜色使用两位。因此，平均消息长度为2。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mb"><img src="../Images/74f65025100f4f88acaa2dfd65e50f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dCQVRVwkqmAdnrBgNc-RNQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="141f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Julia可以接受这种编码，但John不能接受。约翰的选择的概率分布的熵是1.19比特，因此平均使用2比特来发送关于他的选择的信息不是最佳方式。</p><p id="865e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在情况2中，交叉熵结果是1.3比特。它仍然大于1.19，但肯定比情况1更好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/bc9497c5a5252af1ef3e1b98f9b39204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D2ym1-btUzuUbsQCcGRMfA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="50da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是,“十字架”这个词从何而来？在计算交叉熵时，我们实际上是在比较两种不同的概率分布。一个是变量的实际概率分布，另一个是比特选择的预测概率分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/c7bdec23a4e57dd4a6af7689856b436e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*h0U3bpvNVuC_VIOgQSeNkw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="5bf8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">交叉熵可以表示为真实分布和预测分布的函数，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/d9249003d1c46fc2c33ac74fb9dae993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*888h6X0DX6Z7z_rXJ-VQaQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">交叉熵公式(图片由作者提供)</p></figure><p id="94bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你看看我们为寻找交叉熵所做的计算，你会注意到这些步骤与这个公式重叠。</p><p id="90b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以开始讨论交叉熵如何用于机器学习领域。交叉熵损失(即对数损失)是机器学习和深度学习模型中广泛使用的成本函数。</p><p id="3ebd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">交叉熵量化了两个概率分布的比较。在监督学习任务中，我们有一个目标变量，我们试图预测。使用交叉熵比较目标变量的实际分布和我们的预测。结果是交叉熵损失，也称为对数损失。</p><p id="80af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">交叉熵和交叉熵损失略有不同。计算损失时，通常使用自然对数，而不是以2为底的对数。</p><p id="21db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">交叉熵损失:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/1fe112a63f8de5f3a65f34ee73a427de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*gS5wk5pOeOVbcN5Cx2NFYQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">交叉熵损失(图片由作者提供)</p></figure><p id="e3e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来做一个例子。我们有一个4类的分类问题。我们的模型对特定观测值的预测如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/62017c73f6da127f453481c9129bc941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*PUg5Oz36tD5hXPtReSfIxQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="ea9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们知道真实的概率分布，所以对于真实类是100%，对于所有其他类是0。根据我们的模型，这个观察值所属的类是有80%概率的类1。该特定观察的交叉熵损失计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/48fa900302c2f1397e13d100ff499e2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*96_dV_PJf8bvErX6jnVcvA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="3c63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为除了实际类别之外，所有类别的真实概率都是零，所以只有实际类别的预测概率对交叉熵损失有贡献。</p><p id="8424" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，这是特定观察的损失。训练集或测试集的损失是该集中所有观测值的交叉熵的平均值。</p><h1 id="d9e6" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated"><strong class="ak">为什么会日志丢失？</strong></h1><p id="1ebf" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">您可能想知道为什么使用对数损失而不是分类精度作为成本函数。</p><p id="bd41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下表显示了两个不同模型对一个由5个观察值组成的相对较小的集合的预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/c9c87564408b4f4c77fe280af275a29d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*qw9pvKNjmds4-t5OVxdQVQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="afa8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两个模型都正确地对5个观察值中的4个进行了分类。因此，在分类精度方面，这些模型具有相同的性能。然而，概率揭示了模型1在预测中更确定。因此，总体而言，它的表现可能会更好。</p><p id="18d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对数损失(即交叉熵损失)提供了对分类模型的更稳健和准确的评估。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="f5b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="3670" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="9702" class="nn no it lb b lc nh lf ni li np lm nq lq nr lu ns nt nu nv bi translated"><a class="ae ky" href="http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf" rel="noopener ugc nofollow" target="_blank">http://people . math . Harvard . edu/~ CTM/home/text/others/Shannon/entropy/entropy . pdf</a></li><li id="1e91" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=ErfnhcEV1O8" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=ErfnhcEV1O8</a></li></ul></div></div>    
</body>
</html>