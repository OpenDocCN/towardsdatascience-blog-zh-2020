<html>
<head>
<title>Using neural networks with embedding layers to encode high cardinality categorical variables</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用具有嵌入层的神经网络来编码高基数分类变量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2?source=collection_archive---------18-----------------------#2020-10-23">https://towardsdatascience.com/using-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2?source=collection_archive---------18-----------------------#2020-10-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ca2a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们如何使用具有数千个不同值的分类特征？</h2></div><p id="972b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有多种方式对分类特征进行编码。如果类别之间不存在有序关系，one-hot-encoding是一个受欢迎的候选(即，为每个类别添加一个二进制特征)，还有<a class="ae lb" rel="noopener" target="_blank" href="/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159">和许多其他的</a>。但是一次性编码有一些缺点——可以通过使用嵌入来解决。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/bbc7a4a76202b21e500a024f7489ef17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJxmrZUbha21jizo5cqFXg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">稍微有帮助的插图，图片由作者使用<a class="ae lb" href="http://draw.io" rel="noopener ugc nofollow" target="_blank"> draw.io </a>完成</p></figure><p id="e142" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个缺点是，它不适合高基数类别:它将产生非常大/宽和稀疏的数据集，并且由于大量的要素，将需要大量的内存和正则化。此外，一次性编码没有利用类别之间的关系。假设你有<em class="ls">动物种类</em>作为特征，值类似于<em class="ls">家猫</em>、<em class="ls">老虎</em>和<em class="ls">大象</em>。与<em class="ls">家猫</em>和<em class="ls">大象</em>相比，<em class="ls">家猫</em>和<em class="ls">老虎</em>可能有更多的相似之处。这是一个更聪明的编码可以考虑的事情。在非常实际的场景中，这些类别可以是诸如客户、产品或位置之类的东西——非常高的基数类别，在各个观察之间具有相关的关系。</p><p id="6572" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解决这些问题的一种方法是使用嵌入，一个流行的例子是NLP问题中的单词嵌入。我们使用更小的密集空间，而不是使用巨大的二进制空间对类别进行编码。我们不是手动编码它们，而是定义嵌入空间的大小，然后尝试让模型学习有用的表示。对于我们的动物物种示例，我们可以检索类似于代表<em class="ls">家猫</em>、<code class="fe lt lu lv lw b">[1, 0.4, 0.1]</code>代表<em class="ls">老虎</em>、<code class="fe lt lu lv lw b">[0, 0.1, 0.6]</code>代表<em class="ls">大象</em>、<code class="fe lt lu lv lw b">[-0.5, 0.5, 0.4]</code>代表<em class="ls">鲨鱼</em>等等的表示。</p><p id="d72e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在接下来的文章中，我们将使用嵌入来构建一个神经网络来编码分类特征，此外，我们将针对一个没有分类变量的非常简单的线性模型和一个具有一次性编码特征的更复杂的正则化线性模型来对该模型进行基准测试。</p><h1 id="18d2" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">玩具的例子</h1><p id="d334" class="pw-post-body-paragraph kf kg iq kh b ki mp jr kk kl mq ju kn ko mr kq kr ks ms ku kv kw mt ky kz la ij bi translated">让我们看一个产生的玩具问题。假设我们反复从不同的供应商那里购买不同的产品，并且我们想要预测它们的大小。现在让我们假设每个产品都标有一个<code class="fe lt lu lv lw b">supplier_id</code>和一个<code class="fe lt lu lv lw b">product_id</code>，这是供应商和产品本身的标识符。我们还假设物品具有一些明显的尺寸/特征<code class="fe lt lu lv lw b">x1</code>和<code class="fe lt lu lv lw b">x2</code>，如<code class="fe lt lu lv lw b">price</code>和<code class="fe lt lu lv lw b">weight</code>，以及一些秘密的、不可测量的特征<code class="fe lt lu lv lw b">s1</code>、<code class="fe lt lu lv lw b">s2</code>和<code class="fe lt lu lv lw b">s3</code>，理论上可以这样计算出<code class="fe lt lu lv lw b">size</code>(S3对尺寸没有影响):</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="f138" class="my ly iq lw b gy mz na l nb nc">y = f(price, weight, s1, s2, s3) <br/>  = price + s1 + weight * s2</span></pre><p id="2959" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">问题是我们不知道秘密特征<code class="fe lt lu lv lw b">s1</code>、<code class="fe lt lu lv lw b">s2</code>和<code class="fe lt lu lv lw b">s3</code>，我们无法直接测量它们，这实际上是机器学习中一个相当常见的问题。但是我们在这里还有一点余地，因为我们有产品和供应商id——但是数量太多了，无法一次性编码并直接使用它们。让我们从经验中假设我们知道来自不同卖家的产品具有不同的尺寸，因此有理由假设来自卖家的物品具有非常相似的秘密属性。</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="9a93" class="my ly iq lw b gy mz na l nb nc">y = g(supplier_id, product_id, size, weight)</span></pre><p id="ec7b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">问题是，即使我们有几十万个不同的id，我们的模型能从产品和供应商id中学习到上面的关系<code class="fe lt lu lv lw b">g</code>吗？答案是肯定的，如果我们有足够的观测数据。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="b21c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看一个小数据集，以获得更好的图片。我们生成300个样本，在<code class="fe lt lu lv lw b">s1</code>中有4个不同的值，在<code class="fe lt lu lv lw b">s2</code>中有3个不同的值(记住<code class="fe lt lu lv lw b">s3</code>没有影响)，并可视化秘密属性对价格、重量和尺寸之间关系的明显影响。</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="9241" class="my ly iq lw b gy mz na l nb nc">import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="acfb" class="my ly iq lw b gy nf na l nb nc">data = generate_secret_data(n=300, s1_bins=3, s2_bins=6, s3_bins=2)<br/>data.head(10)<br/>##   s1   s2  s3  price  weight       y<br/>## 0  1  0.0   2  1.269   2.089   4.055<br/>## 1  3  2.0   1  2.412   1.283   9.764<br/>## 2  2  1.0   2  3.434   1.010   8.230<br/>## 3  1  3.0   1  4.493   1.837  12.791<br/>## 4  3 -2.0   2  4.094   2.562   3.756<br/>## 5  1  2.0   2  1.324   1.802   7.714<br/>## 6  1  2.0   1  2.506   1.910   9.113<br/>## 7  3 -2.0   1  3.626   1.864   4.685<br/>## 8  2  1.0   1  2.830   2.064   8.681<br/>## 9  1  2.0   1  4.332   1.100   9.319<br/>g = sns.FacetGrid(data, col='s2', hue='s1', col_wrap=3, height=3.5);<br/>g = g.map_dataframe(plt.scatter, 'weight', 'y');<br/>plt.show()</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ng"><img src="../Images/53db85a9bcf48d17625098fab21f6bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Esc-FxDwVc6FxxCu.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">每个s2属性的不同重量与尺寸(y)关系</p></figure><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="6ccf" class="my ly iq lw b gy mz na l nb nc">g = sns.FacetGrid(data, col='s2', hue='s1', col_wrap=3, height=3.5); g = g.map_dataframe(plt.scatter, 'price', 'y'); <br/>plt.show()</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ng"><img src="../Images/8f0ba5435c9a9fb50a928b7471259ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xb0-hoDMU9oIgQEB.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">每个s2物业的不同价格与规模(y)的关系</p></figure><h1 id="23ba" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">一个玩具的例子，它将如何出现在(一种)现实</h1><p id="9a18" class="pw-post-body-paragraph kf kg iq kh b ki mp jr kk kl mq ju kn ko mr kq kr ks ms ku kv kw mt ky kz la ij bi translated">但现在的问题是:我们不知道特性<code class="fe lt lu lv lw b">s1</code>、<code class="fe lt lu lv lw b">s2</code>和<code class="fe lt lu lv lw b">s3</code>，只知道产品id&amp;供应商id。为了模拟这个数据集在野外是如何出现的，让我们引入一个散列函数，我们将使用它来模糊我们不可测量的特性，并生成一些产品和供应商id。</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="35fc" class="my ly iq lw b gy mz na l nb nc">import hashlib</span><span id="9875" class="my ly iq lw b gy nf na l nb nc">def generate_hash(*args):<br/>    s = '_'.join([str(x) for x in args])<br/>    return hashlib.md5(s.encode()).hexdigest()[-4:]</span><span id="54ec" class="my ly iq lw b gy nf na l nb nc">generate_hash('a', 2)<br/>## '5724'<br/>generate_hash(123)<br/>## '4b70'<br/>generate_hash('a', 2)<br/>## '5724'</span></pre><p id="f93d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在可以用模糊属性生成数据，用产品id代替:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nd ne l"/></div></figure><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="a40b" class="my ly iq lw b gy mz na l nb nc">data = generate_data(n=300, s1_bins=4, s2_bins=1, s3_bins=2)<br/>data.head(10)<br/>##   product_id supplier_id  price  weight      y<br/>## 0       7235        a154  2.228   2.287  4.470<br/>## 1       9cb6        a154  3.629   2.516  8.986<br/>## 2       3c7e        0aad  3.968   1.149  8.641<br/>## 3       4184        0aad  3.671   2.044  7.791<br/>## 4       4184        0aad  3.637   1.585  7.528<br/>## 5       38f9        a154  1.780   1.661  4.709<br/>## 6       7235        a154  3.841   2.201  6.040<br/>## 7       efa0        0aad  2.773   2.055  4.899<br/>## 8       4184        0aad  3.094   1.822  7.104<br/>## 9       4184        0aad  4.080   2.826  8.591</span></pre><p id="eb87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们仍然看到不同的产品倾向于具有不同的值，但是我们不再能够容易地从产品id计算出<code class="fe lt lu lv lw b">size</code>值:</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="eae2" class="my ly iq lw b gy mz na l nb nc">sns.relplot(<br/>    x='price',<br/>    y='y',<br/>    hue='product_id',<br/>    sizes=(40, 400),<br/>    alpha=1,<br/>    height=4,<br/>    data=data<br/>);</span><span id="2b09" class="my ly iq lw b gy nf na l nb nc">plt.show()</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nh"><img src="../Images/edadab0edae3d0eabbfcd7815655ddf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QmujXeqa3KPJ4XWW.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">每个产品id的价格与尺寸(y)的关系—图片由作者制作</p></figure><p id="0523" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们生成一个更大的数据集。为了能够公平地将我们的嵌入模型与更简单的基线进行比较，并且为了能够验证我们的方法，我们将为我们的分类基数<code class="fe lt lu lv lw b">S1_BINS</code>、<code class="fe lt lu lv lw b">S2_BINS</code>、<code class="fe lt lu lv lw b">S3_BINS</code>假设相当小的值。</p><p id="7544" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果<code class="fe lt lu lv lw b">S1_BINS, S2_BINS, S3_BINS &gt;&gt; 10000</code>基准模型会遇到内存问题，性能会很差。</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="d9fc" class="my ly iq lw b gy mz na l nb nc">from sklearn.model_selection import train_test_split</span><span id="9d46" class="my ly iq lw b gy nf na l nb nc">N = 100000<br/>S1_BINS = 30<br/>S2_BINS = 3<br/>S3_BINS = 50<br/>data = generate_data(n=N, s1_bins=S1_BINS, s2_bins=S2_BINS, s3_bins=S3_BINS)</span><span id="c576" class="my ly iq lw b gy nf na l nb nc">data.describe()</span><span id="ce06" class="my ly iq lw b gy nf na l nb nc"># cardinality of c1 is approx S1_BINS * S3_BINS,<br/># c2 is approx. S2_BINS * S3_BINS<br/>##             price      weight           y<br/>## count  100000.000  100000.000  100000.000<br/>## mean        3.005       2.002      17.404<br/>## std         0.997       0.502       8.883<br/>## min        -1.052      -0.232      -2.123<br/>## 25%         2.332       1.664       9.924<br/>## 50%         3.004       2.002      17.413<br/>## 75%         3.676       2.341      24.887<br/>## max         7.571       4.114      38.641<br/>data.describe(include='object')<br/>##        product_id supplier_id<br/>## count      100000      100000<br/>## unique       1479         149<br/>## top          8851        0d98<br/>## freq          151        1376</span></pre><p id="c973" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在将数据分为特征和响应，以及训练和测试。</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="2ae0" class="my ly iq lw b gy mz na l nb nc">x = data[['product_id', 'supplier_id', 'price', 'weight']]<br/>y = data[['y']]<br/>x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=456)</span></pre><h1 id="8564" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">构建基准模型:天真和基线</h1><p id="368c" class="pw-post-body-paragraph kf kg iq kh b ki mp jr kk kl mq ju kn ko mr kq kr ks ms ku kv kw mt ky kz la ij bi translated">让我们首先组装一个非常简单的线性模型，它的性能非常差，并且只试图从<code class="fe lt lu lv lw b">price</code>和<code class="fe lt lu lv lw b">weight</code>中估计<code class="fe lt lu lv lw b">size</code>:</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="87ef" class="my ly iq lw b gy mz na l nb nc">from sklearn.linear_model import LinearRegression<br/>from sklearn.metrics import mean_squared_error, mean_absolute_error</span><span id="1268" class="my ly iq lw b gy nf na l nb nc">naive_model = LinearRegression()<br/>naive_model.fit(x_train[['price', 'weight']], y_train);</span><span id="964b" class="my ly iq lw b gy nf na l nb nc">y_pred_naive = naive_model.predict(x_test[['price', 'weight']])</span><span id="a212" class="my ly iq lw b gy nf na l nb nc">mean_squared_error(y_test, y_pred_naive)<br/>## 77.63320758421973<br/>mean_absolute_error(y_test, y_pred_naive)<br/>## 7.586725358761727</span></pre><p id="2baa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们观察<code class="fe lt lu lv lw b">price</code>和<code class="fe lt lu lv lw b">weight</code>与响应<code class="fe lt lu lv lw b">size</code>之间的相关性，忽略id，接近响应总体方差的不良性能是显而易见的:</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="8275" class="my ly iq lw b gy mz na l nb nc">sns.pairplot(data[['price', 'weight', 'y']].sample(1000));<br/>plt.show()</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ni"><img src="../Images/1538e1ca57d3b825c1c8e0603154f901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wYX7f0BFWqRu6ySBA14tgQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">价格、重量和尺寸(y)之间的相互关系—到处都是—图片由作者制作</p></figure><p id="2688" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了获得更好的基准，我们可以对分类特性进行一次性编码，并标准化数字数据，使用<code class="fe lt lu lv lw b">sklearns</code> <code class="fe lt lu lv lw b">ColumnTransformer</code>将这些转换应用于不同的列。由于要素的数量，我们将使用岭回归而不是正常的线性回归来保持系数较小(但非零，不像Lasso，这将导致丢失特定类的信息)。</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="9a48" class="my ly iq lw b gy mz na l nb nc">from sklearn.preprocessing import OneHotEncoder, StandardScaler<br/>from sklearn.compose import ColumnTransformer</span><span id="f5a1" class="my ly iq lw b gy nf na l nb nc">def create_one_hot_preprocessor():<br/>    return ColumnTransformer([<br/>        ('one_hot_encoder', OneHotEncoder(sparse=False, handle_unknown='ignore'), ['product_id', 'supplier_id']),<br/>        ('standard_scaler', StandardScaler(), ['price', 'weight'])]<br/>    )</span></pre><p id="649b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">one hot预处理器将特征扩展到列并使数据变宽，此外，数字特征标准化为零均值和单位方差:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h1 id="0513" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">建立具有嵌入层的神经网络，包括预处理和未知类别处理</h1><p id="daba" class="pw-post-body-paragraph kf kg iq kh b ki mp jr kk kl mq ju kn ko mr kq kr ks ms ku kv kw mt ky kz la ij bi translated">现在让我们为我们的类别建立一个具有嵌入层的神经网络模型。为了将它们提供给嵌入层，我们需要首先将分类变量映射到数字序列，即分别来自区间<code class="fe lt lu lv lw b">[0, #supplier ids]</code>的整数。<code class="fe lt lu lv lw b">[0, #product ids]</code>。</p><p id="1a70" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于<code class="fe lt lu lv lw b">sklearns</code> <code class="fe lt lu lv lw b">OrdinalEncoder</code>目前还不能处理未知值，我们需要随机应变。当随机分割测试数据或在预测期间看到新数据时，可能会出现未知类别。因此，我们必须使用编码器的简单实现(使用数据帧而不是数组，没有针对速度进行优化),它可以处理未知值:我们本质上使用有序字典作为散列表来将值映射到正整数范围，其中未知值将被映射到0(我们需要映射到非负值以符合稍后的嵌入层)。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="8050" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">编码器现在可以编码和解码我们的数据:</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="62f0" class="my ly iq lw b gy mz na l nb nc">ce = ColumnEncoder()</span><span id="1aa5" class="my ly iq lw b gy nf na l nb nc">ce.fit_transform(x_train)<br/>##        product_id  supplier_id  price  weight<br/>## 17414         941          104  2.536   1.885<br/>## 54089         330          131  3.700   1.847<br/>## 84350         960          122  3.517   2.341<br/>## 68797         423           77  4.942   1.461<br/>## 50994         617          138  4.276   1.272<br/>## ...           ...          ...    ...     ...<br/>## 55338         218          118  2.427   2.180<br/>## 92761         528           10  1.705   1.368<br/>## 48811         399           67  3.579   1.938<br/>## 66149         531          126  2.216   2.997<br/>## 30619        1141           67  1.479   1.888<br/>## <br/>## [75000 rows x 4 columns]<br/>ce.inverse_transform(ce.transform(x_train))<br/>##       product_id supplier_id  price  weight<br/>## 17414       a61d        b498  2.536   1.885<br/>## 54089       36e6        e41f  3.700   1.847<br/>## 84350       a868        d574  3.517   2.341<br/>## 68797       4868        80cf  4.942   1.461<br/>## 50994       69f3        eb54  4.276   1.272<br/>## ...          ...         ...    ...     ...<br/>## 55338       2429        cc4a  2.427   2.180<br/>## 92761       5c02        0ec5  1.705   1.368<br/>## 48811       426c        7a7d  3.579   1.938<br/>## 66149       5d45        dc6f  2.216   2.997<br/>## 30619       c73d        7a7d  1.479   1.888<br/>## <br/>## [75000 rows x 4 columns]<br/>x_train.equals(ce.inverse_transform(ce.transform(x_train)))<br/>## True</span></pre><p id="5432" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它还可以通过将未知数据映射到零类别来处理未知数据:</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="16d2" class="my ly iq lw b gy mz na l nb nc">unknown_data = pd.DataFrame({<br/>    'product_id': ['!§$%&amp;/()'],<br/>    'supplier_id': ['abcdefg'],<br/>    'price': [10],<br/>    'weight': [20],<br/>  })</span><span id="8902" class="my ly iq lw b gy nf na l nb nc">ce.transform(unknown_data)<br/>##    product_id  supplier_id  price  weight<br/>## 0           0            0     10      20<br/>ce.inverse_transform(ce.transform(unknown_data))<br/>##   product_id supplier_id  price  weight<br/>## 0       None        None     10      20</span></pre><p id="7de1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了将数据输入到模型中，我们需要分割输入，将其传递到不同的层，本质上是传递到<code class="fe lt lu lv lw b">X = [X_embedding1, X_embedding2, X_other]</code>。我们可以再次使用转换器来实现这一点，这次使用的是<code class="fe lt lu lv lw b">np.arrays</code>，因为<code class="fe lt lu lv lw b">StandardScaler</code>返回数组:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nd ne l"/></div></figure><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="2b64" class="my ly iq lw b gy mz na l nb nc">emb = EmbeddingTransformer(cols=[0, 1])<br/>emb.fit_transform(x_train.head(5))<br/>## [array([['a61d'],<br/>##        ['36e6'],<br/>##        ['a868'],<br/>##        ['4868'],<br/>##        ['69f3']], dtype=object), array([['b498'],<br/>##        ['e41f'],<br/>##        ['d574'],<br/>##        ['80cf'],<br/>##        ['eb54']], dtype=object), array([[2.5360678952988436, 1.8849677601403312],<br/>##        [3.699501628053666, 1.8469279753798342],<br/>##        [3.5168780519630527, 2.340554963373134],<br/>##        [4.941651644756232, 1.4606898248596456],<br/>##        [4.27624682317603, 1.2715509823965785]], dtype=object)]</span></pre><p id="98cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们将这两者结合起来，用训练数据装配预处理器，对类别进行编码，执行缩放并将其转换为正确的格式:</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="507c" class="my ly iq lw b gy mz na l nb nc">def create_embedding_preprocessor():<br/>  encoding_preprocessor = ColumnTransformer([<br/>      ('column_encoder', ColumnEncoder(), ['product_id', 'supplier_id']),<br/>      ('standard_scaler', StandardScaler(), ['price', 'weight'])<br/>  ])</span><span id="7831" class="my ly iq lw b gy nf na l nb nc">embedding_preprocessor = Pipeline(steps=[<br/>      ('encoding_preprocessor', encoding_preprocessor),<br/>      # careful here, column order matters:<br/>      ('embedding_transformer', EmbeddingTransformer(cols=[0, 1])),<br/>  ])</span><span id="57b7" class="my ly iq lw b gy nf na l nb nc">return embedding_preprocessor</span><span id="670a" class="my ly iq lw b gy nf na l nb nc">embedding_preprocessor = create_embedding_preprocessor()<br/>embedding_preprocessor.fit(x_train);</span></pre><p id="23bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们现在将这些数据提供给模型，它将无法学习未知类别的任何合理内容，即当我们进行<code class="fe lt lu lv lw b">fit</code>时，在训练数据<code class="fe lt lu lv lw b">x_train</code>中不存在的类别。因此，一旦我们试图对这些进行预测，我们可能会得到不合理的估计。</p><p id="1637" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解决这个词汇外问题的一种方法是将一些随机的训练观察值设置到未知的类别中。因此，在模型的训练期间，转换将使用0对这些进行编码，这是未知类别的标记，并且它将允许模型学习接近未知类别的平均值的东西。有了更多的领域知识，我们还可以选择任何其他类别作为默认类别，而不是随机抽样。</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="5568" class="my ly iq lw b gy mz na l nb nc"># vocab sizes<br/>C1_SIZE = x_train['product_id'].nunique()<br/>C2_SIZE = x_train['supplier_id'].nunique()</span><span id="186c" class="my ly iq lw b gy nf na l nb nc">x_train = x_train.copy()<br/>n = x_train.shape[0]</span><span id="75d8" class="my ly iq lw b gy nf na l nb nc"># set a fair share to unknown<br/>idx1 = np_random_state.randint(0, n, int(n / C1_SIZE))<br/>x_train.iloc[idx1,0] = '(unknown)'</span><span id="999c" class="my ly iq lw b gy nf na l nb nc">idx2 = np_random_state.randint(0, n, int(n / C2_SIZE))<br/>x_train.iloc[idx2,1] = '(unknown)'</span><span id="367b" class="my ly iq lw b gy nf na l nb nc">x_train.sample(10, random_state=1234)<br/>##       product_id supplier_id  price  weight<br/>## 17547       7340        6d30  1.478   1.128<br/>## 67802       4849        f7d5  3.699   1.840<br/>## 17802       de88        55a0  3.011   2.306<br/>## 36366       0912        1d0d  2.453   2.529<br/>## 27847       f254        56a6  2.303   2.762<br/>## 19006       2296   (unknown)  2.384   1.790<br/>## 34628       798f        5da6  4.362   1.775<br/>## 11069       2499        803f  1.455   1.521<br/>## 69851       cb7e        bfac  3.611   2.039<br/>## 13835       8497        33ab  4.133   1.773</span></pre><p id="6d06" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在可以转换数据</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="4edf" class="my ly iq lw b gy mz na l nb nc">x_train_emb = embedding_preprocessor.transform(x_train)<br/>x_test_emb = embedding_preprocessor.transform(x_test)</span><span id="1bd6" class="my ly iq lw b gy nf na l nb nc">x_train_emb[0]<br/>## array([[ 941.],<br/>##        [ 330.],<br/>##        [ 960.],<br/>##        ...,<br/>##        [ 399.],<br/>##        [ 531.],<br/>##        [1141.]])<br/>x_train_emb[1]<br/>## array([[104.],<br/>##        [131.],<br/>##        [122.],<br/>##        ...,<br/>##        [ 67.],<br/>##        [126.],<br/>##        [ 67.]])<br/>x_train_emb[2]<br/>## array([[-0.472, -0.234],<br/>##        [ 0.693, -0.309],<br/>##        [ 0.51 ,  0.672],<br/>##        ...,<br/>##        [ 0.572, -0.128],<br/>##        [-0.792,  1.976],<br/>##        [-1.53 , -0.229]])</span></pre><p id="577c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是时候建立神经网络了！我们有3个输入(2个嵌入，1个正常)。嵌入输入都被传递到嵌入层，被展平并与正常输入连接。下面的隐藏层由Dense &amp; Dropout组成，最后线性激活。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nd ne l"/></div></figure><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="9efc" class="my ly iq lw b gy mz na l nb nc">import tensorflow as tf</span><span id="6544" class="my ly iq lw b gy nf na l nb nc">model = create_model(embedding1_vocab_size=C1_SIZE+1, <br/>                     embedding2_vocab_size=C2_SIZE+1)</span><span id="6822" class="my ly iq lw b gy nf na l nb nc">tf.keras.utils.plot_model(<br/>    model,<br/>    to_file='../../static/img/keras_embeddings_model.png',<br/>    show_shapes=True,<br/>    show_layer_names=True,<br/>)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nj"><img src="../Images/d12d523d4f5cba2b18026982aae6dca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yYjyZK9dpQnj3buu.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">具有嵌入层的神经网络，图像由作者使用tensor flow . keras . utils . plot _ model完成</p></figure><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="8a9e" class="my ly iq lw b gy mz na l nb nc">num_epochs = 50</span><span id="d48f" class="my ly iq lw b gy nf na l nb nc">model.fit(<br/>    x_train_emb,<br/>    y_train,<br/>    validation_data=(x_test_emb, y_test),<br/>    epochs=num_epochs,<br/>    batch_size=64,<br/>    verbose=0,<br/>);</span></pre><p id="28f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您看到类似<code class="fe lt lu lv lw b">InvalidArgumentError: indices[37,0] = 30 is not in [0, 30)</code>的错误，则您选择了错误的词汇表大小，根据<a class="ae lb" href="https://keras.io/layers/embeddings/" rel="noopener ugc nofollow" target="_blank">文档</a>，它应该是嵌入值的最大索引加1。</p><p id="72c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所看到的，对于我们的线性问题，该模型的表现与基线线性模型相当好，甚至更好，尽管它的真正优势只有在我们扩大类别空间或向响应添加非线性时才会发挥作用:</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="4ab2" class="my ly iq lw b gy mz na l nb nc">y_pred_emb = model.predict(x_test_emb)</span><span id="3c50" class="my ly iq lw b gy nf na l nb nc">mean_squared_error(y_pred_baseline, y_test)<br/>## 0.34772562548572583<br/>mean_squared_error(y_pred_emb, y_test)<br/>## 0.2192712407711225<br/>mean_absolute_error(y_pred_baseline, y_test)<br/>## 0.3877675893786461<br/>mean_absolute_error(y_pred_emb, y_test)<br/>## 0.3444112401247173</span></pre><p id="65c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还可以提取嵌入层的权重(第一行包含零标签的权重):</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="ec42" class="my ly iq lw b gy mz na l nb nc">weights = model.get_layer('embedding1').get_weights()<br/>pd.DataFrame(weights[0]).head(11)<br/>##         0      1      2<br/>## 0  -0.019 -0.057  0.069<br/>## 1   0.062  0.059  0.014<br/>## 2   0.051  0.094 -0.043<br/>## 3  -0.245 -0.330  0.410<br/>## 4  -0.224 -0.339  0.576<br/>## 5  -0.087 -0.114  0.324<br/>## 6   0.003  0.048  0.093<br/>## 7   0.349  0.340 -0.281<br/>## 8   0.266  0.301 -0.275<br/>## 9   0.145  0.179 -0.153<br/>## 10  0.060  0.050 -0.049</span></pre><p id="8eac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些权重有可能被保存在某个地方，并在其他模型中用作特征(预训练嵌入)。查看前10个类别，我们可以看到来自<code class="fe lt lu lv lw b">supplier_id</code>的具有相似响应的值<code class="fe lt lu lv lw b">y</code>具有相似的权重:</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="716c" class="my ly iq lw b gy mz na l nb nc">column_encoder = (embedding_preprocessor<br/>  .named_steps['encoding_preprocessor']<br/>  .named_transformers_['column_encoder'])</span><span id="01d5" class="my ly iq lw b gy nf na l nb nc">data_enc = column_encoder.transform(data)</span><span id="2235" class="my ly iq lw b gy nf na l nb nc">(data_enc<br/>  .sort_values('supplier_id')<br/>  .groupby('supplier_id')<br/>  .agg({'y': np.mean})<br/>  .head(10))<br/>##                   y<br/>## supplier_id        <br/>## 1            19.036<br/>## 2            19.212<br/>## 3            17.017<br/>## 4            15.318<br/>## 5            17.554<br/>## 6            19.198<br/>## 7            17.580<br/>## 8            17.638<br/>## 9            16.358<br/>## 10           14.625</span></pre><p id="e4af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，该模型对于未知数据表现合理，因为其表现类似于我们非常简单的基线模型，也类似于假设简单的条件均值:</p><pre class="ld le lf lg gt mu lw mv mw aw mx bi"><span id="98b8" class="my ly iq lw b gy mz na l nb nc">unknown_data = pd.DataFrame({<br/>    'product_id': ['!%&amp;/§(645h'],<br/>    'supplier_id': ['foo/bar'],<br/>    'price': [5],<br/>    'weight': [1]<br/>})</span><span id="db45" class="my ly iq lw b gy nf na l nb nc">np.mean(y_test['y'])<br/>## 17.403696362314747</span><span id="fe0e" class="my ly iq lw b gy nf na l nb nc"># conditional mean<br/>idx = x_test['price'].between(4.5, 5.5) &amp; x_test['weight'].between(0.5, 1.5)<br/>np.mean(y_test['y'][idx])<br/>## 18.716701011038868</span><span id="aad0" class="my ly iq lw b gy nf na l nb nc"># very naive baseline<br/>naive_model.predict(unknown_data[['price', 'weight']])<br/>## array([[18.905]])</span><span id="afeb" class="my ly iq lw b gy nf na l nb nc"># ridge baseline<br/>baseline_pipeline.predict(unknown_data)<br/>## array([[18.864]])</span><span id="6993" class="my ly iq lw b gy nf na l nb nc"># embedding model<br/>model.predict(embedding_preprocessor.transform(unknown_data))<br/>## array([[19.045]], dtype=float32)</span></pre><h1 id="91cc" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">包裹</h1><p id="e3cb" class="pw-post-body-paragraph kf kg iq kh b ki mp jr kk kl mq ju kn ko mr kq kr ks ms ku kv kw mt ky kz la ij bi translated">我们已经看到了如何利用嵌入层来编码高基数分类变量，并且根据基数，我们还可以调整密集特征空间的维度以获得更好的性能。这样做的代价是一个更加复杂的模型，而不是运行一个带有一次性编码的经典ML方法。<br/>如果经典模型是优选的，则类别权重可以从嵌入层中提取，并在更简单的模型中用作特征，因此取代了一次性编码步骤。</p></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><p id="9c35" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ls">最初发表于</em><a class="ae lb" href="https://blog.telsemeyer.com/2020/02/23/using-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables/" rel="noopener ugc nofollow" target="_blank"><em class="ls"/></a><em class="ls">。</em></p></div></div>    
</body>
</html>