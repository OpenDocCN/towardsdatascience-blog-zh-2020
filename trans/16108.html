<html>
<head>
<title>Tensorflow for Complete Beginners: Getting Started with Tensors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向完全初学者的Tensorflow:张量入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tensorflow-for-complete-begineers-getting-started-with-tensors-780f846f007?source=collection_archive---------7-----------------------#2020-11-06">https://towardsdatascience.com/tensorflow-for-complete-begineers-getting-started-with-tensors-780f846f007?source=collection_archive---------7-----------------------#2020-11-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d044" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对“张量”及其运算的理解，可以帮助完全初学者从零开始。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b5a72c280577068cfb638cc34316e579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iSeNEzIIJUzdsVvMsuzG3Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1013613" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>(免费使用)</p></figure><p id="fe75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个令人兴奋的人工智能(AI)世界，Tensorflow如今是一个热门词汇，尤其是在深度学习继续快速加速人工智能进展的情况下。但是对于刚开始使用Tensorflow的人来说，这种体验可能会令人害怕，因为漂亮的库的术语和用法可能会让完全的初学者感到困惑。当我刚开始学习Tensorflow时，我面临着类似的挑战，希望通过这篇文章简化一些错综复杂的事情。本文需要对Python有一个基本的了解，才能对Tensorflow有一个更清晰的了解。</p><p id="9888" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">张量流是如何工作的？</strong></p><p id="56ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">描述Tensorflow的基本(也是最简单的)方式是，它是Python(可能还有其他编程语言)中一个很酷的库，允许我们创建计算图来开发神经网络模型。组成Tensorflow对象的基本元素是一个<strong class="ky ir"> <em class="ls">张量</em> </strong>，所有执行的计算都发生在这些张量中。所以从字面上看(用我的话说)，当你开发任何神经网络模型时，这些张量以有序的方式流动，并在评估时产生最终输出。所以是的，了解张量到底是什么以及我们如何使用它们是开始使用Tensorflow的第一步。</p><p id="6085" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">那么什么是张量呢？</strong></p><p id="1d2d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们都很熟悉编程中的数据类型，对吗？<em class="ls"> 1、2、3 </em>等。是整数，带小数点的数字<em class="ls"> (1.5，3.141，5.2555等。)</em>是浮点值，另一个常见的是字符串(比如<em class="ls">“Hello。你今天怎么样？”</em>)。当我们把多个元素集合在一起时，我们一般称之为<strong class="ky ir"><em class="ls"/></strong><em class="ls">(例如【1，4，7】)</em><strong class="ky ir"><em class="ls">。</em> </strong></p><p id="21d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">列表集合可用于开发如下矩阵:</p><p id="2e49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls"> [[1，4，7]，[5，8，12]，[1，66，88]] </em></p><p id="f105" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们来看看张量——它们基本上是常规矩阵和向量的高维表示/概括。它甚至可以是1维矩阵、2维矩阵、4维矩阵或n维矩阵！所以我们可以有多维数组，而不仅仅是张量中的一个列表。就是这样，这就是张量在最基本的意义上的真正含义。就像我们可以在传统编程中给变量赋值一样，我们也可以在Tensorflow中这样做。只是有一种专门的方法来做这件事。类似地，就像你可以在编程中对常规变量(或常数)执行多重操作一样，我们也可以用张量来做。我们可以将张量切片并选择一部分元素，张量有不同的数据类型(整数、浮点、字符串等。)等等。</p><p id="3f3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">开始使用Python中的tensor flow</strong></p><p id="6f54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一步是安装漂亮的库！<em class="ls">匹普</em>是你在这里需要的一切。请注意，如果您使用的是Google Colab(与Python中的Jupyter notebooks非常相似)，您甚至不需要安装这个库，因为它已经安装好了，可以随时使用。谢谢你，谷歌！</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="8cd6" class="ly lz iq lu b gy ma mb l mc md">#Install the tensorflow module (not required in Google Colab, but is needed in your local PC)</span><span id="bb96" class="ly lz iq lu b gy me mb l mc md">!pip install tensorflow</span></pre><p id="7bb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们在本文中使用Colab时，输出中显示了消息“需求已经满足”。</p><blockquote class="mf mg mh"><p id="9ab0" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated"><em class="iq">需求已经满足:tensor flow in/usr/local/lib/python 3.6/dist-packages(2 . 3 . 0)</em></p></blockquote><p id="a282" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Tensorflow目前有一个更老的版本type 1.x，而最新发布的版本使用的是2.x，所以我们选择当前的tensorflow版本。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="ec1d" class="ly lz iq lu b gy ma mb l mc md">#Here, we would go with the latest (2.x) release of tensorflow by selecting the version</span><span id="4d4a" class="ly lz iq lu b gy me mb l mc md">#Note: By default, Colab tends to use the 2.x version</span><span id="773f" class="ly lz iq lu b gy me mb l mc md">%tensorflow_version 2.x</span></pre><p id="e15f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们使用黄金关键词<em class="ls"> import来导入tensorflow模块！是的，你可以通过简单地打印版本属性来检查你正在使用的库的版本。</em></p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="176d" class="ly lz iq lu b gy ma mb l mc md">#import the tensorflow module</span><span id="e288" class="ly lz iq lu b gy me mb l mc md">import tensorflow as tf</span><span id="3538" class="ly lz iq lu b gy me mb l mc md">#Display the tensorflow version we are using</span><span id="bfb4" class="ly lz iq lu b gy me mb l mc md">print(tf.version)</span></pre><blockquote class="mf mg mh"><p id="58c2" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated"><module from=""/></p></blockquote><p id="e265" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们开始创建张量。我们将在这里为int，float和string创建一些。注意使用<em class="ls"> tf创建张量的特殊方式。变量</em>属性。这意味着我们正在创建一个具有可变性质的张量，因此我们可以通过执行专门的操作来改变/修改它的值，就像我们在使用变量的常规编程中所做的那样。请注意，虽然我们在传统编程中已经使用了<em class="ls"> int、float和string </em>来声明这些数据类型的相应变量，但我们将在Tensorflow中使用<em class="ls"> tf.int16 </em>(这意味着我们正在定义一个16位整数)、<em class="ls"> tf.float32 </em>(定义一个32位浮点值)<em class="ls"> </em>和<em class="ls"> tf.string </em>来这样做。注意，我们也可以使用<em class="ls"> tf.float16、tf.int32等等，这取决于我们想要存储的值的需求。</em>如果你曾经使用过C++，你应该知道像<em class="ls"> int，short int，long long int，float，double </em>等。用于声明位数更少(或更多)的变量，所以我们在Tensorflow中做一些类似的事情。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="d710" class="ly lz iq lu b gy ma mb l mc md">#The science (and art) of creating tensors</span><span id="5d73" class="ly lz iq lu b gy me mb l mc md">scalar_val = tf.Variable(123,tf.int16)</span><span id="1373" class="ly lz iq lu b gy me mb l mc md">floating_val = tf.Variable(123.456,tf.float32)</span><span id="2d3a" class="ly lz iq lu b gy me mb l mc md">string_val = tf.Variable(“hello everyone. Nice to learn tensorflow!”,tf.string)</span></pre><p id="9d0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而现在，非常容易打印出这些张量的值！</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="362e" class="ly lz iq lu b gy ma mb l mc md">#Let us display the values (print) these tensors</span><span id="ea27" class="ly lz iq lu b gy me mb l mc md">print(scalar_val)</span><span id="62a1" class="ly lz iq lu b gy me mb l mc md">print(floating_val)</span><span id="8c97" class="ly lz iq lu b gy me mb l mc md">print(string_val)</span></pre><blockquote class="mf mg mh"><p id="ea98" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated"><tf.variable shape="()" dtype="int32," numpy="123"/></p><p id="8543" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated"><tf.variable shape="()" dtype="float32," numpy="123.456"> <tf.variable shape="()" dtype="string," numpy="b’hello" everyone.="" nice="" to="" learn="" tensorflow=""/></tf.variable></p></blockquote><p id="5831" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以确定张量的<strong class="ky ir">形状</strong>和<strong class="ky ir">等级</strong>，就像你可能在学校的数学模块中听到的那样。如果不是，形状只是指张量的每个维度所包含的元素的数量。而秩是张量中嵌套的最深层次。进入一些代码可能会更清楚。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="94aa" class="ly lz iq lu b gy ma mb l mc md">#The idea behind shape and rank of tensors</span><span id="4812" class="ly lz iq lu b gy me mb l mc md">#Shape: Describes the dimension of the tensor (total elements contained along each dimension)</span><span id="1684" class="ly lz iq lu b gy me mb l mc md">scalar_val_shap = tf.shape(scalar_val)</span><span id="5d5b" class="ly lz iq lu b gy me mb l mc md">print(scalar_val_shap)</span><span id="2927" class="ly lz iq lu b gy me mb l mc md">floating_val_shap = tf.shape(floating_val)</span><span id="3dfc" class="ly lz iq lu b gy me mb l mc md">print(floating_val_shap)</span></pre><blockquote class="mf mg mh"><p id="59fa" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([]，shape=(0，)，dtype=int32)</p><p id="2885" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([]，shape=(0，)，dtype=int32)</p></blockquote><p id="84fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以我们得到0的形状，对吗？这只是意味着这些是标量值，而不是列表或嵌套列表。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="5c65" class="ly lz iq lu b gy ma mb l mc md">#Now, if we use e.g. lists/nested lists instead of just a “single” scalar value</span><span id="2329" class="ly lz iq lu b gy me mb l mc md">list_tensor1 = tf.Variable([1,3,5,6],tf.int16)</span><span id="fa1c" class="ly lz iq lu b gy me mb l mc md">print(list_tensor1)</span><span id="8e13" class="ly lz iq lu b gy me mb l mc md">print(tf.shape(list_tensor1))</span><span id="02cc" class="ly lz iq lu b gy me mb l mc md">list_tensor2 = tf.Variable([[1,2,3],[4,5,6]],tf.int16)</span><span id="f3cf" class="ly lz iq lu b gy me mb l mc md">print(list_tensor2)</span><span id="3b93" class="ly lz iq lu b gy me mb l mc md">print(tf.shape(list_tensor2))</span><span id="c8e0" class="ly lz iq lu b gy me mb l mc md">#how about the rank? It describes the level of nesting within the tensor in simple words.</span><span id="342d" class="ly lz iq lu b gy me mb l mc md">print(tf.rank(list_tensor1))</span><span id="e337" class="ly lz iq lu b gy me mb l mc md">print(tf.rank(list_tensor2))</span></pre><p id="caed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以看到<em class="ls"> list_tensor1 </em>有4个元素，所以它的形状是(4，)。注意,( 4，)只不过是(4，1 ),只显示了单个列表中的4个元素。接下来，对于<em class="ls"> list_tensor2 </em>，观察(2，3)的形状，表示我们有一个嵌套列表——有2个列表，每个列表包含3个元素。</p><blockquote class="mf mg mh"><p id="b203" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated"><tf.variable shape="(4,)" dtype="int32," numpy="array([1,"/></p><p id="f533" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([4]，shape=(1，)，dtype=int32)</p><p id="1ab0" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated"><tf.variable shape="(2," dtype="int32," numpy="array([[1,"/></p><p id="e4c5" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([2 ^ 3]，shape=(2，)，dtype=int32)</p><p id="8c08" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量(1，shape=()，dtype=int32)</p><p id="4cce" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量(2，shape=()，dtype=int32)</p></blockquote><p id="f23e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，如果你看一下秩，你会看到,<em class="ls"> list_tensor1，</em>的秩为1，因为我们只有一个列表，这里没有嵌套！现在，对于<em class="ls"> list_tensor2，</em>你可以看到等级为2，表示我们有一个2级嵌套，即一个大列表包含更多的列表。如果它是一个大列表，包含另一个小列表，而那个小列表包含更多更小的列表，我们的排名就会是3。希望这能让你明白。</p><blockquote class="mf mg mh"><p id="5faa" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量(1，shape=()，dtype=int32)</p><p id="7042" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量(2，shape=()，dtype=int32)</p></blockquote><p id="7011" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用<em class="ls"> tf.reshape. </em>来改变张量的形状(当然，直到它在数学上有效为止)。同样，请注意使用Python来做这件事的特殊方式。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="de17" class="ly lz iq lu b gy ma mb l mc md">#Reshaping tensors</span><span id="af03" class="ly lz iq lu b gy me mb l mc md">reshaped_list_tensor2 = tf.reshape(list_tensor2,[6])</span><span id="f00f" class="ly lz iq lu b gy me mb l mc md">print(reshaped_list_tensor2)</span><span id="a61a" class="ly lz iq lu b gy me mb l mc md">list_tensor3 = tf.Variable([[1,2,3,1],[1,9,10,11],[1,5,11,22],[16,17,18,19]],tf.int16)</span><span id="89d2" class="ly lz iq lu b gy me mb l mc md">print(list_tensor3)</span><span id="d679" class="ly lz iq lu b gy me mb l mc md">print(tf.rank(list_tensor3))</span><span id="dfa7" class="ly lz iq lu b gy me mb l mc md">print(tf.shape(list_tensor3))</span><span id="c8fe" class="ly lz iq lu b gy me mb l mc md">reshaped_list_tensor3 = tf.reshape(list_tensor3,[2,8,1])</span><span id="08d9" class="ly lz iq lu b gy me mb l mc md">print(reshaped_list_tensor3)</span><span id="373b" class="ly lz iq lu b gy me mb l mc md">#or like this</span><span id="0576" class="ly lz iq lu b gy me mb l mc md">reshaped_list_tensor3 = tf.reshape(list_tensor3,[8,2,1])</span><span id="3ff7" class="ly lz iq lu b gy me mb l mc md">print(reshaped_list_tensor3)</span><span id="c7ab" class="ly lz iq lu b gy me mb l mc md">#or automatically determine the shape by only giving one dimension!</span><span id="018a" class="ly lz iq lu b gy me mb l mc md">reshaped_list_tensor3 = tf.reshape(list_tensor3,[1,-1])</span><span id="59a5" class="ly lz iq lu b gy me mb l mc md">print(reshaped_list_tensor3)</span></pre><p id="50ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你会得到下面的输出。它太长了，可能无法阅读，但我猜如果你在这里查看Colab笔记本<a class="ae kv" href="https://github.com/joyjitchatterjee/DeepLearning-Tensorflow-Basics/blob/master/Tensorflow_Beginning.ipynb." rel="noopener ugc nofollow" target="_blank">会更清楚。这里发生的基本事情是，如果你在这里为形状指定[6]，tensorflow将把你的张量重新整形为具有6个一维元素(因此它将是一个简单的1D列表)。如果你做一个[2，8，1]，它会产生2个列表，每个列表有8个元素。你也可以做[8，2，1]-创建8个列表，每个列表有2个元素，就像你看到的那样。如果您不想亲自指定整形的整个尺寸，您也可以简单地使用-1。因此，[1，-1]的整形将整形<em class="ls"> list_tensor3 </em>以创建<em class="ls">reshaved _ list _ tensor 3，那么</em> 1 list与？？？是的，16种元素。因此，请注意，尽管您可以按照自己的意愿使用reshape，但必须确保reshape后的元素总数在整个过程中不会发生变化。</a></p><blockquote class="mf mg mh"><p id="8493" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([1 2 3 4 5 6]，shape=(6，)，dtype=int32)</p><p id="5ab9" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated"><tf.variable shape="(4," dtype="int32," numpy="array([["/></p><p id="6943" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量(2，shape=()，dtype=int32)</p><p id="956a" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([4 ^ 4]，shape=(2，)，dtype=int32)</p><p id="2c1d" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([[1][2][3][1][1][9][10][11]][[1][5][11][22][16][17][18][19]])，shape=(2，8，1)，dtype=int32)</p><p id="844a" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([[[1][2]][[3][1]][[1][9]][[10][11]][[1][5]][[11][22]][[16][17]][[18][19]])，shape=(8，2，1)，dtype=int32)</p><p id="3c40" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([[ 1 2 3 1 1 9 10 11 1 5 11 22 16 17 18 19]]，shape=(1，16)，dtype=int32)</p></blockquote><p id="8b22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以使用特殊命令<em class="ls"> tf.ones </em>或<em class="ls"> tf.zeros. </em>创建一个全是1(或0)的张量，然后也可以对它们执行类似的操作。</p><p id="3f6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们来切片。就像你可以从一个矩阵或者一个Python列表中切分和提取一些元素一样，你也可以对张量做同样的事情。请看下面的例子，其中，我们首先创建一个全张量(我的名字是全1张量)和另一个全0张量。您想要创建的张量的维数在方括号[]内，因此a [4，4，4，1]创建了一个4D张量，而a [5，5]创建了一个2D张量——在最简单的意义上，它有5行5列(不过是一个矩阵)。</p><p id="f428" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面显示了这些创建的张量的一些切片操作。这样你就可以从张量中得到你喜欢的元素而不考虑你不喜欢的:-)</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="5433" class="ly lz iq lu b gy ma mb l mc md">#creating a tensor full of 1s (or 0s)</span><span id="c9e9" class="ly lz iq lu b gy me mb l mc md">tensor_onefull = tf.ones([4,4,4,1])</span><span id="b1fb" class="ly lz iq lu b gy me mb l mc md">print(tensor_onefull)</span><span id="1662" class="ly lz iq lu b gy me mb l mc md">tensor_zerofull = tf.zeros([5,5])</span><span id="600b" class="ly lz iq lu b gy me mb l mc md">print(tensor_zerofull)</span><span id="a7da" class="ly lz iq lu b gy me mb l mc md">#extracting specific values from tensors (similar to slicing in conventional programming)</span><span id="9ef6" class="ly lz iq lu b gy me mb l mc md">tensor_sliced_onefull = tensor_onefull[0]</span><span id="6b1f" class="ly lz iq lu b gy me mb l mc md">print(tensor_sliced_onefull)</span><span id="800f" class="ly lz iq lu b gy me mb l mc md">tensor_sliced_zerofull = tensor_zerofull[0,1]</span><span id="92cb" class="ly lz iq lu b gy me mb l mc md">print(tensor_sliced_zerofull)</span></pre><p id="5d3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是很好的输出(在Colab笔记本中可读性更好)。</p><blockquote class="mf mg mh"><p id="6c62" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([[[[1。] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]]] [[[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]]] [[[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]]] [[[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]]]]，shape=(4，4，4，1)，dtype=float32)</p><p id="3e76" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([[0。0.0.0.0.] [0.0.0.0.0.] [0.0.0.0.0.] [0.0.0.0.0.] [0.0.0.0.0.]]，shape=(5，5)，dtype=float32)</p><p id="7943" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([[[1。] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]] [[1.] [1.] [1.] [1.]]]，shape=(4，4，1)，dtype=float32)</p><p id="335f" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量(0.0，shape=()，dtype=float32)</p></blockquote><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="37d4" class="ly lz iq lu b gy ma mb l mc md">#another example from previously created tensor</span><span id="03d9" class="ly lz iq lu b gy me mb l mc md">print(list_tensor3)</span><span id="9e57" class="ly lz iq lu b gy me mb l mc md">tf_slicedexampleagain = list_tensor3[0,-2:]</span><span id="d181" class="ly lz iq lu b gy me mb l mc md">print(tf_slicedexampleagain)</span><span id="4ed6" class="ly lz iq lu b gy me mb l mc md">#selecting multiple rows</span><span id="e000" class="ly lz iq lu b gy me mb l mc md">tf_slicedexampleagain = list_tensor3[1::]</span><span id="fd5c" class="ly lz iq lu b gy me mb l mc md">print(tf_slicedexampleagain)</span></pre><p id="e887" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，一些很酷的输出(基本但没错，最简单的东西是最酷的)。</p><blockquote class="mf mg mh"><p id="2b69" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated"><tf.variable shape="(4," dtype="int32," numpy="array([["/></p><p id="c629" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([3 1]，shape=(2，)，dtype=int32)</p><p id="3b36" class="kw kx ls ky b kz la jr lb lc ld ju le mi lg lh li mj lk ll lm mk lo lp lq lr ij bi translated">tf。张量([[ 1 9 10 11] [ 1 5 11 22] [16 17 18 19]]，shape=(3，4)，dtype=int32)</p></blockquote><p id="c2da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在我的Github<a class="ae kv" href="https://github.com/joyjitchatterjee/DeepLearning-Tensorflow-Basics/blob/master/Tensorflow_Beginning.ipynb" rel="noopener ugc nofollow" target="_blank">https://Github . com/joyjitchatterjee/deep learning-tensor flow-Basics/blob/master/tensor flow _ beginning . ipynb</a>上找到这篇文章的完整的Colab笔记本。</p><p id="74ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">仅此而已。希望你喜欢阅读这篇文章。感谢阅读，并为任何错别字/错误道歉。我希望在我的下一篇文章中涉及更多关于Tensorflow的内容。</p><p id="d414" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你愿意，你可以通过LinkedIn联系我，电话是<a class="ae kv" href="http://linkedin.com/in/joyjitchatterjee/" rel="noopener ugc nofollow" target="_blank">http://linkedin.com/in/joyjitchatterjee/</a></p><p id="4785" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献</strong></p><ol class=""><li id="8dd4" class="ml mm iq ky b kz la lc ld lf mn lj mo ln mp lr mq mr ms mt bi translated">张量流文档(<a class="ae kv" href="https://www.tensorflow.org/guide" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide</a>)</li><li id="8f51" class="ml mm iq ky b kz mu lc mv lf mw lj mx ln my lr mq mr ms mt bi translated">TensorFlow 2.0完整教程— Python神经网络初学者教程(<a class="ae kv" href="https://www.youtube.com/watch?v=tPYj3fFJGjk&amp;list=PLYLyA78Q4izuAlaaOER3qwUZkqLTZf7qk" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=tPYj3fFJGjk&amp;list = ply lya 78 q 4 izualaaoer 3 qwuzkqltzf 7 qk</a>)</li><li id="922c" class="ml mm iq ky b kz mu lc mv lf mw lj mx ln my lr mq mr ms mt bi translated">TensorFlow和深度学习入门(<a class="ae kv" href="https://medium.com/@rishit.dagli/get-started-with-tensorflow-and-deep-learning-part-1-72c7d67f99fc" rel="noopener">https://medium . com/@ rishit . dagli/get-started-with-tensor flow-and-Deep-Learning-part-1-72c 7d 67 f 99 fc</a>)</li></ol></div></div>    
</body>
</html>