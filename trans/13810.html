<html>
<head>
<title>The K-bandit Problem with Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习中的K-bandit问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0?source=collection_archive---------30-----------------------#2020-09-22">https://towardsdatascience.com/the-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0?source=collection_archive---------30-----------------------#2020-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3581" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">强化学习的基本概念</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3d792b031e917e89989c564f5fd9a436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*By3gldxEt3RS2pW9cy8EYw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://pixabay.com/es/vectors/m%C3%A1quina-tragaperras-casino-frutas-159972/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><h1 id="9458" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">强化学习中的基本问题</h1><p id="6a99" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">强化学习中一个非常重要的部分是如何评估代理执行的动作。在本帖中，我们将使用K-bandit问题来展示评估这些行为的不同方法。重要的是要记住，K-bandit问题只是许多强化学习情况的一个简单版本。这很简单，因为代理执行的操作是单独评估的，不像其他技术那样，评估是在一系列操作上完成的。</p><h1 id="384e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">K-bandit问题</h1><p id="91f7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">想象一下，你在一个被4台不同的老虎机包围的赌场里。每台机器根据不同的概率法则给你不同的奖励。你的工作将是最大化这些机器给你的奖励，并发现哪一台机器有更好的奖励。你将通过在机器上试验和玩一千次来做到这一点！奖品和获得这些奖品的概率不会随着时间而改变。</p><p id="cad2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">你会使用什么策略来最大化奖励和发现最好的机器？也许第一种方法是在机器上玩相同的次数(每台250次)。然而，在给我们更好奖品的机器上玩得更多不是更好吗？如果我们选错了机器怎么办？</p><p id="3c1c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正是因为这些问题，我们需要协调我们的行动。有时探索会更好(尝试在不同的机器上玩)，有时利用我们的知识会更好(在我们认为最好的机器上玩)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/00d9cf15b25d6661f34241aad88dacfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Ahv2hWGCZiwTDQX5TIiUjw.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://pixabay.com/es/photos/casino-juego-de-azar-3260387/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="3482" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在我们继续解决这个问题之前，我将介绍一些贯穿整个问题的符号:</p><ul class=""><li id="82b6" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">在时间t执行的Aₜ=The动作。(在示例中，这将是机器</li><li id="e166" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">在第t步获得Rₜ=The奖</li><li id="0dde" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">q∫(a)=执行行动a时的预期奖励，数学上是:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/145d379fae8697d719a8204ef84612a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*i_mc3nzGL0AooY6i6wiXsg.png"/></div></figure><p id="86eb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">期望奖金q∑(a)是这个问题中最重要的值，因为如果我们发现它的真实值，我们就知道应该一直在哪个机器上玩。通常这个值是未知的，这就是为什么我们需要在不同的机器上探索(玩)来估计期望值。随着时间的推移，我们将得到期望值的更好的近似值(如果我们可以无限地玩下去，我们将得到精确的值)。近似值使用的符号是Qₜ(a).这是对时间t时a的期望值的估计。</p><h1 id="837a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">估计代理行动的期望值</h1><p id="a11d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">有许多方法可以估计行动的期望值(Qₜ(a)).这里我们将使用对我来说最自然的方法。该方法将执行某个动作获得的所有奖励相加，这将出现在分子中，并将除以该动作执行的次数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/1100fda8a8c830deaa41f279661dcd58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/1*q0grSh9R8aL39fk5tBEY-A.gif"/></div></figure><p id="d12f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">借助于这个方程，随着时间的推移，Qₜ(a的值将更接近于q∑(a ),但探索和利用不同的可能行动将是很重要的。在本文中，ε-贪婪方法将被用来探索。这是一个非常简单的方法，因为我们唯一要做的事情就是选择一个被计算为更好的选项，但是有一定的概率(ε)随机尝试不同的动作。所以我们假设ε=.1成立，那就意味着时间上一千步，会选择900次最佳选项(exploit)，做100次探索。</p><h1 id="26c3" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">使用Python</h1><p id="f749" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">既然已经定义了问题并描述了解决问题的方法，我们将使用python来解决它。所以是时候打开你最喜欢的IDE开始编码了！我将一段一段地描述和解释代码，并将它完整地放在最后。你可以完全复制它，如果有疑问，检查具体的作品。对于这个程序，我们将只使用两个库，因此我们将继续导入它们:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="1de4" class="nl kx iq nh b gy nm nn l no np">###Import necessary libraries<br/>import random as rd<br/>import numpy as np</span></pre><p id="8161" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">第二步是创建两个辅助函数。第一个辅助函数将帮助我们模拟概率结果。这个函数将抛出一个介于0和1之间的随机数。如果随机数的结果低于我们定义的数，函数将返回true，否则将返回false。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="3c64" class="nl kx iq nh b gy nm nn l no np">###Define a random number in the interval [0,1] to simulate results of<br/>###probabilistic experiments.<br/>def decision(probability):<br/>return rd.random() &lt; probability</span></pre><p id="be8f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">第二个函数是ε-贪婪算法，它将决定在哪个机器上玩。它可以是期望值最高的机器，也可以是随机的机器。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="967d" class="nl kx iq nh b gy nm nn l no np">### Choose which machine to play following the E-greedy method.<br/>def greedy(no_machines,probability):<br/>aux=decision(probability)<br/>if(aux==True):<br/>index=rd.randint(0,len(no_machines)-1)<br/><br/>else:<br/>index=np.argmax(no_machines)<br/>return index</span></pre><p id="8a68" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了测试不同的想法，我们将多次执行该算法。在这种情况下，我们将为每个循环播放1000次，重复循环10000次，并尝试ε=[0，. 05，. 1，. 15，. 2，. 25，. 3，. 35，. 4，.. 45，.. 5]。这样我们就会知道我们应该对我们的知识有多贪婪。让我们首先定义有助于实验的变量。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="757a" class="nl kx iq nh b gy nm nn l no np">### This variable holds the real probability of winning that each machine has.<br/>### This variable is unknown to the player and it is what we'll try to estimate. <br/>prob_win=[.8,.85,.9,.7]<br/>### We will try different epsilons to see which one is better. <br/>epsilon=[0,.05,.1,.15,.2,.25,.3,.35,.4,.45,.5]<br/></span><span id="c206" class="nl kx iq nh b gy nq nn l no np">###Variables that hold the total for each different simulation(E=(0,.1,.2,...).<br/>p_total_reward=[]<br/>p_chosen_machine=[]</span></pre><p id="7152" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">鉴于问题3的结构，循环是必要的:</p><ul class=""><li id="5442" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">第一个循环将用于遍历所有ε=[0，. 05，. 1，. 15，. 2，. 25，. 3，. 35，. 4，. 45，.. 5]。</li><li id="6b4f" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">第二个循环在每个游戏中都有10，000个循环。</li><li id="1394" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">第三个循环进行了1000次。</li></ul><p id="b7b7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">代码:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="62bb" class="nl kx iq nh b gy nm nn l no np">for j in range(len(epsilon)):<br/>    ### Here the evolution of the algorithm can be seen. This variable shows<br/>    ### the evolution of the average prize. With the average prize the performance<br/>    ### of the algorithm is shown.<br/>    average_prize=[]<br/>    ###Variable that holds the total prize for each iteration.<br/>    total_reward_acum_g=[]<br/>    ###At the end of each cycle we will choose the machine that has the highest<br/>    ###expected prize and save it in this variable.<br/>    chosen_machine_g=[]<br/>    for x in range(10000):<br/>        ###The algorithm will be tested many times to see it's performance<br/>        ### variable that indicates the prize by playing 1000 times.<br/>        total_reward=0<br/>        ### Number of times played<br/>        i=0<br/>        ### Númber of times played over each machine.<br/>        iteraciones_por_accion=[0,0,0,0]<br/>        ### The expected prize over each machine. The value is started at 10<br/>        ### so that initially all machines are tested.<br/>        expected_prize_action=[10,10,10,10]<br/>        for x in range(100):<br/>          ###Index is the machine that was chosen to play with<br/>          index=greedy(expected_prize_action,epsilon[j])<br/>          ###Esta parte emula si ganaste o perdiste   <br/>          res=decision(prob_win[index])<br/>          if (res==True):<br/>              g=2<br/>          else:<br/>              g=1<br/>          ###Total reward   <br/>          total_reward=total_reward+g<br/>          i=i+1 <br/>          #Total average prize<br/>          average_prize.append(total_reward/i)<br/>          ###Number of times played per machine.<br/>          iteraciones_por_accion[index]=iteraciones_por_accion[index]+1<br/>          ###Update the value of the expected prize<br/>          expected_prize_action[index]=(expected_prize_action[index])+(1/iteraciones_por_accion[index])*(g-expected_prize_action[index])<br/>        ###results after playing 1000 times<br/>        total_reward_acum_g.append(total_reward)<br/>        chosen_machine_g.append(np.argmax(expected_prize_action))<br/>    print(epsilon[j])<br/>    print("On average "+str(sum(total_reward_acum_g)/len(total_reward_acum_g))+" points were obtained.")<br/>    print("The machine was chosen correctly " +str(chosen_machine_g.count(np.argmax(prob_win)))+" times.")<br/>    p_total_reward.append(sum(total_reward_acum_g)/len(total_reward_acum_g))<br/>    p_chosen_machine.append(chosen_machine_g.count(np.argmax(prob_win)))</span></pre><p id="30ad" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，我们将使用matplot库在绘图中可视化结果。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="7098" class="nl kx iq nh b gy nm nn l no np">import matplotlib.pyplot as plt<br/>values=p_total_reward<br/>values2=p_chosen_machine<br/>eje_x=epsilon<br/>eje_x[-1]<br/>fig, ax = plt.subplots(figsize=(20, 14)) <br/>plt.xticks(rotation=90)<br/>plt.plot(eje_x,values , marker ="o",label = "Average Total Prize");<br/><br/>ylabels = ['{:,}'.format(int(x)) + "K" for x in ax.get_yticks()*(1/1000)]<br/>plt.legend(prop={'size': 24})<br/>plt.title("Figure 1", fontdict=None, loc='center', pad=None,fontsize=18)<br/>plt.xlabel("Epsilon", fontdict=None, labelpad=None,fontsize=18)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/a062856988090465d393a80371159955.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNDeGnDmUQqBCmQW7HbW5w.png"/></div></div></figure><p id="afea" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">观察“图1 ”,我们认识到当我们设置ε=0.15时，达到最大奖励</p><p id="5e6a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这意味着15%的时间用来探索是方便的，其余85%的时间是贪婪的。现在，对于问题的第二部分，我们要求算法告诉我们它认为哪台机器最好。让我们也用图表来看看这个:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="7cf8" class="nl kx iq nh b gy nm nn l no np">import matplotlib.pyplot as plt<br/>valores2=p_chosen_machine<br/>eje_x=epsilon<br/>eje_x[-1]<br/>fig, ax = plt.subplots(figsize=(20, 14)) <br/>plt.xticks(rotation=90)<br/>plt.plot(eje_x,values2 , marker ="o",label = "Númber of times the best machine was chosen correctly");<br/>#plt.plot(x, y, marker ="o", label = "Modelo Atribución");<br/>ylabels = ['{:,}'.format(int(x)) + "K" for x in ax.get_yticks()*(1/1000)]<br/>plt.legend(prop={'size': 16})<br/>plt.title("Figure 2", fontdict=None, loc='center', pad=None,fontsize=18)<br/>plt.xlabel("Epsilon", fontdict=None, labelpad=None,fontsize=18)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/4e698845d9d3ae68241f936e1b0ca75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0hFwfQAe-3DPa_9RacnuQ.png"/></div></div></figure><p id="ed09" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">图2向我们展示了，随着探索的深入，算法倾向于更准确地选择最好的机器。这是一个显而易见的结果，因为有了更多关于不同机器的信息，我们就能更好地选择。然而，这伴随着一个奖励，因为做更多的探索伴随着降低奖励的成本。</p><h1 id="c712" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">更进一步</h1><p id="ba89" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">本文只是对k-bandit问题的初步探讨，所以对于感兴趣的读者，我想留下一些问题供您思考:</p><ol class=""><li id="bffb" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj ns mw mx my bi translated">如果我们玩100次而不是1000次，ε的结果会不同吗？</li><li id="6a6d" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj ns mw mx my bi translated">如果这些机器给出的奖品大相径庭，ε会有什么变化？</li><li id="d4ff" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj ns mw mx my bi translated">随着时间的推移，改变ε是否方便？</li><li id="24af" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj ns mw mx my bi translated">这三个问题有什么联系？</li></ol><p id="7107" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">非常感谢您的关注，希望能再次见到您。您可以提前找到并运行代码。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nt nu l"/></div></figure></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="632a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="oc">原载于2020年9月22日</em><a class="ae kv" href="https://datasciencestreet.com/the-k-bandit-problem-with-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank"><em class="oc">【https://datasciencestreet.com】</em></a><em class="oc">。</em></p></div></div>    
</body>
</html>