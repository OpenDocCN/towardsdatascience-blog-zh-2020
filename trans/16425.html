<html>
<head>
<title>Reasons why surrogate loss functions are pivotal for classification in machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">代理损失函数对机器学习中的分类至关重要的原因</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reasons-why-surrogate-loss-functions-are-pivotal-for-classification-in-machine-learning-e33974ce6d29?source=collection_archive---------30-----------------------#2020-11-12">https://towardsdatascience.com/reasons-why-surrogate-loss-functions-are-pivotal-for-classification-in-machine-learning-e33974ce6d29?source=collection_archive---------30-----------------------#2020-11-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/86bfcdcaa845471dc4d7361928456b3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IEQCtWiWnt_vYqRRB9GoTw.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片—巴西卡诺阿·克夫拉达</p></figure><p id="72ba" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这篇文章将深入探究机器学习中分类的铰链损失、逻辑损失和二进制损失背后的概念和理论。我们将在MATLAB上实现感知器算法，看看如何基于代理损失函数选择最佳分类器。</p><p id="b04a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在这篇文章结束时，你将知道如何使用感知器进行分类，以及每个损失函数的优缺点。</p></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="a2e6" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">资料组</h1><p id="44d4" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">我们将使用N = 200个数据点的三维数据集。这个数据集最初是由Ruth Urner博士在她的一次机器学习课程作业中提出的。在下面的库中，你会发现两个TXT文件:<a class="ae mk" href="https://github.com/jaimedantas/perceptron-classification/blob/main/dataset/fg_inputs.txt" rel="noopener ugc nofollow" target="_blank"> fg_inputs.txt </a>和<a class="ae mk" href="https://github.com/jaimedantas/perceptron-classification/blob/main/dataset/fg_outputs.txt" rel="noopener ugc nofollow" target="_blank"> fg_outputs.txt </a>。</p><div class="ml mm gp gr mn mo"><a href="https://github.com/jaimedantas/perceptron-classification" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd ir gy z fp mt fr fs mu fu fw ip bi translated">jaimedantas/感知器-分类</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">感知器算法用于机器学习中的线性分类。感知器在MATLAB上实现…</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">github.com</p></div></div><div class="mx l"><div class="my l mz na nb mx nc jw mo"/></div></div></a></div><p id="be5a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这些文件包含输入和输出向量。使用MATLAB，我们可以在Home &gt; Import Data中导入它们。现在，让我们绘制这个数据集。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/78360d07ec3949d1f02da911aefb78e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1zKuS2t427ocRuW-vKdqqw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">数据集FG</p></figure><h1 id="ef02" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">感知器</h1><p id="7713" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">感知器算法是最古老的机器学习算法之一，至今仍被广泛使用。它用于不同类型数据集的线性分类。感知器是用于二进制分类的许多算法之一，它非常容易理解。我推荐阅读这篇简短的<a class="ae mk" href="https://en.wikipedia.org/wiki/Perceptron" rel="noopener ugc nofollow" target="_blank">文章</a>，这样你可以详细了解它是如何工作的。如果你真的想学习背后的数学，这些<a class="ae mk" href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote03.html" rel="noopener ugc nofollow" target="_blank">讲座</a>可能会帮助你。</p><p id="a187" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">长话短说，感知器的工作方式是通过分析<strong class="ke ir"> x </strong>和<strong class="ke ir"> w </strong>乘以其标签<strong class="ke ir"> t </strong>之间的点积信号。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/7def5d7319ee9adc0082154ce3bbb8f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/1*ZQiYdQObk5mAzJVGmJYFgQ.gif"/></div></figure><p id="b6c1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">每次<strong class="ke ir"> <em class="nq"> h(x) </em> </strong>小于或等于<strong class="ke ir"> 0 </strong>，我们就用一个新值更新<strong class="ke ir"> w </strong>。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/585bd2617c2bd19b64f378a21074804a.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/1*i2IIefj3y2nFu3vk1CzWFg.gif"/></div></figure><p id="f318" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">说的够多了，还是在MATLAB上实现吧！我在MATLAB上创建了一个实现感知器算法的函数。它接收输入矩阵<strong class="ke ir"> x </strong>和标签向量<strong class="ke ir"> t </strong>。我做的第一件事是在<strong class="ke ir">x</strong>【4】中添加一列1。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/de8d244eb7fc4c9c849429cb50eead49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*e2CgIauvBEHuwlyH1GbEvA.png"/></div></figure><p id="7503" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">然后，我将<strong class="ke ir"> x </strong>和<strong class="ke ir"> t </strong>连接起来。这样做的原因是因为这样会更容易打乱数据。我使用了MATLAB的函数<em class="nq"> randperm() </em>对数据集进行混洗。之后，我用零初始化了向量<strong class="ke ir"> w </strong>。由于我们正在添加术语<em class="nq"> b </em>，即<em class="nq"> w0 </em>，我将用<em class="nq">b = 0</em>【2】<em class="nq">进行初始化。</em></p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/617f1b1bd14c3d66c8b0ba8758116185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*jhJisykPB0H9rgL_PJtLiA.png"/></div></figure><p id="3a55" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我还在变量<em class="nq">更新</em>中跟踪算法在数据集上运行的次数。最后，在算法的最后，我执行了向量<strong class="ke ir">w</strong>【3】的欧几里德归一化。算法如下所示。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="dd34" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了运行我们的函数，我们必须在MATLAB的命令窗口中执行命令<strong class="ke ir">【w，updates】=感知器(输入，输出)</strong>。</p><p id="e377" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">感知器函数将返回标准化向量<strong class="ke ir"> w </strong>和执行的更新次数。</p><h1 id="1c0b" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">替代损失函数分析</h1><p id="d39a" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">为了计算损失，我执行感知器函数20次，每次用存储向量<strong class="ke ir">。我还在矩阵中添加了一列1</strong></p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="9afa" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们首先计算预测值的经验<strong class="ke ir">二进制损失</strong>，并在向量<em class="nq">二进制损失</em>中对它们进行平均。这个损失函数由下面的等式给出[4]。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b813cb837ce20c1bf09e6ed308e0c23f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*n-0t5BnpbwD49770SyfybQ.png"/></div></figure><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="0d62" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对<strong class="ke ir">铰链损失</strong>进行相同的处理。然而，这一次的损失由下式给出:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4e6268802a7ec2fab0eb12d02309641d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*IRl2EY6jWF4b3acWLjUQEQ.png"/></div></figure><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="cd2a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最后，我计算了<strong class="ke ir">物流损失</strong>。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ebaf9f523f43642fdb6947db1d095179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*lT6C_Zc6r4REJjIAiu04jQ.png"/></div></figure><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="7b46" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了分析这个数据集的训练损失，我创建了<a class="ae mk" href="https://github.com/jaimedantas/perceptron-classification/blob/main/code/compare_losses.m" rel="noopener ugc nofollow" target="_blank">compare _ loss . m</a>脚本来绘制20次执行的三次损失。</p><p id="50fb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">下图的左图显示了20次执行的损失。从那里，我们可以注意到，二进制损失是所有三个损失中最小的。此外，替代损失是一个更加明确的指标，现在我们的分类器的“好”。当我们看一看执行<em class="nq"> i = 17 </em>和<em class="nq"> i = 18 </em>时，我们就能明白其中的原因。虽然它们的二进制损失大致相同，但后者的铰链和逻辑损失明显小于前者。这是因为替代损失是分类器与其在两个类中最接近的数据点之间的距离的指示器。因此，位于错误一侧的数据点会造成很大的枢纽和逻辑损失。如果数据点被正确分类，但它非常接近线，则替代损失不像二进制损失一样为零。所以，<em class="nq"> i = 18 </em>的分级机比<em class="nq"> i = 17 </em>的分级机好。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/9959caee76b975b054bc72dcd385a1dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sgzw65BXlsh3xt0MqJ3kvg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">左边是感知器算法的20次执行的二进制损失、铰链损失和逻辑损失，以及感知器算法在200个数据点上的一次执行(w1)的二进制损失、铰链损失和逻辑损失。根据<a class="ae mk" href="https://github.com/jaimedantas/perceptron-classification/blob/main/code/compare_losses.m" rel="noopener ugc nofollow" target="_blank">compare _ loss . m</a>脚本绘制。</p></figure><p id="b4a6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当我们观察点<em class="nq"> i = 4 </em>和<em class="nq"> i = 5 </em>时，可以进行另一个很好的比较。前者具有较小的二进制损失，而后者具有较小的铰链和逻辑损失。所以，从这个分析来看，<em class="nq"> i = 5 </em>中的分类器可能比<em class="nq"> i = 4 </em>更好。这使我们得出结论，物流<strong class="ke ir">和铰链</strong>和<strong class="ke ir">损失</strong>是独立于<strong class="ke ir">和</strong>损失的<strong class="ke ir">。我们也可以说这两个替代损失是相互独立的。当我们观察他们在点<em class="nq"> i = 12 </em>和<em class="nq"> i = 13 </em>所采取的相反方向时，这一点可以得到证明。</strong></p><p id="15ea" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">转到上图的右图，我们可以看到，当我们对一个数据点进行错误分类时，后勤损失和枢纽损失都给了我们很大的损失，当我们正确分类时，后勤损失接近于零，枢纽损失接近于零(当t⟨ <strong class="ke ir"> w </strong>，<strong class="ke ir"> x </strong> ⟩ ≥ 1时)。另外，我们可以得出结论，当t⟨ <strong class="ke ir"> w </strong>，<strong class="ke ir"> x </strong> ⟩在0和1之间时，铰链不为零。这是因为数据点太靠近分类这两类的线[5]。</p><p id="4bbd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">另外，当数据点分类正确但离线太近时，铰链损耗给我们一个小值。类似地，当数据点被错误地分类并且它位于离线太远的位置时，铰链和逻辑损耗给我们一个非常大的值。事实上，对于t⟨ <strong class="ke ir"> w </strong>、<strong class="ke ir"> x </strong> ⟩的大负值，两个损失都变成平行线[5]。</p><p id="7cdd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于错误分类的数据点，逻辑损失遵循铰链损失的趋势。然而，对于正确分类的数据点来说，它永远不会为零，如上图右侧的图表所示。类似地，对于太靠近线的数据点，逻辑损失分配接近1的误差。</p><p id="b616" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最后，铰链损耗是二元损耗的上界，它比二元损耗更能证明真实损耗。下面我们可以看到每种损失类型的最小损失值。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/d17fcf5db65cb27dbd3151785ec40ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r4jMl0AQkylxB89dOzmcYA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">最小损失</p></figure><p id="24dd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">结果显示，二进制损失确实是最小的损失，而且是在执行中发现的<em class="nq"> i = 1 </em>。然而，最小的铰链和后勤损失是在执行18中发现的。这突出了使用代理函数进行二元分类的重要性。因此，给我们最小二进制损失的分类器并不总是理想的分类器。</p><h2 id="b025" class="nz li iq bd lj oa ob dn ln oc od dp lr kn oe of lv kr og oh lz kv oi oj md ok bi translated">使用MATLAB</h2><p id="c5cb" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">出于好奇，我决定将我们的结果与MATLAB上的一个现成实现进行比较。我在MATLAB上运行这个数据集，并使用<a class="ae mk" href="https://www.mathworks.com/help/stats/classificationlearner-app.html" rel="noopener ugc nofollow" target="_blank">分类线性模块拟合一个具有20个交互的线性分类器。</a>下图显示了学习者的分类错误。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ol"><img src="../Images/f0b166f0dea64559fcd1dd9d48609c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9qesWzV7-O5DHYIzXfxVfA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">MATLAB在数据集FG中进行20次交互的线性分类的最小分类误差。</p></figure><p id="f285" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">正如我们所见，MATLAB拟合了一个误差为<strong class="ke ir"> 0.135 </strong>的分类器，这恰好与运行我们的感知器算法相同。重要的是要记住，在MATLAB上进行比较时使用的学习算法不是感知器。我只是想说明我们的感知器实现为这个数据集估计了一个非常好的分类器。</p><h1 id="a8d9" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">结论</h1><p id="4962" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">我们看到，与铰链和逻辑损失不同，当我们正确分类所有数据点时，二进制损失总是<strong class="ke ir"> 0 </strong>。逻辑和枢纽损失表明预测者实际上“有多正确”或“有多错误”。这就是为什么最挑剔的数据点和分类器线之间的距离会影响整体损失。</p><p id="7bc0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，当选择最佳分类器时，分析替代函数被证明是最重要的，特别是当二元损失不是<strong class="ke ir"> 0 </strong>时。</p><p id="698c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最后，我们看到了如何在MATLAB上实现<em class="nq"> D+ 1 </em>维空间的感知器算法。</p><h1 id="46ea" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">关于我</h1><p id="bda0" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">我是约克大学的一名硕士研究生，骨子里是一名软件工程师。在过去的十年里，我一直在软件开发、云计算和系统工程等领域的几个行业工作。目前，我正在研究云计算和分布式系统。</p><p id="e1e6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你愿意，可以在我的<a class="ae mk" href="http://jaimedantas.com/" rel="noopener ugc nofollow" target="_blank">网站</a>上查看我的作品。</p><p id="a392" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">感谢阅读！</p><h1 id="f388" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">参考</h1><p id="eaab" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">[1]约西·凯舍特。多类分类。2014.网址:<a class="ae mk" href="https://u.cs.biu.ac.il/~jkeshet/teaching/aml2016/multiclass.pdf" rel="noopener ugc nofollow" target="_blank">https://u . cs . biu . AC . il/~ JK eshet/teaching/AML 2016/multi class . pdf</a></p><p id="8abc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[2]道林·耶戈。感知器:解释，实现和一个可视化的例子。2020.网址:<a class="ae mk" href="https: //towardsdatascience.com/perceptron-explanation-implementation-and-a-visual-example- 3c8e76b4e2d1" rel="noopener ugc nofollow" target="_blank">https://towardsdatascience . com/perceptron-explain-implementation-and-a-visual-example-3c 8e 76 B4 e 2d 1</a></p><p id="d566" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[3]玛丽娜·桑蒂尼。语言技术的机器学习第9讲:感知器。2014.网址:【http://santini.se/teaching/ml/2014/Lecture09_Perceptron.pdf T2】</p><p id="df43" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[4] Shai Shalev-Shwartz和Ben-David。理解机器学习:从理论到算法。剑桥大学出版社，2014年。DOI: 10.1017/CBO9781107298019。网址:【https://www . cs . huji . AC . il/~ shais/understanding machine learning/understanding—机器学习—理论—algorithms.pdf。</p><p id="ff60" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[5]基利安·温伯格。经验风险最小化-康奈尔大学。2020.网址:<a class="ae mk" href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote10.html" rel="noopener ugc nofollow" target="_blank">https://www . cs . Cornell . edu/courses/cs 4780/2018 fa/lectures/lessons note 10 . html</a></p></div></div>    
</body>
</html>