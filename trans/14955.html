<html>
<head>
<title>Object Detection: Stopping Karens Before They Can Strike With Keras and OpenCV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对象检测:在Karens攻击Keras和OpenCV之前阻止他们</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/custom-object-detection-using-keras-and-opencv-ddfe89bb3c3?source=collection_archive---------33-----------------------#2020-10-14">https://towardsdatascience.com/custom-object-detection-using-keras-and-opencv-ddfe89bb3c3?source=collection_archive---------33-----------------------#2020-10-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="28b0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于如何构建自己的面部检测系统的演练，该系统可以确定某人是否戴着面具，或者他们是快乐、中立还是愤怒。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/44a7f655b4cf7dc3eb97ec7219244942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*_trnzAW0JZpmL4Mjt5BsjA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">模型演示(作者照片)</p></figure><p id="9685" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在新冠肺炎时代，我们已经看到戴口罩可以大大减少病毒的传播。然而，正如我们有时在网上看到的，有很多人强烈反对这种说法。网上的视频显示，当人们被要求遵守这个协议时，他们变得非常不满。在这个项目中，我将向您展示如何创建一个神经网络，它可以检测某人是否戴着面具，如果没有，我们将检测他们脸上的面部表情。随着规模的扩大，这可以应用于当地企业，以便经理们可以检测到某人是否戴着面具，以及他们是否确实是一个准备罢工的卡伦。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ln lo l"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">照片来自<a class="ae lp" href="https://media.giphy.com/media/j1svspDN51FBUeQpFz/giphy.gif" rel="noopener ugc nofollow" target="_blank"> Giphy </a></p></figure></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi lx"><img src="../Images/255d7bf2e7548e749b30af1058951d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TNxHSkIZMwMLZxxwBecGjw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">模型演示(作者照片)</p></figure><p id="9b77" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="mc">* *本教程的代码可以在我的</em><a class="ae lp" href="https://github.com/HeeebsInc/FaceMaskEmotionDetection" rel="noopener ugc nofollow" target="_blank"><em class="mc">GitHub</em></a><em class="mc">* *</em>上找到</p><h1 id="6c42" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">要求</h1><ul class=""><li id="7a0f" class="mv mw iq kt b ku mx kx my la mz le na li nb lm nc nd ne nf bi translated">克拉斯</li><li id="172a" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">OpenCV</li><li id="66a7" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">NumPy</li><li id="98ea" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">Matplotlib</li><li id="9c4b" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">tqdm</li><li id="3178" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">Sklearn</li></ul><h1 id="7ed9" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">数据</h1><ol class=""><li id="0bcd" class="mv mw iq kt b ku mx kx my la mz le na li nb lm nl nd ne nf bi translated">面罩:~12K图像数据集(<a class="ae lp" href="https://www.kaggle.com/ashishjangra27/face-mask-12k-images-dataset?select=Face+Mask+Dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>)</li><li id="95ef" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nl nd ne nf bi translated">情感:~30K图像数据集(<a class="ae lp" href="https://www.kaggle.com/msambare/fer2013" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>)</li></ol><h1 id="2417" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">掩模检测</h1><p id="96d1" class="pw-post-body-paragraph kr ks iq kt b ku mx jr kw kx my ju kz la nm lc ld le nn lg lh li no lk ll lm ij bi translated">这个项目的第一步是建立一个神经网络，可以检测一个人是否戴着面具。对于这一部分，我们将使用Mobilenet。</p><p id="e73c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在构建模型之前，我们需要提取每张图像并对其进行预处理，以便可以将它输入Mobilenet。在上面的数据集中，我们看到在最外层有三个目录:(1)训练，(2)测试，和(3)验证。下面是如何提取每个图像，为mobilenet预处理它们，并将它们保存到NumPy数组的代码。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np lo l"/></div></figure><p id="282f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">运行上面的单元格后，您应该会看到这样的窗口。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nq"><img src="../Images/a1926edd796fb7f88823f2396f384787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*blgOQ4UrDDFuYB_xC5FQ8Q.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">列车测试拆分(作者供图)</p></figure><p id="0dec" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在后面的小节中，我们将重用函数get_image_value，这就是为什么我们要传递一个参数，说明应该为哪个模型类型检索图像。</p><p id="689a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在我们有了图像值，是时候建立神经网络来检测遮罩了。确保在当前目录中有一个名为ModelWeights的文件夹。在这个文件夹中，我们将保存我们训练的每个模型的权重。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np lo l"/></div></figure><p id="febb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上面，我们指定模型应该训练2000个纪元。因为模型可能不需要2000个历元，我们包括了一个早期停止参数，以防止模型在连续5次迭代后过度拟合，而损失没有变化。运行上面的单元格后，您应该会看到如下窗口:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nr"><img src="../Images/68a630487553b30955b3dc9c593368f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fFk7cilFqguoNiveFU579A.png"/></div></div></figure><p id="dc5c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">训练完成后，您应该会在ModelWeights文件夹中看到一个名为Mobilenet_Masks.h5的. h5文件。在该文件中，存储了模型的权重，稍后当我们将该神经网络应用于实时视频时，将会使用该文件。</p><div class="kg kh ki kj gt ab cb"><figure class="ns kk nt nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/0e86a1b31f74775ad5e6c9f106c75f58.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*6EcT-mjOtUafBXfD6zoVoQ.png"/></div></figure><figure class="ns kk ny nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/829841b5556979d12afa354d0cb909c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*yKKIlWqJZy4qMp4Y-DxLCA.png"/></div></figure><figure class="ns kk nz nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/334311d69c786e816fa8925af2563386.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*pa3IEGWE9NWPpiuAEW2gAA.png"/></div></figure></div><p id="d4eb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上面，我们看到了mobilenet培训的ROC得分、混淆矩阵和损失/准确性。考虑到我们使用的数据量，这些指标已经很不错了。对于验证集，只有13幅图像被错误分类。至于损耗和准确度，损耗能够低于0.1，准确度远高于94%。最后，ROC分数显示了巨大的成功，因为每个班级都有1.0的满分，而每个班级的f 1分数都大于0.98。</p><h1 id="90f7" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">情感检测</h1><p id="189c" class="pw-post-body-paragraph kr ks iq kt b ku mx jr kw kx my ju kz la nm lc ld le nn lg lh li no lk ll lm ij bi translated">与上一个模型一样，我们必须首先提取图像值，并将它们放入一个NumPy数组中。正如我前面提到的，我们将在一个新的函数中重用get_image_value函数，该函数被设计为只提取情感图像。该数据集包含7个类别:愤怒、快乐、中性、悲伤、厌恶、恐惧和惊讶。对于这个项目，我们将只关注前三类:愤怒、快乐和中立。此外，我们将用于训练的模型将采用大小为(48，48，3)的输入图像，这比(224，224，3)的mobilenet维度小得多。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np lo l"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">情绪训练测试分裂(作者照片)</p></figure><p id="736e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如你在上面看到的，我们限制每个类最多只能包含4000张图片。这样做是为了让训练更快，这样我们就可以在平衡班级的情况下正确地跟踪学生的表现。</p><p id="8724" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">运行上面的代码后，您应该会看到一个与之前类似的窗口，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi oa"><img src="../Images/91edb0a9a78135f525ddbb720be08b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LlFLkSeZssxAqOZExzXR4A.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">情绪训练测试分裂(作者照片)</p></figure><p id="8249" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在我们有了训练测试分离阵列，我们将建立神经网络来检测一个人脸上的情绪。下面是如何建立这个神经网络的代码。与mobilenet不同，我们不会对数据集应用增强。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np lo l"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">情绪训练(作者供图)</p></figure><p id="180f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">运行上面的代码后，您应该会看到一个类似这样的窗口:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nr"><img src="../Images/d4805a43023a9d9a2d67e704b7eec59b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZmIFAT71IEEInW7MZeHEgw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">情绪训练(作者供图)</p></figure><p id="0b84" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">一旦训练完成，您应该找到一个名为Normal_Emotions.h5的. h5文件，它将包含模型的权重。与我们之前训练的模型一样，这将用于下面的实时视频部分。</p><div class="kg kh ki kj gt ab cb"><figure class="ns kk ob nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/e9fe9f1ac40da7be436779c96a9e18c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*yawnKpr2qesx_6iIwMc5AA.png"/></div></figure><figure class="ns kk oc nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/6b62b8f8fba756f25c12d4da997fef65.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*D-RkE16L5JP-UxKFrMltxA.png"/></div></figure><figure class="ns kk od nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/525ce0258f637f9fdeb8547a9bb6eef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*jUg9UQi9PFbAgIZ8QuVx7w.png"/></div></figure></div><p id="d6f2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上面，我们看到了情绪模型的ROC分数、混淆矩阵和损失/准确性。考虑到我们使用的数据量，这些指标相当不错，但不如mobilenet。对于训练集，在大约12，000幅图像中，只有1，786幅图像被错误分类。至于损耗和准确度，损耗能够低于0.7，准确度保持在70-75%之间。最后，ROC分数显示了相当好的成功，因为每个类都保持了大于0.9的分数，而每个类的F1分数都在0.7到0.9之间。</p><h1 id="5026" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">部署</h1><p id="985e" class="pw-post-body-paragraph kr ks iq kt b ku mx jr kw kx my ju kz la nm lc ld le nn lg lh li no lk ll lm ij bi translated">现在我们已经训练好了每个模型，我们将使用OpenCV将它们应用于现场视频。对于这一部分，您必须在本地机器上安装网络摄像头。</p><p id="4ebd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我们进入代码之前，让我先解释一下这是如何工作的。</p><ol class=""><li id="ab3a" class="mv mw iq kt b ku kv kx ky la oe le of li og lm nl nd ne nf bi translated">使用<a class="ae lp" href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html" rel="noopener ugc nofollow" target="_blank">哈尔级联分类器</a>，我们将在一个视频帧中找到人脸的坐标(<a class="ae lp" href="https://github.com/HeeebsInc/FaceMaskEmotionDetection/blob/master/ModelWeights/haarcascade_frontalface_default.xml" rel="noopener ugc nofollow" target="_blank">下载链接</a></li><li id="c09b" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nl nd ne nf bi translated">在找到人脸的坐标后，我们将提取该部分，也称为我们的感兴趣区域(ROI)。</li><li id="ecd8" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nl nd ne nf bi translated">因为我们想弄清楚是否存在一个遮罩，所以我们将首先将ROI重新整形为(224，224)，以便可以将其输入到我们的mobilenet模型中。</li><li id="ecef" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nl nd ne nf bi translated">如果mobilenet模型预测有一个掩码，那么它将继续而不使用情感模型。</li><li id="6cf8" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nl nd ne nf bi translated">但是，如果mobilenet模型预测没有遮罩，那么它将再次使用ROI并将其输入到情感模型中。</li></ol><p id="6cfa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是它如何工作的代码。乍一看，for循环中似乎有很多条件，但只要慢慢读，就会明白。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np lo l"/></div></figure><p id="815d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">运行上面的代码后，会弹出一个如下所示的窗口。要停止此过程，请按键盘上的“q”按钮。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi oh"><img src="../Images/a4026b37052504e865dcf60584b9be13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*xc_RsIu9xgOQ2bxdefZmSw.gif"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">模型演示(作者照片)</p></figure><h2 id="a668" class="oi me iq bd mf oj ok dn mj ol om dp mn la on oo mp le op oq mr li or os mt ot bi translated">限制</h2><ul class=""><li id="405b" class="mv mw iq kt b ku mx kx my la mz le na li nb lm nc nd ne nf bi translated">据我所知，如果画面中的人戴着眼镜，这个模型很难区分面具是否存在。</li><li id="1c74" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">更多包含戴口罩戴眼镜的人的数据将改善这种局限性</li></ul><p id="1b84" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">未来方向</strong></p><ul class=""><li id="cf47" class="mv mw iq kt b ku kv kx ky la oe le of li og lm nc nd ne nf bi translated">我打算找一台能探测热量的相机(如果我能找到一台价格实惠的话)</li><li id="8727" class="mv mw iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">使用摄像头来检测热量以及这些其他功能有利于防止人们在发现发烧时进入商店。</li></ul><p id="eab6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">此外，我目前正在寻找一份数据科学方面的工作，所以如果你在这方面有任何建议，请通过</strong><a class="ae lp" href="https://www.linkedin.com/in/samuel-mohebban-b50732139/" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir">LinkedIn</strong></a><strong class="kt ir">联系我！</strong></p><p id="0bfc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="mc">*本教程的代码可以在我的</em><a class="ae lp" href="https://github.com/HeeebsInc/FaceMaskEmotionDetection" rel="noopener ugc nofollow" target="_blank"><em class="mc">GitHub</em></a><em class="mc">* *</em>上找到</p></div></div>    
</body>
</html>