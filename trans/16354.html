<html>
<head>
<title>K-Means Clustering for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">适用于初学者的k-均值聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-for-beginners-ea2256154109?source=collection_archive---------16-----------------------#2020-11-11">https://towardsdatascience.com/k-means-clustering-for-beginners-ea2256154109?source=collection_archive---------16-----------------------#2020-11-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7b3a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个学生用Python对这个有趣而有用的机器学习算法进行了深入的解释和逐步的指导</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f55426d89c96f344c5eeac057015a29e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YHISRXF58B5aoTaqSH1_zQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python K-Means聚类(所有照片按作者分类)</p></figure><h1 id="3719" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">介绍</h1><p id="06a9" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di">K</span>-意思是聚类是我在进入机器学习时学习的第一批算法之一，就在线性和多项式回归之后。</p><p id="1804" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">但是K-Means从根本上背离了后两者。回归分析是一种<em class="na">监督的</em> ML算法，而K-Means是<em class="na">非监督的。</em></p><p id="7f5c" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><strong class="ls iu">这是什么意思？</strong></p><p id="5ecc" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">监督学习和非监督学习是机器学习算法的两大类别:</p><h2 id="5f68" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">监督学习</h2><p id="9c8f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">你输入标有数据的程序。换句话说，你在提供正确答案的数据上训练算法，然后将学习到的规则应用于新数据，以预测它们的答案。</p><p id="ff71" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">这对回归和分类很有用。</p><h2 id="1d32" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">无监督学习</h2><p id="f238" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">您不需要为数据提供标签，而是由程序来发现它们。</p><p id="d65c" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">这对于聚类和发现数据中的隐藏模式非常有用。</p><h1 id="be1f" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">重要提示:</h1><p id="b5a9" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi mm translated">如果你是初学者，我建议你先阅读这些关于线性和多项式回归的文章，我在下面链接了这些文章。在其中，我涵盖了一些基本的机器学习知识和术语，我将在整篇文章中以此为基础。</p><div class="nn no gp gr np nq"><a rel="noopener follow" target="_blank" href="/linear-regression-the-actually-complete-introduction-67152323fcf2"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd iu gy z fp nv fr fs nw fu fw is bi translated">线性回归:(实际上)完全介绍</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">一位同学用Python对这个简单的机器学习算法进行了全面、深入的解释</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe ks nq"/></div></div></a></div><div class="nn no gp gr np nq"><a rel="noopener follow" target="_blank" href="/polynomial-regression-the-only-introduction-youll-need-49a6fb2b86de"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd iu gy z fp nv fr fs nw fu fw is bi translated">多项式回归:你需要的唯一介绍</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">一名学生对Python中机器学习算法背后的理论和应用的深入探究</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="of l ob oc od nz oe ks nq"/></div></div></a></div><h1 id="1a5f" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">该算法</h1><p id="29c5" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di">我</span>想象一个有大量数据点的数据集。我们的目标是将每个点分配给一个集群或组。要做到这一点，我们需要找出集群在哪里，以及哪些点应该属于每一个。在我们的例子中，我们将有两个变量:不同国家的出生率和预期寿命。我们可以创建这个数据的散点图来可视化我们的组。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/66936bcf4a3cc8d66fb6f04c022b178a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RV7NwEQEIRXeVwozEmYKNw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">K-均值聚类的一个例子</p></figure><p id="385a" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><em class="na">为什么我们要对这些数据执行K-Means聚类？</em></p><p id="944e" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">这里有一个实际的例子:假设联合国希望根据这两个指标将国家分为三类，这样它们就可以根据各自的需求提供相应的援助。</p><p id="e01f" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">除了目测，我们可以使用K-Means来自动化这个过程(其中<em class="na"> K </em>表示我们想要创建的集群的数量，而<em class="na"> Mean </em>表示平均值)。</p><p id="2baf" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><strong class="ls iu">K均值背后有两个关键假设:</strong></p><ol class=""><li id="3247" class="oh oi it ls b lt mv lw mw lz oj md ok mh ol ml om on oo op bi translated">每个聚类的中心是属于该聚类的所有数据点的平均值。</li><li id="27f1" class="oh oi it ls b lt oq lw or lz os md ot mh ou ml om on oo op bi translated">每个数据点属于具有最近中心点的聚类。</li></ol><p id="4a7c" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">这两个简单的假设描述了整个算法。我们的程序所做的就是迭代几个步骤，每个步骤都试图满足上述条件。</p><h2 id="e411" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">距离</h2><p id="c185" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在继续之前，我们必须讨论距离的概念。我们可以在ML中使用许多距离度量，例如<em class="na">曼哈顿</em>和<em class="na">切比雪夫</em>，但今天我们将坚持使用更熟悉的<em class="na">欧几里德、</em>，你可能会记得高中数学中的这些。</p><p id="2edf" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">在二维空间中，两点之间的欧几里德距离是</p><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="dc9d" class="nb kz it ow b gy pa pb l pc pd"><em class="na">√((xⱼ — xᵢ)² + (yⱼ — yᵢ)²)</em></span></pre><h2 id="12f5" class="nb kz it bd la nc nd dn le ne nf dp li lz ng nh lk md ni nj lm mh nk nl lo nm bi translated">步伐</h2><p id="41e9" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这是算法的概要:</p><ol class=""><li id="2fe4" class="oh oi it ls b lt mv lw mw lz oj md ok mh ol ml om on oo op bi translated">通过从数据集中随机选取点并使用这些点作为平均值的初始值，初始化每个聚类的平均值。</li><li id="6ef5" class="oh oi it ls b lt oq lw or lz os md ot mh ou ml om on oo op bi translated">将每个点分配给最近的聚类。</li><li id="d4ea" class="oh oi it ls b lt oq lw or lz os md ot mh ou ml om on oo op bi translated">计算每个聚类的平均值，作为属于它的所有点的平均值。</li><li id="5d71" class="oh oi it ls b lt oq lw or lz os md ot mh ou ml om on oo op bi translated">重复2和3预定的次数，或者直到收敛。</li></ol><h1 id="2996" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">这个例子</h1><p id="33e9" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">像往常一样，我们从进口开始:</p><ol class=""><li id="d713" class="oh oi it ls b lt mv lw mw lz oj md ok mh ol ml om on oo op bi translated">matplotlib(py plot &amp; RC params)——创建我们的数据可视化</li><li id="6c7b" class="oh oi it ls b lt oq lw or lz os md ot mh ou ml om on oo op bi translated">sci kit-Learn(pairwise _ distances _ arg min)-执行机器学习</li><li id="1f49" class="oh oi it ls b lt oq lw or lz os md ot mh ou ml om on oo op bi translated">NumPy——做科学计算</li><li id="d749" class="oh oi it ls b lt oq lw or lz os md ot mh ou ml om on oo op bi translated">csv —读取csv文件</li><li id="55f3" class="oh oi it ls b lt oq lw or lz os md ot mh ou ml om on oo op bi translated">集合(计数器和默认值)—用于计数</li></ol><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="2988" class="nb kz it ow b gy pa pb l pc pd">import matplotlib.pyplot as plt<br/>import numpy as np<br/>import csv<br/>from sklearn.metrics import pairwise_distances_argmin<br/>from collections import Counter, defaultdict</span></pre><p id="9e66" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">我有一个csv文件，其中包含如下所示的数据:</p><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="169b" class="nb kz it ow b gy pa pb l pc pd">Countries,BirthRate(Per1000 - 2008),LifeExpectancy(2008)<br/>Afghanistan,46.613,47.532<br/>Albania,14.69,76.492<br/>Algeria,20.804,72.44<br/>...</span></pre><p id="33aa" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">为了处理这些数据，我们需要一些变量来保存我们的<em class="na"> x值、y值、标签</em>和<em class="na">国家名称，在这个csv文件中是</em>。我们可以用一个函数从文件中提取所有这些信息:</p><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="aab3" class="nb kz it ow b gy pa pb l pc pd">x, y, x_label, y_label, countries = read_csv()</span></pre><p id="8cb0" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">我对<em class="na"> read_csv() </em>函数的定义如下。当然，您应该调整它以适合您的csv文件。</p><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="3d3b" class="nb kz it ow b gy pa pb l pc pd">def read_csv():<br/>    x = []<br/>    y = []<br/>    countries = []<br/>    x_label = ""<br/>    y_label = ""<br/>    with open('dataBoth.csv') as csvfile:<br/>        reader = csv.reader(csvfile, delimiter=',')<br/>        lines = 0<br/>        for row in reader:<br/>            if lines &gt;= 1:<br/>                print(', '.join(row))<br/>                x.append(float(row[1]))<br/>                y.append(float(row[2]))<br/>                countries.append(row[0])<br/>                lines += 1<br/>            else:<br/>                x_label = row[1]<br/>                y_label = row[2]<br/>                print(', '.join(row))<br/>                lines += 1<br/>    return x, y, x_label, y_label, countries</span></pre><p id="802f" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">现在我们已经处理了数据，我们需要将x和y组合成(x，y)对的2D列表，我们可以这样做:</p><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="b609" class="nb kz it ow b gy pa pb l pc pd">X = np.vstack((x, y)).T</span></pre><p id="cf54" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">现在我们有了一个2D列表(实际上是一个numpy数组),如下所示:</p><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="806d" class="nb kz it ow b gy pa pb l pc pd">[[46.613, 47.532]<br/> [14.69, 76.492]<br/> [20.804, 72.44]<br/> ...]</span></pre><p id="e3e2" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">让我们绘制这些数据点的散点图，看看我们在这里处理的是什么:</p><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="08b9" class="nb kz it ow b gy pa pb l pc pd">plt.xlabel(x_label)<br/>plt.ylabel(y_label)<br/>plt.scatter(x, y, color='#76c2b4')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/a16c9177d1bef301a6774f2f1c6395b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NnrdxnxyuR4EZMafsn8UGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(出生率、预期寿命)对列表的散点图</p></figure><p id="ae29" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">正如你所看到的，出生率和预期寿命之间的负相关是显而易见的。然而，3个不同的组不会立即出现。这就是K-Means的用武之地。</p><p id="eadb" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">接下来，我们将编写迭代创建集群的函数。该函数的逐行解释作为注释包含在下面的代码片段中，但是我也将在这里提供一些说明。</p><p id="2447" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">我们可以使用<em class="na">NP . random . random state(rseed)</em>选择随机中心，调用<em class="na">。在那上面排列</em>找到一个<em class="na"> i </em>，然后选择X的第个元素<em class="na">，这就是我们的<em class="na"> (x，y) </em>对的2D列表，记住。我们有一个循环，其中我们使用<em class="na">pairwise _ distance _ arg min</em>来计算点和中心之间的距离，然后从这些点的平均值中找到新的中心，然后检查聚类是否已经收敛(如果不能选择新的中心，则平均值变小)。当群集收敛时，此循环终止:</em></p><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="ecd0" class="nb kz it ow b gy pa pb l pc pd">def find_clusters(X, n_clusters, rseed=2):<br/>    # 1. Randomly choose clusters<br/>    rng = np.random.RandomState(rseed)<br/>    i = rng.permutation(X.shape[0])[:n_clusters]<br/>    centers = X[i]<br/><br/>    # The main loop<br/>    # This loop continues until convergence.<br/>    # You could make it run a set number of times by changing<br/>    # it to say while x &gt; 5, for example, and removing the break<br/>    print("\nConverging centres:")<br/>    while True:<br/>        # 2a. Assign labels based on closest center<br/>        # I am using the pairwise_distances_argmin method to<br/>        # calculate distances between points to centres<br/>        labels = pairwise_distances_argmin(X, centers)<br/><br/>        # 2b. Find new centers from means of points<br/>        new_centers = np.array([X[labels == i].mean(0) for i in<br/>        range(n_clusters)])<br/><br/>        # 2c. Check for convergence<br/>        if np.all(centers == new_centers):<br/>            break<br/>        centers = new_centers<br/><br/>        # Print converging centres<br/>        print(centers)<br/>        print()<br/><br/>    return centers, labels</span></pre><p id="1452" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">让我们将集群的数量设置为3:</p><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="27ad" class="nb kz it ow b gy pa pb l pc pd">clust_num = 3</span></pre><p id="a9d5" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">剩下要做的就是将我们的K-Means算法应用于数据的结果可视化:</p><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="2503" class="nb kz it ow b gy pa pb l pc pd">centers, labels = find_clusters(X, clust_num)<br/>plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')<br/>plt.title('K-Means clustering of countries by birth rate vs life expectancy')<br/>plt.xlabel(x_label)<br/>plt.ylabel(y_label)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/eef0506d207cc597b74b364beb3980cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nvbcn2-WAXpn-TkoyvSOPA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据被分成3组</p></figure><p id="ca73" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">那就是成功实现的算法！</p><p id="e399" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">但是如果我们想要更多的信息呢？</p><p id="06ac" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">毕竟，联合国希望看到每个集群中的国家名称。</p><p id="2c87" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">我们可以提取所有这些信息，并将其打印到终端:</p><pre class="kj kk kl km gt ov ow ox oy aw oz bi"><span id="d872" class="nb kz it ow b gy pa pb l pc pd">print("\nNumber of countries in each cluster:")<br/>print(Counter(labels))<br/><br/># Get cluster indices<br/>clusters_indices = defaultdict(list)<br/>for index, c in enumerate(labels):<br/>    clusters_indices[c].append(index)<br/><br/># Print countries in each cluster and means<br/>x = 0<br/>while x &lt; clust_num:<br/>    print("\nCluster " + str(x + 1))<br/>    print("----------")<br/>    for i in clusters_indices[x]:<br/>        print(countries[i])<br/>    print("----------")<br/>    print("Mean birth rate:")<br/>    print(centers[x][0])<br/>    print("Mean life expectancy:")<br/>    print(centers[x][1])<br/>    x+=1</span></pre><p id="669d" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">这将打印每个集群中的国家以及该集群的平均出生率和预期寿命。</p><h1 id="6ab4" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">结论</h1><p id="0c91" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di">至少应该清楚，K-Means聚类是一种非常有用的算法，有许多实际应用。希望您已经学到了足够的知识，可以对一些有趣的数据执行自己的实现，并发现一些隐藏的聚类。</span></p><p id="8438" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><strong class="ls iu">内容概述:</strong></p><ol class=""><li id="5442" class="oh oi it ls b lt mv lw mw lz oj md ok mh ol ml om on oo op bi translated">监督与非监督机器学习的简单比较。</li><li id="3864" class="oh oi it ls b lt oq lw or lz os md ot mh ou ml om on oo op bi translated">该技术应用的一个例子。</li><li id="564f" class="oh oi it ls b lt oq lw or lz os md ot mh ou ml om on oo op bi translated">算法的概要。</li><li id="4bfa" class="oh oi it ls b lt oq lw or lz os md ot mh ou ml om on oo op bi translated">实现的例子。</li></ol><p id="efe0" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">如果您觉得这篇文章有帮助，我很乐意与您合作！在Instagram 上关注我，了解更多机器学习和软件工程内容。</p><p id="d04a" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">编码快乐！</p></div><div class="ab cl ph pi hx pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="im in io ip iq"><p id="5e2c" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><a class="ae pg" href="https://medium.com/subscribe/@adenhaus" rel="noopener"> <strong class="ls iu">订阅</strong> </a>📚为了不错过我的一篇新文章，如果你还不是中等会员，请加入<a class="ae pg" href="https://medium.com/@adenhaus/membership" rel="noopener"><strong class="ls iu"/></a>🚀去读我所有的，还有成千上万的其他故事！</p></div><div class="ab cl ph pi hx pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="im in io ip iq"><h1 id="246f" class="ky kz it bd la lb po ld le lf pp lh li jz pq ka lk kc pr kd lm kf ps kg lo lp bi translated">资源</h1><p id="b509" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu">ski kit Learn</strong><em class="na">sk Learn . cluster . k means</em><a class="ae pg" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-Learn . org/stable/modules/generated/sk Learn . cluster . k means . html</a></p><p id="6bbc" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><strong class="ls iu"> Python文档</strong> <em class="na">收藏</em><a class="ae pg" href="https://docs.python.org/2/library/collections.html" rel="noopener ugc nofollow" target="_blank">https://docs.python.org/2/library/collections.html</a></p><p id="236a" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><strong class="ls iu">SciPy</strong><em class="na">numpy . random . random state</em>T4】https://docs . SciPy . org/doc/numpy-1 . 15 . 0/reference/generated/numpy . random . random state . html</p><p id="8f42" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><strong class="ls iu">sci kit Learn</strong><em class="na">sk Learn . metrics . pairwise _ distance _ arg min</em><a class="ae pg" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-Learn . org/stable/modules/generated/sk Learn . metrics . pairwise _ distance _ arg min . html</a></p><p id="fe1e" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><strong class="ls iu">Numpy.org</strong>T14】numpy . v stackT16】https://numpy . org/doc/stable/reference/generated/numpy . v stack . html</p></div></div>    
</body>
</html>