<html>
<head>
<title>Monte Carlo Dropout</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">蒙特卡洛辍学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/monte-carlo-dropout-7fd52f8b6571?source=collection_archive---------1-----------------------#2020-09-20">https://towardsdatascience.com/monte-carlo-dropout-7fd52f8b6571?source=collection_archive---------1-----------------------#2020-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d2e1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用一个小技巧免费改善你的神经网络，获得模型不确定性估计作为奖励。</h2></div><p id="2f59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">世上没有免费的午餐，至少根据流行的谚语是这样的。嗯，不再是了！也就是说，涉及到神经网络时就不是了。请继续阅读，看看如何用一个简单而巧妙的叫做蒙特卡洛辍学的技巧来提高网络性能。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/77227948dce1d14f51e0dd968d205145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0OJC-Lm25syuA2-X.png"/></div></div></figure><h2 id="7b5d" class="lq lr it bd ls lt lu dn lv lw lx dp ly kr lz ma mb kv mc md me kz mf mg mh mi bi translated">拒绝传统社会的人</h2><p id="98a4" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我们将要介绍的魔术只有在你的神经网络有漏层时才有效，所以让我们先简单介绍一下这些。辍学可以归结为在每个训练步骤中简单地关闭一些神经元。在每一步，一组不同的神经元被关闭。从数学上来说，每个神经元都有某种被忽略的概率<em class="mo"> p </em>，称为<em class="mo">辍学率。</em>退出率通常设置在0(无退出)和0.5(大约50%的神经元将被关闭)之间。确切的值取决于网络类型、图层大小以及网络过拟合训练数据的程度。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mp"><img src="../Images/3d33599d92d181f9e671180216cb921b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOOzhIJWi0IysZmprkrNMQ.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">一个完整的网络(左)和在特定训练步骤中有两个神经元退出的同一个网络(右)。</p></figure><p id="5283" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是为什么要这样做呢？放弃是一种正则化技术，也就是说，它有助于防止过度拟合。在数据很少和/或网络很复杂的情况下，模型可能会记住训练数据，因此，在训练期间看到的数据上效果很好，但在新的、看不见的数据上效果很差。这被称为过度适应，辍学寻求缓解。</p><p id="0d66" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">怎么会？有两种方法可以理解为什么关闭模型的某些部分可能是有益的。首先，信息在网络中传播得更加均匀。想想网络中某处的单个神经元。有几个其他的神经元为它提供输入。使用dropout，这些输入源中的每一个都可能在训练期间的任何时候消失。因此，我们的神经元不能只依赖一个或两个输入，它必须分散其权重，并注意所有的输入。因此，它对输入变化变得不太敏感，这导致模型更好地泛化。</p><p id="21e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从我们蒙特卡罗方法的角度来看，辍学效果的另一个解释甚至更重要。因为在每一次训练迭代中，你随机地对每一层中要被丢弃的神经元进行采样(根据该层的丢弃率)，所以每次都有一组不同的神经元被丢弃。因此，每次模型的架构都略有不同，您可以将结果视为许多不同神经网络的平均集合，每个神经网络仅针对一批数据进行训练。</p><p id="f0c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后一个细节:辍学仅用于培训期间。在推理时，也就是当我们用我们的网络进行预测时，我们通常不会应用任何丢弃——我们希望使用所有训练过的神经元和连接。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/77227948dce1d14f51e0dd968d205145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0OJC-Lm25syuA2-X.png"/></div></div></figure><h2 id="50e6" class="lq lr it bd ls lt lu dn lv lw lx dp ly kr lz ma mb kv mc md me kz mf mg mh mi bi translated">蒙特卡洛</h2><p id="2a7e" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">现在我们已经解决了辍学问题，蒙特卡洛是什么？如果你正在考虑摩纳哥的一个街区，你是对的！但是还有更多。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mu"><img src="../Images/ee8a3a8ef3113b63d15653a20b71a92e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uD-gJmPygsOyZ_-o"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">摩纳哥蒙特卡洛。Geoff Brooks 在<a class="ae mv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="534e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在统计学中，蒙特卡罗指的是一类计算算法，它依靠重复的随机抽样来获得某个数字量的<a class="ae mv" rel="noopener" target="_blank" href="/6-useful-probability-distributions-with-applications-to-data-science-problems-2c0bee7cef28">分布</a>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/77227948dce1d14f51e0dd968d205145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0OJC-Lm25syuA2-X.png"/></div></div></figure><h2 id="9941" class="lq lr it bd ls lt lu dn lv lw lx dp ly kr lz ma mb kv mc md me kz mf mg mh mi bi translated">蒙特卡洛退出:模型准确性</h2><p id="18df" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">Monte Carlo Dropout是由<a class="ae mv" href="https://arxiv.org/abs/1506.02142" rel="noopener ugc nofollow" target="_blank"> Gal &amp; Ghahramani (2016) </a>提出的，它巧妙的实现了常规Dropout的使用可以被解释为一个众所周知的概率模型:高斯过程的<a class="ae mv" rel="noopener" target="_blank" href="/the-gentlest-of-introductions-to-bayesian-data-analysis-74df448da25">贝叶斯</a>近似。我们可以将许多不同的网络(剔除了不同的神经元)视为来自所有可用模型空间的蒙特卡罗样本。这为推理模型的不确定性提供了数学基础，事实证明，这通常会提高模型的性能。</p><p id="b9d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它是如何工作的？我们只是在测试时应用dropout，仅此而已！然后，不是一个预测，我们得到许多，每个模型一个。然后我们可以对它们进行平均或分析它们的分布。最好的部分是:它不需要模型架构的任何改变。我们甚至可以在已经训练好的模型上使用这个技巧！为了看到它在实践中的工作，让我们训练一个简单的网络来识别MNIST数据集中的数字。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="0a3d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">经过30个时期的训练，该模型在测试集上取得了96.7%的准确率。要在预测时打开dropout，我们只需设置<code class="fe my mz na nb b">training=True</code>来确保类似训练的行为，即丢弃一些神经元。这样，每个预测都会略有不同，我们可以生成尽可能多的预测。</p><p id="6414" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们创建两个有用的函数:<code class="fe my mz na nb b">predict_proba()</code>生成所需数量的<code class="fe my mz na nb b">num_samples</code>预测，并对MNIST数据集中10位数字中的每一位的预测类概率进行平均，而<code class="fe my mz na nb b">predict_class()</code>只是选择最高的预测概率来挑选最可能的类。这个和下面的一些代码片段是受Geron (2019)的启发。该书附有一套优秀的<a class="ae mv" href="https://github.com/ageron/handson-ml2" rel="noopener ugc nofollow" target="_blank"> jupyter笔记本</a>。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="7809" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们在测试集上进行100次预测并评估准确性。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="764f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这产生了97.2%的准确度。<strong class="kk iu">与之前的结果相比，我们将错误率从3.3%降低到了2.8%，降低了15%，完全没有改变或重新训练模型！</strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/77227948dce1d14f51e0dd968d205145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0OJC-Lm25syuA2-X.png"/></div></div></figure><h2 id="f539" class="lq lr it bd ls lt lu dn lv lw lx dp ly kr lz ma mb kv mc md me kz mf mg mh mi bi translated">蒙特卡洛退出:预测不确定性</h2><p id="7f24" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">让我们来看看预测不确定性。在分类任务中，从softmax输出中获得的类概率经常被错误地解释为模型置信度。然而，<a class="ae mv" href="https://arxiv.org/abs/1506.02142" rel="noopener ugc nofollow" target="_blank">Gal&amp;Ghahramani(2016)</a>表明，即使softmax输出很高，模型的预测也可能不确定。我们也可以在MNIST预测中看到这一点。让我们比较一下softmax输出和蒙特卡洛预测的单个测试示例的退出概率。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="38d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">soft max _ output:<code class="fe my mz na nb b">[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]</code><br/>MC _ pred _ proba:<code class="fe my mz na nb b">[0. 0. 0.989 0.008 0.001 0. 0. 0.001 0.001 0. ]</code></p><p id="e970" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">双方都同意测试示例最有可能来自第三类。然而，soft max 100%确定是这种情况，这应该已经提醒您有些事情不对劲了。0%或100%的概率估计通常是危险的。蒙特卡洛辍学为我们提供了更多关于预测不确定性的信息:最有可能是第3类，但也有小概率是第4类，例如，第5类虽然不太可能，但仍然比第1类更有可能。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/77227948dce1d14f51e0dd968d205145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0OJC-Lm25syuA2-X.png"/></div></div></figure><h2 id="8c25" class="lq lr it bd ls lt lu dn lv lw lx dp ly kr lz ma mb kv mc md me kz mf mg mh mi bi translated">蒙特卡洛辍学:回归问题</h2><p id="1dbf" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">到目前为止，我们已经讨论了一个分类任务。现在让我们来看一个回归问题，看看蒙特卡洛退出是如何为我们提供预测不确定性的。让我们使用波士顿住房数据集拟合一个回归模型来预测房价。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="9c65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于分类任务，我们定义了函数来预测类别概率和最可能的类别。同样，对于回归问题，我们需要函数来获得预测分布和点估计(让我们使用平均值)。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="6151" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们再次对一个测试示例进行100次预测，并绘制它们的分布，标记其平均值，这是我们的点估计，或最佳猜测。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/8340bcf2c1510b19d13d948e8cc8aebe.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*aE8j5Kz579Yc59xjvSKqzA.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">来自波士顿住房数据的一个测试示例的预测价格分布。红线表示平均值。</p></figure><p id="ee29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这个特定的测试示例，预测分布的平均值为18，但是我们可以看到，其他值也不是不可能的—该模型对其预测不是很确定。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/77227948dce1d14f51e0dd968d205145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0OJC-Lm25syuA2-X.png"/></div></div></figure><h2 id="f061" class="lq lr it bd ls lt lu dn lv lw lx dp ly kr lz ma mb kv mc md me kz mf mg mh mi bi translated">蒙特卡洛辍学:一个实施细节</h2><p id="93ed" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">最后一点:在本文中，我们一直通过将模型的<code class="fe my mz na nb b">training</code>模式设置为<code class="fe my mz na nb b">true</code>来实现蒙特卡洛脱离。这很有效，但是它可能会影响模型的其他部分，这些部分在训练和推断时表现不同，例如批处理规范化。为了确保我们只打开Dropout而不影响其他任何东西，我们应该创建一个自定义的MonteCarloDropout层，该层从常规dropout继承，并在默认情况下将其<code class="fe my mz na nb b">training</code>参数设置为<code class="fe my mz na nb b">true</code>(以下代码改编自Geron (2019))。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/77227948dce1d14f51e0dd968d205145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0OJC-Lm25syuA2-X.png"/></div></div></figure><h2 id="8943" class="lq lr it bd ls lt lu dn lv lw lx dp ly kr lz ma mb kv mc md me kz mf mg mh mi bi translated">结论</h2><ul class=""><li id="690f" class="nd ne it kk b kl mj ko mk kr nf kv ng kz nh ld ni nj nk nl bi translated">蒙特卡洛退出归结为用常规退出训练神经网络，并在推理时保持其开启。这样，我们可以为每个实例生成多个不同的预测。</li><li id="cc42" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">对于分类任务，我们可以平均每个类的softmax输出。这往往会导致更准确的预测，从而更恰当地表达模型的不确定性。</li><li id="c26c" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">对于回归任务，我们可以分析预测分布，以检查哪些值是可能的，或者使用其平均值或中值对其进行汇总。</li><li id="26ef" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">Monte Carlo Dropout很容易在TensorFlow中实现:它只需要在进行预测之前将模型的<code class="fe my mz na nb b">training</code>模式设置为<code class="fe my mz na nb b">true</code>。最安全的方法是编写一个自定义的三行类，继承常规Dropout。</li></ul><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/77227948dce1d14f51e0dd968d205145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0OJC-Lm25syuA2-X.png"/></div></div></figure><h2 id="7078" class="lq lr it bd ls lt lu dn lv lw lx dp ly kr lz ma mb kv mc md me kz mf mg mh mi bi translated">来源</h2><ul class=""><li id="8b36" class="nd ne it kk b kl mj ko mk kr nf kv ng kz nh ld ni nj nk nl bi translated">Gal Y. &amp; Ghahramani Z .，2016年，辍学作为贝叶斯近似:表示深度学习中的模型不确定性，第33届机器学习国际会议论文集</li><li id="ce63" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">Geron A .，2019，第二版，使用Scikit-Learn和TensorFlow进行机器学习:构建智能系统的概念、工具和技术</li></ul><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/77227948dce1d14f51e0dd968d205145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0OJC-Lm25syuA2-X.png"/></div></div></figure><p id="1e9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读！</p><p id="4d86" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你喜欢这篇文章，为什么不订阅电子邮件更新我的新文章呢？并且通过<a class="ae mv" href="https://michaloleszak.medium.com/membership" rel="noopener"> <strong class="kk iu">成为媒介会员</strong> </a>，可以支持我的写作，获得其他作者和我自己的所有故事的无限访问权限。</p><p id="5b7f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">需要咨询？你可以问我任何事情，也可以在这里 为我预约1:1 <a class="ae mv" href="http://hiretheauthor.com/michal" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">。</strong></a></p><p id="0f9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你也可以试试我的另外一篇文章<a class="ae mv" href="https://michaloleszak.github.io/blog/" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"/></a>。不能选择？从这些中选择一个:</p><div class="nr ns gp gr nt nu"><a rel="noopener follow" target="_blank" href="/svm-kernels-what-do-they-actually-do-56ce36f4f7b8"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd iu gy z fp nz fr fs oa fu fw is bi translated">SVM内核:他们实际上做什么？</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">直观的视觉解释</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">towardsdatascience.com</p></div></div><div class="od l"><div class="oe l of og oh od oi lo nu"/></div></div></a></div><div class="nr ns gp gr nt nu"><a rel="noopener follow" target="_blank" href="/calibrating-classifiers-559abc30711a"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd iu gy z fp nz fr fs oa fu fw is bi translated">校准分类器</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">你确定你的模型返回概率吗？🎲</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">towardsdatascience.com</p></div></div><div class="od l"><div class="oj l of og oh od oi lo nu"/></div></div></a></div><div class="nr ns gp gr nt nu"><a rel="noopener follow" target="_blank" href="/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd iu gy z fp nz fr fs oa fu fw is bi translated">线性回归中收缩法和选择法的比较</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">详细介绍7种流行的收缩和选择方法。</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">towardsdatascience.com</p></div></div><div class="od l"><div class="ok l of og oh od oi lo nu"/></div></div></a></div></div></div>    
</body>
</html>