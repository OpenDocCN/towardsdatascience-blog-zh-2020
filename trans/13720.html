<html>
<head>
<title>Proximal Policy Optimization (PPO) With TensorFlow 2.x</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 2.x的近似策略优化(PPO)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/proximal-policy-optimization-ppo-with-tensorflow-2-x-89c9430ecc26?source=collection_archive---------10-----------------------#2020-09-21">https://towardsdatascience.com/proximal-policy-optimization-ppo-with-tensorflow-2-x-89c9430ecc26?source=collection_archive---------10-----------------------#2020-09-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c767" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解PPO强化学习算法并用TensorFlow 2.x实现</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5c1b144de9f4a77c2e392713e2337c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XreRjuz6MmATuoRvmprsdA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Neenu Vimalkumar 在<a class="ae kv" href="https://unsplash.com/s/photos/clip?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="39b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将尝试理解Open-AI用于强化学习的近似策略优化算法。在一些基础理论之后，我们将使用TensorFlow 2.x实现PPO。在你进一步阅读之前，我建议你看一下来自<a class="ae kv" rel="noopener" target="_blank" href="/actor-critic-with-tensorflow-2-x-part-2of-2-b8ceb7e059db">的演员-评论家方法，因为我们将为PPO修改那篇文章的代码。</a></p><h2 id="5428" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">为什么选择PPO？</h2><ol class=""><li id="879c" class="ml mm iq ky b kz mn lc mo lf mp lj mq ln mr lr ms mt mu mv bi translated"><strong class="ky ir">不稳定的策略更新</strong>:在许多策略梯度方法中，由于较大的步长，策略更新是不稳定的，这导致坏的策略更新，并且当这个新的坏策略用于学习时，它导致甚至更坏的策略。如果步子迈得小，就会导致学习速度变慢。</li><li id="b643" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated"><strong class="ky ir">数据低效</strong>:很多学习方法是从当前经验中学习，在梯度更新后丢弃经验。这使得学习过程缓慢，因为神经网络需要大量的数据来学习。</li></ol><p id="bc01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PPO可以很方便地克服上述问题。</p><h2 id="f853" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">PPO背后的核心理念</h2><p id="7a7b" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在早期的政策梯度方法中，目标函数类似于<strong class="ky ir">LPG(θ)= Et[logπθ(At | ST)ˇAt]。</strong>但是现在我们将采用当前策略和旧策略的比率，而不是当前策略的日志。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/086e23e6005a5fc4b151398420ceb209.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*Zgk2-ZhrDPzFI0ZYCxHc4A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片经由<a class="ae kv" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1707.06347</a></p></figure><p id="0784" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还将削减比率，并将削减和不削减两者中的最小值，即b/w。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/27cb507b7664629eb2d1898ec923c9d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*2-rWCA-oqVxsw-MnVd_lKQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片经由<a class="ae kv" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1707.06347</a></p></figure><p id="3a6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如下所示，这个缩减的目标将限制大型策略更新。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/90dc3472d1e335387f35607cf8d614e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VN01Obh5VyJ6QuA0qfyq6w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片经由<a class="ae kv" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1707.06347</a></p></figure><p id="7198" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，最终目标包含3个部分，第一个是Lclip，第二个是我们的critic net的MSE，即预测的和目标的状态值的平方损失。第三部分是鼓励探索的熵。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b38f4fc12a1b8b390e96a40a197e2bae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*tMqQTOzn0K-qm6Qv4z1Jnw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片经由<a class="ae kv" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1707.06347</a></p></figure><h1 id="6d48" class="ni lt iq bd lu nj nk nl lx nm nn no ma jw np jx md jz nq ka mg kc nr kd mj ns bi translated">算法步骤</h1><ol class=""><li id="e742" class="ml mm iq ky b kz mn lc mo lf mp lj mq ln mr lr ms mt mu mv bi translated">玩游戏n步，存储状态，行动概率，奖励，完成变量。</li><li id="4bd4" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">将广义优势估计方法应用于上述经验。我们将在编码部分看到这一点。</li><li id="875b" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">通过计算它们各自的损失来训练一些时期的神经网络。</li><li id="d6f9" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">测试这个训练好的模型的“m”次发作。</li><li id="a271" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">如果测试集的平均奖励大于您设定的目标奖励，则停止，否则从第一步开始重复。</li></ol></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h1 id="aeab" class="ni lt iq bd lu nj oa nl lx nm ob no ma jw oc jx md jz od ka mg kc oe kd mj ns bi translated">密码</h1><p id="0a1d" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated"><strong class="ky ir">神经网络:</strong></p><ol class=""><li id="6090" class="ml mm iq ky b kz la lc ld lf of lj og ln oh lr ms mt mu mv bi translated">在导入了所需的库并初始化了我们的环境之后，我们定义了我们的神经网络，类似于Actor-Critic文章中的神经网络。</li><li id="ef7f" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">行动者网络将当前状态作为输入，输出每个动作的概率。</li><li id="f631" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">批评家网络输出一个州的价值。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="ead9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">动作选择:</strong></p><ol class=""><li id="90bc" class="ml mm iq ky b kz la lc ld lf of lj og ln oh lr ms mt mu mv bi translated">我们定义我们的代理类，并初始化优化器和学习率。</li><li id="d5a9" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">我们还定义了一个clip_pram变量，它将在演员损失函数中使用。</li><li id="1c19" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">对于动作选择，我们将使用TensorFlow概率库，它将概率作为输入，并将其转换为分布。</li><li id="8994" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">然后，我们使用该分布进行动作选择。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="1b1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">测试模型知识:</strong></p><ol class=""><li id="59fa" class="ml mm iq ky b kz la lc ld lf of lj og ln oh lr ms mt mu mv bi translated">这个函数将用于测试我们代理的知识，并返回一集的总奖励。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="4a27" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">训练循环:</strong></p><ol class=""><li id="45dd" class="ml mm iq ky b kz la lc ld lf of lj og ln oh lr ms mt mu mv bi translated">我们将循环“步骤”时间，即我们将收集“步骤”时间的经验。</li><li id="e87d" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">下一个循环是代理与环境交互的次数，我们将体验存储在不同的列表中。</li><li id="3e49" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">在上述循环之后，我们计算并添加最后一个状态旁边的状态的值，用于广义优势估计方法中的计算。</li><li id="3a0a" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">然后，我们处理广义优势估计方法中的所有列表，以获得回报，优势。</li><li id="0782" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">接下来，我们为10个纪元训练我们的网络。</li><li id="5a4d" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">培训结束后，我们将在测试环境中对我们的代理进行五集测试。</li><li id="dcfd" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">如果测试集的平均奖励大于您设定的目标奖励，则停止，否则从第一步开始重复。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="770f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">广义优势估计:</strong></p><ol class=""><li id="4550" class="ml mm iq ky b kz la lc ld lf of lj og ln oh lr ms mt mu mv bi translated">我们定义了一个预处理函数，它实现了计算回报和收益的GAE方法。</li><li id="b23e" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">将变量“g”初始化为零，将lambda初始化为0.95。</li><li id="d86c" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">我们通过颠倒奖励列表从后面循环奖励。</li><li id="889d" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">将delta计算为(当前动作的回报+ gamma *下一状态的值*对于终端状态为零的done变量-当前状态的值)。</li><li id="cdf5" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">将变量“g”计算为(delta + gamma * lambda * done变量* g)。</li><li id="2fdf" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">将返回计算为(当前状态的g +值)。</li><li id="48b1" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">反转返回列表，就像我们从后往前计算一样。</li><li id="9723" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">优势的计算方式为(returns-values)。<strong class="ky ir">这里我们使用值[:-1]。毕竟，</strong>我们的值列表比所有其他列表大一个，因为为了计算的目的，我们添加了紧挨着最后一个状态的状态的值。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="b33c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">学习功能:</strong></p><ol class=""><li id="5e23" class="ml mm iq ky b kz la lc ld lf of lj og ln oh lr ms mt mu mv bi translated">学习函数将(在与环境交互期间存储或计算的状态、动作、优势、概率、回报的数组)作为输入。</li><li id="648e" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">我们计算当前的概率和损失。评论家的损失是MSE。</li><li id="158f" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">该函数使用渐变抽头执行渐变更新。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="c7a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">演员流失:</strong></p><ol class=""><li id="89e7" class="ml mm iq ky b kz la lc ld lf of lj og ln oh lr ms mt mu mv bi translated">行动者损失将当前概率、行动、优势、旧概率和批评家损失作为输入。</li><li id="136d" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">首先，我们计算熵并计算平均值。</li><li id="748a" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">然后，我们循环遍历概率、优势和旧概率，计算比率、裁剪比率，并将其添加到列表中。</li><li id="0586" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">然后，我们计算损失。<strong class="ky ir">注意这里的</strong>我们采取了负损失，因为我们想执行梯度上升，而不是梯度下降。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="26f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是关于编码的全部内容。现在让我们看看您的代理没有学习的原因和一些技巧。</p></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h1 id="9196" class="ni lt iq bd lu nj oa nl lx nm ob no ma jw oc jx md jz od ka mg kc oe kd mj ns bi translated">实施时需要注意的事项:</h1><p id="f2d7" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在编写RL代码时，要记住以下几点。</p><ol class=""><li id="7513" class="ml mm iq ky b kz la lc ld lf of lj og ln oh lr ms mt mu mv bi translated">神经元的数量、隐藏层数、学习速率对学习有巨大的影响。</li><li id="24d6" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">张量和NumPy数组的形状应该是正确的。很多时候，实现是正确的，代码也是有效的，但是代理没有学到任何东西，仅仅是因为张量的形状不正确，并且当对这些张量进行运算时会给出错误的结果</li></ol></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><p id="ea73" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在这里找到这篇文章<a class="ae kv" href="https://github.com/abhisheksuran/Atari_DQN/blob/master/PPO.ipynb" rel="noopener ugc nofollow" target="_blank">的完整代码。敬请关注即将发布的文章，我们将在TensorFlow 2中实现更多RL算法和深度学习算法。</a></p><p id="eb26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，本文到此结束。谢谢你的阅读，希望你喜欢并且能够理解我想要解释的内容。希望你阅读我即将发表的文章。哈里奥姆…🙏</p><h1 id="ccbc" class="ni lt iq bd lu nj nk nl lx nm nn no ma jw np jx md jz nq ka mg kc nr kd mj ns bi translated">参考资料:</h1><div class="ok ol gp gr om on"><a href="https://arxiv.org/abs/1707.06347" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ir gy z fp os fr fs ot fu fw ip bi translated">近似策略优化算法</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">我们提出了一种新的强化学习策略梯度方法</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">arxiv.org</p></div></div></div></a></div><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ow oj l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ow oj l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ow oj l"/></div></figure></div></div>    
</body>
</html>