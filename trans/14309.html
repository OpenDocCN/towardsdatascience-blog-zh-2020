<html>
<head>
<title>What is a Siamese Neural Network?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是连体神经网络？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-a-siamese-neural-network-b0dbeb1c6db7?source=collection_archive---------33-----------------------#2020-10-02">https://towardsdatascience.com/what-is-a-siamese-neural-network-b0dbeb1c6db7?source=collection_archive---------33-----------------------#2020-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1d9e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">专门测量相似性的神经网络</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b7c86d0211496d028c74d9038e590a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wHCA3VNAkyX1mt2K"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@kimtheris?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Sereja Ris </a>拍摄</p></figure><p id="5da9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习的许多应用都围绕着检查两个对象是否相似。例如:</p><ul class=""><li id="8e64" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">对于面部识别，检查输入的面部图像是否与数据库中的任何图像相似</li><li id="7fbc" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在问答网站上，检查新问题是否与任何存档的问题相似</li></ul><p id="d7d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如今，测量两个对象之间的相似性的最常见和最方便的方法之一如下:</p><ol class=""><li id="ef00" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu mj mb mc md bi translated">获取每个对象的矢量表示(也称为嵌入)。例如预训练ResNet的中间层的输出、句子中所有单词的预训练单词向量的平均值</li><li id="5b64" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu mj mb mc md bi translated">计算步骤(1)中两个向量之间的余弦相似度</li><li id="19fe" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu mj mb mc md bi translated">使用步骤(2)中的值来确定这两个对象是否相似</li></ol><p id="b04c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这种方法通常表现不佳，因为步骤(1)中的矢量表示在我们的应用中不是专用的。如果我们想微调这些向量表示，甚至从头开始训练一个新的模型，我们可以使用连体神经网络架构。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="3447" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">网络体系结构</h2><p id="9923" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">连体神经网络由两个相同的子网组成，也称为双网络，它们的输出端相连。这两个网络不仅具有相同的架构，而且它们还共享权重。它们并行工作，负责为输入创建矢量表示。例如，如果我们的输入是图像，我们可以使用ResNet作为孪生网络。我们可以将连体神经网络视为孪生网络的包装器。它们通过测量向量之间的相似性来帮助产生更好的向量表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/96ba15aa4c5315adfccde2db9a81baf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*J5STzH7YMPvYDnA_50MkOA.png"/></div></figure><p id="ab06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上图中，<em class="nq"> x </em> ₁和<em class="nq"> x </em> ₂是我们要比较的两个对象，<em class="nq"> v </em> ₁和<em class="nq"> v </em> ₂是<em class="nq"> x </em> ₁和<em class="nq"> x </em> ₂.的矢量表示比较层的架构取决于损失函数和训练数据的标签。由于我们的目标是在矢量表示中获得尽可能多的信息，比较层通常具有非常简单的架构。以下是一些常见的选择:</p><ul class=""><li id="28a2" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">计算<em class="nq"> v </em> ₁和<em class="nq"> v </em> ₂ <br/>的余弦相似度标签为-1到1之间的实数，损失函数为均方误差。</li><li id="1056" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">串联<em class="nq"> v </em> ₁、<em class="nq"> v </em> ₂和绝对元素差异|<em class="nq">v</em>₁-<em class="nq">v</em>₂|，接着是全连接层和一个softmax层<br/>这是用于多类分类的，损失函数是交叉熵。</li><li id="eca2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">计算<em class="nq"> v </em> ₁和<em class="nq"> v </em> ₂ <br/>之间的欧几里德距离<em class="nq">v</em>₁<em class="nq">v</em>₂‖损失函数是对比损失或三重损失。由于这两个损失不太为人所知，我们将在下一节简要解释它们。</li></ul><p id="cb1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，比较层应该是对称的w.r.t. <em class="nq"> v </em> ₁和<em class="nq"> v </em> ₂.结合双胞胎网络具有相同架构和权重的事实，这使得整个连体神经网络对称w.r.t. <em class="nq"> x </em> ₁和<em class="nq"> x </em> ₂.</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="8c65" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">损失函数</h2><p id="62bd" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">对比损失和三重损失都是基于距离的损失函数，主要用于学习向量表示，并且通常与连体神经网络结合使用。</p><h2 id="1c9b" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">对比损失</h2><p id="902f" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">假设我们的数据集由不同类别的对象组成。例如，ImageNet数据集由汽车图像、狗图像等组成。然后对于每一对输入(<em class="nq"> x </em> ₁，<em class="nq"> x </em> ₂)，</p><ul class=""><li id="3d50" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">如果<em class="nq"> x </em> ₁和<em class="nq"> x </em> ₂属于同一个类，我们希望它们的矢量表示相似。因此，我们希望最小化<em class="nq">l</em>=<em class="nq">v</em>₁-v₂‖。</li><li id="a509" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">另一方面，如果<em class="nq"> x </em> ₁和<em class="nq"> x </em> ₂属于不同的类，我们希望<em class="nq">v</em>₁-v₂‖比较大。我们要最小化的项是<br/> <em class="nq"> L </em> = max(0，<em class="nq">m</em>‖<em class="nq">v</em>₁<em class="nq">v</em>₂‖)，其中<em class="nq"> m </em>是一个超参数，称为余量。余量的想法是，当<em class="nq"> v </em> ₁和<em class="nq"> v </em> ₂相差足够大的时候，<em class="nq"> L </em>就已经是0了，不能再小了。因此，该模型不会浪费精力来进一步分离<em class="nq"> v </em> ₁和<em class="nq"> v </em> ₂，而是会关注其他输入对。</li></ul><p id="5b9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将这两种情况合并成一个公式:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="5f84" class="mr ms it ns b gy nw nx l ny nz"><em class="nq">L</em> = <em class="nq">y * </em>‖<em class="nq">v</em>₁ − <em class="nq">v</em>₂‖² + (1 - y) * max(0, <em class="nq">m</em> − ‖<em class="nq">v</em>₁ − <em class="nq">v</em>₂‖)²</span></pre><p id="3528" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="nq"> y </em> = 1如果<em class="nq"> x </em> ₁和<em class="nq"> x </em> ₂属于同一类，否则<em class="nq"> y </em> = 0。</p><h2 id="9a40" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">三重损失</h2><p id="ae07" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">通过考虑输入的三元组(<em class="nq"> xa </em>、<em class="nq"> xp </em>、<em class="nq"> xn </em>)，三元组损失将上述思想向前推进了一步。这里<em class="nq"> xa </em>是锚对象，<em class="nq"> xp </em>是正对象(即<em class="nq"> xa </em>和<em class="nq"> xp </em>属于同一类)，而<em class="nq"> xn </em>是负对象(即<em class="nq"> xa </em>和<em class="nq"> xn </em>属于不同类)。我们的目标是使矢量表示<em class="nq"> va </em>更类似于<em class="nq"> vp </em>而不是<em class="nq"> vn </em>。精确公式由下式给出</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="d823" class="mr ms it ns b gy nw nx l ny nz"><em class="nq">L</em> = max(0, <em class="nq">m</em> + ‖<em class="nq">va</em> − <em class="nq">vp</em>‖ - ‖<em class="nq">va</em> − <em class="nq">vn</em>‖)</span></pre><p id="cb5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="nq"> m </em>是超参数余量。就像对比损失的情况一样，裕量决定了‖<em class="nq">va</em><em class="nq">VP</em>和‖<em class="nq">va</em><em class="nq">VN</em>之间的差异何时变得足够大，使得模型将不再从这三个一组中调整其权重。</p><p id="ae1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于对比损失和三元组损失，我们如何从不同类别的对象中采样输入对(<em class="nq"> x </em> ₁，<em class="nq"> x </em> ₂)和三元组(<em class="nq"> xa </em>，<em class="nq"> xp </em>，<em class="nq"> xn </em>)对模型训练过程有很大影响。理想情况下，我们希望输入对和三元组对我们的模型来说不太容易，但也不太难。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="cf54" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">履行</h2><p id="d639" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">尽管我们在上图中有孪生网络A和B，但实际上只有孪生网络的单个副本通常更方便。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="e0e5" class="mr ms it ns b gy nw nx l ny nz">def forward(self, x1, x2):<br/>    v1 = self.twin_network(x1)<br/>    v2 = self.twin_network(x2)<br/>    return self.comparison_layers(v1, v2)</span></pre><p id="ab9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TensorFlow和PyTorch都内置了一些常见的损失函数。</p><h2 id="8aa4" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">张量流</h2><ul class=""><li id="7917" class="lv lw it lb b lc nk lf nl li oa lm ob lq oc lu ma mb mc md bi translated"><a class="ae ky" href="https://www.tensorflow.org/addons/api_docs/python/tfa/losses/ContrastiveLoss" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/addons/API _ docs/python/TFA/loss/ContrastiveLoss</a></li></ul><h2 id="a5b2" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">PyTorch</h2><ul class=""><li id="58c7" class="lv lw it lb b lc nk lf nl li oa lm ob lq oc lu ma mb mc md bi translated"><a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss" rel="noopener ugc nofollow" target="_blank">https://py torch . org/docs/stable/generated/torch . nn . cosineembeddingloss . html</a></li><li id="3009" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss" rel="noopener ugc nofollow" target="_blank">https://py torch . org/docs/stable/generated/torch . nn . tripletmarginloss . html</a></li></ul></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="6659" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">进一步阅读</h2><ul class=""><li id="7ff9" class="lv lw it lb b lc nk lf nl li oa lm ob lq oc lu ma mb mc md bi translated">[1]是对连体神经网络的一个很好的概述。特别是，它包含了在不同领域，如图像分析，文本挖掘，生物学，医学和健康的暹罗神经网络应用的许多参考。</li><li id="c294" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">[3]使用连体神经网络来微调由BERT产生的矢量表示。这是利用预训练模型的一个很好的例子。</li><li id="8f99" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">三联体丢失在连体神经网络中非常普遍，并且在小批量设置中引入了它的变体。[4]在一批中为每一对锚或阳性选择“半硬”阴性，并仅在这些三元组上训练网络。下图最能说明“半硬”底片的概念:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/666f9d44da92a15af0725d8e34d2ce13.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*3igb6NAWNGHk8w7OL9yNWg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[5]: <em class="oe">三种类型的否定，给定一个锚和一个肯定</em></p></figure><ul class=""><li id="997c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">[2]在形成三元组时，为每个锚点选择一个批次中最难的肯定值和最难的否定值。这里最难是指最大的<em class="nq">va</em>—<em class="nq">VP</em>为正，或者最小的<em class="nq">va</em>—<em class="nq">VN</em>为负。我们参考[5]对这两种三重态损失变量的更详细解释，以及在TensorFlow中使用它们的TensorFlow附件<a class="ae ky" href="https://www.tensorflow.org/addons/api_docs/python/tfa/losses/TripletSemiHardLoss" rel="noopener ugc nofollow" target="_blank">文档</a>。</li></ul></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="c569" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">参考</h2><ol class=""><li id="0f1e" class="lv lw it lb b lc nk lf nl li oa lm ob lq oc lu mj mb mc md bi translated">D.奇克。<a class="ae ky" href="https://link.springer.com/protocol/10.1007%2F978-1-0716-0826-5_3" rel="noopener ugc nofollow" target="_blank">连体神经网络:概述</a> (2020)，<em class="nq">人工神经网络</em>，分子生物学方法，第2190卷，Springer协议，第73–94页</li><li id="7ec7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu mj mb mc md bi translated">A.赫尔曼斯、l .拜尔和b .莱贝。<a class="ae ky" href="https://arxiv.org/pdf/1703.07737.pdf" rel="noopener ugc nofollow" target="_blank">为人身重新鉴定三重缺失辩护</a> (2017)，arXiv</li><li id="142c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu mj mb mc md bi translated">名词（noun的缩写）赖默斯和我古雷维奇。<a class="ae ky" href="https://www.aclweb.org/anthology/D19-1410.pdf" rel="noopener ugc nofollow" target="_blank">句子-BERT:使用连体BERT网络的句子嵌入</a> (2019)，em NLP ijc NLP 2019</li><li id="99d9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu mj mb mc md bi translated">F.Schroff，D. Kalenichenko和J. Philbin。<a class="ae ky" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf" rel="noopener ugc nofollow" target="_blank"> FaceNet:人脸识别和聚类的统一嵌入</a> (2015)，CVPR 2015</li><li id="63af" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu mj mb mc md bi translated"><a class="ae ky" href="https://omoindrot.github.io/triplet-loss" rel="noopener ugc nofollow" target="_blank">tensor flow中的三元组丢失和在线三元组挖掘</a></li></ol></div></div>    
</body>
</html>