<html>
<head>
<title>K-means, DBSCAN, GMM, Agglomerative clustering — Mastering the popular models in a segmentation problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-means、DBSCAN、GMM、凝聚聚类——掌握细分问题中的流行模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-dbscan-gmm-agglomerative-clustering-mastering-the-popular-models-in-a-segmentation-c891a3818e29?source=collection_archive---------2-----------------------#2020-11-08">https://towardsdatascience.com/k-means-dbscan-gmm-agglomerative-clustering-mastering-the-popular-models-in-a-segmentation-c891a3818e29?source=collection_archive---------2-----------------------#2020-11-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e9f0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为无监督聚类实现python中最广泛使用的模型的一站式商店</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c456a41f0e2e62ea11f2701da4b18deb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_c-A_nt4YPUzNlFR"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梅尔·普尔在<a class="ae kv" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="746e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在当今时代，大量客户/产品的粒度数据的可用性以及高效处理数Pb数据的技术能力正在快速增长。正因为如此，现在有可能提出非常有战略意义和有意义的集群来进行有效的定位。并且识别目标分段需要稳健的分段练习。在这篇博客中，我们将讨论最流行的无监督聚类算法，以及如何用python实现它们。</p><p id="ce79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇博客中，我们将使用一家为孕妇提供服装的在线商店的点击流<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/clickstream+data+for+online+shopping" rel="noopener ugc nofollow" target="_blank">数据</a>。它包括产品类别、照片在网页上的位置、IP地址的来源国以及产品的美元价格等变量。它有2008年4月到2008年8月的数据。</p><p id="3e11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">第一步是为分割准备数据。我建议您在继续下一步之前，查看下面的文章，以深入了解为分段准备数据的不同步骤:</em></p><p id="b30d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/one-hot-encoding-standardization-pca-data-preparation-steps-for-segmentation-in-python-24d07671cf0b"> <em class="ls"> One Hot编码、标准化、PCA:python中分割的数据准备</em> </a></p><p id="d839" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">选择最佳聚类数是处理分割问题时需要注意的另一个关键概念。如果您阅读下面的文章，将有助于您理解选择集群的流行指标的完整列表:</em></p><p id="5dd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad"><em class="ls">Python中选择最佳聚类数的7种方法实现清单</em> </a></p><p id="7eb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在本博客中讨论4类模型:</p><ol class=""><li id="5130" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">k均值</li><li id="af4f" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">凝聚聚类</li><li id="bdb4" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">基于密度的空间聚类</li><li id="d4da" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">高斯混合模型(GMM)</li></ol><p id="293b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">K-表示</strong></p><p id="3a7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">K均值算法是一个迭代过程，有三个关键阶段:</p><ol class=""><li id="36a4" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated"><strong class="ky ir">选择初始聚类质心</strong></li></ol><p id="dc81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该算法从选取初始的k个聚类中心开始，这些聚类中心被称为质心。确定最佳聚类数(即k)以及正确选择初始聚类对于模型的性能极其重要。聚类的数量应该总是取决于数据集的性质，而初始聚类的不良选择会导致局部收敛的问题。幸运的是，我们对这两者都有解决方案。</p><p id="3c84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有关选择最佳集群数量的更多详细信息，请参考这篇详细的<a class="ae kv" rel="noopener" target="_blank" href="/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad">博客</a>。对于初始聚类的选择，我们可以运行具有各种初始化的模型的多次迭代来挑选最稳定的一个，或者使用具有以下步骤的“k-means++”算法:</p><ol class=""><li id="11fb" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated"><em class="ls">从数据集中随机选择第一个质心</em></li><li id="e652" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><em class="ls">计算数据集中所有点与所选质心的距离</em></li><li id="0bfd" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><em class="ls">选择一个点作为新的质心，该点具有与该距离成比例的最大概率</em></li><li id="eebf" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><em class="ls">重复步骤2和3，直到k个质心被采样</em></li></ol><p id="9212" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该算法将质心初始化为彼此远离，从而产生比随机初始化更稳定的结果。</p><p id="143e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.<strong class="ky ir">集群分配</strong></p><p id="0495" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">K-means然后根据点和所有质心之间的欧几里德距离将数据点分配给最近的聚类质心。</p><p id="50bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 3。移动质心</strong></p><p id="acb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型最后计算聚类中所有点的平均值，并将质心移动到该平均位置。</p><p id="fd02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">重复步骤2和3，直到聚类中没有变化或者可能满足一些其他停止条件(例如最大迭代次数)。</p><p id="9c1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了在python中实现模型，我们需要首先指定集群的数量。我们使用了肘方法、间隙统计、轮廓评分、Calinski Harabasz评分和Davies Bouldin评分。对于这些方法中的每一种，最佳聚类数如下:</p><ol class=""><li id="5164" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">肘法:8</li><li id="9831" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">差距统计:29</li><li id="38a7" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">剪影评分:4</li><li id="46ee" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">卡林斯基·哈拉巴斯得分:2</li><li id="333a" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">戴维斯·波尔丁得分:4</li></ol><p id="55fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上所述，5种方法中有2种建议我们应该使用4个集群。如果每个模型建议不同数量的聚类，我们可以取平均值或中值。找到最佳k数的代码可以在<a class="ae kv" href="https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git" rel="noopener ugc nofollow" target="_blank">这里</a>找到，关于每种方法的更多细节可以在<a class="ae kv" rel="noopener" target="_blank" href="/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad">博客</a>中找到。</p><p id="0dc9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们有了最佳数量的聚类，我们就可以拟合模型，并使用Silhouette评分、Calinski Harabasz评分和Davies Bouldin评分来获得模型的性能。</p><pre class="kg kh ki kj gt mh mi mj mk aw ml bi"><span id="132d" class="mm mn iq mi b gy mo mp l mq mr"># K means</span><span id="c797" class="mm mn iq mi b gy ms mp l mq mr">from sklearn.cluster import KMeans<br/>from sklearn.metrics import silhouette_score <br/>from sklearn.metrics import calinski_harabasz_score<br/>from sklearn.metrics import davies_bouldin_score</span><span id="51f3" class="mm mn iq mi b gy ms mp l mq mr"># Fit K-Means<br/>kmeans_1 = KMeans(n_clusters=4,random_state= 10)</span><span id="53b0" class="mm mn iq mi b gy ms mp l mq mr"># Use fit_predict to cluster the dataset<br/>predictions = kmeans_1.fit_predict(cluster_df)</span><span id="0090" class="mm mn iq mi b gy ms mp l mq mr"># Calculate cluster validation metrics</span><span id="a1f6" class="mm mn iq mi b gy ms mp l mq mr">score_kemans_s = silhouette_score(cluster_df, kmeans_1.labels_, metric='euclidean')</span><span id="29dc" class="mm mn iq mi b gy ms mp l mq mr">score_kemans_c = calinski_harabasz_score(cluster_df, kmeans_1.labels_)</span><span id="d257" class="mm mn iq mi b gy ms mp l mq mr">score_kemans_d = davies_bouldin_score(cluster_df, predictions)</span><span id="bc54" class="mm mn iq mi b gy ms mp l mq mr">print('Silhouette Score: %.4f' % score_kemans_s)<br/>print('Calinski Harabasz Score: %.4f' % score_kemans_c)<br/>print('Davies Bouldin Score: %.4f' % score_kemans_d)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/380ad09760f682920344c050581e885c.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*Pq7pumyfOF70twgqkEmBMQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:K-Means的聚类验证指标(作者图片)</p></figure><p id="1f89" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还可以使用簇间距离图来检查簇的相对大小和分布。</p><pre class="kg kh ki kj gt mh mi mj mk aw ml bi"><span id="00e3" class="mm mn iq mi b gy mo mp l mq mr"># Inter cluster distance map<br/>from yellowbrick.cluster import InterclusterDistance</span><span id="8a93" class="mm mn iq mi b gy ms mp l mq mr"># Instantiate the clustering model and visualizer</span><span id="47b7" class="mm mn iq mi b gy ms mp l mq mr">visualizer = InterclusterDistance(kmeans_1)</span><span id="4e46" class="mm mn iq mi b gy ms mp l mq mr">visualizer.fit(cluster_df)        # Fit the data to the visualizer<br/>visualizer.show()        # Finalize and render the figure</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/3da017b6a38e644298f5c4a1728ffca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*mioXtxo6BicwN5G1lNLZ8Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:聚类间距离图:K均值(图片由作者提供)</p></figure><p id="0588" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上图所示，两个集群相对于其他集群来说相当大，并且它们之间似乎有适当的间隔。然而，如果两个聚类在2D空间重叠，这并不意味着它们在原始特征空间重叠。关于这款车型的更多细节可以在<a class="ae kv" href="https://projecteuclid.org/euclid.bsmsp/1200512992" rel="noopener ugc nofollow" target="_blank">这里</a>找到。最后，K-means的其他变体，如小批量K-Means，K-Medoids将在单独的博客中讨论。</p><h1 id="d3c8" class="mv mn iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">凝聚聚类</h1><p id="0471" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">凝聚聚类是一类通用的聚类算法，它通过连续合并数据点来构建嵌套聚类。这种聚类层次结构可以表示为一个树形图，称为树状图。树的顶部是包含所有数据点的单个聚类，而底部包含各个点。以连续方式链接数据点有多种选择:</p><ul class=""><li id="5b96" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr nr lz ma mb bi translated"><strong class="ky ir">单连锁:</strong>它<strong class="ky ir">T5】最小化成对聚类的最近观测值之间的距离</strong></li><li id="1424" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nr lz ma mb bi translated"><strong class="ky ir">完全或最大关联:</strong>试图<strong class="ky ir"> </strong>最小化成对聚类的观测值之间的最大距离</li><li id="54bb" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nr lz ma mb bi translated"><strong class="ky ir">平均关联:</strong>最小化所有聚类对之间的平均距离</li><li id="9947" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nr lz ma mb bi translated"><strong class="ky ir"> Ward: </strong>类似于k-means，因为它最小化了所有聚类内的平方差之和，但采用了分层方法。我们将在练习中使用该选项。</li></ul><p id="516d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过基于聚类验证指标(Silhouette评分、Calinski Harabasz评分和Davies Bouldin评分)检查哪种连锁方法表现最佳，可以挑选出理想的选项。与K-means相似，我们必须指定该模型中的聚类数，树状图可以帮助我们做到这一点。</p><pre class="kg kh ki kj gt mh mi mj mk aw ml bi"><span id="aad5" class="mm mn iq mi b gy mo mp l mq mr"># Dendrogram for Hierarchical Clustering<br/>import scipy.cluster.hierarchy as shc<br/>from matplotlib import pyplot<br/>pyplot.figure(figsize=(10, 7))  <br/>pyplot.title("Dendrograms")  <br/>dend = shc.dendrogram(shc.linkage(cluster_df, method='ward'))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/6ac332642e9ffb917bf9cf5b2cef075e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*v8MPcCMLEynkpYq9gukmUQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:树状图(图片由作者提供)</p></figure><p id="d646" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从图3中，我们可以看到我们可以选择4或8个集群。我们还使用肘方法、剪影得分和Calinski Harabasz得分来寻找最佳聚类数，并得到以下结果:</p><ol class=""><li id="7ad4" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">肘法:10</li><li id="26b9" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">戴维斯·波尔丁得分:8</li><li id="eb1b" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">剪影评分:3</li><li id="8b02" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">卡林斯基·哈拉巴斯得分:2</li></ol><p id="f31c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将按照戴维斯·波尔丁评分和树状图的建议进行8分。如果指标给出了不同的集群数量，我们可以继续使用树状图建议的数量(因为它是基于这个特定的模型),或者取所有指标的平均值/中值。寻找最佳集群数量的代码可以在这里<a class="ae kv" href="https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git" rel="noopener ugc nofollow" target="_blank">找到</a>，关于每种方法的更多细节可以在这个<a class="ae kv" rel="noopener" target="_blank" href="/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad">博客</a>中找到。</p><p id="c71a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与K-means类似，我们可以用最佳的集群数量和链接类型来拟合模型，并使用K-means中使用的三个指标来测试其性能。</p><pre class="kg kh ki kj gt mh mi mj mk aw ml bi"><span id="9ddd" class="mm mn iq mi b gy mo mp l mq mr"># Agglomerative clustering<br/>from numpy import unique<br/>from numpy import where<br/>from sklearn.cluster import AgglomerativeClustering<br/>from matplotlib import pyplot</span><span id="6beb" class="mm mn iq mi b gy ms mp l mq mr"># define the model<br/>model = AgglomerativeClustering(n_clusters=4)<br/># fit model and predict clusters<br/>yhat = model.fit(cluster_df)<br/>yhat_2 = model.fit_predict(cluster_df)<br/># retrieve unique clusters<br/>clusters = unique(yhat)</span><span id="d15a" class="mm mn iq mi b gy ms mp l mq mr"># Calculate cluster validation metrics</span><span id="9ea8" class="mm mn iq mi b gy ms mp l mq mr">score_AGclustering_s = silhouette_score(cluster_df, yhat.labels_, metric='euclidean')</span><span id="f3be" class="mm mn iq mi b gy ms mp l mq mr">score_AGclustering_c = calinski_harabasz_score(cluster_df, yhat.labels_)</span><span id="4638" class="mm mn iq mi b gy ms mp l mq mr">score_AGclustering_d = davies_bouldin_score(cluster_df, yhat_2)</span><span id="fdb9" class="mm mn iq mi b gy ms mp l mq mr">print('Silhouette Score: %.4f' % score_AGclustering_s)<br/>print('Calinski Harabasz Score: %.4f' % score_AGclustering_c)<br/>print('Davies Bouldin Score: %.4f' % score_AGclustering_d)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/2dd0ccd6ebfff21efd6d3efc6e0bea05.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*RtABDlq5QdXVT7_VLEFVhA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:聚类验证指标:聚集聚类(作者图片)</p></figure><p id="7cd9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">比较图1和图4，我们可以看到，基于所有的聚类验证指标，K-means优于凝聚聚类。</p><h1 id="da3f" class="mv mn iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">基于密度的空间聚类</h1><p id="c9bb" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">DBSCAN将紧密聚集在一起的点组合在一起，同时将其他点标记为孤立在低密度区域中的异常值。定义“密度”需要模型中的两个关键参数:形成密集区域所需的最小点数<code class="fe nu nv nw mi b">min_samples</code>和定义邻域的距离<code class="fe nu nv nw mi b">eps</code>。更高的<code class="fe nu nv nw mi b">min_samples</code>或更低的<code class="fe nu nv nw mi b">eps</code>需要更大的密度来形成集群。</p><p id="94ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于这些参数，DBSCAN从任意点x开始，根据<code class="fe nu nv nw mi b">eps</code>识别x邻域内的点，并将x分类为以下类型之一:</p><ol class=""><li id="ef2d" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated"><strong class="ky ir">核心点</strong>:如果邻域内的点数至少等于<code class="fe nu nv nw mi b">min_samples</code>参数，则称之为核心点，并在x周围形成一个聚类。</li><li id="16e0" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><strong class="ky ir">边界点</strong>:如果x是具有不同核心点的聚类的一部分，但其邻域内的点数小于<code class="fe nu nv nw mi b">min_samples</code>参数，则x被视为边界点。直观上，这些点位于星团的边缘。</li></ol><p id="4ce2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.<strong class="ky ir">异常值或噪声</strong>:如果x不是核心点，且与任何核心样本的距离至少等于或大于<code class="fe nu nv nw mi b">eps</code>，则认为是异常值或噪声。</p><p id="9070" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了调整模型的参数，我们首先通过找到一个点的相邻点之间的距离并绘制最小距离来确定最佳的<code class="fe nu nv nw mi b">eps</code>值。这给了我们找到数据点密度的肘形曲线，并且可以在拐点找到最佳的<code class="fe nu nv nw mi b">eps</code>值。我们使用<code class="fe nu nv nw mi b">NearestNeighbours</code>函数来获得最小距离，使用<code class="fe nu nv nw mi b">KneeLocator</code>函数来识别拐点。</p><pre class="kg kh ki kj gt mh mi mj mk aw ml bi"><span id="cec0" class="mm mn iq mi b gy mo mp l mq mr"># parameter tuning for eps<br/>from sklearn.neighbors import NearestNeighbors<br/>nearest_neighbors = NearestNeighbors(n_neighbors=11)<br/>neighbors = nearest_neighbors.fit(cluster_df)<br/>distances, indices = neighbors.kneighbors(cluster_df)<br/>distances = np.sort(distances[:,10], axis=0)</span><span id="16ca" class="mm mn iq mi b gy ms mp l mq mr">from kneed import KneeLocator<br/>i = np.arange(len(distances))<br/>knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')<br/>fig = plt.figure(figsize=(5, 5))<br/>knee.plot_knee()<br/>plt.xlabel("Points")<br/>plt.ylabel("Distance")</span><span id="c2d3" class="mm mn iq mi b gy ms mp l mq mr">print(distances[knee.knee])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/87c3683219a13e096d77ec36a2425065.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*HH-UpFENydvBTuIleccTxQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:每股收益的最佳值(图片由作者提供)</p></figure><p id="bea3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上所示，<code class="fe nu nv nw mi b">eps</code>的最佳值是1.9335816413107338。我们将该值用于参数向前发展，并尝试根据剪影得分、Calinski Harabasz得分和Davies Bouldin得分找到<code class="fe nu nv nw mi b">min_samples</code>参数的最佳值。对于这些方法中的每一种，最佳聚类数如下:</p><ol class=""><li id="b373" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">剪影得分:18</li><li id="e8bb" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">卡林斯基·哈拉巴斯得分:29</li><li id="6df3" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">戴维斯·波尔丁得分:2</li></ol><p id="9495" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">找到最佳数量<code class="fe nu nv nw mi b">min_samples</code>的代码可以在<a class="ae kv" href="https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git" rel="noopener ugc nofollow" target="_blank">这里找到</a>，关于每种方法的更多细节可以在这个<a class="ae kv" rel="noopener" target="_blank" href="/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad">博客</a>中找到。我们继续采用建议的中间值，即侧影得分为18。如果我们没有时间对这些指标进行网格搜索，一个快速的经验法则是将<code class="fe nu nv nw mi b">min_samples</code>参数设置为特性数量的两倍。</p><pre class="kg kh ki kj gt mh mi mj mk aw ml bi"><span id="5cec" class="mm mn iq mi b gy mo mp l mq mr"># dbscan clustering<br/>from numpy import unique<br/>from numpy import where<br/>from sklearn.cluster import DBSCAN<br/>from matplotlib import pyplot<br/># define dataset<br/># define the model<br/>model = DBSCAN(eps=1.9335816413107338, min_samples= 18)</span><span id="01b3" class="mm mn iq mi b gy ms mp l mq mr"># rule of thumb for min_samples: 2*len(cluster_df.columns)</span><span id="09f7" class="mm mn iq mi b gy ms mp l mq mr"># fit model and predict clusters<br/>yhat = model.fit_predict(cluster_df)<br/># retrieve unique clusters<br/>clusters = unique(yhat)</span><span id="d267" class="mm mn iq mi b gy ms mp l mq mr"># Calculate cluster validation metrics</span><span id="8908" class="mm mn iq mi b gy ms mp l mq mr">score_dbsacn_s = silhouette_score(cluster_df, yhat, metric='euclidean')</span><span id="0a7c" class="mm mn iq mi b gy ms mp l mq mr">score_dbsacn_c = calinski_harabasz_score(cluster_df, yhat)</span><span id="4d59" class="mm mn iq mi b gy ms mp l mq mr">score_dbsacn_d = davies_bouldin_score(cluster_df, yhat)</span><span id="0ce0" class="mm mn iq mi b gy ms mp l mq mr">print('Silhouette Score: %.4f' % score_dbsacn_s)<br/>print('Calinski Harabasz Score: %.4f' % score_dbsacn_c)<br/>print('Davies Bouldin Score: %.4f' % score_dbsacn_d)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/464b40a2b7c65537aa96ce74fc256179.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*5tcxEhpgOVV1JaaLDD8kxQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6:集群验证指标:DBSCAN(作者图片)</p></figure><p id="f24a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">比较图1和图6，我们可以看到DBSCAN在轮廓得分上比K-means表现得更好。该模型在论文中描述为:</p><ul class=""><li id="ddf8" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr nr lz ma mb bi translated"><a class="ae kv" href="https://www.osti.gov/biblio/421283" rel="noopener ugc nofollow" target="_blank">一种基于密度的算法，用于在带有噪声的大型空间数据库中发现聚类</a>，1996。</li></ul><p id="72c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在另一篇博客中，我们将讨论更高级版本的DBSCAN，称为基于层次密度的空间聚类(HDBSCAN)。</p><h1 id="4288" class="mv mn iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">高斯混合模型(GMM)</h1><p id="b035" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">高斯混合模型是一种基于距离的概率模型，它假设所有数据点都是由具有未知参数的多元高斯分布的线性组合生成的。像K-均值一样，它考虑了潜在高斯分布的中心，但与K-均值不同，它还考虑了分布的协方差结构。该算法实现了期望最大化(EM)算法，以迭代方式查找使模型质量度量(称为对数似然)最大化的分布参数。该模型中执行的关键步骤是:</p><ol class=""><li id="4dc7" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">初始化k个高斯分布</li><li id="c05c" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">计算每个点与每个分布关联的概率</li><li id="7c45" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">根据与分布相关的每个点的概率重新计算分布参数</li><li id="0413" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">重复该过程，直到对数似然最大化</li></ol><p id="b256" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在GMM，有4种计算协方差的方法:</p><ol class=""><li id="33e8" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated"><strong class="ky ir">完整:</strong>每个分布都有自己的一般协方差矩阵</li><li id="2392" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><strong class="ky ir">并列:</strong>所有分布共享一般协方差矩阵</li><li id="882f" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><strong class="ky ir"> Diag: </strong>每个分布都有自己的对角协方差矩阵</li><li id="e196" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><strong class="ky ir">球形:</strong>每个分布都有自己的单方差</li></ol><p id="4ab1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了选择协方差类型，我们还需要选择模型中的最佳聚类数。我们使用BIC评分、剪影评分、Calinski Harabasz评分和Davies Bouldin评分来使用网格搜索选择两个参数。对于这些方法中的每一种，最佳聚类数如下:</p><ol class=""><li id="575a" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">BIC分数:协方差-“满”和聚类数- 26</li><li id="246e" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">轮廓得分:协方差-“并列”和聚类数- 2</li><li id="38d4" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">Calinski Harabasz得分:协方差-“球形”和聚类数- 4</li><li id="50dc" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">戴维斯·波尔丁得分:协方差-“满”和聚类数- 8</li></ol><p id="9156" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">寻找最佳参数值的代码可以在<a class="ae kv" href="https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git" rel="noopener ugc nofollow" target="_blank">这里</a>找到，关于每种方法的更多细节可以在这个<a class="ae kv" rel="noopener" target="_blank" href="/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad">博客</a>中找到。我们选择协方差为“完全”,基于BIC评分的聚类数为26，因为它是基于这个特定的模型。如果我们有来自多个指标的相似配置，我们可以取所有指标的平均值/中值/众数。我们现在可以拟合模型并检查模型性能。</p><pre class="kg kh ki kj gt mh mi mj mk aw ml bi"><span id="b84d" class="mm mn iq mi b gy mo mp l mq mr"># gaussian mixture clustering<br/>from numpy import unique<br/>from numpy import where<br/>from sklearn.mixture import GaussianMixture<br/>from matplotlib import pyplot<br/># define the model<br/>model = GaussianMixture(n_components= 26,covariance_type= "full", random_state = 10)<br/># fit the model<br/>model.fit(cluster_df)<br/># assign a cluster to each example<br/>yhat = model.predict(cluster_df)<br/># retrieve unique clusters<br/>clusters = unique(yhat)</span><span id="5d8c" class="mm mn iq mi b gy ms mp l mq mr"># Calculate cluster validation score</span><span id="fb19" class="mm mn iq mi b gy ms mp l mq mr">score_dbsacn_s = silhouette_score(cluster_df, yhat, metric='euclidean')</span><span id="6ecb" class="mm mn iq mi b gy ms mp l mq mr">score_dbsacn_c = calinski_harabasz_score(cluster_df, yhat)</span><span id="9173" class="mm mn iq mi b gy ms mp l mq mr">score_dbsacn_d = davies_bouldin_score(cluster_df, yhat)</span><span id="bd83" class="mm mn iq mi b gy ms mp l mq mr">print('Silhouette Score: %.4f' % score_dbsacn_s)<br/>print('Calinski Harabasz Score: %.4f' % score_dbsacn_c)<br/>print('Davies Bouldin Score: %.4f' % score_dbsacn_d)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/7dcd08d1137260e6237bb6313cc972e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*KP5bMcTXIpP8zr9T553Wcw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7:集群验证指标:GMM(图片由作者提供)</p></figure><p id="005d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">比较图1和图7，我们可以看到K-means在所有集群验证指标上都优于GMM。在另一篇博客中，我们将讨论GMM的更高级版本，称为变分贝叶斯高斯混合。</p><h1 id="324b" class="mv mn iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">结论</h1><p id="8a35" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">这篇博客的目的是帮助读者理解4种流行的聚类模型是如何工作的，以及它们在python中的具体实现。如下所示，每种模式都有其优缺点:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/2773d773b2f3049c0daa1da314dbcd9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*onJwlGtdSW3mJmmuB2bCdw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8:聚类算法的利与弊(图片由作者提供)</p></figure><p id="eff0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，重要的是要理解这些模型只是一种手段，用于找到合理的、易于理解的、可以有效定位的客户/产品细分市场。因此，在大多数实际情况下，我们将最终尝试多种模型，并从每次迭代中创建客户/产品档案，直到我们找到最具商业意义的细分市场。因此，分段既是一门艺术也是一门科学。</p><p id="3073" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你对这个博客有什么问题或建议吗？请随时留言。</p><h1 id="f5e6" class="mv mn iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">感谢您的阅读！</h1><p id="d19f" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">如果你和我一样，对人工智能、数据科学或经济学充满热情，请随时添加/关注我的<a class="ae kv" href="http://www.linkedin.com/in/indraneel-dutta-baruah-ds" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae kv" href="https://github.com/IDB-FOR-DATASCIENCE" rel="noopener ugc nofollow" target="_blank"> Github </a>和<a class="ae kv" href="https://medium.com/@indraneeldb1993ds" rel="noopener"> Medium </a>。</p><h1 id="cb77" class="mv mn iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">参考</h1><ol class=""><li id="d34e" class="lt lu iq ky b kz nm lc nn lf ob lj oc ln od lr ly lz ma mb bi translated">Ester，M .，Kriegel，H . P .，Sander，J .，，Xu .<em class="ls">一种基于密度的算法，用于在带有噪声的大型空间数据库中发现聚类</em>。美国:不扩散条约，1996年。网络。</li><li id="948b" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">多变量观察的分类和分析的一些方法。第五届伯克利数理统计和概率研讨会会议录，第1卷:统计，281–297，加州大学出版社，伯克利，加利福尼亚州，1967年。<a class="ae kv" href="https://projecteuclid.org/euclid.bsmsp/1200512992" rel="noopener ugc nofollow" target="_blank">https://projecteuclid.org/euclid.bsmsp/1200512992</a></li><li id="59c4" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><a class="ae kv" href="http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html" rel="noopener ugc nofollow" target="_blank">sci kit-learn:Python中的机器学习</a>，Pedregosa <em class="ls">等人</em>，JMLR 12，第2825–2830页，2011年。</li></ol></div></div>    
</body>
</html>