<html>
<head>
<title>A Step-by-Step Tutorial for Conducting Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">进行情感分析的分步指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-step-by-step-tutorial-for-conducting-sentiment-analysis-a7190a444366?source=collection_archive---------6-----------------------#2020-10-01">https://towardsdatascience.com/a-step-by-step-tutorial-for-conducting-sentiment-analysis-a7190a444366?source=collection_archive---------6-----------------------#2020-10-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/971b74393b6d8e226746de79a7c05d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XkH8b5_bvDIFr0yPkis_6Q.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">科塞拉·博尔塔在<a class="ae jg" href="https://unsplash.com/s/photos/feeling?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="2400" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">第1部分:预处理文本数据</h2></div><p id="fe6c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" href="https://www.ibm.com/blogs/watson/2016/05/biggest-data-challenges-might-not-even-know/" rel="noopener ugc nofollow" target="_blank">据估计，全球80%的数据是非结构化的。</a>因此，从非结构化数据中获取信息是数据分析的重要组成部分。文本挖掘是从非结构化的文本数据中获取有价值见解的过程，情感分析是文本挖掘的一个应用。它使用自然语言处理和机器学习技术从文本数据中理解和分类主观情绪。在商业环境中，情感分析广泛用于理解客户评论、检测电子邮件中的垃圾邮件等。本文是教程的第一部分，介绍了使用Python进行情感分析的具体技术。为了更好地说明这个过程，我将使用我的一个项目作为例子，在那里我对WTI原油期货价格进行新闻情绪分析。我将展示重要的步骤以及相应的Python代码。</p><p id="c8ff" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">一些背景信息</strong></p><p id="a867" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">原油期货价格短期波动较大。虽然任何产品的长期均衡都是由供求状况决定的，但价格的短期波动反映了市场对该产品的信心和预期。在这个项目中，我使用原油相关的新闻文章来捕捉不断更新的市场信心和预期，并通过对新闻文章进行情绪分析来预测原油未来价格的变化。以下是完成此分析的步骤:</p><blockquote class="lu"><p id="d820" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">1、<a class="ae jg" href="https://zzhu17.medium.com/web-scraping-yahoo-finance-news-a18f9b20ee8a" rel="noopener">收集数据:网络抓取新闻文章</a></p><p id="c276" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">2、预处理文本数据(本文)</p><p id="0e86" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">3、<a class="ae jg" href="https://medium.com/@zzhu17/a-step-by-step-tutorial-for-conducting-sentiment-analysis-9d1a054818b6" rel="noopener">文本矢量化:TFIDF </a></p><p id="b6e7" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">4、<a class="ae jg" href="https://medium.com/@zzhu17/a-step-by-step-tutorial-for-conducting-sentiment-analysis-cf3e995e3171" rel="noopener">使用逻辑回归的情感分析</a></p><p id="29dc" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">5、使用python flask web app在Heroku部署模型</p></blockquote><p id="96a0" class="pw-post-body-paragraph ky kz jj la b lb me kk ld le mf kn lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">我将讨论第二部分，即本文中的文本数据预处理。如果您对其他部分感兴趣，请点击链接阅读更多内容(即将推出)。</p><p id="f97a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">预处理文本数据</strong></p><p id="2849" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我使用NLTK、Spacy和一些正则表达式中的工具对新闻文章进行预处理。要导入库并在Spacy中使用预构建的模型，您可以使用以下代码:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="6ca5" class="ms mt jj mo b gy mu mv l mw mx">import spacy<br/>import nltk</span><span id="c0d7" class="ms mt jj mo b gy my mv l mw mx"># Initialize spacy ‘en’ model, keeping only component needed for lemmatization and creating an engine:</span><span id="9412" class="ms mt jj mo b gy my mv l mw mx">nlp = spacy.load(‘en’, disable=[‘parser’, ‘ner’])</span></pre><p id="6fcc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">之后，我用熊猫来读取数据:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/73e2e027f2109f57b1b0da1b058a83f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hS6K0DNCeS4frNTb0YOnFw.png"/></div></div></figure><p id="399f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">“Subject”和“Body”是我将对其应用文本预处理过程的列。我按照标准的文本挖掘程序对新闻文章进行预处理，从新闻内容中提取有用的特征，包括标记化、去除停用词和词条化。</p><p id="13c7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="na">标记化</em> </strong></p><p id="86c2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">预处理文本数据的第一步是将每一个句子分解成单个的单词，这就是所谓的标记化。采用单个单词而不是句子会破坏单词之间的联系。但是，这是一种用于分析大量文本数据的常用方法。通过检查什么词在文章中出现以及这些词出现了多少次，计算机分析文本数据是高效和方便的，并且足以给出有见地的结果。</p><p id="160c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以我的数据集中的第一篇新闻文章为例:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/68bae8fe95d3145d2adee5e9825f1f68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qaSwtNfKuQIH3OWL7isJOw.png"/></div></div></figure><p id="9507" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以使用NLTK标记器:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/f6c6c1492509a92ae4dd2e510d9b8756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C6mLVqIEjE49jaLMFi5N9Q.png"/></div></div></figure><p id="ae67" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">或者您可以使用Spacy，记住NLP是上面定义的Spacy引擎:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nd"><img src="../Images/28126762297e9e63476d849689d9521d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bRlvb4BObpfTfNl5F-_lnQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">需要将每个令牌改为字符串变量</p></figure><p id="c3da" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在标记化之后，每篇新闻都将转化为一系列单词、符号、数字和标点符号。您可以指定是否要将每个单词都转换成小写。下一步是删除无用的信息。例如，符号、数字、标点符号。我将使用spacy结合正则表达式来删除它们。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="bcd3" class="ms mt jj mo b gy mu mv l mw mx">import re</span><span id="93b0" class="ms mt jj mo b gy my mv l mw mx">#tokenization and remove punctuations<br/>words = [str(token) for token in nlp(text) if not token.is_punct] </span><span id="42e3" class="ms mt jj mo b gy my mv l mw mx">#remove digits and other symbols except "@"--used to remove email<br/>words = [re.sub(r"[^A-Za-z@]", "", word) for word in words]</span><span id="8138" class="ms mt jj mo b gy my mv l mw mx">#remove websites and email address<br/>words = [re.sub(r”\S+com”, “”, word) for word in words]<br/>words = [re.sub(r”\S+@\S+”, “”, word) for word in words]</span><span id="8600" class="ms mt jj mo b gy my mv l mw mx">#remove empty spaces <br/>words = [word for word in words if word!=’ ‘]</span></pre><p id="c74c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">应用上述转换后，原始新闻文章看起来是这样的:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ne"><img src="../Images/90e823b7d3d485781d0c5d1a05fcc217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mq-IflGzgMoN4lDlOfuvLg.png"/></div></div></figure><p id="6f55" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="na">停用词</em> </strong></p><p id="a010" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">经过一些改造后，新闻文章更干净了，但我们仍然会看到一些我们不希望看到的词，例如，“和”，“我们”等。下一步是删除无用的词，即停用词。停用词是在许多文章中频繁出现但没有重要意义的词。停用词的例子有' I '，' the '，' a '，' of '。这些词如果被删除，将不会影响对文章的理解。要删除停用词，我们可以从NLTK库中导入停用词。除此之外，我还包括<a class="ae jg" href="https://sraf.nd.edu/textual-analysis/resources/StopWords." rel="noopener ugc nofollow" target="_blank">其他在经济分析中广泛使用的停用词列表</a>，包括日期和时间，更一般的没有经济意义的词等。我是这样构造停用词列表的:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="ddbd" class="ms mt jj mo b gy mu mv l mw mx">#import other lists of stopwords<br/>with open(‘StopWords_GenericLong.txt’, ‘r’) as f:<br/> x_gl = f.readlines()<br/>with open(‘StopWords_Names.txt’, ‘r’) as f:<br/> x_n = f.readlines()<br/>with open(‘StopWords_DatesandNumbers.txt’, ‘r’) as f:<br/> x_d = f.readlines()</span><span id="249a" class="ms mt jj mo b gy my mv l mw mx">#import nltk stopwords<br/>stopwords = nltk.corpus.stopwords.words(‘english’)</span><span id="ab76" class="ms mt jj mo b gy my mv l mw mx">#combine all stopwords<br/>[stopwords.append(x.rstrip()) for x in x_gl][stopwords.append(x.rstrip()) for x in x_n][stopwords.append(x.rstrip()) for x in x_d]</span><span id="4fc1" class="ms mt jj mo b gy my mv l mw mx">#change all stopwords into lowercase<br/>stopwords_lower = [s.lower() for s in stopwords]</span></pre><p id="99b3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后从新闻文章中排除停用词:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="c226" class="ms mt jj mo b gy mu mv l mw mx">words = [word.lower() for word in words if word.lower() not in stopwords_lower]</span></pre><p id="8d8d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">应用到前面的例子，它看起来是这样的:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/40ef1755737ab9eff5c6f8f3a8419f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ylp2etWizSE0n_ydKQ2zqg.png"/></div></div></figure><p id="8757" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="na">词汇化</em> </strong></p><p id="0149" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除去停用词、符号、数字和标点符号，每篇新闻文章都将转化为一系列有意义的单词。然而，要统计每个单词的出现次数，就必须去除语法时态，将每个单词转换为其原始形式。例如，如果我们想计算单词“open”在一篇新闻文章中出现了多少次，我们需要计算“open”、“opens”、“opened”的出现次数。因此，词汇化是文本转换的一个重要步骤。另一种将单词转换成原始形式的方法叫做词干提取。它们之间的区别如下:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ng"><img src="../Images/fe5563721c20147349453df62855cf69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-KP6ZxVtMjE10YTZrC4gKA.png"/></div></div></figure><p id="5ebc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">词干化是将一个单词提取到它的原始词干中，词干化是提取一个单词的语言学词根。我选择词汇化而不是词干化，因为词干化之后，一些单词变得难以理解。为了解释的目的，引理比语言根更好。</p><p id="db09" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如上所示，用Spacy实现引理化非常容易，这里我调用。引理_函数从空间开始。在词汇化之后，每篇新闻文章都将转化为一个单词列表，这些单词都是它们的原始形式。新闻报道现在变成了这样:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nh"><img src="../Images/a8886d6bf6e4f12088ca773fffb18ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQxgwjv-UjQlsGaaKFNbLw.png"/></div></div></figure><p id="5fd9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">总结步骤</strong></p><p id="9e57" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们总结一个函数中的步骤，并在所有文章中应用该函数:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="c9db" class="ms mt jj mo b gy mu mv l mw mx">def text_preprocessing(str_input): </span><span id="375c" class="ms mt jj mo b gy my mv l mw mx">     #tokenization, remove punctuation, lemmatization<br/>     words=[token.lemma_ for token in nlp(str_input) if not         token.is_punct]<br/> <br/>     # remove symbols, websites, email addresses <br/>     words = [re.sub(r”[^A-Za-z@]”, “”, word) for word in words] <br/>     words = [re.sub(r”\S+com”, “”, word) for word in words]<br/>     words = [re.sub(r”\S+@\S+”, “”, word) for word in words] <br/>     words = [word for word in words if word!=’ ‘]<br/>     words = [word for word in words if len(word)!=0] <br/> <br/>     #remove stopwords     <br/>     words=[word.lower() for word in words if word.lower() not in     stopwords_lower]</span><span id="f02f" class="ms mt jj mo b gy my mv l mw mx">     #combine a list into one string   <br/>     string = “ “.join(words)</span><span id="af46" class="ms mt jj mo b gy my mv l mw mx">     return string</span></pre><p id="3059" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的函数text_preprocessing()结合了所有的文本预处理步骤，这里输出的是第一篇新闻文章:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/fb603f19907843a7161c1d7efacd1b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aBWvrniDvXwEwbxa88h_Eg.png"/></div></div></figure><p id="5764" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在推广到所有新闻文章之前，按照下面的代码，将它应用到随机的新闻文章中并看看它是如何工作的是很重要的:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="4650" class="ms mt jj mo b gy mu mv l mw mx">import random</span><span id="5bc6" class="ms mt jj mo b gy my mv l mw mx">index = random.randint(0, df.shape[0])<br/>text_preprocessing(df.iloc[index][‘Body’])</span></pre><p id="f89d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果有一些额外的词你想排除这个特定的项目或一些额外的多余信息你想删除，你可以随时修改该功能之前，适用于所有的新闻文章。这是一篇在标记化前后随机选择的新闻文章，去掉了停用词和词条。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/2a3eb5d14a8d734f4552ebac63f3b242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*faYdmu-YLssMR3qnE8HGrQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">预处理前的新闻文章</p></figure><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/0059e043fc1684f3050ed02dc8dd754e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LBkAxz41gZS-8WsHy8IYCA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">预处理后的新闻文章</p></figure><p id="8eab" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果一切正常，您可以将该功能应用于所有新闻文章:</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="4957" class="ms mt jj mo b gy mu mv l mw mx">df[‘news_cleaned’]=df[‘Body’].apply(text_preprocessing)<br/>df[‘subject_cleaned’]=df[‘Subject’].apply(text_preprocessing)</span></pre><p id="2d2a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">一些言论</strong></p><p id="ad0d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">文本预处理是文本挖掘和情感分析中非常重要的一部分。有很多方法可以对非结构化数据进行预处理，使其对计算机可读，以便将来进行分析。下一步，我将讨论我用来将文本数据转换成稀疏矩阵的矢量器，以便它们可以用作定量分析的输入。</p><p id="3f1d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您的分析很简单，并且在预处理文本数据时不需要太多的定制，那么矢量器通常有嵌入式函数来执行基本步骤，比如标记化、删除停用词。或者，您可以编写自己的函数并在矢量器中指定自定义函数，这样您就可以同时对数据进行预处理和矢量化。如果您希望这样，您的函数需要返回一个标记化单词的列表，而不是一个长字符串。不过个人来说，我更倾向于先对文本数据进行预处理，再进行矢量化。通过这种方式，我不断地监控我的函数的性能，实际上这样更快，尤其是当你有一个大的数据集的时候。</p><p id="c4af" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将在我的<a class="ae jg" href="https://medium.com/@zzhu17/a-step-by-step-tutorial-for-conducting-sentiment-analysis-9d1a054818b6" rel="noopener">下一篇文章</a>中讨论转换过程。感谢您的阅读！这是我所有博客帖子的列表。如果你感兴趣的话，可以去看看！</p><div class="is it gp gr iu nl"><a href="https://zzhu17.medium.com/my-blog-posts-gallery-ac6e01fe5cc3" rel="noopener follow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd jk gy z fp nq fr fs nr fu fw ji bi translated">我的博客文章库</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">我快乐的地方</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">zzhu17.medium.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz ja nl"/></div></div></a></div><div class="is it gp gr iu nl"><a href="https://zzhu17.medium.com/membership" rel="noopener follow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd jk gy z fp nq fr fs nr fu fw ji bi translated">阅读朱(以及媒体上成千上万的其他作家)的每一个故事</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">zzhu17.medium.com</p></div></div><div class="nu l"><div class="oa l nw nx ny nu nz ja nl"/></div></div></a></div></div></div>    
</body>
</html>