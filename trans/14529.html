<html>
<head>
<title>K-Nearest Neighbours (kNN) Algorithm: Common Questions and Python Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k近邻(kNN)算法:常见问题和Python实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-nearest-neighbours-knn-algorithm-common-questions-and-python-implementation-14377e45b738?source=collection_archive---------14-----------------------#2020-10-07">https://towardsdatascience.com/k-nearest-neighbours-knn-algorithm-common-questions-and-python-implementation-14377e45b738?source=collection_archive---------14-----------------------#2020-10-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/6e168a483a69c708ea6d329d6014e09b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mkOw8cgteG-Fulwt0cWXVw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">由<a class="ae jd" href="https://unsplash.com/@ninastrehl" rel="noopener ugc nofollow" target="_blank">妮娜·斯特雷尔</a>在<a class="ae jd" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">上Unsplash </a></p></figure><div class=""/><div class=""><h2 id="2c79" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">测试数据科学家关于kNN算法及其Python实现的问题</h2></div><p id="c9bb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">k近邻被认为是最直观的机器学习算法之一，因为它易于理解和解释。此外，直观地演示一切是如何进行的也很方便。然而，kNN算法仍然是一种常见的非常有用的算法，用于各种分类问题。如果你是机器学习的新手，请确保测试自己对这个简单而又精彩的算法的理解。关于它是什么以及它是如何工作的，有很多有用的资料来源，因此我想以我个人的观点来看一下你应该知道的5个常见或有趣的问题。</p><blockquote class="lr ls lt"><p id="78e0" class="kv kw lu kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated"><strong class="kx jh">k-NN算法在测试时间而不是训练时间上做更多的计算。</strong></p></blockquote><p id="36db" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那绝对是真的。kNN算法的思想是找到一个k长的样本列表，该列表接近我们想要分类的样本。因此，训练阶段基本上是存储训练集，而在预测阶段，算法使用存储的数据寻找k个邻居。</p><blockquote class="lr ls lt"><p id="a108" class="kv kw lu kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated"><strong class="kx jh">为什么需要为k-NN算法缩放数据？</strong></p></blockquote><p id="7a90" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设一个数据集有<em class="lu"> m </em>个“示例”和<em class="lu"> n </em>个“特征”。有一个特征维度的值正好在0和1之间。同时，还有一个从-99999到99999不等的特征维度。考虑到<em class="lu">欧几里德距离的公式，</em>这将通过给予具有较高幅度的变量较高的权重来影响性能。</p><p id="a35d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">阅读更多:</strong><a class="ae jd" href="https://medium.com/analytics-vidhya/why-is-scaling-required-in-knn-and-k-means-8129e4d88ed7" rel="noopener"><strong class="kx jh">KNN和K-Means为什么需要缩放？</strong>T19】</a></p><blockquote class="lr ls lt"><p id="1cff" class="kv kw lu kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated"><strong class="kx jh">k-NN算法可用于输入分类变量和连续变量的缺失值。</strong></p></blockquote><p id="78cc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是真的。在处理缺失值时，k-NN可以用作许多技术中的一种。通过确定训练集中“最接近”的样本来估算新样本，并对这些要估算的邻近点进行平均。scikit学习库提供了一种使用这种技术快捷方便的方法。</p><p id="892c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注:在计算距离时，NaNs被省略。</p><p id="6b8d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">示例:</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="16e4" class="mh mi jg md b gy mj mk l ml mm">from sklearn.impute import KNNImputer<br/># define imputer<br/>imputer = KNNImputer() #default k is 5=&gt; <em class="lu">n_neighbors=5</em><br/># fit on the dataset<br/>imputer.fit(X)<br/># transform the dataset<br/>Xtrans = imputer.transform(X)</span></pre><p id="58c6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，缺失值将被其“邻居”的平均值所替代。</p><blockquote class="lr ls lt"><p id="b0f3" class="kv kw lu kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated"><strong class="kx jh"> <em class="jg">欧氏距离总是这样吗？</em>T25】</strong></p></blockquote><p id="f3b9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然欧几里德距离是最常用和教授的方法，但它并不总是最佳决策。事实上，仅仅通过查看数据很难得出正确的指标，所以我建议尝试一组数据。但是，也有一些特殊情况。例如，汉明距离用于分类变量的情况。</p><p id="c229" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">阅读更多:</strong> <a class="ae jd" rel="noopener" target="_blank" href="/3-text-distances-that-every-data-scientist-should-know-7fcdf850e510"> <strong class="kx jh">每个数据科学家都应该知道的3个文本距离</strong> </a></p><blockquote class="lr ls lt"><p id="2363" class="kv kw lu kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated"><strong class="kx jh">为什么我们不应该对大型数据集使用KNN算法？</strong></p></blockquote><p id="63bf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是KNN算法中出现的数据流的概述:</p><ol class=""><li id="288a" class="mn mo jg kx b ky kz lb lc le mp li mq lm mr lq ms mt mu mv bi translated">计算到训练集中所有向量的距离并存储它们</li><li id="e6f7" class="mn mo jg kx b ky mw lb mx le my li mz lm na lq ms mt mu mv bi translated">对计算的距离进行排序</li><li id="b393" class="mn mo jg kx b ky mw lb mx le my li mz lm na lq ms mt mu mv bi translated">存储K个最近的向量</li><li id="d578" class="mn mo jg kx b ky mw lb mx le my li mz lm na lq ms mt mu mv bi translated">计算K个最近向量显示的最频繁类别</li></ol><p id="fa31" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设你有一个非常大的数据集。因此，存储大量数据不仅是一个糟糕的决定，而且不断计算和排序所有值的计算成本也很高。</p><h1 id="e5cd" class="nb mi jg bd nc nd ne nf ng nh ni nj nk km nl kn nm kp nn kq no ks np kt nq nr bi translated">Python实现</h1><p id="d235" class="pw-post-body-paragraph kv kw jg kx b ky ns kh la lb nt kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">我将把实施分为以下几个阶段:</p><ol class=""><li id="dca1" class="mn mo jg kx b ky kz lb lc le mp li mq lm mr lq ms mt mu mv bi translated">计算到训练集中所有向量的距离并存储它们</li><li id="af66" class="mn mo jg kx b ky mw lb mx le my li mz lm na lq ms mt mu mv bi translated">对计算的距离进行排序</li><li id="0687" class="mn mo jg kx b ky mw lb mx le my li mz lm na lq ms mt mu mv bi translated">计算K个最近向量显示的最频繁类别，并进行预测</li></ol><blockquote class="lr ls lt"><p id="1472" class="kv kw lu kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated"><strong class="kx jh">计算训练集中所有向量的距离并存储它们</strong></p></blockquote><p id="9bfc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">值得注意的是，有大量不同的选项可供选择作为衡量标准；但是，我想用欧几里德距离作为例子。这是计算向量间距离最常用的度量，因为它简单明了，易于解释。一般公式如下:</p><blockquote class="nx"><p id="633a" class="ny nz jg bd oa ob oc od oe of og lq dk translated">欧几里德距离= sqrt(sum I to N(x1 _ I-x2 _ I))</p></blockquote><p id="c13a" class="pw-post-body-paragraph kv kw jg kx b ky oh kh la lb oi kk ld le oj lg lh li ok lk ll lm ol lo lp lq ij bi translated">因此，让我们用下面的Python代码来总结一下。<strong class="kx jh"> <em class="lu">注意:不要忘记从“数学”模块导入sqrt()。</em>T15】</strong></p><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="om on l"/></div></figure><blockquote class="lr ls lt"><p id="0541" class="kv kw lu kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated"><strong class="kx jh">对计算出的距离进行排序</strong></p></blockquote><p id="9023" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，我们需要计算单个<em class="lu">测试样本</em>和我们训练集中所有样本之间的所有距离。获得距离后，我们应该对距离列表进行排序，并通过查看它们与测试样本的距离，从我们的训练集中挑选“最近”的k个向量。</p><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="om on l"/></div></figure><blockquote class="lr ls lt"><p id="23b9" class="kv kw lu kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated">C <strong class="kx jh"> <em class="jg">计算K个最近向量显示的最频繁类别并进行预测</em> </strong></p></blockquote><p id="330c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，为了做一个预测，我们应该通过调用我在上面附上的函数来得到我们的k个“最近的”邻居。因此，剩下的唯一事情就是计算每个标签出现的次数，并选择最频繁的一个。</p><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="657a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们通过将所有的函数组合成一个单独的类对象来总结一切。这里有一个更一般化的代码版本，请花些时间浏览一下。</p><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="om on l"/></div></figure><blockquote class="lr ls lt"><p id="f95c" class="kv kw lu kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated"><strong class="kx jh">比较</strong></p></blockquote><p id="d21b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们将我们的实现与scikit learn提供的实现进行比较。我将使用一个简单的玩具数据集，它包含两个预测值，即<em class="lu">年龄</em>和<em class="lu">薪水</em>。因此，我们希望预测客户是否愿意购买我们的产品。</p><figure class="ly lz ma mb gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d82926573f8a804e6bfa44d7d2897aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*lGVsZQikPUdtgTj14CozFw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">显示前5个条目的表格</p></figure><p id="a370" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我将跳过预处理，因为这不是我想关注的；然而，我使用了一种训练测试分割技术，并在之后应用了一个标准缩放器。不管怎样，如果你感兴趣，我会在Github   上提供源代码。</p><p id="9f55" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我将定义两个模型并拟合我们的数据。请参考上面提供的KNN实现。我选择5作为我们的默认k值。注意，对于后一种模型，默认度量是<em class="lu">闵可夫斯基</em>，并且p=2相当于标准欧几里德度量。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="398e" class="mh mi jg md b gy mj mk l ml mm">model=KNN(5) <em class="lu">#our model</em> <br/>model.fit(X_train,y_train)<br/></span><span id="7ec4" class="mh mi jg md b gy op mk l ml mm">predictions=model.predict(X_test)<em class="lu">#our model's predictions</em></span><span id="6ac4" class="mh mi jg md b gy op mk l ml mm"><strong class="md jh">from</strong> <strong class="md jh">sklearn.neighbors</strong> <strong class="md jh">import</strong> KNeighborsClassifier<br/>classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)<em class="lu">#The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric.</em><br/>classifier.fit(X_train, y_train)<br/><br/>y_pred = classifier.predict(X_test)</span></pre><blockquote class="lr ls lt"><p id="234e" class="kv kw lu kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated"><strong class="kx jh">结果</strong></p></blockquote><figure class="ly lz ma mb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oq"><img src="../Images/9a231597960a42ded116114a93b59dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IweGrjYKv44-k9AYcPD2ZQ.png"/></div></div></figure><p id="50dd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上图显示，两种型号表现出相同的性能。精度结果是0.93，这是一个相当好的结果。下图是我们测试集结果的可视化。我提供一个单一的数字，因为两个模型是相同的。然而，我个人建议使用已经提供的实现，因为我们的实现简单而低效。此外，不要每次都写完全相同的代码会更方便。</p><figure class="ly lz ma mb gt is gh gi paragraph-image"><div class="gh gi or"><img src="../Images/bc26accd398968b78091d40f1e4e6743.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*QGip6CNUg-IeWGwvJ_SkrQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">我们测试集结果的可视化</p></figure><h1 id="ab4f" class="nb mi jg bd nc nd ne nf ng nh ni nj nk km nl kn nm kp nn kq no ks np kt nq nr bi translated">摘要</h1><p id="a9e0" class="pw-post-body-paragraph kv kw jg kx b ky ns kh la lb nt kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">总之，K-最近邻被认为是最直观的机器学习算法之一，因为它易于理解和解释。此外，直观地演示一切是如何进行的也很方便。在本文中，我们回答了以下问题:</p><ul class=""><li id="bf5b" class="mn mo jg kx b ky kz lb lc le mp li mq lm mr lq os mt mu mv bi translated">k-NN算法在测试时间而不是训练时间上做更多的计算。</li><li id="7bc1" class="mn mo jg kx b ky mw lb mx le my li mz lm na lq os mt mu mv bi translated">为什么需要为k-NN算法缩放数据？</li><li id="9404" class="mn mo jg kx b ky mw lb mx le my li mz lm na lq os mt mu mv bi translated">k-NN算法可用于输入分类变量和连续变量的缺失值。</li><li id="729f" class="mn mo jg kx b ky mw lb mx le my li mz lm na lq os mt mu mv bi translated">欧氏距离总是这样吗？</li><li id="9262" class="mn mo jg kx b ky mw lb mx le my li mz lm na lq os mt mu mv bi translated">为什么我们不应该对大型数据集使用KNN算法？</li></ul><p id="1378" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此外，我提供了KNN算法的python实现，以便加深您对内部发生的事情的理解。完整的实现可以在我的<a class="ae jd" href="https://github.com/chingisooinar/KNN-python-implementation" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。</p><p id="6674" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">有用的资源</strong></p><div class="ip iq gp gr ir ot"><a href="https://medium.com/analytics-vidhya/why-is-scaling-required-in-knn-and-k-means-8129e4d88ed7" rel="noopener follow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd jh gy z fp oy fr fs oz fu fw jf bi translated">为什么KNN和K-Means需要缩放？</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">KNN和K-Means是最常用和最广泛使用的机器学习算法之一。KNN是监督学习…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">medium.com</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph ix ot"/></div></div></a></div><div class="ip iq gp gr ir ot"><a rel="noopener follow" target="_blank" href="/3-text-distances-that-every-data-scientist-should-know-7fcdf850e510"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd jh gy z fp oy fr fs oz fu fw jf bi translated">每个数据科学家都应该知道的3种文本距离</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">无论您是刚刚开始接触数据科学，还是已经在该领域工作了很长时间，您都需要了解这三个文本…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">towardsdatascience.com</p></div></div><div class="pc l"><div class="pi l pe pf pg pc ph ix ot"/></div></div></a></div><div class="ip iq gp gr ir ot"><a rel="noopener follow" target="_blank" href="/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd jh gy z fp oy fr fs oz fu fw jf bi translated">基于K-最近邻算法的机器学习基础</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">k-最近邻(KNN)算法是一个简单，易于实现的监督机器学习算法，可以…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">towardsdatascience.com</p></div></div><div class="pc l"><div class="pj l pe pf pg pc ph ix ot"/></div></div></a></div></div></div>    
</body>
</html>