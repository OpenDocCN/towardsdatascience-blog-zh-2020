<html>
<head>
<title>16 Tips about Machine Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于机器学习算法的16个技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/16-tips-about-machine-learning-algorithms-7c5eaaf4bf45?source=collection_archive---------47-----------------------#2020-11-15">https://towardsdatascience.com/16-tips-about-machine-learning-algorithms-7c5eaaf4bf45?source=collection_archive---------47-----------------------#2020-11-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2b13" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">加深您对常见机器学习算法的了解</h2></div><p id="6834" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能听说过瑞士军刀。如果没有，就看看下图。它包含许多刀片和工具。每个人都专门从事一项特定的任务。在某些情况下，不同的刀片可以执行相同的任务，但性能程度不同。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/628654c15b5f1d7a09e893d6a92d0cde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zw3kRvoSWoBtbL5t4mNlGA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图片来自<a class="ae lu" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2186" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="258f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我认为机器学习算法是瑞士军刀。有许多不同的算法。某些任务需要使用特定的算法，而有些任务可以用许多不同的算法来完成。根据任务和数据的特征，性能可能会有所变化。</p><p id="54fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我将分享16个技巧，我认为它们会帮助你更好地理解算法。我的目标不是详细解释算法是如何工作的。我宁愿给出一些提示或细节。</p><p id="b16c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一些技巧会更通用，而不是集中在某个特定的算法上。例如，日志损失是与所有分类算法相关的成本函数。</p><p id="af02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我假设你对算法有基本的了解。即使你不知道，你也可以挑选一些细节来帮助你。</p><p id="2c85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们开始吧。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="ec12" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">1.支持向量机的c参数(SVM)</h2><p id="fb0a" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">SVM的c参数为每个错误分类的数据点增加了一个惩罚。如果c很小，则对误分类点的惩罚也很低，因此以更大数量的误分类为代价选择了具有大余量的决策边界。</p><p id="3380" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果c很大，SVM试图最小化由于高惩罚导致的错误分类的例子的数量，这导致了具有较小裕度的决策边界。对于所有错误分类的例子，惩罚是不同的。它与到决策边界的距离成正比。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="2b4e" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">2.基于RBF核的SVM伽玛参数</h2><p id="063b" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">RBF核SVM的Gamma参数控制单个训练点的影响距离。低gamma值表示较大的相似性半径，这将导致更多的点被组合在一起。</p><p id="75bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于高gamma值，这些点需要彼此非常接近，才能被视为在同一组(或类)中。因此，gamma值非常大的模型往往会过度拟合。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="17b1" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">3.什么使逻辑回归成为线性模型</h2><p id="2f4c" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">逻辑回归的基础是逻辑函数，也称为sigmoid函数，它接受任何实数值并将其映射到0到1之间的值。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a2a12153a14dd920155a61b045fa0070.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/0*Piz8xt2rAYn702xH.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">(图片由作者提供)</p></figure><p id="332a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它是一个非线性函数，但逻辑回归是一个线性模型。</p><p id="b56f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是我们如何从sigmoid函数得到一个线性方程:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nb"><img src="../Images/a96dc15960221e139a95da7aea67a846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VMW0ZlCroj3AxSFc.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">(图片由作者提供)</p></figure><p id="9b02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">取两侧的自然对数:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/d9dff99ca7c50a0fce80319f3d707085.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/0*4S7YQuq1uJ2wz4xO.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">(图片由作者提供)</p></figure><p id="df89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在等式(1)中，代替x，我们可以使用线性等式<strong class="kk iu"> z </strong>:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/d6ceb9b764ee684abac06ca5f59796f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/0*tQ-iU0cFlATnRu-I.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">(图片由作者提供)</p></figure><p id="6d47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么等式(1)变成:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f301c76469ee5bbea5a2a8476dbc0f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/0*8PmZS9qbf2wj9qas.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">(图片由作者提供)</p></figure><p id="6c27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设y是正类的概率。如果它是0.5，那么上面等式的右边变成0。</p><p id="13ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在有一个线性方程要解。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="ca10" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">4.主成分分析中的主要成分</h2><p id="4d54" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">PCA(主成分分析)是一种线性降维算法。PCA的目标是在减少数据集的维数(特征数量)的同时保留尽可能多的信息。</p><p id="f926" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">信息量是用方差来衡量的。具有高方差的特征告诉我们关于数据的更多信息。</p><p id="2088" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主成分是原始数据集特征的线性组合。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="eef6" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">5.随机森林</h2><p id="7ad9" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">随机森林是使用一种叫做<strong class="kk iu">装袋</strong>的方法建立的，其中每个决策树都被用作并行估计器。</p><p id="ebb1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机森林的成功高度依赖于使用不相关的决策树。如果我们使用相同或非常相似的树，总体结果将不会比单个决策树的结果有太大的不同。随机森林通过<strong class="kk iu">自举</strong>和<strong class="kk iu">特征随机性</strong>实现不相关的决策树。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nf"><img src="../Images/34f1a3baed001c19b718a1a1ef0aab27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dG9cyp780MOtjeVg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="d5c2" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">6.梯度推进决策树(GBDT)</h2><p id="b4f9" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">GBDT使用<strong class="kk iu">提升</strong>方法来组合个体决策树。Boosting是指将一个学习算法串联起来，从许多顺序连接的弱学习器中实现一个强学习器。</p><p id="0c68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每棵树都符合前一棵树的残差。与装袋不同，增压不涉及自举取样。每次添加新树时，它都适合初始数据集的修改版本。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ng"><img src="../Images/e2d3c1b1cc49d07e87a11a327e4006d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Gfj2np3begSDmzer.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="2705" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">7.增加随机森林和GBDT的树木数量</h2><p id="3cec" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">增加随机森林中的树木数量不会导致过度拟合。在某个点之后，模型的准确性不会因为添加更多的树而增加，但是也不会因为添加过多的树而受到负面影响。由于计算原因，您仍然不希望添加不必要数量的树，但是没有与随机森林中的树的数量相关联的过度拟合的风险。</p><p id="870f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，梯度提升决策树中的树的数量在过度拟合方面非常关键。添加太多的树会导致过度拟合，所以在某个时候停止添加树是很重要的。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="7c2f" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">8.层次聚类与K-均值聚类</h2><p id="6d57" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">分层聚类不需要预先指定聚类的数量。必须为k-means算法指定聚类数。</p><p id="5f7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它总是生成相同的聚类，而k-means聚类可能会根据质心(聚类中心)的初始化方式产生不同的聚类。</p><p id="d616" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与k-means相比，层次聚类是一种较慢的算法。运行时间很长，尤其是对于大型数据集。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="cb8a" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">9.DBSCAN算法的两个关键参数</h2><p id="5d0d" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">DBSCAN是一种聚类算法，适用于任意形状的聚类。这也是一种有效的检测异常值的算法。</p><p id="121b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DBSCAN的两个关键参数:</p><ul class=""><li id="6084" class="nh ni it kk b kl km ko kp kr nj kv nk kz nl ld nm nn no np bi translated"><strong class="kk iu"> eps </strong>:指定邻域的距离。如果两点之间的距离小于或等于eps，则认为这两点是相邻的。</li><li id="11ba" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><strong class="kk iu"> minPts: </strong>定义一个聚类的最小个数据点。</li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="7f93" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">10.DBSCAN算法中的三种不同类型的点</h2><p id="defa" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">基于eps和minPts参数，点被分类为核心点、边界点或异常点:</p><ul class=""><li id="e9bb" class="nh ni it kk b kl km ko kp kr nj kv nk kz nl ld nm nn no np bi translated"><strong class="kk iu">核心点:</strong>如果一个点在其半径为eps的周边区域内至少有minPts个数的点(包括该点本身)，则该点是核心点。</li><li id="aa5e" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><strong class="kk iu">边界点:</strong>如果一个点可以从一个核心点到达，并且其周围区域内的点数少于minPts，那么这个点就是边界点。</li><li id="6c4e" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><strong class="kk iu">离群点:</strong>如果一个点不是核心点并且从任何核心点都不可达，那么这个点就是离群点。</li></ul><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c091e008e7d5a3148abb6496e7b6411e.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/0*BsJQVGR7glXySBxO.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><a class="ae lu" href="https://en.wikipedia.org/wiki/DBSCAN" rel="noopener ugc nofollow" target="_blank">图源</a></p></figure><p id="b288" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，minPts是4。红色点是核心点，因为在其半径为eps的周围区域内至少有<strong class="kk iu">4个点。该区域在图中用圆圈表示。黄色点是边界点，因为它们可以从核心点到达，并且其邻域内的点少于4个。可到达意味着在核心点的周围区域。点B和C在其邻域(即半径为eps的周围区域)内有两个点(包括点本身)。最后，N是一个异常值，因为它不是一个核心点，不能从核心点到达。</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="125e" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">11.朴素贝叶斯为什么叫朴素？</h2><p id="d2a9" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">朴素贝叶斯算法假设特征是相互独立的，特征之间没有相关性。然而，现实生活中并非如此。这种特征不相关的天真假设是这种算法被称为“天真”的原因。</p><p id="6232" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所有特征都是独立的假设使得朴素贝叶斯算法<strong class="kk iu">比复杂算法</strong>更快。<strong class="kk iu"> </strong>在某些情况下，速度优先于更高的精度。</p><p id="8ab0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它可以很好地处理文本分类、垃圾邮件检测等高维数据。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="507d" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">12.什么是日志丢失？</h2><p id="79fd" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">对数损失(即交叉熵损失)是机器学习和深度学习模型的广泛使用的成本函数。</p><p id="d41c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">交叉熵量化了两个概率分布的比较。在监督学习任务中，我们有一个目标变量，我们试图预测。使用交叉熵比较目标变量的实际分布和我们的预测。结果是交叉熵损失，也称为对数损失。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="1eba" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">13.原木损失是如何计算的？</h2><p id="0736" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">对于每个预测，计算真实类的预测概率的负自然对数。所有这些值的总和给出了对数损耗。</p><p id="5e08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一个例子可以更好地解释这个计算。</p><p id="f0a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们有一个4类的分类问题。我们的模型对特定观测值的预测如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/9c16dacea9ac6ed21ffeac9a57264b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/0*KQuceLhbGah-YR2f.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">(图片由作者提供)</p></figure><p id="a322" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">来自该特定观察(即数据点或行)的对数损失是-log(0.8) = 0.223。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="95b1" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">14.为什么我们用log loss代替分类精度？</h2><p id="f995" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">在计算对数损失时，我们取预测概率的自然对数的负值。我们对预测越有把握，测井损失就越低(假设预测是正确的)。</p><p id="cd48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，-log(0.9)等于0.10536，而-log(0.8)等于0.22314。因此，90%的把握比80%的把握会导致更低的测井损失。</p><p id="c973" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">传统的度量标准，如分类准确度、精确度和召回率，通过比较预测类和实际类来评估性能。</p><p id="dbfd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下表显示了两个不同模型对一个由5个观察值组成的相对较小的集合的预测。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/d8e3d310877eee5671409eaf53991b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/0*9p6AfarmlDxrtu39.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">(图片由作者提供)</p></figure><p id="566c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两个模型都正确地对5个观察值中的4个进行了分类。因此，在分类精度方面，这些模型具有相同的性能。然而，概率揭示了模型1在预测中更确定。因此，总体而言，它的表现可能会更好。</p><p id="81cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对数损失(即交叉熵损失)提供了对分类模型的更稳健和准确的评估。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="cade" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">15.ROC曲线和AUC</h2><p id="fab8" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated"><strong class="kk iu"> ROC曲线</strong>通过组合所有阈值处的混淆矩阵来总结性能。<strong class="kk iu"> AUC </strong>将ROC曲线转化为二元分类器性能的数字表示。AUC是ROC曲线下的面积，取0到1之间的值。AUC表示一个模型在区分正类和负类方面有多成功。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="1297" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">16.精确度和召回率</h2><p id="2371" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">精确度和召回率度量将分类准确性向前推进了一步，并允许我们获得对模型评估的更具体的理解。选择哪一个取决于任务和我们的目标。</p><p id="a844" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> Precision </strong>衡量我们的模型在预测为正时有多好。精度的焦点是<strong class="kk iu">正面预测</strong>。它表明有多少积极的预测是正确的。</p><p id="84c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">回忆</strong>测量我们的模型在正确预测积极类方面有多好。召回的重点是<strong class="kk iu">实际正班</strong>。它表示模型能够正确预测的阳性类别的数量。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="52b0" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">结论</h2><p id="86fb" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">我们已经介绍了一些基本信息以及关于机器学习算法的一些细节。</p><p id="feb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有些点与多种算法有关，比如关于日志丢失的算法。这些也很重要，因为评估模型和实现它们一样重要。</p><p id="09f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所有的机器学习算法在某些任务中都是有用和有效的。根据你正在做的工作，你可以掌握其中的一些。</p><p id="0999" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，了解这些算法的工作原理是很有价值的。</p><p id="4c9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>