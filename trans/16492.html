<html>
<head>
<title>Machine Learning Algorithms from Start to Finish in Python: SVM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从头到尾的机器学习算法:SVM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1?source=collection_archive---------1-----------------------#2020-11-14">https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1?source=collection_archive---------1-----------------------#2020-11-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f701" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习、理解和实现最强大的通用机器学习算法之一。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ffaf8a8d8766b79fda8dd587964b8939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lo7_FEfN98l4mVec8x8ntQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">迈克·洛索在<a class="ae ky" href="https://unsplash.com/s/photos/street?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="d6ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">支持向量机是非常通用的机器学习算法。它们受欢迎的主要原因是它们能够使用所谓的<em class="lv">内核技巧执行线性和非线性分类和回归；如果你不知道那是什么，不要担心。完成本文后，您将能够:</em></p><ul class=""><li id="8adf" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">了解什么是SVM及其工作原理</li><li id="8f22" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">区分<em class="lv">硬边际SVM </em>和<em class="lv">软边际SVM </em></li><li id="9d07" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">用Python从头开始编写SVM代码</li></ul><p id="c713" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，事不宜迟，让我们开始吧！</p><h1 id="07fd" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">什么是SVM，我为什么需要它？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/b5a7dfb339c5feb52c47ce22d5c476bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1y5RF6dmVDsBFNA6hdJZqg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@paul_nic?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Paolo Nicolello </a>在<a class="ae ky" href="https://unsplash.com/s/photos/confused?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="c820" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这么多其他算法(线性回归、逻辑回归、神经网络等..)你可能想知道为什么你的工具箱里还需要一个！也许这些问题可以借助图表来回答:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/1c1831a3f470784fa00b6a507fc2d700.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*EYSC4qcKI43P8b1EYcVtPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Support_vector_machine" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="6c6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们看到了对数据进行分类的三个潜在决策边界:H1、H2和H3。首先，H1根本不区分阶级，所以它不是一个好的超平面。H2确实分离了类别，但是请注意，点与点之间的差距(或<em class="lv">街道</em>)是如此之小，并且这个分类器不太可能在看不见的实例上表现良好。</p><p id="4a0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第三个超平面，H3，代表SVM分类器的决策边界；这条线不仅将两个类别分开，而且在两个类别的最极端点之间保持最宽的距离。</p><p id="e976" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以把SVM看作是两个阶层之间最大的差距。这就是所谓的<em class="lv">大幅度分类。</em></p><h1 id="6bd9" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">大幅度分类</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/b64882887bed8721f373d27cfb939e76.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*IbmJi_Zg0w_zpRaafsqCjg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://en.wikipedia.org/wiki/Support_vector_machine" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="cfde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如我所说，大间隔SVM分类器本质上试图在两个类之间拟合尽可能宽的街道(用平行虚线表示)。需要注意的是，添加更多“远离街道”(不在虚线上)的实例不会影响决策边界。</p><p id="8353" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策边界完全由类的最极端的实例确定(或由<em class="lv">支持</em>),或者换句话说，位于街道边缘的实例。这些被称为<em class="lv">支持向量(</em>它们在图<em class="lv">中用黑色圈出)。</em></p><h2 id="9ff6" class="nf ml it bd mm ng nh dn mq ni nj dp mu li nk nl mw lm nm nn my lq no np na nq bi translated">硬利润分类的限制</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/4df664f3474bc0601795789ddca3e8bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1bDAJ5ASFQryFaN9Dygp3A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@ludo_photos?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Ludovic Charlet </a>在<a class="ae ky" href="https://unsplash.com/s/photos/limit?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4dbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以本质上，硬边界SVM基本上试图拟合一个决策边界，最大化两个类的支持向量之间的距离。但是，这种模式存在一些问题:</p><ol class=""><li id="2a01" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu ns mc md me bi translated">它对异常值非常敏感</li><li id="f26c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">它只对线性可分的数据有效</li></ol><p id="2976" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个概念可以在这个观想中清晰地突出出来:</p><h2 id="05e6" class="nf ml it bd mm ng nh dn mq ni nj dp mu li nk nl mw lm nm nn my lq no np na nq bi translated">问题1:它对异常值非常敏感</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/3404de124bbfe206309b0b7839585c9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*_Rq-5o94mpbgtwa9W7o7tQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://stackoverflow.com/questions/4629505/svm-hard-or-soft-margins/4630731" rel="noopener ugc nofollow" target="_blank"> StackOverflow </a>拍摄</p></figure><p id="6af5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意红点是一个极端的异常值，因此SVM算法将它用作支持向量。因为硬边界分类器找到支持向量之间的最大距离，所以它使用红色异常值和蓝色支持向量来设置决策边界。</p><p id="9cbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这导致很差的决策边界，很可能过度拟合，并且将不能很好地预测新的类。</p><h2 id="c257" class="nf ml it bd mm ng nh dn mq ni nj dp mu li nk nl mw lm nm nn my lq no np na nq bi translated">问题2:它只对线性可分的数据有效</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/92b12b72625f4a345df3ec3bec0e4144.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*qJPvEFG4ppXTEkCsTz1pmw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="9701" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，我们可以清楚地观察到，没有可能的线性分类器将类别分开。此外，还有一个主要的异常值。所以，问题是，SVM如何分离非线性可分数据？</p><h1 id="6bb9" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">处理异常值和非线性数据</h1><h2 id="c798" class="nf ml it bd mm ng nh dn mq ni nj dp mu li nk nl mw lm nm nn my lq no np na nq bi translated">方法一:软利润SVM</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/0705a731cca7e26718586b51647bdf77.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*vn28agSd8EPxXTbN_6DHsw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="fc5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种方法是在保持街道尽可能宽(最大化裕量)和限制<em class="lv">裕量违规(</em>这些实例最终出现在街道中间，甚至出现在街道的错误一侧<em class="lv">)之间找到良好的平衡。</em>这叫做<em class="lv">软边际SVM。</em></p><p id="eb00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，您是在控制两个目标之间的权衡:</p><ol class=""><li id="6c65" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu ns mc md me bi translated"><strong class="lb iu">最大化</strong>决策边界和支持向量之间的<strong class="lb iu">距离</strong></li><li id="4385" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated"><strong class="lb iu">最大化</strong>被决策边界正确分类的<strong class="lb iu">点数</strong></li></ol><p id="9355" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种权衡通常由一个超参数控制，这个超参数可以用λ表示，或者更常见的(在scikit-learn中)是<strong class="lb iu"> C </strong>参数。这实质上控制了错误分类成本。具体来说，</p><ul class=""><li id="8599" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><strong class="lb iu"> C </strong>的小<strong class="lb iu">值导致更宽的街道，但更多的边界违规(更高的偏差，更低的方差)</strong></li><li id="d985" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">a<strong class="lb iu">C</strong>的值越大，街道越窄，但边界违规越少(低偏差、高方差)。</li></ul><p id="4d36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管这种方法可行，但我们必须使用交叉验证技术找出最佳的<strong class="lb iu"> C </strong>参数。这可能需要相当长的时间。此外，人们可能希望创建一个最佳模型，并且没有任何跨越边界违规的“松弛”变量。那么我们现在的解决方案是什么？</p><h2 id="a9d0" class="nf ml it bd mm ng nh dn mq ni nj dp mu li nk nl mw lm nm nn my lq no np na nq bi translated">方法二:内核</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/cf0f5b3415cbe244db2f7566d7f8f4b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9VdCs7rch1ZDFU5Qkfx3tg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/File:Kernel_trick_idea.svg" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="f52a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然线性SVM在大多数情况下都很有效，但是很少有数据集是线性可分的。解决这个问题的一个方法是添加更多的特性，比如多项式特性(这些特性本质上是通过将值提升到一个<strong class="lb iu"> N </strong>次多项式来改变你的特性(想想X，X，等等..)).</p><p id="1a11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，假设我们有以下数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/2f75aef03a46877911f5bba862e0c92a.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*jIzTzhAcz7mxx4j3th8xnw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="6db8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，这个数据集不是线性可分的。</p><p id="1bde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，当我们通过将根提升到20的幂来应用多项式变换时:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/d2eb796253a708828af59e82a564dc8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*7-GMzF8Ik00DfoYzhiuCNQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="2114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们得到一个线性可分的数据集。</p><p id="71b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这对于大型数据集是不可行的；进行多项式变换所需的计算复杂度和时间太长，计算成本太高。</p><p id="1e91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，使用高次多项式次数会创建大量的要素，从而使模型太慢。</p><p id="bcda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是SVM的魅力所在。更确切地说，<em class="lv">内核的妙处在于</em></p><h1 id="ea60" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">内核技巧</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/19103f07ebadb97bcc410c1006e053d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IcvxVsBw3V4AYiMPYHqjjQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@juliusdrost?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">朱利叶斯德罗斯特</a>在<a class="ae ky" href="https://unsplash.com/s/photos/magic?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="eeb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，核是计算非线性可分离数据点之间的关系并将它们映射到更高维度的不同函数。然后它适合一个标准的支持向量分类器。它有效地将特征从相对低的维度映射到相对高的维度。</p><p id="ae45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，核函数仅计算数据点之间的高维度关系，就好像它们在更高维度中一样；它们实际上不做转换，这意味着内核函数不添加任何特性，但是我们得到的结果和我们做的一样。</p><p id="d111" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种技巧(计算数据点之间的高维关系，而不实际创建或转换它们)被称为<strong class="lb iu">内核技巧</strong>。</p><p id="2840" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">内核技巧通过避免将特征从低维转换到高维的数学运算，降低了SVM的计算复杂度！</p><p id="548b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看两个常见的内核函数:</p><p id="2005" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1 .多项式核</p><p id="60a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.高斯径向基函数</p><h1 id="8751" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">核函数1:多项式核</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/725e588529ef5beb79264a7c096acb53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mbZ4d3IzTmi46SzfihlJqw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@chrislawton?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">克里斯·劳顿</a>在<a class="ae ky" href="https://unsplash.com/s/photos/transformation?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4b47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，这使用多项式核来计算数据点之间的高维关系，并将数据映射到更高维度，而不添加任何特征。</p><p id="d3ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多项式核的公式如下(我很抱歉没有警告就用数学来打击你！):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/e7eba5b30079f9003fb1734356b04d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:252/format:webp/1*BG0s-SnFz1taFXaVYS9riw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><ul class=""><li id="f53d" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">这里的<em class="lv"> d </em>是一个超参数，指的是函数应该使用的多项式的次数。</li></ul></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h2 id="675b" class="nf ml it bd mm ng nh dn mq ni nj dp mu li nk nl mw lm nm nn my lq no np na nq bi translated">一个例子</h2><p id="3e5d" class="pw-post-body-paragraph kz la it lb b lc oi ju le lf oj jx lh li ok lk ll lm ol lo lp lq om ls lt lu im bi translated">举个具体的例子，假设我们有这样的数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/6177e396bb82528a8f1e095000335965.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*uAQAyK4jfaYGUuwz1U--zw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="43bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，这个数据不是线性可分的。然而，如果我们使用具有多项式核的SVM，我们将得到以下高维映射:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/91f344533f49047fcd428d6b5133cebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*O_cmjKl3dSR0MuhbOn0SOw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="1417" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，从中得出的重要思想是，核函数仅计算点之间的高维关系，就像它们在高维空间中一样，但不创建或变换新的要素。</p><p id="06ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下代码使用scikit-learn中的SVC类实现了一个多项式内核:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="d563" class="nf ml it oq b gy ou ov l ow ox">from sklearn.svm import<strong class="oq iu"> </strong>SVC </span><span id="16e4" class="nf ml it oq b gy oy ov l ow ox">svc = SVC(kernel="poly", degree=3, coef0=1, C=5))<br/>svc.fit(X_train,y_train)</span></pre><p id="3105" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，如果你的模型过拟合，你可能需要降低多项式的次数。您可能已经注意到这里的一些参数。让我简单解释一下:</p><ul class=""><li id="bd87" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><strong class="lb iu">内核:</strong>定义要使用的内核(我们将在后面探索一些其他选项)</li><li id="047b" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">次数:</strong>定义多项式内核的次数</li><li id="b25d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu"> C </strong>:分类误差，主要控制具有最大可能裕度和最大化决策边界正确分类的点数之间的权衡。</li></ul><h1 id="0f7a" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">核函数2:高斯RBF核</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/53b61092c112fdeed74a6c45b573bbb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23UJCdCux06d5KMVujjXUA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">苏珊·d·威廉姆斯在<a class="ae ky" href="https://unsplash.com/s/photos/transformation?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="67c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个非常流行的SVM核是<em class="lv">高斯径向基函数(高斯RBF)。</em>本质上，这是一个计算实例和一个<em class="lv">地标之间距离的相似度函数。</em>内核函数的公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/c522fff01cc215f17b6b4dc9948666d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*g6Y-W3UvMGcyymYH_3s6iA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="708b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">澄清一些符号(注意:任何不清楚的地方将很快解释清楚):</p><ul class=""><li id="20be" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">x:指一个实例</li><li id="934e" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">' x ':指的是一个<em class="lv">地标</em></li><li id="d070" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">σ:这是指<em class="lv">伽玛</em></li></ul><p id="8ccc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该函数本身遵循一条钟形曲线(因此它是<em class="lv">高斯</em>)，范围从0(离地标非常远)到1(在地标处)。</p><p id="2f25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我确信这在你的脑海中仍然是模糊的，所以让我们用我所知道的唯一方法来澄清这种困惑；举个例子！</p><h2 id="e787" class="nf ml it bd mm ng nh dn mq ni nj dp mu li nk nl mw lm nm nn my lq no np na nq bi translated"><strong class="ak">一个例子</strong></h2><p id="fba8" class="pw-post-body-paragraph kz la it lb b lc oi ju le lf oj jx lh li ok lk ll lm ol lo lp lq om ls lt lu im bi translated">观察下面的1D数据图表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/055edd8331ea3efeab2fd5961843146c.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*J04IQHsElTrJ1Eqbiu37Gg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="c190" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">界标</em>本质上是数据集中的一个点，我们将使用它来获得。这里我们有两个地标，<strong class="lb iu"> X2 </strong>和<strong class="lb iu"> X3。</strong></p><p id="aa2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在准备计算新的特征。比如我们来看实例<strong class="lb iu"> X，</strong>等于-1。它位于距第一地标1的距离和距第二地标2的距离处。因此，其新的映射特征将是:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="b17d" class="nf ml it oq b gy ou ov l ow ox"><em class="lv">x</em>2 = exp (–0.3 × 12) ≈ 0.74</span></pre><p id="6754" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">和</strong></p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="be2e" class="nf ml it oq b gy ou ov l ow ox"><em class="lv">x</em>3 = exp (–0.3 × 22) ≈ 0.30</span></pre><p id="0856" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如下图所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/e3ccecd87f756dcfc26bc0e7c165eecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*bWvd3NlnptPmC37mgtf9mw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="df61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们简单地用一个普通的SVC来分类这些点！</p><p id="76a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可能在想，酷！但是:</p><ol class=""><li id="ae21" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu ns mc md me bi translated">你如何选择地标？</li><li id="5d94" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">你还没解释伽马到底是什么意思！</li></ol><p id="3cd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">选择地标</strong></p><p id="5c22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如此真实。为了解决第一个问题，通常在数据集中每个实例的位置创建一个地标。这产生了许多维度，从而增加了变换后的训练集是线性可分的机会。</p><p id="f774" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，再一次，就像多项式变换一样，这在计算上是昂贵的，并且需要添加许多功能，想象一下，如果您有一个具有<em class="lv"> m </em>个实例和<em class="lv"> n </em>个功能的训练集被变换成一个具有<em class="lv"> m </em>个实例和<em class="lv"> m </em>个功能的训练集(假设您删除了原始功能)。</p><p id="5bfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，如果您有20个包含20个特征的实例，那么计算这些转换将得到400个特征！</p><p id="59e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，内核技巧发挥了它的魔力；它使得计算这些更高维度的关系成为可能，而不需要实际转换或创建新的特征，并且仍然得到与你所得到的相同的结果！</p><p id="23f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">伽玛</strong></p><p id="ae60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，伽马是一个特殊的超参数，它特定于<em class="lv"> rbf </em>内核。回到我们上面的rbf函数图，gamma控制每个钟形函数的宽度。</p><p id="8a4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，<em class="lv"> gamma </em>的<strong class="lb iu">大值</strong>将缩短钟形曲线的宽度，并减小每个实例的影响范围，从而导致在各个数据点之间摆动的更不规则的决策边界。相反，<em class="lv"> gamma </em>的<strong class="lb iu">小</strong>值增加了每个数据点的影响范围，并导致更平滑的决策边界。</p><p id="92e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">带有rbf内核的SVC的scikit-learn实现如下:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="cf22" class="nf ml it oq b gy ou ov l ow ox">SVC(kernel="rbf", gamma=5, C=0.001)</span></pre><blockquote class="pd pe pf"><p id="80a2" class="kz la lv lb b lc ld ju le lf lg jx lh pg lj lk ll ph ln lo lp pi lr ls lt lu im bi translated"><strong class="lb iu">注意</strong>:在这篇文章中，我将只编码一个软硬边距的SVM。但是在将来，我会写一些关于如何在SVM实现内核技巧的文章，所以请一定要继续关注</p></blockquote></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="4921" class="mk ml it bd mm mn pj mp mq mr pk mt mu jz pl ka mw kc pm kd my kf pn kg na nb bi translated">硬利润和软利润SVM的数学</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/4babfc8677447717ce5124487c1e6490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oOw6bbjKo1zyxTTUB3QlCg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安妮·斯普拉特在<a class="ae ky" href="https://unsplash.com/s/photos/maths?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="0655" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是的，我很抱歉，但不幸的是，你需要理解数学，以编码它。如果你真的讨厌数学，请随意跳过这一部分，但我强烈建议至少尝试理解正在发生的事情，让你对手头的问题有更好的感觉。</p><p id="dc93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我真正开始计算之前，让我一步一步地指导您SVM是如何工作的:</p><ol class=""><li id="b2a2" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu ns mc md me bi translated">为数据拟合一个超平面，并尝试对这些点进行分类</li><li id="873e" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">使用<strong class="lb iu"> <em class="lv">优化算法</em></strong><em class="lv"/>调整模型的参数，使其在支持向量之间保持最大可能的余量。</li><li id="7e82" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">重复n次迭代，或者直到<strong class="lb iu"> <em class="lv">成本函数</em> </strong>最小化。</li></ol><p id="dd24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，在我们深入研究SVM数学之前，让我解释一些关键术语。</p><h1 id="f869" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">成本函数</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/dd3b3e1d932fc28fe5c9c31a9d801b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*baxcUSIKVOwWsTLc4e07yQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">佩皮·斯托扬诺夫斯基在<a class="ae ky" href="https://unsplash.com/s/photos/cost?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="b32a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">成本函数本质上是一个衡量损失的公式，或模型的“成本”。如果你曾经参加过任何Kaggle比赛，你可能会遇到一些。一些常见的包括:</p><ul class=""><li id="e43d" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">均方误差</li><li id="f309" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">均方根误差</li><li id="5a7f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">绝对平均误差</li><li id="cf75" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">原木损失</li></ul><p id="8fad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用的成本函数被称为<em class="lv">铰链损耗。</em>函数的<em class="lv"> </em>公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/0ab76c21d7211bdeab0e18a9b57a5b86.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*tfQTImRDs17dVWz1bm0HYQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Hinge_loss" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="b092" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从图形上看，铰链损耗如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/3590f9b3405f0f0d91df0c5aa6d74585.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*FzVJ4RZCsQNVPUeTTj5e6g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Hinge_loss" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="0ed9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在该图中，蓝色代表正确分类的损失，绿色代表错误分类的损失。</p><p id="2594" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，即使我们对数据点进行了正确的分类，铰链损失也会影响数据点在边缘内的预测。</p><p id="1e21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，我们将使用这一点来衡量我们的算法的性能，并确保我们达到我们的目标(最大限度地提高利润)</p><h1 id="398e" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">优化算法</h1><p id="27ce" class="pw-post-body-paragraph kz la it lb b lc oi ju le lf oj jx lh li ok lk ll lm ol lo lp lq om ls lt lu im bi translated">优化通常被定义为改进某样东西的过程，以使其发挥最大潜力。这也适用于机器学习。在ML的世界中，优化本质上是试图为某个数据集找到最佳的参数组合。这本质上是机器学习的“学习”部分。</p><p id="67a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然存在许多优化算法，但我将讨论其中最常见的两种:梯度下降和正规方程。</p><h1 id="a39b" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">梯度下降</h1><p id="ce82" class="pw-post-body-paragraph kz la it lb b lc oi ju le lf oj jx lh li ok lk ll lm ol lo lp lq om ls lt lu im bi translated">梯度下降是一种优化算法，旨在找到一个函数的最小值。它通过在斜率的负方向迭代地采取步骤来实现这个目标。在我们的例子中，梯度下降通过移动函数切线的斜率来不断更新权重。好极了，听起来很棒。请说英语。:)</p><h1 id="6bad" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">梯度下降的一个具体例子</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/6105ff9d47406e14d9b31a428741b72e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JLJK4oRsZaNh-pkL.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卢卡斯·克拉拉在<a class="ae ky" href="https://unsplash.com/s/photos/mountain?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1171" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更好地说明梯度下降，让我们通过一个简单的例子。想象一个人在山顶，他/她想到达山下。他们可以做的是环顾四周，看看他们应该朝哪个方向迈一步，以便更快地下来。然后，他们可能会朝那个方向迈出<em class="lv">一步</em>，现在他们离目标更近了。然而，他们下来时必须小心，因为他们可能会让<em class="lv">卡在某个点上</em>，所以我们必须确保<em class="lv">相应地选择我们的步长。</em></p></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><p id="d95a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，梯度下降的目标是最小化一个函数。在我们的案例中，是为了最小化我们模型的成本。这是通过找到函数的切线并向那个方向移动来实现的。算法的“<em class="lv">步骤</em>的大小由所谓的<em class="lv">学习率来定义。</em>这基本上控制了我们向下移动的距离。有了这个参数，我们必须小心两种情况:</p><ol class=""><li id="3458" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu ns mc md me bi translated">学习率太大，算法可能不收敛(达到最小值)并在最小值附近跳动，但永远不会收敛</li><li id="63d0" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">学习率太小，算法将花费太长时间达到最小值，还可能“卡”在次优点。</li></ol><p id="b589" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还有一个参数来控制算法在数据集上迭代的次数。</p><p id="0991" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从视觉上看，该算法会做这样的事情:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/e4a28e1c581d57e7f50a630ddcc0e58b.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*jsYtb9LN_JdhJSFF.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="0c24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为这种算法在机器学习中非常重要，所以让我们回顾一下它的作用:</p><ol class=""><li id="2edd" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu ns mc md me bi translated">随机初始化权重。这叫做(你猜对了)<em class="lv">随机初始化</em></li><li id="0ef2" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">然后，该模型使用这些随机权重进行预测。</li><li id="d340" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">模型的预测通过成本函数进行评估。</li><li id="2e3a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">然后，模型运行梯度下降，通过找到函数的切线，然后在切线的斜率中采取一个步骤</li><li id="c26f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">该过程重复N次迭代，或者如果满足标准。</li></ol><p id="bd2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个过程在数学上显示如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/562809426fa87ebb61feda1224ed8d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*VP3X1ULNOx7I3fvXlOz-Ew.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="5f14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里需要注意的重要事项:</p><p id="0a2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> α: </em>这是学习率的符号(记住:步长的大小)</p><p id="e42f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> m </em>:训练样本数</p><p id="4730" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> h(θx) </em>:我们的预测</p><p id="71e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> θn: </em>我们算法的第n个系数</p><h1 id="9f8d" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">梯度下降的优点和缺点</h1><h1 id="777c" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">优势:</h1><ol class=""><li id="3ce7" class="lw lx it lb b lc oi lf oj li pu lm pv lq pw lu ns mc md me bi translated">梯度下降很可能将成本函数降低到全局最小值(非常接近或= 0)</li><li id="fb50" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">最有效的优化算法之一</li></ol><h1 id="d48d" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">缺点:</h1><ol class=""><li id="79aa" class="lw lx it lb b lc oi lf oj li pu lm pv lq pw lu ns mc md me bi translated">在大型数据集上可能会很慢，因为它使用整个数据集来计算函数切线的梯度</li><li id="7512" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">容易陷入次优点(或局部最小值)</li><li id="4f44" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">用户必须手动选择学习速率和迭代次数，这可能很耗时</li></ol><p id="f418" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好的，我知道你真的想开始实际的编码，但是这最后一部分是SVM数学中最重要的部分，所以坚持住！</p><h1 id="b12b" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">SVM的数学</h1><p id="7a70" class="pw-post-body-paragraph kz la it lb b lc oi ju le lf oj jx lh li ok lk ll lm ol lo lp lq om ls lt lu im bi translated"><strong class="lb iu">对于预测</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi px"><img src="../Images/858f5af0b91b1b1eb24b81a25692c1ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*i4-LxkYWxXZYTgMH9R1--w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="18a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们主要做以下工作:</p><ul class=""><li id="a35e" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">如果权重乘以特征减去偏差项的点积的符号大于或等于1，则预测<strong class="lb iu"> 1 </strong></li><li id="71a6" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">如果权重乘以特征减去偏差项的点积的符号小于或等于-1，则预测<strong class="lb iu"> 0 </strong></li></ul><p id="0342" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">条件</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/50bd2dfe1c75accb599a8cd771095b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*fM8Ohhovpxa_cw2xeaH27A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="9b2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是正分类的铰链损失的条件。这由我们之前看到的铰链损耗图上的蓝线表示。基本上，这将检查给定实例的分类是正确的还是错误的。</p><p id="4751" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">正确分类:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/6ef531766664ca80c7e478af805f6d80.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*3F8RSO3CTZecbFUfmU3Qnw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="cbfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们正确分类的公式。澄清:</p><p id="8988" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> w </em>:算法的权重</p><p id="84e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> α: </em>我们之前谈到的<em class="lv"> </em>梯度下降的学习率</p><p id="ecd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">λ:正则化参数(相当于<strong class="lb iu"> C </strong>参数)</p><p id="3885" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">分类不正确:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/26afd3d532b11ec0bc486cac5c3d3165.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*X6srt8i1YhUlcWTQL18BBA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="41de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是不正确分类的公式。请注意我们是如何调整偏置项的。</p></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><p id="f9b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，最后，女士们先生们，重头戏:SVM从零开始！</p><h1 id="5f94" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">从头开始编写SVM代码</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/8f335a6d8775cb8922da8dd8d6c72478.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DB-CSRBOFeMk5y8I.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯里德T21在Unsplash上的照片</p></figure><p id="bfa4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们终于开始了！首先，让我们做一些基本的导入:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="24a6" class="nf ml it oq b gy ou ov l ow ox">import numpy as np <br/>import matplotlib.pyplot as plt<br/>from sklearn import datasets<br/>from sklearn.model_selection import train_test_split</span></pre><p id="89a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是的，没有sklearn型号！这将用简单的数字编码！</p><p id="9b6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们创建一个基本数据集，并将数据分为训练和测试两部分:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="32f6" class="nf ml it oq b gy ou ov l ow ox">X, y =  datasets.make_classification(n_samples=500, n_features=5, random_state=42, class_sep=0.7)<br/>X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42, test_size=0.2)</span></pre><p id="8d36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我准备为这个教程制作一个合成数据集，将<em class="lv"> class_sep </em>设置为0.7，意思是if应该相对简单的对数据进行分类。</p><p id="1d20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，为了遵守软件工程原则，我将创建一个SVM类，并构建它以使代码看起来更整洁，更易于共享:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="c50a" class="nf ml it oq b gy ou ov l ow ox">class SVM:</span><span id="ee41" class="nf ml it oq b gy oy ov l ow ox">def __init__(self, learning_rate=0.001, C=0.001, n_iters=100):<br/>        self.lr = learning_rate<br/>        self.C = C<br/>        self.n_iters = n_iters<br/>        self.w = None<br/>        self.b = None</span></pre><p id="ac29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在这里，我们基本上初始化我们的学习率，正则化参数，迭代次数，我们设置权重和偏差等于零。</p><p id="4e2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们定义我们的<em class="lv">拟合</em>方法:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="cf45" class="nf ml it oq b gy ou ov l ow ox">def fit(self, X, y):<br/>        n_samples, n_features = X.shape<br/>        <br/>        # Convert 0 labels to be -1 to set threshold with hinge loss<br/>        y_ = np.where(y &lt;= 0, -1, 1)<br/>        <br/>        self.w = np.random.rand(n_features)<br/>        self.b = 0</span></pre><p id="f796" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦给定了训练特征和目标向量，我们就可以随机地将我们的权重初始化为特征数量的向量。请注意我们如何将数据集中的0值转换为等于-1，这样我们就可以使用铰链损耗。</p><p id="dd4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们继续这个方法的核心:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="014c" class="nf ml it oq b gy ou ov l ow ox">for _ in range(self.n_iters):<br/>    for idx, x_i in enumerate(X):<br/>        condition = y_[idx] * (np.dot(x_i, self.w) - self.b) &gt;= 1</span></pre><p id="1168" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们主要做了以下工作:</p><ol class=""><li id="6e51" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu ns mc md me bi translated">循环n_iters次(默认情况下n_iters=100)</li><li id="deeb" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">对于选定的索引和值X，我们设置我们的条件，检查我们选定的目标值乘以我们选定的实例和权重的点积减去偏差是否大于或等于1。这实质上检验了我们根据铰链损耗分类是否正确。</li></ol><p id="9d4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将用于<strong class="lb iu">正确分类</strong>的公式翻译成代码:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="8fe1" class="nf ml it oq b gy ou ov l ow ox">if condition:<br/>    self.w -= self.lr * (2 * self.C * self.w)</span></pre><p id="7534" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而我们对<strong class="lb iu">的公式不正确的归类</strong>成代码:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="f9b9" class="nf ml it oq b gy ou ov l ow ox">else:<br/>    self.w -= self.lr * (2 * self.C * self.w - np.dot(x_i, y_[idx]))<br/>    self.b -= self.lr * y_[idx]</span></pre><p id="3757" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们定义我们的<em class="lv">预测</em>函数:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="d770" class="nf ml it oq b gy ou ov l ow ox">def predict(self, X):<br/>    preds = np.sign(np.dot(X, self.w) - self.b)<br/>    return np.where(preds == -1,0,1)</span></pre><p id="8890" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们确保将等于-1的标签转换回零。</p><p id="4efb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们只需调用我们的函数，并在测试集上获得我们的模型的准确性:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="6478" class="nf ml it oq b gy ou ov l ow ox">clf = SVM()<br/>clf.fit(X_train, y_train)</span><span id="d829" class="nf ml it oq b gy oy ov l ow ox">preds = clf.predict(X_test)</span><span id="4160" class="nf ml it oq b gy oy ov l ow ox">(preds == y_test).mean()</span><span id="50b6" class="nf ml it oq b gy oy ov l ow ox">OUT:<br/>0.82</span></pre><p id="2632" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我添加了一个visualise_svm()函数来帮助可视化svm，可以从我在本文末尾添加的Github repo中访问它。然而，运行该函数会输出以下内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/eab5b410ecb05f14b0e4211e869bcd7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*TDQALW3TzOzpSkW9BFjjIw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="b836" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，如果你还没有猜到，我们刚刚实施了软保证金SVM。我们将<strong class="lb iu"> C </strong>的值设置为0.001，我们可以清楚地看到，决策边界允许一些点位于边缘和错误的一侧，但却产生了更好的超平面。</p><p id="4859" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，当我们将<strong class="lb iu"> C </strong>值更改为0.9(非常小的正则化)时，我们会得到以下图形:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b1fe832cfe82b0e0a430710b0aa1405c.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*cZVarr8CimWppVlzx3gDRw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="034e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的准确率从0.82降到了0.7</p><p id="e99e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">锁定作业</strong></p><p id="3e9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我想让你尝试一些任务:</p><ol class=""><li id="1ba9" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu ns mc md me bi translated">试着把C调成一个很小很大的值？决策边界如何变化？</li><li id="3dd6" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">调整梯度下降算法的学习率。你会得到更好的结果吗？</li></ol></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><p id="58eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">真的很感谢每一个激励我写出好文章的人。我感谢我的忠实追随者和所有决定阅读我作品的人。我向你保证，这不会被忽视。我希望总能为读者创作出有趣的、引人入胜的作品。</p><p id="e25c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你学到了新的东西，并可能刷新了一些旧的知识。一定要继续关注更多，祝你一切顺利！</p><p id="d78f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PS:这里是<a class="ae ky" href="https://github.com/Vagif12/ML-Algorithms-From-Scratch/blob/main/SVM%20from%20Scratch.py" rel="noopener ugc nofollow" target="_blank"> Github代码</a>的链接</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qb"><img src="../Images/79f050733d12bcc83ce13aaceff626cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pqPmYQLNFGwldjEokOd_HA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@kellysikkema?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Kelly Sikkema </a>在<a class="ae ky" href="https://unsplash.com/s/photos/thank-you?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div></div>    
</body>
</html>