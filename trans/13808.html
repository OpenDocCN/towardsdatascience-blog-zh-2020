<html>
<head>
<title>Image Segmentation Using Keras and W&amp;B</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Keras和W&amp;B的图像分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-segmentation-using-keras-and-w-b-98223c38e4d4?source=collection_archive---------28-----------------------#2020-09-22">https://towardsdatascience.com/image-segmentation-using-keras-and-w-b-98223c38e4d4?source=collection_archive---------28-----------------------#2020-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="339c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">该报告利用Keras中类似UNET的架构探索语义分割，并交互式可视化模型对权重偏差的预测&amp;。</strong></h2></div><h2 id="3f59" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">点击查看互动报道<a class="ae lb" href="https://wandb.ai/ayush-thakur/image-segmentation/reports/Image-Segmentation-Using-Keras-and-W-B--VmlldzoyNTE1Njc" rel="noopener ugc nofollow" target="_blank">。这里有</a><a class="ae lb" href="https://colab.research.google.com/drive/1rXV31gdyqEiXCtmSgff-H-VRuOSzv7IH?usp=sharing" rel="noopener ugc nofollow" target="_blank">的Colab笔记本</a>。</h2><h1 id="bd41" class="lc kg iq bd kh ld le lf kk lg lh li kn jw lj jx kr jz lk ka kv kc ll kd kz lm bi translated">介绍</h1><p id="58b8" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv ko lw lx ly ks lz ma mb kw mc md me mf ij bi translated">您是否有兴趣了解图像中某个对象的位置？这个物体的形状是什么？哪些像素属于对象？为了实现这一点，我们需要分割图像，即将图像的每个像素分类到它所属的对象，或者给图像的每个像素一个标签，而不是给图像一个标签。</p><p id="6985" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated"><strong class="lp ir">因此，图像分割是为图像中的每个对象学习逐像素掩模的任务。</strong>与为图像中出现的每个对象给出边界框坐标的对象检测不同，图像分割对图像中的对象给出了更精细的理解。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/fdd859833edccfebe5adb18f3edb7077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*at8_d54v1l9DbWhF9SAovg.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd mx">图1 </strong>:语义切分和实例切分。(<a class="ae lb" href="https://www.researchgate.net/figure/Semantic-segmentation-left-and-Instance-segmentation-right-8_fig1_339616270" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="6805" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">图像分割可以大致分为两种类型:</p><ul class=""><li id="95ff" class="my mz iq lp b lq mg lt mh ko na ks nb kw nc mf nd ne nf ng bi translated"><strong class="lp ir">语义分割:</strong>这里，每个像素属于一个特定的类。图1中的左图是语义分割的一个例子。像素或者属于人(一个类别)，或者属于背景(另一个类别)。</li><li id="f76f" class="my mz iq lp b lq nh lt ni ko nj ks nk kw nl mf nd ne nf ng bi translated"><strong class="lp ir">实例分割:</strong>这里，每个像素属于一个特定的类。但是，属于离散对象的像素用不同的颜色(蒙版值)标记。图1中的右图是一个实例分割的例子。属于该人物类别的像素被不同地着色。</li></ul><p id="b2de" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">该报告将<strong class="lp ir">建立一个语义分割模型</strong>，并在<a class="ae lb" href="https://www.robots.ox.ac.uk/%7Evgg/data/pets/" rel="noopener ugc nofollow" target="_blank">牛津-IIIT Pet数据集</a>上对其进行训练。我们将<strong class="lp ir">交互可视化我们模型的预测</strong>权重&amp;偏差。</p><h1 id="2825" class="lc kg iq bd kh ld le lf kk lg lh li kn jw lj jx kr jz lk ka kv kc ll kd kz lm bi translated">数据集</h1><p id="b5b2" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv ko lw lx ly ks lz ma mb kw mc md me mf ij bi translated">我们将使用<a class="ae lb" href="https://www.robots.ox.ac.uk/%7Evgg/data/pets/" rel="noopener ugc nofollow" target="_blank">牛津-IIIT Pet数据集</a>来训练我们的类UNET语义分割模型。</p><p id="489c" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">数据集由图像及其像素式掩膜组成。逐像素遮罩是每个像素的标签。</p><ul class=""><li id="572a" class="my mz iq lp b lq mg lt mh ko na ks nb kw nc mf nd ne nf ng bi translated">第1类:属于宠物的像素。</li><li id="9294" class="my mz iq lp b lq nh lt ni ko nj ks nk kw nl mf nd ne nf ng bi translated">第2类:属于宠物轮廓的像素。</li><li id="6605" class="my mz iq lp b lq nh lt ni ko nj ks nk kw nl mf nd ne nf ng bi translated">第三类:属于背景的像素。</li></ul><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/fa2fa2cd655c9a0c05b2d01b850ec885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*YBiN4_DGAoLi6SpqrvUBBQ.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd mx">图2 </strong>:宠物和它们的像素式遮罩。</p></figure><h2 id="dedc" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">下载数据集</h2><pre class="mm mn mo mp gt nn no np nq aw nr bi"><span id="d210" class="kf kg iq no b gy ns nt l nu nv">!curl -O <a class="ae lb" href="http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz" rel="noopener ugc nofollow" target="_blank">http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz</a></span><span id="7b40" class="kf kg iq no b gy nw nt l nu nv">!curl -O <a class="ae lb" href="http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz" rel="noopener ugc nofollow" target="_blank">http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz</a></span><span id="8a1d" class="kf kg iq no b gy nw nt l nu nv">!tar -xf images.tar.gz<br/>!tar -xf annotations.tar.gz</span></pre><h2 id="8358" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">数据集准备</h2><p id="03ef" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv ko lw lx ly ks lz ma mb kw mc md me mf ij bi translated"><code class="fe nx ny nz no b">images/</code>和<code class="fe nx ny nz no b">annotations/trimaps</code>目录包含提取的图像及其注释(按像素的遮罩)。所需图像为<code class="fe nx ny nz no b">.jpg</code>格式，而注释为<code class="fe nx ny nz no b">.png</code>格式。但是，在这些目录中有一些我们不需要的文件。因此，我们将准备两个列表- <code class="fe nx ny nz no b">input_img_paths</code>和<code class="fe nx ny nz no b">annotation_img_paths</code>，其中包含所需图像和注释的路径。</p><pre class="mm mn mo mp gt nn no np nq aw nr bi"><span id="beb9" class="kf kg iq no b gy ns nt l nu nv">IMG_PATH = 'images/'<br/>ANNOTATION_PATH = 'annotations/trimaps/'<br/><br/>input_img_paths = sorted(<br/>    [<br/>        os.path.join(IMG_PATH, fname)<br/>        for fname in os.listdir(IMG_PATH)<br/>        if fname.endswith(".jpg")<br/>    ]<br/>)<br/>annotation_img_paths = sorted(<br/>    [<br/>        os.path.join(ANNOTATION_PATH, fname)<br/>        for fname in os.listdir(ANNOTATION_PATH)<br/>        if fname.endswith(".png") and not fname.startswith(".")<br/>    ]<br/>)<br/><br/>print(len(input_img_paths), len(annotation_img_paths))</span></pre><p id="9011" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">总共有7390张图片和注释。我们将使用1000幅图像及其注释作为验证集。</p><h2 id="35d1" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">使用<code class="fe nx ny nz no b">tf.data</code>的数据加载器</h2><p id="ff85" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv ko lw lx ly ks lz ma mb kw mc md me mf ij bi translated">我们将使用<code class="fe nx ny nz no b">tf.data.Dataset</code>构建我们的输入管道。</p><pre class="mm mn mo mp gt nn no np nq aw nr bi"><span id="716c" class="kf kg iq no b gy ns nt l nu nv">IMG_SHAPE = 128<br/>AUTO = tf.data.experimental.AUTOTUNE<br/>BATCH_SIZE = 32<br/><br/>def scale_down(image, mask):<br/>  # apply scaling to image and mask<br/>  image = tf.cast(image, tf.float32) / 255.0<br/>  mask -= 1<br/>  return image, mask<br/><br/>def load_and_preprocess(img_filepath, mask_filepath):<br/>   # load the image and resize it<br/>    img = tf.io.read_file(img_filepath)<br/>    img = tf.io.decode_jpeg(img, channels=3)<br/>    img = tf.image.resize(img, [IMG_SHAPE, IMG_SHAPE])<br/><br/>    mask = tf.io.read_file(mask_filepath)<br/>    mask = tf.io.decode_png(mask, channels=1)<br/>    mask = tf.image.resize(mask, [IMG_SHAPE, IMG_SHAPE])<br/><br/>    img, mask = scale_down(img, mask)<br/><br/>    return img, mask<br/><br/># shuffle the paths and prepare train-test split<br/>input_img_paths, annotation_img_paths = shuffle(input_img_paths, annotation_img_paths, random_state=42)<br/>input_img_paths_train, annotation_img_paths_train = input_img_paths[: -1000], annotation_img_paths[: -1000]<br/>input_img_paths_test, annotation_img_paths_test = input_img_paths[-1000:], annotation_img_paths[-1000:]<br/><br/>trainloader = tf.data.Dataset.from_tensor_slices((input_img_paths_train, annotation_img_paths_train))<br/>testloader = tf.data.Dataset.from_tensor_slices((input_img_paths_test, annotation_img_paths_test))<br/><br/>trainloader = (<br/>    trainloader<br/>    .shuffle(1024)<br/>    .map(load_and_preprocess, num_parallel_calls=AUTO)<br/>    .batch(BATCH_SIZE)<br/>    .prefetch(AUTO)<br/>)<br/><br/>testloader = (<br/>    testloader<br/>    .map(load_and_preprocess, num_parallel_calls=AUTO)<br/>    .batch(BATCH_SIZE)<br/>    .prefetch(AUTO)<br/>)</span></pre><h1 id="6dde" class="lc kg iq bd kh ld le lf kk lg lh li kn jw lj jx kr jz lk ka kv kc ll kd kz lm bi translated">模型</h1><p id="5fc2" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv ko lw lx ly ks lz ma mb kw mc md me mf ij bi translated">这里使用的模型是香草<a class="ae lb" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> UNET建筑</a>。它由编码器和解码器网络组成。这个架构的输入是图像，而输出是逐像素的贴图。您可以通过W &amp; B 报告在<a class="ae lb" href="https://wandb.ai/ayush-thakur/keras-gan/reports/Towards-Deep-Generative-Modeling-with-W-B--Vmlldzo4MDI4Mw" rel="noopener ugc nofollow" target="_blank">深度生成建模中了解更多关于编码器-解码器(Autoencoder)网络的信息。</a></p><p id="23e8" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">类似UNET的架构在自我监督的深度学习任务中很常见，如<a class="ae lb" href="https://www.wandb.com/articles/introduction-to-image-inpainting-with-deep-learning" rel="noopener ugc nofollow" target="_blank">图像修复</a>。</p><p id="3696" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">你可以在这个<a class="ae lb" rel="noopener" target="_blank" href="/unet-line-by-line-explanation-9b191c76baf5">逐行解释</a>中了解更多关于UNET建筑的信息。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/4c18c4c37b8f56fb8c304351503518e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DtGOVs833TJOtWEbLTBoOg.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图3 :典型的UNET建筑。(<a class="ae lb" rel="noopener" target="_blank" href="/unet-line-by-line-explanation-9b191c76baf5">来源</a>)</p></figure><p id="2236" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">下面显示的代码片段构建了我们的语义分割模型架构。</p><pre class="mm mn mo mp gt nn no np nq aw nr bi"><span id="8b3e" class="kf kg iq no b gy ns nt l nu nv">class SegmentationModel:<br/>  '''<br/>  Build UNET like model for image inpaining task.<br/>  '''<br/>  def prepare_model(self, OUTPUT_CHANNEL, input_size=(IMG_SHAPE,IMG_SHAPE,3)):<br/>    inputs = Input(input_size)<br/><br/>    # Encoder <br/>    conv1, pool1 = self.__ConvBlock(32, (3,3), (2,2), 'relu', 'same', inputs) <br/>    conv2, pool2 = self.__ConvBlock(64, (3,3), (2,2), 'relu', 'same', pool1)<br/>    conv3, pool3 = self.__ConvBlock(128, (3,3), (2,2), 'relu', 'same', pool2) <br/>    conv4, pool4 = self.__ConvBlock(256, (3,3), (2,2), 'relu', 'same', pool3) <br/>    <br/>    # Decoder<br/>    conv5, up6 = self.__UpConvBlock(512, 256, (3,3), (2,2), (2,2), 'relu', 'same', pool4, conv4)<br/>    conv6, up7 = self.__UpConvBlock(256, 128, (3,3), (2,2), (2,2), 'relu', 'same', up6, conv3)<br/>    conv7, up8 = self.__UpConvBlock(128, 64, (3,3), (2,2), (2,2), 'relu', 'same', up7, conv2)<br/>    conv8, up9 = self.__UpConvBlock(64, 32, (3,3), (2,2), (2,2), 'relu', 'same', up8, conv1)<br/>    <br/>    conv9 = self.__ConvBlock(32, (3,3), (2,2), 'relu', 'same', up9, False)<br/>    <br/>    # Notice OUTPUT_CHANNEL and activation<br/>    outputs = Conv2D(OUTPUT_CHANNEL, (3, 3), activation='softmax', padding='same')(conv9)<br/><br/>    return Model(inputs=[inputs], outputs=[outputs])  <br/><br/>  def __ConvBlock(self, filters, kernel_size, pool_size, activation, padding, connecting_layer, pool_layer=True):<br/>    conv = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(connecting_layer)<br/>    conv = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(conv)<br/>    if pool_layer:<br/>      pool = MaxPooling2D(pool_size)(conv)<br/>      return conv, pool<br/>    else:<br/>      return conv<br/><br/>  def __UpConvBlock(self, filters, up_filters, kernel_size, up_kernel, up_stride, activation, padding, connecting_layer, shared_layer):<br/>    conv = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(connecting_layer)<br/>    conv = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(conv)<br/>    up = Conv2DTranspose(filters=up_filters, kernel_size=up_kernel, strides=up_stride, padding=padding)(conv)<br/>    up = concatenate([up, shared_layer], axis=3)<br/><br/>    return conv, up</span></pre><p id="329f" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated"><strong class="lp ir">注意</strong>对于我们的数据集来说<code class="fe nx ny nz no b">OUTPUT_CHANNEL</code>是3。这是因为有三类像素，如数据集部分所述。考虑我们正在进行多类分类，其中每个像素可以属于三类中的任何一类。</p><p id="99a9" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">还有，<strong class="lp ir">注意</strong>由于是每像素多类分类问题，所以输出激活函数是<code class="fe nx ny nz no b">softmax</code>。</p><pre class="mm mn mo mp gt nn no np nq aw nr bi"><span id="dadf" class="kf kg iq no b gy ns nt l nu nv">OUTPUT_CHANNEL = 3<br/><br/>model = SegmentationModel().prepare_model(OUTPUT_CHANNEL)<br/>model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")</span></pre><p id="bc9d" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">最后用<code class="fe nx ny nz no b">sparse_categorical_crossentropy</code>编译模型。稀疏，因为按像素的遮罩/注释是整数。</p><h1 id="bb7e" class="lc kg iq bd kh ld le lf kk lg lh li kn jw lj jx kr jz lk ka kv kc ll kd kz lm bi translated"><code class="fe nx ny nz no b">SemanticLogger</code>回调-预测的交互式可视化</h1><p id="a8f5" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv ko lw lx ly ks lz ma mb kw mc md me mf ij bi translated">在进行语义分割时，您可以在权重和偏差中交互式地可视化模型的预测。如果您的图像带有用于语义分段的遮罩，您可以记录遮罩并在UI中打开和关闭它们。点击查看官方文档<a class="ae lb" href="https://docs.wandb.com/library/log#images-and-overlays" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="e43c" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">Stacey Svetlichnaya<a class="ae lb" href="https://wandb.ai/stacey" rel="noopener ugc nofollow" target="_blank">的报告</a><a class="ae lb" href="https://wandb.ai/stacey/deep-drive/reports/Image-Masks-for-Semantic-Segmentation--Vmlldzo4MTUwMw" rel="noopener ugc nofollow" target="_blank">语义分割的图像遮罩</a>将带您了解该工具的交互控件。它涵盖了日志图像和遮罩的各种麻烦。</p><p id="1fbf" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">下面显示的代码片段是我们的<code class="fe nx ny nz no b">SemanticLogger</code>回调的助手函数。函数<code class="fe nx ny nz no b">labels</code>返回一个字典，其中<code class="fe nx ny nz no b">key</code>是类值，<code class="fe nx ny nz no b">value</code>是标签。函数<code class="fe nx ny nz no b">wandb_mask</code>以所需的格式返回图像、预测掩码和基本事实掩码。</p><pre class="mm mn mo mp gt nn no np nq aw nr bi"><span id="0657" class="kf kg iq no b gy ns nt l nu nv">segmentation_classes = ['pet', 'pet_outline', 'background']<br/><br/># returns a dictionary of labels<br/>def labels():<br/>  l = {}<br/>  for i, label in enumerate(segmentation_classes):<br/>    l[i] = label<br/>  return l<br/><br/># util function for generating interactive image mask from components<br/>def wandb_mask(bg_img, pred_mask, true_mask):<br/>  return wandb.Image(bg_img, masks={<br/>      "prediction" : {<br/>          "mask_data" : pred_mask, <br/>          "class_labels" : labels()<br/>      },<br/>      "ground truth" : {<br/>          "mask_data" : true_mask, <br/>          "class_labels" : labels()<br/>      }<br/>    }<br/>  )</span></pre><p id="6b4a" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">我们的<code class="fe nx ny nz no b">SemanticLogger</code>是一个定制的Keras回调函数。我们可以将它传递给<code class="fe nx ny nz no b">model.fit</code>来记录我们的模型在一个小型验证集上的预测。权重和偏差将自动覆盖图像上的蒙版。</p><pre class="mm mn mo mp gt nn no np nq aw nr bi"><span id="5266" class="kf kg iq no b gy ns nt l nu nv">class SemanticLogger(tf.keras.callbacks.Callback):<br/>    def __init__(self):<br/>        super(SemanticLogger, self).__init__()<br/>        self.val_images, self.val_masks = next(iter(testloader))<br/><br/>    def on_epoch_end(self, logs, epoch):<br/>        pred_masks = self.model.predict(self.val_images)<br/>        pred_masks = np.argmax(pred_masks, axis=-1)<br/>        # pred_masks = np.expand_dims(pred_masks, axis=-1)<br/><br/>        val_images = tf.image.convert_image_dtype(self.val_images, tf.uint8)<br/>        val_masks = tf.image.convert_image_dtype(self.val_masks, tf.uint8)<br/>        val_masks = tf.squeeze(val_masks, axis=-1)<br/>        <br/>        pred_masks = tf.image.convert_image_dtype(pred_masks, tf.uint8)<br/><br/>        mask_list = []<br/>        for i in range(len(self.val_images)):<br/>          mask_list.append(wandb_mask(val_images[i].numpy(), <br/>                                      pred_masks[i].numpy(), <br/>                                      val_masks[i].numpy()))<br/><br/>        wandb.log({"predictions" : mask_list})</span></pre><p id="ea1a" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">我们很快就会看到结果。</p><h1 id="ec32" class="lc kg iq bd kh ld le lf kk lg lh li kn jw lj jx kr jz lk ka kv kc ll kd kz lm bi translated">结果</h1><p id="ad98" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv ko lw lx ly ks lz ma mb kw mc md me mf ij bi translated">现在到了激动人心的部分。我已经训练了15个纪元的模型。损失和验证损失指标如下图所示。<em class="of">随意训练更长时期的模型，并使用其他超参数。</em></p><div class="og oh gp gr oi oj"><a href="https://colab.research.google.com/drive/1rXV31gdyqEiXCtmSgff-H-VRuOSzv7IH?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd ir gy z fp oo fr fs op fu fw ip bi translated">谷歌联合实验室</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">编辑描述</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">colab.research.google.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox mr oj"/></div></div></a></div><p id="e838" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">培训和验证损失如图<strong class="lp ir">图3 </strong>所示。经过一些时期后，模型开始过度拟合。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oy"><img src="../Images/2f5e976ab5893530f80033e0c59cd473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pIf6-Ay_UXzHlHv4t-_caA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd mx">图4 </strong>:培训和验证损失指标。(<a class="ae lb" href="https://wandb.ai/ayush-thakur/image-segmentation/reports/Image-Segmentation-Using-Keras-and-W-B--VmlldzoyNTE1Njc" rel="noopener ugc nofollow" target="_blank">点击这里查看互动报道。</a>)</p></figure><p id="6d68" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated"><code class="fe nx ny nz no b">SemanticLogger</code>的结果如下所示。<strong class="lp ir">点击下面</strong> <a class="ae lb" href="https://wandb.ai/ayush-thakur/image-segmentation/reports/Image-Segmentation-Using-Keras-and-W-B--VmlldzoyNTE1Njc#Results-7" rel="noopener ugc nofollow" target="_blank"> <strong class="lp ir">媒体面板</strong> </a> <strong class="lp ir">中的⚙️图标(SemanticLogger的结果)查看交互控件</strong>。您可以分别可视化图像和遮罩，并可以选择要可视化的语义类。</p><h1 id="4544" class="lc kg iq bd kh ld le lf kk lg lh li kn jw lj jx kr jz lk ka kv kc ll kd kz lm bi translated">观察</h1><ul class=""><li id="0313" class="my mz iq lp b lq lr lt lu ko oz ks pa kw pb mf nd ne nf ng bi translated">该模型学习很好地预测<code class="fe nx ny nz no b">pet</code>和<code class="fe nx ny nz no b">background</code>类。</li><li id="dd24" class="my mz iq lp b lq nh lt ni ko nj ks nk kw nl mf nd ne nf ng bi translated">我们可以看到模型很难细分<code class="fe nx ny nz no b">pet_outline</code>类。这是因为高等级的不平衡，并且模型没有被正则化以对抗这种不平衡。</li></ul><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi pc"><img src="../Images/6794a02795a377202382e239c603c97e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A6yZSwXxXP73pVLZddGm2g.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd mx">图5 </strong>:语义记录器回调结果。(<a class="ae lb" href="https://wandb.ai/ayush-thakur/image-segmentation/reports/Image-Segmentation-Using-Keras-and-W-B--VmlldzoyNTE1Njc" rel="noopener ugc nofollow" target="_blank">点击此处查看互动报道。</a>)</p></figure><h1 id="3148" class="lc kg iq bd kh ld le lf kk lg lh li kn jw lj jx kr jz lk ka kv kc ll kd kz lm bi translated">结论和最终想法</h1><p id="c3ff" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv ko lw lx ly ks lz ma mb kw mc md me mf ij bi translated">我希望你喜欢这篇关于语义分割的报告。这份报告有两个目的:</p><ul class=""><li id="3e94" class="my mz iq lp b lq mg lt mh ko na ks nb kw nc mf nd ne nf ng bi translated">让感兴趣的人更容易使用语义分割技术。</li><li id="3f3c" class="my mz iq lp b lq nh lt ni ko nj ks nk kw nl mf nd ne nf ng bi translated">展示权重和偏差如何帮助交互式地可视化模型的预测和度量。此外，展示人们可以从这些可视化中得到的观察结果。</li></ul><p id="310b" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">最后，以下是一些值得一读的资源:</p><ul class=""><li id="d74d" class="my mz iq lp b lq mg lt mh ko na ks nb kw nc mf nd ne nf ng bi translated"><a class="ae lb" href="https://www.jeremyjordan.me/semantic-segmentation/" rel="noopener ugc nofollow" target="_blank">语义图像分割概述</a></li><li id="d999" class="my mz iq lp b lq nh lt ni ko nj ks nk kw nl mf nd ne nf ng bi translated"><a class="ae lb" href="https://www.tensorflow.org/tutorials/images/segmentation" rel="noopener ugc nofollow" target="_blank">图像分割</a></li><li id="843b" class="my mz iq lp b lq nh lt ni ko nj ks nk kw nl mf nd ne nf ng bi translated"><a class="ae lb" href="https://wandb.ai/stacey/deep-drive/reports/The-View-from-the-Driver-s-Seat--Vmlldzo1MTg5NQ" rel="noopener ugc nofollow" target="_blank">驾驶座上的视野</a></li></ul><p id="2279" class="pw-post-body-paragraph ln lo iq lp b lq mg jr ls lt mh ju lv ko mi lx ly ks mj ma mb kw mk md me mf ij bi translated">我很想在评论区得到你的反馈。😄</p></div></div>    
</body>
</html>