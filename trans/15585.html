<html>
<head>
<title>Speech Emotion Recognition Using RAVDESS Audio Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于RAVDESS音频数据集的语音情感识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speech-emotion-recognition-using-ravdess-audio-dataset-ce19d162690?source=collection_archive---------5-----------------------#2020-10-27">https://towardsdatascience.com/speech-emotion-recognition-using-ravdess-audio-dataset-ce19d162690?source=collection_archive---------5-----------------------#2020-10-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/fafd55cfde6e908558ab10b284316b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJwImDciI9gYWvJgs_L5MA.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片由<a class="ae kf" href="https://unsplash.com/@tengyart" rel="noopener ugc nofollow" target="_blank">腾雅特</a>在<a class="ae kf" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="b622" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated">通过所有可用的感官，人类可以感觉到他们交流伙伴的情绪状态。这种情绪检测对人类来说是自然的，但对计算机来说是非常困难的任务；虽然他们可以很容易地理解基于内容的信息，但访问内容背后的深度是困难的，这就是语音情感识别(SER)要做的事情。它是通过计算机将各种音频语音文件分类为快乐、悲伤、愤怒、中性等不同情绪的系统。语音情感识别可用于医疗领域或客户呼叫中心等领域。我的目标是使用<a class="ae kf" href="https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上提供的RAVDESS Audio数据集来演示SER。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="e0fa" class="lw lx it ls b gy ly lz l ma mb"># IMPORT NECESSARY LIBRARIES<br/>import librosa<br/>%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>import librosa.display<br/>from IPython.display import Audio<br/>import numpy as np<br/>import tensorflow as tf<br/>from matplotlib.pyplot import specgram<br/>import pandas as pd<br/>from sklearn.metrics import confusion_matrix<br/>import IPython.display as ipd  # To play sound in the notebook<br/>import os # interface with underlying OS that python is running on<br/>import sys<br/>from sklearn.model_selection import StratifiedShuffleSplit<br/>from sklearn.preprocessing import LabelEncoder<br/>import keras<br/>from keras.models import Sequential<br/>from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D<br/>from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization, Dense<br/>from sklearn.model_selection import GridSearchCV<br/>from keras.wrappers.scikit_learn import KerasClassifier<br/>from keras.optimizers import SGD<br/>from keras.regularizers import l2<br/>import seaborn as sns<br/>from keras.callbacks import EarlyStopping, ModelCheckpoint<br/>from keras.utils import to_categorical<br/>from sklearn.metrics import classification_report</span></pre><h1 id="f678" class="mc lx it bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">电子设计自动化(Electronic Design Automation)</h1><p id="2e23" class="pw-post-body-paragraph kg kh it ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">首先，让我们使用IPython从数据集中加载并播放一个样本音频文件。显示和Python的librosa库:</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="c963" class="lw lx it ls b gy ly lz l ma mb"># LOAD IN FILE<br/>x, sr = librosa.load('/Users/murielkosaka/Desktop/capstone_project/audio/audio_speech_actors_01-24/Actor_01/03-01-01-01-01-01-01.wav')</span><span id="84b7" class="lw lx it ls b gy ne lz l ma mb"># PLAY AUDIO FILE<br/>librosa.output.write_wav('ipd.Audio Files/MaleNeutral.wav', x, sr)<br/>Audio(data=x, rate=sr)</span></pre><p id="57a8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们正在看一个男演员用中性语气说话的音频样本。</p><p id="ff95" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来让我们使用<em class="nf"> librosa.display.waveplot </em>来看看这个音频文件的波形图:</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="09ff" class="lw lx it ls b gy ly lz l ma mb"># DISPLAY WAVEPLOT<br/>plt.figure(figsize=(8, 4))<br/>librosa.display.waveplot(x, sr=sr)<br/>plt.title('Waveplot - Male Neutral')<br/>plt.savefig('Waveplot_MaleNeutral.png')</span></pre><figure class="ln lo lp lq gt ju gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/93434eea4d07ae157c4f5883ecd30bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*o4Ur6fpq5DL2sEgCX6YkIg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><p id="5bbc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">波形图描绘了一段时间内的信号幅度包络，看到情绪的整体形状可以帮助确定哪种特征提取方法(MFCC、STFT、对数-梅尔谱图、过零率、谱形心等)。)用于建模。特征提取在建模中很重要，因为它将音频文件转换成模型可以理解的格式。</p><p id="7aca" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在检查了每种情绪样本的波形图后，我决定使用Log-Mel频谱图作为特征提取的方法。我们可以使用<em class="nf">librosa . display . spec show:</em>显示相同样本音频的Log-Mel频谱图</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="a270" class="lw lx it ls b gy ly lz l ma mb"># CREATE LOG MEL SPECTROGRAM<br/>spectrogram = librosa.feature.melspectrogram(y=x, sr=sr, n_mels=128,fmax=8000) <br/>spectrogram = librosa.power_to_db(spectrogram)</span><span id="c2a2" class="lw lx it ls b gy ne lz l ma mb">librosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');<br/>plt.title('Mel Spectrogram - Male Neutral')<br/>plt.savefig('MelSpec_MaleNeutral.png')<br/>plt.colorbar(format='%+2.0f dB');</span></pre><figure class="ln lo lp lq gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/c9eaac6f083aac21f7f903eef0f93e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*-GBl4RSZy-hKym62m2hGZQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><h1 id="f18e" class="mc lx it bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">准备数据</h1><p id="2f60" class="pw-post-body-paragraph kg kh it ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">在建模之前，我通过创建音频文件的目录，然后创建一个函数来提取每个文件的情感标签和性别标签(虽然我只对情感分类感兴趣，但我也提取了性别标签，以防我也决定尝试分类性别)，将数据结构化为Pandas数据帧，然后最后将提取的标签和相关文件路径放入数据帧<em class="nf"> audio_df。</em></p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="532b" class="lw lx it ls b gy ly lz l ma mb"># CREATE FUNCTION TO EXTRACT EMOTION NUMBER, ACTOR AND GENDER LABEL<br/>emotion = []<br/>gender = []<br/>actor = []<br/>file_path = []<br/>for i in actor_folders:<br/>    filename = os.listdir(audio + i) #iterate over Actor folders<br/>    for f in filename: # go through files in Actor folder<br/>        part = f.split('.')[0].split('-')<br/>        emotion.append(int(part[2]))<br/>        actor.append(int(part[6]))<br/>        bg = int(part[6])<br/>        if bg%2 == 0:<br/>            bg = "female"<br/>        else:<br/>            bg = "male"<br/>        gender.append(bg)<br/>        file_path.append(audio + i + '/' + f)</span><span id="a3ef" class="lw lx it ls b gy ne lz l ma mb"># PUT EXTRACTED LABELS WITH FILEPATH INTO DATAFRAME<br/>audio_df = pd.DataFrame(emotion)<br/>audio_df = audio_df.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})<br/>audio_df = pd.concat([pd.DataFrame(gender),audio_df,pd.DataFrame(actor)],axis=1)<br/>audio_df.columns = ['gender','emotion','actor']<br/>audio_df = pd.concat([audio_df,pd.DataFrame(file_path, columns = ['path'])],axis=1)</span></pre><h2 id="848b" class="lw lx it bd md ni nj dn mh nk nl dp ml kr nm nn mp kv no np mt kz nq nr mx ns bi translated">特征抽出</h2><p id="4d6f" class="pw-post-body-paragraph kg kh it ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">接下来，最重要的是，我使用了librosa的<em class="nf">librosa . feature . Mel spectrogram</em>和<em class="nf"> librosa.power_to_db </em>来获得每个音频文件的log-mel频谱图值，然后对频谱图值进行平均，并将数据加载到标记为<em class="nf"> df的新数据帧中。</em></p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="0540" class="lw lx it ls b gy ly lz l ma mb"># ITERATE OVER ALL AUDIO FILES AND EXTRACT LOG MEL SPECTROGRAM MEAN VALUES INTO DF FOR MODELING <br/>df = pd.DataFrame(columns=['mel_spectrogram'])</span><span id="44dc" class="lw lx it ls b gy ne lz l ma mb">counter=0</span><span id="ce0f" class="lw lx it ls b gy ne lz l ma mb">for index,path in enumerate(audio_df.path):<br/>    X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=3,sr=44100,offset=0.5)<br/>    <br/>    #get the mel-scaled spectrogram (ransform both the y-axis (frequency) to log scale, and the “color” axis (amplitude) to Decibels, which is kinda the log scale of amplitudes.)<br/>    spectrogram = librosa.feature.melspectrogram(y=X, sr=sample_rate, n_mels=128,fmax=8000) <br/>    db_spec = librosa.power_to_db(spectrogram)<br/>    #temporally average spectrogram<br/>    log_spectrogram = np.mean(db_spec, axis = 0)<br/>        <br/>    <br/>    df.loc[counter] = [log_spectrogram]<br/>    counter=counter+1</span><span id="249c" class="lw lx it ls b gy ne lz l ma mb">print(len(df))<br/>df.head()</span></pre><p id="a4ad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为这在一列下创建了一个值数组，所以我使用<em class="nf"> pd.concat </em>将该数组转换成一个列表，并与我之前的数据帧<em class="nf"> audio_df </em>连接，并删除必要的列以给出最终的数据帧。</p><figure class="ln lo lp lq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/56075417cbe1bcd912fe0c79525a66a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pK8sa4CdP5YokuXCPkDq_Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><h1 id="ee7c" class="mc lx it bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">数据预处理</h1><p id="7a11" class="pw-post-body-paragraph kg kh it ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">模型的数据预处理分五步进行:</p><p id="f01f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1.训练，测试分割数据</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="1e46" class="lw lx it ls b gy ly lz l ma mb">train,test = train_test_split(df_combined, test_size=0.2, random_state=0,<br/>                               stratify=df_combined[['emotion','gender','actor']])</span><span id="64d3" class="lw lx it ls b gy ne lz l ma mb"><br/>X_train = train.iloc[:, 3:]<br/>y_train = train.iloc[:,:2].drop(columns=['gender'])</span><span id="54cf" class="lw lx it ls b gy ne lz l ma mb">X_test = test.iloc[:,3:]<br/>y_test = test.iloc[:,:2].drop(columns=['gender'])</span></pre><p id="3bec" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.标准化数据-提高模型稳定性和性能</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="5468" class="lw lx it ls b gy ly lz l ma mb"># NORMALIZE DATA<br/>mean = np.mean(X_train, axis=0)<br/>std = np.std(X_train, axis=0)<br/>X_train = (X_train - mean)/std<br/>X_test = (X_test - mean)/std</span></pre><p id="7f71" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.转换为Keras的数组</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="2dbb" class="lw lx it ls b gy ly lz l ma mb">X_train = np.array(X_train)<br/>y_train = np.array(y_train)<br/>X_test = np.array(X_test)<br/>y_test = np.array(y_test)</span></pre><p id="8bc2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.目标变量的一键编码</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="4ef7" class="lw lx it ls b gy ly lz l ma mb"># CNN REQUIRES INPUT AND OUTPUT ARE NUMBERS<br/>lb = LabelEncoder()<br/>y_train = to_categorical(lb.fit_transform(y_train))<br/>y_test = to_categorical(lb.fit_transform(y_test))</span></pre><p id="7c6d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5.重塑数据以包含3D张量</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="7be0" class="lw lx it ls b gy ly lz l ma mb">X_train = X_train[:,:,np.newaxis]<br/>X_test = X_test[:,:,np.newaxis]</span></pre><h2 id="2c29" class="lw lx it bd md ni nj dn mh nk nl dp ml kr nm nn mp kv no np mt kz nq nr mx ns bi translated">基础模型</h2><p id="ffdd" class="pw-post-body-paragraph kg kh it ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">对于基线模型，我决定从简单的虚拟分类器开始，该分类器通过尊重训练数据的类别分布来生成预测，并且具有11.81%的低准确度分数。然后，我尝试了决策树，因为这是一个使用平均log-mel光谱图的多分类问题，并获得了29.17%的准确率。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="16b7" class="lw lx it ls b gy ly lz l ma mb">dummy_clf = DummyClassifier(strategy="stratified")<br/>dummy_clf.fit(X_train, y_train)<br/>DummyClassifier(strategy='stratified')<br/>dummy_clf.predict(X_test)<br/>dummy_clf.score(X_test, y_test)</span><span id="f49f" class="lw lx it ls b gy ne lz l ma mb">clf = tree.DecisionTreeClassifier()<br/>clf = clf.fit(X_train, y_train)<br/>clf.predict(X_test)<br/>clf.score(X_test, y_test)</span></pre><h2 id="372d" class="lw lx it bd md ni nj dn mh nk nl dp ml kr nm nn mp kv no np mt kz nq nr mx ns bi translated">初始模型</h2><p id="8251" class="pw-post-body-paragraph kg kh it ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">对于我的初始模型，我训练了一个具有三个卷积层和一个输出层的1D CNN，并在我的测试集上获得了48%的准确率，这比我的基线决策树略好(*在此插入悲伤表情*)。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="01cf" class="lw lx it ls b gy ly lz l ma mb"># BUILD 1D CNN LAYERS<br/>model = Sequential()<br/>model.add(Conv1D(64, kernel_size=(10), activation='relu', input_shape=(X_train.shape[1],1)))<br/>model.add(Conv1D(128, kernel_size=(10),activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))<br/>model.add(MaxPooling1D(pool_size=(8)))<br/>model.add(Dropout(0.4))<br/>model.add(Conv1D(128, kernel_size=(10),activation='relu'))<br/>model.add(MaxPooling1D(pool_size=(8)))<br/>model.add(Dropout(0.4))<br/>model.add(Flatten())<br/>model.add(Dense(256, activation='relu'))<br/>model.add(Dropout(0.4))<br/>model.add(Dense(8, activation='sigmoid'))<br/>opt = keras.optimizers.Adam(lr=0.0001)<br/>model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])<br/>model.summary()</span></pre><div class="ln lo lp lq gt ab cb"><figure class="nu ju nv nw nx ny nz paragraph-image"><img src="../Images/fd0e483d3bf893dbdbfe9d892b30fd0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*xAniE2KDM7DLUJQo_r8bbA.png"/></figure><figure class="nu ju nv nw nx ny nz paragraph-image"><img src="../Images/c26e7663a02ffcc42e172cd650fe9918.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*-OFVM4fIJIGqnQLNy1hoVw.png"/><p class="kb kc gj gh gi kd ke bd b be z dk oa di ob oc translated">作者图片</p></figure></div><h2 id="9bc1" class="lw lx it bd md ni nj dn mh nk nl dp ml kr nm nn mp kv no np mt kz nq nr mx ns bi translated">数据扩充</h2><p id="253a" class="pw-post-body-paragraph kg kh it ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">为了提高我的初始模型的可推广性，我探索了数据增强，但是我没有增强音频文件的图像，而是增强了音频文件本身。利用余金乐在<a class="ae kf" href="https://www.kaggle.com/ejlok1/audio-emotion-part-5-data-augmentation" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上提供的自定义功能，我给原始音频文件添加了噪音、拉伸、速度和音高。音频文件的数据扩充也可以使用Numpy和Librosa来完成，本文<a class="ae kf" href="https://medium.com/@makcedward/data-augmentation-for-audio-76912b01fdf6" rel="noopener">将对此进行探讨。</a></p><p id="0a20" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的代码中可以看出，将数据扩充应用于音频文件并使用特征提取方法(也使用log-mel频谱图)，产生了四个数据帧。这四个数据帧以与数据准备步骤相似的方式组合，并遵循与上述相同的预处理步骤。与我的初始模型的1，440幅图像相比，数据扩充方法产生了5，760幅图像的大得多的训练集。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="7b38" class="lw lx it ls b gy ly lz l ma mb"># BUILD 1D CNN LAYERS<br/>model = Sequential()<br/>model.add(Conv1D(64, kernel_size=(20), activation='relu', input_shape=(X_train.shape[1],1)))<br/>model.add(Conv1D(128, kernel_size=(20),activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))<br/>model.add(MaxPooling1D(pool_size=(8)))<br/>model.add(Dropout(0.4))<br/>model.add(Conv1D(128, kernel_size=(20),activation='relu'))<br/>model.add(MaxPooling1D(pool_size=(8)))<br/>model.add(Dropout(0.4))<br/>model.add(Flatten())<br/>model.add(Dense(256, activation='relu'))<br/>model.add(Dropout(0.4))<br/>model.add(Dense(8, activation='softmax'))<br/>model.summary()<br/>opt = keras.optimizers.Adam(lr=0.0001)</span></pre><p id="9051" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练一个具有三个卷积层和一个输出层的1D CNN得到了略高的58%的准确度分数(*在此插入另一张悲伤的脸*)。</p><div class="ln lo lp lq gt ab cb"><figure class="nu ju nv nw nx ny nz paragraph-image"><img src="../Images/9e1e9e17cca056cf12e9e097f9b9fdf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*l9fKNe1T6hKGKY3XspgbxQ.png"/></figure><figure class="nu ju nv nw nx ny nz paragraph-image"><img src="../Images/637a6ba70b4bd1b1afe2ab508e99f196.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*HdfGjSmRRvxBY6aGSMo40A.png"/><p class="kb kc gj gh gi kd ke bd b be z dk oa di ob oc translated">作者图片</p></figure></div><h1 id="29b0" class="mc lx it bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">后续步骤</h1><p id="ceb5" class="pw-post-body-paragraph kg kh it ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">在接下来的步骤中，我将探索迁移学习来提高模型的性能。感谢您的阅读！:)完整代码可在我的<a class="ae kf" href="https://github.com/mkosaka1/Speech_Emotion_Recognition" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。</p></div></div>    
</body>
</html>