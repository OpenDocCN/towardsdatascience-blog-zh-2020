<html>
<head>
<title>Understanding Linear Regression using the Singular Value Decomposition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用奇异值分解理解线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33?source=collection_archive---------32-----------------------#2020-10-12">https://towardsdatascience.com/understanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33?source=collection_archive---------32-----------------------#2020-10-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9fe3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">介绍奇异值分解以及如何用它来解决线性回归问题</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/252a5d64e12cdaff952f03618b8b04c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2PxxKkra63LAcJecBdGu8g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="aa3a" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">介绍</h1><p id="e19e" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">解释线性回归的博客文章和教育材料非常常见。在大多数情况下，可能是因为大数据和深度学习偏见，这些教育资源中的大多数都采用梯度下降方法来拟合线、平面或超平面以适应高维数据。在本帖中，我们也将讨论如何解决线性回归问题，但是是通过不同的视角。更具体地说，我们将讨论线性代数最基本的应用之一，以及我们如何用它来解决回归问题。是的，我说的是奇异值分解。这种计算工具被用作解决无数问题的基础，包括使用<a class="ae mm" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> PCA </a>进行维度缩减，以及使用线性回归进行统计学习。</p><h1 id="c202" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">线性模型和线性方程组</h1><p id="f0b5" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">通过线性代数的视角，回归问题简化为求解形式为<strong class="ls iu"> Ax = b </strong>的线性方程组。这里，<strong class="ls iu"> A </strong>和<strong class="ls iu"> b </strong>是已知的，<strong class="ls iu"> x </strong>是未知的。我们可以把<strong class="ls iu"> x </strong>当做我们的模型。换句话说，我们想要为<strong class="ls iu"> x </strong>求解系统，因此，<strong class="ls iu"> x </strong>是将<strong class="ls iu"> A </strong>中的观察值与<strong class="ls iu"> b </strong>中的测量值相关联的变量。</p><p id="892a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在这里，<strong class="ls iu"> A </strong>是一个数据矩阵。我们可以认为<strong class="ls iu">和</strong>的行代表同一现象的不同实例。它们可以代表提交给医院的单个患者的记录、正在出售的不同房屋的记录或不同人的脸部照片。作为补充，我们可以将矩阵<strong class="ls iu"> A </strong>的列视为记录了<strong class="ls iu"> A </strong>的行中每个实例的不同特征。在患者医院的例子中，这样的特征可以包括他/她到达医院时的血压，或者患者是否进行过外科手术。</p><p id="1d09" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">另外，注意矩阵<strong class="ls iu">和</strong>可能有不同的形状。首先，<strong class="ls iu"> A </strong>可以是一个方阵。是的，这是非常不可能的(对于我们通常在数据科学中遇到的情况)，但在其他方面是可能的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/fcfa5f4a519874e9f91cadf5333b41ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Qt5aZ-9YVKJ0o0fv.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵<strong class="bd mt">和</strong>可以具有不同的形状。可以平方。它可以又宽又矮，也可以又高又瘦——图片由作者提供</p></figure><p id="ed1f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">第二，<strong class="ls iu"> A </strong>的列数可以多于行数。在这个场景中，<strong class="ls iu"> A </strong>会有一个又短又宽的形状。最后，(这是数据科学中最常见的情况)，矩阵<strong class="ls iu"> A </strong>呈现出又高又瘦的矩阵形式，行数比列数多得多。</p><blockquote class="mu"><p id="fc67" class="mv mw it bd mx my mz na nb nc nd ml dk translated">但是我为什么要关心矩阵A的形状呢？</p></blockquote><p id="403a" class="pw-post-body-paragraph lq lr it ls b lt ne ju lv lw nf jx ly lz ng mb mc md nh mf mg mh ni mj mk ml im bi translated">有趣的是，<strong class="ls iu"> A </strong>的形状将决定线性方程组是否有解，是否有无穷多个解，或者根本没有解。</p><p id="b9ca" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">先说无聊的案子。如果矩阵是平方的(行数等于列数)并且是可逆的，这意味着矩阵<strong class="ls iu"> A </strong>具有满秩(所有列线性无关)，这就很好地解决了问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/845d2cf5f9b4469f04b80943c4d99e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pZ6flnK-mf_neYhs.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">如果矩阵<strong class="bd mt"> A </strong>是平方且可逆的，则方程组有解——作者图片</p></figure><p id="16c2" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">然而，如果矩阵的列数比行数多，我们可能会遇到有无穷多个解的情况。为了形象化这个奇怪的场景，想象一个3 × 6的矩阵，即3行6列。我们可以认为它有一个3D空间和6个不同的向量，我们可以用它们来跨越3D空间。然而，要跨越一个3D空间，我们只需要3个线性无关的向量，但我们有6个！这留下了3个相关向量，可用于制定无限多的解决方案。</p><p id="3f7d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">最后，通过类比，如果我们有一个行数比列数多的矩阵<strong class="ls iu"> A </strong>，我们可以将它视为试图用比我们需要的更少的向量来跨越一个非常高维的空间。例如，想象一个6行2列的矩阵。这里，我们有一个6D空间，但是我们只有两个向量来跨越它。不管我们怎么努力，在最好的情况下，我们只能在6D上跨越一个平面。这是至关重要的，因为如果向量<strong class="ls iu"> b </strong>在<strong class="ls iu"> A </strong>的列空间中，我们只有<strong class="ls iu"> Ax = b </strong>的解。但是在这里，<strong class="ls iu"> A </strong>的列空间跨越了一个更大的6D空间上的2D子空间(一个平面)。这使得向量<strong class="ls iu"> b </strong>在由<strong class="ls iu"> A </strong>的列所跨越的子空间中的概率变得不可能。</p><p id="b7e1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">为了形象化这种可能性有多大，想象一个3D空间和一个由两个向量构成的子空间(一个3D平面)。现在，想象你随机选择3个值。这将给你一个三维空间的点。现在，问问你自己:<em class="nk">我随机选择的点在平面上的概率是多少？</em></p><p id="2e0e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">尽管如此，在我们没有一个线性方程组<strong class="ls iu"> Ax = b </strong>的解的情况下(或者我们有无穷多个解)，我们仍然想尽力而为。为此，我们需要找到最佳近似解。这就是奇异值分解发挥作用的地方。</p><h1 id="fb70" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">SVD的简短介绍</h1><p id="9b67" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">奇异值分解或SVD的主要思想是，我们可以将任意形状的矩阵<strong class="ls iu"> A </strong>分解成3个其他矩阵的乘积。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/a8755afb3b47ecc8f877814bc4a09573.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Mv0GfpOSD5qewI6O.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">给定任意形状的矩阵，奇异值分解将<strong class="bd mt"> A </strong>分解成3个矩阵的乘积:<strong class="bd mt"> U，σ</strong>，<strong class="bd mt"> Vᵀ </strong> — <strong class="bd mt"> </strong>作者图片</p></figure><p id="d8d7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里，<strong class="ls iu"> U </strong>是一个<em class="nk"> m × m </em>的方阵，<strong class="ls iu">σ</strong>是一个形状为<em class="nk"> m × n </em>的矩形矩阵，<strong class="ls iu"> Vᵀ </strong>是一个形状为<em class="nk"> n × n </em>的方阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/44fc2ac50a97cd7b6c3b08e22f8bb093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KjME62i_M2XOt4Xm.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">完整的奇异值分解矩阵——作者图片</p></figure><p id="eead" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">矩阵<strong class="ls iu"> U </strong>和<strong class="ls iu"> Vᵀ </strong>有一个非常特殊的性质。它们是<strong class="ls iu"> <em class="nk">酉矩阵</em> </strong>。拥有像<strong class="ls iu"> U </strong>和<strong class="ls iu"> Vᵀ </strong>这样的酉矩阵的一个主要好处是，如果我们将这些矩阵中的一个乘以它的转置(或者反过来)，结果等于单位矩阵。</p><p id="0884" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">另一方面，矩阵<strong class="ls iu">σ</strong>是对角的，它存储按相关性排序的非负奇异值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/68e9200438874b653103b62d5353b1af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tsLuAPY0P3yxPlY1.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">酉矩阵的性质——作者图片</p></figure><p id="515b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">注意，由于<strong class="ls iu">σ</strong>矩阵是对角的，只有第一<em class="nk"> n </em>行对角值值得保留。事实上，<strong class="ls iu">σ</strong>的最后<em class="nk"> n </em>行都是用0填充的。为此，通常只保留<strong class="ls iu">σ</strong>的第一个<em class="nk"> r × r </em>非负对角线值，以及相应的<em class="nk"> r </em>列和<strong class="ls iu"> U </strong>和<strong class="ls iu"> Vᵀ </strong>行。注意<em class="nk"> r = min(m，n) </em>。这通常被称为经济(或紧凑)SVD，从这一点开始，我们将假设矩阵<strong class="ls iu"> U </strong>、<strong class="ls iu">σ</strong>和<strong class="ls iu"> Vᵀ </strong>来自经济过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/6d4c9f28faa8d441a0c021373fba4331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*27R8qVZmPpcm9Aj3.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">经济奇异值分解数据矩阵—作者图片</p></figure><p id="d462" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">值得注意的是，经济SVD产生矩阵<strong class="ls iu"> U </strong>和<strong class="ls iu">σ</strong>的形状变化(如果<strong class="ls iu">σ</strong>的对角线值之一为零，<strong class="ls iu"> Vᵀ </strong>也会发生形状变化)。如果<strong class="ls iu">σ</strong>的对角线值都是正的，因此<em class="nk"> r = n </em>，我们丢弃<strong class="ls iu"> U </strong>矩阵的右半部分(<strong class="ls iu"> U </strong>的正交补)，这给出了<strong class="ls iu"> U </strong>一个矩形<em class="nk"> m × r </em>的形状。更关键的是，<strong class="ls iu"> U </strong>，可能还有<strong class="ls iu"> Vᵀ </strong>，现在都是半酉矩阵，也就是说只有<strong class="ls iu"> UᵀU = VᵀV = I </strong>。</p><p id="d51b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">奇异值分解提供了一个基础，允许我们根据低秩矩阵近似来重构输入信号。让我说得更清楚些。如果我们将<strong class="ls iu"> U </strong>的每一列与<strong class="ls iu"> Vᵀ </strong>的相应行组合，并通过相应的<strong class="ls iu"> σ </strong>值缩放所得矩阵，我们将得到<strong class="ls iu"> A </strong>的最佳秩1近似，以最小二乘法表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/8b874d068d673f5a27a72d1096adceb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SL65BuO6v1OFQD9s.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输入矩阵的秩n近似(一个图像)<strong class="bd mt">一个</strong>对于不同的秩-图像由作者提供</p></figure><p id="1f2c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">并且当我们继续将<strong class="ls iu"> U </strong>的列与<strong class="ls iu"> Vᵀ </strong>的行组合时，通过相应的<strong class="ls iu"> σ </strong>进行缩放，我们得到数据矩阵<strong class="ls iu"> A </strong>的下一个最佳秩I近似。事实上，这是奇异值分解的另一个优秀应用——数据压缩。但那是另一篇文章的主题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/e21a45945f7d37da59f5b5bb6e6f1ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*PcF7B3PJp_LVZ3KfaPbaJg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用奇异值分解的<strong class="bd mt"> U </strong>、<strong class="bd mt">σ、</strong>和<strong class="bd mt"> Vᵀ </strong>矩阵进行图像重建。使用前256个奇异值，我们得到原始输入图像的最佳秩-256近似(它可以在视觉上完美地重建原始图像)-作者图像</p></figure><p id="0579" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">正如我们之前说过的，使用非方阵<strong class="ls iu"> A </strong>的问题是我们不能求逆。这就是为什么我们不能像解方阵<strong class="ls iu"> A </strong>那样解方程组的主要原因。然而，如果我们不能将矩阵<strong class="ls iu"> A </strong>求逆，我邀请你问自己以下问题。</p><blockquote class="mu"><p id="6145" class="mv mw it bd mx my mz na nb nc nd ml dk translated">什么是最佳矩阵<strong class="ak"> A⁺ </strong>，当乘以<strong class="ak"> A </strong>时，会尽可能接近单位矩阵<strong class="ak"> I </strong>？</p></blockquote><p id="8863" class="pw-post-body-paragraph lq lr it ls b lt ne ju lv lw nf jx ly lz ng mb mc md nh mf mg mh ni mj mk ml im bi translated">这个问题的答案解决了当方程组有无穷多解或无解时，寻找最佳可能解的问题。幸运的是，答案也在SVD中。</p><p id="2351" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果我们知道SVD总是存在(对于任何形状的矩阵)，并且通过组合<strong class="ls iu"> U </strong>的列、<strong class="ls iu"> Vᵀ </strong>的行和奇异值<strong class="ls iu"> σ </strong>，我们可以几乎完美地重构原始输入矩阵，如果我们试图对SVD求逆会发生什么呢？</p><p id="2b55" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我就不多说了吧。原来近似解决问题<strong class="ls iu">a⁺a</strong>T10】≈I的最佳矩阵<strong class="ls iu"> A⁺ </strong>是SVD的逆。换句话说，A⁻的最佳近似值是SVD⁻的最佳近似值。让我们跟着数学走。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/aeb918528242a8d87a2f146b073c45ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RkVyp3tErxlOXIdC.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过奇异值分解求<strong class="bd mt"> A </strong>的伪逆。伪逆的<strong class="bd mt"> A⁺ </strong>是我们能得到的最接近作者不存在的<strong class="bd mt"> A⁻ — </strong>的图像</p></figure><p id="bb7f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">首先，我们计算<strong class="ls iu"> A </strong>的奇异值分解，得到矩阵<strong class="ls iu"> USVᵀ </strong>。为了求解<strong class="ls iu"> x </strong>的方程组，我需要将方程的两边乘以SVD矩阵的逆矩阵。幸运的是，现在很容易对3个SVD矩阵中的每一个求逆。为了求3个矩阵<strong class="ls iu"> USVᵀ </strong>的乘积的逆，我取逆矩阵的乘积！</p><p id="8643" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在对<strong class="ls iu"> USVᵀ </strong>矩阵求逆后，如果我们仔细观察左边，我们可以看到大多数矩阵会疯狂地抵消，给我们留下最佳近似解<strong class="ls iu"> x̂ </strong>。注意，由于矩阵<strong class="ls iu"> U </strong>是半酉矩阵，只有<strong class="ls iu"> UᵀU = I </strong>成立。此外，如果(并且我们假设)所有奇异值都是非负的，那么V <strong class="ls iu"> ᵀ </strong>连续成为酉矩阵。因此，为了反转<strong class="ls iu"> U </strong>和<strong class="ls iu"> Vᵀ </strong>，我们只需将每一个乘以它们的转置，即<strong class="ls iu"> UᵀU = I </strong>和<strong class="ls iu"> VVᵀ = I </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/6a45dd7dcae59e924585a06f2feef45d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nSR6BOceeFH5ZkbO.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">找到作者的<strong class="bd mt"> b </strong> — <strong class="bd mt"> </strong>图像的投影</p></figure><p id="d814" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果我们更进一步，将我们的最佳解<strong class="ls iu"> x̂ </strong>代入<strong class="ls iu"> Ax̂ </strong>，我们将看到大多数矩阵也相互抵消，直到我们到达<strong class="ls iu"> UUᵀ </strong>。我们之前说过，<strong class="ls iu"> U </strong>是半酉矩阵，<strong class="ls iu"> UUᵀ </strong>不是单位矩阵。相反，<strong class="ls iu"> UUᵀ </strong>是<strong class="ls iu"> b </strong>在<strong class="ls iu"> U </strong>的列(因此是<strong class="ls iu"> A </strong>的列)所生成的子空间上的投影，这是最小二乘意义上的最佳近似解，即我们找到了最小二乘解<strong class="ls iu"> x̂ </strong> = <em class="nk">最小值</em>(<strong class="ls iu">ax-b</strong>‖₂).</p><p id="09f5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">注意，如果<strong class="ls iu"> A </strong>的列数多于行数，并且有无穷多个解，那么奇异值分解会选择具有最小2范数的解，即<strong class="ls iu"> x̂ </strong> = <em class="nk">最小值</em>(<strong class="ls iu">x̂</strong>‖₂).</p><h1 id="975c" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">SDV线性回归</h1><p id="a57c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">一旦我们建立了所需的SVD术语，我们就可以用它来寻找现实世界问题的近似解决方案。在这个例子中，我将使用<a class="ae mm" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html" rel="noopener ugc nofollow" target="_blank">波士顿房价数据集</a>。房价数据矩阵<strong class="ls iu"> A </strong>包含506行(代表单个房屋)和13列(每一列描述房屋的不同特征)。这13项功能包括:</p><ul class=""><li id="6dcc" class="ns nt it ls b lt mn lw mo lz nu md nv mh nw ml nx ny nz oa bi translated">按城镇分列的人均犯罪率</li><li id="8e85" class="ns nt it ls b lt ob lw oc lz od md oe mh of ml nx ny nz oa bi translated">每个住宅的平均房间数</li><li id="592f" class="ns nt it ls b lt ob lw oc lz od md oe mh of ml nx ny nz oa bi translated">到五个波士顿就业中心的加权距离</li></ul><p id="c970" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">你可以在这里看到<a class="ae mm" href="https://scikit-learn.org/stable/datasets/index.html#boston-dataset" rel="noopener ugc nofollow" target="_blank">的完整描述</a>。</p><p id="5e5a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们希望预测<strong class="ls iu">中值房价为</strong> $1000。这些测量值是从5到50的真实值，它们代表我们的方程组<strong class="ls iu"> Ax = b </strong>中的<strong class="ls iu"> b </strong>向量。</p><p id="ab97" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">通常，矩阵的行数比列数多得多。这意味着我们不能将<strong class="ls iu"> A </strong>求反来找到<strong class="ls iu"> Ax = b </strong>的解。此外，它大大降低了找到解决方案的可能性。事实上，只有当<strong class="ls iu"> b </strong>是<strong class="ls iu"> A </strong>的列的线性组合时，这样的解决方案才是可能的。然而，使用SVD，我们将能够导出伪逆<strong class="ls iu"> A⁺ </strong>，以找到最小二乘法的最佳近似解— <strong class="ls iu"> <em class="nk">，这是向量b到由a的列所跨越的子空间上的投影。</em> </strong></p><p id="441a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">代码非常简单，结果非常好。事实上，它们是线性模型的最佳选择。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="0a2a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">简单说明一下，请看上面python代码的第9行。在这一行，我在数据矩阵<strong class="ls iu"> A </strong>中添加了一列全1。该列将允许线性模型学习偏置向量，该偏置向量将向超平面添加偏移，使得它不穿过原点。</p><p id="a6c7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">看看下面的训练和测试结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/e3f3cac427ac3c8ac0a1758e41a15cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nTKFAmQGTLrrOYAY.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于奇异值分解的线性模型的训练预测—图片由作者提供</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/aeef38ffab760ad05c601debe7b1f7c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*w3zn9ZcXojKGvfEw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于奇异值分解的线性模型的测试预测—图片由作者提供</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/8ddb4c8be94c5e8481cfebd146467adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_OWGetyjbpJx7Kdb.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标/预测图允许我们直观地评估目标值和模型预测之间的相关性。非常准确的预测使点非常接近虚线-图片由作者提供</p></figure><p id="bfae" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">感谢阅读！</strong></p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="6b21" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><em class="nk">原载于2020年10月12日</em><a class="ae mm" href="https://sthalles.github.io/svd-for-regression/" rel="noopener ugc nofollow" target="_blank"><em class="nk">https://sthalles . github . io</em></a><em class="nk">。</em></p></div></div>    
</body>
</html>