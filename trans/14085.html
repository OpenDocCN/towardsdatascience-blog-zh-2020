<html>
<head>
<title>Getting started with TensorFlow Serving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow服务入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-tensorflow-serving-b03c130bdb5c?source=collection_archive---------25-----------------------#2020-09-28">https://towardsdatascience.com/getting-started-with-tensorflow-serving-b03c130bdb5c?source=collection_archive---------25-----------------------#2020-09-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2373" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">将深度学习模型部署到生产中的简单高效的方法</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/f80a537ba0af22dd7d0eae72788c7f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WSEePtBQ5Zwgfupi2dQTAw.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">由<a class="ae kz" href="https://unsplash.com/@ianjbattaglia?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">伊恩·巴塔格利亚</a>在<a class="ae kz" href="https://unsplash.com/s/photos/server?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="0cb5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">TensorFlow服务是TensorFlow Extended(TFX)的一部分，它使将您的机器学习模型部署到服务器比以往任何时候都更加舒适。在Google发布TensorFlow服务之前，您的模型必须使用Docker部署到生产中。使用Docker来部署您的模型是乏味的、耗时的，并且容易出现许多错误。TensorFlow Serving为我们提供了一个API，可以使用HTTP请求调用该API在服务器上运行推理。在这篇博客中，我们将提供一个情感识别模型，并通过它了解TensorFlow服务的基础。</p><blockquote class="lw lx ly"><p id="b653" class="la lb lz lc b ld le ju lf lg lh jx li ma lk ll lm mb lo lp lq mc ls lt lu lv im bi translated">PS:我不会在这个博客中解释关于训练情绪识别模型的事情。我只会说为模特服务。你可以在这里找到训练情绪识别模型<a class="ae kz" href="https://gist.github.com/CleanPegasus/a822445bf7c48b1d601e59495e5ee304" rel="noopener ugc nofollow" target="_blank">的要领。您可以在这里</a>找到本地运行模型的存储库<a class="ae kz" href="https://github.com/CleanPegasus/Emotion-Recognition" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><p id="9328" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">为什么要为模特服务？</strong></p><p id="e69e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一旦您训练了您的模型，它必须被部署到生产中以便可以使用。可以使用各种方法来部署模型，比如使用TFlite在手机上本地部署，使用TFjs在网站上部署，创建docker容器在云上部署模型，等等。TensorFlow服务优于其他方法，原因如下。</p><ol class=""><li id="07d8" class="md me it lc b ld le lg lh lj mf ln mg lr mh lv mi mj mk ml bi translated">使用TensorFlow Serving比使用Docker更容易部署您的模型，它可以节省您的时间并防止不必要的错误。</li><li id="b3e4" class="md me it lc b ld mm lg mn lj mo ln mp lr mq lv mi mj mk ml bi translated">与TFlite或TFjs相比，更容易管理模型的不同版本。</li><li id="520c" class="md me it lc b ld mm lg mn lj mo ln mp lr mq lv mi mj mk ml bi translated">当模型被更新时，所有的客户端将使用相同版本的模型，因此结果将是一致的。</li><li id="bd7f" class="md me it lc b ld mm lg mn lj mo ln mp lr mq lv mi mj mk ml bi translated">由于模型将在服务器上运行，您可以使用强大的计算资源，如GPU或TPUs来更快地运行推理。</li><li id="a428" class="md me it lc b ld mm lg mn lj mo ln mp lr mq lv mi mj mk ml bi translated">由于该模型由一个API提供服务，因此它可以由TensorFlow不支持的不同编程语言使用。</li></ol><p id="2f21" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">安装张量流服务</strong></p><p id="dabe" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">TensorFlow服务可以使用Docker、apt package installer for Linux或pip package manager进行安装。在这篇博客中，我们将重点介绍如何使用apt和pip来安装TensorFlow服务。</p><p id="724a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在您的生产环境的终端中运行下面给出的代码，以安装TensorFlow服务。您可以使用apt软件包安装程序或pip软件包管理器。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mr ms l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">安装TensorFlow服务</p></figure><p id="a25f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">建立和服务您的模型。</strong></p><p id="13e5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一旦您训练、测试和验证了您的模型，就该为您的模型服务了。</p><blockquote class="lw lx ly"><p id="2b67" class="la lb lz lc b ld le ju lf lg lh jx li ma lk ll lm mb lo lp lq mc ls lt lu lv im bi translated">步骤1:指定模型的版本号，并使用tf.keras.models.save_model()函数或model.save()函数将模型保存到tmp目录中。</p></blockquote><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mr ms l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">保存模型</p></figure><blockquote class="lw lx ly"><p id="c51d" class="la lb lz lc b ld le ju lf lg lh jx li ma lk ll lm mb lo lp lq mc ls lt lu lv im bi translated">步骤2:模型将通过bash命令提供服务。为了让bash命令访问模型目录，需要将模型目录添加到环境中。</p></blockquote><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mr ms l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">向环境中添加模型目录</p></figure><blockquote class="lw lx ly"><p id="c4e1" class="la lb lz lc b ld le ju lf lg lh jx li ma lk ll lm mb lo lp lq mc ls lt lu lv im bi translated">第3步:既然模型已经保存，就可以开始服务了。运行下面给出的bash命令来服务模型。</p></blockquote><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mr ms l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">为模型服务</p></figure><blockquote class="lw lx ly"><p id="d6f1" class="la lb lz lc b ld le ju lf lg lh jx li ma lk ll lm mb lo lp lq mc ls lt lu lv im bi translated">第4步:既然已经为您的模型提供了服务，那么您可以使用tail命令来检查server.log文件。如果您在日志中发现“状态:成功:正常”行，则您的模型已被成功服务。</p></blockquote><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mt"><img src="../Images/7ad04ecd30ce0dff0f42a50d54a103ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b0MjGIyjUfStmv2Ek48ylA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">检查状态:成功:正常日志</p></figure><p id="f9bf" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">使用HTTP请求在模型上运行推理</strong></p><p id="777b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一旦您的模型被提供，就可以通过您定义的端口访问它。你的模型可以在任何地方使用；像GCE、AWS lambda、AWS EC2、Google Colab、您本地计算机等云计算引擎。请注意，当您将模型部署到生产环境中时，在Colab或您的本地计算机上提供它没有多大意义。为了简单起见，我将在Google Colab上部署该模型。</p><p id="bd6d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">您可以使用HTTP请求在服务模型上运行推理。发出HTTP请求后，服务器会将数据传递给模型，获取响应并将其发送回客户端。API端点的输入是一个JSON，其中嵌入了预处理过的图像。您可以使用<strong class="lc iu">请求</strong>库向API发出HTTP请求。</p><p id="7a2e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在调用API之前，必须对输入图像进行预处理。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mr ms l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图像预处理</p></figure><p id="28f5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">图像经过预处理后，将图像嵌入到JSON中。这个JSON对象将是发送到API端点的数据。JSON数据将有两项，“签名名”和“实例”“实例”项将包含列表形式的图像数据。</p><p id="d967" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">指定API请求的头，告诉端点JSON文件正在作为输入传递。使用POST方法从API端点请求，并将URL、JSON数据和头作为参数。API将返回图像的预测。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mr ms l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">来自服务模型的预测</p></figure><p id="644f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这是模型的输出样本。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mu"><img src="../Images/a07a5773b8ceb0fb049eebc4ae95456c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WhT7XpeOo9AheVk6dVcGfw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">输出</p></figure><p id="f949" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">结论</strong></p><p id="6955" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">TensorFlow服务使深度学习模型部署到生产中变得前所未有的简单。通过在模型中使用预处理图像的lambda层，可以避免对输入图像进行预处理。这使得调用API更加容易。</p></div></div>    
</body>
</html>