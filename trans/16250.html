<html>
<head>
<title>Fundamentals of Supervised Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督情感分析的基础</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fundamentals-of-supervised-sentiment-analysis-1975b5b54108?source=collection_archive---------26-----------------------#2020-11-09">https://towardsdatascience.com/fundamentals-of-supervised-sentiment-analysis-1975b5b54108?source=collection_archive---------26-----------------------#2020-11-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e511" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">NLP预处理、BoW、TF-IDF、朴素贝叶斯、SVM、Spacy、Shapely、LSTM等等</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5e73095f68779c9d1764c1572caec09e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VN97gAG6Io8LbERY"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯·j·戴维斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="9349" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将解释一些基本的机器学习方法来对推特情感进行分类，以及如何在Python中运行它们。</p><h1 id="621a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">情感分析</h1><p id="c353" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">情感分析用于识别数据的影响或情绪(积极的、消极的或中性的)。对于一个企业来说，这是一个简单的方法来确定客户对产品或服务的反应，并迅速发现任何可能需要立即关注的情绪变化。解决这个问题的最基本的方法是使用监督学习。我们可以让真正的人来确定和标记我们数据的情感，并像对待文本分类问题一样对待它。这正是我将在这篇文章中讨论的，并且将在后面的文章中重新讨论这个话题，讨论无监督的方法。</p><h1 id="1dd9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据</h1><ol class=""><li id="afcf" class="ms mt it lb b lc mn lf mo li mu lm mv lq mw lu mx my mz na bi translated">你可能会在<a class="ae ky" href="https://data.world/crowdflower/brands-and-product-emotions" rel="noopener ugc nofollow" target="_blank"> data.world </a>上找到一些人为标注的推文数据。数据包含超过8000条被标记为积极、消极、中立或未知(“我不能说”)的推文。</li><li id="fa4c" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">斯坦福大学的一个团队提供了训练数据。此数据集的标签被自动标注。</li></ol></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="1eae" class="lv lw it bd lx ly nn ma mb mc no me mf jz np ka mh kc nq kd mj kf nr kg ml mm bi translated">自然语言处理预处理</h1><p id="a085" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在清理数据(删除带有缺失文本的观察结果、删除“未知”类、删除RTs)并将我们的数据分成训练集、测试集和验证集之后，我们需要预处理文本，以便它们可以在我们的分析中正确量化。我将一个一个地检查它们。</p><h2 id="3bd5" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">1.仅保留ASCII字符</h2><p id="08bd" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">不同的编码会导致数据中出现一些奇怪的字符。因此，首先我们将确保我们只保留ASCII字符。下面的代码将过滤掉所有非ASCII字符。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="f9b3" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">def </strong>ascii_only(str_):<br/>    <strong class="of iu">return</strong> str_.encode("ascii", "ignore").decode()</span></pre><h2 id="62bf" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">2.全部小写</h2><p id="fa67" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这个很简单。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="8ede" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">def</strong> make_lower(str_):<br/>    <strong class="of iu">return</strong> str_.lower()</span></pre><h2 id="9077" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">3.删除HTML符号、提及和链接</h2><p id="4e21" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">由于编码错误，一些推文包含HTML符号，如<code class="fe on oo op of b">&amp;nbsp;</code>。在我们删除标点符号之前，我们将首先删除这些单词，这样我们就不会在删除标点符号后留下胡言乱语。我们要删除的其他字母单词是用户名(例如@stereopickle)和超链接(在这个数据集中写成{link})。我们将使用regex来完成这项工作。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="b063" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">import</strong> <strong class="of iu">re <br/>def</strong> remove_nonwords(str_):<br/>    <strong class="of iu">return</strong> re.sub("[^A-Za-z0-9 ]\w+[^A-Za-z0-9]*", ' ', str_)</span></pre><p id="5285" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个表达式的意思是用空格替换所有不以字母数字字符开头的单词。这些单词可能以非字母数字字符结尾，也可能不以非字母数字字符结尾。</p><h2 id="5311" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">4.删除品牌词</h2><p id="670f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这是特定于该数据集的步骤。因为我们的数据集包含关于产品的推文，所以有很多对实际品牌和产品名称的引用。我们不希望不同品牌或产品的总体情绪分布影响我们的模型，因此我们将删除其中一些。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="98bc" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">def</strong> remove_brandwords(str_):<br/>    p = '''#?(iphone|ipad|sxsw|hcsm|google|apple|cisco|austin|<br/>    atari|intel|mac|pc|blackberry|android|linux|ubuntu)[a-z0-9]*'''<br/>    <strong class="of iu">return</strong> re.sub(p, ' ', str_)</span></pre><h2 id="22a7" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">5.删除标点符号</h2><p id="a1a5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">又一次简单的清洗。请注意，为了简单起见，我在这里删除了所有标点符号，但在现实中，许多推文可能会使用标点符号来表达情绪，例如:)和:(。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="69d6" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">import</strong> <strong class="of iu">string</strong><br/>punctuations = string.punctuation<br/>punctuations = punctuations + '�' + string.digits<br/><br/><strong class="of iu">def</strong> remove_punctuations(str_, punctuations):<br/>    table_ = str.maketrans('', '', punctuations)<br/>    <strong class="of iu">return</strong> str_.translate(table_)</span></pre><p id="925f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对上面的方法感到好奇，请看看我的另一篇关于如何有效去除标点符号的帖子。</p><div class="oq or gp gr os ot"><a rel="noopener follow" target="_blank" href="/how-to-efficiently-remove-punctuations-from-a-string-899ad4a059fb"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd iu gy z fp oy fr fs oz fu fw is bi translated">如何有效地从字符串中删除标点符号</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">Python中清理字符串的8种不同方法</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">towardsdatascience.com</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph ks ot"/></div></div></a></div><h2 id="9265" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">6.词汇化&amp;删除停用词</h2><p id="18f7" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">最后，我们将使用NLTK对我们的词汇表进行词汇化并删除停用词。总之，lemmatizer将把词汇表还原成它的基本形式lemma。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/e4a977340f24861705f5c59413c11757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QYXbCUkL1RtaTkqG-kwz_A.png"/></div></div></figure><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="0fdf" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">from</strong> <strong class="of iu">nltk.stem</strong> <strong class="of iu">import</strong> WordNetLemmatizer<br/><strong class="of iu">from nltk.corpus import</strong> stopwords</span><span id="18e4" class="ns lw it of b gy pj ok l ol om">sw = stopwords.words('english')</span><span id="e33c" class="ns lw it of b gy pj ok l ol om"><strong class="of iu">def </strong>lemmatize(str_, sw):<br/>    wnl = WordNetLemmatizer()<br/>    <strong class="of iu">return</strong> ' '.join([wnl.lemmatize(w) <strong class="of iu">for</strong> w <strong class="of iu">in</strong> x.split() <strong class="of iu">if</strong> w <strong class="of iu">not</strong> <strong class="of iu">in</strong> sw])</span></pre><p id="0368" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里阅读更多关于词汇化或词干化的内容。</p><div class="oq or gp gr os ot"><a href="https://medium.com/swlh/understanding-art-through-art-description-data-part-1-2682e899dfe5" rel="noopener follow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd iu gy z fp oy fr fs oz fu fw is bi translated">通过艺术描述数据理解艺术</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">使用Python预处理艺术描述数据</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">medium.com</p></div></div><div class="pc l"><div class="pk l pe pf pg pc ph ks ot"/></div></div></a></div><h2 id="639e" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">7.特征选择</h2><p id="a9c0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在将上述函数应用到我们的文本数据之后，我们应该有一组非常干净的单词可以使用。但我喜欢在这里添加一个特征选择步骤，因为当每个词汇或二元或三元语法被标记化时，NLP问题往往会以太多的特征结束。</p><p id="452c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，有许多方法可以清除特征，但是我喜欢使用Spacy来组合相似的单词。基本思想是遍历只出现几次的单词，并找到一个具有高相似性(在向量空间中更接近)的现有单词。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pl pm l"/></div></figure><p id="6e81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个自定义函数返回一个字典，将出现频率低的单词作为关键字，将它们的替换词作为值。这一步通常会修复一些拼写错误，并纠正术语化可能遗漏的地方。我们可以使用这个替换词典将单词转换为更频繁出现的对应词，并删除出现次数少于指定次数的单词。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="10d5" class="lv lw it bd lx ly nn ma mb mc no me mf jz np ka mh kc nq kd mj kf nr kg ml mm bi translated">模型评估</h1><h2 id="914a" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">估价</h2><p id="c688" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">通常我们的数据会有很高的阶层不平衡问题，因为在大多数情况下，人们更可能写中立或积极的推文，而不是消极的推文。但对于大多数商业问题，我们的模型必须检测到这些负面推文。因此，我们将通过观察宏观平均f1分数以及精确召回曲线来关注这些问题。下面的函数将绘制ROC曲线和精度-召回曲线，并打印关键评估指标。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pl pm l"/></div></figure><h2 id="ac16" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">基线模型</h2><p id="945d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们可以使用scikit-learn的<code class="fe on oo op of b">DummyClassifier</code>来首先查看我们的基线度量是什么，如果我们只是根据每个类出现的频率来分类的话。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="fc1b" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">from sklearn.dummy import</strong> DummyClassifier<br/>dummy_classifier = <strong class="of iu">DummyClassifier</strong>()<br/>dummy_classifier.<strong class="of iu">fit</strong>(tweets_train, labels_train)</span><span id="6bf1" class="ns lw it of b gy pj ok l ol om">y_pred_p = dummy_classifier.predict_proba(tweets_validation)    <br/>y_pred = dummy_classifier.predict(tweets_validation)</span></pre><h2 id="53c6" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">词袋模型(计数向量)</h2><p id="df72" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">量化文本数据的一个最简单的方法是计算每个单词的频率。scikit-learn的<code class="fe on oo op of b">CountVectorizer</code>可以轻松完成这项工作。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="ea64" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">from</strong> <strong class="of iu">sklearn.feature_extraction.text</strong> <strong class="of iu">import</strong> CountVectorizer<br/><br/>countvec = <strong class="of iu">CountVectorizer</strong>(ngram_range = (1, 2), min_df = 2)<br/>count_vectors = countvec.<strong class="of iu">fit_transform</strong>(tweets_train)</span></pre><p id="702f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将返回至少出现两次的单个词汇和二元模型的计数向量。然后我们可以使用这些计数向量来训练不同的分类算法。</p><h2 id="d0f8" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">TF-IDF载体</h2><p id="e508" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">计数向量的一个问题是，它只关注单个单词的频率，而不关心单词出现的上下文。没有办法评估一条推文中的特定单词有多重要。这就是术语频率-逆文档频率(TF-IDF)得分的由来。TF-IDF得分对在一条推文中出现频率更高的词的权重大于在所有推文中出现频率更高的词。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="3bb7" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">from</strong> <strong class="of iu">sklearn.feature_extraction.text</strong> <strong class="of iu">import</strong> TfidfVectorizer<br/><br/>tfvec = <strong class="of iu">TfidfVectorizer</strong>(ngram_range = (1, 2), min_df = 2)<br/>tf_vectors = tfvec.fit_transform(tweets_train)</span></pre><p id="aa8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了两个矢量化的文本，我们可以为每个矢量测试不同的分类器。</p><h2 id="aa39" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">朴素贝叶斯</h2><p id="c839" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">朴素贝叶斯是文本分类中比较流行的选择之一。这是对每个类和预测器的简单应用<strong class="lb iu">贝叶斯定理</strong>，并且它假设每个单独的特征(在我们的例子中是单词)是相互独立的。</p><p id="5cee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比方说，我们有一条推特，上面写着… <em class="pn">“我爱我的新手机。真的很快，很可靠，设计的很好！”。</em>这条推文明显带有积极的情绪。在这种情况下，朴素贝叶斯模型假设像“爱”、“新”、“真的”、“快”、“可靠”这样的单个单词都独立地贡献给它的正类。换句话说，当使用“可靠”一词时，推文是正面的可能性不会因其他词而改变。这并不意味着这些词在外观上是独立的。一些单词可能经常一起出现，但这并不意味着每个单词对其类别的贡献是独立的。</p><p id="69a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当上述假设成立时，朴素贝叶斯算法使用简单且可靠。因为在我们的模型上测试需要矢量化，所以我们可以将管道构建到我们的模型中。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="4835" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">from</strong> <strong class="of iu">sklearn.naive_bayes</strong> <strong class="of iu">import</strong> MultinomialNB<br/><strong class="of iu">from</strong> <strong class="of iu">sklearn.pipeline</strong> <strong class="of iu">import</strong> Pipeline</span><span id="03a9" class="ns lw it of b gy pj ok l ol om">mn_nb = <strong class="of iu">MultinomialNB</strong>()</span><span id="59af" class="ns lw it of b gy pj ok l ol om"># change countvec to tfvec for tf-idf <br/>model = <strong class="of iu">Pipeline</strong>([('vectorize', countvec), ('classify', mn_nb)])</span><span id="f213" class="ns lw it of b gy pj ok l ol om"># fitting training count vectors (change to tf_vectors for tf-idf)<br/><strong class="of iu">model</strong>['classify'].<strong class="of iu">fit</strong>(count_vectors, labels_train)</span><span id="1018" class="ns lw it of b gy pj ok l ol om">y_pred_p = <strong class="of iu">model</strong>.predict_proba(tweets_validation)    <br/>y_pred = <strong class="of iu">model</strong>.predict(tweets_validation)<br/><strong class="of iu">evaluating</strong>(labels_validation, y_pred, y_pred_p)</span></pre><p id="1a7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于朴素贝叶斯假设要素之间相互独立，因此它高估了每个要素对标注贡献的置信度，从而使其成为一个糟糕的估计器。所以对预测的概率要有所保留。</p><h2 id="e59b" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated"><strong class="ak">支持向量机(SVM) </strong></h2><p id="15fb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">文本分类算法的另一个流行选择是支持向量机(SVM)。简而言之，SVM找到了一个超平面，这个超平面以最大的间距来划分这些类。在文本分类中SVM是首选的主要原因是我们倾向于以大量的特征结束。如果我们在这样一个拥有我们所有特征的高维空间中工作，就会导致一个被称为<strong class="lb iu">维度诅咒</strong>的问题。基本上，我们的空间太大了，我们的观察开始失去意义。但是SVM在处理大量特性时更加健壮，因为它使用了<strong class="lb iu">内核技巧</strong>。SVM实际上并不在高维度中工作，它只是看着观察之间的成对距离，就好像它们在高维度中一样。做这项工作确实需要很长时间，但它很健壮。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="0db8" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">from</strong> <strong class="of iu">sklearn.svm</strong> <strong class="of iu">import</strong> SVC</span><span id="13f4" class="ns lw it of b gy pj ok l ol om">svm_classifier = SVC(class_weight = 'balanced', probability= <strong class="of iu">True</strong>)<br/># don't forget to adjust the hyperparameters! </span><span id="cf03" class="ns lw it of b gy pj ok l ol om"># change countvec to tfvec for tf-idf <br/>svm_model = <strong class="of iu">Pipeline</strong>([('vectorize', countvec), ('classify', svm_classifier)])</span><span id="787e" class="ns lw it of b gy pj ok l ol om"># fitting training count vectors (change to tf_vectors for tf-idf)<br/><strong class="of iu">svm_model</strong>['classify'].<strong class="of iu">fit</strong>(count_vectors, labels_train)</span><span id="8b11" class="ns lw it of b gy pj ok l ol om">y_pred_p = <strong class="of iu">svm_model</strong>.predict_proba(tweets_validation)    <br/>y_pred = <strong class="of iu">svm_model</strong>.predict(tweets_validation)<br/><strong class="of iu">evaluating</strong>(labels_validation, y_pred, y_pred_p)</span></pre><h2 id="333c" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">SHAP评估</h2><p id="1b1c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">当SVM使用内核技巧时，就可解释性而言，事情就进入了一个灰色地带。但是我们可以使用Shapley值来解释单个特征是如何对分类起作用的。我们将使用<a class="ae ky" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> SHAP </a>的友好界面来可视化Shapley值。关于这方面的详细教程，我推荐阅读关于SHAP的文档。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="341e" class="ns lw it of b gy oj ok l ol om"><strong class="of iu">import</strong> <strong class="of iu">shap<br/></strong>shap.initjs() <br/>sample = shap.<strong class="of iu">kmeans</strong>(count_vectors, 10)<br/>e = shap.<strong class="of iu">KernelExplainer</strong>(svm_model.predict_proba, sample, link = 'logit')<br/>shap_vals = e.<strong class="of iu">shap_values</strong>(X_val_tf, nsamples = 100)<br/>shap.<strong class="of iu">summary_plot</strong>(shap_vals, <br/>                  feature_names = countvec.get_feature_names(),<br/>                  class_names = svm_model.classes_)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/cf51d77124708a731da03a90816b78b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xQ9BtfBZwEQx6NYu"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">唷，太多了。让我们休息一下。<a class="ae ky" href="https://unsplash.com/@freestocks?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> freestocks </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h2 id="26b8" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">LSTM</h2><p id="314f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让我们再深入一点(字面上)。到目前为止，我们用两种不同的频率度量来量化我们的文本数据。但是每个单词的出现频率只讲述了故事的一小部分。理解语言及其意义需要理解句法，或者至少是单词的顺序。因此，我们将研究一种关心词汇序列的深度学习架构:<strong class="lb iu">长短期记忆(LSTM) </strong>架构。</p><p id="514f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于LSTM，我们需要按顺序输入文本。以下步骤概述了运行和评估LSTM分类器的步骤。我解释了代码中的每一步。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pl pm l"/></div></figure><h2 id="7eab" class="ns lw it bd lx nt nu dn mb nv nw dp mf li nx ny mh lm nz oa mj lq ob oc ml od bi translated">单词嵌入(手套)</h2><p id="da52" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们的LSTM模型的一个缺点是，它只包含我们训练数据中存在的信息，而词汇具有推文之外的语义。了解每个词汇在语义相似性方面的相互关系可能有助于我们的模型。我们可以基于预先训练的单词嵌入算法对我们的词汇应用权重。</p><p id="b530" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们将使用由斯坦福大学的一个团队获得的矢量表示法<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>。我用他们在20亿条推特上训练的200维词汇向量。你需要从他们的网站上下载载体。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pl pm l"/></div></figure><p id="93e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，您可以将获得的向量矩阵作为嵌入权重添加到我们的LSTM架构的嵌入层中。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="d14b" class="ns lw it of b gy oj ok l ol om"># adding the bolded part<br/>model.add(Embedding(num_vocab, 200, <strong class="of iu">weights = [vector_matrix]</strong>, <br/>                    input_length = max_len))</span></pre><p id="59c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用单词嵌入和LSTM，我的模型显示整体准确性提高了20%，宏观平均F1分数提高了16%。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="6252" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们回顾了使用tweets数据构建情感分析模型的基础。我将以一些需要思考和扩展的问题来结束这篇文章。</p><ol class=""><li id="502b" class="ms mt it lb b lc ld lf lg li pp lm pq lq pr lu mx my mz na bi translated">如果我们没有标签，我们如何解决同样的问题？(无监督学习)</li><li id="eb4e" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">在保持其可解释性的同时，还有哪些方法可以降低维度？</li></ol><h1 id="ace7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">快乐学习！</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/301931178dd7fc0aa0bdf0db75384336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tD5m9WUS0tcEUnYn"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@dear_jondog?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jonathan Daniels </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div></div>    
</body>
</html>