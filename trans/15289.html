<html>
<head>
<title>Casual Intro to Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/casual-intro-to-reinforcement-learning-4a78b57d4686?source=collection_archive---------49-----------------------#2020-10-20">https://towardsdatascience.com/casual-intro-to-reinforcement-learning-4a78b57d4686?source=collection_archive---------49-----------------------#2020-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="6b31" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">AWS DeepRacer系列</h2><div class=""/><div class=""><h2 id="6690" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">对强化学习的直观解释</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/2d3a0b33b4f60fef4ea1f07d8300501b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1JDOiG3l_xqDLx2M"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae le" href="https://unsplash.com/@picsea?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Picsea </a>拍摄的照片</p></figure><p id="b060" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">首先，我要感谢<a class="ae le" href="https://jakartamachinelearning.com/" rel="noopener ugc nofollow" target="_blank"> Jakarta机器学习</a>和AWS让我有机会加入<a class="ae le" href="https://aws.amazon.com/deepracer/" rel="noopener ugc nofollow" target="_blank"> AWS DeepRacer </a>新兵训练营。我一定会通过我的文章分享我在这次训练营的学习经历。所以，请继续关注并了解更多关于我的新兵训练营经历！</p><p id="ec7b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这次训练营中，我和其他8名参与者将为AWS DeepRacer联赛做准备。对于那些不知道的人来说，AWS DeepRacer基本上是一辆自主规模的赛车，AWS DeepRacer League是第一个全球自主赛车联盟。</p><p id="19e6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了自主比赛，AWS DeepRacer需要<em class="mb">学习</em>如何自己驾驶，就像职业赛车手驾驶自己的汽车一样。这种<em class="mb">学习</em>机制也类似于学步儿童学习如何走路。和前面的例子类似，AWS DeepRacer也应用了这种<em class="mb">学习</em>的方法，称为<em class="mb">强化学习</em>。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="7fa7" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">什么是强化学习？</h1><p id="ab9e" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">此时，你可能会疑惑强化学习是什么意思。简单来说，强化学习就是通过<strong class="lh ja">与环境</strong>互动来学习。它是<strong class="lh ja">主动的和顺序的</strong>，这意味着未来取决于早期的互动。除此之外，它指向一个<strong class="lh ja">目标</strong>，并且系统可以在没有最佳行为示例的情况下进行学习(<strong class="lh ja"><em class="mb"/></strong><em class="mb">)。</em></p><p id="75c6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于自动驾驶汽车，你可能会问以下问题。</p><blockquote class="ng"><p id="d3c9" class="nh ni iq bd nj nk nl nm nn no np ma dk translated">为什么我们不能给汽车编程，让它准确地知道在哪里左转和右转？</p></blockquote><p id="ad99" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated"><em class="mb">嗯</em>，学习有两个原因。首先，我们想找到以前<strong class="lh ja">未知的</strong>解。比如能打败人类象棋大师的程序。其次，我们想为<strong class="lh ja">无法预见的</strong>情况找到解决方案。例如，一辆自动驾驶汽车可以在不同于任何已知轨道的轨道上行驶。</p><p id="3b76" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">换句话说，强化学习是从与环境的相互作用中学习做出决策的科学。这个概念被应用于许多领域，从计算机科学到经济学。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/9a9a947f05428d2743f8c9432a0dba70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fplsPxZ9QlXMyKVdzYRj9Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="b658" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">几个特征使得强化学习不同于其他类型的学习。第一，没有<strong class="lh ja">没有监督</strong>，只给出奖励信号。它只是告诉你它有多喜欢一个决定，但并没有告诉你你必须做什么。另一个特点是反馈可以<strong class="lh ja">延迟</strong>，而不是瞬间获得。<strong class="lh ja">序列</strong>也很重要，因为<strong class="lh ja">之前的</strong>决定会影响<strong class="lh ja">之后的</strong>互动。</p><p id="10b6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了更好地理解强化学习，我们需要看一个决策问题的例子。一个例子是管理一个投资组合，其中你根据<strong class="lh ja">回报</strong> ( <em class="mb">利润</em>)做出决定。这个奖励可能是短期的，也可能是长期的，也就是说奖励是<strong class="lh ja">延迟</strong>的。此外，买卖顺序也会影响你的投资组合。它表明这是一个强化学习问题的例子。</p><blockquote class="ng"><p id="7821" class="nh ni iq bd nj nk nl nm nn no np ma dk translated">强化学习的特征是奖励信号、延迟反馈和顺序。</p></blockquote></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="c3ff" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">核心概念</h1><p id="58a4" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">在理解强化学习时，有几个方面是我们必须知道的。他们是<strong class="lh ja"><em class="mb"/></strong><em class="mb"/><strong class="lh ja"><em class="mb">奖励信号</em></strong><em class="mb"/>和<strong class="lh ja"> <em class="mb">代理人。</em></strong><em class="mb">代理</em>还包括<strong class="lh ja"> <em class="mb">代理状态</em> </strong> <em class="mb">，</em> <strong class="lh ja"> <em class="mb">策略</em> </strong> <em class="mb">，</em><em class="mb">值函数</em> 。</p><p id="015c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">代理</em>和<em class="mb">环境</em>之间的交互可以描述如下。在每个时间步，<em class="mb">代理</em>接收<em class="mb">观察</em>和<em class="mb">奖励</em>，同时执行一个<em class="mb">动作</em>。另一方面，<em class="mb">环境</em>接收一个<em class="mb">动作</em>然后发出<em class="mb">观察</em>和<em class="mb">奖励</em>。</p><p id="d703" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">奖励</em>是一个反馈信号，表明<em class="mb">代理</em>在每个时间步做得有多好。<em class="mb">代理人的</em>工作是<strong class="lh ja">最大化</strong>从现在到未来的累计<em class="mb">奖励</em>。累计<em class="mb">奖励</em>也称为<strong class="lh ja">回报<em class="mb">回报</em>回报</strong>。这向我们展示了强化学习是基于<strong class="lh ja">奖励假设</strong>的。</p><blockquote class="ng"><p id="9a48" class="nh ni iq bd nj nk nl nm nn no np ma dk translated">任何目标都可以被认为是回报最大化的结果。</p></blockquote><p id="4429" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">现在，我要你考虑一下。你接受还是拒绝这个假设？</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="00df" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">基于<em class="mb">回报</em>，我们还对<em class="mb">值</em>感兴趣，它是从现在开始的预期<em class="mb">回报</em>。“预期”一词意味着我们考虑到了未来回报发生的可能性。因此，代理的目标变为<strong class="lh ja">通过选择<strong class="lh ja">最佳行动</strong>最大化</strong> <em class="mb">值</em>。</p><p id="0adc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，行动可能会产生长期后果，奖励可能<strong class="lh ja">不会</strong>立即收到。在某些情况下，牺牲眼前的回报从长远来看可能是有益的。例如，一项金融投资可能需要几年时间，直到它变得非常有利可图，尽管它在开始时可能会有一些损失。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/c73d833d60bababd3a81c0acf31d316e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2v-nOxh9UhF0c4pd"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@gillyberlin?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">吉利</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="89d3" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">代理组件</h1><blockquote class="ng"><p id="16c1" class="nh ni iq bd nj nk nl nm nn no np ma dk translated">一个<em class="nx">代理的三个基本组成部分:代理状态、策略、</em>和<em class="nx">价值函数</em>。</p></blockquote><p id="7b97" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">动作取决于代理的<strong class="lh ja">状态。一个<em class="mb">状态</em>可以被定义为用于确定接下来发生什么的信息。在最简单的情况下，只有一个<em class="mb">状态</em>。然而，根据具体情况，可能有许多不同的<em class="mb">状态</em>。例如，当我们想要购买食物时，我们会无意识地访问我们身体的状态，例如饥饿程度和食物偏好。</strong></p><p id="7e9e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从<em class="mb">状态</em>到<em class="mb">动作</em>的映射称为<strong class="lh ja"> <em class="mb">策略</em> </strong>。<em class="mb">策略</em>根据代理的当前状态定义代理的行为。以我自己为例，每次下雨我都会想要一杯热巧克力。相反，我会在大热天喝一杯冰柠檬茶。换句话说，我的行为是由我的<em class="mb">政策</em>决定的，它将天气状况<em class="mb">状态</em>转化为我的饮料选择<em class="mb">动作</em>。</p><p id="bd85" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">状态</em>和<em class="mb">政策</em>决定代理人的行为，而<strong class="lh ja"> <em class="mb">价值函数</em> </strong>计算预期<em class="mb">回报</em>或对未来<em class="mb">回报</em>的预测。它用于评估<em class="mb">状态</em>的可取性，以及哪些操作最有益。有时，使用<em class="mb">折扣系数</em>是很有用的，它权衡了即时回报和长期回报的重要性。低折扣系数意味着你只关心眼前的<em class="mb">回报</em>，反之亦然。</p><p id="7bc2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然强化学习解决了很多决策问题，但是有一个问题叫做<strong class="lh ja"> <em class="mb">剥削和探索</em> </strong>。<em class="mb">剥削</em>是指代理人利用<strong class="lh ja">已知的</strong>信息来最大化报酬。另一方面，<em class="mb">探索</em>正在寻找<strong class="lh ja">新的</strong>信息，希望获得更多的回报。</p><p id="5911" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">比如，在决定你想吃什么的时候，你可以认为<em class="mb">奖励</em>就是你的进食满意度。为了充分利用它，你可以选择吃你最喜欢的食物。或者，你可以尝试新的菜肴，寻找味道更好的东西。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/178d3a7dab7432250827f7a642fd26e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VcePqevMp_RXv_3c"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@therachelstory?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">雷切尔·帕克</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="d697" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">结论</h1><blockquote class="ng"><p id="81bf" class="nh ni iq bd nj nk nl nm nn no np ma dk translated">简而言之，强化学习是指通过试错来学习，在此期间，代理人应该通过与环境的交互来发现一个使回报最大化的好政策。</p></blockquote></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="b125" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">参考</h1><p id="4a34" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">如果你想了解更多关于强化学习的知识，我建议你像我一样查阅这些资料。</p><ol class=""><li id="e810" class="ny nz iq lh b li lj ll lm lo oa ls ob lw oc ma od oe of og bi translated">Hado van Hasselt关于强化学习的讲座——课程1:强化学习简介。</li><li id="df19" class="ny nz iq lh b li oh ll oi lo oj ls ok lw ol ma od oe of og bi translated">《强化学习:导论》( 2015 ),作者理查德·萨顿和安德鲁·巴尔托。</li></ol></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="0d9e" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">结束语</h1><p id="189c" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">感谢您花时间阅读这篇文章！希望你能学到新的东西，能把这个理念融入到你的生活中。请务必关注我即将发表的关于强化学习和AWS DeepRacer细节的文章。我希望你有美好的一天！</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="98a7" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">关于作者</h1><p id="c5d8" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">Alif Ilham Madani是一名有抱负的数据科学和机器学习爱好者，他热衷于从他人那里获得洞察力。他在印尼顶尖大学<a class="ae le" href="https://www.itb.ac.id/" rel="noopener ugc nofollow" target="_blank"><em class="mb">Institut Teknologi Bandung</em></a>主修电子工程。</p><p id="38cc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你有任何要讨论的话题，你可以通过<a class="ae le" href="https://www.linkedin.com/in/alif-ilham-madani/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae le" href="https://twitter.com/_alifim" rel="noopener ugc nofollow" target="_blank"> Twitter </a>与Alif联系。</p></div></div>    
</body>
</html>