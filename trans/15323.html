<html>
<head>
<title>5 Must-Know Activation Functions Used in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中使用的5个必须知道的激活函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-must-know-activation-functions-used-in-neural-networks-8c5052757750?source=collection_archive---------31-----------------------#2020-10-21">https://towardsdatascience.com/5-must-know-activation-functions-used-in-neural-networks-8c5052757750?source=collection_archive---------31-----------------------#2020-10-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7176" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">非线性的本质</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a86a0f748188389489af4dc00dacd400.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WGS18KomS4TvWq6S2xYkbA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@drewpatrickmiller?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">德鲁·帕特里克·米勒</a>在<a class="ae ky" href="https://unsplash.com/s/photos/adjust?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="153d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通用逼近定理意味着神经网络可以逼近将输入(X)映射到输出(y)的任何连续函数。表示任何函数的能力是神经网络如此强大和广泛使用的原因。</p><p id="3f2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了能够逼近任何函数，我们需要非线性。这就是激活功能发挥作用的地方。它们用于向神经网络添加非线性。在没有激活函数的情况下，神经网络可以被认为是线性模型的集合。</p><p id="37e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络是包含许多节点的层的组合。因此，构建过程从一个节点开始。下面是一个没有激活功能的节点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/43a8ee5c19158516353353512e2945ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*5a6S_km9nN22Pe6K.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">没有激活功能的神经元(图片由作者提供)</p></figure><p id="038b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出y是输入和偏置的线性组合。我们需要以某种方式添加非线性元素。考虑下面的节点结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/2039db2fe3b3c43358d7aab5e96dc401.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1QFveOPhuMjZPHTs.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有激活功能的神经元(图片由作者提供)</p></figure><p id="7b82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非线性是通过将激活函数应用于输入和偏置的线性组合的总和来实现的。增加的非线性取决于激活函数。</p><p id="2cbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本帖中，我们将讨论神经网络中5种常用的激活方式。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="121b" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">1.乙状结肠的</h1><p id="2dd3" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">sigmoid函数的取值范围介于0和1之间。它也用于逻辑回归模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/ce2a80a9441ef0e5224848bc8ddcd61f.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*b9AIrXKaJtCCgDp4.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="27c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论sigmoid函数的输入值是什么，输出值都将在0和1之间。因此，每个神经元的输出被标准化为范围0-1。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/9e8bc7376464bfb89451c880dec66504.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/0*drAuKBlVC2LXSV9J.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="f29a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当x值接近0时，输出(y)对输入(x)的变化更敏感。随着输入值远离零，输出值变得不那么敏感。在某个时间点之后，即使输入值有很大的变化，输出值也几乎没有变化。这就是sigmoid函数实现非线性的方式。</p><p id="e4c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种非线性也有不利的一面。我们先来看sigmoid函数的导数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/940c9993084f817be539a50b7ca06eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/0*0wNHZxgfjxmMhaDa.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="27c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们远离零时，导数趋向于零。神经网络的“学习”过程取决于导数，因为权重是基于梯度更新的，梯度基本上是函数的导数。如果梯度非常接近零，则以非常小的增量更新权重。这导致神经网络学习速度非常慢，永远无法收敛。这也被称为<strong class="lb iu">消失梯度</strong>问题。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="008d" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated"><strong class="ak"> 2。Tanh(双曲正切)</strong></h1><p id="28ca" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">它与sigmoid非常相似，只是输出值在-1至+1范围内。因此，我们说tanh是以零为中心的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/81ef2b568ac2c94d3b7335f4f642555f.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/0*R1jVTlIdhLhagoNi.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="f0c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">sigmoid和tanh之间的区别在于，对于tanh，梯度并不局限于在一个方向上移动。因此，双曲正切函数可能比sigmoid函数收敛得更快。</p><p id="bb5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">双曲正切激活函数也存在消失梯度问题。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="ce4b" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">3.整流线性单位</h1><p id="987d" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">relu函数只对正值感兴趣。它保持输入值大于0。所有小于零的输入值都变成0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/efde18817b50322c1de12b8a354210fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*ChWt3--L3kJ5aF_V.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="76ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经元的输出值可以设置为小于0。如果我们将relu函数应用于该神经元的输出，则从该神经元返回的所有值都变成0。因此，relu允许抵消一些神经元。</p><p id="4951" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们能够用relu函数仅激活一些神经元，而用tanh和sigmoid激活所有神经元，这导致了密集的计算。因此，relu比tanh和sigmoid收敛得更快。</p><p id="0c8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于小于0的输入值，relu的导数为0。对于这些值，权重在反向传播期间从不更新，因此神经网络无法学习。这个问题被称为<strong class="lb iu">将死再禄</strong>问题。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="32f5" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">4.泄漏ReLU</h1><p id="a62e" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">它可以被认为是对垂死的relu问题的解决方案。漏relu为负输入输出一个小值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/5c2aec954cc7587e703f33a3cd03a308.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*1j1CY9lCIC2R_pWm.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="38b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管leaky relu似乎正在解决垂死的relu问题，但一些人认为在大多数情况下准确性没有显著差异。我想这可以归结为尝试两者，看看对于一个特定的任务是否有什么不同。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="55a4" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated"><strong class="ak"> 5。Softmax </strong></h1><p id="a570" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">Softmax通常用于多类分类任务，并应用于输出神经元。它所做的是将输出值归一化为概率分布，使概率值加起来等于1。</p><p id="c717" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Softmax函数将每个输出的指数除以所有输出的指数之和。结果值形成一个概率分布，其概率总和为1。</p><p id="4b9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来做一个例子。考虑目标变量有4个类的情况。以下是5个不同数据点(即观察值)的神经网络输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/5b5fe5f9acbd4d4bbcc42fe4a34c1b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*7-V3LlHB9eSX3IgxhcQM0w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每一列代表一个观察的输出(图片由作者提供)</p></figure><p id="f438" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将softmax函数应用于这些输出，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/0cf031578dd105a2d71bf38385454fec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*DJn6j_LW1_SY9ljzWyKoeg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="32ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第一行中，我们将softmax函数应用于矩阵a中的值。第二行将浮点精度降低到2位小数。</p><p id="e7b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是softmax函数的输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b38c94515bf63d9c0176579143033451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*B3h66eugX1waAv9YcdYByQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="fddb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，每列中的概率值加起来为1。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="58a5" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated"><strong class="ak">结论</strong></h1><p id="48ed" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">我们已经讨论了神经网络中使用的5种不同的激活函数。为了增加非线性，必须在神经网络中使用激活函数。</p><p id="9ba6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">天下没有免费的午餐！就计算复杂性而言，激活函数也给神经网络带来负担。它们也对模型的收敛有影响。</p><p id="d8b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">了解激活函数的属性以及它们的行为方式非常重要，这样我们就可以选择最适合特定任务的激活函数。</p><p id="b389" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，激活函数的期望属性是:</p><ul class=""><li id="a8f3" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">计算成本低</li><li id="9def" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">零居中</li><li id="ef9a" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">可微的。激活函数的导数需要携带关于输入值的信息，因为权重是基于梯度更新的。</li><li id="405e" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">不会导致消失梯度问题</li></ul></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="4358" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>