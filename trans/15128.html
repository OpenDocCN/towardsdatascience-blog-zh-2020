<html>
<head>
<title>Understanding Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-linear-regression-94a6ab9595de?source=collection_archive---------7-----------------------#2020-10-18">https://towardsdatascience.com/understanding-linear-regression-94a6ab9595de?source=collection_archive---------7-----------------------#2020-10-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/15a0835b0249a43e7e2e166e34847a2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TF2PluTXCARhG3hJ"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@drew_beamer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">德鲁·比默</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><div class=""/><div class=""><h2 id="2bdc" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">数据科学的主力</h2></div><p id="b0dc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管近年来流行更复杂和更奇特的模型，但线性回归仍然难以击败，因为它具有通用性、稳健性和可解释性。</p><p id="5267" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个简单但功能强大的模型，只需完成工作。如果你想进入定量领域，你需要了解它是如何工作的。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="45e1" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">线路拟合</h1><p id="84da" class="pw-post-body-paragraph ky kz jj la b lb mt kk ld le mu kn lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">线性回归基本就是直线拟合。它会问问题— <strong class="la jk">“最符合我的数据的直线方程是什么？”</strong>好看又简单。</p><p id="11b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一条线的方程式是:</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="cb0b" class="nh mc jj nd b gy ni nj l nk nl">Y = b0 + b1*X</span></pre><p id="e8e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">目标变量y是我们试图建模的东西。我们想了解(也就是解释)它的变化。在统计学中，方差是不确定性的度量。这是一种价值在它的平均值附近波动的趋势。</p><figure class="my mz na nb gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/eabdfac5dff98713e5dca453d6d36e29.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*mFgx8-wx0ThiDzsxmDDcPw.jpeg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">波动性的可视化(图形由作者创建)</p></figure><p id="f2bb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">左图说明了这一点。绿色变量具有很高的方差，因此在其平均值(圆锥的中心)周围有很大的不确定性圆锥。红色变量具有较低的方差。在对变量一无所知的情况下，当猜测红色变量的值时，我们会对自己不会太离谱的能力更有信心。</p><p id="e513" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">但是在数据科学和统计学中，我们通常不担心方差，我们只担心无法解释的方差。也就是说，如果我们能找到一些其他的变量(特征变量，X ),来解释Y的大部分变化，那么我们是好的。</strong></p><p id="2708" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有了这样一个特征变量，如果有人问我对Y的预测，我只需要查找X等于什么，我就可以合理地确定Y会是什么。Y的大部分变化已经通过引入x得到了解释。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="8804" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">相关特征有助于解释差异</h1><p id="0365" class="pw-post-body-paragraph ky kz jj la b lb mt kk ld le mu kn lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">那么为什么直线拟合能帮助我们解释方差呢？想想这条线的方程代表什么。它定义了X和Y之间的关系。通过将X乘以b1并与b0 (b0 + b1*X)求和，我们可以根据给定的X值来预测Y。</p><figure class="my mz na nb gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/35fd0898488647b33badff5a05ec192a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/0*93RsSNUjNcChfilr.jpeg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">正相关的事物往往会一起移动(作者创建的图形)</p></figure><p id="82e2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">当然，这只适用于X和Y相关的情况(正相关或负相关)。</strong>相关性衡量两个事物一起运动的趋势(或者在负相关的情况下彼此相反)。比如人的身高和体重是正相关的。你越高，体重越重。</p><p id="a3f1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以直线拟合允许我们从已知的X值来推断未知的Y值，这要归功于X和Y之间的相关性。</p><p id="5f0c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们用线性回归方程中的X变量来解释Y的方差。通过以这种方式解释方差，我们实际上减少了方差——解释方差是我们不再需要担心的方差。</p><p id="950b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="no">关键的假设是X变量是容易测量的，容易理解的，并且它们本身不是一个谜。</em> </strong>一个常见的建模错误是使用本身无法实时观察到的变量来预测某事。用未来的数据预测未来的模型是不好的。</p><p id="dbeb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看这个过程是如何进行的。当我们从一个Y变量开始时，我们所知道的只是它的分布。在下图中，所有的方差(绿色圆锥)都是无法解释的，我们对Y的未来值的最佳猜测是它的平均值。</p><figure class="my mz na nb gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/a5466741f24174740c32f1d897fdfffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ek7h627cQpGA4h01WmLTQg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">仅Y变量(由作者创建的图形)</p></figure><p id="fa0a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，假设我们发现了一个与y正相关的特征变量X。下图显示了X如何帮助解释方差。我们可以根据Y的X值将Y的观测值分成两部分。对于较低的X值，Y的期望值为平均值1，方差近似为左边的红色圆锥。对于较高的X值，Y的期望值为平均值2，方差近似为右边的红色圆锥。</p><figure class="my mz na nb gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/59544b7e3186424971889d74e8828e4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*chCYOl4KiV7Py16BMJjsUQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">特征X有助于解释Y的一些变化(图形由作者创建)</p></figure><p id="7ced" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">注意通过这种方式分割，方差(红色圆锥)相对于原始绿色圆锥有所减少。我们对Y的新预测，或者是均值1，或者是均值2，这取决于X是什么，比我们以前的天真预测(Y的均值)要精确得多。</strong></p><p id="d23e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是为什么要在两个分段上停下来呢？如果我们画更多的蓝色垂直虚线，将数据分成更多的桶，我们可以解释更多的差异，并产生更精确的预测。</p><p id="2795" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是线性回归的作用。<strong class="la jk">实际上，它绘制了大量的垂直线，将数据分成许多微小的片段。这最大化了解释的方差和我们预测的精确度。</strong></p><figure class="my mz na nb gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/d5beb730a38532fb590b49a75316a894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RxVb1lU8ZN7QQEGUR9BGog.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">线条拟合(作者创建的图形)</p></figure></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="4e86" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">普通最小二乘法(OLS)</h1><p id="4f2b" class="pw-post-body-paragraph ky kz jj la b lb mt kk ld le mu kn lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">让我们快速了解一下线性回归如何找到最佳拟合线。虽然有一个解析解，但你可以把它看作一个优化问题。线性回归算法想要:</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="f49d" class="nh mc jj nd b gy ni nj l nk nl">Find the values for <strong class="nd jk">b0</strong> and <strong class="nd jk">b1</strong></span><span id="22fa" class="nh mc jj nd b gy ns nj l nk nl">that <strong class="nd jk">minimizes the sum of squared errors.</strong></span></pre><p id="5b86" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">误差是数据(黑点)和蓝线</strong>之间的距离——在下图中，红色箭头表示误差。我们平方误差，因为它可以是正的，也可以是负的。</p><figure class="my mz na nb gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/ca17d6e8f25f8bd55641988c2df48c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aN_xDvjp3S08Hx0T2rudqA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">OLS可视化(图形由作者创建)</p></figure><p id="2af0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">既然我们知道误差是实际观察值(Y，黑点)和我们的预测值(蓝线)之间的差异，我们可以更详细地写出我们的优化问题:</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="0cdb" class="nh mc jj nd b gy ni nj l nk nl">Find the values for <strong class="nd jk">b0</strong> and <strong class="nd jk">b1</strong></span><span id="fb9a" class="nh mc jj nd b gy ns nj l nk nl">that <strong class="nd jk">minimizes the sum of squared errors.</strong></span><span id="b92d" class="nh mc jj nd b gy ns nj l nk nl">Where squared error for observation i<br/>= <strong class="nd jk">(Yi - (b0 + b1*Xi))^2</strong></span><span id="ff58" class="nh mc jj nd b gy ns nj l nk nl">And the sum of squared error is just the sum of <br/>(Yi - (b0 + b1*Xi))^2 for all observations.</span></pre><p id="1565" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了计算误差平方和，我们取一个黑点和蓝线之间的每一个垂直距离，对它们求平方，然后求和。找到使误差平方和最小的b0和b1值，就能得到最佳拟合线。</p><h2 id="a0cc" class="nh mc jj bd md nu nv dn mh nw nx dp ml lh ny nz mn ll oa ob mp lp oc od mr oe bi translated">回归参数</h2><p id="7d50" class="pw-post-body-paragraph ky kz jj la b lb mt kk ld le mu kn lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">参数(b0，b1等。)、<strong class="la jk">被称为贝塔</strong>，这种回归是很重要的。在我们之前的例子中，我们只有一个特征变量。但是假设我们有三个特征变量:</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="7fa5" class="nh mc jj nd b gy ni nj l nk nl">Y = b0 + b1*X1 + b2*X2 + b3*X3</span></pre><ul class=""><li id="0297" class="of og jj la b lb lc le lf lh oh ll oi lp oj lt ok ol om on bi translated"><strong class="la jk"> b0 </strong>是Y轴和蓝线之间的截距，告诉我们当所有特征变量都为0时Y的期望值。</li><li id="b78e" class="of og jj la b lb oo le op lh oq ll or lp os lt ok ol om on bi translated"><strong class="la jk"> b1 </strong>告诉我们，当保持X2和X3不变时，改变X1如何影响y</li><li id="85d6" class="of og jj la b lb oo le op lh oq ll or lp os lt ok ol om on bi translated"><strong class="la jk"> b2 </strong>告诉我们，当保持X1和X3不变时，改变X2如何影响y</li><li id="4964" class="of og jj la b lb oo le op lh oq ll or lp os lt ok ol om on bi translated"><strong class="la jk"> b3 </strong>告诉我们，当保持X1和X2不变时，改变X3如何影响y</li></ul><p id="96aa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">解释贝塔系数的一个关键假设是，你可以保持其他特征变量不变。</strong>在某些情况下，很难在不改变一个特性的情况下改变另一个特性。在这种情况下，我们希望包括这些交互效应(另一篇文章的主题)。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="3825" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">r——量化解释的差异量</h1><p id="ae98" class="pw-post-body-paragraph ky kz jj la b lb mt kk ld le mu kn lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">最后，我们想弄清楚我们能够解释多少原始方差(因为目标是解释，因此减少方差)。</p><p id="5642" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以通过计算R:</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="7f3e" class="nh mc jj nd b gy ni nj l nk nl">R^2 = 1 - (residual sum of squares / total sum of squares)</span></pre><p id="e1d6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">残差(也称为误差)平方和与我们预测误差的方差(黑点和蓝线之间的垂直距离)成比例。预测误差是实际观测值和我们的模型预测值之间的差异。这是一种无法解释的方差的度量。</p><p id="522c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总平方和与我们试图解释的(Y的)总方差成正比——它实际上是Y方差公式的分子。</p><p id="1910" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">残差平方和与总平方和的比率衡量运行线性回归后未解释的方差的比例。换句话说，它是不确定性的比例，我们不能用线性回归使其消失。</p><p id="5711" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">因为R是1减去该比率，所以它可以被认为是解释的方差的比例:</strong></p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="3bbf" class="nh mc jj nd b gy ni nj l nk nl"><strong class="nd jk">R^2</strong> <br/>= 1 - (residual sum of squares / total sum of squares)<br/>= 1 - proportion of variance unexplained<br/>= <strong class="nd jk">proportion of variance explained</strong></span><span id="e9c1" class="nh mc jj nd b gy ns nj l nk nl">Since: <br/>prop. of variance unexplained + prop of variance explained = 1</span></pre><p id="45aa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，如果在运行我们的回归后，我们看到R为0.90，这意味着我们在模型中包括的X变量有助于解释y中观察到的90%的方差。换句话说，我们成功地解释了我们感兴趣的变量周围的许多不确定性。</p><p id="0729" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在以后的文章中，我们会做一些例子，看看当一些支持线性回归的假设被违反时会发生什么。在那之前，干杯！</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><p id="ce3e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" href="https://tonester524.medium.com/membership" rel="noopener"> <strong class="la jk"> <em class="no">如果你总体上喜欢这篇文章和我的写作，请考虑通过我在这里的推荐链接注册Medium来支持我的写作。谢谢！</em> </strong> </a></p><h1 id="1c2e" class="mb mc jj bd md me ot mg mh mi ou mk ml kp ov kq mn ks ow kt mp kv ox kw mr ms bi translated"><strong class="ak">延伸阅读:</strong></h1><div class="is it gp gr iu oy"><a rel="noopener follow" target="_blank" href="/the-art-of-forecasting-d2a8806b7aa0"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jk gy z fp pd fr fs pe fu fw ji bi translated">预测的艺术</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">当我们试图预测未来时，到底发生了什么</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">towardsdatascience.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm ja oy"/></div></div></a></div><div class="is it gp gr iu oy"><a rel="noopener follow" target="_blank" href="/understanding-logistic-regression-using-a-simple-example-163de52ea900"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jk gy z fp pd fr fs pe fu fw ji bi translated">用一个简单的例子理解逻辑回归</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">逻辑回归是进行分类的基本工具之一。作为未来的数据科学家，我…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">towardsdatascience.com</p></div></div><div class="ph l"><div class="pn l pj pk pl ph pm ja oy"/></div></div></a></div><div class="is it gp gr iu oy"><a rel="noopener follow" target="_blank" href="/understanding-maximum-likelihood-estimation-mle-7e184d3444bd"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jk gy z fp pd fr fs pe fu fw ji bi translated">理解最大似然估计</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">这是什么？它是用来做什么的？</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">towardsdatascience.com</p></div></div><div class="ph l"><div class="po l pj pk pl ph pm ja oy"/></div></div></a></div></div></div>    
</body>
</html>