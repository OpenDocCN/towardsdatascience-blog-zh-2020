<html>
<head>
<title>Attention is all you need: Discovering the Transformer paper</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意力是你所需要的:发现变压器纸</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634?source=collection_archive---------0-----------------------#2020-11-02">https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634?source=collection_archive---------0-----------------------#2020-11-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="be8c" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="c933" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">Tensorflow中变压器模型的详细实现</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/593905f21cdf2d49fa9c63812115c609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*jV9SQddZ54IQmB1TQwCa9w.jpeg"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">图片来自Pixabay的罗宇胜·谭</p></figure><p id="c753" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">本帖我们将对论文中的相关神器<a class="ae lw" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"><em class="lx"/></a><em class="lx">【瓦斯瓦尼、阿希什&amp;沙泽尔、诺姆&amp;帕尔马、尼基&amp;乌斯科雷特、雅各布&amp;琼斯、利翁&amp;戈麦斯、艾丹&amp;凯泽、卢卡兹&amp;波洛舒欣、伊利亚进行描述和揭秘。(2017))【1】</em>。这篇论文在注意力机制的使用上是一个巨大的进步，是对一个叫做Transformer的模型的主要改进。在NLP任务中出现的最著名的当前模型由几十个变压器或它们的一些变体组成，例如，GPT-2或BERT。</p><p id="ecbe" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们将描述这个模型的组成部分，分析它们的运作，并建立一个简单的模型，我们将应用于一个小规模的NMT问题(神经机器翻译)。为了阅读更多关于我们将要解决的问题，并了解基本的注意力机制是如何工作的，我推荐你阅读我以前的帖子<a class="ae lw" href="https://medium.com/better-programming/a-guide-on-the-encoder-decoder-model-and-the-attention-mechanism-401c836e2cdb" rel="noopener">“编码器-解码器模型和注意力机制指南”</a>。</p><h1 id="7d52" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">为什么我们需要变压器</h1><p id="dc7e" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj ms ll lm ln mt lp lq lr mu lt lu lv ij bi translated">在诸如神经机器翻译的序列到序列问题中，最初的提议是基于在编码器-解码器架构中使用RNNs。这些体系结构在处理长序列时有很大的局限性，当新元素加入序列时，它们保留第一个元素信息的能力就丧失了。在编码器中，每一步中的隐藏状态都与输入句子中的某个词相关联，通常是最近的一个词。因此，如果解码器只访问解码器的最后一个隐藏状态，它将丢失序列的第一个元素的相关信息。然后为了处理这个限制，引入了一个新概念<strong class="lc ja">注意力机制</strong>。</p><p id="50c1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">与RNNs通常关注编码器的最后状态不同，在解码器的每个步骤中，我们会查看编码器的所有状态，从而能够访问关于输入序列所有元素的信息。这就是attention所做的，它从整个序列中提取信息，一个所有过去编码器状态的<strong class="lc ja">加权和。这允许解码器为输出的每个元素分配更大的权重或重要性给输入的某个元素。在每一步中学习聚焦于输入的正确元素，以预测下一个输出元素。</strong></p><p id="4c7d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是这种方法仍然有一个重要的限制，每个序列必须一次处理一个元素。编码器和解码器都必须等到完成<code class="fe mv mw mx my b">t-1</code>步骤才能处理<code class="fe mv mw mx my b">t-th</code>步骤。<strong class="lc ja">因此，在处理庞大的语料库时，这是非常耗时和计算效率低下的</strong>。</p><h1 id="e0d4" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">变压器是什么？</h1><blockquote class="mz na nb"><p id="9bda" class="la lb lx lc b ld le ka lf lg lh kd li nc lk ll lm nd lo lp lq ne ls lt lu lv ij bi translated">在这项工作中，我们提出了Transformer，这是一种避免递归的模型架构，它完全依赖于一种注意机制来绘制输入和输出之间的全局依赖关系。Transformer允许显著提高并行性……Transformer是第一个完全依赖自我关注来计算其输入和输出表示而不使用序列比对RNNs或卷积的转导模型。</p><p id="3b77" class="la lb lx lc b ld le ka lf lg lh kd li nc lk ll lm nd lo lp lq ne ls lt lu lv ij bi translated">“注意力是你所需要的全部”论文[1]</p></blockquote><p id="0f51" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">Transformer模型使用自我关注机制提取每个单词的特征，以<strong class="lc ja">计算出句子中所有其他单词对前面提到的单词的重要性</strong>。并且没有使用递归单元来获得该特征，它们只是加权和与激活，因此它们可以是非常并行和高效的。</p><p id="e25c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是我们将更深入地研究它的架构(下图)，以理解所有这些部分的作用[1]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/76376db5917c7a2598ef1188b2aa0c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*ZCFSvkKtppgew3cc7BIaug.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">摘自瓦斯瓦尼等人2017年发表的论文《注意力是你所需要的一切》</p></figure><p id="feee" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们可以看到，左边是编码器模型，右边是解码器模型。两者都包含一个重复N次的“注意力和前馈网络”的核心模块。但首先我们需要深入探讨一个核心概念:自我关注机制。</p><h1 id="1dbf" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">自我关注:基本操作</h1><blockquote class="mz na nb"><p id="f4ca" class="la lb lx lc b ld le ka lf lg lh kd li nc lk ll lm nd lo lp lq ne ls lt lu lv ij bi translated">自我关注是一个序列对序列的操作:一个向量序列进去，一个向量序列出来。我们称输入向量为<code class="fe mv mw mx my b">x1</code>、<code class="fe mv mw mx my b">x2</code>、…、<code class="fe mv mw mx my b">xt</code>以及相应的输出向量为<code class="fe mv mw mx my b">y1</code>、<code class="fe mv mw mx my b">y2</code>、…、<code class="fe mv mw mx my b">yt</code>。向量都具有维度k。为了产生输出向量<code class="fe mv mw mx my b">yi</code>，自关注操作简单地对所有输入向量进行加权平均<em class="iq">，</em>最简单的选择是点积。</p><p id="faf5" class="la lb lx lc b ld le ka lf lg lh kd li nc lk ll lm nd lo lp lq ne ls lt lu lv ij bi translated">彼得·布鲁姆的《从零开始的变形金刚》</p></blockquote><p id="8faf" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在我们模型的自我关注机制中，我们需要引入三个元素:查询、值和键</p><h2 id="a51c" class="ng lz iq bd ma nh ni dn me nj nk dp mi lj nl nm mk ln nn no mm lr np nq mo iw bi translated">查询、值和键</h2><p id="c4be" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj ms ll lm ln mt lp lq lr mu lt lu lv ij bi translated">在自我关注机制中，每个输入向量都以三种不同的方式使用:查询、键和值。在每个角色中，它都与其他向量进行比较，以获得自己的输出<code class="fe mv mw mx my b">yi</code>(查询)，获得第j个输出<code class="fe mv mw mx my b">yj</code>(键)，并在建立权重(值)后计算每个输出向量。</p><p id="313d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">为了获得这个角色，我们需要三个维度为<em class="lx">k×k</em>的权重矩阵，并为每个<code class="fe mv mw mx my b">xi</code>计算三个线性变换:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nr"><img src="../Images/f766eb5d49cf494cb08394b957be2ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r6S_FHm9h82w1hDnpyaHeA.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">彼得·布鲁姆的《从零开始的变形金刚》</p></figure><p id="310f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这三个矩阵通常被称为<em class="lx"> K </em>、<em class="lx"> Q </em>和<em class="lx"> V、</em>、<strong class="lc ja">三个可学习的权重层，它们被应用于相同的编码输入</strong>。因此，由于这三个矩阵中的每一个都来自相同的输入，我们可以将输入向量的注意机制应用于其自身，即“自我注意”。</p><h2 id="18e4" class="ng lz iq bd ma nh ni dn me nj nk dp mi lj nl nm mk ln nn no mm lr np nq mo iw bi translated">成比例的点积注意力</h2><blockquote class="mz na nb"><p id="1023" class="la lb lx lc b ld le ka lf lg lh kd li nc lk ll lm nd lo lp lq ne ls lt lu lv ij bi translated">输入包括维度<code class="fe mv mw mx my b">dk</code>的查询和关键字，以及维度<code class="fe mv mw mx my b">dv</code>的值。我们计算查询与所有键的点积，将每个键除以<code class="fe mv mw mx my b">dk</code>的平方根，并应用<code class="fe mv mw mx my b">softmax</code>函数来获得值的权重。</p><p id="a9ef" class="la lb lx lc b ld le ka lf lg lh kd li nc lk ll lm nd lo lp lq ne ls lt lu lv ij bi translated">“注意力是你所需要的全部”论文[1]</p></blockquote><p id="dfe6" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">然后我们使用<em class="lx"> Q </em>、<em class="lx"> K </em>和<em class="lx"> V </em>矩阵来计算注意力得分。<strong class="lc ja">分数衡量的是在某个位置的一个单词上对输入序列的其他位置或单词的关注程度</strong>。也就是说，查询向量与我们正在评分的相应单词的关键向量的点积。因此，对于位置1，我们计算点积。)的<code class="fe mv mw mx my b">q1</code>和<code class="fe mv mw mx my b">k1</code>，然后是<code class="fe mv mw mx my b">q1</code>。<code class="fe mv mw mx my b">k2</code>，<code class="fe mv mw mx my b">q1</code>。<code class="fe mv mw mx my b">k3</code>以此类推，…</p><p id="cc09" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">接下来，我们应用“缩放”因子来获得更稳定的梯度。softmax函数无法在大值下正常工作，导致梯度消失并减慢学习速度[2]。在“软最大化”之后，我们乘以值矩阵以保留我们想要关注的单词的值，并最小化或移除不相关单词的值(它在V矩阵中的值应该非常小)。</p><p id="5b36" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这些操作的公式是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/325021008447eb2a22c4c51e2e09e18e.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*P9sV1xXM10t943bXy_G9yg.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">摘自瓦斯瓦尼等人2017年发表的论文《注意力是你所需要的一切》[1]。标度点积注意力公式。</p></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h2 id="fb0d" class="ng lz iq bd ma nh ni dn me nj nk dp mi lj nl nm mk ln nn no mm lr np nq mo iw bi translated">多头注意力</h2><p id="ac51" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj ms ll lm ln mt lp lq lr mu lt lu lv ij bi translated">在前面的描述中，注意力分数一次集中在整个句子上，这将产生相同的结果，即使两个句子以不同的顺序包含相同的单词。相反，我们希望关注单词的不同部分。<em class="lx">“我们可以给自我注意更大的区分能力，</em> <strong class="lc ja"> <em class="lx">通过组合几个自我注意头，</em> </strong> <em class="lx">把词向量分成固定数量(h，头数)的组块，然后在相应的组块上应用自我注意，使用Q，K，V子矩阵。</em>、【2】彼得·布鲁姆、<a class="ae lw" href="http://peterbloem.nl/blog/transformers" rel="noopener ugc nofollow" target="_blank">《从零开始的变形金刚》</a>。这产生了不同的分数输出矩阵。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/de3da116bb6f130c36dbbbfeeffc4c7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*LpDpZojgoKTPBBt8wdC4nQ.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">摘自瓦斯瓦尼等人2017年发表的论文《注意力是你所需要的一切》</p></figure><p id="d91c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是下一层(前馈层)只需要一个矩阵，每个单词一个向量，所以<em class="lx">“在计算每个头部的点积之后，我们将输出矩阵连接起来，并乘以一个额外的权重矩阵</em> <code class="fe mv mw mx my b"><em class="lx">Wo</em></code> <em class="lx">，</em>【3】。这个最终的矩阵从所有的注意力头获取信息。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><h2 id="3827" class="ng lz iq bd ma nh ni dn me nj nk dp mi lj nl nm mk ln nn no mm lr np nq mo iw bi translated">位置编码</h2><p id="c7b1" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj ms ll lm ln mt lp lq lr mu lt lu lv ij bi translated">我们简单地提到，句子中单词的顺序是这个模型中要解决的问题，因为网络和自我注意机制是排列不变的。如果我们打乱输入句子中的单词，我们会得到相同的解。我们需要创建单词在句子中的位置的表示，并将其添加到单词嵌入中。</p><blockquote class="mz na nb"><p id="5027" class="la lb lx lc b ld le ka lf lg lh kd li nc lk ll lm nd lo lp lq ne ls lt lu lv ij bi translated">为此，我们将“位置编码”添加到编码器和解码器堆栈底部的输入嵌入中。位置编码与嵌入具有相同的维数，因此两者可以相加。有许多位置编码的选择。</p><p id="5881" class="la lb lx lc b ld le ka lf lg lh kd li nc lk ll lm nd lo lp lq ne ls lt lu lv ij bi translated">“关注是你所需要的”论文</p></blockquote><p id="f6e0" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，我们应用一个函数将句子中的位置映射到一个实值向量。网络将学习如何使用这些信息。另一种方法是使用位置嵌入，类似于单词嵌入，用向量对每个已知位置进行编码。<em class="lx">“在训练循环期间，需要所有接受的位置的句子，但是位置编码允许模型推断出比训练期间遇到的序列长度更长的序列长度”</em>，【2】。</p><p id="b1af" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在该论文中，应用了正弦函数:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a6df099c06908fef3561ddbf489c355e.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*QuZrGIMQDnhK_vBlwGNPqA.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">摘自瓦斯瓦尼等人2017年发表的论文《注意力是你所需要的一切》[1]。位置编码</p></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h1 id="88dd" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">编码器</h1><p id="a312" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj ms ll lm ln mt lp lq lr mu lt lu lv ij bi translated">现在已经描述了模型的所有主要部分，我们可以介绍编码器组件了，[4]:</p><ul class=""><li id="71c4" class="oi oj iq lc b ld le lg lh lj ok ln ol lr om lv on oo op oq bi translated"><strong class="lc ja">位置编码</strong>:将位置编码添加到输入嵌入中(我们的输入单词被转换成嵌入向量)。<em class="lx">“在两个嵌入层(编码器和解码器)和预softmax线性变换之间共享相同的权重矩阵。在嵌入层中，我们将这些权重乘以模型维度的平方根“</em>”[1]。</li><li id="f4d7" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated"><code class="fe mv mw mx my b">N=6</code>相同层，包含两个子层:一个<strong class="lc ja">多头自关注</strong>机构，和一个<strong class="lc ja">全连接前馈网络(</strong>两个线性变换和一个ReLU激活)。但是它是按位置应用于输入的，这意味着同一个神经网络被应用于属于句子序列的每一个“标记”向量。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/c0c848183beb1d07fe5d0213676d1c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*lZfks2ibauj0ARyouqA1Kg.png"/></div></figure><ul class=""><li id="0d34" class="oi oj iq lc b ld le lg lh lj ok ln ol lr om lv on oo op oq bi translated">每个子层(attention和FC网络)周围都有一个<strong class="lc ja">残差连接</strong>，将该层的输出与其输入相加，之后是一个<strong class="lc ja">层归一化</strong>。</li><li id="ed76" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">在每个残差连接之前，应用一个<strong class="lc ja">正则化</strong>:<em class="lx">“在将每个子层的输出添加到子层输入并归一化之前，我们对每个子层的输出应用丢失。此外，我们对编码器和解码器堆栈“</em>”[1]中的嵌入和位置编码的总和应用丢失，丢失率为0.1。</li></ul><blockquote class="ox"><p id="0282" class="oy oz iq bd pa pb pc pd pe pf pg lv dk translated"><em class="ph">归一化和残差连接是用来帮助深度神经网络更快更准确地训练的标准技巧。层标准化仅应用于嵌入维度。</em></p><p id="2ecc" class="oy oz iq bd pa pb pi pj pk pl pm lv dk translated">彼得·布鲁姆，《从零开始的变形金刚》[2]</p></blockquote><p id="af7c" class="pw-post-body-paragraph la lb iq lc b ld pn ka lf lg po kd li lj pp ll lm ln pq lp lq lr pr lt lu lv ij bi translated">首先，我们实现编码器层，六个模块中的每一个都包含在编码器中:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="9fa3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">下图将显示详细的组件:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/04cd0947e650eb628db2a5815f747669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*y05XJlCykLUE5wPdWfVoiQ.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">杰伊·阿拉玛的《变形金刚》</p></figure><p id="7221" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">编码器代码:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="cf32" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">记住<strong class="lc ja">只有来自最后一层(第6层)的向量被发送到解码器</strong>。</p><h1 id="cc5e" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">解码器</h1><p id="359a" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj ms ll lm ln mt lp lq lr mu lt lu lv ij bi translated">解码器与编码器共享一些组件，但考虑到编码器输出，它们的使用方式有所不同[4]:</p><ul class=""><li id="3767" class="oi oj iq lc b ld le lg lh lj ok ln ol lr om lv on oo op oq bi translated"><strong class="lc ja">位置编码</strong>:类似编码器中的位置编码</li><li id="59a8" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated"><code class="fe mv mw mx my b">N=6</code>相同的层，包含3个三子层。第一，掩蔽多头注意或<strong class="lc ja">掩蔽因果注意</strong>，以防止位置注意后续位置。<em class="lx">“这种屏蔽，结合输出嵌入偏移一个位置的事实，确保了位置I的预测可以仅依赖于小于I的位置处的已知输出”[1] </em>。它是通过设置为与点积注意模块的softmax层中的禁止状态相对应的值来实现的。<strong class="lc ja">第二组件</strong>或<strong class="lc ja">“编码器-解码器关注”</strong>对解码器的输出执行多头关注，键和值向量来自编码器的输出，但是查询来自先前的解码器层。<em class="lx">“这使得解码器中的每个位置都能处理输入序列中的所有位置”【1】</em>。最后是全连接网络。</li><li id="d170" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated"><strong class="lc ja">残差连接</strong>和<strong class="lc ja">层归一化</strong>围绕每个子层，类似于编码器。</li><li id="11a9" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">并重复在编码器中执行的相同的<strong class="lc ja">残差下降</strong>。</li></ul><p id="9b2a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">解码器层:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi pt"><img src="../Images/a75a1d27533ccc2b8c146a9a60bcc2d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zuBmEZDoS2WvNRfIACwrCA.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">杰伊·阿拉玛的《变形金刚》</p></figure><p id="1e3c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在N个堆叠解码器的末端，<strong class="lc ja">线性层</strong>，一个全连接的网络，将堆叠输出转换为一个更大的向量<em class="lx">，即逻辑值</em>。<em class="lx">“</em><strong class="lc ja"><em class="lx">soft max层</em> </strong> <em class="lx">然后把那些分数(logits)变成概率(都是正的，加起来都是1.0)。选择具有最高概率的单元，并产生与之相关的单词作为该时间步长的输出“，</em> [3] Jay Alammar，<a class="ae lw" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">“图示变压器”</a>。</p><p id="5814" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">解码器组件:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><h1 id="53fe" class="ly lz iq bd ma mb pu md me mf pv mh mi kf pw kg mk ki px kj mm kl py km mo mp bi translated">连接所有部件:变压器</h1><p id="e292" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj ms ll lm ln mt lp lq lr mu lt lu lv ij bi translated">一旦我们定义了我们的组件并创建了编码器、解码器和linear-softmax最终层，我们就可以将这些部分连接起来，形成我们的模型，即Transformer。</p><p id="3ae4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">值得一提的是<strong class="lc ja">我们创建了3个遮罩</strong>，每个遮罩都允许我们:</p><ul class=""><li id="4b4f" class="oi oj iq lc b ld le lg lh lj ok ln ol lr om lv on oo op oq bi translated"><em class="lx">编码器掩码</em>:填充掩码，用于从注意力计算中丢弃填充标记。</li><li id="cbff" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated"><em class="lx">解码器掩码1 </em>:该掩码是填充掩码和前瞻掩码的联合，有助于因果注意力“在未来”丢弃令牌。我们取填充掩码和前瞻掩码之间的最大值。</li><li id="c821" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated"><em class="lx">解码器掩码</em> 2:它是填充掩码，应用于编码器-解码器关注层。</li></ul><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="5c8e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">正如你所看到的，然后我们调用编码器、解码器和最终的linear-softmax层来获得我们的Transformer模型的预测输出。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/79c3e61676169587abea20836921d75e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*mLR6lC56lGNLPmERmvFUPg.jpeg"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">图片来自<a class="ae lw" href="https://pixabay.com/es/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2167835" rel="noopener ugc nofollow" target="_blank">pix abay</a>Gerd Altmann</p></figure><h1 id="5254" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">训练变压器模型</h1><p id="0a02" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj ms ll lm ln mt lp lq lr mu lt lu lv ij bi translated">现在，我们已经详细描述了本文中的组件，我们准备实现它们，并在NMT问题上训练一个转换器模型。这是一个出于教育目的的玩具问题。</p><p id="08a7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们不会在这篇博文中处理数据争论。请点击我在简介中提到的链接，获取更多信息，并查看所提供的代码，以了解数据是如何加载和准备的。总之，创建词汇，标记化(包括一个<code class="fe mv mw mx my b">eos</code>和<code class="fe mv mw mx my b">sos</code>标记)并填充句子。然后我们创建一个数据集，一个批量数据生成器，用于批量训练。</p><p id="b641" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们需要<strong class="lc ja">创建一个定制的损失函数</strong>来屏蔽填充标记。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="3ec7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们使用论文中描述的Adam优化器，包括<code class="fe mv mw mx my b">beta1=0.9</code>、<code class="fe mv mw mx my b">beta2=0.98</code>和<code class="fe mv mw mx my b">epsilon=10e-9</code>。然后，我们<strong class="lc ja">创建一个调度程序，根据以下内容改变训练过程中的学习率</strong>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/dcb7421332899ee629f65c87b51de354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*jyiw6G4PiYIluy4qVhUNdA.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">“你所需要的只是关注”论文。学习率衰减。</p></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h2 id="46f9" class="ng lz iq bd ma nh ni dn me nj nk dp mi lj nl nm mk ln nn no mm lr np nq mo iw bi translated">主训练功能</h2><p id="efdc" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj ms ll lm ln mt lp lq lr mu lt lu lv ij bi translated">训练功能类似于许多其他Tensorflow训练，是序列到序列任务的常见训练循环:</p><ul class=""><li id="7fe1" class="oi oj iq lc b ld le lg lh lj ok ln ol lr om lv on oo op oq bi translated">对于生成批量输入和输出的批量生成器上的每次迭代</li><li id="dbe5" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">得到从0到length-1的输入序列，以及从1到length的实际输出，每个序列步骤中预期的下一个字。</li><li id="3a24" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">调用变压器来获得预测</li><li id="eeb2" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">计算实际输出和预测之间的损失函数</li><li id="af55" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">应用梯度来更新模型中的权重并更新优化器</li><li id="6b5e" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">计算批次数据的平均损失和准确度</li><li id="a473" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">显示一些结果并保存每个时期的模型</li></ul><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="afdd" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">就这样，我们有了训练模型的所有必要元素，我们只需要创建它们并调用训练函数:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi qa"><img src="../Images/0e0c66ea9cb1062243a834013f835b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SDFgMkZWo7cnsfgrXChCkA.jpeg"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><a class="ae lw" href="https://unsplash.com/@korpa?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Jr Korpa </a>在<a class="ae lw" href="https://unsplash.com/s/photos/language-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="527a" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">做预测</h1><p id="a35a" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj ms ll lm ln mt lp lq lr mu lt lu lv ij bi translated">当训练ML模型时，我们不仅对优化损失或准确性感兴趣，我们还希望我们的模型做出足够好的预测，在这种情况下，看看模型如何处理新句子。<strong class="lc ja">预测函数</strong>将向模型输入标记化的句子，并返回预测的新句子，在我们的示例中，是从英语到西班牙语的翻译。</p><p id="6e86" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这是该过程中的步骤:</p><ul class=""><li id="da51" class="oi oj iq lc b ld le lg lh lj ok ln ol lr om lv on oo op oq bi translated">将输入句子标记为一系列标记</li><li id="713a" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">将初始输出序列设置为<code class="fe mv mw mx my b">sos</code>标记</li><li id="5a14" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">直到我们达到最大长度或者模型返回了<code class="fe mv mw mx my b">eos</code>令牌</li><li id="1b72" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">预测下一个单词。该模型返回logits，请记住，在损失计算中应用了softmax函数。</li><li id="73f1" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">获取概率最高的单词在词汇表中的索引</li><li id="cf2a" class="oi oj iq lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">将预测的下一个单词连接到输出序列</li></ul><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="d89d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">最后，我们的最后一个函数接收一个英语句子，调用转换器将其翻译成西班牙语并显示结果。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="ad18" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">对于这个例子，我们只是用模型维度的一些值和前馈网络的单元进行实验，来训练模型一个小时。如果您想要优化模型，您可能应该对其进行更长时间的训练，并为超参数设置许多不同的值。</p><p id="3163" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">代码在我的github库<a class="ae lw" href="https://github.com/edumunozsala/Transformer-NMT" rel="noopener ugc nofollow" target="_blank"> Transformer-NMT </a>中</strong>可用。代码部分摘自Udemy上SuperDataScience团队的一门名为《Python中的现代自然语言处理》的优秀课程。我强烈推荐。</p><p id="1b88" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">翻译的一些例子是:</p><pre class="kp kq kr ks gt qb my qc qd aw qe bi"><span id="e964" class="ng lz iq my b gy qf qg l qh qi"><em class="lx">#Show some translations</em><br/>sentence = "you should pay for it."<br/>print("Input sentence: {}".format(sentence))<br/>predicted_sentence = translate(sentence)<br/>print("Output sentence: {}".format(predicted_sentence))</span><span id="3500" class="ng lz iq my b gy qj qg l qh qi"><strong class="my ja">Input sentence: you should pay for it. <br/>Output sentence: Deberías pagar por ello.</strong></span><span id="b08e" class="ng lz iq my b gy qj qg l qh qi"><em class="lx">#Show some translations</em><br/>sentence = "we have no extra money."<br/>print("Input sentence: {}".format(sentence))<br/>predicted_sentence = translate(sentence)<br/>print("Output sentence: {}".format(predicted_sentence))</span><span id="265a" class="ng lz iq my b gy qj qg l qh qi"><strong class="my ja">Input sentence: we have no extra money. <br/>Output sentence: No tenemos dinero extra.</strong></span><span id="a0f6" class="ng lz iq my b gy qj qg l qh qi"><em class="lx">#Show some translations<br/></em>sentence = "This is a problem to deal with."<br/>print("Input sentence: {}".format(sentence))<br/>predicted_sentence = translate(sentence)<br/>print("Output sentence: {}".format(predicted_sentence))</span><span id="3a92" class="ng lz iq my b gy qj qg l qh qi"><strong class="my ja">Input sentence: This is a problem to deal with. <br/>Output sentence: Este problema es un problema con eso.</strong></span></pre><p id="b3f7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我希望您喜欢尝试Transformer模型。在以后的帖子中，我们将处理另一个NLP任务。</p><h1 id="349d" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">参考</h1><p id="1269" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj ms ll lm ln mt lp lq lr mu lt lu lv ij bi translated">[1]瓦斯瓦尼、阿希什&amp;沙泽尔、诺姆&amp;帕马尔、尼基&amp;乌兹科雷特、雅各布&amp;琼斯、利翁&amp;戈麦斯、艾丹&amp;凯泽、卢卡什&amp;波洛舒欣、伊利亚、<a class="ae lw" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">《注意力就是你需要的一切》</a>，2017。</p><p id="1a52" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[2]彼得·布洛姆，<a class="ae lw" href="http://peterbloem.nl/blog/transformers" rel="noopener ugc nofollow" target="_blank">《从零开始的变形金刚》</a>博文，2019。</p><p id="74b4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[3]杰·阿拉玛，<a class="ae lw" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">《变形金刚》</a>博文，2018。</p><p id="34b2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[4]莉莲翁，<a class="ae lw" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">“注意？立正！!"</a>博文，2018。</p><p id="8622" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[5]里卡多·法因德斯-卡拉斯科，<a class="ae lw" href="https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html" rel="noopener ugc nofollow" target="_blank">《关注是你所需要的全部》的评论</a>博文，2017</p><p id="c0d0" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[6]亚历山大·拉什，<a class="ae lw" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">《带注释的变形金刚》</a>，2018，哈佛NLP小组。</p></div></div>    
</body>
</html>