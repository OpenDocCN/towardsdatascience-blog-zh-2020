<html>
<head>
<title>Practical Introduction to Automation Music Transcription</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动化音乐转录实用介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-introduction-to-automation-music-transcription-3ad8ad40eab6?source=collection_archive---------15-----------------------#2020-09-28">https://towardsdatascience.com/practical-introduction-to-automation-music-transcription-3ad8ad40eab6?source=collection_archive---------15-----------------------#2020-09-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="882f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/03aefd600eb48ea0d5baabc476ae3f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYjN9p8Vpy7rwXunpoNyIA.png"/></div></div></figure><h2 id="2287" class="kh ki iq bd kj kk kl dn km kn ko dp kp kq kr ks kt ku kv kw kx ky kz la lb iw bi translated">1.介绍</h2><p id="a944" class="pw-post-body-paragraph lc ld iq le b lf lg lh li lj lk ll lm kq ln lo lp ku lq lr ls ky lt lu lv lw ij bi translated">FindYourRhythm(【https://www.findyourrhythm.us/home】)是我们团队在MIDS项目顶点设计期间开发的MVP。它采用用户提供的音频，使用开源Python库进行处理，并通过我们的双向长短期记忆(BLSTM)神经网络为多达13种鼓组和音乐打击乐器常见的乐器生成开始预测。预测被转换成MuseScore格式(。mscx)乐谱文件并作为可下载文件提供给用户。如果您对我们的产品感兴趣，请访问我们的网站并试用该产品。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ly"><img src="../Images/406005c3d1b306a986d588995c59bc39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pwzHT8Vt1-JGnAj2"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图1数据管道。(图片由作者提供)</p></figure><p id="908a" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi mm translated">在这里，我希望与任何对音频分析、自动音乐转录或LSTM模型感兴趣的人分享我从这个项目中获得的以下实践经验和知识:</p><ol class=""><li id="92ff" class="mv mw iq le b lf mh lj mi kq mx ku my ky mz lw na nb nc nd bi translated">音频转换(预处理)，</li><li id="e11a" class="mv mw iq le b lf ne lj nf kq ng ku nh ky ni lw na nb nc nd bi translated">递归神经网络(LSTM和比尔斯特姆模型)，</li><li id="6e54" class="mv mw iq le b lf ne lj nf kq ng ku nh ky ni lw na nb nc nd bi translated">和事件分段(峰值拾取方法)</li></ol><h2 id="9e34" class="kh ki iq bd kj kk kl dn km kn ko dp kp kq kr ks kt ku kv kw kx ky kz la lb iw bi translated">2.音频转换</h2><p id="d6bd" class="pw-post-body-paragraph lc ld iq le b lf lg lh li lj lk ll lm kq ln lo lp ku lq lr ls ky lt lu lv lw ij bi translated">我没有任何音频处理或语音识别的经验。所以对我来说，第一个挑战是理解我们听到的声音是如何存储在内存中的，以及机器如何识别不同的声音(鼓乐器)？</p><p id="6a07" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">你和我一样有问题吗？你很想知道答案吗？</p><p id="1f2a" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">我将试着一步一步地解释我是如何得到这些问题的答案的？所以我们走吧！</p><p id="90cf" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">当有人打鼓时，你可以听到声音，但你看不到任何东西。因此，模拟波被用来以声音频率和振幅的信息形象化这种声音。下面是鼓混合音频与声音模拟波。它表明，在视频中，声波随着声音而变化，而且，当几个鼓乐器一起演奏时，声波变得更加复杂。</p><figure class="lz ma mb mc gt ka"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="7e98" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">如果只有一种鼓乐器在演奏，声波会显示出统一的频率和振幅，如下图中的粉红色和黄色实线所示。例如，粉红色实线代表频率为2 Hz的声音，意思是每秒两个波。然而，如果这两个声音(粉红色和黄色的线)重叠，声波就结合了。对于组合声波(绿色实线)，很难提取声音及其频率的信息。这就是傅立叶变换填补这一空白的地方。利用组合声波的傅立叶变换，很容易获得如图所示的具有尖峰的两个声音的频率。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi nm"><img src="../Images/5d4cdb3b2da12c511e652248ecebcce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*I_TFuFRnBW2n5PvG"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图二。声波和声波的傅立叶变换(图片来源:<a class="ae lx" href="https://www.youtube.com/watch?v=spUNpyF58BY&amp;t=626s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=spUNpyF58BY&amp;t = 626s</a></p></figure><p id="f34d" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">限于时间和篇幅，我就不深究傅里叶变换背后的原理，以及如何通过傅里叶变换得到声音频率了。如果你对这个话题感兴趣，并希望更好地理解它，下面的视频肯定会帮助你。</p><figure class="lz ma mb mc gt ka"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="a15a" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">回到图2。，你会意识到一个非常重要的特征，因为音频在傅立叶变换后消失在声波中。<strong class="le ja"> <em class="nj">这个特征就是时间！</em> </strong>所以应用声谱图来组合图2中的两种图形，如下图。这是以前的鼓混合音频的短期傅立叶变换(STFT)频谱图，包括y轴上的频率、x轴上的时间和用颜色编码的振幅。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e9c676f2359f251bbb51b6478b5e88ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*ZBmlJY1o08ci6FvXzkcFrQ.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">图三。鼓混合音频的短时傅立叶变换谱图(图片由作者提供)。</p></figure><p id="bb36" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">到目前为止，我们已经将音频信息可视化。以下步骤用于将此信息转换为模型输入:</p><p id="2a1e" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated"><strong class="le ja">第一步:</strong>使用<a class="ae lx" href="https://librosa.org/doc/latest/index.html" rel="noopener ugc nofollow" target="_blank"> Librosa </a>将音频转换为STFT声谱图(打开python库进行音频和音乐分析)。选择作为馈入RNN模型的输入的固定时间窗口的片段长度。这将在模拟会议中进一步讨论。</p><p id="e8b5" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated"><strong class="le ja">第二步:</strong>将声谱图转移到数组:数组中的每一行代表频率级别，列是时间帧。数组中的值代表振幅。为了避免两首歌曲连接成一个片段，值为0的静音会根据音频的大小填充在歌曲的结尾。</p><p id="56eb" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated"><strong class="le ja">步骤3: </strong>每首歌曲将具有相同数量的频率级别，但总时间不同。因此，该数组被转置，以便每首歌曲都有相同的列数。</p><p id="fa4c" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated"><strong class="le ja">步骤4: </strong>使用这种数组形状，可以很容易地将开放数据库中可用歌曲的频谱图连接在一起，并输入到模型中。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi no"><img src="../Images/46aebc7d0dd78e74bf4ed27cbf30611e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7r5-Kpw7kCgdLefrIPeMNA.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图4从光谱图到模型输入(图片由作者提供)</p></figure><h2 id="0fb3" class="kh ki iq bd kj kk kl dn km kn ko dp kp kq kr ks kt ku kv kw kx ky kz la lb iw bi translated">3.1基本递归神经网络(RNN)</h2><p id="9c02" class="pw-post-body-paragraph lc ld iq le b lf lg lh li lj lk ll lm kq ln lo lp ku lq lr ls ky lt lu lv lw ij bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di">R</span>nn代表DNNs的扩展，其特征在于与每层的额外连接。递归连接为单层提供了前一时间步的输出作为附加输入，因此在对序列相关行为(如语音识别、股票市场事件和自动音乐转录)进行建模时，它表现得更好。</p><p id="6df6" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">图5示出了在鼓自动转录(DAT)上应用RNN的步骤:</p><p id="ab1e" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated"><strong class="le ja">步骤a) </strong>将图4中说明的连接的STFT频谱图(X)输入到RNN模型中</p><p id="9f19" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated"><strong class="le ja">步骤b) </strong>根据定义的段长度(本项目使用5秒)将输入分割成小时间窗口(段)。这意味着声谱图帧(X <em class="nj"> t </em>)被分割成5秒的片段，并依次作为输入特征，与前一时间步的输出(h <em class="nj"> t-1 </em>)一起馈入隐含层。</p><p id="02dc" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated"><strong class="le ja">步骤c) </strong>图5 c和d显示了带有tanh激活函数的隐藏层的输出。</p><p id="6adb" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated"><strong class="le ja">步骤d) </strong>图5 e是来自未折叠RNN的乙状结肠神经元的输出。参见图6中折叠和展开的RNN之间的差异。基本上，展开的RNN将提供整个时间的所有输出，而折叠的RNN将只输出一个片段中的最后一个时间步长。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi np"><img src="../Images/516f2086f790599290999cebb7fa303f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eiR3ORVXI4nXGxKBZf5rg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图5 RNN在自动鼓转录中的应用(图片来源:DOI 10.1109/taslp . 2830113)</p></figure><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/50fc945a54194e44a3a36f75bec79c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*IrzUVwbeA6d4xxp6gJAWlQ.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">图6 RNN折叠和展开时的区别(图片来源:<a class="ae lx" href="https://adventuresinmachinelearning.com/keras-lstm-tutorial/" rel="noopener ugc nofollow" target="_blank">https://adventuresinmachinehlearning . com/keras-lstm-tutorial/</a>)</p></figure><p id="97ee" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated"><em class="nj">然而</em>香草RNN在实践中有渐变消失的问题。对RNN来说，我们希望有长久的记忆，这样网络就能在理解音乐或语言如何工作方面取得真正的进展。然而，随着时间的推移，我们实际上是在我们的网络中添加越来越深的层。如图7所示，当输出(f)接近0和1时，s形梯度(f’)变得非常小。图7中的等式是随时间反向传播的粗略近似。很明显，当在反向传播期间将许多sigmoid梯度(图7中的等式)与非常小的值相乘时，会出现消失梯度。处理梯度消失问题最流行的方法是使用长短期记忆(LSTM)网络。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi nr"><img src="../Images/0d14222753e881d6ecfc9e55d21a355d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KhTc_6z2--kCD2OgHAg6uQ.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图7 Sigmoid激活函数及其梯度(图片来源:<a class="ae lx" href="https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" rel="noopener ugc nofollow" target="_blank">https://adventuresinmachine learning . com/recurrent-neural-networks-lstm-tutorial-tensor flow/</a>)</p></figure><h2 id="e82b" class="kh ki iq bd kj kk kl dn km kn ko dp kp kq kr ks kt ku kv kw kx ky kz la lb iw bi translated">3.2 LSTM和比尔斯特姆网络</h2><p id="ba8d" class="pw-post-body-paragraph lc ld iq le b lf lg lh li lj lk ll lm kq ln lo lp ku lq lr ls ky lt lu lv lw ij bi translated">为了减少消失梯度问题，并且同样允许更深的网络和长记忆更好地理解音乐/语言，需要一种方法来减少在通过时间反向传播期间具有小值的梯度的乘法。</p><p id="87d9" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">LSTM模型是专门为解决消失梯度问题而设计的，它通过创建一个内存状态来解决消失梯度问题，该内存状态通过遗忘门简单地对<em class="nj">进行过滤</em>，并通过<em class="nj">将</em>添加到已处理的输入中。遗忘门决定哪些先前状态应该被记住(遗忘门输出= 1)，哪些应该被遗忘(遗忘门输出=0)。这允许LSTM细胞只学习有用的上下文。此外，遗忘门滤波状态被添加到输入，而不是乘以它，这大大降低了小梯度的乘法效应。LSTM模型非常灵活，用门控函数来控制什么是输入，什么是在内存状态中记忆的，以及什么是最终从LSTM单元输出的。由于这些原因，LSTM模型目前被广泛用于依赖于时间的行为预测。这里我只分享我从LSTM模型中学到的一些要点，如果你对LSTM模型以及它如何帮助解决消失梯度问题感兴趣，博客<a class="ae lx" href="https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" rel="noopener ugc nofollow" target="_blank">“Python和TensorFlow中的递归神经网络和LSTM教程”</a>非常有用，将是对LSTM网络的一个很好的介绍。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ns"><img src="../Images/bb03e727fc04e5f8404468ba0d87e091.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V1GzcFN6noKxJReADkXpKQ.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图8 LSTM细胞图(图片来源:<a class="ae lx" href="https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" rel="noopener ugc nofollow" target="_blank">https://adventuresinmachine learning . com/recurrent-neural-networks-lstm-tutorial-tensor flow/</a>)</p></figure><p id="ef0b" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">最后，我们采用双向LSTM (BiLSTM)模型，该模型通过保留过去和未来的信息来进一步提高模型性能。这是通过以两种方式运行输入来实现的，一种是从过去到未来，另一种是从未来到过去，如图9所示。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/efe8c0e86b496119e66f0aef62385935.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*9oHlutuLykqF89HE9DTZhg.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">图9 BiLSTM模型(图片来源:colah的博客)</p></figure><h2 id="08c2" class="kh ki iq bd kj kk kl dn km kn ko dp kp kq kr ks kt ku kv kw kx ky kz la lb iw bi translated">3.3使用Keras创建LSTM网络</h2><blockquote class="nu nv nw"><p id="d0e0" class="lc ld nj le b lf mh lh li lj mi ll lm nx mj lo lp ny mk lr ls nz ml lu lv lw ij bi translated"><strong class="le ja"><em class="iq">LSTM模型的尺寸输入</em> </strong></p></blockquote><p id="8a53" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">该项目使用了三个数据库:</p><ol class=""><li id="55c5" class="mv mw iq le b lf mh lj mi kq mx ku my ky mz lw na nb nc nd bi translated"><strong class="le ja">IDMT-SMT-鼓:</strong>超过2小时的音频，包括104首歌曲，只有3种鼓的音高，分别为小军鼓、踢鼓和踩镲。开始和乐器都包含在音乐XML转录中。</li><li id="8dfe" class="mv mw iq le b lf ne lj nf kq ng ku nh ky ni lw na nb nc nd bi translated"><strong class="le ja"> MDB鼓:</strong> &gt; 20分钟音频，23首独特歌曲，15种鼓点。音乐标签与开始和乐器一起呈现。</li><li id="98f6" class="mv mw iq le b lf ne lj nf kq ng ku nh ky ni lw na nb nc nd bi translated"><strong class="le ja">电子GMD: </strong>超过400小时的鼓乐歌曲，20种鼓点和标签，包括开始，持续时间，速度，MIDI音高，时间信号，风格和套件。</li></ol><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oa"><img src="../Images/0932152b97fd2a97290bf357d04d0409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tfpZd_sQxMyktAH7C6WufA.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图10数据来源(图片由作者提供)</p></figure><p id="0864" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">来自这三个数据库的音频被转换和连接，如图4所示。LSTM模型训练成功的重要步骤是理解图11所示的模型输入形状。</p><ol class=""><li id="ff39" class="mv mw iq le b lf mh lj mi kq mx ku my ky mz lw na nb nc nd bi translated"><strong class="le ja">浴槽尺寸:</strong>使用梯度下降来训练神经网络，其中基于训练数据集的子集来计算用于更新权重的误差估计。用于估计误差梯度的训练数据集中的样本数称为批量。它是影响学习算法动态的一个重要的超参数。</li><li id="620a" class="mv mw iq le b lf ne lj nf kq ng ku nh ky ni lw na nb nc nd bi translated"><strong class="le ja">片段长度:</strong>内存大小对LSTM模型的性能至关重要，由片段长度决定，片段长度是输入我们展开的LSTM网络的音频的子序列(时间窗口)。</li><li id="1392" class="mv mw iq le b lf ne lj nf kq ng ku nh ky ni lw na nb nc nd bi translated"><strong class="le ja">频段数:</strong>输入我们LSTM网络的每一帧音频将是1025(代表声谱图的频率级数)长度向量。</li></ol><p id="2161" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated"><strong class="le ja">综上</strong>，我们这里的模型输入是多维的，大小为(批量大小，段长度，频率仓数量)。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/454d079b56a08ae98204fa57e3637de4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*nSjqd4XBxx0Hujoe8Lmd3Q.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">图11模型输入形状的图示(作者提供的图片)</p></figure><blockquote class="nu nv nw"><p id="8127" class="lc ld nj le b lf mh lh li lj mi ll lm nx mj lo lp ny mk lr ls nz ml lu lv lw ij bi translated"><strong class="le ja"> <em class="iq">区别Keras。适合和。Keras中的fit _ generator</em></strong></p></blockquote><p id="e97d" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">Keras深度学习库包括两个独立的函数，可用于训练我们的模型(。适合和。fit_generator)。下面列出了这两种功能之间的区别:</p><p id="c5bf" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated"><strong class="le ja"> <em class="nj">注意:</em> </strong> <em class="nj">在未来的版本中，Model.fit_generator将被移除。这可以通过使用同样支持生成器的Model.fit来实现。</em></p><ol class=""><li id="c1a0" class="mv mw iq le b lf mh lj mi kq mx ku my ky mz lw na nb nc nd bi translated"><strong class="le ja">配合</strong></li></ol><p id="55fc" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">Keras拟合函数在以下情况下效果更好:</p><ul class=""><li id="2db6" class="mv mw iq le b lf mh lj mi kq mx ku my ky mz lw oc nb nc nd bi translated">原始数据本身将适合内存——我们不需要将旧的数据批次移出RAM，并将新的数据批次移入RAM</li><li id="bab2" class="mv mw iq le b lf ne lj nf kq ng ku nh ky ni lw oc nb nc nd bi translated">我们不会使用数据扩充(填充、裁剪和水平翻转)来操作训练数据</li></ul><p id="c347" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">与fit_generator函数相比，Keras fit函数要简单得多，尽管它需要很大的内存空间来保存模型训练期间的全部原始数据。当使用拟合函数时，我们只需要将我们的级联音频频谱图(图4)转换为LSTM模型形状(批量大小、分段长度、频率仓数量)，如前所述。这里，我们创建一个函数来为训练、验证和测试数据执行这种数据转换。</p><figure class="lz ma mb mc gt ka"><div class="bz fp l di"><div class="od nl l"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">Python代码1:将音频分割成5秒窗口的分段函数</p></figure><p id="83d2" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">2.fit_generator </p><p id="b9b9" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">Keras fit_generator函数在以下情况下工作得更好</p><ul class=""><li id="4803" class="mv mw iq le b lf mh lj mi kq mx ku my ky mz lw oc nb nc nd bi translated">真实世界的数据集通常太大而无法放入内存。fit_generator使用用户创建的生成器函数将成批数据移入和移出RAM。</li><li id="caa7" class="mv mw iq le b lf ne lj nf kq ng ku nh ky ni lw oc nb nc nd bi translated">需要数据扩充来避免过度拟合，并提高我们模型的泛化能力</li></ul><p id="6a82" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">Keras fit_generator函数能够从用户创建的预定义Python生成器对象中自动提取训练和验证数据，并将其输入到模型中。下面是本项目中使用的生成器对象的示例:</p><figure class="lz ma mb mc gt ka"><div class="bz fp l di"><div class="od nl l"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">Python代码2:生成函数来提取训练批次以输入到模型中</p></figure><blockquote class="nu nv nw"><p id="8adc" class="lc ld nj le b lf mh lh li lj mi ll lm nx mj lo lp ny mk lr ls nz ml lu lv lw ij bi translated"><strong class="le ja"> <em class="iq"> Keras BiLSTM型号</em> </strong></p></blockquote><p id="7da1" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">Keras是一个强大的API，非常容易应用于深度学习。下面是Keras中的BiLSTM网络，使用Keras.model.fit函数进行模型训练。每一步都有注释。<em class="nj">现在模型已经准备好了！</em></p><figure class="lz ma mb mc gt ka"><div class="bz fp l di"><div class="od nl l"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">Python代码Keras中的BiLSTM网络</p></figure><p id="0b09" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">在上面Keras.model.fit函数的callbacks参数中，有一个自己创建的名为plot _ losses的函数，用于绘制每个历元之后的训练和验证损失。通过绘图评估每个时期的模型性能比检查每个时期的度量更容易。下面是plot _ losses函数的代码。</p><figure class="lz ma mb mc gt ka"><div class="bz fp l di"><div class="od nl l"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">Python代码4:在模型训练期间创建动态损失图的函数</p></figure><p id="38b7" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">如果使用大型数据集和有限的内存大小来训练模型，建议使用Keras.model.fit_generator函数。下面是用fit_generator代替fit函数的方法。Keras fit_generator函数与python迭代函数(上一节中介绍的python代码2中的generate函数)的第一次增强将每次提取一批数据，执行小批量梯度下降法以更新权重，记录每个批量训练和验证数据集的度量(如准确度、MSE、混淆矩阵),并执行回调。</p><p id="85ea" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">函数中的steps_per_epoch参数用于定义每个时期的迭代次数，以终止上一节中介绍的生成函数中的无限循环，并在达到步数时开始新的时期(与validation_steps相同)。</p><figure class="lz ma mb mc gt ka"><div class="bz fp l di"><div class="od nl l"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">Python代码5:使用Keras.model. fit_generator函数的BiLSTM模型训练</p></figure><p id="f76a" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">在模型讨论的最后，我希望简单说一下神经网络(NN)的注意事项。对于NN，通过大量的训练，它可以容易地记忆训练数据集，具有高方差和低偏差，如图12所示，但是记忆不是学习。所以在这项研究中，我们应用了以下方法来避免记忆:</p><ul class=""><li id="eae9" class="mv mw iq le b lf mh lj mi kq mx ku my ky mz lw oc nb nc nd bi translated">通过减少层和隐藏节点的数量来简化模型</li><li id="b541" class="mv mw iq le b lf ne lj nf kq ng ku nh ky ni lw oc nb nc nd bi translated">添加漏失层并评估最佳漏失率，以获得高精度的概化模型</li><li id="53ee" class="mv mw iq le b lf ne lj nf kq ng ku nh ky ni lw oc nb nc nd bi translated">当验证损失增加时，应用提前停止来停止培训</li><li id="cc5f" class="mv mw iq le b lf ne lj nf kq ng ku nh ky ni lw oc nb nc nd bi translated">收集更多数据</li></ul><p id="7a04" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">这些是广泛用于减少NN模型方差并确保训练模型平衡的通用方法。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/41351ca39dc0b35cd32be5456aea1ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*oDYEqOdOVgn-t9jpqkK1Yw.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">图12模型欠拟合和过拟合(作者图片，灵感来源:<a class="ae lx" href="https://subscription.packtpub.com/book/data/9781838556334/7/ch07lvl1sec82/underfitting-and-overfitting" rel="noopener ugc nofollow" target="_blank">https://subscription . packtpub . com/book/data/9781838556334/7/ch 07 lvl 1 sec 82/欠拟合和过拟合</a></p></figure><h2 id="872d" class="kh ki iq bd kj kk kl dn km kn ko dp kp kq kr ks kt ku kv kw kx ky kz la lb iw bi translated">4.峰值拾取方法和模型评估</h2><p id="3968" class="pw-post-body-paragraph lc ld iq le b lf lg lh li lj lk ll lm kq ln lo lp ku lq lr ls ky lt lu lv lw ij bi translated">图13a示出了三种鼓乐器的来自BiLSTM模型的sigmoid神经元的输出。应该应用峰值拾取方法来检测每个仪器的开始。传统的方法是对乙状结肠神经元的输出应用阈值函数(如果输出&gt;阈值，则为1，否则为0)。然而，这种方法的问题在于，如图14b所示，基于各种歌曲类型、架子鼓类型以及鼓乐器，开始峰值显示不同的值。如果使用这种常规方法，它将在高概率峰值周围提供许多值为1的开始，并以低概率错过真正的峰值，从而导致高FPs和FNs。因此，使用图15中的峰值拾取方法。在应用这种峰值拾取方法之后，我们可以成功地将乙状结肠神经元的输出转移到图13b中的发作。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div class="gh gi of"><img src="../Images/6a3a169a4dab3424bb236e65f8bb2ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*HLrmqzaUkvIw2HLIgizDzg.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">图13 (a)来自BiLSTM模型的乙状结肠神经元的输出(b)使用峰值拾取方法的发作检测(作者提供的图像)</p></figure><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi og"><img src="../Images/9eaff50c921fa1b5e4be85d2f8b43a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M_a_BufwXZfxPjIqs7YMmw.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图14 (a)阈值函数和(b)Sigmoid概率输出的示意图，峰值代表仪器开始(由来源<a class="ae lx" href="https://www.vojtech.net/img/machine-learning/sigmoid-function.png" rel="noopener ugc nofollow" target="_blank">https://www . vojtech . net/img/machine-learning/Sigmoid-Function . png</a>成像)</p></figure><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/621bc49570c88d640a1fc7134c492399.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*iWrbqcrygG5oZdRqD-yvFg.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">图15峰值拾取方法的等式(图片由作者提供)</p></figure><p id="2cba" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">图16示出了文献中的模型和我们的BiLSTM模型对于各种鼓乐器的f1分数。对于这个项目，我们的模型不仅为大多数ADT文献中发现的三种常见乐器(底鼓、小军鼓和踩镲)提供了准确的开始检测，而且还专注于对涵盖鼓转录中各种音符的10多种鼓乐器进行建模。</p><figure class="lz ma mb mc gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oi"><img src="../Images/7041c8b0d4b19bf51b78116a8e62d8ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZAj_nWankAWiCtqzRCju1A.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">图16模型评估(数据来源:R.S .，J.H .和C.S .，“使用双向递归神经网络的自动鼓转录”dblp，2016。图片作者)</p></figure><h2 id="a97b" class="kh ki iq bd kj kk kl dn km kn ko dp kp kq kr ks kt ku kv kw kx ky kz la lb iw bi translated">5.结论</h2><p id="d345" class="pw-post-body-paragraph lc ld iq le b lf lg lh li lj lk ll lm kq ln lo lp ku lq lr ls ky lt lu lv lw ij bi translated"><a class="ae lx" href="https://docs.google.com/presentation/d/1NehI9P8tiGj2s54kRSn7hcAxPuIaepOsx3iZemTDkkc/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>是我们最终的FindYourRhythm应用程序的演示和介绍，为所有鼓手提供免费的鼓点转录，无论音乐品味、熟练程度或预算如何。所有资源(如GitHub库和顶点页)都可以在我们的<a class="ae lx" href="https://www.findyourrhythm.us/home" rel="noopener ugc nofollow" target="_blank">网站</a>上找到。最后，我要感谢<a class="ae lx" href="https://www.findyourrhythm.us/team" rel="noopener ugc nofollow" target="_blank">我的FindYourRhythm团队和指导老师</a>一起致力于这个奇妙的项目。</p><p id="ad3a" class="pw-post-body-paragraph lc ld iq le b lf mh lh li lj mi ll lm kq mj lo lp ku mk lr ls ky ml lu lv lw ij bi translated">感谢您的关注。如果你有任何问题，你可以打电话给我:<a class="ae lx" href="https://www.linkedin.com/in/yue-hu-69883469/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/yue-hu-69883469/</a></p></div></div>    
</body>
</html>