<html>
<head>
<title>[Deep learning] Introduction of Generative Adversarial Networks (GANs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">【深度学习】引入生成对抗网络(GANs)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-introduction-of-generative-adversarial-networks-gans-ae22c4350b1f?source=collection_archive---------26-----------------------#2020-10-31">https://towardsdatascience.com/deep-learning-introduction-of-generative-adversarial-networks-gans-ae22c4350b1f?source=collection_archive---------26-----------------------#2020-10-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9d05" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">甘斯的直觉和客观功能</h2></div><p id="251c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">生成对抗网络(GANs)形成于2014年[1]，是一种先进的深度神经网络，具有许多应用。与无监督学习中的传统机器学习(它不需要目标标签)不同，GANs是一种通过给定数据生成新内容的生成模型。</p><p id="5513" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有趣的是，GANs首先从原始论文的右侧看到了MNIST(手写数字数据库)的生成图像:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/98bc9eabb588b1bd2929a754d54d5e16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IImVhNJiknlTNqTZwCLk-A.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><a class="ae lu" href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="noopener ugc nofollow" target="_blank">MNIST GANs从原纸生成的图像</a></p></figure><h1 id="5377" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">直觉</strong></h1><p id="7def" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">GANs的类比被认为是伪钞制造者和警察之间的假币检测游戏[1]。根据Goodfellow [2]对GANs的教程，GANs由两个角色组成，分别是生成者(造假者)和鉴别者(警察)。</p><p id="edde" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">伪造者试图制造假币，并通过查看真钞来欺骗警察(鉴别者)。鉴别器的工作就是鉴别给定的钱是不是真的。首先，伪造者制造的货币太粗糙，很容易辨别。基于这些失败，现在伪造者努力生产更复杂的货币。与此同时，警察现在更有经验来辨别真假钱。</p><p id="577d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着这一过程重复多次，双方都向对方学习(这就是它被称为“对抗性”的原因)，因此变得足够成熟和老练。现在，造假者生产的假币对除了警察以外的其他人来说都是非常逼真的。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ms"><img src="../Images/ab73c32524f60dfdd85aae7beffb1e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AqtlCqIeXiyRzU--_6Ue1A.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">打假人与警察博弈的类比。图片由作者提供。</p></figure></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="45be" class="lv lw it bd lx ly na ma mb mc nb me mf jz nc ka mh kc nd kd mj kf ne kg ml mm bi translated">GANs的目标函数</h1><p id="47b5" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">回想一下，目标函数是在训练时要优化的函数(通常是最大化它，如果我们要最小化它，通常它被称为损失函数)。寻找目标函数的最优点有几种方法，如最大似然估计(MLE)和不同的梯度下降法等。</p><p id="ee36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GANs由两个深度神经网络组成，生成器网络(表示为G)和鉴别器网络(表示为D)。G的目的是从分布p_data中输出假数据G(z)来欺骗D。另一方面，鉴别器网络D输出真实数据的概率，其目标是最大化真实标签的概率和最小化虚假标签的概率(分布为p_z)。</p><p id="8df9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在训练期间，G和D基于下面的最小-最大目标函数V(G；d):</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nf"><img src="../Images/a58ea4f1fb07a4d3436eeab56064e4a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZwLn3oB823Waj1UkNqwPlg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式1。作者图片</p></figure><p id="155a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">鉴别器应该给真实图像(D_θd(x))一个高值，这与对它取逻辑函数((logD_θd(x)))是一样的。它还应该给假图像(G_θg(z))一个低值，这与减去它并取逻辑函数(log(1- D_θd(x))是一样的。简而言之，可以通过上述目标函数V(G；d)为d。</p><p id="6ce4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于生成器来说，最好是愚弄生成器，如果D_θd(x)接近1，就可以实现，这与最小化log(1- D_θd(x))相同。对于生成器来说是有区别的，也就是说，最小-最大函数V(G；d)不依赖于发电机。(在打假人与警察的博弈类比中，警察能否识别真钱与打假人无关。)简而言之，它与最小化目标函数V(G；d)对于θg。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="7c24" class="lv lw it bd lx ly na ma mb mc nb me mf jz nc ka mh kc nd kd mj kf ne kg ml mm bi translated"><strong class="ak">训练甘斯</strong></h1><p id="5acc" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">由于训练中的实际原因，对发生器部分进行了修改，而不是原来的目标函数:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ng"><img src="../Images/4b804e986317516faedc285fc2b6b473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1m-cmbkQxlKv1vYq8e-LA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式2。作者图片</p></figure><p id="d674" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">原因是在训练开始时，生成的伪图像太明显(即D_θd(x)接近于零),因此损失几乎为零并且具有零梯度。</p><p id="c7c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于新的目标函数，当D_θd(x)较小时，梯度比原来的高:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nh"><img src="../Images/c748c2b37db382f669463f406819f815.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K4ZGQ3Y9zMpgVrzssxAXGg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式3。作者图片</p></figure><p id="5be6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是对于一个梯度:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ni"><img src="../Images/5d769392a411bb7ca92a1adda35f0716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TyWwJumr-NpLYJJgZXc85A.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式4。作者图片</p></figure><p id="646d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当D_θd(x)较小时，等式3中的分母比等式4中的分母小得多。</p><p id="6f99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">原始论文陈述了鉴别器D的最佳点和训练过程，这里跳过这些以避免数学细节和直接复制，但是这里提供了用于说明的非正式想法:</p><ul class=""><li id="132f" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld no np nq nr bi translated">训练在小批量的噪声样本和真实例子中进行。minibatch的目标是加快训练过程，因为通过所有数据训练DNN是非常繁重的。</li><li id="7133" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">发电机的原始随机梯度下降通常被方程2的随机梯度上升代替。</li></ul></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="c403" class="lv lw it bd lx ly na ma mb mc nb me mf jz nc ka mh kc nd kd mj kf ne kg ml mm bi translated">参考资料:</h1><ol class=""><li id="5551" class="nj nk it kk b kl mn ko mo kr nx kv ny kz nz ld oa np nq nr bi translated">古德费勒、j .普热-阿巴迪、m .米尔扎、b .徐、d .沃德-法利、s .奥泽尔、a .库维尔和y .本吉奥。生成对抗网络。在NIPS，2014。</li><li id="b2f0" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld oa np nq nr bi translated">好家伙。Nips 2016教程:生成性对抗网络。arXiv预印本arXiv:1701.00160，2016。</li></ol></div></div>    
</body>
</html>