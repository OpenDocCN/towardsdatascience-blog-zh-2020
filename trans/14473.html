<html>
<head>
<title>How to Compute Sentence Similarity Using BERT and Word2Vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用BERT和Word2Vec计算句子相似度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-compute-sentence-similarity-using-bert-and-word2vec-ab0663a5d64?source=collection_archive---------6-----------------------#2020-10-06">https://towardsdatascience.com/how-to-compute-sentence-similarity-using-bert-and-word2vec-ab0663a5d64?source=collection_archive---------6-----------------------#2020-10-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9d15" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">NLP-生产中</h2><div class=""/><div class=""><h2 id="0c86" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">以及防止计算句子嵌入时常见错误的一些见解</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d23ecb3afaf7fa56a4fba33b6e4188df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wQDsvGFCdoTa057M"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">卡蒂亚·奥斯丁在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="acb9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们经常需要将文本数据，包括单词、句子或文档编码成高维向量。句子嵌入是各种自然语言处理任务中的一个重要步骤，例如情感分析和文摘。<strong class="lk jd">需要一个灵活的句子嵌入库来快速原型化，并针对各种上下文进行调整。</strong></p><p id="773e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">过去，我们大多使用one-hot、term-frequency或TF-IDF(也称为归一化term-frequency)等编码器。然而，在这些技术中没有捕获单词的语义和句法信息。最近的进步使我们能够以更有意义的形式对句子或单词进行编码。word2vec技术和BERT语言模型是其中两个重要的技术。注意，在这个上下文中，我们交替使用嵌入、编码或矢量化。</p><p id="5155" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">开源的<a class="ae lh" href="https://github.com/pdrm83/Sent2Vec" rel="noopener ugc nofollow" target="_blank"> sent2vec </a> Python库可以让你高度灵活地编码句子。<strong class="lk jd"> </strong>您目前可以访问库中的标准编码器。更高级的技术将在以后的版本中添加。在本文中，我想介绍这个库，并分享我在这方面学到的经验。</p><p id="9817" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您不熟悉Word2Vec模型，我推荐您先阅读下面的文章。<strong class="lk jd">你会发现为什么Word2Vec模型在机器学习中既简单又具有革命性。</strong></p><div class="me mf gp gr mg mh"><a rel="noopener follow" target="_blank" href="/word2vec-models-are-simple-yet-revolutionary-de1fef544b87"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd jd gy z fp mm fr fs mn fu fw jc bi translated">Word2Vec模型简单而具有革命性</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">Gensim还是spaCy？不了解Word2Vec机型的基础知识也没关系。</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">towardsdatascience.com</p></div></div><div class="mq l"><div class="mr l ms mt mu mq mv lb mh"/></div></div></a></div></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="8826" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">—如何使用“sent 2 vec”Python包</h1><h2 id="9bc9" class="nv ne it bd nf nw nx dn nj ny nz dp nn lr oa ob np lv oc od nr lz oe of nt iz bi translated">如何安装</h2><p id="9ff4" class="pw-post-body-paragraph li lj it lk b ll og kd ln lo oh kg lq lr oi lt lu lv oj lx ly lz ok mb mc md im bi translated">由于<a class="ae lh" href="https://github.com/pdrm83/Sent2Vec" rel="noopener ugc nofollow" target="_blank"> sent2vec </a>是一个高级库，它依赖于<a class="ae lh" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> spaCy </a>(用于文本清理)<a class="ae lh" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank"> Gensim </a>(用于word2vec模型)<a class="ae lh" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> Transformers </a>(用于各种形式的BERT模型)。因此，确保在使用下面的代码安装sent2vec之前安装这些库。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="279a" class="nv ne it om b gy oq or l os ot">pip3 install sent2vec</span></pre><h2 id="5f6c" class="nv ne it bd nf nw nx dn nj ny nz dp nn lr oa ob np lv oc od nr lz oe of nt iz bi translated">如何使用伯特方法</h2><p id="eacb" class="pw-post-body-paragraph li lj it lk b ll og kd ln lo oh kg lq lr oi lt lu lv oj lx ly lz ok mb mc md im bi translated">如果要使用<code class="fe ou ov ow om b">BERT</code>语言模型(更确切地说是<code class="fe ou ov ow om b">distilbert-base-uncased</code>)为下游应用程序编码句子，必须使用下面的代码。目前，<a class="ae lh" href="https://github.com/pdrm83/Sent2Vec" rel="noopener ugc nofollow" target="_blank"> sent2vec </a>库只支持DistilBERT模型。未来将支持更多型号。由于这是一个开源项目，您还可以深入研究源代码，找到更多的实现细节。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://github.com/pdrm83/Sent2Vec" rel="noopener ugc nofollow" target="_blank"><strong class="ak">sent 2 vec</strong></a><strong class="ak">——</strong>如何使用BERT计算句子嵌入</p></figure><p id="5ab0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以用句子的向量来计算句子之间的距离。在示例中，正如所料，<code class="fe ou ov ow om b">vectors[0]</code>和<code class="fe ou ov ow om b">vectors[1]</code>之间的距离小于<code class="fe ou ov ow om b">vectors[0]</code>和<code class="fe ou ov ow om b">vectors[2]</code>之间的距离。</p><p id="7b77" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，默认的矢量器是<code class="fe ou ov ow om b">distilbert-base-uncased</code>，但是可以传递参数<code class="fe ou ov ow om b">pretrained_weights</code>来选择另一个<code class="fe ou ov ow om b">BERT</code>模型。例如，您可以使用下面的代码来加载基本的多语言模型。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="3cd4" class="nv ne it om b gy oq or l os ot">vectorizer = Vectorizer(pretrained_weights='distilbert-base-multilingual-cased')</span></pre><h2 id="041b" class="nv ne it bd nf nw nx dn nj ny nz dp nn lr oa ob np lv oc od nr lz oe of nt iz bi translated">如何使用Word2Vec方法</h2><p id="393f" class="pw-post-body-paragraph li lj it lk b ll og kd ln lo oh kg lq lr oi lt lu lv oj lx ly lz ok mb mc md im bi translated">如果您想使用Word2Vec方法，您必须向模型权重传递一个有效的路径。在引擎盖下，使用来自<code class="fe ou ov ow om b">Splitter</code>类的<code class="fe ou ov ow om b">sent2words</code>方法将句子拆分成单词列表。该库首先提取句子中最重要的单词。然后，它使用与这些单词相对应的向量的平均值来计算句子嵌入。你可以使用下面的代码。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://github.com/pdrm83/Sent2Vec" rel="noopener ugc nofollow" target="_blank"><strong class="ak">sent 2 vec</strong></a><strong class="ak">—</strong>如何使用<code class="fe ou ov ow om b">word2vec</code>计算句子嵌入</p></figure><p id="0ea8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">可以通过在默认列表中添加或删除来自定义停用词列表。当调用矢量器的方法<code class="fe ou ov ow om b">.run</code>时，必须传递两个附加参数(两个列表):<code class="fe ou ov ow om b">remove_stop_words</code>和<code class="fe ou ov ow om b">add_stop_words</code>。在进行任何计算之前，研究停用词表是至关重要的。在这一步中稍有变化，最终结果很容易出现偏差。</p><p id="b2e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，您可以使用预先训练的模型或定制的模型。这对于获得有意义的结果至关重要。你需要一个上下文化的向量化，Word2Vec模型可以解决这个问题。你只需要在初始化矢量器类的时候，把路径发送给Word2Vec模型(即<code class="fe ou ov ow om b">PRETRAINED_VECTORS_PATH</code>)。</p><h1 id="e3dd" class="nd ne it bd nf ng oz ni nj nk pa nm nn ki pb kj np kl pc km nr ko pd kp nt nu bi translated">—什么是最好的句子编码器</h1><p id="1a94" class="pw-post-body-paragraph li lj it lk b ll og kd ln lo oh kg lq lr oi lt lu lv oj lx ly lz ok mb mc md im bi translated">句子编码或嵌入技术的最终结果植根于各种因素，如相关的停用词表或语境化的预训练模型。你可以在下面找到更多的解释。</p><ul class=""><li id="4f81" class="pe pf it lk b ll lm lo lp lr pg lv ph lz pi md pj pk pl pm bi translated"><strong class="lk jd">文本清理— </strong>假设您将spaCy用于<strong class="lk jd"> </strong>文本清理步骤，因为我也在sent2vec库中使用了它。如果您错误地忘记从默认停用词表中删除“Not ”,那么句子嵌入结果可能会完全误导。一个简单的“不”字，就能彻底改变一句话的情调。每个环境中的默认停用词表都不同。因此，在进行任何计算之前，您必须根据您的需求来筛选这个列表。</li><li id="4b71" class="pe pf it lk b ll pn lo po lr pp lv pq lz pr md pj pk pl pm bi translated"><strong class="lk jd">情境化模型— </strong>你必须使用情境化模型。例如，如果目标数据是金融数据，则必须使用在金融语料库上训练的模型。否则，句子嵌入的结果可能不准确。所以，如果用<code class="fe ou ov ow om b">word2vec</code>的方法，想用一般的英文模型，句子嵌入结果可能会不准确。</li><li id="1051" class="pe pf it lk b ll pn lo po lr pp lv pq lz pr md pj pk pl pm bi translated"><strong class="lk jd">聚合策略— </strong>当您使用<code class="fe ou ov ow om b">word2vec</code>方法计算句子嵌入时，您可能需要使用更高级的技术来聚合单词向量，而不是取它们的平均值。目前，sent2vec库只支持“平均”技术。使用加权平均来计算句子嵌入，是一种可以改善最终结果的简单增强。未来的版本将支持更高级的技术。</li></ul><p id="41bf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了强调word2vec模型的重要性，我使用两种不同的word2vec模型对一个句子进行编码(即<code class="fe ou ov ow om b">glove-wiki-gigaword-300</code>和<code class="fe ou ov ow om b">fasttext-wiki-news-subwords-300</code>)。然后，我计算两个向量之间的余弦相似度:<code class="fe ou ov ow om b">0.005</code>这可能解释为“两个独特的句子非常不同”。不对！通过这个例子，我想证明如果我们使用两个不同的word2vec模型，句子的向量表示甚至可以是垂直的。换句话说，<strong class="lk jd">如果你用一个随机的word2vec模型来盲目计算句子嵌入，你可能会在过程中大吃一惊。</strong></p><h1 id="0118" class="nd ne it bd nf ng oz ni nj nk pa nm nn ki pb kj np kl pc km nr ko pd kp nt nu bi translated">— Sent2Vec是一个开源库，所以…</h1><p id="f6d0" class="pw-post-body-paragraph li lj it lk b ll og kd ln lo oh kg lq lr oi lt lu lv oj lx ly lz ok mb mc md im bi translated">sent2vec是一个开源库。这个项目的主要目标是<strong class="lk jd">加快NLP项目中概念验证的构建</strong>。大量的自然语言处理任务需要句子矢量化，包括摘要和情感分析。所以，请考虑贡献力量，推动<a class="ae lh" href="https://github.com/pdrm83/Sent2Vec" rel="noopener ugc nofollow" target="_blank">这个项目</a>向前发展。我也希望你能在你激动人心的NLP项目中使用这个库。</p><div class="me mf gp gr mg mh"><a href="https://github.com/pdrm83/Sent2Vec" rel="noopener  ugc nofollow" target="_blank"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd jd gy z fp mm fr fs mn fu fw jc bi translated">pdrm83/Sent2Vec</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">在过去，我们主要使用，例如，一个热点，术语频率，或TF-IDF(标准化术语…</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">github.com</p></div></div><div class="mq l"><div class="ps l ms mt mu mq mv lb mh"/></div></div></a></div><h1 id="76d2" class="nd ne it bd nf ng oz ni nj nk pa nm nn ki pb kj np kl pc km nr ko pd kp nt nu bi translated">感谢阅读！</h1><p id="5340" class="pw-post-body-paragraph li lj it lk b ll og kd ln lo oh kg lq lr oi lt lu lv oj lx ly lz ok mb mc md im bi translated">如果你喜欢这个帖子，想支持我…</p><ul class=""><li id="72b2" class="pe pf it lk b ll lm lo lp lr pg lv ph lz pi md pj pk pl pm bi translated"><em class="pt">跟我上</em> <a class="ae lh" href="https://medium.com/@pedram-ataee" rel="noopener"> <em class="pt">中</em> </a> <em class="pt">！</em></li><li id="2ca5" class="pe pf it lk b ll pn lo po lr pp lv pq lz pr md pj pk pl pm bi translated"><em class="pt">在</em> <a class="ae lh" href="https://www.amazon.com/Pedram-Ataee/e/B08D6J3WNW" rel="noopener ugc nofollow" target="_blank"> <em class="pt">亚马逊</em> </a> <em class="pt">上查看我的书！</em></li><li id="6029" class="pe pf it lk b ll pn lo po lr pp lv pq lz pr md pj pk pl pm bi translated"><em class="pt">成为</em> <a class="ae lh" href="https://pedram-ataee.medium.com/membership" rel="noopener"> <em class="pt">中的一员</em> </a> <em class="pt">！</em></li><li id="9a4e" class="pe pf it lk b ll pn lo po lr pp lv pq lz pr md pj pk pl pm bi translated"><em class="pt">连接上</em><a class="ae lh" href="https://www.linkedin.com/in/pedrama/" rel="noopener ugc nofollow" target="_blank"><em class="pt">Linkedin</em></a><em class="pt">！</em></li><li id="6b66" class="pe pf it lk b ll pn lo po lr pp lv pq lz pr md pj pk pl pm bi translated"><em class="pt">关注我</em> <a class="ae lh" href="https://twitter.com/pedram_ataee" rel="noopener ugc nofollow" target="_blank"> <em class="pt">推特</em> </a> <em class="pt">！</em></li></ul><div class="me mf gp gr mg mh"><a href="https://pedram-ataee.medium.com/membership" rel="noopener follow" target="_blank"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd jd gy z fp mm fr fs mn fu fw jc bi translated">通过我的推荐链接加入Medium—Pedram Ataee博士</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">pedram-ataee.medium.com</p></div></div><div class="mq l"><div class="pu l ms mt mu mq mv lb mh"/></div></div></a></div></div></div>    
</body>
</html>