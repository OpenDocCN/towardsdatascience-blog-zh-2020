<html>
<head>
<title>How to Improve your Supply Chain with Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何通过深度强化学习改善您的供应链</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58?source=collection_archive---------13-----------------------#2020-10-08">https://towardsdatascience.com/deep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58?source=collection_archive---------13-----------------------#2020-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ee2b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用雷和DFO优化多级供应链</h2></div><p id="59ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是什么让亚马逊在网上零售的竞争中脱颖而出？他们的供应链。事实上，这一直是他们的主要竞争对手之一沃尔玛的最大优势之一。</p><p id="0fab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">供应链是高度复杂的系统，由全球数百家甚至数千家制造商和物流承运商组成，他们整合资源，创造出我们每天使用和消费的产品。要跟踪<em class="le">所有的</em>投入到<a class="ae lf" href="https://www.youtube.com/watch?v=U3W2v7LN-88" rel="noopener ugc nofollow" target="_blank">一个单一的、简单的产品将是惊人的</a>。然而，纵向一体化公司内部的供应链组织的任务是管理从原材料到制造、仓储和向客户分销的投入。在这方面做得最好的公司减少了过量储存造成的浪费，减少了不必要的运输成本，减少了将产品和材料运送到系统后期的时间。优化这些系统是像苹果和Saudi Aramco这样不同的企业的关键组成部分。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/67238c4c635306a92f3e277a6f43a02f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*avIST6L4XzFPOV_A"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">照片由<a class="ae lf" href="https://unsplash.com/@shawnanggg?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Shawn Ang </a>在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ff57" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">已经投入了大量的时间和精力来构建有效的供应链优化模型，但是由于它们的规模和复杂性，它们可能难以构建和管理。随着机器学习的进步，特别是强化学习，我们可以训练一个机器学习模型来为我们做出这些决定，在许多情况下，比传统方法做得更好！</p><h1 id="fa78" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="7d2b" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">我们使用<a class="ae lf" rel="noopener" target="_blank" href="/ray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c">射线</a>和<code class="fe mt mu mv mw b"><a class="ae lf" href="https://arxiv.org/abs/2008.06319" rel="noopener ugc nofollow" target="_blank">or-gym</a></code>来训练深度强化学习模型，以优化多级库存管理模型，并使用鲍威尔的方法<a class="ae lf" href="https://www.datahubbs.com/how-to-use-deep-reinforcement-learning-to-improve-your-supply-chain/" rel="noopener ugc nofollow" target="_blank">根据无导数优化模型对其进行基准测试</a>。</p><h1 id="a78a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">多级供应链</h1><p id="a246" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在我们的例子中，我们将使用一个有提前期的<strong class="kk iu">多级</strong>供应链模型。这意味着我们需要为供应链的不同阶段做出决策，我们在不同层面做出的每个决策都会影响下游的决策。在我们的案例中，我们有从原材料生产商到客户的几个阶段。过程中的每个阶段都有不同的<strong class="kk iu">提前期</strong>，即一个阶段的输出到达并成为链中下一个阶段的输入所需的时间。这可能是5天，10天，无论如何。这些交付周期变得越长，您就需要越早预测客户订单和需求，以确保您不会缺货或失去销售！</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/5626f9ac73fb37247fabe814f2d4c51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*HGX0qaUkpZAZzAb6bDvf4g.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">多级供应链示意图(图片由作者<a class="ae lf" href="https://arxiv.org/abs/2008.06319" rel="noopener ugc nofollow" target="_blank">提供，来自Hubbs等人</a></p></figure><h1 id="90d5" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">使用OR-Gym进行库存管理</h1><p id="524c" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated"><a class="ae lf" href="https://github.com/hubbs5/or-gym" rel="noopener ugc nofollow" target="_blank"> OR-Gym </a>库有几个多级供应链模型可以用来模拟这种结构。为此，我们将使用<code class="fe mt mu mv mw b">InvManagement-v1</code>环境，它具有如上所示的结构，但是如果您没有足够的库存来满足客户需求，就会导致销售损失。</p><p id="089b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您还没有，请继续安装该软件包:</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="bd61" class="nc lx it mw b gy nd ne l nf ng">pip install or-gym</span></pre><p id="332e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">安装完成后，我们可以使用以下工具设置我们的环境:</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="48cb" class="nc lx it mw b gy nd ne l nf ng">env = or_gym.make('InvManagement-v1')</span></pre><p id="ddc3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">默认情况下，这是一个四级供应链。这些行动决定了在每个时间步从上面的层级订购多少材料。订单数量受限于供应商的能力及其当前库存。因此，如果你从一个供应商那里订购150个部件，而该供应商的发货能力是100个部件，而你手头只有90个部件，那么你只能收到90个部件。</p><p id="2be4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个层级都有自己的成本结构、定价和交付周期。最后一个梯队(在这种情况下是阶段3)提供原材料，我们在这个阶段没有任何库存限制，假设矿山、油井、森林或任何生产原材料的东西足够大，这不是我们需要关心的限制。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/55bcd39a32f33a2e6c8bf3ae02659a98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*p-a1AKum4JWOmfBKbORsTQ.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated"><code class="fe mt mu mv mw b">Invmanagement-v</code> 1环境的默认参数值。</p></figure><p id="e63c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与所有的<code class="fe mt mu mv mw b">or-gym</code>环境一样，如果这些设置不适合您，只需将一个环境配置字典传递给<code class="fe mt mu mv mw b">make</code>函数来相应地定制您的供应链(<a class="ae lf" href="https://www.datahubbs.com/action-masking-with-rllib/" rel="noopener ugc nofollow" target="_blank">这里给出了一个例子</a>)。</p><h1 id="32b2" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">和雷一起训练</h1><p id="1930" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">为了训练你的环境，我们将利用<a class="ae lf" href="https://www.datahubbs.com/ray-and-rllib-fast-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">射线库</a>来加速我们的训练，所以继续导入你的包。</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="7665" class="nc lx it mw b gy nd ne l nf ng">import or_gym<br/>from or_gym.utils import create_env<br/>import ray<br/>from ray.rllib import agents<br/>from ray import tune</span></pre><p id="bb6f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们需要一个简单的注册函数来确保Ray知道我们想要运行的环境。我们可以用下面显示的<code class="fe mt mu mv mw b">register_env</code>函数注册它。</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="b14c" class="nc lx it mw b gy nd ne l nf ng">def register_env(env_name, env_config={}):<br/>    env = create_env(env_name)<br/>    tune.register_env(env_name, <br/>        lambda env_name: env(env_name,<br/>            env_config=env_config))</span></pre><p id="f9f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这里，我们可以设置我们的RL配置和训练模型所需的一切。</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="6471" class="nc lx it mw b gy nd ne l nf ng"># Environment and RL Configuration Settings<br/>env_name = 'InvManagement-v1'<br/>env_config = {} # Change environment parameters here<br/>rl_config = dict(<br/>    env=env_name,<br/>    num_workers=2,<br/>    env_config=env_config,<br/>    model=dict(<br/>        vf_share_layers=False,<br/>        fcnet_activation='elu',<br/>        fcnet_hiddens=[256, 256]<br/>    ),<br/>    lr=1e-5<br/>)</span><span id="59f7" class="nc lx it mw b gy ni ne l nf ng"># Register environment<br/>register_env(env_name, env_config)</span></pre><p id="e961" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<code class="fe mt mu mv mw b">rl_config</code>字典中，您可以设置所有相关的超参数，或者将您的系统设置为在GPU上运行。这里，我们将使用2个工作线程进行并行化，并训练一个具有ELU激活功能的双层网络。此外，如果您打算使用<code class="fe mt mu mv mw b"><a class="ae lf" href="https://www.datahubbs.com/hyperparameter-tuning-with-tune/" rel="noopener ugc nofollow" target="_blank">tune</a></code> <a class="ae lf" href="https://www.datahubbs.com/hyperparameter-tuning-with-tune/" rel="noopener ugc nofollow" target="_blank">进行超参数调优</a>，那么您可以使用<code class="fe mt mu mv mw b">tune.gridsearch()</code>之类的工具来系统地更新学习率、改变网络或任何您喜欢的东西。</p><p id="78dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦你满意了，就去选择你的算法并开始训练吧！下面，我只使用PPO算法，因为我发现它在大多数环境下训练得很好。</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="0d30" class="nc lx it mw b gy nd ne l nf ng"># Initialize Ray and Build Agent<br/>ray.init(ignore_reinit_error=True)<br/>agent = agents.ppo.PPOTrainer(env=env_name,<br/>    config=rl_config)</span><span id="4b01" class="nc lx it mw b gy ni ne l nf ng">results = []<br/>for i in range(500):<br/>    res = agent.train()<br/>    results.append(res)<br/>    if (i+1) % 5 == 0:<br/>        print('\rIter: {}\tReward: {:.2f}'.format(<br/>                i+1, res['episode_reward_mean']), end='')<br/>ray.shutdown()</span></pre><p id="6a98" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的代码将初始化<code class="fe mt mu mv mw b">ray</code>，然后根据您之前指定的配置构建代理。如果你对此感到满意，那么让它运行一段时间，看看效果如何！</p><p id="c0dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种环境下需要注意的一点是:如果学习率太高，政策函数将开始发散，损失将变得巨大。此时，您将得到一个错误，通常来自Ray的默认预处理器，状态显示奇怪的值，因为网络给出的动作都是<code class="fe mt mu mv mw b">nan</code>。这很容易通过降低学习速度并再次尝试来解决。</p><p id="3192" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看表演。</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="fb7e" class="nc lx it mw b gy nd ne l nf ng">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from matplotlib import gridspec</span><span id="bfbe" class="nc lx it mw b gy ni ne l nf ng"># Unpack values from each iteration<br/>rewards = np.hstack([i['hist_stats']['episode_reward'] <br/>    for i in results])<br/>pol_loss = [<br/>    i['info']['learner']['default_policy']['policy_loss'] <br/>    for i in results]<br/>vf_loss = [<br/>    i['info']['learner']['default_policy']['vf_loss'] <br/>    for i in results]</span><span id="987a" class="nc lx it mw b gy ni ne l nf ng">p = 100<br/>mean_rewards = np.array([np.mean(rewards[i-p:i+1]) <br/>                if i &gt;= p else np.mean(rewards[:i+1]) <br/>                for i, _ in enumerate(rewards)])<br/>std_rewards = np.array([np.std(rewards[i-p:i+1])<br/>               if i &gt;= p else np.std(rewards[:i+1])<br/>               for i, _ in enumerate(rewards)])</span><span id="60c9" class="nc lx it mw b gy ni ne l nf ng">fig = plt.figure(constrained_layout=True, figsize=(20, 10))<br/>gs = fig.add_gridspec(2, 4)<br/>ax0 = fig.add_subplot(gs[:, :-2])<br/>ax0.fill_between(np.arange(len(mean_rewards)), <br/>                 mean_rewards - std_rewards, <br/>                 mean_rewards + std_rewards, <br/>                 label='Standard Deviation', alpha=0.3)<br/>ax0.plot(mean_rewards, label='Mean Rewards')<br/>ax0.set_ylabel('Rewards')<br/>ax0.set_xlabel('Episode')<br/>ax0.set_title('Training Rewards')<br/>ax0.legend()</span><span id="e1b3" class="nc lx it mw b gy ni ne l nf ng">ax1 = fig.add_subplot(gs[0, 2:])<br/>ax1.plot(pol_loss)<br/>ax1.set_ylabel('Loss')<br/>ax1.set_xlabel('Iteration')<br/>ax1.set_title('Policy Loss')</span><span id="5a51" class="nc lx it mw b gy ni ne l nf ng">ax2 = fig.add_subplot(gs[1, 2:])<br/>ax2.plot(vf_loss)<br/>ax2.set_ylabel('Loss')<br/>ax2.set_xlabel('Iteration')<br/>ax2.set_title('Value Function Loss')</span><span id="969b" class="nc lx it mw b gy ni ne l nf ng">plt.show()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nj"><img src="../Images/283c91460d22e2da815758abfcc992aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cnc2aJ4TLSvxZocPrZaJQw.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">图片作者。</p></figure><p id="79fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看起来我们的代理学会了一个体面的政策！</p><p id="b805" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些经典运筹学问题的深度强化学习的困难之一是缺乏最优性保证。换句话说，我们可以看看上面的训练曲线，看到它正在学习越来越好的政策——它似乎正在向一个政策靠拢——但我们不知道这个政策有多好。我们能做得更好吗？我们应该在超参数调优上投入更多的时间(和金钱)吗？要回答这个问题，我们需要转向一些不同的方法，并开发一个基准。</p><h1 id="99c0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">无导数优化</h1><p id="1b7f" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">测试RL模型的一个好方法是使用<strong class="kk iu">无导数优化</strong> (DFO)。像RL一样，DFO将系统视为一个黑盒模型，提供输入并获得一些反馈，作为回报，在寻求最优值时再次尝试。</p><p id="9989" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与RL不同，DFO没有国家的概念。这意味着我们将试图找到一个固定的再订购政策，使库存达到一定水平，以平衡持有成本和销售利润。例如，如果阶段0的策略是重新订购多达10个小部件，而目前我们有4个小部件，则策略声明我们将重新订购6个。在RL的情况下，它将考虑当前的管道和我们提供给状态的所有其他信息。因此RL更具适应性，并且应该优于直接的DFO实现。如果没有，那么我们知道我们需要重新开始。</p><p id="8692" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然这听起来可能过于简单，但这种固定的再订购策略在工业应用中并不罕见，部分原因是真实的供应链由比我们在这里建模更多的变量和相关决策组成。因此，一个固定的政策是易处理的，供应链专业人员可以很容易地处理。</p><h2 id="5b26" class="nc lx it bd ly nk nl dn mc nm nn dp mg kr no np mi kv nq nr mk kz ns nt mm nu bi translated">实施DFO</h2><p id="c813" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">对于DFO，有很多不同的算法和求解器。出于我们的目的，我们将利用Scipy的<code class="fe mt mu mv mw b">optimize</code>库来实现<a class="ae lf" href="https://en.wikipedia.org/wiki/Powell%27s_method" rel="noopener ugc nofollow" target="_blank"> Powell的方法</a>。我们不会在这里进入细节，但这是一种快速找到函数最小值的方法，并可用于离散优化-就像我们在这里。</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="be67" class="nc lx it mw b gy nd ne l nf ng">from scipy.optimize import minimize</span></pre><p id="143c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们将使用固定的再订购策略，所以我们需要一个快速的功能来将库存水平转化为要评估的行动。</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="1ef6" class="nc lx it mw b gy nd ne l nf ng">def base_stock_policy(policy, env):<br/>    '''<br/>    Implements a re-order up-to policy. This means that for<br/>    each node in the network, if the inventory at that node <br/>    falls below the level denoted by the policy, we will <br/>    re-order inventory to bring it to the policy level.<br/>    <br/>    For example, policy at a node is 10, current inventory<br/>    is 5: the action is to order 5 units.<br/>    '''<br/>    assert len(policy) == len(env.init_inv), (<br/>        'Policy should match number of nodes in network' + <br/>        '({}, {}).'.format(<br/>            len(policy), len(env.init_inv)))<br/>    <br/>    # Get echelon inventory levels<br/>    if env.period == 0:<br/>        inv_ech = np.cumsum(env.I[env.period] +<br/>            env.T[env.period])<br/>    else:<br/>        inv_ech = np.cumsum(env.I[env.period] +<br/>            env.T[env.period] - env.B[env.period-1, :-1])<br/>        <br/>    # Get unconstrained actions<br/>    unc_actions = policy - inv_ech<br/>    unc_actions = np.where(unc_actions&gt;0, unc_actions, 0)<br/>    <br/>    # Ensure that actions can be fulfilled by checking <br/>    # constraints<br/>    inv_const = np.hstack([env.I[env.period, 1:], np.Inf])<br/>    actions = np.minimum(env.c,<br/>                np.minimum(unc_actions, inv_const))<br/>    return actions</span></pre><p id="de89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe mt mu mv mw b">base_stock_policy</code>函数获取我们提供的策略级别，并如上所述计算该级别和库存之间的差异。需要注意的一点是，当我们计算库存水平时，我们也包括所有在运输途中的库存(在<code class="fe mt mu mv mw b">env.T</code>中给出)。例如，如果阶段0的当前现有库存为100，并且阶段0和阶段1之间有5天的提前期，那么我们也会考虑过去5天的所有订单。因此，如果阶段0每天订购10件，那么该层级的库存将为150件。这使得策略级别大于容量变得有意义，因为我们不仅要查看我们今天仓库中的库存，还要查看所有正在运输的物品。</p><p id="8c1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的DFO方法需要进行函数评估调用，以查看所选变量的执行情况。在我们的例子中，我们有一个要评估的环境，所以我们需要一个函数来运行我们环境中的一集并返回适当的结果。</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="f505" class="nc lx it mw b gy nd ne l nf ng">def dfo_func(policy, env, *args):<br/>    '''<br/>    Runs an episode based on current base-stock model <br/>    settings. This allows us to use our environment for the <br/>    DFO optimizer.<br/>    '''<br/>    env.reset() # Ensure env is fresh<br/>    rewards = []<br/>    done = False<br/>    while not done:<br/>        action = base_stock_policy(policy, env)<br/>        state, reward, done, _ = env.step(action)<br/>        rewards.append(reward)<br/>        if done:<br/>            break<br/>            <br/>    rewards = np.array(rewards)<br/>    prob = env.demand_dist.pmf(env.D, **env.dist_param)<br/>    <br/>    # Return negative of expected profit<br/>    return -1 / env.num_periods * np.sum(prob * rewards)</span></pre><p id="695d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们不是返回奖励的总和，而是返回对奖励的负面期望。消极的原因是我们使用的Scipy函数寻求最小化，而我们的环境是为了最大化回报而设计的，所以我们反转这个函数以确保一切都指向正确的方向。我们通过乘以基于分布的需求概率来计算预期回报。我们可以获取更多样本来估计分布，并以这种方式计算我们的期望(对于许多现实世界的应用来说，这是必需的)，但在这里，我们可以访问真实的分布，因此我们可以使用它来减少我们的计算负担。</p><p id="c94c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们准备优化。</p><p id="eebd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面这个函数会根据你的配置设置搭建一个环境，拿我们的<code class="fe mt mu mv mw b">dfo_func</code>来评估，应用鲍威尔的方法来解决问题。它将返回我们的策略，并确保我们的答案只包含正整数(例如，我们不能订购半个部件或负数部件)。</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="4a6f" class="nc lx it mw b gy nd ne l nf ng">def optimize_inventory_policy(env_name, fun,<br/>    init_policy=None, env_config={}, method='Powell'):<br/>    <br/>    env = or_gym.make(env_name, env_config=env_config)<br/>    <br/>    if init_policy is None:<br/>        init_policy = np.ones(env.num_stages-1)<br/>        <br/>    # Optimize policy<br/>    out = minimize(fun=fun, x0=init_policy, args=env, <br/>        method=method)<br/>    policy = out.x.copy()<br/>    <br/>    # Policy must be positive integer<br/>    policy = np.round(np.maximum(policy, 0), 0).astype(int)<br/>    <br/>    return policy, out</span></pre><p id="225f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在是时候把它们放在一起了。</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="a669" class="nc lx it mw b gy nd ne l nf ng">policy, out = optimize_inventory_policy('InvManagement-v1',<br/>    dfo_func)<br/>print("Re-order levels: {}".format(policy))<br/>print("DFO Info:\n{}".format(out))</span><span id="28ab" class="nc lx it mw b gy ni ne l nf ng">Re-order levels: [540 216  81]<br/>DFO Info:<br/>   direc: array([[  0.        ,   0.        ,   1.        ],<br/>       [  0.        ,   1.        ,   0.        ],<br/>       [206.39353826,  81.74560612,  28.78995703]])<br/>     fun: -0.9450780368543933<br/> message: 'Optimization terminated successfully.'<br/>    nfev: 212<br/>     nit: 5<br/>  status: 0<br/> success: True<br/>       x: array([539.7995151 , 216.38046861,  80.66902905])</span></pre><p id="8ec7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的DFO模型发现了一个固定库存策略，第0阶段的再订购水平为540，第1阶段为216，第2阶段为81。它只用了212次功能评估就做到了这一点，即它模拟了212集来寻找最佳值。</p><p id="466b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以运行该策略，然后将其输入到我们的环境中，比如说1000次，以生成一些统计数据，并将其与我们的RL解决方案进行比较。</p><pre class="lh li lj lk gt my mw mz na aw nb bi"><span id="5fe7" class="nc lx it mw b gy nd ne l nf ng">env = or_gym.make(env_name, env_config=env_config)<br/>eps = 1000<br/>rewards = []<br/>for i in range(eps):<br/>    env.reset()<br/>    reward = 0<br/>    while True:<br/>        action = base_stock_policy(policy, eenv)<br/>        s, r, done, _ = env.step(action)<br/>        reward += r<br/>        if done:<br/>            rewards.append(reward)<br/>            break</span></pre><h1 id="bad0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">比较性能</h1><p id="d792" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在我们开始报酬比较之前，请注意这些并不是完美的，1:1的比较。如前所述，DFO为我们提供了一个固定的策略，而RL提供了一个更加灵活、动态的策略，可以根据状态信息进行更改。我们的DFO方法也提供了一些需求概率方面的信息来计算期望，RL必须从额外的抽样中进行推断。因此，虽然RL从近65k的剧集中学习，而DFO只需要进行212次函数调用，但他们并不完全可比。考虑到枚举每一个有意义的固定政策一次将需要大约2亿集，那么RL看起来并不那么低效。</p><p id="04a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么，这些是如何叠加的呢？</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nv"><img src="../Images/5232104b2955470a282f41aab1d41143.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3L2vrZtuHd5MlQQagAVPxw.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">图片作者。</p></figure><p id="4120" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面我们可以看到，RL确实比我们的DFO策略平均高出11%(460比414)。RL模型在大约15k集后超过了DFO策略，并在此后稳步改进。然而，RL政策有一些更高的差异，其中有一些可怕的插曲。总的来说，我们确实从RL方法中得到了更好的结果。</p><p id="efcc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，这两种方法都很难实现，计算量也不大。我忘记将我的<code class="fe mt mu mv mw b">rl_config</code>设置改为在我的GPU上运行，在我的笔记本电脑上训练仍然只需要大约25分钟，而DFO模型需要大约2秒钟。更复杂的模型可能在这两种情况下都不那么友好。</p><p id="d118" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一点需要注意的是，这两种方法对初始条件都非常敏感，而且都不能保证在每种情况下都能找到最优策略。如果你有一个想应用RL的问题，也许可以先使用一个简单的DFO解算器，尝试一些初始条件来了解问题，然后旋转完整的RL模型。你会发现DFO政策足以完成你的任务。</p><p id="5b15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望这很好地概述了如何使用这些方法和<code class="fe mt mu mv mw b">or-gym</code>库。如果您有任何反馈或问题，请留下！</p></div></div>    
</body>
</html>