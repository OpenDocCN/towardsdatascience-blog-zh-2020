<html>
<head>
<title>Topic Modeling with BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用BERT进行主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6?source=collection_archive---------0-----------------------#2020-10-05">https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6?source=collection_archive---------0-----------------------#2020-10-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/964dad91bf7bf87b3e27d7397bbc5524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lpTPT9dLLFgHw6vG.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由作者提供。</p></figure><div class=""/><div class=""><h2 id="00eb" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">利用BERT和TF-IDF创建易于解释的主题。</h2></div><p id="6856" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">当一个产品负责人来找我做一些基于NLP的分析时，我通常会被问到以下问题:</p><blockquote class="lt"><p id="e9ee" class="lu lv ji bd lw lx ly lz ma mb mc ls dk translated">“在这些文档中可以经常找到哪个主题？”</p></blockquote><p id="40a6" class="pw-post-body-paragraph kx ky ji kz b la md kj lc ld me km lf lg mf li lj lk mg lm ln lo mh lq lr ls im bi translated">由于没有任何类别或标签，我被迫寻找无监督的技术来提取这些主题，即<strong class="kz jj">主题建模</strong>。</p><p id="fd6d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">虽然主题模型如LDA和NMF已经被证明是很好的起点，但我总是觉得通过超参数调整来创建有意义的主题需要相当大的努力。</p><p id="8585" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">此外，我想使用基于transformer的模型，如<strong class="kz jj"> BERT </strong>，因为它们在过去几年的各种NLP任务中显示了惊人的结果。预训练模型特别有用，因为它们应该包含更准确的单词和句子的表示。</p><p id="2804" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">几周前，我看到了这个名为<a class="ae mi" href="https://github.com/ddangelov/Top2Vec" rel="noopener ugc nofollow" target="_blank"> Top2Vec </a> *的伟大项目，它利用文档和单词嵌入来创建易于解释的主题。我开始查看代码来概括Top2Vec，以便它可以用于预先训练的transformer模型。</p><p id="1eda" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">Doc2Vec的最大优点是，得到的文档和单词嵌入共同嵌入在同一个空间中，这使得文档嵌入可以由邻近的单词嵌入来表示。不幸的是，这被证明是困难的，因为BERT嵌入是基于令牌的，不一定占用相同的空间**。</p><p id="b215" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">相反，我决定想出一个不同的算法，可以使用伯特和🤗变压器嵌入。结果是BERTopic，一种使用最先进的嵌入技术生成主题的算法。</p><p id="33f9" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这篇文章的主题不是BERTopic的使用，而是一个关于如何使用BERT创建你自己的主题模型的教程。</p><p id="73cf" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">论文 *:安杰洛夫D. (2020)。<a class="ae mi" href="https://arxiv.org/abs/2008.09470" rel="noopener ugc nofollow" target="_blank"> Top2Vec:主题的分布式表示。</a> <em class="mj"> arXiv预印本arXiv:2008.09470 </em>。</p><p id="3151" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">注意* </strong> *:虽然你可以让它们占据相同的空间，但是由于BERT的上下文特性，单词嵌入的结果大小相当大。此外，生成的句子或文档嵌入有可能会降低质量。</p><h1 id="078e" class="mk ml ji bd mm mn mo mp mq mr ms mt mu ko mv kp mw kr mx ks my ku mz kv na nb bi translated">1.数据和数据包</h1><p id="e5f8" class="pw-post-body-paragraph kx ky ji kz b la nc kj lc ld nd km lf lg ne li lj lk nf lm ln lo ng lq lr ls im bi translated">对于这个例子，我们使用著名的<code class="fe nh ni nj nk b">20 Newsgroups</code>数据集，它包含大约18000篇关于20个主题的新闻组帖子。使用Scikit-Learn，我们可以快速下载和准备数据:</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="f120" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">如果你想加速训练，你可以选择子集<code class="fe nh ni nj nk b">train</code>，因为它会减少你提取的文章数量。</p><p id="1eef" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">注意</strong>:如果你想在段落层次而不是在整个文档上应用主题建模，我建议在创建嵌入之前分割你的数据。</p><h1 id="1e61" class="mk ml ji bd mm mn mo mp mq mr ms mt mu ko mv kp mw kr mx ks my ku mz kv na nb bi translated">2.嵌入</h1><p id="995e" class="pw-post-body-paragraph kx ky ji kz b la nc kj lc ld nd km lf lg ne li lj lk nf lm ln lo ng lq lr ls im bi translated">我们要做的第一步是将文档转换成数字数据。我们使用<strong class="kz jj"> BERT </strong>来实现这个目的，因为它基于单词的上下文提取不同的嵌入。不仅如此，还有许多<a class="ae mi" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">预训练的模型</a>可供使用。</p><p id="d536" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">如何为文档生成BERT嵌入取决于您。然而，我更喜欢使用<code class="fe nh ni nj nk b">sentence-transformers</code>包，因为最终的嵌入已经证明是高质量的，并且通常对于文档级的嵌入工作得很好。</p><p id="9a99" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在生成文档嵌入之前，用<code class="fe nh ni nj nk b">pip install sentence-transformers</code>安装包。如果你在安装这个包时遇到问题，那么首先安装<a class="ae mi" href="https://pytorch.org/get-started/locally/" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>是值得的。</p><p id="ef93" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然后，运行以下代码将文档转换为512维向量:</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="5f26" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们使用<strong class="kz jj"> Distilbert </strong>，因为它在速度和性能之间提供了一个很好的平衡。包装中有几款<a class="ae mi" href="https://www.sbert.net/docs/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">多语言型号</a>供您使用。</p><p id="3338" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">注意</strong>:由于transformer型号有令牌限制，您在输入大型文档时可能会遇到一些错误。在这种情况下，您可以考虑将文档分成段落。</p><h1 id="08f3" class="mk ml ji bd mm mn mo mp mq mr ms mt mu ko mv kp mw kr mx ks my ku mz kv na nb bi translated">3.使聚集</h1><p id="9226" class="pw-post-body-paragraph kx ky ji kz b la nc kj lc ld nd km lf lg ne li lj lk nf lm ln lo ng lq lr ls im bi translated">我们希望确保具有相似主题的文档被聚集在一起，以便我们可以在这些集群中找到主题。在这样做之前，我们首先需要降低嵌入的维数，因为许多聚类算法处理高维数据的能力很差。</p><h2 id="a307" class="nr ml ji bd mm ns nt dn mq nu nv dp mu lg nw nx mw lk ny nz my lo oa ob na oc bi translated">UMAP</h2><p id="b91a" class="pw-post-body-paragraph kx ky ji kz b la nc kj lc ld nd km lf lg ne li lj lk nf lm ln lo ng lq lr ls im bi translated">在为数不多的降维算法中，<a class="ae mi" href="https://github.com/lmcinnes/umap" rel="noopener ugc nofollow" target="_blank"> UMAP </a>可以说是性能最好的，因为它在较低的维度中保留了高维局部结构的重要部分。</p><p id="be4e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在我们降低文档嵌入的维度之前，安装带有<code class="fe nh ni nj nk b">pip install umap-learn</code>的包。我们将维数减少到5，同时保持局部邻域的大小为15。您可以使用这些值来优化您的主题创建。请注意，过低的维度会导致信息丢失，而过高的维度会导致较差的聚类结果。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><h2 id="2096" class="nr ml ji bd mm ns nt dn mq nu nv dp mu lg nw nx mw lk ny nz my lo oa ob na oc bi translated">HDBSAN</h2><p id="1aa5" class="pw-post-body-paragraph kx ky ji kz b la nc kj lc ld nd km lf lg ne li lj lk nf lm ln lo ng lq lr ls im bi translated">在将文档嵌入的维数减少到5后，我们可以用<strong class="kz jj"> HDBSCAN </strong>对文档进行聚类。HDBSCAN是一种基于密度的算法，非常适合UMAP，因为UMAP即使在低维空间中也保留了大量的局部结构。此外，HDBSCAN不会将数据点强制归类，因为它认为它们是异常值。</p><p id="89f1" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">用<code class="fe nh ni nj nk b">pip install hdbscan</code>安装软件包，然后创建集群:</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="7211" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">太好了！我们现在已经将相似的文档聚集在一起，这些文档应该代表它们所包含的主题。为了可视化得到的聚类，我们可以进一步将维数减少到2，并将异常值可视化为灰点:</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/9d95e065a6408ad58c28b22253c6c274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W94GjvT6vBzDGY50qPHZDA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">通过将判决嵌入减少到二维空间来可视化主题。图片由作者提供。</p></figure><p id="01af" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">由于生成的主题数量众多(约55个)，很难直观显示各个集群。然而，我们可以看到，即使在二维空间中，一些局部结构仍然存在。</p><p id="899b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">注意</strong>:如果你使用的聚类算法可以处理高维数据，比如基于余弦的k-Means，你可以跳过降维步骤。</p><h1 id="6ea7" class="mk ml ji bd mm mn mo mp mq mr ms mt mu ko mv kp mw kr mx ks my ku mz kv na nb bi translated">4.话题创作</h1><p id="f08c" class="pw-post-body-paragraph kx ky ji kz b la nc kj lc ld nd km lf lg ne li lj lk nf lm ln lo ng lq lr ls im bi translated">我们想从我们生成的聚类中知道的是，是什么使一个聚类基于它们的内容而不同于另一个？</p><blockquote class="oe of og"><p id="5717" class="kx ky mj kz b la lb kj lc ld le km lf oh lh li lj oi ll lm ln oj lp lq lr ls im bi translated">我们如何从集群文档中获取主题？</p></blockquote><p id="4974" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了解决这个问题，我想到了TF-IDF的一个基于类的变体(<strong class="kz jj"> c-TF-IDF </strong>)，它允许我提取每组文档与其他文档相比的独特之处。</p><p id="528c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">该方法背后的直觉如下。当您像往常一样对一组文档应用TF-IDF时，您基本上是在比较文档之间单词的重要性。</p><p id="562a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">如果我们将单个类别(例如，一个集群)中的所有文档视为单个文档<strong class="kz jj">然后应用TF-IDF会怎么样？每个类别的结果将是一个非常长的文档，并且产生的TF-IDF分数将展示主题中的重要单词。</strong></p><h2 id="0184" class="nr ml ji bd mm ns nt dn mq nu nv dp mu lg nw nx mw lk ny nz my lo oa ob na oc bi translated">c-TF-IDF</h2><p id="0c45" class="pw-post-body-paragraph kx ky ji kz b la nc kj lc ld nd km lf lg ne li lj lk nf lm ln lo ng lq lr ls im bi translated">为了创建这个基于类的TF-IDF分数，我们需要首先为每个文档集群创建一个文档:</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="e6c4" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然后，我们应用基于类的TF-IDF:</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/c146c28640afbb9a263ef192367ad0d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XV925DDdq5x_Gio4"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">基于类的TF-IDF，通过连接类内的文档来实现。图片由作者提供。</p></figure><p id="43b2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">其中为每个类别<code class="fe nh ni nj nk b"><strong class="kz jj">i</strong></code>提取每个单词<code class="fe nh ni nj nk b"><strong class="kz jj">t</strong></code>的<strong class="kz jj">频率</strong>并除以单词总数<code class="fe nh ni nj nk b"><strong class="kz jj">w</strong></code>。这个动作可以看作是对课堂上常用词的一种正则化形式。接下来，未连接的文档总数<code class="fe nh ni nj nk b"><strong class="kz jj">m</strong></code>除以所有类别中单词<code class="fe nh ni nj nk b"><strong class="kz jj">t</strong></code>的总频率<code class="fe nh ni nj nk b"><strong class="kz jj">n</strong></code>。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="9234" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">现在，我们对一个聚类中的每个单词都有一个单独的<strong class="kz jj">重要性</strong>值，它可以用来创建主题。如果我们在每个聚类中取前10个最重要的单词，那么我们将得到一个聚类的良好表示，从而得到一个主题。</p><h2 id="f29c" class="nr ml ji bd mm ns nt dn mq nu nv dp mu lg nw nx mw lk ny nz my lo oa ob na oc bi translated">主题表征</h2><p id="76c9" class="pw-post-body-paragraph kx ky ji kz b la nc kj lc ld nd km lf lg ne li lj lk nf lm ln lo ng lq lr ls im bi translated">为了创建主题表示，我们根据每个主题的c-TF-IDF得分，选取前20个单词。分数越高，越能代表其主题，因为分数是信息密度的代表。</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="8639" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们可以使用<code class="fe nh ni nj nk b">topic_sizes</code>来查看某些主题出现的频率:</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/941624b8b614eb8be370f1d5eab4c93a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KaTFpM8TFgP44HMyVbKvng.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由作者提供。</p></figure><p id="29f6" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">主题名<code class="fe nh ni nj nk b">-1</code>指的是没有分配任何主题的所有文档。HDBSCAN的优点在于，并非所有文档都被强制指向某个集群。如果找不到聚类，那么它就是一个离群值。</p><p id="27d0" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们可以看到，主题7、43、12和41是我们可以创建的最大集群。要查看属于这些主题的单词，我们可以简单地使用字典<code class="fe nh ni nj nk b">top_n_words</code>来访问这些主题:</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/887dcfafb6417a09b20ed2749278e1e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xXOMglHWQUcqQugQhppUtg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由作者提供。</p></figure><p id="954b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">看看最大的四个主题，我会说这些似乎很好地代表了容易解释的主题！</p><p id="9b2a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我可以把体育、计算机、太空和宗教看作是从数据中提取出来的清晰的主题。</p><h1 id="aea2" class="mk ml ji bd mm mn mo mp mq mr ms mt mu ko mv kp mw kr mx ks my ku mz kv na nb bi translated">5.话题缩减</h1><p id="6f35" class="pw-post-body-paragraph kx ky ji kz b la nc kj lc ld nd km lf lg ne li lj lk nf lm ln lo ng lq lr ls im bi translated">根据数据集的不同，您可能会获得数百个已创建的主题！您可以调整HDBSCAN的参数，以便通过其<code class="fe nh ni nj nk b">min_cluster_size</code>参数获得更少的主题，但是它不允许您指定集群的确切数量。</p><p id="2ddc" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><a class="ae mi" href="https://github.com/ddangelov/Top2Vec" rel="noopener ugc nofollow" target="_blank"> Top2Vec </a>使用的一个巧妙技巧是通过合并彼此最相似的主题向量来减少主题的数量。</p><p id="c145" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们可以使用类似的技术，通过<strong class="kz jj">比较</strong>主题间的c-TF-IDF向量，<strong class="kz jj">合并</strong>最相似的向量，最后<strong class="kz jj">重新计算</strong>c-TF-IDF向量来更新我们主题的表示:</p><figure class="nl nm nn no gt iv"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="6760" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">上面，我们把最不常见的话题和最相似的话题合并。通过重复这19次，我们将主题数量从<strong class="kz jj"> 56 </strong>减少到<strong class="kz jj"> 36 </strong>！</p><p id="0b3f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">注</strong>:我们可以跳过这条流水线的重算部分，以加快话题缩减步骤。然而，重新计算c-TF-IDF向量更准确，因为这将更好地表示新生成的主题内容。你可以尝试这样做，例如，每n步更新一次，这样既能加快进程，又能保持良好的主题表现。</p><p id="d409" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">提示</strong>:您可以使用本文描述的方法(或者简单地使用<a class="ae mi" href="https://github.com/MaartenGr/BERTopic" rel="noopener ugc nofollow" target="_blank"> BERTopic </a>)来创建句子级嵌入。这样做的主要优点是可以查看单个文档中主题的分布。</p><h1 id="0d4c" class="mk ml ji bd mm mn mo mp mq mr ms mt mu ko mv kp mw kr mx ks my ku mz kv na nb bi translated">感谢您的阅读！</h1><p id="45f9" class="pw-post-body-paragraph kx ky ji kz b la nc kj lc ld nd km lf lg ne li lj lk nf lm ln lo ng lq lr ls im bi translated">如果你像我一样，对人工智能、数据科学或心理学充满热情，请随时在<a class="ae mi" href="https://www.linkedin.com/in/mgrootendorst/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上添加我，或者在<a class="ae mi" href="https://twitter.com/MaartenGr" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我。</p><p id="bf60" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">本文中的所有示例和代码都可以在这里找到:</p><div class="is it gp gr iu on"><a href="https://github.com/MaartenGr/BERTopic" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jj gy z fp os fr fs ot fu fw jh bi translated">马尔滕格尔/贝尔托皮</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">BERTopic是一种主题建模技术，它利用BERT嵌入和c-TF-IDF来创建密集的集群，从而允许…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">github.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb ja on"/></div></div></a></div></div></div>    
</body>
</html>