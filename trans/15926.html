<html>
<head>
<title>Election Special: Train a GPT-2 to generate Donald Trump Speeches</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">选举特辑:训练一台GPT-2生成唐纳德·川普的演讲</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/election-special-train-a-gpt-2-to-generate-donald-trump-speeches-b66fc3aa92b9?source=collection_archive---------39-----------------------#2020-11-02">https://towardsdatascience.com/election-special-train-a-gpt-2-to-generate-donald-trump-speeches-b66fc3aa92b9?source=collection_archive---------39-----------------------#2020-11-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="01a5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用数据做很酷的事情</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3adfb56d6ceb6737be95c9c04a1a0563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9meZjVfvGg2qDJq3ZD4bzA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自Unsplash的免版税—<a class="ae ky" href="https://unsplash.com/photos/ls8Kc0P9hAA" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/ls8Kc0P9hAA</a></p></figure><h1 id="5ec9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="32a0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇关注美国2020年大选的第二篇博客中，我们根据唐纳德·特朗普的演讲训练了一个GPT-2模型。通过最少的训练，GPT-2模型成功地复制了他的风格，并开始以他的风格写作。参见GPT-2生成的Donald Trump演讲样本。听起来确实像他！</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="f35f" class="ms la it mo b gy mt mu l mv mw">As president, I kept my promise. Nobody else did. We set records. We set records. Thank you very much. Great job. We set records. And by the way, there's two really greats, right? There's Sonny Perdue and there's Barack Hussein Obama. The two greats. Really great. One has become the most powerful president in the history of our country. I said, "How powerful is that?"</span></pre><p id="4ff1" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated"><a class="ae ky" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2模型</a>去年由Open AI发布。这是一个已经在40GB互联网文本上训练过的语言模型。该模型可以在任何文本样本上进行微调，以根据输入文本调整其生成样式。在这篇博客中，我们使用<a class="ae ky" href="https://huggingface.co/transformers/model_doc/gpt2.html" rel="noopener ugc nofollow" target="_blank"> HuggingFace repo的</a>实现GPT-2，看看你如何调整它来为DT写演讲稿。</p><p id="c727" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">我们还写了一篇关于使用T5模型检测假新闻的选举专题博客。点击查看<a class="ae ky" rel="noopener" target="_blank" href="/election-special-detect-fake-news-using-transformers-3e38d0c5b5c7">。</a></p><p id="cf34" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">在<a class="ae ky" href="https://deeplearninganalytics.org/" rel="noopener ugc nofollow" target="_blank">深度学习分析</a>，我们非常热衷于使用数据科学和机器学习来解决现实世界的问题。请联系我们，与我们讨论NLP项目。</p><h1 id="319e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">数据集</h1><p id="7ebf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对于这篇博客，我们使用了<a class="ae ky" href="https://www.kaggle.com/christianlillelund/donald-trumps-rallies" rel="noopener ugc nofollow" target="_blank"> Kaggle数据集</a>——唐纳德·特朗普的集会。这个数据集有他的35次演讲。一半的演讲来自去年年底，其余的来自今年。最新的演讲来自他2020年9月在塞勒姆、费耶特维尔和亨德森的集会。</p><p id="3d09" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">在我们开始根据这些数据训练语言模型之前，我们需要以GPT新协议要求的格式获取数据。GPT-2需要逐句传递数据。</p><p id="965b" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">作为数据预处理的一部分，我们执行以下操作:</p><ol class=""><li id="5e8e" class="nc nd it lt b lu mx lx my ma ne me nf mi ng mm nh ni nj nk bi translated">把演讲分成句子</li><li id="a17a" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated">将前85%的句子保留在训练集中，其余的放在验证集中。验证集将用于测量困惑分数</li></ol><p id="103b" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">实现这一点的代码片段如下。NLTK库用于将语音句子标记成句子。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="b157" class="ms la it mo b gy mt mu l mv mw">train_sentences = []<br/>val_sentences = []<br/>for filename in sorted(glob.glob(os.path.join('DT_speeches', '*txt'))):<br/>    print(filename)<br/>    f = open(filename, 'r')<br/>    file_input = f.readlines()<br/>    for cnt, line in enumerate(file_input):<br/>        sentences = nltk.sent_tokenize(line)<br/>        for i , sent in enumerate(sentences):<br/>            if i &lt;= len(sentences)*0.85:<br/>                train_sentences.append(sent)<br/>            else:<br/>                val_sentences.append(sent)</span></pre><p id="9fc1" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">训练和赋值语句列表被写入文本文件。GPT-2模型将使用这些文本文件进行训练。</p><h1 id="2fb2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">微调GPT-2语言建模</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">让我们从Giphy训练它GIF—<a class="ae ky" href="https://media.giphy.com/media/g0Kzu8bTbGPPEH0hSm/giphy.gif" rel="noopener ugc nofollow" target="_blank">https://media.giphy.com/media/g0Kzu8bTbGPPEH0hSm/giphy.gif</a></p></figure><p id="2e7c" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">GPT-2是一种语言模型，可以在许多下游任务上进行微调，包括语言建模、摘要等。要了解更多关于语言建模的知识，请阅读我的这个<a class="ae ky" rel="noopener" target="_blank" href="/train-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db">博客</a>。</p><p id="802d" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">要微调你的模型，首先克隆最新的拥抱脸变形金刚回购。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="3d21" class="ms la it mo b gy mt mu l mv mw">git clone <a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/transformers</a></span></pre><p id="c8bd" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">语言模型微调脚本位于examples/language-modeling下。使用以下命令开始训练:</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="66e0" class="ms la it mo b gy mt mu l mv mw">python run_clm.py \<br/>    --model_name_or_path gpt2 \<br/>    --train_data_file &lt;path to your train text file&gt; \<br/>    --eval_data_file &lt;path to your val text file&gt; \<br/>    --do_train \<br/>    --do_eval \<br/>    --output_dir output-gpt2 \<br/>    --block_size=200\<br/>    --per_device_train_batch_size=1\<br/>    --save_steps 5000\<br/>    --num_train_epochs=5 \</span></pre><p id="18a0" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">上面的主要超参数是:</p><ul class=""><li id="5552" class="nc nd it lt b lu mx lx my ma ne me nf mi ng mm ns ni nj nk bi translated">model_name_or_path: gpt2将训练gpt2小模型。如果要训练中等型号，请指定gp T2-中等</li><li id="7227" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm ns ni nj nk bi translated">块大小:输入训练集将被截断成这个大小的块用于训练。该参数是注意窗口。在我个人的GPU上，200的块大小是我可以训练的最大值。</li><li id="b17c" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm ns ni nj nk bi translated">save_steps:保存检查点之前的持续时间</li></ul><p id="19c7" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">除此之外，我们还有批量大小和次数。</p><p id="d9a1" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">我能在大约20分钟内在GTX-1080 Ti上用上述配置训练GPT-2小型飞机。经训练的模型在验证集上具有14的困惑分数。这还不错。我认为，如果你有更多的计算，你可以尝试更长的块大小和GPT-2中等模型，以进一步提高困惑分数。</p><h1 id="3757" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">测试训练好的GPT-2模型</h1><p id="75dd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了测试训练好的模型，我们导航到transformers repo中的examples/text-generation文件夹。为了生成文本，我们需要将路径传递给训练好的模型。我们也可以通过一个提示来给生成指明方向。运行生成的命令如下。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="3948" class="ms la it mo b gy mt mu l mv mw">python run_generation.py \<br/> --model_type gpt2 \<br/> --model_name_or_path output-gpt2/ \<br/> --length 300 \<br/> --prompt "It's a fine Tuesday morning."</span></pre><p id="a9b5" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">我微调的模型生成了以下有趣的片段。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="82cf" class="ms la it mo b gy mt mu l mv mw">It's a fine Tuesday morning. Democrats want to give illegal immigrants free healthcare, free education, and even free healthcare to their children. Republicans want to give all citizens the right to self-defense. Today, we've added a great new provision to the US healthcare law called Obamacare. That means if you go to the doctor, you're protected from having to pay a fortune for the plan of your choice. You're not going to have to worry about premiums.</span></pre><p id="7f88" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">可以看出，GPT-2了解到民主党人更喜欢开放移民和免费医疗。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="44e6" class="ms la it mo b gy mt mu l mv mw">I think we are going to have the greatest year we've ever had. We're going to have the greatest year in the history of our country. Can you believe it? And I say that because for years and years, these guys in the back were saying, "We love this guy." I'm talking about Donald Trump, he's great. Remember all those debates when you couldn't see the crowd? Well, it was like the H1NI infected people getting small amounts of water. He was very weak. It wasn't quite as strong. I guess he should have never been given the treatment. Remember all those debates? I was in the middle of debating him and he went, "Well, this is going to be short." He was just beginning. Now they have him talking all over. I was watching him. He was talking about himself. "Well, it's true. I'll never be able to compete with you, Donald Trump."</span></pre><p id="6f9b" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">在这里，GPT-2正在复制他自我鼓掌的风格——“我们爱这个家伙。”，“嗯，是真的。我永远无法和你竞争，唐纳德·特朗普。”</p><h1 id="7a5e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="424f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在唐纳德·特朗普的演讲上微调GPT2模型既有趣又容易。令人鼓舞的是，只需很少的训练，该模型就能够复制DT词汇表中的风格和关键词。</p><p id="dce7" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">我希望您尝试一下代码，并训练自己的模型。请在下面的评论中分享你的经历。</p><p id="c9e3" class="pw-post-body-paragraph lr ls it lt b lu mx ju lw lx my jx lz ma mz mc md me na mg mh mi nb mk ml mm im bi translated">在<a class="ae ky" href="https://deeplearninganalytics.org/" rel="noopener ugc nofollow" target="_blank">深度学习分析</a>，我们非常热衷于使用机器学习来解决现实世界的问题。我们已经帮助许多企业部署了创新的基于人工智能的解决方案。如果您看到合作的机会，请通过我们的网站<a class="ae ky" href="https://deeplearninganalytics.org/contact-us/" rel="noopener ugc nofollow" target="_blank">这里</a>联系我们。</p><h1 id="61ad" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><ul class=""><li id="7871" class="nc nd it lt b lu lv lx ly ma nt me nu mi nv mm ns ni nj nk bi translated"><a class="ae ky" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">开启艾GPT-2 </a></li><li id="5414" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm ns ni nj nk bi translated"><a class="ae ky" href="https://huggingface.co/transformers/model_doc/t5.html" rel="noopener ugc nofollow" target="_blank">拥抱脸变形金刚</a></li><li id="fff0" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm ns ni nj nk bi translated">唐纳德·特朗普演讲</li></ul></div></div>    
</body>
</html>