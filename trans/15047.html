<html>
<head>
<title>ELMo: Why it’s one of the biggest advancements in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ELMo:为什么这是NLP最大的进步之一</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/elmo-why-its-one-of-the-biggest-advancements-in-nlp-7911161d44be?source=collection_archive---------19-----------------------#2020-10-16">https://towardsdatascience.com/elmo-why-its-one-of-the-biggest-advancements-in-nlp-7911161d44be?source=collection_archive---------19-----------------------#2020-10-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5307" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">语言模型嵌入(ELMo)是一种先进的语言建模思想。是什么让它如此成功？</h2></div><p id="a1f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">2018年发布的“深度语境化单词嵌入”提出了从语言模型嵌入(ELMo)的想法，在许多热门任务上实现了最先进的性能，包括问答、情感分析和命名实体提取。ELMo已经被证明可以提高近5%的性能。但是是什么让这个想法如此具有革命性呢？</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/a7f33710ad32064f10907883ebd21780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BbvtIpw6DowScMkACXivIA.jpeg"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated"><a class="ae ma" href="https://www.flickr.com/photos/40486157@N00" rel="noopener ugc nofollow" target="_blank">创作的</a><a class="ae ma" href="https://www.flickr.com/photos/40486157@N00/3438225685" rel="noopener ugc nofollow" target="_blank">《Elmo近距离》</a>在2.0 的<a class="ae ma" href="https://creativecommons.org/licenses/by/2.0/?ref=ccsearch&amp;atype=rich" rel="noopener ugc nofollow" target="_blank"> CC下授权</a></p></figure><p id="0d3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">ELMo是什么？ELMo不仅是一个提线木偶，还是一个强大的计算模型，可以将单词转换成数字。这一重要过程允许机器学习模型(接受数字，而不是单词作为输入)根据文本数据进行训练。</strong></p><p id="d427" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">ELMo为什么这么好？</strong>当我通读原文时，有几个要点引起了我的注意:</p><ol class=""><li id="25d2" class="mb mc iq kh b ki kj kl km ko md ks me kw mf la mg mh mi mj bi translated">ELMo说明了一个单词的上下文。</li><li id="acd2" class="mb mc iq kh b ki mk kl ml ko mm ks mn kw mo la mg mh mi mj bi translated">ELMo是在大型文本语料库上训练的。</li><li id="2b92" class="mb mc iq kh b ki mk kl ml ko mm ks mn kw mo la mg mh mi mj bi translated">ELMo是开源的。</li></ol><p id="94c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们详细讨论这些要点，并谈谈它们为什么重要。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><p id="6b47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> #1: ELMo可以唯一说明一个单词的上下文。</strong>以前的语言模型，如GloVe、Bag of Words和Word2Vec，只是基于单词的字面拼写生成嵌入。他们不考虑这个词的用法。例如，在以下示例中，这些语言模型将为“信任”返回相同的嵌入:</p><blockquote class="mw mx my"><p id="74de" class="kf kg mz kh b ki kj jr kk kl km ju kn na kp kq kr nb kt ku kv nc kx ky kz la ij bi translated">我不能信任你。</p><p id="2be5" class="kf kg mz kh b ki kj jr kk kl km ju kn na kp kq kr nb kt ku kv nc kx ky kz la ij bi translated">他们不再信任他们的朋友。</p><p id="ebaa" class="kf kg mz kh b ki kj jr kk kl km ju kn na kp kq kr nb kt ku kv nc kx ky kz la ij bi translated">他有一个信托基金。</p></blockquote><p id="fc06" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，ELMo根据周围的单词为同一个单词返回不同的嵌入——它的嵌入是<em class="mz">上下文相关的</em>。在这些例子中，它实际上会为“信任”返回不同的答案，因为它会识别出这个词在不同的上下文中使用。这种独特的能力本质上意味着ELMo的嵌入有更多的可用信息，因此性能可能会提高。一个类似的考虑上下文的语言建模方法是<a class="ae ma" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT </a>。</p><p id="cd98" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">埃尔莫接受了大量数据的训练。无论你是一名资深的机器学习研究人员还是一名不经意的观察者，你可能都熟悉大数据的力量。最初的ELMo模型是在55亿词的语料库上训练的，即使是“小”版本也有10亿词的训练集。数据真多啊！在如此多的数据上接受训练意味着ELMo学到了大量的语言知识，并将在广泛的数据集上表现良好。</p><p id="c05e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> #3:任何人都可以使用ELMo！</strong>推动机器学习作为一个领域发展的最重要因素之一是开源研究的文化。通过开源代码和数据集，研究人员可以让该领域的其他人轻松应用和建立现有的想法。按照这种文化，ELMo是广泛开源的。它有一个<a class="ae ma" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank">网站</a>，不仅包括它的基本信息，还包括该型号的小型、中型和原始版本的下载链接。希望使用ELMo的人绝对应该访问这个网站，快速获得该模型的副本。此外，代码<a class="ae ma" href="https://github.com/allenai/allennlp" rel="noopener ugc nofollow" target="_blank">发布在GitHub </a>上，包括一个非常详尽的自述文件，让用户知道如何使用ELMo。如果有人花了几个多小时才让一个ELMo模型运行起来，我会感到很惊讶。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nd"><img src="../Images/d055fbd71fcff76f6d7fb3ee6a92a9fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z3ZhNlJwDE259SPJEYxV_g.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">ELMo在著名的任务上实现了最先进的性能，例如SQuAD、NER和SST。图片来源于Peters等人，ELMo论文的原始作者。</p></figure><p id="aa8b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ELMo是上下文感知单词嵌入和大数据的强大组合，在NLP的大数据集上取得了一流的性能，包括SQuAD、NER和SST。ELMo彻底改变了我们处理计算语言学任务的方式，如问答和情感检测，这显然是该领域的一个关键进步，因为它被引用了4500多次。此外，向计算语言学协会(ACL)会议(最大的国际NLP会议)提交的论文在ELMo发表后翻了一番，从2018年的1，544篇论文增加到2019年的2，905篇论文(尽管这也可能归因于2019年初BERT的发表)。</p><p id="7002" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我还想指出，埃尔默和伯特非常相似，因为他们都来自芝麻街！好吧，它们都是语言模型，说明了一个单词的上下文，在一个大的数据集上训练，并且正如我们所知，正在彻底改变自然语言处理领域。(我也写了一篇关于BERT的博文，如果你感兴趣，可以在这里找到<a class="ae ma" rel="noopener" target="_blank" href="/bert-why-its-been-revolutionizing-nlp-5d1bcae76a13"/>)。</p><p id="a5b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ELMo是NLP中最大的进步之一，因为它本质上是第一个关注上下文的语言模型，允许在多个任务中实现更好的性能。</p><p id="3b74" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">延伸阅读:</p><ul class=""><li id="de59" class="mb mc iq kh b ki kj kl km ko md ks me kw mf la ne mh mi mj bi translated"><a class="ae ma" href="https://arxiv.org/pdf/1802.05365.pdf" rel="noopener ugc nofollow" target="_blank">Peters等人的原始论文</a></li><li id="ce50" class="mb mc iq kh b ki mk kl ml ko mm ks mn kw mo la ne mh mi mj bi translated"><a class="ae ma" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"> ELMo的网站，其中包括它的下载链接。</a></li><li id="89ef" class="mb mc iq kh b ki mk kl ml ko mm ks mn kw mo la ne mh mi mj bi translated"><a class="ae ma" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT，一个使用上下文嵌入的类似语言模型。</a></li></ul></div></div>    
</body>
</html>