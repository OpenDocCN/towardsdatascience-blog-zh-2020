<html>
<head>
<title>Latent Dirichlet Allocation: Intuition, math, implementation and visualisation with pyLDAvis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">潜在的狄利克雷分配:直觉，数学，实施和可视化与pyLDAvis</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/latent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094?source=collection_archive---------11-----------------------#2020-09-26">https://towardsdatascience.com/latent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094?source=collection_archive---------11-----------------------#2020-09-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="6665" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">TL；DR — <a class="ae ko" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank">潜在狄利克雷分配</a> (LDA，有时LDirA/LDiA)是在文本数据中寻找<strong class="js iu">主题的最流行和可解释的生成模型之一。我已经提供了一个基于网络搜集的职位描述数据的<a class="ae ko" href="https://nbviewer.jupyter.org/github/Ioana-P/MLEng_vs_DScientist_analysis/blob/master/2_Topic_modelling.ipynb#topic=0&amp;lambda=1&amp;term=" rel="noopener ugc nofollow" target="_blank">示例笔记本</a>。虽然在像<a class="ae ko" href="https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups" rel="noopener ugc nofollow" target="_blank"> 20Newsgroups </a>这样的规范数据集上运行LDA会提供<a class="ae ko" href="https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb" rel="noopener ugc nofollow" target="_blank">更清晰的主题</a>，但重要的是要见证主题识别在“野外”有多么困难，以及你可能如何实际上找不到清晰的主题——使用无监督学习，你<em class="kp">永远无法保证找到答案！</em></strong></p><ul class=""><li id="c93b" class="kq kr it js b jt ju jx jy kb ks kf kt kj ku kn kv kw kx ky bi translated"><strong class="js iu">致谢</strong>:对我理解<em class="kp">最有帮助的</em>是路易斯·塞拉诺在LDA (2020)上的两个视频。很多直觉部分都是基于他的解释，我强烈建议你访问他的<a class="ae ko" href="https://www.youtube.com/watch?v=T05t-SqKArY" rel="noopener ugc nofollow" target="_blank">视频</a>进行更深入的剖析。</li></ul><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi kz"><img src="../Images/c40eb9cc7f4d9edebfd97f271ee26364.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xs1Xe1Hh4P6IGyWN8fImXw.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图1.0 —生成文档的LDA“机器”</p></figure><h1 id="ce9c" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">内容:</h1><p id="7896" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated"><a class="ae ko" href="#f1d8" rel="noopener ugc nofollow">直觉</a></p><p id="d52c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="#7b8f" rel="noopener ugc nofollow">数学</a></p><p id="0f15" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="#b9d6" rel="noopener ugc nofollow">实施和可视化</a></p><h1 id="f1d8" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">直觉</h1><p id="e163" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">假设您有一个不同新闻文章的集合(您的<em class="kp">文档</em>的<em class="kp">语料库</em>，并且您怀疑在所述语料库中有几个经常出现的主题——您的目标是找出它们是什么！为了达到这个目标，你需要做一些关键的假设</p><ul class=""><li id="2fa4" class="kq kr it js b jt ju jx jy kb ks kf kt kj ku kn kv kw kx ky bi translated"><em class="kp">d</em><a class="ae ko" href="https://en.wikipedia.org/wiki/Distributional_semantics#:~:text=The%20distributional%20hypothesis%20suggests%20that,occur%20in%20similar%20linguistic%20contexts." rel="noopener ugc nofollow" target="_blank"><em class="kp">分布假设</em> </a> <em class="kp"> : </em>频繁出现在一起的词，很可能意义相近；</li><li id="95cc" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn kv kw kx ky bi translated">每个题目都是不同单词的混合(图1.1)；</li><li id="1fba" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn kv kw kx ky bi translated">每个文档都是不同主题的混合体(图1.2)。</li></ul><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi mx"><img src="../Images/7d8a87789f63328e3a4de55d69e3a0da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bgPL1Ex8dfxBSM7bSE3HlA.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图1.1 —混合单词的主题</p></figure><p id="5012" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在图1.1中，你会注意到主题“健康和医学”有各种各样的与之相关的词，其关联程度<em class="kp"/>(“癌症”比“血管”或“锻炼”更紧密)。请注意，不同的单词可以与不同的主题相关联，例如单词“cardio”。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi my"><img src="../Images/1d78d379afdbaedd772146fc6a57ab12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-dW-PbkYomLrP6XtNTYwHA.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图1.2 —混合主题的文档</p></figure><p id="a3b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在图1.2中，你会看到一个单独的文档可能涉及多个主题(如左边的颜色代码所示)。像“受伤”和“恢复”这样的词也可能属于多个主题(因此我用了不止一种颜色)。</p><p id="6604" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在LDA是一个<em class="kp">生成模型</em>——它试图确定<em class="kp">生成</em>文章和话题的潜在机制。想象一下，如果有一台具有特定设置的机器可以输出文章，但我们看不到机器的设置，只能看到它产生的内容。LDA创建一组具有不同设置的机器，并选择给出最佳拟合结果的机器(Serrano，2020)。一旦找到最好的一个，我们看一看它的“设置”,并从中推断出主题。</p><p id="1f2b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">那么这些<em class="kp">设置</em>是什么呢？</p><p id="a447" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，我们有一个叫做<em class="kp">狄利克雷</em>(发音像迪-里什-雷)<em class="kp">的东西，先于主题</em>。这是一个表明我们的话题有多稀疏或者有多混杂的数字。在L Serrano的视频里(我强烈推荐！)他阐述了如何在视觉上将其视为一个三角形(图1.3)，其中点代表文档，它们相对于角的位置(即主题)代表它们与每个主题的关系(2020)。所以一个非常接近“运动”顶点的点将几乎完全是关于运动的。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi mz"><img src="../Images/a85e97239f65dcf7eb141aea83c35128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*46pYVxXIOAL7qd40Bs_xHQ.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图1.3 —主题的狄利克雷分布</p></figure><p id="27b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在左边的三角形中，文档被很好地分开，大部分文档被整齐地塞到角落里(这对应于一个低的Dirichlet先验，alpha &lt;1); on the right they are in the middle and represent a more even mix of topics (a higher Dirichlet prior, alpha&gt; 1)。查看图1.2中的文档，考虑一下主题的组合，想想你认为它应该放在右边三角形的什么位置(我的答案是它应该是最靠近体育角的上方的点<em class="kp">)。</em></p><p id="ed7a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第二，我们有<em class="kp">项</em>(我们词汇表中的所有单词)的狄利克雷先验。这个数字(它的名字是<em class="kp">贝塔)</em>与阿尔法的功能几乎完全相同——除了它决定了<strong class="js iu">主题</strong>如何在<strong class="js iu">术语</strong> <em class="kp">中分配。</em></p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi na"><img src="../Images/1dac2c4278b6c32a7efdf7ff0baf4873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ctgYvHaDDkcDKAzYcVigHg.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图1.4各项的狄利克雷分布；这些数字与每个单词与每个主题的关联程度成正比</p></figure><p id="43c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们之前所说的，主题被假设为不同术语的混合(更准确地说是<em class="kp">分布</em>)。在图1.4中，“运动”主要是指“伤害”。“健康&amp;医学”在“心脏”和“损伤”之间徘徊，与“祈祷”一词毫无关联。</p><p id="7636" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="kp">但是等等，我们的词汇不仅仅由三个单词组成！</em>你说得对！我们可以有一个由<em class="kp"> 4个单词</em>组成的词汇表(如图1.5所示)！麻烦在于，可视化一个典型的由<em class="kp"> N </em>个单词组成的词汇表(其中<em class="kp"> N </em>可能是10000)需要一个<a class="ae ko" href="https://en.wikipedia.org/wiki/Simplex#The_standard_simplex" rel="noopener ugc nofollow" target="_blank">广义版本的三角形，</a>但是是在<em class="kp">N-1</em>维度中(这个术语是n-1 <em class="kp">单形</em>)。这就是视觉效果停止的地方，我们相信更高维度的数学将会像预期的那样发挥作用。这也适用于主题——我们经常会发现自己有3个以上的主题。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi nb"><img src="../Images/6800a8cd6adde7e77a8fa071a47d25fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iq3bjiBg_Pchh0upPmnmMQ.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图1.5-根据术语的分布，哪个主题是红色的？</p></figure><p id="fb01" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个重要的澄清:在LDA中，我们从α和β的值开始作为超参数，但是这些数字<em class="kp">仅仅</em>告诉我们我们的点(文档/主题)是否<strong class="js iu">通常</strong>集中在它们三角形的中间或者更靠近角落。三角形(单纯形)内的<em class="kp">实际位置</em>由机器猜测——猜测不是随机的，它被狄利克雷先验加权。</p><p id="9e47" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，机器创建了两个狄利克雷分布，<em class="kp">将文档和主题分配给</em>，然后<em class="kp">基于这些分布生成</em>文档(图1.6)。那么，最后一步是如何发生的，即<em class="kp">代</em>部分？</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi kz"><img src="../Images/c40eb9cc7f4d9edebfd97f271ee26364.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xs1Xe1Hh4P6IGyWN8fImXw.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图1.6 —生成文档的LDA“机器”</p></figure><p id="d971" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">还记得在开始时我们说过主题被看作是单词的混合/分布，文档被看作是主题的混合/分布吗？在图1.7中从左到右，我们从一个文档开始，在三角形的某个地方，在我们的3个主题之间撕裂。如果它在“体育”角附近，这意味着文档将主要是关于体育的，也会提到一些“宗教”和“健康医学”。所以我们知道文档的主题构成→因此我们可以估计会出现什么<em class="kp">单词</em>。我们将主要从体育中抽取(即随机抽取)单词，一些来自健康&amp;医学，极少量来自宗教(图1.7)。有个问题问你:看图1.7底部的三角形，你觉得<em class="kp">字2 </em>会不会上来？</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi nc"><img src="../Images/41ea9a9f43d93f6e8905f3ee345ae1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hDZIC8V8IyX-otJ1eblCuw.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图1.7 —两种狄利克雷分布如何影响我们的文档生成</p></figure><p id="70d3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">答案是<strong class="js iu">它可能</strong>:记住主题是单词的混合。你可能会想，<em class="kp">单词2 </em>与黄色(宗教)主题有很强的相关性，由于这个主题在本文档中很少出现，<em class="kp">单词2 </em>不会出现太多。但是要记住a. <em class="kp">单词2 </em>也与蓝色、体育话题和b .单词是概率上的样本，所以每个单词都有一些非零的出现几率。</p><p id="f5e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们最终生成的文档中的单词(在图1.7的右端)将与原始文档中的单词进行比较。我们不会得到相同的文档，但是当我们将一系列不同的LDA“机器”与一系列不同的发行版进行比较时，我们发现其中一个比其他的更接近于生成文档，这就是我们选择的LDA模型。</p><h1 id="7b8f" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">数学</h1><p id="cf4d" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">正常的统计语言模型假设您可以通过从单词的概率分布中进行采样来生成文档，即对于我们词汇表中的每个单词，都有一个该单词出现的关联概率。</p><p id="a943" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">LDA给这种安排增加了一层复杂性。它假设了一个主题列表。每个文档<em class="kp"> m </em>是这些<em class="kp"> k </em>主题的概率分布，每个主题是我们词汇表<em class="kp"> V </em>中所有不同术语的概率分布。也就是说每个词在每个题目中出现的概率是各种各样的。</p><p id="6011" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">生成文档的全概率公式如下图2.0所示。如果我们将它分解，在右边我们会得到三个乘积和:</p><ul class=""><li id="a690" class="kq kr it js b jt ju jx jy kb ks kf kt kj ku kn kv kw kx ky bi translated"><strong class="js iu">主题随词条的狄利克雷分布:</strong>(对应图1.4和1.5)对于每个主题<em class="kp"> i </em>在<em class="kp"> K个主题</em>中，<em class="kp"> i. </em>的词的概率分布是什么</li><li id="5280" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn kv kw kx ky bi translated"><strong class="js iu">文档在主题上的狄利克雷分布:</strong>(对应图1.3)对于我们规模为<em class="kp"> M、</em>的语料库中的每个文档<em class="kp"> j、</em>的主题概率分布是什么</li><li id="25ab" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn kv kw kx ky bi translated"><strong class="js iu">给定文档中某个主题出现的概率X给定主题中某个单词出现的概率:</strong>(对应图1.7中的两个矩形)某个主题、<em class="kp">、</em>出现在该文档中的可能性有多大，然后给定那些主题，某个单词、<em class="kp">、</em>出现的可能性有多大。</li></ul><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi nd"><img src="../Images/c6bf507d9bbbc84bcae267f52f26b209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUTv6gS_8GDQodj4TlGTgw.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图2.0 — LDA公式</p></figure><p id="a07e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">前两个和包含<strong class="js iu">对称</strong>狄利克雷分布，这是我们的文档和主题的先验概率分布(图2.1显示了一组一般的狄利克雷分布，包括对称分布)。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/c7d56d15e2604235a19a794e5292ff58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*YJbCG2oZI6prRgIBmHiQtg.png"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图2.1-由emperistor-Own工作，CC BY-SA 4.0，<a class="ae ko" href="https://commons.wikimedia.org/w/index.php?curid=49908662" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/w/index.php?curid=49908662</a></p></figure><p id="989e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第三个和包含两个多项式分布，一个针对主题，一个针对单词，即我们从主题的概率分布中抽取主题，然后对于每个主题实例，我们从该特定主题的单词的概率分布中抽取单词。</p><p id="400c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如在直觉部分末尾提到的，使用最终概率，我们试图生成与原始文档中相同的单词分布。实现这一点的概率<em class="kp">非常非常低</em>，但是对于α和β的某些值，概率会更低。</p><h2 id="180f" class="nf lq it bd lr ng nh dn lv ni nj dp lz kb nk nl md kf nm nn mh kj no np ml nq bi translated">解读LDA模型及其主题</h2><p id="ce3a" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">我们用什么标准来发现我们的潜在主题？正如雪莉和西维尔所说:</p><blockquote class="nr ns nt"><p id="1572" class="jq jr kp js b jt ju jv jw jx jy jz ka nu kc kd ke nv kg kh ki nw kk kl km kn im bi translated">“为了解释一个主题，人们通常检查该主题中最可能出现的术语的排序列表，[……]。以这种方式解释主题的问题是，语料库中的常用术语经常出现在多个主题的列表顶部附近，这使得很难区分这些主题的含义。”(2014)</p></blockquote><p id="db80" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这正是我们在下一节<em class="kp">实现</em>中遇到的问题。因此，我们使用另一种衡量标准来解释我们的主题— <em class="kp">相关性</em> (Shirley和Sievert，2014)。</p><h2 id="fcc4" class="nf lq it bd lr ng nh dn lv ni nj dp lz kb nk nl md kf nm nn mh kj no np ml nq bi translated">关联</h2><p id="8ee7" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">这是一个可调整的指标，它可以平衡某个术语在特定主题中的出现频率与该术语在整个文档语料库中的出现频率。</p><p id="7aee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">换句话说，如果我们有一个在某个主题中非常流行的术语，相关性允许我们衡量它的流行程度有多少是因为它非常特定于该主题，有多少是因为它只是一个到处出现的作品。后者的一个例子是工作描述数据中的“学习”。当我们用一个较低的lambda调整相关性时(即惩罚那些碰巧在<strong class="js iu">所有</strong>主题中频繁出现的术语)，我们看到“学习”并不是一个特别的术语，它之所以频繁出现只是因为它在语料库中普遍存在。</p><p id="2750" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">相关性的数学定义是:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a5437986e3644ef7ae58cda2c7648a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/0*tL0f-BtwU3oSv-8-"/></div></figure><ul class=""><li id="fa6b" class="kq kr it js b jt ju jx jy kb ks kf kt kj ku kn kv kw kx ky bi translated"><em class="kp">相关度</em></li><li id="d454" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn kv kw kx ky bi translated"><em class="kp">⍵——我们词汇中的一个术语</em></li><li id="4136" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn kv kw kx ky bi translated"><em class="kp"> k — </em>我们的LDA制作的主题之一</li><li id="2637" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn kv kw kx ky bi translated"><em class="kp"> λ — </em>可调权重参数</li><li id="956d" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn kv kw kx ky bi translated">𝝓kw —某个术语出现在特定主题中的概率</li><li id="df24" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn kv kw kx ky bi translated"><strong class="js iu"><em class="kp">p</em></strong><em class="kp">w—</em>一个词在整个语料库中出现的概率</li></ul><p id="8ab0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">除了λ，<em class="kp"> λ，</em>所有项都是从LDA数据和模型中导出的。我们将在下一节中调整lambda，以帮助我们获得更有用的见解。原论文作者将lambda保持在0.3到0.6的范围内(Shirley和Sievert，2014)。</p><h1 id="b9d6" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">实现和可视化</h1><p id="d857" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">sklearn的LatentDirichletAllocation模型的实现遵循大多数sklearn模型的模式。在我的<a class="ae ko" href="https://nbviewer.jupyter.org/github/Ioana-P/MLEng_vs_DScientist_analysis/blob/master/2_Topic_modelling.ipynb#topic=0&amp;lambda=1&amp;term=" rel="noopener ugc nofollow" target="_blank">笔记本</a>中，我:</p><ol class=""><li id="9c52" class="kq kr it js b jt ju jx jy kb ks kf kt kj ku kn ny kw kx ky bi translated">预处理我的文本数据，</li><li id="1edb" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn ny kw kx ky bi translated">将其矢量化(产生文档术语矩阵)，</li><li id="f7c9" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn ny kw kx ky bi translated">Fit _使用LDA转换它，然后</li><li id="e48c" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn ny kw kx ky bi translated">检查结果，看看是否有任何紧急的，可识别的主题。</li></ol><p id="0964" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后一部分是高度主观的(记住这是<em class="kp">无监督学习</em>)，并不能保证揭示任何真正有趣的东西。此外，识别主题(如聚类)的能力取决于您对数据的领域知识。我还建议修改alpha和beta参数，以符合您对文本数据的期望。</p><p id="27fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我使用的数据是来自indeed.co.uk的职位描述数据。除了文本，dataframe还有许多其他属性，包括我是否使用了搜索术语“数据科学家”、“数据分析师”或“机器学习工程师”。我们可以在LDA主题中找到一些原始搜索类别吗？</p><p id="d34e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在下面的要点中，你会看到我已经将我的数据矢量化，并将其传递给LDA模型(这发生在data_to_lda函数下)。</p><figure class="la lb lc ld gt le"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="d600" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">运行这段代码和print_topics函数将产生如下结果:</p><pre class="la lb lc ld gt ob oc od oe aw of bi"><span id="7283" class="nf lq it oc b gy og oh l oi oj">Topics found via LDA on Count Vectorised data for ALL categories:<br/><br/>Topic #1:<br/>software; experience; amazon; learning; opportunity; team; application; business; work; product; engineer; problem; development; technical; make; personal; process; skill; working; science<br/><br/>Topic #2:<br/>learning; research; experience; science; team; role; work; working; model; skill; deep; please; language; python; nlp; quantitative; technique; candidate; algorithm; researcher</span><span id="38c8" class="nf lq it oc b gy ok oh l oi oj">Topic #3:<br/>learning; work; team; time; company; causalens; business; high; platform; exciting; award; day; development; approach; best; holiday; fund; mission; opportunity; problem<br/><br/>Topic #4:<br/>client; business; team; work; people; opportunity; service; financial; role; value; investment; experience; firm; market; skill; management; make; global; working; support</span><span id="5e23" class="nf lq it oc b gy ok oh l oi oj">...</span></pre><p id="3df6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">“print_topics”功能按概率降序给出每个主题的术语，其中<strong class="js iu">可以</strong>提供信息。正是在这个阶段，我们可以<strong class="js iu">开始</strong>尝试从我们的模型中标记出新出现的、潜在的主题。例如，主题1似乎与ML工程师的技能和需求关系不大(提到“amazon”与使用AWS有关——这是我在另一个笔记本中从项目的EDA阶段发现的)；与此同时，鉴于“市场”、“金融”、“全球”等术语，主题4显然更面向客户或面向业务。</p><p id="791c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在这两个类别对你来说可能有点牵强，这是一个公平的批评。你可能已经注意到，使用这种方法来确定主题是很难的。所以，让我们转向pyLDAvis！</p><h2 id="b27b" class="nf lq it bd lr ng nh dn lv ni nj dp lz kb nk nl md kf nm nn mh kj no np ml nq bi translated">皮尔戴维斯</h2><p id="2d20" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">使用pyLDAvis，LDA数据(在我们的例子中是10维的)已经通过PCA(主成分分析)被分解为仅2维的。因此，为了可视化的目的，它被展平了。我们有十个圆，每个圆的中心代表我们的主题在潜在特征空间中的位置；主题之间的距离说明了主题相似(不相似)的程度，圆圈的面积与每个主题对应的文档数量成正比。</p><p id="048a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面我展示了如何在pyLDAvis中插入一个已经训练好的sklearn LDA模型。令人欣慰的是，负责将最初的LDAvis (是R模型)改编成python的人让它与sklearn有效地通信。</p><figure class="la lb lc ld gt le"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="1411" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在图3.0中是我们生成的图:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi ol"><img src="../Images/ba410bcfcb008a9d6a5785a9abbeb87e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e9Fj031z3H1s_eNx_KnfWg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图3.0 — pyLDAvis交互图</p></figure><p id="18da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">解读皮尔戴维斯地块</strong></p><p id="0ebb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">LDAvis图分为两部分——一部分是我们的n维LDA数据的二维“扁平”重绘图，另一部分是交互式的、变化的术语分布水平条形图。图A1.0显示了这两种情况。需要注意的一个重要特征是，右边的条形图以相关性递减顺序显示了<em class="kp">主题中的术语，但条形显示了术语的频率。红色部分代表纯粹在特定主题内的术语频率；红色和蓝色表示文档语料库中的总术语频率。</em></p><p id="0676" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">调整</strong><em class="kp">λ(λ)</em></p><p id="fa0f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们设置λ等于1，那么我们的相关性完全由该词与该主题的概率给出。将它设置为0将导致我们的相关性由该词对主题的特异性决定-这是因为右边的词将某个词在特定主题中出现的概率除以该词一般出现的概率-因此，当我们使用较低的<em class="kp"> λ </em>值时，高频词(如“团队”、“技能”、“业务”)的相关性将大大降低。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi om"><img src="../Images/27ebae84bd34ef186f1d49caaa075983.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gZJYETiTTlLPyi2VsXam9Q.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图3.1-将λ设置为1</p></figure><p id="f1c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在图3.1中<em class="kp"> λ </em>被设置为1，你可以看到这些术语通常与占主导地位的术语相匹配(例如，在我们打印出的每个主题的最流行术语中)。这只针对主题1，但是当我改变主题时，前30个最相关的术语的分布几乎没有变化！</p><p id="f4ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，在图3.2中<em class="kp"> λ </em>被设置为0，术语完全改变了！</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi on"><img src="../Images/f7cda7e123b68cc7ad4c2bc98ac738fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vHiv2kJNqRAZdsC23_O3sg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图3.2-λ设置为0</p></figure><p id="1c2c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们有了非常具体的术语，但是请注意顶部的刻度——最相关的词出现了大约60次。超过6000英镑后，下降幅度相当大！还有，这些话不一定会告诉我们什么有趣的东西。如果你用这个lambda值选择一个不同的主题，你将继续得到不一定那么重要的垃圾术语。</p><p id="8a52" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在图3.3中，我将lambda设置为0.6，我正在探索主题2。马上就有一个围绕工程师工作的重要主题，比如“aws”、“云”和“平台”。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi oo"><img src="../Images/8caae04cafb8cf438732c405083ab14b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Z1RjZ39WG4OCe6uyFE4uA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图3.3-λ= 0.6</p></figure><p id="2b95" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">用pyLDAvis可以做的另一件大事是直观地检查给定单词的条件主题分布，只需将鼠标悬停在单词上即可(图3.4)。下面我们可以看到有多少“NLP”被分成几个主题——不是很多！这让我更有理由相信主题6关注的是NLP和基于文本的工作(像“语音”、“语言”、“文本”这样的术语在这方面也有帮助)。对我来说，一个有趣的发现是“研究”和“博士”在这个话题中如此强烈地同时出现。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi op"><img src="../Images/264aa3fd33cc0c35550e73affdaec1f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lsD8XKNR7YSUlqkcVp-ZjA.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图3.4——“自然语言处理”的条件主题分布</p></figure><p id="4cac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是否意味着行业中NLP为重点的角色比其他角色需要更高的教育程度？他们是否比其他职位更需要以前的研究经验？NLP的角色可能更专注于实验技术，因此需要具有前沿知识的人吗？</p><p id="4fbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然生成的交互图不能提供具体的答案，但它可以为我们提供一个进一步研究的起点。如果你在一个可以运行主题建模的组织中，你可以使用LDA的潜在主题为调查设计、A/B测试提供信息，甚至将其与其他可用数据相关联，以找到有趣的相关性！</p><p id="a838" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">祝你在话题建模上好运。如果你喜欢这篇冗长的文章，请给我你认为合适的掌声。如果你对LDA有所了解，并且认为我做了一些<strong class="js iu">甚至是部分错误的</strong>的事情，请给我留言(反馈是一份礼物，诸如此类)！</p><h2 id="dddf" class="nf lq it bd lr ng nh dn lv ni nj dp lz kb nk nl md kf nm nn mh kj no np ml nq bi translated"><strong class="ak">参考文献</strong></h2><ol class=""><li id="74eb" class="kq kr it js b jt mn jx mo kb oq kf or kj os kn ny kw kx ky bi translated">Serrano L. (2020)。在线访问:<a class="ae ko" href="https://www.youtube.com/watch?v=T05t-SqKArY" rel="noopener ugc nofollow" target="_blank">潜在狄利克雷分配(第1部分，共2部分)</a></li><li id="17c8" class="kq kr it js b jt ms jx mt kb mu kf mv kj mw kn ny kw kx ky bi translated">希沃特c .和雪莉K (2014)。LDAvis:一种可视化和解释主题的方法。在线访问:<a class="ae ko" href="https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf" rel="noopener ugc nofollow" target="_blank">交互式语言学习、可视化和界面研讨会会议录</a></li></ol></div></div>    
</body>
</html>