<html>
<head>
<title>Gaussian Mixture Models vs K-Means. Which One to Choose?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高斯混合模型与K-均值。选哪个？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gaussian-mixture-models-vs-k-means-which-one-to-choose-62f2736025f0?source=collection_archive---------1-----------------------#2020-10-08">https://towardsdatascience.com/gaussian-mixture-models-vs-k-means-which-one-to-choose-62f2736025f0?source=collection_archive---------1-----------------------#2020-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bb9e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">两种流行聚类算法的性能比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/912d9f6dc08136919b8c3750f5c54d66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c1Bu6ZNg528fWZyU"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">G.混合物对k .平均(1957)。帆布油画。顺便说一句，没有明显的赢家。伯明翰博物馆信托基金会在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="e02f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> K </span> -Means和高斯混合(GMs)都是聚类模型。然而，许多数据科学家倾向于选择更流行K-Means算法。即使GMs可以证明在某些聚类问题上是优越的。</p><p id="53ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将看到这两种模型在速度和健壮性方面提供了不同的性能。我们还将看到，使用K-Means作为GMs的初始化器是可能的，这有助于提高聚类模型的性能。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="583b" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">它们是如何工作的</h1><p id="b578" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">首先，让我们回顾一下这些算法的理论部分。这将有助于我们在文章的后面理解他们的行为。</p><h2 id="b694" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">k均值</h2><p id="f861" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">K-Means是一种流行的非概率聚类算法。该算法的目标是最小化失真度量<em class="nu">J</em>T8】。我们通过以下迭代程序实现这一点【1】:</p><ol class=""><li id="35bc" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">选择集群的数量<em class="nu"> K </em></li><li id="0757" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">初始化定义每个聚类中心点的向量<strong class="lb iu"> μ_k </strong></li><li id="a948" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">将每个数据点<strong class="lb iu"> x </strong>分配到最近的聚类中心</li><li id="1a68" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">为<strong class="lb iu">和</strong>每个集群重新计算中心点<strong class="lb iu"> μ_k </strong></li><li id="121d" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">重复3–4，直到中心点停止移动</li></ol><p id="452b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的GIF很好地说明了这个迭代过程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/1156ee462eb2f9928ab6780a69af9eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*lPcP9mUtfq9sApyWtPIvQg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">K=3时的K均值。重新计算每个聚类的数据点标签和平均值，直到收敛。<a class="ae ky" href="https://commons.wikimedia.org/wiki/File:K-means_convergence.gif" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></figure><p id="0d8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-Means算法将收敛，但它可能不是全局最小值。为了避免收敛到局部最小值的情况，K-Means应该用不同的参数重新运行几次。</p><p id="b32b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-Means执行硬分配，这意味着每个数据点必须属于某个类，并且没有分配给每个数据点的概率。</p><p id="eb34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K均值的计算成本是O(KN)，其中K是聚类的数量，N是数据点的数量。</p><h2 id="d922" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">高斯混合</h2><p id="9836" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">高斯混合基于<em class="nu"> K </em>个独立的高斯分布，用于建模<em class="nu"> K </em>个独立的聚类。提醒一下，多元高斯分布如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/13c0ffc870bc901b6b0e54ef46220377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QAkYq08Dcw3p5WBvbn9mlQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多元高斯分布。<strong class="bd ol"> μ </strong>是D维均值向量，<strong class="bd ol"> ∑ </strong>是DxD协方差矩阵。修改自[1]</p></figure><p id="9b4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">高斯混合的推导是相当复杂的，所以为了更深入的数学解释，我建议看看这篇文章。</p><p id="36dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于GMs要知道的最重要的事情是，这个模型的收敛是基于EM(期望最大化)算法的。它有点类似于K-Means，可以总结如下[1]:</p><ol class=""><li id="74c5" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">初始化<strong class="lb iu"> μ，∑，</strong>和混合系数<strong class="lb iu"> π </strong>并评估对数似然L的初始值</li><li id="4e2f" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">使用当前参数评估责任函数</li><li id="28b7" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">使用新获得的职责获得新的<strong class="lb iu"> μ、∑、</strong>和<strong class="lb iu"> π </strong></li><li id="4bb1" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">再次计算对数似然L。重复步骤2–3，直到收敛。</li></ol><p id="dca5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">高斯混合也将收敛到局部最小值。</p><p id="fc0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过下面的GIF，我们可以很容易地看到高斯混合的收敛情况:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/eb18f82ef4e36db5a8ce59eadecdb7c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*kJYirC6ewCqX1M6UiXmLHQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">高斯混合的收敛性。<a class="ae ky" href="https://tenor.com/view/gaussian-mixture-models-emmethod-math-gauss-computer-science-nerd-gif-15288262" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></figure><p id="71fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-均值和高斯混合的第一个明显区别是决策边界的形状。GMs更加灵活，使用协方差矩阵<strong class="lb iu"> ∑ </strong>我们可以使边界<em class="nu">为椭圆形，</em>与使用K-means <em class="nu">的<em class="nu">圆形</em>边界相反。</em></p><p id="5003" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有一点就是GMs是一个概率算法。通过将概率分配给数据点，我们可以表达我们对给定数据点属于特定聚类的信念有多强。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/57826b8388eaf17636dff6f9be5bf09f.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*CEbtVKo31qC8xYoE1C-C6w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用每个数据点属于某个簇的概率的簇的软分配。图片作者[1]</p></figure><p id="1fd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们比较这两种算法，高斯混合似乎更稳健。然而，GMs通常比K-Means慢，因为它需要EM算法的更多迭代来达到收敛。它们也可以快速收敛到局部最小值，这不是一个非常理想的解决方案。</p><p id="6434" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文的剩余部分，我们将使用Scikit-learn库来研究这些模型在实践中的表现。</p><h1 id="922d" class="ml mm it bd mn mo oo mq mr ms op mu mv jz oq ka mx kc or kd mz kf os kg nb nc bi translated">比较性能</h1><p id="7893" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated"><a class="ae ky" href="https://github.com/KacperKubara/ml-cookbook/tree/master/kmeans_and_gms" rel="noopener ugc nofollow" target="_blank"> <em class="nu">谷歌Colab笔记本这部分可以在这里找到</em> </a></p><p id="a766" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将从为此任务创建一个合成数据集开始。为了使它更具挑战性，我们将创建2个重叠的高斯分布，并在边上添加一个均匀分布。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="1f5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成的数据集如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/f62d6807e96165f56ae349993ccd3342.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*3_qUDpQ40JGCUDAZV7jQew.png"/></div></figure><h2 id="5246" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">聚类形状</h2><p id="09d8" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">让我们动手用K-Means和高斯混合进行初始聚类。我们将使用从Scikit-Learn导入的模型。此外，我们将以相同的方式为两种模型设置参数。两个模型的最大迭代次数、聚类数和收敛容差设置相同。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="004a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从聚类数据的第一眼看，我们可以看出它们的表现并不太好。虽然K-Means以与真实聚类相似的方式对数据进行聚类，但是GM聚类看起来相当不可靠。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/b98e4aa9e50bff7892b11ba519fe8700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iCdIXWZAnPYKYvObEQeVRQ.png"/></div></div></figure><h2 id="3047" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">k均值+高斯混合= ❤️</h2><p id="8ccf" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">GMs的问题是它们很快收敛到局部最小值，这对这个数据集来说不是很理想。为了避免这个问题，<em class="nu"> GMs通常用K-Means </em>初始化。这通常工作得很好，它改善了用K-Means生成的聚类。我们可以通过改变GaussianMixture类中的一个参数来创建带有K-Means初始化器的GM:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="7d14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以利用GMs的概率特性。通过添加阈值，在本例中为0.33，我们能够标记模型不确定的带标签的数据点。这里变得非常有趣，因为对于普通GM来说，大多数数据点的概率都很低，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/15fd1cce27931aadc6adbb28de2ebd97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dMa_mvv1y3f2YQXrvSUdWA.png"/></div></div></figure><p id="f7d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，具有K均值初始化器的GMs似乎表现最好，并且聚类几乎与原始数据相同。</p><h2 id="c45a" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">计算时间</h2><p id="b473" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">现在让我们看看这些算法的计算时间。结果相当令人惊讶。计算时间是用不同数量的聚类和上述所有3个模型来测量的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="4f80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/c7c88743aa268012ca917d43ed2bdd8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*iXzgNtJUzjdTLbaAuPdM3A.png"/></div></figure><p id="53d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很奇怪，普通的K-Means比带有K-Means初始化器的GM要慢。在幕后，Scikit-Learn似乎应用了K-Means的优化版本，它需要更少的迭代来收敛。</p><p id="a0df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，香草转基因需要很短的时间。这是因为它很快就找到了一个局部最小值，而这个最小值甚至还没有接近全局最小值。</p><h1 id="ceac" class="ml mm it bd mn mo oo mq mr ms op mu mv jz oq ka mx kc or kd mz kf os kg nb nc bi translated">那么我应该选择哪种算法呢？</h1><p id="78fc" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">如果你寻找鲁棒性，带有K-Means初始化器的GM似乎是最好的选择。如果你用不同的参数进行实验，K-Means理论上应该更快，但是从上面的计算图中我们可以看出，带有K-Means初始化器的GM是最快的。GM本身并没有太大的用处，因为它对于这个数据集收敛得太快而不是最优解。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="0ee7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nu">感谢您的阅读，希望您喜欢这篇文章！</em></p><h1 id="f154" class="ml mm it bd mn mo oo mq mr ms op mu mv jz oq ka mx kc or kd mz kf os kg nb nc bi translated">关于我</h1><p id="d657" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我是阿姆斯特丹大学的人工智能硕士学生。在我的业余时间，你可以发现我摆弄数据或者调试我的深度学习模型(我发誓这很有效！).我也喜欢徒步旅行:)</p><p id="9e8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解我的最新文章和其他有用的内容，以下是我的其他社交媒体资料:</p><ul class=""><li id="e4c2" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oy ob oc od bi translated"><a class="ae ky" href="https://www.linkedin.com/in/kacperkubara/" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="adc8" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oy ob oc od bi translated"><a class="ae ky" href="https://github.com/KacperKubara" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li></ul><h1 id="9e9b" class="ml mm it bd mn mo oo mq mr ms op mu mv jz oq ka mx kc or kd mz kf os kg nb nc bi translated">参考</h1><p id="e71c" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">[1] <a class="ae ky" href="https://www.goodreads.com/book/show/55881.Pattern_Recognition_and_Machine_Learning" rel="noopener ugc nofollow" target="_blank">模式识别与机器学习(信息科学与统计)</a></p></div></div>    
</body>
</html>