<html>
<head>
<title>Busted Assumptions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">失败的假设</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/busted-assumptions-3e224b7706eb?source=collection_archive---------34-----------------------#2020-10-09">https://towardsdatascience.com/busted-assumptions-3e224b7706eb?source=collection_archive---------34-----------------------#2020-10-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="945c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用Python检查回归假设</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0aa31f2ce58c48df8dc98196fb0c8fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EJeUvjlzdOPs5Ah4s0bxJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由来自Pixabay的Gordon Johnson提供</p></figure><h1 id="f97c" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">资料组</h1><p id="568d" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们将在本教程中使用的数据集来自Kaggle的“房价:高级回归技术”竞赛(<a class="ae mm" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" rel="noopener ugc nofollow" target="_blank">链接</a>)，因为我目前正在提交我的结果。由于这是一个回归问题，我们的任务是预测房价，我们需要检查我们是否满足回归背后的所有主要假设</p><p id="9bc0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">一般来说，如果你违反了这些假设中的任何一个，那么从你的模型中得到的结果可能会非常误导人。违反某些假设比其他假设更严重，但我们应该非常小心地正确处理我们的数据。</p><h1 id="916d" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated"><strong class="ak"> OLS回归</strong></h1><p id="2a09" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在检查回归背后的许多假设之前，我们首先要拟合一个回归模型(在我们的例子中是OLS)。这是因为许多假设检验依赖于计算的残差或我们模型中的误差。残差只不过是实际值和预测值之间的绝对距离。记住这一点也很重要，我拟合到回归模型的数据已经过彻底处理，包括输入缺失值、移除异常值、处理高基数和低基数、转换偏斜(要素和目标)、目标编码分类要素，以及使用StandardScaler()进行标准化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/429b7d70c157286380786372b7cfe4f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*PV62HsRzl__7O3SEEP8-iw.png"/></div></div></figure><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="53aa" class="my kz it mu b gy mz na l nb nc">X = sm.add_constant(X_train_df)<br/>y_train = y_train.reshape(-1,1)<br/>model = sm.OLS(y_train, X)<br/>results = model.fit()<br/>print(results.summary())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/6bd656d265ff5fa5dfa63e2a1fc941b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mdL3kS5AhWdM4rm7_T3WMQ.png"/></div></div></figure><p id="ca96" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">首先，我们添加一个常数来指定初始截距值。在拟合模型时，我们的y_train值需要是2D数组而不是1D。最后，我们拟合数据并打印结果。我不想解释结果表。</p><h1 id="846c" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">回归假设</h1><p id="f857" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu">线性:</strong>线性回归假设目标与每个独立变量或特征之间存在线性关系。这是最重要的假设之一，因为违反这一假设意味着您的模型试图在非线性数据中找到线性关系。这将导致你的模型严重不符合你的数据。检查异常值也很重要，因为它们会对这一假设产生重大影响。</p><p id="b36e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们可以对目标和/或独立变量应用变换来纠正问题。在我将注意力转向目标之前，我通常首先转换特征。不幸的是，应用转换的过程本质上是反复试验。应用对数、平方根、指数和boxcox变换是我的第一道防线。此外，我发现在添加多项式特征以更好地捕捉非线性关系方面取得了一些成功。</p><p id="7fee" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">实际值和预测值的散点图通常用于检查线性。我们希望看到实际值和预测值紧密地分散在回归线周围。我们还可以生成一组显示每个特征和目标之间关系的配对图。但是，这种方法非常耗时，而且容易混淆，尤其是当您有许多功能时。我更喜欢先绘制实际值与预测值，如果我发现有大量违反这一假设的情况，我将使用pairplot方法来确定哪些特征破坏了我的假设。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="9b8a" class="my kz it mu b gy mz na l nb nc">def linearity_assum(model, y):<br/>    <br/>    y_pred = model.predict()</span><span id="7509" class="my kz it mu b gy ne na l nb nc">plt.figure(figsize=(15,10))<br/>    sns.regplot(x=y_pred, y=y, lowess=True, line_kws={'color': 'red'})<br/>    plt.title('Observed vs. Predicted Values', fontsize=16)</span><span id="d89a" class="my kz it mu b gy ne na l nb nc"># Results is the object which contains the trained OLS model<br/>linearity_assum(results, y_train)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/507df9043a774b5a159942aff884505c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2IFyWRJQ7Zo1g9bK25bQ0Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk">:)</p></figure><p id="7ecb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">自相关:</strong>在处理时间序列数据时，这一假设经常被违反，因为它指的是变量与自身之间在不同时间点的相关性。例如，在不同时间点测量的股票价格(同一变量)。横截面数据也可能出现自相关。例如，不同社区之间的房价可能是相关的。</p><p id="1d38" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">不幸的是，我没有太多处理时间序列数据的经验，因此，我不想假定一个修正。然而，在横截面数据的情况下，我发现在拟合模型之前标准化数据有减少自相关的趋势。</p><p id="6057" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们通常使用德宾-沃森测试来检验这一假设。接近2的结果表示没有自相关。然而，当我们接近零(0)时，正自相关的证据就越多，越接近4，负自相关的证据就越多。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="b5d3" class="my kz it mu b gy mz na l nb nc">from statsmodels.stats.stattools import durbin_watson</span><span id="b01f" class="my kz it mu b gy ne na l nb nc">durbin_watson(results.resid)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/a96ac1849d5b89a2fd1215f84481f6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*1CP_zdAaGRmG4FfmkJf6Pg.png"/></div></div></figure><p id="292b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">同方差:</strong>假设回归模型的残差具有相同的可变性或沿回归线分布。如果他们不这样做，这被称为“异方差”。一个被打破的同方差假设会使你的系数不太准确，但不会增加系数的偏差。</p><p id="8ee3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在被破坏的同方差假设中，散点图将显示数据点的模式。如果你碰巧在散点图上看到一个<strong class="ls iu">漏斗形状</strong>，这将表明一个错误的假设。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/9c51e3500a7d6fc9f8c05f924ae7358e.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*IJ9JzxDeExxYZWz5gwk-qA.png"/></div></figure><p id="d7f9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">再一次，变换是你最好的朋友来纠正一个失败的同质性假设。首先，尝试改变你的目标(即。y)使用平方根、对数、平方根倒数或倒数变换。我们也可以使用加权普通最小二乘法代替普通最小二乘法来拟合你的数据，当你已经打破了同方差假设。</p><p id="a2e5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们通常使用散点图来检查残差的方差在整个目标上是否恒定。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="f72b" class="my kz it mu b gy mz na l nb nc">def homoscedasticity_assum(model):<br/>    y_pred = model.predict() <br/>    residuals = model.resid<br/>    <br/>    plt.figure(figsize=(15,10))<br/>    sns.regplot(x=y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})<br/>    plt.title('Residuals vs Fitted', fontsize=16)<br/>    plt.xlabel('Fitted Values')<br/>    plt.ylabel('Residuals')</span><span id="36ca" class="my kz it mu b gy ne na l nb nc">homoscedasticity_assum(results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/1b0cd2ccca1201c8aa68a77ad577f0ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EoNje5XOmjdd4MqmE1Jf9Q.png"/></div></div></figure><p id="3a5e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">残差的正态性:</strong>残差服从正态或高斯分布。幸运的是，人们普遍认为违反这一假设对结果的影响最小。就我个人而言，我尽量确保分布“接近正态”。我这样说是因为中心极限定理表明，随着样本量的增加，样本的均值和标准差将开始接近正态分布。反之亦然，因为样本量较小的数据集很少具有高斯分布，因此，您的残差将不会具有正态分布。</p><p id="1182" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在这种情况下，对目标进行对数或平方根变换有助于使残差更加正常。</p><p id="9e86" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">也就是说，简单地使用直方图绘制残差将有助于您可视化它们的分布。你也可以使用夏皮罗-维尔克统计。在夏皮罗-维尔克检验中，零假设假设正态分布，因此我们不想拒绝零假设。换句话说，夏皮罗-维尔克检验将产生一个p值，我们希望看到p值高于0.05(即。我们不能拒绝零假设)。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="1a14" class="my kz it mu b gy mz na l nb nc">def normality_assum(model):<br/>    residuals = model.resid<br/>    <br/>    plt.figure(figsize=(15,10))<br/>    sns.distplot(residuals)<br/>    plt.title('Residuals Distribution', fontsize=16)</span><span id="9816" class="my kz it mu b gy ne na l nb nc">normality_assum(results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/175c1c939852e792ed74b4f595aace96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_PSabAPYmtDA55PwvrX7Cg.png"/></div></div></figure><p id="8439" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">多重共线性:</strong>当模型中的要素彼此高度相关时，会出现多重共线性。当相关性达到或超过0.80时，我们通常说多重共线性发生。违反这一假设会导致回归系数不可靠。</p><p id="44ac" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">检查数据多重共线性的主要方法是通过相关矩阵。我们也可以使用“方差通货膨胀系数(VIF)”。高于10的VIF分数表示可以观察到多重共线性，但是100或更高的分数表示明确的多重共线性。</p><p id="d3ff" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">处理这个问题最简单的方法是删除这些功能。我们可以移除具有高VIF的特征，或者当比较两个高度相关的特征之间的相关性时，我们移除与目标具有较小相关性的那个。我们还可以进行主成分分析，因为每个成分都是正交的(即与其他组件不相关)。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="fb6e" class="my kz it mu b gy mz na l nb nc">sns.set_style('whitegrid')<br/>correlation = abs(X_train_df.corr())<br/>mask = np.zeros_like(correlation, dtype=np.bool)<br/>mask[np.triu_indices_from(mask)] = True<br/>plt.figure(figsize=(70,40))<br/>plt.xticks(fontsize=20)</span><span id="814b" class="my kz it mu b gy ne na l nb nc">plt.yticks(fontsize=20)<br/>sns.heatmap(correlation, <br/>            cmap='coolwarm', <br/>            annot=True, <br/>            fmt=".2f", <br/>            annot_kws={"size": 15}, <br/>            linewidths=2, <br/>            vmin=-1.5, <br/>            mask=mask,<br/>           center=0)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/5808c0dc77e89de88702f269b334d0ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mR9ud0I-aAUdnvSqDtclmA.png"/></div></div></figure><p id="3b89" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如您所见，当要素数量较少时，相关矩阵会很有用。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="c8e5" class="my kz it mu b gy mz na l nb nc">upper = correlation.where(abs(np.triu(np.ones(correlation.shape), k=1).astype(np.bool)))<br/>to_drop = [var for var in upper.columns if any(upper[var] &gt; .80)]<br/>to_drop</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/fe5b053d18da6a9f9e5127e59f8fb6a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*Z-FKPtoRblC3NGkB8eJBeA.png"/></div></div></figure><p id="ccdb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们可以过滤相关性矩阵以获得高度相关的特征，但一定要检查相关对，并查看每对中的哪个特征与目标的相关性最高。与目标高度相关的特征被保留，而另一个被丢弃。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="6887" class="my kz it mu b gy mz na l nb nc">from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF</span><span id="0457" class="my kz it mu b gy ne na l nb nc">vif = pd.DataFrame()<br/>vif['VIF'] = [VIF(X_train_df.values, i) for i in range(X_train_df.shape[1])]<br/>vif['Features'] = X_train_df.columns</span><span id="04ac" class="my kz it mu b gy ne na l nb nc">vif_drop = [(feat, num) for num, feat in zip(vif['VIF'], vif['Features']) if num &gt; 5.00]<br/>vif_drop</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8ea46a814f7c3dd9dc3c9dcc20f2463d.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*EkLl1kTRr21P77SOYO4qJw.png"/></div></figure><p id="ee15" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">检查VIF时，首先删除VIF值极高的要素，然后重新计算VIF。由于删除了这些极端要素，您将会看到剩余要素的VIF值有所下降。</p><h1 id="38bb" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">摘要</h1><p id="117c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">通过验证您的线性回归假设，您可以放心，您的结果是由于要素和目标之间的真实关系而不是处理错误。一旦你修正了任何错误的假设，你还会看到你的模型性能的显著提高。</p><p id="cff7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">感谢您的阅读！</p></div></div>    
</body>
</html>