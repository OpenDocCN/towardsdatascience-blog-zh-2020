<html>
<head>
<title>CubeTrack: Deep RL for active tracking with Unity + ML-Agents</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CubeTrack:使用Unity + ML-Agents进行主动跟踪的深度RL</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cubetrack-deep-rl-for-active-tracking-with-unity-ml-agents-6b92d58acb5d?source=collection_archive---------35-----------------------#2020-11-04">https://towardsdatascience.com/cubetrack-deep-rl-for-active-tracking-with-unity-ml-agents-6b92d58acb5d?source=collection_archive---------35-----------------------#2020-11-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/133fd40ad9e962d7a2e18bb21bb65fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dqldWScNFasZAhkZkmcmKQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">游戏视图中的立方体追踪(图片由作者提供)</p></figure><p id="aefc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> CubeTrack </strong>，对我来说，是一次用<strong class="ke ir"> <em class="la">主动对象跟踪</em> </strong> (AOT)和游戏引擎支持的<strong class="ke ir"> <em class="la">深度强化学习</em> </strong> (RL)来弄脏我的手的练习，同时我等待着开发一个更复杂、更逼真的模拟我的真实世界部署环境的工具。到目前为止，我一直在现成的健身房环境中工作，但是对定制环境的需求让我选择了Unity和ML-Agents。</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lc"><img src="../Images/c24c5d1a3532ed08128834052f87346e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5i-YDPxleoJGnVj2iHkNOg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">ML-Agents示例环境(图片来自<a class="ae lb" href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/images/example-envs.png" rel="noopener ugc nofollow" target="_blank">Unity ML-Agents Toolkit Github库</a>)</p></figure><p id="d6af" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">ML-Agents <strong class="ke ir"> </strong>附带的示例环境不包含跟踪问题，因此我遵循创建新环境的指导原则，在平台上保持相同的多维数据集设置。有了这样一个相似、简单的设计，我认为整理这个项目并不需要太多的努力，把我和一些ML-Agents项目预置放在一起的游戏对象交换出去(只是为了风格的一致性)，并使这个项目成为一个公共回购。示例环境对于学习、实践、测试和构建非常有用。希望CubeTrack可以以同样的方式使用，但在不同的问题空间。</p><blockquote class="lh"><p id="03ab" class="li lj iq bd lk ll lm ln lo lp lq kz dk translated">该项目在<a class="ae lb" href="https://github.com/kirstenrichardson/CubeTrack" rel="noopener ugc nofollow" target="_blank">https://github.com/kirstenrichardson/CubeTrack</a>进行</p></blockquote><p id="e183" class="pw-post-body-paragraph kc kd iq ke b kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv lv kx ky kz ij bi translated">如果您只是想使用环境，那么您需要知道的一切都在自述文件中。如果你是RL的新手，我建议你看一下史云光·斯廷布鲁格的Youtube频道上的一些视频，以获得一个总体概述，并进一步了解这里介绍的工作中使用的算法PPO(近似策略优化)。或者，如果你有更多的时间，并且想更深入地研究，我推荐《深度思维》的大卫·西尔弗的《RL简介》系列讲座。</p><figure class="ld le lf lg gt jr"><div class="bz fp l di"><div class="ly lz l"/></div></figure><p id="5041" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你已经熟悉RL，但对如何在Unity中用自定义环境解决自定义问题感兴趣，那么请继续阅读，我将介绍ML-Agents以及CubeTrack是如何组装的。如果你已经熟悉Unity和ML-Agents，只是想知道基于学习的决策促进了成功的跟踪，那么跳到<a class="ae lb" href="https://medium.com/p/6b92d58acb5d#e28a" rel="noopener">设计选择</a>。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><p id="ca18" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">ML-Agents是<strong class="ke ir"> <em class="la">将常规Unity场景转化为学习环境</em> </strong>的插件。它提供了一个Python低级API(包含在Python包<code class="fe mh mi mj mk b">mlagents_envs</code>中),用于处理Unity环境中的通信器和Python训练脚本之间的通信(要么是工具包附带的实现，它们是自己的Python包<code class="fe mh mi mj mk b">mlagents</code>的一部分，要么是您选择的实现，可以选择使用第三个Python包<code class="fe mh mi mj mk b">gym-unity</code>将Unity环境包装在gym包装器中)。在将<a class="ae lb" href="https://github.com/Unity-Technologies/ml-agents" rel="noopener ugc nofollow" target="_blank"> ML-Agents repo </a>克隆到本地目录(设置为<a class="ae lb" href="https://docs.pipenv.org/" rel="noopener ugc nofollow" target="_blank"> pipenv </a>)并<a class="ae lb" href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md" rel="noopener ugc nofollow" target="_blank">安装</a>各种ML-Agents包之后，我打开Unity Hub并在同一位置创建了一个新的3D Unity项目。</p><pre class="ld le lf lg gt ml mk mm mn aw mo bi"><span id="e320" class="mp mq iq mk b gy mr ms l mt mu">Version information:<br/>  ml-agents: 0.18.0,<br/>  ml-agents-envs: 0.18.0,<br/>  Communicator API: 1.0.0,<br/>  TensorFlow: 1.14.0,<br/>  Unity: 2019.4.4f1,<br/>  Python: 3.6.9</span></pre><h1 id="1c4a" class="mv mq iq bd mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr bi translated">统一场景</h1><p id="e87f" class="pw-post-body-paragraph kc kd iq ke b kf ns kh ki kj nt kl km kn nu kp kq kr nv kt ku kv nw kx ky kz ij bi translated">这里没有华而不实。我所需要的是一个非玩家角色(NPC)(不受人类玩家控制的游戏对象)在一个区域内随机移动，提供一个移动的<strong class="ke ir"><em class="la"/></strong>，以及一个游戏对象代表跟踪器(或<strong class="ke ir"> <em class="la">代理</em> </strong>，使用RL术语)响应一些外部“玩家”输入而移动。令人困惑的是，Unity可以将NPC称为人工智能控制的，但人工智能在这里是指预编程的，因此是独立的，而不是像我们的RL模型“玩家”那样主动学习，它位于Unity的外部。</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/2f21a838f3c64a39bbc21d0e6f517a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*ESKICXCSWHQ8gtxcZUyPFQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">层级窗口示例(按作者排序的图像)</p></figure><p id="7783" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> CubeTrack </strong>有两个几乎相同的场景，CubeTrack和VisualCubeTrack，后者在代理立方体上增加了第一人称视角摄像头。除了游戏相机和照明，场景只包含一个名为Area(或VisualArea)的预设——一个包含所有组成RL环境的组件的空游戏对象。这个容器预置使复制环境变得容易，拥有多个环境实例可以加速训练，将我们的代理立方体车队的“共享大脑”暴露给并行的多种体验。该区域可分为三个部分:</p><ul class=""><li id="364e" class="ny nz iq ke b kf kg kj kk kn oa kr ob kv oc kz od oe of og bi translated">训练场:由一系列立方体组成的场地和墙壁。此外，13个圆柱体(航路点)以网格模式放置在地面上，它们的网格渲染器组件和胶囊碰撞器组件未选中，因此它们都是不可见的，并且能够通过。</li><li id="95dd" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated"><strong class="ke ir">目标:</strong>一个紫色的立方体，用更小的子对象立方体作为头带、眼睛和嘴巴。为了让立方体在平台上移动，添加了一个名为NavMesh代理的组件和一个名为<code class="fe mh mi mj mk b">TargetMovement</code>的脚本。一个NavMesh被烤到了地上(区域预置是开放的，因此它存在于每个区域实例中)，每个航路点都被赋予了标签<code class="fe mh mi mj mk b">RandomPoint</code>。然后可以通过脚本从标签为<code class="fe mh mi mj mk b">RandomPoint</code>的对象列表中选择一个随机航路点，并将航路点的位置设置为NavMesh代理的目的地，即目标立方体。更多细节请参见此<a class="ae lb" href="https://www.youtube.com/watch?v=3AoBCabLIxY" rel="noopener ugc nofollow" target="_blank">视频</a>。</li><li id="1959" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated"><strong class="ke ir">代理:</strong>一个蓝色的立方体，有头带、眼睛和嘴巴，这次有一个刚体组件，使它能够在物理的控制下行动，并施加力。你可以使用刚体的质量和阻力参数来改变立方体滑动的方式。当使用视觉观察时，添加了一个相机作为子对象，并定位为指向正面(向下看的那个<code class="fe mh mi mj mk b">transform.forward</code>)。</li></ul><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi om"><img src="../Images/94f981bd85ee1c09729250765bd82b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*beCgZWxBpsWApmwjtoaDmw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">场景视图显示航路点对象和代理摄像机FOV(图片由作者提供)</p></figure><h1 id="3bec" class="mv mq iq bd mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr bi translated">学习环境建立</h1><p id="f9bf" class="pw-post-body-paragraph kc kd iq ke b kf ns kh ki kj nt kl km kn nu kp kq kr nv kt ku kv nw kx ky kz ij bi translated">通过ML-Agents，将这个普通的Unity场景转变为人工智能的训练场所变得超级简单。做这件事的文档是<a class="ae lb" href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md" rel="noopener ugc nofollow" target="_blank">这里是</a>，但是这些是基本步骤..</p><ul class=""><li id="bc95" class="ny nz iq ke b kf kg kj kk kn oa kr ob kv oc kz od oe of og bi translated">安装<strong class="ke ir"> Unity包</strong></li></ul><p id="1d83" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">选择游戏对象作为你的RL代理..</p><ul class=""><li id="3b8b" class="ny nz iq ke b kf kg kj kk kn oa kr ob kv oc kz od oe of og bi translated">添加一个<strong class="ke ir">脚本</strong>组件</li><li id="fa0d" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">添加一个<strong class="ke ir">行为参数</strong>组件</li><li id="b346" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">添加一个<strong class="ke ir">决策请求器</strong>组件</li><li id="f224" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">(可选)添加一个<strong class="ke ir">摄像头传感器</strong>组件</li></ul><p id="8d39" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">转到您的本地ML-Agents repo目录(或本地CubeTrack repo目录)并..</p><ul class=""><li id="30f8" class="ny nz iq ke b kf kg kj kk kn oa kr ob kv oc kz od oe of og bi translated">添加一个<strong class="ke ir">配置文件</strong></li></ul><p id="db95" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">所有这些都准备好了，你的Unity场景就可以训练一个模型了。要开始训练，请确保代理多维数据集的行为参数组件下的模型字段显示无。打开终端并使用单一命令行实用程序…</p><pre class="ld le lf lg gt ml mk mm mn aw mo bi"><span id="8994" class="mp mq iq mk b gy mr ms l mt mu">mlagents-learn ./config/ppo/CubeTrack.yaml --run-id=runName</span></pre><p id="1926" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">…带有结尾标记的任何选项(使用-h探索mlagents-learn命令的完整用法)。然后转到Unity编辑器，点击▶️.提供了很大的灵活性，包括恢复尚未结束的训练运行(全局max_steps参数)或在现有模型的基础上进行训练。更多详情见<a class="ae lb" href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-ML-Agents.md" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="4548" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当训练运行完成或提前终止时，模型权重被保存到。“结果”文件夹中的nn(神经网络)文件。mlagents-learn命令需要从与您正在使用的配置和结果文件夹相同的位置调用。如果您已经克隆了ML-Agents repo，那么您可以将您的配置文件与示例环境的所有yaml文件放在一起，并从ML-Agents repo目录的<strong class="ke ir">根目录运行命令。</strong>cube track repo有自己的配置和结果文件夹，因此可以在没有ML-Agents repo本地副本的情况下使用。在这种情况下，从CubeTrack repo目录的<strong class="ke ir">根目录发出命令。</strong></p><p id="9ff1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">推断非常简单，只需复制并粘贴。nn文件到您的项目资产文件夹，拖动到模型领域，并点击▶️.</p><h1 id="e28a" class="mv mq iq bd mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr bi translated">设计选择</h1><h2 id="4e25" class="mp mq iq bd mw on oo dn na op oq dp ne kn or os ni kr ot ou nm kv ov ow nq ox bi translated">观察</h2><p id="c57a" class="pw-post-body-paragraph kc kd iq ke b kf ns kh ki kj nt kl km kn nu kp kq kr nv kt ku kv nw kx ky kz ij bi translated">首先，我试图用向量观察来培养追踪行为。被选作观察集的变量组在不同的实验运行中有所不同，但最终成为以下12个观察集:</p><ul class=""><li id="5128" class="ny nz iq ke b kf kg kj kk kn oa kr ob kv oc kz od oe of og bi translated">代理的位置(2个obs，对象变换位置的<em class="la"> x </em>和<em class="la"> z </em>组件)</li><li id="cae9" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">目标的位置</li><li id="6079" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">代理的速度(<code class="fe mh mi mj mk b">velocity.magnitude</code>)</li><li id="c5a8" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">目标的速度</li><li id="036f" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">代理的面向方向(<code class="fe mh mi mj mk b">transform.forward</code>)(矢量so 3 obs)</li><li id="14b3" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">目标面对的方向</li></ul><p id="e7eb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">一旦以这种方式实现了跟踪，就添加了一个相机传感器，删除了<code class="fe mh mi mj mk b">CollectObservations()</code>功能，并将检查器窗口中的空间大小设置为0。相机传感器设置为传送84 x 84 PNG型号的图像。下面的GIF演示了代理的POV(注意，这是在调整图像大小之前——模型从同一台相机接收图像，但分辨率较低)。喊出身临其境的极限<a class="ae lb" href="https://www.immersivelimit.com/tutorials/visual-chameleons" rel="noopener ugc nofollow" target="_blank">视觉变色龙</a>教程由<a class="lw lx ep" href="https://medium.com/u/9d96550e35f4?source=post_page-----6b92d58acb5d--------------------------------" rel="noopener" target="_blank">亚当凯利</a>在这里。</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f2f6d2350db37e0032faabf5c3491d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*ebd5YcMpFCJXQrmmdxoQfA.gif"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">从代理的机载摄像头查看-游戏视图显示2(图片由作者提供)</p></figure><p id="2762" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在我整理项目和利用ML-Agents示例环境中使用的游戏对象之前，我已经通过选择具有最高对比度的材质颜色(黑色目标立方体、浅灰色地面和白色墙壁)、关闭阴影和关闭阴影投射，给了我的代理成功进行视觉观察的最佳机会。在此设置中，足以传递模型灰度84 x 84图像(相机传感器组件下的勾选框)，但在VisualCubeTrack的较新版本中，需要彩色图像。</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oz"><img src="../Images/7bf2b2ed82236429daa07e2ed99e8b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S1DjNZsuX11EEAkwMpsymw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">(左)最新版本的CubeTrack(右)早期版本的CubeTrack(图片由作者提供)</p></figure><h2 id="d5f4" class="mp mq iq bd mw on oo dn na op oq dp ne kn or os ni kr ot ou nm kv ov ow nq ox bi translated">行动</h2><p id="be55" class="pw-post-body-paragraph kc kd iq ke b kf ns kh ki kj nt kl km kn nu kp kq kr nv kt ku kv nw kx ky kz ij bi translated">最初，动作空间被设置为连续的。该模型传入了两个浮点值，这两个值在用作同时沿<em class="la"> x </em>和<em class="la"> z </em>轴施加的力的大小之前，都使用<code class="fe mh mi mj mk b">Mathf.Clamp</code>在-1和1之间进行了限幅。我后来决定了一个简单的5选离散行动空间..</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pa"><img src="../Images/0f98b7541bf1bb294eff29ec0aaa09c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCjHyDXF-SUoYa-uzuZbUQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">行动空间(图片由作者提供，灵感来自<a class="lw lx ep" href="https://medium.com/u/5178b198735a?source=post_page-----6b92d58acb5d--------------------------------" rel="noopener" target="_blank">托马斯·西蒙尼尼</a>的<a class="ae lb" rel="noopener" target="_blank" href="/an-introduction-to-unity-ml-agents-6238452fcf4c">Unity ML-Agents简介</a>)</p></figure><p id="0318" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">通过将每个动作选项分配给一个箭头键，并对看起来合理的选项进行实验，来测试每种情况下施加的力的大小。前进/后退的幅度为500，转弯的幅度为250。</p><p id="5b82" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我遇到的一个问题可以被描述为代理持续“超调”，导致旋转行为或钟摆般的向前然后向后移动。我意识到我将决策请求者的决策周期设置为10，但勾选了“在决策之间采取行动”，导致连续十步采取相同的行动。将决策周期更改为1并没有使sim不堪重负，也没有消除超调。</p><h2 id="20d1" class="mp mq iq bd mw on oo dn na op oq dp ne kn or os ni kr ot ou nm kv ov ow nq ox bi translated">奖励</h2><p id="74e2" class="pw-post-body-paragraph kc kd iq ke b kf ns kh ki kj nt kl km kn nu kp kq kr nv kt ku kv nw kx ky kz ij bi translated">我第一次尝试奖励工程学是受到了下面的博客的启发，这篇博客是由<a class="lw lx ep" href="https://medium.com/u/9e52daf12ce1?source=post_page-----6b92d58acb5d--------------------------------" rel="noopener" target="_blank">亚当·普莱斯</a>写的关于利用课程学习进行追逃(见他的开源追逃游戏<a class="ae lb" href="https://github.com/adamprice97/UnityPursitEvasionGame" rel="noopener ugc nofollow" target="_blank">这里</a>)。</p><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/curriculum-learning-with-unity-ml-agents-c8e7a1aa5415"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd ir gy z fp pj fr fs pk fu fw ip bi translated">使用Unity ML-agent的课程学习</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">教一队猎人捕捉猎物。</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps jw pe"/></div></div></a></div><p id="8fd9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">复制“延伸”行为非常有效。我增加了目标的速度，减少了被认为“足够接近”目标的距离。然后，我尝试在这个模型的基础上进行新的培训，鼓励使用计数器变量进行跟踪，计算目标范围内连续<code class="fe mh mi mj mk b">FixedUpdate()</code>呼叫的数量。目标速度再次增加，这一次“持续时间”阈值(计数器变量需要达到的值)也增加了。第二阶段进展缓慢！一旦剧集终止代码被删除，代理人的最终行为是只跟踪导致奖励的时间长度，然后失去兴趣(显然是回想起来)。</p><p id="8717" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我需要一个函数来分配每一步的比例奖励，我在的一篇论文中看到了一个这样的例子<a class="ae lb" href="https://arxiv.org/abs/1705.10561" rel="noopener ugc nofollow" target="_blank">罗等人(2018) </a>。该功能如下所示:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/4bba7f8bcdb4f64a53a3a25cd720d662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*eCK2NwptNKet1oXlSekTow.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">奖励函数(图片来自<a class="ae lb" href="http://proceedings.mlr.press/v80/luo18a/luo18a.pdf" rel="noopener ugc nofollow" target="_blank">罗等(2018) </a>)</p></figure><ul class=""><li id="bd64" class="ny nz iq ke b kf kg kj kk kn oa kr ob kv oc kz od oe of og bi translated"><em class="la"> A </em>、<em class="la"> d </em>、<em class="la"> c </em>和<em class="la"> λ </em> <strong class="ke ir"> </strong>都是调谐参数。<em class="la"> A </em>和<em class="la"> d </em>纯属设计选择，<em class="la"> A </em>为一步可分配的最大奖励(设置为1，与罗纸相同)<em class="la"> d </em>为最佳前进/后退距离(根据游戏视图中看似合理的差距设置为3)。</li><li id="fe27" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">括号的左边是奖励代理的位置，右边是代理面对的方向。为了生成[-1，1]范围内的奖励，需要设置<em class="la"> c </em>和<em class="la"> λ </em>以使括号中的最大值为2。Unity中的旋转从-180°到180°，因此如果旋转和定位的权重相等，则<em class="la"> λ </em>应该总是1/180。<em class="la"> c </em>的值根据所用训练场地的大小而变化。</li><li id="47d4" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated"><em class="la"> x </em>和<em class="la"> y </em>代表从代理到目标的航向矢量的<em class="la"> x </em>和<em class="la"> z </em>分量。取方向分量总和的平方根，计算矢量的长度，即距离。当代理和目标在<em class="la"> x </em>平面上的位置相同，但<code class="fe mh mi mj mk b">heading.z</code>为+3时，整项为零，奖励最高。我意识到<code class="fe mh mi mj mk b">heading.z</code>的符号根据目标的行进方向而变化，导致代理人有时因在目标后面而被奖励，有时因在目标前面而被奖励(没有帮助！).因此，有一个额外的代码块来确保在目标的后面<strong class="ke ir">发布最高奖励，即使根据世界z轴，代理在技术上在目标的前面。</strong></li></ul><pre class="ld le lf lg gt ml mk mm mn aw mo bi"><span id="8bc2" class="mp mq iq mk b gy mr ms l mt mu">if (Vector3.Dot(heading.normalized, Target.transform.forward) &gt; 0)<br/> {<br/>  heading.z = Mathf.Abs(heading.z);<br/> } else {<br/>  heading.z = -Mathf.Abs(heading.z);<br/> }</span></pre><ul class=""><li id="0a2d" class="ny nz iq ke b kf kg kj kk kn oa kr ob kv oc kz od oe of og bi translated"><em class="la"> a </em>是代理的<code class="fe mh mi mj mk b">transform.forward</code>矢量和航向矢量之间的角度。这鼓励目标去<strong class="ke ir">看</strong>目标。最初我将<em class="la"> a </em>(在下面的引用中)的描述解释为代理的<code class="fe mh mi mj mk b">transform.forward</code>和目标的<code class="fe mh mi mj mk b">transform.forward</code>之间的角度，但是后来的实现(指向的角度)工作得更好，我认为这是有意义的，特别是在视觉设置中。</li></ul><blockquote class="pu pv pw"><p id="a8fb" class="kc kd la ke b kf kg kh ki kj kk kl km px ko kp kq py ks kt ku pz kw kx ky kz ij bi translated">“当物体完美地站在智能体前面距离d处，且<strong class="ke ir">不旋转</strong>时，奖励A最大”—罗等(2018)</p></blockquote><p id="1939" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">下面是该项目的函数的代码实现。</p><pre class="ld le lf lg gt ml mk mm mn aw mo bi"><span id="14a7" class="mp mq iq mk b gy mr ms l mt mu">var rDist = Mathf.Sqrt(Mathf.Pow(heading.x, 2f) + Mathf.Pow((heading.z - d), 2f));        <br/> var r = A - ((rDist/c) + (a*lam));<br/> AddReward(r);.</span></pre><h1 id="2573" class="mv mq iq bd mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr bi translated">结果</h1><p id="3280" class="pw-post-body-paragraph kc kd iq ke b kf ns kh ki kj nt kl km kn nu kp kq kr nv kt ku kv nw kx ky kz ij bi translated">培训在我的联想Thinkpad T480s笔记本电脑(英特尔酷睿i7–8550 u处理器，4核，8线程，1.80 GHz处理器主频)上进行，将<code class="fe mh mi mj mk b">max_steps</code>设置为5M。下图是TensorBoard生成的平滑度为0.99的累积奖励图。</p><p id="a1b8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使用矢量观察(粉色)进行的训练最初需要大约5个小时，但使用六个训练场后减少到大约2个小时(参见立方体轨道场景中的游戏视图显示2)。在这种情况下，奖励值代表六个区域实例的平均奖励。曲线图的坐标轴模糊不清，但累积奖励稳定在2500左右。注意，在一集中可实现的最大奖励是3000(除非该集的累积奖励下降到-450以下，否则该集在3000步处终止)，但是将要求代理的随机开始位置和面向方向是从奖励函数中引出直接+1的位置和面向方向。</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi qa"><img src="../Images/85a3adf7121396fb2323e884b6ba8de2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XqToJL9vaomeBNWU115ZA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">5次不同训练跑的累计奖励超过500万步(图片由作者提供)</p></figure><p id="fb82" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">下面的深蓝色线条说明了VisualCubeTrack的第一个版本中视觉观察跟踪的成功，具有高对比度的颜色，没有阴影或阴影。在较新版本的VisualCubeTrack中使用灰度图像进行训练导致了显著的性能下降(浅橙色)，而仅改变为彩色观察对改善事情(红色)几乎没有作用。在这两种情况下，训练都被缩短了。直到视觉编码器从<code class="fe mh mi mj mk b">simple</code> (2个卷积层)变为<code class="fe mh mi mj mk b">resenet</code> ( <a class="ae lb" href="https://arxiv.org/pdf/1802.01561.pdf" rel="noopener ugc nofollow" target="_blank"> IMPALA ResNet </a> —三个堆叠层，每个层有两个剩余块)，进展才有所改善(浅蓝色)，累积奖励稳定在2300左右，推理呈现合理行为。这次训练包括一个训练场地，持续了大约56个小时。</p><h1 id="71a2" class="mv mq iq bd mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr bi translated">范围</h1><p id="b7b2" class="pw-post-body-paragraph kc kd iq ke b kf ns kh ki kj nt kl km kn nu kp kq kr nv kt ku kv nw kx ky kz ij bi translated">本博客主要讨论的是第一个“设置”(奖励功能，环境设置，训练配置等。等等。)来引出令人信服的跟随行为，仅此而已。受过训练的代理在VisualCubeTrack中的表现尤其有改进的余地。改进的途径包括..</p><ul class=""><li id="94d4" class="ny nz iq ke b kf kg kj kk kn oa kr ob kv oc kz od oe of og bi translated">进一步调整配置文件中的超参数</li><li id="4c74" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">进一步奖励工程，例如增加长期奖励</li><li id="6aae" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">使用不同的算法(ML-Agents提供软演员评论家的实现)或不同的实现(在使用ML-Agents之前，我选择的RL库是来自<a class="lw lx ep" href="https://medium.com/u/2409d35123af?source=post_page-----6b92d58acb5d--------------------------------" rel="noopener" target="_blank"> Antonin RAFFIN </a>和其他人的<a class="ae lb" href="https://stable-baselines.readthedocs.io/en/master/" rel="noopener ugc nofollow" target="_blank">稳定基线</a></li><li id="dd2f" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">使用额外的技术，例如已经提到的课程学习或称为好奇心的内在奖励信号</li><li id="2249" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">例如，使用域随机化使学习到的策略更加健壮</li><li id="1a00" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">通过在多个环境实例上进行训练或使用可执行文件而不是在编辑器中进行训练来提高训练效率</li></ul><p id="b8e2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">除了在当前版本的环境中提高性能之外，具有深度RL的AOT还可以通过以下方式进一步提高..</p><ul class=""><li id="429c" class="ny nz iq ke b kf kg kj kk kn oa kr ob kv oc kz od oe of og bi translated">增加学习问题的复杂性，例如引入与目标或增加的障碍碰撞的惩罚</li><li id="4b6e" class="ny nz iq ke b kf oh kj oi kn oj kr ok kv ol kz od oe of og bi translated">使用更加复杂和真实的sim卡！！</li></ul><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/4fbcedd2e93d34fc747d2e62d88a3758.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*Yc5QgWT7sArID3Dvs3QZCQ.gif"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">CubeTrack.nn上的推论(图片由作者提供)</p></figure><p id="355d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你发现任何疏忽或有任何想法或问题等。那么，请在下面发表评论或在@ KN _ Richardson给我发推文。注意安全！</p><p id="972c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[1]:朱利安尼，a .，伯格斯，v .，滕，e .，科恩，a .，哈珀，j .，埃利翁，c .，戈伊，c .，高，y .，亨利，h .，马塔，m .，兰格，D. (2020)。Unity:智能代理的通用平台。<em class="la"> arXiv预印本</em><a class="ae lb" href="https://arxiv.org/abs/1809.02627" rel="noopener ugc nofollow" target="_blank">T3】arXiv:1809.02627</a><em class="la">。</em><a class="ae lb" href="https://github.com/Unity-Technologies/ml-agents" rel="noopener ugc nofollow" target="_blank">https://github.com/Unity-Technologies/ml-agents</a>。</p><p id="96eb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[2]:罗文伟、孙平、钟、冯芳、刘文伟、张、汤、王(2018年7月)。基于强化学习的端到端主动目标跟踪。在<em class="la">机器学习国际会议</em>(第3286–3295页)。</p><p id="9056" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[3]: A .希尔，a .拉芬，m .埃内斯托斯，a .格莱夫，a .卡内维斯托，r .特拉奥雷，p .达里瓦尔，c .黑塞，o .克里莫夫，a .尼科尔，m .普拉波特，a .拉德福德，j .舒尔曼，s .西多尔，y .吴，《稳定的基线》，<a class="ae lb" href="https://github.com/hill-a/stable-baselines" rel="noopener ugc nofollow" target="_blank">，</a>，2018。</p></div></div>    
</body>
</html>