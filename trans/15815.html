<html>
<head>
<title>Distilling the Knowledge in a Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">提取神经网络中的知识</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distilling-knowledge-in-neural-network-d8991faa2cdc?source=collection_archive---------7-----------------------#2020-10-31">https://towardsdatascience.com/distilling-knowledge-in-neural-network-d8991faa2cdc?source=collection_archive---------7-----------------------#2020-10-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e07a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学习用较小的模型学得更好</h2></div><p id="2929" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在传统的机器学习中，为了达到最先进的(SOTA)性能，我们经常训练一系列集成模型来克服单个模型的弱点。然而，实现SOTA性能通常伴随着使用具有数百万个参数的大模型的大计算。像VGG16/19、ResNet50这样的SOTA模型分别具有1.38亿和2，300万以上的参数。在边缘部署这些模型是不可行的。</p><p id="59a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">智能手机和物联网传感器等边缘设备是资源受限的设备，不可能在不影响设备性能的情况下执行训练或实时推理。因此，研究的重点是将大型模型压缩成小型紧凑的模型，在边缘部署时性能损失最小甚至为零。</p><p id="a64e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是一些可用的模型压缩技术，但不限于此</p><ul class=""><li id="4518" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">剪枝和量化</li><li id="bf45" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">低秩因子分解</li><li id="87c7" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">神经结构搜索(NAS)</li><li id="43c6" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">知识的升华</li></ul><p id="ad96" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，重点将放在[1]提出的知识提炼上，参考文献链接[2]提供了上面列出的模型压缩技术的一个很好的概述。</p><h1 id="6953" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">知识的升华</h1><p id="0fd5" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">知识提取是使用从<strong class="kh ir">大型模型</strong>或<strong class="kh ir">模型集合</strong>中推断出的提取知识来训练一个紧凑的神经网络。使用提取的知识，我们能够有效地训练小而紧凑的模型，而不会严重损害紧凑模型的性能。</p><h2 id="279f" class="mm lq iq bd lr mn mo dn lv mp mq dp lz ko mr ms mb ks mt mu md kw mv mw mf mx bi translated">大型和小型模型</h2><p id="63bd" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">我们称<strong class="kh ir">大型模特</strong>或<strong class="kh ir">模特组合</strong>为<strong class="kh ir">笨重模特</strong>或<strong class="kh ir">教师</strong>网络，而<strong class="kh ir">小型模特</strong>和紧凑型模特<strong class="kh ir">学生</strong>网络。</p><p id="43c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个简单的类比是，一个大脑小而紧凑的学生在准备考试时，试图从老师那里吸收尽可能多的信息。然而老师只是教所有的东西，不知道考试中会出现哪些问题的学生尽力吸收所有的东西。</p><p id="c749" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是压缩发生的地方，从老师那里提取知识给学生。</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c2b9de26cff9b90db2806d56aa45ffa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*CifI2LGhV3eEAo6y1z94pg.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">图一。教师模型将知识提炼为学生模型</p></figure><p id="9c84" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在提取知识之前，笨重的模型或教师网络应该达到SOTA性能，这种模型通常由于其记忆数据的能力而过度拟合。尽管过度拟合，这个笨重的模型也应该能很好地推广到新的数据。繁琐模型的目标是最大化正确类别的平均对数概率。更有可能正确的类将被分配高概率分数，而不正确的类将被分配低概率分数。</p><p id="f44a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的例子显示了当对给定的“鹿”图像执行推理时，softmax图2后的结果。为了得到预测，我们取最大类概率分数的argmax，这将给我们60%的正确机会。</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/cbbd28529a33e89b46412a3d5359d145.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*W5ajoMhTLiFTwl7TRMxB3g.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">图二。“马”看起来像“鹿”(Softmax预测)</p></figure><p id="af7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，给定上面的图2。(为了便于说明)，我们知道与“船”相比，“马”与“鹿”非常相似。因此，在推理过程中，我们有60%的正确率和39%的错误率。由于“鹿”和“马”之间有一些空间相似性，我们不能责怪网络预测“马”。如果网络被提供了诸如<strong class="kh ir">“我认为这个图像有60%的几率是鹿，39%的几率是马”</strong>的信息，例如【鹿:0.6，马:0.39，船:0.01】，那么网络被提供了更多的信息(高熵)。使用类别概率作为目标类别提供了比仅仅使用原始目标更多的信息。</p><h2 id="8373" class="mm lq iq bd lr mn mo dn lv mp mq dp lz ko mr ms mb ks mt mu md kw mv mw mf mx bi translated">蒸馏</h2><p id="5350" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">教师向学生提炼预测类概率的知识，作为“S <strong class="kh ir">经常的目标</strong>”。这些数据集也被称为“<strong class="kh ir">转移集”</strong>，其目标是教师提供的类别概率，如图2所示。蒸馏过程通过将超参数<strong class="kh ir"> T </strong>(温度)引入softmax函数来进行，使得教师模型可以为学生模型产生转移集的合适的软目标集。</p><blockquote class="nl nm nn"><p id="d74e" class="kf kg no kh b ki kj jr kk kl km ju kn np kp kq kr nq kt ku kv nr kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">软目标</em> </strong> <em class="iq">善于帮助模型泛化，可以作为防止模型过于自信的正则化子。</em></p></blockquote><figure class="mz na nb nc gt nd gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c862f69df9a4300a5367a863b87f3416.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*TQkWEfX9sQy6qmK3YNF3lw.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">Eq 1。Softmax随温度变化(<strong class="bd nt"> T=1 </strong>只是softmax，随着<strong class="bd nt"> T </strong>的增加，概率的等级分布将变得更柔和)。</p></figure><h1 id="3e59" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">培训教师和学生模型</h1><p id="95de" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">首先训练繁琐的/教师模型，因为我们需要繁琐的模型很好地概括新数据。在提取过程中，学生目标函数是两个不同目标函数<strong class="kh ir"> Loss1 </strong>和<strong class="kh ir"> Loss2 </strong>的加权平均值。</p><h2 id="4635" class="mm lq iq bd lr mn mo dn lv mp mq dp lz ko mr ms mb ks mt mu md kw mv mw mf mx bi translated"><strong class="ak"> Loss1(软目标的交叉熵损失)</strong></h2><p id="286d" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">Loss1是教师<strong class="kh ir"> q </strong>和学生<strong class="kh ir"> p </strong>与温度<strong class="kh ir"> T &gt; 1 </strong>的两个温度softmax的交叉熵(CE)损失乘以权重参数<strong class="kh ir">α</strong>。</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/eb4345070745548edc292605584d9c28.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*cRatpyr669GPOGPu_cPSwA.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">Eq 2。损失函数1</p></figure><h2 id="2f0d" class="mm lq iq bd lr mn mo dn lv mp mq dp lz ko mr ms mb ks mt mu md kw mv mw mf mx bi translated"><strong class="ak"> <em class="nv"> Loss2(硬目标交叉熵损失)</em> </strong></h2><p id="f838" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">Loss2是正确标签和学生硬目标在<strong class="kh ir"> T = 1 </strong>时的交叉熵(CE)损失。Loss2对学生模型做出的硬目标(<strong class="kh ir"> student_pred </strong>)的【T2(1-alpha)】关注不多，以匹配教师模型的软目标<strong class="kh ir"> q </strong>。</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/bc48f8ef2a66aaf3098d92e7a70e6d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*0PKqnNcvA64GnQLVGPPUfA.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">Eq 3。损失函数2</p></figure><p id="2b02" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">学生模型的目标是蒸馏损失，即<strong class="kh ir">损失1 </strong>和<strong class="kh ir">损失2 </strong>之和。</p><p id="f354" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，学生模型在<strong class="kh ir">传输装置</strong>上接受训练，以使其蒸馏损失最小化。</p><h1 id="52a6" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">结果</h1><h2 id="e634" class="mm lq iq bd lr mn mo dn lv mp mq dp lz ko mr ms mb ks mt mu md kw mv mw mf mx bi translated">MNIST实验</h2><p id="0dbd" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">如下表1。是来自论文[1]的结果，该论文显示了使用在具有60，000个训练案例的MNIST数据集上训练的教师、学生和精选模型的性能。所有模型是两层神经网络，分别具有1200、800和800个神经元，用于教师、学生和提取模型。当使用与学生模型相比的提取模型时，在温度设置为20的情况下，教师和提取模型之间的测试误差是可比较的。然而，当只使用学生模型和硬目标时，它的泛化能力很差。</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi nw"><img src="../Images/891d0863db71b0e01f21de004b7d26c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qx1lufJu599rd_o4KlyhEw.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">表1。MNIST的初步实验[1]</p></figure><h2 id="a85a" class="mm lq iq bd lr mn mo dn lv mp mq dp lz ko mr ms mb ks mt mu md kw mv mw mf mx bi translated">语音识别实验</h2><p id="7671" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">如下表2。是文[1]的另一个结果。繁琐模型是由85M参数组成的语音模型，该模型是在2000小时的英语口语数据上用大约700M训练样本训练的。表2中的第一行。是在100%的训练示例上训练的基线(繁琐)模型，其产生58.9%的准确度。第二行仅用3%的训练样本进行训练，这导致了严重的过拟合，最后，第三行是用同样3%的训练样本训练的相同语音模型，其具有仅使用3%的训练数据就达到57%准确度的软目标。</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/bdb87680380ad3ffc31f3645269bee2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*zB254cyOfuDlJSZaPbLkCw.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">表2:软目标允许一个新模型仅从3%的训练集中进行很好的概括。软目标是通过在全训练集上训练获得的。[1]</p></figure><h1 id="91bd" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated"><strong class="ak">结论</strong></h1><p id="4146" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">知识提炼是一种将计算带到边缘设备的模型压缩技术。目标是有一个小而紧凑的模型来模仿笨重模型的性能。这是通过使用软目标来实现的，软目标充当正则化器，以允许小而紧凑的学生模型进行归纳，并从教师模型中恢复几乎所有的信息。</p><p id="d050" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据Statista [3]的数据，到2025年，已安装的物联网(IoT)连接设备总数预计将达到<strong class="kh ir">215亿</strong>。随着边缘设备数量的增加，为边缘设备带来计算能力对于让边缘设备变得更加智能来说是一个不断增长的挑战。知识提炼允许我们在不影响边缘设备性能的情况下执行模型压缩。</p><h1 id="bf05" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated"><strong class="ak">参考文献</strong></h1><p id="6a4b" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">【1】<a class="ae oc" href="https://arxiv.org/pdf/1503.02531.pdf" rel="noopener ugc nofollow" target="_blank">辛顿、杰弗里、奥里奥尔·维尼亚尔斯和杰夫·迪恩。"从神经网络中提取知识."arXiv预印本arXiv:1503.02531 (2015)。</a></p><p id="b57f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] <a class="ae oc" href="https://medium.com/gsi-technology/an-overview-of-model-compression-techniques-for-deep-learning-in-space-3fd8d4ce84e5" rel="noopener">空间深度学习的模型压缩技术综述</a></p><p id="4214" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] <a class="ae oc" href="https://www.statista.com/statistics/1101442/iot-number-of-connected-devices-worldwide/" rel="noopener ugc nofollow" target="_blank">全球联网设备数量</a></p></div></div>    
</body>
</html>