<html>
<head>
<title>Adversarial Examples in Deep Learning — A Primer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的对立例子——初级读本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adversarial-examples-in-deep-learning-a-primer-feae6153d89?source=collection_archive---------22-----------------------#2020-11-05">https://towardsdatascience.com/adversarial-examples-in-deep-learning-a-primer-feae6153d89?source=collection_archive---------22-----------------------#2020-11-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c4bd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在深度学习视觉模型中引入对立的例子</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d669fef8f2ed0aa010a19423fa14903b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCmsnioukUFf-151_ziRBQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://pixabay.com/photos/structure-blue-rust-texture-3215624" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><h1 id="aabd" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="7c7e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">自从我们开始获得更大更好的计算(GPU和TPU)、更多的数据(ImageNet等)以来，我们已经看到了最先进的(SOTA)计算机视觉深度学习模型的出现。)以及易于使用的开源软件和工具(TensorFlow和PyTorch)。每年(现在每隔几个月！)我们看到下一个SOTA深度学习模型在基准数据集的Top-k准确性方面击败了上一个模型。下图描绘了一些最新的SOTA深度学习视觉模型(并没有描绘一些像谷歌的BigTransfer！).</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/89993bae56a9455a792eb0733c4cd601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*R_mA26nqowS5YdZP.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">https://arxiv.org/abs/1905.11946 SOTA深度学习视觉模型(来源:<a class="ae kv" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank"/>)</p></figure><p id="89ab" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">然而，当这些SOTA深度学习模型试图对一类特定的图像(称为敌对图像)进行预测时，它们中的大多数都陷入困境。对立例子的整体概念可以是自然例子，也可以是合成例子。我们将通过本文中的几个例子来熟悉不同的对抗性例子和攻击。</p><h1 id="3729" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">对立的例子</h1><p id="4ff7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">自然对立的例子是模型难以理解的自然的、有机的图像。一个合成的对抗性示例是，攻击者(恶意用户)故意将一些噪声注入到图像中，该图像在视觉上与原始图像非常相似，但该模型最终做出了非常不同(并且错误)的预测。让我们更详细地看看其中的几个！</p><h1 id="acfc" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">自然对立的例子</h1><p id="b536" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这些例子，如论文<a class="ae kv" href="https://arxiv.org/abs/1907.07174" rel="noopener ugc nofollow" target="_blank"> <em class="mq">【亨德里克斯等人的自然对抗例子】</em> </a>中所定义的，是<em class="mq">现实世界中的、未修改的、自然发生的例子，导致分类器精度显著降低</em>。他们引入了两个新的自然对立例子的数据集。第一个数据集包含7，500个ImageNet分类器的自然对立示例，用作硬ImageNet分类器测试集，称为IMAGENET-A。下图显示了ImageNet-A数据集中的一些对立示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/291df9456309db9e73227eee6893435a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b17eCCaO1mp6q1XQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ResNet-50在ImageNet-A的例子上失败得很惨(来源:<a class="ae kv" href="https://arxiv.org/abs/1907.07174)" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1907.07174)</a></p></figure><p id="bc35" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">你可以清楚地看到多么错误(和愚蠢！)是最新型(SOTA) ResNet-50模型对上述示例的预测。事实上，DenseNet-121预训练模型在ImageNet-A上仅获得2%的准确度！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/cc047588cbf8aec94a8be13e12f2acf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*D43KUyo_bOAbMTWb.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://github.com/dipanjanS/adversarial-learning-robustness" rel="noopener ugc nofollow" target="_blank">https://github . com/dipanjanS/adversarial-learning-robustness</a></p></figure><p id="2c5f" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">作者还策划了一个名为IMAGENET-O的对抗性分布外检测数据集，他们声称这是第一个为IMAGENET模型创建的分布外检测数据集。下图显示了对ImageNet-O数据集中的图像进行ResNet-50推理的一些有趣示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/2a817b3b7928f0e90c98fff2adc8c016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6ORyf0d4qHTz0w7y.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ResNet-50在ImageNet-O的例子上非常失败(来源:【https://arxiv.org/abs/1907.07174】T2)</p></figure><p id="cfb9" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">这些例子确实很有趣，并展示了SOTA预训练视觉模型在这些图像中的一些图像上的局限性，这些图像对于这些模型来说解释起来更复杂。失败的一些原因可以归因于深度学习模型在对特定图像进行预测时试图关注的内容。让我们看更多的例子来理解这一点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/f29b495094e2b92dfc3a6d36f0bef618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_lbTNnyHyzArGUHc.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ImageNet-A中的自然对抗性例子(来源:【https://arxiv.org/abs/1907.07174】T4)</p></figure><p id="6f2c" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">根据上图中展示的例子，很明显，深度学习视觉模型做出了一些特定的模式错误解释。例如:</p><ul class=""><li id="bb3e" class="mu mv iq lq b lr ml lu mm lx mw mb mx mf my mj mz na nb nc bi translated">蜡烛被预测为南瓜灯，尽管没有南瓜，因为模型更关注火焰及其照明等方面</li><li id="91d5" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj mz na nb nc bi translated">由于模型更注重颜色和纹理，蜻蜓被预测为臭鼬或香蕉</li><li id="9635" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj mz na nb nc bi translated">蘑菇被归类为钉子，因为模型学会了将某些元素联系在一起，例如木钉</li><li id="d806" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj mz na nb nc bi translated">模型最终也会遭遇泛化问题，比如日晷的阴影</li></ul><p id="be00" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">如下图所示，SOTA深度学习视觉模型在这些示例中的整体性能非常差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/4c47f256d757219679c4f51b002df424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*K6VFfkX-xq8lrPbI.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SOTA深度学习视觉模型在ImageNet-A上的表现(来源:<a class="ae kv" href="https://arxiv.org/abs/1907.07174)" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1907.07174)</a></p></figure><p id="20d0" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">可悲的是，强大的对抗训练方法几乎无助于处理与错误解释自然对抗示例相关的问题，如Hendrycks等人在同一篇论文中提到的。其中一些方法包括针对特定合成攻击的训练，如投影梯度下降(PGD)和快速梯度符号方法(FGSM)，我们将在后续文章中更详细地讨论。幸运的是，这些方法对于处理恶意合成攻击非常有效，而恶意合成攻击通常是一个更大的问题。</p><h1 id="5100" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">合成对立例子</h1><p id="9f9a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这些例子基本上涉及在输入图像中人工引入一些噪声，使得它在视觉上仍然保持与原始图像非常相似，但是注入的噪声最终降低了分类器的精度。虽然有各种各样的合成对抗性攻击，但所有这些攻击都遵循一些核心原则，如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/518d45419b80e7313a824b6508d6a45d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*26J3IiePR-mLW1bF.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://github.com/dipanjanS/adversarial-learning-robustness" rel="noopener ugc nofollow" target="_blank">https://github . com/dipanjanS/adversarial-learning-robustness</a></p></figure><p id="a784" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">重点始终是找出一种方法来完善噪声\扰动张量(值的矩阵)，它可以叠加在原始图像的顶部，使得这些扰动对人眼不可见，但最终使深度学习模型无法做出正确的预测。上面描述的示例展示了一种快速梯度符号方法(FGSM)攻击，其中我们在输入图像的梯度符号中添加了一个小乘数，并叠加了一幅熊猫图像，使模型无法预测图像是一只长臂猿。下图展示了一些更常见的对抗性攻击类型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/5433b9ee670bf30da7210ae570633c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yi1BL2H_HhyMs4Qd.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://github.com/dipanjanS/adversarial-learning-robustness" rel="noopener ugc nofollow" target="_blank">https://github . com/dipanjanS/adversarial-learning-robustness</a></p></figure><h1 id="55cf" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">下一步是什么？</h1><p id="d93f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在接下来的几篇文章中，我们将讨论上述每一种对抗性攻击方法，并通过实际操作的代码示例展示如何欺骗最新和最好的SOTA视觉模型。敬请期待！</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><p id="0327" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">这篇文章的内容改编自<a class="ae kv" href="https://blog.djsarkar.ai/adversarial-learning-attacks-1" rel="noopener ugc nofollow" target="_blank"> <em class="mq">我最近的博客文章</em> </a>关于由<a class="ae kv" href="https://www.linkedin.com/in/dipanzan/" rel="noopener ugc nofollow" target="_blank">本人</a>和<a class="ae kv" href="https://in.linkedin.com/in/sayak-paul" rel="noopener ugc nofollow" target="_blank"> Sayak </a>完成的对抗性学习，你可以在<a class="ae kv" href="https://github.com/dipanjanS/adversarial-learning-robustness" rel="noopener ugc nofollow" target="_blank">这个GitHub库</a>中找到详细的例子。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><blockquote class="ns nt nu"><p id="a4f9" class="lo lp mq lq b lr ml jr lt lu mm ju lw nv mn lz ma nw mo md me nx mp mh mi mj ij bi translated">喜欢这篇文章吗？请<a class="ae kv" href="https://www.linkedin.com/in/dipanzan/" rel="noopener ugc nofollow" target="_blank">联系我</a>进行更多讨论或提供反馈！</p></blockquote><div class="ny nz gp gr oa ob"><a href="https://www.linkedin.com/in/dipanzan/" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">Dipanjan Sarkar -数据科学领导-应用材料| LinkedIn</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">我是一名数据科学家，领导多个垂直领域的ML\DL\CV\NLP工作。*专注机器学习，深度…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">www.linkedin.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op kp ob"/></div></div></a></div></div></div>    
</body>
</html>