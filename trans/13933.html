<html>
<head>
<title>Logistic Regression for Binary Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">二元分类的逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-for-binary-classification-56a2402e62e6?source=collection_archive---------12-----------------------#2020-09-25">https://towardsdatascience.com/logistic-regression-for-binary-classification-56a2402e62e6?source=collection_archive---------12-----------------------#2020-09-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fe9e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">机器学习中的监督学习方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1f7a3719412f4fb5950084985760e7fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HW8Bqd5UCYWnYacCRVARaQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:Sigmoid.jpg" rel="noopener ugc nofollow" target="_blank"> wikicommons </a></p></figure><h1 id="dbe2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">二元分类</h1><p id="3280" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在之前的文章中，我谈到了深度学习和用于预测结果的函数。在本文中，我们将使用逻辑回归来执行二元分类。二进制分类之所以这样命名，是因为它将数据分为两种结果。简单来说，结果将是“是”(1)或“否”(0)。为了确定结果是“是”还是“否”，我们将使用一个概率函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/05129f0a469db13b5199d92de33630a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*CtZdBwQCuaMbIV3ffnRbYw.png"/></div></figure><p id="e3fd" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">这个概率函数将给出一个从0到1的数字，表示这个观察结果属于我们目前确定为“是”的分类的可能性有多大。有了这个，我们知道我们打算如何处理我们的预测。现在，我们将了解如何使用逻辑回归来计算(1)中的方程。</p><h1 id="a110" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">逻辑回归</h1><p id="8112" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了进行逻辑回归，使用了sigmoid函数，如下图所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/b7f3ce6f26f803b74699945195c1f4c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P2X0WOQXx9Gtu0_hXUqSSA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:Sigmoid.jpg" rel="noopener ugc nofollow" target="_blank">维基公共</a></p></figure><p id="0988" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我们可以看到，这个函数满足概率函数和等式(1)的特征。同样，我们可以看到，当S(t)是非常大的正值时，函数趋近于1，当S(t)是非常大的负值时，函数趋近于0。</p><h1 id="e1bc" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">例子</h1><p id="4812" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了能够理解逻辑回归是如何操作的，我们将举一个例子，在这个例子中，我们的函数将人们分为高或不高。我们将用于校准我们的函数的数据对应于下表(表1):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/10f272815e1eb860c2570b5cedbc7f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*DFnLWQ8p0elVyVt2FHn3_w.png"/></div></div></figure><p id="2499" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">假设<em class="ms"> t=wx+b </em>，我们的目标将是找到<em class="ms"> w </em>和<em class="ms"> b </em>，通过将其放入S(t)中，它将给出正确的预测。我们可以从一个随机数开始，比如说<em class="ms"> w=1 </em>和<em class="ms"> b=0 </em>，如果<em class="ms"> S(t) </em>大于0.5，那么我们就认为它是一个高个子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/16b611d0f40394a4ca66be768904bd6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*tqFsZkcbkxmfQXGzTESbyQ.png"/></div></figure><p id="1282" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我们注意到参数<em class="ms"> w=1 </em>和<em class="ms"> b=0 </em>不起作用，因为在所有情况下S(x) &gt; 0.5。现在让我们用<em class="ms"> w=6 </em>和<em class="ms"> b=-10.5 </em>试试，结果将是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/8b9646f1d24b01063949e956dc3ed676.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*fpgAAklmlvJLTF0A2xq-Sw.png"/></div></figure><p id="bb1b" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">太好了，我们已经找到了<em class="ms"> w </em>和<em class="ms"> b </em>参数，这样我们的函数就能正确地做出预测！</p><p id="33f0" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">这些参数用于进行预测；然而，出现了许多问题，例如:</p><ul class=""><li id="0fae" class="mv mw iq lq b lr ml lu mm lx mx mb my mf mz mj na nb nc nd bi translated">我如何计算这些参数？</li><li id="aeef" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">这些参数是最值得推荐的吗？</li></ul><p id="6347" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">为了回答这些问题，我们将不得不引入两个新的主题来帮助我们优化函数和理解损失函数。</p><h1 id="c378" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">损失误差函数和成本函数</h1><p id="3382" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">最好的<em class="ms"> w </em>和<em class="ms"> b </em>参数是什么？这个问题的答案非常简单，因为我们希望参数给我们的误差尽可能小。为此，我们使用损失误差函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/56e340833b06d6114073ce69801fbce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*Oltu2Yqon94K1InklUOoJQ.png"/></div></figure><p id="be89" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">这个函数基本上告诉我们，我们对实际值的估计有多远(<em class="ms"> ŷ </em>估计，<em class="ms"> y </em>实际值)。如果我们对所有估计进行总结，我们会得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/287b47d97f7bc97da8f02460588acb48.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*T8ORTQbmiN-_XIbluwAmmg.png"/></div></figure><p id="2faa" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">这个总和是我们所有估计的总误差，试图将这个函数减小到0意味着试图将我们的误差减小到0。然而，当在逻辑回归中使用该函数时，我们会得到一个非凸的函数(我们稍后将回到这个主题),由于它不是凸的，所以可能会有几个局部最优点，并且在计算最佳的<em class="ms"> w </em>和<em class="ms"> b </em>时会有很大的困难。为了解决这个问题，我们将使用另一个函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/00a1aeefb5ac436aad307064f193ceb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*cYFo4QEPDHJIOYGNOwQvnw.png"/></div></figure><p id="6156" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">在(5)中构造的函数具有与函数(3)相同的目的，以减少误差。我们将对一个观察值做一个实验，看看函数<em class="ms"> J(ŷ,y) </em>的行为。让我们想象一下<em class="ms"> J(ŷ,y) </em>的4种可能场景</p><p id="4c53" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">场景1- <em class="ms"> y=1，ŷ=.99 </em></p><p id="30cf" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我们观察到，在这种情况下，我们的估计实际上是正确的，当替换时，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/54deab1d5f742532453fbca93de03efa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*KjevLx48Yf8xkcK_LdgA-Q.png"/></div></figure><p id="94db" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">场景2- <em class="ms"> y=1，ŷ=.01 </em></p><p id="cd73" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我们观察到，在这种情况下，我们的估计是不正确的，当替换时，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e4564c90d776262f6faab58fa3e3f4fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*5__e6LS_pOME8tF89z031Q.png"/></div></figure><p id="1b51" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">场景3- <em class="ms"> y=0，ŷ=.99 </em></p><p id="5b1d" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我们观察到，在这种情况下，我们的估计是不正确的，当替换时，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/af841238f282c9895a6350b8aa434bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PUtIXDoobyFDCfqPdXf0Pg.png"/></div></div></figure><p id="fc49" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">场景4- <em class="ms"> y=0，ŷ=.01 </em></p><p id="3533" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我们观察到，在这种情况下，我们的估计是不正确的，当替换时，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/3d9cc3fa6d4eb0499f19f18ead14ab85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0rs_7Ox0FGrnyud5G96nOQ.png"/></div></div></figure><p id="25b5" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">直观上，我们可以观察这个函数做什么。正确的观测值误差很低，不正确的观测值误差很高。如果我们对所有的观察值求和，我们会得到。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/8441fd843d3ab6dc0b130c7155c13593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KwvPwdbe92SL6fMwQtUB5w.png"/></div></div></figure><p id="a9db" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">函数(6)相对于函数(4)的优势在于它是凸的。使用函数6，可以使用梯度下降法以更简单的方式找到最佳点。</p><h1 id="0b24" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">梯度下降</h1><p id="ef92" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">梯度下降法试图告诉我们需要向哪个方向移动我们的<em class="ms"> b </em>和<em class="ms"> w </em>参数，以优化函数并获得最小误差。(6)中描述的函数是凸的，所以你可以看到它如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/907f01a7c3840dacb1db8fd0ea0aa219.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*hBOQA9OBOp1lIZxYEizjdg.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:Grafico_3d_x2%2Bxy%2By2.png" rel="noopener ugc nofollow" target="_blank"> wikicommons </a></p></figure><p id="7694" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">现在，假设我们的损失函数是<em class="ms"> J(w，b) </em>，要调整的参数是<em class="ms"> (w，b) </em>。我们想找到一个点，使<em class="ms"> J(w，b) </em>尽可能小。梯度下降法告诉我们移动<em class="ms"> (w，b) </em>的方向来减少<em class="ms"> J(w，b) </em>。这些是<em class="ms"> (w，b) </em>的偏导数，也就是<em class="ms"> ∂J/∂w </em>和<em class="ms"> ∂J/∂b </em>。</p><p id="473a" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">知道移动<em class="ms"> w </em>和<em class="ms"> b </em>的方向。我们只需要知道它们移动的幅度，这叫做学习率，通常定义为α。最终得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0fde2d57d724b75e6f6e4a551f63b816.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*QRebJoAR6rRrsXI_9qjNNg.png"/></div></figure><p id="be6a" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">最后，我们将总结执行逻辑回归必须遵循的步骤:</p><ol class=""><li id="f3d9" class="mv mw iq lq b lr ml lu mm lx mx mb my mf mz mj nt nb nc nd bi translated">分析问题，容纳数据。</li><li id="e0c4" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj nt nb nc nd bi translated">随机提出<em class="ms"> w </em>和<em class="ms"> b </em>来预测你的数据。</li><li id="711d" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj nt nb nc nd bi translated">计算误差</li><li id="2f41" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj nt nb nc nd bi translated">执行梯度下降以获得新的<em class="ms"> w </em>和<em class="ms"> b. </em></li></ol><p id="5168" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">这4个步骤应该重复，直到你得到一个可接受的误差。</p><h1 id="a1db" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">分析数据(Python示例)</h1><p id="68f4" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们终于有了应用逻辑回归的所有理论要素。了解如何在python等编程语言中应用它们也很重要。这种语言被广泛使用，因此这种算法的实现非常容易。对于这个例子，我们将使用逻辑回归来预测篮球运动员的轨迹。本练习中获得的数据是从data.world中提取的。在文章的最后，显示了完整的代码，这样您就可以在google colab上跟随并运行代码。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/bafabeb309046bd71fcd58695a0d8181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*ifH_7HcaTCjBzF2l-nAdfQ.png"/></div></figure><p id="6a52" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">理解此表中每一列的含义很重要:</p><ul class=""><li id="376f" class="mv mw iq lq b lr ml lu mm lx mx mb my mf mz mj na nb nc nd bi translated">姓名-玩家的姓名</li><li id="4de8" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">GP-玩过的游戏</li><li id="653d" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">播放的分钟数</li><li id="dc2d" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">每场比赛的平均得分</li><li id="5e92" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">FGM-射门得分</li><li id="fc76" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">FGA-尝试射门</li><li id="ff5b" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">% FG-现场目标的百分比</li><li id="a2de" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">3P投进了，三分球投进了</li><li id="8097" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">尝试3PA三分球</li><li id="8712" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">罚球命中</li><li id="43ce" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">尝试罚球</li><li id="4efb" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">罚球百分比</li><li id="2400" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">进攻篮板</li><li id="5ceb" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">DREB-防守篮板</li><li id="ee20" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">篮板球</li><li id="3520" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">AST-助攻</li><li id="dadd" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">STL-抢断</li><li id="5b9d" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">BLK街区</li><li id="8ff0" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">TOV-失误</li><li id="844d" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj na nb nc nd bi translated">TARGET _ 5 yers-如果玩家持续了5年或更长时间，这个值将是1，否则将是0。</li></ul><h1 id="5336" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据清理</h1><p id="b6b1" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在进行逻辑回归之前，应该对数据进行观察和分析。Pandas库是python上一个非常常用的处理数据的库，我们将使用它来读取和描述数据。</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="b281" class="oa kx iq nw b gy ob oc l od oe">##Import library and read data<br/>import pandas as pd<br/>nbalog=pd.read_csv("path_of_file")<br/>###See data description<br/>decri=nbalog.describe()</span></pre><p id="cd29" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">以“#”开头的代码只是一些注释，所以对于这段代码，我们只做了3件事:</p><ol class=""><li id="1f62" class="mv mw iq lq b lr ml lu mm lx mx mb my mf mz mj nt nb nc nd bi translated">导入必要的库。</li><li id="4a71" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj nt nb nc nd bi translated">读取基础文件。</li><li id="9dc5" class="mv mw iq lq b lr ne lu nf lx ng mb nh mf ni mj nt nb nc nd bi translated">描述现有数据。</li></ol><p id="e9a0" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">描述表明我们拥有的数据量、最大值、最小值、标准偏差等。通过观察数据，可以看出一些字段是空的。在训练模型之前，必须解决这些问题。空白字段将被替换为0，皮尔逊相关系数将用于观察具有最高相关性的数据。</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="5346" class="oa kx iq nw b gy ob oc l od oe">###Using the data described we notice that 3P% has some blank ###fields.These fields will be filled with 0. nbalog=nbalog.fillna(0) <br/>###We check the correlation that exists between the data. pearson=nbalog.corr(method='pearson')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/95b99f0de847ff4eb95d04846e161d7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xd1I3bhB368Ie8OqI4_wUw.png"/></div></div></figure><p id="24f6" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">使用皮尔逊相关系数，我们注意到具有最高相关性的列。这样，更有用的列应该保留，其他的应该删除。</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="8d69" class="oa kx iq nw b gy ob oc l od oe">###Some variables are higly correlated so they will be dropped <br/>###(pearson&gt;.8). <br/>nbalog=nbalog.drop(['MIN', 'PTS','FGM','3P Made','FTM','FTA','TOV','OREB','DREB'], axis=1)</span></pre><h1 id="8ac0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">逻辑回归</h1><p id="c64d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">有了干净的数据，我们可以开始训练模型。为此，将使用库sklearn。该库包含许多模型，并不断更新，使其非常有用。为了训练模型，我们将指出哪些是预测变量，哪些是被预测变量。</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="62d6" class="oa kx iq nw b gy ob oc l od oe">### X are the variables that predict and y the variable we are  ###trying to predict. X=nbalog[['GP','FGA','FG%','3PA','3P%','FT%','REB','AST','STL','BLK']] <br/>y=nbalog[['TARGET_5Yrs']]</span></pre><p id="4704" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">现在，使用库sklearn，数据将被分成训练集和测试集。通过训练集，模型将被调整，通过测试集，我们将看到模型有多好。</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="0fe0" class="oa kx iq nw b gy ob oc l od oe">### The data has to be divided in training and test set. from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25)</span></pre><p id="b0cf" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">前面的代码将数据分为训练集和测试集。变量X代表自变量，y代表因变量。这一次，该集合的75%用于训练，25%用于测试。在分离数据之后，它可以用于拟合模型，在这种情况下，该模型是“逻辑回归”模型。</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="f3c6" class="oa kx iq nw b gy ob oc l od oe">###We import the model that will be used. from sklearn.linear_model import LogisticRegression. <br/># Create an instance of the model. <br/>logreg = LogisticRegression() <br/># Training the model. <br/>logreg.fit(X_train,y_train) <br/># Do prediction. <br/>y_pred=logreg.predict(X_test)</span></pre><p id="2855" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">因此，已经使用函数<strong class="lq ir">校准了模型。拟合</strong>并准备好使用测试数据进行预测。这是使用功能<strong class="lq ir">完成的。预测</strong>并使用自变量测试<strong class="lq ir"> (X_test)。</strong>得到的结果可以与真实值<strong class="lq ir"> (y_test) </strong>进行比较，看是否是一个好的模型。</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="6051" class="oa kx iq nw b gy ob oc l od oe"># Analyzing the results. <br/>from sklearn import metrics <br/>cnf_matrix = metrics.confusion_matrix(y_test, y_pred)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/06d7fd0a9f8cec4c5f5210dc3f7b25af.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*XnwB18w9N0Q8OIw9tbXOvw.png"/></div></figure><p id="be67" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">由此产生的矩阵被称为混淆矩阵。在第一象限中，显示了被正确分类为0的条目的数量(61)。第二和第三象限合计不正确的分类(99)。最后，第四象限显示了数字为1 (175)的正确分类。精度可以通过下式计算:</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="47bd" class="oa kx iq nw b gy ob oc l od oe">print("Accuracy:",metrics.accuracy_score(y_test, y_pred))</span><span id="77eb" class="oa kx iq nw b gy oh oc l od oe"><em class="ms">Output:.7045</em></span></pre><p id="283f" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">以此，我们结束本文。像往常一样，我将代码留给您，以便您可以测试、运行和尝试不同的模型。祝你愉快，祝贺你学会了如何做逻辑回归。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="c950" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">文献学</p><p id="6db1" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">新泽西州里普纳(2017年1月24日)。<em class="ms">二元分类练习数据集</em>。检索于2020年9月24日，来自<a class="ae kv" href="https://data.world/." rel="noopener ugc nofollow" target="_blank">https://data.world/.</a></p><p id="5526" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">indeed 123/CC BY-SA(https://creativecommons.org/licenses/by-sa/3.0)</p><p id="0b5a" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">loli kar/CC BY-SA(https://creativecommons.org/licenses/by-sa/4.0)</p></div><div class="ab cl ok ol hu om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="ij ik il im in"><p id="21db" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><em class="ms">原载于2020年9月25日https://datasciencestreet.com</em><a class="ae kv" href="https://datasciencestreet.com/logistic-regression-for-binary-classification/" rel="noopener ugc nofollow" target="_blank"><em class="ms"/></a><em class="ms">。</em></p></div></div>    
</body>
</html>