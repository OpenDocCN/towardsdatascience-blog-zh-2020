<html>
<head>
<title>k-Nearest Neighbors (kNN) for anomaly detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于异常检测的k-最近邻(kNN)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-nearest-neighbors-knn-for-anomaly-detection-fdf8ee160d13?source=collection_archive---------1-----------------------#2020-10-24">https://towardsdatascience.com/k-nearest-neighbors-knn-for-anomaly-detection-fdf8ee160d13?source=collection_archive---------1-----------------------#2020-10-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9aee" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用于异常值和异常检测的小数据科学</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bd715eb9f15884babcbe06d6ac6b22fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jD3ikl7TIJgaXA_W"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">妮娜·斯特雷尔在Unsplash<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">拍摄的照片</a></p></figure><h2 id="7e0b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">k-最近邻</h2><p id="d75f" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">kNN是一种监督ML算法，经常用于数据科学中的分类问题(有时也用于回归问题)。这是一种最简单但被广泛使用的算法，有很好的使用案例，如构建推荐系统、人脸检测应用等。</p><p id="cb43" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">最近邻族的基本假设是相似的观测值彼此接近，异常值通常是孤立的观测值，远离相似观测值的聚类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/457b16f4095a6cac03d95f0399f67791.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*Z4pEsMtjy0svozfUx6haow.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">kNN概念图(图片:作者)</p></figure><p id="1292" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">由于本文的目的是讨论一个用例——异常检测，所以我不会深入讨论kNN的更多细节。但是如果你感兴趣的话，可以看看所有最近邻算法的文档，网上有很多描述kNN如何工作的资料。如果你在评论中留言，我可以提供一些有用的资源。</p><h2 id="c41a" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">用于异常检测的kNN</h2><p id="e605" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">虽然kNN是一种有监督的ML算法，但当涉及到异常检测时，它采用一种无监督的方法。这是因为在该过程中没有实际的“学习”,并且在数据集中没有预先确定的“异常值”或“非异常值”的标记，相反，它完全基于阈值。数据科学家任意决定截止值，超过该值的所有观察结果都被称为异常(我们将在后面看到)。这也是为什么没有训练测试数据分割或准确性报告。</p><p id="433a" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">今天的文章是我的异常、异常值和欺诈检测算法系列的继续，并附有实际操作的示例代码。我之前的8篇文章谈到了异常检测领域中可用的不同工具和技术，如果您有兴趣了解它们，请访问以下链接:</p><ul class=""><li id="5162" class="my mz it lx b ly mo mb mp li na lm nb lq nc mn nd ne nf ng bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/support-vector-machine-svm-for-anomaly-detection-73a8d676c331">支持向量机(SVM) </a></li><li id="8538" class="my mz it lx b ly nh mb ni li nj lm nk lq nl mn nd ne nf ng bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/dbscan-a-density-based-unsupervised-algorithm-for-fraud-detection-887c0f1016e9"> DBSCAN，一种无监督算法</a></li><li id="f302" class="my mz it lx b ly nh mb ni li nj lm nk lq nl mn nd ne nf ng bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/machine-learning-for-anomaly-detection-elliptic-envelope-2c90528df0a6">椭圆形信封</a></li><li id="fb30" class="my mz it lx b ly nh mb ni li nj lm nk lq nl mn nd ne nf ng bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/anomaly-detection-with-local-outlier-factor-lof-d91e41df10f2">局部异常因子(LOF) </a></li><li id="9eec" class="my mz it lx b ly nh mb ni li nj lm nk lq nl mn nd ne nf ng bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/z-score-for-anomaly-detection-d98b0006f510"> Z分数</a></li><li id="41f9" class="my mz it lx b ly nh mb ni li nj lm nk lq nl mn nd ne nf ng bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/boxplot-for-anomaly-detection-9eac783382fd">箱线图</a></li><li id="f377" class="my mz it lx b ly nh mb ni li nj lm nk lq nl mn nd ne nf ng bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/statistical-techniques-for-anomaly-detection-6ac89e32d17a">统计技术</a></li><li id="2562" class="my mz it lx b ly nh mb ni li nj lm nk lq nl mn nd ne nf ng bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/time-series-anomaly-detection-with-anomalize-library-67472003c003">时间序列异常检测</a></li></ul><p id="54ed" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">现在让我们继续用Python编程语言做一个kNN算法的简单演示。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="4b82" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">步骤1:导入库</strong></h2><p id="5688" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">这个演示只需要很少的库:<code class="fe mu mv mw mx b">pandas</code>和<code class="fe mu mv mw mx b">numpy</code>用于处理数据，<code class="fe mu mv mw mx b">matplotlib</code>用于可视化(可选)，而<code class="fe mu mv mw mx b">sklearn</code>用于导入kNN算法。</p><pre class="kj kk kl km gt nt mx nu nv aw nw bi"><span id="a6b1" class="kz la it mx b gy nx ny l nz oa"># import libraries<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.neighbors import NearestNeighbors</span></pre><h2 id="cd6f" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">第二步:数据准备</strong></h2><p id="2a27" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">我正在使用来自Github repo的著名的<a class="ae ky" href="https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>，所以你可以跟着练习，不用担心从哪里得到数据，如何清理它。</p><pre class="kj kk kl km gt nt mx nu nv aw nw bi"><span id="5c67" class="kz la it mx b gy nx ny l nz oa"># import data<br/>data = pd.read_csv("<a class="ae ky" href="https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv</a>")</span><span id="c58d" class="kz la it mx b gy ob ny l nz oa"># input data<br/>df = data[["sepal_length", "sepal_width"]]</span></pre><p id="aef0" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">现在，让我们将选择用于建模的两个变量可视化。</p><pre class="kj kk kl km gt nt mx nu nv aw nw bi"><span id="c5aa" class="kz la it mx b gy nx ny l nz oa"># scatterplot of inputs data<br/>plt.scatter(df["sepal_length"], df["sepal_width"])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a8578baad5868e97703e13e9076bdb64.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*g8k1esJr30YyK2E5jPuM0g.png"/></div></figure><p id="b3ec" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">数据准备的最后一步是将特征列转换为数组。</p><pre class="kj kk kl km gt nt mx nu nv aw nw bi"><span id="b1a6" class="kz la it mx b gy nx ny l nz oa"># create arrays<br/>X = df.values</span></pre><h2 id="3507" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">第三步:建模</strong></h2><p id="a49f" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">像大多数机器学习实现一样，实际建模只需很少的努力。首先，用您选择的参数实例化模型，然后使模型适合您的数据。就是这样！</p><p id="5147" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">kNN中的关键参数是<em class="od"> n_neighbors </em>，它决定了用于计算测量点距离的邻居数量。</p><pre class="kj kk kl km gt nt mx nu nv aw nw bi"><span id="5eff" class="kz la it mx b gy nx ny l nz oa"># instantiate model<br/>nbrs = NearestNeighbors(n_neighbors = 3)</span><span id="72c3" class="kz la it mx b gy ob ny l nz oa"># fit model<br/>nbrs.fit(X)</span></pre><h2 id="9801" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">步骤4:异常检测</strong></h2><p id="d45b" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">既然我们已经拟合了模型，接下来是提取模型输出的时候了—(a)数据点之间的距离和(b)相关的指数值—这些输出可用于检测异常。</p><pre class="kj kk kl km gt nt mx nu nv aw nw bi"><span id="6126" class="kz la it mx b gy nx ny l nz oa"># distances and indexes of k-neaighbors from model outputs<br/>distances, indexes = nbrs.kneighbors(X)</span><span id="d4a4" class="kz la it mx b gy ob ny l nz oa"># plot mean of k-distances of each observation<br/>plt.plot(distances.mean(axis =1))</span></pre><p id="3e04" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">绘制数据集中每个观测值的平均距离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/d99b0398196e727749b73fae3a46040d.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*odEo86skhIaFDzfwlR1iDg.png"/></div></figure><p id="a20b" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">正如我们所看到的，距离度量中有一些尖峰，这些尖峰是数据集中潜在的异常或异常值。</p><p id="8e73" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">现在，数据科学家面临一个最重要的决策——确定过滤异常的截止值。</p><p id="e835" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">如上图所示，一些截止值可能是-0.25、0.20、0.15 (y轴)，每个值过滤的异常值数量越来越多。</p><p id="5a18" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">对于这个演示，让我们慷慨地选择0.15作为临界值，以获得更多的异常值。慷慨的原因是能够进一步检查数据，这样我们就不会遗漏异常值。</p><pre class="kj kk kl km gt nt mx nu nv aw nw bi"><span id="1ed4" class="kz la it mx b gy nx ny l nz oa"># visually determine cutoff values &gt; 0.15<br/>outlier_index = np.where(distances.mean(axis = 1) &gt; 0.15)<br/>outlier_index</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/1261088e10adbbd83dc02f50ddd0293d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*N9BhhyVsr4bXchC9JjbC7A.png"/></div></figure><pre class="kj kk kl km gt nt mx nu nv aw nw bi"><span id="7f37" class="kz la it mx b gy nx ny l nz oa"># filter outlier values<br/>outlier_values = df.iloc[outlier_index]<br/>outlier_values</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/d7c4f14ac2a0575967cabaa411f8ace2.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*oBgfvISHhhKm-h1ZMsSDgw.png"/></div></figure><h2 id="f58a" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">绘图5(可选):绘制异常</strong></h2><p id="e388" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">我们已经在步骤4中识别了异常数据点，但是我们可以采取额外的步骤来可视化异常数据点。</p><pre class="kj kk kl km gt nt mx nu nv aw nw bi"><span id="60bd" class="kz la it mx b gy nx ny l nz oa"># plot data<br/>plt.scatter(df["sepal_length"], df["sepal_width"], color = "b", s = 65)</span><span id="c34b" class="kz la it mx b gy ob ny l nz oa"># plot outlier values<br/>plt.scatter(outlier_values["sepal_length"], outlier_values["sepal_width"], color = "r")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1457575c5bd6bf431fc4221e151af608.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*z8PiWePLH9P82hJ2P0N0Jg.png"/></div></figure><h1 id="283f" class="oi la it bd lb oj ok ol le om on oo lh jz op ka ll kc oq kd lp kf or kg lt os bi translated">摘要</h1><p id="55cd" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">在本文中，我演示了如何实现kNN——一种机器学习算法——来识别数据集中的异常。目的是展示一些简单的步骤来建立直觉，但是当然，现实世界的实现需要更多的实验来找出对于特定的环境和行业什么是有效的，什么是无效的。例如，在本演示中，我们停在了第4步，但现实世界中的数据科学家会更进一步，检查并重新检查过滤器数据点，以便与其他工具或领域专家进行双重检查。</p><p id="623d" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">感谢关注，要了解更多关于我的工作，你可以在<a class="ae ky" href="https://twitter.com/DataEnthus" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或<a class="ae ky" href="https://www.linkedin.com/in/mab-alam/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上关注我</p></div></div>    
</body>
</html>