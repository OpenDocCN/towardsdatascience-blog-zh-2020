<html>
<head>
<title>Hyperparameter tuning for Machine learning models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习模型的超参数调整</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-for-machine-learning-models-1b80d783b946?source=collection_archive---------3-----------------------#2020-10-31">https://towardsdatascience.com/hyperparameter-tuning-for-machine-learning-models-1b80d783b946?source=collection_archive---------3-----------------------#2020-10-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d1b8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用随机搜索、网格搜索和超点优化方法提高机器学习模型的准确性</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/291987bccd2b6184b0fd2433ae7cbc8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*m8wA3V3rQYAMh69B"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@joeyc?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔·凯恩</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="6b54" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">简介</strong></h1><p id="34bb" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">本文介绍了使用Sci-kit learn和HyperOpt库对机器学习模型的超参数调整进行随机搜索、网格搜索和贝叶斯优化方法的比较和实现。超参数调整至关重要，因为它们控制机器学习模型的整体行为。每个机器学习模型都有不同的可以设置的超参数。</p><blockquote class="mk"><p id="b40e" class="ml mm iq bd mn mo mp mq mr ms mt mj dk translated">超参数是在学习过程开始之前设置其值的参数。</p></blockquote><p id="62b9" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">我将使用来自Kaggle的<a class="ae kv" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank"> Titanic数据集</a>进行比较。本文的目的是探索随机森林模型的性能和计算时间如何随着各种超参数调整方法而变化。毕竟，机器学习就是要在计算时间和模型性能之间找到正确的平衡。</p><h2 id="f2d7" class="mz kx iq bd ky na nb dn lc nc nd dp lg lx ne nf li mb ng nh lk mf ni nj lm nk bi translated">具有默认参数的基线模型:</h2><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="ab07" class="mz kx iq nm b gy nq nr l ns nt">random_forest = RandomForestClassifier(random_state=1).fit(X_train, y_train)<br/>random_forest.score(X_test,y_test)</span></pre><p id="0182" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">当在测试装置上使用时，该模型的精度为<strong class="lq ir"> 81.56 </strong>。</p><p id="4dd7" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">我们可以使用命令获取用于模型的默认参数。<code class="fe nz oa ob nm b"><strong class="lq ir">randomforest.get_params()</strong></code></p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="c014" class="mz kx iq nm b gy nq nr l ns nt"><strong class="nm ir">The default parameters are:<br/></strong>{'bootstrap': True, 'ccp_alpha': 0.0,  'class_weight': None,  'criterion': 'gini',  'max_depth': None,  'max_features': 'auto',  'max_leaf_nodes': None,  'max_samples': None,  'min_impurity_decrease': 0.0,  'min_impurity_split': None,  'min_samples_leaf': 1,  'min_samples_split': 2,  'min_weight_fraction_leaf': 0.0,  'n_estimators': 100,  'n_jobs': None,  'oob_score': False,  'random_state': 1,  'verbose': 0,  'warm_start': False}</span></pre><p id="8a72" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">如果您不知道这些参数以及它们的使用方法，也不必担心。通常，关于所有参数的信息可以在模型的Scikit文档中找到。</p><p id="16dd" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated"><strong class="lq ir">随机森林中的一些重要参数:</strong></p><ol class=""><li id="f9b0" class="oc od iq lq b lr nu lu nv lx oe mb of mf og mj oh oi oj ok bi translated"><strong class="lq ir"> <em class="ol"> max_depth: int，default=None </em> </strong>该选项用于选择你希望森林中每棵树的深度。树越深，它就有越多的分支，它就能获取更多的数据信息。</li><li id="9da6" class="oc od iq lq b lr om lu on lx oo mb op mf oq mj oh oi oj ok bi translated"><strong class="lq ir"> <em class="ol">判据</em> </strong> <em class="ol"> :{"Gini "，" entropy"}，default=" Gini": </em>衡量每个拆分的质量。“基尼系数”使用基尼系数杂质，而“熵”则根据信息增益进行分割。</li><li id="3627" class="oc od iq lq b lr om lu on lx oo mb op mf oq mj oh oi oj ok bi translated"><strong class="lq ir"> <em class="ol"> max_features: {"auto "，" sqrt "，" log2"}，int或float，default=" auto": </em> </strong>这表示在寻找最佳分割时，在分割前级别上考虑的特征数量。这提高了模型的性能，因为每个树节点现在都在考虑更多的选项。</li><li id="da88" class="oc od iq lq b lr om lu on lx oo mb op mf oq mj oh oi oj ok bi translated"><strong class="lq ir">min _ samples _ leaf<em class="ol">:</em><em class="ol">int或float，default=1: </em> </strong>该参数帮助确定在随机森林中的每个决策树节点的末端对其进行拆分所需的最小观察次数。</li><li id="ddc7" class="oc od iq lq b lr om lu on lx oo mb op mf oq mj oh oi oj ok bi translated"><strong class="lq ir"> min_samples_split <em class="ol"> : </em></strong></li><li id="af9c" class="oc od iq lq b lr om lu on lx oo mb op mf oq mj oh oi oj ok bi translated"><strong class="lq ir"> n_estimators <em class="ol"> : int，default=100: </em> </strong>这可能是最重要的参数。这表示在计算预测值之前，要在随机森林中构建的树的数量。通常，数字越高越好，但是这在计算上更昂贵。</li></ol><p id="fe68" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">关于其他参数的更多信息可以在随机森林分类器模型<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">文档</a>中找到。</p><h1 id="7d3c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">网格搜索</h1><p id="913a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">执行超参数调整的一种传统且流行的方法是使用Scikit learn的穷举网格搜索。该方法尝试每组超参数的每种可能的组合。使用这种方法，我们可以在参数搜索空间中找到最佳的一组值。这通常使用更多的计算能力，并需要很长时间来运行，因为这种方法需要尝试网格大小中的每个组合。</p><p id="1538" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">参数网格大小将是所有参数的乘积。即，对于我们模型中的以下参数，网格大小将是10*2*4*5*3*5 = 12000</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="56c2" class="mz kx iq nm b gy nq nr l ns nt">parameters ={'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],<br/>     'criterion' : ['gini', 'entropy'],<br/>     'max_features': [0.3,0.5,0.7,0.9],<br/>     'min_samples_leaf': [3,5,7,10,15],<br/>     'min_samples_split': [2,5,10],<br/>     'n_estimators': [50,100,200,400,600]}</span><span id="d488" class="mz kx iq nm b gy or nr l ns nt"><strong class="nm ir">from</strong> sklearn.model_selection <strong class="nm ir">import</strong> ParameterGrid<br/>param_size = ParameterGrid(parameters)<br/>len(param_size)</span><span id="7a83" class="mz kx iq nm b gy or nr l ns nt"><strong class="nm ir">Output:<br/>12000</strong></span></pre><p id="00ba" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">使用sklearn的<code class="fe nz oa ob nm b">GridSearchCV</code>，我们可以搜索我们的网格，然后运行网格搜索。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="2faa" class="mz kx iq nm b gy nq nr l ns nt">%%time</span><span id="1f3d" class="mz kx iq nm b gy or nr l ns nt"><strong class="nm ir">from</strong> sklearn.model_selection <strong class="nm ir">import</strong> GridSearchCV<br/>grid_search = RandomForestClassifier()</span><span id="1283" class="mz kx iq nm b gy or nr l ns nt">grid_search = GridSearchCV(<br/>    grid_search, <br/>    parameters, <br/>    cv=5,<br/>    scoring='accuracy',n_jobs=-1)<br/><br/>grid_result= grid_search.fit(X_train, y_train)<br/><strong class="nm ir">print</strong>('Best Params: ', grid_result.best_params_)<br/><strong class="nm ir">print</strong>('Best Score: ', grid_result.best_score_)</span></pre><h2 id="98bc" class="mz kx iq bd ky na nb dn lc nc nd dp lg lx ne nf li mb ng nh lk mf ni nj lm nk bi translated">输出</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/977959785e2ae8b14068acd1ab63b24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrmGpbXXmFFVmRjCgnN4pw.png"/></div></div></figure><p id="a79b" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">与基线模型相比，网格搜索CV模型的交叉验证分数从81.56%提高到84.12%。这是3.3%的改进。计算时间几乎是5小时，这对于这样一个简单的问题是不可行的。</p><p id="0ee4" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">关于实现Gridsearch的不同方法的更多信息可以在这里找到。</p><h1 id="372a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">随机搜索</h1><p id="4c23" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">与GridCV相比，RandomizedSearch CV的主要区别在于，它不是尝试每个可能的组合，而是从网格空间中随机选择超参数样本组合。因为这个原因，不能保证我们会像网格搜索一样找到最好的结果。但是，这种搜索在实践中非常有效，因为计算时间非常少。</p><p id="51bb" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">计算时间和模型执行主要取决于<code class="fe nz oa ob nm b">n_iter</code>值。因为该值指定了模型应该搜索参数的次数。如果这个值很高，就有更好的机会获得更高的精度，但这也带来了更多的计算能力。</p><p id="2cf7" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">我们可以通过使用sklearn的库来实现<code class="fe nz oa ob nm b">RandomizedSearchCV</code>。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="b366" class="mz kx iq nm b gy nq nr l ns nt">%%time<br/><strong class="nm ir">from</strong> sklearn.model_selection <strong class="nm ir">import</strong> RandomizedSearchCV<br/>random_search=RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions=parameters,verbose=1, n_jobs=-1,<br/>                            n_iter=200)<br/>random_result = random_search.fit(X_train, y_train)<br/>print('Best Score: ', random_result.best_score_*100)<br/>print('Best Params: ', random_result.best_params_)</span></pre><h2 id="1cf0" class="mz kx iq bd ky na nb dn lc nc nd dp lg lx ne nf li mb ng nh lk mf ni nj lm nk bi translated">输出</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/80e5bf4b190212886f2dd715b0c75609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6LU6aA0LMRQrxYQAYbDnUg.png"/></div></div></figure><p id="10f0" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">与基线模型相比，使用随机搜索CV模型，我们的交叉验证分数从81.56%提高到83.57%。这是2.5%的改进，比Grid CV少0.8%。但计算时间不到5分钟，几乎快了60倍。对于大多数简单的问题，这种随机搜索将是超参数调整的最可行的选择。</p><p id="7b51" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">更多关于实现随机搜索的不同方法的信息可以在<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="23fe" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">使用HyperOpt的贝叶斯模型优化</strong></h1><p id="9b45" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了在Hyperopt中公式化优化问题，我们需要一个<strong class="lq ir">目标函数</strong>，该函数<strong class="lq ir"> </strong>接受一个输入并返回一个损失以最小化模型。以及一个<strong class="lq ir">域空间</strong>对于超参数<strong class="lq ir"> </strong>类似于网格搜索，我们要用输入值的范围创建一个参数空间来评估。</p><p id="7217" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">函数可以简单到f(x) = sin(x)，也可以复杂到深度神经网络的误差。该模型根据前面的步骤选择超参数。</p><p id="82a4" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated"><strong class="lq ir">这个模型的工作过程很简单:</strong></p><ol class=""><li id="6851" class="oc od iq lq b lr nu lu nv lx oe mb of mf og mj oh oi oj ok bi translated">创建目标函数的替代概率模型。</li><li id="2f46" class="oc od iq lq b lr om lu on lx oo mb op mf oq mj oh oi oj ok bi translated">找到在代理模型上表现最好的超参数。</li><li id="09c5" class="oc od iq lq b lr om lu on lx oo mb op mf oq mj oh oi oj ok bi translated">在真实模型上使用这些值来返回目标函数并更新代理模型。</li><li id="b67a" class="oc od iq lq b lr om lu on lx oo mb op mf oq mj oh oi oj ok bi translated">重复步骤2和3，直到达到最大评估值。</li></ol><p id="0e75" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">简单来说，如果我们想找到精度最高的超参数。计算精度的函数称为<strong class="lq ir">目标函数</strong>。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="b1f3" class="mz kx iq nm b gy nq nr l ns nt">%%time<br/>import <strong class="nm ir">numpy</strong> as <strong class="nm ir">np</strong><br/>from <strong class="nm ir">hyperopt</strong> import <strong class="nm ir">hp</strong>, <strong class="nm ir">tpe</strong>, <strong class="nm ir">fmin</strong>,<strong class="nm ir">STATUS_OK</strong>,<strong class="nm ir">Trials</strong></span><span id="f783" class="mz kx iq nm b gy or nr l ns nt"><strong class="nm ir">def</strong> accuracy_model(params):<br/>   clf = RandomForestClassifier(**params)<br/>   <strong class="nm ir">return </strong>cross_val_score(clf, X_train, y_train).mean()</span><span id="704b" class="mz kx iq nm b gy or nr l ns nt">param_space = {'max_depth': hp.choice('max_depth', range(10,100)),<br/>'max_features': hp.uniform('max_features', 0.1,1),<br/>'n_estimators': hp.choice('n_estimators', range(50,500)),<br/>'min_samples_leaf': hp.choice('min_samples_leaf',range(3,5)),<br/>'min_samples_split': hp.choice('min_samples_split',range(2,10)),<br/>'criterion': hp.choice('criterion', ["gini", "entropy"])}</span><span id="1d3d" class="mz kx iq nm b gy or nr l ns nt">best = 0<br/><strong class="nm ir">def</strong> f(params):<br/>    global best<br/>    acc = accuracy_model(params)<br/>    if acc &gt; best:<br/>       best = acc<br/>    <strong class="nm ir">return </strong>{'loss': -acc, 'status': STATUS_OK}</span><span id="4e08" class="mz kx iq nm b gy or nr l ns nt">Trials = Trials()<br/>best_params = fmin(f, param_space , algo=tpe.suggest,max_evals=500, trials= Trials)</span><span id="4afb" class="mz kx iq nm b gy or nr l ns nt"><strong class="nm ir">print</strong>('New best:', best, best_params)<br/><strong class="nm ir">print</strong>(best_params)</span></pre><h2 id="5ee7" class="mz kx iq bd ky na nb dn lc nc nd dp lg lx ne nf li mb ng nh lk mf ni nj lm nk bi translated">输出</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/9967f13ffac116914edc740ef04e31d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MViehIO5P_bIVCk3muHcNQ.png"/></div></div></figure><p id="00f8" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">我们使用贝叶斯优化的交叉验证交叉为84.44%，优于随机搜索和网格搜索。计算时间为20分钟，考虑到这种方法的性能最好，这是合理的。</p><p id="335b" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Bayesian_optimization#:~:text=Bayesian%20optimization%20is%20a%20sequential,expensive%2Dto%2Devaluate%20functions." rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a>模型的另一个好处是不同于随机搜索或网格搜索；我们可以跟踪用于形成概率模型的所有过去的评估模型，该概率模型将超参数映射到目标函数的得分概率。</p><p id="c102" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">关于安装和实现Hyperopt库的更多信息可以在<a class="ae kv" href="https://hyperopt.github.io/hyperopt/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="0a08" class="mz kx iq bd ky na nb dn lc nc nd dp lg lx ne nf li mb ng nh lk mf ni nj lm nk bi translated">结论</h2><p id="f1c7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">超参数调整对于提高机器学习模型的准确性非常有利。在我们的例子中，随机森林模型在预测存活率方面已经很好了，所以使用超参数调整方法在准确性方面没有太大的提高。</p><p id="fbb2" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">总之，使用网格搜索选择最佳超参数可能非常耗时。随机搜索速度快但不可靠。然而，即使这些方法也比贝叶斯优化效率低，因为它们不基于先前的结果选择下一个超参数来评估。由于这个原因，这些方法消耗更多的时间来评估无用的参数。</p><h2 id="b41d" class="mz kx iq bd ky na nb dn lc nc nd dp lg lx ne nf li mb ng nh lk mf ni nj lm nk bi translated">额外资源</h2><p id="c7eb" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">除了随机搜索、网格搜索和贝叶斯优化，还有一些高级方法，如超波段和BOHB，它们结合了超波段和贝叶斯优化，更适合超参数调整。关于它们的详细解释在n <strong class="lq ir"> eptune.ai </strong>的精彩博客中有所涉及，可以在<a class="ae kv" href="https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><p id="64e0" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">完整的数据和代码可以在我的<a class="ae kv" href="https://github.com/JaswanthBadvelu/hypermater_tuning" rel="noopener ugc nofollow" target="_blank"> Github库</a>中找到。</p><p id="dd51" class="pw-post-body-paragraph lo lp iq lq b lr nu jr lt lu nv ju lw lx nw lz ma mb nx md me mf ny mh mi mj ij bi translated">希望那有用！非常感谢你读到这里。如果你对这篇文章有任何问题，或者想要联系和交谈，请随时<a class="ae kv" href="https://www.linkedin.com/in/jaswanth-badvelu/" rel="noopener ugc nofollow" target="_blank">在LinkedIn上直接给我发消息</a>。我非常乐意和你聊天，并尽我所能提供帮助。</p></div></div>    
</body>
</html>