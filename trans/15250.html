<html>
<head>
<title>Expectation-Maximization Algorithm, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">期望值最大化算法，已解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/expectation-maximization-algorithm-explained-ffaf0655357e?source=collection_archive---------10-----------------------#2020-10-20">https://towardsdatascience.com/expectation-maximization-algorithm-explained-ffaf0655357e?source=collection_archive---------10-----------------------#2020-10-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="230b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="9c69" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">EM算法的综合指南，包含直觉、示例、Python实现和数学</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f71cb58330a5692afbba5e78524f8ddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b2gwj7HdI5rzZW2j"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">登上富士山与EM寻找最大似然估计值的方式惊人地相似。请继续阅读，找出原因。由<a class="ae lh" href="https://unsplash.com/@hobz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">吉勒·德贾尔丁</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b4fb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">是啊！先说期望最大化算法(简称EM)。如果您处于数据科学“泡沫”中，您可能在某个时间点遇到过EM，并且想知道:什么是EM，我需要知道它吗？</p><p id="dcf9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它是解决高斯混合模型的算法，这是一种流行的聚类方法。对于<strong class="lk jd">隐马尔可夫模型</strong>至关重要的Baum-Welch算法是EM的一种特殊类型。</p><blockquote class="me"><p id="ac0f" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">它适用于大数据和小数据；当其他技术失败时，当信息缺失时，它会蓬勃发展。</p></blockquote><p id="f736" class="pw-post-body-paragraph li lj it lk b ll mo kd ln lo mp kg lq lr mq lt lu lv mr lx ly lz ms mb mc md im bi translated">这是一种非常经典、强大和通用的统计学习技术，几乎在所有的计算统计学课程中都有教授。阅读完本文后，您可以对EM算法有一个很深的理解，并知道何时以及如何使用它。</p><p id="3ddb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们从两个激励性的例子开始(无监督学习和进化)。接下来，我们看看什么是EM的一般形式。我们跳回现实，用EM来解决这两个例子。然后，我们从直觉和数学两方面解释为什么EM像魔咒一样起作用。最后，对本文进行了总结，并提出了一些进一步的课题。</p><p id="656e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章改编自我的博文，省略了推导、证明和Python代码。如果你更喜欢LaTex格式的数学，或者想获得这里所有问题的Python代码，你可以在我的博客上阅读这篇文章。</p><h1 id="288e" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">激励的例子:我们为什么关心？</h1><p id="5a4e" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">可能你已经知道为什么要用EM，也可能你不知道。不管怎样，让我用两个激励人心的例子来为新兴市场做准备。我知道这些很长，但它们完美地突出了EM最擅长解决的问题的共同特征:缺失信息的存在。</p><h2 id="edca" class="nq mu it bd mv nr ns dn mz nt nu dp nd lr nv nw nf lv nx ny nh lz nz oa nj iz bi translated">无监督学习:求解用于聚类的高斯混合模型</h2><p id="384a" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">假设您有一个包含n个数据点的数据集。它可以是一群访问你的网站的客户(客户档案)或一个有不同对象的图像(图像分割)。聚类是在您不知道(或不指定)真正的分组时，为您的数据找出k个自然组的任务。这是一个无监督的学习问题，因为没有使用基本事实标签。</p><p id="03f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种聚类问题可以通过几种类型的算法来解决，例如，诸如k-means的组合类型或诸如Ward的分层聚类的分层类型。但是，如果您认为您的数据可以更好地建模为正态分布的混合，您会选择高斯混合模型(GMM)。</p><p id="6d96" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">GMM的基本思想是，你假设在你的数据背后有一个数据生成机制。该机制首先选择k个正态分布中的一个(具有一定的概率),然后从该分布中传递一个样本。因此，一旦您估计了每个分布的参数，您就可以通过选择可能性最高的数据点来轻松地对每个数据点进行聚类。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/6a936d8caeac27a2e6931b26d72bc923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YFE3gmBUnkIRoykr.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd oc">图一。</strong> <em class="od">安</em> <a class="ae lh" href="https://commons.wikimedia.org/wiki/File:ClusterAnalysis_Mouse.svg" rel="noopener ugc nofollow" target="_blank"> <em class="od">例</em> </a> <em class="od">混合高斯数据，使用k-means和GMM进行聚类(EM求解)。注意EM发现的组看起来像正态分布。</em></p></figure><p id="75d7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，估计参数并不是一项简单的任务，因为我们不知道哪个分布产生了哪些点(<strong class="lk jd">缺失信息</strong>)。EM是一种算法，可以帮助我们解决这个问题。这就是为什么EM是scikit-learn的GMM <a class="ae lh" href="https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture" rel="noopener ugc nofollow" target="_blank">实现</a>中的底层求解器。</p><h2 id="1d58" class="nq mu it bd mv nr ns dn mz nt nu dp nd lr nv nw nf lv nx ny nh lz nz oa nj iz bi translated">群体遗传学:估计蛾的等位基因频率以观察自然选择</h2><p id="94d0" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">你以前听过“工业黑变病”这个说法吗？生物学家在19世纪创造了这个术语来描述动物如何由于城市的大规模工业化而改变它们的肤色。他们观察到，以前罕见的深色胡椒蛾开始主宰以煤为燃料的工业化城镇的人口。当时的科学家对此观察感到惊讶和着迷。随后的研究表明，工业化城市的树皮颜色更深，比浅色的更能掩饰深色的蛾子。你可以玩这个胡椒飞蛾<a class="ae lh" href="https://askabiologist.asu.edu/peppered-moths-game/play.html" rel="noopener ugc nofollow" target="_blank">游戏</a>来更好的理解这个现象。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/1004351e64af003324e0a1f75e50ef0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*XoYDTBMI7LzjEaI6.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd oc">图二。</strong> <em class="od">深色(上)和浅色(下)的胡椒蛾。图片由Jerzy Strzelecki通过维基共享资源提供</em></p></figure><p id="421e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果，暗蛾在捕食中存活得更好，并把它们的基因传递下去，从而形成了一个以暗斑蛾为主的种群。为了证明他们的自然选择理论，科学家们首先需要估计蛾类种群中产生黑色和产生光亮的基因/等位基因的百分比。负责蛾的颜色的基因有C、I、T三种类型的等位基因，基因型<strong class="lk jd"> C </strong> C、<strong class="lk jd"> C </strong> I、<strong class="lk jd"> C </strong> T产生深色的胡椒蛾(<em class="of">Carbonaria</em>)；产生轻微的胡椒蛾。<strong class="lk jd"> I </strong> I和<strong class="lk jd"> I </strong> T产生中间色的蛾(<em class="of"> Insularia </em>)。</p><p id="593e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里有一个手绘的图表，显示了<strong class="lk jd">观察到的</strong>和<strong class="lk jd">缺失的</strong>信息。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/3ac98e6bb680847ddeb627720579e3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Q5F6Gx_KImH8QAvx.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd oc">图三。</strong> <em class="od">花椒螟等位基因、基因型和表型之间的关系。我们观察了表型，但希望估计人群中等位基因的百分比。作者图片</em></p></figure><p id="3880" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们想知道C、I和T在人口中所占的百分比。然而，我们只能通过捕捉来观察到<em class="of">炭螟</em>、<em class="of">典型</em>和<em class="of">岛螟</em>的数量，而不能观察到基因型(<strong class="lk jd">缺失信息</strong>)。事实上，我们没有观察到基因型和多个基因型产生相同的亚种，这使得计算等位基因频率变得困难。这就是EM发挥作用的地方。利用EM，我们可以很容易地估计等位基因频率，并为由于环境污染而在人类时间尺度上发生的微进化提供具体证据。</p><p id="18ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在信息缺失的情况下，EM如何处理GMM问题和胡椒蛾问题？我们将在后面的部分说明这些。但是首先，让我们看看EM到底是什么。</p><h1 id="df27" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">一般框架:什么是EM？</h1><p id="bde7" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">此时，你一定在想(我希望):所有这些例子都很精彩，但真正的EM是什么？让我们深入研究一下。</p><blockquote class="me"><p id="ae73" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">EM算法是一种迭代优化方法，可在存在隐藏/缺失/潜在变量的问题中找到参数的最大似然估计(MLE)。</p></blockquote><p id="5c48" class="pw-post-body-paragraph li lj it lk b ll mo kd ln lo mp kg lq lr mq lt lu lv mr lx ly lz ms mb mc md im bi translated">Dempster、Laird和Rubin (1977)在其著名的论文(目前引用62k)中首次全面介绍了这一概念。它因其易于实现、数值稳定和稳健的经验性能而被广泛使用。</p><p id="f98a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们为一个一般问题建立EM，并引入一些符号。假设Y是我们的观察变量，X是隐藏变量，我们说对(X，Y)是完全数据。我们还将任何感兴趣的未知参数表示为θ∈θ。大多数参数估计问题的目标是在给定模型和数据的情况下找到最可能的θ，即，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/e29c9184427b92a3712d933e75eb7a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nH81Qa8XPSiBub7jYWHFqg.png"/></div></div></figure><p id="c084" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中被最大化的项是不完全数据可能性。使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Law_of_total_probability" rel="noopener ugc nofollow" target="_blank">总概率</a>定律，我们也可以将不完全数据的可能性表示为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/f98831d871b2bf5c7f64ac1e312ac619.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LLPE2BOC6-AvekQfZMP33g.png"/></div></div></figure><p id="a48b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中被积分的项称为完全数据似然。</p><p id="2cd1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所有这些完整和不完整数据的可能性是怎么回事？</p><blockquote class="oj ok ol"><p id="2ea2" class="li lj of lk b ll lm kd ln lo lp kg lq om ls lt lu on lw lx ly oo ma mb mc md im bi translated">在许多问题中，由于信息缺失，不完全数据似然的最大化是困难的。另一方面，使用完全数据可能性通常更容易。</p></blockquote><p id="e9ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">EM算法就是为了利用这种观察而设计的。它在一个<strong class="lk jd">期望步骤</strong> (E步骤)和一个<strong class="lk jd">最大化步骤</strong> (M步骤)之间迭代，以找到MLE。</p><p id="eb48" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">假设上标为(n)的θ是在第n <em class="of">次</em>次迭代中获得的估计值，该算法在两个步骤之间迭代如下:</p><ul class=""><li id="2eae" class="op oq it lk b ll lm lo lp lr or lv os lz ot md ou ov ow ox bi translated"><strong class="lk jd"> E步</strong>:定义Q(θ |θ^(n))为完全数据对数似然的条件期望w.r.t .隐变量，给定观测数据和当前参数估计，即，</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/84d8cbe14b2f3e5d22ce1d8f3f666c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rPGmXqsItKZJAylRswfRqg.png"/></div></div></figure><ul class=""><li id="9dbd" class="op oq it lk b ll lm lo lp lr or lv os lz ot md ou ov ow ox bi translated"><strong class="lk jd"> M步</strong>:找到一个使上述期望最大化的新θ，设为θ^(n+1)，即，</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/fc13eef8c9a5faba0ac3d6c68192f2c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gp-gvpajANpM5GMLr8dodw.png"/></div></div></figure><p id="f322" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">乍一看，上述定义似乎很难理解。一些直观的解释可能会有所帮助:</p><ul class=""><li id="cbe3" class="op oq it lk b ll lm lo lp lr or lv os lz ot md ou ov ow ox bi translated"><strong class="lk jd"> E-step </strong>:这一步是问，给定我们的观测数据<strong class="lk jd"> y </strong>和当前参数估计θ^(n)，不同<em class="of"> X </em>的概率是多少？还有，在这些可能的<em class="of"> X，</em>下，对应的对数似然是什么？</li><li id="0143" class="op oq it lk b ll pa lo pb lr pc lv pd lz pe md ou ov ow ox bi translated"><strong class="lk jd"> M步</strong>:这里我们问，在这些可能的<em class="of"> X </em>下，给我们最大期望对数似然的θ值是多少？</li></ul><p id="46ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该算法在这两个步骤之间迭代，直到达到停止标准，例如，当Q函数或参数估计已经收敛时。整个过程可以用下面的流程图来说明。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/e06455de1d2b95bfa605a9d80a358c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/0*BMC5aTnuCW5Un_wK.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd oc">图4。</strong><em class="od">EM算法在E步和M步之间迭代，以获得最大似然估计，并在估计值收敛时停止。作者图片</em></p></figure><p id="3d9d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就是这样！有了两个方程和一堆迭代，你刚刚解开了一个最优雅的统计推断技术！</p><h1 id="d64f" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">EM在行动:真的有用吗？</h1><p id="15f7" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">上面我们看到的是EM的一般框架，而不是它的实际实现。在这一节中，我们将一步一步地看到EM是如何被实现来解决前面提到的两个例子的。在验证了EM确实可以解决这些问题之后，我们将在下一节中直观地从数学上理解它为什么可以解决这些问题。</p><h2 id="d443" class="nq mu it bd mv nr ns dn mz nt nu dp nd lr nv nw nf lv nx ny nh lz nz oa nj iz bi translated">求解聚类的GMM</h2><p id="83a6" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">假设我们有一些数据，并想模拟它们的密度。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/60845685b9ddb3b1658028e291f41eaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/0*MRRDLLDesQyIARlZ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd oc">图5。</strong> <em class="od"> 400个点生成为四种不同正态分布的混合物。作者图片</em></p></figure><p id="d5d2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你能看到不同的底层分布吗？显然，这些数据来自不止一个发行版。因此，单一的正态分布是不合适的，我们使用混合方法。一般来说，基于GMM的聚类是将(y1，…，yn)个数据点聚类成<em class="of"> k </em>组的任务。我们让</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/66ba89430bbd0eb8126d9ef59eef1f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pVV0Ej7Ko0ZXsiYzbaOLGg.png"/></div></div></figure><p id="1204" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，x_i是数据y_i的一位热码，例如，如果<em class="of"> k </em> = 3并且y_i来自组3，则x_i = [0，0，1]。在这种情况下，数据点集合<strong class="lk jd"> y </strong>为不完整数据，(<strong class="lk jd"> x </strong>，<strong class="lk jd"> y </strong>)为扩充后的完整数据。我们进一步假设每个组遵循正态分布，即，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/7029f888b69f19cd24f92b83ae15d6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pQiAYFka0X9XKNptHCz7Lg.png"/></div></div></figure><p id="8a52" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在通常的混合高斯模型建立之后，以概率<em class="of"> w_k，</em>从第k个<em class="of">组生成新点，并且所有组的概率总和为1。假设我们只处理不完整的数据<strong class="lk jd"> y </strong>。GMM下一个数据点的可能性为</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/182abefb571dc1cc6a5bf562935193ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fxwFZdfGsmmSZAHoEf6Cow.png"/></div></div></figure><p id="de2a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中φ(；μ，σ)是具有均值μ和协方差σ的正态分布的PDF。n个点的总对数似然为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/0a74294822404c055b7387bd8159ecb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UpaYbVczD2YFgtIZoB19nA.png"/></div></div></figure><p id="a923" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们的问题中，我们试图估计三组参数:组混合概率(<strong class="lk jd"> w </strong>)和每个分布的均值和协方差矩阵(<strong class="lk jd"> μ </strong>，<strong class="lk jd">σ</strong>)。参数估计的通常方法是最大化上述总对数似然函数与每个参数的比值(MLE)。然而，由于对数项内的求和，这很难做到。</p><h2 id="bafc" class="nq mu it bd mv nr ns dn mz nt nu dp nd lr nv nw nf lv nx ny nh lz nz oa nj iz bi translated">期望步骤</h2><p id="9880" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">让我们使用EM方法来代替！记住，我们首先需要在E-step中定义Q函数，它是完整数据对数似然的条件期望。由于(<strong class="lk jd"> x </strong>，<strong class="lk jd"> y </strong>)是完整数据，一个数据点对应的可能性为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/aa35266f9c91990a01011430634acd50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xboi_BdKEcZbRqKL-mwNfQ.png"/></div></div></figure><p id="7193" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">并且只有x_{ij} = 1的项是有效的，因为它是一次性编码。因此，我们的总完全数据对数似然是</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/4d83d89b2b93d26f5d38820124d4c0fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DVukvFGXhNPzYYhF5iM4Cg.png"/></div></div></figure><p id="f698" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将θ表示为未知参数的集合(<strong class="lk jd"> w </strong>，<strong class="lk jd"> μ </strong>，<strong class="lk jd">σ</strong>)。按照(2)中的E步公式，我们获得Q函数如下</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pn"><img src="../Images/5af41a4e25f7a10f402044aaa9cd92fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3rS8s_AzeXbItqaO_aoKw.png"/></div></div></figure><p id="5dfa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在哪里</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi po"><img src="../Images/4d15b38c969712c242584eeea2b0c2bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vw5hDUP96XTsEb-94GCNIA.png"/></div></div></figure><p id="9efb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的<em class="of"> z </em>项是数据y_i在当前参数估计的类别<em class="of"> j </em>中的概率。这种概率在某些文本中也被称为责任。意味着每个类对这个数据点的责任。给定观测数据和当前参数估计，它也是一个常数。</p><h2 id="4701" class="nq mu it bd mv nr ns dn mz nt nu dp nd lr nv nw nf lv nx ny nh lz nz oa nj iz bi translated">最大化步骤</h2><p id="7d94" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">回想一下，EM算法是通过在E步和M步之间迭代来进行的。我们已经在上面的E步骤中获得了最新迭代的Q函数。接下来，我们继续进行M步，找到一个新的θ，使(6)中的Q函数最大化，即我们找到</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pp"><img src="../Images/404c4ffbafa70d32b02c8f7c0a58769f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i8VMI7ASC0b351EicxfHdQ.png"/></div></div></figure><p id="5f88" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">仔细观察得到的Q函数，会发现它实际上是一个加权正态分布MLE问题。这意味着，新的θ具有封闭形式的公式，可以使用微分法轻松验证:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pq"><img src="../Images/9bd43d3cc1c259d7aa11fa8535dcefdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FVy5Fp7HyFmka49UE6UXFg.png"/></div></div></figure><p id="1600" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于<em class="of"> j </em> = 1，…，<em class="of"> k </em>。</p><h2 id="f0fa" class="nq mu it bd mv nr ns dn mz nt nu dp nd lr nv nw nf lv nx ny nh lz nz oa nj iz bi translated">表现如何？</h2><p id="e2b7" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">我们在这一节回到开头的问题。我用四种不同的正态分布模拟了400个点。如果我们不知道底层的真实分组，我们会看到图5。我们运行上面导出的EM程序，并设置算法在对数似然不再变化时停止。</p><p id="f985" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们找到了混合概率和所有四组的平均和协方差矩阵。下面的图6显示了EM发现的叠加在数据上的每个分布的密度等值线，这些数据现在通过其地面实况分组进行了颜色编码。四个基本正态分布的位置(均值)和尺度(协方差)都被正确识别。<strong class="lk jd">与k-means不同，EM为我们提供了数据的聚类和它们背后的生成模型(GMM)。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/048821bdff438b17fe2a8c0d5ce8683e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/0*HCWBcumXgL6LtXqG.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd oc">图六。</strong> <em class="od">叠加在四个不同正态分布样本上的密度等值线。作者图片</em></p></figure><h1 id="8937" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">估计等位基因频率</h1><p id="fc62" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">我们回到前面提到的群体遗传学问题。假设我们捕获了n只<em class="of">蛾，其中有三种不同的类型:<em class="of">炭蛾</em>、<em class="of">典型蛾</em>和<em class="of">岛蛾</em>。然而，除了典型蛾外，我们不知道每种蛾的基因型，见上图3。我们希望估计群体等位基因频率。让我们用EM术语来说。以下是我们所知道的:</em></p><ol class=""><li id="5792" class="op oq it lk b ll lm lo lp lr or lv os lz ot md ps ov ow ox bi translated">观察到:</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/cf1bf43b6036a65c338031d62b468780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FizhXGM8oU1Dy3JmtuLMEA.png"/></div></div></figure><p id="946e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">2.未观察到的:不同基因型的数量</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/726832fdfc59d645dd4c863d1de7f9f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bBp1YsVyLqOoDc43h30x0g.png"/></div></div></figure><p id="e884" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">3.但是我们知道它们之间的关系:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pv"><img src="../Images/2e795ccea7d3945271cfa7404ff2ac1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L9thGPoyByfelfhKyNl7Bw.png"/></div></div></figure><p id="97f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">4.感兴趣的参数:等位基因频率</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/19aedb6c37512309828fff1701575c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePaKTLT06kpeWUn5nfFQ7g.png"/></div></div></figure><p id="fd8c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们需要使用另一个重要的建模原则:Hardy-Weinberg原则，该原则认为基因型频率是相应等位基因频率的乘积，或者是两个等位基因不同时的两倍。也就是说，我们可以预期基因型频率为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi px"><img src="../Images/b5fa8223f0b3c654f8a422d4ac617e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7pR_P2z2TMp1CZseZd_cbQ.png"/></div></div></figure><p id="49ae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">很好！现在我们准备插入EM框架。第一步是什么？</p><h2 id="0205" class="nq mu it bd mv nr ns dn mz nt nu dp nd lr nv nw nf lv nx ny nh lz nz oa nj iz bi translated">期望步骤</h2><p id="229f" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">就像GMM的情况一样，我们首先需要计算出完整数据的可能性。注意，这实际上是一个多项式分布问题。我们有一群飞蛾，捕捉到CC基因型飞蛾的机会是p_{C}，其他基因型也是如此。因此，完全数据似然就是多项式<a class="ae lh" href="https://en.wikipedia.org/wiki/Multinomial_distribution#Probability_mass_function" rel="noopener ugc nofollow" target="_blank">分布PDF </a>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi py"><img src="../Images/66509d47de4e07958b3dc3e4c5dcfa3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mOvXanDKAvicoktul-i1ag.png"/></div></div></figure><p id="3878" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">并且完整数据对数似然可以写成以下分解形式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pz"><img src="../Images/5d9465d41da083fbf8cb466b3af8d7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HCxjKnApZeUBcnPUM0VbCg.png"/></div></div></figure><p id="84a7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请记住，在给定最新迭代的参数估计值的情况下，E-step根据未观测数据<em class="of"> Y </em>对上述可能性进行有条件的期望。发现Q函数是</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qa"><img src="../Images/cf28c76d26388e6552026eb9925afb23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5EhqcTi0aRRaPebowVmuJg.png"/></div></div></figure><p id="d3b7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中n_{CC}^(n)是给定当前等位基因频率估计值的CC型蛾的预期数量，对于其他类型也类似。k()是一个不涉及θ的函数。</p><h2 id="12c7" class="nq mu it bd mv nr ns dn mz nt nu dp nd lr nv nw nf lv nx ny nh lz nz oa nj iz bi translated">最大化步骤</h2><p id="f59f" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">由于我们获得了每种表型的预期数量，估计等位基因频率就很容易了。直观地，等位基因C的频率计算为群体中存在的等位基因C的数量与等位基因总数之间的比率。这也适用于其他等位基因。因此，在M步骤中，我们获得</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qb"><img src="../Images/c9888bea964483a92e494d49ad2f9f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gWgOXEJZF4yN9LGz5F_fRg.png"/></div></div></figure><p id="9820" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">事实上，我们可以通过对Q函数求微分并将它们设置为零(通常的优化程序)来获得相同的M步公式。</p><h2 id="20df" class="nq mu it bd mv nr ns dn mz nt nu dp nd lr nv nw nf lv nx ny nh lz nz oa nj iz bi translated">表现如何？</h2><p id="cb19" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">让我们试着用上面推导出的em过程来解决胡椒蛾问题。假设我们捕获了622只胡椒蛾。其中85种是<em class="of">卡波尼亚</em>，196种是<em class="of">海岛</em>，341种是<em class="of">典型</em>。我们运行EM迭代10步，图7显示我们在不到5步的时间内获得了收敛的结果。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qc"><img src="../Images/48a4890fa8c3727e0a36e0a12d1e855d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Zgo1PY9kbB1_n-wh.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd oc">图7。</strong> <em class="od"> EM算法收敛不到五步，找到等位基因频率。作者图片</em></p></figure><h2 id="fdfe" class="nq mu it bd mv nr ns dn mz nt nu dp nd lr nv nw nf lv nx ny nh lz nz oa nj iz bi translated">我们从例子中学到了什么？</h2><p id="7efe" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">由于缺少表型信息，估计等位基因频率是困难的。EM帮助我们解决这个问题，它用丢失的信息来补充这个过程。如果我们回头看看E-step和M-step，我们会看到E-step在给出最新频率估计的情况下计算最可能的表型计数；M-step然后计算给定最新表型计数估计的最可能频率。这个过程在GMM问题中也很明显:在给定当前类参数估计的情况下，E-step计算每个数据的类责任；然后，M-step使用这些责任作为数据权重来估计新的类参数。</p><h1 id="1f22" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">解释:为什么有效？</h1><p id="5319" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">通过前面的两个例子，我们清楚地看到EM的本质在于用缺失的信息增加观察到的信息的<strong class="lk jd"> E步/M步</strong>迭代过程。我们看到它确实有效地找到了最大似然估计。但是为什么这个迭代过程会起作用呢？EM只是一个聪明的黑客，还是有很好的理论支撑？让我们找出答案。</p><h2 id="0003" class="nq mu it bd mv nr ns dn mz nt nu dp nd lr nv nw nf lv nx ny nh lz nz oa nj iz bi translated">直观的解释</h2><p id="011a" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">我们首先对EM的工作原理有一个直观的理解。</p><blockquote class="me"><p id="f989" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">EM通过在一些小步骤中将最大化不完全数据似然(困难)的任务转移到最大化完全数据似然(容易)来解决参数估计问题。</p></blockquote><p id="8741" class="pw-post-body-paragraph li lj it lk b ll mo kd ln lo mp kg lq lr mq lt lu lv mr lx ly lz ms mb mc md im bi translated">想象你正在富士山徒步旅行🗻第一次。在到达顶峰之前有九个站要到达，但是你不知道路线。幸运的是，有徒步旅行者从山顶下来，他们可以给你一个到下一站的大致方向。因此，下面是你可以达到顶端的方法:从基站开始，向人们询问到第二站的方向；去第二个车站，向那里的人打听去第三个车站的路，以此类推。在一天结束的时候(或者一天开始的时候，如果你在看日出的话🌄)，你很有可能到达顶峰。</p><p id="2433" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是EM为我们有缺失数据的问题寻找最大似然估计所做的事情。EM不是最大化<strong class="lk jd"> ln </strong> p( <strong class="lk jd"> x </strong>)(找到登顶的路线)，而是最大化Q函数，找到下一个同样增加<strong class="lk jd"> ln </strong> p( <strong class="lk jd"> x </strong>)(问下一站方向)的θ。下面的图8通过两次迭代说明了这个过程。请注意，G函数只是Q函数和其他几项常数w.r.t. θ的组合。最大化G函数w.r.t. θ相当于最大化Q函数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/a81917d0f40afc74911d5402f1c48016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*b3Nx7Z1f0liIfkiz.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd oc">图8。</strong><em class="od">EM的迭代过程分两步说明。当我们从当前参数估计建立并最大化G函数(等价地，Q函数)时，我们获得下一个参数估计。在这个过程中，不完全数据对数似然也会增加。作者图片</em></p></figure><h1 id="c329" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">摘要</h1><p id="8992" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">在本文中，我们看到EM通过优化转移框架将一个有缺失信息的难题转化为一个简单的问题。我们还通过用Python实现一步一步地解决两个问题(高斯混合聚类和胡椒蛾种群遗传学)来看EM的作用。<strong class="lk jd">更重要的是，我们看到EM不仅仅是一个聪明的黑客，而且有坚实的数学基础来解释它为什么会起作用。</strong></p><p id="b752" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望这篇介绍性文章对您了解EM算法有所帮助。从这里开始，如果您感兴趣，可以考虑探索以下主题。</p><h1 id="bc7c" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">更多主题</h1><p id="42d0" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">深入挖掘，你可能会问的第一个问题是:那么，EM完美吗？当然不是。有时，Q函数很难用解析方法获得。我们可以使用蒙特卡洛技术来估计Q函数，例如，检查蒙特卡洛<a class="ae lh" href="https://amstat.tandfonline.com/doi/abs/10.1198/106186001317115045" rel="noopener ugc nofollow" target="_blank"> EM </a>。有时，即使有完整的数据信息，Q函数仍然很难最大化。我们可以考虑替代的最大化技术，例如，参见期望条件最大化(<a class="ae lh" href="https://academic.oup.com/biomet/article-abstract/80/2/267/251605" rel="noopener ugc nofollow" target="_blank"> ECM </a>)。EM的另一个缺点是它只提供给我们点估计。如果我们想知道这些估计中的不确定性，我们将需要通过其他技术进行方差估计，例如Louis的方法、补充EM或bootstrapping。</p><p id="1292" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">感谢阅读！请考虑在下面给我留下反馈。如果你对更多的统计学习感兴趣，可以看看我的其他文章:</strong></p><div class="qe qf gp gr qg qh"><a rel="noopener follow" target="_blank" href="/linear-discriminant-analysis-explained-f88be6c1e00b"><div class="qi ab fo"><div class="qj ab qk cl cj ql"><h2 class="bd jd gy z fp qm fr fs qn fu fw jc bi translated">线性判别分析，已解释</h2><div class="qo l"><h3 class="bd b gy z fp qm fr fs qn fu fw dk translated">直觉、插图和数学:它如何不仅仅是一个降维工具，为什么它在现实世界中如此强大…</h3></div><div class="qp l"><p class="bd b dl z fp qm fr fs qn fu fw dk translated">towardsdatascience.com</p></div></div><div class="qq l"><div class="qr l qs qt qu qq qv lb qh"/></div></div></a></div><div class="qe qf gp gr qg qh"><a rel="noopener follow" target="_blank" href="/a-math-free-introduction-to-convolutional-neural-network-ff38fbc4fc76"><div class="qi ab fo"><div class="qj ab qk cl cj ql"><h2 class="bd jd gy z fp qm fr fs qn fu fw jc bi translated">卷积神经网络:它与其他网络有何不同？</h2><div class="qo l"><h3 class="bd b gy z fp qm fr fs qn fu fw dk translated">CNN有什么独特之处，卷积到底是做什么的？这是一个无数学介绍的奇迹…</h3></div><div class="qp l"><p class="bd b dl z fp qm fr fs qn fu fw dk translated">towardsdatascience.com</p></div></div><div class="qq l"><div class="qw l qs qt qu qq qv lb qh"/></div></div></a></div><h1 id="88c2" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">参考</h1><ol class=""><li id="f660" class="op oq it lk b ll nl lo nm lr qx lv qy lz qz md ps ov ow ox bi translated">登普斯特，A. P .，莱尔德，N. M .，，鲁宾，D. B. (1977)。通过EM算法不完全数据的最大似然。<em class="of">英国皇家统计学会杂志:B辑(方法论)</em>，<em class="of"> 39 </em> (1)，1–22。</li></ol></div><div class="ab cl ra rb hx rc" role="separator"><span class="rd bw bk re rf rg"/><span class="rd bw bk re rf rg"/><span class="rd bw bk re rf"/></div><div class="im in io ip iq"><p id="5a24" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="of">原发布于</em><a class="ae lh" href="https://yangxiaozhou.github.io/data/2020/10/20/EM-algorithm-explained.html" rel="noopener ugc nofollow" target="_blank"><em class="of">https://yang xiaozhou . github . io</em></a><em class="of">。</em></p></div></div>    
</body>
</html>