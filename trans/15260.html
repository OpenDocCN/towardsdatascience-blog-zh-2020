<html>
<head>
<title>Infer your Reward by Observing an Expert</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过观察专家来推断你的奖励</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/infer-your-reward-by-observing-an-expert-140b685fd5b5?source=collection_archive---------20-----------------------#2020-10-20">https://towardsdatascience.com/infer-your-reward-by-observing-an-expert-140b685fd5b5?source=collection_archive---------20-----------------------#2020-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="319c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">实现反向强化学习算法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/p/140b685fd5b5"><div class="gh gi kf"><img src="../Images/1cb69a6286691db806563703774169d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5GlT-2CZXJCeBZjA"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated">照片由Jehyun Sung在<a class="ae kr" href="https://unsplash.com/photos/6U5AEmQIajg" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="9558" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在强化学习(RL)中设计奖励函数可能很麻烦。很简单，我们的目标是采取行动增加未来的累积回报，并避免那些损害它的行为。然而，选择如何奖励现实任务中的行为，并以一种既可学又能表达代理人期望的目标的方式，并不简单。例如，如何将“可接受的”社会行为指定为一种功能？</p><p id="9b81" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">逆向强化学习(iRL)是一种缓解这种设计问题的方法。我们没有试图自己设计一个奖励函数<em class="lo"> r(s，a) </em>，而是让RL代理观察专家演示我们希望它学习的内容，并从他们的行为中推断专家的意图。通过这样做，代理使奖励函数符合专家的意图。</p><p id="a300" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">让我们深入研究一下。</p><blockquote class="lp lq lr"><p id="1b85" class="ks kt lo ku b kv kw jr kx ky kz ju la ls lc ld le lt lg lh li lu lk ll lm ln ij bi translated">我们的iRL实现将连接到生成敌对网络(GANs)。对于这一点，你会发现对【GANs如何工作有一个基本的了解是非常有用的，尽管我在下面会做简要介绍。</p></blockquote><h1 id="642c" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated"><strong class="ak"> RL为GAN </strong></h1><p id="c90a" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated"><strong class="ku ir">GANs的快速细节</strong></p><p id="9ea9" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">一个GAN由鉴别器<strong class="ku ir"> <em class="lo"> D </em> </strong>和发生器<strong class="ku ir"> <em class="lo"> G </em> </strong>组成。生成器旨在生成看起来尽可能接近训练中使用的真实图像的<em class="lo">假</em>图像。另一方面，鉴别器将给定的图像分类为真品或赝品。因此，<strong class="ku ir"> <em class="lo"> D </em> </strong>和<strong class="ku ir"> <em class="lo"> G </em> </strong>都玩一个游戏，其中<strong class="ku ir"> <em class="lo"> D </em> </strong>试图最大化它正确地将输入<em class="lo"> x </em>分类为真或假<em class="lo">，</em>和<strong class="ku ir"> <em class="lo"> G </em> </strong>最小化<strong class="ku ir"> <em class="lo"> D </em> </strong>将其输出标记为<em class="lo">的可能性</em></p><p id="dcdc" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">实现这一点的损失函数是D和g输出的<em class="lo">对数</em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/@mugoh/140b685fd5b5"><div class="gh gi ms"><img src="../Images/81ab9e2753425ed1766e42476865ccea.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*IdZa8RASKad_0kEbflhm7g.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated">GAN损失函数</p></figure><p id="c361" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">其中:</p><ul class=""><li id="2f6a" class="mt mu iq ku b kv kw ky kz lb mv lf mw lj mx ln my mz na nb bi translated"><strong class="ku ir"> D </strong> (x) —鉴别器预测<em class="lo"> x </em>为真/假</li><li id="0bd6" class="mt mu iq ku b kv nc ky nd lb ne lf nf lj ng ln my mz na nb bi translated"><strong class="ku ir"> G </strong> (x) —发电机输出</li><li id="caa1" class="mt mu iq ku b kv nc ky nd lb ne lf nf lj ng ln my mz na nb bi translated"><strong class="ku ir"> D </strong> ( <strong class="ku ir"> G </strong> (x)) —鉴别器对发电机输出的预测</li></ul><p id="be93" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们可以将这一培训过程描述如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://mugoh.github.io/mug-log/2020/10/20/adversarial-inverse-rl.html"><div class="gh gi nh"><img src="../Images/a1207ae4bfb431f969adad7cbcd7ea34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fybULg0gMklSnH-B6r4d9A.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated"><em class="ni">甘的作品(作者插画)</em></p></figure><h2 id="1dd2" class="nj lw iq bd lx nk nl dn mb nm nn dp mf lb no np mh lf nq nr mj lj ns nt ml nu bi translated">gan与反向RL的关系</h2><p id="bddb" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">反向RL <a class="ae kr" href="https://stats.stackexchange.com/a/487726/296297" rel="noopener ugc nofollow" target="_blank">使用GANs的鉴别器</a>的概念。iRL中的鉴别器是政策和奖励函数的比率。我们一会儿就能看到它的全貌。</p><p id="e909" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">与GAN中的假图像和真图像相似，iRL有两组数据——专家演示和策略与环境交互生成的过渡数据。两个转换集都包括状态-动作对，直到一个有限的时间步长<em class="lo"> T </em> (s₀，a₀，s₁，a₁,…，sT，aT)。将iRL与GANs联系起来，专家论证可以说是真实数据，而政策收集的样本是虚假数据。这意味着该策略现在充当生成器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://mugoh.github.io/mug-log/2020/10/20/adversarial-inverse-rl.html"><div class="gh gi nv"><img src="../Images/b23ca5f55af69942eff7a5719f9f705f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nMcj8_xQZV2mTaRRJT1lHA.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated"><em class="ni"> iRL为GAN(修改自</em> <a class="ae kr" href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="noopener ugc nofollow" target="_blank"> <em class="ni">来源</em> </a> <em class="ni"> ) </em></p></figure><p id="0803" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">鉴频器的目标以与GAN中相同的方式表示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/@mugoh/140b685fd5b5"><div class="gh gi nw"><img src="../Images/a357d7090547642aa4cf999d844efed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*1jZwqp_t2nnjWldHssft3A.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated"><em class="ni">iRL中鉴别器的最小最大物镜</em></p></figure><p id="20ed" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">该目标函数的第一部分试图增加所看到的样本是专家演示的可能性。第二部分降低了样本被正在运行的策略收集的可能性。</p><h2 id="36c0" class="nj lw iq bd lx nk nl dn mb nm nn dp mf lb no np mh lf nq nr mj lj ns nt ml nu bi translated"><strong class="ak"> <em class="ni">实现对鉴别器的培训</em> </strong></h2><p id="9fec" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">正如所看到的，训练鉴别器的目的是最大化输入正确分类为真或假的概率。为此，我们最大化损失函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/@mugoh/140b685fd5b5"><div class="gh gi nx"><img src="../Images/2750f89000f85583b41e9dc381cdb652.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*4FZsQOH7KVHYVEWcvb9mlA.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated">iRL鉴别器损耗函数</p></figure><p id="a0d6" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在实施中，这可以通过两个简单的步骤实现:</p><p id="ce08" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">1.样本批专家轨迹<em class="lo">τE</em>T20<em class="lo">，</em>T23】正向通过<strong class="ku ir">T25】DT27】计算损耗<em class="lo">log</em>(<em class="lo">D</em>(<em class="lo">τE</em>))。</strong></p><p id="cf04" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">2.对收集到的一批策略轨迹<em class="lo"> τF，</em>顺传<strong class="ku ir">DT39】进行采样，计算损失<em class="lo">log</em>(<em class="lo">D</em>(<em class="lo">τF</em>))。这里，我们避免做(最小化)<em class="lo">log</em>(1—<em class="lo">D</em>(<em class="lo">τF</em>))因为这无法在学习过程中提供足够的梯度。所以我们最大化<em class="lo">log</em>(<em class="lo">D</em>(<em class="lo">τF</em>))来代替。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h2 id="7ffc" class="nj lw iq bd lx nk nl dn mb nm nn dp mf lb no np mh lf nq nr mj lj ns nt ml nu bi translated"><strong class="ak">代表iRL中的鉴别器</strong></h2><p id="2f5d" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">鉴别器<strong class="ku ir"> <em class="lo"> D </em> </strong>是奖励函数<strong class="ku ir"> <em class="lo"> r </em> </strong> <em class="lo"> : </em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/@mugoh/140b685fd5b5"><div class="gh gi oa"><img src="../Images/0d9899dd061d177db8d71ca04dc8e005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*xoT8EclAbrVTvpintpwzKw.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated"><em class="ni">iRL中的鉴别器:指数奖励函数与学习策略的比率</em></p></figure><p id="d048" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><em class="lo">ψ</em>代表奖励函数的可学习参数。作为学习奖励的函数，鉴别器也使用参数<em class="lo">ψ。</em></p><p id="bb08" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">更新鉴别器<em class="lo"> D </em>更新学习的奖励函数<em class="lo"> r(τ) </em>。当鉴别器是最优的，我们得到一个最优的回报函数。然而，<em class="lo"> r(τ) </em>以上的奖励函数在奖励的估计中使用整个轨迹<em class="lo"> τ </em>。与使用单个状态、动作对<em class="lo"> r </em> (s，a)相比，这给出了较高的方差估计，导致较差的学习。</p><p id="8838" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">使用单个状态-动作对将解决高方差估计问题，但是也有一个缺点——它使得最优奖励函数<em class="lo">与最优策略提出的监督动作</em>严重纠缠。换句话说，习得的奖励会鼓励模仿专家的政策，当环境发生变化时，不能产生明智的行为。</p><p id="2874" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这就是我们对鉴频器的最后改进。</p><h2 id="9bd9" class="nj lw iq bd lx nk nl dn mb nm nn dp mf lb no np mh lf nq nr mj lj ns nt ml nu bi translated"><strong class="ak">创建一个解开的奖励函数</strong></h2><p id="3582" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">为了提取从环境中分离出来的奖励，<a class="ae kr" href="https://arxiv.org/pdf/1710.11248" rel="noopener ugc nofollow" target="_blank">对抗性逆RL </a> (AIRL)提出用这种形式修改鉴别器:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/@mugoh/140b685fd5b5"><div class="gh gi ob"><img src="../Images/dce410c8e514b5dedc3d511415ff33bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*QvH8uyyCwOMC99seYqTB-w.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated"><em class="ni">使用单个【状态-行动对】作为输入的鉴别器</em></p></figure><p id="9b3a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们可以进一步简化奖励函数<em class="lo"> r(s，a，s’)</em>为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/@mugoh/140b685fd5b5"><div class="gh gi oc"><img src="../Images/b43084214bc5926256005980282662c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*O4fy6amSGHSDXfV_ESsNzA.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated"><em class="ni"/>对抗性反RL <em class="ni">奖励功能恢复优势</em></p></figure><p id="8a60" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">它现在由以下部分组成:</p><ol class=""><li id="534d" class="mt mu iq ku b kv kw ky kz lb mv lf mw lj mx ln od mz na nb bi translated">g(s，a):估计状态-动作对的回报的函数逼近器。它被表达为只有状态g(s)的函数，以将奖励从环境动态中分离出来</li><li id="7518" class="mt mu iq ku b kv nc ky nd lb ne lf nf lj ng ln od mz na nb bi translated">h(s):控制g(s)上不需要的整形的整形术语</li></ol><p id="e727" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">下面是代码中的鉴别器和奖励函数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="4a74" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">g(s)收回最优报酬；h(s)类似于一个价值函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/c0fd7d7e31b026625af16bb4e31a891f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*zepJOmtdLxbW3_T5YPDgdg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><em class="ni">从学习的奖励函数中恢复优势A(s，A)</em></p></figure><p id="66a0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这意味着r(s，a，s’)恢复了优势。这就是有趣的地方。</p><h2 id="d2d8" class="nj lw iq bd lx nk nl dn mb nm nn dp mf lb no np mh lf nq nr mj lj ns nt ml nu bi translated"><strong class="ak">执行策略更新</strong></h2><p id="96d7" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">策略更新包括找到策略的对数的梯度乘以优势。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/@mugoh/140b685fd5b5"><div class="gh gi of"><img src="../Images/d824bd47ecb61a8753b2097ff77d5c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*NarcI8P5_OztRSZCFDxnxA.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated">政策梯度更新</p></figure><p id="f774" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在iRL中，由于我们没有观察到评估优势时使用的环境奖励，更新期间的更改将使用奖励函数来估算这些优势。更简单地说，由奖励功能恢复的优势在策略更新中找到用途。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/@mugoh/140b685fd5b5"><div class="gh gi og"><img src="../Images/bc46bc1eb92f26897e01d4c8b7f5e1a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*NRnqi7iHv9Bv5PzjAiJzAg.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated"><em class="ni">在iRL中使用优势评估进行政策更新</em></p></figure><p id="93e3" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">为了对iRL有一个直观的概述，这里有一个普通政策梯度(VPG)和应用于VPG的反向RL之间的整个伪代码的并排比较。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/@mugoh/140b685fd5b5"><div class="gh gi oh"><img src="../Images/e04a3e7daf9a8a2594054abcdd513b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yhai0xZ4JwrXuQJtlfEHrg.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated"><em class="ni">普通政策梯度(VPG)比较了VPG和反向RL </em></p></figure><p id="c479" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">策略<strong class="ku ir"> <em class="lo"> π </em> </strong>被训练来最大化这个估计的回报r(s，a，s’)，并且当被更新时，学习收集与专家演示更难区分的轨迹。</p><p id="2d53" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">运行反向RL </strong></p><p id="728e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">反向RL培训的第一步是运行基于策略的RL算法来收集专家演示。我从两个方面着手:</p><ol class=""><li id="0042" class="mt mu iq ku b kv kw ky kz lb mv lf mw lj mx ln od mz na nb bi translated">收集最后n个策略更新的<em class="lo"> </em>轨迹。例如，如果训练250个纪元，收集230–250个(最后20个)。这是AIRL论文中的方法。</li><li id="10c4" class="mt mu iq ku b kv nc ky nd lb ne lf nf lj ng ln od mz na nb bi translated">收集平均剧集奖励高于某个奖励阈值的轨迹</li></ol><p id="1b4c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">上述两种收集专家数据的方法在反向RL平均回报方面似乎没有显著差异(至少在250个时期内)。我对此的解释是,( b)中的奖励阈值收集了(a)中看到的大部分发生在最后训练阶段的轨迹。</p><p id="a830" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这里是在HalfCheetah-v2上的iRL在100个步骤上的平滑性能，使用了来自五个最终时期的专家数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/@mugoh/140b685fd5b5"><div class="gh gi oi"><img src="../Images/0065e61cf5bfd74baca28cc2a8b299e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*FvmDEl-Ay7kmH2FjbsQ8AQ.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated"><em class="ni">iRL策略和使用观察奖励的相同策略的半猎豹奖励比较。</em></p></figure><p id="48e5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">完整的iRL实现和复制细节在<a class="ae kr" href="https://github.com/mugoh/rl-base/tree/master/rlbase/aiRL" rel="noopener ugc nofollow" target="_blank">https://github.com/mugoh/rl-base/tree/master/rlbase/aiRL</a>上。</p><p id="aa8b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">下面是用于收集这些演示的策略的单次运行示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/@mugoh/140b685fd5b5"><div class="gh gi oi"><img src="../Images/a530ceb83946b0097593a551335563ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ieoHbAE_xZkQWuamnfzaiA.png"/></div></a><p class="kn ko gj gh gi kp kq bd b be z dk translated"><em class="ni">收集专家论证</em></p></figure><h2 id="9b9e" class="nj lw iq bd lx nk nl dn mb nm nn dp mf lb no np mh lf nq nr mj lj ns nt ml nu bi translated">结论</h2><p id="e49d" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">反向强化学习允许我们向代理演示期望的行为，并试图使代理从演示中推断出我们的目标。将这个目标与演示结合起来可以恢复奖励功能。然后，收回的奖励鼓励代理人采取与专家试图实现的意图相似的行动。</p><p id="bd0a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">恢复意图的好处是代理学习达到目标的最佳方式——它不会盲目模仿专家的次优行为或错误。因此，与专家策略相比，iRL承诺了更理想的性能。</p><h2 id="aa3e" class="nj lw iq bd lx nk nl dn mb nm nn dp mf lb no np mh lf nq nr mj lj ns nt ml nu bi translated">资源</h2><p id="4eed" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">反向RL知识库:<a class="ae kr" href="https://github.com/mugoh/rl-base/tree/master/rlbase/aiRL" rel="noopener ugc nofollow" target="_blank">Github上的air</a></p><p id="0f6a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">关于反向RL的更多方法，请看这些作品:</p><p id="9569" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[1] C. Finn、P. Christiano、P. Abbeel和S. Levine。<a class="ae kr" href="https://arxiv.org/pdf/1611.03852" rel="noopener ugc nofollow" target="_blank">生成对抗网络、逆向强化学习和基于能量的模型之间的联系</a>，NIPS，2016。</p><p id="7368" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[2] J. Fu，K. Luo，S. Levine，<a class="ae kr" href="https://arxiv.org/pdf/1710.11248" rel="noopener ugc nofollow" target="_blank">用对抗性逆强化学习学习鲁棒报酬</a>，2018。</p><p id="aadd" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[3] X .彭，a .金泽，s .托耶，p .阿贝耳，s .莱文，<a class="ae kr" href="https://arxiv.org/pdf/1810.00821" rel="noopener ugc nofollow" target="_blank">变分鉴别器瓶颈:通过约束信息流改进模仿学习，逆RL，和GANs</a>，，2019。</p></div></div>    
</body>
</html>