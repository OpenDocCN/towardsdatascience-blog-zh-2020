<html>
<head>
<title>The Basic Building Block of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的基本构件</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-basic-building-block-of-neural-networks-a9b2e8f5c056?source=collection_archive---------37-----------------------#2020-10-29">https://towardsdatascience.com/the-basic-building-block-of-neural-networks-a9b2e8f5c056?source=collection_archive---------37-----------------------#2020-10-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="362d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">探索Keras的密集层，直到源代码</h2></div><p id="9228" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">密集层(一个规则的全连接层)可能是最广泛使用和最著名的神经网络层。它是许多神经网络体系结构的基本构件。</p><p id="290c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">了解密集层为进一步探索其他类型的层和更复杂的网络体系结构打下了坚实的基础。让我们深入到密集层，直到实现它的代码。</p><p id="525a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我使用Keras(<a class="ae le" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank">https://keras.io/</a>)来探索层的实现和源代码，但是一般来说，大多数类型的层都非常通用，主要原理并不太依赖于实现它们的实际库。</p><h1 id="7693" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">密集层概述</h1><p id="067b" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">让我们先来看看这种层的可视化表示:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/cda38668171f2b606c00fd0a97384163.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*guIlDajMnLgAk6DJICI2Cw.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">密集图层表示(由作者创建)</p></figure><p id="10ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本例中，密集层有3个输入、2个单元(和输出)和一个偏置。让我们来看看每一个。</p><p id="c7c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">层输入在这里由x1、x2、x3表示。这就是数据的来源-这些数据可以是输入要素值，也可以是前一图层的输出。从技术上讲，这些可以是任何数值，但在大多数情况下，输入值将被规范化为[-1，1]的区间。可以手动或使用特殊图层(例如Keras中的BatchNormalization图层)进行归一化。</p><p id="6f4c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大圆圈代表单位。这是输入值被转换成输出的地方。在可视化中，输出由y1和y2表示。输出的数量总是与单位的数量相匹配。密集层是完全连接的，这意味着它将每个输入连接到每个输出。这自然意味着每个输入值都会影响(或者至少能够影响-如果相应的权重值不为零)每个输出值。</p><p id="c604" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从输入到输出的转换由激活函数定义。该函数应用于输入值x1、x2、x3，以获得输出值y。该函数的结果受权重的影响，权重在可视化中表示为w11、w21、w31。权重赋予模型学习的能力-基本上，神经网络模型学习的是所有层中的权重值。</p><p id="f526" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可视化中还有一件事——红色数字“1”与所有单元连接，代表“偏差”。偏差指定了对输出值的一些外部影响，这些影响未被输入中提供的特征所覆盖。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ms"><img src="../Images/3b1b5f6ce84580969f46f6632a098c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*woi70f7cg_qpGlMCvuXVzg.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">Keras标志(来自keras.io)</p></figure><h1 id="488f" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">喀拉斯的致密层</h1><p id="6d4e" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">现在，让我们尝试将Keras中的密集层实现与我们的可视化结合起来。在Keras中，致密层定义如下:</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="3064" class="my lg it mu b gy mz na l nb nc">tf.keras.layers.Dense(<br/>    units,<br/>    activation=None,<br/>    use_bias=True,<br/>    kernel_initializer="glorot_uniform",<br/>    bias_initializer="zeros",<br/>    kernel_regularizer=None,<br/>    bias_regularizer=None,<br/>    activity_regularizer=None,<br/>    kernel_constraint=None,<br/>    bias_constraint=None,<br/>    **kwargs<br/>)</span></pre><p id="6371" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从数学的角度来看，密集层实现了特征值和权值之间的矩阵乘法。</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="7ec8" class="my lg it mu b gy mz na l nb nc">outputs = mat_mul(inputs, weights)</span></pre><p id="2a3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我不会在这里解释矩阵乘法背后的数学，我只提供一个例子，第一个输出基值是什么样子的:</p><p id="1bbb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">T3】y1 = x1 * w11+x2 * w21+x3 * w31T5】</strong></p><p id="ffc1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，实际上，这不是最终的输出值，因为它可以通过激活函数和/或偏置向量进一步改变，我们将在本文稍后讨论。</p><p id="3ff3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们通过Keras密集层的参数来了解它们的含义以及它们如何影响该层。</p><h2 id="1974" class="my lg it bd lh ne nf dn ll ng nh dp lp kr ni nj lr kv nk nl lt kz nm nn lv no bi translated">单位</h2><p id="b967" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">第一个(也是唯一必须的)参数是“单位”。</p><ul class=""><li id="80a7" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated"><strong class="kk iu">单位</strong>:正整数，输出空间的维数。</li></ul><p id="2a37" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“单位”指定层的单位数(和输出数)。在我们的可视化示例中<strong class="kk iu"> <em class="nd">单位=2 </em> </strong>。请注意，Keras密集图层没有用于指定输入数量的参数。在Keras中，输入的数量由前一NN层的输出数量定义。</p><p id="aa31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实际上，单元的数量定义了神经网络的“宽度”。这是为某些特定任务创建神经网络架构时需要调整的一个重要参数。密集层中的许多单元可能导致过度拟合，尤其是如果网络也很深(具有许多层)。另一方面，太少的单元会导致学习模式的能力有限。通常，使用的单元越多，应用的正则化就越多。</p><h2 id="58be" class="my lg it bd lh ne nf dn ll ng nh dp lp kr ni nj lr kv nk nl lt kz nm nn lv no bi translated">激活功能</h2><p id="f370" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">参数“activation”是可选的，它定义了激活函数。</p><ul class=""><li id="4696" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated"><strong class="kk iu">激活</strong>:要使用的激活功能。如果你没有指定任何东西，没有激活应用(即。“线性”激活:<code class="fe ny nz oa mu b">a(x) = x</code>)。</li></ul><p id="1d1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们查看Keras库的源代码，激活函数就在返回输出之前应用于输出。</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="03fa" class="my lg it mu b gy mz na l nb nc">if activation is not None:<br/>    outputs = activation(outputs)</span></pre><p id="b4bf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果未指定激活功能(默认值为<code class="fe ny nz oa mu b">None</code>，输出按原样返回。实际上，通常你需要一些激活函数来使神经网络预测合理的值。有许多可能的激活功能，具有不同的特征。对这些的概述超出了本文的范围。</p><h2 id="5281" class="my lg it bd lh ne nf dn ll ng nh dp lp kr ni nj lr kv nk nl lt kz nm nn lv no bi translated">偏见</h2><p id="a3e8" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">正如我们从图层定义中看到的，在Keras中默认使用偏移，但可以选择关闭。</p><ul class=""><li id="4276" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated"><strong class="kk iu"> use_bias </strong>:布尔型，该层是否使用偏置矢量。</li></ul><p id="8693" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的视觉化中，偏见用红色表示。有几种方法可以将它形象化——我喜欢把它想象成一个常量值为“1”的特征。注意，仍然为偏差向量的所有单元计算和优化权重，但是偏差向量对输出的影响不受真实特征值的影响。</p><p id="5055" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了更好地理解为什么需要偏差，让我们考虑一些特征值都为“0”的输入样本。如果没有偏差向量，所有输出也将始终为“0 ”,因为无论权重值是多少，当乘以“0”特征值时，它都将为零。因此，如果没有偏差，图层将无法正确响应“0”特征值。</p><p id="4ca5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实际上，大多数情况下，您不需要移除偏置向量。</p><p id="4a5e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们查看Keras实现中关于密集层使用<code class="fe ny nz oa mu b">use_bias</code>参数的代码，我们会发现以下内容(为了更好的可读性，跳过了一些代码部分):</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="3c5e" class="my lg it mu b gy mz na l nb nc">if use_bias:<br/>    bias = self.add_weight(<br/>        'bias',<br/>        shape=[self.units,],<br/>        ...<br/>        trainable=True)<br/>else:<br/>    bias = None</span><span id="b1ff" class="my lg it mu b gy ob na l nb nc">if bias is not None:<br/>    outputs = nn_ops.bias_add(outputs, bias)</span></pre><p id="5fa0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，如果我们使用偏差，那么偏差权重会添加到输出值中。请注意，这发生在应用激活函数之前，因此伪代码中的整个公式如下所示:</p><blockquote class="oc"><p id="767b" class="od oe it bd of og oh oi oj ok ol ld dk translated">输出=激活(点(输入，内核)+偏置)</p></blockquote><h2 id="0110" class="my lg it bd lh ne om dn ll ng on dp lp kr oo nj lr kv op nl lt kz oq nn lv no bi translated">初始值设定项</h2><p id="989a" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">密集层有两个初始化器参数——内核(权重)和偏向初始化器。</p><ul class=""><li id="e42c" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated"><strong class="kk iu">内核初始化器</strong>:权重矩阵<code class="fe ny nz oa mu b">kernel</code>的初始化器。</li><li id="8bc1" class="np nq it kk b kl or ko os kr ot kv ou kz ov ld nu nv nw nx bi translated"><strong class="kk iu"> bias_initializer </strong>:偏置向量的初始化器。</li></ul><p id="1b28" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简而言之，这些定义了所有权重的初始值，包括特征权重和偏置向量权重。</p><p id="3287" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型将在训练过程中改变层权重。但是，如果合理地选择初始权重，模型可以更快地收敛到最优值。</p><p id="e805" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">默认情况下，要素权重将随机初始化为均匀分布。偏置权重将用零初始化。</p><h2 id="6825" class="my lg it bd lh ne nf dn ll ng nh dp lp kr ni nj lr kv nk nl lt kz nm nn lv no bi translated">正则化子</h2><p id="b71a" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">密集层有3个正则化参数:</p><ul class=""><li id="622b" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated"><strong class="kk iu">kernel _ regulator</strong>:应用于<code class="fe ny nz oa mu b">kernel</code>权重矩阵的正则化函数。</li><li id="cd21" class="np nq it kk b kl or ko os kr ot kv ou kz ov ld nu nv nw nx bi translated"><strong class="kk iu">bias _ regulator</strong>:应用于偏置向量的正则化函数。</li><li id="e2f2" class="np nq it kk b kl or ko os kr ot kv ou kz ov ld nu nv nw nx bi translated"><strong class="kk iu">activity _ regulator</strong>:应用于层输出的正则化函数(其“激活”)。</li></ul><p id="1843" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正则化的主要任务是控制这些值，使它们尽可能低。受控制的值取决于正则化类型。</p><p id="0709" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">核正则化降低了权重的平均值。为什么避免巨大的重量值很重要？简单地说——使用的权重越大，模型在为它们选择不同值时的自由度就越大。这增加了模型方差，因此过度拟合的风险增加。</p><p id="0f97" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们谈论技术上正则化是如何工作的——它们增加了额外的损失，这取决于权重的平均值。权重越大，损失越大——这反过来会迫使模型减少权重。</p><p id="4f15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，偏差正则化控制偏差的权重。并且活动正则化器控制输出值。</p><h2 id="fd7f" class="my lg it bd lh ne nf dn ll ng nh dp lp kr ni nj lr kv nk nl lt kz nm nn lv no bi translated">限制</h2><p id="987a" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">定义约束函数的密集层有两个参数:</p><ul class=""><li id="f465" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated"><strong class="kk iu"> kernel_constraint </strong>:应用于<code class="fe ny nz oa mu b">kernel</code>权重矩阵的约束函数。</li><li id="8abf" class="np nq it kk b kl or ko os kr ot kv ou kz ov ld nu nv nw nx bi translated"><strong class="kk iu"> bias_constraint </strong>:应用于偏置向量的约束函数。</li></ul><p id="f140" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些函数类似于正则化函数，因为它们也控制权重。区别在于实现控制的方式。如果正则化引入了大权重的惩罚和附加损失，则约束直接限制权重的值，例如通过定义最大值。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ow"><img src="../Images/37c4069990babe804ba9b56b3778b04a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_vooDvwQv95kXHVy"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@robynnexy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Robynne Hu </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="f668" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">结论</h1><p id="ce2b" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">如果您刚刚开始学习神经网络，了解密集层是一个非常好的起点。</p><p id="29f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当然，理解神经网络背后的一般思想和数学是非常重要的，但要成功地设计有效的架构，您需要大量的实践和试错方法。那么为什么不现在就开始用Keras致密层做一些自己的实验呢？</p><p id="1f91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读！</p></div></div>    
</body>
</html>