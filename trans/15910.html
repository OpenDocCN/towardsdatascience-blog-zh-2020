<html>
<head>
<title>Gain Trust in Your Model and Generate Explanations With LIME and SHAP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">获得对你的模型的信任，用石灰和SHAP做出解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gain-trust-in-your-model-and-generate-explanations-with-lime-and-shap-94288694c154?source=collection_archive---------23-----------------------#2020-11-02">https://towardsdatascience.com/gain-trust-in-your-model-and-generate-explanations-with-lime-and-shap-94288694c154?source=collection_archive---------23-----------------------#2020-11-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="94c3" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/model-interpretability" rel="noopener" target="_blank">模型可解释性</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/5ef20158f033b297c1dc116614066731.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SjvONvsbBfRXWmUG"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">照片由<a class="ae kl" href="https://unsplash.com/@noguidebook?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">瑞秋</a>在<a class="ae kl" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d879" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">使用机器学习来自动化流程在全球许多组织中被广泛采用。作为戴尔数据科学工厂团队的一员，我们与不同的业务部门合作，提供基于数据的创新解决方案来优化现有流程。</p><p id="75ad" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在我们与定价业务部门的一次合作中，我们专注于改进导致许多延迟的手动密集型流程。手动流程包括审计销售代表提交的交易(其中一些产品的价格低于某一点)，并根据交易的特点决定是批准还是拒绝交易(您可以在此处阅读更多信息<a class="ae kl" href="https://www.cio.com/article/3563414/leading-pricing-decisions-into-the-future-with-ai.html" rel="noopener ugc nofollow" target="_blank"/>)。如果交易被拒绝，定价分析师应向销售代表提供解释。提供解释可以帮助销售代表调整交易，使其获得批准，这对双方都有利。</p><p id="a14a" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由于这非常耗时，并可能导致失去机会，我们希望通过减少需要手动审查的交易量来优化它。为了实现这一点，我们使用机器学习来自动批准和自动拒绝较简单的案例，这使得分析师可以专注于更复杂的交易，从而产生更多价值。</p><p id="f377" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了保持相同的质量水平，并在用户之间建立信任(这在实现机器学习模型时非常关键)，我们必须了解决策是如何做出的。考虑到这一点，我寻找现有的工具来支持我们正在进行的转型，我遇到了莱姆和SHAP。在整个过程中，我检查了两种方法的数据，以了解哪种方法更适合我，我获得了很多知识，我想分享。在这篇博文中，我将分享我所学到的东西，并分享一些帮助我更好地理解每个工具如何工作的细节。</p><p id="261e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这篇博文将涵盖以下内容:</p><p id="2c8a" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">1.SHAP和莱姆-背景</p><p id="ad3c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2.例子</p><p id="b8ac" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">3.摘要</p><p id="86bc" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">4.进一步阅读</p><h1 id="fac0" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">1.SHAP和莱姆-背景</h1><p id="05c3" class="pw-post-body-paragraph km kn iq ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated">LIME(局部可解释模型不可知解释)提供局部解释；它可以帮助解释为什么单个数据点被归类为特定的类。LIME将模型视为黑盒；它不区分随机森林、决策树或神经网络。它使用线性模型来提供局部解释。围绕LIME的主要概念是，即使线性模型不能很好地解释复杂的数据/模型，但当试图解释局部观察时，它可以提供足够的解释。下面你可以看到一个解释石灰直觉的图。这是摘自论文:<a class="ae kl" href="https://arxiv.org/abs/1602.04938" rel="noopener ugc nofollow" target="_blank">“我为什么要相信你？”解释任何分类器的预测。</a></p><p id="51c6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">LIME提供了三种不同的解释器:表格解释器(将重点介绍)、图像解释器和文本解释器来处理不同类型的数据。</p><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/a2d47e50a79b58e536fae7a5f7c4d9d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*3miawSbi8YO-tbCIxthEVw.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">从上面提到的论文中，有一个例子证明了关于石灰的直觉</p></figure><p id="5847" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">SHAP(SHapley Additive explaints)旨在通过计算每个特征对预测的贡献来解释单个数据点的预测。生成的解释是基于从联盟博弈论中计算Shapley值(你可以在这里<a class="ae kl" href="https://en.wikipedia.org/wiki/Shapley_value" rel="noopener ugc nofollow" target="_blank">阅读更多信息</a>)。它基于下面的场景:假设你作为团队的一员玩某个游戏，团队赢了，结果赚了一些钱(支出)。在玩家之间分配奖金的最佳方式是什么，以最好地反映每个玩家对游戏的贡献？在我们的案例中，我们希望了解如何在模型中的不同特征(玩家)之间划分预测(支出)，以最好地反映每个特征所做的贡献。</p><p id="6ab5" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一般来说，要计算某个特征对预测的贡献，应该获取不包含该特征的所有子集，并计算添加该特征前后的预测之间的差异，然后对不同的子集进行平均。</p><p id="ad81" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所描述的过程可能计算量很大，而且运行一个没有某个特征的模型会创建一个全新的模型，这不是我们想要解释/理解的(你可以在Adi Watzman的<a class="ae kl" href="https://www.youtube.com/watch?v=0yXtdkIL3Xk" rel="noopener ugc nofollow" target="_blank">这个</a>精彩讲座中听到更多)。幸运的是，SHAP使用不同的近似和优化方法克服了这些困难。</p><p id="f3f4" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">SHAP提供了三种不同的解释器:KernalSHAP，它与LIME类似，是模型不可知的，TreeSHAP是针对基于树的模型优化的，DeepSHAP是深度学习模型中SHAP值的高速近似算法。</p><p id="8642" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这篇文章中，我将重点介绍树解释器，因为我们在项目中使用了基于树的模型。</p><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ms"><img src="../Images/3d0d816e10ed1731772104d33c8c5506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZAE6vnBx-EARejNal26pjw.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">一个展示SHAP输入和输出的例子，摘自SHAP的GitHub页面</p></figure><h1 id="2fa1" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">2.例子</h1><p id="ae09" class="pw-post-body-paragraph km kn iq ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated">继续看一些基于我们使用案例的例子，使用石灰和SHAP。</p><h2 id="8732" class="mt ll iq bd lm mu mv dn lq mw mx dp lu kx my mz ly lb na nb mc lf nc nd mg iw bi translated">石灰</h2><p id="4f1f" class="pw-post-body-paragraph km kn iq ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated">为了提供一个解释，首先我们必须创建一个解释器。解释者应该得到以下信息:</p><ul class=""><li id="e568" class="ne nf iq ko b kp kq kt ku kx ng lb nh lf ni lj nj nk nl nm bi translated">我们应该指定我们是在处理回归模型还是分类模型。</li><li id="7c68" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">我们应该传递我们的训练数据、特性名和类名(这是可选的，默认为0和1)。</li><li id="12fb" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">Verbose-表示我们是否希望在解释中提供更多的细节。</li><li id="c69a" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">因为在这个过程中有一些随机性，为了能够重现结果，您还应该设置random_state变量。</li></ul><pre class="mo mp mq mr gt ns nt nu nv aw nw bi"><span id="3b78" class="mt ll iq nt b gy nx ny l nz oa">from lime.lime_tabular import LimeTabularExplainer</span><span id="7134" class="mt ll iq nt b gy ob ny l nz oa">lime_explainer = LimeTabularExplainer(train_data.values, <br/>mode = ’classification’, feature_names = new_feature_names, class_names = [‘Deny’, ’Approve’], verbose=True, <br/>random_state = 42)</span></pre><p id="e583" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">关于分类特征的补充说明</strong> -如果你的模型包含分类变量，事情会变得复杂一些。这里有一个简短的解释:为了提供解释，LIME使用原始数据集对我们想要解释的观察周围的数据点进行采样。如果我们提供的数据中的分类变量采用统一的编码格式，那么采样过程可能会创建不太可能出现在数据中的样本，并且生成的解释不会代表实际数据。为了克服这一点，分类特征应该被转换成整数标签，为了了解更多关于如何使用带有分类特征的LIME，我鼓励你观看凯文·勒芒恩的<a class="ae kl" href="https://www.youtube.com/watch?v=C80SQe16Rao&amp;t=2355s" rel="noopener ugc nofollow" target="_blank">演讲</a>，或者访问他的<a class="ae kl" href="https://github.com/klemag" rel="noopener ugc nofollow" target="_blank"> GitHub </a>页面。</p><p id="cba3" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">解释器设置好之后，让我们生成一个解释。要获得解释，您应该提供以下信息:</p><ul class=""><li id="c276" class="ne nf iq ko b kp kq kt ku kx ng lb nh lf ni lj nj nk nl nm bi translated">你想解释的例子。</li><li id="0a4d" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">生成概率的函数(在分类模型的情况下)，或者预测数据集的函数(在回归模型的情况下)(在我们的情况下是predict_proba)。</li><li id="2636" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">您还可以指定用于构建局部线性模型的最大要素数，目前我们使用默认值10。</li></ul><pre class="mo mp mq mr gt ns nt nu nv aw nw bi"><span id="d7b6" class="mt ll iq nt b gy nx ny l nz oa">exp = lime_explainer.explain_instance(instance, trained_model.predict_proba)</span><span id="61fc" class="mt ll iq nt b gy ob ny l nz oa">exp.show_in_notebook(show_table=True)</span></pre><p id="caa4" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">接下来，让我们检查输出:</p><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oc"><img src="../Images/cb7686e3d28cae319b93d0efbd8813e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YZ9A1Hf8qNIEi6XU5Wxz3A.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">作者图片</p></figure><p id="0a25" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在顶部，呈现了由LIME创建的线性模型的截距，随后是由线性模型生成的局部预测，以及来自我们的模型的实际预测(这是将explainer中的verbose设置为True的结果)。如您所见，线性模型生成的预测和我们的模型生成的结果非常接近。接下来，您可以看到每个类别的概率，就像原始模型预测的那样。</p><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi od"><img src="../Images/b3fcc5dbb3c6b28ed15bedd2796a5afe.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*EN1NnvPbsStsq7q09CcsMQ.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">作者图片</p></figure><p id="fc8b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在最右边，您可以看到特定观察的每个特性的值。这些特征根据其所属的类别进行颜色编码；橙色的特征促成了交易的批准，而蓝色的特征促成了交易的拒绝。此外，它们按对预测的影响进行排序，影响最大的要素位于顶部。在中间的图表中，您还可以看到每个类别中每个特征的大小。理解这一点也很重要，为了提供更直观的解释，数值被离散化成几组，这就是为什么在中间的图表中，特征沿着一定的范围提供。这是LIME中的默认值，可以通过将变量dicretize_continuous设置为false来更改。</p><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oe"><img src="../Images/6d2200d35c8c2531776bb60e57202786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-RZxuxMZYjFgn7YJWv_o0g.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">作者图片</p></figure><p id="2578" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">您可以通过运行以下代码获得有关生成的本地模型的更多信息:</p><pre class="mo mp mq mr gt ns nt nu nv aw nw bi"><span id="c271" class="mt ll iq nt b gy nx ny l nz oa">print(‘R²: ‘+str(exp.score))</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi of"><img src="../Images/848508ae9d783709fc611eebc5cb744f.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*F_JXzugDkFxyxYUgWw_IEQ.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">作者图片</p></figure><p id="4ea8" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这会给我们本地模型的R平方，我们可以用分数来决定我们是否可以相信某个解释。</p><p id="fc2b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，您可以使用以下代码访问模型中每个特性的权重:</p><pre class="mo mp mq mr gt ns nt nu nv aw nw bi"><span id="0ebc" class="mt ll iq nt b gy nx ny l nz oa">exp.local_exp</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi og"><img src="../Images/74f937fad51058aaf696c1711b9c782d.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*pEjtW-cP72A8j5uoQd1ztg.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">作者图片</p></figure><p id="83cf" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对权重和截距求和将导致由LIME生成的局部预测。</p><pre class="mo mp mq mr gt ns nt nu nv aw nw bi"><span id="76c3" class="mt ll iq nt b gy nx ny l nz oa">exp.intercept[1] + sum([weight[1] for weight in exp.local_exp[1]])</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/620751daebab1ad0178a1a007d6f3565.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*iv5w81BsXHzlVypTN1xmYQ.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">作者图片</p></figure><h2 id="3bda" class="mt ll iq bd lm mu mv dn lq mw mx dp lu kx my mz ly lb na nb mc lf nc nd mg iw bi translated">SHAP</h2><p id="6174" class="pw-post-body-paragraph km kn iq ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated">转到TreeSHAP，让我们为我们用于LIME的同一个实例生成一个解释。</p><p id="6710" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先，与LIME类似，我们应该导入SHAP，创建解释器并传入训练好的模型:</p><pre class="mo mp mq mr gt ns nt nu nv aw nw bi"><span id="4e7e" class="mt ll iq nt b gy nx ny l nz oa">import shap<br/>shap.initjs() #This is for us to be able to see the visualizations <br/>explainer = shap.TreeExplainer(trained_model)</span></pre><p id="1f0b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">接下来，让我们生成一个解释并将其可视化。第一行计算我们提供的实例的SHAP值，下一行生成一个图，该图将显示每个特征对最终预测的贡献。SHAP所做的计算是针对<strong class="ko ja">两个类别进行的，这里您应该使用索引</strong>选择您想要引用的类别(在回归模型中不需要)，在这种情况下，我想探究批准类别(类别1)的结果。</p><p id="8123" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了计算我们想要解释的实例的SHAP值，您应该提供实例本身，并且为了生成绘图，您应该提供以下内容:</p><ul class=""><li id="9537" class="ne nf iq ko b kp kq kt ku kx ng lb nh lf ni lj nj nk nl nm bi translated">解释器生成的期望值，本质上是训练集预测的平均值，也称为基值。</li><li id="574e" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">接下来，您应该提供在前一行中计算的SHAP值(这些是每个要素对模型预测的贡献)。</li><li id="946e" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">您要解释的实例和功能名称。</li></ul><pre class="mo mp mq mr gt ns nt nu nv aw nw bi"><span id="32ca" class="mt ll iq nt b gy nx ny l nz oa">shap_values = explainer.shap_values(instance)</span><span id="0ff0" class="mt ll iq nt b gy ob ny l nz oa">shap.force_plot(base_value=explainer.expected_value[1], shap_values[1], features=instance, feature_names=new_feature_names)</span></pre><p id="372c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下图中的粗体数字是我们的预测。在左侧，您可以看到基值，如前所述，它是我们训练集中预测的平均值。顾名思义，这是我们预测的起始值，将模型中各要素的贡献相加，将得到我们的最终预测。</p><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/40041a2edf359cc39e288009f21036d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*aeZ0vMcN4dlKs0MmMY2M9w.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">作者图片</p></figure><p id="962b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">特征以这样的方式排列，使得促成交易批准(类别1)的所有特征在左边，并且用红色着色，并且促成交易拒绝(类别0)的所有特征在右边，用蓝色着色。请注意，每一侧的要素都是按大小排序的，这在每个要素所占的空间中也很明显。您可能还会注意到，在图表的边缘，有一些附加要素也对不同的类有所贡献，但它们的贡献远远小于所显示的那些，您可以将鼠标悬停在图中的边缘，您将能够看到这些要素的名称及其值。</p><p id="adde" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">TreeSHAP利用树的结构，可以计算精确值而不是近似值。将预期/基础值相加将生成准确的预测，如下所示。</p><pre class="mo mp mq mr gt ns nt nu nv aw nw bi"><span id="faa8" class="mt ll iq nt b gy nx ny l nz oa">explainer.expected_value[1] + sum(shap_values[1])</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/13d4d676117130eba48d71f5ece30823.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*HyPz_GWW1PJIBNB8om7SRA.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">作者图片</p></figure><p id="667f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">SHAP还提供了不同的可视化效果，可以根据您选择的类为您提供每个功能行为的总体概述。为此，首先您应该计算整个测试数据的SHAP值(注意这可能需要一些时间)，让我们看一个例子:</p><pre class="mo mp mq mr gt ns nt nu nv aw nw bi"><span id="9634" class="mt ll iq nt b gy nx ny l nz oa">test_shap_values = explainer.shap_values(test_data.values)<br/>shap.summary_plot(shap_values[1], test_data.values)</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/ff9acf884ee318bbcba20024506b78c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*4nhDnnXFX-qfsGr0u7lDDw.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">作者图片</p></figure><p id="182d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们检查图表的不同方面:</p><ul class=""><li id="b983" class="ne nf iq ko b kp kq kt ku kx ng lb nh lf ni lj nj nk nl nm bi translated">x轴代表对预测的贡献，这些是SHAP值。</li><li id="d6ce" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">左侧的y轴显示影响最大的特征(您可以指定想要显示的特征数量-默认值为20)。这些特征按其对预测的影响进行排序，其中影响最大的位于顶部。</li><li id="d527" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">右侧的y轴表示如何解释每个特征的原始值。在每一行中，数据点用蓝色或红色着色，蓝色表示该特征的低值，红色表示该特征的高值。</li><li id="2243" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">该图由一条水平线分隔，在图的右侧，您可以看到促成交易批准的数据点，在图的左侧，您可以看到促成交易否决的数据点。</li><li id="35ef" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj nj nk nl nm bi translated">例如，特征x_29中的低值将有助于批准类，而高值将有助于拒绝类。</li></ul><p id="97db" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这可以帮助您了解功能是否像SME(主题专家)期望的那样运行，并且可以帮助我们找到以前没有注意到的与数据相关的问题。</p><p id="9505" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在SHAP还有其他的全球可视化选项，查看作者的GitHub页面可以看到不同的选项。</p><h1 id="298a" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">3.摘要</h1><p id="4bd7" class="pw-post-body-paragraph km kn iq ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated">在这篇博文中，我与你分享了我使用工具来解释ML模型的动机，我们回顾了莱姆和SHAP的基础知识，并给出了基于我们用例的例子。</p><p id="dece" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">回顾我们所做的工作，我觉得这些工具的使用在我们的业务合作伙伴之间产生了信任，这使得我们构建的模型的实施更加顺利。此外，使用这些工具确实帮助我们发现了一些与数据相关的问题，并最终得以解决。</p><p id="2b0d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">莱姆和SHAP都是很好的工具，你可以用来建立信任，也可以用来解释你所做的某个决定。当我们在相同的情况下检查两者时，我们注意到大多数影响变量在石灰和SHAP中几乎是相同的。但是，两者之间的顺序有点不同。</p><p id="45fa" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">LIME的主要优点是速度更快(至少对于表格数据来说是这样)，而且它的工作方式非常直观。然而，生成的解释有时可能不可靠，为了使用它，需要做一些预处理。SHAP易于使用，还可以提供要素的总体视图，以及要素值对模型预测的影响。此外，TreeSHAP利用了树的结构，并计算精确的值，而不是近似值，但是在向业务合作伙伴解释时不太直观。</p><p id="d5f2" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果你想了解更多关于这两个工具的知识，我鼓励你看看这篇文章中嵌入的链接，以及下面的链接。</p><h1 id="cdb9" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">4.进一步阅读</h1><p id="1022" class="pw-post-body-paragraph km kn iq ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated">以下是一些额外的阅读推荐:</p><ol class=""><li id="d185" class="ne nf iq ko b kp kq kt ku kx ng lb nh lf ni lj ol nk nl nm bi translated"><a class="ae kl" rel="noopener" target="_blank" href="/explain-your-model-with-the-shap-values-bc36aac4de3d">用SHAP价值观解释你的模型</a>——这是一篇很棒的博客文章，用一个数字例子来解释SHAP。</li><li id="e770" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj ol nk nl nm bi translated"><a class="ae kl" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">一个让黑盒模型变得可解释的指南</a> -提供了可解释模型、石灰、SHAP等等的全面补偿！</li><li id="87b8" class="ne nf iq ko b kp nn kt no kx np lb nq lf nr lj ol nk nl nm bi translated"><a class="ae kl" href="https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/" rel="noopener ugc nofollow" target="_blank">本地可解释的模型不可知解释(LIME):介绍</a>——LIME作者写的博客文章。</li></ol><p id="d87b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">特别感谢Or Herman-Saffar和Idan Richman-Goshen对这篇博文的评论和宝贵反馈。</p></div></div>    
</body>
</html>