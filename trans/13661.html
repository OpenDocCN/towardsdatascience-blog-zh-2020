<html>
<head>
<title>Convolution Neural Network Maths Intuition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络数学直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/convolution-neural-network-maths-intuition-6b047cb48e90?source=collection_archive---------29-----------------------#2020-09-19">https://towardsdatascience.com/convolution-neural-network-maths-intuition-6b047cb48e90?source=collection_archive---------29-----------------------#2020-09-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="1097" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我花了很长时间才明白CNN是如何运作的。相信我，这上面的内容少得令人难以置信，真的很少。无论在哪里，他们都会告诉你在CNN中前向传播是如何工作的，但从来没有开始向后传播。不了解全貌，一个人的理解总是半吊子。</p><h1 id="7665" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">先决条件</h1><ol class=""><li id="fc08" class="lm ln it js b jt lo jx lp kb lq kf lr kj ls kn lt lu lv lw bi translated">应熟悉CNN的基础知识——卷积层、最大池、全连接层。做一个基本的谷歌搜索，理解这些概念。应该需要一个小时左右才能开始。</li><li id="64db" class="lm ln it js b jt lx jx ly kb lz kf ma kj mb kn lt lu lv lw bi translated">微分学——应该知道链式法则是如何工作的，以及微分的基本规则。</li><li id="4cd4" class="lm ln it js b jt lx jx ly kb lz kf ma kj mb kn lt lu lv lw bi translated">应该知道反向传播数学在人工神经网络中的实际工作原理。如果你不知道的话，我强烈推荐你阅读我之前关于这个的文章。</li></ol><h2 id="d793" class="md kp it bd kq me mf dn ku mg mh dp ky kb mi mj lc kf mk ml lg kj mm mn lk mo bi translated"><strong class="ak">本文性质</strong></h2><p id="e0e7" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">所以，我对其余文章的主要问题是——它们没有提到整体流程。每一层和概念都得到了很好的解释，但是反向传播是如何跨层工作的——这方面的信息缺失了。所以，对我来说，很难想象错误是如何从整体上回流的。因此，这篇文章将采取CNN的几个场景，并试图让你了解整个流程。</p><p id="96a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">目的不是覆盖深度，而是覆盖广度和整体流程。至于深度，我会在需要的地方给你指出相关的文章，以帮助你有更深的直觉。把这篇文章当作CNN数学的索引。只是为了清楚地设定期望，这不会是一个5分钟的阅读。但是，我会要求你在需要的时候阅读相关的文章。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="3c53" class="ko kp it bd kq kr mz kt ku kv na kx ky kz nb lb lc ld nc lf lg lh nd lj lk ll bi translated">场景1: 1个卷积层+ 1个全连接层</h1><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ne"><img src="../Images/78781ce8dd835af46c1c2f31b2ab59fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Bo-_wJ_S6QKQUHZD7cKQA.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">作者图片</p></figure><h2 id="5483" class="md kp it bd kq me mf dn ku mg mh dp ky kb mi mj lc kf mk ml lg kj mm mn lk mo bi translated">前进传球</h2><p id="52c6" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">x是输入图像，比如说(3*3矩阵)，滤波器是a (2*2矩阵)。两者都将被卷积以给出输出XX (2*2矩阵)。</p><p id="93a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，XX将被拉平，并馈入一个以w (1*4矩阵)为权重的全连通网络，该网络将给出一个—输出。</p><p id="bdc2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们将通过计算Y(预期)和输出(实际)之间的均方误差来计算误差。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nu"><img src="../Images/d522aec1527e699898a12fd689e1e30f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*grd5BqUyWCC1hWP_KePE4w.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">作者图片</p></figure><p id="ce68" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我强烈建议你自己计算XX。这会给你一个卷积层的直觉。</p><div class="nv nw gp gr nx ny"><a href="https://www.datadriveninvestor.com/2020/01/13/the-future-of-humanity-is-genetic-engineering-and-neural-implants/" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">人类的未来是基因工程和神经移植|数据驱动投资者</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">领先的技术、音乐和电影节将于2020年3月13日至22日举行。它将以前沿的谈话为特色…</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om no ny"/></div></div></a></div><h2 id="2eed" class="md kp it bd kq me mf dn ku mg mh dp ky kb mi mj lc kf mk ml lg kj mm mn lk mo bi translated">偶数道次</h2><p id="76f2" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated"><em class="on">向后传递的目标是选择</em> <strong class="js iu"> <em class="on">滤镜</em></strong><em class="on"/><strong class="js iu"><em class="on">w</em></strong><em class="on">这样我们就可以减少</em> <strong class="js iu"> <em class="on"> E </em> </strong> <em class="on">了。基本上，我们的目标是如何改变w和滤波器，使E降低。</em></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/7ff7d404e2d9ae1dcd0fe53661a03479.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*TVcoZTsdoz5AkIdz0TSrRg.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">作者图片</p></figure><p id="5f9c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="on">让我们从第一个学期开始。</em> </strong></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi op"><img src="../Images/72a8060fe1fb977b2849364d847b00a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*gXL2H4RrSf2Ut96aW_ByFw.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">作者图片</p></figure><p id="89fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">猜猜怎么回事？！</p><p id="c1cd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第1行:使用链式法则。<br/>第二行:运用微分学。花一分钟在这上面。应该很容易理解。如果不是查我以前的<a class="ae mc" href="https://medium.com/datadriveninvestor/neural-network-maths-in-5-minutes-f385eeddf783" rel="noopener">文章</a>(前提中提到的)。或者检查这个<a class="ae mc" href="https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c" rel="noopener">一个</a>。</p><p id="d057" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在下一步之前，一定要确保你自己做了这些计算。如果这不容易理解，请评论。</p><p id="81d7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="on">移到第二学期。</em> </strong></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oq"><img src="../Images/ab52c4e7c41d093831e5e4d0168c70c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p5TJ9My4tTU6ljuAT7o0TQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">作者图片</p></figure><p id="68cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">不在办公室/假面骑士..这里有太多奇怪的逻辑吗？跟着我，会帮助你理解这一点。</p><p id="d636" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第1行:基本链规则<br/>第2行:第一项和第三项位于上述计算本身的线上。再次花一分钟或在纸上做它来理解这一点。</p><p id="77cf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，这个<strong class="js iu">转w </strong>到底是什么鬼！？我花了很长时间才明白这是怎么计算出来的。为此，您只需按照提到的顺序浏览这些概念。</p><ol class=""><li id="82db" class="lm ln it js b jt ju jx jy kb or kf os kj ot kn lt lu lv lw bi translated">转置卷积—输出为[1*1矩阵]，XX为[1*4矩阵(因为这里被扁平化)]，对。所以，当我们反向传播时，我们增加了矩阵的大小。<a class="ae mc" href="https://www.youtube.com/watch?v=96_oGE8WyPg" rel="noopener ugc nofollow" target="_blank">转置卷积有助于此。</a>快进视频，看看他们进行转置卷积的逻辑。</li><li id="6308" class="lm ln it js b jt lx jx ly kb lz kf ma kj mb kn lt lu lv lw bi translated">现在深呼吸，通读<a class="ae mc" href="https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c" rel="noopener">和</a>。<strong class="js iu">这是理解计算输出如何随过滤器和x变化的直觉的最重要的一个</strong>粘贴上面文章的结论。JFYI，不要被全卷积弄糊涂了，它只不过是转置卷积(你刚刚在上面理解的)。</li></ol><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ou"><img src="../Images/4bae22a575bf9cac1b64cfe3dd4aa826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lJpn9kFHAQpXvaTDmsF0fg.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae mc" href="https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c" rel="noopener">来源</a></p></figure><p id="d4b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们可以像这样减小滤波器和w值。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/bd659b4c5ef3e672c91967379c8ef88a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*PAOcb1MFdcdrewItrFzjhg.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">作者图片</p></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="f81e" class="ko kp it bd kq kr mz kt ku kv na kx ky kz nb lb lc ld nc lf lg lh nd lj lk ll bi translated">场景2–2个卷积层+ 1个全连接层</h1><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ow"><img src="../Images/ca00f6d17540c9bd3d0f92440f946358.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KZISRysQBP2p0ke_vrxVsQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">作者图片</p></figure><p id="f752" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，添加尽可能多的卷积层，我们的方法将保持不变。像往常一样，目标是:</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/cfc8c01bd041b1637306f78793b51e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*iWAatbDqKQPvModhKUMygw.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">作者图片</p></figure><p id="f26c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">前面两项我们已经计算过了。让我们看看最后一个学期的公式是什么。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oy"><img src="../Images/9668541fcbf199b2808199a73a702c40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*00XFHG5fyXTpnI_2bf7Cbw.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">作者图片</p></figure><p id="584a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以防你需要深入研究这个。我推荐这篇<a class="ae mc" rel="noopener" target="_blank" href="/only-numpy-understanding-back-propagation-for-transpose-convolution-in-multi-layer-cnn-with-c0a07d191981">文章</a>。并相应地计算新的F1、F2和w。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="832a" class="ko kp it bd kq kr mz kt ku kv na kx ky kz nb lb lc ld nc lf lg lh nd lj lk ll bi translated">场景3—最大池层如何？？</h1><p id="ebcf" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">最大池是CNN的一个重要概念，反向传播是如何实现的？</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ow"><img src="../Images/9313195705346bd8192220db5503db5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2eR7LqPHnK7TyLLxNR1fxw.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">作者图片</p></figure><p id="c64c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">仔细想想，max pooling layer中没有像filters这样的变量。所以，我们不需要在这里调整任何值。</p><p id="91d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是，它影响了我以前的层，对不对？！它通过将矩阵中的几个元素合并成一个数字来减小矩阵的大小。基本上，它确实会影响反向传播。它说有非最大值的值不会有任何梯度。</p><p id="3718" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以，我们在这里说的是，所有没有最大值的值都是0。更有<a class="ae mc" href="https://medium.com/the-bioinformatics-press/only-numpy-understanding-back-propagation-for-max-pooling-layer-in-multi-layer-cnn-with-example-f7be891ee4b4" rel="noopener">深度</a>。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/2534ca2bf80c86b0ac586bade2e81e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*quCea_BrZHbv_Zu-fx1BoA.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">作者图片</p></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="917e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">已经尝试把所有好的相关文章放在一个地方，并帮助你看到卷积的全貌。请仔细阅读以上内容，如果在整个流程中仍然缺少一些东西，请告诉我——我很乐意编辑它以适应相同的内容。</p></div></div>    
</body>
</html>