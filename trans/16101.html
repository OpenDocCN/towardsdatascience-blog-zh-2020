<html>
<head>
<title>Batch normalization in 3 levels of understanding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">3个理解层次的批量标准化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338?source=collection_archive---------0-----------------------#2020-11-06">https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338?source=collection_archive---------0-----------------------#2020-11-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="7910" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="68d2" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">到目前为止，我们对它了解多少:从30秒摘要到综合指南</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/4a9d7867bde0bdcfece4fccf78e684b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*du-ZoEanVZA3ZAoHLxIWvA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">鸣谢:<a class="ae le" href="https://unsplash.com/@jabari21?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">贾巴里·蒂莫西</a>上<a class="ae le" href="https://unsplash.com/s/photos/dam?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="15de" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"> T </span>网上有很多关于批量归一化(BN)的内容。然而，他们中的许多人都在捍卫一种过时的直觉。我花了很多时间将所有这些零散的信息放在一起，以建立对这个基本方法的良好直觉，我认为一步一步的演练可能对一些读者有用。</p><p id="c13c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">特别是，这个故事旨在带来:</p><ul class=""><li id="bce9" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">从到<strong class="lh ja">三级理解</strong>的<strong class="lh ja">更新解释</strong>:30秒，3分钟，全面指导；</li><li id="0160" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">涵盖<strong class="lh ja">关键要素</strong>以充分利用BN；</li><li id="9559" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja">在Google Colab <strong class="lh ja">、</strong>中使用Pytorch实现BN层的简单实现</strong>，从官方论文中复制基于MNIST的实验(随意摆弄<a class="ae le" href="https://github.com/Johann-Huber/batchnorm_pytorch/blob/main/batch_normalization_in_pytorch.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)；</li><li id="a9bc" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">理解<strong class="lh ja">为什么BN仍然不为人所知</strong>(甚至在阅读了高质量作者的解释之后！).</li></ul><p id="1714" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们投入进去吧！</p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="91a9" class="nf ng iq bd nh ni nj nk nl nm nn no np kf nq kg nr ki ns kj nt kl nu km nv nw bi translated">概观</h1><p id="5797" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">a)<strong class="lh ja"/><a class="ae le" href="https://medium.com/p/14c2da90a338#b93c" rel="noopener"><strong class="lh ja">30秒</strong> </a></p><p id="b2d8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">B) <a class="ae le" href="https://medium.com/p/14c2da90a338#ad2e" rel="noopener"> <strong class="lh ja">再过3分钟</strong> </a></p><p id="0f46" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— 1.<a class="ae le" href="https://medium.com/p/14c2da90a338#3164" rel="noopener">原理</a></p><p id="04f3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— — 1.1.<a class="ae le" href="https://medium.com/p/14c2da90a338#dc3f" rel="noopener">培训</a></p><p id="2fd5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— — 1.2.<a class="ae le" href="https://medium.com/p/14c2da90a338#5ce9" rel="noopener">评估</a></p><p id="1c83" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— 2.<a class="ae le" href="https://medium.com/p/14c2da90a338#5920" rel="noopener">在实践中</a></p><p id="74f4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— 3.<a class="ae le" href="https://medium.com/p/14c2da90a338#5c4f" rel="noopener">结果概述</a></p><p id="925e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">C) <a class="ae le" href="https://medium.com/p/14c2da90a338#e5ba" rel="noopener"> <strong class="lh ja">了解批量归一化(BN) </strong> </a></p><p id="ad01" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— 1.<a class="ae le" href="https://medium.com/p/14c2da90a338#6cd2" rel="noopener">实施</a></p><p id="6aea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— 2.<a class="ae le" href="https://medium.com/p/14c2da90a338#018e" rel="noopener">实践中的BN</a></p><p id="ff2b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— — 2.1.<a class="ae le" href="https://medium.com/p/14c2da90a338#1f09" rel="noopener">原文章结果</a></p><p id="632f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— — 2.2.<a class="ae le" href="https://medium.com/p/14c2da90a338#9a0a" rel="noopener">正规化，一个BN副作用</a></p><p id="1f17" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— — 2.3.<a class="ae le" href="https://medium.com/p/14c2da90a338#36f2" rel="noopener">评估期间的标准化</a></p><p id="c779" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— — 2.4.<a class="ae le" href="https://medium.com/p/14c2da90a338#6775" rel="noopener">稳定性问题</a></p><p id="7dbd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— — 2.5.<a class="ae le" href="https://medium.com/p/14c2da90a338#d000" rel="noopener">递归网络&amp;图层归一化</a></p><p id="bd03" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— — 2.6.<a class="ae le" href="https://medium.com/p/14c2da90a338#3321" rel="noopener">非线性之前还是之后？</a></p><p id="aa1a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— 3.<a class="ae le" href="https://medium.com/p/14c2da90a338#acf0" rel="noopener">BN为什么管用？</a></p><p id="215d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— — 3.1.<a class="ae le" href="https://medium.com/p/14c2da90a338#8c6f" rel="noopener">第一个假设:围绕内部协变量移位(ICS)的困惑</a></p><p id="816a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— — 3.2.<a class="ae le" href="https://medium.com/p/14c2da90a338#e773" rel="noopener">第二个假设:减轻分布之间的相互依赖性</a></p><p id="ae4c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— — 3.3.<a class="ae le" href="https://medium.com/p/14c2da90a338#fb36" rel="noopener">第三个假设:使优化前景更加平滑</a></p><p id="4a7b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">— 4.总结:我们现在了解了什么</p><p id="5ba1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://medium.com/p/14c2da90a338#ff33" rel="noopener"> <strong class="lh ja">结论</strong> </a></p><p id="3417" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://medium.com/p/14c2da90a338#6bf0" rel="noopener"> <strong class="lh ja">开题</strong> </a></p><p id="cd0c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://medium.com/p/14c2da90a338#73dc" rel="noopener">致谢</a></p><p id="473e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://medium.com/p/14c2da90a338#c2a9" rel="noopener">参考文献</a></p><p id="acd9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://medium.com/p/14c2da90a338#75d3" rel="noopener">更进一步</a></p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="b93c" class="nf ng iq bd nh ni nj nk nl nm nn no np kf nq kg nr ki ns kj nt kl nu km nv nw bi translated"><strong class="ak"> A)在30秒内</strong></h1><p id="791a" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated"><strong class="lh ja">【BN】</strong>是一种算法方法，使深度神经网络<em class="oc"/>【DNN】<strong class="lh ja"/><strong class="lh ja">训练更快</strong>。</p><p id="1d8f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它由<strong class="lh ja">使用当前批次的第一和第二统计矩(均值和方差)归一化来自隐藏层的激活向量</strong>组成。这个归一化步骤正好在非线性函数之前(或之后)应用。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/6a1ac7d1d078bfce8b598b6cca01c768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tcvRJN-OadhUyps6HSO0og.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">多层感知器(MLP) <strong class="bd oe">无批量归一化(BN) | </strong>鸣谢:作者-设计:<a class="ae le" href="https://www.instagram.com/louhacquetdelepine/" rel="noopener ugc nofollow" target="_blank">娄高清</a></p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/0595fb35f4c3f1c363e4ccd9ad9c4867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QcSkw489NgtpaMuwDhehaQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">多层感知器(MLP)<strong class="bd oe">(BN)|</strong>鸣谢:作者-设计:<a class="ae le" href="https://www.instagram.com/louhacquetdelepine/" rel="noopener ugc nofollow" target="_blank">娄高清</a></p></figure><p id="7922" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所有当前的深度学习框架都已经实现了应用批量规范化的方法。它通常用作模块，可以作为标准层插入到DNN中。</p><p id="c21c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">备注:对于那些喜欢阅读代码而不是文本的人，我在</em> <a class="ae le" href="https://github.com/Johann-Huber/batchnorm_pytorch/blob/main/batch_normalization_in_pytorch.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="oc">这个repo </em> </a> <em class="oc">中写了一个简单的批量规范化实现。</em></p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="ad2e" class="nf ng iq bd nh ni nj nk nl nm nn no np kf nq kg nr ki ns kj nt kl nu km nv nw bi translated">b)3分钟后</h1><h2 id="3164" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">B.1)原则</h2><p id="8ab6" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">在训练和测试阶段，批处理规范化的计算方式不同。</p><p id="dc3f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> B.1.1)培训</strong></p><p id="4c3b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在每个隐藏层，批量标准化将信号转换如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/ddb0c2172fa5156f3fef63880e9390e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*zdscSj6nEGsxUxplWHN-kg.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/96b9d6ac2a6cfdc2578078f016cd6757.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/1*89JEFNiKgg16hPtBY25YdQ.gif"/></div></figure><p id="c377" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">BN层首先使用(1)和(2)确定整批激活值的<strong class="lh ja">均值𝜇 </strong>和<strong class="lh ja">方差σ </strong>。</p><p id="90aa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后<strong class="lh ja">用(3)归一化激活向量</strong><strong class="lh ja">【z^(i】</strong>。这样，每个神经元的输出都遵循一个标准的正态分布。(𝜀是用于数值稳定性的常数)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/55b971999f35e74a03d163f193c5f127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TrjyZmHj_wInh6kFARuLZw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">批量归一化第一步。</strong>一个3神经元隐藏层的例子，批量大小为b。每个神经元遵循标准正态分布。<strong class="bd oe"> | </strong>鸣谢:作者-设计:<a class="ae le" href="https://www.instagram.com/louhacquetdelepine/" rel="noopener ugc nofollow" target="_blank">娄高清</a></p></figure><p id="01ae" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它最终通过应用带有两个可训练参数(4)的<em class="oc"> 𝛾 </em>和<em class="oc"> 𝛽 </em>的线性变换来计算<strong class="lh ja">层的输出</strong><strong class="lh ja">ẑ(i】</strong>。该步骤允许模型通过调整以下两个参数来为每个隐藏层选择最佳分布:</p><ul class=""><li id="48e0" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated"><em class="oc"> 𝛾 </em>允许调整标准偏差；</li><li id="ad75" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><em class="oc"> 𝛽 </em>允许调整偏差，将曲线移向右侧或左侧。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ot"><img src="../Images/e8daec97bbed275be40652461749108d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zEkSLa9rpfEmINn5DmJbOA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">𝛾<em class="ou">和𝛽 </em>参数的好处。</strong>修改分布(顶部)允许我们使用非线性函数的不同机制(底部)。<strong class="bd oe"> | </strong>鸣谢:作者—设计:<a class="ae le" href="https://www.instagram.com/louhacquetdelepine/" rel="noopener ugc nofollow" target="_blank">娄高清</a></p></figure><p id="e353" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">备注:解释BN层有效性的理由容易受到误解和错误的影响(即使在原文中也是如此)。最近的一篇论文[2]反驳了一些错误的假设，提高了社区对这种方法的理解。我们将在第C.3节讨论这个问题:“BN为什么起作用？”。</em></p><p id="ca16" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在每次迭代中，网络计算与当前批次相对应的平均𝜇和标准差σ。然后，它通过梯度下降训练<em class="oc"> 𝛾 </em>和<em class="oc"> 𝛽，使用<a class="ae le" href="https://en.wikipedia.org/wiki/Moving_average" rel="noopener ugc nofollow" target="_blank">指数移动平均(EMA) </a>赋予最新迭代更多的重要性。</em></p><p id="5ce9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> B.1.2)评估</strong></p><p id="6baa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">与训练阶段不同，<strong class="lh ja">在评估阶段</strong>，我们可能没有完整的批次输入到模型中。</p><p id="0f56" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了解决这个问题，我们计算(<strong class="lh ja"> 𝜇 </strong> _pop，<strong class="lh ja"> σ </strong> _pop)，如:</p><ul class=""><li id="9aef" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated"><strong class="lh ja"> 𝜇 </strong> _pop:研究人群的估计均值；</li><li id="e493" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja"> σ </strong> _pop:研究人群的估计标准差。</li></ul><p id="4b58" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用在训练期间确定的所有(<strong class="lh ja"> 𝜇 </strong> _batch，<strong class="lh ja"> σ </strong> _batch)来计算那些值，并且在评估期间直接馈入等式(3)(而不是调用(1)和(2))。</p><p id="c6a2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">备注:我们将在第C.2.3节“评估期间的标准化”中深入讨论该问题。</em></p><h2 id="5920" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">B.2)实际上</h2><p id="c5b5" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">在实践中，我们将批量归一化视为一个标准层，如感知器、卷积层、激活函数或漏失层。</p><p id="2a68" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">每个流行的框架都已经实现了批处理规范化层。例如:</p><p id="860d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">py torch</strong>:<a class="ae le" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html" rel="noopener ugc nofollow" target="_blank">torch . nn . batch norm 1d</a>，<a class="ae le" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html" rel="noopener ugc nofollow" target="_blank"> torch.nn.BatchNorm2d </a>，<a class="ae le" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html" rel="noopener ugc nofollow" target="_blank"> torch.nn.BatchNorm3d </a></p><p id="cf82" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">tensor flow/Keras</strong>:<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization" rel="noopener ugc nofollow" target="_blank">TF . nn . batch _ normalization</a>，<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization" rel="noopener ugc nofollow" target="_blank">TF . Keras . layers . batch normalization</a></p><p id="4e55" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所有BN实现都允许您独立设置每个参数。然而，<strong class="lh ja">输入向量大小是最重要的一个</strong>。它应该设置为:</p><ul class=""><li id="7b99" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">当前隐层有多少神经元(针对MLP)；</li><li id="14ab" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">当前隐藏层中有多少个过滤器(对于卷积网络)。</li></ul><p id="c6c2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">看看你最喜欢的框架的在线文档，并阅读BN层页面:它们的实现有什么特别的吗？</p><h2 id="5c4f" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">B.3)结果概述</h2><p id="bc0f" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">即使我们还不理解批处理规范化的所有底层机制(参见C.3)，但有一点大家都同意:<strong class="lh ja">它工作</strong>。</p><p id="e58c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了获得第一个洞察力，让我们看看官方文章的结果[1]:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/36b7ea1c3a485fb147ec09ed93ecb3ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*gfzbC-GrOoVUfk1yU4DdfQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图1:<strong class="bd oe">BN如何影响训练。</strong>ImageNet(2012)验证集上的准确度，相对于训练迭代次数。比较了五个网络:“初始”是标准初始网络[3]，“BN-X”是具有BN层的初始网络(对于3种不同的学习速率:x1，x5，x30，初始最优网络，“BN-X-Sigmoid”是具有BN层的初始网络，其中所有的ReLU非线性被Sigmoid代替。|来源:[1]</p></figure><p id="1f4d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">结果很明显:BN层<strong class="lh ja">使训练更快</strong>，而<strong class="lh ja">允许更大范围的学习速率</strong>而不会影响训练的收敛性。</p><p id="2143" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">备注:此时，你应该对BN层有足够的了解来使用它们。然而，如果您想从批处理规范化中获得最大收益，您将需要更深入地挖掘！</em></p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/0a09b3098b02575bbe2e6f311a1506ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7tK-KHAf4ai7S-kXuZw0PQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">批量归一化和这张图有什么联系？</strong> |信用:<a class="ae le" href="https://unsplash.com/@daniloalvesd?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">达尼洛·阿尔维斯</a>上<a class="ae le" href="https://unsplash.com/s/photos/taps?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="e5ba" class="nf ng iq bd nh ni nj nk nl nm nn no np kf nq kg nr ki ns kj nt kl nu km nv nw bi translated">c)了解批量标准化(BN)</h1><h1 id="6cd2" class="nf ng iq bd nh ni ox nk nl nm oy no np kf oz kg nr ki pa kj nt kl pb km nv nw bi translated">C.1) <strong class="ak">实施</strong></h1><p id="edc6" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">我已经用Pytorch重新实现了批处理规范化层，以再现原始的论文结果。代码可以在这个github库上找到。</p><p id="99be" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我建议你看看网上的一些BN层的实现。看看在自己喜欢的框架里是怎么编码的，很有启发！</p><h1 id="018e" class="nf ng iq bd nh ni ox nk nl nm oy no np kf oz kg nr ki pa kj nt kl pb km nv nw bi translated">实践中的BN层</h1><p id="c7ca" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">在深入理论之前，让我们先从批处理规范化有哪些确定性开始。</p><p id="2ac9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本节中，我们将看到:</p><ul class=""><li id="f2b6" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">BN如何影响<strong class="lh ja">培训绩效</strong>？为什么这种方法在如今的深度学习中如此重要？</li><li id="b2b2" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">我们必须注意哪些副作用？</li><li id="78b4" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja">当</strong>和<strong class="lh ja">我们应该如何</strong>使用BN？</li></ul><h2 id="1f09" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">C.2.1)来自原始文章的结果</h2><p id="19e8" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">如前所述，BN被广泛使用，因为它几乎总是<strong class="lh ja">让深度学习模型表现得更好</strong>。</p><p id="e691" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">官方文章[1]进行了3个实验来说明其方法的有效性。</p><p id="b018" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">首先，他们在MNIST数据集(手写数字)上训练了一个分类器。该模型由3个完全连接的层组成，每层100个神经元，全部由sigmoid功能激活。他们已经使用随机梯度下降(SGD)和相同的学习率(0.01)对该模型训练了两次(有和没有BN层)50 000次迭代。请注意，BN层正好放在激活函数之前。</p><p id="6224" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">您可以轻松地再现这些结果，而无需GPU，这是一个更熟悉这个概念的好方法！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/0e20e15ed1e18b6df7a34f3552b62323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eQjN8WPOkLYx15536hD2xg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2:<strong class="bd oe">BN如何影响简单多层感知器(MLP)的训练</strong> |左:训练精度w.r.t .迭代|右:训练损失w.r.t .迭代|作者</p></figure><p id="a349" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">看起来不错！批处理规范化提高了我们的网络性能，包括损失和准确性。</p><p id="8c6e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第二个实验包括查看隐藏层中的激活值。以下是对应于最后一个隐藏层(就在非线性之前)的图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pd"><img src="../Images/3d71a68c87e1d52b651e7b8ec55d536a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B2BG3gslTvx7EIbW2NqiBg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图3 : <strong class="bd oe">批量归一化对最后一个隐藏层激活值的影响| </strong>鸣谢:作者</p></figure><p id="af6d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果没有批处理规范化，激活的值在第一次迭代中会有很大的波动。相反，当使用BN时，活化曲线更平滑。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pe"><img src="../Images/a80e9f5b96110afef4a4ee3375a3b5fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Hmpy0cyC6ne4PG86Ev2iQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图4 : <strong class="bd oe">批量标准化对隐藏层激活的影响</strong> |有BN的模型比没有BN的模型有更平滑的激活曲线| Credit : author</p></figure><p id="73fa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，当添加BN层时，信号噪声更小。这似乎使模型的收敛更容易。</p><p id="1fd0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这个例子没有显示批处理规范化的所有好处。</p><p id="e173" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">官方文章进行了第三次实验。他们希望在更大的数据集ImageNet (2012)上添加BN图层时比较模型性能。为此，他们训练了一个强大的神经网络(当时)，名为Inception [3]。最初，这个网络没有任何BN层。他们添加了一些，并通过修改其学习率来训练模型(x1，x5，x30以前的最佳值)。他们还试图在另一个网络中用sigmoid替换每个ReLU激活功能。然后，他们将这些新网络的性能与原来的进行了比较。</p><p id="3268" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以下是他们得到的结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/36b7ea1c3a485fb147ec09ed93ecb3ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*gfzbC-GrOoVUfk1yU4DdfQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">图5 : </strong> <strong class="bd oe">批量归一化对训练的影响(ImageNet) </strong> |《盗梦空间》:原网[3]；“BX-基线”:与BN的Inception相同，学习率(LR)相同；“BX-x5”:与BN、LR x5的《盗梦空间》相同；“BX-x30”:与BN的Inception相同，LR x30“BN-x5-Sigmoid”:与Inception相同，使用BN、LR x5和Sigmoid代替ReLU | Source : [1]</p></figure><p id="cf66" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从这些曲线中我们可以得出以下结论:</p><ul class=""><li id="a3f0" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">增加BN层导致<strong class="lh ja">更快和更好的收敛</strong>(其中更好意味着更高的精度)；</li></ul><p id="4a0b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在如此大的数据集上，这种改善比在小MNIST数据集上观察到的改善要显著得多。</p><ul class=""><li id="d7d0" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">增加BN层允许我们使用<strong class="lh ja">更高的学习速率(LR)而不影响收敛</strong>；</li></ul><p id="7c99" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作者已经成功训练了他们的Inception-with-BN网络，使用了比原始网络高30倍的学习速率<strong class="lh ja">。请注意，5倍大的LR已经使普通网络发散了！</strong></p><p id="b1c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这样，就更容易找到一个“可接受的”学习率:位于欠拟合和梯度爆炸之间的LR的间隔要大得多。</p><p id="d284" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，较高的学习率有助于优化器避免局部极小值收敛。受到鼓励去探索，优化器将<strong class="lh ja">更容易收敛于更好的解决方案</strong>。</p><ul class=""><li id="c500" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">基于<strong class="lh ja"> sigmoid的模型</strong> <strong class="lh ja">达到了与基于ReLU的模型的竞争结果</strong></li></ul><p id="c9b1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们需要后退一步，看看更大的图景。我们可以清楚地看到，与sigmoid模型相比，基于ReLU的模型可以获得略好的性能，但这不是这里的重点。</p><p id="4b6a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了说明为什么这个结果是有意义的，让我重新表述一下Ian good fellow(GANs[6]的发明者，著名的“<a class="ae le" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">深度学习</a>”手册的作者)对此的看法:</p><blockquote class="pf pg ph"><p id="c791" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">在BN之前，我们认为在隐藏层中使用sigmoid有效地训练深度模型几乎是不可能的。我们考虑了几种解决训练不稳定性的方法，比如寻找更好的初始化方法。这些解决方案很大程度上是启发式的，而且太脆弱，不令人满意。<strong class="lh ja">批量归一化使那些不稳定的网络可训练</strong>；这个例子说明了这一点。</p><p id="4138" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">——伊恩·古德菲勒(改编自:<a class="ae le" href="https://www.youtube.com/watch?v=Xogn6veSyxA" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></blockquote><p id="fc02" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在我们明白了为什么BN对深度学习领域有如此重要的影响。</p><p id="f4b3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这些结果为<strong class="lh ja">提供了批处理标准化对网络性能的益处的概述</strong>。然而，有一些副作用，你应该记住，以获得最大的BN。</p><h2 id="9a0a" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">C.2.2)规则化，一个BN副作用</h2><p id="ed98" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">BN依靠批量第一和第二统计矩(均值和方差)来归一化隐藏层激活。然后，输出值与当前批次统计数据紧密相关。根据当前批处理中使用的输入示例，这种转换会增加一些噪声。</p><p id="720e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">添加一些噪声以避免过度拟合…听起来像是一个<strong class="lh ja">正则化过程</strong>，不是吗？；)</p><p id="951a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在实践中，<strong class="lh ja">我们不应该依靠批量归一化来避免过拟合</strong>，因为<a class="ae le" href="https://en.wikipedia.org/wiki/Orthogonality_(programming)" rel="noopener ugc nofollow" target="_blank">正交性</a>很重要。简而言之，我们应该始终确保一个模块解决一个问题。依靠几个模块来处理不同的问题使得开发过程比需要的要困难得多。</p><p id="cd25" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">尽管如此，了解正则化效应还是很有趣的，因为它可以解释网络中的一些意外行为(尤其是在健全性检查期间)。</p><p id="d456" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">备注:批量越大，正则化越小(因为它减少了噪声影响)。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/b1802314142ecb87ef2099cc6b5bb379.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uzhM7EYE6GTNR6zw03ZccA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">如果我们想在如此出色的嵌入式系统上部署批量规范化模型，该怎么办？</strong> |信用:<a class="ae le" href="https://unsplash.com/@liacastelli?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">玛利亚·卡斯特利</a>于<a class="ae le" href="https://unsplash.com/s/photos/robotics?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h2 id="36f2" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">评估过程中的标准化</h2><p id="b09f" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">有两种情况可以在<strong class="lh ja">评估模式</strong>中调用模型:</p><ul class=""><li id="8399" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">在进行<strong class="lh ja">交叉验证</strong>或<strong class="lh ja">测试</strong>时，在模型训练和开发过程中；</li><li id="706e" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">当<strong class="lh ja">部署</strong>模型时。</li></ul><p id="b0fd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在第一种情况下，为了方便起见，我们可以使用当前的批处理统计数据来应用批处理规范化。在第二个例子中，<strong class="lh ja">使用同样的方法没有意义</strong>，因为我们不一定要预测整个批次。</p><p id="14b9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们看一个带有嵌入式摄像头的机器人的例子。我们可以有一个模型，使用当前的框架来预测任何即将到来的障碍的位置。因此，我们希望每次迭代都在单个帧(即一个rgb图像)上计算推断。如果训练批量为N，<strong class="lh ja">我们应该为模型期望的(N - 1)个其他输入选择什么来进行前向传播？</strong></p><p id="3f96" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">记住，对于每一个BN层，(<em class="oc"> 𝛽 </em>，<em class="oc"> 𝛾 </em>)都是使用归一化信号训练的。所以<strong class="lh ja">我们需要确定</strong> <strong class="lh ja"> (𝜇，σ) </strong>才能有有意义的结果。</p><p id="7f29" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一种解决方案是选择任意值来完成批量。通过将第一批输入到模型中，我们将获得感兴趣的图像的某个结果。如果我们用其他随机值构建第二批，我们将对同一幅图像有不同的预测。<strong class="lh ja">一个输入两个不同的输出不是理想的模型行为</strong>。</p><p id="1f15" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这一招就是定义(<strong class="lh ja"> 𝜇 </strong> _pop，<strong class="lh ja"> σ </strong> _pop)，分别是<strong class="lh ja">目标人群</strong>的估计均值和标准差。这些参数计算为训练期间确定的所有(<strong class="lh ja"> 𝜇_ </strong>批次，<strong class="lh ja">σ</strong>_批次)的平均值。</p><p id="30e6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们就是这么做的！</p><p id="3a86" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">备注:这个技巧可能会导致评估阶段的不稳定性:让我们在下一部分讨论它。</em></p><h2 id="6775" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">C.2.4) BN层稳定性</h2><p id="77a2" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">尽管批处理规范化工作得很好，但它有时会导致稳定性问题。存在BN层在评估阶段使激活值爆炸的情况(使模型返回loss = NaN)。</p><p id="5ea5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们刚刚提到如何确定(<strong class="lh ja"> 𝜇 </strong> _pop，<strong class="lh ja"> σ </strong> _pop)，以便在评估期间使用它们:我们计算在训练期间计算的所有(<strong class="lh ja"> 𝜇_ </strong>批次，<strong class="lh ja">σ</strong>_批次)的平均值。</p><p id="f8af" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们考虑一个只在包含运动鞋的图像上训练的模型。如果测试集中有类似德比的鞋子呢？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pl"><img src="../Images/9896820ec7758d4432cd3271a4577c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*evTAYxc9esmDF8Fkfux3ew.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">如果输入分布从训练到评估变化太大，模型可能对某些信号</strong>、<strong class="bd oe">反应过度，导致活动发散。</strong> |信用:<a class="ae le" href="https://unsplash.com/@grailify?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">格里夫</a>(左)<a class="ae le" href="https://unsplash.com/@jimmy2018?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">叶佳</a>(右)在<a class="ae le" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="c21d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们假设隐藏层的激活值在训练和评估期间将具有显著不同的分布——可能太多了。在这种情况下，<strong class="lh ja">估计的(𝜇_pop，σ_pop)不能正确估计真实总体均值和标准差。</strong>应用(<strong class="lh ja"> 𝜇 </strong> _pop，<strong class="lh ja"> σ </strong> _pop)可能会将激活值推离(𝜇 = 0，σ = 1)，导致<strong class="lh ja">高估激活值</strong>。</p><p id="54f5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">备注:训练集和测试集之间分布的移动称为“协变量移动”。我们将在(C.3 .)节中再次讨论这种效应。</em></p><p id="2391" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">BN的一个众所周知的特性增加了这种效果:在训练期间，激活值使用它们自己的值进行归一化。在推断期间，使用训练期间已经计算的(<strong class="lh ja"> 𝜇 </strong> _pop，<strong class="lh ja"> σ </strong> _pop)对信号进行归一化。因此，用于标准化的<strong class="lh ja">系数不考虑实际激活值本身。</strong></p><p id="211d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一般来说，训练集必须与测试集“足够相似”:否则，就不可能在目标任务上正确地训练模型。所以在大多数情况下，<strong class="lh ja"> 𝜇 </strong> _pop和<strong class="lh ja"> σ </strong> _pop应该很适合测试集。如果不是，我们可以得出结论，训练集不够大，或者它的质量对于目标任务来说不够好。</p><p id="ca68" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是有时候，<a class="ae le" href="https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561/38" rel="noopener ugc nofollow" target="_blank">就这么发生了</a>。这个问题并不总是有一个干净的解决方案。</p><p id="0470" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我曾亲自面对过一次，在<a class="ae le" href="https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression" rel="noopener ugc nofollow" target="_blank">肺纤维化进展卡格尔比赛</a>期间。训练数据集由元数据和与每个患者相关的肺部3D扫描组成。这些扫描的内容是复杂多样的，但是我们只有大约100名患者来分成训练集和验证集。结果，当模型从训练模式切换到评估模式时，我想用于特征提取的卷积网络返回NaN。调试愉快。</p><p id="d897" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当您无法轻松获得额外的数据来增强您的训练集时，您必须找到一种变通方法。在上面的例子中，我已经手动强制BN层也在验证集上进行计算(<strong class="lh ja"> 𝜇 </strong> _batch，<strong class="lh ja"> σ </strong> _batch)。(我同意，这是一种丑陋的修复方式，但我没有时间了。；) )</p><p id="68bb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在你的网络中增加BN层——假设它不会有负面影响——并不总是最好的策略！</p><h2 id="d000" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">C.2.5)递归网络和层标准化</h2><p id="612c" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">在实践中，人们普遍承认:</p><ul class=""><li id="3e0f" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated"><strong class="lh ja">对于卷积网络(CNN) </strong> : <strong class="lh ja">批量归一化(BN) </strong>比较好</li><li id="af3f" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja">对于递归网络(RNN) </strong> : <strong class="lh ja">图层归一化(LN) </strong>比较好</li></ul><p id="e596" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">BN使用当前批次来规格化每个单个值，而LN使用所有当前层来进行规格化。换句话说，使用来自单个示例的其他特征来执行归一化，而不是在所有当前批量示例中使用相同的特征。这种解决方案对于循环网络似乎更有效。注意，很难为这些种类的神经元定义一致的策略，因为它们依赖于相同权重矩阵的多次乘法。我们是否应该独立地规范化每一步？或者我们应该计算所有步骤的平均值，然后递归地应用标准化？<em class="oc">(直觉论证的来源</em> <em class="oc"> : </em> <a class="ae le" href="https://www.youtube.com/watch?v=Xogn6veSyxA" rel="noopener ugc nofollow" target="_blank"> <em class="oc">这里</em> </a> <em class="oc"> ) </em></p><p id="0552" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我不会在这个问题上做任何进一步的详述，因为这不是本文的目的。</p><h2 id="3321" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">C.2.6)非线性之前还是之后？</h2><p id="d624" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated"><strong class="lh ja">从历史上看，</strong> BN层位于<strong class="lh ja">非线性函数之前，这与作者当时的目标和假设一致:</strong></p><p id="f3a1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">他们在文章中指出:</p><blockquote class="pf pg ph"><p id="939e" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">“我们希望确保，对于任何参数值，网络总是产生具有期望分布的激活。”</p><p id="dbd0" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">——谢尔盖·约菲和克里斯蒂安·塞格迪(来源:[1])</p></blockquote><p id="e487" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一些实验表明，在非线性函数之后立即定位BN层<strong class="lh ja">会导致更好的结果</strong>。这里有一个<a class="ae le" href="https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu" rel="noopener ugc nofollow" target="_blank">的例子</a>。</p><p id="ea56" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Keras的创始人、现任谷歌工程师的Franç ois Chollet表示:</p><blockquote class="pf pg ph"><p id="483f" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">“我还没有回去检查他们在原始论文中的建议，但我可以保证克里斯蒂安[塞格迪] <strong class="lh ja">最近写的代码在BN </strong>之前应用relu。不过，这偶尔还是一个争论的话题。</p><p id="e6f8" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">—弗朗索瓦·乔莱(<a class="ae le" href="https://github.com/keras-team/keras/issues/1802" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></blockquote><p id="80c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">尽管如此，许多常用的迁移学习架构在非线性之前应用BN(ResNet，mobilenet-v2，…)。</p><p id="7794" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请注意，文章[2]质疑了原文章[1]所捍卫的解释BN有效性的假设(见C.3.3))，将BN层放在激活函数之前，但没有给出充分的理由。</p><p id="a5c4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">据我所知，<strong class="lh ja">这个问题还在讨论</strong>。</p><p id="66a5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">延伸阅读:这里有一个有趣的</em> <a class="ae le" href="https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/dgqaksn/" rel="noopener ugc nofollow" target="_blank"> <em class="oc"> reddit线程</em></a><em class="oc">——即使有些论点没有说服力——激活后大多赞成BN。</em></p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pm"><img src="../Images/539710e2dc9a333167c9f7f43ef14dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByFB3iYtweUdsIvIRXLHnw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">我那些乱七八糟的代码到底为什么能工作？| </strong>信用:<a class="ae le" href="https://unsplash.com/@rohitfarmer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Rohit农民</a>上<a class="ae le" href="https://unsplash.com/s/photos/child-computer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><h1 id="acf0" class="nf ng iq bd nh ni nj nk nl nm nn no np kf nq kg nr ki ns kj nt kl nu km nv nw bi translated">C.3)批量归一化为什么有效？</h1><p id="3ba0" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">在大多数情况下，批处理规范化提高了深度学习模型的性能。太好了。但是我们想知道黑匣子里到底发生了什么。</p><p id="3406" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这就是事情变得有点棘手的地方。</p><p id="653b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">问题是:<strong class="lh ja">我们还不确切知道</strong>是什么让批处理规范化工作得这么好<strong class="lh ja">还是</strong>。DL社区中经常讨论一些假设:我们将一步一步地探索它们。</p><p id="2eb5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在开始讨论之前，我们将看到以下内容:</p><ul class=""><li id="3ea4" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">原文[1]假设BN有效性是由于<strong class="lh ja"> </strong>的<strong class="lh ja">减少</strong>他们称之为<strong class="lh ja">内部协变量移位</strong> (ICS)。最近的一篇论文[2]驳斥了这一假设。(参见C.3.1)</li><li id="d16c" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">另一个假设更加谨慎地取代了第一个假设:BN <strong class="lh ja">减轻了训练过程中各层之间的相互依赖性</strong>。(参见C.3.2)</li><li id="bb34" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">麻省理工学院最近的一篇论文[2]强调了BN对<strong class="lh ja">优化景观平滑度</strong>的影响，使得训练更加容易。(参见C.3.3)</li></ul><p id="cf0c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我敢打赌，探索这些假设将有助于您建立一个关于批处理规范化的强大直觉。</p><p id="27e4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们走吧！</p><h2 id="8c6f" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">C.3.1)假设n 1 — BN减少了内部协方差偏移(ICS)</h2><p id="89b9" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated"><strong class="lh ja">批量正常化尽管对DNN的表现有着根本性的影响，但仍然会受到误解</strong>。</p><p id="b445" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关于BN的困惑大多是由于原始文章【1】支持的<strong class="lh ja">错误假设。</strong></p><p id="ea39" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Sergey Ioffe和Christian Szegedy对BN介绍如下:</p><blockquote class="pf pg ph"><p id="448f" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">“在训练过程中，我们将深度网络内部节点分布的变化称为<strong class="lh ja">内部协变量移位</strong>。[……]我们提出了一种新的机制，我们称之为<strong class="lh ja">批量标准化</strong>，它向减少内部协变量偏移迈出了<strong class="lh ja">的一步，并在这样做的过程中极大地加速了深度神经网络的训练。”</strong></p><p id="fd50" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">——谢尔盖·约菲和克里斯蒂安·塞格迪(来源:[1])</p></blockquote><p id="49bd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">换句话说，BN是有效的，因为它部分地解决了内部协变量转移的问题。</p><p id="ebb5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">这种说法受到了后来作品的严峻挑战【2】</strong>。</p><p id="dbe3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">批注:从现在开始，</em> <strong class="lh ja"> <em class="oc"> ICS </em> </strong> <em class="oc">指内部协变移位。</em></p><p id="2b13" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了理解是什么导致了这样的混乱，让我们从讨论什么是协变量移位i <em class="oc">，</em>开始，以及它是如何受到归一化的影响的。</p><h2 id="21f9" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">什么是协变量转移(分布稳定性观点)？</h2><p id="7eb6" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">[1]的作者明确地定义了它:<strong class="lh ja">协变量转移</strong>——从分布稳定性的角度——描述了一个模型输入分布的<strong class="lh ja">转移。推而广之，<strong class="lh ja">内部协变量移位</strong>描述了发生在<strong class="lh ja">深度神经网络</strong>的<strong class="lh ja">隐层</strong>中的这种现象。</strong></p><p id="8533" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们通过一个例子来看看为什么它会成为一个问题。</p><p id="23e1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们假设我们想要训练一个能够回答以下问题的分类器:<strong class="lh ja">这个图像包含一辆汽车吗？如果我们想从一个非常大的无标签数据集中提取所有的汽车图像，这样的模型将会节省我们大量的时间。</strong></p><p id="4daa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们会有一个RGB图像作为输入，一些卷积层，然后是完全连接的层。输出应该是一个单一的值，输入到一个逻辑函数中，使最终值介于0和1之间——描述输入图像包含一辆汽车的概率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/e160dc9dcb356f7a4712879f46ea2294.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PrEddXFhA52q6k4Kk9nqww.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图5 : <strong class="bd oe">一个</strong> <strong class="bd oe">简单的CNN分类器。| </strong>鸣谢:作者-设计:<a class="ae le" href="https://www.instagram.com/louhacquetdelepine/" rel="noopener ugc nofollow" target="_blank">娄高清</a></p></figure><p id="f417" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了训练这样一个模型，我们需要大量带标签的图像。</p><p id="5ec5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，假设我们只有用于训练的“普通”汽车。如果我们让模型给一级方程式赛车分类，它会有什么反应？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pl"><img src="../Images/66874337da546909f275555efe45b0fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UkkceCexOHwLvV-iXOsuKw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">如前所述，协变量转移会使网络激活发散(第C.2.4节)。即使没有，也会降低整体性能！</strong> |演员表:<a class="ae le" href="https://unsplash.com/@dhivakrishna?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">迪瓦·克里希纳</a>(左)和<a class="ae le" href="https://unsplash.com/@ferhat?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">弗哈特·德尼兹·福斯</a>(右)在<a class="ae le" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="c872" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个例子中，在<strong class="lh ja">培训和测试分布之间有一个转移。</strong>更广泛地说，不同的汽车方向、闪电或天气状况足以影响我们的车型性能。这里，<strong class="lh ja">我们的模型不能很好地概括。</strong></p><p id="eb0e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果我们在特征空间中绘制提取的特征，我们将得到如下结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi po"><img src="../Images/7fac9412182b7884c0d01e4bc3cad9ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cVFsnaXsZOTsYWI6WCFX7Q.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">图表6.a : </strong> <strong class="bd oe">为什么我们需要归一化模型输入值？案例没有规范化。</strong>在训练过程中，输入值是分散的:在点密度较高的地方，近似函数会非常精确。相反，在点的密度较低的情况下，它将是不准确的，并且服从随机性。(例如，近似曲线可以是绘制的3条线之一。)<strong class="bd oe"> | </strong>鸣谢:作者-设计:<a class="ae le" href="https://www.instagram.com/louhacquetdelepine/" rel="noopener ugc nofollow" target="_blank">楼高清</a></p></figure><p id="70f1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">假设<strong class="lh ja">十字</strong>描述与不包含任何汽车的<strong class="lh ja">图像相关的特征，而<strong class="lh ja">圆环</strong>描述包含汽车</strong>的<strong class="lh ja">图像。在这里，一个函数可以将两个系综分开。但是在图的右上角，函数可能不太准确:没有足够的点来确定一个好的函数。这可能会导致分类器在评估过程中出现许多错误。</strong></p><p id="b7fc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了有效地训练我们的模型，我们需要许多汽车图像，在任何我们可以想象的可能的环境中。即使这仍然是我们今天训练CNN的方式，我们也要确保我们的模型能够使用尽可能少的例子进行很好的概括。</p><p id="bcb5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这些问题可以总结如下:</p><blockquote class="pf pg ph"><p id="7ca6" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">从模型的角度来看，训练图像在统计上与测试图像差别太大。<br/>有一个<strong class="lh ja">协变档</strong>。</p></blockquote><p id="f6a9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以用更简单的模型来面对这个问题。众所周知，<strong class="lh ja">线性回归</strong>模型在输入值归一化时更容易优化(即使其分布接近(<strong class="lh ja"> 𝜇 </strong> = 0，<strong class="lh ja"> σ </strong> = 1)):这就是为什么<strong class="lh ja">我们通常归一化一个模型的输入值。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pp"><img src="../Images/62f21f4b419d3c456a841cde1252c17a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jGTvhWXnltmk_9MWMdRPGQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图表6.b : <strong class="bd oe">为什么我们需要归一化模型输入值？规范化案例。</strong>在训练期间，标准化输入信号使特征空间中的点彼此更接近:现在更容易找到一个良好的概括函数。<strong class="bd oe"> | </strong>鸣谢:作者—设计:<a class="ae le" href="https://www.instagram.com/louhacquetdelepine/" rel="noopener ugc nofollow" target="_blank">楼高清</a></p></figure><p id="3c71" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在BN论文发表之前，这种解决方案已经广为人知。对于BN，[1]的作者希望将这种方法扩展到隐藏层，以帮助训练。</p><h2 id="e452" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">内部协变量转移恶化训练:原始论文假说</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pq"><img src="../Images/1dfad9965ae8c6e4f6a880b5a986e479.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uGhOg55BEqkmJA93d_TgrQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">图7:分布稳定性视角下的内部协变量移位(ICS)原理</strong>(ICS _ distrib)。<strong class="bd oe"> | </strong>鸣谢:作者-设计:<a class="ae le" href="https://www.instagram.com/louhacquetdelepine/" rel="noopener ugc nofollow" target="_blank">楼高清</a></p></figure><p id="f60e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们的汽车分类器中，我们可以将隐藏层视为单元，当它们识别出一些与汽车相关的“概念”特征时，就会被激活:它可能是一个车轮、一个轮胎或一扇门。我们可以假设前面描述的效果会发生在隐藏的单元中。有一定方向角的轮子会激活特定分布的神经元。理想情况下，我们希望<strong class="lh ja">让一些神经元对任何车轮方向</strong>的可比分布做出反应，这样模型就可以有效地推断出输入图像包含汽车的概率。</p><p id="7db3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果输入信号中存在巨大的协变量偏移，<strong class="lh ja">优化器将很难很好地推广</strong>。相反，如果输入信号总是遵循标准的正态分布，那么优化器将更容易泛化。考虑到这一点，[1]的作者应用了在隐藏层标准化信号的策略。他们假设强制(𝜇 = 0，σ = 1)中间信号分布将<strong class="lh ja">有助于网络在特征的“概念”级别上的泛化</strong>。</p><p id="18b4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，我们并不总是希望隐藏单元中的标准正态分布。这会降低模型的代表性:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pr"><img src="../Images/bd003401a4c5012fa6eadba62f9395e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_9pcc30N9SBPgmIbxVQ8EA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">图表8 : </strong> <strong class="bd oe">我们为什么不总是要一个标准的正态分布在隐单元中。</strong>此处，sigmoid函数仅在其线性范围内有效。<strong class="bd oe"> | </strong>鸣谢:作者-设计:<a class="ae le" href="https://www.instagram.com/louhacquetdelepine/" rel="noopener ugc nofollow" target="_blank">娄高清</a></p></figure><p id="f729" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最初的文章以sigmoid函数为例，说明为什么标准化本身就是一个问题。<strong class="lh ja"> </strong>如果输入信号值介于0和1之间，<strong class="lh ja">非线性函数仅在其线性范围内工作</strong>。听起来有问题。</p><p id="bfec" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了解决这个问题，他们添加了两个可训练参数<em class="oc"> 𝛽 </em>和<em class="oc"> 𝛾 </em>，允许优化器为特定任务定义最佳均值(使用<em class="oc"> 𝛽 </em>和标准偏差(使用<em class="oc"> 𝛾 </em>)。</p><p id="68b6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">⚠Warning:下面的假设现在已经过时了。许多关于BN的伟大内容仍然声称它是使方法在实践中起作用的原因。然而，最近的作品对它提出了严峻的挑战。</p><p id="6a48" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在[1]发布后的几年里，DL社区对BN有效性的解释如下:</p><blockquote class="pf pg ph"><p id="4cab" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated"><strong class="lh ja"> —假设1———</strong></p><p id="473a" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">BN ➜ <strong class="lh ja">输入信号的标准化</strong>在隐藏单元内➜添加两个可训练参数到<strong class="lh ja">调整分布</strong>并最大限度地利用非线性➜更容易训练</p></blockquote><p id="6603" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里，归一化到(𝜇 = 0，σ = 1)是BN有效性的主要解释。<strong class="lh ja">这一假设已经受到挑战</strong>(见第C.3.3节)，被另一个假设所取代:</p><blockquote class="pf pg ph"><p id="e6fe" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated"><strong class="lh ja"> —假设2————</strong></p><p id="4617" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">隐藏单元内输入信号的BN ➜归一化➜ <strong class="lh ja">减少隐藏层之间的相互依赖性</strong>(从分布稳定性角度看)➜更容易训练</p></blockquote><p id="ce3e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有一个微小但非常重要的区别。这里，<strong class="lh ja">规范化的目的是减少层之间的相互依赖性</strong>(从分布稳定性的角度来看)，因此优化器可以通过只调整两个参数来选择最佳分布！让我们进一步探讨这个假设。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/0a09b3098b02575bbe2e6f311a1506ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7tK-KHAf4ai7S-kXuZw0PQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">批量归一化和这张图有什么联系？</strong> |信用:<a class="ae le" href="https://unsplash.com/@daniloalvesd?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">达尼洛·阿尔维斯</a>在<a class="ae le" href="https://unsplash.com/s/photos/taps?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上</p></figure><h2 id="e773" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">C.3.2)假设n 2 — BN减轻了训练期间隐藏层之间的相互依赖性</h2><p id="b311" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">关于这一节:我找不到任何关于这一节所讨论的假设的确凿证据。因此，我决定主要依靠Ian Goodfellow对那件事的解释(特别是在 <a class="ae le" href="https://www.youtube.com/watch?v=Xogn6veSyxA" rel="noopener ugc nofollow" target="_blank"> <em class="oc">这个精彩的视频</em> </a> <em class="oc">)。</em></p><p id="420c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">考虑下面的例子:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ps"><img src="../Images/9a2429729433368bccd21aa0fd08583a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PjRlq0sMfVpZKMkFvzc6CA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图9 : <strong class="bd oe">一个简单的DNN </strong>，它由线性变换组成。|灵感来自Ian Goodfellow</p></figure><p id="0b7a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中(a)、(b)、(c)、(d)和(e)是网络的顺序层。这里有一个非常简单的例子，所有的层都通过线性变换连接起来。让我们假设我们想要使用SGD来训练这样的模型。</p><p id="aded" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了更新层(a)的权重，我们需要根据网络的输出计算梯度:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/f6273ecdbd2d24e8c13745998198a4e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/1*uz4MGKFGA1HYB0XqSSGjBg.gif"/></div></figure><p id="886f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们首先考虑一个没有BN层的网络。从上面的等式，我们得出结论，如果所有梯度都很大，grad(a)就会很大。相反，如果所有梯度都很小，grad(a)几乎可以忽略不计。</p><p id="b68c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过观察隐藏单元的输入分布，很容易看出各层之间的依赖程度:( a)权重的修改将修改(b)权重的输入分布，这将最终修改(d)和(e)的输入信号。这种相互依赖性对于训练稳定性可能是有问题的:<strong class="lh ja">如果我们想要</strong> <strong class="lh ja">调整特定隐藏单元的输入分布，我们需要考虑整个层序列</strong>。</p><p id="53ff" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，SGD考虑层之间的一阶关系。所以他们没有考虑到上面提到的更高程度的关系！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pu"><img src="../Images/05e9edf3509bc5db476f75f9d2a5544f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i4zttMKhRAfArHL6_yvn_g.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">图解10 : </strong> <strong class="bd oe">假设n 2原理</strong>。BN层通过标准化每个隐藏单元内部的信号，使信号调节更加容易，并允许使用<em class="ou"> 𝛽 </em>和<em class="ou"> 𝛾 </em>进行分布调节。BN就像一个阀门，在某些点上使流量控制更容易，而不会恶化网络的潜在复杂性！<strong class="bd oe"> | </strong>鸣谢:作者-设计:<a class="ae le" href="https://www.instagram.com/louhacquetdelepine/" rel="noopener ugc nofollow" target="_blank">楼高清</a></p></figure><p id="f536" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">添加BN层显著降低了训练期间各层之间的相互依赖性(从分布稳定性的角度来看)。<strong class="lh ja">批量正常化就像一个阀门，阻止流量，并允许使用<em class="oc"> 𝛽 </em> et <em class="oc"> 𝛾 </em> </strong>进行调节。然后，不再需要考虑所有参数来获得隐藏单元内部分布的线索。</p><p id="fa39" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">备注:优化器可以进行更大的权重修改，而不会影响其他隐藏层中调整后的参数。它使超参数调谐方式更容易！</em></p><p id="db06" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这个例子抛开了声称BN有效性是由于中间信号分布的归一化到(𝜇 = 0，σ = 1)的假设。<br/>在这里，BN的目标是<strong class="lh ja">使优化器的工作更容易</strong>，允许它<strong class="lh ja">调整隐藏层分布</strong>，一次只需<strong class="lh ja">两个参数</strong>。</p><p id="e3c5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> ⚠ </strong>然而，请记住<strong class="lh ja">这主要是猜测</strong>。这些讨论应该被用作建立对BN的直觉的见解。我们仍然不知道为什么BN在实践中是有效的！</p><p id="d74d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">2019年，来自麻省理工学院的一组研究人员进行了一些关于BN的有趣实验[2]。他们的结果严重挑战了假设n 1 (仍然被许多严肃的博客帖子和MOOCS分享！).</p><p id="b63a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果我们想避免关于BN对训练的影响的“局部极小假设”,我们应该看看这篇论文。)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/33ac522834924fde0c8ba41e99d08b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dsRT__3smf0_ORKoqiWTpQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">好吧…你最好初始化好。 |鸣谢:<a class="ae le" href="https://unsplash.com/@tracyzhang?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">翠西张</a>上<a class="ae le" href="https://unsplash.com/s/photos/desert-landscape?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h2 id="fb36" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">C.3.3)假设n3-BN使优化前景更加平滑</h2><p id="421d" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated"><em class="oc">关于这一部分:我已经综合了[2]的结果，这些结果可以帮助我们对BN建立更好的直觉。我不可能面面俱到，这篇文章很密集，如果你对这些概念感兴趣，我建议你通读一遍。</em></p><p id="2aae" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们直接进入[2]的第二个实验。他们的目标是<strong class="lh ja">检验ICS和BN对训练表现的益处之间的相关性</strong>(假设1)。</p><p id="fa0b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">符号:我们现在用</em><strong class="lh ja"><em class="oc">ICS _ distrib</em></strong><em class="oc">来指代这个协变量移位。</em></p><p id="dc64" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为此，研究人员训练了三个VGG网络(在CIFAR-10上) :</p><ul class=""><li id="6a07" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">第一个没有任何BN层；</li><li id="837f" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">第二个确实有BN层；</li><li id="5062" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">第三个类似于第二个，除了在激活之前<strong class="lh ja">他们已经在隐藏单元</strong>中明确添加了一些ICS_distrib(通过添加随机偏差&amp;方差)。</li></ul><p id="6876" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">他们测量了每个模型达到的精度，以及分布值随迭代的演变。以下是他们得到的结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pv"><img src="../Images/db3be87cfaa2dee73704f94ea721d180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMX7fn4-q_cMG1CyLjUIjA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图6:<strong class="bd oe">ICS _ distrib |</strong>上的BN，有BN的网络比标准的网络训练速度快；在受管制的网络上明确增加ICS_distrib不会损害BN的利益。|来源:[2]</p></figure><p id="4396" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以看到，第三个网络的ICS非常高。然而，<strong class="lh ja">嘈杂的网络仍然比标准网络训练得更快。</strong>其达到的性能可与使用标准BN网络获得的性能相媲美。这个结果表明<strong class="lh ja"> BN有效性与ICS_distrib无关。</strong>哎呀！</p><p id="d1d6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们不应该太快抛弃ICS理论:如果BN有效性不是来自ICS_distrib，<strong class="lh ja">它可能与ICS的另一个定义</strong>有关。毕竟，假设n 1背后的直觉是有道理的，不是吗？</p><p id="f118" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">ICS_distrib的主要问题是它的定义与隐藏单元的输入分布有关。所以和优化问题本身没有直接联系。</p><p id="3ccc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2]的作者提出了ICS的另一个定义:</p><p id="5d48" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们考虑一个固定的输入x。</p><blockquote class="pf pg ph"><p id="52e5" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">我们从<strong class="lh ja">优化</strong>的角度将<strong class="lh ja">内部协变量移位</strong>定义为在反向传播误差<strong class="lh ja"> L(X)_it </strong>之后在隐藏层k上计算的<strong class="lh ja">梯度</strong>与在权重的<strong class="lh ja">迭代= </strong> <strong class="lh ja"> it </strong>更新之后根据损失<strong class="lh ja"> L(X)_it+1 </strong>在同一层k上计算的梯度之间的差。</p></blockquote><p id="929f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">该定义旨在<strong class="lh ja">更多地关注梯度</strong>而不是隐藏层输入分布，假设它可以为我们提供更好的线索，说明ICS如何影响<strong class="lh ja">底层优化问题</strong>。</p><p id="5137" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="oc">符号:</em><strong class="lh ja"><em class="oc">ICS _ opti</em></strong><em class="oc">现在指的是从优化角度定义的ICS。</em></p><p id="73f6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在下一个实验中，作者评估ICS_opti对训练绩效的影响。为此，他们测量了具有和不具有BN层的DNN在训练期间ICS_opti的变化。为了量化ICS_opti定义中提到的梯度变化，他们计算:</p><ul class=""><li id="8bb5" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated"><strong class="lh ja"> L2差:</strong>在权重更新之前和之后，梯度是否具有接近的范数？<em class="oc">理想情况下:0 </em></li><li id="07ba" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja">余弦角度</strong>:在权重更新前后，渐变是否具有相近的方向？<em class="oc">理想情况下:1 </em></li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pw"><img src="../Images/6818b05ab116f0d42c028c6412eb1f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h8ColkG0OCM_I0Ykddm6Cw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">图7 : </strong> <strong class="bd oe"> BN对ICS_opti的影响</strong> <strong class="bd oe"> | </strong> L2 diff和余弦角暗示BN并没有阻止ICS_opti(似乎稍微增加了一点，不知怎么的)。|来源[2]</p></figure><p id="1bc8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">结果有点令人惊讶:依赖于BN <strong class="lh ja">的网络似乎比标准网络具有更高的ICS_opti </strong>。记住带BN(蓝色曲线)的网络比标准网络(红色曲线)训练得快！</p><p id="1c77" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">ICS似乎肯定与训练表现无关…至少对于ICS的定义是这样的。</p><p id="6baf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不知何故，批量归一化对网络产生了<strong class="lh ja">另一个</strong> <strong class="lh ja">的影响</strong>，这使得收敛更容易。</p><p id="2c24" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，让我们研究一下<strong class="lh ja">BN如何影响</strong> <strong class="lh ja">优化格局</strong>。我们可能会在那里找到线索。</p><p id="2342" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是这个故事中的最后一个实验:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi px"><img src="../Images/5fe2d4eefdd1de09bac6c403aac1ff81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJLWWgpDxsQpp1mBK4SxwA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">图11 : </strong> <strong class="bd oe">渐变方向的优化景观探索</strong>。论文中进行的实验[2] |灵感来自<a class="ae le" href="https://www.microsoft.com/en-us/research/video/how-does-batch-normalization-help-optimization/" rel="noopener ugc nofollow" target="_blank">安德鲁·</a>-设计:<a class="ae le" href="https://www.instagram.com/louhacquetdelepine/" rel="noopener ugc nofollow" target="_blank">娄高清</a></p></figure><p id="80fd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从单个梯度，我们用不同的<strong class="lh ja">优化步骤</strong>更新权重(其作用类似于学习率)。直观地说，我们从特征空间中的<strong class="lh ja">某个点</strong>(即<strong class="lh ja">网络配置ω) </strong>定义一个方向，然后<strong class="lh ja">在这个方向</strong>上越来越远地探索优化景观。</p><p id="2d9f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在每一步，我们测量<strong class="lh ja">坡度</strong>和<strong class="lh ja">损失</strong>。因此，我们可以将优化领域的不同点与起点进行比较。如果我们测量大的变化，那么<strong class="lh ja">景观非常不稳定</strong>并且梯度是不确定的:<strong class="lh ja">大的步骤可能会恶化优化</strong>。相反，如果测量的变化很小，则<strong class="lh ja">景观是稳定的</strong>并且梯度是可信的:<strong class="lh ja">我们可以应用更大的步长而不损害优化</strong>。换句话说，我们可以用一个<strong class="lh ja">更大的学习速率</strong>，并使<strong class="lh ja">收敛得更快</strong>(BN的一个众所周知的性质)。</p><p id="67fe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们来看看结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi py"><img src="../Images/6b0fa8570fd1e9644aacf59ebe01f82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mUd9NnIbovywfaqPFU77rw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oe">图8 : </strong> <strong class="bd oe"> BN对优化景观平滑的影响</strong> |使用BN显著减少梯度变化。|来源:[2]</p></figure><p id="d60d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以清楚地看到<strong class="lh ja">BN层的优化前景更加平滑</strong>。</p><p id="d5ec" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们终于有了可以用来解释BN有效性的结果:BN层在某种程度上使得优化前景更加平滑。这使得优化器的工作更容易:我们可以定义一个更大的学习率，而不会受到梯度消失(权重停留在突然平坦的表面上)或梯度爆炸(权重突然落入局部最小值)的影响。</p><p id="8c26" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以知道公式化的第三个假设，由[2]提出:</p><blockquote class="pf pg ph"><p id="04cb" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated"><strong class="lh ja"> —假设三</strong> : — — — — — —</p><p id="9f1e" class="lf lg oc lh b li lj ka lk ll lm kd ln pi lp lq lr pj lt lu lv pk lx ly lz ma ij bi translated">BN ➜对隐单元➜内部输入信号的归一化使得<strong class="lh ja">优化景观更平滑</strong> ➜更快更稳定的训练</p></blockquote><p id="058b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这又引出了另一个问题:<strong class="lh ja">BN是如何让优化版图变得更加平滑的？</strong></p><p id="9999" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2]的作者也从理论的角度探讨了这些问题。他们的工作很有启发性，有助于更好地掌握批处理规范化的平滑效果。特别是，他们表明BN使优化景观更加平滑，而<strong class="lh ja">保留了正常景观的所有最小值</strong>。换句话说，<strong class="lh ja"> BN重新参数化底层优化问题，使训练更快更容易</strong>！</p><p id="a34b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> ⚠ </strong>在额外的研究中，【2】的作者观察到<strong class="lh ja">这种效应并不是BN </strong>独有的。他们用其他<strong class="lh ja">归一化方法</strong>(例如L1或L2归一化)获得了相似的训练性能。这些观察表明BN的有效性主要是由于<strong class="lh ja">偶然发现</strong>，利用了我们还没有完全识别的潜在机制。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pz"><img src="../Images/12901bd8d470964c7874a3f048f1d6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8meVdXwZETot-72nKXAd5A.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">现在是时候设定一个非常高的学习率了。 |信用:<a class="ae le" href="https://unsplash.com/@finding_dan?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">寻找丹|丹·格林维斯</a>上<a class="ae le" href="https://unsplash.com/s/photos/desert?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="b621" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作为本节的总结，本文<strong class="lh ja">对</strong>广为接受的观点提出了严峻挑战，即<strong class="lh ja"> BN的有效性主要是由于ICS的减少</strong>(从训练稳定性分布的角度，以及从优化的角度)。但是，它强调了<strong class="lh ja"> BN平滑效果对优化前景</strong>的影响。</p><p id="09f8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然本文陈述了一个关于<strong class="lh ja"> BN对训练速度</strong>影响的假设，但它并没有回答为什么<strong class="lh ja"> BN有助于推广</strong>。</p><p id="af81" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">他们简要地争辩说,<strong class="lh ja">使最优化前景更加平滑可以帮助模型收敛于平坦的最小值</strong>,后者具有<strong class="lh ja">更好的概括属性</strong>。不过，没有更多细节了。</p><p id="a6dd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">他们的主要贡献是挑战了普遍承认的BN效应对ICS的影响——这已经很显著了！</p><h2 id="3fb5" class="of ng iq bd nh og oh dn nl oi oj dp np lo ok ol nr ls om on nt lw oo op nv iw bi translated">C.4)总结:BN为什么管用？目前我们知道些什么</h2><ul class=""><li id="bba9" class="mk ml iq lh b li nx ll ny lo qa ls qb lw qc ma mp mq mr ms bi translated"><strong class="lh ja">假设1 </strong> : BN层<strong class="lh ja">减轻内部协变量偏移</strong> (ICS)</li></ul><p id="a777" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">❌ <strong class="lh ja">错:</strong>【2】表明，在实践中，<strong class="lh ja">ics和训练成绩之间没有相关性</strong>。</p><ul class=""><li id="5f78" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated"><strong class="lh ja">假设2 </strong> : BN层通过允许其<strong class="lh ja">仅用两个参数</strong>调整隐藏单元的输入分布，使优化器的工作<strong class="lh ja"> </strong>更加容易。</li></ul><p id="88ba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">❓ <strong class="lh ja">也许:</strong>这种假设强调了参数之间的相互依赖性，使得优化任务更加困难。<strong class="lh ja">没有确凿的证据，</strong>尽管如此。</p><ul class=""><li id="3c2d" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated"><strong class="lh ja">假设三</strong> : BN层<strong class="lh ja">重新参数化底层优化</strong>问题，使其更加平滑稳定。</li></ul><p id="6b61" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">❓:也许吧:他们的结果是最近才有的。据我所知，到目前为止，他们还没有受到挑战。他们提供了经验证明和一些理论论证，但是一些基本问题仍然没有答案(比如“BN如何帮助一般化？”).</p><p id="e90c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">讨论</strong>:在我看来，后两种假设是兼容的。直观地说，我们可以把假设n 2看作是从一个有许多参数的问题到许多有几个参数的问题的投影；一种降维，这将有助于推广。有什么想法吗？</p><p id="b62c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">许多问题仍然悬而未决，批处理规范化仍然是当今的一个研究课题。讨论这些假设仍然有助于更好地理解这种常用的方法，抛弃一些我们已经想了几年的错误说法。</p><p id="6192" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，这些问题并不妨碍我们在实践中利用BN的优势！</p><h1 id="ff33" class="nf ng iq bd nh ni ox nk nl nm oy no np kf oz kg nr ki pa kj nt kl pb km nv nw bi translated">结论</h1><p id="5e91" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated"><strong class="lh ja">批量归一化</strong> (BN) <strong class="lh ja"> </strong>是<strong class="lh ja">近年来在<strong class="lh ja">深度学习</strong> (DL)领域最重要的进展</strong>之一。依靠两次连续的线性变换，这种方法使得深度神经网络(DNN) <strong class="lh ja">训练更快</strong>和<strong class="lh ja">更稳定</strong>。</p><p id="5528" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关于什么使得BN在实践中有效，最广为接受的假设是在训练期间隐藏层之间的<strong class="lh ja">相互依赖</strong>的<strong class="lh ja">减少</strong>。然而，<strong class="lh ja">优化景观平滑度</strong>的归一化变换影响似乎是BN <strong class="lh ja">有效性</strong>的重要机制。</p><p id="32a1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在许多常用的DNN都依赖于BN(例如:ResNet [4]，EfficientNet [5]，…)。</p><p id="a47c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你对深度学习感兴趣，你肯定要熟悉这种方法！</p><h1 id="6bf0" class="nf ng iq bd nh ni ox nk nl nm oy no np kf oz kg nr ki pa kj nt kl pb km nv nw bi translated">开放问题</h1><p id="097f" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">即使BN在多年的实践中看起来是有效的，许多关于其潜在机制的问题仍然没有答案。</p><p id="32db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以下是关于BN的未决问题的非详尽列表:</p><ul class=""><li id="f102" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">BN如何帮助<strong class="lh ja">泛化</strong>？</li><li id="b866" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">BN是使优化更容易的最佳归一化方法吗？</li><li id="de77" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja"> <em class="oc"> 𝛽 </em>等人<em class="oc"> 𝛾 </em> </strong>如何影响优化景观平滑度？</li><li id="36a5" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">[2]进行的探索优化景观的实验集中在<strong class="lh ja">BN对梯度</strong>的短期影响:他们测量了几个步长值的<strong class="lh ja">单次迭代</strong>的梯度&amp;损失的变化。BN如何长期影响坡度<strong class="lh ja"/>？权重的相互依赖性对优化前景有其他有趣的影响吗？</li></ul><h1 id="73dc" class="nf ng iq bd nh ni ox nk nl nm oy no np kf oz kg nr ki pa kj nt kl pb km nv nw bi translated">承认</h1><p id="ab2e" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">非常感谢Lou haquet-dele pine绘制的所有图表，以及她在校对方面的全面帮助！</p><h1 id="c2a9" class="nf ng iq bd nh ni ox nk nl nm oy no np kf oz kg nr ki pa kj nt kl pb km nv nw bi translated">参考</h1><p id="5af4" class="pw-post-body-paragraph lf lg iq lh b li nx ka lk ll ny kd ln lo nz lq lr ls oa lu lv lw ob ly lz ma ij bi translated">[1]约夫，s .，&amp;塞格迪，C. (2015)。<a class="ae le" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">批量归一化:通过减少内部协变量移位加速深度网络训练</a>，<em class="oc"> arXiv预印本arXiv:1502.03167 </em>。</p><p id="5587" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2]桑图尔卡尔、齐普拉斯、易勒雅斯和马德瑞(2018年)。<a class="ae le" href="https://arxiv.org/abs/1805.11604" rel="noopener ugc nofollow" target="_blank">批量规范化如何帮助优化？</a>、<em class="oc">神经信息处理系统的进展</em></p><p id="1834" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3] Szegedy，c .，Liu，w .，Jia，y .，Sermanet，p .，Reed，s .，Anguelov，d，…和Rabinovich，A. (2015)。<a class="ae le" href="https://arxiv.org/pdf/1409.4842.pdf" rel="noopener ugc nofollow" target="_blank">深入卷积</a>，<em class="oc">IEEE计算机视觉和模式识别会议论文集</em></p><p id="77d3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[4]何，王，张，徐，任，孙等(2016)。<a class="ae le" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a>。在<em class="oc">IEEE计算机视觉和模式识别会议论文集</em></p><p id="4276" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[5]谭，m .，，乐，2019 .<a class="ae le" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank"> Efficientnet:反思卷积神经网络的模型缩放</a>，<em class="oc"> arXiv预印本arXiv:1905.11946 </em>。</p><p id="d0b9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[6] Goodfellow，I .、Pouget-Abadie，j .、Mirza，m .、Xu，b .、Warde-Farley，d .、Ozair，s .、A. Bengio，Y. (2014)，<a class="ae le" href="http://papers.nips.cc/paper/5423-generative-adversarial-nets" rel="noopener ugc nofollow" target="_blank">生成对抗网络，</a> <em class="oc">神经信息处理系统进展</em></p><h1 id="75d3" class="nf ng iq bd nh ni ox nk nl nm oy no np kf oz kg nr ki pa kj nt kl pb km nv nw bi translated">更进一步</h1><ul class=""><li id="7a14" class="mk ml iq lh b li nx ll ny lo qa ls qb lw qc ma mp mq mr ms bi translated">伊恩·古德费勒的精彩演讲。他在课程开始时谈到了批量标准化:<a class="ae le" href="https://www.youtube.com/watch?v=Xogn6veSyxA" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="fcd8" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">一名作者口头介绍论文[2]。观众提出尖锐的问题，开启关于国阵的精彩辩论:<a class="ae le" href="https://www.microsoft.com/en-us/research/video/how-does-batch-normalization-help-optimization/" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="020f" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">"我们应该把BN放在激活之前还是之后？"在stackoverflow上:<a class="ae le" href="https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="3497" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">"我们应该把BN放在激活之前还是之后？"在reddit上:<a class="ae le" href="https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/dgqaksn/" rel="noopener ugc nofollow" target="_blank">链接</a></li></ul></div></div>    
</body>
</html>