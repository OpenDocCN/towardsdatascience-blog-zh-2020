<html>
<head>
<title>Understanding SPPNet for Object Classification and Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解用于对象分类和检测的SPPNet</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-sppnet-for-object-detection-and-classification-682d6d2bdfb?source=collection_archive---------8-----------------------#2020-11-01">https://towardsdatascience.com/understanding-sppnet-for-object-detection-and-classification-682d6d2bdfb?source=collection_archive---------8-----------------------#2020-11-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3bba" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">SPPNet的分析与评述</h2><div class=""/><div class=""><h2 id="67f8" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">SPPNet允许可变大小的CNN输入图像，并可用于分类和对象检测</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e4c78533de640c499ee5b590402f6830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Dm5SH1rHazvgSeYM"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">谢尔盖·阿库利奇在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="723f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在论文“<a class="ae le" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络中的空间金字塔池</a>”中，介绍了一种称为空间金字塔池层的技术，该技术使得CNN模型与输入图像大小无关。它是ILSVRC 2014 中物体检测的亚军和分类挑战的亚军，因此值得一读。</p><p id="048a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇文章中，我解释了SPP层，然后回顾了整篇文章。博客的结构是学生和老师之间的对话。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="4b34" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">学生</h2><p id="0d13" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">我希望了解更多关于物体检测的知识。上次，你已经解释了用于物体探测的R-CNN网络。<strong class="lh ja">在R-CNN之后，在目标检测领域还有其他研究吗？</strong></p><h2 id="aa1f" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">教师</h2><p id="3b1f" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">在R-CNN之后不久，SPP-Net被引入。与R-CNN相比，SPPNet使模型与输入图像大小无关，并显著提高了边界框预测速度，而不影响地图。我将首先解释如何使模型不可知的输入图像大小。</p><p id="d007" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">CNN网络的固定大小限制不是因为卷积层，而是因为全连接(FC)层。单个卷积层或一组卷积层获取图像，并产生与输入图像的特定比率(称为子采样比率，稍后解释)成比例的特征图。但是对于完全连接的层，输入必须是固定长度的向量。为了解决这个问题，作者用一个<strong class="lh ja">空间金字塔池(SPP) </strong>层替换了最后一个池层(FC层之前的层)。</p><blockquote class="nf ng nh"><p id="df51" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">注意:</strong>SPP方法受<a class="ae le" rel="noopener" target="_blank" href="/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb">词汇袋</a>方法的启发。不需要对单词包有深入的理解，但是了解它将有助于对概念的理解。</p></blockquote><h2 id="8967" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">学生</h2><p id="1a26" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">我所看到的池层有一个固定的大小，比如说，2 x 2，两个方向的步幅都是2。所以在这种情况下，输出与输入保持比例。<strong class="lh ja">池层如何解决问题？</strong></p><h2 id="0ecf" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">教师</h2><p id="54f3" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">在上面描述的池层中，您已经设置了固定步幅(2)和固定窗口大小(2 x 2)。正因为如此，你的输出总是与输入成正比。现在<strong class="lh ja">如果你让池窗口和步幅与输入图像成比例，你总能得到固定大小的输出。</strong>此外，SPP层不仅仅应用一个池化操作，它还应用了几个不同输出大小的池化操作(这就是相同操作的来源——空间金字塔池化),并在将结果发送到下一层之前将其合并。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/9af4ce0aebaf2713ae301cb926502a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fJsQD0oKJJLkBIUQwHEw_g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">空间金字塔池(SPP)层— <a class="ae le" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="e595" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在上图中，作者使用了三个池操作，其中一个操作只为每个地图输出一个数字。另一个为每个贴图提供2 x 2的网格输出，同样，最后一个提供4 x 4的输出。该操作被应用于由先前卷积操作给出的每个特征图(在上述情况下为256个图)。SPP层输出被展平并发送到FC层。</p><p id="f64f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在要计算窗口大小和步幅大小，让我们声明一些变量和公式。</p><ul class=""><li id="ec86" class="nn no iq lh b li lj ll lm lo np ls nq lw nr ma ns nt nu nv bi translated">假设输入到SPP层的特征地图的大小为<code class="fe nw nx ny nz b">[a x a]<strong class="lh ja">↔</strong>[13 x 13]</code></li><li id="9ef3" class="nn no iq lh b li oa ll ob lo oc ls od lw oe ma ns nt nu nv bi translated">我们需要单个地图的输出大小为<code class="fe nw nx ny nz b">[n x n]<strong class="lh ja">↔</strong>[4 x 4]</code></li><li id="c3e4" class="nn no iq lh b li oa ll ob lo oc ls od lw oe ma ns nt nu nv bi translated"><code class="fe nw nx ny nz b">window = ceiling(a/n)</code> <strong class="lh ja"> ↔ </strong>天花板(13/4) <strong class="lh ja"> ↔4 </strong></li><li id="bd89" class="nn no iq lh b li oa ll ob lo oc ls od lw oe ma ns nt nu nv bi translated"><code class="fe nw nx ny nz b">stride = floor(a/n)</code> <strong class="lh ja"> ↔ </strong>楼(13/4) <strong class="lh ja"> ↔3 </strong></li></ul><p id="3b02" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在在[13 x 13]地图上，应用上面的窗口([4 x 4])和步幅([3 x 3])，我们得到的输出是[4 x 4]。该操作应用于所有的特征映射，对于256个特征映射，我们得到[4 x 4 x 256]的输出。因此，现在我们可以改变[n x n]网格大小，以获得所需的输出大小。</p><h2 id="c141" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">学生</h2><p id="1e56" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">SPP层是网络的直观设计，因为它使CNN模型产生的结果与输入图像大小无关。所以根据我的理解，<strong class="lh ja">在把特征发送到全连接层之前，我可以把这个层应用到任何CNN。作者在本文中使用了哪些模型？</strong>此外，由于在FC层之前引入了一个全新的层，因此<strong class="lh ja">对于分类任务，作者是否使用了任何预训练的架构或者他们从一开始就训练了一个模型？</strong></p><h2 id="5e56" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">教师</h2><p id="8792" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">人们可以将SPP层应用于任何CNN架构。但是，我们谈论的是2014年的时间。当时出席的模特不多。作者使用了ZF网、AlexNet和over fat(5层和7层)架构。然而，通过改变填充来修改这些网络，以获得期望的输出特征图。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/db7aa038a25173b21eaca60867db1252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ox_XQXLZ7fTznLSJQaCRgA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">SPPNet中使用的CNN模型架构— <a class="ae le" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="8db2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作者在ImageNet 2012数据集上训练了该模型，并提供了对训练细节的详细分析，并将它们与没有在其上使用SPP层的当代模型进行了比较。然而，在深入本质细节之前，让我们先弄清楚一些定义。</p><blockquote class="nf ng nh"><p id="38d8" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">多尺寸/多比例图像:</strong>改变输入图像尺寸</p><p id="a879" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">多视图:</strong>使用图像增强——从输入图像中截取部分并翻转。</p></blockquote><p id="fe7f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">首先，作者在模型上用一个单一大小的输入[224 x 224]训练，然后用一个可变大小的输入[224 x 224]和[180 x 180]训练。作者在固定大小的图像上执行训练，以利用GPU实现的优势。在为<strong class="lh ja">多尺寸</strong>训练时，它们在一个时期发送相同的图像尺寸，然后改变到另一个时期。用于上述两种训练的金字塔是:{6 x 6，3 x 3，2 x 2，1 x 1}。</p><blockquote class="nf ng nh"><p id="5a9a" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">注意:</strong>为了训练，图像被调整大小，使得最小值(高度、宽度)等于256，并且剩余部分基于纵横比被调整。之后，从中心和四个角拾取224 x 224的作物，给我们总共5个224 x 224的图像。图像被翻转以从相同的位置产生5个以上的图像，从每个输入图像给出总共10个图像。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/c3a65afad6c0e2d5aab213c5d0d0f5ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B_J3I84Ae7-wlbslSxAlww.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">ImageNet 12验证集分析— <a class="ae le" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="a86e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作者将<strong class="lh ja">单一大小</strong>训练的SPP模型与没有SPP的模型进行了比较，发现误差减少了。为了证明误差的减少是由于SPP层而不是由于参数的增加，他们将SPP层的金字塔更改为{4 x 4，3 x 3，2 x 2，1 x 1}，这大大减少了参数，但误差仅略有增加。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/8f23a78342780dd0366e815497de795c.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*zOzriM-CvkxaQJ-JPt8FgA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">对填充尺寸和裁剪图像的验证— <a class="ae le" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">纸张</a></p></figure><p id="d8b4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作者还尝试用224次裁剪和不用224次裁剪来检验该模型，并对结果进行了分析。观察到误差减少，如上图所示。根据这种损失的减少，作者说:</p><blockquote class="oi"><p id="175a" class="oj ok iq bd ol om on oo op oq or ma dk translated">由此可见维护完整内容的重要性。</p></blockquote><h2 id="517e" class="mi mj iq bd mk ml os dn mn mo ot dp mq lo ou ms mt ls ov mv mw lw ow my mz iw bi translated">学生</h2><p id="a84d" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">从上面的数字，我们可以说SPPNet论文中引入的新层确实减少了分类任务的误差。<strong class="lh ja">我还想知道作者是如何推断出对象检测任务的模型的？</strong></p><h2 id="e683" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">教师</h2><p id="08af" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">作者可以使用该模型进行对象检测，并可以添加一个包围盒检测器来拍摄地图。然而，为了理解这是如何发生的，我们将首先理解<strong class="lh ja">子采样率</strong>的概念</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ox"><img src="../Images/7c394f68bc8cf1fbd0806329eec7ddb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9WidiTJXfLD0ATxYPqOfLg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">了解子采样率-作者提供的图像</p></figure><p id="566d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">子采样率可用于从输入图像形状确定CNN特征图的输出形状。简单地将输入图像尺寸乘以<strong class="lh ja">子采样率</strong>将得到特征图尺寸。我们一会儿会用到这个概念。</p><blockquote class="nf ng nh"><p id="4ea9" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">注意:</strong>填充确实会影响子采样率，因为我们可能需要根据填充增加/减少一个常数。文件附录中提供了详细信息。</p></blockquote><p id="cf81" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">SPPNet论文已经显示了用于对象检测的ZF网论文的分析。ZF网论文的次抽样比率为16。现在，对于对象检测，他们仍然使用选择性搜索算法来确定区域(每幅图像约2k个区域)的建议。但与R-CNN不同，它们不会将每个建议区域发送到CNN，而是将这些区域映射到最后一个卷积层(Conv5)的输出特征图。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/61fbc21550dc8602e5af5d4adc7b17d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5uhFdrDee3YmWJWrMnDrDQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">从图像到特征图的对象映射——来自视频的<a class="ae le" href="https://www.youtube.com/watch?v=wGa6ddEXg7w&amp;list=PL1GQaVhO4f_jLxOokW7CS5kY_J1t1T17S&amp;index=72" rel="noopener ugc nofollow" target="_blank">SS</a></p></figure><p id="08fb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在上面的图像中，图像中的对象被映射到特征映射。让我们看看数学，看看如何做到这一点。</p><ul class=""><li id="241c" class="nn no iq lh b li lj ll lm lo np ls nq lw nr ma ns nt nu nv bi translated">设图像尺寸为<code class="fe nw nx ny nz b">[img_height, img_width] </code><strong class="lh ja">↔</strong>【688×920】</li><li id="e1bb" class="nn no iq lh b li oa ll ob lo oc ls od lw oe ma ns nt nu nv bi translated">让出现在图像中的一个物体的中心在<code class="fe nw nx ny nz b">[x, y]</code><strong class="lh ja">↔</strong>【340，450】</li><li id="f4df" class="nn no iq lh b li oa ll ob lo oc ls od lw oe ma ns nt nu nv bi translated">上述物体的高度和宽度是<code class="fe nw nx ny nz b">[obj_height, obj_width]</code><strong class="lh ja">↔</strong>【320，128】</li><li id="c91e" class="nn no iq lh b li oa ll ob lo oc ls od lw oe ma ns nt nu nv bi translated">现在，为了将它们映射到特征图上相应的空间位置，我们简单地将它们乘以<code class="fe nw nx ny nz b">sub-sampling ratio (S)</code> <strong class="lh ja"> ↔ </strong> 16。</li><li id="c4d1" class="nn no iq lh b li oa ll ob lo oc ls od lw oe ma ns nt nu nv bi translated">特征地图的尺寸将会是<code class="fe nw nx ny nz b">[img_height * S, img_width * S]</code> ↔[43 x 58】</li><li id="6736" class="nn no iq lh b li oa ll ob lo oc ls od lw oe ma ns nt nu nv bi translated">特征图上的物体中心将在<code class="fe nw nx ny nz b">[x * S, y * S]</code> ↔[21，28】的空间位置</li><li id="5634" class="nn no iq lh b li oa ll ob lo oc ls od lw oe ma ns nt nu nv bi translated">特征图上对象的高度和宽度将为<code class="fe nw nx ny nz b">[obj_height * S, obj_width * S]</code> ↔[20 x 8】</li></ul><p id="7e9d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这样，我们可以将输入图像中的任何对象映射到输出特征图。对象的坐标也被投影到特征地图，然后只有该区域被发送到SPP层进行特征提取，然后发送到FC层。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/2525937a487e74acd095ad38a3836f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*IBz-tUp1CrHdqTa-DdibnA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">对象窗口到特征图的映射— <a class="ae le" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="77e2" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">学生</h2><p id="2faa" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">将输入图像中的区域投影到特征图上以避免在图像的相同部分上进行冗余的卷积运算是减少计算量的一种聪明的方法。<strong class="lh ja">与R-CNN相比，这种机制是否影响了检测图谱？</strong></p><h2 id="b1d9" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">教师</h2><p id="9de5" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">在比较它的性能之前，我们还需要了解一些细节。作者使用了单一大小([224 x 224]而已)的ImageNet预训练ZF-5网络。为了检测，他们使用了两种不同的方法进行预测:<strong class="lh ja">单尺度</strong> —其中最小值(高度，宽度)= 688，<strong class="lh ja">多尺度</strong>其中最小值(高度，宽度)= {480，576，688，864，1200}和其他尺寸根据长宽比调整大小。</p><p id="881b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在<strong class="lh ja">多尺度</strong>检测的情况下，作者使用了一种选择尺度的新策略，该尺度具有最接近对象的像素总数(224 x 224 = 50，176)，以确定该尺度中对象的存在。</p><p id="1fec" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">与R-CNN相比，在单尺度下，性能下降了0.5%，而在多尺度下，性能提高了0.7%。虽然地图的增加并不大，但速度却有相当大的提高。作者还比较了有和没有微调FC层的模型，并将其与R-CNN进行了比较，结果如下所示。</p><blockquote class="nf ng nh"><p id="907e" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">注:</strong>卷积层没有微调，只有FC层。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/c2b44c9ad8da70b4c3838e86feb019be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*Zr68Ie0zKylvRztZH-46dQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">与R-CNN在Pascal VOC 07 — <a class="ae le" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">论文上的比较</a></p></figure><blockquote class="nf ng nh"><p id="5c8c" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">注:</strong>我们已经讨论了Pascal VOC 2007数据集上的对象检测结果。</p></blockquote><p id="2e27" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">论文中的大部分关键概念都已经讨论过了。但是，我建议您阅读这篇论文，因为他们在分类和检测部分提供了更详细的分析。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="2827" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">理解SPP层使得学习在此之后构建的两阶段模型变得更加容易。我将很快在FastR-CNN的报纸上写作。敬请期待！！</p><h1 id="d8bb" class="pb mj iq bd mk pc pd pe mn pf pg ph mq kf pi kg mt ki pj kj mw kl pk km mz pl bi translated">参考</h1><p id="de6b" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">K.何，X张，s任，孙军，<a class="ae le" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络空间金字塔池</a>，，2014</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pm pn l"/></div></figure></div></div>    
</body>
</html>