<html>
<head>
<title>Selenium in Action</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">硒在起作用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/selenium-in-action-2fd56ad91be6?source=collection_archive---------25-----------------------#2020-09-27">https://towardsdatascience.com/selenium-in-action-2fd56ad91be6?source=collection_archive---------25-----------------------#2020-09-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="748a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">自动化示例</h2><div class=""/><div class=""><h2 id="db30" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">Python中的网络抓取和自动化</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/6a5c2fd01b891c962da00922f9ff101b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j8f4g_fewDD7A-Cn9ayv2g.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">卡尔·海尔达尔在<a class="ae le" href="https://unsplash.com/s/photos/web?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="4813" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">1.硒简介</h1><p id="93c9" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">你们中的许多人可能以前在Python中玩过web抓取。有几个Python包可以根据网页(HTML)元素提取网站信息；一些例子是<a class="ae le" href="https://pypi.org/project/beautifulsoup4/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>、<a class="ae le" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> Scrapy </a>和<a class="ae le" href="https://docs.python.org/3/library/urllib.html" rel="noopener ugc nofollow" target="_blank"> urllib </a> 2。这些包通常依赖XPATH或CSS等web元素来提取数据。它们各有利弊，但它们可以毫无问题地处理来自许多网站的抓取。</p><p id="12fc" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">然而，越来越多的网站采用JavaScript和其他方法来增加抓取的难度，甚至完全阻止抓取。例如，<a class="ae le" href="https://data.cityofnewyork.us/Social-Services/NYPD/fjn5-bxwg" rel="noopener ugc nofollow" target="_blank"> NYPD开放数据门户</a>的“下载”按钮直到用户点击“导出”按钮后才会显示。因此，使用XPATH或CSS的常规抓取方法将无法正常工作，因为直到用户单击按钮时才会找到web元素。在你开始拔头发之前，硒来拯救你了！</p><p id="a574" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">传统上，Selenium被设计用于web应用程序的自动化测试。这个包的本质是自动化浏览器行为，它可以在很多语言中使用，比如Java、Python、Ruby等。因为Selenium模仿用户行为，所以一个明显的缺点是速度。然而，由于像人一样自由地行动，Selenium是一个强大的web抓取工具，与传统的web抓取包相比，它的限制最少。让我们深入研究一下，看看它是如何工作的。</p><h1 id="3cbb" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">2.设置Selenium Webdriver</h1><p id="957b" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">为了部署Selenium，您需要根据您的操作系统为您的浏览器设置Webdriver。这里我将以Chrome为例。</p><ul class=""><li id="ef80" class="my mz iq lz b ma mt md mu mg na mk nb mo nc ms nd ne nf ng bi translated">第一步:找到你的Chrome版本。你可以在“关于谷歌浏览器”找到你的浏览器版本</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nh"><img src="../Images/7b7e1bf977561d00b59a2eb51576e281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hoPmioTA2s8UKWtyJ5hj5Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">关于谷歌浏览器</p></figure><ul class=""><li id="6638" class="my mz iq lz b ma mt md mu mg na mk nb mo nc ms nd ne nf ng bi translated">第二步:根据你的Chrome版本下载<a class="ae le" href="https://chromedriver.chromium.org/downloads" rel="noopener ugc nofollow" target="_blank"> <strong class="lz ja">网络驱动</strong> </a>。</li><li id="8b20" class="my mz iq lz b ma ni md nj mg nk mk nl mo nm ms nd ne nf ng bi translated">Windows用户的第3步:你需要下载基于你的系统(32位或64位)的Chrome Webdriver，然后将<code class="fe nn no np nq b">chromedriver.exe</code>放在你的路径中。</li><li id="b6ba" class="my mz iq lz b ma ni md nj mg nk mk nl mo nm ms nd ne nf ng bi translated">Mac用户第三步:下载Mac版Chrome驱动后，导航到这个目录<code class="fe nn no np nq b">/usr/local/</code>。然后检查<code class="fe nn no np nq b">bin</code>文件夹是否存在。如果不存在，创建一个<code class="fe nn no np nq b">bin</code>文件夹，将驱动程序放在<code class="fe nn no np nq b">/usr/local/bin</code>中。运行<code class="fe nn no np nq b">cd ~/bin &amp;&amp; chmod +x chromedriver</code>使其可执行。</li></ul><h1 id="5570" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">3.一个例子:USAC开放数据——电子汇率数据</h1><p id="909c" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">以下是练习中需要的包。让我们导入它们吧！</p><pre class="kp kq kr ks gt nr nq ns nt aw nu bi"><span id="802d" class="nv lg iq nq b gy nw nx l ny nz">from selenium import webdriver<br/>from selenium.webdriver.common.keys import Keys<br/>from selenium.webdriver import ActionChains<br/>from bs4 import BeautifulSoup<br/>import requests<br/>from urllib.parse import urljoin<br/>import pandas as pd<br/>import time</span></pre><p id="4c91" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">现在你已经设置好了Chrome Webdriver环境，让我们将Selenium投入使用吧！在本文中，我将使用USAC E-Rate数据门户作为一个例子。假设用户想要下载所有与E-rate相关的数据集，让我们在这个数据门户上设计用户路径，然后我们将尝试用Selenium复制用户操作。</p><h2 id="dfbd" class="nv lg iq bd lh oa ob dn ll oc od dp lp mg oe of lr mk og oh lt mo oi oj lv iw bi translated">步骤1:收集所有与E-Rate相关的数据集URL</h2><p id="dac3" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">我们将前往<a class="ae le" href="https://opendata.usac.org/" rel="noopener ugc nofollow" target="_blank"> USAC开放数据</a>搜索E-rate数据集。我们发现结果页面只有2页，因此我们可以编写以下代码来抓取所有与E-Rate数据相关的URL，供Selenium稍后使用。</p><pre class="kp kq kr ks gt nr nq ns nt aw nu bi"><span id="f7dc" class="nv lg iq nq b gy nw nx l ny nz">result_links = []<br/># Get all e-rate URLs from USAC data page<br/>for i in range(2): # 2 is the total number of pages<br/> url = ‘<a class="ae le" href="https://opendata.usac.org/browse?category=E-rate&amp;limitTo=datasets&amp;page='" rel="noopener ugc nofollow" target="_blank">https://opendata.usac.org/browse?category=E-rate&amp;limitTo=datasets&amp;page='</a> + str(i + 1)<br/> req = requests.get(url)<br/> soup = BeautifulSoup(req.content)<br/> erate_links = lambda tag: (getattr(tag, ‘name’, None) == ‘a’ and<br/> ‘href’ in tag.attrs and<br/> ‘e-rate’ in tag.get_text().lower())<br/> results = soup.find_all(erate_links)<br/> links = [urljoin(url, tag[‘href’]) for tag in results]<br/> links = [link for link in links if link.startswith(‘<a class="ae le" href="https://opendata.usac.org/E-rate'" rel="noopener ugc nofollow" target="_blank">https://opendata.usac.org/E-rate’</a>)]<br/> result_links.extend(links)</span><span id="00b3" class="nv lg iq nq b gy ok nx l ny nz">print(‘In total, there are ‘ + str(len(result_links)) + ‘ links retrieved.’)</span></pre><p id="b5b5" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">执行时，会弹出一条消息:“总共检索到15个链接。”</p><h2 id="ed09" class="nv lg iq bd lh oa ob dn ll oc od dp lp mg oe of lr mk og oh lt mo oi oj lv iw bi translated">步骤2:下载E-Rate数据集，用数据字典pdf抓取相关元数据</h2><p id="3a05" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">现在我们已经有了想要下载的数据集的所有链接，下一步将是下载它们。如果我们是用户，操作链将是:我们将点击链接，然后点击“导出”按钮，最后选择CSV选项进行下载。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/eda4ac92c2827a8d2afb445982fcf933.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GramK-9snVmY0A84wmSnpw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">导出按钮</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/1ee01d2df6c8fac4f305c6006d3f1e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bpjPnBZ2eAULQQjVCt_aSQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">CSV按钮</p></figure><p id="f299" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">一些额外的元数据也将有助于数据集文档。在数据集页面上，我们希望获得一些关键项目，包括简介、更新日期、联系人、电子邮件、更新频率和更新url。这些元数据可以在“关于这个数据集”一节中找到。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/15d09fe528c91b63942d631dd73ba4f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jntISo51vYHC5yuBVMCHNQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">元数据部分</p></figure><p id="788c" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这里有一个简单的例子，展示了如何使用Selenium来自动化重复性工作和构建数据收集。请注意，睡眠时间是任意的。这里的目标是像人类一样行动，所以确保你的行动链有意义，并分配一个适当的睡眠时间。</p><pre class="kp kq kr ks gt nr nq ns nt aw nu bi"><span id="0ed7" class="nv lg iq nq b gy nw nx l ny nz"># set up driver<br/>driver = webdriver.Chrome()</span><span id="1871" class="nv lg iq nq b gy ok nx l ny nz"># set up dataframe for metadata<br/>metadata = pd.DataFrame(columns = ['Intro', 'Update Date', 'Contact', 'Email', 'Update Freq', 'URL'])</span><span id="2fe9" class="nv lg iq nq b gy ok nx l ny nz">for i in range(len(result_links)):<br/>    # extract metadata by XPATH<br/>    data_dict = {}<br/>    driver.get(result_links[i])<br/>    intro = driver.find_element_by_xpath("//*[<a class="ae le" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>="app"]/div/div[2]/div[1]/section/div[2]/div/div[1]/table/tbody/tr[1]/td[2]/span").text<br/>    update_date = driver.find_element_by_xpath("//*[<a class="ae le" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>="app"]/div/div[2]/div[1]/section/div[2]/dl/div[1]/div/div[1]/div/dd").text<br/>    contact = driver.find_element_by_xpath("//*[<a class="ae le" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>="app"]/div/div[2]/div[1]/section/div[2]/dl/div[3]/div/div[2]/dd").text<br/>    email = driver.find_element_by_xpath("//*[<a class="ae le" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>="app"]/div/div[2]/div[1]/section/div[2]/div/div[1]/table/tbody/tr[4]/td[2]/span/a").text<br/>    update_freq = driver.find_element_by_xpath("//*[<a class="ae le" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>="app"]/div/div[2]/div[1]/section/div[2]/div/div[1]/table/tbody/tr[6]/td[2]/span").text<br/>    <br/>    # update data_dict with new metadata<br/>    data_dict.update({'Intro': intro, 'Update Date': update_date,<br/>                     'Contact': contact, 'Email': email, <br/>                     'Update Freq': update_freq, 'URL': result_links[i]})<br/>    # update dataframe<br/>    metadata = metadata.append(data_dict, ignore_index = True)<br/>    <br/>    time.sleep(5)<br/>    action = ActionChains(driver)<br/>    <br/>    # Click Show More<br/>    showmore = driver.find_element_by_xpath("//*[<a class="ae le" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>="app"]/div/div[2]/div[1]/section/div[2]/div/div[5]/a[1]")<br/>    action.move_to_element(showmore).perform()<br/>    showmore.click()<br/>    time.sleep(5)<br/>    <br/>    # Download glossary PDFs<br/>    glossary = driver.find_element_by_xpath("//a[contains(<a class="ae le" href="http://twitter.com/href" rel="noopener ugc nofollow" target="_blank">@href</a>, '.pdf')]")<br/>    glossary[0].click()<br/>    time.sleep(5)<br/>    <br/>    # Click Export<br/>    action = ActionChains(driver)<br/>    exportmenu = driver.find_element_by_xpath("//*[<a class="ae le" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>="app"]/div/div[1]/div/div/div[1]/div/div[2]/div/div[2]/button")<br/>    action.move_to_element(exportmenu).perform()<br/>    exportmenu.click()<br/>    <br/>    # Click to download CSV file<br/>    downloadmenu = driver.find_element_by_xpath("//*[<a class="ae le" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>="export-flannel"]/section/ul/li[1]/a")<br/>    action.move_to_element(downloadmenu).perform()<br/>    downloadmenu.click()</span></pre><p id="c0f0" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">如前所述，Selenium比大多数web抓取包都要慢。另一个明显的缺点是，如果网站的所有者改变了设计，所有的XPATHs都需要更新。但除此之外，它是一个非常强大的工具，我鼓励你去尝试和探索更多。</p><p id="48f3" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我的下一篇文章将是一篇教程，介绍如何使用Airflow来调度Selenium作业，使其更加强大。敬请期待:)</p><p id="fa31" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">参考:</p><ol class=""><li id="74aa" class="my mz iq lz b ma mt md mu mg na mk nb mo nc ms oo ne nf ng bi translated"><a class="ae le" href="https://zwbetz.com/download-chromedriver-binary-and-add-to-your-path-for-automated-functional-testing/" rel="noopener ugc nofollow" target="_blank">下载chromedriver二进制文件并添加到您的自动化功能测试路径中</a></li><li id="71f2" class="my mz iq lz b ma ni md nj mg nk mk nl mo nm ms oo ne nf ng bi translated"><a class="ae le" href="https://selenium-python.readthedocs.io/" rel="noopener ugc nofollow" target="_blank">硒与蟒蛇</a></li></ol></div></div>    
</body>
</html>