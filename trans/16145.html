<html>
<head>
<title>A Bigram Analysis of the EU General Data Protection Regulation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">欧盟通用数据保护法规的二元模型分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-bigram-analysis-of-the-eu-general-data-protection-regulation-33685e32bde7?source=collection_archive---------44-----------------------#2020-11-06">https://towardsdatascience.com/a-bigram-analysis-of-the-eu-general-data-protection-regulation-33685e32bde7?source=collection_archive---------44-----------------------#2020-11-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="82b4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用单词云可视化</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7c7393c7a24ab39db882455aabfab0a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZnYUBLcDMk5GdONR9FBug.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae kv" href="https://pixabay.com/users/mohamed_hassan-5229782/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3256079" rel="noopener ugc nofollow" target="_blank">穆罕默德·哈桑</a>拍摄，来自<a class="ae kv" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3256079" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a></p></figure><p id="554f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">《通用数据保护条例》( GDPR)于2018年5月实施，是欧盟乃至全世界迄今为止最全面的数据保护法。在这篇面向初学者的教程中，我将使用RWeka把GDPR的语料库可视化成R语言的bigram单词云。</p><h2 id="46e6" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated"><strong class="ak">字云及其弱点</strong></h2><p id="b44b" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">文字云是一个简单而强大的文本可视化工具。它允许您根据关键字在数据集中出现的频率，按比例打印不同大小的关键字。这是直观的和视觉上令人愉快的。但致命的是，它断章取义。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/a50182b279063c79ab596ab5e5ae9432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dCdL1yQ7atAopZS70xeQPw.png"/></div></div></figure><p id="1f1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，在上面的表示中，很容易理解“权威”是我们数据集中的一个突出的词。但是，因为它已经被剥离了周围的词，我们不知道它是在“宣战权”中作为权利使用，还是在“地方权力”中作为政府机构使用，还是在“有约束力的权力”中作为引用来源使用。</p><h2 id="f829" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated"><strong class="ak">二元模型和GDPR </strong></h2><p id="62a7" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">缓解这个问题的一个方法是制作一个两个单词的单词云。当然，即使是<em class="mr">二元词</em>——技术上称为双词复合词——也不能完全捕捉原始完整句子文本数据的细微差别和上下文。但幸运的是，GDPR的许多核心概念通常由两个词组成:</p><ul class=""><li id="751b" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated"><a class="ae kv" href="https://gdpr-info.eu/art-51-gdpr/" rel="noopener ugc nofollow" target="_blank">监督机构</a>:监督GDPR在欧盟成员国的应用的公共机构</li><li id="a1b9" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><a class="ae kv" href="https://gdpr-info.eu/art-63-gdpr/" rel="noopener ugc nofollow" target="_blank">一致性机制</a>:确保GDPR在整个欧盟范围内统一适用的合作体系</li><li id="a672" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><a class="ae kv" href="https://gdpr-info.eu/art-6-gdpr/" rel="noopener ugc nofollow" target="_blank">合法利益</a>:处理个人数据的六个合法理由之一</li></ul><p id="c61d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，进行二元语法分析而不是默认的一元语法分析是可取的，这将在语义上撕裂那些只有在复合形式下才有意义的概念。</p><h2 id="c9d7" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated"><strong class="ak">数据导入和语料库创建</strong></h2><p id="7b5f" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">让我们从安装和加载必要的包开始。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="08a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ni nj nk nl b">tm</code>包是一个广泛使用的文本挖掘工具，用于提炼原始文本数据。同样，<code class="fe ni nj nk nl b">RWeka</code>是一个简化复杂的预处理技术和其他机器学习算法的接口。在本教程中，我将专门使用它来将成对的单词组合成二元模型。然后我们将使用<code class="fe ni nj nk nl b">ggplot2</code>绘制一个探索图，最后打印我们的单词云。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="0c87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，加载文本数据并检查是否一切都完好无损。正文以该法案的官方长标题开始，以时任欧盟理事会主席的名字结束。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="b19b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">检查数据的前几行和后几行总是一个好习惯。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="12fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们使用<code class="fe ni nj nk nl b">VCorpus()</code>函数创建一个语料库。一个<a class="ae kv" href="https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf" rel="noopener ugc nofollow" target="_blank">语料库</a>是一个抽象结构，当我们使用<code class="fe ni nj nk nl b">tm</code>包时，它保存文档。在硬件层面上，<code class="fe ni nj nk nl b">VCorpus</code>(易失性语料库)将文档存储在内存中，而<code class="fe ni nj nk nl b">PCorpus</code>(永久性语料库)则利用单独的数据库。对于我们的目的来说，<code class="fe ni nj nk nl b">VCorpus</code>已经足够了——只是要记住，在清除内存或关闭RStudio时，corpus对象将会丢失。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="051d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在已经从文本文件创建了一个语料库，并准备挖掘。</p><h2 id="2c19" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">预处理</h2><p id="89ef" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">正如我们从上面的<code class="fe ni nj nk nl b">head()</code>和<code class="fe ni nj nk nl b">tail()</code>函数的结果中看到的，在机器看来，原始文本数据充满了拼写不一致。有数字、标点符号和大写字母，文本似乎也是双倍行距。我们将把它和<code class="fe ni nj nk nl b">tm</code>包统一起来，使它更容易使用。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="9d66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ni nj nk nl b">tm_map()</code>函数是简单文本细化的银弹。通过在第二个参数中提供不同的参数，如<code class="fe ni nj nk nl b">removeNumbers</code>和<code class="fe ni nj nk nl b">removePunctuation</code>，该函数从语料库中删除数字和标点符号。为了改变内容而不是简单地删除，例如从大写变成小写，我们使用<code class="fe ni nj nk nl b">content_transformer()</code>。</p><p id="c6d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后是停用词。停用词是由于它们在自然语言源中的语义可忽略性而应该被过滤掉的词。尽管它们缺乏实质意义，但它们经常在一篇文章中出现多次，并扭曲了频率分析的结果。例子包括冠词“the”和“a/an”以及介词“on”、“of”和“about”。"</p><p id="cb80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ni nj nk nl b">tm</code>包提供了一组不同语言的校对过的停用词。为了访问英语集，我们将参数设置为<code class="fe ni nj nk nl b">stopwords()</code>中的<code class="fe ni nj nk nl b">"english"</code>。因为这里只包含了最常见的例子，所以我添加了一组自定义的停用词。使用<code class="fe ni nj nk nl b">removeWords</code>，选择的停用词将被删除。最后，通过<code class="fe ni nj nk nl b">stripWhitespace</code>，我们去除了语料库中的空行和空格。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="13a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同一文本的第21行现在看起来“干净”了，可以让<code class="fe ni nj nk nl b">RWeka</code>包捆绑成有意义的二元模型。</p><p id="87a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，在我们继续之前，我们还必须执行基本的引理运算，以防止将同一个单词的不同形式作为单独的单词进行计数。词条释义是将一个单词的词形变化合并成标准词典形式的过程。例如，它可以帮助我们的机器将“runs”、“ran”和“running”都视为“run”的规范实例。</p><p id="c544" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有像<a class="ae kv" href="https://cran.r-project.org/web/packages/textstem/textstem.pdf" rel="noopener ugc nofollow" target="_blank"> textstem </a>这样的R包来促进这个过程。请随意试验它们，但是请记住，一些lemmatisation函数可能弊大于利。例如，他们可能会将“通用数据保护条例”变成“通用数据保护条例”在我们的例子中，我们想要做的就是将某些二元模型的复数形式视为单数，因此不值得冒这种破坏性改造的风险。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="2db2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了用单数形式替换复数形式，我们提供了<code class="fe ni nj nk nl b">gsub</code>函数作为<code class="fe ni nj nk nl b">content_transformer()</code>的参数。<code class="fe ni nj nk nl b">pattern</code>参数接收要替换的字符模式，而<code class="fe ni nj nk nl b">replacement</code>参数定义如何改变该模式。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="7abe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不幸的是，<code class="fe ni nj nk nl b">RWeka</code>包并不评估二元模型的语义可行性，而是简单地返回它在语料库中可以识别的每个词对。检查最重要的结果并消除无意义的分组是非常重要的。</p><h2 id="e83f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">符号化</h2><p id="d704" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">为了用<code class="fe ni nj nk nl b"><a class="ae kv" href="https://cran.r-project.org/web/packages/RWeka/RWeka.pdf" rel="noopener ugc nofollow" target="_blank">RWeka</a></code>生成二元模型，必须对语料库进行标记。记号化是将字符串分割成称为记号的更小单元的过程。令牌可以是一个单词、几个单词、一个句子或任何其他逻辑片段。在我们的二元模型分析中，一个token应该由两个单词组成，所以在<code class="fe ni nj nk nl b">NGramTokenizer()</code>中<code class="fe ni nj nk nl b">Weka_control()</code>的<code class="fe ni nj nk nl b">min</code>和<code class="fe ni nj nk nl b">max</code>参数中，分别输入<code class="fe ni nj nk nl b">2</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="0791" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们将数据塞进一个<a class="ae kv" href="https://www.rdocumentation.org/packages/tm/versions/0.7-7/topics/TermDocumentMatrix" rel="noopener ugc nofollow" target="_blank">文档术语矩阵</a>，这是一个数学矩阵，将术语的频率组织成行和列。接下来，按降序对矩阵进行排序，从最频繁到最少排列术语。前十名和后十名的结果如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="121e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经成功地从语料库中提取出二元模型。</p><h2 id="7906" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">使用ggplot2进行探索性可视化</h2><p id="b889" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">让我们用一个带<code class="fe ni nj nk nl b">ggplot2</code>的条形图来简单地想象一下。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="2b0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个柱状图，显示了15个最常见的二元模型。我将x轴标为“二元模型”，y轴标为“频率”，并沿着x轴调整了术语的位置和角度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/1c782c9f32cad592f2d62d97140a528b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h1XS2ZrZF1cCK8-uocJZhg.png"/></div></div></figure><p id="1576" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图表显示了前两个二元模型的压倒性出现。这意味着我们的词云将既不平衡也不漂亮，但在世界上最广泛的个人数据立法中有太多的“个人数据”调用可能并不令人惊讶。</p><h2 id="1ed2" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">词云</h2><p id="d14a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">接下来让我们打印出word cloud。首先，我们按原样打印所有内容，以检查数据的形状，然后，将最小频率设置为2，以丢弃只出现一次的统计上无关紧要的二元模型。最后，我们可以应用一些格式，用深蓝色强调出现超过20次的主要二元模型，而用浅蓝色保留其余部分。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/c86bdb6c3978607f4a5e8f3daab86761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nm4DG4wkYVLlg6fBJw8tnA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">#全部打印</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/915f221497ece394d5a2a7a732c872d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YzD1aLSInWFuj81Ra6klOg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">#最小尺寸2</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/8278285935476223a854d9829c4a1b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*76iWPX0H0qUSwIgQ-YfCOQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">#颜色编码</p></figure><p id="6497" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们暂时将前两个数据点视为异常值，我们也可以得出如下表示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/ebb1db1cfa19187aba49d62c484c2f03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sd9P5On37gbGBg6mYukeHw.png"/></div></div></figure><p id="74d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">众所周知，bigram词云没有克服词云或频率分析的固有缺点。然而，它们可以为文本提供双重有趣的洞察力。</p></div></div>    
</body>
</html>