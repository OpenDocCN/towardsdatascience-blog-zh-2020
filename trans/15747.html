<html>
<head>
<title>Facebook Sets a New Milestone in Language Translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">脸书树立了语言翻译的新里程碑</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/facebook-sets-a-new-milestone-in-language-translation-4c9ae0a5a347?source=collection_archive---------40-----------------------#2020-10-29">https://towardsdatascience.com/facebook-sets-a-new-milestone-in-language-translation-4c9ae0a5a347?source=collection_archive---------40-----------------------#2020-10-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1302" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解M2M-100，一种多语言机器翻译模型，可在100种语言对之间进行翻译。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c7cad971053594621c8c143fb816bc15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*viR5vFoqD_QkHk_D"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">格伦·卡丽在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="6da2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> L </span>语言翻译是一项具有挑战性的自然语言处理任务，需要大量的数据来训练模型。然而，在过去几年中已经取得了很大进展。最近，脸书发布了一个新的多语言机器翻译(MMT)模型，为这项挑战树立了一个新的里程碑[1]。</p><p id="d690" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是第一个在任意两种语言之间翻译时<em class="me">不依赖英语数据</em>的模型。使用<a class="ae ky" href="https://en.wikipedia.org/wiki/BLEU" rel="noopener ugc nofollow" target="_blank"> <em class="me"> BLEU </em> </a> <em class="me"> </em>指标，它也以10分的优势击败了以前以英语为中心的模型。凭借其创造性的数据挖掘方法和数量惊人的计算资源，他们建立了一个真正令人印象深刻的模型。</p><p id="5215" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将研究模型的架构，并强调它们用来创建数据集的最重要的方法。</p><h1 id="0f80" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">创建数据集</h1><p id="e521" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">大模型需要大量的数据。为了训练这个模型，来自脸书的研究人员使用了75亿个句子！可想而知，要为100种语言找到一个高质量的句对句翻译对是很难的。</p><h2 id="0c73" class="nc mg it bd mh nd ne dn ml nf ng dp mp li nh ni mr lm nj nk mt lq nl nm mv nn bi translated">所有语言翻译都重要吗？</h2><p id="befb" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">有些译本非常罕见。例如，土耳其语到缅甸语，或者阿尔巴尼亚语到斯瓦希里语。这些语言之间的高质量翻译数据也很难找到。脸书大学的研究人员通过将语言分成具有相似特征的语系来简化翻译任务。例如，斯拉夫语和德语是两个独立的语系。在每一个语系中，他们确定了一个语系中最常见的过渡语言。这些桥接语言被用作在两种不同语系的语言之间进行翻译的中间步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/54814c7c798c23e0161bb8ec3c770a06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*FFiEdwnIEXvA605tsLmAzw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">以英语为中心的数据与桥梁策略。修改自[1]</p></figure><p id="cc97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，不是有一个直接的阿尔巴尼亚语到斯瓦希里语的翻译，而是有阿尔巴尼亚语-希腊语(桥)-斯瓦希里语(桥)。这有助于减少数据集的大小，并证明有助于训练每个模型的特定模型参数[1]。</p><p id="6f2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这种方法，他们简化了翻译任务，同时没有牺牲太多的性能损失。总共，它使用100种语言产生2200个方向的翻译。这比平移方向的最大可能数量9900 (100 x 99)小得多。</p><h2 id="03a0" class="nc mg it bd mh nd ne dn ml nf ng dp mp li nh ni mr lm nj nk mt lq nl nm mv nn bi translated">数据挖掘技术</h2><p id="ee85" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">很难找到直接的句子到句子的翻译。然而，更容易找到以不同语言出版的相同文件。但是，为了创建相应的句子对，我们需要在文档中“关联”不同语言的句子。</p><p id="b924" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了找到相应的句子对，脸书的研究人员使用了CCMatrix [2]和CCAligned [3]。</p><p id="1022" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CCMatrix通过比较两种语言中所有可能的、独特的句子来工作[1]。这种方法计算量很大，CAligned可能是一个更有效的选择，因为它预先选择了要一起比较的文档[1]。</p><h1 id="1c54" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">M2M 100建筑</h1><p id="2ffc" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">该模型的架构基于众所周知的<a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">变压器架构</a>。由于任务的复杂性，他们增加了标准架构的规模。M2M-100的参数如下:</p><ul class=""><li id="67f2" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated">24个编码器和24个解码器层(因不同实验而异)</li><li id="7b76" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">嵌入尺寸大小为1024</li><li id="21df" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">FFN(前馈网络)尺寸为8192</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/fc7a63b27c18d4686cd967f1b8b6341a.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*r_VKjDKlR_BPSSYBhxi0og.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">变压器架构的架构。修改自[4]</p></figure><p id="f042" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">变压器采用序列对序列架构。给定输入序列，模型将产生另一个输出序列。这个模型的问题在于它是为双语数据设计的[1]。为了适应多对多语言的情况，在编码器和解码器层添加了特殊的标记来表示源语言和目标语言[1]。</p><p id="27bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型的另一个改进是添加了特定于语言的参数。这提高了模型的性能，但也增加了总参数的数量。</p><h1 id="e012" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">结果</h1><p id="ce1b" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">最终的M2M-100模型包含150亿个参数。他们还有一个更小的版本(只有？)15亿个参数。</p><p id="59b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测试数据集上的结果显示了M2M模型的优势。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f257dee7483cd226f66960f2251447ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*BYjVcTWeLjnJyVQYrXKLNg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多对多和以英语为中心的模型的BLEU分数。</p></figure><p id="13e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以英语为中心的模型和多对多模型在从任意语言到英语的翻译中表现相似，反之亦然(前两列)。然而，对于非英语翻译，例如瑞典语到斯瓦希里语，或者德语到波兰语，M2M模型实现了显著更高的BLEU分数。这就是这种模式的精髓和新颖之处。</p><p id="648c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，该模型在由人类评估语义准确性(得分从1到10)时表现更好。对于每一对语言，每个评估者给每个模型50个句子打分。我们可以从下面的图中看到，M2M-100对于非英语翻译来说要好得多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/d8c91840142fb35fb393b9b4402981e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SM2V5dwvLOPD-r0r91-8nw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">翻译质量的人工评估(语义准确性)。M2M-100 vs以英语为中心的模式。修改自[1]。</p></figure><h1 id="e8bc" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">是开源的吗？</h1><p id="2a20" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">你可以在这里找到他们的开源实现<a class="ae ky" href="https://github.com/pytorch/fairseq/tree/master/examples/m2m_100?fbclid=IwAR2O-IplQzV20cv2cDLxYWVgz7WraQ2CFISXJbOPUaEyXX2Y4OaKm5R6Vuo" rel="noopener ugc nofollow" target="_blank">。</a> <em class="me">但是，要注意:</em>数据集和模型是<em class="me">巨大的</em>。它是在多汁的脸书服务器上训练的，而不是在普通人的个人电脑上。尽管他们提供了模型检查点，但是您需要至少64GB的内存来将其存储在内存中。</p><h1 id="23fa" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">结论</h1><p id="6410" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">M2M-100是语言翻译任务的又一个里程碑。当英语作为中间翻译步骤被去除时，翻译的质量显著提高。不幸的是，这种型号很难使用，即使是在好的电脑上，因为它需要64 GB的内存。</p></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><h1 id="192f" class="mf mg it bd mh mi on mk ml mm oo mo mp jz op ka mr kc oq kd mt kf or kg mv mw bi translated">关于我</h1><p id="738a" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">我是阿姆斯特丹大学的人工智能硕士学生。在我的业余时间，你可以发现我摆弄数据或者调试我的深度学习模型(我发誓这很有效！).我也喜欢徒步旅行:)</p><p id="6e19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解我的最新文章和其他有用的内容，以下是我的其他社交媒体资料:</p><ul class=""><li id="57a8" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated"><a class="ae ky" href="https://www.linkedin.com/in/kacperkubara/" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="80a3" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated"><a class="ae ky" href="https://github.com/KacperKubara" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li></ul><h1 id="6140" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">参考</h1><p id="341a" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">[1] <a class="ae ky" href="https://scontent-ams4-1.xx.fbcdn.net/v/t39.8562-6/122141102_1284164108584699_8596121596808227915_n.pdf?_nc_cat=108&amp;ccb=2&amp;_nc_sid=ae5e01&amp;_nc_ohc=TtvMzFbJ4NEAX90frd_&amp;_nc_ht=scontent-ams4-1.xx&amp;oh=3b1a88ced973a76d2d85a32dd008f77f&amp;oe=5FBF6A34" rel="noopener ugc nofollow" target="_blank">超越以英语为中心的多语言机器翻译</a></p><p id="4981" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] <a class="ae ky" href="https://arxiv.org/pdf/1911.04944.pdf" rel="noopener ugc nofollow" target="_blank"> CCMatrix:挖掘网络上数十亿高质量的平行句</a></p><p id="07c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]<a class="ae ky" href="https://arxiv.org/pdf/1911.06154.pdf" rel="noopener ugc nofollow" target="_blank">c aligned:跨语言网络文档对的大规模集合</a></p><p id="8966" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你所需要的只是注意力</p><p id="aa99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] <a class="ae ky" href="https://ai.facebook.com/blog/introducing-many-to-many-multilingual-machine-translation/" rel="noopener ugc nofollow" target="_blank">第一个不依赖英文数据翻译100种语言的AI模型</a>(来自论文的博文)</p></div></div>    
</body>
</html>