<html>
<head>
<title>All the ~Eigen-stuff they never thought you should know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">所有他们认为你不应该知道的事情</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-the-eigen-stuff-they-never-thought-you-should-know-3d87ddfa5346?source=collection_archive---------19-----------------------#2020-10-18">https://towardsdatascience.com/all-the-eigen-stuff-they-never-thought-you-should-know-3d87ddfa5346?source=collection_archive---------19-----------------------#2020-10-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="60df" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="2f21" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">无限和…线性代数？！</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d4cb02ed5463381923a8c4ec74be1ed7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xWizE-ob9EDXo5fVyHoToA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">信用:Pixabay</p></figure><p id="f877" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">特征值(向量、值、分解)是线性代数的基础概念之一。这种东西往往会让STEM学生在挫折中“一头撞墙”。对我来说，特征向量(和行列式)是这个数学分支中最具挑战性的想法。更重要的是，它是统计学、物理学、工程学、计算机科学等领域的基础。在这篇文章中，我将把Eigen-stuff描述得非常简单，希望它能在你处理基于它的更复杂的话题时提升你的自信。</p><p id="d253" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">万圣节就要到了，因此，我将使用一个牵强的例子来说明特征分解:比方说，你正在某条街上“不给糖就捣蛋”——所有奇数编号的房子都在左边，偶数编号的房子在右边——所以当你沿着糖果巷前进时，你从左到右反弹回来。街道左侧的邻居只分发KitKats，而右侧的邻居只分发M&amp;Ms。一个“回合”将包括敲街道左侧的一扇门，然后敲街道右侧的一扇门。</p><p id="dc91" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">不管出于什么原因，所有的邻居都遵守一些规则，即左边的邻居会根据你目前拥有的糖果给你M &amp; Ms。即:</p><pre class="ks kt ku kv gt md me mf mg aw mh bi"><span id="e732" class="mi mj it me b gy mk ml l mm mn">MM = 1*MM + 2*KK #left<br/>KK = 3*MM + 7*KK #right</span></pre><p id="b256" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这些规则由下面矩阵中的“线性变换”表示:</p><pre class="ks kt ku kv gt md me mf mg aw mh bi"><span id="a280" class="mi mj it me b gy mk ml l mm mn">[[1,3],<br/> [2,7]]</span></pre><p id="2279" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所以问题变成了——<em class="mo">当你走过整条街(或者任意长的一段距离)</em> <strong class="lj jd"> <em class="mo">你的糖果包会是什么样子？</em> </strong>和<strong class="lj jd"> <em class="mo">你的糖果包的首发成分有多重要？</em> </strong>假设你从包里的每种糖果开始:</p><pre class="ks kt ku kv gt md me mf mg aw mh bi"><span id="b130" class="mi mj it me b gy mk ml l mm mn">[1,1]<br/>#KK, MM </span></pre><p id="5314" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一轮(街道的左右两侧)后，你更新的袋子有3个KitKats和5个M &amp; Ms换句话说，你增加了2个KK和9毫米。<em class="mo">(记住要做出区分:线性变换矩阵缩放你的糖果数量，它不只是简单地增加它们-尽管我们可以计算加法量。)</em></p><pre class="ks kt ku kv gt md me mf mg aw mh bi"><span id="dcb6" class="mi mj it me b gy mk ml l mm mn">[1*1 + 1*2, 1*3 + 1*7] = [3,10] </span></pre><p id="10c8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">那么两轮过后你的包包是什么样子的呢？</p><pre class="ks kt ku kv gt md me mf mg aw mh bi"><span id="120d" class="mi mj it me b gy mk ml l mm mn">[3*1 + 10*2, 3*3 + 7*10] = [23, 79] </span></pre><p id="baa1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们看看我们的包在10个回合后是什么样子，使用python:</p><pre class="ks kt ku kv gt md me mf mg aw mh bi"><span id="19a3" class="mi mj it me b gy mk ml l mm mn">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="bb76" class="mi mj it me b gy mp ml l mm mn">j = np.array([[1,3],[2,7]])<br/>curr = np.array([1,1])<br/>results = [curr]</span><span id="ae95" class="mi mj it me b gy mp ml l mm mn">for i in range(10):<br/>    curr = np.matmul(j,curr)<br/>    results.append(curr)</span><span id="bf72" class="mi mj it me b gy mp ml l mm mn">x = [i[0] for i in results]<br/>y = [i[1] for i in results]<br/>plt.plot(x,y)<br/>plt.scatter(x,y)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/695b801f5655365289e9d0715f4277d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*eTzF9nQi22FCl98T_5ufkg.png"/></div></figure><p id="8230" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一个非常明显的趋势出现了:首先，迭代1-8彼此映射得非常接近，靠近原点，而迭代9和10伸展得更远，这表明缩放效应的幅度随着每次迭代而复合。第二个趋势是这些迭代遵循一条线！回想一下高中代数——直线由两个参数描述，斜率和y截距。如果直线通过原点(0，0)，则直线完全由坡度单独定义。如果你还记得“上升超过下降”，这条线描述了y与x或者M&amp;Ms与KitKats的比率。<strong class="lj jd">这条线显示，几乎就在你开始不给糖就捣蛋之后，你的书包也会被定义成同样的比例！这真是太棒了。</strong></p><p id="666e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们兴奋之前，让我们试试不同的起始包组合:</p><pre class="ks kt ku kv gt md me mf mg aw mh bi"><span id="ca1c" class="mi mj it me b gy mk ml l mm mn">j = np.array([[1,3],[2,7]])<br/>curr = np.array([3,2])<br/>results = [curr]</span><span id="3610" class="mi mj it me b gy mp ml l mm mn">for i in range(10):<br/>    curr = np.matmul(j,curr)<br/>    results.append(curr)</span><span id="f40e" class="mi mj it me b gy mp ml l mm mn">x = [i[0] for i in results]<br/>y = [i[1] for i in results]<br/>plt.plot(x,y,color='r')<br/>plt.scatter(x,y,color='r')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/5ef87111a4af77014336b9c3f3272fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*EwV_i17ulM8lVphy7y4YuA.png"/></div></figure><p id="6e99" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在这是令人兴奋的！我们已经看到了<em class="mo">两种不同的开始糟糕组合</em>，然而<strong class="lj jd">它们汇聚在相同的KitKat对M &amp; M比率上。</strong></p><p id="6d7c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，这并不是普遍适用的。请注意，如果您从任一糖果的0开始，由于线性变换的乘法性质，所述糖果将永远不会增加，因此您的旅行将糖果巷不会收敛于此比率。(但是，如果在一个或两个糖果中以负数开始，收敛仍然会发生！然而，这在我们的问题中毫无意义。)</p><p id="0be5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，让我们在特征向量上绘制这些数据点:</p><pre class="ks kt ku kv gt md me mf mg aw mh bi"><span id="2240" class="mi mj it me b gy mk ml l mm mn">eigVec1 = np.linalg.eig(j)[1][:,0]<br/>eigVec2 = np.linalg.eig(j)[1][:,1]</span><span id="e823" class="mi mj it me b gy mp ml l mm mn">e1_x = [eigVec1[0]*-i for i in range(0,1_000_000,100)]<br/>e1_y= [eigVec1[1]*-i for i in range(0,1_000_000,100)]<br/>e2_x = [eigVec2[0]*-i for i in range(0,1_000_000,100)]<br/>e2_y= [eigVec2[1]*-i for i in range(0,1_000_000,100)]</span><span id="c712" class="mi mj it me b gy mp ml l mm mn">plt.scatter(x[:7],y[:7],color='r') #points<br/>plt.plot(e2_x,e2_y) # dominant eigenvector </span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/509109e1bacff80e7080301b7709feae.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*b0zhc2-yXtVkvozfoaDUYg.png"/></div></figure><p id="ab55" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">想象一下，在第一次迭代之后，包的KK与MM的比率遵循特征向量中的<em class="mo">，完全正确！但这意味着什么呢？</em></p><p id="49f5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以像考虑微积分极限一样考虑线性代数的特征向量！它们描述了系统的最终行为。注意，就像不是所有的代数函数都有极限(有时极限不存在)——不是所有的矩阵都有实特征向量:比如旋转矩阵就有虚特征向量。为什么？因为，你可以旋转一个向量无限次，但它永远不会以任何比例收敛！鉴于这种线性变换收敛于KitKats与Peanut M的特定比率，本征分解在数学中有许多应用；一个这样的应用是解微分方程系统。为什么？因为我们说的正是比率！</p><p id="70d7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">注意:沿特征向量有无穷多个点，由于共享相同的斜率，它们只是相同比值的倍数(虽然没有明确限定为整数倍)。记住这条线和点的概念。</p><p id="6b73" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">到目前为止，我们已经忽略了非常重要的考虑:一个系统可以有多个特征向量，然而，我们的分析表明，我们只收敛于一个特征向量。解释在于特征值:一个特征向量拉伸了一个向量(或者旋转——同样是虚数),但是特征值定义了这些效应的相对大小。</p><pre class="ks kt ku kv gt md me mf mg aw mh bi"><span id="829e" class="mi mj it me b gy mk ml l mm mn">np.linalg.eig(j)</span><span id="6fb4" class="mi mj it me b gy mp ml l mm mn">    #eigenvalue 1, eigenvalue 2 <br/>array([0.12701665, 7.87298335])</span><span id="62f5" class="mi mj it me b gy mp ml l mm mn">     #eigenvector 1, #eigenvector 2<br/>array([[-0.96017331, -0.40004303],         <br/>       [ 0.2794051 , -0.91649636]]))</span></pre><p id="142a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意，第一个特征值约为0.13，而第二个特征值约为7.88。当你用7.88除以0.13时，得到的数大约是60。这意味着，每当线性变换矩阵被(递归地)应用于一个向量时，<strong class="lj jd">第二个特征向量的效果是另一个的60倍。</strong>经过几次迭代后，第一个特征向量的影响可以忽略不计，并被第二个特征向量的主要影响所抵消。这就引出了特征向量的第二个(软)定义:<strong class="lj jd">线性变换的主要效应。</strong></p><p id="ca80" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从我们的不给糖就捣蛋的例子中，你可以看到特征向量可以描述随机变量的最终行为。我们会很快接近KK和MM的特定比例<strong class="lj jd">，一旦达到这个比例</strong>，<em class="mo">敲更多的门只会巩固这个比例，真的无法逃避！如果这听起来像马尔可夫链，那么恭喜你，你很好地掌握了上下文线索。</em></p><p id="c824" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">考虑下面的网页网络；下面矩阵的列描述了你<em class="mo">来自</em>的网页，行代表你<em class="mo">来到</em>的网页。所以<code class="fe mt mu mv me b">chain[0,1]</code>代表从页面1过渡到页面0的概率。这个矩阵从何而来？它基于网页上的链接；例如，第0列表示，在第0页的所有链接中，25%指向自身，20%指向第1页，25%指向第2页，最后30%指向第3页。</p><pre class="ks kt ku kv gt md me mf mg aw mh bi"><span id="9c9c" class="mi mj it me b gy mk ml l mm mn">chain = np.array([[0.25, 0.20, 0.25, 0.30],<br/>                  [0.20, 0.30, 0.25, 0.30],<br/>                  [0.25, 0.20, 0.40, 0.10],<br/>                  [0.30, 0.30, 0.10, 0.30]])</span><span id="e4b7" class="mi mj it me b gy mp ml l mm mn">np.linalg.eig(chain)</span></pre><p id="882c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们想知道，哪些页面或多或少相关。做到这一点的一种方法是使用随机行走:我们让一个机器人从上述任意页面开始执行任务，随机选择，然后根据当前网页对应的列定义的转移概率随机移动。如果允许机器人无限期行走(或任意长的一段时间)，并且我们将它对每个页面的访问次数归一化，我们会发现<em class="mo">平稳分布</em>，这是机器人在每个页面上被发现的概率，如果它的无限行走在某个时间被随机停止。</p><p id="b738" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这种平稳分布非常重要。这是最终行为比率的另一个例子！这意味着，<strong class="lj jd">它可以通过特征分解很快找到。</strong>但它变得更好了——马尔可夫链的转移矩阵的一个方便的属性是主要特征值等于1。这意味着<em class="mo">它将输入向量拉向最终行为比率，而不缩放它</em>。换句话说，它将输入向量拉向空间中的特定点，而不仅仅是比率的任意倍数(就像前面的例子一样)。)</p><p id="c36b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意，该属性非常有用，计算机通常就是这样计算任何本征向量的(然而，在每一步都需要进行一些缩放/调整，以迫使系统表现得好像本征值等于1一样。)有关代码和解释，请参见<a class="ae mw" href="https://en.wikipedia.org/wiki/Power_iteration" rel="noopener ugc nofollow" target="_blank">幂迭代法</a>。</p><pre class="ks kt ku kv gt md me mf mg aw mh bi"><span id="d68c" class="mi mj it me b gy mk ml l mm mn"># eigenvalues<br/>array([ 1.       , -0.0961781,  0.0773547,  0.2688234]),</span><span id="bcbf" class="mi mj it me b gy mp ml l mm mn"># eigenvectors<br/>array([[ 0.49854055,  0.55341887, -0.62120306,  0.08851168],              <br/>       [ 0.52623725,  0.41236876,  0.75039074,  0.11472596],           <br/>       [ 0.46746621, -0.31362083,  0.0814811 , -0.79384381],         <br/>       [ 0.50597137, -0.6521668 , -0.21066878,  0.59060617]])</span></pre><p id="e899" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">将注意力集中在特征值数组的第一个元素和特征向量矩阵的第一列。这是平稳分布。我们看到页面1的访问频率比页面0、2和3稍高。<strong class="lj jd">注意:</strong>在实践中，使用了PageRank算法，它解决了各种棘手的问题——比如一个网页只链接到它自己(一种吸收状态)，或者成对的网页只链接到彼此。这些问题往往会从网络中“窃取”比他们应得的更多的可信度，从而导致错误的排名。但这是完全不同的问题。</p><p id="47e2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为什么马尔可夫链如此重要？它是许多统计技术的理论基础——贝叶斯统计中的MCMC抽样、随机过程的蒙特卡罗方法(密切相关)以及许多其他应用。总体思路是，如果你有一些随机过程，比如糖果街上的不给糖就捣蛋，你可以估计这个过程的最终行为，一些事件发生的可能性，或者一些结果的相对频率。<em class="mo">这一切都始于Eigens-stuff！</em></p><p id="e22e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">注意:</strong>毫无疑问，你知道我从未使用过，<code class="fe mt mu mv me b">A*x = lambda*x,</code>，它用简单的英语说:“给定的向量x是矩阵A的特征向量，当(且仅当)当被A变换时，返回一个缩放的向量——按特征值缩放。”你可以从我上面展示的一切中证实这是真的。事实上，既然我们已经从自顶向下的方法探索了一些核心思想，那么这条规则就更加合理了。对我来说，自下而上的学习在数学教育中太常见了。</p><p id="e7bb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你喜欢这篇文章，请关注我的类似内容！欢呼:)</p></div></div>    
</body>
</html>