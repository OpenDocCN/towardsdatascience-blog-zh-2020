<html>
<head>
<title>Introduction to decision tree classifiers from scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">scikit-learn决策树分类器简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-decision-tree-classifiers-from-scikit-learn-32cd5d23f4d?source=collection_archive---------5-----------------------#2020-11-16">https://towardsdatascience.com/introduction-to-decision-tree-classifiers-from-scikit-learn-32cd5d23f4d?source=collection_archive---------5-----------------------#2020-11-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4ed2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将决策树分类器应用于虹膜数据集</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5c7968978456cfe6cc376f8eadd85558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q_xTjDBUfuccMQVX"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@nateggrant?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">内特·格兰特</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="5857" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有很多文章解释了什么是决策树以及决策树的作用:</p><div class="lv lw gp gr lx ly"><a href="https://medium.com/swlh/decision-tree-classification-de64fc4d5aac" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">决策树分类</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">决策树是对例子进行分类的简单表示。这是一种受监督的机器学习，其中数据…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">medium.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/the-basics-decision-tree-classifiers-b0d20394eaeb"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">基础知识:决策树分类器</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">对决策树如何工作和构建的直觉</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mn l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a href="https://medium.com/@borcandumitrumarius/decision-tree-classifiers-explained-e47a5b68477a" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">决策树分类器解释</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">决策树分类器是一种简单的机器学习模型，用于分类问题。这是一个…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">medium.com</p></div></div><div class="mh l"><div class="mo l mj mk ml mh mm ks ly"/></div></div></a></div><p id="c176" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在这里，我将重点介绍如何在iris数据集上使用python中的scikit-learn库实现决策树，以及一些有助于分析算法性能的功能。</p><p id="f6e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">什么是量词？</strong></p><p id="73bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">分类器算法用于通过决策规则将输入数据映射到目标变量，并可用于预测和理解与特定类别或目标相关联的特征。这意味着这是一种受监督的机器学习算法，因为我们已经有了最终的标签，我们只是想知道如何预测它们。为了我们的目的，我们可以使用决策树分类器来基于以下特征预测我们所拥有的鸢尾花的类型:花瓣长度、花瓣宽度、萼片长度和萼片宽度。</p><p id="2123" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">什么是决策树？</strong></p><p id="2b19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树是一种树状结构，其中内部节点代表属性，分支代表决策规则，叶节点代表结果。这是通过根据属性选择度量将数据分成单独的分区来实现的，在这种情况下，属性选择度量是基尼指数(尽管如果我们愿意，我们可以将其更改为信息增益)。这实质上意味着我们每个分裂都旨在减少基尼不纯，基尼不纯根据错误分类的结果来测量节点的不纯程度。</p><p id="6eb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">实施决策树</strong></p><p id="80fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先希望将数据转换成正确的格式，以便创建决策树。这里，我们将使用来自<code class="fe mp mq mr ms b">sklearn</code> datasets数据库的iris数据集，它非常简单，是如何实现决策树分类器的展示。</p><p id="8109" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来自<code class="fe mp mq mr ms b">scikit-learn</code>的决策树分类器的好处是目标变量可以是分类的或数字的。为了清楚起见，给定iris数据集，我更喜欢保持花的分类性质，因为稍后解释起来更简单，尽管如果需要的话，标签可以稍后引入。因此，以下代码可用于在此导入数据集:</p><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="7b80" class="mx my it ms b gy mz na l nb nc">import pandas as pd<br/>import numpy as np<br/>from sklearn.datasets import load_iris</span><span id="6d4c" class="mx my it ms b gy nd na l nb nc">#load in the data<br/>data = load_iris()<br/>#convert to a dataframe<br/>df = pd.DataFrame(data.data, columns = data.feature_names)<br/>#create the species column<br/>df['Species'] = data.target</span><span id="be44" class="mx my it ms b gy nd na l nb nc">#replace this with the actual names<br/>target = np.unique(data.target)<br/>target_names = np.unique(data.target_names)<br/>targets = dict(zip(target, target_names))<br/>df['Species'] = df['Species'].replace(targets)</span></pre><p id="4638" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来我们要做的是提取我们的训练和测试数据集。这样做的目的是确保模型不是在所有可用的数据上进行训练，以便我们可以测试它在看不见的数据上的表现。如果我们使用所有数据作为训练数据，那么我们可能会过度拟合模型，这意味着它可能会在看不见的数据上表现不佳。</p><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="ad6a" class="mx my it ms b gy mz na l nb nc">#extract features and target variables<br/>x = df.drop(columns="Species")<br/>y = df["Species"]</span><span id="da22" class="mx my it ms b gy nd na l nb nc">#save the feature name and target variables<br/>feature_names = x.columns<br/>labels = y.unique()</span><span id="050a" class="mx my it ms b gy nd na l nb nc">#split the dataset<br/>from sklearn.model_selection import train_test_split<br/>X_train, test_x, y_train, test_lab = train_test_split(x,y,<br/>                                                 test_size = 0.4,<br/>                                                 random_state = 42)</span></pre><p id="fae7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了正确格式的数据，我们可以开始创建决策树，这样我们就可以尝试预测不同花的分类。为此，首先要做的是从<code class="fe mp mq mr ms b">sklearn</code>包中导入<code class="fe mp mq mr ms b">DecisionTreeClassifier</code>。更多信息可在找到<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="ac24" class="mx my it ms b gy mz na l nb nc">from sklearn.tree import DecisionTreeClassifier</span></pre><p id="6fd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来要做的是将它应用于训练数据。为此，将分类器分配给<code class="fe mp mq mr ms b">clf</code>，并设置<code class="fe mp mq mr ms b">max_depth = 3</code>和<code class="fe mp mq mr ms b">random_state = 42</code>。这里，<code class="fe mp mq mr ms b">max_depth</code>参数是树的最大深度，我们控制它以确保没有过度拟合，并且我们可以容易地跟踪最终结果是如何实现的。<code class="fe mp mq mr ms b">random_state</code>参数确保结果可以在进一步的分析中重复。</p><p id="ab99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将算法用于训练数据:</p><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="063b" class="mx my it ms b gy mz na l nb nc">clf = DecisionTreeClassifier(max_depth =3, random_state = 42)<br/>clf.fit(X_train, y_train)</span></pre><p id="a6f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们希望能够理解算法是如何表现的，使用决策树分类器的一个好处是输出直观易懂，易于可视化。</p><p id="4542" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可以通过两种方式实现:</p><ol class=""><li id="534f" class="ne nf it lb b lc ld lf lg li ng lm nh lq ni lu nj nk nl nm bi translated">作为树形图:</li></ol><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="5ef6" class="mx my it ms b gy mz na l nb nc">#import relevant packages<br/>from sklearn import tree<br/>import matplotlib.pyplot as plt</span><span id="a87f" class="mx my it ms b gy nd na l nb nc">#plt the figure, setting a black background<br/>plt.figure(figsize=(30,10), facecolor ='k')</span><span id="c0a5" class="mx my it ms b gy nd na l nb nc">#create the tree plot<br/>a = tree.plot_tree(clf,<br/>                   #use the feature names stored<br/>                   feature_names = feature_names,<br/>                   #use the class names stored<br/>                   class_names = labels,<br/>                   rounded = True,<br/>                   filled = True,<br/>                   fontsize=14)</span><span id="43bc" class="mx my it ms b gy nd na l nb nc">#show the plot<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/3d78732dadcf707c8b0963d9f33a0d8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S4Re9uy4G2qFRlhAnfE20w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自作者</p></figure><p id="f067" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2)作为基于文本的图表</p><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="361f" class="mx my it ms b gy mz na l nb nc">#import relevant functions<br/>from sklearn.tree import export_text</span><span id="08d2" class="mx my it ms b gy nd na l nb nc">#export the decision rules<br/>tree_rules = export_text(clf,<br/>                        feature_names = list(feature_names))</span><span id="5d94" class="mx my it ms b gy nd na l nb nc">#print the result<br/>print(tree_rules)</span><span id="ae23" class="mx my it ms b gy nd na l nb nc">#out:<br/>|--- PetalLengthCm &lt;= 2.45<br/>|   |--- class: Iris-setosa<br/>|--- PetalLengthCm &gt;  2.45<br/>|   |--- PetalWidthCm &lt;= 1.75<br/>|   |   |--- PetalLengthCm &lt;= 5.35<br/>|   |   |   |--- class: Iris-versicolor<br/>|   |   |--- PetalLengthCm &gt;  5.35<br/>|   |   |   |--- class: Iris-virginica<br/>|   |--- PetalWidthCm &gt;  1.75<br/>|   |   |--- PetalLengthCm &lt;= 4.85<br/>|   |   |   |--- class: Iris-virginica<br/>|   |   |--- PetalLengthCm &gt;  4.85<br/>|   |   |   |--- class: Iris-virginica</span></pre><p id="908b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它清楚地显示了算法的表现。在这里，第一次分裂是基于花瓣长度，小于2.45厘米的被识别为鸢尾-刚毛鸢尾，而大于2.45厘米的被分类为鸢尾-海滨鸢尾。然而，对于花瓣长度大于2.45的那些，会发生进一步的分裂，其中两个进一步的分裂以更精确的最终分类结束。</p><p id="35e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，我们不仅对它在训练数据上的表现感兴趣，还对它在看不见的测试数据上的表现感兴趣。这意味着我们必须使用它从测试值中预测类，这是使用<code class="fe mp mq mr ms b">predict()</code>方法完成的。</p><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="ef3e" class="mx my it ms b gy mz na l nb nc">test_pred_decision_tree = clf.predict(test_x)</span></pre><p id="7acd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们感兴趣的是它在真阳性(预测为真，实际为真)、假阳性(预测为真，但实际为假)、假阴性(预测为假，但实际为真)和真阴性(预测为假，实际为假)方面的表现。</p><p id="14a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种方法是在混淆矩阵中检查结果。混淆矩阵通过在一个轴上显示预测值，在另一个轴上显示实际值，让我们直观地看到预测标签和真实标签是如何匹配的。这有助于确定我们在哪里可能得到假阳性或假阴性，从而确定算法的执行情况。</p><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="a2a6" class="mx my it ms b gy mz na l nb nc">#import the relevant packages<br/>from sklearn import metrics<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="6aba" class="mx my it ms b gy nd na l nb nc">#get the confusion matrix<br/>confusion_matrix = metrics.confusion_matrix(test_lab,  <br/>                                            test_pred_decision_tree)</span><span id="389e" class="mx my it ms b gy nd na l nb nc">#turn this into a dataframe<br/>matrix_df = pd.DataFrame(confusion_matrix)</span><span id="ac1c" class="mx my it ms b gy nd na l nb nc">#plot the result<br/>ax = plt.axes()<br/>sns.set(font_scale=1.3)<br/>plt.figure(figsize=(10,7))<br/>sns.heatmap(matrix_df, annot=True, fmt="g", ax=ax, cmap="magma")</span><span id="1017" class="mx my it ms b gy nd na l nb nc">#set axis titles<br/>ax.set_title('Confusion Matrix - Decision Tree')<br/>ax.set_xlabel("Predicted label", fontsize =15)<br/>ax.set_xticklabels(['']+labels)<br/>ax.set_ylabel("True Label", fontsize=15)<br/>ax.set_yticklabels(list(labels), rotation = 0)</span><span id="fe55" class="mx my it ms b gy nd na l nb nc">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/3d27374b06acb815126ca1e4f3ef55b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*Z4uHLOCkVozyxreEGBA11Q.png"/></div></figure><p id="3427" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如这里可以看到的，从看不见的数据中，只有一个值未能从Iris-versicolor类中预测出来，这表明总体上该算法在预测看不见的数据方面做得很好。</p><h2 id="fdba" class="mx my it bd np nq nr dn ns nt nu dp nv li nw nx ny lm nz oa ob lq oc od oe of bi translated"><strong class="ak">测量性能</strong></h2><p id="f64a" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">然而，为了测量性能，可以产生几个度量标准。</p><p id="187e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">准确度</strong></p><p id="9a5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">准确度分数是真阳性和真阴性占分配的标签总数的分数，计算如下:</p><blockquote class="ol om on"><p id="d5b1" class="kz la oo lb b lc ld ju le lf lg jx lh op lj lk ll oq ln lo lp or lr ls lt lu im bi translated">sum(混淆矩阵中的对角线)/ sum(混淆矩阵中的所有方框)</p></blockquote><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="86c0" class="mx my it ms b gy mz na l nb nc">metrics.accuracy_score(test_lab, test_pred_decision_tree)</span><span id="2737" class="mx my it ms b gy nd na l nb nc">#out: <br/>0.9833333333333333</span></pre><p id="dd11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">精密</strong></p><p id="eb74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这告诉我们有多少我们预测在某个类中的值实际上在那个类中。本质上，这告诉我们如何在假阳性方面表现。其计算方法如下:</p><blockquote class="ol om on"><p id="1651" class="kz la oo lb b lc ld ju le lf lg jx lh op lj lk ll oq ln lo lp or lr ls lt lu im bi translated">真阳性(对角线上的数字)/所有阳性(列和)</p></blockquote><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="491c" class="mx my it ms b gy mz na l nb nc">#get the precision score<br/>precision = metrics.precision_score(test_lab,<br/>                                   test_pred_decision_tree, <br/>                                   average=None)</span><span id="72e3" class="mx my it ms b gy nd na l nb nc">#turn it into a dataframe<br/>precision_results = pd.DataFrame(precision, index=labels)</span><span id="4207" class="mx my it ms b gy nd na l nb nc">#rename the results column<br/>precision_results.rename(columns={0:'precision'}, inplace =True)</span><span id="5c45" class="mx my it ms b gy nd na l nb nc">precision_results</span><span id="14a4" class="mx my it ms b gy nd na l nb nc">#out: </span><span id="6bf0" class="mx my it ms b gy nd na l nb nc">                  <strong class="ms iu">Precision</strong><br/>    Iris-setosa        1.00<br/>Iris-versicolor        0.95<br/> Iris-Virginica        1.00</span></pre><p id="1229" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">召回</strong></p><p id="1d6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这告诉我们每个类中有多少值被赋予了正确的标签，从而告诉use它相对于假阴性的表现如何。其计算方法如下:</p><blockquote class="ol om on"><p id="16b7" class="kz la oo lb b lc ld ju le lf lg jx lh op lj lk ll oq ln lo lp or lr ls lt lu im bi translated">真正数(对角线上的数字)/所有赋值(行和)</p></blockquote><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="6d45" class="mx my it ms b gy mz na l nb nc">recall = metrics.recall_score(test_lab, test_pred_decision_tree, <br/>                              average =None)<br/>recall_results = pd.DataFrame(recall, index= labels)</span><span id="cb6a" class="mx my it ms b gy nd na l nb nc">recall_results.rename(columns ={0:'Recall'}, inplace =True)<br/>recall_results</span><span id="59e4" class="mx my it ms b gy nd na l nb nc">#out:</span><span id="4b7b" class="mx my it ms b gy nd na l nb nc"><strong class="ms iu">                     Recall</strong><br/>    Iris-setosa        1.00<br/>Iris-versicolor        1.00<br/> Iris-Virginica        0.94</span></pre><p id="1a9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> f1 </strong></p><p id="4160" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个精度和召回规模的加权平均值，1为最好，0为最差。这使用了调和平均值，因此该值更接近较小的数字，并防止在一个参数高而另一个参数低的情况下高估模型的性能。其计算方法如下:</p><blockquote class="ol om on"><p id="15a4" class="kz la oo lb b lc ld ju le lf lg jx lh op lj lk ll oq ln lo lp or lr ls lt lu im bi translated">2 *(精度*召回)/(精度+召回)</p></blockquote><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="bb2d" class="mx my it ms b gy mz na l nb nc">f1 = metrics.f1_score(test_lab, test_pred_decision_tree, average=None)<br/>f1_results = pd.DataFrame(f1, index=labels)</span><span id="163d" class="mx my it ms b gy nd na l nb nc"><br/>f1_results.rename(columns={0:'f1'}, inplace=True)<br/>f1_results</span><span id="e587" class="mx my it ms b gy nd na l nb nc">#out:</span><span id="5002" class="mx my it ms b gy nd na l nb nc"><strong class="ms iu">                         f1</strong><br/>    Iris-setosa        1.00<br/>Iris-versicolor        0.97<br/> Iris-Virginica        0.97</span></pre><p id="4b60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，我们可以通过下面这段代码在一个输出中获得所有这些指标:</p><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="9192" class="mx my it ms b gy mz na l nb nc">print(metrics.classification_report(test_lab,<br/>                                    test_pred_decision_tree))</span><span id="5dcd" class="mx my it ms b gy nd na l nb nc">#out:</span><span id="0153" class="mx my it ms b gy nd na l nb nc">                 precision    recall  f1-score   support<br/><br/>    Iris-setosa       1.00      1.00      1.00        23<br/>Iris-versicolor       0.95      1.00      0.97        19<br/> Iris-virginica       1.00      0.94      0.97        18<br/><br/>       accuracy                           0.98        60<br/>      macro avg       0.98      0.98      0.98        60<br/>   weighted avg       0.98      0.98      0.98        60</span></pre><p id="f159" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中对每个类的支持仅仅是测试标签中每个类中出现的次数。</p><p id="c72f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">特征重要性</strong></p><p id="ded0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个有用的特性是计算最终树输出中每个特性的重要性。这是基尼指数或熵指数(在我们的例子中是基尼)由于给定特征的分割而降低的总量。这可以从以下方面获得:</p><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="c630" class="mx my it ms b gy mz na l nb nc">#extract importance<br/>importance = pd.DataFrame({'feature': X_train.columns, <br/>'importance' : np.round(clf.feature_importances_, 3)})</span><span id="1687" class="mx my it ms b gy nd na l nb nc">importance.sort_values('importance', ascending=False, inplace = True)</span><span id="1c8a" class="mx my it ms b gy nd na l nb nc">print(importance)</span><span id="cc3d" class="mx my it ms b gy nd na l nb nc">#out:</span><span id="9c45" class="mx my it ms b gy nd na l nb nc">         feature  importance<br/>2  PetalLengthCm       0.589<br/>3   PetalWidthCm       0.411<br/>0  SepalLengthCm       0.000<br/>1   SepalWidthCm       0.000</span></pre><p id="9adf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这表明花瓣长度是最重要的，因为第一次划分就是以此为基础的。但是，由于只运行了一个决策树，这并不意味着其他功能不重要，只是在此决策树中不需要它们。对于一个真实的视角，决策树必须运行多次(就像在一个随机的森林中一样)并且结果是聚合的。然后，可以将它与随机变量进行比较，或者基于某个标准删除特征。</p><p id="a4f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">改进模型</strong></p><p id="d470" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过改变所使用的特征来尝试改进模型，但是我们也可以通过使用GridSearchCV来查看它如何响应超参数的变化。这通过对训练集的集合的多次运行执行算法来对模型执行交叉验证，并告诉我们模型如何响应。</p><p id="c733" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于我们的目的，我们可以改变<code class="fe mp mq mr ms b">max_depth</code>和<code class="fe mp mq mr ms b">min_samples_split</code>参数，它们控制树的深度，以及分割内部节点所需的样本数。</p><pre class="kj kk kl km gt mt ms mu mv aw mw bi"><span id="1ba0" class="mx my it ms b gy mz na l nb nc">from sklearn.model_selection import GridSearchCV</span><span id="46e1" class="mx my it ms b gy nd na l nb nc">tuned_parameters = [{'max_depth': [1,2,3,4,5], <br/>                     'min_samples_split': [2,4,6,8,10]}]</span><span id="37bd" class="mx my it ms b gy nd na l nb nc">scores = ['recall']</span><span id="3112" class="mx my it ms b gy nd na l nb nc">for score in scores:<br/>    <br/>    print()<br/>    print(f"Tuning hyperparameters for {score}")<br/>    print()<br/>    <br/>    clf = GridSearchCV(<br/>        DecisionTreeClassifier(), tuned_parameters,<br/>        scoring = f'{score}_macro'<br/>    )<br/>    clf.fit(X_train, y_train)<br/>    <br/>    print("Best parameters set found on development set:")<br/>    print()<br/>    print(clf.best_params_)<br/>    print()<br/>    print("Grid scores on development set:")<br/>    means = clf.cv_results_["mean_test_score"]<br/>    stds = clf.cv_results_["std_test_score"]<br/>    for mean, std, params in zip(means, stds,<br/>                                 clf.cv_results_['params']):<br/>        print(f"{mean:0.3f} (+/-{std*2:0.03f}) for {params}")</span><span id="9ce5" class="mx my it ms b gy nd na l nb nc">#out:</span><span id="5e1a" class="mx my it ms b gy nd na l nb nc">Tuning hyperparameters for recall</span><span id="65ce" class="mx my it ms b gy nd na l nb nc">Best parameters set found on development set:<br/><br/>{'max_depth': 2, 'min_samples_split': 2}<br/><br/>Grid scores on development set:<br/>nan (+/-nan) for {'max_depth': 2, 'min_samples_split': 1}<br/>0.916 (+/-0.194) for {'max_depth': 2, 'min_samples_split': 2}<br/>0.916 (+/-0.194) for {'max_depth': 2, 'min_samples_split': 3}<br/>0.887 (+/-0.177) for {'max_depth': 2, 'min_samples_split': 4}<br/>0.887 (+/-0.177) for {'max_depth': 2, 'min_samples_split': 5}<br/>nan (+/-nan) for {'max_depth': 3, 'min_samples_split': 1}<br/>0.916 (+/-0.183) for {'max_depth': 3, 'min_samples_split': 2}<br/>0.916 (+/-0.183) for {'max_depth': 3, 'min_samples_split': 3}<br/>0.906 (+/-0.179) for {'max_depth': 3, 'min_samples_split': 4}<br/>0.916 (+/-0.183) for {'max_depth': 3, 'min_samples_split': 5}<br/>nan (+/-nan) for {'max_depth': 4, 'min_samples_split': 1}<br/>0.916 (+/-0.194) for {'max_depth': 4, 'min_samples_split': 2}<br/>0.905 (+/-0.179) for {'max_depth': 4, 'min_samples_split': 3}<br/>0.905 (+/-0.179) for {'max_depth': 4, 'min_samples_split': 4}<br/>0.905 (+/-0.179) for {'max_depth': 4, 'min_samples_split': 5}</span></pre><p id="8dc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这告诉我们最好的超参数是<code class="fe mp mq mr ms b">max_depth =2</code>和<code class="fe mp mq mr ms b">min_smaples_split =2.</code>其他要改变的超参数可以在<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">这里找到</a>。</p><p id="f1ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从而向您展示如何在实践中实现一个简单的决策树！</p></div><div class="ab cl os ot hx ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="im in io ip iq"><p id="5560" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用决策树的优点是易于理解和解释，可以处理数字和分类数据，限制不良预测的影响，并且我们可以提取它们的结构来可视化。当然，也有不利的一面，如果一个类占主导地位，它们可能会创建有偏见的树，我们可能会得到过于复杂的树，导致过度拟合，数据中的小变化可能会产生非常不同的结果。但是，它们在实践中非常有用，可以与其他分类算法(如k-最近邻或随机森林)一起使用，以帮助做出决策和理解分类是如何产生的。</p></div><div class="ab cl os ot hx ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="im in io ip iq"><p id="7212" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您喜欢您所阅读的内容，并且还不是medium会员，请考虑通过使用下面的我的推荐代码注册来支持我和平台上的其他作者:</p><div class="lv lw gp gr lx ly"><a href="https://philip-wilkinson.medium.com/membership" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">通过我的推荐链接加入媒体-菲利普·威尔金森</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">philip-wilkinson.medium.com</p></div></div><div class="mh l"><div class="oz l mj mk ml mh mm ks ly"/></div></div></a></div><p id="7031" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者，如果你对我的其他故事感兴趣，请随意阅读下面的一些故事:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/ucl-data-science-society-python-fundamentals-3fb30ec020fa"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">UCL数据科学协会:Python基础</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">研讨会1: Jupyter笔记本，变量，数据类型和操作</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="pa l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/an-introduction-to-object-oriented-programming-for-data-scientists-879106d90d89"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">面向数据科学家的面向对象编程介绍</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">面向对象的基础知识，适合那些以前可能没有接触过这个概念或者想知道更多的人</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="pb l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/introduction-to-random-forest-classifiers-9a3b8d8d3fa7"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">随机森林分类器简介</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">预测NBA球员的位置——我们正在看到一个真正的“无位置”联盟吗？</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="pc l mj mk ml mh mm ks ly"/></div></div></a></div></div></div>    
</body>
</html>