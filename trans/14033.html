<html>
<head>
<title>Best practices for Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的最佳实践</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/best-practices-for-reinforcement-learning-1cf8c2d77b66?source=collection_archive---------23-----------------------#2020-09-27">https://towardsdatascience.com/best-practices-for-reinforcement-learning-1cf8c2d77b66?source=collection_archive---------23-----------------------#2020-09-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="38f2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解除时间和基数的诅咒。</h2></div><p id="17d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习是研究密集型的。与经典编程相比，它包含了更高程度的不确定性。这对产品管理和产品开发有重大影响。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/12ce6c8f96cd59e9298b8564a4b55a01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*65deHs7QPIoRznPDZbXzOw.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">Nicolas Maquaire授权的Shutterstock提供的图片。</p></figure><p id="6fc4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">开发一个性能良好的智能产品是非常困难的。此外，生产环境的成本也很高。这种挑战的组合会使许多创业公司的商业模式充满风险。</p><p id="2540" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我的<a class="ae lv" rel="noopener" target="_blank" href="/inference-on-the-edge-21234ea7633">上一篇文章</a>中，我描述了新人在雾计算中使用人工智能时面临的挑战。更具体地说，我详细说明了在边缘做出推断需要什么。</p><p id="ba36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我将描述一些我认为是开始强化学习(RL)项目的最佳实践。我将通过举例说明我在视频游戏上复制<a class="ae lv" rel="noopener" target="_blank" href="/are-the-space-invaders-deterministic-or-stochastic-595a30becae2?source=friends_link&amp;sk=bf9fb38db5ce1055ceae1e0f64f92613"> Deepmind的表现时学到的一些经验来做到这一点。这是我从事的一个有趣的兼职项目。</a></p><p id="310b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谷歌在42款Atari游戏上用同样的网络实现了超人类的表现(见<a class="ae lv" href="https://www.nature.com/articles/nature14236" rel="noopener ugc nofollow" target="_blank">通过深度强化学习实现人类级别的控制</a>)。那么，让我们看看我们是否能达到同样的结果，并找出成功所需的最佳实践！</p><p id="b859" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在下面的<a class="ae lv" href="https://github.com/NicMaq/Reinforcement-Learning" rel="noopener ugc nofollow" target="_blank"> Github库</a>中找到源代码；此外，对于想了解我的算法如何工作的读者，我发表了<a class="ae lv" href="https://colab.research.google.com/drive/1nzH8TZ8zFth0oLwXwIXbn9OuAaJ4mVXG?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Breakout解释了</a>和<a class="ae lv" href="https://colab.research.google.com/drive/1--qFcl5QuTuudC-yYcE1odKx_htui4h6?usp=sharing" rel="noopener ugc nofollow" target="_blank"> e-greedy和softmax解释了</a>。这是两个Google Colab笔记本，我在其中解释了预期的sarsa和两个策略e-greedy和softmax的实现。</p><h1 id="acd8" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">时间和基数诅咒</h1><p id="627f" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">每个RL从业者处理的主要问题是不确定性和无限的技术选择，以及非常长的培训时间。</p><p id="9087" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我称之为RL的时间和基数诅咒。我认为每个人或每个团队开始强化学习项目的最佳实践是:</p><ol class=""><li id="2beb" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated">构建一个工作原型，即使它的性能很差或者是一个简单的问题</li><li id="1e47" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">尽可能减少训练时间和内存需求</li><li id="5767" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">通过测试不同的网络配置或技术选项来提高准确性</li><li id="2962" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">检查，再检查，然后再检查你的每一行代码</li></ol><p id="2861" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这些最佳实践，我想补充一点:</p><ol class=""><li id="7004" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated">监控可靠性。有时候，运气是不可重复的</li><li id="56d7" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">平行是你的朋友。平行测试不同的想法</li></ol><p id="ca5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">先来攻坚一个很简单的教科书案例:<a class="ae lv" href="https://gym.openai.com/envs/Acrobot-v1/" rel="noopener ugc nofollow" target="_blank">打开AI健身房Acrobot </a>。然后，我们将转向一些更具挑战性的游戏:突围和太空入侵者。</p><p id="40cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你有兴趣在继续之前积累知识，我推荐你阅读理查德·萨顿和安德鲁·巴尔托的《T2强化学习》。</p><p id="f740" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你正在积极参与一个项目，我推荐你读一读Andrew NG的“<a class="ae lv" href="https://www.deeplearning.ai/machine-learning-yearning/" rel="noopener ugc nofollow" target="_blank">机器学习向往</a>”</p><h1 id="7069" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">用开放的AI Acrobot学习</h1><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/dc54d39790591a249ce2f3b468b9e864.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*DWL-GnhfD9wWyoy0"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><p id="f0cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在RL中处理复杂的项目之前，我的建议是从简单的项目开始，因为你会在互联网上找到更多的文献。找到解决方案会更容易，更重要的是，因为测试新想法会更快(失败得快，失败得好)。</p><p id="4956" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">开放人工智能提供了丰富的选择。因为我专攻控制系统，所以我决定使用Acrobot。我在攻读工程学位的时候做过一个非常类似的项目:<a class="ae lv" href="https://www.youtube.com/watch?v=B6vr1x6KDaY" rel="noopener ugc nofollow" target="_blank">双摆</a>。</p><p id="3c48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这个兼职项目，我决定从Acrobot开始。</p><p id="51e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如图所示，acrobot系统具有两个关节和两个连杆，其中只有两个连杆之间的关节被驱动。最初，链接向下悬挂，目标是将较低链接的末端摆动到水平线以上。</p><h2 id="14cb" class="ni lx it bd ly nj nk dn mc nl nm dp mg kr nn no mi kv np nq mk kz nr ns mm nt bi translated">代理的剖析</h2><p id="f9bc" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">什么是强化学习？</p><p id="c888" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我们通过与环境互动来学习这一理念的直接实现。这样，它就模仿了大脑。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/68719044adc88f30102b2fee4bae64b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*nl5bC3hQuaRphHeYTeHgAw.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><p id="d898" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在RL中，在游戏的每一步，我们决定最好的行动，然后我们取回奖励并进入下一个状态。对于Acrobot，状态由两个旋转关节角度和关节角速度的sin()和cos()组成。</p><p id="7d62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[cos(θ1)sin(θ1)cos(θ2)sin(θ2)θ1θ2]</p><p id="1a53" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在《强化学习》一书中，萨顿和巴尔托描述了不同的时间差(TD)技术。TD学习是指一类<a class="ae lv" href="https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)" rel="noopener ugc nofollow" target="_blank">无模型</a>强化学习，其中使用深度网络来逼近价值函数。价值函数估计每个动作有多好。</p><p id="27aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我从最常见的RL算法Expected Sarsa及其特例Q-Learning开始。两种算法都使用一种策略。粗略地说，策略是代理行为函数。该策略使用价值函数估计来决定最佳行动。我们将使用软策略(ɛ-greedy和软最大)，这意味着每个行动都有机会被执行。当以1-ɛ的概率选择最佳行动，并以ɛ.的概率随机选择最佳行动时，策略是ɛ-greedy我最喜欢的是，softmax策略根据动作值估计值为每个动作分配一个优先级。</p><p id="4ed1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae lv" href="https://www.nature.com/articles/nature14236" rel="noopener ugc nofollow" target="_blank">通过深度强化学习的人类级控制</a>中，Deepmind使用Q-Learning和e-greedy策略。</p><p id="3c57" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将要使用的两种算法都带有一些超参数和许多选项。举几个例子，最重要的是学习(𝜶)和折现率(𝞬)，批量大小，ε(ɛ)或τ(𝝉)，用于在探索和开发之间找到正确的平衡，经验重放记忆的大小，探索步骤的数量，退火步骤的数量，模型更新频率，权重初始化，以及优化器。</p><p id="d084" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当然，超参数的列表还有很多。</p><p id="6479" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是我所说的基数的诅咒。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nv"><img src="../Images/568532d4b3fe6d12907eed10ac569eb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OyY1mygnzVuXtdiBuNAiGA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><em class="lu">基数的诅咒。图片由Nicolas Maquaire </em>授权的Shutterstock提供。</p></figure><p id="c31a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是为什么我的第一个最佳实践是构建一个“工作”原型。然后专注于性能。</p><p id="2e55" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么。我们如何解除基数的诅咒？</p><p id="33ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我建议在web上搜索类似的实现，以便理解其他从业者使用的超参数。除了帮助你获得成功，这将有助于发展你的直觉，这是非常重要的。事实上，“直觉”是吴恩达在他名为<a class="ae lv" href="https://www.coursera.org/learn/neural-networks-deep-learning/" rel="noopener ugc nofollow" target="_blank">深度学习专业化</a>的奇妙课程中使用最多的词语之一。</p><p id="3548" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我第一次尝试摇动Acrobot时，我的一些跑步收敛了。下面，我们可以看到十场以上的成功率。当然，肯定还有改进的空间，但我的算法正在学习，这是一个好的开始。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nw"><img src="../Images/365811cd0a83b184ad14411e12c37b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OC951wzjcSj-BFvC"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><em class="lu">横轴单位是训练小时数。纵轴单位是十场以上的成功率。作者配图。</em></p></figure><p id="cbba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在图表上，你也可以看到我训练网络超过一个星期。这些网络在第二天左右开始融合。这给我们上了很好的一课:作为人工智能从业者，我们等啊等啊等。这是时间的诅咒！</p><p id="e143" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在回想起来，如果我计算我在我的机器前花费的小时数，仔细检查我的许多尝试的损失和准确性，这当然相当于获得了RL心理学学位！我很自豪我成为了RL算法的心理学专家。</p><p id="e4e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我训练的一些人工智能表现得像冠军。少数人有自杀倾向，表现不那么纯粹随机(不好)。其他人则有经常性的倦怠(和丑陋)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nx"><img src="../Images/e3b98aca3e66928cee374cb45c8f8965.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*p_GMxUdyRynqHfs9"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><a class="ae lv" href="https://en.wikipedia.org/wiki/The_Good,_the_Bad_and_the_Ugly" rel="noopener ugc nofollow" target="_blank"> <em class="lu">好人(蓝色)、坏人(绿色)和丑人</em> </a> <em class="lu">(红色)。作者配图。</em></p></figure><p id="6403" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">发展你的直觉有助于你找到解决问题的方法。这里，坏人对最佳行动选择有问题，而坏人对其目标网络有问题。</p><p id="a483" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就像深度学习中的偏差和方差方法一样，你可以发展自己的直觉来更快地诊断你的问题。</p><p id="4cd3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们有了冠军，让我们看看如何提高它的性能。</p><h2 id="69b1" class="ni lx it bd ly nj nk dn mc nl nm dp mg kr nn no mi kv np nq mk kz nr ns mm nt bi translated">培训代理</h2><p id="6c2c" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated"><strong class="kk iu"> GPU vs CPU </strong></p><p id="a15f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">深度学习和强化学习的一个主要区别是没有预先存在的数据集。当代理与环境交互时，数据集(或体验重放，或RL中的存储器)被创建。这导致了性能瓶颈，因为流水线依赖于CPU操作。这也是为什么大部分教程的张量流运算都发生在CPU上的原因。我非常肯定，当将所有tensorflow操作放在CPU上时，您的第一次尝试将显示更好的性能。</p><p id="c6bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GPU上的训练绝对不是自动的。但是如果做得正确，它会提高速度，正如你从下图中看到的。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ny"><img src="../Images/56a5c9498298c4fa023439089eb71ce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oTWkf50-YSFfDORl"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><em class="lu">横轴表示训练时间。用GPU上的大部分操作红；仅在CPU上运行时为蓝色。作者配图。</em></p></figure><p id="44b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">性能瓶颈是由在CPU内存和GPU内存之间来回移动大量小对象造成的。因此，了解在哪里创建tensorflow变量以及如何利用tensorflow 2.x的急切执行非常重要。</p><p id="ec75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我的机器上，限制因素通常是CPU的内存，它存储了体验缓冲区的所有图像。我通常不会看到高GPU内存消耗，因为数据是在CPU的内存上创建和管理的。我使用tensorflow数据API来创建数据集。数据集加载图像并将其提供给GPU。我也没有看到GPU的高利用率，因为每个进程都在等待CPU执行下一步，并等待数据集产生一个小批量。</p><p id="1528" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，所使用的数据类型对性能也有重要影响。使用int16代替float32将提高速度，并有助于在您的机器上管理多个1.000.000经验回放。</p><p id="e8ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">小心你的数据类型有助于你并行训练多个网络，走得更快，这进一步解除了时间的诅咒。</p><p id="c0fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些步骤显然支持我们的第二个最佳实践:尽可能减少训练时间和内存需求。</p><p id="1fd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">超参数和网络架构</strong></p><p id="bbf2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然我们已经使算法在速度和内存消耗方面尽可能高效，我们可以专注于解除基数的诅咒。当然，我们仍有许多部分需要整合，但这更容易管理。我们现在可以并行进行多次运行，并更快获得结果。</p><p id="7569" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我建议将您的所有超参数设置为社区普遍接受的值。找到这些值的最好方法是找到挖掘类似用例的论文，看看它们使用的是什么参数。然后，我建议对2或3个最重要的超参数进行手动搜索。不要忘记使用10的幂来有效地扫过你的超参数的整个范围。有些非常敏感(尤其是softmax温度)。</p><p id="3dbb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于网络架构，我推荐复制你找到的任何有趣论文中的网络架构。然后，尝试不同数量的隐藏层或不同数量的节点。此外，请记住，层的初始化内核极其重要。</p><p id="8ea6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，我注意到，当最后一个密集层使用方差缩放而不是默认的Glorot初始化时，使用softmax策略的预期Sarsa不会收敛。</p><p id="82a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了说明不同数量的节点可以带来的改进，我们在之前的CPU/GPU比较中添加了一次新的运行。粉红色和红色图之间的唯一区别是我交换了最后两层(从256和128感知器到128和256)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nz"><img src="../Images/9b6d52a53f6ee550e0b934010869f0b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SS-5xpgiLtK0EF4e"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><em class="lu">以小时为横轴的训练。新图层排列的红色。作者配图。</em></p></figure><p id="e108" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您使用经典网络架构和4个状态的堆栈作为输入，数百次运行使我认为最重要的超参数是:</p><ol class=""><li id="b77d" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated">学习率</li><li id="8bf5" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">探测和退火参数(或softmax的温度)</li><li id="994d" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">图层的初始化</li><li id="4012" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">主要网络参数更新的频率</li><li id="94ed" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">目标网络更新的频率</li></ol><p id="d688" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过遵循我们的最佳实践，我们获得了非常好的改进。如下所示，我们显著提高了精确度和计算时间。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oa"><img src="../Images/1a0b6b1b253121951913a30199f73f69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xu1cFHbq25Irw8TF"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><em class="lu">通过GPU和一点网络调整获得的改进。作者配图。</em></p></figure><p id="1ed3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不用说，这会对新产品的上市和相关成本产生非常重要的影响。这显然支持了我们的第三个最佳实践:通过测试不同的网络配置或技术选项来提高准确性。</p><p id="ef8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为对进一步提高类似用例的准确性和收敛时间感兴趣的读者的补充说明:我的下一步将是使用tile编码。我很确定这将进一步提高速度和性能。</p><p id="8a27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于我们在Acrobot上的工作，我们现在有了一个很好的平台来尝试更具挑战性的东西:计算机视觉的强化学习。</p><h1 id="c66a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">雅达利的RL和计算机视觉</h1><h2 id="e8a7" class="ni lx it bd ly nj nk dn mc nl nm dp mg kr nn no mi kv np nq mk kz nr ns mm nt bi translated">突围和太空入侵者</h2><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/5fc1c886e620328067cd874763748c1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/1*rzD4850AlldAzR5A7FvjcA.gif"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><p id="72ae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Breakout是雅达利于1976年5月13日开发并发布的一款街机游戏。玩法:一层砖块排列在屏幕的上三分之一处，目标是将它们全部摧毁！</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/03db28d21ca10bc56c8e8a26a54b1211.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/1*8V7FhDSln62ay2aOCNTx6w.gif"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><p id="0bbc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">《太空入侵者》是1978年由Tomohiro Nishikado创作的街机游戏。《太空入侵者》是第一款固定射击游戏，它为射击类游戏设定了模板。目标是用水平移动的激光击败一波又一波下降的外星人，以获得尽可能多的分数。</p><p id="ef51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看看我们的算法在那些游戏上表现如何！先睹为快，这两张动画gif是在我们的评估会议期间拍摄的。</p><p id="cd8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">支持这些新游戏没什么可做的。您需要声明新环境，但最重要的是，您需要调整您的网络和数据管道。对于新网络，我们将使用经典的ConvNet架构，与Google Deepmind在《自然》杂志论文中使用的网络相同:三个卷积层，后跟几个密集层。然后，我们需要更新数据管道。这有点棘手，因为我们必须存储一百万张图像。</p><p id="1105" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你能在互联网上找到的大多数教程都存储了各州的历史。状态历史堆叠了由环境生成的五个连续图像。四个图像用于估计当前动作值函数，接下来的四个图像用于估计下一个动作值。考虑到存储历史对内存消耗的影响以及我们的第二个最佳实践，我们将只存储状态(图像)。数据管道将动态地重新堆叠状态。</p><p id="a4f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以找到我在这个<a class="ae lv" href="https://colab.research.google.com/drive/1nzH8TZ8zFth0oLwXwIXbn9OuAaJ4mVXG?usp=sharing" rel="noopener ugc nofollow" target="_blank">谷歌协作</a>上所做的详细解释。</p><p id="3c7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">顺便提一下，如果像我一样，你使用开放的人工智能健身房来测试不同的算法或技术选项，并希望从开放的人工智能健身房环境转移到现实世界的问题，请注意，确保你的训练在随机环境中进行是至关重要的。你可以在互联网上找到的许多教程都使用了决定论的环境。随机性是鲁棒性的关键要求。如果你在一个开放的人工智能体育馆确定性环境中训练和验证，你的算法不太可能在现实世界的问题上起作用。我在“<a class="ae lv" rel="noopener" target="_blank" href="/are-the-space-invaders-deterministic-or-stochastic-595a30becae2">中谈到了这个话题，太空入侵者是确定性的还是随机的？</a></p><p id="6193" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不幸的是，在这次更新中，我犯了很多错误，花了大量的时间来清除这些错误。为什么？因为即使有一些错误，网络也在学习和融合。精确度显然差得很远，但它正在工作。</p><p id="7eca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是与经典编程的一个重要区别。</p><p id="d232" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当网络正在学习并稳定在一个适中的分数时，很容易得出超参数需要调整的结论。我掉进这个陷阱好几次了。通常，您从数据管道中删除了一个bug，并启动了一系列运行，但几天后却发现又出现了另一个bug。这不仅限于数据管道。我承认我在代码的每一部分都发现了错误。</p><p id="8516" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这支持了我们的第四个最佳实践:检查，再检查，然后再检查你的每一行代码。</p><p id="a8b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据我的经验，最好的处理方法是创建一个单独的笔记本来证明每一行代码都在工作。很容易不经意地使用矩阵乘法而不是元素乘法，并且很容易在Tensorflow或Numpy转换中出错。我鼓励你对你的张量流代码非常谨慎。</p><h1 id="439b" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结论</h1><p id="32cb" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">虽然我们只是触及了构建智能产品所面临的技术挑战的表面，但我希望这篇文章能够让您很好地理解一些最佳实践，以便成功启动您的RL项目。</p><p id="848c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于任何研究密集型项目来说，时间和基数诅咒应该总是被考虑到你的产品管理和团队组织中。</p><p id="5675" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我坚信，机器学习可以真正提供我们解决许多日益增长的问题所需的推动力。</p><p id="79cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我很乐意帮助任何有远见的人！而且，我非常乐于接受反馈。</p><p id="18ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您阅读本文！</p></div></div>    
</body>
</html>