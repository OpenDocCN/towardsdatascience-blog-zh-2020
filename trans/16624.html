<html>
<head>
<title>Reinforcement Learning based Recommender Systems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于强化学习的推荐系统</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-based-recommender-systems-492a2674fcc?source=collection_archive---------25-----------------------#2020-11-16">https://towardsdatascience.com/reinforcement-learning-based-recommender-systems-492a2674fcc?source=collection_archive---------25-----------------------#2020-11-16</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="c22f" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="877f" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">结合强化学习和NLP/聊天机器人开发个性化应用</h2></div><p id="dc37" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated"><strong class="ku je">摘要。</strong> <em class="lo">我们提出了一种基于强化学习的方法来实现推荐系统。这些结果是基于一个现实生活中的健康应用程序，该应用程序能够以互动的方式向用户提供个性化的健康/活动相关内容。不幸的是，当前的推荐系统无法适应不断发展的特征，例如用户情绪，以及需要基于多个不可靠的反馈通道(例如传感器、可穿戴设备)来计算RL奖励的场景。为了克服这一点，我们提出了三个构造:(I)加权反馈通道，(ii)延迟奖励，和(iii)奖励提升，我们认为这对于RL用于推荐系统是必不可少的。</em></p><p id="8ad9" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">这篇论文发表在<strong class="ku je"> AAI4H </strong> —医疗保健人工智能进展研讨会的会议录上，与第24届欧洲人工智能会议(<strong class="ku je"> ECAI </strong> 2020)，2020年9月<em class="lo"> ( </em> <a class="ae lp" href="http://ceur-ws.org/Vol-2820/AAI4H-10.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lo">论文pdf</em></a><em class="lo">)(</em><a class="ae lp" href="https://www.slideshare.net/DebmalyaBiswas/delayed-rewards-in-the-context-of-reinforcement-learning-based-recommender-systems" rel="noopener ugc nofollow" target="_blank"><em class="lo">PPT</em></a><em class="lo">)</em></p><figure class="lr ls lt lu gu lv gi gj paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gi gj lq"><img src="../Images/145866b90adc518289140a0c029753ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4cJA21eCjdKp0gyv.png"/></div></div><p class="mc md gk gi gj me mf bd b be z dk translated">基于强化学习的推荐系统:基于来自<a class="ae lp" href="https://www.pexels.com/photo/woman-in-red-long-sleeve-writing-on-chalk-board-3769714/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae lp" href="https://www.pexels.com/@olly?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Andrea Piacquadio </a>的pic</p></figure><h1 id="03da" class="mg mh iu bd mi mj mk ml mm mn mo mp mq kj mr kk ms km mt kn mu kp mv kq mw mx bi translated">1导言</h1><p id="8f73" class="pw-post-body-paragraph ks kt iu ku b kv my ke kx ky mz kh la lb na ld le lf nb lh li lj nc ll lm ln in bi translated">健康/保健应用程序的采用率一直很低。通过向用户提供越来越相关和及时的建议，个性化推荐有可能提高采用率。虽然推荐引擎(以及基于它们的应用程序)已经成熟，但它们仍然受到“冷启动”问题的困扰，事实上它基本上是一种基于推送的机制，缺乏使这种应用程序吸引千禧一代所需的交互性。</p><blockquote class="nd"><p id="461f" class="ne nf iu bd ng nh ni nj nk nl nm ln dk translated"><em class="nn">我们展示了一个健康应用案例研究，其中我们应用了强化学习(RL)和自然语言处理(NLP) /聊天机器人的组合，以向用户提供高度个性化的交互式体验。我们专注于应用程序的交互方面，应用程序能够实时描述用户并与用户交谈，提供适应用户当前情绪和过去偏好的相关内容。</em></p></blockquote><p id="27e4" class="pw-post-body-paragraph ks kt iu ku b kv no ke kx ky np kh la lb nq ld le lf nr lh li lj ns ll lm ln in bi translated">这种聊天机器人的核心是一个意图识别自然语言理解(NLU)引擎，它通过问题变体的硬编码示例进行训练。当没有符合30%以上置信度的意图时，聊天机器人会返回一个后备答案。基于(显式的)用户响应和(隐式的)环境方面来计算用户情绪，例如位置(住宅、办公室、市场……)、温度、照明、一天中的时间、天气、附近的其他家庭成员等等；来进一步适应聊天机器人的反应。</p><p id="bf8c" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">RL指的是人工智能(AI)的一个分支，它能够通过实时最大化一个奖励函数来实现复杂的目标。奖励功能的工作原理类似于用糖果和打屁股来激励孩子，这样算法在做出错误决定时会受到惩罚，在做出正确决定时会受到奖励——这就是强化。强化方面还允许它更快地适应用户情绪的实时变化。有关RL框架的详细介绍，感兴趣的读者可以参考[1]。</p><p id="1696" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">以前的工作已经在推荐系统的背景下探索了RL[2，3，4，5]，并且随着云API(例如Azure Personalizer [6，7])和Google的RecSim [8]的最近可用性，企业采用也似乎正在获得动力。然而，它们仍然像典型的推荐系统一样工作。给定用户简档和分类推荐，系统基于流行度、兴趣、人口统计、频率和其他特征来做出推荐。这些系统的主要新颖之处在于，它们能够识别推荐的特征(或特征的组合),从而为特定用户获得更高的回报；然后可以为该用户定制以提供更好的推荐[9]。</p><blockquote class="nd"><p id="e179" class="ne nf iu bd ng nh ni nj nk nl nm ln dk translated"><em class="nn">不幸的是，这对于现实生活中的系统来说仍然是低效的，这些系统需要适应不断发展的功能，例如用户情绪，并且需要根据多个不可靠的反馈渠道(例如传感器、可穿戴设备)来计算奖励。</em></p></blockquote><p id="790f" class="pw-post-body-paragraph ks kt iu ku b kv no ke kx ky np kh la lb nq ld le lf nr lh li lj ns ll lm ln in bi translated">本文的其余部分组织如下:第2节概述了问题场景，并将其表述为RL问题。在第3节中，我们建议</p><blockquote class="nd"><p id="69d3" class="ne nf iu bd ng nh ni nj nk nl nm ln dk translated"><em class="nn">需要三种RL结构来克服上述限制:(I)加权反馈通道，(ii)延迟奖励，以及(iii)奖励提升，我们认为这是RL用于推荐系统的基本结构。</em></p></blockquote><p id="bbe2" class="pw-post-body-paragraph ks kt iu ku b kv no ke kx ky np kh la lb nq ld le lf nr lh li lj ns ll lm ln in bi translated">在这种情况下,“延迟回报”不同于延迟RL [10]的概念，在延迟RL中，遥远未来的回报被认为不如即时回报有价值。这与我们的“延迟奖励”概念有很大不同，在延迟奖励中，只有在后续行动验证了奖励的一致性后，才会应用奖励。第四部分总结了本文，并提出了未来研究的方向。</p><h1 id="6c45" class="mg mh iu bd mi mj mk ml mm mn mo mp mq kj mr kk ms km mt kn mu kp mv kq mw mx bi translated">2问题场景</h1><h1 id="660d" class="mg mh iu bd mi mj mk ml mm mn mo mp mq kj mr kk ms km mt kn mu kp mv kq mw mx bi translated">2.1健康应用</h1><p id="78e0" class="pw-post-body-paragraph ks kt iu ku b kv my ke kx ky mz kh la lb na ld le lf nb lh li lj nc ll lm ln in bi translated">该应用程序支持基于推送的通知，其中个性化的健康，健身，活动等。向用户推送相关推荐；以及应用程序响应用户查询的交互式聊天。我们假设存在一个知识库KB的文章、图片和视频，根据它们与不同用户简档/情感的相关性对工件进行排序。</p><p id="129e" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">图1中描述了健康应用架构，其示出了用户和环境条件(包括用户反馈)如何:</p><p id="9185" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">1.使用可用传感器收集以计算“当前”反馈，包括环境背景(例如，用户的网络摄像头pic可用于推断用户对聊天机器人响应/通知的情绪、房间照明条件和附近存在的其他用户)，</p><p id="5e5f" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">2.其然后与用户对话历史相结合，以量化用户情感曲线，并排除由于不相关因素引起的情感的任何突然变化；</p><p id="3352" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">3.导致与提供给用户的最后聊天机器人响应/应用通知相对应的合计奖励值。</p><p id="8bb6" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">然后，该奖励值作为反馈提供给RL代理，以从知识库中选择下一个最佳聊天机器人响应/应用程序通知。</p><figure class="lr ls lt lu gu lv gi gj paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gi gj nt"><img src="../Images/391c54e824dc5ffa027a5441d0523301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0EiEGnh81mm1YCpc.png"/></div></div><p class="mc md gk gi gj me mf bd b be z dk translated">图一。健康应用架构(图片由作者提供)</p></figure><h1 id="c0c9" class="mg mh iu bd mi mj mk ml mm mn mo mp mq kj mr kk ms km mt kn mu kp mv kq mw mx bi translated">2.2互动聊天— RL公式</h1><p id="44a3" class="pw-post-body-paragraph ks kt iu ku b kv my ke kx ky mz kh la lb na ld le lf nb lh li lj nc ll lm ln in bi translated">我们将上述场景的RL引擎公式化如下:</p><p id="9fdb" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated"><em class="lo">动作</em> ( <em class="lo"> a </em>):在这种情况下，动作<em class="lo"> a </em>对应于一篇知识库文章，该文章作为推送通知或者作为对用户查询的响应或者作为正在进行的对话的一部分被传递给用户。</p><p id="faf6" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated"><em class="lo"> Agent </em> ( <em class="lo"> A </em>):是执行动作的一方。在这种情况下，代理是向用户交付操作的应用程序，其中操作是基于其策略选择的(如下所述)。</p><p id="17c8" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated"><em class="lo">环境</em>:指智能体与之交互的世界，它响应智能体的动作。在我们的例子中，环境对应于与应用程序交互的用户。<em class="lo"> U </em>对<em class="lo"> A </em>的行为做出反应，提供不同类型的反馈，既有显性的(以聊天反应的形式)，也有隐性的(如面部表情的变化)。</p><p id="cba2" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated"><em class="lo">策略</em> (𝜋):是代理用来选择下一个基于动作的策略。给定一个用户简档<em class="lo"> Up </em>、【当前】情绪<em class="lo"> Us </em>，并查询<em class="lo">Uq</em>；策略功能计算分别由NLP和推荐引擎返回的文章分数的乘积，选择具有最高分数的文章作为下一个最佳动作:(a)NLP引擎(<em class="lo"> NE </em>)解析查询，并基于文章与用户查询的“文本相似性”输出每个知识库文章的分数。(b)类似地，推荐引擎(<em class="lo"> RE </em>)基于与每篇文章相关联的奖励，针对用户简档和情感，为每篇文章提供分数。策略函数可以形式化为如下形式:</p><figure class="lr ls lt lu gu lv gi gj paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gi gj nu"><img src="../Images/addd8517a61839bfdcb25d2a4087fe4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lQ2aDS-vGHioKHvw.png"/></div></div></figure><p id="d910" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated"><em class="lo">奖励</em> ( <em class="lo"> r </em>):是指我们用来衡量代理推荐行为的成败的反馈。反馈可以例如指用户阅读推荐文章所花费的时间量。我们考虑两步奖励函数计算，其中接收到的关于推荐动作的反馈<em class="lo"> fa </em>首先被映射到情感分数，然后情感分数被映射到奖励。</p><figure class="lr ls lt lu gu lv gi gj paragraph-image"><div class="gi gj nv"><img src="../Images/419f29c63bd28c2cfb49159fa4d83015.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/0*aLzDZBj-z5j1T8uk.png"/></div></figure><p id="1126" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">其中<em class="lo"> r </em>和<em class="lo"> s </em>分别指奖励和情感函数。图2中示出了上述RL公式。</p><figure class="lr ls lt lu gu lv gi gj paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gi gj nw"><img src="../Images/c7fb342105afaf2ba9651d26dd50a51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4O64s2-KoWyBQIek.png"/></div></div><p class="mc md gk gi gj me mf bd b be z dk translated">图二。RL公式(图片由作者提供)</p></figure><h1 id="6689" class="mg mh iu bd mi mj mk ml mm mn mo mp mq kj mr kk ms km mt kn mu kp mv kq mw mx bi translated">3 RL奖励和政策扩展</h1><h1 id="588f" class="mg mh iu bd mi mj mk ml mm mn mo mp mq kj mr kk ms km mt kn mu kp mv kq mw mx bi translated">3.1加权(多个)反馈通道</h1><p id="5115" class="pw-post-body-paragraph ks kt iu ku b kv my ke kx ky mz kh la lb na ld le lf nb lh li lj nc ll lm ln in bi translated">如图1所示，我们考虑一个多反馈渠道，从用户(边缘)设备/传感器捕获反馈，例如网络摄像头、恒温器、智能手表，或者嵌入在托管应用的移动设备中的摄像头、麦克风、加速度计。例如，捕捉用户面部表情的网络摄像头帧、用户智能手表提供的心率，可以与用户提供的文本响应“谢谢你的好建议”一起考虑；在计算用户对推荐动作的情绪时。</p><p id="9905" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">设<em class="lo"> {fa1，fa2，… fan} </em>表示行动<em class="lo"> a </em>收到的反馈。回想一下<em class="lo"> s(f) </em>表示基于各自的感官反馈<em class="lo"> f </em>独立计算的用户情绪。用户情感计算可以被认为是输出在<em class="lo">1–10</em>之间的值的分类器。然后，可以将奖励计算为情感分数的加权平均值，如下所示:</p><figure class="lr ls lt lu gu lv gi gj paragraph-image"><div class="gi gj nx"><img src="../Images/c8aea7fcc3f14d5a4872341cff970bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/0*8tldTQnNNecJ3qg8.png"/></div></figure><p id="72aa" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">其中权重<em class="lo"> {wa1，wa2，… wan} </em>允许系统协调接收到的反馈，因为一些反馈信道可能遭受低可靠性问题。例如，如果<em class="lo"> fi </em>对应于用户键入的响应，<em class="lo"> fj </em>对应于网络摄像头快照；则给予<em class="lo"> fi </em>更高的权重。这里的理由是，用户可能在快照中“微笑”，然而“微笑”是由于他的孩子进入房间(也在帧中被捕捉)，而不一定是对所接收的推荐/动作的响应。同时，如果基于用户文本响应计算的情感指示他“有压力”，那么在这种情况下，我们给予用户明确(文本响应)反馈更高的权重。</p><h1 id="b70d" class="mg mh iu bd mi mj mk ml mm mn mo mp mq kj mr kk ms km mt kn mu kp mv kq mw mx bi translated">3.2延迟奖励</h1><blockquote class="nd"><p id="9acb" class="ne nf iu bd ng nh ni nj nk nl nm ln dk translated"><em class="nn">在奖励不一致的情况下应用“延迟奖励”策略，其中(当前)计算的奖励对于已知用户积极响应的行为(对于历史上相同类型的行为)是“负面的”；反之亦然。给定这种不一致，延迟奖励策略缓冲在时间t的行动的计算的奖励比率；并且向R1代理策略(𝜋)提供指示，以在更新时间t和t+1的行为奖励之前尝试相同类型的另一个推荐，从而验证用户情绪。</em></p></blockquote><p id="1d5e" class="pw-post-body-paragraph ks kt iu ku b kv no ke kx ky np kh la lb nq ld le lf nr lh li lj ns ll lm ln in bi translated">为了适应“延迟奖励”策略，奖励功能扩展了一个内存缓冲区，允许从时间(<em class="lo"> t+m)到t </em>的最后<em class="lo"> m </em>个动作的奖励被聚集并在时间(<em class="lo"> t+m </em>)追溯应用。延迟奖励功能<em class="lo"> dr </em>表示如下:</p><figure class="lr ls lt lu gu lv gi gj paragraph-image"><div class="gi gj ny"><img src="../Images/5a41ae14631c7bf60d2db5b4c4442da1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/0*lqThDjKidkwnjrX-.png"/></div></figure><p id="4cbb" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">其中，|𝑡+𝑚暗示对行动的报酬从时间(<em class="lo"> t+m)到t </em>行动，尽管是单独计算的；只能在时间(<em class="lo"> t+m </em>)应用。如前所述，各自的权重<em class="lo">和</em>允许我们协调不一致的反馈的影响，其中对𝑎t时间𝑡𝑖的行为的奖励是基于在时间(𝑡+1)𝑖.</p><p id="5839" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">为了有效地执行“延迟奖励”策略，策略𝜋也被扩展为推荐与先前推荐的动作相同类型的动作；如果延迟标志<em class="lo"> d </em>置位(<em class="lo">d = 1</em>)；则“延迟”策略𝜋𝑑𝑡表示如下:</p><figure class="lr ls lt lu gu lv gi gj paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gi gj nz"><img src="../Images/fac5e997c98aabbc925691d614b57abc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KScm2uUUuIAQC3TV.png"/></div></div></figure><p id="38bb" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">图3示出了用延迟奖励/策略扩展的RL公式。</p><figure class="lr ls lt lu gu lv gi gj paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gi gj oa"><img src="../Images/b49a89a4b18de6300c4d87b83acf9de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*x8v5OOM-28rZi4Yg.png"/></div></div><p class="mc md gk gi gj me mf bd b be z dk translated">图3。延迟奖励——基于政策的RL公式(图片由作者提供)</p></figure><h1 id="2ce9" class="mg mh iu bd mi mj mk ml mm mn mo mp mq kj mr kk ms km mt kn mu kp mv kq mw mx bi translated">3.3奖励提升</h1><blockquote class="nd"><p id="cdb9" class="ne nf iu bd ng nh ni nj nk nl nm ln dk translated">奖励增加，或者说奖励正常化，主要适用于持续的聊天互动。在这种情况下，如果用户对推荐动作的情绪是“负面的”；这可能不仅仅是上一个动作的错。可能对话情绪已经在下降，最后推荐的操作只是跟随下降趋势。另一方面，在对话情绪恶化的情况下，对推荐行为的“积极”情绪意味着它对用户产生了非常积极的影响；因此相应的回报应该增加。</p></blockquote><p id="ca55" class="pw-post-body-paragraph ks kt iu ku b kv no ke kx ky np kh la lb nq ld le lf nr lh li lj ns ll lm ln in bi translated">对于在时间<em class="lo"> t </em>的处的动作<em class="lo">的提升的奖励𝑟𝑏𝑎𝑡计算如下:</em></p><figure class="lr ls lt lu gu lv gi gj paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gi gj ob"><img src="../Images/53c6150bc57d2bd4ebbba6bd7460fe22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ImGAsZ3QmDiqgOkT.png"/></div></div></figure><p id="3d37" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">我们把它作为将来的工作来扩展‘boost’函数到最后的<em class="lo"> n </em>动作(而不仅仅是上面的最后一个动作)。在这个扩展场景中，系统维护最后<em class="lo"> n </em>个动作的情绪曲线，并且相对于曲线而不是离散值来计算偏差。这里预期的好处是，它应该允许系统对用户情绪趋势做出更好的反应。</p><h1 id="9ceb" class="mg mh iu bd mi mj mk ml mm mn mo mp mq kj mr kk ms km mt kn mu kp mv kq mw mx bi translated">4结论</h1><p id="bef7" class="pw-post-body-paragraph ks kt iu ku b kv my ke kx ky mz kh la lb na ld le lf nb lh li lj nc ll lm ln in bi translated">在这项工作中，我们考虑在现实生活中的健康应用程序的背景下实现基于RL的推荐系统。RL是解决此类问题的强大原语，因为它允许应用程序实时学习和适应用户偏好/情绪。然而，在案例研究中，我们意识到当前的RL框架缺乏应用于这种推荐系统所需的某些结构。为了克服这个限制，我们引入了三个RL构造，我们必须为我们的Wellness应用程序实现它们。建议的RL结构在本质上是基本的，因为它们影响奖励和政策功能之间的相互作用；我们希望将它们添加到现有的RL框架中会增加企业的采用。</p><h1 id="68bb" class="mg mh iu bd mi mj mk ml mm mn mo mp mq kj mr kk ms km mt kn mu kp mv kq mw mx bi translated">参考</h1><p id="f16d" class="pw-post-body-paragraph ks kt iu ku b kv my ke kx ky mz kh la lb na ld le lf nb lh li lj nc ll lm ln in bi translated">[1]巴尔托，a .，萨顿，R.S.: <em class="lo">强化学习:导论</em>。麻省理工学院出版社，麻省剑桥，2018年，<a class="ae lp" href="http://incompleteideas.net/book/RLbook2018.pdf" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/RLbook2018.pdf</a></p><p id="6029" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">[2]崔顺实，哈，h，黄，U..Kim，c .，Ha，j .，Yoon，S.: <em class="lo">使用双聚类技术的基于强化学习的推荐系统</em>。arXiv:1801.05532，2018。</p><p id="4268" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">[3]刘，李，李，叶，陈，郭，张，杨:<em class="lo">基于深度强化学习的推荐与显式用户-项目交互建模</em>。arXiv:1810.12027，2018</p><p id="08f5" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">[4] Taghipour，n .，Kardan，a .，Ghidary，S.S.: <em class="lo">基于使用的网络推荐:强化学习方法</em>。摘自:美国计算机学会推荐系统会议，第113-120页(2007)。</p><p id="35f4" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">[5] Ricciardelli，e .，Biswas，D.: <em class="lo">基于强化学习的自我改进聊天机器人</em>。In:第四届强化学习与决策制定多学科会议(2019)。</p><p id="14a3" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">[6] <em class="lo">微软Azure Personalizer </em>、<br/>T3】https://Azure . Microsoft . com/en-us/services/cognitive-services/Personalizer/</p><p id="984a" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">[7]李，李，朱，w，兰福德，J.C .，沙皮雷，R.E.: <em class="lo">一种语境强盗式的个性化新闻文章推荐方法</em>。摘自:第19届万维网国际会议，第661–670页(2010年)。</p><p id="f170" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">[8] <em class="lo">谷歌RecSim </em>，<br/><a class="ae lp" href="https://ai.googleblog.com/2019/11/recsim-configurable-simulation-platform.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2019/11/rec sim-configurable-simulation-platform . html</a>，最后访问时间2019/02/03。</p><p id="d3af" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">[9]用于更好的推荐系统的强化学习。<a class="ae lp" href="https://analyticsindiamag.com/reinforcement-learning-recommender-system-recsim-google-ai/" rel="noopener ugc nofollow" target="_blank">https://analyticsindiamag . com/reinforcement-learning-recommender-system-recsim-Google-ai/</a></p><p id="c7e2" class="pw-post-body-paragraph ks kt iu ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln in bi translated">[10] <em class="lo">延迟强化学习。</em><a class="ae lp" href="http://heim.ifi.uio.no/~mes/inf1400/COOL/REF/Standford/ch11.pdf" rel="noopener ugc nofollow" target="_blank">http://heim . ifi . uio . no/~ mes/INF 1400/COOL/REF/Standford/ch11 . pdf</a></p></div></div>    
</body>
</html>