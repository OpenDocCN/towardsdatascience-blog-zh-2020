<html>
<head>
<title>K-Means Clustering in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的K-Means聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-in-python-4061510145cc?source=collection_archive---------8-----------------------#2020-10-24">https://towardsdatascience.com/k-means-clustering-in-python-4061510145cc?source=collection_archive---------8-----------------------#2020-10-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="046b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用K-Means直观地介绍数据科学</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7c231ba939c65186ba9e3124c7fc1dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0rVA7UeeHXy8PDt_"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@nasa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> NASA </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="a558" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">意味着聚类是一种无监督的最大似然算法，我们可以用它来将我们的数据集分成逻辑分组——称为聚类。因为它是无监督的，所以我们不需要依赖已标记的数据来进行训练。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi me"><img src="../Images/ec2920411b338c5df7624960d7f084d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ps83bf6-iX4Xto2SHLQQOg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用K-均值识别的五个聚类。</p></figure><p id="d32b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些分类是通过将数据分成明显不同的组来创建的，其中组成每个组的值是相似的，而不同组之间的值是不同的。</p><p id="d5fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-means中的<strong class="lb iu"> <em class="mf"> K </em> </strong>是指聚类的个数。聚类机制本身的工作方式是将数据集中的每个数据点标记为一个随机的聚类。然后，我们循环执行以下过程:</p><ol class=""><li id="a345" class="mg mh it lb b lc ld lf lg li mi lm mj lq mk lu ml mm mn mo bi translated">取每个聚类中所有数据点的平均值</li><li id="568c" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu ml mm mn mo bi translated">将该平均值设置为新的聚类中心(质心)</li><li id="ec4e" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu ml mm mn mo bi translated">将每个数据点重新标记到其最近的聚类质心。</li></ol><p id="5ac0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们继续这个过程，直到质心停止移动，这意味着我们已经成功地对数据集进行了聚类！</p><p id="757b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种算法的应用范围很广，一些最常见也最有趣的应用包括:</p><ul class=""><li id="07bf" class="mg mh it lb b lc ld lf lg li mi lm mj lq mk lu mu mm mn mo bi translated">文档聚类(例如，政策、法规、技术规范)</li><li id="ba7f" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu mu mm mn mo bi translated">市场分割</li><li id="a34e" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu mu mm mn mo bi translated">欺诈检测[1]</li><li id="e747" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu mu mm mn mo bi translated">识别犯罪热点[2]</li></ul><p id="69d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将使用一个简单的数据集。然而，K-Means可以有效地用于极其丰富多样的数据集-有些数据集有几十个特征和数百万个样本。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="08bc" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">实现算法</h1><p id="4f12" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">我们将使用Sci-kit Learn实现K-means。但在此之前，我们需要数据。这里我们可以使用Sci-kit Learn的<code class="fe nz oa ob oc b">make_blobs</code>函数来生成给定数量的人工生成的集群:</p><pre class="kj kk kl km gt od oc oe of aw og bi"><span id="a85d" class="oh nd it oc b gy oi oj l ok ol">from sklearn.datasets import make_blobs<br/>X, y = make_blobs(n_samples=500, n_features=3, centers=5,<br/>                  cluster_std=2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi me"><img src="../Images/7bd69b7b41f13a0556e144ff39ca382e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*_a5bhPhIvPnQpLK5i9OMmA.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在这个玩具示例中，我们可以分辨出我们的聚类——然而，当处理数百万个数据点和数十个特征/维度时，就变得困难多了。</p></figure><p id="c4ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样，在变量<code class="fe nz oa ob oc b">X</code>中，我们现在有了包含<code class="fe nz oa ob oc b">500</code>数据点的人工数据，每个数据点由<code class="fe nz oa ob oc b">3</code>特征/尺寸组成，并且每个数据点属于<code class="fe nz oa ob oc b">5</code>斑点中的一个。变量<code class="fe nz oa ob oc b">y</code>告诉我们每个数据点属于哪个集群——通常，我们不知道这一点。</p><h2 id="b03b" class="oh nd it bd ne om on dn ni oo op dp nm li oq or no lm os ot nq lq ou ov ns ow bi translated">k均值</h2><p id="4736" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">现在我们有了数据。我们可以运行K-Means对其进行聚类！我们再次使用sci-kit learn:</p><pre class="kj kk kl km gt od oc oe of aw og bi"><span id="6fd1" class="oh nd it oc b gy oi oj l ok ol">from sklearn.cluster import KMeans<br/>model = KMeans(n_clusters=5)</span></pre><p id="5459" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们已经用<code class="fe nz oa ob oc b">5</code>聚类质心初始化了K均值模型。然后，我们可以使用<code class="fe nz oa ob oc b">fit</code>方法用我们的数据<code class="fe nz oa ob oc b">X</code>运行我们的模型:</p><pre class="kj kk kl km gt od oc oe of aw og bi"><span id="788c" class="oh nd it oc b gy oi oj l ok ol">model.fit(X)</span></pre><p id="0bfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以分别使用<code class="fe nz oa ob oc b">labels_</code>和<code class="fe nz oa ob oc b">cluster_centers_</code>方法访问我们的数据分类和质心坐标。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/15dc28a7af1ad9ed875d735c277860a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V8cieOfw2nwOC-_lMzysUw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们分别用<strong class="bd oy"> model.cluster_centers_ </strong>和<strong class="bd oy"> model.labels_ </strong>访问<strong class="bd oy"> X </strong>数据聚类中心和标签。</p></figure><p id="1dd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当绘制质心和标记的数据点时，我们可以看到该算法已经识别了五个聚类:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi me"><img src="../Images/78f8a6ecada24218a69c044509dce67a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1YymybBBghks70gKahuGlA.gif"/></div></div></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="89aa" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">用肘法选择K</h1><p id="3779" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">我们对K-Means的理解中缺少的一部分是如何选择K？</p><p id="1482" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，最流行的方法是<em class="mf">肘法</em>。幸运的是，这也非常简单。</p><p id="6bfd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要用K的某个值运行我们的算法—一旦完成，我们可以从<code class="fe nz oa ob oc b">intertia_</code>属性中提取X值的误差平方和(SSE ):</p><pre class="kj kk kl km gt od oc oe of aw og bi"><span id="d666" class="oh nd it oc b gy oi oj l ok ol">sse = model.inertia_</span></pre><p id="62ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SSE计算为每个数据点与其分配的聚类质心之间的平方距离之和。如果所有数据点都紧紧聚集在分配给它们的质心周围，那么SSE将会很低，否则就会很高。</p><p id="ea46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重要的是，我们要有足够的聚类来匹配我们数据集中的聚类，但不要有太多的聚类，通过简单地为每个数据点分配它自己的聚类，SSE就会最小化。</p><pre class="kj kk kl km gt od oc oe of aw og bi"><span id="e3dc" class="oh nd it oc b gy oi oj l ok ol">sse = []</span><span id="1b34" class="oh nd it oc b gy oz oj l ok ol">for K in range(1, 10):<br/>    model = KMeans(n_clusters=K)<br/>    model.fit(X)<br/>    sse.append(model.inertia_)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/2f01c81584ba0a6414ea55adb26787d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YWa82oiw3h3g08wKjI3YCg.png"/></div></div></figure><p id="0bb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要找到一个神奇的地方，在那里我们可以正确地确定最合理的集群数量。可以通过计算K值范围的SSE并识别上图中的“弯头”来识别该点。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="7fdf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是对K-Means算法聚类的介绍！我们涵盖了:</p><ul class=""><li id="0fde" class="mg mh it lb b lc ld lf lg li mi lm mj lq mk lu mu mm mn mo bi translated">K的意思是</li><li id="3db4" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu mu mm mn mo bi translated">用在哪里</li><li id="b5eb" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu mu mm mn mo bi translated">我们如何使用它</li><li id="8f13" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu mu mm mn mo bi translated">用肘法选择K</li></ul><p id="7ccb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢这篇文章——如果你有任何问题或想法，请通过<a class="ae ky" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中告诉我。</p><p id="5128" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="3776" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">参考</h1><p id="f29d" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">[1] A. Ghorbani，S. Farzai，<a class="ae ky" href="http://www.aeuso.org/includes/files/articles/Vol8_Iss27_3764-3771_Fraud_Detection_in_Automobile_Insur.pdf" rel="noopener ugc nofollow" target="_blank">使用基于数据挖掘方法的汽车保险欺诈检测</a> (2018)，IJMEC</p><p id="f3af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] M. Zulfadhilah等人，<a class="ae ky" href="https://thesai.org/Downloads/Volume7No7/Paper_59-Cyber_Profiling_Using_Log_Analysis_And_K_Means_Clustering.pdf" rel="noopener ugc nofollow" target="_blank">使用日志分析和K-Means聚类的网络分析</a> (2016)，IJACSA</p><p id="3c76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mf">*除非另有说明，所有图片均出自作者之手</em></p></div></div>    
</body>
</html>