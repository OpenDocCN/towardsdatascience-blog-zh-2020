<html>
<head>
<title>Controlling a Mouse With Your Eyes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用眼睛控制鼠标</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/controlling-a-mouse-with-your-eyes-f1097e7cf2e9?source=collection_archive---------29-----------------------#2020-09-21">https://towardsdatascience.com/controlling-a-mouse-with-your-eyes-f1097e7cf2e9?source=collection_archive---------29-----------------------#2020-09-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/23218c3602e0311fdab155a616b4b6a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*15ZuFhSoN1JLG5zWJCf73g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">根据眼睛位置自动导航到坐标的鼠标(图片由作者提供)</p></figure><div class=""/><div class=""><h2 id="fc3b" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">仅从单个正面视角作为输入的眼睛姿态估计的机器学习方法</h2></div><p id="36a1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在这个项目中，我们将编写代码，在你每次点击鼠标时裁剪你眼睛的图像。使用这些数据，我们可以反向训练一个模型，从你的眼睛预测鼠标的位置。</p><p id="d8f3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们需要几个图书馆</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="1ea2" class="lz ma jf lv b gy mb mc l md me"># For monitoring web camera and performing image minipulations<br/>import cv2</span><span id="a7de" class="lz ma jf lv b gy mf mc l md me"># For performing array operations<br/>import numpy as np</span><span id="9018" class="lz ma jf lv b gy mf mc l md me"># For creating and removing directories<br/>import os<br/>import shutil</span><span id="9553" class="lz ma jf lv b gy mf mc l md me"># For recognizing and performing actions on mouse presses<br/>from pynput.mouse import Listener</span></pre><p id="919b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们先来了解一下pynput的<code class="fe mg mh mi lv b">Listener</code>是如何工作的。</p><p id="f8af" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><code class="fe mg mh mi lv b">pynput.mouse.Listener</code>创建一个记录鼠标移动和鼠标点击的后台线程。下面是一个简化代码，当鼠标按下时，它打印鼠标的坐标:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="9ae8" class="lz ma jf lv b gy mb mc l md me">from pynput.mouse import Listener</span><span id="0304" class="lz ma jf lv b gy mf mc l md me">def on_click(x, y, button, pressed):<br/>  """<br/>  Args:<br/>    x: the x-coordinate of the mouse<br/>    y: the y-coordinate of the mouse<br/>    button: 1 or 0, depending on right-click or left-click<br/>    pressed: 1 or 0, whether the mouse was pressed or released<br/>  """<br/>  if pressed:<br/>    print (x, y)</span><span id="b5fc" class="lz ma jf lv b gy mf mc l md me">with Listener(on_click = on_click) as listener:<br/>  listener.join()</span></pre><p id="0771" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，为了我们的目的，让我们扩展这个框架。然而，我们首先需要编写裁剪眼睛边框的代码。稍后我们将从<code class="fe mg mh mi lv b">on_click</code>函数中调用这个函数。</p><p id="09c6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们使用<a class="ae mj" href="https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html" rel="noopener ugc nofollow" target="_blank"> Haar级联对象检测</a>来确定用户眼睛的包围盒。你可以点击下载探测器文件<a class="ae mj" href="https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye.xml" rel="noopener ugc nofollow" target="_blank">。让我们做一个简单的演示来说明这是如何工作的:</a></p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="2543" class="lz ma jf lv b gy mb mc l md me">import cv2</span><span id="4886" class="lz ma jf lv b gy mf mc l md me"># Load the cascade classifier detection object<br/>cascade = cv2.CascadeClassifier("haarcascade_eye.xml")</span><span id="0c21" class="lz ma jf lv b gy mf mc l md me"># Turn on the web camera<br/>video_capture = cv2.VideoCapture(0)</span><span id="498f" class="lz ma jf lv b gy mf mc l md me"># Read data from the web camera (get the frame)<br/>_, frame = video_capture.read()</span><span id="10b6" class="lz ma jf lv b gy mf mc l md me"># Convert the image to grayscale<br/>gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</span><span id="f635" class="lz ma jf lv b gy mf mc l md me"># Predict the bounding box of the eyes<br/>boxes = cascade.detectMultiScale(gray, 1.3, 10)</span><span id="dd3e" class="lz ma jf lv b gy mf mc l md me"># Filter out images taken from a bad angle with errors<br/># We want to make sure both eyes were detected, and nothing else<br/>if len(boxes) == 2:<br/>  eyes = []<br/>  for box in boxes:<br/>    # Get the rectangle parameters for the detected eye<br/>    x, y, w, h = box<br/>    # Crop the bounding box from the frame<br/>    eye = frame[y:y + h, x:x + w]<br/>    # Resize the crop to 32x32<br/>    eye = cv2.resize(eye, (32, 32))<br/>    # Normalize<br/>    eye = (eye - eye.min()) / (eye.max() - eye.min())<br/>    # Further crop to just around the eyeball<br/>    eye = eye[10:-10, 5:-5]<br/>    # Scale between [0, 255] and convert to int datatype<br/>    eye = (eye * 255).astype(np.uint8)<br/>    # Add the current eye to the list of 2 eyes<br/>    eyes.append(eye)</span><span id="25a7" class="lz ma jf lv b gy mf mc l md me">  # Concatenate the two eye images into one<br/>  eyes = np.hstack(eyes)</span></pre><p id="f264" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，让我们用这些知识写一个函数来裁剪眼睛的图像。首先，我们需要一个帮助函数来进行规范化:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="c25a" class="lz ma jf lv b gy mb mc l md me">def normalize(x):<br/>  minn, maxx = x.min(), x.max()<br/>  return (x - minn) / (maxx - minn)</span></pre><p id="42ce" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这是我们的眼睛裁剪功能。如果找到了眼睛，它将返回图像。否则，返回<code class="fe mg mh mi lv b">None</code>:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="9a26" class="lz ma jf lv b gy mb mc l md me">def scan(image_size=(32, 32)):<br/>  _, frame = video_capture.read()</span><span id="0403" class="lz ma jf lv b gy mf mc l md me">  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)<br/>  boxes = cascade.detectMultiScale(gray, 1.3, 10)</span><span id="3d96" class="lz ma jf lv b gy mf mc l md me">  if len(boxes) == 2:<br/>    eyes = []<br/>    for box in boxes:<br/>      x, y, w, h = box<br/>      eye = frame[y:y + h, x:x + w]<br/>      eye = cv2.resize(eye, image_size)<br/>      eye = normalize(eye)<br/>      eye = eye[10:-10, 5:-5]<br/>      eyes.append(eye)</span><span id="f17f" class="lz ma jf lv b gy mf mc l md me">    return (np.hstack(eyes) * 255).astype(np.uint8)<br/>  else:<br/>    return None</span></pre><p id="23df" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，让我们编写我们的自动化程序，它将在我们每次按下鼠标按钮时运行。(假设我们已经在代码中将变量<code class="fe mg mh mi lv b">root</code>定义为我们想要存储图像的目录):</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="20c2" class="lz ma jf lv b gy mb mc l md me">def on_click(x, y, button, pressed):<br/>  # If the action was a mouse PRESS (not a RELEASE)<br/>  if pressed:<br/>    # Crop the eyes<br/>    eyes = scan()<br/>    # If the function returned None, something went wrong<br/>    if not eyes is None:<br/>      # Save the image<br/>      filename = root + "{} {} {}.jpeg".format(x, y, button)<br/>      cv2.imwrite(filename, eyes)</span></pre><p id="5e07" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，我们可以回忆起我们对pynput的<code class="fe mg mh mi lv b">Listener</code>的实现，并进行完整的代码实现:</p><figure class="lq lr ls lt gt is"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="e2be" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">当我们运行这个时，每次我们点击鼠标(如果我们的双眼都在视野中)，它将自动裁剪网络摄像头并将图像保存到适当的目录中。图像的文件名将包含鼠标坐标信息，以及是右键还是左键单击。</p><p id="6cb5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这是一个示例图像。在此图像中，我在分辨率为2560x1440的显示器上的坐标(385，686)处执行左键单击:</p><figure class="lq lr ls lt gt is gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/45a0d801505b6d517d6bec585c0acfeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*J_9_Xt04Y1bQFlvPCo3Bfg.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">一个例子(图片由作者提供)</p></figure><p id="eb0d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">级联分类器的准确率很高，我自己的数据目录到目前为止还没有看到任何错误。</p><p id="573a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，让我们编写用于训练神经网络的代码，以在给定眼睛图像的情况下预测鼠标位置。</p><p id="e2e5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们导入一些库</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="a85a" class="lz ma jf lv b gy mb mc l md me">import numpy as np<br/>import os<br/>import cv2<br/>import pyautogui</span><span id="2a04" class="lz ma jf lv b gy mf mc l md me">from tensorflow.keras.models import *<br/>from tensorflow.keras.layers import *<br/>from tensorflow.keras.optimizers import *</span></pre><p id="1384" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，让我们添加级联分类器:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="fd1f" class="lz ma jf lv b gy mb mc l md me">cascade = cv2.CascadeClassifier("haarcascade_eye.xml")<br/>video_capture = cv2.VideoCapture(0)</span></pre><p id="f784" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们添加我们的助手函数。</p><p id="7850" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">标准化:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="5e81" class="lz ma jf lv b gy mb mc l md me">def normalize(x):<br/>  minn, maxx = x.min(), x.max()<br/>  return (x - minn) / (maxx - minn)</span></pre><p id="6386" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">捕捉眼球:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="0cd1" class="lz ma jf lv b gy mb mc l md me">def scan(image_size=(32, 32)):<br/>  _, frame = video_capture.read()</span><span id="130a" class="lz ma jf lv b gy mf mc l md me">  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)<br/>  boxes = cascade.detectMultiScale(gray, 1.3, 10)</span><span id="488d" class="lz ma jf lv b gy mf mc l md me">  if len(boxes) == 2:<br/>    eyes = []<br/>    for box in boxes:<br/>      x, y, w, h = box<br/>      eye = frame[y:y + h, x:x + w]<br/>      eye = cv2.resize(eye, image_size)<br/>      eye = normalize(eye)<br/>      eye = eye[10:-10, 5:-5]<br/>      eyes.append(eye)</span><span id="3d79" class="lz ma jf lv b gy mf mc l md me">    return (np.hstack(eyes) * 255).astype(np.uint8)<br/>  else:<br/>    return None</span></pre><p id="545a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们定义一下显示器的尺寸。您必须根据自己电脑屏幕的分辨率来更改这些参数:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="137a" class="lz ma jf lv b gy mb mc l md me"># Note that there are actually 2560x1440 pixels on my screen<br/># I am simply recording one less, so that when we divide by these<br/># numbers, we will normalize between 0 and 1. Note that mouse<br/># coordinates are reported starting at (0, 0), not (1, 1)<br/>width, height = 2559, 1439</span></pre><p id="cdfa" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，让我们加载我们的数据(同样，假设您已经定义了<code class="fe mg mh mi lv b">root</code>)。我们并不关心是右键还是左键，因为我们的目标只是预测鼠标的位置:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="93e9" class="lz ma jf lv b gy mb mc l md me">filepaths = os.listdir(root)<br/>X, Y = [], []</span><span id="7aaf" class="lz ma jf lv b gy mf mc l md me">for filepath in filepaths:<br/>  x, y, _ = filepath.split(' ')<br/>  x = float(x) / width<br/>  y = float(y) / height<br/>  X.append(cv2.imread(root + filepath))<br/>  Y.append([x, y])</span><span id="90e9" class="lz ma jf lv b gy mf mc l md me">X = np.array(X) / 255.0<br/>Y = np.array(Y)<br/>print (X.shape, Y.shape)</span></pre><p id="09aa" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们定义我们的模型架构:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="fc65" class="lz ma jf lv b gy mb mc l md me">model = Sequential()<br/>model.add(Conv2D(32, 3, 2, activation = 'relu', input_shape = (12, 44, 3)))<br/>model.add(Conv2D(64, 2, 2, activation = 'relu'))<br/>model.add(Flatten())<br/>model.add(Dense(32, activation = 'relu'))<br/>model.add(Dense(2, activation = 'sigmoid'))<br/>model.compile(optimizer = "adam", loss = "mean_squared_error")<br/>model.summary()</span></pre><p id="140a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">以下是我们的总结:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="818f" class="lz ma jf lv b gy mb mc l md me">_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #<br/>=================================================================<br/>conv2d (Conv2D)              (None, 5, 21, 32)         896<br/>_________________________________________________________________<br/>conv2d_1 (Conv2D)            (None, 2, 10, 64)         8256<br/>_________________________________________________________________<br/>flatten (Flatten)            (None, 1280)              0<br/>_________________________________________________________________<br/>dense (Dense)                (None, 32)                40992<br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 2)                 66<br/>=================================================================<br/>Total params: 50,210<br/>Trainable params: 50,210<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="4434" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们训练我们的模型。我们将在图像数据中添加一些噪声:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="a20a" class="lz ma jf lv b gy mb mc l md me">epochs = 200<br/>for epoch in range(epochs):<br/>  model.fit(X, Y, batch_size = 32)</span></pre><p id="5a84" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">不，让我们用我们的模型用我们的眼睛移动鼠标。请注意，这需要大量数据才能正常工作。然而，作为一个概念的证明，你会注意到只有大约200张图片，它确实可以将鼠标移动到你正在查看的大致区域。当然，除非你有更多的数据，否则这是不可控的。</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="0084" class="lz ma jf lv b gy mb mc l md me">while True:<br/>  eyes = scan()</span><span id="7ab9" class="lz ma jf lv b gy mf mc l md me">  if not eyes is None:<br/>      eyes = np.expand_dims(eyes / 255.0, axis = 0)<br/>      x, y = model.predict(eyes)[0]<br/>      pyautogui.moveTo(x * width, y * height)</span></pre><p id="36f4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这里有一个概念验证的例子。请注意，在拍摄这个屏幕记录之前，我用很少的数据进行了训练。这是我的鼠标根据眼睛自动移动到终端应用窗口的视频。就像我说的，这很不稳定，因为数据很少。随着更多的数据，它将有望足够稳定，以更高的特异性进行控制。只有几百张图片，你只能把它移动到你的视线范围内。此外，如果在整个数据收集过程中，没有拍摄到您注视屏幕特定区域(比如边缘)的图像，则该模型不太可能在该区域内进行预测。这是我们需要更多数据的众多原因之一。</p><div class="ip iq gp gr ir mn"><a href="https://drive.google.com/file/d/1TkU3rRS68U9vk4t7AB--NEQgyYH77Irm/view?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jg gy z fp ms fr fs mt fu fw je bi translated">eye_mouse_movement.mp4</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">根据凝视自动移动鼠标的视频</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">drive.google.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb ix mn"/></div></div></a></div><p id="caf2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">如果您自己测试代码，记得在代码文件<code class="fe mg mh mi lv b">prediction.py</code>中将<code class="fe mg mh mi lv b">width</code>和<code class="fe mg mh mi lv b">height</code>的值更改为您显示器的分辨率。</p><p id="c472" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">您可以在此处查看本教程中的代码:</p><div class="ip iq gp gr ir mn"><a href="https://github.com/Ryan-Rudes/eye_mouse_movement" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jg gy z fp ms fr fs mt fu fw je bi translated">Ryan-Rudes/眼睛_鼠标_运动</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">GitHub是一个使用机器学习方法来控制具有眼睛姿态估计的计算机鼠标的项目</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">github.com</p></div></div><div class="mw l"><div class="nc l my mz na mw nb ix mn"/></div></div></a></div></div></div>    
</body>
</html>