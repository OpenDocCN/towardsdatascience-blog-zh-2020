<html>
<head>
<title>Bagging on Low Variance Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于低方差模型的Bagging</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bagging-on-low-variance-models-38d3c70259db?source=collection_archive---------33-----------------------#2020-10-12">https://towardsdatascience.com/bagging-on-low-variance-models-38d3c70259db?source=collection_archive---------33-----------------------#2020-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="7742" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="9cab" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><em class="ko">简单线性回归装袋的奇特案例</em></h2></div><p id="b469" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">Bagging(也称为bootstrap aggregation)是一种技术，在这种技术中，我们根据均匀的概率分布重复地采取多个样本进行替换，并在其上拟合一个模型。它将多个预测组合在一起，通过多数投票或汇总预测来给出更好的预测。这种技术对于倾向于过度拟合数据集的模型(高方差模型)是有效的。Bagging减少了方差而不会使预测有偏差。这项技术是许多合奏技术的基础，所以理解它背后的直觉是至关重要的。</p><p id="dbaf" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">如果这种技术这么好，为什么我们只在显示高方差的模型上使用它？当我们将它用于低方差模型时会发生什么？让我们借助演示来理解潜在的问题。</p><p id="cd5e" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们将在决策树上使用bagging来证明bagging提高了高方差模型的准确性，并将其与基于简单线性回归的bagging进行比较，后者因数据集而有偏差。当预测值与目标变量不完全相关时，简单线性回归是有偏差的。</p><h1 id="f9b8" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated"><strong class="ak">偏差和方差</strong></h1><p id="09a4" class="pw-post-body-paragraph kp kq iq kr b ks md ka ku kv me kd kx ky mf la lb lc mg le lf lg mh li lj lk ij bi translated">我们将在整篇文章中讨论偏差和方差，所以让我们先了解一下它是什么。</p><h2 id="4812" class="mi lm iq bd ln mj mk dn lr ml mm dp lv ky mn mo lx lc mp mq lz lg mr ms mb iw bi translated"><strong class="ak">偏置</strong></h2><p id="e1d6" class="pw-post-body-paragraph kp kq iq kr b ks md ka ku kv me kd kx ky mf la lb lc mg le lf lg mh li lj lk ij bi translated">高偏差是指模型过于简化。即当我们不能捕捉数据的真实关系时。我们创建模型的目的是捕捉数据的真实性质，并根据趋势进行预测，这使得高偏差成为一种不受欢迎的现象。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/4d3d5840e2da49628f3cc8a281089ed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*7XxGJ10uEqkPJpyWIk8JnQ.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">在这种情况下，正在使用的模型并不合适。<em class="ko">作者图片</em></p></figure><h2 id="7354" class="mi lm iq bd ln mj mk dn lr ml mm dp lv ky mn mo lx lc mp mq lz lg mr ms mb iw bi translated"><strong class="ak">差异</strong></h2><p id="bf64" class="pw-post-body-paragraph kp kq iq kr b ks md ka ku kv me kd kx ky mf la lb lc mg le lf lg mh li lj lk ij bi translated">高方差指的是模型过于复杂的情况。即，在捕捉模型的真实性质的过程中，我们正在创建一个模型，该模型很好地学习了训练数据，以至于它的准确性在任何其他数据集上都恶化了。这种情况也是不希望的，因为我们的目标是对看不见的数据进行预测。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/8f3d112c26d843a09ee30213935f0e67.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*KkWMbcHMqRnNvfghc3exNQ.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">在这种情况下创建的模型不适用于训练数据集中不存在的数据。<em class="ko">作者提供的图片</em></p></figure><h2 id="0493" class="mi lm iq bd ln mj mk dn lr ml mm dp lv ky mn mo lx lc mp mq lz lg mr ms mb iw bi translated"><strong class="ak">偏差-方差权衡</strong></h2><p id="5e6c" class="pw-post-body-paragraph kp kq iq kr b ks md ka ku kv me kd kx ky mf la lb lc mg le lf lg mh li lj lk ij bi translated">当我们创建一个模型时，我们希望在偏差和方差之间取得平衡。偏差和方差是相反的，所以每当我们试图减少方差时，我们就同时增加了模型的偏差。这种过拟合/欠拟合的困境被称为偏差-方差权衡。这张图片很好地展示了他们之间的关系。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/85884ef1b4b718fdd5ada61850989c1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*B3vUjQ0rVRzI-Q2D50HUug.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">偏差-方差权衡。<em class="ko">作者图片</em></p></figure><h1 id="0dc2" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated"><strong class="ak">高方差模型—决策树</strong></h1><p id="f9b3" class="pw-post-body-paragraph kp kq iq kr b ks md ka ku kv me kd kx ky mf la lb lc mg le lf lg mh li lj lk ij bi translated">决策树对目标变量进行分类，在默认设置下，它不会停止，除非它对每个类别进行了完美的分类。这使得树过度适应所提供的数据，并且模型在测试数据集上的准确性将会很低。让我们用一个数据集来验证这一点。我们将使用<a class="ae nf" href="https://github.com/akshayamrit/Bagging-on-low-variance-models/blob/master/pima_indians_diabetes.data.csv" rel="noopener ugc nofollow" target="_blank">皮马印第安人糖尿病数据集</a>。</p><p id="a45b" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">决策树的准确性:</p><figure class="mu mv mw mx gt my"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/19e8d63ca01896adf5d118bf722f7b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*xyB2tq00iFS2GASZmXTlMA.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated"><em class="ko">作者图片</em></p></figure><h1 id="75b7" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated"><strong class="ak">决策树的打包</strong></h1><p id="5cbc" class="pw-post-body-paragraph kp kq iq kr b ks md ka ku kv me kd kx ky mf la lb lc mg le lf lg mh li lj lk ij bi translated">正如我们之前讨论的，bagging应该在不增加偏差的情况下减少我们预测的方差。这一特性的直接影响可以从预测准确性的变化中看出。装袋会让训练精度和测试精度的差异变小。我们可能不会总是观察到训练精度的变化，但是在这种情况下，测试精度总是更好。让我们检查一下装袋对数据集的影响。</p><figure class="mu mv mw mx gt my"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/c6b7404cc21aaafc8e5f751ab01acbfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*Fq2Y1CFopEh8pnE3D0Yhrg.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated"><em class="ko">作者图片</em></p></figure><p id="670a" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">这个结果证明了我们的观点！装袋提高了高方差模型的性能！</p><h1 id="f8ae" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated"><strong class="ak">低方差模型—简单线性回归</strong></h1><p id="1998" class="pw-post-body-paragraph kp kq iq kr b ks md ka ku kv me kd kx ky mf la lb lc mg le lf lg mh li lj lk ij bi translated">到目前为止，我们在本文中讨论的一切都是已知和直观的，但是当我们试图在简单线性回归这样的低方差模型上使用bagging时会发生什么呢？让我们通过演示每个袋装模型会发生什么来探索这个场景。我们可以直观地宣称，由于bagging只影响高方差模型，它应该对有偏模型没有影响。我们将在文章的其余部分验证我们的假设是否正确。</p><p id="7645" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们将使用<a class="ae nf" href="https://github.com/akshayamrit/Bagging-on-low-variance-models/blob/master/kc_house_data.csv" rel="noopener ugc nofollow" target="_blank">金县房价数据集</a>。我们只使用一个变量来简化可视化。让我们看看price和sqft_living之间的散点图，以大致了解它们之间的关系，并在此基础上建立一个简单的线性回归模型。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3d8fd2ed3003824396ac1ff97dce0d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*s5zFacySotcCPuQFlNmbvg.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated"><em class="ko">作者提供的图片</em></p></figure><p id="9b46" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">这种情况下简单线性回归模型的准确性:</p><figure class="mu mv mw mx gt my"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/c5529d64e65e29be4ee02ecf562473ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*LfI4tU5_TI6FyIBL1RKugQ.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated"><em class="ko">作者图片</em></p></figure><h1 id="33b7" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated"><strong class="ak">简单线性回归装袋</strong></h1><p id="2806" class="pw-post-body-paragraph kp kq iq kr b ks md ka ku kv me kd kx ky mf la lb lc mg le lf lg mh li lj lk ij bi translated">在讨论“简单线性回归装袋的奇怪案例”之前，让我们快速检查装袋简单线性回归后的准确性是否提高或甚至保持模型性能不变。</p><figure class="mu mv mw mx gt my"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/533de75f630d52ee6c303daa9b845de6.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*hlnpQqi9Ms0Ma9P_C8QPcw.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated"><em class="ko">作者图片</em></p></figure><p id="c8e9" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">尽管打包了200个简单的线性回归模型，RMSE(均方根误差)还是下降了2.7个百分点！简单线性回归的有偏性是其背后的原因。我们观察到RMSE的变化非常小，因为我们的模型能够很好地捕捉数据集的趋势。sqft_living和价格之间的相关性为0.7，简单线性回归模型捕捉了变量之间的线性关系。如果简单线性回归的相关性较低，RMSE的差异会更大。让我们在下一节讨论为什么。</p><h1 id="87b5" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated"><strong class="ak">为什么简单线性回归对装袋效果不好？</strong></h1><p id="4008" class="pw-post-body-paragraph kp kq iq kr b ks md ka ku kv me kd kx ky mf la lb lc mg le lf lg mh li lj lk ij bi translated">简单线性回归是一种有条件偏差的模型。即当变量之间存在明确的线性关系时，该模型可以被认为是稳定的。在这种情况下，即使我们对其使用bagging，模型的准确性也不会降低。当变量之间的这种关系改变时，简单线性回归试图创建一条直线来捕捉数据的趋势。在这种情况下，单个袋装模型的偏差会增加。</p><p id="635d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">由于我们的变量之间的相关性是0.7，它不是完全相关的，但它足够好，在这里使用简单的线性回归是有意义的。这就是为什么当我们在RMSE装袋时，它没有大幅度下降的原因。bagging技术创建多个线性回归模型，并取其预测值的平均值。所有这些点将位于回归线上，该回归线可以通过取每个模型的截距和系数的平均值来产生。让我们将平均回归线的预测与最终的袋装预测进行比较。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nk"><img src="../Images/51c6f432791a4147f34fd4a1d40128d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*ZbTtLXbTzPZOoyn0n9JNgQ.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated"><em class="ko">作者图片</em></p></figure><p id="1a57" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">该图中的每条灰线代表单独的简单线性回归模型。它们的偏差高于直接从原始数据集获得的回归线，因为bootstrap样本中存在许多重复点。这使得一些点比其余的点更有影响力。这导致平均回归线偏离了我们在没有装袋的情况下可以获得的回归线。</p><p id="c46c" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">这意味着我们假设bagging对高偏差模型没有影响是不正确的！Bagging确实会影响具有高偏差的模型，但它反而会降低其准确性。这是不是意味着对有高偏差的模型进行打包，总会得到比我们从原始模型中得到的结果更差的结果？让我们再看几张图来总结一下。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/264c9994801521d52e9bc3df8e530bef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*RD--l6QiFbfmUAyoHtQEGg.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">引导样本数= 10。<em class="ko">作者图片</em></p></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/c782cf3f44dbc83be9db95c23b54685a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Sg4WYejW0wb_58U-oy3Y-Q.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">引导样本数= 100。"<em class="ko">作者图片"</em></p></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/c9a511a2f0e52add71077b3c154a88a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*uTpjVZfAz4RpbRbV5kaUoQ.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">引导样本数= 200。<em class="ko">作者图片</em></p></figure><p id="e7e7" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们观察到，随着自助样本数量的增加，范围、标准偏差会缩小。RMSE中值也向简单线性回归线的RMSE移动。这表明袋装预测越来越接近简单的线性回归预测，而没有偏离太多。</p><h1 id="31d7" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated"><strong class="ak">结论</strong></h1><p id="e21a" class="pw-post-body-paragraph kp kq iq kr b ks md ka ku kv me kd kx ky mf la lb lc mg le lf lg mh li lj lk ij bi translated">从这篇文章中可以得出以下几点:</p><ol class=""><li id="1a81" class="nr ns iq kr b ks kt kv kw ky nt lc nu lg nv lk nw nx ny nz bi translated">在高方差模型上装袋:在不增加偏差的情况下，模型的方差会减少。这种模型的性能会更好，所以建议装袋。</li><li id="8339" class="nr ns iq kr b ks oa kv ob ky oc lc od lg oe lk nw nx ny nz bi translated">对高偏差模型进行Bagging:与没有bagging的模型相比，模型的准确性总是会下降。查看上面的直方图，我们可以得出结论，装袋精度随着装袋模型数量的增加而增加，并且随着n达到无穷大，装袋模型的精度将等于直接模型的精度。由于在有偏差模型的情况下，模型的准确性不会增加，因此不建议使用bagging。</li></ol><p id="807b" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">本文的重点是展示当我们使用bagging中方差较低的模型时会发生什么。目的是让读者直观地了解哪种模型适合装袋。</p><p id="d698" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">请注意，文章中给出的一些代码可能无法工作，因为python中没有这些函数。我不得不从头开始编写代码来展示每个模型的偏见，所以如果你正在寻找代码，请参考这个链接— <a class="ae nf" href="https://github.com/akshayamrit/Bagging-on-low-variance-models" rel="noopener ugc nofollow" target="_blank"> GitHub </a></p><h1 id="7bee" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated">参考</h1><p id="2d6d" class="pw-post-body-paragraph kp kq iq kr b ks md ka ku kv me kd kx ky mf la lb lc mg le lf lg mh li lj lk ij bi translated">[1] Aishwarya Singh，<a class="ae nf" href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/" rel="noopener ugc nofollow" target="_blank">《集成学习综合指南》(附Python代码)</a>(2018)<br/>【2】Daniel t . la rose-Chantal d . la rose，7.5偏差-方差权衡，数据挖掘和预测分析<br/>【3】Trevor Hastie等人，8.7 Bagging，The Elements of Statistical Learning<br/>【4】Bradley Boehmke等人，10 Bagging，Hands on Machine Learning</p></div></div>    
</body>
</html>