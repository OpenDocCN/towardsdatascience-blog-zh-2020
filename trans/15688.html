<html>
<head>
<title>The ins and outs of Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降的来龙去脉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-ins-and-outs-of-gradient-descent-1cf23dc90f83?source=collection_archive---------45-----------------------#2020-10-28">https://towardsdatascience.com/the-ins-and-outs-of-gradient-descent-1cf23dc90f83?source=collection_archive---------45-----------------------#2020-10-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1125" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">优化您对优化器的选择</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/22185f5bd20801dd7b0e012bb0ac5533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5RR1B99f5T5Upkt-"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">查德·特兹拉夫在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="c564" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">梯度下降</strong>是一种优化算法，用于通过在<strong class="ky ir">最陡下降</strong>的方向上迭代移动来最小化一些成本函数。也就是说，在具有最大负梯度的方向上移动。在机器学习中，我们使用梯度下降来不断调整模型中的参数，以最小化成本函数。我们从模型参数的一些值(神经网络中的权重和偏差)开始，并慢慢地改进它们。在这篇博文中，我们将从探索经典机器学习中常用的一些基本优化器开始，然后转向神经网络和深度学习中使用的一些更流行的算法。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="1fcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">想象一下，你正在山里跑步(或散步，随便什么能让你开心的事)，突然一股浓雾遮住了你的视线。下山的一个好策略是从各个方向感受地面，并向地面下降最快的方向迈一步。重复这个过程，你应该会到达山的底部(尽管你也可能会到达一些看起来不像底部的地方——稍后会详细介绍)。这正是梯度下降所做的:它测量代价函数<em class="lz">J(</em><strong class="ky ir"><em class="lz">ω</em></strong><em class="lz">)(</em>由模型参数<strong class="ky ir"> <em class="lz"> ω </em> </strong>参数化)的局部梯度，并向梯度下降的方向移动。一旦我们达到零梯度，我们就达到了最小值。</p><p id="dd23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设现在山的陡度对你来说不是很明显(也许你太冷了，感觉不到你的脚，发挥你的想象力！)，所以你得用手机的加速度计来测量梯度。外面真的很冷，所以要检查你的手机，你必须停止跑步，脱下手套。因此，如果你想尽快把手机关机，你需要尽量少用手机！因此，你需要选择正确的频率来测量山坡的倾斜度，这样才不会偏离轨道，同时在日落前到家。两次检查之间的时间就是所谓的<strong class="ky ir">学习率</strong>，也就是我们走下坡路的步数。如果我们的学习率太小，那么算法就需要很长时间才能收敛。但是，如果我们的学习率太高，算法可能会发散，并刚刚超过最小值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ma"><img src="../Images/9eeb10912d5cc6ac8e786617304fc53e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p6vvO_g4mARRcBFDt9hylw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片:不同学习率下梯度下降的描述</p></figure><p id="546c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">需要注意的重要一点是，并非所有的成本函数都是凸的(然而，线性回归的MSE成本函数是凸的)。如果函数图上任意两点之间的线段位于两点之间的图的上方，则该函数是凸的。凸性是好的，因为它意味着有一个单一的全局最小值。当成本函数不是凸的时，问题就开始出现了:可能有脊、平台、洞等等。这使得收敛到最小值变得困难。幸运的是，有一些技术可以帮助我们解决这些问题。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mb"><img src="../Images/d84add86b26769c3368bf9d21555baae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TsFL3gJTZCRz9Xf1IHUzdA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片:描述梯度下降可能出现的问题</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="7abf" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">批量梯度下降</h2><p id="7230" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">梯度下降世界的香草。在批量梯度下降中，我们在每个梯度步骤使用完整的训练集<strong class="ky ir"> <em class="lz"> X </em> </strong>。因此，当使用大型训练集时，速度会非常慢。</p><p id="1aab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从寻找成本函数的梯度向量开始，我们将其表示为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/084a5093033dec963af07adab881d3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*LIcRPB5lm72C35pePJzHtQ.png"/></div></figure><p id="f7ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，<em class="lz">j(</em><strong class="ky ir"><em class="lz">ω</em></strong><em class="lz">)</em>是由模型参数<strong class="ky ir"> <em class="lz"> ω </em> </strong>参数化的代价函数，∇表示向量微分算子。现在我们有了梯度向量，我们只需要它往相反的方向(下坡)。然后，乘以学习速率η，我们得到一步梯度下降更新方程:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/5c982968e571b876fb1e364cdd9ce7cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*PKqR13LxQLbAxH14BBTGSQ.png"/></div></figure><p id="5ced" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用python实现批量梯度下降再简单不过了！</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="4a09" class="mc md iq nd b gy nh ni l nj nk">import numpy as np <br/>eta = 0.01 # our chosen learning rate <br/>iterations = 10^3<br/>n = 100</span><span id="5040" class="mc md iq nd b gy nl ni l nj nk">theta = np.random.randn(d,1) # random initialization, with d being <br/># the dimension of theta</span><span id="ceb3" class="mc md iq nd b gy nl ni l nj nk">for i in range(iterations):<br/>    gradients = 2/n * X.T.dot(X.dot(theta) - y)<br/>    theta = theta - eta * gradients</span></pre><p id="1d8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们愿意，我们可以使用网格搜索来找到一个好的学习率。为了找到正确的迭代次数，通常将迭代次数设置为非常大的数就足够了，但是当梯度向量(<code class="fe nm nn no nd b">gradients</code>)变得小于某个预设容差(通常表示为ϵ).)时，中断算法这是因为当梯度向量变小时，我们非常接近最小值。</p><p id="4a7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一个重要注意事项是，当使用梯度下降时，您应该确保所有要素都具有相似的比例，否则，算法将需要更长的时间才能收敛-Sckikit-Learn的标准缩放器通常可以做到这一点。但是，我们可以做些什么来加速我们的优化程序呢？进入随机梯度下降。</p><h2 id="6384" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">随机梯度下降</h2><p id="eb2d" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">随机GD采用与批处理GD相反的极端，而不是在每一步使用整个训练集，它在每一步采用训练集的随机实例，并使用它来计算梯度。这显然要比批量GD快得多，尤其是在使用巨大的训练集时。然而，缺点是，由于其随机行为，该算法不太规则:它只是平均减少，而不是在每一步逐渐减少。因此，我们的最终参数值将是好的，但不是最佳的(不同于批量梯度下降)，因为一旦我们接近最小值，算法将继续反弹。</p><p id="c2ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然这听起来不太理想，但它确实有帮助！例如，如果成本函数非常不规则(并且不是凸的),则随机性可以帮助跳出局部最小值。所以实际上，随机GD比普通的批量GD更有机会找到全局最小值。</p><p id="d8de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们接近最小值时，我们实际上也可以做一些事情来帮助处理缺乏收敛性:逐渐降低学习率。我们可以从(相对)大的步长开始，这有助于更快地收敛到最小值，也有助于跳出局部最小值。然后，我们可以逐渐降低学习率，这使得算法可以稳定在全局最小值。这个过程被称为<strong class="ky ir">学习计划</strong>。学习进度计划是一个非常可爱的技术，也许将来我会给它一个自己的博客。</p><p id="6acb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码实现了随机梯度下降以及一个(简单的)学习计划:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="ff1d" class="mc md iq nd b gy nh ni l nj nk">epochs = 50<br/>t0, t1 = 1, 10 # Learning schedule hyperparams<br/>n = 100</span><span id="8a6c" class="mc md iq nd b gy nl ni l nj nk">def learning_schedule(t):<br/>    return t0/(t + t1)</span><span id="af08" class="mc md iq nd b gy nl ni l nj nk">theta = np.random.randn(d,1) # random initialization, with d being <br/># the dimension of theta</span><span id="571a" class="mc md iq nd b gy nl ni l nj nk">for epoch in range(epochs):<br/>    for i in range(n):<br/>        rand_idx = np.random.randint(n)<br/>        xi = X[rand_idx:rand_idx+d]<br/>        yi = y[rand_idx:rand_idx+d]<br/>        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)<br/>        eta = learning_schedule(epoch * n + i)<br/>        theta = theta - eta * gradients </span></pre><p id="9095" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们迭代了几轮<em class="lz"> n </em>次迭代，每一轮被称为一个<em class="lz">时期</em>。</p><p id="aa92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另外值得一提的是另一种方法，<strong class="ky ir">小批量梯度下降</strong>。简单地说，它只是批处理和随机GD方法的结合:在每一步，小批处理GD计算训练实例的一个小的随机子集上的梯度(一个<em class="lz">小批处理</em>)。这具有比随机GD更少不稳定的优点；然而，它更有可能陷入局部最小值。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="49a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，我们已经讨论了更经典的GD算法，这些算法在训练更经典的ML模型(例如，支持向量机、随机森林等)时表现出令人满意的水平。但是，使用这些方法来训练大型深度自然网络可能会慢得令人麻木。更快的优化程序可以在训练中为我们提供额外的速度提升。下面将讨论一些最流行的算法。</p><h2 id="b112" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">动量优化</h2><p id="adf3" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">动量优化不是仅使用当前步骤的损失函数的梯度来引导搜索，而是使用动量的概念并累积先前的梯度来帮助确定前进的方向。回想一下我们之前的类比:想象你现在骑着一辆自行车，你正在下山，你开始会很慢，但你会逐渐开始获得越来越多的动量，直到你达到最终速度。这正是动量算法所做的。另一种思考方式是，梯度向量现在用于加速度，而不是速度。因此，我们的模型参数的更新方程由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/17bc0ed75682daa413c068b840b5ffc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*LyWMPlOunBYuYCWG70xztw.png"/></div></figure><p id="b55a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在每次迭代中，我们从动量向量<strong class="ky ir"><em class="lz"/></strong>中减去局部梯度(乘以η)，然后通过添加动量向量来更新权重<strong class="ky ir"><em class="lz"/></strong>。除了总体上收敛得更快之外，动量还有助于算法更快地脱离平稳状态，并滚过局部最小值。超参数β是系统中的摩擦量，β越高，终端速度越慢。</p><h2 id="7f82" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">内斯特罗夫加速梯度</h2><p id="7314" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">只要对动量算法稍加调整，我们几乎总能获得更快的收敛。NAG方法在局部位置<strong class="ky ir"> <em class="lz"> ω </em> </strong>稍前测量成本函数的梯度。单步更新由下式给出</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/33dee103e8e9c84e00fda5fb4a54a42c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*z6-bcRbJ0cTq7RDdXBC_Ew.png"/></div></figure><p id="7091" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这样做的原因很简单:一般来说，动量向量指向最小值，所以在那个方向上测量稍微远一点的梯度会有帮助。这些微小的速度提升累积起来，最终NAG算法的速度明显加快。在Keras中实现这一点非常简单，我们只需输入:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="900f" class="mc md iq nd b gy nh ni l nj nk">optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)</span></pre><h2 id="50a1" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">RMSProp</h2><p id="af88" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">RMSProp算法是由Geoffrey Hinton和Tijmen Tieleman在2012年提出的<strong class="ky ir"> AdaGrad </strong>算法的修改版本(从数学角度来说，这是最近的事了！).想象一个场景，其中成本函数看起来像一个细长的碗，GD算法将开始沿着最陡的斜率下降，但是这个斜率并不直接指向全局最小值。AdaGrad算法为我们提供了一个修正，因此该算法确实向全局最小值移动。这是通过沿着最陡的维度按比例缩小梯度向量来实现的。另一种思考方式是，我们在更陡峭的维度上衰减学习速率更快，在斜率更平缓的维度上衰减学习速率更慢。理解其工作原理的最简单方法是亲自动手操作单步更新方程:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/4a429a754fe94c7389f91e726c9493c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*iKx_9ueV71axYuSMAFB2Qg.png"/></div></figure><p id="a10c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，𝇇表示元素间的乘法。因此，s的每个元素由下式给出</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/9cb2b87f35973f48ba386874904def41.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*_JkLx9Sr7KKFUHfT2uYHig.png"/></div></figure><p id="9e3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以如果代价函数<em class="lz">J(</em><strong class="ky ir"><em class="lz">ω</em></strong><em class="lz">)</em>在第<em class="lz">I</em>-维中是陡峭的，那么<strong class="ky ir"> <em class="lz"> s </em> </strong>将会随着我们的迭代而累积并变得越来越大。这里的第二步实际上与通常的梯度下降更新相同，除了我们相对于<strong class="ky ir"> s </strong>缩小梯度向量。⊘代表元素式划分，因此梯度向量的每个元素越大，我们在该方向上采取的步骤就越小。<em class="lz"> ε </em>项是一个非常小的平滑项(例如<em class="lz"> ε = 1e-10 </em>)，它只是为了避免被零除(我曾经被零除，流了鼻血，这是魔鬼的工作)。</p><p id="cba7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，AdaGrad的问题是，在训练神经网络时，它通常会过早停止。这是因为学习率下降太多。RMSProp做了一个简单的调整。相反，我们指数衰减最近迭代中累积的梯度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/6b5a59c220511543cfabb49663103560.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*qRDsmxTnpLxm3fjJmx9FtA.png"/></div></figure><p id="0327" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管我们已经引入了一个新的超参数来进行调整(<em class="lz"> ρ </em>)，但是值<em class="lz"> ρ = 0.9 </em>通常可以达到这个目的。现在您可能已经猜到，在Keras中实现RMSProp非常容易:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="b99a" class="mc md iq nd b gy nh ni l nj nk">optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)</span></pre><p id="e8cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想现在是时候谈谈优化大师了。</p><h2 id="47be" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">亚当和那达慕优化</h2><p id="374c" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">自适应矩估计(Adam)结合了我们已经研究过的两种方法:动量和RMSProp优化。它从动量优化中窃取了跟踪先前梯度的指数衰减平均值的思想，并且从RMSProp中窃取了跟踪过去平方梯度的指数衰减平均值的思想。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/aac44b49bf90d0a06e69cdaf4204f088.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*3YzFL9CbLpnyRRD13WrIVw.png"/></div></figure><p id="8f82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，<em class="lz"> t </em>是迭代次数(从1开始)。第一个、第二个和第五个方程几乎与动量和RMSProp优化算法相同，不同之处在于超参数。第三个和第四个等式只是为了在算法开始时提升<strong class="ky ir"> <em class="lz"> m </em> </strong>和<strong class="ky ir"> <em class="lz"> s </em> </strong>而存在，因为最初，它们偏向0。与Keras中的所有东西一样，Adam非常容易实现(下面也给出了超参数的默认值)。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="2ae8" class="mc md iq nd b gy nh ni l nj nk">optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)</span></pre><p id="f0a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Nadam </strong>算法与adam算法相同，但它也包括内斯特罗夫技巧，这意味着它的收敛速度通常比标准Adam稍快。</p><p id="8b56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Adam(以及Nadam和RMSProp)的另一个不太明显的优点是，它需要对η进行更少的调整——因此，在某些方面，它甚至比标准的普通GD方法更容易实现！</p><h2 id="57e3" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">结论</h2><p id="7bee" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">我们从经典的梯度下降开始，逐渐变得越来越复杂。当使用计算能力不太密集的模型时，我发现通常简单的SGD/小批量模型工作得非常好。当训练大型神经网络时，像Adam/Nadam和RMSProp往往工作得很好。就像ML世界中的所有事情一样，尝试不同的方法通常会带来最好的结果。</p><p id="d5c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后一句话:已经发现<a class="ae kv" href="https://arxiv.org/abs/1705.08292" rel="noopener ugc nofollow" target="_blank">有时自适应优化方法(Adam、Nadam、RMSProp)有时会导致解决方案泛化能力差，所以如果你发现这个问题，不妨试试NAG方法！</a></p><p id="b678" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你觉得这篇文章很有趣，我强烈推荐你拿一本Aurélien Géron的《用Scikit-Learn和TensorFlow进行机器实践学习》。这绝对是一本不可思议的书，也是我第一次读到这篇文章中的很多内容的地方！</p><p id="4f24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>