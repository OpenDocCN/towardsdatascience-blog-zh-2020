<html>
<head>
<title>Introduction to Graph Representation Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形表示学习简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-graph-representation-learning-a51c963d8d11?source=collection_archive---------24-----------------------#2020-10-15">https://towardsdatascience.com/introduction-to-graph-representation-learning-a51c963d8d11?source=collection_archive---------24-----------------------#2020-10-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="57f0" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="6f81" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">图上机器学习的主要概念和挑战</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ff05c471cd3eb885620d71b2e1f3310d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1JQraNmKyPMUFTAc"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@jens_johnsson?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">延斯·约翰森</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="584d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">近年来，图形表示学习的受欢迎程度激增。只有几个活跃的研究人员的时代已经成为遥远的过去。今天，我们看到谷歌或Twitter等顶级公司在这一领域进行了大量投资。</p><p id="5f3f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">令人惊讶的是，许多可用的数据可以被建模为图表。以Reddit上的在线社交互动为例。我们可以将每个用户表示为一个连接到其他用户的节点。但是，我们如何分析这种数据，甚至应用机器学习呢？</p><p id="7aca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在图形表示学习中，我们旨在回答这些问题。在这篇文章中，我们将看看图形表示学习的主要概念和挑战。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="404e" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">图表基础</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/e7007cc8ebf617d0a8e7c7bf5e5ab819.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OU880jHuARcEBkzu.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">从图中创建邻接矩阵。图片作者。</p></figure><p id="11f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们从基础开始，好吗？</p><p id="9400" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将图定义为一组顶点，这些顶点之间有连接(边)。有许多类型的图，例如，它们可以是有向的、无向的或无环的。</p><p id="1b7f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了使图形对任何算法都有用，我们需要将它们转换成数字形式。为了实现这一点，我们可以使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Adjacency_matrix#:~:text=In%20graph%20theory%20and%20computer,with%20zeros%20on%20its%20diagonal." rel="noopener ugc nofollow" target="_blank">邻接</a>、<a class="ae lh" href="https://en.wikipedia.org/wiki/Laplacian_matrix" rel="noopener ugc nofollow" target="_blank">拉普拉斯矩阵</a>或<a class="ae lh" href="https://en.wikipedia.org/wiki/Degree_matrix" rel="noopener ugc nofollow" target="_blank">度矩阵</a>，它们提供了图形的方便的矩阵表示。每个顶点和每个连接也可以有一个定义其属性的特征向量。</p><p id="2b49" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在[1]中可以找到对图的很好的介绍。一旦你对基础知识相当有信心，学习更高级的概念是有好处的，比如<a class="ae lh" href="https://en.wikipedia.org/wiki/Graph_isomorphism#:~:text=Graph%20isomorphism%20is%20an%20equivalence,an%20isomorphism%20class%20of%20graphs." rel="noopener ugc nofollow" target="_blank">图同构</a>和<a class="ae lh" href="https://www.whitman.edu/mathematics/cgt_online/book/section05.08.html" rel="noopener ugc nofollow" target="_blank">图着色</a>。</p><h1 id="7875" class="mu mv it bd mw mx nn mz na nb no nd ne ki np kj ng kl nq km ni ko nr kp nk nl bi translated">图的特征提取技术</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/326971f0ddca2df9363a91527296b71a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/0*WTpleSCMl9ofptI-.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">节点级特征的一个例子，即中心性测量。左图说明了特征向量的中心性。右边的例子说明了程度中心性。<a class="ae lh" href="https://commons.wikimedia.org/w/index.php?curid=39064835" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></figure><p id="edcc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">图形特征提取背后的主要思想是以一种更方便的、类似向量的格式来表示关于局部和全局图形结构的信息。我们同样对提取单个顶点的属性信息以及它们之间的连通性感兴趣。</p><p id="1ca7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从图中提取的特征可以分为三种类型:节点级、图级和邻域重叠特征。</p><p id="6d0f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于节点级特征，我们的目标是为每个顶点生成一个特征向量。一些最流行的特征提取方法是节点度、中心性方法或聚类系数。为了计算这些特征，我们可以使用来自顶点的最近邻居的信息，或者来自更远的K跳邻居的信息。</p><p id="1c17" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们也可以采取更一般的方法，为<em class="nt">整个图</em>创建一个特征。它被称为图形级特征。这可以通过邻接矩阵等基本特征来实现，也可以通过Weisfeiler-Lehman或Graphlet内核等更复杂的迭代方法来实现。</p><p id="2694" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">图的最后一种特征提取是邻域重叠特征。它们是专门为提取节点之间的连接信息而设计的。它们可以进一步细分为局部和全局方法。前者描述了两个节点之间的邻域的相似性，而后者描述了某些节点是否属于图中的同一社区(即一组密集聚集的节点)。</p><p id="b226" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以在这篇文章中找到对特征提取技术<a class="ae lh" rel="noopener" target="_blank" href="/feature-extraction-for-graphs-625f4c5fb8cd">的深入解释。</a></p><h1 id="afd1" class="mu mv it bd mw mx nn mz na nb no nd ne ki np kj ng kl nq km ni ko nr kp nk nl bi translated">图上的预测任务</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/15e9cadca2ee52558c6124d03f684918.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9ACpmtPnTZRp9hqT.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">预测顶点的动物类型。图片作者。图标按<a class="ae lh" href="https://icons8.com/" rel="noopener ugc nofollow" target="_blank">图标8 </a></p></figure><p id="585b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在图上使用机器学习的主要问题是节点是<em class="nt">相互连接</em>的。这打破了独立数据点的假设，这迫使我们使用更精细的特征提取技术或新的机器学习模型来处理这个问题。</p><p id="25a8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">预测任务在图上的定义也有很大不同。我们可以将其分为4种主要类型:节点分类、链接预测、整个图的学习和社区检测。</p><p id="1e27" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">节点分类的目的是预测图中顶点的属性。链接预测也非常相似，但它预测两个顶点之间的链接(连接)的属性。这些任务被称为<em class="nt">半监督学习</em>，因为该图将同时包含训练和测试数据<em class="nt"/>。</p><p id="ad21" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">学习整个图形是最直观的方法。我们将整个图形作为输入，并基于它生成预测。它非常类似于标准的机器学习回归和分类任务。</p><p id="3023" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，对于社区检测，我们的目标是识别图中密集的顶点簇。</p><p id="afa0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你想了解更多关于图表上的预测任务，请看这里的<a class="ae lh" rel="noopener" target="_blank" href="/machine-learning-tasks-on-graphs-7bc8f175119a"/>。</p><h1 id="7218" class="mu mv it bd mw mx nn mz na nb no nd ne ki np kj ng kl nq km ni ko nr kp nk nl bi translated">图形神经网络</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1b9583cdba4a323c52b45702f7b4e350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/0*NK-YefmJdxEy2RVN.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由Duvenaud等人创建的第一个图形神经网络架构之一。它是一种<a class="ae lh" rel="noopener" target="_blank" href="/introduction-to-message-passing-neural-networks-e670dc103a87">消息传递神经网络。</a> <a class="ae lh" href="https://arxiv.org/abs/1509.09292" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></figure><p id="06fb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了重新定义图上的神经网络，我们必须提出全新的深度学习架构[2]。</p><p id="fbd6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最简单的架构是<a class="ae lh" rel="noopener" target="_blank" href="/introduction-to-message-passing-neural-networks-e670dc103a87?source=---------5----------------------------">消息传递神经网络</a>。在这里，前向过程的一个等效过程是来自顶点邻域的要素的迭代聚合。每个聚合操作都被视为一个层。在<em class="nt"> n </em>次迭代之后，我们使用一个最终的聚合操作将所有信息提取为一个向量/矩阵。</p><p id="5b38" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们也可以在图上定义图卷积神经网络。它们可以分为空间方法和光谱方法。空间方法是最直观的方法:我们定义一个核来对图形进行卷积，类似于CNN。频谱方法稍微复杂一些:我们考虑傅立叶域中的图形，并集中处理通过图形传播的信号。</p><h1 id="5001" class="mu mv it bd mw mx nn mz na nb no nd ne ki np kj ng kl nq km ni ko nr kp nk nl bi translated">生成图模型</h1><p id="b6bf" class="pw-post-body-paragraph li lj it lk b ll nw kd ln lo nx kg lq lr ny lt lu lv nz lx ly lz oa mb mc md im bi translated">这些模型的目的是创建具有所需属性的新图。这里，我们希望模型的输出是一个图[2]。但是我们为什么需要生成图表呢？</p><p id="a0ec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，我们可以使用这些图表作为机器学习模型的基准数据[2]。例如，如果我们想要检测社区，我们可以创建一个生成图模型，该模型可以创建具有社区的图。通过定义这些模型，我们还可以知道生成图的特定实例的可能性有多大。</p><p id="d95a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">鄂尔多斯-雷尼模式就是一个很好的例子。这是一个非常简单的模型，它将两个任意节点之间的连接概率指定为<em class="nt"> r. </em>通过这个参数，我们可以控制生成的图的连接密度。</p><p id="cb55" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">更复杂的模型包括随机块模型、优先附件和不同的变分自动编码器方法。在[2](第三部分——生成图模型)中可以找到对这些模型的详细解释。</p><h1 id="67ef" class="mu mv it bd mw mx nn mz na nb no nd ne ki np kj ng kl nq km ni ko nr kp nk nl bi translated">几句遗言</h1><p id="a9d3" class="pw-post-body-paragraph li lj it lk b ll nw kd ln lo nx kg lq lr ny lt lu lv nz lx ly lz oa mb mc md im bi translated">图上的机器学习并不简单。我们已经看到，大多数标准的机器学习方法都必须重新定义到图形域中。图形上的特征提取、预测任务与正常的机器学习问题非常不同。我们希望通过创建处理图表数据的新技术，我们可以充分利用图表中的信息，并创建性能更好的模型。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="f194" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">关于我</h1><p id="ac8b" class="pw-post-body-paragraph li lj it lk b ll nw kd ln lo nx kg lq lr ny lt lu lv nz lx ly lz oa mb mc md im bi translated">我是阿姆斯特丹大学的人工智能硕士学生。在我的业余时间，你可以发现我摆弄数据或者调试我的深度学习模型(我发誓这很有效！).我也喜欢徒步旅行:)</p><p id="434c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你想了解我的最新文章和其他有用的内容，以下是我的其他社交媒体资料:</p><ul class=""><li id="b186" class="ob oc it lk b ll lm lo lp lr od lv oe lz of md og oh oi oj bi translated"><a class="ae lh" href="https://www.linkedin.com/in/kacperkubara/" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="655e" class="ob oc it lk b ll ok lo ol lr om lv on lz oo md og oh oi oj bi translated"><a class="ae lh" href="https://github.com/KacperKubara" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li></ul><h1 id="99cc" class="mu mv it bd mw mx nn mz na nb no nd ne ki np kj ng kl nq km ni ko nr kp nk nl bi translated">参考</h1><p id="8a40" class="pw-post-body-paragraph li lj it lk b ll nw kd ln lo nx kg lq lr ny lt lu lv nz lx ly lz oa mb mc md im bi translated">[1] <a class="ae lh" href="https://www.maths.ed.ac.uk/~v1ranick/papers/wilsongraph.pdf" rel="noopener ugc nofollow" target="_blank">罗宾·j·威尔逊的图论</a></p><p id="5c4c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2] <a class="ae lh" href="https://www.cs.mcgill.ca/~wlh/grl_book/" rel="noopener ugc nofollow" target="_blank">林子幸·汉密尔顿的《图形表示学》一书</a></p></div></div>    
</body>
</html>