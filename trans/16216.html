<html>
<head>
<title>Introduction To Neural Networks — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络导论—第1部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-neural-networks-part-1-3bb27a8d314a?source=collection_archive---------17-----------------------#2020-11-08">https://towardsdatascience.com/introduction-to-neural-networks-part-1-3bb27a8d314a?source=collection_archive---------17-----------------------#2020-11-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ed31" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="27c5" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">理解最早的模型:感知器和Adaline</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/9a4c69266db33e881191b8ced4234521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*n-NbCUcuz-36nPwa"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@urielsc26?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乌列尔SC </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="b115" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di">W</span>AI的目的是什么？毫无疑问，这是为了满足人类使用不同的概念、模型和技术创造类似人类的机器的渴望，这些概念、模型和技术仍在不断改进，因为你越成长，你的需求就越大。1943年，沃伦·麦卡洛克和沃尔特·皮茨用电路模拟了一个简单的神经元，由此产生了模拟人类神经元或者换句话说创造一个人造神经元的想法。当然，这只是人工神经网络(ANN)历史上的一小部分。如果你想知道更多，那就看看这个。</p><h2 id="6283" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">动机</h2><p id="fcd8" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">多年来，AI了解了许多不同的模型，例如:</p><ul class=""><li id="8d10" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated">一元/多元线性回归</li><li id="a3cd" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">一类/多类的逻辑回归(分类)(一对多技术)</li></ul><p id="8d81" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所有这些提到的模型之间的共同因素是它们的线性，它们只能应用于<strong class="lh ja">可线性分离的</strong>数据，这实际上是导致创建神经网络模型的原因之一，即使最早的模型也是线性分类器(线性分隔符)。本质上，它过去是，现在仍然是一种试图模仿大脑的算法，大脑是迄今为止我们知道的最强大的学习机器。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/549b425717004ec3b66b331bd414e4a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TCSOJaAkpezApGb4w2Zrfw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="4282" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所以，在开始理论之旅之前，我们先来了解一下什么是<strong class="lh ja">前馈神经网络</strong>；</p><ul class=""><li id="9f25" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated"><strong class="lh ja">前馈神经网络:</strong>是一种神经网络，其中节点(定义如下)之间的连接不形成循环。不同于它的后代<strong class="lh ja">递归神经网络</strong> (RNN)(在以后的文章中可以看到)。本文讨论的模型属于前馈神经网络类型。</li></ul><h1 id="e760" class="nw ml iq bd mm nx ny nz mp oa ob oc ms kf od kg mv ki oe kj my kl of km nb og bi translated">感知器，根</h1><p id="a481" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">一个单层神经网络，一个线性阈值单元(<strong class="lh ja"> LTU </strong> ) <strong class="lh ja"> / </strong>门(<strong class="lh ja"> LTG </strong>)，甚至一个神经元……随便你怎么称呼。人类神经元是一种从其他细胞接收电信号作为输入并基于此输出信号的细胞。这个神经元使用数值来做完全相同的工作，这就是数学成为你最好的朋友的时候！</p><p id="6343" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你熟悉前面提到的线性分类器，那么你肯定知道什么是<strong class="lh ja">假设函数</strong>，什么是<strong class="lh ja">决策边界</strong>代表什么。如果没有，让我快速介绍一下:</p><ul class=""><li id="3152" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated"><strong class="lh ja">假设:</strong>最能描述目标的函数称为<strong class="lh ja"> <em class="oh"> Y </em> </strong>。</li><li id="25ee" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated"><strong class="lh ja">决策边界:</strong>最佳拟合线，能够根据不同的类标签分离数据。每个分类器的目标是找到决策边界。注意，在二进制分类中，<em class="oh"> i </em> t是<em class="oh">真</em>和<em class="oh">假</em>值<strong class="lh ja">即</strong> <em class="oh">之间的战斗。一</em>和<em class="oh">零</em>。</li></ul><p id="cbd0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">模型描述如下:</p><ul class=""><li id="eebf" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated"><strong class="lh ja">输入值(X): </strong>单样本<strong class="lh ja"> <em class="oh"> x </em>，</strong>特征向量(属性)<em class="oh"/><strong class="lh ja"><em class="oh">x₁…xₙ</em></strong></li><li id="59e7" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated"><strong class="lh ja">权值(W): </strong>称为<em class="oh">θ</em>(theta)偶尔，一个向量的参数用来形成不同节点之间的连接，每个权值都与一个输入<strong class="lh ja"><em class="oh"/></strong>相关联，并显示它将对输出产生多大的影响。</li><li id="8487" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated"><strong class="lh ja">偏置(b): </strong>节点<strong class="lh ja"> <em class="oh"> x₀ </em> </strong>，带有<strong class="lh ja"/><strong class="lh ja"><em class="oh">1</em></strong>的常数值。即使所有输入都为空，也能保证神经元的激活。</li><li id="5701" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated"><strong class="lh ja">净输入(net): </strong>权重和输入的点积，很多时候叫做<strong class="lh ja"> <em class="oh"> z </em> </strong>。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/73b9c60d6b0d7e912d7c7b9095d4dd03.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*wHXbLD9HT5kwZhnKgx1DuA.png"/></div></figure><ul class=""><li id="fb11" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated"><strong class="lh ja">激活函数:</strong>阈值函数<strong class="lh ja"> <em class="oh"> g(z) </em> </strong>，使感知器成为二元分类器(2类标签函数)。阈值表示为<strong class="lh ja"> <em class="oh"> θ </em> </strong>。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/dc9db7e2f71877b286749a4f64020cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*2lGgfW0lJ4qgA6ZLYjco6w.png"/></div></figure><p id="9f8d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了简化<strong class="lh ja"> <em class="oh"> g(z) </em>，</strong>我们把<strong class="lh ja"> <em class="oh"> θ </em> </strong>带到左边，认为它是与偏差相关联的<strong class="lh ja"> </strong>参数<strong class="lh ja">【w₀】</strong>，<strong class="lh ja"><em class="oh">【g(z)</em></strong>，<strong class="lh ja"> <em class="oh"> z </em> </strong>变成:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/eb953dfe0e978a035843c6ff2fec9672.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*lGIXdpce8eE4gkCtuSF2mg.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/4b1d39261843eafbcf849aee2b48ba26.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*IFQA6u6qpzZ56j1sxllOmA.png"/></div></figure><ul class=""><li id="dc58" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated"><strong class="lh ja">输出(假设):</strong>激活函数的结果，简称为<strong class="lh ja"><em class="oh">【o(x)</em></strong>这也是假设的结果<strong class="lh ja"><em class="oh">【h(x)</em></strong>。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/7b8b587f7d5b4c9d01b76f2e053bb984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*FDzNWSY0zqqX99u_29O1TA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">2D决策边界图解—作者图片</p></figure><h2 id="068f" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">感知器训练规则…谢谢罗森布拉特</h2><p id="4eb0" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">假设权重已经用小数字随机初始化，第一个样本<strong class="lh ja"> <em class="oh"> x </em> </strong>正在运行感知器模型，最终输出指示一个错误的类，现在该怎么办？这是AI吗？</p><p id="262b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">幸运的是没有，该模型将通过改进<strong class="lh ja"><em class="oh"/></strong>值在每次迭代中学习，因此它可以使用<strong class="lh ja"> Rosenblatt </strong>在1959年引入的以下训练/学习规则来更好地执行:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi on"><img src="../Images/f3371cea63700113eaae31be1d325dc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*VWsoJbOe8rebghlU7EhYFw.png"/></div></figure><ul class=""><li id="35d3" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated"><strong class="lh ja">学习率:</strong>一个<strong class="lh ja">超参数</strong> <strong class="lh ja"> </strong>，它控制每次更新模型权重时响应估计误差而改变模型的程度。传统值为<em class="oh"> 0.1 </em>或<em class="oh"> 0.01 </em>，始终在<em class="oh"> 0 </em>和<em class="oh"> 1 </em>的范围内。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/7f06dca0ad3b9599bd0708a328ae0bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*fjawqxeLk8ThPh57pVBtUw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">感知器分类器示意图——由<a class="ae le" href="http://rasbt.github.io/mlxtend/" rel="noopener ugc nofollow" target="_blank"> mlxtend </a>拍摄</p></figure><p id="7d21" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，我们已经为算法做好了准备:</p><pre class="kp kq kr ks gt op oq or os aw ot bi"><span id="f998" class="mk ml iq oq b gy ou ov l ow ox">- Random initialization of the weights, with small numbers<br/>- For each epoch:<br/>  - For each training sample xϵX<strong class="oq ja">:<br/>     .  </strong>Calculate output value o(x)<br/>     .  For i from 1..n:<br/>           w<em class="oh">ᵢ = </em>w<em class="oh">ᵢ + </em>∆w<em class="oh">ᵢ</em></span></pre><p id="2691" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这个小怪物实际上能够表示各种功能，如<strong class="lh ja"> And、Or、Nand、Nor、m-of-n</strong>…还有很多。但也有很多情况下它会失败，例如<strong class="lh ja"> XOR </strong>函数，因为这个函数不是<strong class="lh ja">线性可分的</strong>，所以模型将无法为它找到正确的决策边界，并且会无限地使用相同的权重留在循环内，除非你组合多个感知器，换句话说，形成感知器的<strong class="lh ja">网络</strong>(不能推广到所有线性不可分的数据):</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/6df1eaae31989cb58664b32dddbdb34b.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*yQV1bzqgC82m4A4nIoUx3Q.png"/></div></figure><p id="4b35" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果我们给上面显示的每个逻辑操作分配一个感知器，我们将使用三个已经训练好的模型(<strong class="lh ja">和</strong>、<strong class="lh ja">或</strong>和<strong class="lh ja">而不是</strong>)来训练一个复杂的感知器。更多详情，请查看<a class="ae le" rel="noopener" target="_blank" href="/perceptrons-logical-functions-and-the-xor-problem-37ca5025790a">本</a>出。</p><p id="1210" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">感知器模型仍然是上面提到的<strong class="lh ja">麦卡洛克-皮茨</strong>神经元的计算版本，它当然不能满足现代人工智能的要求。这就是为什么<strong class="lh ja"> Adaline </strong>出现了！！<strong class="lh ja">(不是电影)</strong></p><h1 id="f873" class="nw ml iq bd mm nx ny nz mp oa ob oc ms kf od kg mv ki oe kj my kl of km nb og bi translated">自适应线性神经元时代(Adaline)</h1><h2 id="7162" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">和感知器一样的结构，不同的引擎！</h2><p id="69f5" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">由<strong class="lh ja"> Widrow </strong>和<strong class="lh ja"> Hoff </strong>于<strong class="lh ja"> 1960年</strong>开发，被认为是许多复杂的机器学习算法的基础，如逻辑回归和支持向量机(SVMs)。还有一个二元分类器和一个线性分隔符。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/1da66b2ed91be08d652519e0812b4c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*ZdhLB5cO16NZR9G5JRhmHQ.png"/></div></figure><ul class=""><li id="47d0" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated"><strong class="lh ja">激活函数:</strong>净输入<strong class="lh ja"><em class="oh">【z】</em></strong>的线性函数(恒等函数)记为<strong class="lh ja"><em class="oh">【x】</em></strong>。</li><li id="8a8f" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">仍然需要使用阈值函数<strong class="lh ja"> <em class="oh"> g(z) </em> </strong>。</li><li id="f1d0" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">与感知器模型不同，Adaline使用连续输出<strong class="lh ja"> <em class="oh"> z </em> </strong>来学习模型权重。这比仅仅利用预测类标签要有效得多。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/a0fe2941b6666f604d723886bd958164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*tjjHKP0v3PRJmEIEXXR-BQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Adaline分类器示意图—由<a class="ae le" href="http://rasbt.github.io/mlxtend/" rel="noopener ugc nofollow" target="_blank"> mlxtend </a>成像</p></figure><p id="4446" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Adaline类似于应用<strong class="lh ja">线性回归</strong>，唯一的区别是使用阈值函数将最终输出转换为分类输出。该模型使用所谓的<strong class="lh ja">代价函数</strong> <strong class="lh ja"> <em class="oh"> J(w) </em> </strong>来估计其误差。这背后的逻辑是，最佳拟合线被定义为<strong class="lh ja">最小化误差平方和(SSE) </strong>的线。误差被认为是数据点(一个样本)与拟合线(判定边界)的垂直偏差(残差)。在这个阶段，一些细节可能看起来有点模糊，但如果你继续阅读，将会得到进一步的解释。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/bb4bbc0d0b5f2c9dfd505a7b36dd1041.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*NXUEHoeQfttDMBBVwAUS7Q.png"/></div></figure><p id="f803" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中<strong class="lh ja"> <em class="oh"> X </em> </strong>是训练样本的集合，<strong class="lh ja"> <em class="oh"> y(x) </em> </strong>一个训练样本的目标<strong class="lh ja"><em class="oh">X</em></strong><strong class="lh ja"><em class="oh">o(X)</em></strong>是连续输出(净输入)<strong class="lh ja"> <em class="oh"> z </em> </strong>。</p><ul class=""><li id="6b70" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated">成本函数是误差平方和(<strong class="lh ja">凸</strong>)除以2，以便于推导过程。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/cd2f9f138c26d04258d998096d33006a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EoB5RVlBh0auhSeQPIpeCw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">2D线性回归模型中垂直偏差的图示-图片由作者提供</p></figure><h2 id="079b" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">优化…我们来了</h2><p id="b184" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">我们讨论了成本函数，并且碰巧提到了术语<strong class="lh ja">最小化</strong>…因为误差函数存在的主要原因是<strong class="lh ja">最小化</strong>误差，因此得名！</p><p id="2cde" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们认为有两种主要方法可以做到这一点:</p><ul class=""><li id="5dfb" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated">正规方程(封闭解)</li><li id="0a08" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">标准/随机梯度下降</li></ul><p id="45a4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本文中，我们将涵盖标准和随机梯度下降技术。该过程是直接的，我们在每次迭代中计算成本函数的值，通过在特定点<strong class="lh ja"> ( <em class="oh"> w，J(w) </em> ) </strong>寻找正切值(<strong class="lh ja">偏导数</strong>)向函数的全局最小值前进一步，并使用<strong class="lh ja">Delta规则</strong>更新权重。当达到全局最小值(或局部最小值)时，循环停止(<strong class="lh ja">零导数</strong>)。</p><p id="8649" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">嗯..德尔塔法则…这是怎么回事？</strong></p><p id="d99e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">还记得在<strong class="lh ja">感知器</strong>中，权重是如何根据<strong class="lh ja"> Rosenblatt </strong>的学习规则更新的吗，与<strong class="lh ja"> Adaline </strong>中的前提相同，但公式不同。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/4ee14b8b0df863e56dd984b84613a74e.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*GjRlba4Y8l_4b5_b_716tQ.png"/></div></figure><ul class=""><li id="2021" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated">减号表示我们正朝着成本梯度的相反方向前进一步，如图所示。</li></ul><h2 id="f28c" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">梯度下降</h2><p id="0379" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">现在我们已经到了推导部分，如果你只对结果表达式感兴趣，你可以跳过它。否则，推导过程如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pe"><img src="../Images/28fca2542f6963c059aeddb822a06ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*NtBGe88TCkKqvvVOCtquSw.png"/></div></div></figure><p id="d5f0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">成本函数的偏导数<strong class="lh ja"> <em class="oh"> J(w) </em> </strong> w.r.t各权重<strong class="lh ja"> <em class="oh"> wᵢ (i ϵ 1..n) </em> </strong>是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/5b193ee38e9d0006f6c0c05d7e9de97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*4jARn_vYG86rWp-pixW6og.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pg"><img src="../Images/452441d8508bb0f342721142945c5e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JVb4jVlAmgWKZYoCbKbdFg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">梯度下降—由<a class="ae le" href="http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/" rel="noopener ugc nofollow" target="_blank"> mlxtend </a>成像</p></figure><p id="cff0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">标准梯度下降(物品)</strong></p><p id="d788" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">称为<strong class="lh ja"> Batch-GD </strong>模式，用于在处理整个数据集后更新权重，因此步长较大。然而，它不能保证找到全局最小值，并且如果代价函数是非凸的，它可能容易收敛到局部最小值。</p><pre class="kp kq kr ks gt op oq or os aw ot bi"><span id="132c" class="mk ml iq oq b gy ou ov l ow ox">- Random initialization of the weights, with small numbers<br/>- For each epoch :<strong class="oq ja"><br/>  .  </strong>Calculate output value o(x)for each training sample xϵX<br/>  .  Calculate cost gradient ∇J<br/>  .  For i from 1..n :<br/>        w<em class="oh">ᵢ = </em>w<em class="oh">ᵢ + </em>∆w<em class="oh">ᵢ</em></span></pre><p id="05db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">随机梯度下降</strong></p><p id="e644" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">也被称为<strong class="lh ja">迭代/在线模式</strong>，用于计算成本梯度并在每个训练样本之后更新权重，这使得即使成本函数是非凸的，也更有可能找到全局最小值。因此，收敛需要更多的时间(达到最小成本)。</p><pre class="kp kq kr ks gt op oq or os aw ot bi"><span id="3ccf" class="mk ml iq oq b gy ou ov l ow ox">- Random initialization of the weights, with small numbers<br/>- For each epoch :<br/>  - For each batch training sample xϵX<strong class="oq ja"> :<br/>     .  </strong>Calculate the output value o(x)<br/>     .  Calculate cost gradient ∇J<br/>     .  For i from 1..n :<br/>           w<em class="oh">ᵢ = </em>w<em class="oh">ᵢ + </em>∆w<em class="oh">ᵢ</em></span></pre><p id="1c00" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">让GD算法收敛更快的方法</strong></p><ul class=""><li id="2ad7" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated">在大规模机器学习系统中，通常使用<strong class="lh ja">小批量</strong>，这是<strong class="lh ja"> GD </strong>和<strong class="lh ja"> SGD之间的折衷。</strong>比<strong class="lh ja"> SGD </strong>产生更平滑的收敛。</li><li id="d4cf" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">当特征相差<strong class="lh ja">数量级</strong>时，<strong class="lh ja">特征缩放</strong>提供数值稳定性，使得<strong class="lh ja"> </strong>更有可能使GD收敛得更快。存在各种方法，例如<strong class="lh ja">最小-最大归一化</strong>和<strong class="lh ja">标准化</strong>。</li></ul></div><div class="ab cl ph pi hu pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="ij ik il im in"><h2 id="b7f1" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">检查学习率值</h2><p id="2b7b" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">本节展示了绘制学习曲线对于梯度下降算法的重要性。有时，当GD没有很好地收敛或者根本没有收敛时，问题可能出在你选择的学习率值上。</p><ul class=""><li id="9416" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated"><strong class="lh ja">太大:</strong>算法可能超过最小值并发散，如左图所示。</li><li id="adc8" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated"><strong class="lh ja">太小:</strong>可能需要太多的历元来收敛(小步)，并且可能导致局部最小值而不是全局最小值。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi po"><img src="../Images/712a266202127347430d613ac2eb31fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZVQV-NoUyLnfMq5adW0sw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">学习率较大和较小的GD行为</p></figure><h2 id="d368" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">决策边界方程</h2><p id="1deb" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">当您有一个二维输入向量时，分隔线方程的类型为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/42eadf32931f6edd71249ce5cb20cceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*XEiQhismzTkspkHMddhQAw.png"/></div></figure><p id="89e0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中<strong class="lh ja"> <em class="oh"> a </em> </strong>称为<strong class="lh ja">斜率</strong>和<strong class="lh ja"><em class="oh">b</em></strong>y轴截距<strong class="lh ja"/>。</p><p id="5fdc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">先将净输入<strong class="lh ja"> <em class="oh"> z </em> </strong>设置为<strong class="lh ja"><em class="oh">0；</em> </strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/0bc7a81c6438179d3c4563d420217e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*eyqq3ihPpBC8ehePuCtqUQ.png"/></div></figure><p id="e21c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果把<strong class="lh ja"><em class="oh">【x₁】</em></strong>、<strong class="lh ja"> <em class="oh"> x₂、</em> </strong>和<strong class="lh ja"> <em class="oh"> w₀ </em> </strong>分别换成<strong class="lh ja"> <em class="oh">、</em> </strong> <strong class="lh ja"> <em class="oh"> y、a、b、</em> </strong>和<strong class="lh ja"> <em class="oh"> C </em> </strong>，看起来就像是一条线的标准方程。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pr"><img src="../Images/d8ca53f972bb169d5b1634704dbfb9a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*_YRwjDkwkPkE4r2Bgi5P5Q.png"/></div></div></figure><p id="8779" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下一步是寻找<strong class="lh ja"> x轴截距</strong>和<strong class="lh ja"> y轴截距</strong>，这样你就得到直线上的两点，这样你就可以最终找到斜率，即<strong class="lh ja"><em class="oh"/></strong>。</p></div><div class="ab cl ph pi hu pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="ij ik il im in"><h2 id="447b" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">摘要</h2><p id="8adc" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">神经网络的两个初始模型是:</p><ul class=""><li id="12c7" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated">感知器</li><li id="1258" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">自适应线性神经元</li></ul><p id="19fe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">相似之处:</p><ul class=""><li id="8fa7" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated">线性分类器</li><li id="818b" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">二元分类器</li><li id="3465" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">阈值函数的使用</li></ul><p id="a3e2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">差异:</p><ul class=""><li id="3489" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated">感知器模型使用其预测的类别标签(分类输出)和<strong class="lh ja"> <em class="oh">感知器学习规则</em> </strong>来学习其系数(权重)。</li><li id="1973" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">另一方面，Adaline使用其连续结果和<strong class="lh ja"> <em class="oh"> delta规则</em> </strong>以获得更高的准确性。</li></ul><p id="fcaa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果GD算法在几个纪元后仍然停滞不前，那么您可能需要考虑调整您的学习率值以获得更好的性能。</p></div><div class="ab cl ph pi hu pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="ij ik il im in"><p id="2aa1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[1]:阿卜杜勒·拉森。(2020年7月)。https://morioh.com/p/1d9e1c91f4a3的FNN和CNN使用Pytorch <a class="ae le" href="https://morioh.com/p/1d9e1c91f4a3" rel="noopener ugc nofollow" target="_blank"/></p><p id="a99f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2]:帕布。(2018年7月3日)。了解超参数及其优化技术<a class="ae le" rel="noopener" target="_blank" href="/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568#:~:text=In%20statistics%2C%20hyperparameter%20is%20a,Model%20parameters%20vs%20Hyperparameters">https://towards data science . com/understanding-hyperparameters-and-its-optimization-techniques-f 0 debba 07568 #:~:text = In % 20 statistics % 2C % 20 hyperparameter % 20 is % 20a，Model % 20 parameters % 20 vs % 20 hyperparameters</a></p><p id="7724" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3]:杰森·布朗利。(2020年9月12日)。了解学习率对神经网络性能的影响<a class="ae le" href="https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine Learning mastery . com/understand-the-dynamics-of-Learning-Rate-on-deep-Learning-Neural-networks/</a></p></div></div>    
</body>
</html>