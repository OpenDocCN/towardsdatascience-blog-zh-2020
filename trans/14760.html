<html>
<head>
<title>What Makes LightGBM Light</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">是什么让光变轻</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-makes-lightgbm-light-8eb4dc258046?source=collection_archive---------36-----------------------#2020-10-11">https://towardsdatascience.com/what-makes-lightgbm-light-8eb4dc258046?source=collection_archive---------36-----------------------#2020-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e6fb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">速度很重要。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/24672830e55e3633c3c302deab689fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4c6E-Z61-LmxfC-kNQ7Mpw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@shiroscope?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Shiro hatori </a>在<a class="ae ky" href="https://unsplash.com/s/photos/fast?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="b387" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度增强决策树(GBDT)是一种性能非常好的算法，它已经成为许多先进算法的基础，如XGBoost、LightGBM和CatBoost。</p><p id="d03a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将关注是什么让LightGBM变得又轻又快。LightGBM是由微软的研究人员创建的，旨在建立一个比其他正在使用的GDBT更有效的实现。</p><p id="38b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们先简要讨论一下GBDT算法是如何工作的。我们将关注LightGBM的特别之处。</p><p id="3bca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度提升意味着以这样一种方式顺序地组合弱学习器，即每个新学习器都符合来自前一步骤的残差。因此，每个新的学习者都会改进整个模型。最终的模型汇总了每一步的结果，从而形成了一个强学习者。在GBDT的例子中，弱学习者是决策树。</p><p id="62d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GBDT的弱学习器决策树通过基于特征值分割观察值(即数据实例)来学习。该算法寻找将导致最高信息增益的最佳分割。</p><p id="0df3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实证明，寻找最佳分裂是决策树学习过程中最耗时的部分。GBDT的先前实现使用预先排序或基于直方图的算法来寻找最佳分割。</p><ul class=""><li id="65d9" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">预先排序:特征值预先排序，并评估所有可能的分割点。</li><li id="547d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">基于直方图:连续特征被分成离散的箱，并创建特征直方图。</li></ul><p id="42ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于直方图的算法比预先排序的算法更有效。随着数据集在观测值和要素方面的增大，这两种方法的速度都会变慢。</p><p id="4c4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LightGBM从基于直方图的算法开始，因为它是更有效的算法。</p><p id="bd41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于直方图的算法的问题是扫描所有数据实例以找到关于信息增益的最佳分割。对每个特征都这样做。因此，基于直方图的算法的复杂性取决于数据实例和特征的数量。</p><p id="0a96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了解决这个问题，LightGBM使用了两种技术:</p><ul class=""><li id="9b98" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">梯度单侧采样</li><li id="e8ae" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">EFB(独家功能捆绑)</li></ul><p id="d6ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面我们来详细介绍一下这些技术的作用，以及它们是如何让LightGBM变得“轻”的。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="569e" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">梯度单侧采样</h1><p id="fe7d" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">扫描所有数据实例以找到最佳分割是一种蛮力，这肯定不是最佳的。我们需要找到一种方法，以某种方式根据信息增益对数据实例进行采样。</p><p id="c4f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种方法是根据权重对数据进行采样。但是，它不适用于GBDT，因为在GBDT没有样品重量。</p><p id="29f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GOSS采用的解决方案是使用梯度对数据进行采样。梯度告诉我们:</p><ul class=""><li id="f5c8" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">小梯度:该算法已经在这种情况下进行了训练，与此相关的误差很小。</li><li id="9feb" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">大梯度:与该实例相关的误差很大，因此它将提供更多的信息增益。</li></ul><p id="887c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度小的数据实例提供不了多少东西。因此，我们可以排除梯度小的实例，只关注梯度大的实例。但是，在这种情况下，数据分布将会改变。我们不希望这样，因为这将对学习模型的准确性产生负面影响。</p><p id="26fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GOSS提供了一种基于梯度的数据采样方法，同时考虑了数据分布。</p><p id="40ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选择具有较大梯度的数据实例。从具有较小梯度的剩余数据实例中，仅选择随机样本。小梯度的随机样本乘以一个常数以保持数据分布。</p><p id="6c8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，只对数据集的一部分进行了采样。这就是该算法被称为“单侧采样”的原因。</p><p id="d2d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GOSS最终实现的是，模型的重点倾向于导致更多损失(即训练不足)的数据实例，而不会对数据分布产生太大影响。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="31c6" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">EFB(独家功能捆绑)</h1><p id="94f2" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">简而言之，EFB以一种新特征携带组合特征的信息的方式组合稀疏特征。</p><p id="a334" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有大量要素的数据集可能具有高稀疏性(即大量零值)。稀疏特征通常是互斥的，这意味着它们不会同时具有非零值。</p><p id="cbbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在典型的稀疏特征空间中，一行可能仅在一列中具有非零值(例如，编码文本数据)。</p><p id="dab3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EFB是一种使用贪婪算法将这些互斥特征组合(或捆绑)成单个特征(例如，互斥特征束)并因此降低维度的技术。</p><p id="32f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EFB减少了GDBT的训练时间，而不太影响准确性，因为创建特征直方图的复杂性现在与束的数量而不是特征的数量成比例(束的数量远小于特征的数量)。</p><p id="bb1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EFB面临的挑战之一是找到最佳捆绑包。微软的研究人员设计了一种算法，将捆绑问题转化为图形着色问题。</p><p id="85f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在图着色问题中，将特征作为顶点，在不互斥的特征之间添加边。然后，使用贪婪算法来产生束。</p><p id="b03a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更进一步，该算法还允许捆绑很少同时具有非零值(即几乎互斥)的特征。这意味着牺牲少量信息来加快训练速度。</p><p id="30f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">捆绑的特性需要以智能的方式创建。考虑一组3个特性。捆绑特性的值应该能够为我们提供前3个特性的值。LightGBM利用基于直方图的算法创建的离散箱。</p><p id="a817" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">包中特性的唯一值放在不同的容器中。这是通过向原始特征值添加偏移来实现的。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="f878" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">戈斯和EFB都让LightGBM快速运行，同时保持相当高的精确度。在一个典型的现实生活中，我们可能有大型数据集，所以效率和准确性一样重要。</p><p id="e9eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>