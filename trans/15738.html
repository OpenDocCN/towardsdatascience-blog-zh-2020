<html>
<head>
<title>GPT-3 vs PET: Not Big but Beautiful</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPT-3 vs宠物:不大但很美</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gpt-3-vs-pet-not-big-but-beautiful-7a73d17af981?source=collection_archive---------31-----------------------#2020-10-29">https://towardsdatascience.com/gpt-3-vs-pet-not-big-but-beautiful-7a73d17af981?source=collection_archive---------31-----------------------#2020-10-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/af6709a78ff003ae7545d94ef3ae02e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cOyBQilEGDNpeZvCcMn5pw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">简约之美(作者图片)</p></figure><div class=""/><div class=""><h2 id="09e3" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">模式开发训练导论</h2></div><h1 id="736e" class="kx ky ji bd kz la lb lc ld le lf lg lh ko li kp lj kr lk ks ll ku lm kv ln lo bi translated">庞大语言模型的压倒性世界</h1><p id="c03c" class="pw-post-body-paragraph lp lq ji lr b ls lt kj lu lv lw km lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">自从迁移学习在自然语言处理中出现以来，为了使越来越复杂的语言任务成为可能，已经出现了越来越大的模型。</p><p id="a9a9" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">但是模型越复杂，它需要训练的时间和数据就越多。最新的GPT-3模型在大多数自然语言任务中取得了最先进的结果，但它有近1750亿个参数要训练，并且需要<strong class="lr jj">年</strong>来训练！</p><h2 id="e109" class="mq ky ji bd kz mr ms dn ld mt mu dp lh ly mv mw lj mc mx my ll mg mz na ln nb bi translated">那么有没有解决的办法呢？</h2><p id="23a8" class="pw-post-body-paragraph lp lq ji lr b ls lt kj lu lv lw km lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">Timo Schick和Hinrich Schutze提出了一种集合掩蔽语言模型训练<a class="ae nc" href="https://arxiv.org/pdf/2009.07118.pdf" rel="noopener ugc nofollow" target="_blank">方法</a>，这种方法已经被证明与Open AI革命性的GPT-3模型一样有效，但只需要GPT-3所需参数的0.1%！很精致，不是吗？</p><p id="89f9" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">但是他们是如何实现这一惊人壮举的呢？<br/>他们使用一种叫做模式开发训练的方法，将大多数NLP任务建模为一组固定的问题模式，并在这些模式上训练多个掩蔽语言模型。这使得他们能够使用更小、相对更弱的语言模型来建立一个集合，这是一个非常强的语言模型。</p><h1 id="62d2" class="kx ky ji bd kz la lb lc ld le lf lg lh ko li kp lj kr lk ks ll ku lm kv ln lo bi translated">介绍模式开发</h1><p id="ba4a" class="pw-post-body-paragraph lp lq ji lr b ls lt kj lu lv lw km lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">如果你曾经有过语言包容性考试的经历(雅思、托福、GRE或GMAT，或任何有语言部分的考试)，你一定遇到过这样的问题:句子中有空格，给出了四种可能的单词选择，从中可以填充空格。这种测试被称为完形填空。</p><p id="e9f7" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">Schick和Schutze以完形填空的形式模拟NLP任务，允许一组屏蔽语言模型预测能够有意义地填补空白的前<strong class="lr jj"> k </strong>个可能的单词/标记。</p><p id="55f9" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">原始的基于PET的模型只能预测单个单词(每个输入记录只能预测一个单词)。然而，作者通过修改PET方法来预测多个表征，克服了这一限制。</p><figure class="nd ne nf ng gt iv"><div class="bz fp l di"><div class="nh ni l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">原文:<a class="ae nc" href="https://arxiv.org/pdf/2009.07118.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2009.07118.pdf</a></p></figure><h2 id="45e3" class="mq ky ji bd kz mr ms dn ld mt mu dp lh ly mv mw lj mc mx my ll mg mz na ln nb bi translated">成对动词</h2><p id="9460" class="pw-post-body-paragraph lp lq ji lr b ls lt kj lu lv lw km lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">对于像GPT-3这样的庞大模型，该模型通过给出32个任务的例子来微调特定的任务。这个过程被称为“引发”。</p><p id="61b6" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">在PET中，选择了另一种微调方法:创建模式描述符对。</p><p id="6c16" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">我将使用论文中对核心思想的描述，并解释其工作原理:</p><p id="efc6" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">“设M是一个屏蔽语言模型(MLM)，T是它的词汇，而∈ T是屏蔽令牌。”<br/> 文中举了一个单MLM的例子。这个想法可以扩展到用户希望在他们的系综中使用的任何数量的MLM。与所讨论的任务相关联的词汇被认为是T(这是您的输入/训练文本数据中的唯一单词集)。被屏蔽的令牌由-。由于宠物已经被修改为包含不止一个面具，该模型被认为根据用户的要求有<strong class="lr jj"> k </strong>个面具。</p><p id="21ac" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated"><strong class="lr jj">对于一些至少包含k个掩码的z ∈ T_*和令牌t ∈ T，我们用q_k_M(t | z)表示M在z中第k个掩码位置赋给T的概率；</strong> <strong class="lr jj">应用softmax前的模型逻辑用s_k_M(t | z)表示。z表示完形填空格式的输入记录，它至少由k个掩码组成。对于任务词汇表中的每个标记t，屏蔽语言模型M分配给输入记录z中第k个屏蔽位置的标记的概率由q_k_M(t|z)给出。M对应的logit(指数函数)由s_k_M(t|z)给出。</strong></p><p id="421b" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">一组输入(在图中用x表示；输入集合由X表示)需要被映射到相应的输出(在图示中由y表示；输出组用Y表示)。为了完成这项任务，需要一组模式描述器。每个模式描述符对由以下部分组成:<br/>模式<strong class="lr jj"> P </strong>:将X中的输入映射到T_*中包含单个掩码的完形填空问题<br/>描述符<strong class="lr jj"> v </strong>:将Y中的每个输出映射到T中的单个标记。这些标记中的每一个都代表它们在模式<strong class="lr jj"> P </strong>中的特定于任务的含义。</p><p id="7f2b" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">这些含义将描述者与模式联系起来。</p><p id="0790" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated"><strong class="lr jj">PET的核心思想是从v(y)在P(x)中的屏蔽位置成为“正确”令牌的概率，推导出y成为x的正确输出的概率。<br/> </strong>现在我们正在使用模式(代表输入)和动词(代表输出)，我们需要使用某个动词输出对于给定模式是正确的概率，来获得给定输入的正确输出(条件概率模型)。</p><p id="dad6" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">为了完成一个单一的任务，对一个单一的屏蔽语言模型进行PET就是这样做的。</p><p id="9407" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">用有限的数据集(32个例子)很难对上述模型进行微调。因此，不是使用单个语言模型，而是在一个未标记的数据集上使用一个<strong class="lr jj"> n </strong>语言模型的集合(软标记取决于输出的概率分布)。常规监督分类器使用这种软标记数据来微调PET模型。</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/b058f80cb67e95574ea62295d5dd750c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ps3xPUyx7oc9GVgo"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自原纸:<a class="ae nc" href="https://arxiv.org/pdf/2009.07118.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2009.07118.pdf</a></p></figure><h2 id="1e06" class="mq ky ji bd kz mr ms dn ld mt mu dp lh ly mv mw lj mc mx my ll mg mz na ln nb bi translated"><strong class="ak">插图:</strong></h2><p id="3d80" class="pw-post-body-paragraph lp lq ji lr b ls lt kj lu lv lw km lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><strong class="lr jj">应用PVP p = (P，v)识别文本蕴涵:</strong></p><p id="c442" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">一个输入x = (x1，x2)转换成完形填空题P(x)；这里x是一个问题和一个答案，分解成一个问题x1，一个面具，一个答案x2。动词化器输出需要预测屏蔽位置v(y)中的标记。基于预测的令牌，将推断出输出(y)。如图所示，y有两个选择:包含和不包含(“not _ impertisement”)。每个y的q_p(y | x)是从v(y)成为屏蔽位置的合理选择的概率中导出的。具有最高概率的y值将被加到输入x上</p><p id="457e" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">这就是PET的工作原理。</p><h1 id="ec65" class="kx ky ji bd kz la lb lc ld le lf lg lh ko li kp lj kr lk ks ll ku lm kv ln lo bi translated">与GPT 3号的性能比较</h1><p id="d279" class="pw-post-body-paragraph lp lq ji lr b ls lt kj lu lv lw km lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">作者(Schick和Schutze)使用强力胶作为基准，比较了PET和GPT-3的性能。选择了一系列语言任务:BoolQ，CB，COPA，RTE，WiC，WSC，MultiRC和ReCoRD。由于GPT-3已经在一个巨大的数据集上进行训练，为了消除它对PET的优势(创造一个公平的竞争环境)，他们创建了一个新的32个例子的训练数据集。</p><p id="e8cb" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">此外，他们创建了FewGLUE数据集，它基本上是每个任务的20，000个未标记示例的集合。这个数据集用于训练模型。</p><p id="bd52" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">下表摘自论文，定量比较了GPT-3和PET:</p><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/50fa1d4983f30610e5237263355dfe3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mgKh4EaOUY7YSs6U"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片形式原文:<a class="ae nc" href="https://arxiv.org/pdf/2009.07118.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2009.07118.pdf</a></p></figure><p id="a191" class="pw-post-body-paragraph lp lq ji lr b ls ml kj lu lv mm km lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">这篇论文本身相当全面，讨论了如何端到端地应用PET的思想，还描述了PET的迭代风格及其优点。关于如何定制模型的更多实验和细节，请参考参考资料中提到的论文和git库。</p><h1 id="bba5" class="kx ky ji bd kz la lb lc ld le lf lg lh ko li kp lj kr lk ks ll ku lm kv ln lo bi translated">结论</h1><p id="13dc" class="pw-post-body-paragraph lp lq ji lr b ls lt kj lu lv lw km lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">这项研究已经证明，语言模型不一定是人类语言才有效。探索句型可以在语言任务中带来同样有效的表现，而不需要那么多的参数。</p><h1 id="d54b" class="kx ky ji bd kz la lb lc ld le lf lg lh ko li kp lj kr lk ks ll ku lm kv ln lo bi translated"><strong class="ak">参考</strong></h1><p id="6035" class="pw-post-body-paragraph lp lq ji lr b ls lt kj lu lv lw km lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><a class="ae nc" href="https://arxiv.org/pdf/2009.07118.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2009.07118.pdf</a>T12<a class="ae nc" href="https://github.com/timoschick/pet" rel="noopener ugc nofollow" target="_blank">https://github.com/timoschick/pet</a></p></div></div>    
</body>
</html>