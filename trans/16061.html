<html>
<head>
<title>MultiClass Classification Using K-Nearest Neighbours</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用K近邻的多类分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multiclass-classification-using-k-nearest-neighbours-ca5281a9ef76?source=collection_archive---------1-----------------------#2020-11-05">https://towardsdatascience.com/multiclass-classification-using-k-nearest-neighbours-ca5281a9ef76?source=collection_archive---------1-----------------------#2020-11-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cf94" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在本文中，了解什么是多类分类以及它是如何工作的</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5eb1b6e0f6b85bc8ccf1270154e64800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uC2VnZq6ivoScNFH"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马库斯·斯皮斯克在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="257f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">T5】简介:T7】</strong></p><p id="7d68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分类是一个经典的机器学习应用。分类基本上将你的产出分为两类，也就是说，你的产出可以是两种情况之一。例如，一家银行想知道客户是否能够支付他/她的每月投资？我们可以使用机器学习算法来确定这个问题的输出，这个输出要么是，要么不是(两个类)。但是，如果你想对有两个以上类别的东西进行分类，并且不是一个简单的是/否问题，该怎么办呢？</p><p id="e4cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是多类分类的用武之地。多类分类可以定义为将实例分类为三个或更多类中的一个。在本文中，我们将使用K个最近邻进行多类分类。KNN是一个超级简单的算法，它假设相似的事物彼此非常接近。因此，如果一个数据点靠近另一个数据点，它假设它们都属于相似的类。要更深入地了解KNN算法，我建议你去看看这篇文章:</p><div class="lt lu gp gr lv lw"><a rel="noopener follow" target="_blank" href="/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761"><div class="lx ab fo"><div class="ly ab lz cl cj ma"><h2 class="bd ir gy z fp mb fr fs mc fu fw ip bi translated">基于K-最近邻算法的机器学习基础</h2><div class="md l"><h3 class="bd b gy z fp mb fr fs mc fu fw dk translated">k-最近邻(KNN)算法是一个简单，易于实现的监督机器学习算法，可以…</h3></div><div class="me l"><p class="bd b dl z fp mb fr fs mc fu fw dk translated">towardsdatascience.com</p></div></div><div class="mf l"><div class="mg l mh mi mj mf mk kp lw"/></div></div></a></div><p id="370a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们已经完成了所有的基础工作，让我们开始一些实现工作。我们将使用多个python库，如pandas(读取我们的数据集)、Sklearn(训练我们的数据集并实现我们的模型)和Seaborn和Matplotlib(可视化我们的数据)。如果您还没有安装这个库，您可以在您的PC/笔记本电脑上使用pip或Anaconda安装它们。或者另一种方式，我个人建议，使用google colab在线执行实验，并预装所有的库。我们将使用的数据集称为鸢尾花数据集，它基本上有4个特征，共150个数据点，分为3个不同的物种，即每个物种50朵花。该数据集可从以下链接下载:</p><div class="lt lu gp gr lv lw"><a href="https://www.kaggle.com/arshid/iris-flower-dataset" rel="noopener  ugc nofollow" target="_blank"><div class="lx ab fo"><div class="ly ab lz cl cj ma"><h2 class="bd ir gy z fp mb fr fs mc fu fw ip bi translated">鸢尾花数据集</h2><div class="md l"><h3 class="bd b gy z fp mb fr fs mc fu fw dk translated">用于多类分类的鸢尾花数据集。</h3></div><div class="me l"><p class="bd b dl z fp mb fr fs mc fu fw dk translated">www.kaggle.com</p></div></div><div class="mf l"><div class="ml l mh mi mj mf mk kp lw"/></div></div></a></div><p id="b704" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，当我们开始编写代码时，要做的第一步是导入代码中的所有库。</p><pre class="kg kh ki kj gt mm mn mo mp aw mq bi"><span id="aac7" class="mr ms iq mn b gy mt mu l mv mw"><strong class="mn ir">from</strong> sklearn <strong class="mn ir">import</strong> preprocessing</span><span id="434b" class="mr ms iq mn b gy mx mu l mv mw"><strong class="mn ir">from</strong> sklearn.model_selection <strong class="mn ir">import</strong> train_test_split</span><span id="88a7" class="mr ms iq mn b gy mx mu l mv mw"><strong class="mn ir">from</strong> sklearn.neighbors <strong class="mn ir">import</strong> KNeighborsClassifier</span><span id="6a7f" class="mr ms iq mn b gy mx mu l mv mw"><strong class="mn ir">import</strong> matplotlib.pyplot <strong class="mn ir">as</strong> plt</span><span id="e5bd" class="mr ms iq mn b gy mx mu l mv mw"><strong class="mn ir">import</strong> seaborn <strong class="mn ir">as</strong> sns</span><span id="b739" class="mr ms iq mn b gy mx mu l mv mw"><strong class="mn ir">import</strong> pandas <strong class="mn ir">as</strong> pd</span></pre><p id="06e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦导入了库，下一步就是读取数据。我们将使用熊猫图书馆来实现这一功能。在阅读时，我们还会检查数据中是否有空值以及不同物种的数量。(应该是3，因为我们的数据集有3个物种)。我们还将为所有三个物种类别分配一个特定的数字，0、1和2。</p><pre class="kg kh ki kj gt mm mn mo mp aw mq bi"><span id="0a95" class="mr ms iq mn b gy mt mu l mv mw">df = pd.read_csv(‘IRIS.csv’)<br/>df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/dd1b8441c143dedf45f08e83815d22dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PcZJqsg7ZeLLy8cxf0gR-Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">按作者分类的图像(原始数据集的前5列)</p></figure><pre class="kg kh ki kj gt mm mn mo mp aw mq bi"><span id="7e51" class="mr ms iq mn b gy mt mu l mv mw">df[‘species’].unique()</span></pre><p id="f449" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">输出</em>:数组(['Iris-setosa '，' Iris-versicolor '，' Iris-virginica']，dtype=object)</p><pre class="kg kh ki kj gt mm mn mo mp aw mq bi"><span id="e13e" class="mr ms iq mn b gy mt mu l mv mw">df.isnull().values.any()</span></pre><p id="3d70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">输出</em>:假</p><pre class="kg kh ki kj gt mm mn mo mp aw mq bi"><span id="68c3" class="mr ms iq mn b gy mt mu l mv mw">df[‘species’] = df[‘species’].map({‘Iris-setosa’ :0, ‘Iris-versicolor’ :1, ‘Iris-virginica’ :2}).astype(int) #mapping numbers</span><span id="e7ee" class="mr ms iq mn b gy mx mu l mv mw">df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/560a447adae2571f234d2047d4da6161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xnXx--Sb_DA1NF_21zqxhQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">按作者分类的图像(带有输出映射数字的新表格)</p></figure><p id="02d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们完成了库和CSV文件的导入，下一步就是探索性数据分析(EDA)。EDA对于任何问题都是必要的，因为它帮助我们可视化数据，并通过查看数据而不是执行任何算法来推断一些结论。我们使用库seaborn执行所有特征之间的关联，并使用相同的库绘制所有数据集的散点图。</p><pre class="kg kh ki kj gt mm mn mo mp aw mq bi"><span id="c4d2" class="mr ms iq mn b gy mt mu l mv mw">plt.close();</span><span id="79cb" class="mr ms iq mn b gy mx mu l mv mw">sns.set_style(“whitegrid”);</span><span id="a5c4" class="mr ms iq mn b gy mx mu l mv mw">sns.pairplot(df, hue=”species”, height=3);</span><span id="87c2" class="mr ms iq mn b gy mx mu l mv mw">plt.show()</span></pre><p id="9abe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">输出:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/e51fdcf60435238be55ce0f49e46098d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QxfIyMHlTGEZntXKFPQ80Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">按作者分类的图像(所有4个特征之间的相关性)</p></figure><pre class="kg kh ki kj gt mm mn mo mp aw mq bi"><span id="142a" class="mr ms iq mn b gy mt mu l mv mw">sns.set_style(“whitegrid”);</span><span id="77c7" class="mr ms iq mn b gy mx mu l mv mw">sns.FacetGrid(df, hue=’species’, size=5) \</span><span id="ce49" class="mr ms iq mn b gy mx mu l mv mw">.map(plt.scatter, “sepal_length”, “sepal_width”) \</span><span id="83ec" class="mr ms iq mn b gy mx mu l mv mw">.add_legend();</span><span id="9e22" class="mr ms iq mn b gy mx mu l mv mw">plt.show()</span></pre><p id="affc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">输出:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/cd8454b89d676ad93dffef43da090e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UbQS7UwPjQdf0kUDw5iejA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="abfd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">EDA的推论:</p><ol class=""><li id="0515" class="nb nc iq ky b kz la lc ld lf nd lj ne ln nf lr ng nh ni nj bi translated">虽然Setosa很容易识别，但Virnica和Versicolor有一些重叠。</li><li id="138b" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">长度和宽度是识别各种花型最重要的特征。</li></ol><p id="31ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在EDA之后，在数据集上训练我们的模型之前，剩下要做的最后一件事是归一化。标准化基本上是将不同特征的所有值放在同一尺度上。由于不同的特征具有不同的尺度，归一化有助于我们和模型更有效地优化其参数。我们将所有输入标准化，范围从0到1。这里，X是我们的输入(因此去掉了分类的物种)，Y是我们的输出(3个类)。</p><pre class="kg kh ki kj gt mm mn mo mp aw mq bi"><span id="8a45" class="mr ms iq mn b gy mt mu l mv mw">x_data = df.drop([‘species’],axis=1)</span><span id="2b9a" class="mr ms iq mn b gy mx mu l mv mw">y_data = df[‘species’]</span><span id="d780" class="mr ms iq mn b gy mx mu l mv mw">MinMaxScaler = preprocessing.MinMaxScaler()</span><span id="2fba" class="mr ms iq mn b gy mx mu l mv mw">X_data_minmax = MinMaxScaler.fit_transform(x_data)</span><span id="25f7" class="mr ms iq mn b gy mx mu l mv mw">data = pd.DataFrame(X_data_minmax,columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])</span><span id="3361" class="mr ms iq mn b gy mx mu l mv mw">df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/09925b58c237c9c3b3285bc52a5fae0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WBtTDa_57vz5YPlyWPjkMw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">按作者分类的图像(标准化数据集)</p></figure><p id="d219" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们已经达到了训练数据集的程度。我们使用sci-kit learn内置的KNN算法。我们将输入输出数据分为训练数据和测试数据，在训练数据上训练模型，在测试模型上测试模型的准确性。对于我们的培训和测试数据，我们选择80%–20%的分割。</p><pre class="kg kh ki kj gt mm mn mo mp aw mq bi"><span id="890e" class="mr ms iq mn b gy mt mu l mv mw">X_train, X_test, y_train, y_test = train_test_split(data, y_data,test_size=0.2, random_state = 1)</span><span id="def7" class="mr ms iq mn b gy mx mu l mv mw">knn_clf=KNeighborsClassifier()</span><span id="7158" class="mr ms iq mn b gy mx mu l mv mw">knn_clf.fit(X_train,y_train)</span><span id="77d0" class="mr ms iq mn b gy mx mu l mv mw">ypred=knn_clf.predict(X_test) #These are the predicted output values<br/></span></pre><p id="0c04" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">输出:</em></p><p id="8274" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">KNeighborsClassifier(algorithm = ' auto '，leaf_size=30，metric='minkowski '，metric_params=None，n_jobs=None，n_neighbors=5，p=2，weights='uniform ')</p><p id="4b5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我们看到分类器选择5作为最近邻的最佳数量，以对数据进行最佳分类。现在我们已经建立了模型，我们的最后一步是可视化的结果。我们计算了混淆矩阵、精确召回参数和模型的整体准确度。</p><pre class="kg kh ki kj gt mm mn mo mp aw mq bi"><span id="9881" class="mr ms iq mn b gy mt mu l mv mw">from sklearn.metrics import classification_report, confusion_matrix, accuracy_score</span><span id="c2ba" class="mr ms iq mn b gy mx mu l mv mw">result = confusion_matrix(y_test, ypred)</span><span id="a9d2" class="mr ms iq mn b gy mx mu l mv mw">print(“Confusion Matrix:”)</span><span id="85e3" class="mr ms iq mn b gy mx mu l mv mw">print(result)</span><span id="5aab" class="mr ms iq mn b gy mx mu l mv mw">result1 = classification_report(y_test, ypred)</span><span id="6367" class="mr ms iq mn b gy mx mu l mv mw">print(“Classification Report:”,)</span><span id="5527" class="mr ms iq mn b gy mx mu l mv mw">print (result1)</span><span id="a09c" class="mr ms iq mn b gy mx mu l mv mw">result2 = accuracy_score(y_test,ypred)</span><span id="ab64" class="mr ms iq mn b gy mx mu l mv mw">print(“Accuracy:”,result2)</span></pre><p id="2001" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">输出:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/9f77fbe03d3c43cc5324e18e9858075f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CbLhZluxdgSkPvHxniUtdw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者提供的图片(我们模型的结果)</p></figure><p id="8509" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">总结/结论:</em> </strong></p><p id="e1a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们成功地实现了虹膜数据集的KNN算法。我们通过EDA找出了最具影响力的特征，并对我们的数据集进行了标准化，以提高准确性。我们用我们的算法得到了96.67%的准确率，并且得到了混淆矩阵和分类报告。从分类报告和混淆矩阵中我们可以看到，它把云芝误认为是海滨锦葵。</p><p id="04f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是如何使用KNN算法进行多类分类。希望你今天学到了新的有意义的东西。</p><p id="17c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谢谢你。</p></div></div>    
</body>
</html>