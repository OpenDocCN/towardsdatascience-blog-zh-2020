<html>
<head>
<title>Polynomial Regression with a Machine Learning Pipeline</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有机器学习流水线的多项式回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/polynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87?source=collection_archive---------19-----------------------#2020-10-12">https://towardsdatascience.com/polynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87?source=collection_archive---------19-----------------------#2020-10-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e097" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">依次应用多个转换器和一个最终回归器来构建您的模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/db255998f572e47a40ae46c4652483c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t21Cwc3pxW0k2ReHgOmGCA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约书亚·索蒂诺在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="28fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">欢迎回来！应用我们已经拥有的知识，用一些真实的数据建立机器学习模型，这是非常令人兴奋的。<strong class="lb iu"> <em class="lv">多项式回归</em> </strong>，我们今天讨论的主题，就是这样一个模型，它可能需要一些复杂的工作流程，这取决于问题陈述和数据集。</p><p id="027d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天我们讨论如何建立多项式回归模型，以及在做模型之前如何对数据进行预处理。实际上，我们按照特定的顺序应用一系列步骤来构建完整的模型。Python Scikit-learn机器学习库中提供了所有必要的工具。</p><h2 id="29e6" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">先决条件</h2><p id="1dc6" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">如果你不熟悉Python、numpy、pandas、机器学习和Scikit-learn，请阅读我以前的文章，这些是本文的先决条件。</p><ul class=""><li id="e3b1" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated"><a class="ae ky" href="https://medium.com/data-science-365/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0?source=friends_link&amp;sk=1405bca22ed8b76f0100b5093b56b36e" rel="noopener">使用Scikit-learn进行主成分分析(PCA)</a></li><li id="519f" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><a class="ae ky" href="https://medium.com/data-science-365/linear-regression-with-gradient-descent-895bb7d18d52?source=friends_link&amp;sk=085716d97c63a0419a8323520a889c4a" rel="noopener">带梯度下降的线性回归</a></li><li id="73d5" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><a class="ae ky" href="https://medium.com/data-science-365/numpy-for-data-science-part-1-21e2c5ddbbd3?source=friends_link&amp;sk=bbaaf3a0894ef18e15ebfc5c53fb5c3b" rel="noopener">数据科学的数字:第一部分</a></li><li id="c0ce" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><a class="ae ky" href="https://medium.com/data-science-365/pandas-for-data-science-part-1-89bc231b3478?source=friends_link&amp;sk=9a2fd6ff88dda68a561dce240a8cd9d5" rel="noopener">数据科学的熊猫:第一部分</a></li></ul><p id="19dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不再拖延，让我们进入问题定义。</p><h1 id="7a70" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">问题定义</h1><p id="3d14" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我们有一个数据集，其中包含71人的年龄、身高和体重信息。我们希望建立一个回归模型，根据年龄和身高捕捉一个人的体重，并评估其性能。然后我们用这个模型来预测新病例。</p><p id="c123" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，在我们绘制数据之前，我们不知道数据的性质或复杂性。有时候，我们可以很容易地拟合出一条直线来描述模型。但大多数时候，现实世界的数据并非如此。数据可能很复杂，您需要考虑不同的方法来解决问题。</p><h1 id="7e24" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">数据集</h1><p id="3c90" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我们有一个名为<strong class="lb iu"><em class="lv">person _ data . CSV</em></strong>(<a class="ae ky" href="https://drive.google.com/file/d/1U-Wj1nYje_1EZ2HA0hZ86tEEyWA_lKGH/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">在此下载</a>)的数据集，其中包含71个人的年龄、身高和体重信息。让我们使用pandas<strong class="lb iu"><em class="lv">read _ CSV()</em></strong>函数加载它，并将其存储在<strong class="lb iu"> <em class="lv"> df </em> </strong>变量中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/824cf4397a659133ff25171cdbbb5885.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*9f-ydwL79oUwHnNivrVpBw.png"/></div></figure><p id="20ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查一下数据是否有缺失值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b140165cea72bcaebd3b98f02c891abf.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*NWhOSYHGOH-jF1Pz7lG3Hg.png"/></div></figure><p id="e89e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太好了！这3列的所有值都有71个非空值，这意味着没有缺失值。</p><p id="ad4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们定义我们的特征矩阵和目标向量。根据问题定义，特征矩阵— <strong class="lb iu"> X </strong>包含年龄和身高的值。目标向量— <strong class="lb iu"> y </strong>包含权重值。构建回归模型是一项监督学习任务，因此我们将输入<strong class="lb iu"> X </strong>映射到输出<strong class="lb iu"> y=f(X) </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/aef5a64812692a9ac6508a2f9c43c812.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*w1ZDx6K5d4Hn0Gp_gqSo3w.png"/></div></figure><p id="adf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们数据的维数是2，因为<strong class="lb iu"> X </strong>是二维的。那么，我们如何用<strong class="lb iu"> y </strong>绘制出<strong class="lb iu"> X </strong>的二维数据呢？显然，我们需要创建一个3D图。但是还有另一种方法。我们可以将年龄和身高组合成一个变量，称为<strong class="lb iu"> Z </strong>，然后用<strong class="lb iu"> Z </strong>和<strong class="lb iu"> y </strong>绘制2D图。通过减少<strong class="lb iu"> X </strong>中的特征数量将年龄和身高组合成一个变量，称为<strong class="lb iu"><em class="lv"/></strong>降维，我们用来执行降维的技术是利用这两个变量相关性的<strong class="lb iu"><em class="lv">【PCA】</em></strong>。</p><p id="3ff5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在建立模型之前执行降维有两个优点:</p><ol class=""><li id="b7d8" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu nw na nb nc bi translated">这对于在2D图中可视化我们的数据是非常有用的，这样可以更容易地看到重要的模式</li><li id="c62a" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu nw na nb nc bi translated">去除特征矩阵<strong class="lb iu"> X </strong>中的相关变量极其有用。这样做可以避免误导性的预测</li></ol><p id="14b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查年龄和身高是否相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/bdefebea8edb5fa8b37a9126dd6dcb71.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*XpgG_ivV8p8kdYBaKy4U7Q.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d969e1f23c385ed61aee66d6e6c5d283.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*OPxWQVbzOYvy6N1_DiY-SQ.png"/></div></figure><p id="d78a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个特征似乎高度相关。自变量(特征矩阵中的变量<strong class="lb iu"> X </strong>)之间存在显著相关性或关联性的情况称为<strong class="lb iu"><em class="lv"/></strong>。多重共线性会导致误导性预测。</p><p id="6f4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在制作模型之前，我们的下一个任务是移除这些相关变量。正如我前面所说的，我们使用主成分分析(PCA)来做这件事。但是，在运行PCA之前，如果数据集的要素之间的比例存在显著差异，则有必要执行要素缩放。这是因为PCA对原始特征的相对范围非常敏感。因此，我们通过使用Scikit-learn中<em class="lv">预处理</em>子模块中的Scikit-learn<strong class="lb iu">standard scaler()</strong>类，应用<strong class="lb iu"> <em class="lv"> z分数标准化</em> </strong>以将所有特征纳入相同的尺度<em class="lv"> </em>。</p><p id="d430" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意:</strong>我们仅对特征矩阵<strong class="lb iu"> X </strong>应用特征缩放。我们不需要将它应用于我们的目标向量<strong class="lb iu"> y </strong>。</p><p id="e46b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，构建我们的模型的工作流程如下。它包含一系列应该按照给定顺序应用的步骤。建立一个机器学习模型不是一次性的任务，你可能会回到前面的步骤并做一些修改，然后你会再次经历接下来的步骤。</p><h2 id="aef1" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">一般的工作流程是:</h2><ol class=""><li id="e531" class="mu mv it lb b lc mp lf mq li nz lm oa lq ob lu nw na nb nc bi translated">对特征矩阵X应用特征缩放[使用Scikit-learn<strong class="lb iu">standard scaler()</strong>class]。</li><li id="985d" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu nw na nb nc bi translated">运行PCA算法[使用Scikit-learn<strong class="lb iu">PCA()</strong>class]。</li><li id="4891" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu nw na nb nc bi translated">创建散点图并确定关系的性质[使用Seaborn <strong class="lb iu">散点图()</strong>函数]。</li><li id="151b" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu nw na nb nc bi translated">我们可以用直线拟合数据吗？还是需要添加多项式要素来精确拟合模型？</li><li id="d3ec" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu nw na nb nc bi translated">如果我们可以对我们的数据拟合一条直线，然后运行线性回归算法[使用Scikit-learn<strong class="lb iu">Linear Regression()</strong>class]</li><li id="9972" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu nw na nb nc bi translated">如果我们不能用直线拟合我们的数据，那么我们需要添加多项式特性[使用Scikit-learn<strong class="lb iu">polynomial features()</strong>class]</li><li id="ece7" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu nw na nb nc bi translated">添加多项式特性后，运行线性回归算法[使用Scikit-learn<strong class="lb iu">Linear Regression()</strong>类]</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/c038517356e8d785c8ab7e35c9a64018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4V18EPSAuUsCSOWuHP7maQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="8eb2" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">应用要素缩放</h1><p id="df20" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我们通过使用Scikit-learn<strong class="lb iu">standard scaler()</strong>应用<strong class="lb iu"> <em class="lv"> z-score标准化</em> </strong>将所有特征纳入同一尺度<em class="lv"> </em>。所有特征都根据以下公式进行缩放。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/475bfec3e3317a464b4046df612e404a.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*cET8d5iVq9RaKU1VLmsHww.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/2c17d7cc466e6de531849c26330733de.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*ZVv3WThtX6JWd7luHVgmLQ.png"/></div></div></figure><p id="239c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> X </strong>的缩放值存储在<strong class="lb iu"> <em class="lv"> X_scaled </em> </strong>变量中，该变量是一个二维numpy数组。</p><p id="a475" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查缩放后的<strong class="lb iu"> X </strong>值的平均值和标准偏差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/ca841498acd11362d2ace789aa6ddc2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*lqv-LNxin56fU2v9sRYvMg.png"/></div></figure><p id="0796" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，您可以看到在<strong class="lb iu"> <em class="lv"> X_scaled </em> </strong>矩阵中，每个变量的均值为零，标准差为1。</p><p id="0c81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们的数据可以运行PCA了。</p><h1 id="f57a" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">运行PCA算法</h1><p id="1d09" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">现在，我们准备对数据集应用PCA。我们需要将<strong class="lb iu"> <em class="lv"> X_scaled </em> </strong>矩阵中的二维数据缩减为一维数据。</p><p id="2252" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Scikit-learn中，使用<strong class="lb iu"> PCA() </strong>类来应用PCA。该类中最重要的<em class="lv">超参数</em>是<strong class="lb iu"> n_components </strong>。由于我们对获取一维数据感兴趣，所以<strong class="lb iu"> n_components </strong>的值为1。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f83c8ea0c633ffaca6aa69c6c83f85d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*aTWOIR-W1XwvSCnromN3ug.png"/></div></figure><p id="b1f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们对<strong class="lb iu"> <em class="lv"> X_scaled </em> </strong>值应用了PCA，输出值存储在<strong class="lb iu"> <em class="lv"> X_pca </em> </strong>变量中，该变量现在是一维数组。</p><p id="7c08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们降低了数据的维度，与原始数据集相比，也损失了一些信息。在PCA中，算法找到数据的低维表示，同时尽可能多地保留变化。</p><p id="5924" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查算法在运行时保留了多少变化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1d802ef707303f069692956aa5f891b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*SC1297q8svgv-nBcPhUKvA.png"/></div></figure><p id="9fbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">哇！超值！可以看到，第一个主成分保留了数据集中约97.36%的可变性，同时减少了数据集中的1(2–1)个特征。所以<strong class="lb iu"> <em class="lv"> X_pca </em> </strong>的值更准确地表示特征矩阵<strong class="lb iu"> X </strong>的值。</p><p id="8d1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们把我们的数据绘制成散点图。</p><h1 id="0b88" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">绘制数据</h1><p id="bff3" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">现在我们有了一维数据(<strong class="lb iu"> <em class="lv"> X_pca </em> </strong>)，分别代表<strong class="lb iu"> <em class="lv"> X </em> </strong>。现在我们可以用我们在2D图中的<strong class="lb iu"> y </strong>(权重)值绘制<strong class="lb iu"> <em class="lv"> X_pca </em> </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/df523b5083c05318fff9d8c68fe2f55d.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*s5liRGd9qrIJWj43dEdiQw.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b483b62c5cd9a23b973c46a7fbd282d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Quw_GUXdX9_0lMU8kVwP2g.png"/></div></figure><p id="c60e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很明显，一条直线永远不会完全符合这些数据。让我们看看，如果我们试图用一条直线来拟合我们的数据，会发生什么。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/e2adf0eabb212501f63b074fb309ee98.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*hhTVIi-bwWTQAn-Kh0_y8Q.png"/></div></figure><p id="092d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> <em class="lv"> θ </em> 1 </strong>和<strong class="lb iu"> 𝛼 </strong>是在训练过程中学习的模型<em class="lv">参数</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/76c5d3a876e120fabb39fa052d07415f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*EcMfRGBb-qlE8Vo-TVO24w.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/1122ac6cc91247f6a2bd66f2650b6783.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*DGHViIYyhSuaxq_TX6XHtQ.png"/></div></figure><p id="f708" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拟合如此之差，以至于直线法永远不会为我们的数据提供最佳模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/36c267ca76888849e75ab8465d3c1246.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*5GUNXs3t5s3jsfjfxljEnw.png"/></div></figure><p id="07df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的模型只能解释或捕捉到约61%的重量变化。RMSE值为10.99。这意味着平均而言，模型的预测值与实际值相差10.99个单位。</p><p id="6033" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们尝试一种称为<strong class="lb iu"> <em class="lv">多项式回归</em> </strong>的不同方法，以获得最适合我们的数据。</p><h1 id="abd6" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">多项式回归</h1><p id="019d" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">您可以使用线性模型来拟合非线性数据。一种简单的方法是将每个特征的幂作为新特征添加，然后在这个扩展的特征集上训练线性模型。这种技术叫做<strong class="lb iu"> <em class="lv">多项式回归</em> </strong>。</p><p id="1159" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，使用多项式的多项式回归在参数方面仍然是线性的。这是因为你只需将各项相加就能建立方程。因此，像R平方(R决定系数)这样的性能指标对于多项式回归仍然有效。不要混淆多项式回归和非线性回归，其中R是无效的！</p><h2 id="1a65" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">为每个变量添加幂</h2><p id="97fb" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">在我们的模型中，唯一的变量是<strong class="lb iu">T5 X _ PCAT7。为该变量增加幂后，模型变为:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/f1624eaa5c4395dd17acc104dfa67189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PdB_SgCPIkeQf-x-YcGlEA.png"/></div></div></figure><p id="6000" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> <em class="lv"> θ </em> 1 </strong>，<strong class="lb iu"> <em class="lv"> θ </em> 2 </strong>，<strong class="lb iu"> <em class="lv"> θ </em> 3 <em class="lv"> </em> </strong>和<strong class="lb iu"> 𝛼 </strong>是在训练过程中学习的模型<em class="lv">参数</em>。</p><p id="f0a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使用Scikit-learn<strong class="lb iu">polynomial features()</strong>类将多项式特征添加到我们的数据中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/7d026fda712e36874d15e7e74c3f1689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*rR8y5oRq5jcwDbh_pw9GOg.png"/></div></figure><p id="7281" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">多项式Features() </strong>类中最重要的<em class="lv">超参数</em>是<strong class="lb iu"> <em class="lv">次</em> </strong>。我们设置<strong class="lb iu"> <em class="lv"> degree=4 </em> </strong>，这样当输入(<strong class="lb iu"><em class="lv">x _ PCA</em></strong>，<strong class="lb iu"> <em class="lv"> X_pca </em> </strong>，<strong class="lb iu"> <em class="lv"> X_pca⁴ </em> </strong>)是一维时，它创建了3个额外的特征，称为<strong class="lb iu"><em class="lv">x _ PCA</em><em class="lv">。<strong class="lb iu"> <em class="lv"> X_poly </em> </strong>变量保存特征的所有值。</em></strong></p><h2 id="4cff" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">运行算法</h2><p id="763c" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">现在，我们已经将数据转换为多项式特征。所以，我们可以再次使用<strong class="lb iu"> LinearRegression() </strong>类来构建模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/fcdc2e4f5374d6886b994c29734f41fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*WyaGWEbXxYJWggD2AH-9MQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/3fc1ad2f81b3613c25d01d71f630f363.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*D1AXzO9zCU0MQ-i3uRElUg.png"/></div></figure><p id="f406" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">哇！似乎多项式方法给了我们一个更好的模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/d6c3f85ea453314eba3e1299e6bc987e.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*DAe5h-Mv225fCz1EcdDi3Q.png"/></div></figure><p id="d34b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的模型解释或捕获了重量中观察到的约90%的可变性。RMSE值为5.526。这意味着平均而言，模型的预测值与实际值相差5.526个单位。</p><p id="f253" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们通过多项式回归显著提高了模型的性能。</p><p id="3cc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查残差(误差)的分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/8413e90bcd59fa0ce7d008f6d800e2b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*VxG5ydSSpCNImrYcmJ0pEQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/f8eff5db133f6955e592b036dd24691a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*eFp2nxOTgGw1D0_2qDp6nA.png"/></div></figure><p id="34e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过查看直方图，我们可以验证残差近似正态分布，平均值为0。</p><h1 id="2951" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">超参数调谐</h1><h2 id="177c" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">模型参数与超参数</h2><p id="20c1" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">模型参数是在训练过程中学习的参数。我们不手动设置参数值，它们从我们提供的数据中学习。比如<strong class="lb iu"> <em class="lv"> θ </em> 1 </strong>，<strong class="lb iu"> <em class="lv"> θ </em> 2 </strong>，<strong class="lb iu"> <em class="lv"> θ </em> 3 <em class="lv"> </em> </strong>和<strong class="lb iu"> 𝛼 </strong>都是我们多项式回归模型中的参数。</p><p id="dfb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，模型超参数是不从数据中学习的参数。因此，我们必须手动为它们设置值。我们总是在创建特定模型时和开始训练过程之前设置模型超参数的值。例如，在从<strong class="lb iu"> PCA() </strong>类创建<strong class="lb iu"> <em class="lv"> pca </em> </strong>对象时，我们已经手动设置了<strong class="lb iu"> n_components= </strong> 1。我们还在从<strong class="lb iu"> PolynomialFeatures() </strong>类创建<strong class="lb iu"><em class="lv">poly _ features</em></strong>对象时手动设置了<strong class="lb iu"> degree=4 </strong>。</p><p id="e782" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在大多数情况下，为模型超参数设置正确的值是最具挑战性的任务之一。这背后没有什么神奇的公式。您可以尝试不同的值，并获得可视化表示来验证您的选择。有时，您可能会尝试不同的值，评估模型性能并选择最佳值。有时，问题的领域知识将帮助您推导出超参数的正确值。</p><p id="d46b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们为超参数<strong class="lb iu"><em class="lv"/></strong>尝试一些不同的值(2–9)(最初，我将此设置为<strong class="lb iu"> degree=4 </strong>)并获得一些可视化表示和模型评估指标。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/fbf19cd5cff8104f832390148ee3c1fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BudqHw7xY8H3U3eP-L5neg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/9fb554da0a15f7cb3f811790a74d0046.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*6vlNaAcUGZXBtBglOpJ00g.png"/></div></figure><p id="dd16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">度数过高会导致<strong class="lb iu"> <em class="lv">过拟合</em> </strong>。过拟合问题是指统计模型开始描述数据中的随机误差，而不是变量之间的关系。在过度拟合中，该模型非常适合训练数据，但无法对不在我们的数据集中的新输入数据进行归纳。</p><p id="ad62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降低度数会导致<strong class="lb iu"> <em class="lv">欠配合</em> </strong>。在欠拟合中，模型不能很好地拟合训练数据和新数据。</p><p id="45ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们设置<strong class="lb iu"> <em class="lv">度</em> </strong>超参数的值时，我们应该总是尽量避免过拟合和欠拟合的情况</p><p id="8adf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过查看可视化表示和性能指标的值，我们可以确定degree=4或degree=5是<strong class="lb iu"> <em class="lv"> degree </em> </strong>超参数的理想值。</p><h1 id="baad" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">根据新数据做出预测</h1><p id="d073" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">假设我们有5个新的观察值，我们想使用我们的模型预测权重。新的输入数据存储在二维的<strong class="lb iu"> <em class="lv"> X_new </em> </strong>数组中。第一列是年龄，第二列是身高。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/ae403961dbb008a480915038bc3805ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*bESebvsCCVwBF4nUe105oQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">新输入数据</p></figure><p id="0f15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们构建模型时，我们已经对原始数据进行了3次缩放和转换。因此，当我们使用我们的模型对新数据进行预测时，有必要使用相同的方法对新数据进行缩放和转换。所以，我们必须调用<strong class="lb iu"> <em class="lv"> fit_transform() </em> </strong>方法3次，然后调用<strong class="lb iu"> <em class="lv"> predict() </em> </strong>方法1次。所以，这对我们来说很烦人。为了克服这个问题，我们可以为我们的多项式回归模型建立一个机器学习管道。</p><h1 id="303f" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">构建机器学习管道</h1><p id="074d" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">Scikit-learn将机器学习算法称为<strong class="lb iu">估计器</strong>。<strong class="lb iu"> </strong>有三种不同类型的估计器:<strong class="lb iu">分类器</strong>、<strong class="lb iu">回归器</strong>和<strong class="lb iu">变压器</strong>。分类器和回归器被称为<strong class="lb iu">预测器</strong>。</p><p id="0857" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着我们的分析和工作流变得越来越复杂，您可能需要在数据准备好用于受监督的机器学习模型之前对其进行多次转换。管道依次应用一系列转换器和最终预测器(分类器或回归器)。管道的中间步骤必须是“transformers”，即它们必须实现<strong class="lb iu"> fit( </strong>)和<strong class="lb iu"> transform() </strong>方法。最终预测器只需要实现<strong class="lb iu"> fit() </strong>方法。</p><p id="549e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的工作流程中:</p><ol class=""><li id="4d61" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu nw na nb nc bi translated"><strong class="lb iu"> StandardScaler() </strong>是变压器。</li><li id="682e" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu nw na nb nc bi translated"><strong class="lb iu"> PCA() </strong>是变压器。</li><li id="f8d5" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu nw na nb nc bi translated"><strong class="lb iu">多项式特性()</strong>是变压器。</li><li id="c112" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu nw na nb nc bi translated"><strong class="lb iu"> LinearRegression() </strong>是预测器。</li></ol><p id="9b12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们可以使用Scikit-learn<strong class="lb iu">Pipeline()</strong>类为我们的模型构建一个管道。它依次应用上面的变压器列表和最终预测值。这是代码。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/1ef7920009b86bed47434a5cb4e0188b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*pAJ_XkheDE435-y1hHR65Q.png"/></div></figure><p id="832b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用管道，我们可以用更少的代码轻松构建复杂的模型！</p><p id="e7cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们调用我们管道的<strong class="lb iu"> fit() </strong>方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/729ba7da5c8527f2c3f1c812468572dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*oaD4b1J_DaghVnDys6OkxA.png"/></div></figure><p id="a323" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当调用<strong class="lb iu"> <em class="lv"> poly_reg_model.fit(X，y) </em> </strong>时，会发生以下过程:</p><ul class=""><li id="cf09" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">X_scaled = sc.fit_transform(X)</li><li id="dec8" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">X _ PCA = PCA . fit _ transform(X _ scaled)</li><li id="61cb" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">X _ poly = poly _ features . fit _ transform(X _ PCA)</li><li id="300f" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">lin_reg.fit(X_poly，y)</li></ul><p id="df29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们调用我们管道的<strong class="lb iu"> predict() </strong>方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/9613129f5543d7bf779be4c444d679e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*WiWJtjgR9OiRCsQq2yqsxw.png"/></div></figure><p id="4983" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当调用<strong class="lb iu"><em class="lv">poly _ reg _ model . predict(X _ new)</em></strong>时，发生以下过程:</p><ul class=""><li id="ebb1" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">X_new_scaled = sc.transform(X)</li><li id="9a2f" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">X _ new _ PCA = PCA . transform(X _ scaled)</li><li id="91b7" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">X _ new _ poly = poly _ features . transform(X _ PCA)</li><li id="5208" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">lin_reg.predict(X_new_poly)</li></ul><p id="6e63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，<strong class="lb iu"><em class="lv">poly _ reg _ model . predict(X _ new)</em></strong>返回对我们新数据的预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/5db724ddd03af0bd157ad4e5a91f040e.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*AzvvzZEKHEXaBX9jIRxY9A.png"/></div></figure><h1 id="7cd1" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">保存我们的模型</h1><p id="990b" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">让我们用Python <strong class="lb iu"> <em class="lv"> dill </em> </strong>包保存我们的模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/a591b9c89dc27c77b491205bc1db5f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*sCbOayWLWVY2yOmzmdE5HQ.png"/></div></figure><p id="2d14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行完这段代码后，<strong class="lb iu"><em class="lv">poly _ reg _ model . dill</em></strong>(<a class="ae ky" href="https://drive.google.com/file/d/1C3_GAgJtrBiJWNxkBMI30SpUtnAP538r/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">此处下载</a>)文件会出现在你当前的工作目录下。</p><p id="8225" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们加载我们的模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/ab37153f2fa1ef40744acc2797ca4479.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*qGoC1-zlG0kN6F6c-lo40w.png"/></div></figure><p id="96a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样。您可以与他人共享<strong class="lb iu"><em class="lv">poly _ reg _ model . dill</em></strong>文件，他们可以使用该模型而无需重新构建。它在内存中只有743字节！</p><p id="b9e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本教程由<a class="ae ky" href="https://www.linkedin.com/in/rukshan-manorathna-700a3916b/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">鲁克山·普拉莫迪塔</em></a><em class="lv"/><a class="ae ky" href="https://medium.com/data-science-365" rel="noopener">数据科学365博客</a>作者设计创作。</p><h1 id="842a" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">本教程中使用的技术</h1><ul class=""><li id="9e42" class="mu mv it lb b lc mp lf mq li nz lm oa lq ob lu mz na nb nc bi translated"><strong class="lb iu"> Python </strong>(高级编程语言)</li><li id="7efd" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> numPy </strong>(数值Python库)</li><li id="5d97" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu">熊猫</strong> (Python数据分析与操纵库)</li><li id="66ea" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> matplotlib </strong> (Python数据可视化库)</li><li id="35f2" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> seaborn </strong> (Python高级数据可视化库)</li><li id="1ce8" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> Scikit-learn </strong> (Python机器学习库)</li><li id="4061" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> Jupyter笔记本</strong>(集成开发环境)</li></ul><h1 id="ca5d" class="ni lx it bd ly nj nk nl mb nm nn no me jz np ka mh kc nq kd mk kf nr kg mn ns bi translated">本教程中使用的机器学习</h1><ul class=""><li id="f76f" class="mu mv it lb b lc mp lf mq li nz lm oa lq ob lu mz na nb nc bi translated"><strong class="lb iu">主成分分析</strong></li><li id="3050" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu">多项式回归</strong></li></ul><p id="be98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">2020–10–12</p></div></div>    
</body>
</html>