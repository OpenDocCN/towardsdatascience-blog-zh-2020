<html>
<head>
<title>Big Data Engineering — Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大数据工程— Apache Spark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/big-data-engineering-apache-spark-d67be2d9b76f?source=collection_archive---------17-----------------------#2020-09-26">https://towardsdatascience.com/big-data-engineering-apache-spark-d67be2d9b76f?source=collection_archive---------17-----------------------#2020-09-26</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="72c3" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="d639" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">为什么Apache Spark非常适合各种ETL工作负载。</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/412f6e445c626ed209b66456ab159cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QTjAxSeaQZBHyCR7"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">由<a class="ae li" href="https://unsplash.com/@seiseisei?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Seika I </a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="76f9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这是大数据环境中的数据工程系列的第2部分。它将反映我个人的经验教训之旅，并在我创建的开源工具<a class="ae li" href="https://flowman.readthedocs.io" rel="noopener ugc nofollow" target="_blank"> Flowman </a>中达到高潮，以承担在几个项目中一遍又一遍地重新实现所有boiler plate代码的负担。</p><ul class=""><li id="60d1" class="mf mg iu ll b lm ln lp lq ls mh lw mi ma mj me mk ml mm mn bi translated"><a class="ae li" href="https://medium.com/@kupferk/big-data-engineering-best-practices-bfc7e112cf1a" rel="noopener">第1部分:大数据工程—最佳实践</a></li><li id="f51c" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me mk ml mm mn bi translated">第2部分:大数据工程— Apache Spark</li><li id="9469" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me mk ml mm mn bi translated"><a class="ae li" rel="noopener" target="_blank" href="/big-data-engineering-declarative-data-flows-3a63d1802846">第3部分:大数据工程——声明性数据流</a></li><li id="272b" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me mk ml mm mn bi translated"><a class="ae li" rel="noopener" target="_blank" href="/big-data-engineering-flowman-up-and-running-cd234ac6c98e">第4部分:大数据工程— Flowman启动并运行</a></li></ul><h1 id="a861" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">期待什么</h1><p id="f000" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">本系列是关于用Apache Spark构建批处理数据管道的。但是有些方面对于其他框架或流处理也是有效的。最后，我将介绍<a class="ae li" href="https://flowman.readthedocs.io" rel="noopener ugc nofollow" target="_blank"> Flowman </a>，这是一个基于Apache Spark的应用程序，它简化了批处理数据管道的实现。</p></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><h1 id="1e7a" class="mt mu iu bd mv mw nx my mz na ny nc nd kj nz kk nf km oa kn nh kp ob kq nj nk bi translated">介绍</h1><p id="41c2" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">第二部分强调了为什么Apache Spark非常适合作为实现数据处理管道的框架。还有许多其他的选择，尤其是在流处理领域。但是从我的角度来看，当在批处理世界中工作时(有很好的理由这样做，特别是如果涉及许多需要大量历史记录的非平凡转换，如分组聚合和巨大连接)，Apache Spark是一个几乎无可匹敌的框架，在批处理领域表现尤为突出。</p><p id="756f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">本文试图揭示Spark提供的一些功能，这些功能为批处理提供了坚实的基础。</p><h1 id="5329" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">数据工程的技术要求</h1><p id="6e16" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">我已经在第一部分中评论了数据处理管道的典型部分。让我们重复这些步骤:</p><ol class=""><li id="9762" class="mf mg iu ll b lm ln lp lq ls mh lw mi ma mj me oc ml mm mn bi translated"><strong class="ll je">提取。从某个源系统读取数据(可以是像HDFS这样的共享文件系统，也可以是像S3这样的对象存储，或者像MySQL或MongoDB这样的数据库)</strong></li><li id="5beb" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">转型。</strong>应用一些转换，如数据提取、过滤、连接甚至聚合。</li><li id="0305" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">装货。</strong>将结果再次存储到某个目标系统中。同样，这可以是共享文件系统、对象存储或某个数据库。</li></ol><p id="1a3c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，我们可以通过将这些步骤中的每一步映射到期望的功能，并在最后添加一些额外的需求，来推导出用于数据工程的框架或工具的一些需求。</p><ol class=""><li id="99e8" class="mf mg iu ll b lm ln lp lq ls mh lw mi ma mj me oc ml mm mn bi translated"><strong class="ll je">广泛的连接器。</strong>我们需要一个能够从广泛的数据源读入数据的框架，比如分布式文件系统中的文件、关系数据库或列存储中的记录，甚至是键值存储。</li><li id="0cb5" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">广泛而可扩展的变换范围。为了“应用转换”,框架应该明确地支持和实现转换。典型的转换是简单的列式转换，如字符串操作、过滤、连接、分组聚合——所有这些都是传统SQL提供的。最重要的是，框架应该提供一个简洁的API来扩展转换集，特别是列式转换。这对于实现不能用核心功能实现的定制逻辑非常重要。</strong></li><li id="40ea" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">广泛的连接器。</strong>同样，我们需要各种各样的连接器来将结果写回到所需的目标存储系统中。</li><li id="f6c2" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">扩展性。</strong>我已经在上面的第二个需求中提到了这一点，但是我觉得这一点对于一个明确的点来说足够重要。可扩展性可能不仅限于转换类型，还应该包括新输入/输出格式和新连接器的扩展点。</li><li id="c89f" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">可扩展性。</strong>无论选择哪种解决方案，都应该能够处理不断增长的数据量。首先，在许多情况下，你应该准备好处理比RAM所能容纳的更多的数据。这有助于避免完全被数据量卡住。其次，如果数据量使处理速度太慢，您可能希望能够将工作负载分布到多台机器上。</li></ol><h1 id="ee53" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">什么是Apache Spark</h1><p id="2fdf" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">Apache Spark为上述所有需求提供了良好的解决方案。Apache Spark本身是一个库集合，是一个开发定制数据处理管道的框架。这意味着Apache Spark本身并不是一个成熟的应用程序，而是需要您编写包含转换逻辑的程序，而Spark负责以一种高效的方式在集群中的多台机器上执行逻辑。</p><p id="bb4f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">Spark最初于2009年在加州大学伯克利分校的AMPLab启动，并于2010年开源。最终在2013年，这个项目被捐赠给了Apache软件基金会。该项目很快就受到了关注，尤其是以前使用Hadoop Map Reduce的人。最初，Spark围绕所谓的rdd(弹性分布式数据集)提供了核心API，与Hadoop相比，rdd提供了更高层次的抽象，从而帮助开发人员更高效地工作。</p><p id="307c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">后来，添加了更新的on preferred DataFrame API，它实现了一个关系代数，其表达能力可与SQL相媲美。这个API提供了与数据库中的表非常相似的概念，这些表具有命名的和强类型的列。</p><p id="4e06" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">虽然Apache Spark本身是用Scala(一种在JVM上运行的混合函数式和面向对象的编程语言)开发的，但它提供了使用Scala、Java、Python或r编写应用程序的API。当查看<a class="ae li" href="https://spark.apache.org/examples.html" rel="noopener ugc nofollow" target="_blank">官方示例</a>时，您会很快意识到该API非常有表现力和简单。</p><ol class=""><li id="3c8b" class="mf mg iu ll b lm ln lp lq ls mh lw mi ma mj me oc ml mm mn bi translated"><strong class="ll je">连接器。</strong>由于Apache Spark只是一个处理框架，没有内置的持久层，它一直依赖于通过JDBC连接到HDFS、S3或关系数据库等存储系统。这意味着从一开始就建立了清晰的连接设计，特别是随着数据帧的出现。如今，几乎每一种存储或数据库技术都只需要<em class="od">为Apache Spark提供一个适配器，而Apache Spark被认为是许多环境中的一个可能选择。</em></li><li id="e61b" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">变换。</strong>原始核心库为RDD抽象提供了许多常见的转换，如过滤、连接和分组聚合。但是现在更新的DataFrame API更受欢迎，它提供了大量模仿SQL的转换。这应该足够满足大多数需求了。</li><li id="200c" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">扩展性。</strong>新的转换可以通过所谓的<em class="od">用户定义函数</em>(UDF)轻松实现，在这种情况下，您只需提供一小段处理单个记录或列的代码，Spark将它包装起来，以便该函数可以并行执行并分布在一个计算机集群中。<br/>由于Spark具有非常高的代码质量，您甚至可以深入一两层，使用内部开发人员API实现新的功能。这可能有点困难，但是对于那些无法使用UDF实现的罕见情况来说，这是非常有益的。</li><li id="5a54" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">可扩展性。</strong> Spark从一开始就被设计成一个大数据工具，因此它可以扩展到不同类型集群(当然是Hadoop YARN、Mesos和最近的Kubernetes)中的数百个节点。它可以处理比内存大得多的数据。一个非常好的方面是，Spark应用程序也可以在没有任何集群基础设施的单个节点上非常高效地运行，从开发人员的角度来看，这是一个很好的测试，但这也使Spark能够用于不太大的数据量，并且仍然受益于Sparks的特性和灵活性。</li></ol><p id="0237" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">从这四个方面来看，Apache Spark非常适合以前由Talend或Informatica等供应商提供的专用且昂贵的ETL软件完成的典型数据转换任务。通过使用Spark，您可以获得生动的开源社区的所有好处，并且可以根据您的需求自由定制应用程序。</p><p id="ecad" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">尽管Spark在创建时就考虑到了海量数据，但我总是会考虑使用它，即使是少量数据，因为它非常灵活，可以随着数据量无缝增长。</p><h1 id="1961" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">可供选择的事物</h1><p id="3567" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">当然，Apache Spark并不是实现数据处理管道的唯一选择。像Informatica和Talend这样的软件供应商也为喜欢购买完整生态系统的人提供非常可靠的产品(包括所有的优点和缺点)。</p><p id="60ae" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">但是，即使在大数据开源世界中，有些项目乍看起来似乎是备选方案。</p><p id="781c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">首先，我们仍然有Hadoop。但是Hadoop实际上由三个组件组成，这三个组件被清晰地划分:首先，我们有分布式文件系统HDFS，它能够存储非常大量的数据(比如说Pb)。接下来，我们有运行分布式应用程序的集群调度器。最后，我们有一个Map Reduce框架，用于开发非常特殊类型的分布式数据处理应用程序。虽然前两个组件HDFS和YARN仍然被广泛使用和部署(尽管他们感到来自云存储的压力，Kubernetes是可能的替代品)，但Map Reduce框架现在根本不应该被任何项目使用。编程模型太复杂了，编写重要的转换会变得非常困难。所以，是的，HDFS和YARN作为基础设施服务(存储和计算)很好，Spark与这两者很好地集成在一起。</p><p id="e655" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">其他备选方案可以是SQL执行引擎(没有集成的持久层)，如Hive、Presto、Impala等。虽然这些工具通常也提供了与不同数据源的广泛连接，但它们都局限于SQL。首先，对于带有许多公共表表达式(cte)的长链转换，SQL查询本身会变得非常棘手。其次，用新特性扩展SQL通常更加困难。我不会说Spark在总体上比这些工具更好，但我认为Spark更适合数据处理管道。这些工具在<em class="od">查询</em>现有数据方面大放异彩。但是我不想用这些工具为<em class="od">创建</em>数据——那从来都不是他们的主要工作范围。另一方面，虽然你<em class="od">可以</em>通过<a class="ae li" href="https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html" rel="noopener ugc nofollow" target="_blank"> Spark Thrift Server </a>使用Spark来执行SQL以提供数据，但它并不是真正为这种场景而创建的。</p><h1 id="e831" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">发展</h1><p id="0f87" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">我经常听到的一个问题是，应该使用什么编程语言来访问Spark的功能。正如我在上面写的，Spark out of the box提供了Scala、Java、Python和R的绑定——所以这个问题真的很有意义。</p><p id="b936" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我的建议是根据任务使用Scala或Python(也许是R——我没有这方面的经验)。千万不要用Java(感觉真的比干净的Scala API复杂多了)，投资点时间学点基础的Scala吧。</p><p id="7ec6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在留给我们的问题是“Python还是Scala”。</p><ul class=""><li id="f2c6" class="mf mg iu ll b lm ln lp lq ls mh lw mi ma mj me mk ml mm mn bi translated">如果你做的是数据工程(读取、转换、存储)，那么我强烈建议使用Scala。首先，因为Scala是一种静态类型语言，所以实际上比Python更容易编写正确的程序。第二，每当您需要实现Spark中没有的新功能时，最好使用Spark的本地语言。尽管Spark well支持Python中的UDF，但您将付出性能代价，并且无法再深入研究。用Python实现新的连接器或文件格式将非常困难，甚至是不可能的。</li><li id="c3df" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me mk ml mm mn bi translated">如果你正在研究数据科学(这不在本系列文章的讨论范围之内)，那么Python是更好的选择，包括所有那些Python包，比如Pandas、SciPy、SciKit Learn、Tensorflow等等。</li></ul><p id="0f56" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">除了上面两个场景中的不同库之外，典型的开发工作流也有很大的不同:数据工程师开发的应用程序通常每天甚至每小时都在生产中运行。另一方面，数据科学家经常与数据交互工作，一些见解是最终的成果。因此，生产就绪性对数据工程师来说比对数据科学家来说更重要。即使许多人不同意，Python或任何其他动态类型语言的“生产就绪”要困难得多。</p><h1 id="3f00" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">框架的缺点</h1><p id="8915" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">既然Apache Spark对于复杂的数据转换来说是一个非常好的框架，我们可以简单地开始实现我们的管道。在几行代码中，我们可以指示Spark执行所有的魔法，将我们的多TB数据集处理成更容易访问的东西。</p><p id="65ab" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">等等，别这么快！我在过去为不同的公司多次这样做，过了一段时间后，我发现许多方面必须反复实现。虽然Spark擅长数据处理本身，但我在本系列的第一部分中指出，健壮的数据工程不仅仅是处理本身。日志、监控、调度、模式管理都出现在我的脑海中，所有这些方面都需要在每个严肃的项目中得到解决。</p><p id="1669" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">那些非功能方面通常需要编写重要的代码，其中一些可能是非常低级和技术性的。因此，Spark不足以实现生产质量数据管道。由于这些问题的出现与特定的项目和公司无关，我建议将应用程序分成两层:一个顶层包含编码在数据转换中的业务逻辑以及数据源和数据目标的规范。一个较低的层应该负责执行整个数据流，提供相关的日志记录和监控指标，负责模式管理。</p></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><h1 id="6896" class="mt mu iu bd mv mw nx my mz na ny nc nd kj nz kk nf km oa kn nh kp ob kq nj nk bi translated">最后的话</h1><p id="56f3" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">这是关于使用Apache Spark构建健壮的数据管道的系列文章的第二部分。我们非常关注为什么Apache Spark非常适合取代传统的ETL工具。下一次我将讨论为什么另一个抽象层将帮助您关注业务逻辑而不是技术细节。</p></div></div>    
</body>
</html>