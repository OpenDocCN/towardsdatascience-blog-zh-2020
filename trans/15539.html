<html>
<head>
<title>CharacterBERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CharacterBERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/characterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21?source=collection_archive---------29-----------------------#2020-10-26">https://towardsdatascience.com/characterbert-reconciling-elmo-and-bert-for-word-level-open-vocabulary-representations-from-94037fe68b21?source=collection_archive---------29-----------------------#2020-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a268" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">来自字符的单词级开放词汇表示</h2></div><h2 id="5657" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">CharacterBERT是什么，和BERT有什么不同？</h2><p id="52a3" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">CharacterBERT是BERT的一个变体，它试图回到更简单的时代，那时模型为单个单词(或者更确切地说，令牌)生成单个嵌入。在实践中，唯一的区别是<strong class="ld ir">不依赖于文字块</strong> , <strong class="ld ir"> CharacterBERT使用一个CharacterCNN模块</strong> <strong class="ld ir">，就像ELMo</strong><em class="lu">【1】</em>中使用的那个一样。</p><p id="8a61" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">下图显示了CharacterCNN的内部机制，并将其与BERT中的原始单词块系统进行了比较。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/5b3efe786c779469ffb1000d30aa5cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LrrhEvFojh5i6qUU"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">BERT与CharacterBERT中的上下文无关标记表示(来源:<em class="mq">【2】)</em></p></figure><p id="5ef8" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">我们假设单词“Apple”是一个<strong class="ld ir">未知单词</strong>(即它没有出现在BERT的词块词汇表中)，那么BERT将其拆分为<strong class="ld ir">已知词块</strong>:【Ap】和[##ple】，其中##用于指定不在单词开头的词块。然后，使用<strong class="ld ir">字块嵌入矩阵</strong>嵌入每个子字单元，产生<strong class="ld ir">两个输出向量</strong>。</p><p id="d5f7" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">另一方面，<strong class="ld ir"> CharacterBERT没有单词表，可以处理任何*输入标记</strong>，只要它不是不合理的长(即少于50个字符)。CharacterBERT并没有拆分“Apple”，而是将它解读为一个<strong class="ld ir">的字符序列</strong>:【A】，【p】，【p】，【l】，【e】。然后使用<strong class="ld ir">字符嵌入矩阵</strong>表示每个字符，产生一系列<strong class="ld ir">字符嵌入</strong>。然后这个序列被送到多个CNN，每个CNN负责一次扫描n个字符，n =[1..7].所有CNN的输出被<strong class="ld ir">聚集成一个单一的矢量</strong>，然后被<strong class="ld ir">投射</strong>到期望的维度，使用高速公路层<em class="lu">【3】</em>。这个最终投影是单词“Apple”的<strong class="ld ir">上下文无关表示</strong>，它将与位置和片段嵌入相结合，然后像在BERT中一样被馈送到多个变换器层。</p><h2 id="dd24" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">为什么是CharacterBERT而不是BERT？</h2><p id="78d2" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">CharacterBERT几乎充当了BERT 的替代品</p><ul class=""><li id="826b" class="mr ms iq ld b le lv lh lw ko mt ks mu kw mv lt mw mx my mz bi translated">为任何输入标记生成单个嵌入</li><li id="f0f5" class="mr ms iq ld b le na lh nb ko nc ks nd kw ne lt mw mx my mz bi translated">不依赖于单词表</li></ul><p id="67ef" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">第一点显然是可取的，因为使用单个嵌入比每个令牌使用可变数量的单词块向量要方便得多。至于第二点，当在<strong class="ld ir">专业领域</strong>(如医疗领域、法律领域……)工作时，这一点尤为重要。事实上，在构建BERT的专门版本(例如BioBERT<em class="lu">【4】</em>、blue BERT<em class="lu">【5】</em>和一些SciBERT<em class="lu">【6】</em>模型)时，通常的做法是<strong class="ld ir">在一组专门的文本上重新训练</strong> <strong class="ld ir">原始模型</strong>。因此，大多数SOTA专用模型保留了原来的<strong class="ld ir">通用领域</strong>词表，这<strong class="ld ir">不适合专用领域应用</strong>。</p><p id="6721" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">下表显示了原始通用领域词汇与建立在医学语料库上的<strong class="ld ir">医学词块词汇</strong>之间的差异:MIMIC<em class="lu">【7】</em>和PMC OA<em class="lu">【8】</em>。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nf"><img src="../Images/5fb3602ea63806741e8381732acc1288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sI3m4UcF5h95Luyaazt9lw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">使用不同领域的词块词汇表对特定领域术语进行标记化(来源:<em class="mq">【2】)</em></p></figure><p id="0e39" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">我们可以清楚地看到，伯特的词汇是<strong class="ld ir">不适合专业术语</strong>(例如“胆总管结石”被拆分为【cho，led，och，oli，thi，asi，s】)。医学术语更适用于<strong class="ld ir">，但它也有其局限性</strong> <strong class="ld ir">和</strong>(例如，将“borygmi”转换为[bor，bor，yg，mi])。</p><p id="e945" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">因此，为了<strong class="ld ir">避免任何可能来自使用错误单词表的偏见</strong>，并努力回到<strong class="ld ir">概念上更简单的模型</strong>，提出了BERT的变体:CharacterBERT。</p><h2 id="cfe7" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">CharacterBERT如何与BERT进行对比测试？</h2><p id="972e" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">BERT和CharacterBERT在一个经典场景中进行比较，在该场景中，通用模型在作为专用版本的预训练的初始化之前被预训练。</p><blockquote class="ng nh ni"><p id="a37b" class="lb lc lu ld b le lv jr lg lh lw ju lj nj lx ll lm nk ly lo lp nl lz lr ls lt ij bi translated"><strong class="ld ir"> <em class="iq">注</em> </strong> <em class="iq"> : </em>我们在这里重点关注英语和医学领域。</p></blockquote><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nm"><img src="../Images/9cc62efcb2f2e6e003032599580f028a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4ecdxHl4YdeOwJJ2"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">培训前语料库(来源:<em class="mq">【2】)</em></p></figure><p id="0177" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">为了尽可能的公平起见，<strong class="ld ir">BERT和CharacterBERT </strong> <strong class="ld ir">都是在</strong> <strong class="ld ir">完全相同的条件下</strong>从头开始预训练的。然后，在多个医疗任务上评估每个预训练模型。我们举个例子。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nn"><img src="../Images/1103135e977fb0da398a8378dc51ef4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7trEcjkZho_eyx07"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">评估任务(来源:<em class="mq">【2】)</em></p></figure><p id="05cf" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated"><strong class="ld ir">i2b 2/VA 2010</strong><em class="lu">【9】</em>是一项包含多项任务的竞赛，其中包括用于评估我们模型的<strong class="ld ir">临床概念检测</strong>任务。目标是检测三种类型的临床概念:<strong class="ld ir">问题</strong>、<strong class="ld ir">治疗</strong>和<strong class="ld ir">测试</strong>。上图的最左边给出了一个例子。</p><p id="f76c" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">像往常一样，我们通过在<strong class="ld ir">训练集</strong>上进行首次训练来评估我们的模型。在每次迭代中，模型在<strong class="ld ir">单独的验证集</strong>上被测试，允许我们<strong class="ld ir">保存最佳迭代</strong>。最后，在经历所有迭代之后，使用来自最佳迭代的模型在<strong class="ld ir">测试集</strong>上计算分数(这里是严格的F1分数)。然后，使用不同的随机种子将整个过程<strong class="ld ir">重复9次以上</strong>，这允许我们考虑一些<strong class="ld ir">方差</strong>，并将最终模型性能报告为:<strong class="ld ir">均值标准差</strong>。</p><blockquote class="ng nh ni"><p id="fea5" class="lb lc lu ld b le lv jr lg lh lw ju lj nj lx ll lm nk ly lo lp nl lz lr ls lt ij bi translated"><strong class="ld ir"> <em class="iq">注</em> </strong> <em class="iq"> : </em>更多细节详见论文[2]。</p></blockquote><h2 id="f160" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结果如何？</h2><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi no"><img src="../Images/0cc9b850e16f6c03507728bdb94a80ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uApzZ47rkvu3MXGK"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated"><em class="mq">来源:[2] </em></p></figure><p id="709c" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">在大多数情况下，<strong class="ld ir"> <em class="lu"> </em> CharacterBERT的表现优于其对应的BERT</strong>。</p><blockquote class="ng nh ni"><p id="bc42" class="lb lc lu ld b le lv jr lg lh lw ju lj nj lx ll lm nk ly lo lp nl lz lr ls lt ij bi translated"><strong class="ld ir"> <em class="iq">注</em> </strong> <em class="iq"> : </em>唯一的例外是ClinicalSTS任务，其中医疗角色BERT的得分(平均)低于BERT版本。这可能是因为任务数据集很小(1000个示例，而其他任务平均为30，000个示例)，应该进行调查。</p></blockquote><h2 id="303d" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">好处:对噪音的鲁棒性</h2><p id="7ea4" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">除了纯粹的性能，另一个有趣的方面是模型是否对噪声输入具有鲁棒性。事实上，我们在MedNLI任务<em class="lu">【10】</em>的<strong class="ld ir">嘈杂版本上评估了BERT和CharacterBERT，其中(简单地说)目标是说出两个医学句子是否相互矛盾。这里，X%的噪声水平意味着文本中的每个字符都有X%的概率被替换或交换。结果显示在下图中。</strong></p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi np"><img src="../Images/b0812466f4fccacc81a19cd859808d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*asf4rR9ZxTU_2r9w"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">BERT和CharacterBERT在嘈杂的(拼错的)MEDNLI版本上进行微调(来源:<em class="mq">【2】)</em></p></figure><p id="0bde" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">正如你所看到的，医学CharacterBERT模型似乎比医学BERT更健壮:当向所有分割添加噪声时，两个模型之间大约1%精度的初始差距增长到大约3%,当仅在测试集中用噪声给模型带来惊喜时，大约5%。</p><h2 id="1c83" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">CharacterBERT的缺点呢？</h2><p id="cc41" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">CharacterBERT的<strong class="ld ir">主要缺点</strong>是其<strong class="ld ir">较慢的预训速度</strong> <em class="lu">。<br/> </em>这是由于:</p><ol class=""><li id="8807" class="mr ms iq ld b le lv lh lw ko mt ks mu kw mv lt nq mx my mz bi translated">训练速度较慢的CharacterCNN模块；</li><li id="aa82" class="mr ms iq ld b le na lh nb ko nc ks nd kw ne lt nq mx my mz bi translated">但主要是因为该模型工作在<strong class="ld ir">记号级</strong> : <br/>它在每次预训练迭代中更新大量记号词汇表。</li></ol><p id="90f5" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated"><strong class="ld ir"> /！\ </strong>:然而，<strong class="ld ir"> CharacterBERT在推理过程中与BERT一样快</strong>(实际上，甚至更快一点)并且<a class="ae nr" href="https://github.com/helboukkouri/character-bert#pre-trained-models" rel="noopener ugc nofollow" target="_blank">预训练模型可用</a>所以你可以完全跳过预训练步骤😊！</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h2 id="3e73" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结论</h2><p id="4654" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">总而言之，CharacterBERT是用一个CharacterCNN(就像ELMo一样)代替了WordPiece系统的BERT的简单变种。对多个医疗任务的评估结果表明，这种变化是有益的:提高了性能和对拼写错误的鲁棒性。希望这个模型能激发更多基于单词级开放词汇转换器的语言模型的研究:将同样的想法应用于<strong class="ld ir">艾伯特</strong><em class="lu">【11】</em>，<strong class="ld ir">厄尼</strong><em class="lu">【12】</em>…</p><blockquote class="ng nh ni"><p id="d03f" class="lb lc lu ld b le lv jr lg lh lw ju lj nj lx ll lm nk ly lo lp nl lz lr ls lt ij bi translated"><strong class="ld ir"> <em class="iq">原文</em></strong><em class="iq">:</em><br/><a class="ae nr" href="https://arxiv.org/abs/2010.10392" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2010.10392</a></p><p id="1bd9" class="lb lc lu ld b le lv jr lg lh lw ju lj nj lx ll lm nk ly lo lp nl lz lr ls lt ij bi translated"><strong class="ld ir"> <em class="iq">代号&amp;预训车型</em></strong><em class="iq">:</em><br/><a class="ae nr" href="https://github.com/helboukkouri/character-bert" rel="noopener ugc nofollow" target="_blank">https://github.com/helboukkouri/character-bert</a></p><p id="a2ac" class="lb lc lu ld b le lv jr lg lh lw ju lj nj lx ll lm nk ly lo lp nl lz lr ls lt ij bi translated"><strong class="ld ir"> <em class="iq">参考文献</em></strong><em class="iq">:</em><br/><strong class="ld ir"><em class="iq">【1】</em></strong>彼得斯、马修·e等<em class="iq"/><a class="ae nr" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank"><em class="iq">深层语境化的词语表述。</em></a><em class="iq"><br/>arXiv预印本arXiv:1802.05365 </em> (2018)。<em class="iq"><br/></em><strong class="ld ir"><em class="iq">【2】</em></strong>El Boukkouri，Hicham，et al .<em class="iq"/><a class="ae nr" href="https://arxiv.org/abs/2010.10392" rel="noopener ugc nofollow" target="_blank"><em class="iq">character BERT:调和ELMo和BERT，用于来自字符的单词级开放词汇表示。</em></a><em class="iq"><br/>arXiv预印本arXiv:2010.10392 (2020)。<br/></em><strong class="ld ir"><em class="iq">【3】</em></strong>斯利瓦斯塔瓦、鲁佩什库马尔、克劳斯格雷夫、于尔根施密德胡伯。<a class="ae nr" href="https://arxiv.org/abs/1505.00387" rel="noopener ugc nofollow" target="_blank">公路网。</a><em class="iq">arXiv预印本arXiv:1505.00387 </em> (2015)。<br/><strong class="ld ir"><em class="iq">【4】</em></strong>Lee，Jinhyuk，et al .<a class="ae nr" href="https://arxiv.org/abs/1901.08746" rel="noopener ugc nofollow" target="_blank">BioBERT:一种用于生物医学文本挖掘的预训练生物医学语言表示模型。</a><em class="iq">生物信息学</em>36.4(2020):1234–1240。<br/><strong class="ld ir"><em class="iq"><strong class="ld ir"/></em></strong>彭、王一凡、阎、。生物医学自然语言处理中的迁移学习:在十个基准数据集上对bert和elmo的评估。<em class="iq">arXiv预印本arXiv:1906.05474 </em> (2019)。<br/><strong class="ld ir"><em class="iq">【6】</em></strong>贝尔塔吉、伊兹、凯尔·洛、阿尔曼·科汉。SciBERT:科学文本的预训练语言模型。<em class="iq">arXiv预印本arXiv:1903.10676 </em> (2019)。<br/><strong class="ld ir"><em class="iq">【7】</em></strong>Johnson，Alistair等《MIMIC-III临床数据库》(1.4版)。<em class="iq">生理网</em>(2016)<a class="ae nr" href="https://doi.org/10.13026/C2XW26" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.13026/C2XW26</a>。<br/><strong class="ld ir"><em class="iq">【8】</em></strong>PMC OA语料库:<a class="ae nr" href="https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/" rel="noopener ugc nofollow" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/</a><br/><strong class="ld ir"><em class="iq">【9】</em></strong>uz uner，zlem等.<a class="ae nr" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168320/" rel="noopener ugc nofollow" target="_blank"> 2010 i2b2/VA对临床文本中概念、断言、关系的挑战。</a><em class="iq">美国医学信息学协会杂志</em>18.5(2011):552–556。<br/><strong class="ld ir"><em class="iq">【10】</em></strong>Shiva de，柴坦尼亚。“MedNLI —临床领域的自然语言推理数据集”(版本1.0.0)。<em class="iq">生理网</em>(2019)<a class="ae nr" href="https://doi.org/10.13026/C2RS98" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.13026/C2RS98</a>。<br/><strong class="ld ir"><em class="iq">【11】</em></strong>兰，等.<a class="ae nr" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank">阿尔伯特:一个用于语言表征自我监督学习的lite BERT。</a><em class="iq">arXiv预印本arXiv:1909.11942 </em> (2019)。<br/><strong class="ld ir"><em class="iq">【12】</em></strong>孙，于等.<a class="ae nr" href="https://arxiv.org/abs/1907.12412" rel="noopener ugc nofollow" target="_blank"> ERNIE 2.0:语言理解的持续预训练框架。</a><em class="iq">AAAI</em>。2020.</p></blockquote></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="bacc" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">完成人:<a class="ae nr" href="https://helboukkouri.github.io/" rel="noopener ugc nofollow" target="_blank"> Hicham El Boukkouri </a>、Olivier Ferret、Thomas Lavergne、Hiroshi Noji、Pierre Zweigenbaum和Junichi Tsujii</p></div></div>    
</body>
</html>