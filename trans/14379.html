<html>
<head>
<title>Meta-labeling and Stacking</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">元标签和堆叠</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/meta-labeling-and-stacking-f17a7f9804ec?source=collection_archive---------4-----------------------#2020-10-04">https://towardsdatascience.com/meta-labeling-and-stacking-f17a7f9804ec?source=collection_archive---------4-----------------------#2020-10-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9cb6" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" rel="noopener" target="_blank" href="/feature-engineering-feature-selection-8c1d57af18d2">📈Python for finance系列</a></h2><div class=""/><div class=""><h2 id="ea00" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何提高你的机器学习分数</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/67dda5225a9c543276407d8aacc3fb34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyeV7fN6uVo-0VRSrrJ_Pg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="http://skuawk.com/" rel="noopener ugc nofollow" target="_blank">戴夫·甘迪</a>根据<a class="ae lh" href="https://creativecommons.org/licenses/publicdomain/" rel="noopener ugc nofollow" target="_blank">公共领域专用许可</a>拍摄的照片</p></figure><p id="638b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="me">来自《走向数据科学》编辑的提示:</em> </strong> <em class="me">虽然我们允许独立作者根据我们的</em> <a class="ae lh" rel="noopener" target="_blank" href="/questions-96667b06af5"> <em class="me">规则和指南</em> </a> <em class="me">发表文章，但我们并不认可每个作者的贡献。你不应该在没有寻求专业建议的情况下依赖一个作者的作品。详见我们的</em> <a class="ae lh" rel="noopener" target="_blank" href="/readers-terms-b5d780a700a4"> <em class="me">读者术语</em> </a> <em class="me">。</em></p><p id="8cc6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">警告:这里没有神奇的公式或圣杯，尽管一个新的世界可能会为你打开大门。</p><p id="c911" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">注1: </strong> <em class="me">如何安装</em><a class="ae lh" href="https://github.com/hudson-and-thames/mlfinlab" rel="noopener ugc nofollow" target="_blank"><em class="me">mlfinlab</em></a><em class="me">包没有错误信息可以在这里找到</em><a class="ae lh" href="https://medium.com/@kegui/how-to-install-mlfinlab-without-error-messages-896e2fb43c2f" rel="noopener"><em class="me"/></a><em class="me">。</em></p><p id="17b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">注2: </strong> <em class="me">如果你正在读马科斯·普拉多的</em> <a class="ae lh" href="https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089" rel="noopener ugc nofollow" target="_blank"> <em class="me">【金融机器学习进展】</em> </a> <em class="me">。</em> <strong class="lk jd"> <em class="me"> 7。</em></strong><a class="ae lh" href="https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55" rel="noopener"><strong class="lk jd"><em class="me"/></strong></a><strong class="lk jd"><em class="me"/></strong><em class="me">是第五章关于</em> <a class="ae lh" href="https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55" rel="noopener"> <em class="me">的细分特征</em> </a> <em class="me">。</em> <strong class="lk jd"> <em class="me"> 8。</em> </strong> <a class="ae lh" rel="noopener" target="_blank" href="/the-triple-barrier-method-251268419dcd"> <strong class="lk jd"> <em class="me">数据标注</em> </strong> </a> <em class="me">是第三章关于三重关卡的方法。和</em> <strong class="lk jd"> <em class="me"> 9。</em> </strong> <a class="ae lh" rel="noopener" target="_blank" href="/meta-labeling-and-stacking-f17a7f9804ec"> <strong class="lk jd"> <em class="me">元标注</em></strong></a><strong class="lk jd"><em class="me"/></strong><em class="me">是第50页第3.6章。</em></p><p id="db6a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">注3 </strong> : <em class="me">由于算法或评估程序的随机性或数值精度的差异，您的结果可能会有所不同。但是我确实发现很多人取得了更高的分数，因为他们以</em> <strong class="lk jd"> <em class="me">错误的</em> </strong> <em class="me">方式标准化了他们的训练和测试数据。在这篇文章的最后，我将揭示高分的大秘密。</em></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h2 id="2dee" class="mm mn it bd mo mp mq dn mr ms mt dp mu lr mv mw mx lv my mz na lz nb nc nd iz bi translated">📈Python For Finance系列</h2><ol class=""><li id="b687" class="ne nf it lk b ll ng lo nh lr ni lv nj lz nk md nl nm nn no bi translated"><a class="ae lh" href="https://medium.com/python-in-plain-english/identifying-outliers-part-one-c0a31d9faefa" rel="noopener">识别异常值</a></li><li id="9376" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><a class="ae lh" href="https://medium.com/better-programming/identifying-outliers-part-two-4c00b2523362" rel="noopener">识别异常值—第二部分</a></li><li id="62cb" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><a class="ae lh" href="https://medium.com/swlh/identifying-outliers-part-three-257b09f5940b" rel="noopener">识别异常值—第三部分</a></li><li id="58d2" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/data-whispering-eebb77a422da">程式化的事实</a></li><li id="53ed" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><a class="ae lh" href="https://medium.com/@kegui/feature-engineering-feature-selection-8c1d57af18d2" rel="noopener">特征工程&amp;特征选择</a></li><li id="eae1" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/data-transformation-e7b3b4268151">数据转换</a></li><li id="2d00" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><a class="ae lh" href="https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55" rel="noopener">微小差异特征</a></li><li id="6243" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/the-triple-barrier-method-251268419dcd">数据标签</a></li><li id="d6c4" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/meta-labeling-and-stacking-f17a7f9804ec">元标签和堆叠</a></li></ol></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="3e56" class="nu mn it bd mo nv nw nx mr ny nz oa mu ki ob kj mx kl oc km na ko od kp nd oe bi translated">介绍</h1><p id="e6cc" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">元标签已经在我的写作清单上坐了很长时间了。它是一个有用而强大的机器学习工具，可以收集在任何数据科学家的工具箱中，不管你使用的是什么模型。不幸的是，我几乎没有找到任何关于这个话题的像样的教程。而堆叠是一种流行的<strong class="lk jd"> <em class="me">集成方法</em> </strong> <em class="me"> </em>用于匹配学习。堆叠包括训练一个学习算法来组合其他几个学习算法的预测。正如我们所知，集成学习的基本思想是<strong class="lk jd"> </strong>促进比单独从任何组成学习算法获得的更好的预测性能。这是一种“群体智慧”方法，从几个模型中提取信息，形成一组高度准确的结果。根据这个定义，元标记也应该属于集成方法。</p><p id="d6f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然堆叠和元标记有一些相似之处，但它们是根本不同的。堆叠基本上包括两个步骤。首先，使用可用数据训练所有其他算法，然后使用其他算法的所有预测作为附加输入，训练组合器算法以进行最终预测。堆叠的过程是:</p><ol class=""><li id="db55" class="ne nf it lk b ll lm lo lp lr oi lv oj lz ok md nl nm nn no bi translated"><em class="me">建立第一个基础模型，得到预测</em></li><li id="c7b7" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><em class="me">用阈值过滤预测</em></li><li id="47e7" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><em class="me">将预测与输入合并为新的输入</em></li><li id="8e79" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><em class="me">建立第二个模型，并用新的输入对其进行训练</em></li><li id="c438" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><em class="me">用第二个模型预测</em></li></ol><p id="6678" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">很简单，我们可以把它看作是给我们的训练数据增加额外的特征。</p><p id="6144" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">而元标记利用了两层模型，但目的不同。根据Marcos Lopez de Prado在他的书《金融机器学习进展》第三章第50页(除了这本书，网上没有太多有用的信息)。</p><blockquote class="ol"><p id="9d12" class="om on it bd oo op oq or os ot ou md dk translated">当你想获得更高的F1分数时，元标签尤其有用。首先，我们建立一个实现高召回率的模型，即使精度不是特别高。第二，我们通过将元标记应用于由主要模型预测的阳性来校正低精度。</p></blockquote><p id="279a" class="pw-post-body-paragraph li lj it lk b ll ov kd ln lo ow kg lq lr ox lt lu lv oy lx ly lz oz mb mc md im bi translated">中心思想是创建一个学习如何使用主模型的辅助ML模型。这导致了改进的性能指标，包括:准确度、精确度、召回率和F1分数<em class="me">等。</em>。</p><p id="4a5e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在马科斯的书中，元标签之所以如此有效的原因是:</p><blockquote class="ol"><p id="682d" class="om on it bd oo op oq or os ot ou md dk translated">二进制分类问题提出了I型错误(假阳性)和II型错误(假阴性)之间的折衷。一般来说，增加二元分类器的真阳性率将倾向于增加其假阳性率。二元分类器的接收器操作特征(ROC)曲线根据接受更高的假阳性率来测量增加真阳性率的成本。</p></blockquote><p id="8d30" class="pw-post-body-paragraph li lj it lk b ll ov kd ln lo ow kg lq lr ox lt lu lv oy lx ly lz oz mb mc md im bi translated">一般来说，元标记的过程是这样的:</p><ol class=""><li id="6c09" class="ne nf it lk b ll lm lo lp lr oi lv oj lz ok md nl nm nn no bi translated"><em class="me">建立第一个基本模型，获得预测</em></li><li id="d79c" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><em class="me">用阈值</em>过滤预测</li><li id="90de" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><em class="me">将预测与x_train合并作为新的输入</em></li><li id="fab5" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><em class="me">将预测与y_train合并为一个新标签</em></li><li id="27f9" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><em class="me">建立第二个模型，并用新的输入和标签对其进行训练</em></li><li id="49e3" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><em class="me">用第二个模型预测</em></li><li id="9508" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated"><em class="me">基本模型预测和元模型预测交集的最终预测结果。</em></li></ol><p id="b9f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">元标签是我花了大量时间试图找出应用这种方法的最佳方式的主题。但是仍然有太多的未知，例如:</p><ol class=""><li id="7751" class="ne nf it lk b ll lm lo lp lr oi lv oj lz ok md nl nm nn no bi translated"><strong class="lk jd">元标签指标是否依赖于？</strong></li></ol><p id="9a70" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">当我们评估一种方法时，这实际上取决于我们选择哪些指标，以及您对模型架构、超参数等的优化程度..</em></p><p id="1f3c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">2.<strong class="lk jd">能否适用于不同的车型？</strong></p><p id="7cd5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于这种方法将使用两种不同的模型，模型差异是我们需要考虑的另一个问题。</p><p id="0e71" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">3.元标签只适用于某些数据吗？</p><p id="fa10" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">关于这个话题的参考文献大多来自时序数据，那么非序列数据呢？</em></p><p id="e0d8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">诸如此类，但是这些问题都是关于元标签的局限性，这也是我在本文中想要探讨的。但是由于这篇文章的长度限制，我可能无法涵盖所有的内容。</p><p id="dc14" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">元标记与集成方法的区别，尤其是堆叠，在于元标记将来自主要模型的预测添加到特征和标记中，而堆叠仅将其用作新特征。我可以理解那些额外的特征(预测)是由用来做预测的模型来表示的。但是为什么要在标签中使用呢？尽管这可能就是元标签这个名字的由来。最重要的是，在标签中加入预测是否有信息泄露？嗯，我想肯定会有从初级模型到次级模型的泄漏，这是为了得到更好的分数。</p><p id="5cb8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于可以在网上找到许多关于集成学习的讨论，特别是堆叠，元标记很少得到足够的研究。因此，本文将集中讨论元标记及其与堆叠的比较。</p><h1 id="02e7" class="nu mn it bd mo nv pa nx mr ny pb oa mu ki pc kj mx kl pd km na ko pe kp nd oe bi translated">图书馆</h1><p id="44a7" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">下面是本文中使用的库。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="578e" class="mm mn it pg b gy pk pl l pm pn">'''Main'''<br/>import numpy as np<br/>import pandas as pd</span><span id="f022" class="mm mn it pg b gy po pl l pm pn">'''Data Viz'''<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="5b60" class="mm mn it pg b gy po pl l pm pn">#plt.style.use('seaborn')<br/>plt.rcParams['figure.figsize'] = [16, 9]<br/>plt.rcParams['figure.dpi'] = 300<br/>plt.rcParams['font.size'] = 20<br/>plt.rcParams['axes.labelsize'] = 16<br/>plt.rcParams['axes.titlesize'] = 18<br/>plt.rcParams['xtick.labelsize'] = 12<br/>plt.rcParams['ytick.labelsize'] = 12<br/>plt.rcParams['font.family'] = 'serif'<br/>%matplotlib inline</span><span id="6500" class="mm mn it pg b gy po pl l pm pn">'''Data Prep'''<br/>from sklearn import preprocessing as pp<br/>from scipy.stats import pearsonr<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.model_selection import StratifiedKFold</span><span id="1a5d" class="mm mn it pg b gy po pl l pm pn">'''Metrics'''<br/>from sklearn.metrics import log_loss, accuracy_score, f1_score<br/>from sklearn.metrics import precision_recall_curve, average_precision_score<br/>from sklearn.metrics import roc_curve, auc, roc_auc_score<br/>from sklearn.metrics import confusion_matrix, classification_report</span><span id="d334" class="mm mn it pg b gy po pl l pm pn">'''Algos'''<br/>from sklearn.linear_model import LogisticRegression<br/>import lightgbm as lgb</span><span id="6765" class="mm mn it pg b gy po pl l pm pn">import tensorflow as tf<br/>from tensorflow import keras<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense<br/>from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping</span></pre><h1 id="a326" class="nu mn it bd mo nv pa nx mr ny pb oa mu ki pc kj mx kl pd km na ko pe kp nd oe bi translated">数据</h1><p id="5876" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">为了研究元标记和堆叠，我们可以通过<code class="fe pp pq pr pg b">sklearn.datasets.make_classification().</code>生成一个虚拟的二进制分类数据集，但是使用真实的数据集更有趣。我们这里使用的数据集是ULB机器学习小组的<strong class="lk jd"> <em class="me">信用卡欺诈检测</em> </strong>。我们先快速看一下数据。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="ef88" class="mm mn it pg b gy pk pl l pm pn">#pd.read_csv can read the url and unzip the zipped file at<br/>#same time, but will take a few seconds, just be patient.</span><span id="19a3" class="mm mn it pg b gy po pl l pm pn">url = "<a class="ae lh" href="https://clouda-datasets.s3.amazonaws.com/creditcard.csv.zip" rel="noopener ugc nofollow" target="_blank">https://clouda-datasets.s3.amazonaws.com/creditcard.csv.zip</a>"<br/>data_original = pd.read_csv(url) <br/>data_original.head()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ps"><img src="../Images/32f665eed1ac30fa14ba3b6dad9e8462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zA9AGwkSaEYESDMpt122Ow.png"/></div></div></figure></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="7786" class="nu mn it bd mo nv nw nx mr ny nz oa mu ki ob kj mx kl oc km na ko od kp nd oe bi translated">✍Tip！</h1><p id="c834" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated"><code class="fe pp pq pr pg b">pd.read_csv()</code> <em class="me">可以同时读取网址和解压压缩文件。我们不再需要任何其他函数或库来完成这项工作。</em></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="1600" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数据集包含28个匿名要素、1个“数量”要素、1个“时间”要素和1个目标变量类。该数据集显示了两天内发生的交易，其中284，807笔交易中有492笔欺诈。这些特征被匿名化以保护客户的隐私，这是PCA变换的结果，因为数据集在公共域中。唯一没有被PCA转换的特征是“时间”和“数量”。特征“时间”包含数据集中每个事务和第一个事务之间经过的秒数。特征“金额”是交易金额，该特征可用于依赖于示例的成本感知学习。特征“类”是响应变量，“0”作为目标变量对应于非欺诈情况，而目标变量中的“1”对应欺诈情况。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="0e22" class="mm mn it pg b gy pk pl l pm pn">data_original.info()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/804348913fb4918859b2c385186c813d.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*fGLSuI7eF5jl0REPrwVvHQ.png"/></div></figure><p id="e2fc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">变量之间也有最小的相关性——这可能是PCA变换变量的结果。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="4931" class="mm mn it pg b gy pk pl l pm pn">#see the cluster and corralation of features and classes<br/>def plot_corr(data = data_original):<br/>   <br/>    ax1 = data.corrwith(data.Class).plot.bar(figsize = (20, 10),<br/>         title = "Correlation with class",<br/>         fontsize = 18, color='r',<br/>         rot = 45, grid = True)<br/>    ax1.title.set_size(28)<br/>    <br/>    sns.set(style="white")<br/>    cmap = sns.diverging_palette(220, 20, as_cmap=True)<br/>    <br/>    corr =data.corr()<br/>    sns.clustermap(corr,cmap=cmap,<br/>                  linewidths=1,linecolor='w')<br/>                 <br/>plot_corr();</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/e20847b97c93e684c39d7554f75697f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*l40r2PdQPM32WBo3a2x2tA.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/c8e7ba54c63ef876e13e91e58318ba09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*wTKKrXGTLXY8cd2f96uCYQ.png"/></div></figure><p id="a40a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我之所以选择这个数据集，是因为从这个数据集无论是精度还是召回率都很难达到高分。该数据集非常不平衡，因为在284，807笔交易中有492笔(0.17%)欺诈。该数据集中99.83%的交易不是欺诈性的，而只有0.17%是欺诈性的。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="e5d5" class="mm mn it pg b gy pk pl l pm pn">val_counts = data_original[['Class']].value_counts()<br/>ax = sns.barplot(x=val_counts.index,<br/>                 y=val_counts/len(data_original))<br/>ax.set(title=f'Frequency Percentage by {val_counts}',<br/>       xlabel='Class',<br/>       ylabel='Frequency Percentage');</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/8512bb4eeb91adcdfe55d40231aa1183.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*76_sL8aDylIGbuaLDRfaPg.png"/></div></figure><p id="6beb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">面对如此少量的欺诈性数据，我们必须小心我们的数据处理和模型选择。算法可以很容易地通过预测所有的测试数据是欺诈性的来欺骗我们。随着99.9%的数据集是负面的(非欺诈)，网络将巧妙地预测所有是负面的，导致超过99%的准确性。结果看起来很好，但没有用。</p><p id="94da" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是为什么除了准确性之外，我们还需要更好的指标。</p><h1 id="1c20" class="nu mn it bd mo nv pa nx mr ny pb oa mu ki pc kj mx kl pd km na ko pe kp nd oe bi translated">衡量标准</h1><p id="65c5" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">结果质量没有单一的最佳衡量标准，问题域和数据决定了合适的方法。</p><p id="2d62" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">大多数机器学习使用准确度作为默认度量，如果我们使用准确度作为度量，正如我们所知，准确度是真阴性和真阳性的总和除以总数据集大小。考虑到真实的负值压倒真实的正值，准确性可能非常高，但不会表明您的模型对欺诈性数据进行分类的能力。也就是说，由于概念简单、易于实现和用途广泛，有一些度量方法被普遍采用。因为我们不知道哪一个最适合这个数据集，所以我将在本文中列出它们。</p><ul class=""><li id="9de3" class="ne nf it lk b ll lm lo lp lr oi lv oj lz ok md px nm nn no bi translated">准确(性)</li><li id="85c5" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md px nm nn no bi translated">平均精度</li><li id="4cae" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md px nm nn no bi translated">地下区域</li><li id="0179" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md px nm nn no bi translated">精确</li><li id="2a49" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md px nm nn no bi translated">召回</li><li id="e170" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md px nm nn no bi translated">f1-分数</li><li id="166d" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md px nm nn no bi translated">困惑_矩阵</li><li id="2274" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md px nm nn no bi translated">精确回忆曲线</li><li id="edc4" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md px nm nn no bi translated">曲线下面积</li></ul><p id="4cff" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面将对每个指标做一些介绍。当然，在网上可以找到大量关于度量的资料。如果你已经知道他们中的大部分，我们仍然可以在跳到下一部分之前刷新我们的记忆。</p><h2 id="e653" class="mm mn it bd mo mp mq dn mr ms mt dp mu lr mv mw mx lv my mz na lz nb nc nd iz bi translated">1.召回率、精确度和AUC ( <strong class="ak">曲线下面积</strong> ) ROC</h2><p id="5ca6" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">我把这三个指标放在一起，因为它们有很好的相关性。</p><p id="95c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据Wikipedia，precision是正确结果的数量除以所有返回结果的数量，而recall是正确结果的数量除以应该返回的结果的数量。</p><p id="41f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">而下面这张图比1000字更好的解释了这两个概念。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi py"><img src="../Images/30fe03b5f942a195ce9fcc4c62ca78ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*AppvGDLiPrGMzJVHtXVhzg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">精确和召回来自维基百科</a></p></figure><p id="d6e9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接收算子特征(ROC)是曲线下的面积，其中x是假阳性率(FPR ), y是真阳性率(TPR ),通常用于呈现机器学习中二元决策问题的结果。AUC是ROC曲线下的面积，代表召回(TPR)和特异性(FPR)之间的权衡。与其他指标一样，AUC介于0和1之间，0.5是随机预测的预期值。</p><p id="0d2f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">AUC-ROC曲线是在各种阈值设置下对分类问题的性能测量。ROC是概率曲线，AUC代表可分性的程度或度量。它告诉我们模型在多大程度上能够区分不同的类。</p><blockquote class="ol"><p id="6353" class="om on it bd oo op oq or os ot ou md dk translated"><strong class="ak">与精确召回曲线不同，ROC(接收者操作者特征)曲线最适用于平衡数据集。</strong></p></blockquote><figure class="qa qb qc qd qe kw gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/3f6dc609ea4c07a9741df8886defa452.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*QNbd6-d2n3KXPL-wakK1Zg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="2201" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">AUC接近1，这意味着它具有良好的可分性。差模型的AUC接近0，这意味着它具有最差的可分性度量。事实上，这意味着它在往复结果。</p><p id="2dd4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一篇关于这个话题的文章可以在<a class="ae lh" rel="noopener" target="_blank" href="/understanding-auc-roc-curve-68b2303cc9c5">这里</a>找到。此外，一个很好的ROC和AUC视频可以从Josh Starmer 的<a class="ae lh" href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw" rel="noopener ugc nofollow" target="_blank"> StatQuest中找到。</a></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="qf qg l"/></div></figure><h2 id="e53c" class="mm mn it bd mo mp mq dn mr ms mt dp mu lr mv mw mx lv my mz na lz nb nc nd iz bi translated">2.平均精度</h2><p id="aa3b" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">平均精度是一个用来概括精度-召回曲线(PR AUC)的单一数字，它使得比较不同的模型成为可能。PR AUC是曲线下的面积，其中x是召回率，y是精确度。平均精度(AP)的一般定义是找到上述精度-召回曲线下的面积。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qh"><img src="../Images/8230d2e2fce4369e5a3183a15730d809.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O7wS1zrQtATA7sdg.jpeg"/></div></div></figure><p id="cffe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">精确度和召回率总是在0和1之间。因此，AP也在0和1之间。</p><p id="da30" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://sanchom.wordpress.com/tag/average-precision/" rel="noopener ugc nofollow" target="_blank">这里</a>是一篇关于这个话题的好文章。</p><blockquote class="ol"><p id="e97f" class="om on it bd oo op oq or os ot ou md dk translated"><strong class="ak">然而，当处理高度倾斜的数据集时，精确召回(PR)曲线给出了一个算法性能的</strong> <a class="ae lh" href="http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">更丰富的</strong> </a> <strong class="ak">画面。</strong> <a class="ae lh" href="https://dl.acm.org/doi/10.1145/2808194.2809481" rel="noopener ugc nofollow" target="_blank">当AUC和AP都被重新调整到位于[0，1]时，AP大约是AUC乘以系统的初始精度。</a></p></blockquote><h2 id="64ec" class="mm mn it bd mo mp qi dn mr ms qj dp mu lr qk mw mx lv ql mz na lz qm nc nd iz bi translated"><strong class="ak"> 3。F1分数</strong></h2><p id="cc5c" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">传统的F值或平衡F值(<strong class="lk jd"> F1值</strong>)是精度和召回率的<a class="ae lh" href="https://en.wikipedia.org/wiki/Harmonic_mean#Harmonic_mean_of_two_numbers" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">调和平均值</strong> </a>。我们计算F1分数作为精度的调和平均值，并回想一下如何实现这一点。虽然我们可以取两个分数的简单平均值，但调和平均值更能抵抗异常值。因此，F1分数是一个平衡的度量，它恰当地量化了跨许多领域的模型的正确性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/187f446e6fff0503eb30cb3219e756e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*kOw2sQqCxUSU6KgrfisCuA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd qo"> F1得分</strong></p></figure><p id="af7b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">F1分数适用于ROC曲线的任何特定点。该点可以代表例如二元分类器中的特定阈值，因此对应于特定的精度和召回值。</p><blockquote class="ol"><p id="7970" class="om on it bd oo op oq or os ot ou md dk translated">请记住，F1分数是一种既代表召回率又代表准确率的聪明方法。对于要高的F1分数，精确度和召回率都应该高。</p></blockquote><p id="f92a" class="pw-post-body-paragraph li lj it lk b ll ov kd ln lo ow kg lq lr ox lt lu lv oy lx ly lz oz mb mc md im bi translated">因此，ROC曲线针对各种不同水平的阈值，并且对于其曲线上的各个点具有许多F1分数值。</p><h2 id="b271" class="mm mn it bd mo mp mq dn mr ms mt dp mu lr mv mw mx lv my mz na lz nb nc nd iz bi translated"><strong class="ak"> 4。混乱矩阵</strong></h2><p id="1314" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">对于如此高度不平衡的数据集，混淆矩阵没有多大意义。我将它添加到度量集合中，仅供参考。它可以通过下表自我解释:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/5b15bfa386b3dd73663ccc6501f733ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*MAkM_JC5oSoXYOlVAnuhnw.png"/></div></figure><ul class=""><li id="f5d1" class="ne nf it lk b ll lm lo lp lr oi lv oj lz ok md px nm nn no bi translated"><strong class="lk jd">真阳性(TP): </strong>这些是我们预测是的病例(他们有疾病)，他们确实有疾病。</li><li id="2f36" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md px nm nn no bi translated"><strong class="lk jd">真阴性(TN): </strong>我们预测没有，他们没有这种病。</li><li id="228c" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md px nm nn no bi translated">假阳性(FP): 我们预测是的，但他们实际上并没有患病。(也称为“第一类错误”)</li><li id="67a5" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md px nm nn no bi translated"><strong class="lk jd">假阴性(FN): </strong>我们预测没有，但他们确实有疾病。(也称为“第二类错误”)</li></ul><p id="1d93" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我在一个函数中总结了所有的指标，以便稍后调用。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="be51" class="mm mn it pg b gy pk pl l pm pn">def metrics_summary(true_label, prediction_prob, Threshold=0.5):<br/>    <br/>    #basically, slearn provides all the functions for metrics.</span><span id="4aa1" class="mm mn it pg b gy po pl l pm pn">    average_precision = average_precision_score(true_label<br/>    ,prediction_prob)<br/>    fpr, tpr, thresholds = roc_curve(true_label, prediction_prob)<br/>    areaUnderROC = auc(fpr, tpr)<br/>    <br/>    prediction_int = prediction_prob &gt; Threshold<br/>    <br/>    accuracy = accuracy_score(true_label, prediction_int)<br/>            <br/>    print(f'accuracy: {accuracy}')<br/>    print(f"average_precision: {average_precision}")<br/>    print(f'areaUnderROC: {areaUnderROC } \n')<br/>    print('*'*60)<br/>    print(' '*20, 'classification_report')<br/>    print('*'*60, "\n")<br/>    print(classification_report(true_label, prediction_int))<br/>    <br/>    print('*'*60)<br/>    print(' '*20, 'confusion_matrix \n')<br/>    print('*'*60, "\n")<br/>    display(confusion_matrix(true_label, prediction_int))<br/>    print("\n")<br/>    <br/>    # precision_recall_curve and areaUnderROC <br/>    precision, recall, thresholds = precision_recall_curve( \<br/>                                true_label, prediction_int)<br/>    <br/>    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,9))<br/>    <br/>    ax1.step(recall, precision, color='k', alpha=0.7, where='post')<br/>    ax1.fill_between(recall, precision, step='post', <br/>    alpha=0.3,color='k')</span><span id="a2e6" class="mm mn it pg b gy po pl l pm pn">    ax1.set_xlabel('Recall', fontname="Arial", fontsize=24)<br/>    ax1.set_ylabel('Precision', fontname="Arial", fontsize=24) <br/>    ax1.tick_params(labelsize=20)<br/>     <br/>    ax1.set_title('Precision-Recall curve: Average Precision \<br/>    = {0:0.2f}'.format(average_precision), fontsize=24,<br/>    fontname="Arial")        <br/>               <br/>    ax2.plot(fpr, tpr, color='r', lw=2, label='ROC curve')<br/>    ax2.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')<br/>    ax2.tick_params(labelsize=20)<br/>    ax2.set_xlabel('False Positive Rate', fontname="Arial",<br/>    fontsize=24)<br/>    ax2.set_ylabel('True Positive Rate', fontname="Arial",<br/>    fontsize=24)<br/>    ax2.set_title('areaUnderROC = {0:0.2f}'\<br/>            .format(areaUnderROC), fontsize=24, fontname="Arial",)    <br/>    ax2.legend(loc="lower right", fontsize=24, fancybox=True) <br/>    # Adjust the subplot layout, because the logit one may take <br/>      more space<br/>    # than usual, due to y-tick labels like "1 - 10^{-3}"<br/>    # plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10,<br/>    # right=0.95, hspace=0.25,wspace=0.35)</span></pre><p id="0766" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些评估指标非常重要。这有点像数据科学家和商业人士之间的接口。对于大多数只听说过AI但从未在任何模型上训练过的人来说，他们不会太关注像log loss、交叉熵和其他成本函数这样的东西。这就是为什么我们需要指标来直观地向业务人员解释结果。尽可能简单地向非数据科学家传达复杂结果的能力是应用数据科学家需要掌握的基本技能之一。</p><h1 id="11cd" class="nu mn it bd mo nv pa nx mr ny pb oa mu ki pc kj mx kl pd km na ko pe kp nd oe bi translated">模型</h1><p id="1676" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">以下是我将要探讨的三种模式:</p><ol class=""><li id="f314" class="ne nf it lk b ll lm lo lp lr oi lv oj lz ok md nl nm nn no bi translated">逻辑回归</li><li id="bc9b" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated">lightBGM</li><li id="90cf" class="ne nf it lk b ll np lo nq lr nr lv ns lz nt md nl nm nn no bi translated">DNN</li></ol><p id="d31f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我选择这三个模型的原因是它们高度不相关。独立解决方案需要相对不相关。如果它们非常相关，一旦将它们放入一个集合模型，其中一个的优势将反映其余的优势，劣势也是如此。我们看不到通过合奏实现多样化的好处。</p><p id="05c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们进入模型之前，仍然有一些事情需要做，即，基于特征的标准化、输入和标签分离以及分割训练和测试数据。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="6ce1" class="mm mn it pg b gy pk pl l pm pn">#Normalize training and testing data<br/>def scale_data(x_train, x_test=None):   <br/>    features_to_scale = x_train.copy().columns<br/>    scaler = pp.StandardScaler()<br/>    print(scaler.fit(x_train[features_to_scale]))<br/>    <br/>    x_train.loc[:, features_to_scale] = \<br/>    scaler.transform(x_train[features_to_scale])<br/>    <br/>    #normalize test dataset with the mean and std of train data set <br/>    x_test.loc[:, features_to_scale] = \<br/>    scaler.transform(x_test[features_to_scale])<br/>    <br/>    return x_train, x_test</span><span id="488f" class="mm mn it pg b gy po pl l pm pn">#seperate input and labels    <br/>def get_x_y(data=data_original):<br/>    data_x = data.copy().drop(['Class', 'Time'], axis=1)<br/>    data_y = data['Class'].copy()<br/>    <br/>    return data_x, data_y</span><span id="2700" class="mm mn it pg b gy po pl l pm pn">#split the train and test data<br/>def data_split(data_x, data_y):<br/>    x_train, x_test, y_train, y_test = \<br/>          train_test_split(data_x,data_y,test_size=0.25,<br/>          stratify=data_y,random_state=2020)      <br/>                                                        <br/>     return  x_train, x_test, y_train, y_test</span><span id="8951" class="mm mn it pg b gy po pl l pm pn">#put all together<br/>def data_process(data=data_original):<br/>    data_x, data_y = get_x_y(data)<br/>    <br/>    x_train, x_test, y_train, y_test \<br/>    = data_split(data_x, data_y)<br/>    <br/>    #do not touch the test data by any means!!!!<br/>    x_train, x_test = scale_data(x_train, x_test)<br/>    <br/>    return  x_train, x_test, y_train, y_test</span></pre></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="5d7a" class="nu mn it bd mo nv nw nx mr ny nz oa mu ki ob kj mx kl oc km na ko od kp nd oe bi translated">✍Tip！</h1><p id="7cfb" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">由于数据集高度不平衡，scikit-learn的<code class="fe pp pq pr pg b">train_test_split()</code>函数中的参数<code class="fe pp pq pr pg b">stratify =data_y</code>来得非常方便。数据在一瞬间以分层的方式被分割。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="c60a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可能注意到我用data_original，x_test_orignal，y_test_orignal作为变量名。我想把这三个子数据集放在一边，因为这些原始数据会有很多调整，我们不想以任何方式弄乱测试数据。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="4b98" class="mm mn it pg b gy pk pl l pm pn">x_train, x_test_original, y_train, y_test_original \<br/>= data_process(data_original)</span><span id="f6ae" class="mm mn it pg b gy po pl l pm pn">x_train.shape, x_test_original.shape, \<br/>y_train.shape, y_test_original.shape</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/4970153f203e097b7bb22ab503d4c782.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*dZACBWwXcxdnTB8boLyYKA.png"/></div></figure><p id="f297" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">训练数据集有213605条事务记录和29个特征，而测试数据集有71202条事务记录和29个特征。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="2425" class="mm mn it pg b gy pk pl l pm pn">print(f'No. of fraud in test dataset:\<br/>      {x_test_original[y_test_original==1].shape[0]}')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/db48157761baac6b30788bf12097efbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*eQ2CbKtvKat9MyER2Y_D7g.png"/></div></figure><p id="e736" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在测试数据集中的71202条交易记录中，只有123条记录是欺诈性的。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="eac7" class="nu mn it bd mo nv nw nx mr ny nz oa mu ki ob kj mx kl oc km na ko od kp nd oe bi translated">✍Tip！</h1><p id="7507" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated"><em class="me">注意，测试数据是通过训练数据的均值和标准差来标准化的。你不应该在你的工作流程中使用任何根据测试数据计算的数量，即使是简单的数据标准化。</em> <strong class="lk jd"> <em class="me">换句话说，永远不要碰你的测试数据！</em> </strong></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="cb4a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数据准备好了，指标确定了，模型来了:</p><ol class=""><li id="c957" class="ne nf it lk b ll lm lo lp lr oi lv oj lz ok md nl nm nn no bi translated"><strong class="lk jd">模型1 </strong> <em class="me">(逻辑回归)</em></li></ol><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="34d2" class="mm mn it pg b gy pk pl l pm pn">def build_model_1(x_train, y_train):<br/>    <br/>    logitreg_parameters = {'C': np.power(10.0, np.arange(-9, 1)),<br/>                           'solver' : ('lbfgs', 'liblinear') }<br/>    <br/>    model_1 = LogisticRegression(#solver='liblinear',<br/>                                 class_weight='balanced', <br/>    #uses the values of y to automatically adjust weights<br/>          <br/>                                 warm_start=True,<br/>    #reuse the solution of the previous call to fit<br/>     as initialization<br/>                                 max_iter = 300,<br/>    #Maximum number of iterations taken for the solvers to converge.<br/>                                 random_state=2020, <br/>    #so results can be reproduced<br/>                                 )</span><span id="84de" class="mm mn it pg b gy po pl l pm pn">     logitreg_grid = GridSearchCV(model_1, param_grid = \<br/>           logitreg_parameters,scoring = 'f1', n_jobs = 1, cv=5)          <br/>                             <br/>    logitreg_grid.fit(x_train, y_train)<br/>    <br/>    return logitreg_grid</span><span id="8b55" class="mm mn it pg b gy po pl l pm pn">model_1 = build_model_1(x_train, y_train)</span></pre><p id="1405" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">开始时，由于本文不是关于获得最佳分数，我使用了来自<strong class="lk jd"> <em class="me"> sklearn </em> </strong>的逻辑回归的默认设置。精度分很低，0.07左右。使用相同的默认设置，一些在线教程在使用逻辑回归的相同数据集上显示了非常高的精确度。然而，他们处理测试数据的方式是有问题的。为了得到更好的结果，使用了<code class="fe pp pq pr pg b">GridSearchCV</code>来搜索最佳参数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/f65b41c128240af2b167dd3d7f96af94.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*GmvIwBOuZpDe_ZecVe1sMg.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qt"><img src="../Images/0ddcf6427e9bf5cf4dd9189fe0498e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GsIaCZRAf3JBC18nDqeCyQ.png"/></div></div></figure><p id="5629" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我得到的最好成绩是</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="f8b9" class="mm mn it pg b gy pk pl l pm pn">model_1.best_score_</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qu"><img src="../Images/15c545403297e9d12133134e32d97a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*w6SwhsaADL-fI6uvbe22EA.png"/></div></figure><p id="1ff6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用以下设置:</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="e40d" class="mm mn it pg b gy pk pl l pm pn">model_1.best_estimator_</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qv"><img src="../Images/6b74d2d1c92d80e6e403d6e1d7368cd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*eHXEZaO2ZYY4FFT0u_2edA.png"/></div></figure><p id="7bfe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们检查测试数据集的结果和指标得分:</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="fdb0" class="mm mn it pg b gy pk pl l pm pn"># 0 and 1 two clasese<br/>y_pred_prob_test_1 = model_1.predict_proba(x_test_original)[:,1]<br/># number of fraud is 123 in test dataset<br/>y_pred_int_test_1 = y_pred_prob_test_1 &gt; Threshold<br/>pd.Series(y_pred_int_test_1).value_counts()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qw"><img src="../Images/373d9640bc586dfa27147bc80f573d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*BjK7GKYPsGuARwyYdCn7pg.png"/></div></figure><p id="3d6f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">相当不错，记住欺诈交易的数量是123。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="968c" class="mm mn it pg b gy pk pl l pm pn">metrics_summary(y_test_original, y_pred_int_test_1)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qx"><img src="../Images/48c93ab0ea14c42de566770b1e686aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*CW3I2rnkMJt2daHSUL2LRA.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qy"><img src="../Images/8f3a8d0cfc41e4ede81a0b8ab8650e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wesk5lqGv5EGAT-rRtZSIg.png"/></div></div></figure><p id="6909" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2。型号2 </strong> <em class="me"> (LightBGM) </em></p><p id="d3e1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于lightBGM，我进一步指定1/4的训练数据作为验证数据集。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="87b2" class="mm mn it pg b gy pk pl l pm pn">#prepare data <br/>x_train_, x_cv, y_train_, y_cv = \<br/>train_test_split(x_train, y_train,<br/>                test_size=0.25,<br/>                stratify=y_train,<br/>                random_state=2020)</span><span id="f350" class="mm mn it pg b gy po pl l pm pn">def build_model_2(x_train, y_train, x_cv, y_cv ):<br/>    #most of the parsmeters are default<br/>    params_lightGB = {<br/>    'task': 'train',<br/>    'application':'binary',<br/>    'num_class':1,<br/>    'boosting': 'gbdt',<br/>    'objective': 'binary',<br/>    'metric': 'binary_logloss',<br/>    'metric_freq':50,<br/>    'is_training_metric':False,<br/>    'max_depth':4,<br/>    'num_leaves': 31,<br/>    'learning_rate': 0.01,<br/>    'feature_fraction': 1.0,<br/>    'bagging_fraction': 1.0,<br/>    'bagging_freq': 0,<br/>    'bagging_seed': 2018,<br/>    'verbose': 0,<br/>    'num_threads':16<br/>    }</span><span id="3c34" class="mm mn it pg b gy po pl l pm pn">    lgb_train = lgb.Dataset(x_train, y_train)<br/>    lgb_eval = lgb.Dataset(x_cv, y_cv, reference=lgb_train)</span><span id="8a63" class="mm mn it pg b gy po pl l pm pn">    model_2 = lgb.train(params_lightGB, lgb_train,<br/>                    num_boost_round=2000,<br/>                    valid_sets=lgb_eval,<br/>                    early_stopping_rounds=200,<br/>                    verbose_eval=False)<br/>    return model_2</span><span id="7071" class="mm mn it pg b gy po pl l pm pn">x_train_.shape, y_train_.shape, x_cv.shape, y_cv.shape</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qz"><img src="../Images/62b66d98552cda5e44403ffbe0775241.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*xKlauJylrSdKQhjuOzlK7A.png"/></div></div></figure><p id="193c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果还不错，没有进行进一步的超参数调整。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="a97a" class="mm mn it pg b gy pk pl l pm pn">model_2 = build_model_2(x_train_, y_train_, x_cv, y_cv)</span><span id="789e" class="mm mn it pg b gy po pl l pm pn">y_pred_prob_test_2 = model_2.predict(x_test_original)<br/>y_pred_int_test_2 = y_pred_prob_test_2 &gt; Threshold<br/>pd.DataFrame(y_pred_int_test_2).value_counts()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ra"><img src="../Images/fd50ba28daa9a734a2d6b4522e75e5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*XenE-n5I-RAbmGemMpztYw.png"/></div></figure><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="4815" class="mm mn it pg b gy pk pl l pm pn">metrics_summary(y_test_original, y_pred_int_test_2)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rb"><img src="../Images/358c1983a82414465eeceb398bba1f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*wQtxTZz-eJgEmAqNK6nZbw.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rc"><img src="../Images/746ffa62671dc9346a1eb74e5deb896e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i541VLMTkWbZJKG8er6SfA.png"/></div></div></figure><p id="6f1b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与模型1相同，准确率非常高(99.9%)，精确度很好(93%)，而召回率适中(77%)。</p><p id="bdc1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 3。模型3 </strong> <em class="me">(深度神经元网络)</em></p><p id="014e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于深度神经元网络(DNN)，我使用了两个回调，EarlyStopping()和ReduceLROnPlateau()来获得更好的结果。同样，由于结果还不错，所以没有实现超参数调优。(<a class="ae lh" href="https://medium.com/@kegui/a-few-pitfalls-for-kerastuner-beginner-users-13116759435b" rel="noopener"> Keras-tuner </a>确实不错，可以查一下<a class="ae lh" href="https://medium.com/@kegui/how-to-do-cross-validation-in-keras-tuner-db4b2dbe079a" rel="noopener">我以前的文章</a>更好的了解。)</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="65d2" class="mm mn it pg b gy pk pl l pm pn">callbacks = [EarlyStopping(monitor='loss', patience=3), \<br/>                 ReduceLROnPlateau(monitor='val_loss', factor=0.2, \<br/>                                   patience=3, min_lr=0.001)]</span><span id="3693" class="mm mn it pg b gy po pl l pm pn">def build_model_3(x_train, y_train, x_cv, y_cv, input_dim=29): <br/>    model_3 = Sequential([<br/>                Dense(input_dim = input_dim, units = 32, <br/>                      activation  = 'relu'),<br/>                Dense(units = 16, activation =  'relu'),<br/>                #Dropout(0.5),<br/>                Dense(units = 8, activation =  'relu'),<br/>                Dense(units =1, activation = 'sigmoid'),])</span><span id="92e9" class="mm mn it pg b gy po pl l pm pn">     model_3.compile(optimizer = 'adam', <br/>                 loss = 'binary_crossentropy', <br/>                 metrics = ['accuracy'])<br/>    <br/>    model_3.fit(x_train, y_train, <br/>              validation_data = (x_cv, y_cv),<br/>              batch_size = 64, <br/>              epochs = 50,<br/>              callbacks=callbacks)<br/>    <br/>    return model_3</span></pre><p id="7118" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是一个简单的3层DNN，具有较小的单元数(32，16，8)以避免过度拟合。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="d389" class="nu mn it bd mo nv nw nx mr ny nz oa mu ki ob kj mx kl oc km na ko od kp nd oe bi translated">⚠️Warning</h1><p id="1785" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">DNN对输入特征非常敏感。如果将时间特征添加到训练数据中，结果会有些奇怪。但是一旦你去掉时间特性，它就恢复正常了。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="30da" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们可以在模型3上训练我们的数据。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="d8ed" class="mm mn it pg b gy pk pl l pm pn">model_3 = build_model_3(x_train_, y_train_, \<br/>x_cv, y_cv, input_dim=29)<br/>y_pred_prob_test_3 = model_3.predict(x_test_original)</span><span id="7e11" class="mm mn it pg b gy po pl l pm pn">y_pred_int_test_3 = y_pred_prob_test_3 &gt; Threshold<br/>y_pred_int_test_3.shape<br/>pd.DataFrame(y_pred_int_test_3).value_counts() </span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rd"><img src="../Images/9f2e0d27598a921f9a0376857ec55b6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*3qdKRdXVchRfv0fXagRAiA.png"/></div></figure><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="c2de" class="mm mn it pg b gy pk pl l pm pn">metrics_summary(y_test_original, y_pred_int_test_3)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi re"><img src="../Images/22381e81f5ed68a411a6900ef9f1b7b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*JGo1U9cFp6NhBZzhOZNTxA.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rf"><img src="../Images/2849c8021848e8e2be1a90cf623279b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0WNF-mN7GPxeKVinC_xw6w.png"/></div></div></figure><p id="4aef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在没有任何超参数调整的情况下，结果类似于优化的模型1逻辑回归。一般来说，如果我们包括来自不同机器学习家族的类似强解(例如一个来自随机森林，一个来自神经网络)，这些解的集合将导致比任何独立解更好的结果。这是因为每个独立解决方案都有不同的优点和缺点。通过将独立的解决方案整合在一起，一些模型的优势(T2)弥补了其他模型的劣势，反之亦然。到目前为止，来自3个非常不同的模型的结果似乎满足了我们对堆叠和元标记的要求。</p><p id="cc8f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们现在集合那些模型。</p><h1 id="8104" class="nu mn it bd mo nv pa nx mr ny pb oa mu ki pc kj mx kl pd km na ko pe kp nd oe bi translated">堆垛</h1><p id="2172" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">因为叠加有点像向输入数据添加新特征，而新特征来自于主模型的预测。我们首先做一个特征工程来堆叠所有的数据。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="6e0e" class="mm mn it pg b gy pk pl l pm pn">def data_stack( x, y, m_1=model_1, m_2=model_2, m_3=model_3):<br/>    #All required parameters must be placed before any <br/>     default arguments.<br/>    '''<br/>    x: features<br/>    y: labels<br/>    m_1, m_2, m_3: 3 models<br/>    <br/>    '''<br/>    # build a container to hold all the prediction from 3 models<br/>    pred_all = pd.DataFrame(data=[], index=y.index)</span><span id="685d" class="mm mn it pg b gy po pl l pm pn">    pred_1 = m_1.predict_proba(x)[:,1]<br/>    pred_1_df = pd.DataFrame(pred_1, index=y.index)</span><span id="a4a7" class="mm mn it pg b gy po pl l pm pn">    pred_2 = m_2.predict(x,num_iteration=m_2.best_iteration)<br/>                    <br/>    pred_2_df = pd.DataFrame(pred_2, index=y.index)</span><span id="4ac6" class="mm mn it pg b gy po pl l pm pn">    pred_3 = m_3.predict(x).reshape(x.shape[0]) #to 1D shape<br/>    pred_3_df = pd.DataFrame(pred_3, index=y.index)</span><span id="6204" class="mm mn it pg b gy po pl l pm pn">   # join all the predictions together<br/>    pred_all = pred_all.join(pred_1_df.astype(float),<br/>                             how='left',rsuffix="0")\<br/>                       .join(pred_2_df.astype(float),<br/>                             how='left',rsuffix="1")\<br/>                       .join(pred_3_df.astype(float),<br/>                             how='left',rsuffix="2")<br/>    pred_all.columns = ['pred_1', 'pred_2','pred_3']<br/>    # final training data will be the merge of training data <br/>      and all the predictions<br/>    x_pred = x.merge(pred_all, \<br/>                    left_index=True, right_index=True)<br/>    <br/>    return x_pred</span></pre><p id="ea7a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，将新特征(来自3个模型的预测)添加到训练数据集中。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="aa39" class="mm mn it pg b gy pk pl l pm pn">x_train_stack = data_stack(x_train, y_train)<br/>x_train_stack.shape</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rg"><img src="../Images/474f9a836c8db11ac958c4f0ca6271f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*UKn02IqN1yPpAxxAlhUSVw.png"/></div></figure><p id="fc6e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，相同的过程应用于测试数据集。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="1a26" class="mm mn it pg b gy pk pl l pm pn">x_test_stack = data_stack(x_test_original, y_test_original)<br/>x_test_stack.shape</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rh"><img src="../Images/a81c123f128253bc18a68ef7f9d13ba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/format:webp/1*RHEMrbuzCYPkHo1D06H34g.png"/></div></figure><p id="7352" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于我们在之前的数据集中添加了新的要素，因此查看这些新要素之间的相关性会很有意思。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ri"><img src="../Images/d4776737c2afb5e81f7e3cbe3c5db9ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uazHj5KhDncbbIjq4k_uug.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rj"><img src="../Images/b9fa6cb02341d35b2f36cfc2bb257dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*CDArST19sQGT_7OGIXi1aw.png"/></div></figure><p id="71ef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们确实看到初级模型的预测高度相关，这并不奇怪。<strong class="lk jd">来自第一模型的信息泄漏到第二模型中，因为它们共享相同的训练数据。只要测试数据是完整的，我们将更喜欢更多的信息流入第二个模型，以获得更好的结果。</strong></p><p id="e38c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们需要经历所有那些乏味但必要的数据过程。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="a1a5" class="mm mn it pg b gy pk pl l pm pn">#normalize training and testing data<br/>x_train_stack, x_test_stack = scale_data(x_train_stack,  x_test_stack)</span><span id="df7f" class="mm mn it pg b gy po pl l pm pn">#split the traning data to train and validation<br/>x_train_stack_, x_cv_stack, y_train_, y_cv_ = \<br/>train_test_split(x_train_stack, y_train,<br/>                test_size=0.25,<br/>                stratify=y_train,<br/>                random_state=2020)<br/>#stratify mean samplling with the ratio of each class percentage in #all data.</span><span id="d48d" class="mm mn it pg b gy po pl l pm pn">x_train_stack_.shape, x_cv_stack.shape, y_train_.shape,  y_cv.shape</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rk"><img src="../Images/45265409673c7790b53c8a01bacbf33c.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*HVlhGbUhYtVNbC3hdMKeqA.png"/></div></figure><p id="b171" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">不过，说到堆叠，有一些重要的注意事项。如果独立的解决方案同样强大，集合将比任何独立的解决方案具有更好的性能。但是，如果其中一个解决方案比其他解决方案好得多，集合的性能将等于最佳独立解决方案的性能；不合格的解决方案对整体性能没有任何贡献。</p><ol class=""><li id="24dd" class="ne nf it lk b ll lm lo lp lr oi lv oj lz ok md nl nm nn no bi translated"><strong class="lk jd">型号2 (lightBGM)作为二级型号</strong></li></ol><p id="ea15" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于模型2 (lightBGM)的精度迄今为止最高，我们将使用模型2作为第二模型。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="64dc" class="mm mn it pg b gy pk pl l pm pn">model_2_stack = build_model_2(x_train_stack_, y_train_, x_cv_stack, y_cv_)<br/>y_pred_prob_test_2_stack = model_2_stack.predict(x_test_stack)<br/>y_pred_int_test_2_stack = y_pred_prob_test_2_stack &gt; Threshold<br/>pd.DataFrame(y_pred_int_test_2_stack).value_counts()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rd"><img src="../Images/e970d3268e2e382b20d97171d009c031.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*aiQpEwYHxgkRojBYaxmQEg.png"/></div></figure><p id="6240" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，在没有堆叠的情况下，在相同模型上的102个病例的比较中确定了120个阳性结果。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="095c" class="mm mn it pg b gy pk pl l pm pn">metrics_summary(y_test_original, y_pred_int_test_2_stack)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rl"><img src="../Images/595db27ba0b28b079e7fa27a7d554d32.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*iV_yk50c324bE5uE_W9zpg.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rm"><img src="../Images/e23e2585256bc57831f51954ec726372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*veshO5r-tObSiq_rj_HnSw.png"/></div></figure><p id="fc27" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">召回率从0.77提高到0.85，f1分数和平均准确率也有适度的提高。</p><p id="eb74" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2。第三款DNN作为第二款</strong></p><p id="4d68" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们把DNN作为第二个模型会发生什么？</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="0f1f" class="mm mn it pg b gy pk pl l pm pn">model_3_stack = build_model_3(x_train_stack_, y_train_, \<br/>                        x_cv_stack, y_cv_, input_dim=32)</span><span id="acfe" class="mm mn it pg b gy po pl l pm pn">y_pred_prob_test_3_stack = model_3_stack.predict(x_test_stack)<br/>y_pred_int_test_3_stack = y_pred_prob_test_3_stack &gt; Threshold<br/>y_pred_int_test_3_stack.shape<br/>pd.DataFrame(y_pred_int_test_3_stack).value_counts()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ra"><img src="../Images/66c94396253ea54a39575dd18927f12a.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*sdrpdC4XlLsLhihDVILWSA.png"/></div></figure><p id="7953" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">同样的模型，88比116，结果不太乐观，这表明精确度更高，召回率更低。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="11f2" class="mm mn it pg b gy pk pl l pm pn">metrics_summary(y_test_original, y_pred_int_test_3_stack)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rn"><img src="../Images/563547f87e6d1347553454597f188572.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*sE8HrN-bWEX5ICtc37E-Rg.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ro"><img src="../Images/a2880c0a00c29e9276949b6a4abafbf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*o6s2ahrWvYFKuDZafv26CA.png"/></div></figure><p id="9716" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">不出所料，精度更高(0.95)，但召回率很低(0.68)。</p><p id="b740" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 3。模型1作为二级模型的逻辑回归</strong></p><p id="b3a1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">没想到会有更好的结果，纯粹好奇。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="2f5f" class="mm mn it pg b gy pk pl l pm pn">model_1_stack = build_model_1(x_train_stack, y_train)</span><span id="e775" class="mm mn it pg b gy po pl l pm pn">model_1_stack.best_score_</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rp"><img src="../Images/4264326e909664a80d9cb96b09862034.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*YQ8ZxfRn7AdLh7a-B5lbEA.png"/></div></figure><p id="5e19" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果看起来很有希望。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="fe82" class="mm mn it pg b gy pk pl l pm pn">y_pred_prob_test_1_stack = model_1_stack.predict_proba(x_test_stack)[:,1]# 0 and 1 two clases<br/>y_pred_int_test_1_stack = y_pred_prob_test_1_stack &gt; Threshold<br/>pd.Series(y_pred_int_test_1_stack).value_counts()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rq"><img src="../Images/1332d4e43e9a5eebdb26fb273c39b347.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*mDeS4Af9A2YMj9CICd_7zw.png"/></div></figure><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="2280" class="mm mn it pg b gy pk pl l pm pn">metrics_summary(y_test_original, y_pred_int_test_1_stack)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rr"><img src="../Images/5d2c95bde08e6fda97ae9addd0d45eb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*GQSeGHxysEkkWh2Cf4B5SA.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rm"><img src="../Images/073653af5093526d6658fd6931365797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*QM5apiT9H8SmB2Lcrz4Jvg.png"/></div></div></figure><p id="cdc3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">几乎所有的指标都有或多或少的改进，实际上，与lightBGM或DNN作为第二个模型相比，结果并不太差。</p><p id="8bc7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们去元标签。</p><h1 id="d3da" class="nu mn it bd mo nv pa nx mr ny pb oa mu ki pc kj mx kl pd km na ko pe kp nd oe bi translated">元标记</h1><p id="7264" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">因为元标记将需要向输入和标记添加新的特征。我写了另一个函数来处理这个问题。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="4d90" class="mm mn it pg b gy pk pl l pm pn">def data_meta(id, x, y, model):<br/>    #get prediction from model 1<br/>    pred_prob_meta = model.predict_proba(x)[:,1]<br/>    pred_prob_meta = pd.Series(pred_prob_meta, \<br/>                               index=x.index,<br/>                               name=f'pred_{id}_meta')<br/>    pred_int_meta = pred_prob_meta &gt; Threshold<br/>    y_meta = pd.Series(y &amp; pred_int_meta, name=f'y_train_meta_{id}')<br/>    x_meta = x.join(pred_int_meta)<br/>    <br/>    return x_meta, y_meta</span><span id="521e" class="mm mn it pg b gy po pl l pm pn">    pred_prob_meta = model.predict_proba(x)[:,1]<br/>    pred_prob_meta = pd.Series(pred_prob_meta, \<br/>                               index=x.index,<br/>                               name=f'pred_{id}_meta')<br/>    pred_int_meta = pred_prob_meta &gt; Threshold<br/>    y_meta = pd.Series(y &amp; pred_int_meta, name=f'y_train_meta_{id}')<br/>    x_meta = x.join(pred_int_meta)<br/>    <br/>    return x_meta, y_meta</span></pre><ol class=""><li id="4092" class="ne nf it lk b ll lm lo lp lr oi lv oj lz ok md nl nm nn no bi translated"><strong class="lk jd">第一个型号:logreg，第二个型号:lightBGM </strong></li></ol><p id="0211" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">准备好数据后，对于第一个实验，我将使用逻辑回归作为主要模型，lightBGM作为次要模型。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="20b5" class="mm mn it pg b gy pk pl l pm pn">x_train_meta_1, y_train_meta_1 = \<br/>data_meta(1, x_train, y_train, model_1)<br/>x_train_meta_1.shape, y_train_meta_1.shape</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rs"><img src="../Images/73fe684e8a3bc6c3b019290f5e9b8cd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*JrqoC3R8Zexrcj7kwHUm1A.png"/></div></figure><p id="471a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">再次，让我们检查元数据的相关性。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="7900" class="mm mn it pg b gy pk pl l pm pn">plot_corr_xy(x_train_meta_1, y_train_meta_1);</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rt"><img src="../Images/e68962fe2fffc31ae19a009a533a85f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*MNBuTzuR9bkJXYbgvSiMTg.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ru"><img src="../Images/8de86d1b22706fb267deb6eb9ca2bc22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*ukep9EstO4kU0xGZuVRKMg.png"/></div></figure><p id="31e9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">标签和添加的功能之间的相关性非常强(从模型1预测)。高皮尔逊相关系数表明更多的信息从第一模型泄漏到第二模型中。</p><p id="e4a4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，再一次，我们需要经历所有那些乏味但必要的数据处理。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="a190" class="mm mn it pg b gy pk pl l pm pn"># test data<br/>x_test_meta_1, y_test_meta_1 = \<br/>data_meta(1, x_test_original, y_test_original, model_1)<br/>x_test_meta_1.shape, y_test_meta_1.shape</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rv"><img src="../Images/2d53dfa25370e4657ecda39b16bf3daf.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*VZqpe8Ww2WVZX8RICXazZQ.png"/></div></figure><p id="b76c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后标准化测试和训练数据集。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="c2a3" class="mm mn it pg b gy pk pl l pm pn">x_train_meta_1, x_test_meta_1 = scale_data( \<br/>                                x_train_meta_1, x_test_meta_1)</span></pre><p id="db77" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">并拆分训练数据集以再次获得验证数据。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="8af5" class="mm mn it pg b gy pk pl l pm pn">x_train_meta_1_, x_cv_meta_1, y_train_meta_1_, y_cv_meta_1 = \<br/>train_test_split(x_train_meta_1, y_train_meta_1,<br/>                test_size=0.25,<br/>                stratify=y_train_meta_1,<br/>                random_state=2020)<br/>#stratify mean samplling with the ratio of each class percentage in #all data.</span><span id="9462" class="mm mn it pg b gy po pl l pm pn">x_train_meta_1_.shape, x_cv_meta_1.shape, y_train_meta_1_.shape,  y_cv_meta_1.shape</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/7f7fdb45d268e498beaf1cc0100056d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*cCbXKmltai86KXBvP1axxQ.png"/></div></figure><p id="288b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">做完这些，我们终于可以去看模特了。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="a3b1" class="mm mn it pg b gy pk pl l pm pn">model_2_meta_1 = build_model_2( \<br/>    x_train_meta_1_, y_train_meta_1_, x_cv_meta_1, y_cv_meta_1)</span><span id="4bef" class="mm mn it pg b gy po pl l pm pn">y_pred_prob_test_2_meta_1 = model_2_meta_1.predict(x_test_meta_1)<br/>y_pred_int_test_2_meta_1 = y_pred_prob_test_2_meta_1 &gt; Threshold<br/>pd.DataFrame(y_pred_int_test_2_meta_1).value_counts()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rw"><img src="../Images/ca557b38d2fd306813a1feb563d50db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*aXxiHhq1MxmdDwqpczKYMg.png"/></div></figure><p id="2936" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们有了元模型的预测之后，我们将结果与主模型的预测结合起来。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="4c9b" class="mm mn it pg b gy pk pl l pm pn">final_pred_2_meta_1 = y_pred_int_test_2_meta_1 &amp;  y_pred_int_test_1<br/>pd.DataFrame(final_pred_2_meta_1).value_counts()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rx"><img src="../Images/b0bd0dd3ae25229d0efa490b4584f4ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*Yo3ZqnMCoNaICj2Eaq3QvA.png"/></div></figure><p id="eb2b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">看起来没什么区别。让我们看看所有的指标。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="de50" class="mm mn it pg b gy pk pl l pm pn">metrics_summary(y_test_original, final_pred_2_meta_1)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ry"><img src="../Images/d38733d60d9d9bc079197211340cb60e.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*yDlCEcgcm3vcL2HczvnU6A.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rm"><img src="../Images/f41d718a69879d0d97659b64c245c66a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*d4wj8q9wfXH_c-ncANU1kw.png"/></div></div></figure><p id="20a1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在的结果在精确度和召回率之间更加平衡了。精确度、召回率和f1分数以及其他指标分别从0.93、0.77、0.84提高到0.96、0.81、0.88。</p><p id="de79" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2。第一款车型:logreg，第二款车型:DNN </strong></p><p id="3c1e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于第一个模型仍然是逻辑回归，所以不需要更新数据。我们直接去看模型吧。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="95cd" class="mm mn it pg b gy pk pl l pm pn">#if you receive an error message, try to run the data process again.<br/>model_3_meta_1 = build_model_3( \<br/>    x_train_meta_1_, y_train_meta_1_, \<br/>    x_cv_meta_1, y_cv_meta_1, input_dim=30)</span><span id="15a4" class="mm mn it pg b gy po pl l pm pn">y_pred_prob_test_3_meta_1 = model_3_meta_1.predict(x_test_meta_1)<br/>y_pred_int_test_3_meta_1 = y_pred_prob_test_3_meta_1 &gt; Threshold</span><span id="00ba" class="mm mn it pg b gy po pl l pm pn">pd.DataFrame(y_pred_int_test_3_meta_1).value_counts()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ra"><img src="../Images/f7158f0db83deadc30addba9c03d7c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*PKH9KllR0bipd1akzgiHzA.png"/></div></figure><p id="c104" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">同样，最终结果将是主要预测和次要模型预测的交集。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="458f" class="mm mn it pg b gy pk pl l pm pn"># combine the  meta prediction with primary prediction<br/>final_pred_3_meta_1 = y_pred_int_test_3_meta_1.flatten() &amp; y_pred_int_test_1</span><span id="4ba8" class="mm mn it pg b gy po pl l pm pn">final_pred_3_meta_1.shape</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rz"><img src="../Images/2d4216ec725bb1fc794064135e2fb076.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/format:webp/1*TB_kBQYmQqSobNx8k_B92Q.png"/></div></figure><p id="cd3d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">嗯，我真的开始怀疑最后一步的必要性了。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rr"><img src="../Images/e734d6f0b2656b0caf1afad236243615.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*ofLUc4zK_dFk4a5J6Vnhhg.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rj"><img src="../Images/2700e263e3e93358a4d07288fcf8a149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*TTtQgWyyN-pygKxHVVj9gA.png"/></div></figure><p id="84d1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好吧，精确度确实提高了，但代价是召回率降低了。就像DNN作为第二个模型的堆叠方法一样，但是稍微好一点。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi sa"><img src="../Images/20e20e8f8161dfa8d9d9aaca7f93e1bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*8GV4ARs8XUFq-e0kaf9FVQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">以DNN为第二模型的元标签</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi sb"><img src="../Images/371a6f8cd93f9561db0f9bd2f1511836.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*XR9qnzzo6iOjginGEP2OGQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">将DNN作为第二个模型进行堆叠</p></figure><p id="5317" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 3。第一款:logreg + lightBGM，第二款:DNN </strong></p><p id="087e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">堆叠方法的工作方式让我想知道，如果我将模型1和模型2都作为主要模型，而将模型3作为最终模型，会发生什么？这会改善最终结果吗？让我们这样试一试。</p><p id="186e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因为这一次我们将有额外的功能需要添加到输入中，所以我重新编写了数据处理函数。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="eefa" class="mm mn it pg b gy pk pl l pm pn">def data_meta_2(id, x, y, m_1, m_2):<br/>    '''<br/>    id: the id of new columns<br/>    x: input features<br/>    y: labels<br/>    m_1: model 1, here logreg<br/>    m_2: model 2<br/>    '''<br/>    pred_prob_meta_1 = m_1.predict_proba(x)[:,1]<br/>    pred_prob_meta_1 = pd.Series(pred_prob_meta_1, \<br/>                                 index=x.index,<br/>                                 name=f'pred_{id}_meta')<br/>    pred_int_meta_1 = pred_prob_meta_1 &gt; Threshold<br/>    <br/>    pred_prob_meta_2 = m_2.predict(x)<br/>    #as DNN give 2D prediction that needs to be flatten to 1D for<br/>    #combination<br/>    pred_prob_meta_2 = pd.Series(pred_prob_meta_2.flatten(), \<br/>                                 index=x.index,<br/>                                 name=f'pred_{id+1}_meta')<br/>    pred_int_meta_2 = pred_prob_meta_2 &gt; Threshold<br/>    <br/>    y_meta = pd.Series(y &amp; pred_int_meta_1 &amp; pred_int_meta_2, \<br/>                       name=f'y_train_meta_{id}')<br/>    x_meta = x.join(pred_int_meta_1).join(pred_int_meta_2)<br/>    <br/>    return x_meta, y_meta</span></pre><p id="c3ef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们将该函数应用于训练数据和测试数据。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="5a95" class="mm mn it pg b gy pk pl l pm pn">#meta_1_2: meta data from 1 model and 2 model<br/>x_train_meta_1_2, y_train_meta_1_2 = \<br/>data_meta_2(1, x_train, y_train, model_1, model_2)</span><span id="de49" class="mm mn it pg b gy po pl l pm pn">x_test_meta_1_2, y_test_meta_1_2 = \<br/>data_meta_2(1, x_test_original, y_test_original, model_1, model_2)</span></pre><p id="b179" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">并再次进行归一化。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="5755" class="mm mn it pg b gy pk pl l pm pn">x_train_meta_1_2, x_test_meta_1_2 = \<br/>scale_data(x_train_meta_1_2, x_test_meta_1_2)</span></pre><p id="6b06" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">并分割训练数据以给出验证数据集。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="82de" class="mm mn it pg b gy pk pl l pm pn">x_train_meta_1_2_, x_cv_meta_1_2, y_train_meta_1_2_, y_cv_meta_1_2 = \<br/>train_test_split(x_train_meta_1_2, y_train_meta_1_2,<br/>                test_size=0.25,<br/>                stratify=y_train_meta_1_2,<br/>                random_state=2020)<br/>#stratify mean samplling with the ratio of each class percentage in #all data.</span><span id="01ab" class="mm mn it pg b gy po pl l pm pn">x_train_meta_1_2_.shape, x_cv_meta_1_2.shape, \<br/>y_train_meta_1_2_.shape,  y_cv_meta_1_2.shape</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi sc"><img src="../Images/9760080c1773939d1d0026d123a81893.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*KaA9yc6Dj0VlqLoeKq_g6A.png"/></div></figure><p id="0b9d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好，让我们在模型3上训练我们的数据。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="afea" class="mm mn it pg b gy pk pl l pm pn">model_3_meta_1_2 = build_model_3( \<br/>    x_train_meta_1_2_, y_train_meta_1_2_, \<br/>    x_cv_meta_1_2, y_cv_meta_1_2, input_dim=31)</span><span id="9ddf" class="mm mn it pg b gy po pl l pm pn">y_pred_prob_test_3_meta_1_2 = model_3_meta_1_2.predict(x_test_meta_1_2)<br/>y_pred_int_test_3_meta_1_2 = y_pred_prob_test_3_meta_1_2 &gt; Threshold</span><span id="2644" class="mm mn it pg b gy po pl l pm pn">pd.DataFrame(y_pred_int_test_3_meta_1_2).value_counts()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi sd"><img src="../Images/7c6e368be04bcefccd278c452245956c.png" data-original-src="https://miro.medium.com/v2/resize:fit:230/format:webp/1*TzcfndZKv_O0YsWbZjnm6g.png"/></div></div></figure><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="d676" class="mm mn it pg b gy pk pl l pm pn"># combine the  meta prediction with primary prediction<br/>final_pred_3_meta_1_2 = \<br/>y_pred_int_test_3_meta_1_2.flatten() &amp; \<br/>y_pred_int_test_1 &amp; y_pred_int_test_2<br/>pd.Series(final_pred_3_meta_1_2).value_counts()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi sd"><img src="../Images/7c6e368be04bcefccd278c452245956c.png" data-original-src="https://miro.medium.com/v2/resize:fit:230/format:webp/1*TzcfndZKv_O0YsWbZjnm6g.png"/></div></div></figure><p id="2b78" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">很有可能，最后一步不会改变这个高度不平衡的数据集。让我们看看最后的分数。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="932c" class="mm mn it pg b gy pk pl l pm pn">metrics_summary(y_test_original, y_pred_int_test_3_meta_1_2)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rr"><img src="../Images/578835d9b28659ea86ce89f34d26c7fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*Ah3ouiGIfKYVxWEHBEOJtQ.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rt"><img src="../Images/a22529b43d94b712f073ff88909bf643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*3ixmLcuPrBMYJ6Yl6pD0ww.png"/></div></figure><p id="7d5c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将DNN作为第二个模型或独立的DNN与叠加法相比，所有指标都更好。但不如第二代的lightBGM。</p><p id="840f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 4。第一款:logreg + DNN，第二款:lightBGM </strong></p><p id="af0a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于lightBGM作为第二个模型似乎比其他模型更好，这是我最不想尝试的组合。我们需要再次重新处理数据集。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="67e3" class="mm mn it pg b gy pk pl l pm pn">#meta_1_3: meta data from 1 model and 2 model <br/>#process the train dataset<br/>x_train_meta_1_3, y_train_meta_1_3 = \<br/>data_meta_2(1, x_train, y_train, model_1, model_3)</span><span id="7b8c" class="mm mn it pg b gy po pl l pm pn">#meta_1_3: meta data from 1st model and 3rd model <br/>#process the test dataset<br/>x_test_meta_1_3, y_test_meta_1_3 = \<br/>data_meta_2(1, x_test_original, y_test_original, model_1, model_3)</span><span id="2b60" class="mm mn it pg b gy po pl l pm pn">#normalize the dataset<br/>x_train_meta_1_3, x_test_meta_1_3 = \<br/>scale_data(x_train_meta_1_3, x_test_meta_1_3)</span><span id="b093" class="mm mn it pg b gy po pl l pm pn">#do a train, validation split<br/>x_train_meta_1_3_, x_cv_meta_1_3, y_train_meta_1_3_, y_cv_meta_1_3 = \<br/>train_test_split(x_train_meta_1_3, y_train_meta_1_3,<br/>                test_size=0.25,<br/>                stratify=y_train_meta_1_3,<br/>                random_state=2020)</span></pre><p id="7d6f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们可以开始训练我们的模型。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="120c" class="mm mn it pg b gy pk pl l pm pn">model_2_meta_1_3 = build_model_2( \<br/>    x_train_meta_1_3_, y_train_meta_1_3_, \<br/>    x_cv_meta_1_3, y_cv_meta_1_3)</span></pre><p id="0139" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">并预测测试数据。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="f542" class="mm mn it pg b gy pk pl l pm pn">y_pred_prob_test_2_meta_1_3 = model_2_meta_1_3.predict(x_test_meta_1_3)<br/>y_pred_int_test_2_meta_1_3 = y_pred_prob_test_2_meta_1_3 &gt; Threshold<br/># combine the  meta prediction with primary prediction<br/>final_pred_2_meta_1_3 = \<br/>y_pred_int_test_2_meta_1_3 &amp; \<br/>y_pred_int_test_1 &amp; y_pred_int_test_3.flatten()<br/>pd.Series(final_pred_2_meta_1_3).value_counts(</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi se"><img src="../Images/f8402a9fb8e79ed090506eae4cd47ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*G9rGRdrUQGaY7g8uRm4sWQ.png"/></div></figure><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="8ad8" class="mm mn it pg b gy pk pl l pm pn">metrics_summary(y_test_original, final_pred_2_meta_1_3)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi sf"><img src="../Images/e5ae356dfcdc073ab53309ce8394b78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*HDMwxJUpMsiJCf1-d3tNdA.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi rm"><img src="../Images/e18027097830b9a52b1969cdd230a7fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*g422VmI2KxZuD-G0Wnx5gA.png"/></div></figure><p id="da88" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果比上一个好。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi sg"><img src="../Images/72d0e118f927a3268460d6bfeb847aba.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*RUJToVAE3PRRl9tQhckNAg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用LightBGM作为第二个模型logreg + DNN第一个模型的元标签</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi sh"><img src="../Images/dac4ee99c9ace42e2475bb8a4dcce36a.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*xEUEuCSjDcCnUIapPm87QQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">将DNN作为第二模型logreg + BGM第一模型的元标签</p></figure><h1 id="d731" class="nu mn it bd mo nv pa nx mr ny pb oa mu ki pc kj mx kl pd km na ko pe kp nd oe bi translated">特征重要性</h1><p id="0d48" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">正如我们现在所知，堆叠和元标记有点像向训练数据添加额外特征的特征工程方法。但是这些增加的新功能与原来的功能相比有多重要。感谢scikit-learn中的<code class="fe pp pq pr pg b">feature_importance()</code>函数，我们现在可以实现这个函数来了解这些特性的重要性。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="6247" class="mm mn it pg b gy pk pl l pm pn">def plot_feature_importance(model, X , importance_type = 'split'):<br/>    feature_imp = pd.DataFrame({'Value':model.<br/>                               feature_importance(importance_type),<br/>                               'Feature':X.columns})<br/>    f, ax = plt.subplots(figsize=(40, 30))<br/>    ax.set_title(f'LightGBM Features Importance by \<br/>                 {importance_type}', fontsize=75, fontname="Arial")     <br/>    ax.set_xlabel('Features', fontname="Arial", fontsize=70)<br/>    ax.set_ylabel('Importance', fontname="Arial", fontsize=70)  <br/>    ax.tick_params(labelsize=50)<br/>    <br/>    sns.barplot(x="Value", y="Feature",<br/>                data=feature_imp.sort_values(by="Value", <br/>                ascending=False), ax=ax)</span></pre><p id="f69f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据定义，有两种重要类型，“分割”和“获得”。如果为“分割”，结果将包含该特征在模型中使用的次数。如果为“增益”，则结果包含使用该功能的拆分的总增益。让我们看看他们两个。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="fba7" class="mm mn it pg b gy pk pl l pm pn">plot_feature_importance(model_2_meta_1_3, x_train_meta_1_3_)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi si"><img src="../Images/0f81f52055263942e02ba85ab0faebd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2iXo_MSKigbQmjT2Pd2KOg.png"/></div></div></figure><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="7bbc" class="mm mn it pg b gy pk pl l pm pn">plot_feature_importance(model_2_meta_1_3, x_train_meta_1_3_, 'gain')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi sj"><img src="../Images/0075b1aeb2b20dffaf9cb16a12d3c4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kK9IKtmFH4zku4ZlzBeh_A.png"/></div></div></figure><p id="cd1b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">两种重要性类型都表明元数据远比原始特征重要。我们可以看到，同样的结果也适用于我们的最佳模型。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="92be" class="mm mn it pg b gy pk pl l pm pn">plot_feature_importance(model_2_meta_1, x_train_meta_1_)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi sk"><img src="../Images/bb43d4b056d90070f5315a84683f5bbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oFvy88draoPwg_ahYwJVPA.png"/></div></div></figure><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="6e2b" class="mm mn it pg b gy pk pl l pm pn">plot_feature_importance(model_2_meta_1, x_train_meta_1_, 'gain')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi sj"><img src="../Images/ca082ca104523e276722c4bef1652e09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7KhYAg4u6rOjyVSYbSrbcw.png"/></div></div></figure><h1 id="537f" class="nu mn it bd mo nv pa nx mr ny pb oa mu ki pc kj mx kl pd km na ko pe kp nd oe bi translated">高分的假象</h1><p id="6c2a" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">根据特征重要性值，我们知道，从特征在模型中使用的次数(“分割”)和使用该特征的分割的总增益(“增益”)的角度来看，主模型的预测对第二个模型的结果影响最大。我们还知道，从相关图来看，主要模型的预测与标签(“类别”)具有非常高的相关性(~0.9)。</p><p id="5261" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，我们知道将会有信息从主模型泄漏到第二模型，尽管这是优选的。</p><p id="4dda" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，如果从测试数据到训练数据有轻微的泄漏，泄漏的信息将通过上述方式被放大。像DNN这样的模特真的很擅长走捷径，挑选信息，给高分。</p><p id="0065" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我用代码展示给你看。</p><pre class="ks kt ku kv gt pf pg ph pi aw pj bi"><span id="df8b" class="mm mn it pg b gy pk pl l pm pn">#normalize all the data in one go.<br/>scaler = pp.StandardScaler()<br/>data_x.loc[:, features_to_scale] = scaler.fit_transform(data_x[features_to_scale])</span><span id="1d52" class="mm mn it pg b gy po pl l pm pn">#split training and testing dataset afterwards.<br/>x_train_cv, x_test, y_train_cv, y_test = train_test_split(data_x, data_y, test_size=0.25,stratify=data_y,random_state=2020)</span></pre><p id="faed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果在将数据拆分为训练数据集和测试数据集之前一次性对所有数据进行归一化，则使用的平均值和标准差来自训练数据和测试数据。来自测试数据集的一些信息将与训练数据集共享，然后通过元标记放大。</p><p id="6b84" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用逻辑回归和DNN作为主要模型，使用lightBGM作为第二模型，以下是测试数据的得分。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi sl"><img src="../Images/8b4718b59f118c598a2ce958aa54897c.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*9q7wf7662mziUbpi3luPlg.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi sm"><img src="../Images/57fd2dc694b10026313f589f262391ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*1l581TfiNOrWCPlh6C-VPQ.png"/></div></figure><p id="7b9c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，这些分数是由没有优化和默认设置的基本模型获得的。在数字上看起来不错，但对未知数据没用。</p><h1 id="fab7" class="nu mn it bd mo nv pa nx mr ny pb oa mu ki pc kj mx kl pd km na ko pe kp nd oe bi translated">摘要</h1><p id="ce62" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr of lt lu lv og lx ly lz oh mb mc md im bi translated">总之，最好的结果是使用元标记，lightBGM作为第二模型，logistic回归作为主要模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi sh"><img src="../Images/cc26351ae297195e9adf5774c2f4236e.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*KUSp6LhmJ7-fS1M0BZ1pTA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">将BGM作为第二模型、logreg作为第一模型的元标签</p></figure><p id="fec6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">稍加努力，我们可以取得更好的成绩。额外的提高可能看起来并不引人注目，但在一些比赛中，当第一名和第二名的分数如此接近时，这就是输赢的交易。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="1da0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我想用我最喜欢的关于<strong class="lk jd">指标</strong>的两条定律来结束这篇文章:</p><p id="3ae6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">"当一个度量成为目标时，它就不再是一个好的度量."</p><p id="fa8e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">——<a class="ae lh" href="https://en.wikipedia.org/wiki/Goodhart%27s_law" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">古德哈特定律</strong> </a></p><p id="0be7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">“社会决策中使用的量化社会指标越多，就越容易受到腐败的压力”</p><p id="138b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">——<a class="ae lh" href="https://en.wikipedia.org/wiki/Campbell%27s_law" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">坎贝尔定律</strong> </a></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="8209" class="nu mn it bd mo nv nw nx mr ny nz oa mu ki ob kj mx kl oc km na ko od kp nd oe bi translated">参考</h1><ol class=""><li id="3e90" class="ne nf it lk b ll ng lo nh lr ni lv nj lz nk md nl nm nn no bi translated"><a class="ae lh" href="https://www.quantopian.com/posts/introduction-to-advances-in-financial-machine-learning-by-lopez-de-prado" rel="noopener ugc nofollow" target="_blank">洛佩兹·德·普拉多的《金融机器学习的进展》简介</a></li></ol><p id="2291" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">2.<a class="ae lh" href="https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/" rel="noopener ugc nofollow" target="_blank">衡量业绩:AUC (AUROC) </a></p><p id="d8ab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">3.<a class="ae lh" href="https://blog.floydhub.com/a-pirates-guide-to-accuracy-precision-recall-and-other-scores/" rel="noopener ugc nofollow" target="_blank">盗版者的准确度、精确度、召回率和其他分数指南</a></p><p id="2485" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">4.<a class="ae lh" href="https://sanchom.wordpress.com/tag/average-precision/" rel="noopener ugc nofollow" target="_blank">平均精度</a></p><p id="9dea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">5.<a class="ae lh" href="https://machinelearningmastery.com/calculate-feature-importance-with-python/#:~:text=Feature%20importance%20refers%20to%20techniques,at%20predicting%20a%20target%20variable.&amp;text=The%20role%20of%20feature%20importance%20in%20a%20predictive%20modeling%20problem." rel="noopener ugc nofollow" target="_blank">如何用Python计算特征重要性</a></p></div></div>    
</body>
</html>