<html>
<head>
<title>Transformer-XL Review: Beyond Fixed-Length Contexts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Transformer-XL回顾:超越定长上下文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e?source=collection_archive---------41-----------------------#2020-10-12">https://towardsdatascience.com/transformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e?source=collection_archive---------41-----------------------#2020-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6de4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">“Transformer-XL:超越定长语境的注意力语言模型”述评</h2></div><p id="460f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇论文(“Transformer-XL:固定长度上下文之外的注意力语言模型”)发表在顶级NLP会议之一的ACL 2019上，作者是谷歌AI的研究人员。它提出了Transformer-XL，这是一种新的架构，能够在不破坏时间一致性的情况下，超越固定长度的上下文理解自然语言。它的主要创新是段级递归机制和新颖的位置编码方案。与传统的Transformer模型不同，它可以捕获长期的依赖关系，并解决上下文碎片问题，这是vanilla Transformer的主要局限性。实验表明，Transformer-XL学习依赖的时间比RNNs和vanilla Transformer长得多。Transformer-XL还在大型基准数据集的评估中取得了一流的结果。</p><p id="b4ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">论文链接:<a class="ae lb" href="https://www.aclweb.org/anthology/P19-1285.pdf" rel="noopener ugc nofollow" target="_blank">https://www.aclweb.org/anthology/P19-1285.pdf</a></p><h1 id="f655" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">1.背景</h1><p id="e38b" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">语言建模是自然语言处理中的一个重要课题。人们提出了许多像BERT和ELMo这样的无监督预训练方法。然而，建模长期依赖仍然是一个挑战。递归神经网络(RNNs)，尤其是长短期记忆网络(LSTM)已经成为建模长期依赖的标准解决方案。在LSTMs中引入门控和梯度削波技术提高了对长期依赖性建模的能力，但不足以解决这一挑战。此外，由于梯度消失和爆炸，很难优化用于建模长期依赖性的RNNs。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/5a391425edc729acb93ca5656308421e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8Nj9_UeOZt3Gnun-"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图一。线段长度为4的普通变形金刚。来源:【变压器-XL】【戴等，2019】</p></figure><p id="bc31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">转换器被提出来解决这个问题，它允许单词对之间的直接连接，并且比LSTMs更好地捕捉长期依赖性。作者定义了原始变形金刚和香草变形金刚。然而，转换器是用固定长度的上下文实现的。它将输入分割成段，并在每个段内进行训练(图1)。因此，转换器无法捕获超过预定义上下文长度的长期依赖关系。并且固定长度的片段不考虑句子边界，导致上下文碎片，从而导致低效的优化和性能损失。在评估过程中，它通过在每一步中将输入移动一个位置来一次在一个位置进行一次预测，其中数据段从头开始处理。所以评估程序是昂贵的。</p><p id="fc16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了解决这些限制，作者提出了Transformer-XL。它重用先前片段中的隐藏状态来支持长期依赖并解决上下文碎片。并且它采用相对位置编码方案来避免时间混淆。</p><h1 id="4489" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">2.变压器-XL</h1><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/04386e891fd3bc4db2d73973d21544dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TmITO92QHlhKKiov"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图二。线段长度为4的Transformer-XL型号。来源:<a class="ae lb" href="https://arxiv.org/abs/1901.02860" rel="noopener ugc nofollow" target="_blank"> Transformer-XL </a>【戴等，2019】</p></figure><h1 id="2b1d" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">2.1分段级重现</h1><p id="4bfd" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">在训练期间，为前一段计算的隐藏状态序列是固定的，并被缓存以作为扩展上下文重用(图2)。在每个段中，每个隐藏层接收前一个隐藏层的输出和前一个段的输出。它通过使用来自几个先前片段的上下文信息来增加最大可能的依赖性。尽管解决了上下文碎片问题，但是这种片段级递归机制提高了评估速度，因为它可以前进整个长片段，并且使用来自先前片段的表示而无需重新计算。</p><h1 id="ffd5" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">2.2相对位置编码</h1><p id="7594" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">天真地应用递归引入了另一个技术挑战。也就是说，位置信息是不一致的，并且来自不同片段的标记具有相同的位置编码，这被称为时间混淆。为了应对这一挑战，Transformer-XL采用了新的相对位置编码。位置信息偏差被编码在隐藏状态中，这不同于在初始嵌入中结合偏差的其他方法。使用带有可学习转换的固定嵌入使得它更直观，并且更可推广到更长的序列。相对位置编码使得段级递归成为可能，因此Transformer-XL可以比普通的Transformer模型建立更长期的依赖关系。</p><h1 id="f45a" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">3.实验和结果</h1><p id="73b4" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">作者将Transformer-XL应用于单词级和字符级数据集，包括WikiText-103、text8、enwik8、十亿单词和Penn Treebank，并将其与其他模型进行比较。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/564e2bb9317b59b4d00286a0e615e2c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/0*NCoDTmQ3y1WnYjdQ"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">表1:WikiText-103的结果。来源:【变压器-XL】【戴等，2019】</p></figure><p id="d9db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在WikiText-103数据集上，Transformer-XL达到了18.3的困惑度，相比之下，之前最先进的(SoTA)结果(Baevski &amp; Auli)达到了20.5的困惑度(表1)。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/3feaf22a48a9b376e1138b3fd41ece6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/0*Dl7t0YPhnXT00sye"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">表2:环境观察8的结果。来源:【变压器-XL】T5【戴等，2019】</p></figure><p id="8edb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在enwik8数据集上，12层Transformer-XL实现了1.06比特每字符(bpc)，这与Al-Rfou等人之前的SoTA结果相似..24层Transformer-XL将SoTA bpc从1.06提高到0.99(表2)。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/8ad0dc93e904bb6e499a5f7d27591a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/0*qpIOCuVZ7Wj27jt9"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">表3:文本8的结果。来源:<a class="ae lb" href="https://arxiv.org/abs/1901.02860" rel="noopener ugc nofollow" target="_blank"> Transformer-XL </a>【戴等，2019】</p></figure><p id="e036" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在enwik8上使用相同的超参数时，Transformer-XL将SoTA bpc从1.13降至1.08(表3)。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/1c5335069edc0b9cc6722c456d22ddac.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/0*IQsY5PZl082KTYXz"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">表4:十亿字的结果。来源:【变压器-XL】【戴等，2019】</p></figure><p id="c67e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">十亿字数据集只有短期依赖性，但Transformer-XL也实现了新的SoTA结果，将SoTA困惑度从23.7降低到21.8(表4)。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/b2a4caa4ac552c3cb615fc4cbd61e795.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/0*RNJT3ZSgL4_7364g"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">表5:宾夕法尼亚树库的结果。来源:【变压器-XL】T5【戴等，2019】</p></figure><p id="3376" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在只有100万训练令牌的单词级Penn Treebank数据集上，与没有两步微调的其他模型相比，Transformer-XL将SoTA困惑度从55.3提高到54.52(表5)。这表明Transformer-XL可以很好地在小数据集上推广。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/5165921c2eb28f153aa2c52bd8da31ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/0*5DB9Wqk-uWnt77Sy"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">表6:相对有效上下文长度比较。来源:<a class="ae lb" href="https://arxiv.org/abs/1901.02860" rel="noopener ugc nofollow" target="_blank"> Transformer-XL </a>【戴等，2019】</p></figure><p id="037a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者提出了一个新的度量，相对有效上下文长度(RECL)，它定义在一个模型组上，长上下文的增益通过相对于最佳短上下文模型的相对改进来衡量。RECL中的参数<em class="mv"> r </em>限制了top- <em class="mv"> r </em>硬示例上的比较。如表6所示，Transformer-XL可以模拟比RNN长80%到133%的依赖性，比普通Transformer长291%到447%的依赖性。它表明段级递归和相对位置编码都有助于Transformer-XL的较长RECL。在WikiText-103和十亿字数据集上的消融研究也表明，Transformer-XL优于其他模型，因为它可以对递归和新编码的长期依赖性进行建模。</p><p id="7fee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，由于不需要重新计算，Transformer-XL在评估过程中比普通变压器快1874倍。</p><h1 id="9af5" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">4.结论</h1><p id="5fab" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">Transformer-XL在多个数据集上获得新的SoTA困惑或bpc结果。结合递归和相对位置编码，它可以模拟比RNNs和普通变压器更长期的依赖性，并在评估过程中大大降低计算成本。Transformer-XL在其他领域也很有效，比如生成长文章和改进语言模型预处理方法，比如BERT和ALBERT。</p><h1 id="7bb9" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">5.相关著作</h1><p id="2099" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">你所需要的只是注意力</p><p id="eda1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文提出了Transformer，这是一种完全依赖于注意力机制来模拟输入和输出之间的全局依赖关系的新型模型架构。Transformer模型允许更多的并行化，因此需要更少的培训时间。Transformer在WMT 2014年英语到法语翻译任务上取得了新的SoTA结果。原始变压器是本文介绍的Transformer-XL的基础。</p><p id="1db8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">引文:Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan Gomez、ukasz Kaiser和Illia Polosukhin。2017.你需要的只是关注。在<em class="mv">神经信息处理系统进展</em>中，第5998–6008页。</p><p id="0795" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(2)具有更深自我关注的字符级语言建模</p><p id="5950" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文提出了一种用于角色级建模的深度、非递归的transformer模型。具有因果注意的变压器自我注意层用于处理固定长度的输入和预测即将出现的字符。Al-Rfou等人设计了三个辅助损耗来训练深度变压器网络，其性能优于LSTMs，并在text8和enwik8数据集上获得了新的SoTA结果。但是，它使用固定长度的数据段，因此它无法捕获任何超过预定义上下文长度的长期依赖关系。这一限制促使作者设计trans former-XL来模拟长期依赖性。</p><p id="c25f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">引文:Rami Al-Rfou，Dokook Choe，Noah Constant，Mandy Guo和Llion Jones。2018.具有更深自我关注的字符级语言建模。在<em class="mv">AAAI人工智能会议记录</em>，第3159-3166页。</p><p id="bc59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(3) Bert:用于语言理解的深度双向转换器的预训练</p><p id="1fa4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">介绍了一种新的语言表示模型——来自变压器的双向编码器表示(BERT)。它被设计成用未标记的文本预先训练双向语言表示。然后，预训练的BERT可以通过一个额外的输出层微调到各种任务，并实现SoTA结果。实际上，BERT只是将长文本分成固定长度的较短片段，导致了上下文碎片问题。Transformer-XL解决了上下文碎片问题，因此它可以用于改进BERT，然后在不同类型的任务中获得新的SoTA结果。</p><p id="65e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">引文:雅各布·德夫林、张明蔚、肯顿·李和克里斯蒂娜·图塔诺瓦。2019.Bert:用于语言理解的深度双向转换器的预训练。在<em class="mv">计算语言学协会北美分会2019年会议论文集:人类语言技术</em>，第4171–4186页。</p><p id="19ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(4)具有相对位置表征的自我注意</p><p id="1c81" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文提出了一种将序列元素之间的相对位置表示或距离结合到变压器的自注意机制中的方法。与绝对位置表示相比，相对位置编码可以提高WMT 2014英德数据集上的翻译质量。它启发了Transformer-XL的作者们去衍生一种新形式的相对位置编码。Transformer-XL的新的相对位置编码解决了时间混淆问题，并且在经验上具有更好的通用性。</p><p id="11b5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">引文:彼得·肖、雅各布·乌兹科雷特和阿希什·瓦斯瓦尼。2018.自我注意与相对位置表征。在<em class="mv">计算语言学协会北美分会2018年会议记录:人类语言技术</em>中，第464-468页。</p><p id="305e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(5) ALBERT:一个用于语言表达自我监督学习的Lite BERT</p><p id="2678" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文提出了两种新的技术来减少BERT中的参数，有助于降低存储和训练时间的成本。它还引入了句子顺序预测的自我监督损失，重点是建立句子间的一致性模型。它用比BERT-large更少的参数在不同的基准数据集上建立新的SoTA结果。与BERT一样，它也将长文本分割成固定长度的片段，从而导致潜在的上下文碎片问题。Transformer-XL可用于解决ALBERT中的上下文碎片问题，从而进一步提高其性能。</p><p id="be62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">引文:兰，陈明达，萨巴斯蒂安古德曼，凯文金佩尔，皮尤什夏尔马，拉杜索里科特。2019.ALBERT:一个用于语言表达自我监督学习的Lite BERT。在<em class="mv">国际学术交流会议</em>。</p></div></div>    
</body>
</html>