<html>
<head>
<title>Clustering Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聚类技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/clustering-techniques-hierarchical-and-non-hierarchical-b520b5d6a022?source=collection_archive---------13-----------------------#2020-09-22">https://towardsdatascience.com/clustering-techniques-hierarchical-and-non-hierarchical-b520b5d6a022?source=collection_archive---------13-----------------------#2020-09-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="53aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">聚类属于u <a class="ae ko" href="http://en.wikipedia.xn--org%20%20wiki%20%20unsupervised_learning-s35yka/" rel="noopener ugc nofollow" target="_blank">监督学习</a>技术。在这种技术中，数据没有标记，也没有定义因变量。这种类型的学习通常是为了识别数据中的模式和/或对相似的数据进行分组。</p><p id="c14c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇文章中，详细解释了集群技术的类型，并提供了一个代码遍历。</p><h1 id="eba7" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">什么是集群？</h1><p id="0c4e" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">聚类是一种对相似对象进行分组的方法。聚类的目的是从异类的观察中创建同类的组。假设数据来自多个群体，例如，可能有不同行业的人出于不同目的向银行申请贷款。如果这个人是学生，他/她可以申请教育贷款，想买房的人可以申请住房贷款等等。聚类有助于识别相似的群体，更好地满足需求。</p><h1 id="3a17" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">为什么要集群？</h1><p id="8c55" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">聚类是一种基于距离的算法。聚类的目的是最小化类内距离和最大化类间距离。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/724838bba90b79105639118e430fba74.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/0*D4B9P8ynyXNq69QU.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">未分类的数据(图片由作者提供)</p></figure><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi me"><img src="../Images/84c939996d071d1ebca8afad44022b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/0*7rzDVkQGuwYLWgkC.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">聚类数据(按作者分类的图片)</p></figure><p id="7587" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">聚类作为一种工具可以用来深入了解数据。通过可视化数据可以获得大量的信息。聚类的输出也可以用作其他算法的预处理步骤。这项技术有几个广泛使用的用例，其中一些重要的是市场细分、客户细分和图像处理。</p><p id="1723" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在继续之前，让我们了解一下集群的核心。</p><h1 id="8601" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">距离的度量</h1><p id="2672" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">聚类完全是关于两点之间的距离和两个聚类之间的距离。距离不能为负。对于聚类问题，该算法使用一些常见的距离度量。</p><p id="ba23" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="mf">欧氏距离</em> </strong></p><p id="b081" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是算法使用的默认距离。最好解释为两点之间的距离。如果要测量两点p和q之间的距离，则欧几里德距离为</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/fe148ce2ca4b3137115c9a24325fc20b.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/0*qyHDU1KV5QU7CG-N.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">欧几里德距离(图片由作者提供)</p></figure><p id="33f9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="mf">曼哈顿距离</em> </strong></p><p id="fef4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它是沿轴以垂直角度计算的两点之间的距离。它也被称为出租车距离，因为这代表了曼哈顿市的车辆如何在街道以直角相交的地方行驶。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/0e6b7c963027ddc5c5b4147bc1f390e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/0*IbtDMT0RDA1jdnSC.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">曼哈顿距离(图片作者提供)</p></figure><p id="eb9b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="mf">闵可夫斯基距离</em> </strong></p><p id="d3af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在n维空间中，两点之间的距离称为闵可夫斯基距离。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/3f1101a383e5563710c5c5f7fa165b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/0*Am4jcbYmmu-HsURT.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">闵可夫斯基距离(图片作者提供)</p></figure><p id="11dc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">欧几里德距离和曼哈顿距离的推广是，如果p的值是2，则它成为欧几里德距离，如果p的值是1，则它成为曼哈顿距离。</p><h1 id="f549" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">聚类的类型</h1><p id="c721" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">有两种主要类型的聚类技术</p><ol class=""><li id="413c" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">等级或聚集</li><li id="dd49" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">k均值</li></ol><p id="c82c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看每种类型以及代码走查</p><h1 id="5200" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">分层聚类</h1><p id="1fd4" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">这是一种自下而上的方法。基于记录之间的距离以及聚类之间的距离，数据集中的记录被顺序分组以形成聚类。这是一个逐步实现这个方法的方法</p><ol class=""><li id="1b87" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">从n个群集开始，其中每行被视为一个群集</li><li id="fd63" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">使用基于距离的方法，将彼此最接近的两个记录合并到一个聚类中。在图3中，对于给定的五个记录，假设A和C在距离上最接近，它们形成一个聚类，同样B和E形成另一个聚类，等等</li></ol><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/37b8f6451bc88a596214772fe79bd498.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/0*PAd77av6baDB63-d.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">两个最接近记录的聚类(按作者分类的图片)</p></figure><p id="4d0e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.在每一步，两个最接近的聚类将被合并。要么将单个记录(singleton)添加到现有分类中，要么合并两个分类。在至少一个多元素集群形成后，需要为单个元素和一组观察值计算距离的场景，这就是<strong class="js iu">链接</strong>的概念出现的地方。有五种主要类型的链接。通过使用下面的一个概念，聚类发生了-</p><ul class=""><li id="e6c8" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn my mp mq mr bi translated"><strong class="js iu"> <em class="mf">单联动:</em> </strong>它是两个集群中任意两点之间的最短距离</li><li id="8a7e" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn my mp mq mr bi translated"><strong class="js iu"> <em class="mf">完全联动:</em> </strong>与单联动相反。这是两个集群中任意两点之间的最长距离</li><li id="9bbd" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn my mp mq mr bi translated"><strong class="js iu"> <em class="mf">平均连锁度:</em> </strong>一个聚类中的每一点到另一个聚类中的每一点的平均距离</li><li id="932e" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn my mp mq mr bi translated"><strong class="js iu"> <em class="mf">质心联动:</em> </strong>一个簇的中心点到另一个簇的中心点的距离</li><li id="30e5" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn my mp mq mr bi translated"><strong class="js iu"> <em class="mf">沃德连锁:</em> </strong>平均法和质心法的结合。通过确定分类的中心点和观察值与中心的距离来计算分类内方差。当试图合并两个聚类时，在聚类之间找到方差，并且与另一个组合相比方差较小的聚类被合并。</li></ul><p id="bbb9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">需要注意的一点是，每种链接方法都会产生一个独特的结果。当这些方法中的每一种应用于相同的数据集时，它可能被不同地聚类。</p><p id="4f9f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.重复这些步骤，直到出现一个包含所有记录的聚类</p><h1 id="2da2" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">形象化</h1><p id="0fc4" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">为了形象化聚类，有一个叫做<strong class="js iu">树状图的概念。</strong>树状图是总结聚类过程的树形图。记录在x轴上。相似的记录由直线连接，直线的垂直长度反映了记录之间的距离。身高差距越大，差异越大。展示了一个样本树状图</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/44114995ee0b6fff24b554250ec22718.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/0*du6nkjaX9coEWMS0.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">树状图(图片由作者提供)</p></figure><h2 id="50b8" class="na kq it bd kr nb nc dn kv nd ne dp kz kb nf ng ld kf nh ni lh kj nj nk ll nl bi translated">分层聚类代码演练</h2><p id="cf27" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">分层聚类的代码是使用jupyter notebook用Python 3x编写的。让我们从导入必要的库开始。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="a516" class="na kq it nn b gy nr ns l nt nu">#Import the necessary libraries</span><span id="206d" class="na kq it nn b gy nv ns l nt nu">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>from scipy.cluster.hierarchy import linkage, dendrogram, fcluster</span></pre><p id="3a26" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，加载数据集。这里使用了星巴克食物菜单上的一个数据集。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="c7be" class="na kq it nn b gy nr ns l nt nu">#Read the dataset</span><span id="5c05" class="na kq it nn b gy nv ns l nt nu">df = pd.read_csv('starbucks_menu.csv')</span><span id="f5dd" class="na kq it nn b gy nv ns l nt nu">#Look at the top 5 rows </span><span id="59f0" class="na kq it nn b gy nv ns l nt nu">df.head()</span></pre><p id="accb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">进行必要的探索性数据分析，如查看描述性统计数据，检查空值和重复值。进行单变量和双变量分析，进行异常值处理(如果有的话)。由于这是一种基于距离的算法，因此有必要在适用的情况下执行标准化，以便所有变量都没有任何测量单位。这使得模型能够以最佳状态运行。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="67ab" class="na kq it nn b gy nr ns l nt nu">from scipy.stats import zscore<br/>df.iloc[:,1:6] = df.iloc[:,1:6].apply(zscore)</span><span id="b951" class="na kq it nn b gy nv ns l nt nu">#Check the head after scaling</span><span id="fb2a" class="na kq it nn b gy nv ns l nt nu">df.head()</span></pre><p id="b0be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦数据准备好了，让我们开始构建模型。需要分配一个标签列表，它是分类变量的唯一值的列表。这里，标签列表是从<strong class="js iu">食物</strong>变量中创建的。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="8945" class="na kq it nn b gy nr ns l nt nu">#Before clustering, setup label list from the food variable</span><span id="e1be" class="na kq it nn b gy nv ns l nt nu">labelList = list(df.Food.unique())<br/>labelList</span></pre><p id="1a3d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下一步是形成一个链接，将单例集群和另一个集群连接起来。在这种情况下，<strong class="js iu">沃德的</strong>法是首选。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="8ff8" class="na kq it nn b gy nr ns l nt nu">#Create linkage method using Ward's method</span><span id="2ce3" class="na kq it nn b gy nv ns l nt nu">link_method = linkage(df.iloc[:,1:6], method = 'ward')</span></pre><p id="b52c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">借助树状图可视化聚类。在这种情况下，通过指定显示倒数第二个和倒数第二个聚类的p值，得到一个截断的树状图。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="2bf6" class="na kq it nn b gy nr ns l nt nu">#Generate the dendrogram</span><span id="b53c" class="na kq it nn b gy nv ns l nt nu">dend = dendrogram(link_method,<br/>                  labels = labelList,<br/>                  truncate_mode='lastp', <br/>                  p=10)</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/256454e11e7ad6b4b179b0a1c4430288.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/0*wyghMFTAdkvXkTIQ.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">截断的树状图(图片由作者提供)</p></figure><p id="6d71" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦创建了树状图，就需要切割树来确定最佳的聚类数。有两种方法可以做到这一点(如图所示)。在这种情况下，选择3个集群。可将聚类作为新列附加到数据框中，以获得更多信息。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="a912" class="na kq it nn b gy nr ns l nt nu">#Method 1: criterion = 'maxclust' where a cut is defined based on the number of clusters</span><span id="f4f2" class="na kq it nn b gy nv ns l nt nu">clusters = fcluster(link_method, 3, criterion='maxclust') <br/>clusters</span><span id="8f5a" class="na kq it nn b gy nv ns l nt nu">#Method 2: criterion='distance' where a cut is defined based on distance in the y-axis</span><span id="d4bf" class="na kq it nn b gy nv ns l nt nu">#clusters = fcluster(link_method, 800, criterion='distance')</span><span id="4393" class="na kq it nn b gy nv ns l nt nu">#Apply the clusters back to the dataset</span><span id="480a" class="na kq it nn b gy nv ns l nt nu">df['HCluster'] = clusters<br/>df.head()</span></pre><p id="4a3b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后一步是进行聚类分析，从算法中提取信息和见解，以帮助做出有效的决策。聚类分析是通过对聚类的平均值进行分组并基于频率进行排序来完成的。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="610e" class="na kq it nn b gy nr ns l nt nu">aggdata=df.iloc[:,1:8].groupby('HCluster').mean()<br/>aggdata['Frequency']=df.HCluster.value_counts().sort_index()<br/>aggdata</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a3580be5fffb12c07ddf75dd8d02ef3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/0*YTVYFzOfbIG4BKe8.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">聚类分析(作者图片)</p></figure><p id="19b8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">快速了解一下，第一类食物通常热量较低，因此宏量营养素含量较低。第二类食物的卡路里含量最高，因此宏量营养素含量也较高，第一类和第二类食物的卡路里含量居中，第三类食物的卡路里含量和宏量营养素含量较高。总的来说，简而言之，这种模式聚集得很好。</p><p id="64d3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们继续下一个方法</p><h1 id="0c27" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">k均值聚类</h1><p id="0ee2" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">K-Means是一种无层次的方法。这个想法是事先指定集群的数量。根据分类的数量，每个记录根据与每个分类的距离被分配到分类中。当数据集很大时，这种方法是首选的。单词<strong class="js iu">表示k-means中的</strong>是指数据的平均，也称为寻找质心。这是一个循序渐进的方法</p><ol class=""><li id="9c65" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">事先指定k值</li><li id="0159" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">将每个记录分配给到质心的距离最小的聚类。默认情况下，K均值使用欧几里德距离</li><li id="d5bd" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">重新计算新形成的簇的质心。基于距离，一些数据点可能会移动。</li><li id="26ab" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">重新分配可以在迭代的基础上发生，并且形成新的质心。这个过程将停止，直到没有从一个集群到另一个集群的观察跳跃。</li><li id="770c" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">如果有任何重新分配，请返回步骤3并继续这些步骤。如果没有，则集群被最终确定。</li></ol><p id="63dc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">尽管我们已经事先确定了聚类的数量，但这并不总是正确的，因此有必要确定最优的聚类数量。没有确定集群数量的可靠解决方案，但是有一个通用的方法。对于k的每个值，可以识别平方和(WSS)值。不能选择单个聚类，因此重要是找到k值，在该k值之后WSS值没有显著差异。为了提高效率，可以绘制一个肘形图，y轴为WSS分数，x轴为聚类数，以直观显示最优聚类数。</p><p id="421b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有一种方法可以了解模型的表现。这是通过检查两个度量来完成的，即<strong class="js iu">轮廓宽度</strong>和<strong class="js iu">轮廓分数。</strong>这有助于我们根据距离标准分析映射到聚类的每个观察值是否正确。轮廓宽度计算如下</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d9df30eaf3257b22b4565823b7cf64d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/0*23cRjD10mo32TaHI.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">轮廓宽度公式(图片由作者提供)</p></figure><p id="bc0a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中b是观测值和相邻聚类的质心之间的距离，a是观测值和自身聚类的质心之间的距离。</p><p id="c9e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">轮廓宽度的值可以在-1到1的范围内。如果轮廓宽度的值为正，则观测值到当前聚类的映射是正确的。当a &gt; b时，轮廓宽度将返回负值。所有轮廓宽度的平均值称为轮廓得分。如果最终得分是正值，并且接近+1，则平均来说，聚类被很好地分开。如果接近0，说明分离得不够好。如果是负值，则该模型在聚类中犯了一个错误。</p><h2 id="53b0" class="na kq it bd kr nb nc dn kv nd ne dp kz kb nf ng ld kf nh ni lh kj nj nk ll nl bi translated">k-均值聚类代码遍历</h2><p id="e098" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">让我们从导入必要的库开始</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="6731" class="na kq it nn b gy nr ns l nt nu">#Import the necessary libraries</span><span id="4441" class="na kq it nn b gy nv ns l nt nu">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>from sklearn.cluster import KMeans<br/>from sklearn.metrics import silhouette_score</span></pre><p id="7375" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，加载数据集。这里使用了用于分层聚类的相同数据集。</p><p id="2c9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">进行必要的探索性数据分析，如查看描述性统计数据，检查空值和重复值。进行单变量和双变量分析，进行异常值处理(如果有的话)。K-means聚类需要扩展。这样做是为了使所有的变量都没有任何测量单位。这使得模型能够以最佳状态运行。在这种情况下，使用StandardScaler方法。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="cd25" class="na kq it nn b gy nr ns l nt nu">#Importing the standard scaler module and applying it on continuous variables</span><span id="ff8c" class="na kq it nn b gy nv ns l nt nu">from sklearn.preprocessing import StandardScaler <br/>X = StandardScaler()<br/>scaled_df = X.fit_transform(df.iloc[:,1:6])<br/>scaled_df</span></pre><p id="ce7d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下一步是调用KMeans方法，预先定义集群的数量。然后将缩放后的数据集拟合到模型中。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="a4fd" class="na kq it nn b gy nr ns l nt nu"># Create K Means cluster and store the result in the object k_means</span><span id="eb81" class="na kq it nn b gy nv ns l nt nu">k_means = KMeans(n_clusters=2)</span><span id="3821" class="na kq it nn b gy nv ns l nt nu"># Fit K means on the scaled_df</span><span id="807f" class="na kq it nn b gy nv ns l nt nu">k_means.fit(scaled_df)</span><span id="5a52" class="na kq it nn b gy nv ns l nt nu"># Get the labels</span><span id="bb67" class="na kq it nn b gy nv ns l nt nu">k_means.labels_</span></pre><p id="dcf1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在是时候通过分析给定k范围内的平方和(WSS)值来找到最佳聚类数了</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="1c82" class="na kq it nn b gy nr ns l nt nu">#To determine the optimum number of clusters, check the wss score for a given range of k</span><span id="18ee" class="na kq it nn b gy nv ns l nt nu">wss =[] <br/>for i in range(1,11):<br/>    KM = KMeans(n_clusters=i)<br/>    KM.fit(scaled_df)<br/>    wss.append(KM.inertia_)<br/>    <br/>wss</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/e59c96d3a8840701e5be7de6b78b0fbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/0*2vdH4t5_klmh-Pte.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">WSS得分(图片由作者提供)</p></figure><p id="c18c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可以看出，在k=2之后，WSS分数有所下降，因此让我们关注k=3。同样的情况也可以用肘图来显示</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="91f6" class="na kq it nn b gy nr ns l nt nu">#Draw the elbow plot</span><span id="6b45" class="na kq it nn b gy nv ns l nt nu">plt.plot(range(1,11), wss, marker = '*')</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0d3b80a843376d808965e0b9196280d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/0*7gwVDwANYsUrA976.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">肘图(图片由作者提供)</p></figure><p id="d373" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">决定聚类数量的另一个帮助可以是轮廓分数的值。如前所述，分数越高，聚类越好。让我们检查分数。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="ba3f" class="na kq it nn b gy nr ns l nt nu">#Checking for n-clusters=3</span><span id="a0b1" class="na kq it nn b gy nv ns l nt nu">k_means_three = KMeans(n_clusters = 3)<br/>k_means_three.fit(scaled_df)<br/>print('WSS for K=3:', k_means_three.inertia_)<br/>labels_three = k_means_three.labels_<br/>print(labels_three)</span><span id="a0f7" class="na kq it nn b gy nv ns l nt nu">#Calculating silhouette_score for k=3</span><span id="9ab5" class="na kq it nn b gy nv ns l nt nu">print(silhouette_score(scaled_df, labels_three))</span></pre><p id="e21a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">k=3的WSS是261.67，并且这些标签的轮廓分数是0.3054。因为分数是正的，所以这是发生了良好聚类的标志。</p><p id="0d70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后一步是进行聚类分析，以了解聚类是如何发生的，并获得更多的见解。</p><pre class="lt lu lv lw gt nm nn no np aw nq bi"><span id="30e8" class="na kq it nn b gy nr ns l nt nu">clust_profile=df.iloc[:,1:8].groupby('KMCluster').mean()<br/>clust_profile['KMFrequency']=df.KMCluster.value_counts().sort_index()<br/>clust_profile</span></pre><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/c05162938ef9558ae89a542981c395ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/0*M3ZRsskjfASeqLuD.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">KMeans的聚类分析(图片由作者提供)</p></figure><p id="6cbe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就像分层聚类一样，这三个聚类表示三个级别的食物，它们具有不同的热量和宏观营养范围。</p><ul class=""><li id="84e4" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn my mp mq mr bi translated">数据集来源:<a class="ae ko" href="https://www.kaggle.com/starbucks/starbucks-menu?select=starbucks-menu-nutrition-food.csv" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/starbucks/starbucks-menu?select =星巴克-菜单-营养-食物. csv </a></li><li id="230f" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn my mp mq mr bi translated">要查看jupyter笔记本文件，点击<a class="ae ko" href="https://github.com/bharathwaj1607/MachineLearningModels/tree/master/01-Clustering%20Techniques" rel="noopener ugc nofollow" target="_blank">这里的<strong class="js iu"/>这里的</a></li></ul></div></div>    
</body>
</html>