<html>
<head>
<title>Invitation to All Aspiring Reinforcement Learning Practitioner</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">邀请所有有抱负的强化学习实践者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/invitation-to-all-aspiring-reinforcement-learning-practitioner-5f87384cee67?source=collection_archive---------35-----------------------#2020-09-29">https://towardsdatascience.com/invitation-to-all-aspiring-reinforcement-learning-practitioner-5f87384cee67?source=collection_archive---------35-----------------------#2020-09-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="84c3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">感谢AWS和<a class="ae kf" href="https://jakartamachinelearning.com/" rel="noopener ugc nofollow" target="_blank">雅加达机器学习</a></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/24b7c6d7a1f8ae89aa9472438fcd57c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_6CQidxhKuYwUGI4"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><a class="ae kf" href="https://unsplash.com/@drmakete?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> drmakete实验室</a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><blockquote class="kw kx ky"><p id="d388" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ir">公开邀请所有有志于强化学习(RL)的从业者在未来3个月内与我一起学习RL</strong></p></blockquote><p id="2118" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">不久前，我参加了由AWS和Jakarta Machine Learning (JML)举办的RL训练营。我们这些参与者将会得到AWS经验丰富的代表们的悉心指导。在接下来的3个月里，导师们将指导和介绍我们走'<strong class="lc ir">正道</strong>'，学习RL。不仅学习理论，我们还将学习如何在实际应用中应用它！</p><p id="9f8e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">是不是很有意思？？让你更感兴趣的是:</p><blockquote class="kw kx ky"><p id="92ea" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我会把我在这次训练营中学到的东西分享给大家！！</p></blockquote><p id="2e7f" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">所以，深呼吸，泡一杯茶，我邀请你和我一起加入这个激动人心的RL之旅！</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi lz"><img src="../Images/ba5cd7ee87ad435cf39d7896802b6f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*95wFcNzhweFajiEW"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">照片由<a class="ae kf" href="https://unsplash.com/@loverna?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Loverna在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的旅程</a>拍摄</p></figure></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><p id="3afc" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">上周是第一次会议，这基本上是一个关于项目的介绍会议，参与者的概况，我们将学到什么，以及一些关于RL的介绍。当我听到我们在接下来的3个月里将要学习的内容时，我感到无比兴奋，因为我不仅仅是为了自己而学习，我还会与你们分享我的知识！</p><p id="a268" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">首先，我将向您介绍强化学习。尽情享受吧！</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="fc78" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">什么是强化学习？</h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a4cb094aa27bc96aed2e28c64b26a708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*gvheQAUBwt2amaTaxOeTTQ.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">AI领域[图片由作者提供]</p></figure><p id="8c5c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">强化学习(RL)是机器学习的一个子集，它使一个<em class="lb">代理</em>能够在一个特定的<em class="lb">环境</em>中通过<em class="lb">行动</em>的结果进行学习，目的是<em class="lb"> </em>最大化<em class="lb">累积回报</em>。</p><p id="b5a3" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">为了更好地理解，假设你有一只新的宠物狗，名叫布朗，你想教它一些技巧。怎么会？当他做对的时候你可以奖励他，当他做错的时候你什么都不给。当然，你必须这样做几次，直到他意识到他应该做什么。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi na"><img src="../Images/127cbd3d31b808ea4862144bedfcb5e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*glVSuK_VC2vKAKXH"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">安娜·杜德科娃在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="d15f" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">另一个关于RL如何应用于现实生活的有趣例子是YAWScience做的一个实验。在这个实验中，如果小鸡成功地钉住了粉红色的纸而不是其他颜色的纸，它就会得到奖励。结果是，小鸡成功地锁定了正确的一个，尽管在每次迭代中所有的纸都被打乱了！</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nb nc l"/></div></figure></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><p id="9040" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">像其他科学领域一样，我们必须准备一个强大的基础，这样我们才能轻松地学习更复杂的领域。要掌握RL，我们必须了解它的先驱:<em class="lb">(MDP)</em>。在我们了解MDP之前，我们必须了解:</p><ol class=""><li id="3ab1" class="nd ne iq lc b ld le lg lh lw nf lx ng ly nh lv ni nj nk nl bi translated">马尔可夫性质</li><li id="7e35" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated">马尔可夫过程(马尔可夫链)</li><li id="75f3" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated">马尔可夫奖励过程</li></ol><p id="92ae" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">正如我在开始所说，这个训练营提供了学习RL的“正确途径”。导师告诉我们，MDP概念对于更好地理解RL非常重要，并鼓励我们自己学习。</p><p id="2180" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我试图在训练营之外自学这个概念，我做到了！就像我一样，我也相信你可以自学！但我不会让你迷失在野外。我想确保你和我总是在同一页上。所以，我会分享一些我看过/读过的好资源给你:</p><ol class=""><li id="23e7" class="nd ne iq lc b ld le lg lh lw nf lx ng ly nh lv ni nj nk nl bi translated"><a class="ae kf" href="https://www.youtube.com/watch?v=my207WNoeyA" rel="noopener ugc nofollow" target="_blank">马尔可夫决策过程(MDPs)——构建强化学习问题</a></li><li id="0065" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated"><a class="ae kf" href="https://www.youtube.com/watch?v=lfHX2hHRMVQ" rel="noopener ugc nofollow" target="_blank">David Silver的RL课程——第2讲:马尔可夫决策过程</a></li><li id="51a7" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690">强化学习揭秘:马尔可夫决策过程(第一部分)</a></li><li id="60ae" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/reinforcement-learning-demystified-markov-decision-processes-part-2-b209e8617c5a">强化学习揭秘:马尔可夫决策过程(第二部分)</a></li></ol></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="3bb5" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">RL中的重要术语</h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nr"><img src="../Images/c130a58214d5eb47852641b8bb58cf4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bSDrANUmkjGL84oRhQvUSg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">RL模型中涉及的元素。[图片由作者提供]</p></figure><p id="38d8" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">您需要了解几个重要术语:</p><ol class=""><li id="b475" class="nd ne iq lc b ld le lg lh lw nf lx ng ly nh lv ni nj nk nl bi translated"><strong class="lc ir">代理人。</strong>需要根据自身行为和经验接受“教育”的决策者。由策略和学习算法组成。</li><li id="addb" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated"><strong class="lc ir">环境。</strong>代理人的物理世界。基本上，它只不过是一个模拟。</li><li id="4a64" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated"><strong class="lc ir">状态。</strong>代理当前所处的“情况”。这包括过去、现在和未来的状态。</li><li id="6688" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated"><strong class="lc ir">行动。</strong>代理人在环境内部的活动。</li><li id="82fe" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated"><strong class="lc ir">奖励。</strong>环境的反馈。可以是正的，也可以是负的。</li><li id="098a" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated"><strong class="lc ir">政策。</strong>负责根据对环境的观察选择代理将采取什么行动的单元。</li><li id="63b1" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated"><strong class="lc ir">插曲。</strong>以终止状态结束的状态序列。</li><li id="376c" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated"><strong class="lc ir">奖励功能。</strong>激励特定代理行为的功能，是RL的核心。</li></ol><p id="f65f" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">类似于监督机器学习设置中的偏差-方差权衡，RL中也有权衡。这叫做<strong class="lc ir">探索与开发的权衡</strong>。探索是指代理人试图收集一些新信息，并期望这些信息比当前信息更有用。而利用是指代理根据已知信息做出最佳决策。</p><p id="00c5" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">为了更好地理解，假设你和你的朋友亚当计划明天一起吃午饭。考虑这两种情况:</p><ol class=""><li id="811a" class="nd ne iq lc b ld le lg lh lw nf lx ng ly nh lv ni nj nk nl bi translated">“嗨，亚当，我听说这个地区新开了一家餐馆。我们为什么不试试去那家餐馆吃午饭呢？”</li><li id="20bd" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated">“嘿，亚当，我知道我们明天应该去哪里吃午饭了。我们为什么不在上周去过的那家餐馆吃午饭呢？”</li></ol><p id="4eb5" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">第一个场景就是我们所说的<strong class="lc ir">探索</strong>。你和亚当不知道它会是什么味道，它可能比你最喜欢的餐馆差，但也可能好得多。而在第二种情况下，你知道你们两个都不会对味道有问题，而且你们很有可能会喜欢明天的午餐。这个场景类似于<strong class="lc ir">开发</strong>阶段。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="2d24" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">它是如何工作的？</h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nr"><img src="../Images/c130a58214d5eb47852641b8bb58cf4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bSDrANUmkjGL84oRhQvUSg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">RL工作原理的基本概念。[图片由作者提供]</p></figure><p id="eab8" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">现在，我将解释RL在高层是如何工作的。不要担心，因为在以后的帖子中，我们将回到这个问题并了解细节！</p><p id="4503" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">下标<em class="lb"> t </em>指的是我们当前所处的时间步长。</p><p id="871a" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">在第一个时间步(<em class="lb"> t=0 </em>)，代理接收环境的状态作为输入。基于这些输入，它将决定采取什么行动。一旦作出决定，行动就转移回环境中。</p><p id="fa84" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">然后，时间步长递增(t=1 ),环境产生新的状态。除此之外，环境还会输出一个奖励，稍后会给代理人。</p><p id="48f7" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">最后，将当前状态和奖励都提供给代理。代理因其之前的行为而获得奖励(可以是正的或负的)。</p><p id="0242" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">对于未来的时间步长，重复相同的过程。</p><blockquote class="kw kx ky"><p id="4eae" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是……当这个循环结束时？</p></blockquote><p id="f34e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">这取决于你的任务类型。一般来说，有两种不同的类型:</p><ol class=""><li id="5dc8" class="nd ne iq lc b ld le lg lh lw nf lx ng ly nh lv ni nj nk nl bi translated">阶段性任务是具有终结状态的任务。换句话说，这种任务是有结局的。比如下象棋。</li><li id="c562" class="nd ne iq lc b ld nm lg nn lw no lx np ly nq lv ni nj nk nl bi translated"><strong class="lc ir">继续任务</strong>是没有终止状态或永不结束的任务。例如，个人辅助机器人。</li></ol><p id="9b3b" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">因此，如果您正在处理一个临时任务，那么循环将会重复，直到到达终端状态。如果您正在处理一个连续的任务，那么循环将永远不会结束。</p><h1 id="b2b7" class="mh mi iq bd mj mk ns mm mn mo nt mq mr jw nu jx mt jz nv ka mv kc nw kd mx my bi translated">最后的话</h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nx"><img src="../Images/49fcdd65a9da97736d2370e0c7a53031.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4kZPZnVVVKxkxMxE"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">由<a class="ae kf" href="https://unsplash.com/@ravipinisetti?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Ravi Pinisetti </a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="90ce" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">恭喜你坚持到了这一步！</p><p id="d9de" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">到现在为止，你应该知道什么是强化学习，RL中有哪些重要的术语，它在高层是如何工作的。如果你听从了我的建议，那么你也应该已经了解了马尔可夫决策过程的概念。对于那些没有读过这本书的人，我鼓励你花些时间来学习这个概念。</p><p id="d933" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">记住，这只是我们学习RL旅程的开始！我还有很多材料要和大家分享。所以，如果你喜欢这些内容，并且想在接下来的3个月里继续和我一起学习，请关注我的媒体账号，以获得关于我未来帖子的通知！</p><p id="bb87" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">在下一集，我们将了解更多关于RL在自动驾驶赛车中的应用。<a class="ae kf" rel="noopener" target="_blank" href="/reinforcement-learning-in-autonomous-race-car-c25822def9f8">来看看</a>！</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="0eae" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">关于作者</h1><p id="ebb1" class="pw-post-body-paragraph kz la iq lc b ld ny jr lf lg nz ju li lw oa ll lm lx ob lp lq ly oc lt lu lv ij bi translated">Louis Owen是一名数据科学爱好者，他总是渴望获得新知识。他获得了最后一年的全额奖学金，在印度尼西亚最好的大学之一<a class="ae kf" href="https://www.itb.ac.id/" rel="noopener ugc nofollow" target="_blank"><em class="lb">Institut Teknologi Bandung</em></a>攻读数学专业。最近，2020年7月，他刚刚以优异的成绩从他的研究中毕业。</p><p id="ca56" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">Louis曾在多个行业领域担任分析/机器学习实习生，包括OTA(<a class="ae kf" href="https://www.linkedin.com/company/traveloka-com/" rel="noopener ugc nofollow" target="_blank"><em class="lb">Traveloka</em></a>)、电子商务(<a class="ae kf" href="https://www.linkedin.com/company/pt--tokopedia/" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> Tokopedia </em> </a>)、fin tech(<a class="ae kf" href="https://www.linkedin.com/company/doitglotech/" rel="noopener ugc nofollow" target="_blank"><em class="lb">Do-it</em></a>)、智慧城市App ( <a class="ae kf" href="https://www.linkedin.com/company/qluesmartcity/" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> Qlue智慧城市</em> </a>)，目前在<a class="ae kf" href="https://www.linkedin.com/company/the-world-bank/" rel="noopener ugc nofollow" target="_blank"> <em class="lb">世界银行</em> </a>担任数据科学顾问</p><p id="6fae" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">查看路易斯的网站以了解更多关于他的信息！最后，如果您有任何疑问或需要讨论的话题，请通过<a class="ae kf" href="https://www.linkedin.com/in/louisowen/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系Louis。</p></div></div>    
</body>
</html>