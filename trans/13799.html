<html>
<head>
<title>Machine Learning Basics: Random Forest Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习基础:随机森林分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-basics-random-forest-classification-499279bac51e?source=collection_archive---------19-----------------------#2020-09-22">https://towardsdatascience.com/machine-learning-basics-random-forest-classification-499279bac51e?source=collection_archive---------19-----------------------#2020-09-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1e19" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对数据集执行随机森林算法并可视化结果！</h2></div><h2 id="a136" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">随机森林分类综述</h2><p id="85ac" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">随机森林也是一种基于“树”的算法，它使用多个决策树的质量特征来做出决策。因此，它可以被称为一个<em class="lx">‘森林’</em>的树木，因此得名“随机森林”。术语“<em class="lx">随机</em>是因为这个算法是一个<strong class="lg iu"><em class="lx">‘随机生成的决策树’</em></strong>的森林。</p><p id="80f7" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">随机森林算法是对现有决策树算法的改进，现有决策树算法存在一个主要问题<em class="lx">“过拟合”</em>。与决策树算法相比，它被认为更快更准确。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi md"><img src="../Images/c16756513293b7ab200317a865cc21a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/0*KhIFjUN6V8p8WqL1.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">随机森林算法(<a class="ae mp" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="2ea1" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">它结合了多个决策树的结果，并根据结果对输出进行分类。让我们用这个算法实际求解一个数据集。</p><h2 id="c875" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">问题分析</h2><p id="5c1b" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">在这个随机森林分类模型的实现中，我们将使用一个社交网络广告数据集，我已经在构建SVM分类器时使用过它。它由三列组成。前两列是自变量，即'<strong class="lg iu"> <em class="lx">【年龄】'</em> </strong>和'<strong class="lg iu"> <em class="lx">【估计销量】</em> </strong>，最后一列是因变量'<strong class="lg iu"> <em class="lx">【购买量】'</em> </strong>，以二进制格式表示个人是否购买了产品(1)或(0)。</p><p id="f319" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">使用这些数据，我们必须为一家产品公司建立一个分类器，该分类器将对特定年龄和特定薪水的人是否会购买他们在社交媒体平台上做广告的产品进行分类。</p><h2 id="c597" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">步骤1:导入库</h2><p id="4177" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">和往常一样，第一步总是包括导入库，即NumPy、Pandas和Matplotlib。</p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="7fcf" class="ki kj it mr b gy mv mw l mx my">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span></pre><h2 id="a403" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">步骤2:导入数据集</h2><p id="319f" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">在这一步中，我们将从我的github存储库中获取存储为<code class="fe mz na nb mr b">SocialNetworkAds.csv </code>的数据集，并将其存储到变量<em class="lx"> dataset </em>中。然后我们将相应的变量赋给X和y。最后，我们将看到数据集<em class="lx">的前5行。</em></p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="0524" class="ki kj it mr b gy mv mw l mx my">dataset = pd.read_csv('<a class="ae mp" href="https://raw.githubusercontent.com/mk-gurucharan/Classification/master/SocialNetworkAds.csv'" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/mk-gurucharan/Classification/master/SocialNetworkAds.csv'</a>)</span><span id="20ad" class="ki kj it mr b gy nc mw l mx my">X = dataset.iloc[:, [0, 1]].values<br/>y = dataset.iloc[:, 2].values</span><span id="f7c9" class="ki kj it mr b gy nc mw l mx my">dataset.head(5)</span><span id="5fde" class="ki kj it mr b gy nc mw l mx my">&gt;&gt;<br/>Age   EstimatedSalary   Purchased<br/>19    19000             0<br/>35    20000             0<br/>26    43000             0<br/>27    57000             0<br/>19    76000             0</span></pre><h2 id="2f4c" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">步骤3:将数据集分为训练集和测试集</h2><p id="da31" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">我们将把数据分成训练集和测试集。在此，我们保留了<code class="fe mz na nb mr b">test_size=0.20</code>。表示<strong class="lg iu"> <em class="lx">数据的20% </em> </strong>将作为<strong class="lg iu"> <em class="lx">测试集</em> </strong>保留，剩余的<strong class="lg iu"><em class="lx"/></strong>80%<strong class="lg iu"><em class="lx">训练集</em> </strong>用于训练。由于有400行，大约80个数据点将被分配给测试集，剩余的320个数据点将用于训练目的。</p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="e086" class="ki kj it mr b gy mv mw l mx my">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)</span></pre><h2 id="97ff" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">步骤4:特征缩放</h2><p id="5c56" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">这是一个额外的步骤，当我们把X的值缩小到一个更小的范围时，它将提高程序的速度。在这里，我们将<code class="fe mz na nb mr b">X_train</code>和<code class="fe mz na nb mr b">X_test</code>缩小到-2到+2的小范围。例如，工资75000按比例缩减为0.16418997。</p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="1743" class="ki kj it mr b gy mv mw l mx my">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><h2 id="42da" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">步骤5:在训练集上训练随机森林分类模型</h2><p id="f561" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">一旦训练测试准备好了，我们就可以导入<code class="fe mz na nb mr b">RandomForestClassifier </code>类并使训练集适合我们的模型。类别<code class="fe mz na nb mr b">SVC</code>被分配给变量分类器。这里用的判据是“<strong class="lg iu"><em class="lx"/></strong>”。也可以使用的另一个标准是"<strong class="lg iu"> <em class="lx">基尼</em> </strong>"。然后使用<code class="fe mz na nb mr b">classifier.fit() </code>函数来训练模型。</p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="9a53" class="ki kj it mr b gy mv mw l mx my">from sklearn.ensemble import RandomForestClassifier<br/>classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')<br/>classifier.fit(X_train, y_train)</span></pre><h2 id="ea7c" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">步骤6:预测测试集结果</h2><p id="f31b" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">在这一步中，<code class="fe mz na nb mr b">classifier.predict()</code>函数用于预测测试集的值，这些值被存储到变量<code class="fe mz na nb mr b">y_pred.</code></p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="4fb2" class="ki kj it mr b gy mv mw l mx my">y_pred = classifier.predict(X_test) <br/>y_pred</span></pre><h2 id="0d33" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">步骤7:混淆矩阵和准确性</h2><p id="95f7" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">这是分类技术中最常用的一步。在这里，我们看到了训练模型的准确性，并绘制了混淆矩阵。</p><p id="37ce" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">混淆矩阵是一个表，用于显示当测试集的真实值已知时，对分类问题的正确和错误预测的数量。它的格式如下</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/74881daa5f3eb9f2b0c834afe2b00c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*aDcJceSYfH7GBxJJpzwvKA.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">来源—自己</p></figure><p id="f506" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">真实值是正确预测的次数。</p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="2086" class="ki kj it mr b gy mv mw l mx my">from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)</span><span id="91d7" class="ki kj it mr b gy nc mw l mx my">from sklearn.metrics import accuracy_score <br/>print ("Accuracy : ", accuracy_score(y_test, y_pred))<br/>cm</span><span id="10bc" class="ki kj it mr b gy nc mw l mx my">&gt;&gt;Accuracy :  0.9625<br/><br/>&gt;&gt;array([[55,  3],<br/>       [ 0, 22]])</span></pre><p id="c917" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">从上面的混淆矩阵，我们推断，在80个测试集数据中，77个被正确分类，只有3个被错误分类，留给我们96.25%的准确率。</p><h2 id="28d3" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">步骤8:将实际值与预测值进行比较</h2><p id="9813" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">在这个步骤中，创建一个Pandas DataFrame来比较原始测试集(<strong class="lg iu"> <em class="lx"> y_test </em> </strong>)和预测结果(<strong class="lg iu"> <em class="lx"> y_pred </em> </strong>)的分类值。</p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="39c0" class="ki kj it mr b gy mv mw l mx my">df = pd.DataFrame({'Real Values':y_test, 'Predicted Values':y_pred})<br/>df</span><span id="1e61" class="ki kj it mr b gy nc mw l mx my">&gt;&gt; <br/>Real Values   Predicted Values<br/>1             1<br/>1             1<br/>0             0<br/>0             0<br/>0             0<br/>... ...  ... ...<br/>1             1<br/>0             1<br/>0             0<br/>0             0<br/>1             1</span></pre><p id="e10f" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">这个步骤是一个额外的步骤，它不像混淆矩阵那样提供很多信息，并且主要用于回归以检查预测值的准确性。试着找出错误预测的值！</p><h2 id="eadd" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">步骤9:可视化结果</h2><p id="90a1" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">在最后一步中，我们将随机森林分类模型的结果可视化在一个图表上，该图表与两个区域一起绘制。</p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="afc7" class="ki kj it mr b gy mv mw l mx my">from matplotlib.colors import ListedColormap<br/>X_set, y_set = X_test, y_test<br/>X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),<br/>                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))<br/>plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),<br/>             alpha = 0.75, cmap = ListedColormap(('red', 'green')))<br/>plt.xlim(X1.min(), X1.max())<br/>plt.ylim(X2.min(), X2.max())<br/>for i, j in enumerate(np.unique(y_set)):<br/>    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],<br/>                c = ListedColormap(('red', 'green'))(i), label = j)<br/>plt.title('SVM Classification')<br/>plt.xlabel('Age')<br/>plt.ylabel('EstimatedSalary')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/ab6564d4b9247a04a01318e0229623a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*UDqFwV3j95UgVgkLye68Qw.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">随机森林分类</p></figure><p id="6cde" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">在这个图中，有两个区域。<strong class="lg iu"> <em class="lx">红色</em> </strong>区域表示<strong class="lg iu"> <em class="lx"> 0 </em> </strong>，由未购买该产品的人组成，<strong class="lg iu"> <em class="lx">绿色</em> </strong>区域表示<strong class="lg iu"> <em class="lx"> 1 </em> </strong>，由已购买该产品的人组成。</p><p id="b341" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">如果您仔细观察，我们可以看到测试集中红色的3个错误分类的数据点，在特定区域有颜色差异。</p><h2 id="a785" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论—</h2><p id="0caa" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">因此，在这个故事中，我们已经成功地构建了一个<strong class="lg iu"> <em class="lx">随机森林分类</em> </strong>模型，该模型能够根据一个人的年龄和工资来预测他是否会购买一件产品。请随意尝试网上其他各种常见的分类数据集。</p><p id="e41a" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我还附上了我的github资源库的链接，你可以在那里下载这个Google Colab笔记本和数据文件供你参考。</p><div class="nf ng gp gr nh ni"><a href="https://github.com/mk-gurucharan/Classification" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">MK-guru charan/分类</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">这是一个由Python代码组成的知识库，用于构建不同类型的分类模型，以评估和…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">github.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw mj ni"/></div></div></a></div><p id="80c5" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">您还可以在下面找到该程序对其他分类模型的解释:</p><ul class=""><li id="76f8" class="nx ny it lg b lh ly lk lz kr nz kv oa kz ob lw oc od oe of bi translated"><a class="ae mp" rel="noopener" target="_blank" href="/machine-learning-basics-logistic-regression-890ef5e3a272">逻辑回归</a></li><li id="fe62" class="nx ny it lg b lh og lk oh kr oi kv oj kz ok lw oc od oe of bi translated"><a class="ae mp" rel="noopener" target="_blank" href="/machine-learning-basics-k-nearest-neighbors-classification-6c1e0b209542">K-最近邻(KNN)分类</a></li><li id="ee02" class="nx ny it lg b lh og lk oh kr oi kv oj kz ok lw oc od oe of bi translated"><a class="ae mp" rel="noopener" target="_blank" href="/machine-learning-basics-support-vector-machine-svm-classification-205ecd28a09d">支持向量机(SVM)分类</a></li><li id="ecf2" class="nx ny it lg b lh og lk oh kr oi kv oj kz ok lw oc od oe of bi translated"><a class="ae mp" rel="noopener" target="_blank" href="/machine-learning-basics-naive-bayes-classification-964af6f2a965">朴素贝叶斯分类</a></li><li id="1a1d" class="nx ny it lg b lh og lk oh kr oi kv oj kz ok lw oc od oe of bi translated">随机森林分类</li></ul><p id="6940" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">在接下来的文章中，我们将会遇到更复杂的回归、分类和聚类模型。到那时，快乐的机器学习！</p></div></div>    
</body>
</html>