<html>
<head>
<title>Touch-less Display Interfaces on Edge</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">边缘无触摸显示界面</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/touch-less-display-interfaces-on-edge-be8dc277c5b8?source=collection_archive---------55-----------------------#2020-09-21">https://towardsdatascience.com/touch-less-display-interfaces-on-edge-be8dc277c5b8?source=collection_archive---------55-----------------------#2020-09-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3b93" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak"> <em class="kf">启用边缘HCI:</em></strong><em class="kf"/>多线程<em class="kf">手势&amp;采用Intel OpenVINO AI模型的信息亭声音控制。眼睛使眼色&amp;嘴巴方面用数值模式</em></h2></div><p id="c087" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">个人注:</em> </strong> <em class="lc">获得</em> <strong class="ki ir"> <em class="lc">大奖</em> </strong> <em class="lc">在</em> <strong class="ki ir"> <em class="lc">【深度学习超级英雄挑战赛】</em> </strong> <em class="lc">与Intel为这个项目举办的Hackster.io。尽管奖金高达1000美元&amp;的英特尔Movidius NCS 2棒，但在一连串的差点失手之后，终于击中靶心，这给人以安慰。当然，隧道的尽头有光:)</em></p><p id="90b2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><em class="lc">大赛链接:</em><a class="ae ld" href="https://www.hackster.io/contests/DLSuperheroes" rel="noopener ugc nofollow" target="_blank"><em class="lc">https://www.hackster.io/contests/DLSuperheroes</em></a></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/126886c267f7ac2f60db4b200ac77564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*iyY4lS84IsmNhwGh_UCjbA.gif"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><a class="ae ld" href="https://www.cybiant.com/wp-content/uploads/2020/01/CKC-ANN-Gif.gif" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="fef8" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">互动公共信息亭现在被广泛使用。银行(自动取款机)、机场(登记)、政府(电子政务)、零售(产品目录)、医疗保健(预约)、学校(出勤)、企业(注册)、活动(信息)等等。随着企业转向信息亭以提供更好的服务，<strong class="ki ir">所有公共设备的免触摸交互已经成为减缓无处不在的</strong> <strong class="ki ir">电晕病毒传播的必要条件。</strong></p><p id="43a3" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">手势或语音导航</strong>似乎可以解决上述问题，<strong class="ki ir">但是这种设备在分析这种输入时会受到资源的限制。</strong>你有没有注意到你的手机语音助手，无论是Siri还是GAssist，在手机离线时都会放弃？当你在偏远的道路上行驶时，你的语音车载信息娱乐系统无法响应。即使是传统的计算机也无法同时运行多个人工智能模型。</p><p id="27c5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在你的设备上完成这一切不是很好吗？想象一个可以从卧床不起的病人那里获取视觉或声音提示的床边辅助设备。<strong class="ki ir">随着英特尔OpenVINO的出现，这成为可能。它通过进行硬件意识优化，从边缘实现并加速深度学习推理。</strong> OpenVINO支持CPU、iGPU、VPU、FPGA和GNAs。如果你想尝试一下，树莓派和英特尔Movidius NCS 2将是你的最佳选择。</p><p id="c3ac" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这篇博客中，我们将尝试构建一个<strong class="ki ir">人机交互(HCI)模块</strong> <strong class="ki ir">，它智能地协调5个并发运行的AI模型</strong>，一个给另一个喂食。用于面部检测、头部姿态估计、面部标志计算和凝视角度估计的AI模型识别手势控制输入并触发映射的动作。<strong class="ki ir">部署一个子线程来运行离线语音识别，该子线程与父进程</strong>通信以基于用户话语给出并行控制命令，从而辅助和增强手势控制。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="lu lv l"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">博客摘要和项目进展</p></figure><p id="f5ec" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">如果你喜欢这个项目，请竖起大拇指</strong> <a class="ae ld" href="https://www.hackster.io/ananduthaman/touch-less-display-interfaces-on-edge-ac9f43" rel="noopener ugc nofollow" target="_blank"> <strong class="ki ir">这里</strong> </a></p><h2 id="e796" class="lw lx iq bd ly lz ma dn mb mc md dp me kp mf mg mh kt mi mj mk kx ml mm mn mo bi translated">解决方案源代码可以在<a class="ae ld" href="https://github.com/AdroitAnandAI/No-Touch-Display-Interface-at-Edge-IoT" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</h2><h1 id="6ccc" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">架构图</strong></h1><p id="2b28" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">下面解释了体系结构图中的每个组件。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nf"><img src="../Images/128028a2ea83d4aaa82db136acd88c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u0eP_KMZwPJFJ7KCY4_2qQ.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><h1 id="436c" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">控制模式</strong></h1><p id="5a65" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">系统中定义了<strong class="ki ir"> 4种控制模式</strong>，用于确定用户输入的模式。我们可以使用手势在控制模式之间切换。</p><ul class=""><li id="878c" class="ng nh iq ki b kj kk km kn kp ni kt nj kx nk lb nl nm nn no bi translated"><strong class="ki ir">控制模式0:无控制</strong> <br/>手势和声音导航关闭</li><li id="7bf0" class="ng nh iq ki b kj np km nq kp nr kt ns kx nt lb nl nm nn no bi translated"><strong class="ki ir">控制模式1:注视角度控制</strong> <br/>鼠标随眼睛注视角度移动(更快)</li><li id="44f0" class="ng nh iq ki b kj np km nq kp nr kt ns kx nt lb nl nm nn no bi translated"><strong class="ki ir">控制模式2:头部姿态控制<br/> </strong>鼠标随着头部方向的改变而移动(较慢)</li><li id="dd1d" class="ng nh iq ki b kj np km nq kp nr kt ns kx nt lb nl nm nn no bi translated"><strong class="ki ir">控制模式3:声音控制<br/> </strong>鼠标根据用户说话向4个方向滑动并打字</li></ul><h1 id="4eaf" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">校准步骤</strong></h1><p id="f1e7" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">为了将3D凝视取向角度转换为2D屏幕尺寸，系统必须知道与屏幕对角相对应的<strong class="ki ir">偏航和俯仰角度。给定对角的这两个角度，我们可以在屏幕上为中间(偏航、俯仰)角度插入(x，y)位置。</strong></p><p id="e499" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">因此，当应用程序启动时，将提示用户查看屏幕的对角。这样的校准步骤<strong class="ki ir">需要将凝视角度的变化映射到屏幕</strong>的尺寸和形状，以便“凝视模式”正常运行。</p><p id="0671" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">没有校准，系统也可以运行</strong>，尽管牺牲了通用性。为了演示，当系统处于“头部姿态”模式时，头部方向的相对变化被用作移动鼠标指针的度量。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu lv l"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">控制器的功能是用目光和头部移动鼠标</p></figure><h1 id="9528" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">手势检测管道</strong></h1><p id="c03e" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">在输入视频流上执行四个预训练的OpenVINO模型，一个馈送到另一个，以检测<strong class="ki ir"> a)面部位置b)头部姿态c)面部标志和d)凝视角度。</strong></p><p id="780f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> a)面部检测:</strong>使用了具有高效深度方向卷积的修剪过的<a class="ae ld" href="https://docs.openvinotoolkit.org/2019_R1/_face_detection_adas_binary_0001_description_face_detection_adas_binary_0001.html" rel="noopener ugc nofollow" target="_blank"> MobileNet骨干网</a>。该模型输出图像中面部的(x，y)坐标，该坐标作为输入被馈送到步骤(b)和(c)</p><p id="b167" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> b)头部姿态估计:</strong><a class="ae ld" href="https://docs.openvinotoolkit.org/2019_R1/_head_pose_estimation_adas_0001_description_head_pose_estimation_adas_0001.html" rel="noopener ugc nofollow" target="_blank">模型</a> <strong class="ki ir">输出头部的偏航角、俯仰角和滚转角</strong>，将面部图像作为步骤(a)的输入</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nv"><img src="../Images/500108b13fe167a4f0190f69f32c438e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ACyc-5fsw3gW4_oR9EP1A.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">笛卡尔坐标系中偏航角、俯仰角和滚转角的可视化</p></figure><p id="14d5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> c)面部标志:</strong>一个<a class="ae ld" href="https://docs.openvinotoolkit.org/2019_R1/_facial_landmarks_35_adas_0002_description_facial_landmarks_35_adas_0002.html" rel="noopener ugc nofollow" target="_blank">自定义CNN </a>用于估计<strong class="ki ir"> 35个面部标志。</strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/dfb3082a8aaa8a1bb869ed481813a4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*dkV2EW95irW4bTwolBxk5Q.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><a class="ae ld" href="https://docs.openvinotoolkit.org/2019_R1/_facial_landmarks_35_adas_0002_description_facial_landmarks_35_adas_0002.html" rel="noopener ugc nofollow" target="_blank">图片由</a>提供</p></figure><p id="c1e4" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如上所述，该模型将来自步骤(a)的裁剪的面部图像作为输入，并计算面部标志。识别面部手势需要这样一张详细的地图，尽管<strong class="ki ir">的计算需求是</strong> <a class="ae ld" href="https://docs.openvinotoolkit.org/2019_R1/_landmarks_regression_retail_0009_description_landmarks_regression_retail_0009.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ki ir">地标回归模型</strong> </a> <strong class="ki ir">的两倍(0.042对0.021 GFlops)，后者仅给出5个面部地标。</strong></p><p id="15af" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> d)视线估计:</strong>自定义<a class="ae ld" href="https://docs.openvinotoolkit.org/2019_R1/_gaze_estimation_adas_0002_description_gaze_estimation_adas_0002.html" rel="noopener ugc nofollow" target="_blank">类VGG CNN</a>进行视线方向估计。</p><p id="6287" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">该网络采用<strong class="ki ir"> 3个输入:左眼图像、右眼图像和三个头部姿态角度—(偏航、俯仰和滚动)</strong> —并输出笛卡尔坐标系中的3d凝视矢量。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi gj"><img src="../Images/f82609bb09684f4fe4d6628ddbfc7e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pvc2ZyDtIhYcHFalqhMaEg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">视线注视向量的可视化</p></figure><h1 id="3b01" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">后处理模型输出</strong></h1><p id="adcf" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">为了将一个模型的输出作为输入提供给另一个模型，需要对每个模型的返回值进行解码和后处理。</p><p id="4e6f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">例如，为了确定凝视角度，<strong class="ki ir">头部方向需要与来自凝视模型</strong>的矢量输出进行数字组合，如下所示。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu lv l"/></div></figure><p id="f384" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">类似地，<strong class="ki ir">面部标志模型返回输入图像大小的比率。</strong>因此，我们<strong class="ki ir">需要将输出乘以图像宽度和高度</strong>来计算35个地标的(x，y)坐标。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu lv l"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">需要处理从地标模型返回的输出面部点</p></figure><p id="dc5a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">虽然面部标志和凝视估计模型的输出可以如上容易地进行后处理，但是头部姿势估计模型输出的<strong class="ki ir">转换稍微复杂一些。</strong></p><h1 id="0404" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">欧拉角到旋转矩阵</strong></h1><p id="0ee7" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">注意"<strong class="ki ir">头部姿态估计</strong>"模型仅输出姿态，即头部的偏航、俯仰和滚动角。<strong class="ki ir">为了获得相应的方向向量，我们需要使用姿态来计算旋转矩阵。</strong></p><p id="7723" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><em class="lc"> i) </em> <strong class="ki ir"> <em class="lc">偏航</em> </strong>是<strong class="ki ir"> α </strong>绕z轴的逆时针旋转。旋转矩阵由下式给出:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b971f931bd95aaa41cd30273c68bd801.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*wAjnynlt1O1jPlwD_88Phg.png"/></div></figure><p id="4f25" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">ii) <strong class="ki ir"> <em class="lc">俯仰</em> </strong>是<strong class="ki ir"> β </strong>绕y轴逆时针旋转。旋转矩阵由下式给出:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/8c594653e7e1d06f0647792d086078e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*xCrTsqLGOIxuQ_xA1YtalA.png"/></div></figure><p id="be0b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">iii) <strong class="ki ir"> <em class="lc"> Roll </em> </strong>是γ绕x轴逆时针旋转。旋转矩阵由下式给出</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/2027e9f974101089f191fc6246aeb429.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*gZvRf35q96vsIEZBJxSxQQ.png"/></div></figure><p id="cfcb" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们可以沿着三个轴一个接一个地旋转，以任何方向放置3D物体。因此，<strong class="ki ir">要计算方向向量，需要将上述3个矩阵相乘。</strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nz"><img src="../Images/28dc2dfc0b20f0b9362117ee5676ce44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uoyUsZ7SbnzaaQclHIYs-g.png"/></div></div></figure><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu lv l"/></div></figure><h1 id="64df" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">眨眼检测</strong></h1><p id="2e20" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">到目前为止，我们已经使用头部和凝视来控制鼠标指针。但是<strong class="ki ir">要使用kiosk，你还需要触发事件，</strong>比如‘左键单击’，‘右键单击’，‘滚动’，‘拖动’等等。</p><p id="6652" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">为此，<strong class="ki ir">需要将一组预定义的手势映射到每个事件</strong>，并从视觉输入中识别。两个事件可以映射到左眼和右眼的“眨眼”事件，但是它们需要被识别为“眨眼”。</p><p id="49c7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">你可以很容易地注意到，当眼睛睁开时，<strong class="ki ir"/><strong class="ki ir">的白色像素数量会突然增加，而当眼睛闭上时，</strong>则会减少。我们可以通过计算白色像素来区分睁眼和闭眼。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/f7332cf9c67a8941b13a9ba3ec280911.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/1*UJWjCTD6kQx_D1ygIS1kzg.gif"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">眨眼检测</p></figure><p id="0e9c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">但在现实世界中，上述逻辑并不可靠，因为白色像素值本身可以变化。我们总是可以使用深度学习或ML技术来分类，但是为了提高效率，建议使用数值解决方案，尤其是当您为边缘设备编码时。</p><p id="26bd" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">让我们看看<strong class="ki ir">如何通过4个步骤用数字信号来检测眨眼！</strong></p><ol class=""><li id="c5a0" class="ng nh iq ki b kj kk km kn kp ni kt nj kx nk lb ob nm nn no bi translated">计算0–255范围内的像素频率(直方图)</li></ol><p id="ed5f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> 2。计算直方图中非零像素的分布。</strong>当一只眼睛闭上时，价差会突然下降，反之亦然。</p><p id="e396" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> 3。尝试在上述信号的尾端拟合一条反s形曲线。</strong></p><p id="2648" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">4.如果发现拟合成功，则确认拟合曲线的“下降”形状，并声明为<strong class="ki ir">“眨眼”事件。(</strong>无曲线拟合=眼睛没有眨眼)</p><p id="58e2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">算法解释:</strong></p><p id="bf3e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如果以上步骤不清楚，那么看看当睁开的眼睛闭上时，直方图分布图是如何下降的。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/938eacd1dc0dd171fafea8964eaeca2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/1*cO1achOXBCvC549R_ZlSjw.gif"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图1左眼眨眼时的直方图分布</p></figure><p id="1ef4" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">给定上述信号，你可以想象当眼睛睁开几秒钟时,<strong class="ki ir">曲线会变成“S”形。<strong class="ki ir">这可以使用sigmoid函数进行数学参数化</strong> <strong class="ki ir">。</strong></strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/86436476f327bb4f618c7eea50101941.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*H4gYV0XxczLbfhCJnJ27Ww.png"/></div></figure><p id="61a1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">但是由于我们需要检测上面所示的“眨眼”事件，曲线的<strong class="ki ir">形状将采用反s形函数的形式。</strong>要绕x轴翻转sigmoid函数，找到f(-x)</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/a78ace26e3be8bfedd0b23691ca58ed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*h0Dp1sn11OACGrzliUcXcQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><em class="kf">反s形函数</em></p></figure><p id="5b43" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">使用任何在线函数可视化工具来绘制上述函数并<strong class="ki ir">改变参数，以查看反向“S”形状如何变化</strong>(以适应上述<strong class="ki ir">T21【图直方图分布<strong class="ki ir"> ) </strong></strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi of"><img src="../Images/8048735185319b76661d4237320300ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bUbFBxQ6upgMMb10nGFPfA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><a class="ae ld" href="http://fooplot.com" rel="noopener ugc nofollow" target="_blank">曲线绘制学分</a></p></figure><p id="0a12" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">因此<strong class="ki ir">，如果通过参数曲线拟合算法</strong>在直方图分布曲线的末端<strong class="ki ir">发现任何相似的形状，那么我们可以称之为‘眨眼’。</strong>曲线拟合算法试图解决一个<strong class="ki ir">非线性最小二乘问题。</strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi og"><img src="../Images/ed1b3fd1e5c7e7050c2a7718ae3bf996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*XKqOgkby8GNdbcMbAuoeoQ.gif"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">反s形曲线拟合:左眼和右眼眨眼</p></figure><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu lv l"/></div></figure><p id="c39b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">注</strong>:计算上述内容的有效方法可以是，</p><ul class=""><li id="f41f" class="ng nh iq ki b kj kk km kn kp ni kt nj kx nk lb nl nm nn no bi translated">考虑非零直方图分布中“n”个最近值的条带。</li><li id="2868" class="ng nh iq ki b kj np km nq kp nr kt ns kx nt lb nl nm nn no bi translated">计算带材前端和尾端“k”值的<strong class="ki ir">中值&amp;标准值。</strong></li><li id="e45a" class="ng nh iq ki b kj np km nq kp nr kt ns kx nt lb nl nm nn no bi translated">如果<strong class="ki ir">中值&gt;阈值和两个标准&lt;阈值</strong>不同，则检测眨眼事件，因为它很可能是一个反s形。</li></ul><p id="5758" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">或者，我们也可以使用下面的算法来寻找眨眼。</p><ul class=""><li id="5058" class="ng nh iq ki b kj kk km kn kp ni kt nj kx nk lb nl nm nn no bi translated">取直方图分布值的<strong class="ki ir">的第一个微分</strong></li><li id="c955" class="ng nh iq ki b kj np km nq kp nr kt ns kx nt lb nl nm nn no bi translated">在第一个微分值中找到<strong class="ki ir">峰值，以找到突然峰值</strong></li><li id="d1d3" class="ng nh iq ki b kj np km nq kp nr kt ns kx nt lb nl nm nn no bi translated">寻找信号的<strong class="ki ir">反射</strong>和<strong class="ki ir">寻找峰值</strong>寻找突然下降</li><li id="b0ea" class="ng nh iq ki b kj np km nq kp nr kt ns kx nt lb nl nm nn no bi translated">如果在上述两个步骤中都发现<strong class="ki ir">峰值，则<strong class="ki ir">闪烁</strong></strong></li><li id="6c83" class="ng nh iq ki b kj np km nq kp nr kt ns kx nt lb nl nm nn no bi translated">如果只在反射中发现峰值，那么它是眨眼事件。</li></ul><p id="de27" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">上述方法比曲线拟合更有效，但可能导致许多假阳性，因为峰值检测并不总是可靠的，尤其是在低光下。中间道路方法将使用中间值和标准偏差来估计曲线的形状。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu lv l"/></div></figure><h1 id="fe97" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">口长宽比(MAR) </strong></h1><p id="c68a" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">在这篇<a class="ae ld" href="https://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf" rel="noopener ugc nofollow" target="_blank">经典的面部标志论文</a>中，眼睛纵横比(耳朵)被计算出来以确定眨眼。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/6f2f329ac539f9e29a5ce4982b71d387.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*IdkFLkxQ23CJLC6BLBqfmw.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7a23e9d29ee9ef68c4b44076fa31478f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*zS7u0ZH2hAuS_ASfjrs2AA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><em class="kf">眼睛闭上时，耳值会下降。</em></p></figure><p id="e30f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们不能使用上面的公式来确定眼睛姿态，因为我们的模型没有估计这样密集的地标图。然而，<strong class="ki ir">受EAR的启发，我们可以基于从OpenVINO模型获得的可用4个地标来计算MAR </strong>，如下。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b9cccc509967e641ad1b75b54de8366d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*RQM33470lxE68AdxRYOKBQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><em class="kf">高效标记公式</em></p></figure><p id="ebb2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">两个手势事件</strong>可以使用MAR:</p><ol class=""><li id="df89" class="ng nh iq ki b kj kk km kn kp ni kt nj kx nk lb ob nm nn no bi translated">如果<strong class="ki ir">标记了&gt;阈值</strong>，那么人就是<strong class="ki ir">微笑</strong></li><li id="c4e7" class="ng nh iq ki b kj np km nq kp nr kt ns kx nt lb ob nm nn no bi translated">如果<strong class="ki ir">标记了&lt;阈值</strong>，那么这个人正在<strong class="ki ir">打哈欠</strong></li></ol><p id="56b4" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们可以自由地附加两个与这两个手势相对应的命令。</p><h1 id="cdbb" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">线程和进程线程通信</strong></h1><p id="b4e8" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">为了增强控制，我们还可以启用基于声音的导航，以及手势控制。然而，<strong class="ki ir">系统在分析来自输入视频流的图像帧</strong>时，需要连续监控用户话语 <strong class="ki ir">以识别命令。</strong></p><p id="e19b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">因此，在不同的线程中运行语音识别模型并让子线程与父进程通信自然是明智的。子线程将识别移动鼠标或在屏幕上书写的语音命令，<strong class="ki ir">使用Python中的共享队列数据结构</strong>将其传递给父线程(如下所示)。</p><p id="b249" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">父进程将运行所有上述AI模型</strong>和手势识别所需的计算，以启用头部和凝视控制模式。因此，可以并行地获取手势和声音控制命令，但是为了可用性，在这个项目中，我们选择在控制模式3中单独获取声音命令。</p><h1 id="18eb" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">语音识别</strong></h1><p id="7c75" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">为了解码声波，我们使用OpenVINO特征提取和解码器库，<strong class="ki ir">接收并转录来自麦克风的音频。</strong>我们已经使用了这里<a class="ae ld" href="https://docs.openvinotoolkit.org/latest/openvino_inference_engine_samples_speech_libs_and_demos_Live_speech_recognition_demo.html" rel="noopener ugc nofollow" target="_blank">提到的语音库</a>在边缘上运行语音识别，不上线。</p><p id="90ca" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">由于识别模型是以牺牲准确性为代价进行优化的，因此需要进行一些调整来识别口头命令。首先，我们限制命令词汇，只说‘上’，‘下’，‘左’&amp;，‘右’。其次，命令词的<strong class="ki ir">相似发音同义词被存储</strong>在字典中以找到最佳匹配。例如，“右”命令可以被识别为“写”。</p><p id="e781" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这个函数是这样编写的，命令和同义词可以很容易地扩展。为了启用用户输入，还启用了语音写入功能。这使得用户能够输入字母和数字。<strong class="ki ir">如</strong> : PNR号。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nu lv l"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">线程从相似声音“同义词”中识别命令，并插入到STT队列中</p></figure><h1 id="f8d8" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">手势控制和</strong>鼠标导航</h1><p id="bdeb" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">手势控制命令配置如下。但是，您可以轻松地更改手势命令映射。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/eea60404ae93bb722a2c0c1cd9012d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*gTPjDn--GsSo6j4ugncbSg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">手势映射可以随意修改</p></figure><p id="6b7c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">使用<strong class="ki ir"> pyautogui库控制鼠标指针。</strong>move()、moveTo()、click()、drag()、scroll()、write()等函数用于触发上述手势对应的事件。</p><h1 id="03a1" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated">粘性特征及优化</h1><p id="498c" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">眼睛的凝视或头部的姿势至少会持续改变一点，即使是无意的。<strong class="ki ir">此类自然动作不应被视为命令，否则鼠标指针会变得抖动。</strong>因此，我们引入了一个<strong class="ki ir">“粘性”参数，在该参数范围内，运动被忽略。</strong>这大大增加了手势控制的稳定性和可用性。</p><p id="c7d4" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">最后，<a class="ae ld" href="https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-vtune/top.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ki ir">英特尔VTune </strong> </a> <strong class="ki ir">评测器用于查找热点并优化</strong>应用代码。一个shell脚本vtune_script.sh被提供给VTune GUI，它使用合适的参数启动项目。</p><h1 id="da83" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">结论</strong></h1><p id="51f8" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">项目<strong class="ki ir">展示了英特尔OpenVINO顺序和并行处理多个边缘人工智能模型的能力。</strong>许多控制输入也用于证明灵活性。但是要部署定制解决方案，你可以选择你认为合适的控件。</p><p id="8b8c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">例如，凝视控制对于大屏幕可能是理想的，而头部姿势控制对于笔记本电脑屏幕可能是理想的。无论哪种方式，声音控制都有助于接受自定义表单条目或语音命令。手势-动作映射也可以修改。然而，你可以理解的一点是，在边缘上链接多个硬件优化的人工智能模型的可能性，加上高效的数值计算来解决有趣的问题。</p><h2 id="c786" class="lw lx iq bd ly lz ma dn mb mc md dp me kp mf mg mh kt mi mj mk kx ml mm mn mo bi translated">解决方案源代码可以在<a class="ae ld" href="https://github.com/AdroitAnandAI/No-Touch-Display-Interface-at-Edge-IoT" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</h2><p id="361d" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">如果您有任何疑问或建议，您可以<strong class="ki ir">联系我</strong> <a class="ae ld" href="https://www.linkedin.com/in/ananduthaman/" rel="noopener ugc nofollow" target="_blank"> <strong class="ki ir">这里</strong> </a></p><p id="17fa" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">如果你喜欢这个项目，请竖起大拇指</strong> <a class="ae ld" href="https://www.hackster.io/ananduthaman/touch-less-display-interfaces-on-edge-ac9f43" rel="noopener ugc nofollow" target="_blank"> <strong class="ki ir">这里</strong> </a></p><h1 id="3474" class="mp lx iq bd ly mq mr ms mb mt mu mv me jw mw jx mh jz mx ka mk kc my kd mn mz bi translated"><strong class="ak">参考文献</strong></h1><p id="d87b" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp nc kr ks kt nd kv kw kx ne kz la lb ij bi translated">[1] <strong class="ki ir">英特尔OpenVINO官方文档:</strong><a class="ae ld" href="https://docs.openvinotoolkit.org" rel="noopener ugc nofollow" target="_blank">https://docs.openvinotoolkit.org</a></p><p id="ca4c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">[2] <strong class="ki ir">英特尔Edge AI for IoT Nanodegree由Udacity提供。</strong>灵感来自期末课程项目。<strong class="ki ir"/><a class="ae ld" href="https://classroom.udacity.com/nanodegrees/nd131" rel="noopener ugc nofollow" target="_blank">https://classroom.udacity.com/nanodegrees/nd131</a></p><p id="3dc7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">[3] <em class="lc">布拉格捷克技术大学电子工程系Tereza Soukupova和Jan Cech使用面部标志进行实时眨眼检测</em>。</p></div></div>    
</body>
</html>