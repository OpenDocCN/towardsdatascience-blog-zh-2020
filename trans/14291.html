<html>
<head>
<title>A Milestone in Deep Image Inpainting - Review: Globally and Locally Consistent Image Completion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深层图像修复的里程碑——综述:全局和局部一致的图像修复</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df?source=collection_archive---------15-----------------------#2020-10-02">https://towardsdatascience.com/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df?source=collection_archive---------15-----------------------#2020-10-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2fd1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">欢迎回来，伙计们，我希望之前的帖子激起了你们对图像修复的深度生成模型的好奇心。如果你是新朋友，我强烈建议你浏览一下之前的帖子<a class="ae kl" href="https://medium.com/analytics-vidhya/introduction-to-generative-models-for-image-inpainting-and-review-context-encoders-13e48df30244" rel="noopener">这里</a>和<a class="ae kl" href="https://medium.com/@ronct/review-high-resolution-image-inpainting-using-multi-scale-neural-patch-synthesis-4bbda21aa5bc" rel="noopener">这里</a>。根据前一篇文章中的声明，我们今天将进入深层图像修复的另一个里程碑！你准备好了吗？开始吧:)</p><p id="e62a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"><em class="km">*图像修复</em> </strong>和<strong class="jp ir"> <em class="km">图像完成</em> </strong>代表相同的任务</p><h1 id="ff49" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">回忆</h1><p id="6a51" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">这里只是简单回顾一下我们以前学过的内容。</p><ul class=""><li id="6990" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">对于图像修复，填充像素的纹理细节非常重要。有效像素和填充像素应该一致，填充的图像应该看起来逼真。</li><li id="5575" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">粗略地说，研究人员采用像素级重建损失(即L2损失)来确保我们可以用“正确”的结构填充缺失的部分。另一方面，应当使用GAN损失(即对抗性损失)和/或<a class="ae kl" href="https://medium.com/@ronct/review-high-resolution-image-inpainting-using-multi-scale-neural-patch-synthesis-4bbda21aa5bc" rel="noopener">纹理损失</a>来获得具有所生成像素的更清晰纹理细节的填充图像。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="0630" class="kn ko iq bd kp kq ml ks kt ku mm kw kx ky mn la lb lc mo le lf lg mp li lj lk bi translated">动机</h1><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/db4f9bff8e49a3f47999b7540df6599e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FoABBYYiYcwNguXG9dVZzw.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图一。一个例子显示了为图像修补任务生成新片段的需要。摘自并修改自[1]</p></figure><ul class=""><li id="2721" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">对于基于补片的方法，一个重要的假设是，我们相信我们可以在缺失区域之外找到相似的补片，并且这些相似的补片对于填充缺失区域是有用的。这种假设对于自然场景可能是正确的，因为天空和草坪在一幅图像中可能有许多相似的斑块。如果丢失区域之外没有任何相似的补片，就像图1所示的人脸图像修复的情况一样，该怎么办？对于这种情况，我们无法找到任何眼罩来填补相应的缺失部分。因此，鲁棒的修复算法应该<strong class="jp ir"> <em class="km">能够生成新颖的片段</em> </strong>。</li><li id="ecbd" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">现有的基于GAN的修补方法利用鉴别器(对抗损失)通过将填充区域馈送给鉴别器(即欺骗鉴别器)来增强填充区域的锐度。一些人可以在预训练的网络中比较丢失区域内部和外部的局部神经响应，以确保丢失区域内部和外部的局部补片的相似纹理细节。如果我们同时考虑图像的局部和全局信息来加强局部和全局一致性，会怎么样？我们会获得更好的完整图像吗？ 让我们看看。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ng"><img src="../Images/9ceb418aa59da8d443b6f431bfa95ccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EQoA5NuL9Tym2OKb.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图二。上下文编码器的网络体系结构。摘自[2]</p></figure><ul class=""><li id="ec21" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated"><strong class="jp ir"> <em class="km">如何处理高分辨率图像？</em> </strong>我们之前讨论过第一种基于GAN的修复方法，上下文编码器<a class="ae kl" href="https://medium.com/analytics-vidhya/introduction-to-generative-models-for-image-inpainting-and-review-context-encoders-13e48df30244" rel="noopener">这里</a>。他们假设测试图像总是128×128，并且缺少一个64×64的中心孔。然后，我们还介绍了上下文编码器的改进版本，在之前的文章中的<a class="ae kl" href="https://medium.com/analytics-vidhya/review-high-resolution-image-inpainting-using-multi-scale-neural-patch-synthesis-4bbda21aa5bc" rel="noopener">中称为多尺度神经补片合成。他们提出了一种多尺度方法来处理最大分辨率为512×512且具有256×256中心缺失孔的测试图像。简而言之，他们使用三个网络来处理三种比例的图像，即128×128，256×256，然后是512×512。因此，速度是他们提出的方法的瓶颈。用Titan X GPU填充一个512×512的图像大约需要1分钟。所以，一个有趣的问题！<strong class="jp ir"> <em class="km">我们如何仅通过一次网络转发来处理高分辨率图像？</em> </strong>给你几秒钟时间思考，你可能会从如图2所示的架构中找到一些提示(注意中间层)。一个简单的答案是去掉中间的全连接层，采用全卷积网络！你很快就会知道怎么做和为什么！</a></li></ul><h1 id="7f2e" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">介绍</h1><ul class=""><li id="40ba" class="lq lr iq jp b jq ll ju lm jy nh kc ni kg nj kk lv lw lx ly bi translated">现有的大多数方法都假设可以找到相似的图像块来填充同一幅图像中缺失的部分。图像修复并不总是如此，请参见图1。更准确地说，我们应该看整个图像来理解它的上下文，然后根据它的上下文来填充缺失的部分。</li><li id="fb5c" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">如果采用全连接层，则输入图像大小必须是固定的。因此，网络不能仅通过一次向前传递来处理不同分辨率的测试图像。回想一下，全连接层完全连接两层之间的所有神经元，因此它对先前层的输出大小的变化很敏感，并且测试图像大小必须是固定的。另一方面，对于卷积层，神经元之间没有完全连接。较小的输入要素地图会导致较小的输出要素地图。因此，如果网络仅由卷积层组成，它可以处理各种大小的输入图像。我们称这类网络为<strong class="jp ir">全卷积网络</strong>。</li></ul><h1 id="666e" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">解决方案(简而言之)</h1><ul class=""><li id="0d9b" class="lq lr iq jp b jq ll ju lm jy nh kc ni kg nj kk lv lw lx ly bi translated">使用扩展卷积而不是全连接层，这样我们仍然可以理解图像的上下文，并构建一个全卷积网络(FCN)来处理不同大小的图像。</li><li id="a04d" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">使用两个鉴别器来确保完成的(填充的)图像的局部和全局一致性。一个鉴别器在全局意义上查看整个图像，而一个鉴别器在局部意义上查看填充区域周围的子图像。</li><li id="17b9" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">采用简单的后处理。有时很容易发现生成像素和有效像素之间的差异。为了进一步提高视觉质量，作者采用了两种常规技术，即<a class="ae kl" href="https://docs.opencv.org/master/df/d3d/tutorial_py_inpainting.html" rel="noopener ugc nofollow" target="_blank">快速行进法</a>和<a class="ae kl" href="http://www.ctralie.com/Teaching/PoissonImageEditing/" rel="noopener ugc nofollow" target="_blank">泊松图像融合</a>。这两种技术超出了本文的范围，感兴趣的读者可以点击超链接了解更多。后来，在某种程度上，后处理步骤已经以细化网络的形式嵌入到网络中。我们将在以后的文章中讨论它。</li></ul><h1 id="82ee" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">贡献</h1><ul class=""><li id="e37d" class="lq lr iq jp b jq ll ju lm jy nh kc ni kg nj kk lv lw lx ly bi translated">提出一种<strong class="jp ir">全卷积网络，采用扩展卷积</strong>进行图像修复。它允许我们在不使用完全连接的层的情况下理解图像的上下文，因此训练的网络可以用于不同大小的图像。这种架构实际上形成了后来基于深度学习的图像修复方法的基础。这就是为什么我认为这篇论文是修复的里程碑。</li><li id="2c51" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">建议使用<strong class="jp ir">两个鉴别器</strong>(一个<strong class="jp ir">局部</strong>和一个<strong class="jp ir">全局</strong>)。多尺度鉴别器似乎可以在各种尺度下提供完整图像的更好的纹理细节。</li><li id="976c" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">强调在图像修复任务中生成新颖片段的重要性。实际上，训练数据极其重要。简单来说，<strong class="jp ir">你不能生成你以前没见过的东西！</strong></li></ul><h1 id="b420" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">方法</h1><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nk"><img src="../Images/6b1a9666c33e31d8c579b81fe46fc5af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7c9-z1DNVzQ4L265bB7OJg.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图3。提议的体系结构概述。摘自[1]</p></figure><ul class=""><li id="0561" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">图3显示了建议的整体网络架构。它由三个网络组成，即完备网络(即生成器，在训练和测试中都使用)、局部鉴别器和全局鉴别器(在刚训练时用作学习的辅助网络)。快速回忆一下这个GAN框架。生成器负责完成图像以欺骗鉴别器，而鉴别器负责区分完成图像和真实图像。</li></ul><h1 id="1b80" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">细胞神经网络中的扩张卷积</h1><p id="1064" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">扩张卷积的概念对于读者理解本文的网络设计是重要的。所以，我想尽力为不熟悉扩张卷积的读者解释一下。对于非常了解它的读者，也请快速回顾一下。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nl"><img src="../Images/28d9f331823801c34e65ace63bdc7dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Li6OK8cvFJ-3ArDrLVImCg.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图4。标准卷积和扩展卷积的图示</p></figure><ul class=""><li id="d6f4" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">在论文中，作者用了半页的篇幅来描述CNN、标准卷积和扩张卷积。他们还提供了卷积方程供参考。有一点我必须澄清，扩张卷积不是作者在本文中提出的，他们用它来进行图像修复。</li><li id="cc22" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">在这里，我只想用一个简单的图来说明标准卷积和扩张卷积的区别。</li><li id="ed6b" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">图4(a)是一个标准卷积层，具有3×3内核，步幅=1，填充=1，膨胀率=1。在case设置中，8×8输入产生8×8输出，每9个相邻位置构成输出端的一个元素。</li><li id="a799" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">图4(b)也是标准卷积层。这次我们使用一个5×5的内核，步幅=1，填充=2(用于保持相同的输入和输出大小),膨胀率=1。在这种情况下，每25个相邻位置对输出端的每个元素都有贡献。这意味着对于输出端的每个值，我们必须更多地考虑(关注)输入端。我们通常称之为更大的<strong class="jp ir">感受野</strong>。对于一个大的感受野，来自远处空间位置的更多特征将被考虑以给出输出的每个值。</li><li id="18a3" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">然而，对于图4(b)中的情况，我们使用更大的核(5×5)来获得更大的感受野。这意味着需要学习更多的参数(与5×5=25相比，3×3=9)。有什么方法可以在没有更多参数的情况下增加感受野？答案是扩张卷积。</li><li id="9267" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">图4(c)是一个具有3×3内核、步幅=1、填充=2和膨胀率=2的膨胀卷积层。当比较图4(b)和图4(c)中的核的覆盖范围时，我们可以看到它们都覆盖了输入端的一个5×5的局部空间区域。通过跳过连续的空间位置，3×3核可以获得与5×5核一样的感受野。跳跃的步骤由膨胀率决定。举例来说，一个3×3的核，其<strong class="jp ir">扩张率=2 </strong>给出一个5×5的感受野；具有<strong class="jp ir">扩张率=3 </strong>的3×3核给出7×7感受野，依此类推。显然，通过跳过连续的空间位置，扩展卷积增加了感受野，而没有增加额外的参数。优点是我们有更大的感受野和相同数量的参数。缺点是我们跳过了一些位置(我们可能会因此丢失一些信息)。</li></ul><h1 id="214a" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">为什么要扩张卷积？</h1><p id="47ad" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">在回顾了膨胀卷积的概念后，我现在要谈谈作者在他们的模型中使用膨胀卷积的原因。你们中的一些人可能已经猜到了原因。让我们检查一下！</p><ul class=""><li id="86ba" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">如前所述，理解整个图像的上下文对于图像修复任务是很重要的。以前的方法采用全连接层作为中间层，以便理解上下文。请记住，标准卷积层在局部区域执行卷积，而全连接层完全连接所有神经元(即每个输出值取决于所有输入值)。然而，全连接层对输入图像大小施加了限制，并引入了更多可学习的参数。</li><li id="84ef" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">为了解决这些限制，使用扩张卷积来构建允许各种大小输入的全卷积网络。另一方面，通过调整标准核的膨胀率(通常为3×3)，我们可以在不同的层上有更大的感受野，以帮助理解整个图像的上下文。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nm"><img src="../Images/2eca5d7af4f4d76f40aa6246d9303f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-dsUUjOr9UNiAw_iMszL9Q.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图5。感受野大小的影响。摘自并修改自[1]</p></figure><ul class=""><li id="e7c5" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">图5是一个示例，显示了扩张卷积的有效性。你可能认为(a)是具有3×3核(较小感受野)的标准卷积，而(b)是具有3×3核且扩张率≥2(较大感受野)的扩张卷积。位置p1和p2位于孔区域内，其中p1靠近边界，p2大致位于中心点。对于(a)，可以看到位置p1的感受野(影响区域)可以覆盖有效区域。这意味着有效像素可以用来帮助填充位置p1处的像素。另一方面，位置p2的感受野不能覆盖有效区域，因此没有来自有效区域的信息可用于生成。</li><li id="ceca" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">对于(b)，我们使用扩张卷积来增加感受野。这一次，两个位置的感受野可以覆盖有效区域。读者现在可以认识到扩张卷积的有效性。</li></ul><h1 id="b66f" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">完井网络</h1><p id="f7dc" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">让我们回到图3所示的完井网络的结构。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/b268c3ea876a2a2db7feb16aee7764ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*Oljl1QfV4vKVEA7e7YTPSg.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">表1。完井网络的结构。每个卷积层后面都跟有ReLU，除了最后一个卷积层后面跟有Sigmoid [1]</p></figure><ul class=""><li id="b9d6" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">完成网络是一个完全卷积网络，它接受不同大小的输入图像。</li><li id="69ab" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">该网络对输入进行2倍的下采样。这意味着，如果输入为256×256，则中间层的输入大小为64×64。</li><li id="2f48" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">为了充分利用有效像素并保证逐像素精度，作者用有效像素代替空洞区域以外的像素。</li></ul><h1 id="a6de" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">上下文鉴别器</h1><p id="53e8" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">让我们来谈谈本地和全球歧视者。没有什么特别的，和单一鉴别器的情况一样。唯一不同的是我们这次有两个。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/75658c5561cfa80bb5a9efaedbd1c160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*ZevmwSWQjQU-hBjRNqx2JA.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">表二。局部和全局鉴别器的结构。FC代表全连接层。级联层(c)的最后一个FC后面是一个Sigmoid [1]</p></figure><ul class=""><li id="dd49" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">局部和全局鉴别器共享几乎相同的架构。全局鉴别器以256×256的大小(整个图像，为了全局一致性)获取输入图像，而局部鉴别器以128×128的大小围绕缺失区域的中心获取输入，为了局部一致性。</li><li id="73f5" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">需要注意的一点是，在训练过程中，总有一个单一的缺失区域。在测试过程中，图像中可能会有多个缺失区域。此外，对于局部鉴别器，由于实际图像没有填充区域，因此对实际图像采用随机选择128×128的块。</li></ul><h1 id="c91a" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">训练策略和损失函数</h1><ul class=""><li id="fc56" class="lq lr iq jp b jq ll ju lm jy nh kc ni kg nj kk lv lw lx ly bi translated">和以前一样，使用两个损失函数来训练网络，即L2损失和对抗损失(GAN损失)。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ebf205f5091dcb5a0ef374b91d43df97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*6sS5Cv2GI9kYy_x3EwzkrQ.png"/></div></figure><ul class=""><li id="78a2" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated"><em class="km"> C </em> ( <em class="km"> x </em>，<em class="km"> M_c </em>)表示作为函数的完井网络。<em class="km"> x </em>是输入图像，并且<em class="km"> M_c </em>是指示缺失区域的二进制掩码。1表示孔区域，0表示外部区域。你可以看到L2损耗是在空穴区域内计算的。注意，完整图像的外部区域的像素直接被有效像素替换。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/feab5a3e5b0aefc5056f95390a991abe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*xKK97nYbOeDL6FWgHuoaFA.png"/></div></figure><ul class=""><li id="318c" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated"><em class="km"> D </em> ( <em class="km"> x </em>，<em class="km"> M_d </em>)表示作为函数的两个鉴别器。<em class="km"> M_d </em>是一个随机掩码，用于为局部鉴别器随机选择一个图像块。这是标准的GAN损耗。我们希望鉴别器不能区分完整的图像和真实的图像，因此我们可以得到具有真实纹理细节的完整图像。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/f73a32ccd97182ee7489ac6ba069cc8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*N4NHT5MnT3wSdH1Nhg2rIw.png"/></div></figure><ul class=""><li id="bb34" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">这是训练网络的联合损失函数。α是一个加权超参数，用于平衡L2损耗和GAN损耗。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ns"><img src="../Images/53cc74f6804956a54f6010f03d4e5f2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v_o4tmsJANoN4hQxtMrT_Q.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">算法1。提议的培训程序[1]</p></figure><ul class=""><li id="07c9" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">作者将他们的训练分为三个阶段。对于<em class="km"> T_C </em>次迭代，训练仅具有L2损耗的完成网络。<strong class="jp ir"> ii) </strong>固定完成网络，使用GAN损耗训练鉴别器进行<em class="km"> T_D </em>次迭代。<strong class="jp ir"> iii) </strong>交替训练完成网络和鉴别器，直到训练结束。</li><li id="91f5" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">为了稳定的训练，在除了完成网络的最后一层和鉴别器之外的所有卷积层之后使用批量标准化(BN)。</li><li id="9024" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">为了生成训练数据，他们随机将图像的最小边缘调整到[256，384]像素范围。然后，他们随机裁剪一个256×256的图像块作为输入图像。对于掩模图像，他们随机生成一个洞，每个边的范围从[96，128]。</li><li id="d489" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">简单的后处理:如前所述，作者还采用传统的快速行进方法，然后进行泊松图像混合，以进一步提高完成图像的视觉质量。</li></ul><h1 id="000e" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">实验</h1><ul class=""><li id="b246" class="lq lr iq jp b jq ll ju lm jy nh kc ni kg nj kk lv lw lx ly bi translated">作者使用来自Places2数据集[3]的8，097，967幅训练图像来训练他们的网络。联合损失函数中的alpha加权超参数设置为0.0004，批量大小为96。</li><li id="62a8" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">从论文来看，完井网络训练为<em class="km">T _ C</em>= 90000次迭代；鉴别器被训练<em class="km"> T_D </em> = 10，000次迭代，最后所有网络被联合训练400，000次迭代。他们声称，整个训练过程在一台配有4k 80 GPU的计算机上需要大约2个月的时间。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nt"><img src="../Images/2a0e3710ac9081a24caf2b6b72a24e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mbG5cMhVX9ws11nPkhl0TA.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">表3。提议方法的时机[1]</p></figure><ul class=""><li id="4add" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">他们使用8核英特尔酷睿i7–5960 X CPU @ 3.00 GHz和NVIDIA GeForce TITAN X GPU在CPU和GPU上进行评估。其实速度挺快的，半秒多一点完成一张1024×1024的图像。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nu"><img src="../Images/b7b4c77e8e9d2ec7547d4ec8fd876746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8fTWH5KhWJ6jqqtx8KCGA.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图6。与现有修复方法的比较[1]</p></figure><ul class=""><li id="e87c" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">图6显示了与一些现有方法的比较。总的来说，基于补片的方法能够完成局部一致的图像补片，但是它们可能不与整个场景全局一致。最近基于GAN的方法，上下文编码器(第五行)，往往会给出模糊的完整图像。所提出的方法提供了局部和全局一致的完整图像。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nv"><img src="../Images/5effd094770d8c4c853a530f93ed0d54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GE45VoCRfHwdbn4d5qXOtw.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图7。与上下文编码器(CE)对中心缺失孔的比较。CE和我们的(CM)都使用来自<a class="ae kl" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>的相同100k训练图像子集进行训练，用于中心孔洞填充【1】</p></figure><ul class=""><li id="e1a8" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">为了与最新的基于GAN的修复方法进行比较，作者执行了中心区域填充，结果如图7所示。可以看出，对于中心区域完成，CE比任意区域完成执行得更好(图6)。在我看来，CE和所提出的方法在图7中具有相似的性能。读者可以放大来看不同之处。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nw"><img src="../Images/202af1e4e093ba84c3e666c988f50490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zkcHrivN8xNmsOY-s3qKgw.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图8。不同鉴别器设置的影响[1]</p></figure><ul class=""><li id="163f" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">作者对这两种鉴别器进行了消融研究。从图8(b)和(c)可以看出，当不使用局部鉴别器时，完成的区域看起来更加模糊。另一方面，对于(d ),如果仅使用局部鉴别器，我们可以获得良好的局部一致性纹理细节，但不能保证全局一致性。对于(e)中的完整方法，我们实现了具有局部和全局一致性的结果。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nx"><img src="../Images/a0e4ee2dc553158c31b2f2406dae381c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n8bYEi7EiiHIjcaoVUaTog.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图9。简单后处理的效果[1]</p></figure><ul class=""><li id="3a1d" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">图9显示了简单后处理的效果。对于图9(b ),我们可以很容易地观察到边界。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ny"><img src="../Images/237aaffbe797a80b015ac9adc5e54366.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xexe65J1OGWdbCrvC8k8fA.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图10。修复不同数据集的训练结果[1]</p></figure><ul class=""><li id="25d1" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">图10显示了在不同数据集上训练的模型的修复结果。请注意，Places2包含大约800万张不同场景的训练图像，而ImageNet包含100万张用于对象分类的训练图像。我们可以看到，在Places2上训练的模型的结果比在ImageNet上训练的结果要好一些。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nz"><img src="../Images/d72d4b4f74cea6902787dccf3475a6e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6UnpFUnkIpsK9a3PPSNf_w.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图11。提出的方法去除物体的例子[1]</p></figure><ul class=""><li id="308e" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">图像修补的一个潜在应用是对象移除。图11示出了通过使用所提出的方法来移除对象的一些例子。</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oa"><img src="../Images/37074032974e109194b61cf8c7160690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ss18mM9gieY8p1rMfl312g.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图12。更具体数据集的结果，即面部和立面[1]</p></figure><ul class=""><li id="c6cc" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">本文作者还考虑了特定领域的图像修复。他们在CelebA数据集[4](用于人脸修复)和CMP Facade数据集[5](用于立面修复)上微调了他们的预训练模型，这两个数据集分别由202，599和606幅图像组成。他们使用在Places2数据集上训练的预训练模型。对于新的数据集，他们从头开始训练鉴别器，然后完成网络和鉴别器一起交替训练。</li><li id="9170" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">图12示出了由所提出的用于域特定图像修补的方法给出的一些修补结果。对于人脸修复，提出的方法能够生成新颖的片段，如眼睛和嘴巴。对于立面修复，所提出的方法还能够生成像窗口一样的片段，这些片段在局部和全局上与整个图像一致。</li><li id="c93e" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">作者还对完整的人脸图像进行了用户研究。结果表明，77.0%的人脸被10个用户认为是真实人脸。另一方面，96.5%的真实人脸可以被10个用户正确识别。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="205c" class="kn ko iq bd kp kq ml ks kt ku mm kw kx ky mn la lb lc mo le lf lg mp li lj lk bi translated">局限性和讨论</h1><p id="32df" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">以下是作者列出的关于局限性和未来方向的一些观点。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ob"><img src="../Images/5f141b9699091cb710018c1421edcba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ngN4_ij5MWwFR6AespUOkw.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图13。失败情况I)掩模在图像的边界ii)场景复杂。摘自[1]</p></figure><ul class=""><li id="fd92" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">对于图13左侧的情况，我们可以看到<strong class="jp ir">缺失的部分在上部图像</strong>的边界处。作者声称<strong class="jp ir">在这种情况下，可以从邻近位置</strong>借用的信息更少，因此基于GAN的方法(第3行和第4行)比传统的基于补丁的方法(第2行)表现更差。另一个原因是，这个例子是自然场景，所以基于补丁的方法可以很好地工作。</li><li id="3504" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">对于图13右边的例子，<strong class="jp ir">场景要复杂得多</strong>。根据遮罩，我们想要移除一个人，我们必须填充一些建筑物的细节来完成这个复杂的场景。在这种情况下，所有的方法都不能正常工作。所以在复杂场景中填充缺失的部分<strong class="jp ir">还是很有挑战性的。</strong></li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oc"><img src="../Images/254c7bddb7171523fb64185035ce873e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2EXh95B5sEwmohkxZUT3Q.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图14。举例说明生成新片段的重要性，我们只能生成我们以前在训练中见过的内容。摘自[1]</p></figure><ul class=""><li id="918a" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">作者提供了额外的例子来强调另外两点。<strong class="jp ir"> i) </strong>生成眼睛、鼻子、嘴巴等新奇片段的重要性。<strong class="jp ir"> ii) </strong>训练数据集的重要性。</li><li id="4fad" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">对于我们无法找到相似图像补片来填充缺失部分的情况，基于补片的方法(第2行和第3行)无法正常工作，如图14所示。因此，<strong class="jp ir">鲁棒的修复算法必须能够生成新颖的片段</strong>。</li><li id="bf70" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">为了进一步表明选择训练数据集的重要性，作者比较了在Places2(通用数据集，(d))和CelebA(人脸数据集，(e))上训练的两个模型。显然,( d)不能用合理的面部细节来填充缺失的部分，因为它是在不包含任何对齐的面部图像的地点2上训练的。另一方面，(e)工作得很好，因为它是在CelebA上训练的，CelebA是一个具有许多对齐的人脸图像的数据集。因此，<strong class="jp ir">我们只能生成我们在训练</strong>时看到的内容。鲁棒的通用修复还有很长的路要走。</li></ul><h1 id="73e5" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">结论</h1><ul class=""><li id="182e" class="lq lr iq jp b jq ll ju lm jy nh kc ni kg nj kk lv lw lx ly bi translated">所提出的体系结构是大多数后期修复论文的基础。具有扩展卷积的全卷积网络允许我们在不使用全连接层的情况下理解图像的上下文，因此网络可以采用各种大小的输入图像。</li><li id="93e5" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">多尺度鉴别器(这里我们有两个鉴别器，实际上有些可能有三个！)对于增强不同比例的完整图像的纹理细节是有用的。</li><li id="b7ed" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">在场景复杂的情况下，填补缺失的部分仍然具有挑战性。另一方面，自然场景相对容易完成。</li></ul><h1 id="31e7" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">外卖食品</h1><p id="ed5e" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">在这里，我想列出一些对以后的帖子有用的点。</p><ul class=""><li id="3389" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">记住，具有扩展卷积的全卷积网络是用于图像修复的典型网络结构。它允许不同大小的输入，并为完全连接的层提供类似的功能(即帮助理解图像的上下文)。如果你愿意，你可以跳转到[ <a class="ae kl" href="https://medium.com/analytics-vidhya/review-of-deepgin-deep-generative-inpainting-network-for-extreme-image-inpainting-de5b191562b0" rel="noopener">这里</a> ]查看最近的修复论文，看看这种典型结构的变化。</li><li id="42ba" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">实际上，人脸图像修复比一般图像修复相对简单。这是因为我们总是在人脸数据集上训练模型来进行人脸图像修复，而数据集由许多对齐的人脸图像组成。对于一般的图像修复，我们可以在更加多样化的数据集上进行训练，如Places2，它包含来自各种类别的数百万张图像，如城市、建筑物和许多其他图像。对于一个模型来说，学习生成具有良好视觉质量的所有东西要困难得多。不管怎么说，路还很长。</li></ul><h1 id="5411" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">下一步是什么？</h1><ul class=""><li id="e6ce" class="lq lr iq jp b jq ll ju lm jy nh kc ni kg nj kk lv lw lx ly bi translated">到目前为止，我们已经深入研究了三篇非常好的早期修复论文。下次，我想复习一下。我将讨论另一篇论文，它采用了这种具有扩展卷积的全卷积网络。希望你能看到基于深度学习的图像修复的发展。尽情享受吧！:)</li></ul><h1 id="f5a2" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">参考</h1><ol class=""><li id="4a57" class="lq lr iq jp b jq ll ju lm jy nh kc ni kg nj kk od lw lx ly bi translated">饭冢聪，埃德加·西莫-塞拉，石川宽，“<a class="ae kl" href="http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf" rel="noopener ugc nofollow" target="_blank">全球和局部一致的图像完成</a>，<em class="km"> ACM Trans。《论图形》</em>，第36卷第4期第107条，出版日期:2017年7月。</li><li id="b63e" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk od lw lx ly bi translated">Deepak Pathak，Philipp krhenbüHL，Jeff Donahue，Trevor Darrell和Alexei A. Efros，"<a class="ae kl" href="https://arxiv.org/pdf/1604.07379.pdf" rel="noopener ugc nofollow" target="_blank">上下文编码器:通过修补进行特征学习</a>，"<em class="km"> Proc .计算机视觉与模式识别</em> ( <em class="km"> CVPR </em>)，2016年6月27-30日。</li><li id="125e" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk od lw lx ly bi translated">地点2数据集，<a class="ae kl" href="http://places2.csail.mit.edu/download.html" rel="noopener ugc nofollow" target="_blank">http://places2.csail.mit.edu/download.html</a></li><li id="978b" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk od lw lx ly bi translated">、罗平、王晓刚和唐晓鸥，“在野外深度学习人脸属性”，<em class="km"> Proc。计算机视觉与模式识别</em> ( <em class="km"> CVPR </em>)，2015。</li><li id="a6b0" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk od lw lx ly bi translated">Radim Tyleč ek和Radimára，“用于识别具有规则结构的对象的空间模式模板”，<em class="km"> Proc。2013年德国模式识别会议</em>。</li></ol><p id="1038" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢你花时间写这篇文章。如果您有任何问题，请随时询问或在此留下评论。下次见！:)</p></div></div>    
</body>
</html>