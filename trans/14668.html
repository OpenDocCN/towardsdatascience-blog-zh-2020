<html>
<head>
<title>ELECTRA: Developments in Masked Language Modelling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伊莱克特拉:掩蔽语言模型的发展</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/electra-developments-in-masked-language-modelling-3cf1c25fc61a?source=collection_archive---------43-----------------------#2020-10-09">https://towardsdatascience.com/electra-developments-in-masked-language-modelling-3cf1c25fc61a?source=collection_archive---------43-----------------------#2020-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9fe3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这是ELECTRA的主要功能概述，此处描述了一个模型:</h2></div><p id="5977" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://arxiv.org/pdf/2003.10555.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.10555.pdf</a></p><p id="c22b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了BERT和BERT衍生的变形金刚(XLNet，RoBERTa，ALBERT，任何以芝麻街人物命名的变形金刚)，我清楚地知道，作为一个拥有单个GPU和没有行业支持的个人爱好者，我自己不可能训练一个深度学习语言模型。当我在XLNet上为<a class="ae lb" href="https://ai.science/" rel="noopener ugc nofollow" target="_blank"> AISC </a>准备现场演示时(在YouTube上<a class="ae lb" href="https://www.youtube.com/watch?v=Mgck4XFR9GA&amp;t=2166s" rel="noopener ugc nofollow" target="_blank">这里</a>，我看到了埃利奥特·特纳的这条推文:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/f5629d585eae2ccdabcd7af1359601de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*375oxysdHJWvugN1LPLe0w.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">埃利奥特·特纳发的这条推文的截图</p></figure><p id="ea07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我训练自己模特的模糊梦想破灭了。这是回到2019年，即之前的时代。我指的是在GPT-3之前，它使用了如此惊人的计算资源，以至于目前只有在你身后有八位数的情况下才能训练。这个模型引起了我的兴趣，理论上，在单个GPU上训练四天之后，ELECTRA-small可以胜过GPT-1。他们认为完整模型(ELECTRA-base)的性能与XLNet和RoBERTa相当，而使用的资源只有它们的大约四分之一。很有趣。这是什么方法论？</p><p id="e315" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型的架构和大多数超参数与<a class="ae lb" rel="noopener" target="_blank" href="/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"> BERT </a>中的相同，因此这里不再赘述。</p><p id="fcf2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lo">替换令牌检测</em>是他们选择的预训练方法。BERT用[MASK]有选择地替换序列中的记号，而ELECTRA使用生成器用看似合理的替代单词替换记号。</p><p id="cb8a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输入序列是记号列表<strong class="kh ir">x</strong>=【x1，… xn】。MLM试图用其他似是而非的词来替换k个符号，得到一个k个掩码的列表[m1，…，mk]。对于这篇论文，他们建议屏蔽掉大约15%的输入令牌。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/a9f54fafe8494d8652a1bfb4ec141177.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*LFzSE23o-4gap0zhxYmTsw.png"/></div></figure><p id="c37d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个过程产生了<strong class="kh ir"> x </strong> ^corrupt，这是一个在m 个位置插入了看似合理的备选单词的记号列表。</p><p id="a979" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，鉴别者的工作就是确定句子中的单词是原词还是改词。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lq"><img src="../Images/66826dd8795df4dc3af8266f971fbb9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NFumJ_QbnOopk4q92PeN2A.png"/></div></div></figure><p id="7571" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">MLM和鉴别器的损失函数如下:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lv"><img src="../Images/1d9e87f926543e1fc5c641b06805e255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SgqfZwY_yeLO2vsjFr8rfg.png"/></div></div></figure><p id="d763" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然这看起来很像一个甘，发电机是最大的可能性，而不是敌对训练。另一个区别是，如果生成器生成一个与原始令牌相同的令牌，该令牌将被标记为原始令牌，而不是生成令牌。</p><h1 id="4ac0" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">实验</h1><p id="2755" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">这里的基本模型的预训练数据与BERT相同，是来自维基百科和图书语料库的33亿个标记。伊莱克特-拉奇在XLNet数据上接受训练，XLNet数据通过ClueWeb、CommonCrawl和GigaWord添加到BERT数据中。</p><p id="0c77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">模型的评估是在GLUE benchmark和SQuAD上完成的，两者都在这里解释了<a class="ae lb" href="https://mccormickml.com/2019/11/05/GLUE/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="1969" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">车型扩展</strong></p><p id="f4bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">伊莱克特做了一些改进，提高了模型的准确性。</p><p id="803f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">权重共享:</strong>生成器和鉴别器中使用的嵌入是共享的。这在训练相同数量的时期时，在没有重量约束的情况下，使用相同的模型参数，在准确性上产生了小的提升。只有嵌入权重是共享的，共享所有权重具有要求两个网络大小相同的显著缺点。为什么这在这里工作得如此好的一个理论是，利用屏蔽语言建模，要被区分的输入记号和被破坏的记号驻留在相同的向量空间中。在这种情况下，生成器学习一个嵌入空间，鉴别器有效地学习一个嵌入空间和嵌入空间之间的变换是没有意义的。</p><p id="2ddc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">小型发电机:</strong></p><p id="f8b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这在很大程度上是前面扩展的必然结果，因为如果生成器和鉴别器共享所有权重并且大小相同，则模型每个训练步骤需要的计算量是仅使用纯[掩码]令牌时的两倍。在这个模型中，他们探索使用一个unigram生成器来生成屏蔽令牌。由于使用了各种尺寸的发生器，他们选定的发生器大约是鉴频器尺寸的0.25-0.5倍。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mt"><img src="../Images/1c4b0b21b25ba1703e44794fc7b1df13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oXT6X7e3BeusJniPpq3ppw.png"/></div></div></figure><p id="b83a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">训练算法:</strong></p><p id="69e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Clark等人从整体上为ELECTRA尝试了多种高级训练算法，并确定了以下内容:</p><ol class=""><li id="5415" class="mu mv iq kh b ki kj kl km ko mw ks mx kw my la mz na nb nc bi translated">仅训练发生器(最大可能性)进行<em class="lo"> n </em>步。</li><li id="060b" class="mu mv iq kh b ki nd kl ne ko nf ks ng kw nh la mz na nb nc bi translated">用生成器的权重初始化鉴别器的权重。然后在<em class="lo"> n </em>步中用鉴频器损失函数训练鉴频器，保持发电机的重量不变。</li></ol><p id="68f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">大型型号:</strong></p><p id="2f62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了与典型尺寸的SOTA模型相比较，基本的ELECTRA必须相当大。他们使用与ELECTRA-400k相同的超参数和训练时间来训练自己的BERT-Large模型。</p><p id="850f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我很好奇的一点是，在这篇论文中，模型相对于彼此需要多长时间来训练是一个有点困惑的问题。他们说的是ELECTRA-Large和BERT-Large大小一样，但是他们也用和ELECTRA-400k一样的超参数和训练时间训练了自己的BERT。他们描述的ELECTRA的卖点是使用更少的物理计算资源，但我希望他们在其他大型模型旁边更明确地概述这些物理和时间资源，以便进行比较。他们确实列出了train FLOPs(每秒浮点运算次数)，这通常可以作为衡量他们的GPU计算速度的指标——但在附录中，他们澄清了“运算”被算作数学运算，而不是机器指令(通常是这样)。如果能对所使用的资源以及培训前和培训所花费的时间进行比较，我们将不胜感激。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/fc27952e1c1f39995b56b2afdb1b6d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*6Lw1k6HVStTdPqJwAuv1VQ.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">胶水开发组的结果。</p></figure><p id="d0a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他们自己的模型使用ELECTRA-400k中使用的相同超参数进行训练。</p><p id="ed29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">效率分析:</strong></p><p id="6c82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他们怀疑，必须分析字符串中的所有记号，而不是填补显式掩码造成的空白，这导致了伊莱克特的效率提高。他们创造了伊莱克特15%，它只计算被屏蔽掉的15%令牌的鉴别器损耗。基地伊莱克特在胶水任务上得分85%，而伊莱克特15%得分82.4%，与伯特的82.2%不相上下。</p><p id="2896" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">阴性结果</strong>:</p><p id="2f8d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这项研究中，他们尝试了许多训练范式，如模型大小、生成器架构、重量捆绑，有些范式普遍不成功，因此没有出现在论文的主体中。</p><ol class=""><li id="fbdf" class="mu mv iq kh b ki kj kl km ko mw ks mx kw my la mz na nb nc bi translated">战略性地对稀有令牌应用屏蔽。与常规的BERT相比，这并没有导致任何显著的加速。</li><li id="7672" class="mu mv iq kh b ki nd kl ne ko nf ks ng kw nh la mz na nb nc bi translated">提高发生器的温度，或不允许正确的字输出并没有改善结果。</li><li id="9114" class="mu mv iq kh b ki nd kl ne ko nf ks ng kw nh la mz na nb nc bi translated">添加一系列句子级别的对比标记，就像SpanBERT的蒙面语言建模版本。这实际上降低了胶水和小队任务的模型分数。</li></ol><p id="db66" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">结论:</strong></p><p id="3481" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然这不是对抗性训练，但它是对比学习如何有效应用于语言的一个例子。从广义上讲，对比学习包括区分观察到的数据点和虚构的例子。BERT、Ernie、XLNet、ELMo、RoBERTa和SpanBERT都引入或扩展了屏蔽语言建模范例，其中模型猜测单个屏蔽标记的正确标记，伊莱克特通过引入屏蔽可能是鉴别器接收的序列中的任何标记的可能性来进一步扩展它。伊莱克特拉现在是拥抱脸和简单变形金刚的一部分。</p></div></div>    
</body>
</html>