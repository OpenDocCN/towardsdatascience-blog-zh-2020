<html>
<head>
<title>Interpreting Black-Box ML Models using LIME</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用LIME解释黑盒ML模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpreting-black-box-ml-models-using-lime-4fa439be9885?source=collection_archive---------24-----------------------#2020-10-16">https://towardsdatascience.com/interpreting-black-box-ml-models-using-lime-4fa439be9885?source=collection_archive---------24-----------------------#2020-10-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4cb0" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/Model-Interpretability" rel="noopener" target="_blank">模型可解释性</a></h2><div class=""/><div class=""><h2 id="5f4c" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">通过乳腺癌数据建模直观地理解石灰</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a2d6fed28d09afc2b8fb3abf9d4cb71d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bbDbnfz1187varbQ"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@nci?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">国立癌症研究所</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="12ed" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">对于任何人来说，在医学领域支持机器学习的潜力几乎是老生常谈。有很多例子可以支持这种说法——其中一个例子是微软利用医学成像数据帮助临床医生和放射科医生<a class="ae le" href="https://www.microsoft.com/en-us/research/project/medical-image-analysis/" rel="noopener ugc nofollow" target="_blank">做出准确的癌症诊断</a>。同时，复杂的人工智能算法的发展极大地提高了这种诊断的准确性。毫无疑问，医学数据的应用如此惊人，人们有充分的理由对其好处感到兴奋。</p><p id="7d10" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，这种尖端算法是黑盒，可能很难解释。黑盒模型的一个例子是<a class="ae le" href="https://bdtechtalks.com/2020/07/27/black-box-ai-models/" rel="noopener ugc nofollow" target="_blank">深度神经网络</a>，在输入数据通过网络中的数百万个神经元后做出单个决策。这种黑盒模型不允许临床医生用他们先前的知识和经验来验证模型的诊断，使得基于模型的诊断不太可信。</p><p id="6d02" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">事实上，最近对欧洲放射科医生的调查描绘了一幅在放射学中使用黑盒模型的现实画面。<strong class="lh ja">调查显示</strong> <a class="ae le" href="https://insightsimaging.springeropen.com/articles/10.1186/s13244-019-0798-3" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">只有55.4%的临床医生</strong> </a> <strong class="lh ja">认为患者不会接受没有医师监督的纯人工智能应用。</strong> [1]</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/7b44577e228dd3df106cb741bf513ced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*M9Vs90EgD2FbJ_sj5pqixw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在接受调查的635名医生中，超过一半的人认为患者还没有准备好接受仅由AI生成的报告。图片作者。</p></figure><blockquote class="ml"><p id="692e" class="mm mn iq bd mo mp mq mr ms mt mu ma dk translated">下一个问题是:如果人工智能不能完全取代医生的角色，那么人工智能如何帮助医生提供准确的诊断？</p></blockquote><p id="65a0" class="pw-post-body-paragraph lf lg iq lh b li mw ka lk ll mx kd ln lo my lq lr ls mz lu lv lw na ly lz ma ij bi translated"><strong class="lh ja">这促使我探索有助于解释机器学习模型的现有解决方案。</strong>一般来说，机器学习模型可以分为可解释的模型和不可解释的模型。不严格地说，可解释的模型提供与每个输入特征的重要性相关的输出。此类模型的示例包括线性回归、逻辑回归、决策树和决策规则等。另一方面，神经网络形成了大量无法解释的模型。</p><p id="c4e2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有许多<a class="ae le" href="https://christophm.github.io/interpretable-ml-book/agnostic.html" rel="noopener ugc nofollow" target="_blank">解决方案</a>可以帮助解释黑箱模型。这些解决方案包括Shapley值、部分依赖图和局部可解释模型不可知解释(LIME ),这些在机器学习实践者中很流行。今天，我将集中讨论石灰。</p><blockquote class="ml"><p id="b3fa" class="mm mn iq bd mo mp nb nc nd ne nf ma dk translated">根据Ribeiro等人[2] 的LIME <a class="ae le" href="https://arxiv.org/pdf/1602.04938.pdf" rel="noopener ugc nofollow" target="_blank">论文，LIME的目标是<em class="mv">“在对分类器局部忠实的可解释表示上识别一个可解释模型”</em>。<strong class="ak">换句话说，石灰能够解释<em class="mv">某个</em>特定点的分类结果。</strong> LIME也适用于所有类型的模型，使其<em class="mv">与模型无关。</em></a></p></blockquote><h1 id="fe71" class="ng nh iq bd ni nj nk nl nm nn no np nq kf nr kg ns ki nt kj nu kl nv km nw nx bi translated"><span class="l mc md me bm mf mg mh mi mj di"> E </span></h1><p id="167c" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">这听起来很难理解。让我们一步一步地分解它。想象我们有下面的玩具数据集，有两个特征。每个数据点都与一个基本事实标签(正或负)相关联。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/286776332c1f373a55b9741e452b3129.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*-kDCLvugNavRnL4ZSvK8yw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="b5f5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从数据点可以看出，线性分类器将不能识别分隔阳性和阴性标记的边界。因此，我们可以训练一个非线性模型，比如神经网络，来对这些点进行分类。如果模型训练有素，它能够预测落在深灰色区域的新数据点为正，而落在浅灰色区域的另一个新数据点为负。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/a485b1df41187c5598cd71d3a75659c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*ZJ1GWuLmZ3N3RahbzceBIg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="5237" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，我们对模型在特定数据点(紫色)上做出的决定感到好奇。我们问自己，为什么这个特定的点会被神经网络预测为负值？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/a906047cf1ff6d9bcaa27ae1e0519626.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*RVjt7zWVpKp8zdUqZjqrWA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="398e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以用石灰来回答这个问题。LIME首先从原始数据集中识别随机点，并根据它们到感兴趣的紫色点的距离为每个数据点分配权重。采样数据点离感兴趣的点越近，时间就越重要。(在图中，较大的点表示分配给数据点的权重较大。)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f2a5427f992eeca70934b5636e21c956.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*73ZTqP2uUdlZF3osPWbM7g.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><p id="586e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">利用这些不同权重的点，LIME提出了一个具有最高可解释性和局部保真度的解释。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/fedfb48e9131f7d248eb8ee956b5afa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*-nKIEetELN4tibhR7O5i9w.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><p id="9c62" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用这组标准，LIME将紫色线确定为兴趣点的学习解释。我们看到，紫色线能够解释神经网络在感兴趣的数据点附近的决策边界，但无法解释其更远的决策边界。换句话说，学习的解释具有高的局部保真度，但是低的全局保真度。</p><p id="a508" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们看看LIME的作用:现在，我将重点关注LIME在解释使用威斯康星州乳腺癌数据训练的机器学习模型中的使用。</p><h1 id="158e" class="ng nh iq bd ni nj nk nl nm nn no np nq kf oi kg ns ki oj kj nu kl ok km nw nx bi translated">威斯康星乳腺癌数据集:了解癌细胞的预测因子</h1><p id="3b60" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">UCI在1992年发表的<a class="ae le" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)" rel="noopener ugc nofollow" target="_blank">威斯康星州乳腺癌数据集</a>【3】，包含699个数据点。每个数据点代表恶性或良性的细胞样本。对于以下特征，每个样品还被赋予1到10的数字。</p><ul class=""><li id="dc43" class="ol om iq lh b li lj ll lm lo on ls oo lw op ma oq or os ot bi translated">团块厚度</li><li id="e187" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma oq or os ot bi translated">细胞大小的均匀性</li><li id="0772" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma oq or os ot bi translated">细胞形状的均匀性</li><li id="8227" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma oq or os ot bi translated">单一上皮细胞大小</li><li id="5419" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma oq or os ot bi translated">有丝分裂</li><li id="f407" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma oq or os ot bi translated">正常核仁</li><li id="4556" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma oq or os ot bi translated">平淡的染色质</li><li id="b048" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma oq or os ot bi translated">裸核</li><li id="a48b" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma oq or os ot bi translated">边缘粘连</li></ul><p id="8477" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们试着去理解这些特征的意义。下图使用数据集的特征显示了良性和恶性细胞之间的差异。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oz"><img src="../Images/278a5649734a7e306020d66194d1c59d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h9ameOtunjcEeXs8UKg78g.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/09fd85b456db4089a04a37b9ee28e02a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UZuhEIDK9xEuTHFy5HGDBA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">感谢我医学院的朋友岳婷对这些特性的解释。图片作者。</p></figure><blockquote class="ml"><p id="f3d4" class="mm mn iq bd mo mp mq mr ms mt mu ma dk translated">从该图中，我们看到每个特征的值越高，细胞越有可能是恶性的。</p></blockquote><h1 id="c147" class="ng nh iq bd ni nj nk nl nm nn no np nq kf nr kg ns ki nt kj nu kl nv km nw nx bi translated">预测细胞是恶性的还是良性的</h1><p id="c527" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">既然我们理解了数据的含义，让我们开始编码吧！我们首先读取数据，并通过删除不完整的数据点和重新格式化class列来清理数据。</p><h2 id="efdd" class="pb nh iq bd ni pc pd dn nm pe pf dp nq lo pg ph ns ls pi pj nu lw pk pl nw iw bi translated">数据导入、清理和浏览</h2><pre class="kp kq kr ks gt pm pn po pp aw pq bi"><span id="f7c5" class="pb nh iq pn b gy pr ps l pt pu"><strong class="pn ja">#Data Importing and Cleaning<br/></strong>import pandas as pd</span><span id="5435" class="pb nh iq pn b gy pv ps l pt pu">df = pd.read_csv("/BreastCancerWisconsin.csv", <br/>                 dtype = 'float', header = 0)<br/>df = df.dropna() #All rows with missing values are removed.</span><span id="1fad" class="pb nh iq pn b gy pv ps l pt pu"><em class="pw"># The original data set labels benign and malignant cell using a value of 2 and 4 in the Class column. This code block formats it such that a benign cell is of class 0 and a malignant cell is of class 1.</em></span><span id="9b3a" class="pb nh iq pn b gy pv ps l pt pu">def reformat(value):<br/>    if value == 2: <br/>        return 0 #benign<br/>    elif value == 4:<br/>        return 1 #malignant</span><span id="22c8" class="pb nh iq pn b gy pv ps l pt pu">df['Class'] = df.apply(lambda row: reformat(row['Class']), axis = 'columns')</span></pre><p id="b7ce" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">去掉不完整的数据后，我们简单的探究一下数据。通过绘制细胞样品类别(恶性或良性)的分布图，我们发现良性(0类)细胞样品比良性(1类)细胞样品多。</p><pre class="kp kq kr ks gt pm pn po pp aw pq bi"><span id="14b9" class="pb nh iq pn b gy pr ps l pt pu">import seaborn as sns<br/>sns.countplot(y='Class', data=df)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi px"><img src="../Images/13dbbf4cbfce82e7dda029f37652b908.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*Wn8n7QcabsV1_o_pcCjHfw.png"/></div></figure><p id="e12b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过可视化每个特征的直方图，我们发现大多数特征具有1或2的模式，除了团块厚度和空白染色质，它们的分布从1到10更均匀地展开。这表明团块厚度和空白染色质可能是该类别的较弱预测因子。</p><pre class="kp kq kr ks gt pm pn po pp aw pq bi"><span id="9d47" class="pb nh iq pn b gy pr ps l pt pu">from matplotlib import pyplot as plt<br/>fig, axes = plt.subplots(4,3, figsize=(20,15))<br/>for i in range(0,4):<br/>    for j in range(0,3):<br/>        axes[i,j].hist(df.iloc[:,1+i+j])<br/>        axes[i,j].set_title(df.iloc[:,1+i+j].name)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi py"><img src="../Images/8971009abf50b71211d381ff4fade37c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lmZlVQaI2MMQLAWVNkyt1w.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pz"><img src="../Images/5148ff078654dc76abb79da853dc11af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GuZZxj4cnrbEbV8Bc2DCRg.png"/></div></div></figure><h2 id="b41c" class="pb nh iq bd ni pc pd dn nm pe pf dp nq lo pg ph ns ls pi pj nu lw pk pl nw iw bi translated">模型训练和测试</h2><p id="d639" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">然后，将数据集以80%-10%-10%的比例分成典型的训练-验证-测试集，并使用Sklearn建立K-最近邻模型。经过一些超参数调整(未显示)，发现k=10的模型在评估阶段表现良好，其F1值为0.9655。代码块如下所示。</p><pre class="kp kq kr ks gt pm pn po pp aw pq bi"><span id="7da5" class="pb nh iq pn b gy pr ps l pt pu">from sklearn.model_selection import train_test_split<br/>from sklearn.model_selection import KFold<br/></span><span id="7744" class="pb nh iq pn b gy pv ps l pt pu"><strong class="pn ja"># Train-test split</strong><br/>X_traincv, X_test, y_traincv, y_test = train_test_split(data, target, test_size=0.1, random_state=42)<br/></span><span id="ec5b" class="pb nh iq pn b gy pv ps l pt pu"><strong class="pn ja"># K-Fold Validation<br/></strong>kf = KFold(n_splits=5, random_state=42, shuffle=True)</span><span id="2751" class="pb nh iq pn b gy pv ps l pt pu">for train_index, test_index in kf.split(X_traincv):<br/>    X_train, X_cv = X_traincv.iloc[train_index], X_traincv.iloc[test_index]<br/>    y_train, y_cv = y_traincv.iloc[train_index], y_traincv.iloc[test_index]</span><span id="3db3" class="pb nh iq pn b gy pv ps l pt pu">from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.metrics import f1_score,</span><span id="03d0" class="pb nh iq pn b gy pv ps l pt pu"><strong class="pn ja"># Train KNN Model<br/></strong>KNN = KNeighborsClassifier(k=10)<br/>KNN.fit(X_train, y_train)</span><span id="ae73" class="pb nh iq pn b gy pv ps l pt pu"><strong class="pn ja"># Evaluate the KNN model<br/></strong>score =  f1_score(y_testset, y_pred, average="binary", pos_label = 4)<br/>print ("{} =&gt; F1-Score is {}" .format(text, round(score,4)))</span></pre><h1 id="49b6" class="ng nh iq bd ni nj nk nl nm nn no np nq kf oi kg ns ki oj kj nu kl ok km nw nx bi translated">使用石灰的模型解释</h1><p id="e65b" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">一个Kaggle鉴赏家可能会说，这个结果很好，我们可以在这里结束这个项目。然而，人们应该对模型的决策持怀疑态度，即使模型在评估中表现良好。因此，我们用石灰来解释KNN模型对这个数据集做出的决定。这通过检查所做的决定是否符合我们的直觉来验证模型的有效性。</p><pre class="kp kq kr ks gt pm pn po pp aw pq bi"><span id="303a" class="pb nh iq pn b gy pr ps l pt pu">import lime<br/>import lime.lime_tabular</span><span id="0c6d" class="pb nh iq pn b gy pv ps l pt pu"><strong class="pn ja"># Preparation for LIME</strong><br/>predict_fn_rf = lambda x: KNN.predict_proba(x).astype(float)</span><span id="eaf9" class="pb nh iq pn b gy pv ps l pt pu"><strong class="pn ja"># Create a LIME Explainer</strong><br/>X = X_test.values<br/>explainer = lime.lime_tabular.LimeTabularExplainer(X,feature_names =X_test.columns, class_names = ['benign','malignant'], kernel_width = 5)</span><span id="3cc3" class="pb nh iq pn b gy pv ps l pt pu"><strong class="pn ja"># Choose the data point to be explained</strong><br/>chosen_index = X_test.index[j]<br/>chosen_instance = X_test.loc[chosen_index].values</span><span id="1447" class="pb nh iq pn b gy pv ps l pt pu"><strong class="pn ja"># Use the LIME explainer to explain the data point</strong><br/>exp = explainer.explain_instance(chosen_instance, predict_fn_rf, num_features = 10)</span><span id="5668" class="pb nh iq pn b gy pv ps l pt pu">exp.show_in_notebook(show_all=False)</span></pre><p id="41b7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这里，我挑选了三点来说明如何使用石灰。</p><h2 id="be03" class="pb nh iq bd ni pc pd dn nm pe pf dp nq lo pg ph ns ls pi pj nu lw pk pl nw iw bi translated"><em class="mv">解释样本被预测为恶性的原因</em></h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qa"><img src="../Images/847bdfd2d2e313013a134482d39a1c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oCQSNSgmB9PGRnJary8AOw.png"/></div></div></figure><p id="61bf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这里，我们有一个数据点，实际上是恶性的，并预测是恶性的。在左图中，我们看到KNN模型预测该点有接近100%的概率是恶性的。在中间，我们观察到LIME能够按照重要性的顺序，使用感兴趣的数据点的每个特征来解释这个预测。根据莱姆的说法，</p><ol class=""><li id="1cbc" class="ol om iq lh b li lj ll lm lo on ls oo lw op ma qb or os ot bi translated">事实上，样本的裸细胞核值大于6.0，这使其更有可能是恶性的。</li><li id="090a" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma qb or os ot bi translated">由于样本具有较高的边缘粘连，它更可能是恶性的而不是良性的。</li><li id="5a7d" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma qb or os ot bi translated">由于样本的凝块厚度大于4，它更有可能是恶性的。</li><li id="29b9" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma qb or os ot bi translated">另一方面，样本的有丝分裂值≤1.00的事实使其更有可能是良性的。</li></ol><p id="de36" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">总的来说，考虑到样本的所有特征(在右图中)，该样本被预测为恶性的。</p><p id="594c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这四个观察结果符合我们的直觉和我们对癌细胞的了解。了解了这一点，我们更加确信模型根据我们的直觉做出了正确的预测。让我们看另一个例子。</p><h2 id="aa9d" class="pb nh iq bd ni pc pd dn nm pe pf dp nq lo pg ph ns ls pi pj nu lw pk pl nw iw bi translated"><em class="mv">解释为什么样本被预测为良性</em></h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qc"><img src="../Images/fe1396388c02b7d3d3110affd78c6acc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UCw0Le_nvNgqrL9gXa4WQQ.png"/></div></div></figure><p id="7b9a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里，我们有一个细胞样本，预测是良性的，实际上是良性的。LIME通过引用(以及其他原因)解释了为什么会出现这种情况</p><ol class=""><li id="c4b0" class="ol om iq lh b li lj ll lm lo on ls oo lw op ma qb or os ot bi translated">该样品的裸核值≤ 1</li><li id="2c22" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma qb or os ot bi translated">该样品的正常核仁值≤ 1</li><li id="94b1" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma qb or os ot bi translated">它还具有≤1的簇厚度</li><li id="41eb" class="ol om iq lh b li ou ll ov lo ow ls ox lw oy ma qb or os ot bi translated">细胞形状的均匀性也≤ 1</li></ol><p id="e4f3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">同样，这些符合我们对为什么细胞是良性的直觉。</p><h2 id="1f46" class="pb nh iq bd ni pc pd dn nm pe pf dp nq lo pg ph ns ls pi pj nu lw pk pl nw iw bi translated"><em class="mv">解释样本预测不明确的原因</em></h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qd"><img src="../Images/e3615ea3eeebb374d81cfca648af5fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PAdauLYtvDKjiduVrIthHA.png"/></div></div></figure><p id="89f2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在最后一个例子中，我们看到该模型不能以很高的可信度预测细胞是良性的还是恶性的。使用LIME的解释，你能看出为什么会这样吗？</p><h2 id="af8c" class="pb nh iq bd ni pc pd dn nm pe pf dp nq lo pg ph ns ls pi pj nu lw pk pl nw iw bi translated">结论</h2><p id="8d32" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">LIME的用途不仅限于表格数据，还包括文本和图像，这使得它的用途非常广泛。然而，仍有工作要做。例如，这篇论文的作者认为，当前的算法应用于图像时<a class="ae le" href="https://github.com/marcotcr/lime/blob/master/CONTRIBUTING.md" rel="noopener ugc nofollow" target="_blank">太慢，没有用</a>。尽管如此，LIME在弥合黑盒模型的有用性和难处理性之间的差距方面仍然非常有用。如果你想开始使用LIME，一个很好的起点是<a class="ae le" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank"> LIME的Github页面</a>。</p><p id="5481" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你对机器学习的可解释性感兴趣，一定要看看我的文章，这篇文章着重于使用<a class="ae le" rel="noopener" target="_blank" href="/what-makes-a-wine-good-ea370601a8e4">部分依赖图来解释黑盒模型。</a></p><p id="75e9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果有任何反馈，请随时通过LinkedIn联系我。感谢您的宝贵时间！</p><div class="qe qf gp gr qg qh"><a href="https://www.linkedin.com/in/voon-hao-tang/" rel="noopener  ugc nofollow" target="_blank"><div class="qi ab fo"><div class="qj ab qk cl cj ql"><h2 class="bd ja gy z fp qm fr fs qn fu fw iz bi translated">特拉维斯唐| LinkedIn</h2><div class="qo l"><p class="bd b dl z fp qm fr fs qn fu fw dk translated">Gojek的数据分析师</p></div></div><div class="qp l"><div class="qq l qr qs qt qp qu ky qh"/></div></div></a></div><h2 id="45b5" class="pb nh iq bd ni pc pd dn nm pe pf dp nq lo pg ph ns ls pi pj nu lw pk pl nw iw bi translated">参考</h2><p id="4452" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">[1] Codari，m .，Melazzini，l .，Morozov，S.P. <em class="pw">等人</em>，<a class="ae le" href="https://doi.org/10.1186/s13244-019-0798-3" rel="noopener ugc nofollow" target="_blank">人工智能对放射学的影响:欧洲放射学学会成员的EuroAIM调查</a> (2019)，成像洞察</p><p id="6160" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2] M. Ribeiro，S. Singh和C. Guestrin，<a class="ae le" href="http://dx.doi.org/10.1145/2939672.2939778" rel="noopener ugc nofollow" target="_blank">‘我为什么要相信你？’解释任何分类者的预测</a> (2016)，KDD</p><p id="f3d3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3]威廉·h·沃尔伯格博士，<a class="ae le" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)" rel="noopener ugc nofollow" target="_blank">威斯康星乳腺癌数据库</a> (1991)，威斯康星大学医院，麦迪逊</p></div></div>    
</body>
</html>