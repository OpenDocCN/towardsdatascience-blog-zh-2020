<html>
<head>
<title>Board Game Image Recognition using Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于神经网络的棋盘游戏图像识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/board-game-image-recognition-using-neural-networks-116fc876dafa?source=collection_archive---------11-----------------------#2020-10-21">https://towardsdatascience.com/board-game-image-recognition-using-neural-networks-116fc876dafa?source=collection_archive---------11-----------------------#2020-10-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dc09" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何使用计算机视觉技术来识别棋子及其在棋盘上的位置</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6f4843d5d5318720898d6aa74abefccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bpd3UPJtkD7RTugc"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">蒂姆·福斯特在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="45a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">利用计算机视觉技术和卷积神经网络(CNN)，为这个项目创建的算法对棋子进行分类，并识别它们在棋盘上的位置。最终的应用程序保存图像，以可视化的表现，并输出棋盘的2D图像，以查看结果(见下文)。本文的目的是逐步完成这个项目，以便它可以作为新迭代的“基础”。参见<a class="ae kv" href="https://github.com/andrewleeunderwood/project_MYM" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上该项目的代码。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/c2ed2de6f58efcfd31778a8281fa4781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wKITWi9maBvBSpouXAJwFQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(左)来自现场摄像机的画面和(右)棋盘的2D图像</p></figure><h1 id="8cf7" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">数据</h1><p id="5d00" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我对这个项目的数据集有很高的要求，因为我知道它最终会推动我的成果。我在网上找到的国际象棋数据集要么是用不同的国际象棋设置(<a class="ae kv" href="https://github.com/jialinding/ChessVision/tree/master/images" rel="noopener ugc nofollow" target="_blank"> Chess Vision </a>)、不同的相机设置(<a class="ae kv" href="https://www.dropbox.com/s/618l4ddoykotmru/Chess%20ID%20Public%20Data.zip?dl=0" rel="noopener ugc nofollow" target="_blank"> Chess ID Public Data </a>)创建的，要么是两者都用(<a class="ae kv" href="https://www.kaggle.com/joeymeyer/raspberryturk" rel="noopener ugc nofollow" target="_blank"> Raspberry Turk Project </a>)，这让我创建了自己的数据集。我使用我的<a class="ae kv" href="https://www.amazon.com/Best-Value-Tournament-Chess-Set/dp/B003DMITE6/ref=sr_1_39?crid=3AN8U1ERTLVID&amp;dchild=1&amp;keywords=chess+set&amp;qid=1601908425&amp;sprefix=chess+%2Caps%2C195&amp;sr=8-39" rel="noopener ugc nofollow" target="_blank">国际象棋</a>和相机设置(<a class="ae kv" href="https://www.cnet.com/reviews/gopro-hero6-black-review/" rel="noopener ugc nofollow" target="_blank"> GoPro Hero6 Black </a>在“第一人称视角”)生成了一个自定义数据集，这使得我的模型更加准确。该数据集包含2，406幅图像，分为13类(见下文)。<strong class="ky ir">要点:这花费了我大部分的时间，但最终你会希望在图像上训练你的模型，使其尽可能接近你的应用程序中使用的图像。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/3ef0737f0631fe6b5eb5386bdb17230f.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*S_N8KioN7ZvFrQjhrgymqA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">自定义数据集的细分(<em class="lt">图片由作者</em>提供)</p></figure><p id="6842" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了构建这个数据集，我首先创建了<em class="ms"> capture_data.py </em>，它在点击<em class="ms"> S键</em>时从视频流中提取一帧，并将其保存到导演。这个程序允许我无缝地改变棋盘上的棋子，并一遍又一遍地捕捉棋盘的图像，直到我建立了大量不同的棋盘配置。接下来，我创建了<em class="ms"> create_data.py </em>，通过使用下一节讨论的电路板检测技术将帧裁剪成独立的片段。最后，我将裁剪后的图像分类，将它们放入带标签的文件夹中。然后…瞧！</p><h1 id="c0f1" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">棋盘检测</h1><p id="b70c" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">对于棋盘检测，我想做一些比使用<em class="ms">findchesboardcorners</em>(OpenCV)更复杂的事情，但不像CNN那么高级。使用低级和中级计算机视觉技术来寻找棋盘的特征，然后将这些特征转化为外边界和64个独立方格的坐标。该过程围绕实现<a class="ae kv" rel="noopener" target="_blank" href="/canny-edge-detection-step-by-step-in-python-computer-vision-b49c3a2d8123"> Canny边缘检测</a>和<a class="ae kv" rel="noopener" target="_blank" href="/lines-detection-with-hough-transform-84020b3b1549"> Hough变换</a>来生成相交的水平线和垂直线。分层聚类用于根据距离对交叉点进行分组，并对各组进行平均以创建最终坐标(见下文)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/341553142433c2edaffb1b24a2a38ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*srfAGMD1Mk_HsoLzKVCDhg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">完整的棋盘检测过程(<em class="lt">图片作者</em></p></figure><h1 id="70e2" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">棋子分类</h1><p id="72dd" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">当我开始这个项目时，我知道我想使用Keras/TensorFlow创建一个CNN模型，并对棋子进行分类。然而，在创建我的数据集之后，在给定数据集大小的情况下，CNN本身不会给我想要的结果。为了克服这个障碍，我利用了<a class="ae kv" href="https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/" rel="noopener ugc nofollow" target="_blank"><em class="ms">imagedata generator</em></a>和<a class="ae kv" href="https://keras.io/guides/transfer_learning/" rel="noopener ugc nofollow" target="_blank"> transfer learning </a>，它们扩充了我的数据，并使用其他预先训练的模型作为基础。</p><h2 id="9e52" class="mu lv iq bd lw mv mw dn ma mx my dp me lf mz na mg lj nb nc mi ln nd ne mk nf bi translated">创建CNN模型</h2><p id="10dc" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">为了使用GPU，我在云中创建并训练了CNN模型，这大大减少了训练时间。<strong class="ky ir">快速提示:</strong><a class="ae kv" href="https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">Google Colab</strong></a><strong class="ky ir">是一种快速简单地开始使用GPU的方法。</strong>为了提高我的数据的有效性，我使用<em class="ms"> ImageDataGenerator </em>来扩充我的原始图像，并将我的模型暴露给不同版本的数据。函数<em class="ms"> ImageDataGenerator </em>随机旋转、重新缩放和翻转(水平)每个时期的我的训练数据，本质上创建了更多的数据(见下文)。虽然有更多的转换选项，但我发现这些对于这个项目来说是最有效的。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="8b35" class="mu lv iq nh b gy nl nm l nn no"><strong class="nh ir">from</strong> <strong class="nh ir">keras.preprocessing.image</strong> <strong class="nh ir">import</strong> ImageDataGenerator</span><span id="811c" class="mu lv iq nh b gy np nm l nn no">datagen = ImageDataGenerator(<br/>        rotation_range=5,<br/>        rescale=1./255,<br/>        horizontal_flip=<strong class="nh ir">True</strong>,<br/>        fill_mode='nearest')</span><span id="f2e5" class="mu lv iq nh b gy np nm l nn no">test_datagen = ImageDataGenerator(rescale=1./255)</span><span id="27cb" class="mu lv iq nh b gy np nm l nn no">train_gen = datagen.flow_from_directory(<br/>    folder + '/train',<br/>    target_size = image_size,<br/>    batch_size = batch_size,<br/>    class_mode = 'categorical',<br/>    color_mode = 'rgb',<br/>    shuffle=<strong class="nh ir">True</strong>)</span><span id="f21b" class="mu lv iq nh b gy np nm l nn no">test_gen = test_datagen.flow_from_directory(<br/>    folder + '/test',<br/>    target_size = image_size,<br/>    batch_size = batch_size,<br/>    class_mode = 'categorical',<br/>    color_mode = 'rgb',<br/>    shuffle=<strong class="nh ir">False</strong>)</span></pre><p id="edae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我没有从头开始训练一个全尺寸模型，而是通过利用一个预训练模型来实现迁移学习，并添加了一个顶层模型，使用我的自定义数据集进行训练。我遵循典型的迁移学习工作流程:</p><p id="7bb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1.从先前训练的模型(VGG16)中提取图层。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="fbc5" class="mu lv iq nh b gy nl nm l nn no"><strong class="nh ir">from</strong> <strong class="nh ir">keras.applications.vgg16</strong> <strong class="nh ir">import</strong> VGG16</span><span id="256b" class="mu lv iq nh b gy np nm l nn no">model = VGG16(weights='imagenet')<br/>model.summary()</span></pre><p id="637a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.冷冻它们是为了避免在训练中破坏它们所包含的任何信息。</p><p id="c4a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.在冻结层上增加了新的可训练层。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="da20" class="mu lv iq nh b gy nl nm l nn no"><strong class="nh ir">from</strong> <strong class="nh ir">keras.models</strong> <strong class="nh ir">import</strong> Sequential<br/><strong class="nh ir">from</strong> <strong class="nh ir">keras.layers</strong> <strong class="nh ir">import</strong> Dense, Conv2D, MaxPooling2D, Flatten<br/><strong class="nh ir">from</strong> <strong class="nh ir">keras.models</strong> <strong class="nh ir">import</strong> Model</span><span id="8b29" class="mu lv iq nh b gy np nm l nn no">base_model = VGG16(weights='imagenet', include_top=<strong class="nh ir">False</strong>, input_shape=(224,224,3)) <br/> <br/><em class="ms"># Freeze convolutional layers from VGG16</em><br/><strong class="nh ir">for</strong> layer <strong class="nh ir">in</strong> base_model.layers:<br/>    layer.trainable = <strong class="nh ir">False</strong></span><span id="d362" class="mu lv iq nh b gy np nm l nn no"><em class="ms"># Establish new fully connected block</em><br/>x = base_model.output<br/>x = Flatten()(x) <br/>x = Dense(500, activation='relu')(x) <br/>x = Dense(500, activation='relu')(x)<br/>predictions = Dense(13, activation='softmax')(x)</span><span id="11f1" class="mu lv iq nh b gy np nm l nn no"><em class="ms"># This is the model we will train</em><br/>model = Model(inputs=base_model.input, outputs=predictions)<br/>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])</span></pre><p id="46ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.在自定义数据集上训练新图层。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="4ef7" class="mu lv iq nh b gy nl nm l nn no">epochs = 10</span><span id="e30c" class="mu lv iq nh b gy np nm l nn no">history = model.fit(<br/>    train_gen, <br/>    epochs=epochs,<br/>    verbose = 1,<br/>    validation_data=test_gen)</span><span id="b182" class="mu lv iq nh b gy np nm l nn no">model.save_weights('model_VGG16.h5')</span></pre><p id="13ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然我使用VGG16或VGG19作为预训练模型来创建模型，但由于验证准确性更高，我选择了使用VGG16的模型。此外，我发现最佳的历元数是10。任何大于10的数字都不会导致验证准确性的增加，并且会增加训练和验证准确性之间的差异，这暗示着过度拟合。<strong class="ky ir">收获:迁移学习让我能够充分利用深度学习在图像分类方面的优势，而不需要大型数据集。</strong></p><h2 id="76b6" class="mu lv iq bd lw mv mw dn ma mx my dp me lf mz na mg lj nb nc mi ln nd ne mk nf bi translated">结果</h2><p id="c21c" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">为了更好地可视化验证的准确性，我创建了一个模型预测的混淆矩阵。从这个图表中，很容易评估模型的优点和缺点。<strong class="ky ir">优势:</strong>空— <a class="ae kv" href="https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall" rel="noopener ugc nofollow" target="_blank">准确率</a>为99%，<a class="ae kv" href="https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall" rel="noopener ugc nofollow" target="_blank">召回率</a>为100%；白棋和黑棋(WP和BP)——F1值约为95%。<strong class="ky ir">弱点:</strong>《白骑士》(WN)——召回率高(98%)，但准确率很低(65%)；白人主教(WB)——召回率最低，为74%。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/f1650207d099e59adad125e9a2f0411a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N9k4Pfa16zG8lYr4dgTV6Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试数据的混淆矩阵(作者的<em class="lt">图片)</em></p></figure><h1 id="e13e" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">应用</h1><p id="42a3" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">该应用程序的目标是使用CNN模型并可视化每一步的性能。我创建了<em class="ms"> cv_chess.py </em>(见下面的摘录)，它清楚地显示了步骤，还创建了<em class="ms"> cv_chess_functions.py </em>，它显示了每个步骤的细节。这个应用程序保存来自实时视频流的原始帧，每个方格的64个裁剪图像，以及棋盘的最终2D图像。我以这种方式构建应用程序，以快速确定未来改进的优势和劣势。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="d446" class="mu lv iq nh b gy nl nm l nn no">print('Working...') <br/>       <br/># Save the frame to be analyzed        <br/>cv2.imwrite('frame.jpeg', frame)  <br/>      <br/># Low-level CV techniques (grayscale &amp; blur)        <br/>img, gray_blur = read_img('frame.jpeg') <br/>       <br/># Canny algorithm        <br/>edges = canny_edge(gray_blur) <br/>       <br/># Hough Transform        <br/>lines = hough_line(edges)   <br/>     <br/># Separate the lines into vertical and horizontal lines        h_lines, v_lines = h_v_lines(lines) <br/>       <br/># Find and cluster the intersecting        <br/>intersection_points = line_intersections(h_lines, v_lines)        points = cluster_points(intersection_points)   <br/>     <br/># Final coordinates of the board        <br/>points = augment_points(points)       <br/> <br/># Crop the squares of the board a organize into a sorted list        x_list = write_crop_images(img, points, 0)        <br/>img_filename_list = grab_cell_files()        img_filename_list.sort(key=natural_keys)   <br/>     <br/># Classify each square and output the board in Forsyth-Edwards Notation (FEN)        <br/>fen = classify_cells(model, img_filename_list) <br/>       <br/># Create and save the board image from the FEN        <br/>board = fen_to_image(fen)      <br/>  <br/># Display the board in ASCII        <br/>print(board)</span><span id="6393" class="mu lv iq nh b gy np nm l nn no"># Display and save the chessboard image        <br/>board_image = cv2.imread('current_board.png')        cv2.imshow('current board', board_image)  <br/>      <br/>print('Completed!')</span></pre><h2 id="bd3f" class="mu lv iq bd lw mv mw dn ma mx my dp me lf mz na mg lj nb nc mi ln nd ne mk nf bi translated">演示</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nr ns l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示了变化板现场(左)和新的2D图像输出(右)</p></figure><p id="cd8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望你喜欢这个演示！更多我的项目，请看我的<a class="ae kv" href="http://andrewleeunderwood.com" rel="noopener ugc nofollow" target="_blank">网站</a>和<a class="ae kv" href="https://github.com/andrewleeunderwood" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。请随时通过<a class="ae kv" href="https://www.linkedin.com/in/andrewleeunderwood/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系，对这个项目的未来迭代有想法。干杯！</p></div></div>    
</body>
</html>