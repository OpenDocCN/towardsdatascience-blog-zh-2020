<html>
<head>
<title>How to Compute Word Similarity — A Comparative Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何计算词语相似度——比较分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-compute-word-similarity-a-comparative-analysis-e9d9d3cb3080?source=collection_archive---------14-----------------------#2020-09-30">https://towardsdatascience.com/how-to-compute-word-similarity-a-comparative-analysis-e9d9d3cb3080?source=collection_archive---------14-----------------------#2020-09-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="af84" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://pedram-ataee.medium.com/list/nlpinproduction-98dbb687a868" rel="noopener"> NLP-in-Production </a></h2><div class=""/><div class=""><h2 id="d75a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Owl API是一个强大的单词相似性服务，使用高级文本聚类技术和各种word2vec模型</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/dfadea50310b039e45bc2a8af46273ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Uy6OXenAKTgRRVV1"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lh" href="https://unsplash.com/@zmachacek?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">zdenk macha ek</a>拍摄的照片</p></figure><p id="9838" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在自然语言处理或NLP中，大多数任务都是基于提取单词、句子或文档的语义而构建的。例如，我们通过提取句子的语义并基于它们在文档中的重要性对它们进行聚类来构建提取摘要。或者，我们通过提取最能表征一组文档的词组来构建主题建模解决方案。</p><p id="8a08" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有意义的语言的第一个要素是单词。所以，你可以猜测在NLP任务中正确提取单词的语义关系有多重要。<strong class="lk jd">提取单词语义最强大的工具之一是word2vec模型。</strong>这些word2vec模型针对不同的上下文进行训练，并提供单词的高维向量表示。然而，由于高维空间和结果缺乏质量，分析是复杂的。</p><p id="bc59" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章中，我想介绍一个强大的单词相似性API，名为Owl，它可以帮助您提取与目标单词最相似的单词。与NLP项目非常需要的当前解决方案相比，该API以更大的粒度提取最相似的单词。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="9c2f" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">owl——一个强大的词语相似度API</h1><p id="a3e9" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">这个Owl API使用各种word2vec模型和高级文本聚类技术来创建比行业标准更好的粒度。事实上，它将spaCy创建的最大的word2vec英文模型(即<a class="ae lh" href="https://spacy.io/models/en#en_core_web_lg" rel="noopener ugc nofollow" target="_blank"> en-core-web-lg </a>)用于<strong class="lk jd">通用</strong>上下文，并将斯坦福大学创建的word2vec模型之一(即<a class="ae lh" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">glove-wiki-gigaword-300</a><strong class="lk jd">)</strong>用于<strong class="lk jd">新闻</strong>上下文。</p><p id="7c08" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这里，我比较了这三个模型引入的最相似服务的结果。这有助于你找出为什么<strong class="lk jd"> Owl是你的答案</strong>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/9e25808433cb69603e9c77555a3e32a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*x6Dt9mYhEaat4foBT4eHhg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Owl——一个强大的单词相似度API </p></figure><h1 id="880f" class="ml mm it bd mn mo nj mq mr ms nk mu mv ki nl kj mx kl nm km mz ko nn kp nb nc bi translated">一、word2vec模型(spaCy或Glove)哪个更好提取最相似的词？</h1><p id="8db9" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">答案是没有。为什么？因为这些模型是在使用不同文本语料库的各种上下文中训练的。要提取最相似的词，首先必须确定上下文，如新闻、体育或一般。根据上下文，最相似的服务的结果可能完全不同。当您选择上下文时，您应该选择更适合您的问题的word2vec模型。</p><p id="b054" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">en-core-web-lg模型已经在通用英语语料库上接受了训练，而<em class="no"> glove-wiki-gigaword-300 </em>已经在维基百科和gigaword数据集(新闻专线文本数据的综合档案)上接受了训练。它们在两个不同的文本语料库上被训练，目的是提取不同的语义关系。下面，您可以看到使用这两种不同模型的单词“apple”的单词相似度的结果。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="5b97" class="nu mm it nq b gy nv nw l nx ny">"spaCy (en-core-web-lg)": <br/><strong class="nq jd">[</strong>"apples","blackberry","iphone","fruit","blueberry","strawberry",<br/>"ipad","pineapple","pear","cider"<strong class="nq jd">]</strong></span><span id="a7d1" class="nu mm it nq b gy nz nw l nx ny">"Glove (glove-wiki-gigaword-300)": <br/><strong class="nq jd">[</strong>"iphone","macintosh","ipod","microsoft","ipad","intel","ibm",<br/>"google","imac","software"<strong class="nq jd">]</strong></span></pre><blockquote class="oa"><p id="1d92" class="ob oc it bd od oe of og oh oi oj md dk translated">我们需要一个粒度更细的最相似的单词服务，它可以系统地区分子组。这正是Owl介入并解决问题的地方。</p></blockquote><p id="6544" class="pw-post-body-paragraph li lj it lk b ll ok kd ln lo ol kg lq lr om lt lu lv on lx ly lz oo mb mc md im bi translated">正如你所看到的，spaCy的结果更多的是关于日常用语，而Glove的结果更多的是关于你能在新闻上找到的东西。现在，你可以很容易地理解为什么结果是不同的，而两者都是正确的。为了更好地阐明这一点，我们来看看对“丰田”最相似的词的结果。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="4015" class="nu mm it nq b gy nv nw l nx ny">"spaCy (en-core-web-lg)": <strong class="nq jd">[</strong>"honda","subaru","ford","bmw","chevy","volvo","jeep","prius",<br/>"mercedes","dodge"<strong class="nq jd">]</strong><br/><br/>"Glove (glove-wiki-gigaword-300)":<strong class="nq jd">[</strong>"honda","nissan","automaker","camry","prius","mazda","lexus",<br/>"motor","automakers","ford"<strong class="nq jd">]</strong></span></pre><p id="05f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以看到这两个服务的结果是与原始单词相关的单词的组合，但不能在桶中报告。例如，spaCy报告了两种不同类型的“苹果”词:水果和小工具。或者，Glove报告了“丰田”的两种不同类型的词:型号和制造商。我们需要一个粒度更细的最相似的服务，可以系统地区分子群。这正是Owl介入并解决问题的地方。</p><h1 id="8db6" class="ml mm it bd mn mo nj mq mr ms nk mu mv ki nl kj mx kl nm km mz ko nn kp nb nc bi translated">二。为什么Owl API可以成为单词相似度服务的行业标准？</h1><p id="5044" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">word2vec模型通常在高维空间中生成向量。虽然这些模型保留了许多关于内部语言的信息，但它们很难分析。这就是为什么当前最相似单词服务的结果还不成熟的原因。Owl API使用先进的文本聚类技术来改进当前的工具。</p><p id="60f4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以找到Owl API使用下面的<em class="no"> glove-wiki-gigaword-300 </em>模型生成的与“Toyota”最相似的单词的结果。你可以看到结果被很好地划分为模型、制造者和一般子群；一个你在原始模型中找不到的粒度。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="f81f" class="nu mm it nq b gy nv nw l nx ny">"Owl (glove-wiki-gigaword-300)": <strong class="nq jd">{</strong><br/>0: <strong class="nq jd">[</strong>"camry","prius","lexus"<strong class="nq jd">], <br/></strong>1: <strong class="nq jd">[</strong>"honda","nissan", "mazda", "motor", "ford"<strong class="nq jd">], <br/></strong>2: <strong class="nq jd">[</strong>"automaker", "automakers"<strong class="nq jd">]<br/>}</strong></span></pre><p id="ebf7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，您可以找到Owl API使用<em class="no"> en-core-web-lg </em>模型生成的与“apple”最相似的单词的结果。你可以看到结果被很好地分成代表水果、小工具和浆果的组。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="6670" class="nu mm it nq b gy nv nw l nx ny">"Owl (en-core-web-lg)": <strong class="nq jd">{</strong><br/>0: <strong class="nq jd">[</strong>"apples","fruit","pineapple", "pear", "cider"<strong class="nq jd">], <br/></strong>1: <strong class="nq jd">[</strong>"iphone","ipad"<strong class="nq jd">], <br/></strong>2: <strong class="nq jd">[</strong>"blueberry", "strawberry", "blackberry"<strong class="nq jd">]<br/>}</strong></span></pre><h1 id="cedc" class="ml mm it bd mn mo nj mq mr ms nk mu mv ki nl kj mx kl nm km mz ko nn kp nb nc bi translated">三。如何使用Owl API提取最相似的单词？</h1><p id="02b6" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">很简单。首先，你需要从<a class="ae lh" href="https://rapidapi.com/" rel="noopener ugc nofollow" target="_blank"> RapidAPI </a>获取自己的API密匙。然后，您必须选择符合您需求的端点。目前，Owl API公开列出的端点为您提供了以下服务。</p><ul class=""><li id="9b33" class="op oq it lk b ll lm lo lp lr or lv os lz ot md ou ov ow ox bi translated"><strong class="lk jd"> Top_10_G </strong>:一般语境下最相似的前10个词</li><li id="48dc" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated"><strong class="lk jd"> Top_50_G: </strong>一般语境下最相似的前50个词</li><li id="ff81" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated"><strong class="lk jd"> Top_10_N </strong>:新闻语境中最相似的前10个词</li><li id="c334" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated"><strong class="lk jd"> Top_50_N </strong>:新闻语境中最相似的前50个词</li></ul><p id="11f5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，您必须提交请求并等待您的回复<strong class="lk jd">🙂</strong>。如果您使用Python，您可以在下面找到Top_10_N端点的代码。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="af7c" class="nu mm it nq b gy nv nw l nx ny">import requests<br/><br/>url = "https://word-similarity.p.rapidapi.com/news/10/apple"<br/><br/>headers = {<br/>    'x-rapidapi-host': "word-similarity.p.rapidapi.com",<br/>    'x-rapidapi-key': *** YOUR API KEY ***<br/>    }<br/><br/>response = requests.request("GET", url, headers=headers)<br/><br/>print(response.text)</span></pre><p id="2c12" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以在<a class="ae lh" href="https://rapidapi.com/pedram.ataee/api/word-similarity" rel="noopener ugc nofollow" target="_blank"> RapidAPI </a>上找到使用Python和许多其他编程语言的Owl API的完整说明。</p><h1 id="b7e6" class="ml mm it bd mn mo nj mq mr ms nk mu mv ki nl kj mx kl nm km mz ko nn kp nb nc bi translated">外卖食品</h1><p id="2251" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">有很多工具可以提取最相似的单词，来定位单词。然而，上述工具是最常见的。我强烈建议尝试一下<a class="ae lh" href="https://rapidapi.com/pedram.ataee/api/word-similarity" rel="noopener ugc nofollow" target="_blank"> Owl API </a>,因为它是建立在最好的word2vec模型之上的，与它的祖先相比，它能生成更有意义的结果。</p><h1 id="44a8" class="ml mm it bd mn mo nj mq mr ms nk mu mv ki nl kj mx kl nm km mz ko nn kp nb nc bi translated">感谢阅读！</h1><p id="5340" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">如果你喜欢这个帖子，想支持我…</p><ul class=""><li id="72b2" class="op oq it lk b ll lm lo lp lr or lv os lz ot md ou ov ow ox bi translated"><em class="no">跟我上</em> <a class="ae lh" href="https://medium.com/@pedram-ataee" rel="noopener"> <em class="no">中</em> </a> <em class="no">！</em></li><li id="2ca5" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated"><em class="no">在</em> <a class="ae lh" href="https://www.amazon.com/Pedram-Ataee/e/B08D6J3WNW" rel="noopener ugc nofollow" target="_blank"> <em class="no">亚马逊</em> </a> <em class="no">上查看我的书！</em></li><li id="6029" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated"><em class="no">成为</em> <a class="ae lh" href="https://pedram-ataee.medium.com/membership" rel="noopener"> <em class="no">中的一员</em> </a> <em class="no">！</em></li><li id="9a4e" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated"><em class="no">连接上</em><a class="ae lh" href="https://www.linkedin.com/in/pedrama/" rel="noopener ugc nofollow" target="_blank"><em class="no">Linkedin</em></a><em class="no">！</em></li><li id="6b66" class="op oq it lk b ll oy lo oz lr pa lv pb lz pc md ou ov ow ox bi translated"><em class="no">跟我上</em> <a class="ae lh" href="https://twitter.com/pedram_ataee" rel="noopener ugc nofollow" target="_blank"> <em class="no">推特</em> </a> <em class="no">！</em></li></ul><div class="pd pe gp gr pf pg"><a href="https://pedram-ataee.medium.com/membership" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd jd gy z fp pl fr fs pm fu fw jc bi translated">通过我的推荐链接加入Medium—Pedram Ataee博士</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">pedram-ataee.medium.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu lb pg"/></div></div></a></div></div></div>    
</body>
</html>