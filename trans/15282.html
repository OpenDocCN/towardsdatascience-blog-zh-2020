<html>
<head>
<title>The Secret Neural Network Formula</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">秘密神经网络公式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-secret-neural-network-formula-70b41f0da767?source=collection_archive---------42-----------------------#2020-10-20">https://towardsdatascience.com/the-secret-neural-network-formula-70b41f0da767?source=collection_archive---------42-----------------------#2020-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a796" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何在不过度拟合的情况下制作复杂的神经网络！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5e5ce2ceb0254bf4434c04722ccee677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QqHJIgeZyYw-uezoAzR1AQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片来自<a class="ae kv" href="https://startupstockphotos.com/" rel="noopener ugc nofollow" target="_blank">创业股票照片</a>。</p></figure><h1 id="6d74" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="c528" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为深度学习模型选择正确的架构可以极大地改变所取得的结果。使用太少的神经元会导致模型无法发现数据中的复杂关系，而使用太多的神经元会导致过度拟合效应。</p><p id="a549" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于表格数据，通常认为不需要很多层，一两层就足够了。为了帮助理解为什么这就足够了，看看<a class="ae kv" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">通用逼近定理</a>，它(用简单的术语)证明了一个只有一层和有限数量神经元的神经网络可以逼近任何连续函数。</p><blockquote class="mp mq mr"><p id="e592" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated"><strong class="lq ir">然而，你如何为神经网络挑选神经元的数量呢？</strong></p></blockquote><h1 id="0391" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">确定神经元的数量</h1><p id="a1be" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">目标是找到正确数量的神经元，以防止过度拟合和欠拟合，当然，这不是防止过度拟合和欠拟合的全部解决方案，但它有助于减少这种情况的发生。</p><p id="4b14" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在机器和深度学习中，模型的自由度是可以<em class="ms">学习</em>的参数数量。增加模型的自由度可以让模型更灵活地适应更复杂的函数，但是过多的自由度会让模型过度适应数据。</p><p id="b237" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这可以从下图中看出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/672a6137d8414d22efa7c0cbaefe859a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q2w2VBCgWxysJj8nTofADA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">拟合前25个数据点并用于预测最后5个数据点的两条线性回归线。</p></figure><blockquote class="mp mq mr"><p id="bd3d" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated">这意味着减少过度拟合的一种方法是限制模型中的自由度。</p></blockquote><p id="d42f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我在研究这个问题时发现的一个公式是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/ec7edb459e0bd04783e3c397c05a5341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lnd_2uyfJVsRdgJ8c_3seQ.png"/></div></div></figure><p id="2f83" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="ms"> N_h </em>为神经元数量，<em class="ms"> N_s </em>为训练样本数量，<em class="ms"> N_i </em>为输入神经元数量，<em class="ms"> N_o </em>为输出神经元数量，<em class="ms">α</em>为待选超参数。</p><p id="2240" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">数据集中的自由度是<em class="ms"> N_s(N_i + N_o) </em>，目的是将模型中自由参数的数量限制在数据自由度的一小部分。如果你的数据是一个很好的代表，这应该允许模型很好地概括，太多的参数，这意味着模型可以<em class="ms">过度适应</em>训练集。</p><p id="2793" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="ms"> Alpha </em>表示与您的模型相比，您的数据多了多少自由度。有了<em class="ms"> alpha = 2 </em>，你的数据中的自由度将是你的模型中的两倍。建议使用<em class="ms">2–10</em>的<em class="ms"> alpha </em>值，您可以循环查找最佳的<em class="ms"> alpha </em>值。</p><p id="37d3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">帮助我直观理解公式的一个方法是让<em class="ms"> beta </em> = <em class="ms"> 1/alpha </em>(保持alpha在2到10之间)，然后增加<em class="ms"> beta </em>表示复杂度增加。一个更加<strong class="lq ir">非线性</strong>的问题将需要一个<strong class="lq ir">更大的</strong>贝塔。</p><blockquote class="mp mq mr"><p id="cb5d" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated">主要规则是保持<em class="iq"> alpha ≥ 1 </em>，因为这意味着模型自由度永远不会大于数据集中的自由度。</p></blockquote><h1 id="333b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">例子</h1><p id="c133" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这个例子是一个Jupyter笔记本，比较了杰夫·希顿在选择一层中的神经元数量时给出的建议。我选择这个建议是因为新手很可能会听从杰夫·希顿的建议，因为他在这个领域很重要。</p><p id="b662" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这丝毫没有偏离杰夫·希顿的建议，而是试图展示如何使用这篇博客文章中提供的公式选择神经元的数量，数据科学家可以在不过度拟合的情况下创建更复杂的神经网络。创建更复杂的神经网络可以带来更好的结果。</p><p id="7b51" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">杰夫·希顿的建议是:</p><blockquote class="mp mq mr"><p id="b911" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated">1.隐藏神经元的数量应该在输入层的大小和输出层的大小之间。</p><p id="55ed" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated">2.隐藏神经元的数量应该是输入层大小的2/3，加上输出层的大小。</p><p id="8681" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated">3.隐藏神经元的数量应该小于输入层大小的两倍。</p></blockquote><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="00b0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><p id="ab67" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">[1]<strong class="lq ir">stack exchange</strong><a class="ae kv" href="https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-前馈神经网络</a>中的公式参考</p><p id="6b46" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[2]杰夫·希顿的。<em class="ms">隐藏层数。</em> <strong class="lq ir">希顿研究。</strong><a class="ae kv" href="https://www.heatonresearch.com/2017/06/01/hidden-layers.html" rel="noopener ugc nofollow" target="_blank">https://www . Heaton research . com/2017/06/01/hidden-layers . html</a></p><p id="3493" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[3]杰瑞米·霍华德和西尔万·古格。<em class="ms"> fastai:深度学习的分层API</em>。<strong class="lq ir"> arXiv </strong>。arXiv:2002.04688<a class="ae kv" href="https://arxiv.org/abs/2002.04688" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2002.04688</a>。2020.</p></div></div>    
</body>
</html>