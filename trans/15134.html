<html>
<head>
<title>Linear Regression in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-in-python-a4cfbab72c17?source=collection_archive---------13-----------------------#2020-10-18">https://towardsdatascience.com/linear-regression-in-python-a4cfbab72c17?source=collection_archive---------13-----------------------#2020-10-18</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="f861" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">线性回归背后的数学和Python实现方式</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/6cf40e1bf7659afc21fa6401d4559a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pv2AJGKWmqbFC9NjAWXuCg.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://unsplash.com/@isaacmsmith?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">艾萨克·史密斯</a>在<a class="ae kz" href="https://unsplash.com/s/photos/trending-graph?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="d80a" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">Python中的线性回归</h1><p id="7d76" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">线性回归是一种基于监督学习的机器学习算法。线性回归是一种预测模型，用于寻找因变量和一个或多个自变量之间的线性关系。这里，因变量/目标变量(Y)应该是连续变量。</p><p id="b23e" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">让我们使用ski-kit learn来学习简单线性回归背后的数学和Python实现方式</p><h2 id="677c" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">资料组</h2><p id="5382" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">让我们先看看我们的数据集。为了便于解释，我选取了一个简单的数据集。<code class="fe nf ng nh ni b"> Years of Experience vs Salary</code>。</p><p id="463c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们想根据一个人多年的经验来预测他的工资？</p><h1 id="d9dd" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">简单线性回归背后的数学</h1><p id="931c" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated"><strong class="lu iv">数据集</strong></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nj"><img src="../Images/eb0aa957e56b8716a90d3cad13a32521.png" data-original-src="https://miro.medium.com/v2/resize:fit:260/format:webp/1*bAQZzIWsEzuU3Uo7cH6WQQ.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">Exp vs薪水[图片由作者提供]</p></figure><p id="c46f" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在给定的数据集中，我们有Exp vs Salary。现在，我们要预测3.5年经验的工资？让我们看看如何预测？</p><h2 id="6568" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">线性方程</h2><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nk"><img src="../Images/a9182144223da94e93f617406d66beae.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*AjAnLz5b66nmbN1Mr9gk9g.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">线性回归方程[图片由作者提供]</p></figure><p id="a79a" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe nf ng nh ni b">c </code>→<code class="fe nf ng nh ni b">y-intercept</code>→x为零时y的值是多少？<br/>回归线在y轴截距处切割y轴。</p><p id="841b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe nf ng nh ni b">Y</code> →给定X值的预测Y值</p><p id="17f3" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们来计算一下<strong class="lu iv"> m和c </strong>。</p><p id="7190" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe nf ng nh ni b">m</code>又称<strong class="lu iv">回归系数</strong>。它表明因变量和自变量之间是否存在正相关关系。正相关意味着当自变量增加时，因变量的平均值也增加。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nl"><img src="../Images/8ec19ae1a141cbec72063f7f6243df99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*IE6yuOQn5sgf97a7Rqb2Mg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">m →斜率/回归系数[图片由作者提供]</p></figure><p id="51f5" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">回归系数</strong>定义为x和y的协方差除以自变量x的方差。</p><p id="47dd" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">方差</strong> →数据集中的每个数字离均值有多远。<br/>x̄→x的平均值<br/>ȳ→y的平均值</p><p id="0831" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">协方差是两个变量之间关系的量度。</p><p id="51bf" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我已经在excel表格中完成了所有的数学计算，可以从我的<a class="ae kz" href="https://github.com/IndhumathyChelliah/LinearRegression/blob/master/mathcalc.xls" rel="noopener ugc nofollow" target="_blank"> GitHub链接</a>下载。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nm"><img src="../Images/9a6f53d7a459c9f7a82d43d0ec96d8d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0sZguQmlGJvLJpRCxqwhA.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="4344" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">协方差</strong>=(<strong class="lu iv">σ[(x̅—易)])/n </strong> =529.0740741</p><p id="b719" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">方差</strong>=(<strong class="lu iv">σ[(x̅—Xi)])/n</strong>= 1.509876543</p><p id="f5b2" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv"> m=协方差/方差</strong>= 529.0740741/1.509876543 = 350.488878657</p><p id="7b3e" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe nf ng nh ni b">m=350.4088307</code></p><p id="42ab" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">现在来计算<strong class="lu iv">截距</strong>T1】</p><p id="9a99" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">y=mx+c <br/> c=y-mx <br/>应用平均y (ȳ)和平均x (x̅)in)等式并计算c</p><p id="2585" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><br/><code class="fe nf ng nh ni b">c=733.3360589</code>c = 1683.33333-(350.4088307 * 2.7111)</p><p id="fa5b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">计算了m和c之后，现在我们可以做预测了。</p><p id="a374" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们来预测一个有3.5年经验的人的工资。</p><p id="3614" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">y = MX+c<br/>T3】</p><p id="416b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">y预测=(350.4088307 * 3.5)+733.3360589 = 1959.766966</p><p id="46eb" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">x=3.5的预测y值是<code class="fe nf ng nh ni b"><strong class="lu iv">1959.766966</strong></code></p><h1 id="aafc" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">性能赋值</h1><p id="9947" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">为了评估我们的回归模型有多好，我们可以使用以下指标。</p><h2 id="149f" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">SSE-平方和误差</h2><p id="80bb" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">误差或r <strong class="lu iv"> esidual </strong>是<em class="nn">实际</em>值和<em class="nn">预测</em>值之间的差值。所有误差的总和可以抵消，因为它可以包含负号，并给出零。所以，我们把所有的误差平方，然后求和。误差平方和最小的线是最佳拟合线。</p><p id="6f90" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">最佳拟合线总是穿过x̅和ȳ.</p><p id="2ff7" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在线性回归中，通过最小化误差(数据点和直线之间的距离)来计算最佳拟合直线。</p><p id="b1ca" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">误差平方和</strong>又称<strong class="lu iv">残差</strong>或残差平方和</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj no"><img src="../Images/43bf83b223ffbcb3ff2798ddb3845414.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*WmyUp5prG2V72UuGMakBzg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">SSE方程[图片由作者提供]</p></figure><h2 id="942e" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">SSR回归的平方和</h2><p id="5ef4" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">SSR也被称为<strong class="lu iv">回归误差</strong>或解释误差。<br/>是<em class="nn">预测值</em>与<em class="nn">因变量</em> ȳ的<strong class="lu iv">均值</strong>之差的总和</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj np"><img src="../Images/d7dc9e47801e99f3c3933b70aa47332b.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*V17CDOVyjDcAuOnW5p2QGw.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">SSR方程[图片由作者提供]</p></figure><h2 id="3958" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">SST平方和合计</h2><p id="d0b7" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">SST/总误差=误差平方和+回归误差。</p><p id="cdcf" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">数据集的总误差或可变性等于由<strong class="lu iv">回归线</strong>(回归误差)解释的可变性加上被称为<strong class="lu iv">误差或残差</strong>的无法解释的可变性(<strong class="lu iv"> SSE </strong>)。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nq"><img src="../Images/a4a8b21db354d7987d5b5eb25d527264.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*btPemyKR9EQDUN6O8o_ejw.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">SST方程[图片由作者提供]</p></figure><p id="928d" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">已解释的错误或可变性→ SSR <br/>未解释的错误→ SSE</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nr"><img src="../Images/d7980c48c1d2f289cbb760b7e6c5c558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*2zsXR1bjDs3fQ4MOxbmehg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><h2 id="6c80" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">MSE →均方误差</h2><p id="25e2" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">MSE是数据点的实际值和预测值之间的平方差的平均值。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ns"><img src="../Images/e52053d994b3fdba704db3b8afcf3def.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*oBcXl--i-lqibCn2yQJBSw.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">MSE -&gt;方程[图片由作者提供]</p></figure><h2 id="2e59" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">RMSE均方根误差</h2><p id="5c5b" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">RMSE是衡量这些残差分布程度的指标。换句话说，它告诉你数据在最佳拟合线周围的集中程度。<br/> RMSE通过取MSE的平方根来计算。</p><p id="5b12" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">RMSE解释:</strong> <br/> RMSE解释为未解释方差的标准差(MSE)。<br/> RMSE包含与因变量相同的单位。<br/>RMSE值越低，表示拟合度越好。</p><h2 id="2006" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">相关系数</h2><p id="5d9d" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在建立模型之前，必须确定好的预测因素。相关系数(r)用于确定两个变量之间的关系强度。这将有助于确定良好的预测。</p><p id="4abb" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">公式:</strong></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nt"><img src="../Images/5b072d7f0ec1a140a52788ec4b662419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*Gs805iiA91EpXNqLWtV5jA.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">相关系数(r)[图片由作者提供]</p></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nu"><img src="../Images/00e6e595c6389a3ec459a07c1c6ef8ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*cQvT5L76Ny1v1NkhMGu8ag.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">相关系数(r)[图片由作者提供]</p></figure><p id="1983" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">r值范围从-1到1。<br/> <code class="fe nf ng nh ni b">-1</code>表示负相关，即x增加，y减少。<br/> <code class="fe nf ng nh ni b">+1</code>表示正相关，表示x和y向同一个方向移动。<br/> <code class="fe nf ng nh ni b">0</code>或接近0意味着没有相关性。</p><h2 id="e50e" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated"><strong class="ak"> R ( R平方)</strong> →决定系数</h2><p id="f68e" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated"><strong class="lu iv">决定系数</strong> →该指标在建立模型后用于检查模型的可靠性。</p><p id="d70e" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">r→它等于回归解释的方差(回归误差或SSR)除以y中的总方差(SST)</p><p id="3668" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">R →它描述了y中的总方差有多少是由我们的模型解释的。<br/>如果<strong class="lu iv">误差(无法解释的误差或SSE) &lt;方差</strong> (SST)表示模型是好的。最佳拟合是未解释误差(SSE)最小的线。</p><p id="3eae" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">r值的范围从0到1。</p><p id="5100" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">0 →表示差的型号<br/> 1或接近1 →表示最好的型号</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nv"><img src="../Images/761cb0dfa9841784a2532eba85b4bc3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*BHQqT8evnrROLalhJ7MMtg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">决定系数</p></figure><h2 id="a183" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">计算我们数据集中的均方误差，RMSE，R</h2><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nw"><img src="../Images/91e5f8b31032d7c27ef8de35e3924253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gk96vfDH4bjnvNIUe0lWKg.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure></div><div class="ab cl nx ny hy nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="in io ip iq ir"><h1 id="b80d" class="la lb iu bd lc ld oe lf lg lh of lj lk ka og kb lm kd oh ke lo kg oi kh lq lr bi translated">让我们使用scikit learn在Python中做同样的实现。</h1><p id="2fa5" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">使用的代码可以在我的GitHub链接中作为<a class="ae kz" href="https://github.com/IndhumathyChelliah/LinearRegression/blob/master/LinearRegression1.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本下载。</a></p><h2 id="8bbd" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated"><strong class="ak"> 1。导入所需的库</strong></h2><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="84c1" class="mt lb iu ni b gz on oo l op oq"><strong class="ni iv">import </strong>numpy <strong class="ni iv">as </strong>np<br/><strong class="ni iv">import </strong>pandas <strong class="ni iv">as </strong>pd<br/><strong class="ni iv">import </strong>matplotlib.pyplot <strong class="ni iv">as </strong>plt<br/><strong class="ni iv">import </strong>seaborn <strong class="ni iv">as </strong>sns</span></pre><h2 id="37d3" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated"><strong class="ak"> 2。加载数据</strong></h2><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="7e9d" class="mt lb iu ni b gz on oo l op oq">df=pd.read_csv(<strong class="ni iv">"exp1.csv"</strong>)<br/>df</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj or"><img src="../Images/5c787513e521990b34184478146599c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*POvjst20vzMRYSAofgpqCA.png"/></div></figure><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="79c7" class="mt lb iu ni b gz on oo l op oq">df.describe()</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj os"><img src="../Images/580155c7055c94be381204582475b538.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*IUTz_O9T6_daNqLrozm_Lw.png"/></div></figure><h2 id="eb91" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated"><strong class="ak"> 3。EDA —探索性数据分析</strong></h2><ul class=""><li id="47bc" class="ot ou iu lu b lv lw ly lz mb ov mf ow mj ox mn oy oz pa pb bi translated"><strong class="lu iv">散点图</strong></li></ul><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="4d5d" class="mt lb iu ni b gz on oo l op oq">plt.scatter(df.Exp,df.Salary,color=<strong class="ni iv">'red'</strong>)</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pc"><img src="../Images/ed0db824a8be4e4457b242adfd09c794.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*_7F-1F8TNykOl4FZD4IHbw.png"/></div></figure><p id="cbda" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">[我们可以找到x和y之间的线性关系]</p><ul class=""><li id="43f5" class="ot ou iu lu b lv mo ly mp mb pd mf pe mj pf mn oy oz pa pb bi translated"><strong class="lu iv">直方图</strong></li></ul><p id="298a" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe nf ng nh ni b">df.hist()</code></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pg"><img src="../Images/e712dde7ada3b7852c8de7fe3038242f.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*zN-Qtkuj1EDlYyMk_P1G-g.png"/></div></figure><ul class=""><li id="1471" class="ot ou iu lu b lv mo ly mp mb pd mf pe mj pf mn oy oz pa pb bi translated"><strong class="lu iv">求相关系数(r) </strong></li></ul><p id="5145" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe nf ng nh ni b">df.corr()</code></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ph"><img src="../Images/f94b0c71f8e748932d1cb7b4f3872106.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*Q5Q5x_zo1aJvGOGznAgpsA.png"/></div></figure><p id="9c8f" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">r值0.98表明关系密切。</p><p id="f05e" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们可以使用热图来绘制相关性</p><p id="5d74" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe nf ng nh ni b">sns.heatmap(df.corr(),annot=True,vmin=-1,vmax=-1)</code></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pi"><img src="../Images/9f16db19da17f4c04f84142aa2d3efb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*Ut5WPYQj7rFqBlzdbskRDg.png"/></div></figure><ul class=""><li id="d5aa" class="ot ou iu lu b lv mo ly mp mb pd mf pe mj pf mn oy oz pa pb bi translated"><strong class="lu iv">查找缺失值</strong></li></ul><p id="fe94" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe nf ng nh ni b">df.isna().sum()</code></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pj"><img src="../Images/aaeb997566725d9f48de82df5f711d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/format:webp/1*46o9LHiEpxUM_RrqSg_b7w.png"/></div></figure><p id="0ca0" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">[没有丢失的值]</p><h2 id="0ea8" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">4.将特征分配给X和Y</h2><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="5921" class="mt lb iu ni b gz on oo l op oq">x=df.iloc[:,0:1]<br/>x.head(1)<br/><br/>y=df.iloc[:,1:]<br/>y.head(1</span></pre><h2 id="d533" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">5.可视化数据集</h2><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="0b12" class="mt lb iu ni b gz on oo l op oq">plt.scatter(x, y)<br/>plt.title(<strong class="ni iv">'Experience Vs Salary'</strong>)<br/>plt.xlabel(<strong class="ni iv">'Years of Experience'</strong>)<br/>plt.ylabel(<strong class="ni iv">'Salary'</strong>)<br/>plt.show()</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pk"><img src="../Images/cb03ee005f52ed0d945f3a9dba833326.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*QTp5kFzRisqWFCVCe-zyRg.png"/></div></figure><h2 id="c07c" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">6.使用sklearn建立模型</h2><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="bd31" class="mt lb iu ni b gz on oo l op oq"><strong class="ni iv">from </strong>sklearn.linear_model <strong class="ni iv">import </strong>LinearRegression<br/>lin_reg=LinearRegression()<br/>lin_reg.fit(x,y)</span></pre><p id="1ac3" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">可视化模型</strong></p><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="a55b" class="mt lb iu ni b gz on oo l op oq">plt.scatter(x,y)<br/>plt.plot(x,lin_reg.predict(x),color=<strong class="ni iv">'green'</strong>)<br/>plt.title(<strong class="ni iv">"Regression Model"</strong>)<br/>plt.xlabel(<strong class="ni iv">"YOE"</strong>)<br/>plt.ylabel(<strong class="ni iv">"Salary"</strong>)</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pl"><img src="../Images/0ce4d08ee898c7ae1b17d347a6b83ac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*1jUCcE2pNU6hR6MqVRyxjg.png"/></div></figure><p id="f9af" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">7 .<strong class="lu iv">。使用模型</strong>预测3.5年工作经验的工资</p><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="68ae" class="mt lb iu ni b gz on oo l op oq">ypredict=lin_reg.predict(np.array([[3.5]]))<br/>ypredict<br/><em class="nn">#Output:array([[1959.76696648]])</em></span></pre><p id="682b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv"> 8。m(斜率)和c(截距)值</strong></p><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="d2d2" class="mt lb iu ni b gz on oo l op oq">lin_reg.coef_<br/><em class="nn">#Output:array([[350.40883074]])<br/></em>lin_reg.intercept_<br/><em class="nn">#Output:array([733.33605887])</em></span></pre><h2 id="ef34" class="mt lb iu bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq ne bi translated">9.计算决定系数</h2><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="bb45" class="mt lb iu ni b gz on oo l op oq">ypredict=lin_reg.predict(x)<br/>ypredict</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pm"><img src="../Images/ae73e11561f3874bcb06bf6b14b467ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*nOFrrwqLbn0Gq9rHH23DrQ.png"/></div></figure><pre class="kk kl km kn gu oj ni ok ol aw om bi"><span id="4737" class="mt lb iu ni b gz on oo l op oq"><strong class="ni iv">from </strong>sklearn.metrics <strong class="ni iv">import </strong>mean_squared_error,r2_score,explained_variance_score<br/>print (<strong class="ni iv">"Coefficient of determination :"</strong>,r2_score(y,ypredict))<br/>print (<strong class="ni iv">"MSE: "</strong>,mean_squared_error(y,ypredict))<br/>print(<strong class="ni iv">"RMSE: "</strong>,np.sqrt(mean_squared_error(y,ypredict)))</span><span id="8f1e" class="mt lb iu ni b gz pn oo l op oq">#Output:<br/>Coefficient of determination : 0.9729038186936964<br/>MSE:  5163.327882256747<br/>RMSE:  71.85630022661024</span></pre><p id="fad2" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们使用数学计算和python实现得到相同的值。<br/>如果这是一个大型数据集，我们必须分割数据进行训练和测试。</p><p id="cf01" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv"> GitHub链接</strong></p><p id="d8cd" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在我的<a class="ae kz" href="https://github.com/IndhumathyChelliah/LinearRegression" rel="noopener ugc nofollow" target="_blank"> GitHub链接</a>中可以找到这个故事中用到的代码、数据集和excel表格</p><h1 id="9c27" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">结论</h1><p id="aae1" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在这个故事中，我们采用了简单的数据集，并使用scikit learn学习了简单线性回归背后的数学和python实现方式。<br/>我们还可以使用<strong class="lu iv"> statsmodel </strong>实现线性回归。</p><h1 id="c451" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">我关于机器学习的其他博客</h1><div class="po pp gq gs pq pr"><a rel="noopener follow" target="_blank" href="/line-of-best-fit-in-linear-regression-13658266fbc8"><div class="ps ab fp"><div class="pt ab pu cl cj pv"><h2 class="bd iv gz z fq pw fs ft px fv fx it bi translated">线性回归中的最佳拟合线</h2><div class="py l"><h3 class="bd b gz z fq pw fs ft px fv fx dk translated">相关系数、决定系数、模型系数</h3></div><div class="pz l"><p class="bd b dl z fq pw fs ft px fv fx dk translated">towardsdatascience.com</p></div></div><div class="qa l"><div class="qb l qc qd qe qa qf kt pr"/></div></div></a></div><div class="po pp gq gs pq pr"><a rel="noopener follow" target="_blank" href="/logistic-regression-in-python-2f965c355b93"><div class="ps ab fp"><div class="pt ab pu cl cj pv"><h2 class="bd iv gz z fq pw fs ft px fv fx it bi translated">Python中的逻辑回归</h2><div class="py l"><h3 class="bd b gz z fq pw fs ft px fv fx dk translated">详细的逻辑回归</h3></div><div class="pz l"><p class="bd b dl z fq pw fs ft px fv fx dk translated">towardsdatascience.com</p></div></div><div class="qa l"><div class="qg l qc qd qe qa qf kt pr"/></div></div></a></div><div class="po pp gq gs pq pr"><a rel="noopener follow" target="_blank" href="/an-introduction-to-support-vector-machine-3f353241303b"><div class="ps ab fp"><div class="pt ab pu cl cj pv"><h2 class="bd iv gz z fq pw fs ft px fv fx it bi translated">支持向量机简介</h2><div class="py l"><h3 class="bd b gz z fq pw fs ft px fv fx dk translated">如何在分类问题中使用SVM？</h3></div><div class="pz l"><p class="bd b dl z fq pw fs ft px fv fx dk translated">towardsdatascience.com</p></div></div><div class="qa l"><div class="qh l qc qd qe qa qf kt pr"/></div></div></a></div><div class="po pp gq gs pq pr"><a rel="noopener follow" target="_blank" href="/an-introduction-to-k-nearest-neighbours-algorithm-3ddc99883acd"><div class="ps ab fp"><div class="pt ab pu cl cj pv"><h2 class="bd iv gz z fq pw fs ft px fv fx it bi translated">K-最近邻算法简介</h2><div class="py l"><h3 class="bd b gz z fq pw fs ft px fv fx dk translated">什么是KNN？</h3></div><div class="pz l"><p class="bd b dl z fq pw fs ft px fv fx dk translated">towardsdatascience.com</p></div></div><div class="qa l"><div class="qi l qc qd qe qa qf kt pr"/></div></div></a></div><div class="po pp gq gs pq pr"><a href="https://pub.towardsai.net/naive-bayes-classifier-in-machine-learning-b0201684607c" rel="noopener  ugc nofollow" target="_blank"><div class="ps ab fp"><div class="pt ab pu cl cj pv"><h2 class="bd iv gz z fq pw fs ft px fv fx it bi translated">机器学习中的朴素贝叶斯分类器</h2><div class="py l"><h3 class="bd b gz z fq pw fs ft px fv fx dk translated">使用sklearn的数学解释和python实现</h3></div><div class="pz l"><p class="bd b dl z fq pw fs ft px fv fx dk translated">pub.towardsai.net</p></div></div><div class="qa l"><div class="qj l qc qd qe qa qf kt pr"/></div></div></a></div><div class="po pp gq gs pq pr"><a href="https://betterprogramming.pub/understanding-decision-trees-in-machine-learning-86d750e0a38f" rel="noopener  ugc nofollow" target="_blank"><div class="ps ab fp"><div class="pt ab pu cl cj pv"><h2 class="bd iv gz z fq pw fs ft px fv fx it bi translated">理解机器学习中的决策树</h2><div class="py l"><h3 class="bd b gz z fq pw fs ft px fv fx dk translated">决策树背后的数学以及如何使用Python和sklearn实现它们</h3></div><div class="pz l"><p class="bd b dl z fq pw fs ft px fv fx dk translated">better编程. pub</p></div></div><div class="qa l"><div class="qk l qc qd qe qa qf kt pr"/></div></div></a></div></div><div class="ab cl nx ny hy nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="in io ip iq ir"><p id="9bab" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><em class="nn">关注此空间，了解更多关于Python和数据科学的文章。如果你喜欢看我的更多教程，就关注我的</em> <a class="ae kz" href="https://medium.com/@IndhumathyChelliah" rel="noopener"> <strong class="lu iv"> <em class="nn">中</em></strong></a><a class="ae kz" href="https://www.linkedin.com/in/indhumathy-chelliah/" rel="noopener ugc nofollow" target="_blank"><strong class="lu iv"><em class="nn">LinkedIn</em></strong></a><strong class="lu iv"><em class="nn"/></strong><a class="ae kz" href="https://twitter.com/IndhuChelliah" rel="noopener ugc nofollow" target="_blank"><strong class="lu iv"><em class="nn">推特</em> </strong> </a> <strong class="lu iv"> <em class="nn">。</em> </strong></p><p id="d314" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv"> <em class="nn">点击这里成为中等会员:</em> </strong></p><p id="8416" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><a class="ae kz" href="https://indhumathychelliah.medium.com/membership" rel="noopener"><em class="nn">https://indhumathychelliah.medium.com/membership</em></a></p></div></div>    
</body>
</html>