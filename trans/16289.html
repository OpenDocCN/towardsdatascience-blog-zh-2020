<html>
<head>
<title>5 Neural network architectures you must know for Computer Vision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算机视觉领域你必须知道的5种神经网络架构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-neural-network-architectures-you-must-know-for-computer-vision-31d2991fe24e?source=collection_archive---------7-----------------------#2020-11-10">https://towardsdatascience.com/5-neural-network-architectures-you-must-know-for-computer-vision-31d2991fe24e?source=collection_archive---------7-----------------------#2020-11-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="aeaa" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在这篇文章中，我列出了计算机视觉的前5个神经网络架构，排名不分先后</h2></div><h1 id="1a53" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">卷积神经网络</h1><h2 id="a012" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">历史</h2><p id="2c9a" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">卷积的概念是由福岛国彦在T2的论文中首次提出的。<a class="ae mf" href="https://en.wikipedia.org/wiki/Neocognitron" rel="noopener ugc nofollow" target="_blank"> neocognitron </a>引入了两种类型的层，卷积层和下采样层。</p><p id="a892" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">然后下一个关键进展是由Yann LeCun等人提出的，他们使用反向传播从图像中学习卷积核的系数。这使得学习自动化，而不是费力的手工制作。根据维基百科，这种方法成为了现代计算机视觉的基础。</p><p id="d1bc" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">然后是2012年由<em class="ml"> Alex Krizhevsky，Ilya Sutskever，Geoffrey E. Hinton，</em>撰写的“使用深度卷积神经网络的ImageNet分类”，这被广泛认为是卷积神经网络方面最有影响力的论文。他们创建了Alexnet，并凭借他们的模型赢得了2012年的Imagenet竞赛。</p><h2 id="36a9" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">机制</h2><p id="93f0" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">我不会深入探究CNN的机制，但想简单概述一下它是如何工作的。</p><p id="cb0f" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">常规的香草神经网络在计算WX + b的层上训练，其中W是通过反向传播学习的权重矩阵，卷积神经网络使用称为过滤器的权重。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/28c7a51737e592777b0565cb01da554a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/1*ror4mkfhRV-DbEFoo4m09w.gif"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">卷积运算。图片来自<a class="ae mf" href="https://giphy.com/explore/convolution" rel="noopener ugc nofollow" target="_blank">https://giphy.com/explore/convolution</a>。</p></figure><p id="3d51" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">你可以想象一个卷积的<a class="ae mf" href="https://en.wikipedia.org/wiki/Kernel_(image_processing)" rel="noopener ugc nofollow" target="_blank">内核</a>或过滤器，就像输入矩阵上的滑动窗口。在上面的gif中，过滤器是带有红色数字的橙色阴影矩阵。输入矩阵是带有黑色数字的绿色矩阵。在每个阶段，滤波器与输入矩阵的重叠部分逐元素相乘，然后对值求和。这给出了第一个输出。然后过滤器向左移动一步，依此类推，如gif所示。<br/>可以根据滤波器值计算输出和标签的损失，通过反向传播，我们可以学习滤波器的值。</p><p id="fc75" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">当你展开内核和输入时，卷积神经网络实际上只是矩阵乘法。这在<a class="ae mf" rel="noopener" target="_blank" href="/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215">这篇非常好的博文</a>中有所展示。CNN非常强大有两个主要原因。</p><ol class=""><li id="9209" class="my mz iq lo b lp mg ls mh lc na lf nb li nc me nd ne nf ng bi translated">它们每层的参数明显更少，因此可以堆叠形成更深的层。</li><li id="f8bc" class="my mz iq lo b lp nh ls ni lc nj lf nk li nl me nd ne nf ng bi translated">它们处理输入的位置。图像中像素的局部性得以保持，因为内核一次只作用于图像的一部分，并且输入中接近的像素会创建同样接近的输出值。这不同于不考虑位置的传统网络。</li></ol><h1 id="6234" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">剩余网络</h1><p id="3043" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated"><a class="ae mf" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">残差网络</a>是由<a class="ae mf" href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K" rel="noopener ugc nofollow" target="_blank">、</a>等人在他们极具影响力的论文《图像识别的深度残差学习》中引入的。来自微软研究团队的论文赢得了2015年Imagenet竞赛。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nm"><img src="../Images/12bb7f01161a006d9675a2db7bc4b490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2oCKA9vEqldBDjZSlgkvQA.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">跳过连接。图片由作者提供。</p></figure><p id="230c" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">ResNets有一个非常简单却非常优雅的想法。这个想法是添加跳过连接或快捷连接，这创造了一个梯度公路。这使得梯度在后退过程中流动得更好，大大增加了收敛，训练时间，减少了梯度爆炸和消失。</p><p id="b663" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">Resnets的微妙之处在于，最好的情况是跳过连接主动添加到输出中并计算有用的信息，最坏的情况是忽略跳过的连接，最坏的情况是与没有跳过连接的网络相同。<strong class="lo ir">因此，跳过连接增加了如此多的价值，而且没有负面影响！</strong></p><h1 id="cc69" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">u网</h1><p id="bb1b" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">U-Net是由Olaf Ronneberger、Philipp Fischer和Thomas Brox在他们的论文“U-Net:生物医学图像分割的卷积网络”中介绍的，你可以在这里阅读<a class="ae mf" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank"/>。这篇2015年的论文对于图像分割来说是革命性的。图像分割的任务是用类别标记图像中的每个像素。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nr"><img src="../Images/7510239bf47554c2aaed5c12eb0fe228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*soHtXw7ROtXMBQBJMgD_cQ.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">U-Net架构。图片取自<a class="ae mf" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank">原U网论文</a>。</p></figure><p id="90c5" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">u网有两部分，收缩路径(下采样路径)和扩展路径(上采样路径)。在传统的图像分类卷积网络中，图像被送入执行卷积和汇集操作的网络，这两种操作都降低了输出的高度和宽度，但增加了输出的深度。随着高度和宽度的损失，获得的深度向分类输出添加了特征。</p><p id="102d" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">然而，在分割任务中，我们希望输出与输入图像具有相同的形状，并添加标记像素的特征。因此，传统Conv架构的下采样由上采样路径补充，以将图像的高度和宽度添加回输出，同时保持特征。有许多上采样方法，但大多数库中最常用的方法是转置卷积上采样。你可以在这里阅读这个方法<a class="ae mf" href="https://naokishibuya.medium.com/up-sampling-with-transposed-convolution-9ae4f2df52d0" rel="noopener">。</a></p><h1 id="a05f" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">YOLO</h1><p id="3004" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">YOLO最初是由<a class="ae mf" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Redmon%2C+J" rel="noopener ugc nofollow" target="_blank">约瑟夫·雷德蒙</a>、<a class="ae mf" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Divvala%2C+S" rel="noopener ugc nofollow" target="_blank">桑托什·迪瓦拉</a>、<a class="ae mf" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Girshick%2C+R" rel="noopener ugc nofollow" target="_blank">罗斯·吉斯克</a>、<a class="ae mf" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Farhadi%2C+A" rel="noopener ugc nofollow" target="_blank">阿里·法尔哈迪</a>在他们的论文《你只看一次:统一的实时物体检测》中介绍的，你可以在这里阅读<a class="ae mf" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank"/>。该论文被提议作为2015年物体检测的快速、最先进的模型。多年来，YOLO有4个官方版本(论文发表的地方)。前三个是由原作者，最后一个是由不同的作者。我现在不会讨论YOLO的版本，也许在另一篇文章中；-)</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi ns"><img src="../Images/8a46a6101989968013aaa1451d53c96b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*949cCfDSdpyV0F3DHjgikw.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">YOLO建筑。图片取自<a class="ae mf" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">原YOLO纸</a>。</p></figure><p id="6be1" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">YOLO代表你只看一眼。当这篇论文发表时，目标检测的流行方法是重用分类器对图像的局部区域进行分类，并使用滑动窗口方法来检查图像的每个区域是否有目标。YOLO通过将目标检测作为一个回归问题提出来，改变了这种范式，在这种情况下，他们仅对整个管道使用一个单一的网络，并一次性处理整个图像，而不是在区域中处理。</p><p id="b812" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">YOLO把输入图像分成一个SxS网格。并且对于每个网格，预测对象的中心是否存在于网格内。如果对象的中心在网格中，则网格将预测具有5个值的边界框，x，y，w，h，c。(x，y)是对象中心相对于网格的坐标，(w，h)是对象相对于整个图像的宽度和高度，以及(c)是对象的类别。</p><blockquote class="nt"><p id="93bd" class="nu nv iq bd nw nx ny nz oa ob oc me dk translated">“YOLO有三大优势。松散引用自<a class="ae mf" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">原论文【1】</a>。</p><p id="7860" class="nu nv iq bd nw nx ny nz oa ob oc me dk translated">首先，YOLO速度极快。由于论文将检测框架作为一个回归问题，因此不需要复杂的流水线。</p><p id="e746" class="nu nv iq bd nw nx ny nz oa ob oc me dk translated">第二，YOLO在做预测时会对图像进行全局推理。与基于滑动窗口和区域提议的技术不同，YOLO在训练和测试期间看到整个图像，因此它隐式地编码了关于类及其外观的上下文信息。</p><p id="2602" class="nu nv iq bd nw nx ny nz oa ob oc me dk translated">第三，YOLO学习对象的概括表示。由于YOLO是高度概括的，当应用于新的领域或意想不到的输入时，它不太可能崩溃。"</p></blockquote><h1 id="0e79" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw od jx kr jz oe ka kt kc of kd kv kw bi translated">生成对抗网络</h1><p id="00a2" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated"><a class="ae mf" href="https://arxiv.org/search/stat?searchtype=author&amp;query=Goodfellow%2C+I+J" rel="noopener ugc nofollow" target="_blank">伊恩·j·古德费勒</a>、<a class="ae mf" href="https://arxiv.org/search/stat?searchtype=author&amp;query=Pouget-Abadie%2C+J" rel="noopener ugc nofollow" target="_blank">让·普盖-阿巴迪</a>、<a class="ae mf" href="https://arxiv.org/search/stat?searchtype=author&amp;query=Mirza%2C+M" rel="noopener ugc nofollow" target="_blank">迈赫迪·米尔扎</a>、<a class="ae mf" href="https://arxiv.org/search/stat?searchtype=author&amp;query=Xu%2C+B" rel="noopener ugc nofollow" target="_blank">徐炳</a>、<a class="ae mf" href="https://arxiv.org/search/stat?searchtype=author&amp;query=Warde-Farley%2C+D" rel="noopener ugc nofollow" target="_blank">大卫·沃德-法利</a>、<a class="ae mf" href="https://arxiv.org/search/stat?searchtype=author&amp;query=Ozair%2C+S" rel="noopener ugc nofollow" target="_blank">谢尔吉尔·奥泽尔</a>、<a class="ae mf" href="https://arxiv.org/search/stat?searchtype=author&amp;query=Courville%2C+A" rel="noopener ugc nofollow" target="_blank">亚伦·库维尔</a>、<a class="ae mf" href="https://arxiv.org/search/stat?searchtype=author&amp;query=Bengio%2C+Y" rel="noopener ugc nofollow" target="_blank">约舒阿·本吉奥</a>在他们的论文《生成对抗网络》中介绍了生成对抗网络，你可以阅读【生成对抗网络】</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi og"><img src="../Images/5f53fed90b088dd22d67385c61d5f7d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zRcEh3S_YBrQHshaGhzh4A.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">甘形象。图片由作者提供。</p></figure><p id="18b7" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">GANs是通过对抗过程训练的神经网络对。GAN的两个部分是发生器和鉴别器。生成器的作用是生成类似于训练数据的高质量数据，而批评家的作用是区分生成的数据和真实的数据。发电机的目标函数是使批评家的损失最大化，批评家的目标函数是使其损失最小化。</p><p id="272e" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">把这个过程想象成一个小偷和警察。小偷想愚弄警察，不断改进他们的工具和技术，而警察想抓住小偷，所以他们也在改进。发电机就像小偷，批评家就像警察。</p><p id="6f83" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">GANs有许多应用，并且一直有许多新的应用出现。但是因为这篇文章是关于计算机视觉的，GANs的两个非常有趣的应用是:</p><ol class=""><li id="8855" class="my mz iq lo b lp mg ls mh lc na lf nb li nc me nd ne nf ng bi translated"><strong class="lo ir">超分辨率</strong> <br/>超分辨率是指拍摄低质量的图像，并从中生成高质量的图像。英伟达的新DLSS可能会使用这种技术。<br/>来自<a class="ae mf" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>的Jeremey Howard有一个非常有趣的方法，叫做超分辨率的noGAN方法。这个过程是对GANs的一种预训练，其中高质量的图像被转换成较低质量的图像用于生成器的训练数据，并且评论家在生成的图像上被预训练。这样，生成器和评论家都有一个良好的开端，并且这种方法被发现可以显著地提高GANs的训练时间。</li><li id="3f33" class="my mz iq lo b lp nh ls ni lc nj lf nk li nl me nd ne nf ng bi translated"><strong class="lo ir">深度假货<br/> </strong>相信大家都从媒体上听说过深度假货。深度假货也是GANs，其中生成器被训练来执行假货操作，而评论家的任务是检测假货。发电机可以被训练足够长的时间来愚弄大多数人类。这是一种有点危险的技术，在互联网上需要注意。</li></ol><h1 id="51ed" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">参考</h1><p id="571f" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">[1] <a class="ae mf" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Redmon%2C+J" rel="noopener ugc nofollow" target="_blank">约瑟夫·雷德蒙</a>，<a class="ae mf" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Divvala%2C+S" rel="noopener ugc nofollow" target="_blank">桑托什·迪夫瓦拉</a>，<a class="ae mf" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Girshick%2C+R" rel="noopener ugc nofollow" target="_blank">罗斯·吉尔希克</a>，<a class="ae mf" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Farhadi%2C+A" rel="noopener ugc nofollow" target="_blank">阿里·法尔哈迪</a>，《你只看一次:统一的、实时的物体检测》(2015)，<a class="ae mf" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank"> <br/> arXiv:1506.02640 </a></p></div></div>    
</body>
</html>