<html>
<head>
<title>Building Face Recognition Model Under 30 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">30分钟内建立人脸识别模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-face-recognition-model-under-30-minutes-2d1b0ef72fda?source=collection_archive---------6-----------------------#2020-09-27">https://towardsdatascience.com/building-face-recognition-model-under-30-minutes-2d1b0ef72fda?source=collection_archive---------6-----------------------#2020-09-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="322d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="8628" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">微调VGG-16以建立在三重损失函数上训练的用于面部识别任务的暹罗网络</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/735093e042ebe50c3976be28397a78a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*feME8KHgrReOTvBU"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@codestorm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">萨法尔萨法罗夫</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="4a14" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">介绍</h1><p id="5664" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在这篇博文中，我将介绍人脸识别模型的一些实现细节。我还设计了一个基于浏览器的UI，用于向数据库添加一个新人。解释web开发部分超出了本博客的范围。</p><p id="1c4a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这篇文章假设读者理解<a class="ae le" rel="noopener" target="_blank" href="/siamese-network-triplet-loss-b4ca82c1aec8">连体网络模型和三重损失函数</a>。如果您喜欢先运行模型，那么您可以从我的库<a class="ae le" href="https://github.com/dedhiaparth98/face-recognition" rel="noopener ugc nofollow" target="_blank">这里</a>克隆它。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/f809be5300cde02e43a93903b0cb6caa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*ACdNm8PgjYPKTimpqp95ig.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">从<a class="ae le" href="https://github.com/dedhiaparth98/face-recognition" rel="noopener ugc nofollow" target="_blank">库</a>中试用工作模型</p></figure><p id="f258" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这篇博文的结构如下:</p><ul class=""><li id="01f6" class="mz na iq lz b ma mt md mu mg nb mk nc mo nd ms ne nf ng nh bi translated"><strong class="lz ja">模型架构</strong></li><li id="4e3d" class="mz na iq lz b ma ni md nj mg nk mk nl mo nm ms ne nf ng nh bi translated"><strong class="lz ja">数据集</strong></li><li id="05b5" class="mz na iq lz b ma ni md nj mg nk mk nl mo nm ms ne nf ng nh bi translated"><strong class="lz ja">三胎生成</strong></li><li id="1723" class="mz na iq lz b ma ni md nj mg nk mk nl mo nm ms ne nf ng nh bi translated"><strong class="lz ja">其他详细信息</strong></li><li id="93ca" class="mz na iq lz b ma ni md nj mg nk mk nl mo nm ms ne nf ng nh bi translated"><strong class="lz ja">结论</strong></li></ul><h1 id="3508" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">模型架构</h1><p id="91ca" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">我们都知道从头开始训练卷积神经网络(CNN)需要大量数据和计算能力。因此，我们转而使用<a class="ae le" rel="noopener" target="_blank" href="/transfer-learning-from-pre-trained-models-f2393f124751">迁移学习</a>，根据我们的要求对基于类似数据训练的模型进行微调。牛津大学的视觉几何小组(VGG)已经建立了三个模型——VGG-16、雷斯内特-50和塞内特-50，用于人脸识别和人脸分类。我使用了VGG-16模型，因为它是一个较小的模型，实时预测可以在我的本地系统上工作，无需GPU。</p><blockquote class="nn no np"><p id="965e" class="lx ly nq lz b ma mt ka mc md mu kd mf nr mv mi mj ns mw mm mn nt mx mq mr ms ij bi translated"><strong class="lz ja">注:</strong>为了避免混淆VGG-16深度学习模型和牛津视觉几何小组(VGG)，我将把后者称为牛津小组。</p></blockquote><p id="c0fb" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><a class="ae le" href="https://github.com/rcmalli/keras-vggface" rel="noopener ugc nofollow" target="_blank">这个</a>实现将Keras中的整个模型与TensorFlow v1.14作为后端。我计划在TensorFlow v2.3中构建相同的模型，所以我在本地系统中创建了一个virtualenv并提取了模型权重。这些提取的权重存储在<code class="fe nu nv nw nx b">vgg_face_weights.h5</code>中，随后加载到未经训练的VGG-16(在TensorFlow v2.3中)网络上，如本文<a class="ae le" href="www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf" rel="noopener ugc nofollow" target="_blank">的</a>所示。如果您希望使用ResNet-50或SeNet-50，那么您可以使用<a class="ae le" href="https://github.com/rcmalli/keras-vggface" rel="noopener ugc nofollow" target="_blank"> Refik Can Malli的存储库</a>来获取模型和重量。</p><p id="08fd" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">VGG-16模型是在这篇<a class="ae le" href="www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中显示的数据集上训练的，他们已经在2622张不同的人脸上训练了分类模型。倒数第二层有4096个密集单元，我们在其上附加一个128个单元的密集层，没有偏差项，并删除包含2622个单元的分类/softmax层。128密层之前的所有层都被冻结(<code class="fe nu nv nw nx b">trainable = False</code>)，只需要训练新加入的密层。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ny nz l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">加载VGG-16预训练的重量，然后定制模型</p></figure><p id="b2a5" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">现在为了训练这个网络，我们使用一个三重损失函数。三重损失函数采用从上述网络生成的三个128维特征。让这三个被称为锚，积极和消极的地方</p><ul class=""><li id="8257" class="mz na iq lz b ma mt md mu mg nb mk nc mo nd ms ne nf ng nh bi translated">主持人:一个人的形象，将用于比较。</li><li id="4e51" class="mz na iq lz b ma ni md nj mg nk mk nl mo nm ms ne nf ng nh bi translated">正面:与主播同一个人的形象。</li><li id="2f5a" class="mz na iq lz b ma ni md nj mg nk mk nl mo nm ms ne nf ng nh bi translated">负面:与主播不同的人的形象。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/48e092b3e4558b0bc47e5467cfca1cd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*jDVOh3GYZVHoIJVAMMwUvg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">三重损失函数— <a class="ae le" href="https://arxiv.org/pdf/1503.03832.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="2c43" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">三重态损失试图减少锚和正对之间的距离，增加锚和负对之间的距离。还有另一个参数<code class="fe nu nv nw nx b">alpha = 0.2</code>，它增加了一个余量，从而使训练更加困难，并给出更好的收敛性。参数<code class="fe nu nv nw nx b">128D</code>密集单元和损失函数参数<code class="fe nu nv nw nx b">alpha</code>是根据本<a class="ae le" href="https://arxiv.org/pdf/1503.03832.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中的分析选择的。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ny nz l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">三重损失函数的实现</p></figure><p id="3424" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">总结到现在吧！！VGG-16网络为我们提供了锚、正和负图像的128D特征，这些特征然后被馈送到损失函数。</p><p id="6fdd" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">现在对于训练，一种选择是在每个锚、正面和负面图像上调用相同的模型三次，然后将值赋予损失函数。然而，一个接一个地运行它们并不是一个好主意。因此，我将它们包装在一个扩展了<code class="fe nu nv nw nx b">tf.keras.Model</code>的Siamese Network类中，并将并行化留给了TensorFlow。此外，还有一件事添加到模型中，<strong class="lz ja"> L2正则化</strong>应用于128D密集层的输出。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ny nz l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">暹罗网络类</p></figure><p id="5b98" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我已经在SiameseNetwork类中添加了一个函数<code class="fe nu nv nw nx b">get_features</code>，这只是一个优化，在测试过程中会很有用。</p><p id="4a3b" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">太好了，我们建立了一个模型！！现在让我们来看看用于训练的数据集。</p><h1 id="e28e" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">资料组</h1><p id="38a2" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">由2622个不同的名人图像组成的<a class="ae le" href="https://www.robots.ox.ac.uk/~vgg/data/vgg_face/" rel="noopener ugc nofollow" target="_blank"> VGGFace </a>数据集用于训练上面使用的VGG-16模型。后来，牛津小组还发布了由8631个名人图像组成的<a class="ae le" href="https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/data_infor.html" rel="noopener ugc nofollow" target="_blank"> VGGFace2 </a>用于训练，测试中有500个，每个都很独特。由于训练集是39GB，所以我只下载了测试集，是2BG，训练了最后一个密集层。</p><p id="db8a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">虽然使用一个测试集进行训练听起来可能有违直觉，但这是关于他们训练的模型的测试集。至于我，我已经把它作为一个训练集，并在我的家人和朋友身上测试了我的模型。</p><p id="4fbd" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">预处理通常取决于底层模型。因此，为了训练和测试，输入图像必须经过由牛津小组实施的VGG-16模型定义的相同预处理。输入到模型中的图像首先通过这篇<a class="ae le" href="http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/ECCV_2014/papers/8692/86920720.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中描述的人脸检测器，然后发送到这里<a class="ae le" href="https://github.com/rcmalli/keras-vggface/blob/master/keras_vggface/utils.py" rel="noopener ugc nofollow" target="_blank">给出的<code class="fe nu nv nw nx b">preprocess_input</code>函数</a>。在我的实现中，我使用了dlib库提供的正面人脸检测器，然后将图像发送到<code class="fe nu nv nw nx b">preprocess_input</code>函数。</p><blockquote class="nn no np"><p id="acb3" class="lx ly nq lz b ma mt ka mc md mu kd mf nr mv mi mj ns mw mm mn nt mx mq mr ms ij bi translated"><strong class="lz ja">注意:</strong>这里<a class="ae le" href="https://github.com/rcmalli/keras-vggface" rel="noopener ugc nofollow" target="_blank">定义的预处理_输入函数</a>与ImageNet上训练的VGG-16使用的不同。因此，在我的存储库中，预处理的代码取自pip installed <a class="ae le" href="https://github.com/rcmalli/keras-vggface" rel="noopener ugc nofollow" target="_blank"> VGGFace </a>库。</p></blockquote><p id="91c1" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">现在，我将展示数据集的目录结构，因为它成为在训练期间优化内存的一种方式。让我们先来看看下载的数据集目录结构。在下面的目录结构中，每个目录(n000001，n000009等。)被分配给一个名人的所有图像。</p><pre class="kp kq kr ks gt ob nx oc od aw oe bi"><span id="5f37" class="of lg iq nx b gy og oh l oi oj">.<br/>└── vggface2_test<br/>    └── test<br/>        ├── n000001<br/>        │   ├── 0001_01.jpg<br/>        │   ├── 0002_01.jpg<br/>        │   ├── 0003_01.jpg ...<br/>        ├── n000009<br/>        │   ├── 0001_01.jpg<br/>        │   ├── 0002_01.jpg<br/>        │   ├── 0003_01.jpg ...<br/>(so on and so forth)</span></pre><p id="8dda" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">如上所述，我们使用dlib的正面人脸检测器来检测包含人脸的图像，并将它们存储在一个名为dataset的不同文件夹中。下面是人脸检测图像的目录树。<a class="ae le" href="https://github.com/dedhiaparth98/face-recognition/blob/master/notebooks/DatagGeneration.ipynb" rel="noopener ugc nofollow" target="_blank">本笔记本</a>也有同样的实现。</p><pre class="kp kq kr ks gt ob nx oc od aw oe bi"><span id="14ae" class="of lg iq nx b gy og oh l oi oj">.<br/>└── dataset<br/>    └── list.txt<br/>    └── images<br/>        ├── n000001<br/>        │   ├── 0001_01.jpg<br/>        │   ├── 0002_01.jpg<br/>        │   ├── 0003_01.jpg ...<br/>        ├── n000009<br/>        │   ├── 0001_01.jpg<br/>        │   ├── 0002_01.jpg<br/>        │   ├── 0003_01.jpg ...<br/>(so on and so forth)</span></pre><p id="5a2a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">vggface_test和dataset的目录结构几乎是相似的。但是，数据集目录可能包含更少的图像，因为一些面部可能没有被dlib的检测器检测到。此外，在数据集目录中有一个文件<code class="fe nu nv nw nx b">list.txt</code>，它包含每个图像的如下数据<code class="fe nu nv nw nx b">directory-name/image-name</code>。这个list.txt用于训练时的内存优化。</p><h1 id="ed9f" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">三胞胎世代</h1><p id="f0d0" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">为了训练，模型需要三个图像——锚、正面和负面图像。我脑海中的第一个想法是生成所有可能的三胞胎对。这似乎给出了很多数据，但研究文献表明这是低效的。所以我用了一个随机数发生器来选择锚，积极和消极的图像对。我使用了一个在训练循环中产生数据的数据生成器。如果你不熟悉数据生成器，请参考这个博客。</p><blockquote class="nn no np"><p id="23c4" class="lx ly nq lz b ma mt ka mc md mu kd mf nr mv mi mj ns mw mm mn nt mx mq mr ms ij bi translated"><strong class="lz ja">有趣的事实:</strong>我花了比模型训练更多的时间来编写DataGenerator类。</p></blockquote><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ny nz l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">三元组数据发生器</p></figure><p id="030d" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><code class="fe nu nv nw nx b">__getitem__</code>是最重要的功能。然而，为了理解这一点，让我们也检查一下构造函数和其他方法。</p><ul class=""><li id="9554" class="mz na iq lz b ma mt md mu mg nb mk nc mo nd ms ne nf ng nh bi translated"><strong class="lz ja"> __init__: </strong>构造函数采用前面小节中定义的数据集目录的路径。构造函数使用<code class="fe nu nv nw nx b">list.txt</code>来创建一个字典。这个字典将目录名作为它的键，将目录中的一系列<strong class="lz ja">图像作为它的值。正是在这里，在混洗步骤中，list.txt成为我们了解数据集概况的一种简单方式，从而避免了加载图像进行混洗。</strong></li><li id="0d20" class="mz na iq lz b ma ni md nj mg nk mk nl mo nm ms ne nf ng nh bi translated"><strong class="lz ja"> __getitem__: </strong>我们从上面的字典键中获取人名。对于第一批，前32个(批量大小)人物图像用作锚，同一个人的不同图像用作阳性。从任何其他目录中选择一个负面图像用于训练。对于所有的三胞胎，锚，积极和消极的形象是随机选择的。接下来的32人将成为下一批的主播。</li><li id="c704" class="mz na iq lz b ma ni md nj mg nk mk nl mo nm ms ne nf ng nh bi translated"><strong class="lz ja">策展_数据集:</strong>创建在<code class="fe nu nv nw nx b">__init__</code>中解释的字典</li><li id="86e4" class="mz na iq lz b ma ni md nj mg nk mk nl mo nm ms ne nf ng nh bi translated"><strong class="lz ja"> on_epoch_end: </strong>在每个epoch结束时，人的顺序被打乱，因此在下一个epoch中，前32个图像与前一个epoch中看到的图像不同。</li><li id="a75e" class="mz na iq lz b ma ni md nj mg nk mk nl mo nm ms ne nf ng nh bi translated"><strong class="lz ja"> get_image: </strong>获取图像功能在将图像调整到(224 x 224)大小后使用<code class="fe nu nv nw nx b">preprocess_input</code>。</li><li id="9c5a" class="mz na iq lz b ma ni md nj mg nk mk nl mo nm ms ne nf ng nh bi translated"><strong class="lz ja"> __len__: </strong>这将返回定义一个时期的批次数量。</li></ul><p id="4615" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">搞定了。！！</p><h1 id="ecd6" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">培训和测试</h1><p id="1be8" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">我已经用tqdm使用了一个定制的训练循环(你仍然可以感觉到Keras ),并训练了50个时期的模型。在colab上，每个历元的训练时间是<strong class="lz ja"> 24秒，</strong>所以是的，训练相当快。</p><p id="7155" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">对于测试，您可以将家人、朋友和您自己的图像保存在一个目录中，还可以存储从每个人的密集图层中生成的128D特征。您可以使用<code class="fe nu nv nw nx b">get_features()</code>函数，该函数在此处的SiameseNetwork类中定义。此外，为了节省您的时间，我制作了一个笔记本<a class="ae le" href="https://github.com/dedhiaparth98/face-recognition/blob/master/notebooks/Real-time-prediction.ipynb" rel="noopener ugc nofollow" target="_blank"> Real-time-prediction.ipynb，</a>，它加载模型检查点，并提供收集图像以进行动态测试并在网络摄像头视频上预测它们的说明。</p><h1 id="ef0a" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">杂项详细信息</h1><h2 id="5365" class="of lg iq bd lh ok ol dn ll om on dp lp mg oo op lr mk oq or lt mo os ot lv iw bi translated">提高Colab的训练速度</h2><p id="12b7" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在DataGenerator中，并不是所有的图像都加载到内存中，而是加载它们的索引进行操作。如果您有自己的GPU，那么这一小节中的细节可能不太相关。</p><p id="0f3a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我最初认为从colab到drive的读写操作应该很快，但结果是它们变得比我的甚至没有GPU的本地系统慢。为了解决这个问题，我将数据集压缩到<code class="fe nu nv nw nx b">dataset.7z</code>并上传到我的硬盘上。然后将zip文件从我的Google drive复制到colab的空间，在那里提取，然后用于训练。使用colab的空间大大提高了训练过程的速度。</p><p id="8e78" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">但是，我的tensorboard摘要和模型检查点存储在驱动器中，因为它们每个时期都被访问一次，不会显著降低性能。</p><h2 id="90d5" class="of lg iq bd lh ok ol dn ll om on dp lp mg oo op lr mk oq or lt mo os ot lv iw bi translated">基于用户界面的工具</h2><p id="00d3" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">我想学习一些网络技术，比如HTML、CSS和Javascript。最好的学习方法是做一个小项目。因此，我试图开发一个基于UI的工具来收集测试和预测数据。运行相同程序的步骤在我的<a class="ae le" href="https://github.com/dedhiaparth98/face-recognition" rel="noopener ugc nofollow" target="_blank">库</a>中有解释。</p><h1 id="dc22" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">结论</h1><p id="3a29" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在这篇博客中，我们讨论了关于微调现有网络和在其上构建连体网络的关键细节。当前模型的结果比预期好得多，但是我们也可以通过手动创建好的三胞胎来改进它们。也可以下载整个训练数据集来训练模型。文献表明，手动选择一组硬三元组将显著减少训练时间并提高模型的收敛速度。</p><p id="152b" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">你可以参考<a class="ae le" href="https://github.com/dedhiaparth98/face-recognition" rel="noopener ugc nofollow" target="_blank">我的知识库</a>来尝试基于浏览器的工具以及查看培训用的笔记本。该工具还可以检测多人！！</p><h1 id="4245" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">参考</h1><p id="952a" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">O.M. Parkhi，A. Vedaldi，A. Zisserman，<a class="ae le" href="https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/" rel="noopener ugc nofollow" target="_blank">深度人脸识别</a>，英国机器视觉大会，2015。</p><p id="58be" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">Q. Cao，L. Shen，W. Xie，O. M. Parkhi，A. Zisserman，<a class="ae le" href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Cao18/cao18.pdf" rel="noopener ugc nofollow" target="_blank"> VGGFace2:跨姿态和年龄的人脸识别数据集</a> <strong class="lz ja">，</strong>自动人脸和手势识别国际会议，2018。</p><p id="943c" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">F.Schroff，D. Kalenichenko，J. Philbin，<a class="ae le" href="https://arxiv.org/pdf/1503.03832.pdf" rel="noopener ugc nofollow" target="_blank"> FaceNet:用于人脸识别和聚类的统一嵌入，</a> CVPR，2015年。</p><p id="0013" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">G.Koch，R. Zemel，R. Salakhutdinov，<a class="ae le" href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" rel="noopener ugc nofollow" target="_blank">用于一次性图像识别的连体神经网络</a>，<em class="nq"> ICML深度学习研讨会</em>。第二卷。2015.</p><div class="ou ov gp gr ow ox"><a href="https://github.com/rcmalli/keras-vggface" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd ja gy z fp pc fr fs pd fu fw iz bi translated">rcmalli/keras-vggface</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">使用Keras Functional Framework v2+模型的Oxford VGGFace实现是从原来的caffe网络转换而来的…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">github.com</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl ky ox"/></div></div></a></div><div class="ou ov gp gr ow ox"><a href="https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd ja gy z fp pc fr fs pd fu fw iz bi translated">使用Keras的数据生成器的详细示例</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">python keras 2 fit _ generator Afshine Amidi和Shervine Amidi的大型数据集多重处理您是否曾经不得不…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">stanford.edu</p></div></div><div class="pg l"><div class="pm l pi pj pk pg pl ky ox"/></div></div></a></div><div class="ou ov gp gr ow ox"><a href="https://medium.com/datadriveninvestor/speed-up-your-image-training-on-google-colab-dc95ea1491cf" rel="noopener follow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd ja gy z fp pc fr fs pd fu fw iz bi translated">在Google Colab上加速你的图像训练</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">获得一个因素20加速训练猫对狗分类器免费！</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">medium.com</p></div></div><div class="pg l"><div class="pn l pi pj pk pg pl ky ox"/></div></div></a></div><div class="ou ov gp gr ow ox"><a href="https://neptune.ai/blog/content-based-image-retrieval-with-siamese-networks" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd ja gy z fp pc fr fs pd fu fw iz bi translated">用PyTorch - neptune.ai中的连体网络实现基于内容的图像检索</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">图像检索是寻找与给定查询相关的图像的任务。对于基于内容的图像检索，我们指的是…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">海王星. ai</p></div></div><div class="pg l"><div class="po l pi pj pk pg pl ky ox"/></div></div></a></div></div></div>    
</body>
</html>