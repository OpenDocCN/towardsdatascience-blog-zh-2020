<html>
<head>
<title>A New Way to BOW Analysis &amp; Feature Engineering — Part1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一种新的BOW分析方法&amp;特征工程(一)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-new-way-to-bow-analysis-feature-engineering-part1-e012eba90ef?source=collection_archive---------63-----------------------#2020-10-13">https://towardsdatascience.com/a-new-way-to-bow-analysis-feature-engineering-part1-e012eba90ef?source=collection_archive---------63-----------------------#2020-10-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8518" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在不建立ML模型的情况下，比较跨标签的频率分布。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9d5bd4631fe1f989c52fd7eb6df333af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PQ-IS1SPTTHASHzq"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">卢克·切瑟在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="0186" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我的一个朋友问了我一个问题——“我们如何在不同的类别或标签之间比较蝴蝶结？其中类别或标签可以是情绪或状态或某个客户群。</p><p id="f2a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我的直觉反应是——为每个类别创建一个词频条形图。这确实是一个易于实现的解决方案，但是有各种缺点，其中一些是:</p><ul class=""><li id="64ae" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">从事这项工作的数据科学家/分析师需要比较所有类别中的单词及其频率，在某些情况下，如国家或地区，可能会轻松超过100个</li><li id="26ee" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">比较频率可能不会给出任何见解。例如，我们有一个词——“数据”，它在标签1和标签2上的出现频率分别是150和100。是的，的确有50英镑的差别，但这种差别有意义吗？</li><li id="5b6b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如果——我想知道在没有建立模型/分类器的情况下，区分这些类别的最热门的词是什么，有没有一种方法可以分辨出来？</li></ul><p id="b053" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后下一个想法是创建一个单词云，但即使它也没有解决上述问题。</p><p id="a834" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">经过一段时间的思考，我知道解决方案在于比较不同类别的频率，但在一键编码或计数矢量化或TF-IDF中找不到答案，因为在使用这些方法时存在一个常见问题。</p><p id="e40f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">问题是这些为每个文档创建特性和它们的值，然后我们如何在标签/类别级别上进行汇总？甚至让我们以计数矢量器为例——我们将获得每个文档中出现的单词的频率——但是要在标签/类别级别进行任何分析，我们将需要合计计数并汇总到类别级别。一旦我们积累起来，我们肯定可以使不同标签上的词的频率有所不同，但同样——这不能解决上面提到的第三个问题。也就是说，如果单词“数据”的频率差是50，它告诉我们什么？这有意义吗？</p><p id="59df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，你可以猜测我们正在走向哪里——我们在频率上有差异，我们想知道这些差异是否显著。随之而来的是帮助我们的<strong class="ky ir">统计数据</strong>。</p><p id="38d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我假设你知道各种测试，如z-测试，t-测试，方差分析等。我们使用这些测试来比较多个平均值或分布，这与我们在这里试图解决的问题是一致的。</p><p id="ee68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下表包含一个词在标签上的出现频率—目标0和目标1。，也就是分布图。我们可以很容易地使用像z-检验，t-检验等测试。并且比较这些分布，并且如果差异被证明是显著的(给定显著性水平),我们可以说这样的词在标签上具有不同的分布，并且因此可以是模型中的区分因素。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/78d9390a893111bcc7e13c66efe4a92b.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/0*GNWPsa2OrHj1PgqO.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">标签中单词的出现频率—目标0和目标1</p></figure><p id="984b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，这样我们甚至可以将这种技术用于特征工程，并减少模型中的特征数量。</p><p id="d0b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简单对吗？是的，确实只有在我们处理完最后一点之后。</p><p id="7ba4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> z检验</strong>、<strong class="ky ir"> t检验、</strong>或<strong class="ky ir"> ANCOVA </strong>是<strong class="ky ir">参数</strong>检验，这意味着它们对数据做出假设，其中一个主要假设是数据呈正态分布。</p><p id="fbdb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，这种假设可能对数据集中的所有特征/单词都成立，也可能不成立。因此，我们将不得不依靠一些<strong class="ky ir">非参数</strong>测试，这些测试不做任何这样的假设。</p><p id="961a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们很少有这样的测试，我们将使用的唯一测试是<strong class="ky ir">曼惠特尼U测试</strong>。该测试一次比较两个分布，非常适合我们使用<strong class="ky ir">二项式</strong>标签的情况，以及使用<strong class="ky ir">多项式</strong>的情况，我们可以使用<strong class="ky ir">克鲁斯卡尔-沃利斯测试</strong>。</p><p id="1b0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Kruskal-Wallis检验类似于ANOVA，它告诉我们标签之间的分布是否相同，但它不告诉哪些标签的分布不同，为了得到相同的结果，我们可以应用成对的Mann Whitney U检验。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="8fa4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们应用所有的学习，同样的，来自Kaggle的<strong class="ky ir">灾难推文分类</strong>数据集也被使用。</p><p id="ff76" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">读取数据集。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="dbb4" class="mt mu iq mp b gy mv mw l mx my">train_df = pd.read_csv("/kaggle/input/nlp-getting-started/train.csv")[['target', 'text']]</span><span id="956e" class="mt mu iq mp b gy mz mw l mx my">print("Size of the data: ", train_df.shape[0])<br/>train_df.head(2)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8d2abc5b505438f4ccabbafa2ff6c78e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/0*CrG4ouMgmqA6QeK3.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一些样本记录</p></figure><p id="44a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将使用CrazyTokenizer将数据集中的句子转换为单词包。此外，一些额外的清理是在下面的预处理函数中完成的。</p><p id="db6a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该函数将被传递给CountVectorizer，它将给出输入数据集中每条记录的单词频率。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="85e8" class="mt mu iq mp b gy mv mw l mx my">ctokenizer = CrazyTokenizer(lowercase=True, normalize=1, remove_punct=True, ignore_stopwords=True, stem='lemm', <br/>                            remove_breaks=True, hashtags=' ', twitter_handles=' ', urls=' ')</span><span id="afdc" class="mt mu iq mp b gy mz mw l mx my">ctokenizer.tokenize("There's an emergency evacuation happening now in the building across the street")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/b72a5683be9b74e88469acaedf342c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/0*RRTrw9oas2HX_wN2.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">句子的结尾</p></figure><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="e9c9" class="mt mu iq mp b gy mv mw l mx my"># the preprocessor function for CountVectorizer<br/>def preprocessor(text):</span><span id="fd7d" class="mt mu iq mp b gy mz mw l mx my"># split the sentence into tokens<br/>    tokens = ctokenizer.tokenize(text)</span><span id="fdab" class="mt mu iq mp b gy mz mw l mx my"># remove any numeric character<br/>    tokens = [re.sub(r'[0-9-]+', '', token) for token in tokens]</span><span id="8318" class="mt mu iq mp b gy mz mw l mx my"># remove stop words or any token having size &lt;3<br/>    tokens = [token for token in tokens if len(token) &gt;= 3]<br/> <br/>    return " ".join(tokens)</span><span id="5538" class="mt mu iq mp b gy mz mw l mx my"># create the CountVectorizer object<br/>cvectorizer = CountVectorizer(preprocessor=preprocessor, min_df=20)</span><span id="17f9" class="mt mu iq mp b gy mz mw l mx my"># to get the features<br/>features = cvectorizer.fit_transform(train_df['text'])</span><span id="ba51" class="mt mu iq mp b gy mz mw l mx my"># create a dataframe of features<br/>features = pd.DataFrame(features.toarray(), columns=cvectorizer.get_feature_names())<br/>features.head(5)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/7e463e01cb81ec28322510e6b6e18a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mozNTyECHFeRQHk9.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用计数矢量器创建的要素</p></figure><p id="058e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们把目标变量和特性一起带来。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="3a5d" class="mt mu iq mp b gy mv mw l mx my"># merge features with the reviews_data to get labels<br/>train_df_w_ft = pd.concat([train_df[['target']], features], axis=1)<br/>train_df_w_ft.head(5)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/098f317423e02f697d450ba78f4f360d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Fx5GFPSppIJKVFwl.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有目标变量的特征数据</p></figure><p id="4af7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将绘制一个条形图，显示属于每个目标值的前30个单词。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="6d63" class="mt mu iq mp b gy mv mw l mx my">def plot_bar_graph(xs, ys, names, xlabel, ylabel, title):</span><span id="c18e" class="mt mu iq mp b gy mz mw l mx my"># create figure object<br/>    fig = go.Figure()</span><span id="0391" class="mt mu iq mp b gy mz mw l mx my"># create bar chart for each of the series provided <br/>    for (x, y), name in zip(zip(xs, ys), names):</span><span id="44d1" class="mt mu iq mp b gy mz mw l mx my">fig.add_trace(go.Bar(x=x, y=y, name=name, orientation='v'))</span><span id="fda6" class="mt mu iq mp b gy mz mw l mx my"># Here we modify the tickangle of the xaxis, resulting in rotated labels.<br/>    fig.update_layout(<br/>        barmode='group',<br/>        autosize=False,<br/>        width=1300,<br/>        height=500,<br/>        margin=dict(l=5, r=5, b=5, t=50, pad=5),<br/>        xaxis={'type': 'category', 'title': xlabel},<br/>        yaxis_title=ylabel,<br/>        title=title<br/>    )<br/>    fig.show()</span><span id="e842" class="mt mu iq mp b gy mz mw l mx my">top_x = 30</span><span id="a2f2" class="mt mu iq mp b gy mz mw l mx my">words_lists = []<br/>frequencies = []<br/>targets = []</span><span id="a20f" class="mt mu iq mp b gy mz mw l mx my">for target in train_df_w_ft['target'].unique():</span><span id="834a" class="mt mu iq mp b gy mz mw l mx my"># add label name<br/>    targets.append("Target-{}".format(target))</span><span id="27ac" class="mt mu iq mp b gy mz mw l mx my"># get the top words<br/>    word_freq = train_df_w_ft[train_df_w_ft['target'] == target].iloc[:, 1:].sum(axis=0)<br/>    word_freq = sorted(word_freq.to_dict().items(), key=lambda x: x[1], reverse=True)[: top_x]</span><span id="e7f9" class="mt mu iq mp b gy mz mw l mx my"># append the words<br/>    words_lists.append([x[0] for x in word_freq])</span><span id="ca24" class="mt mu iq mp b gy mz mw l mx my"># append the frequencies<br/>    frequencies.append([x[1] for x in word_freq])</span><span id="4d5f" class="mt mu iq mp b gy mz mw l mx my">plot_bar_graph(words_lists, frequencies, targets, "Words", "Frequency", "Frequency of Words across Targets")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/898b60c7d4ff097be099f2891a27950d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sF6a5gM3ZIgsA3t2.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">目标值中前30个词的出现频率。</p></figure><p id="be1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上图中，我突出了两个词——“火灾”和“紧急情况”。对于单词“fire ”,目标之间的频率差异很大(~ 180 °),而对于单词“emergency ”,差异非常非常小。</p><p id="80fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们上面讨论的，我们的目的是确定这些差异是否显著，这很重要，因为两个单词都出现在两个目标值的句子中。我们想知道它是否使这些特征变得无关紧要，即它们在决定一个句子是属于目标0还是目标1时是否有发言权。</p><p id="83c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是单词“fire”的曼-惠特尼U检验结果。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="1a06" class="mt mu iq mp b gy mv mw l mx my">fire_data0 = train_df_w_ft[train_df_w_ft['target'] == 0]['fire']<br/>fire_data1 = train_df_w_ft[train_df_w_ft['target'] == 1]['fire']</span><span id="1eb6" class="mt mu iq mp b gy mz mw l mx my">mannwhitneyu(fire_data0, fire_data1)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/56c9542f041ab2b61acbc2274ba7bf0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WTAmcu8znijJYF4C.png"/></div></div></figure><p id="ba11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从测试中，我们得到了小于0.05(我们的显著性水平)的～0的p值，因此，我们可以得出结论，“火灾”的频率分布在目标值之间有显著差异(置信度为95%)。</p><p id="5d8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来看看同样的“紧急情况”。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="cbe5" class="mt mu iq mp b gy mv mw l mx my">emergency_data0 = train_df_w_ft[train_df_w_ft['target'] == 0]['emergency']<br/>emergency_data1 = train_df_w_ft[train_df_w_ft['target'] == 1]['emergency']</span><span id="bb6e" class="mt mu iq mp b gy mz mw l mx my">mannwhitneyu(emergency_data0, emergency_data1)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/6a5c159b1b0ee0006ca3af5dabc0ba8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/0*RqlIscMSJxGL6IMo.png"/></div></figure><p id="4e92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此处，p值为0.07，大于0.05的显著性水平，因此我们可以得出结论，“紧急事件”的频率分布相似，置信度为95%。</p><p id="63c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，该是沙漠的时候了——也就是说，让我们来看看那些在目标上有显著不同分布的单词，这是我们想要解决的问题。</p><p id="6d6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码将对数据集中的所有单词应用Mann Whitney U检验，并将记录那些具有不同分布的单词。同样，将策划这样的50个字。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="810f" class="mt mu iq mp b gy mv mw l mx my">words_significance = []</span><span id="afb7" class="mt mu iq mp b gy mz mw l mx my">for word in cvectorizer.get_feature_names():</span><span id="936a" class="mt mu iq mp b gy mz mw l mx my"># get the pvalue<br/>    _, pval = mannwhitneyu(<br/>        train_df_w_ft[train_df_w_ft['target'] == 0][word],<br/>        train_df_w_ft[train_df_w_ft['target'] == 1][word]<br/>    )</span><span id="ba84" class="mt mu iq mp b gy mz mw l mx my"># check for significance<br/>    if pval &lt; 0.05:<br/>        words_significance.append((word, pval))</span><span id="816f" class="mt mu iq mp b gy mz mw l mx my">print("Total Number of words: ", len(cvectorizer.get_feature_names()))<br/>print("Number of words having different distributions with confidence of 95%: ", len(words_significance))</span><span id="95f5" class="mt mu iq mp b gy mz mw l mx my"># plot the top words by pvalue<br/>top_x = 50</span><span id="8c9d" class="mt mu iq mp b gy mz mw l mx my"># seperate the word and pvalues<br/>words_list = [x[0] for x in words_significance][: top_x]<br/>significance = [0.05 - x[1] for x in words_significance][: top_x]</span><span id="da6e" class="mt mu iq mp b gy mz mw l mx my"># get the total frequencies of significantly different words across labels<br/>freq_label0 = [train_df_w_ft[train_df_w_ft['target'] == 0][x[0]].sum() for x in words_significance]<br/>freq_label1 = [train_df_w_ft[train_df_w_ft['target'] == 1][x[0]].sum() for x in words_significance]</span><span id="6555" class="mt mu iq mp b gy mz mw l mx my"># plot the bar graph<br/>plot_bar_graph([words_list, words_list], [freq_label0, freq_label1], ['target0_freq', 'target1_freq'], "Words", "Frequency", "Frequency of Words across Targets")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/6a37a52b37bcd9862ccab095d64095e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BgDYHMcOoxA3m9_I.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/af924a9286b8f58a14854191c1d22b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZkBANSpA_iKmehAn.png"/></div></div></figure><p id="7623" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面的图表中我们可以看到，有些词同时出现在两个目标值中，但是频率的差异是显著的。</p><p id="3491" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们也只剩下633个单词中的451个，这是原始特征的71%,因此这可以用作特征选择的技术之一。</p><p id="e409" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我还绘制了0.05的差值和上述单词的p值。因此，具有更接近0的y轴值的单词将指示它们的p值更接近显著性水平，因此相对来说不如y轴值更接近0.05的单词重要，因为这指示它们的p值接近0。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/4e8c130e9eac12f5a8b9232074d0031c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7WoH_ifaFZmF8T92.png"/></div></div></figure><p id="42c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像“ago”、“big”、“blue”这样的词与其他词相比意义较低。</p><p id="afd1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些都是为了现在的分析。我将很快写另一篇文章，在这篇文章中，我将建立两个模型，一个具有所有的特征，另一个仅具有重要的单词，并将比较这两个模型的准确性。</p><p id="89ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">更新:</strong>请点击分析第二部分<a class="ae kv" href="https://medium.com/@prateekkrjain/a-new-way-to-bow-analysis-feature-engineering-part2-451d586566c6" rel="noopener">的链接。</a></p><p id="243b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述分析的代码可在Kaggle上找到，并可通过<a class="ae kv" href="https://www.kaggle.com/pikkupr/a-new-way-to-bow-analysis-and-feature-engg" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/pikkupr/a-new-way-to-bow-analysis-and-feature-engg</a>访问</p><p id="60ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望你觉得我的上述工作有价值，请分享你对这种方法的看法，如果有任何改进的建议，请提出意见，我很想听听你的想法:)</p></div></div>    
</body>
</html>