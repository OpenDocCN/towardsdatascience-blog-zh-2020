<html>
<head>
<title>Build a Shallow Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">建立一个浅层神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-shallow-neural-network-a4e2728441e0?source=collection_archive---------13-----------------------#2020-10-31">https://towardsdatascience.com/building-a-shallow-neural-network-a4e2728441e0?source=collection_archive---------13-----------------------#2020-10-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c260" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解释了理论和实现</h2></div><h1 id="64c0" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">一个隐藏层神经网络</h1><p id="e5e5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们将构建一个具有一个隐藏层的浅层密集神经网络，以下结构用于说明目的。</p><p id="7c34" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在试图理解这篇文章之前，我强烈建议你看一下我之前的逻辑回归的<a class="ae mb" href="https://meatba11.medium.com/logistic-regression-step-by-step-implementation-f032a89936ca" rel="noopener">实现，因为逻辑回归可以看作是一个1层神经网络，基本概念实际上是相同的。</a></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/9c80fb90d4d345e11e59f2c660518e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AM6XF_x7GkpFu2w_VA2pgw.png"/></div></div></figure><p id="d08e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">其中在上图中，我们有一个输入向量x = (x_1，x_2)，包含2个特征和4个隐藏单元a1、a2、a3和a4，并在[0，1]中输出一个值y_1。(考虑这是一个带有概率预测的二元分类任务)</p><p id="b611" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在每个隐单元中，以a_1为例，进行一个线性运算，然后是一个激活函数。因此，给定输入x = (x_1，x_2)，在节点a_1内，我们有:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mo"><img src="../Images/81f24f05618b3216168720993d854258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Y7OlcG-WvQ_r9GRbuNr5g.png"/></div></div></figure><p id="e0fc" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这里w_{11}表示节点1的权重1，w_{12}表示节点1的权重2。对于节点a_2也是如此，它应该有:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mp"><img src="../Images/f32b3b3b4d2b8c0b7e46fbad73c1f8de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sv3_xFgQ6Emz1X8k4jZ0SA.png"/></div></div></figure><p id="65d6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">a3和a4也是如此，以此类推…</p><h1 id="7c19" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">一个输入的矢量化</h1><p id="8b58" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在让我们将权重放入矩阵并输入到向量中，以简化表达式。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mq"><img src="../Images/d3c245d22500af891c77fefc9c844ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvk8o7-jBNd3jEy4T_H1rw.png"/></div></div></figure><p id="9201" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这里我们假设第二激活函数为<code class="fe mr ms mt mu b">tanh</code>，输出激活函数为<code class="fe mr ms mt mu b">sigmoid</code>(注意上标[i]表示第I层)。</p><p id="88de" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">对于每个矩阵的维数，我们有:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mv"><img src="../Images/6d9ab8fcb01bec490cae1e46be9e2255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gWGrrdxPZr2uEzXQv_ZLYQ.png"/></div></div></figure><p id="52bd" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">单个值的损失函数L将与逻辑回归的损失函数相同(此处<a class="ae mb" href="https://meatba11.medium.com/logistic-regression-step-by-step-implementation-f032a89936ca" rel="noopener">详细介绍</a>)。</p><p id="dfcd" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">功能<code class="fe mr ms mt mu b">tanh</code>和<code class="fe mr ms mt mu b">sigmoid</code>如下图所示。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/20369f18f11165ce228073bcfe1c3252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*RvNFcE0xf8OEzlyj6W-IJg.png"/></div></figure><p id="3609" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">请注意，这些函数的唯一区别是y的比例。</p><h1 id="6e8b" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">批量训练公式</h1><p id="66f7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">上面显示了单个输入向量的公式，但是在实际训练过程中，一次训练一批而不是一个。公式中应用的更改是微不足道的，我们只需用大小为<code class="fe mr ms mt mu b">n x m</code>的矩阵X替换单个向量X，其中n是特征数量，m是批量大小——样本按列堆叠，同样应用以下结果矩阵。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mx"><img src="../Images/9be6956c0a0a5252c412653a0d93da80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B2ROmQsSxfAeAM8LJvX80w.png"/></div></div></figure><p id="c0dd" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">对于本例中每个矩阵的维数，我们有:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi my"><img src="../Images/7ca69f27f95a68ae70b281f240ec835e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BZLhl2gQsxQOGzDsr5Sk3A.png"/></div></div></figure><p id="4f52" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">与逻辑回归相同，对于批量训练，所有训练样本的平均损失。</p><p id="a3f2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这都是为了正向传播。为了激活我们的神经元进行学习，我们需要获得权重参数的导数，并使用梯度下降来更新它们。</p><p id="5e7a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">但是现在我们先实现正向传播就足够了。</p><h1 id="7251" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">生成样本数据集</h1><p id="d500" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这里，我们生成一个简单的二元分类任务，包含5000个数据点和20个特征，用于以后的模型验证。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="47b6" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">权重初始化</h1><p id="0c4e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们的神经网络有1个隐层，总共2层(隐层+输出层)，所以有4个权重矩阵要初始化(W^[1]，b^[1]和W^[2]，b^[2]).注意，权重被初始化得相对较小，因此梯度会更高，从而在开始阶段学习得更快。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="6ea3" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">正向传播</h1><p id="4d80" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们按照等式(5)至(8)实现正向过程。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="b15d" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">损失函数</h1><p id="ad06" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">根据等式(9)，每批的损耗可计算如下。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="f98b" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">反向传播</h1><p id="00d9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在到了反向传播，这是我们的权重更新的关键。给定我们上面定义的损失函数L，我们有如下梯度:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nb"><img src="../Images/7006c154968ee42d3c898bb94342bf71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tz8SVUi5imMQ11aYXlhm9g.png"/></div></div></figure><p id="ee62" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果你不明白为什么Z^[2的导数如上，你可以在这里查看<a class="ae mb" href="https://meatba11.medium.com/logistic-regression-step-by-step-implementation-f032a89936ca" rel="noopener"/>。事实上，我们网络的最后一层与逻辑回归相同，所以导数是从那里继承的。</p><p id="0a9d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">方程(4)中是元素式乘法，tanh{x}的梯度是1 — x，你可以试着自己推导上面的方程，不过我基本上是从网上拿来的。</p><p id="6712" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">让我们分解每个元素的形状，给定每层的数量等于<code class="fe mr ms mt mu b">(n_x, n_h, n_y)</code>，批量等于<code class="fe mr ms mt mu b">m</code>:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nc"><img src="../Images/8891b75123cf8fb1aec0a785c2c671e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wCUksS8IUyq_OKHPCvYSPA.png"/></div></div></figure><p id="53dd" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">一旦我们理解了这个公式，实现起来就会很容易。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="6683" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">批量训练</h1><p id="8f92" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我将每个部分都放在一个类中，这样它就可以像python的通用包一样训练。此外，还实施批量训练。为避免冗余，我没有放在这里，详细实现请查看我的<a class="ae mb" href="https://github.com/MJeremy2017/deep-learning/tree/main/shallow-neural-network" rel="noopener ugc nofollow" target="_blank"> git repo </a>。</p><p id="4de8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">让我们看看我们实现的神经网络在数据集上的表现。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nd"><img src="../Images/a1eea5d57546f0d6f319dc8bbf8fed59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pqMA3149MiwlAMN7bapTPA.png"/></div></div></figure><p id="c49a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">使用10个隐藏神经元，我们的模型能够在测试集上达到95.1%的准确率，这是非常好的。</p><p id="7c7a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在继续尝试实现你自己，这个过程将真正帮助你获得对一般密集神经网络的更深理解。</p></div></div>    
</body>
</html>