<html>
<head>
<title>Complete Introduction to PySpark-Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark完整介绍-第2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/complete-introduction-to-pyspark-part-2-135d2f2c13e2?source=collection_archive---------39-----------------------#2020-11-13">https://towardsdatascience.com/complete-introduction-to-pyspark-part-2-135d2f2c13e2?source=collection_archive---------39-----------------------#2020-11-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dfd3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用PySpark进行探索性数据分析</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/48687a10e6fee131842716e52eac2b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4hfAXVw8YJ5x6hUo"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·斯皮斯克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="322b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">探索性数据分析</h1><p id="2473" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">探索性数据分析是最关键的部分，无论我们何时处理数据集，都要从它开始。它允许我们分析数据，并让我们探索数据的初始发现，如有多少行和列，不同的列是什么，等等。EDA是一种方法，在这种方法中，我们使用不同的方法主要是可视化来总结数据的主要特征。</p><p id="b029" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们使用PySpark启动EDA，在此之前，如果您还没有安装PySpark，请访问下面的链接并在您的本地机器上配置它。</p><div class="ms mt gp gr mu mv"><a href="https://medium.com/python-in-plain-english/complete-introduction-to-pyspark-part-1-7d16d7c62cc9" rel="noopener follow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">PySpark完整介绍</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">第1部分:从头开始在Windows上安装PySpark</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">medium.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ks mv"/></div></div></a></div><h1 id="a2f7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">导入所需的库和数据集</h1><p id="cc38" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一旦我们在机器上配置了PySpark，我们就可以使用Jupyter Notebook开始探索它。在本文中，我们将使用PySpark执行EDA操作，为此我们将使用可以从Kaggle下载的波士顿数据集。让我们从导入所需的库和加载数据集开始。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="51c1" class="np la it nl b gy nq nr l ns nt">#importing Required Libraries<br/>import findspark<br/>findspark.init()</span><span id="e7fb" class="np la it nl b gy nu nr l ns nt">import pyspark # only run after findspark.init()<br/>from pyspark.sql import SparkSession<br/>from pyspark.sql import SQLContext</span><span id="524b" class="np la it nl b gy nu nr l ns nt">#Creating a pyspark session<br/>spark = SparkSession.builder.getOrCreate()</span><span id="1816" class="np la it nl b gy nu nr l ns nt">#Importing Dataset<br/>df = spark.read.csv('Boston.csv', inferSchema=True, header=True)<br/>df.show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/bc707f99e73cc4ac6fb593ca8979f148.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZkkLvnkF5xyoQ4pZGxgSw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">波士顿数据集(来源:作者)</p></figure><h1 id="f40b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">启动EDA</h1><p id="8393" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">pyspark定义了不同的函数，我们可以使用它们进行探索性的数据分析，让我们探索其中的一些函数，看看它们有多有用。</p><ol class=""><li id="cd73" class="nw nx it lt b lu mn lx mo ma ny me nz mi oa mm ob oc od oe bi translated"><strong class="lt iu"> Schema <br/> </strong> Schema类似熊猫dataframe的Info()函数。它向我们显示了数据集中所有列的信息。</li></ol><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="e909" class="np la it nl b gy nq nr l ns nt">df.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/277b8566f81c83fa76ad9929f1a5537f.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*8jvlTO1fgeyQtgk-RZSiew.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模式(来源:作者)</p></figure><p id="77a1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 2。</strong>Describe<br/>Describe函数用于显示数据集中所有列的统计属性。它向我们展示了平均值、中间值等数值。对于所有的列。在PySpark中，每次我们需要显示信息时，我们都需要调用show()函数，它的工作方式就像python的head()函数一样。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="e27d" class="np la it nl b gy nq nr l ns nt">df.describe().show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/22602d451b725c1193639e9ae4d56e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ivr03F6OyB0wT0ND8pZWQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">统计属性(来源:作者)</p></figure><p id="9cb4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">类似地，我们也可以按列使用describe函数。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="15b8" class="np la it nl b gy nq nr l ns nt">df.describe('AGE').show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/b0db21e1eeefe57688473d4dc698b061.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*AsEg0rVNJllVIN0013uC1Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按列描述(来源:作者)</p></figure><p id="a847" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 3。filter<br/></strong>filter函数用于使用不同的自定义条件过滤数据。让我们看看如何相应地使用它。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="21c4" class="np la it nl b gy nq nr l ns nt">#Filtering data with Indus=7.07<br/>df.filter(df.INDUS==7.07).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/42dd2491012a156ea56b80fe7b505c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WqNhrteBjpicss6DfPBLtQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">过滤器1(来源:作者)</p></figure><p id="3c4f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">类似地，我们可以在一行代码中使用多个过滤器。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="d008" class="np la it nl b gy nq nr l ns nt">df.filter((df.INDUS==7.07) &amp; (df.MEDV=='High')).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/8b72f4b0a900630754184a5c78db18e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xclJZGJJCv_aT3qhhUAKlg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">过滤器2(来源:作者)</p></figure><p id="63f8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 4。分组和排序<br/> </strong> PySpark内置函数可以根据用户需求对数据进行分组，也可以根据需要对数据进行排序。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="58e9" class="np la it nl b gy nq nr l ns nt">df.groupBy('MEDV').count().show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/da7f1c9d9816c8183f0b4d3277d71d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/format:webp/1*kUZ0BtEh8cwfFLOvXckTPA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GroupBy(来源:作者)</p></figure><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="66a0" class="np la it nl b gy nq nr l ns nt">df.sort((df.TAX).desc()).show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/73866d62ccbada0f3801cf201157a386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IPwgDHCN2G3wlGmh39i1Mw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">排序(来源:作者)</p></figure><p id="27f2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 5。select&amp;Distinct<br/></strong>select函数用于选择不同的列，Distinct函数可用于选择该列的不同值。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="0188" class="np la it nl b gy nq nr l ns nt">df.select('MEDV').distinct().count()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/5dace55e1481c264b08eb7a9ae7415e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:56/format:webp/1*qQ9KAla2_0_EXCA_z1y6Dg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">选择和区分(来源:作者)</p></figure><p id="1957" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 6。with column<br/>T5【with column】函数用于通过为新列提供一定的条件并定义新列的名称来创建新列。</strong></p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="231d" class="np la it nl b gy nq nr l ns nt">#Creating New column with values from Age column divided by 2<br/>df.withColumn('HALF_AGE', df.AGE/2.0). select('AGE','HALF_AGE') .show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/aa4782ee1b6e1b949862c2e7f83046b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*OZA9wV-gfILYs-SgzzFAQQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">WithColumn(来源:作者)</p></figure><p id="e516" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本文中，我们介绍了PySpark下定义的一些主要函数，我们可以使用这些函数进行探索性的数据分析，并理解我们正在处理的数据。</p><p id="b6ae" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">继续，用不同的数据集尝试这些函数，如果您遇到任何问题，请在响应部分告诉我。</p><h1 id="5492" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">在你走之前</h1><p id="9317" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu"> <em class="oo">感谢</em> </strong> <em class="oo">的阅读！如果您想与我取得联系，请随时通过hmix13@gmail.com或我的</em> <a class="ae ky" href="http://www.linkedin.com/in/himanshusharmads" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> <em class="oo"> LinkedIn个人资料</em> </strong> </a> <em class="oo">联系我。可以查看我的</em><a class="ae ky" href="https://github.com/hmix13" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="oo">Github</em></strong><em class="oo"/></a><em class="oo">简介针对不同的数据科学项目和包教程。还有，随意探索</em> <a class="ae ky" href="https://medium.com/@hmix13" rel="noopener"> <strong class="lt iu"> <em class="oo">我的简介</em> </strong> </a> <em class="oo">，阅读我写过的与数据科学相关的不同文章。</em></p></div></div>    
</body>
</html>