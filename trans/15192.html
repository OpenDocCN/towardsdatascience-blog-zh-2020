<html>
<head>
<title>Sex Matters And Why I Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">性很重要，为什么我知道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sex-matters-and-why-i-know-712578a6857d?source=collection_archive---------22-----------------------#2020-10-19">https://towardsdatascience.com/sex-matters-and-why-i-know-712578a6857d?source=collection_archive---------22-----------------------#2020-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c578" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Captum，综合梯度和PyTorch闪电的神经网络的可解释性。看着黑盒！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/321ea5f57f1c3e6856aeec179b1548f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zjn7IrTzbWB-R7f9"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">看看黑匣子，由大卫·拉古萨拍摄。</p></figure><h1 id="d703" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="e976" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">神经网络已经席卷了全世界。每周都有关于GPT-3如何自动完成另一项语言任务的好消息。或者在当前的疫情中人工智能是如何帮助医生的。</p><p id="0128" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">但是越来越多的人对人工智能的转变越来越怀疑。它选择这种治疗方法是因为它是正确的治疗方法吗？还是因为它恰好是训练集中另一个5.29岁患者的最佳选择？</p><p id="562f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">整个群体都在警告我们人工智能中糟糕设计的含义，我们应该更好地倾听。毕竟，种族和性别相关的特征已经以歧视的方式在最近的招聘流程算法中使用了！</p><p id="fd4c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为一个人工智能社区，我们如何，</p><ol class=""><li id="3a99" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">确保我们建立包容性的工具。</li><li id="c7fc" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">确保我们的模特做我们认为他们在做的事情。</li><li id="5514" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">向他人解释我们的模型在做什么。</li></ol><p id="3e54" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">1、2、3的答案是明确的可解释性！它在深度学习领域掀起了一阵风暴。让我们今天了解一下它是什么，以及如何轻松地将其集成到我们的流程中。</p><p id="6204" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将建立一个神经网络来预测泰坦尼克号乘客的生存机会。最后，了解我们做了什么，并使用Captum解释我们的模型。全部使用强大的闪电框架。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl ng"><img src="../Images/6bd07d9982af56431d65d46563a6fca2.png" data-original-src="https://miro.medium.com/v2/1*zhlZ8xAM7jirh0Abl6yhcQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Gif总结解释了我们神经网络的学习过程。作者图片</p></figure><h1 id="0d66" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">0.进口</h1><p id="83b3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将从导入依赖项开始。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="3fe4" class="nm la it ni b gy nn no l np nq"># Our ML things<br/>import pytorch_lightning as pl<br/>import torch<br/>from torch.utils.data import DataLoader, Dataset<br/><br/>from captum.attr import IntegratedGradients<br/>from pytorch_lightning import seed_everything<br/>from pytorch_lightning import Trainer</span><span id="99ef" class="nm la it ni b gy nr no l np nq"># Visualization<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="deac" class="nm la it ni b gy nr no l np nq"># Utils<br/>from enum import Enum<br/>import pandas as pd<br/>from sklearn.metrics import classification_report</span></pre><h1 id="ab0c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">1.资料组</h1><p id="3524" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了制作这个最小的例子，我们将使用Titanic数据集(一个小文件)。可以在这里下载<a class="ae ky" href="https://www.kaggle.com/c/titanic/data?select=train.csv" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a><strong class="lt iu">。下载完成后，将其放入一个新文件夹“data/train.csv ..”这个数据集可能是有史以来最受欢迎的Kaggle数据集，因此允许我们验证我们的结果。</strong></p><p id="b801" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它包含了泰坦尼克号的890名不同的乘客。对于每位乘客，我们将预测他们是否幸存！</p><p id="23cf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为此，我们被赋予了几个特性。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="3efb" class="nm la it ni b gy nn no l np nq">PassengerId  <strong class="ni iu">Survived</strong>  Pclass  ...     Fare Cabin  Embarked<br/>0              1         0     ...   7.2500   NaN         S</span></pre><p id="6025" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将只处理其中的一小部分，即:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="698c" class="nm la it ni b gy nn no l np nq"><strong class="ni iu">Survived</strong>  Pclass     Sex   Age  SibSp     Fare<br/>0           0       3    male  22.0      1   7.2500</span></pre><p id="0e27" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在我们有了一个概述，让我们定义数据集。数据集是PyTorch中的一个类。它需要定义3个私有函数，init()、len()、getitem()。</p><p id="b770" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你对此不熟悉，可以在这里  <strong class="lt iu"> ( </strong> 1)找到一个简单的解释<a class="ae ky" rel="noopener" target="_blank" href="/pytorch-lightning-machine-learning-zero-to-hero-in-75-lines-of-code-7892f3ba83c0"> <strong class="lt iu">。数据集)。这里的目标是做一个最小的预处理，以便稍后展示我们的解释。</strong></a></p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="44ab" class="nm la it ni b gy nn no l np nq"><strong class="ni iu"># Simple enum that helps organizing<br/># Not really important</strong><br/>class TrainType(Enum):<br/>    train = 1<br/>    test = 2</span><span id="2966" class="nm la it ni b gy nr no l np nq"><strong class="ni iu"># Our Custom Dataset class<br/></strong>class RMSTitanic(Dataset):<br/>    def __init__(self, type: TrainType):<br/>        file = "data/train.csv"<br/><strong class="ni iu">        # Preprocessing</strong><br/>        df = pd.read_csv(file)<br/><strong class="ni iu">        # Select relevant fields</strong><br/>        df = df[['Survived', 'Pclass',<br/>                 "Sex", "Age", "SibSp",<br/>                 "Fare"]]<br/><strong class="ni iu">        # Convert Sex -&gt; 0/1</strong><br/>        m = {'male': 1, 'female': 0}<br/>        df['Sex'] = df['Sex'].str.lower().map(m)<br/><strong class="ni iu">        # Fix the non available vals<br/>        # Also normalize</strong><br/>        df = df.fillna(df.median())<br/>        df -= df.min()<br/>        df /= df.max()<br/><strong class="ni iu">        # The first 80% are Train</strong><br/>        if type == TrainType.train:<br/>            self.df = df.loc[:int(0.8 * len(df))]<br/>        if type == TrainType.test:<br/>            self.df = df.loc[int(0.8 * len(df)):]<br/><strong class="ni iu">        # We will use this later for interpretation</strong><br/>        self.base = torch.tensor(df[<br/>                                     ['Pclass', "Sex",<br/>                                      "Age", "SibSp",<br/>                                      "Fare"]<br/>                                 ].mean()).cuda().float()<br/><br/>    def __len__(self):<br/>        return len(self.df)<br/><br/>    def __getitem__(self, item):<br/><strong class="ni iu">        # This function return the i-th elem<br/></strong>        row = self.df.iloc[item]<br/>        label = row['Survived']<br/>        features = row[['Pclass', "Sex",<br/>                        "Age", "SibSp",<br/>                        "Fare"]]<br/><strong class="ni iu">        # return the (label,features)<br/></strong>        return (<br/>            torch.tensor(features).cuda().float(),<br/>            torch.tensor(label).cuda().float()<br/>        )</span></pre><p id="157e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这已经是我们最小的数据集了。请记住，这只是一个小的助手类。它帮助Pytorch Lightning施展它的魔法，因此我们可以节省更多的代码。</p><h2 id="787c" class="nm la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">简短解释</h2><p id="11e9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们得到的特征和它们的解释是什么。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="ba43" class="nm la it ni b gy nn no l np nq"><strong class="ni iu">Pclass</strong>   "What class is person traveling in? First being best"<br/><strong class="ni iu">Sex</strong>      "Male or Female"<br/><strong class="ni iu">Age</strong>      "How old in years?"<br/><strong class="ni iu">SibSp</strong>    "How many Sibiling/Spouses are there onboard"<br/><strong class="ni iu">Fare</strong>     "Amount in money spend"</span></pre><h1 id="43b5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">2.模型</h1><p id="9134" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们现在将构建一个简短但强大的模型。它将依次调用下面的层。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="43e3" class="nm la it ni b gy nn no l np nq">  | Name      | Type     | <br/>  ------------------------<br/>0 | input     | Linear   |   <br/>1 | r1        | PReLU    |  <br/>2 | l1        | Linear   |<br/>3 | r2        | PReLU    |    <br/>4 | out       | Linear   |   <br/>5 | sigmoid   | Sigmoid  |</span></pre><p id="d480" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们现在将定义一个名为MyHeartWillGoOn的PytorchModel。基本上只有前进功能是至关重要的。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="d436" class="nm la it ni b gy nn no l np nq">class MyHeartWillGoOn(pl.LightningModule):<br/><br/>    def __init__(self):<br/><strong class="ni iu">        # Setting up our model<br/></strong>        super().__init__()<br/><strong class="ni iu">        # way to fancy model<br/></strong>        self.lr = 0.01<br/>        self.batch_size = 512<br/>        l1 = 128<br/><br/><strong class="ni iu">        # We send our 5 features into first layer<br/></strong>        self.input = torch.nn.Linear(5, l1)<br/><strong class="ni iu">        # PRELU is just a fancy activation function<br/></strong>        self.r1 = torch.nn.PReLU()<br/><strong class="ni iu">        # More Layers<br/></strong>        self.l1 = torch.nn.Linear(l1, l1)<br/>        self.r2 = torch.nn.PReLU()<br/>        self.out = torch.nn.Linear(l1, 1)<br/><strong class="ni iu">        # Befor the Output use a sigmoid<br/></strong>        self.sigmoid = torch.nn.Sigmoid()<br/><strong class="ni iu">        # Define loss<br/></strong>        self.criterion = torch.nn.BCELoss()<br/><br/>    def forward(self, x):<br/><strong class="ni iu">        # Heart of our model<br/></strong>        x = self.input(x)<br/>        x = self.l1(x)<br/>        x = self.r1(x)<br/>        x = self.out(x)<br/>        x = self.r2(x)<br/>        x = self.sigmoid(x)<br/><br/>        return x<br/><br/>    def train_dataloader(self):<br/><strong class="ni iu">        # Load our Dataset: TRAIN<br/></strong>        return DataLoader(<br/>            RMSTitanic(type=TrainType.train),<br/>            batch_size=self.batch_size,<br/>            shuffle=True)<br/><br/>    def val_dataloader(self):<br/><strong class="ni iu">        # Load our Dataset: TEST<br/>        # Simplification: TEST=VAL<br/></strong>        return DataLoader(<br/>            RMSTitanic(type=TrainType.test),<br/>            batch_size=self.batch_size,<br/>            shuffle=False)<br/><br/>    def configure_optimizers(self):<br/>        return torch.optim.Adam(self.parameters(), lr=self.lr)<br/><br/>    def training_step(self, batch, batch_idx):<br/><strong class="ni iu">        # Here we just log some basics<br/>        # We can look at them later in tensorboard<br/></strong>        x, y = batch<br/>        y_hat = self(x)<br/>        y = torch.reshape(y, (y.size()[0], 1))<br/>        loss = self.criterion(y_hat, y)<br/>        tensorboard_logs = {'loss': loss}<br/>        return {'loss': loss, 'log': tensorboard_logs}</span></pre><h1 id="e7c4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">3.火车</h1><p id="7221" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">PyTorch Lightning的训练是由一个不错的小训练师完成的。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="a3c4" class="nm la it ni b gy nn no l np nq">if __name__ == '__main__':<br/><strong class="ni iu">    # Seed so we can reproduce<br/></strong>    seed_everything(404)<br/><strong class="ni iu">    # Load model to GPU<br/></strong>    device = torch.device("cuda")<br/>    model = MyHeartWillGoOn().to(device)<br/><strong class="ni iu">    # Make the Trainer do the work<br/></strong>    trainer = Trainer(max_epochs=20, min_epochs=1, auto_lr_find=True, progress_bar_refresh_rate=10)<br/>    trainer.fit(model)<br/><strong class="ni iu">    # Accuracy #ToBeMeasured<br/></strong>    ts = RMSTitanic(TrainType.test)<br/><strong class="ni iu">    # Bit of Hacking<br/></strong>    x = torch.stack(<br/>        [ts.__getitem__(i)[0] for i in range(ts.__len__())]<br/>    )<br/>    y = torch.stack(<br/>        [ts.__getitem__(i)[1] for i in range(ts.__len__())]<br/>    ).cpu().detach().numpy()<br/>    y_hat = model.forward(x).cpu().detach().numpy()<br/>    y_hat = (y_hat &gt; 0.5)<br/><strong class="ni iu">    # Accuracy and other metrics<br/></strong>    print("REPORT:\n",<br/>          classification_report(y, y_hat))</span></pre><p id="ff1a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在GPU上训练我们的模型20个历元将会得到以下结果。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="afc8" class="nm la it ni b gy nn no l np nq"><strong class="ni iu">SURVIVED</strong>   precision recall  f1-score   support</span><span id="971a" class="nm la it ni b gy nr no l np nq"><strong class="ni iu">FALSE</strong>      0.84      0.94      0.89       115<br/><strong class="ni iu">TRUE</strong>       0.86      0.67      0.75        64</span><span id="58de" class="nm la it ni b gy nr no l np nq"><strong class="ni iu">accuracy</strong>                               <strong class="ni iu">0.84 </strong>      179<br/>macro avg          0.85      0.81      0.82       179<br/>weighted avg       0.85      0.84      0.84       179</span></pre><p id="9f15" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">正如我们所看到的，我们得到了84%的准确率，这对于这个任务来说是可以接受的。这次我们更感兴趣的是解读。如果你想知道如何在这项任务中获得满分，可以看看<a class="ae ky" href="https://www.kaggle.com/c/titanic/notebooks" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>。</p><h1 id="712e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">4.解释</h1><p id="734c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">既然Pytorch闪电真的很新，而Captum更新；)我们在这方面相当领先。</p><p id="b785" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们需要解决一些尚未妥善解决的小问题。为了做到这一点，我们需要添加一个更小的包装函数，稍后呈现给Captum。基本上，Captum是一个有几种可用方法的库。这些方法预测输入特征对神经模型的重要性。他们以几种方式这样做。我们将使用可能是最流行的方法<strong class="lt iu">集成梯度</strong>。下面我就给出这种方法的一个直觉。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="29e3" class="nm la it ni b gy nn no l np nq"><strong class="ni iu"># Let's start with the interpretation<br/></strong>STEP_AMOUNT = 50<br/>SAMPLE_DIM = 5<br/><br/><br/><strong class="ni iu"># Since Captum is not directly<br/># made for Lightning,<br/># we need this wrapper</strong><br/>def modified_f(in_vec):<br/><strong class="ni iu">    # Changes the shapes correctly<br/>    # X:Shape([SAMPLE_DIM*STEP_AMOUNT]=250)</strong><br/>    x = torch.reshape(in_vec,<br/>        (int(in_vec.size()[0] / SAMPLE_DIM), SAMPLE_DIM)<br/>    )<br/><strong class="ni iu">    # RES:Shape([50,5])<br/>    # Since we have 50ti Batches and 5 features</strong><br/>    res = model.forward(x)<br/><strong class="ni iu">    # Again reshape to correct dims</strong><br/>    res = torch.reshape(res, (res.size()[0], 1))<br/>    return res<br/><br/><br/>ig = IntegratedGradients(modified_f)</span></pre><p id="b634" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在我们可以开始学习我们的模型是如何思考的。赋予此对象IntegratedGradients。我们可以使用它的main方法属性()！</p><blockquote class="od oe of"><p id="6d72" class="lr ls og lt b lu mn ju lw lx mo jx lz oh mp mc md oi mq mg mh oj mr mk ml mm im bi translated">attribute()是一个函数，它将一个张量作为输入，并返回给我们一个相同形状的张量</p></blockquote><p id="c65c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这意味着当我们告诉我们的模型，我们的案例研究对象:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="4c62" class="nm la it ni b gy nn no l np nq"><strong class="ni iu">Pclass  Sex   Age  SibSp     Fare</strong><br/>3       male  22.0      1       7.2500 (I am not normalized)</span></pre><p id="2703" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它将使用这5个特征进行预测。假设这个22岁的男子支付了7.25英镑的车费，那么这个数字就是0.3%，也就是0.3%的生存机会。预测后我们会发这个30%存活率-&gt;0 %，意思是他死了。</p><p id="f16e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在积分梯度返回给我们一个张量，也有5个值。这些值中的每一个都会告诉我们各自的特性有多重要。这可能是y. E .的差异</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="2790" class="nm la it ni b gy nn no l np nq"><strong class="ni iu">Pclass  Sex   Age  SibSp     Fare</strong><br/>0.1619, -0.1594,  0.0196, -0.0024,  0.0068</span></pre><h1 id="02cf" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">5.综合梯度</h1><p id="ea07" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">中间上了一堂理论课。集成梯度已经在<a class="ae ky" href="https://arxiv.org/abs/1703.01365" rel="noopener ugc nofollow" target="_blank">中提出，用于深度网络的公理化属性</a>综合梯度用于理解我们的神经网络是如何工作的。集成梯度是所谓的可解释性算法。</p><p id="1dc4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">今天的人工智能社区中使用了几种可解释性算法，但IG是最早和最成熟的算法之一。科学界还没有明确决定哪种算法是最好的。随意测试一些其他的，比如<a class="ae ky" href="https://captum.ai/api/attribution.html" rel="noopener ugc nofollow" target="_blank"> DeepLift </a>。</p><h2 id="61e8" class="nm la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">综合梯度—简而言之</h2><blockquote class="od oe of"><p id="ec74" class="lr ls og lt b lu mn ju lw lx mo jx lz oh mp mc md oi mq mg mh oj mr mk ml mm im bi translated">这是一个简化的解释，更多细节请参考论文“<a class="ae ky" href="https://arxiv.org/abs/1703.01365" rel="noopener ugc nofollow" target="_blank">深度网络的公理化归属</a></p></blockquote><p id="5c98" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了以神经网络的方式优化损失函数，我们建立梯度。为了了解每个输入要素对我们的损失函数的贡献大小，我们颠倒了这个过程(称为积分)，因此我们对梯度进行积分。</p><p id="a312" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，计算积分很难，答案也不清楚。这就是为什么我们必须近似它。我们用黎曼和来近似这个积分。</p><p id="6b43" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们通过使用超参数n_steps和基线来近似该值。基线是我们与输入进行比较的对象(我们将在下面详细讨论)。n_steps是我们反向传播以估计积分的次数。N_steps基本上控制了我们对积分的估计有多精确。</p><h1 id="f160" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">6.资本基础</h1><p id="32a9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们现在有三样东西:</p><ol class=""><li id="3d2f" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">受过训练的模特</li><li id="5c3f" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">测试设备</li><li id="c4d5" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">综合梯度</li></ol><p id="9cc6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用这些东西，我们将了解我们的模型</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="6846" class="nm la it ni b gy nn no l np nq"><strong class="ni iu"># Test to understand the basics</strong><br/><strong class="ni iu"># First get the 6th test example</strong><br/>val = ts.__getitem__(6)[0]<br/>print("IN X1:", val)<br/><strong class="ni iu"># Predict the importance of the features<br/># for the male example</strong><br/>imp_m = ig.attribute(inputs=val,<br/>                     baselines=ts.base,<br/>                     n_steps=STEP_AMOUNT)<br/>print("IMPORTANCE X_m:", imp_m)<br/>print("Probability SURVIVAl X_m:", modified_f(val))<br/><strong class="ni iu"># Predict the importance of the features<br/># for the female example</strong><br/>print("Let's Change the gender -&gt;X2")<br/>val_f = val<br/>val_f[1] = 0<br/>imp_f = ig.attribute(inputs=val_f,<br/>                     baselines=ts.base,<br/>                     n_steps=STEP_AMOUNT)<br/>print("IMPORTANCE X_f:", imp_f)<br/>print("Probability SURVIVAl X_f:", modified_f(val_f))</span></pre><p id="b051" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该代码将打印:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="794c" class="nm la it ni b gy nn no l np nq">IN X1: [1.0000, 1.0000, 0.3466, 0.0000, 0.0303]<br/>IMPORTANCE X_m:[-0.0593, -0.1121, -0.0010,  0.0019, -0.0040]<br/>Probability SURVIVAl X_m:0.1265</span><span id="cb13" class="nm la it ni b gy nr no l np nq">IMPORTANCE X_f:[-0.1178,  0.4049, -0.0020,  0.0037, -0.0080]<br/>Probability SURVIVAl X_f:0.5817</span></pre><p id="5c9c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如你所见，当我们给出Captum，这两个相同的例子。除了性别。我们得到了截然不同的存活率。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="8f27" class="nm la it ni b gy nn no l np nq"><strong class="ni iu"># MALE<br/></strong>Pclass  Sex   Age  SibSp     Fare<br/>3       male  22.0      1       7.2500 (I am not normalized)<br/><strong class="ni iu"># FEMALE<br/></strong>Pclass  Sex     Age  SibSp     Fare<br/>3       female  22.0      1       7.2500 (I am not normalized)</span></pre><p id="7189" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">男性的存活率是12%,女性是58%!我们得到的两个重要值是:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="6982" class="nm la it ni b gy nn no l np nq">                  Pclass  Sex         Age    SibSp     Fare  <br/>IMPORTANCE X_m:[-0.0593, <strong class="ni iu">-0.1121</strong>, -0.0010,  0.0019, -0.0040]<br/>IMPORTANCE X_f:[-0.1178,  <strong class="ni iu">0.4049</strong>, -0.0020,  0.0037, -0.0080]</span></pre><p id="afbc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">正如我们在这两种情况下所看到的，在特征<strong class="lt iu">性</strong>中重要性是最强的(就绝对而言)。这表明这是最重要的功能！</p><blockquote class="od oe of"><p id="d1ac" class="lr ls og lt b lu mn ju lw lx mo jx lz oh mp mc md oi mq mg mh oj mr mk ml mm im bi translated">解读符号:我们可以把它看作是增加到预测(+)或从预测(-)中扣除的东西。即+使其更可能是(1)或更不可能是(1)。</p></blockquote><p id="7787" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以看到，根据性别(男性\女性)的不同，产生的向量也大不相同。</p><h1 id="ccc7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">7.基线的影响</h1><p id="4b1e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">基线是我们用来比较自己价值的东西。为了理解它的影响，我们将比较最明显的选择。在最初的论文中，使用了噪声和黑色图像(零)的概念。</p><p id="6184" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将比较4个不同的基线。</p><blockquote class="od oe of"><p id="c187" class="lr ls og lt b lu mn ju lw lx mo jx lz oh mp mc md oi mq mg mh oj mr mk ml mm im bi translated"><strong class="lt iu">对比:</strong></p><p id="38d1" class="lr ls og lt b lu mn ju lw lx mo jx lz oh mp mc md oi mq mg mh oj mr mk ml mm im bi translated">1 .平均值！</p><p id="d65a" class="lr ls og lt b lu mn ju lw lx mo jx lz oh mp mc md oi mq mg mh oj mr mk ml mm im bi translated">2.噪声，与随机相比</p><p id="54ac" class="lr ls og lt b lu mn ju lw lx mo jx lz oh mp mc md oi mq mg mh oj mr mk ml mm im bi translated">3.全是1</p><p id="519d" class="lr ls og lt b lu mn ju lw lx mo jx lz oh mp mc md oi mq mg mh oj mr mk ml mm im bi translated">4.全是零</p></blockquote><p id="3973" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，代码将产生期望的解释。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="8fd9" class="nm la it ni b gy nn no l np nq"><strong class="ni iu"># define a collection<br/></strong>to_be_df = []<br/><strong class="ni iu"># Compare each element of the test set to out baselines<br/># we will than use this</strong><br/>for i in range(0,<br/>               1):  # ts.__len__()):<br/><strong class="ni iu">    # load our test example</strong><br/>    in_val = ts.__getitem__(i)[0]<br/><strong class="ni iu">    # compare it to the 4 baselines<br/></strong>    att_b = ig.attribute(<br/>        inputs=in_val,<br/>        baselines=ts.base,<br/>        n_steps=STEP_AMOUNT).detach().cpu().numpy()<br/>    att_r = ig.attribute(<br/>        inputs=in_val,<br/>        baselines=torch.rand(<br/>            5).cuda(),<br/>        n_steps=STEP_AMOUNT).detach().cpu().numpy()<br/>    att_z = ig.attribute(<br/>        inputs=in_val,<br/>        baselines=torch.zeros(<br/>            5).cuda(),<br/>        n_steps=STEP_AMOUNT).detach().cpu().numpy()<br/>    att_1 = ig.attribute(<br/>        inputs=in_val,<br/>        baselines=torch.ones(<br/>            5).cuda(),<br/>        n_steps=STEP_AMOUNT).detach().cpu().numpy()<br/><strong class="ni iu">    # save result, this will produce a df<br/>    # you can skip the details</strong><br/>    for base_type, vals in [<br/>        ("mean-base", att_b),<br/>        ("random-base", att_r),<br/>        ("zero-base", att_z),<br/>        ('one-base', att_1),<br/>    ]:<br/>        for i, name in enumerate(['Pclass',<br/>                                  "Sex",<br/>                                  "Age",<br/>                                  "SibSp",<br/>                                  "Fare"]):<br/>            to_be_df.append({<br/>                "base-type": base_type,<br/>                "feature": name,<br/>                "value": vals[i],<br/>            })<br/><strong class="ni iu"># Convert our data to a pandas<br/></strong>df = pd.DataFrame(to_be_df)<br/>df.to_csv('data/interpretation_results.csv')<br/>print("OUR INTERPRETATION:\n\n",<br/>      df)</span></pre><p id="2521" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们的结果看起来像这样。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="8529" class="nm la it ni b gy nn no l np nq"><strong class="ni iu">      base-type feature     value<br/></strong>0     mean-base  Pclass  0.162004<br/>1     mean-base     Sex -0.159421<br/>2     mean-base     Age  0.019652<br/>3     mean-base   SibSp -0.002433<br/>4     mean-base    Fare  0.006821<br/>5   random-base  Pclass  0.117517</span></pre><p id="7b42" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在我们找到了我们要找的东西！</p><h1 id="1c55" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">8.形象化</h1><p id="9f6b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一旦我们完成了工作，我们就可以坐下来享受它了！</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="c4e2" class="nm la it ni b gy nn no l np nq"><strong class="ni iu"># Aggregate and Visualize<br/># Load Data</strong><br/>df = pd.read_csv('data/interpretation_results.csv')<br/><strong class="ni iu"># Defined the color map for our heatmap to be red to green</strong><br/>cmap = sns.diverging_palette(h_neg=10, h_pos=130, s=99,<br/>                             l=55, sep=3, as_cmap=True)<br/><strong class="ni iu"># Aggregate the CSV by mean</strong><br/>df = df.groupby(["base-type", 'feature', 'epoch'],<br/>                as_index=False).mean()<br/>df = df[df['epoch'] == max_epoch]<br/><strong class="ni iu"># Make one plot per baseline to compare</strong><br/>for b in ["mean-base",<br/>          "random-base",<br/>          "zero-base",<br/>          'one-base']:<br/><strong class="ni iu">    # Let's plot them isolated</strong><br/>    tmp = df[df['base-type'] == b]<br/><strong class="ni iu">    # Create a pivot frame</strong><br/>    tmp = tmp.pivot(index='base-type', columns='feature',<br/>                    values='value')<br/>    print("We will plot:\n",tmp)</span></pre><p id="a894" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">枢纽数据框基本上只是对相同的数据采用不同的视图。这里我们告诉它拥有索引(基本类型上的行)和“特性”上的列这看起来像这样。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="66fa" class="nm la it ni b gy nn no l np nq"><strong class="ni iu">feature         Age      Fare    Pclass       Sex     SibSp</strong><br/><strong class="ni iu">mean-base</strong>  0.023202  0.004757  0.116368  0.225421  0.022177</span></pre><p id="6b30" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在剩下要做的就是用</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="9b51" class="nm la it ni b gy nn no l np nq"><strong class="ni iu"># Create a pivot frame</strong><br/> tmp = tmp.pivot(index='base-type', columns='feature',<br/>                    values='value')<br/><strong class="ni iu"># Some code to make a heatmap using seaborn<br/></strong>fig, ax = plt.subplots()<br/>fig.set_size_inches(10, 2.5)<br/><br/>plt.title("Feature Importance ", fontsize=15)<br/>sns.heatmap(tmp, ax=ax, cmap=cmap, annot=True)<br/>plt.text(0, 0,<br/>         'By Sandro Luck\nTwitter:@san_sluck',<br/>         horizontalalignment='center',<br/>         verticalalignment='bottom',<br/>         fontsize=10)<br/><br/>plt.savefig(f'data/{b}.png')</span></pre><h2 id="c47b" class="nm la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated"><code class="fe ok ol om ni b">Mean</code>-基地</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/978af0fd948488540bd508ae4b93452c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uTY3DO1hbkwCu6fXVMQITw.png"/></div></div></figure><p id="cfa4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">太好了，现在我希望最后一个怀疑者也相信性是伟大的！我们可以清楚地看到，特征性别的影响是显著的。当我们查看数据集时，我们可以注意到，作为一个男人，你的死亡几率会大大增加。毕竟，“妇女和儿童优先”是他们对待船只的方式。</p><h2 id="7ede" class="nm la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">一垒对零垒</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ea3b19d490ea4b630d872f58b1423b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M0Ug_IZ49GPTHPB_RfxOQg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一垒</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/168a49c0680c297b3c323a18c65fcc4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UWCPG7_Po9Ww2y9YKkVwzg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">零基数</p></figure><p id="55a8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里我们看到基线之间有很大的差异。如果我们将这两个基线与平均基线进行比较，我们会注意到SibSp的重要性发生了巨大变化。这通常应该说明使用基线(如1和0)是不可取的。这样做的原因是我们要计算预测在基线和值之间的变化量。</p><blockquote class="od oe of"><p id="e132" class="lr ls og lt b lu mn ju lw lx mo jx lz oh mp mc md oi mq mg mh oj mr mk ml mm im bi translated">例如，对于图像，使用0基线可能是有意义的。原因是，在这种情况下，黑色方块代表没有信息。</p></blockquote><h2 id="2ab2" class="nm la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated"><strong class="ak">随机基数</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c9b2e2134daee2253b4fee1308bf5a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMXnCbkZ-J76_E847zMzog.png"/></div></div></figure><p id="4b3a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对我来说，这一天的惊喜来自于随机基线的成功。综合梯度的作者提到使用随机基线是明智的。但是我没有预料到它在这个例子中工作得如此之好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl ng"><img src="../Images/6bd07d9982af56431d65d46563a6fca2.png" data-original-src="https://miro.medium.com/v2/1*zhlZ8xAM7jirh0Abl6yhcQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">显示训练时间重要性的GIF</p></figure><p id="5b70" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在某些平台上让人们关注你的数据时，gif是至关重要的。尤其是在社交媒体上，动画可以决定点击和快速浏览的区别。</p><p id="52d8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于这篇文章已经相当长了，请参考我的文章《<a class="ae ky" href="https://medium.com/python-in-plain-english/how-to-make-gifs-with-python-and-seaborn-from-google-trends-data-fd3fe16cba11" rel="noopener">如何用Python和Seaborn从Google Trends数据制作GIFS</a>》以闪电般的速度学习制作上面的GIF。</p><h1 id="19fa" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="b998" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们已经了解了什么是可解释性，以及如何在PyTorch中使用它。我们已经看到在原型制作中使用它是多么容易，以及如何将其连接到PyTorch Lightning。</p><p id="59a8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我希望这种对可解释性世界的介绍是容易的，没有痛苦的。了解您正在构建什么可以帮助我们的客户和我们。这大大增加了他们对我们预测的信心。</p><p id="3b77" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你喜欢这篇文章，我会很高兴在<a class="ae ky" href="https://twitter.com/san_sluck" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或<a class="ae ky" href="https://www.linkedin.com/in/sandro-luck-b9293a181/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系你。</p><p id="9d07" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一定要看看我的<a class="ae ky" href="https://www.youtube.com/channel/UCHD5o0P16usdF00-ZQVcFog?view_as=subscriber" rel="noopener ugc nofollow" target="_blank"> YouTube </a>频道，我每周都会在那里发布新视频。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="2934" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">整个代码</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oo l"/></div></figure></div></div>    
</body>
</html>