<html>
<head>
<title>JAX: Differentiable Computing by Google</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">JAX:谷歌的差异化计算</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/jax-differentiable-computing-by-google-78310859b4ad?source=collection_archive---------22-----------------------#2020-10-28">https://towardsdatascience.com/jax-differentiable-computing-by-google-78310859b4ad?source=collection_archive---------22-----------------------#2020-10-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c72d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">革命性的机器学习框架</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c6db06eda2bea13289bfcbe5c599ef03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u6vQ-u4azCeilkwS-5QxoQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=969757" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的<a class="ae ky" href="https://pixabay.com/users/thedigitalartist-202249/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=969757&quot;" rel="noopener ugc nofollow" target="_blank">皮特·林福思</a>拍摄</p></figure><p id="1b89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自从深度学习在2010年代初起飞以来，许多框架被编写来促进研究和生产中的深度学习。为了记录在案，让我们提一下<a class="ae ky" href="https://caffe.berkeleyvision.org/" rel="noopener ugc nofollow" target="_blank"> Caffe </a>、<a class="ae ky" href="http://deeplearning.net/software/theano/" rel="noopener ugc nofollow" target="_blank"> Theano </a>、<a class="ae ky" href="http://torch.ch/" rel="noopener ugc nofollow" target="_blank"> Torch </a>、<a class="ae ky" href="https://lasagne.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">千层面</a>、<a class="ae ky" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>、<a class="ae ky" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>或<a class="ae ky" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>。一段时间后，这些框架中的一些消失了。其他的幸存下来并发展到今天，主要是PyTorch和Tensorflow。随着时间的推移，这些框架演变成具有许多不同功能的大型生态系统。一方面，它们支持新模型的训练，包括在多GPU系统上的大规模并行训练。另一方面，它们允许用户在云中或移动设备上轻松部署训练好的模型。</p><p id="a8a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">不幸的是，这种精简有时会以机器学习研究人员测试新模型想法所需的灵活性为代价。谷歌通过其名为<a class="ae ky" href="https://github.com/google/jax" rel="noopener ugc nofollow" target="_blank"> JAX </a>的新框架瞄准的正是这个用户群体。在这篇博文中，我将向你展示JAX是如何通过向量化计算和梯度计算提供一个低级的、高性能的接口来鼓励实验的。</strong></p><h1 id="ff6d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">JAX的设计哲学</h1><p id="b666" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu">JAX的目标是允许用户通过即时编译和自动并行化来加速原始Python和NumPy函数，并计算这些函数的梯度</strong>。为了做到这一点，JAX采用了功能设计。<strong class="lb iu">梯度计算等函数被实现为作用于用户定义的Python函数的函数变换</strong>(或函子)。例如，要计算绝对值函数的梯度，您可以写:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="4555" class="mx lw it mt b gy my mz l na nb">from jax import grad</span><span id="dc4b" class="mx lw it mt b gy nc mz l na nb">def abs_val(x):<br/>  if x &gt; 0:<br/>    return x<br/>  else:<br/>    return -x<br/><br/>abs_val_grad = grad(abs_val)</span></pre><p id="b13b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，<em class="nd"> abs_val </em>是一个普通的Python函数，由仿函数<em class="nd"> grad </em>转化而来。</p><p id="4b72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了能够使用JAX函子，用户定义的函数必须遵循一些限制:</p><ol class=""><li id="2d36" class="ne nf it lb b lc ld lf lg li ng lm nh lq ni lu nj nk nl nm bi translated"><strong class="lb iu">JAX处理的每一个函数都要求是纯的。这意味着当用相同的输入调用时，它应该总是返回相同的结果。从功能上来说，<strong class="lb iu">可能没有任何副作用</strong>。否则，JAX无法保证使用实时编译或并行化时的正确性。功能纯度实际上并没有被强制。虽然有时可能会抛出错误，但确保这种情况主要是程序员的责任。</strong></li><li id="31fe" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">人们不得不使用<em class="nd"> jax.ops </em>包中的替代功能，而不是像<em class="nd"> x[i] += 1 </em>这样的就地突变更新。</li><li id="9ed8" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">实时(JIT)编译对Python控制流有一些限制。</li></ol><p id="7f6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就哲学而言，JAX类似于你在使用纯函数式语言(如Haskell)时会遇到的情况。事实上，开发人员甚至在文档中使用Haskell类型的签名来解释一些转换。</p><h1 id="ecf0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">即时编译</h1><p id="0aff" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu">JAX的主要特性之一是能够通过JIT加速Python代码的执行。在内部，JAX使用XLA编译器来完成这项工作。XLA不仅能为CPU编译代码，还能为<strong class="lb iu">GPU甚至是</strong>TPUs编译代码。这使得JAX非常强大和多才多艺。为了使用XLA编译一个函数，你可以像这样使用<em class="nd"> jit </em>仿函数:</strong></p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9f95" class="mx lw it mt b gy my mz l na nb">from jax import jit<br/>import jax.numpy as jnp</span><span id="31ee" class="mx lw it mt b gy nc mz l na nb"><strong class="mt iu">def</strong> selu(x, alpha=1.67, lmbda=1.05):<br/>  <strong class="mt iu">return</strong> lmbda * jnp.where(x &gt; 0, x, alpha * jnp.exp(x) - alpha)</span><span id="f4b0" class="mx lw it mt b gy nc mz l na nb">selu_jit = jit(selu)</span></pre><p id="8ce9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者，您也可以使用<em class="nd"> jit </em>作为函数定义顶部的装饰器:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="61c8" class="mx lw it mt b gy my mz l na nb"><strong class="mt iu">@jit<br/>def</strong> selu(x, alpha=1.67, lmbda=1.05):<br/>  <strong class="mt iu">return</strong> lmbda * jnp.where(x &gt; 0, x, alpha * jnp.exp(x) - alpha)</span></pre><p id="8b30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nd"> jit </em>仿函数<strong class="lb iu">通过将输入函数的编译版本返回给调用者来转换</strong>输入函数。调用原来的<em class="nd">卢瑟</em>函数会使用Python解释器，而调用<em class="nd">卢瑟_jit </em>会调用编译后的版本，应该会快很多，特别是对于numpy数组这样的矢量化输入。此外，<strong class="lb iu"> JIT编译只发生一次，并在其后被缓存</strong>，使得函数的后续调用非常高效。</p><h1 id="03a5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">使用vmap的自动矢量化</h1><p id="189f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在训练机器学习模型时，通常会计算输入数据子集的一些损失，然后更新模型参数。由于为每个输入顺序计算模型的正向函数会太慢，所以习惯上是将数据的子集一起分批到一个批次中，并在诸如GPU的加速器上以并行方式计算正向函数。这是在JAX通过使用函数<em class="nd"> vmap </em>完成的:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="6e3e" class="mx lw it mt b gy my mz l na nb">def forward(input, w, b):<br/>    input = jnp.dot(input, w) + b<br/>    return jnp.max(input, 0)</span><span id="6d8e" class="mx lw it mt b gy nc mz l na nb">jax.vmap(forward, in_axes=(0, None, None), out_axes=0)(inputs, w, b)</span></pre><p id="d177" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该代码片段采用一个具有ReLU激活的全连接层的转发功能，并在一批输入上并行执行该功能。参数<em class="nd">输入轴</em>和<em class="nd">输出轴</em>指定在哪些参数和轴上发生并行化。在这种情况下，(0，None，None)意味着输入在第0轴上并行化，而w和b保持不变。输出在0轴上并行化。</p><h1 id="9831" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">计算梯度</h1><p id="d4cb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让机器学习研究人员对JAX特别感兴趣的是它计算任意纯函数梯度的能力。JAX从autograd继承了这种能力，autograd是一个计算NumPy数组导数的包。</p><p id="45f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要计算函数的梯度，只需使用<em class="nd"> grad </em>变换:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="4818" class="mx lw it mt b gy my mz l na nb">import jax.numpy as jnp</span><span id="66a1" class="mx lw it mt b gy nc mz l na nb">grad(jnp.tanh))(2.0)</span><span id="4038" class="mx lw it mt b gy nc mz l na nb">[0.070650816]</span></pre><p id="b0bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想计算高阶导数，您可以简单地将多个<em class="nd"> grad </em>变换链接在一起，如下所示:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="6a49" class="mx lw it mt b gy my mz l na nb">import jax.numpy as jnp</span><span id="7bb1" class="mx lw it mt b gy nc mz l na nb">grad(grad(jnp.tanh))(2.0)</span><span id="31c4" class="mx lw it mt b gy nc mz l na nb">[-0.13621889]</span></pre><p id="a026" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然将<em class="nd"> grad </em>应用于R(实数集)中的函数会得到一个单一的数字作为输出，但您也可以将其应用于向量值函数以获得雅可比矩阵:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="8bf3" class="mx lw it mt b gy my mz l na nb">def f(x):<br/>    return jnp.asarray(<br/>        [x[0], 5*x[2], 4*x[1]**2 - 2*x[2], x[2] * jnp.sin(x[0])])</span><span id="e607" class="mx lw it mt b gy nc mz l na nb">print(jax.jacfwd(f)(jnp.array([1., 2., 3.])))</span><span id="4ec8" class="mx lw it mt b gy nc mz l na nb">[[ 1.       0.       0.     ]<br/> [ 0.       0.       5.     ]<br/> [ 0.      16.      -2.     ]<br/> [ 1.6209   0.       0.84147]]</span></pre><p id="1c2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们不使用<em class="nd"> grad </em>变换，因为它只对标量输出函数有效。相反，我们使用类似的函数jacfwd进行自动前向模式区分(关于前向和后向模式区分的详细讨论，请参考<a class="ae ky" href="https://jax.readthedocs.io/en/latest/jax.html?highlight=grad#jax.jacfwd" rel="noopener ugc nofollow" target="_blank"> JAX文档</a>)。</p><p id="d66e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意，我们可以在JAX将功能转换链接在一起。</strong>例如，我们可以使用<em class="nd"> vmap </em>对我们的函数进行矢量化，使用<em class="nd"> grad </em>计算梯度，然后使用<em class="nd"> jit </em>编译结果函数，例如深度学习中的标准小批量训练循环:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="b08c" class="mx lw it mt b gy my mz l na nb">def <strong class="mt iu">loss</strong>(x, y):<br/>    out = net(x)<br/>    cross_entropy = -y * np.log(out) - (1 - y)*np.log(1 - out)<br/>    return cross_entropy</span><span id="ca92" class="mx lw it mt b gy nc mz l na nb">loss_grad = jax.jit(jax.vmap(jax.grad(loss), in_axes=(0, 0), out_axes=0))</span></pre><p id="5aa6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，<em class="nd"> loss_grad </em>计算交叉熵损失的梯度，同时在输入(x，y)和单个输出(损失)上并行。整个函数是jitted的，以允许在GPU上快速计算。产生的函数被缓存，允许用不同的输出调用它，而没有任何额外的开销。</p><p id="8d66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多种转换的组合使得框架非常强大，并为用户设计数据流提供了极大的灵活性。</p><h1 id="571c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="a0a0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">对于需要额外灵活性的研究人员来说，JAX为PyTorch或Tensorflow等更高级的框架提供了一个有用的替代方案。通过原生Python和NumPy函数进行区分的能力令人惊叹，JIT编译和自动矢量化功能极大地简化了为GPU或TPUs等大规模并行架构编写高效代码的工作。然而，我最喜欢JAX的是它干净的功能界面。一个成熟的高级API生态系统将围绕它发展，这肯定只是时间问题。敬请期待！</p></div></div>    
</body>
</html>