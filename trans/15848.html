<html>
<head>
<title>Probabilistic Linear Discriminant Analysis (PLDA) Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">概率线性判别分析(PLDA)解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/probabilistic-linear-discriminant-analysis-plda-explained-253b5effb96?source=collection_archive---------6-----------------------#2020-11-01">https://towardsdatascience.com/probabilistic-linear-discriminant-analysis-plda-explained-253b5effb96?source=collection_archive---------6-----------------------#2020-11-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="858d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/e0df2c6b7f3be96e26cc6b48228f7f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pT2JNNyl3krHVtyp-GgmwQ.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">来源:图片由作者提供。具有不同平均值(中心)的高斯分布(椭圆)的等高线用不同的颜色表示。每个轮廓对应于不同类别。</p></figure><p id="54de" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ja">以<strong class="kn ja">简化</strong>的方式讲解<strong class="kn ja">概率线性判别分析(PLDA) </strong>的</strong>概念和应用。</p><h1 id="dd3d" class="lj lk iq bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">介绍</h1><p id="0809" class="pw-post-body-paragraph kl km iq kn b ko mh kq kr ks mi ku kv kw mj ky kz la mk lc ld le ml lg lh li ij bi translated">顾名思义，<strong class="kn ja">概率线性判别分析</strong>是<strong class="kn ja">线性判别分析</strong>的<strong class="kn ja">概率</strong>版本，能够处理更复杂的数据。尽管PLDA在包括计算机视觉、语音处理、自然语言处理(NLP)在内的许多研究领域有着广泛的应用，但它仍然没有以一种能够达到广泛受众的方式进行解释。PLDA已被用于识别、验证、生成用于聚类的相似性分数、特定于类别的特征提取。</p><blockquote class="mm mn mo"><p id="ab29" class="kl km mp kn b ko kp kq kr ks kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh li ij bi translated">我这里的<strong class="kn ja">目的</strong>是<strong class="kn ja">讨论<strong class="kn ja">介绍和应用了PLDA </strong>的研究论文</strong>。我将使用等式、图表和易于实现的代码来解释这些概念，以便所有数据科学领域的人都可以理解。</p><p id="7135" class="kl km mp kn b ko kp kq kr ks kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh li ij bi translated">这里介绍的推导需要有<strong class="kn ja">基本概率</strong>和<strong class="kn ja">线性代数</strong>的先验知识。可以参考参考文献中提到的来源。</p></blockquote><p id="da5d" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为什么是PLDA？</p><p id="850e" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在我继续之前，我想给出使用PLDA而不是LDA的动机。</p><p id="87c9" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ja"> LDA </strong>是一种监督降维技术。LDA将数据投影到较低维度的子空间，使得在投影的子空间中，与每个类内的分布(最小化<strong class="kn ja">类内协方差Sw </strong>)相比，属于不同类的点更加分散(最大化<strong class="kn ja">类间协方差Sb </strong>)。下图展示了这一点。</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mt"><img src="../Images/f455340455290cb269b37705b0ddc94a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9bGZLX7K_9oZ_c5e-PM4yw.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">来源:毕晓普著<em class="my"/><a class="ae mz" href="https://cds.cern.ch/record/998831" rel="noopener ugc nofollow" target="_blank"><em class="my">模式识别与机器学习</em> </a> <em class="my">。每种颜色代表一个阶层。m1和m2分别是类别1和类别2平均值。左图显示了m1和m2连线上的数据投影。两个类别的样本有很多重叠。右图显示了使用LDA的数据投影，从而最大限度地减少了两个类别之间的重叠</em></p></figure><p id="94d4" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">当我们有来自所见类的数据时，这对于分类很有用。但是，当观察到的数据来自看不见的类时，我们如何执行类似的任务呢？例如，考虑人脸识别的任务，我们使用不同的人脸图像训练模型，使得每个唯一的人脸代表一个类别。</p><p id="c26b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在给定两个图像，我们想要找出它们是否属于同一个人，即使模型以前没有见过那个人的任何图像。常见的方法是将两幅图像投影到低维空间，并找出它们之间的距离。如果距离很小，这意味着他们来自同一个班级。LDA会将图像投影到从训练数据获得的子空间中，因此不是最佳的。因此，我们需要一个更灵活的模型来寻找投影的最佳方向。解决这个问题的一种方法是使用概率方法，而不像LDA是确定性的。它被称为概率LDA。</p><p id="8376" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ja">PLDA的优势</strong></p><ul class=""><li id="e9dc" class="na nb iq kn b ko kp ks kt kw nc la nd le ne li nf ng nh ni bi translated">我们可以使用连续的非线性函数来生成类中心，即使是从看不见的类的单个例子中。</li><li id="3063" class="na nb iq kn b ko nj ks nk kw nl la nm le nn li nf ng nh ni bi translated">在假设检验中，我们可以比较以前从未见过的类中的两个例子，以确定它们是否属于同一类。</li><li id="6d94" class="na nb iq kn b ko nj ks nk kw nl la nm le nn li nf ng nh ni bi translated">对来自未知类别的样本执行聚类</li></ul></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="6ee5" class="lj lk iq bd ll lm nv lo lp lq nw ls lt lu nx lw lx ly ny ma mb mc nz me mf mg bi translated"><strong class="ak">什么是概率LDA？</strong></h1><p id="e66e" class="pw-post-body-paragraph kl km iq kn b ko mh kq kr ks mi ku kv kw mj ky kz la mk lc ld le ml lg lh li ij bi translated">设<em class="mp"> x={x₁,x₂,…,xₙ} </em>为<em class="mp"> D </em>维观测值或数据样本。<strong class="kn ja">概率LDA或PLDA </strong>是一个<strong class="kn ja">生成模型</strong>，它假设给定的数据样本是从分布中生成的。我们需要找到最能描述训练数据的模型参数。假设产生数据的分布的选择基于两个因素:(1)它应该代表不同类型的数据(2)参数的计算简单而快速。满足这些条件的最流行的分布是高斯分布。下图显示了高斯分布的概率分布函数(pdf)、等高线以及从中生成的样本。</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oa"><img src="../Images/066baaf2a831bee3357ade99f3404ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2u_VMSpN1IX6kMA8pU1wTA.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">来源:作者的情节</p></figure><p id="d189" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe ob oc od oe b"><a class="ae mz" href="https://gist.github.com/prachiisc/a5f193212f90c8f82e84b5fe9dcf85d4" rel="noopener ugc nofollow" target="_blank"><strong class="kn ja">show code</strong></a></code></p><p id="656e" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了将数据聚类成类，我们需要用单独的高斯分布来表示每个类，因此我们可以使用高斯混合模型(GMM)。GMM是高斯的加权混合，每个高斯具有不同的均值和协方差，其中每个混合可以代表每个类别。</p><p id="80a5" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">GMM的概率分布函数(pdf)为:</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi of"><img src="../Images/2685b1e5d69a4a08e0a12edb3320ecff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hi0QNjL7CDAbdcKMD9zvoA.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">来源:作者的情节。具有<strong class="bd og"> πₖ、μₖ、</strong>φ<strong class="bd og">ₖ的GMM的pdf为</strong> k- <strong class="bd og">次高斯</strong>的权重、均值和协方差</p></figure><p id="27ee" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe ob oc od oe b"><a class="ae mz" href="https://gist.github.com/prachiisc/4668febcef55f6d857863ca9ffd920bd" rel="noopener ugc nofollow" target="_blank"><strong class="kn ja">show code</strong></a></code></p><p id="1eee" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">设<strong class="kn ja"> <em class="mp"> y </em> </strong>为潜在(隐藏)<strong class="kn ja">类变量</strong>代表GMM某类/混合的<strong class="kn ja">意思。现在给定这个<strong class="kn ja">类变量<em class="mp">y</em>T12】，产生数据样本<strong class="kn ja">xT16】的概率为:</strong></strong></strong></p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oh"><img src="../Images/1eaef1fe29d109a18aa73451690ddd97.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*A7lA7iHJzn9piRoINrJ7Jg.png"/></div></div></figure><p id="d97f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">其中φw表示给定类的类内协方差。这表明，一旦我们知道了高斯的类参数，我们就可以生成这个类的样本。这里<strong class="kn ja">类变量<em class="mp">y</em>T20】本身假设是从单独分布中产生的。从假定的分布中产生代表一个类别的特定实例<strong class="kn ja"> <em class="mp"> y </em> </strong>的概率被称为先验概率。</strong></p><p id="2179" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">LDA也被建模为GMM，其中每个混合的高斯均值是属于各自类别的训练数据的样本均值，并且<strong class="kn ja"> <em class="mp"> y </em> </strong>的先验概率是离散的，其中y只能取离散值，给出如下</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/87a8d6a5e63004677003d2017b649db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*xC0OwG6xN7DcTwXsfLmYag.png"/></div></figure><p id="3f57" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这将产生上面讨论的GMM。最大化该模型关于参数的可能性{ <strong class="kn ja"> πₖ，μₖ，</strong>φw }恢复标准LDA投影。但是，为了处理在训练期间没有看到的类，我们需要修改prior并使其连续，其中y可以取从给定的分布中生成的任何实数值</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/d564603c42e4f0d70b8feb157b3aa2bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*ZHnEtQ-7OsvVZCtodI7v4w.png"/></div></figure><p id="0966" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这表明，每个类别的潜在变量y可以使用高斯分布生成，具有<strong class="kn ja">均值<em class="mp">m</em>T34】和<strong class="kn ja">类别间协方差</strong>φ<strong class="kn ja">b .</strong>，因此它被称为<strong class="kn ja">概率LDA。</strong>使用下图可以更好地解释这一点:</strong></p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ok"><img src="../Images/e746b1d7edcc9edd7dd0f360649b80c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ipBC-91k6baFIc_u7U9tA.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">来源:图片由作者提供。小椭圆代表每个类别的高斯分布等值线，平均值位于中心。数据点由十字标记表示。灰色椭圆根据高斯分布生成每一类的平均值。</p></figure><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oa"><img src="../Images/95138679f25a90379691b79643bed476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVHFhXN31Bg5y8t0czWMlg.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">来源:人物图片来自<a class="ae mz" href="http://vis-www.cs.umass.edu/lfw/" rel="noopener ugc nofollow" target="_blank"> LFW数据集</a>，由作者代表。情节是为了说明。</p></figure><p id="a171" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">样本<strong class="kn ja"> <em class="mp"> y </em> </strong> 1和<strong class="kn ja"> <em class="mp"> y </em> </strong> 2由高斯分布eq (2)生成，代表人物身份。我们对每个类(人)的示例<strong class="kn ja"> <em class="mp"> x </em> </strong> 1和<strong class="kn ja"> <em class="mp"> x </em> </strong> 2进行采样，用等式(1)以<strong class="kn ja"> <em class="mp"> y </em> </strong> 1和<strong class="kn ja"> <em class="mp"> y </em> </strong> 2为均值来表示人的不同取向。</p><h1 id="af51" class="lj lk iq bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated"><strong class="ak">潜在空间</strong></h1><p id="e639" class="pw-post-body-paragraph kl km iq kn b ko mh kq kr ks mi ku kv kw mj ky kz la mk lc ld le ml lg lh li ij bi translated"><strong class="kn ja">PLDA</strong>的目标是<strong class="kn ja">将数据样本</strong>投影到<strong class="kn ja">潜在空间</strong>中，使得来自同一类的样本使用相同的分布建模。这些预测用<strong class="kn ja">潜在变量</strong>表示，这将在本节中讨论。</p><p id="4c00" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如前所述，isφb<strong class="kn ja">是类间协方差半正定矩阵</strong>而φw是<strong class="kn ja">类内协方差正定矩阵</strong>。我们得到一个<strong class="kn ja">变换矩阵</strong> V，它将φw和φb同时转换成对角矩阵，给出如下:</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/68efcbd9ce1a429e4c95b2d3f3fadfc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*HB9XzN9cBqLsMUa4ON4-Sg.png"/></div></figure><p id="879c" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">其中I是<strong class="kn ja">单位矩阵，</strong>ψ<strong class="kn ja">T44】是<strong class="kn ja">对角矩阵</strong>。因此，我们对数据样本的每个维度去相关。现在PLDA模型的参数是{ <strong class="kn ja"> m </strong>，A，ψ}。</strong></p><p id="be82" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">你可以参考下面的推导来得到上面的方程。它需要了解<strong class="kn ja">线性代数</strong>概念，如矩阵的<a class="ae mz" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwifwMC9w8frAhWdzDgGHQwcAqMQmhMwMHoECAoQAg&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FEigenvalues_and_eigenvectors&amp;usg=AOvVaw2--h2kiFfYCXwfK-l_u6Eo" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ja">特征值和特征向量</strong></a><strong class="kn ja"/><a class="ae mz" href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix" rel="noopener ugc nofollow" target="_blank"><strong class="kn ja">特征值分解</strong> </a>。</p><p id="defc" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe ob oc od oe b"><a class="ae mz" href="http://leap.ee.iisc.ac.in/prachi/medium/Eigen_value_derivation.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kn ja">show derivation</strong></a></code></p><p id="591a" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">设<strong class="kn ja"> <em class="mp"> u，v </em> </strong>是潜在空间中的高斯随机变量定义为，</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div class="gh gi om"><img src="../Images/dac8c48da871b93e0831918c07d5d4ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*WGikaRhuOZjHm4cHnIGSVg.png"/></div></figure><p id="f844" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们可以发现<strong class="kn ja">数据样本</strong> <strong class="kn ja"> <em class="mp"> x </em> </strong>，<strong class="kn ja">类变量</strong> <strong class="kn ja"> <em class="mp"> y </em> </strong>与这些潜在变量之间的关系如下:</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div class="gh gi on"><img src="../Images/7d1bc2ce886a6a46045910dae91b255c.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*EjkEX56kSuGhZ2hu81xCkg.png"/></div></figure><p id="de0e" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">因此<strong class="kn ja"> <em class="mp"> u </em> </strong>表示<strong class="kn ja">类</strong>的实例，而<strong class="kn ja"> <em class="mp"> v </em> </strong>表示<strong class="kn ja">类变量</strong>在<strong class="kn ja">投影空间</strong>中。等式(5)和等式(6)中的关系以流程图的形式表示，如下所示:</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oo"><img src="../Images/7ca2747b5e087ba221424367c8f83b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UC73ZN1t2_X2w8ckZ_vZow.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">来源:图片由作者提供。PLDA在变量独立的潜在空间中对分类中心v和示例u1、u2进行建模。原始特征空间中的示例x1、x2通过可逆变换A与其潜在表示u相关联</p></figure><p id="9201" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">有兴趣的可以参考下面的推导:</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div class="gh gi op"><img src="../Images/83fda8e59ef002ee3665fbf5acddee55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*WKVTXPXhtAstuICts-2aLA.png"/></div></figure><h1 id="599a" class="lj lk iq bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">应用程序</h1><p id="f2ea" class="pw-post-body-paragraph kl km iq kn b ko mh kq kr ks mi ku kv kw mj ky kz la mk lc ld le ml lg lh li ij bi translated">PLDA允许对训练中没有出现的课程进行推断。一个例子是说话人识别。模型参数从训练数据中学习，但是模型应该处理来自训练期间不在场的说话者的例子。这里讨论了可以使用PLDA的一些任务。</p><h2 id="c721" class="oq lk iq bd ll or os dn lp ot ou dp lt kw ov ow lx la ox oy mb le oz pa mf iw bi translated"><strong class="ak">分类:</strong></h2><p id="c463" class="pw-post-body-paragraph kl km iq kn b ko mh kq kr ks mi ku kv kw mj ky kz la mk lc ld le ml lg lh li ij bi translated">我们有一组<strong class="kn ja">示例x</strong>g<strong class="kn ja"/>∈(<strong class="kn ja"><em class="mp">x</em></strong><em class="mp">₁</em><strong class="kn ja"><em class="mp">，x</em></strong><em class="mp">₂</em><strong class="kn ja"><em class="mp">，…，x </em> </strong> <em class="mp"> M </em>每个<strong class="kn ja"> M类</strong>一个。现在给出一个探针例子<strong class="kn ja"> <em class="mp"> x </em> </strong> p，任务是找到它属于哪个类。这是通过最大化可能性来确定的。首先，使用等式(6)将示例投影到潜在空间中</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/fc4c61bdc278fa9ded399e940a168775.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*PRynxc8F-7WnBSR0pxZFPg.png"/></div></figure><p id="4f9d" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">当u的协方差为I时，对数据进行去相关。<strong class="kn ja">P(<em class="mp">u</em></strong><em class="mp">P</em><strong class="kn ja">│<em class="mp">u</em></strong><em class="mp">g</em><strong class="kn ja">)</strong>给出来自与已知集合中的样本相同类别的探针样本的概率。</p><p id="26f7" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">因此，分配给探针示例的C类为</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/e8d12e587d222b8273ec9268b03e965a.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*0eVjUG-CM1pPosOfkTzPuw.png"/></div></figure><p id="b639" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe ob oc od oe b"><a class="ae mz" href="http://leap.ee.iisc.ac.in/prachi/medium/computation_of_p.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kn ja">Computation of P(up|ug)</strong></a></code></p><h2 id="50c4" class="oq lk iq bd ll or os dn lp ot ou dp lt kw ov ow lx la ox oy mb le oz pa mf iw bi translated"><strong class="ak">类推断:</strong></h2><p id="870a" class="pw-post-body-paragraph kl km iq kn b ko mh kq kr ks mi ku kv kw mj ky kz la mk lc ld le ml lg lh li ij bi translated">PLDA的一个优点是，我们可以从类的单个例子中找到类变量<strong class="kn ja"> <em class="mp"> y </em> </strong>。对于示例x，我们计算给定x的y的后验概率，表示为p(<strong class="kn ja"><em class="mp">y</em></strong>|<strong class="kn ja"><em class="mp">x</em></strong>)，这将再次是高斯分布。y的估计是通过将p(<strong class="kn ja"><em class="mp">y</em></strong>|<strong class="kn ja"><em class="mp">x</em></strong>)相对于<strong class="kn ja"> <em class="mp"> y </em> </strong>最大化得到的。无非是高斯p的均值(<strong class="kn ja"><em class="mp">y</em></strong>|<strong class="kn ja"><em class="mp">x</em></strong>)。可以这样写:</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/38829195c4cebd92664b716f3ab4a9c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*ndpKXa0AjvbduE0NXy0NBg.png"/></div></figure><h2 id="4806" class="oq lk iq bd ll or os dn lp ot ou dp lt kw ov ow lx la ox oy mb le oz pa mf iw bi translated"><strong class="ak">假设检验:</strong></h2><p id="6b9d" class="pw-post-body-paragraph kl km iq kn b ko mh kq kr ks mi ku kv kw mj ky kz la mk lc ld le ml lg lh li ij bi translated">给定两个来自未知类的例子(<strong class="kn ja"> <em class="mp"> u </em> </strong> 1，<strong class="kn ja"> <em class="mp"> u </em> </strong> 2)，如果我们需要判断它们是否属于同一类，那么我们基于两个假设计算<strong class="kn ja"> <em class="mp">似然比R </em> </strong>。</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi pe"><img src="../Images/61b179ec8f2cf9d369dc744d97f7632f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JF5SqtcvuW0OR3mLw751mg.png"/></div></div></figure><h2 id="2bdd" class="oq lk iq bd ll or os dn lp ot ou dp lt kw ov ow lx la ox oy mb le oz pa mf iw bi translated"><strong class="ak">聚类:</strong></h2><p id="4020" class="pw-post-body-paragraph kl km iq kn b ko mh kq kr ks mi ku kv kw mj ky kz la mk lc ld le ml lg lh li ij bi translated">PLDA也用于将示例分组。基于<strong class="kn ja"> <em class="mp">对数似然比R </em> </strong> <em class="mp">或</em> <strong class="kn ja"> <em class="mp"> PLDA分数</em> </strong>，我们将每个例子与所有其他例子进行比较。这将创建一个<em class="mp"> PLDA得分矩阵</em>，类似于一个相似性得分矩阵，可用于使用k-means、凝聚聚类等可用算法执行聚类。</p><h1 id="a0d4" class="lj lk iq bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated"><strong class="ak">例子</strong></h1><p id="c681" class="pw-post-body-paragraph kl km iq kn b ko mh kq kr ks mi ku kv kw mj ky kz la mk lc ld le ml lg lh li ij bi translated"><strong class="kn ja">说话者二分化:</strong>基于说话者源将输入音频流分割成片段的任务。</p><p id="89b8" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">该过程如下</p><ol class=""><li id="f8e0" class="na nb iq kn b ko kp ks kt kw nc la nd le ne li pf ng nh ni bi translated">将音频分成小段，每个小段只包含一个扬声器。</li><li id="d910" class="na nb iq kn b ko nj ks nk kw nl la nm le nn li pf ng nh ni bi translated">提取每个线段的特征</li><li id="53ac" class="na nb iq kn b ko nj ks nk kw nl la nm le nn li pf ng nh ni bi translated">使用预先训练的PLDA计算PLDA分数矩阵</li><li id="7c65" class="na nb iq kn b ko nj ks nk kw nl la nm le nn li pf ng nh ni bi translated">使用分数矩阵执行聚类</li></ol><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi pg"><img src="../Images/e48751c63f8e9b74f36255480f0f8a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ucKdxbBpRGfwtJD0CSf2Ow.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">来源:图片由作者提供。扬声器双音化管道。</p></figure><p id="59f6" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我使用<a class="ae mz" href="https://kaldi-asr.org/models/m6" rel="noopener ugc nofollow" target="_blank">语音识别工具包Kaldi </a>完成第1步和第2步，并训练PLDA如下。</p><p id="d87e" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">以下代码涉及:读取特征-&gt;应用主成分分析-&gt; PLDA潜在空间项目-&gt;计算PLDA得分矩阵。带有特性和预训练模型的完整代码可以在<a class="ae mz" href="https://github.com/prachiisc/PLDA_scoring" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ja"> GitHub </strong> </a> <strong class="kn ja">中找到。</strong></p><p id="ac74" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe ob oc od oe b"><a class="ae mz" href="https://gist.github.com/prachiisc/81f1d845b02ae1a7103580c74f260a4b" rel="noopener ugc nofollow" target="_blank"><strong class="kn ja">show code</strong></a></code></p><h2 id="a75a" class="oq lk iq bd ll or os dn lp ot ou dp lt kw ov ow lx la ox oy mb le oz pa mf iw bi translated">分析</h2><ol class=""><li id="7451" class="na nb iq kn b ko mh ks mi kw ph la pi le pj li pf ng nh ni bi translated"><strong class="kn ja"> PLDA潜在表象(u): </strong>下图显示了将数据投射到PLDA潜在空间的效果。我们可以看到，当我们将数据投影到PLDA潜像中时，它就变成了可分离的扬声器，即使PLDA模型从未见过来自这些扬声器的音频。</li></ol><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi pk"><img src="../Images/b5539fa242d58ece677e2bb02aef142d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qnwg3GiJBB-qAjJzPkjkMA.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">128维特征的2d投影。左侧图是PCA转化的嵌入。左侧情节是PLDA潜在的表述。每种颜色代表一个扬声器。由作者策划。</p></figure><p id="f244" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe ob oc od oe b"><a class="ae mz" href="https://gist.github.com/prachiisc/5ec4c485c62a1b07eee65745b9d4621f" rel="noopener ugc nofollow" target="_blank"><strong class="kn ja">show code</strong></a></code></p><p id="188f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ja"> 2。对数似然比或PLDA得分矩阵:</strong>我们可以通过找到从音频中提取的所有I和j片段的<strong class="kn ja"> <em class="mp"> x </em> </strong> <em class="mp"> ᵢ </em>和<strong class="kn ja"> <em class="mp"> x </em> </strong> <em class="mp"> ⱼ </em>之间的相似性得分S(i，j)来计算相似性得分矩阵s。下图显示了标准化余弦得分矩阵和PLDA得分矩阵，通过除以矩阵中的最高得分进行标准化。它描绘了在PLDA分数中的较高对比，显示了在对相同和不同说话者做出决定时的较高信心。浅色(黄色阴影)表示高分，而深色(蓝色阴影)表示低分。我们可以看到光块和色块，这有助于轻松识别同一扬声器和不同的扬声器区域。</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi pl"><img src="../Images/7855b9ca428ed258cc529eca8a6a859c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lD9SK7sz92eCblXAXtXQFw.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">余弦亲和度矩阵和PLDA亲和度矩阵的比较。由作者策划。</p></figure><p id="7511" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe ob oc od oe b"><a class="ae mz" href="https://gist.github.com/prachiisc/3519554beccf61275495940890c674b5" rel="noopener ugc nofollow" target="_blank"><strong class="kn ja">show code</strong></a></code></p><p id="472d" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ja"> 3。直方图:</strong>下图显示了PLDA分数的分布。更高的计数出现在极端情况下，这有助于更好的聚类。</p><figure class="mu mv mw mx gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi pm"><img src="../Images/efb0c1b51bd19db55137eccd7b1bda01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DS8pqpdn3JkddGfGFGUXnA.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">x轴:标准化的PLDA分数，y轴:计数。由作者策划。</p></figure><p id="ba0a" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe ob oc od oe b"><a class="ae mz" href="https://gist.github.com/prachiisc/0c0875217f50781735b0c734d4e72824" rel="noopener ugc nofollow" target="_blank"><strong class="kn ja">show code</strong></a></code></p><h1 id="04ae" class="lj lk iq bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated"><strong class="ak">总结</strong></h1><ol class=""><li id="cb01" class="na nb iq kn b ko mh ks mi kw ph la pi le pj li pf ng nh ni bi translated">PLDA是一个生成模型，其中我们假设一个类的数据样本<strong class="kn ja"> X </strong>是从高斯分布生成的。高斯平均值代表类别变量<strong class="kn ja"> y </strong>是从另一个被称为先验的高斯分布生成的。</li><li id="80fe" class="na nb iq kn b ko nj ks nk kw nl la nm le nn li pf ng nh ni bi translated">对于识别的任务，我们可以通过比较来自相同类别的样本的可能性与来自不同类别的样本的可能性，使用PLDA分数来比较两个未知类别的样本。</li><li id="e043" class="na nb iq kn b ko nj ks nk kw nl la nm le nn li pf ng nh ni bi translated">我们可以使用整组样本中所有样本对之间的PLDA分数将样本聚类成类。</li></ol></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="0d7b" class="lj lk iq bd ll lm nv lo lp lq nw ls lt lu nx lw lx ly ny ma mb mc nz me mf mg bi translated">参考</h1><ol class=""><li id="a35c" class="na nb iq kn b ko mh ks mi kw ph la pi le pj li pf ng nh ni bi translated">约夫，谢尔盖。"概率线性判别分析."在<em class="mp">欧洲计算机视觉会议</em> ，第531–542页。施普林格，柏林，海德堡，2006。</li><li id="f372" class="na nb iq kn b ko nj ks nk kw nl la nm le nn li pf ng nh ni bi translated">普林斯、西蒙·杰狄和詹姆斯·埃尔德。"<a class="ae mz" href="https://ieeexplore.ieee.org/abstract/document/4409052/?casa_token=LYpmpjhmz08AAAAA:4v5BMfUpqTi-elPqe4RxIwYV-t6q9qxCHLsK2TbG2CIw6n_Kh3dWb8oPSPUUYuOIR2W_6K0fmlDz" rel="noopener ugc nofollow" target="_blank">概率线性鉴别分析对身份的推断</a>"2007年IEEE第11届计算机视觉国际会议，第1–8页。IEEE，2007年。</li><li id="3f13" class="na nb iq kn b ko nj ks nk kw nl la nm le nn li pf ng nh ni bi translated">毕晓普，克里斯托弗M. <a class="ae mz" href="https://cds.cern.ch/record/998831" rel="noopener ugc nofollow" target="_blank"> <em class="mp">模式识别与机器学习</em> </a> <em class="mp">。斯普林格，2006年。</em></li><li id="df85" class="na nb iq kn b ko nj ks nk kw nl la nm le nn li pf ng nh ni bi translated">杜达，杰瑞克。"高斯自动编码器。"<em class="mp"> arXiv预印本arXiv:1811.04751 </em>，2018</li><li id="41e4" class="na nb iq kn b ko nj ks nk kw nl la nm le nn li pf ng nh ni bi translated">斯特朗吉尔伯特。<em class="mp">线性代数入门</em>。第五版。<a class="ae mz" href="http://www.wellesleycambridge.com/" rel="noopener ugc nofollow" target="_blank">韦尔斯利-剑桥出版社</a>，2016年。国际标准书号:9780980232776</li></ol></div></div>    
</body>
</html>