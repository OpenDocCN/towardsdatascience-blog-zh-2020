<html>
<head>
<title>Learning Rate Schedule in Practice: an example with Keras and TensorFlow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实践中的学习率计划:以Keras和TensorFlow 2.0为例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c?source=collection_archive---------7-----------------------#2020-10-02">https://towardsdatascience.com/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c?source=collection_archive---------7-----------------------#2020-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c57d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">添加和定制<strong class="ak">学习率计划</strong>的教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c85b91328ad93c55dec573899647672f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZo1OSl8GAppQRQqfKIIew.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@nexgenfx?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">卢卡·坎皮奥尼</a>在<a class="ae ky" href="https://unsplash.com/s/photos/speed?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="a3ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练神经网络的一个令人痛苦的事情是我们必须处理的超参数的绝对数量。例如</p><ul class=""><li id="223f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">学习率</li><li id="999f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">Adam优化算法的动量或超参数</li><li id="482e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">层数</li><li id="062e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">隐藏单元的数量</li><li id="f833" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">小批量</li><li id="46c2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">激活功能</li><li id="069c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">等等</li></ul><p id="f072" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中，最重要的参数是学习率。如果你的学习率设置得很低，训练将会进展得很慢，因为你对你的网络中的权重做了非常微小的更新。然而，如果你的学习率设置得太高，它会在你的损失函数中引起不希望的发散行为。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/54363240c8607905c1c88cb3641d374a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Zr7nkI97IGT9e_tpgK-_g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者使用<a class="ae ky" href="https://excalidraw.com/" rel="noopener ugc nofollow" target="_blank">https://excalidraw.com/</a>创建的图像</p></figure><p id="629b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练神经网络时，随着训练的进行降低学习速率通常是有用的。这可以通过使用<strong class="lb iu">学习速率表</strong>或<strong class="lb iu">自适应学习速率</strong>来完成。在本文中，我们将重点关注在我们的机器学习模型中添加和定制<strong class="lb iu">学习率计划</strong>,并查看我们如何在Keras和TensorFlow 2.0中实践它们的示例</p><h1 id="3f8d" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">学习费率表</h1><p id="0b61" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated"><strong class="lb iu">学习率时间表</strong>根据预定义的时间表，通过降低学习率来调整训练期间的学习率。流行的学习率计划包括</p><ol class=""><li id="5684" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nh mb mc md bi translated">恒定学习速率</li><li id="f1d8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nh mb mc md bi translated">基于时间的衰减</li><li id="0030" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nh mb mc md bi translated">阶跃衰减</li><li id="19ac" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nh mb mc md bi translated">指数衰减</li></ol><p id="1048" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于演示目的，我们将构建一个图像分类器来处理时尚MNIST，这是一个具有10个类的70，000个28×28像素的灰度图像的数据集。</p><p id="931b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/006-learning-rate-schedules/learning-rate-schedules.ipynb" rel="noopener ugc nofollow" target="_blank">我的Github repo </a>获取源代码。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h2 id="31e9" class="np ml it bd mm nq nr dn mq ns nt dp mu li nu nv mw lm nw nx my lq ny nz na oa bi translated">使用Keras加载数据集</h2><p id="f715" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">Keras提供了一些实用函数来获取和加载公共数据集，包括时尚MNIST。让我们装载时尚MNIST</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="9999" class="np ml it oc b gy og oh l oi oj">fashion_mnist = keras.datasets.fashion_mnist<br/><strong class="oc iu">(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()</strong></span></pre><p id="19b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集已经分为训练集和测试集。以下是定型集的形状和数据类型:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="2751" class="np ml it oc b gy og oh l oi oj">&gt;&gt;&gt; X_train_full.shape<br/><strong class="oc iu">(60000, 28, 28)</strong><br/>&gt;&gt;&gt; X_train_full.dtype<br/><strong class="oc iu">dtype('uint8')</strong></span></pre><p id="1364" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用梯度下降来训练神经网络，我们必须将输入特征缩小到0-1范围。为了在本地机器上进行更快的训练，我们只使用前10，000幅图像。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="0f74" class="np ml it oc b gy og oh l oi oj">X_train, y_train = <strong class="oc iu">X_train_full[:10000]/255.0</strong>, <strong class="oc iu">y_train_full[:10000]</strong></span></pre><h2 id="c83c" class="np ml it bd mm nq nr dn mq ns nt dp mu li nu nv mw lm nw nx my lq ny nz na oa bi translated">创建模型</h2><p id="65e8" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">现在让我们建立神经网络！用Keras和TensorFlow 2.0 创建机器学习模型有<a class="ae ky" rel="noopener" target="_blank" href="/3-ways-to-create-a-machine-learning-model-with-keras-and-tensorflow-2-0-de09323af4d3"> 3种方法。由于我们正在构建一个简单的全连接神经网络，为了简单起见，让我们使用最简单的方法:带有<code class="fe ok ol om oc b">Sequential()</code>的顺序模型。</a></p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="3158" class="np ml it oc b gy og oh l oi oj">from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense, Flatten</span><span id="b0ab" class="np ml it oc b gy on oh l oi oj">def create_model(): <br/>    model = Sequential([<br/>        Flatten(<strong class="oc iu">input_shape=(28, 28)</strong>),<br/>        Dense(300, activation='relu'),<br/>        Dense(100, activation='relu'),<br/>        Dense(<strong class="oc iu">10, activation='softmax'</strong>),<br/>    ])<br/>    return model</span></pre><p id="b5b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的型号有以下规格:</p><ul class=""><li id="24da" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">第一层(也称为输入层)有<code class="fe ok ol om oc b">input_shape</code>来设置匹配训练数据的输入大小<code class="fe ok ol om oc b">(28, 28)</code>。输入层是一个<code class="fe ok ol om oc b">Flatten</code>层，它的作用只是将每个输入图像转换成一个1D数组。</li><li id="a458" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">然后是2个<code class="fe ok ol om oc b">Dense</code>层，一个300单位，另一个100单位。两者都使用<code class="fe ok ol om oc b">relu</code>激活功能。</li><li id="490b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">输出<code class="fe ok ol om oc b">Dense</code>层有<code class="fe ok ol om oc b">10</code>单元和<code class="fe ok ol om oc b">softmax</code>激活功能。</li></ul><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="19ba" class="np ml it oc b gy og oh l oi oj">model = create_model()<br/>model.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/77dc3d85493357df318261edb7023b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XsvFYiZtThrtCdvPtNrRHQ.png"/></div></div></figure><h1 id="f8cf" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">1.恒定学习速率</h1><p id="c9eb" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">恒定的学习速率是所有Keras优化器的默认时间表。例如，在<a class="ae ky" href="https://keras.io/api/optimizers/sgd/" rel="noopener ugc nofollow" target="_blank"> SGD优化器</a>中，学习率默认为<code class="fe ok ol om oc b">0.01</code>。</p><p id="6d3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要使用定制的学习率，只需实例化一个SGD优化器并传递参数<code class="fe ok ol om oc b">learning_rate=0.01</code>。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="cee6" class="np ml it oc b gy og oh l oi oj">sgd = tf.keras.optimizers.SGD(<strong class="oc iu">learning_rate=0.01</strong>)</span><span id="de44" class="np ml it oc b gy on oh l oi oj">model.compile(<br/>    <strong class="oc iu">optimizer=sgd,</strong> <br/>    loss='sparse_categorical_crossentropy', <br/>    metrics=['accuracy']<br/>)</span></pre><p id="5901" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并使模型符合训练数据:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="dc9b" class="np ml it oc b gy og oh l oi oj">history_constant = model.fit(<br/>    X_train, <br/>    y_train, <br/>    epochs=100, <br/>    validation_split=0.2,<br/>    batch_size=64<br/>)</span></pre><p id="c0c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们画出模型的准确性，这可以作为我们实验其他学习率计划的基线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/5d93e7f5dc8a9b292203cf4dd07f417e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xuf0sHE01-m-SixtQccZ3w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">恒定学习率——准确度图</p></figure><h1 id="e965" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">2.基于时间的衰减</h1><p id="3941" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">基于时间的衰减是最流行的学习速率计划之一。形式上，基于时间的衰减定义为:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="80e9" class="np ml it oc b gy og oh l oi oj">learning_rate = lr * 1 / (1 + decay * epoch)</span></pre><p id="a632" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<code class="fe ok ol om oc b">lr</code>是以前的学习率，<code class="fe ok ol om oc b">decay</code>是超参数，<code class="fe ok ol om oc b">epoch</code>是迭代次数。当<code class="fe ok ol om oc b">decay</code>为零时，这对改变学习率没有影响。当<code class="fe ok ol om oc b">decay</code>被指定时，它将按照给定的固定量减少前一时期的学习率。<code class="fe ok ol om oc b">decay</code>的值通常实现为</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="a2ae" class="np ml it oc b gy og oh l oi oj">decay = initial_learning_rate / num_of_epoches </span></pre><p id="76d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Keras中，实现基于时间的衰减的一种方式是通过定义基于时间的衰减函数<code class="fe ok ol om oc b"><strong class="lb iu">lr_time_based_decay()</strong></code> <strong class="lb iu"> </strong>并将其传递给<code class="fe ok ol om oc b">LearningRateScheduler</code>回调。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="60f8" class="np ml it oc b gy og oh l oi oj">initial_learning_rate = 0.01<br/>epochs = 100<br/>decay = initial_learning_rate / epochs</span><span id="913e" class="np ml it oc b gy on oh l oi oj"><strong class="oc iu">def lr_time_based_decay(epoch, lr):<br/>    return lr * 1 / (1 + decay * epoch)</strong></span><span id="94e1" class="np ml it oc b gy on oh l oi oj">history_time_based_decay = model.fit(<br/>    X_train, <br/>    y_train, <br/>    epochs=100, <br/>    validation_split=0.2,<br/>    batch_size=64,<br/>    <strong class="oc iu">callbacks=[LearningRateScheduler(lr_time_based_decay, verbose=1)],</strong><br/>)</span></pre><p id="a6aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是准确率和学习率的曲线图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/76ccef5300fff2a575cd545efb49ab8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Eu5jOTPABcV4B5KLvqBq6g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于时间的衰减——精度图</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/d339a26dd8803a42d912f9f694b2e481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zyQulWm4KRkiWtL_e5Mnyg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于时间的衰减—学习率图</p></figure><h1 id="a348" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">3.阶跃衰减</h1><p id="ad4b" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">另一种流行的学习率计划是在训练期间的特定时间系统地降低学习率。在形式上，它被定义为:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="c28a" class="np ml it oc b gy og oh l oi oj">learning_rate = initial_lr * drop_rate^floor(epoch / epochs_drop)</span></pre><p id="2a35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<code class="fe ok ol om oc b">initial_lr</code>是初始学习率，例如0.01，<code class="fe ok ol om oc b">drop_rate</code>是每次改变学习率时修改的量，<code class="fe ok ol om oc b">epoch</code>是当前的历元数，<code class="fe ok ol om oc b">epochs_drop</code>是改变学习率的频率，例如10个历元。类似地，我们可以通过定义一个阶跃衰减函数<code class="fe ok ol om oc b"><strong class="lb iu">lr_step_decay()</strong></code>并将其传递给<code class="fe ok ol om oc b">LearningRateScheduler</code>回调函数来实现这一点。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="b94b" class="np ml it oc b gy og oh l oi oj">initial_learning_rate = 0.01</span><span id="15a6" class="np ml it oc b gy on oh l oi oj"><strong class="oc iu">def lr_step_decay(epoch, lr):<br/>    drop_rate = 0.5<br/>    epochs_drop = 10.0<br/>    return initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))</strong></span><span id="c05e" class="np ml it oc b gy on oh l oi oj"># Fit the model to the training data<br/>history_step_decay = model.fit(<br/>    X_train, <br/>    y_train, <br/>    epochs=100, <br/>    validation_split=0.2,<br/>    batch_size=64,<br/>    <strong class="oc iu">callbacks=[LearningRateScheduler(lr_step_decay, verbose=1)],</strong><br/>)</span></pre><p id="30c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是准确率和学习率的曲线图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/85fe5ebac7b28566882f2b52c7e9c908.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFz_K_bsGiZHQkS7e5wcqw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于步进的衰减——精度图</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/529f92b79f31545836cc29f30abf6127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5N95STd9deaYati3Cn4o6g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于步长的衰减—学习速率</p></figure><h1 id="2bca" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">4.指数衰减</h1><p id="c881" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">另一种流行的学习率计划是以指数速度降低学习率。在形式上，它被定义为:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="f5ef" class="np ml it oc b gy og oh l oi oj">learning_rate = initial_lr * e^(−k * epoch)</span></pre><p id="41b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<code class="fe ok ol om oc b">initial_lr</code>是初始学习率，例如0.01，<code class="fe ok ol om oc b">k</code>是超参数，<code class="fe ok ol om oc b">epoch</code>是当前纪元编号。类似地，我们可以通过定义一个指数衰减函数<code class="fe ok ol om oc b"><strong class="lb iu">lr_exp_decay()</strong></code>并将其传递给<code class="fe ok ol om oc b">LearningRateScheduler</code>回调函数来实现这一点。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="dc38" class="np ml it oc b gy og oh l oi oj">initial_learning_rate = 0.01</span><span id="b38b" class="np ml it oc b gy on oh l oi oj"><strong class="oc iu">def lr_exp_decay(epoch, lr):<br/>    k = 0.1<br/>    return initial_learning_rate * math.exp(-k*epoch)</strong></span><span id="9a27" class="np ml it oc b gy on oh l oi oj"># Fit the model to the training data<br/>history_exp_decay = model.fit(<br/>    X_train, <br/>    y_train, <br/>    epochs=100, <br/>    validation_split=0.2,<br/>    batch_size=64,<br/>    <strong class="oc iu">callbacks=[LearningRateScheduler(lr_exp_decay, verbose=1)],</strong><br/>)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/6e3d1385b3189763bcebfc2949673c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mD3-j_w2NMt0NoEoxvLRzA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">指数衰减—精度图</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/d147d355bc5f2291d00323f95e97f6e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CgioTtU7G7mL202dL7CVfg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">指数衰减——学习率图</p></figure><h1 id="f9d6" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">比较模型准确性</h1><p id="c262" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">最后，让我们使用不同的学习率时间表来比较模型的准确性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/0f04be928e6d4c0c4f462f88e6099d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7xeRIjAhnih6EWKrOk_aKw.png"/></div></div></figure><p id="d79f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来<strong class="lb iu">常数</strong>和<strong class="lb iu">基于时间的</strong>学习率比<strong class="lb iu">阶跃衰减</strong>和<strong class="lb iu">指数衰减</strong>对于这个特定的教程有更好的性能。请记住，本教程仅使用前10，000幅图像，其中的<code class="fe ok ol om oc b">initial_learning_rate=0.01</code>、<code class="fe ok ol om oc b">validation_split=0.2</code>和<code class="fe ok ol om oc b">batch_size=64</code>具有任意值。</p><p id="2ba8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在现实世界的应用程序中，为了调整学习速率，需要考虑更多的因素。请查看“深度架构基于梯度的培训实用建议”<a class="ae ky" href="https://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener ugc nofollow" target="_blank"/>一文，了解一些最佳实践。</p><h1 id="aede" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">好了</h1><p id="18fb" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">感谢阅读。这篇文章涵盖了最流行的<strong class="lb iu">学习进度计划</strong>。下次我们就来看看<strong class="lb iu">自适应学习率</strong>。</p><p id="7cdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">源代码请查看我的Github 上的<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/006-learning-rate-schedules/learning-rate-schedules.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本。</a></p><p id="300b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对机器学习的实用方面感兴趣，请继续关注。</p><h2 id="e040" class="np ml it bd mm nq nr dn mq ns nt dp mu li nu nv mw lm nw nx my lq ny nz na oa bi translated">您可能对我的其他TensorFlow文章感兴趣:</h2><ul class=""><li id="9c98" class="lv lw it lb b lc nc lf nd li ox lm oy lq oz lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/the-googles-7-steps-of-machine-learning-in-practice-a-tensorflow-example-for-structured-data-96ccbb707d77">谷歌机器学习实践的7个步骤:结构化数据的TensorFlow示例</a></li><li id="7e2c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/3-ways-to-create-a-machine-learning-model-with-keras-and-tensorflow-2-0-de09323af4d3">用Keras和TensorFlow 2.0创建机器学习模型的3种方法</a></li><li id="8cdd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/batch-normalization-in-practice-an-example-with-keras-and-tensorflow-2-0-b1ec28bde96f">批量规范化实践:以Keras和TensorFlow 2.0为例</a></li><li id="2c0d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/a-practical-introduction-to-early-stopping-in-machine-learning-550ac88bc8fd">实践中的提前停止:以Keras和TensorFlow为例</a></li></ul></div></div>    
</body>
</html>