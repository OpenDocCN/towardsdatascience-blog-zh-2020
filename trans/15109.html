<html>
<head>
<title>How to create a concise image representation using machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用机器学习创建简洁的图像表示</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-create-a-concise-image-representation-using-machine-learning-20156c1e0c19?source=collection_archive---------30-----------------------#2020-10-17">https://towardsdatascience.com/how-to-create-a-concise-image-representation-using-machine-learning-20156c1e0c19?source=collection_archive---------30-----------------------#2020-10-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ba96" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Keras中HRRR图像自动编码器的设计与训练</h2></div><p id="01c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">互联网上的Autoencoder示例似乎要么是关于玩具示例(MNIST，28x28图像)，要么是利用了ImageNet瓶颈层的迁移学习。我将向您展示如何从头开始训练自动编码器，如果您有足够的数据和完全不同于ImageNet组成的照片的数据，您将会这样做。</p><p id="0185" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在之前的一篇文章中，我展示了如何获取天气预报图像并从中创建张量流记录，以使它们为机器学习做好准备。</p><p id="949b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我将展示如何对这些图像执行一项机器学习任务:使用自动编码器从高分辨率快速刷新(HRRR)模型创建雷达反射率“分析”场的简明表示。HRRR的图像是1059x1799，看起来像这样:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/56758635d83cf2f00e22b7f8b0332e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*Ga-UXnREo-tYe51iPEIT5A.png"/></div></figure><h2 id="bb36" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">在Keras中设计自动编码器</h2><p id="1057" class="pw-post-body-paragraph kf kg iq kh b ki md jr kk kl me ju kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">自动编码器由通常如下所示的体系结构组成:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/665ad2f814e509349cfa8851f0425d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*wKE69-fX180Q_gkzYzGbwg.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">自动编码器架构。来源:维基百科上的Chervinskii(<a class="ae lb" href="https://commons.wikimedia.org/wiki/File:Autoencoder_structure.png" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:auto encoder _ structure . png</a>)</p></figure><p id="7ad0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">编码器试图将输入图像表示为一个更小的数字数组。解码器从代码中重建图像。整个模型被训练以最小化输入和输出的差异。</p><p id="8f0b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有几个陷阱需要注意:</p><ol class=""><li id="968e" class="mn mo iq kh b ki kj kl km ko mp ks mq kw mr la ms mt mu mv bi translated">代码(或嵌入)需要比输入小得多。否则，ML模型可以简单地将输入像素复制到代码中。不会有一概而论。</li><li id="fe08" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">具有一百万像素输入的参数数量可能会失控。传统上，减少参数数量的方法是使用卷积层。</li><li id="6063" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">然而，卷积层保留位置信息。所以，从本质上来说，这些层所做的就是让图像越来越模糊。为了超越单纯的模糊，你应该在每个阶段增加卷积滤波器中“通道”的数量。这样，您就可以在各层之间保存信息。</li><li id="060c" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">此外，你至少需要一个密集的连接，在图像的不同部分之间带来“远程连接”。我选择在创建嵌入时这样做。</li><li id="4ea6" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">仔细考虑你是否需要短路层，注意力等。这取决于您正在处理的图像类型和层数。在我的例子中，这些是带有很强的位置约束的地理图像。所以，我决定不使用这些伎俩。</li><li id="bc15" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">减小卷积滤波器大小的方法是使用池层。在解码器中，相反的操作是进行上采样。</li></ol><p id="fc6e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是我最后的建筑:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nb"><img src="../Images/d3688ef4b15ee3a42bd560aa665e14c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eLCBGfRbJOVPbbdvlIb8Gw.png"/></div></div></figure><p id="6ae2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们用代码浏览一下(<a class="ae lb" href="https://github.com/GoogleCloudPlatform/ml-design-patterns/tree/master/02_data_representation/weather_search" rel="noopener ugc nofollow" target="_blank">完整代码在GitHub </a>上)。第一层是输入层:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="1c2c" class="lk ll iq nh b gy nl nm l nn no">input_img = tf.keras.Input(shape=(1059, 1799, 1), name='refc_input')</span></pre><p id="16e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我更喜欢大小是2的幂，这样更容易得到更小的层。我可以将1059一直填充到2048，但裁剪到1024似乎更合理——无论如何，HRRR图像的边缘分辨率都很差。因此，第二层是一个裁剪层:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="6fc6" class="lk ll iq nh b gy nl nm l nn no">x = tf.keras.layers.Cropping2D(cropping=((17, 18),(4, 3)), name='cropped')(input_img)</span></pre><p id="7e4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们不知道需要多少个卷积层，所以让层数(nlayers)成为一个超参数。此外，我们将使过滤器的数量(或通道的数量)成为超参数，因为这控制了从一层到下一层传递多少信息。最后，poolsize也将是一个超参数，因为它控制着图像大小从一层到下一层的缩减量:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="0a96" class="lk ll iq nh b gy nl nm l nn no">    last_pool_layer = None<br/>    for layerno in range(<strong class="nh ir">nlayers</strong>):<br/>        x = tf.keras.layers.Conv2D(2**(layerno + <strong class="nh ir">numfilters</strong>), poolsize, activation='relu', padding='same', name='encoder_conv_{}'.format(layerno))(x)<br/>        last_pool_layer = tf.keras.layers.MaxPooling2D(<strong class="nh ir">poolsize</strong>, padding='same', name='encoder_pool_{}'.format(layerno))<br/>        x = last_pool_layer(x)<br/>    output_shape = last_pool_layer.output_shape[1:]</span></pre><p id="fc9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我还确保捕捉最后一层，以便在嵌入层之前获得输出形状(原因将变得明显)。</p><p id="d26e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦卷积层完成，我们可以让它通过一个密集层。我还将密集节点的数量(本质上是嵌入的长度)作为一个超参数:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="a4e3" class="lk ll iq nh b gy nl nm l nn no">        # flatten, send through dense layer to create the embedding<br/>        x = tf.keras.layers.Flatten(name='encoder_flatten')(x)<br/>        x = tf.keras.layers.Dense(<strong class="nh ir">num_dense</strong>, name='refc_embedding')(x)<br/>        x = tf.keras.layers.Dense(output_shape[0] * output_shape[1] * output_shape[2], name='decoder_dense')(x)<br/>        embed_size = num_dense</span></pre><p id="7ad3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我是如何使用输出形状预嵌入来恢复解码器第一层中的原始展平长度的。</p><p id="0ada" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们一个接一个地颠倒操作。首先是对解码器进行整形，以获得最后一个conv池块的输出形状:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="514f" class="lk ll iq nh b gy nl nm l nn no">x = tf.keras.layers.Reshape(output_shape, name='decoder_reshape')(x)</span></pre><p id="2a9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，创建一个解码器模块，由长度相反的卷积和上采样组成:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="a003" class="lk ll iq nh b gy nl nm l nn no">for layerno in range(nlayers):<br/>        x = tf.keras.layers.Conv2D(2**(nlayers-layerno-1 + numfilters), poolsize, activation='relu', padding='same', name='decoder_conv_{}'.format(layerno))(x)<br/>        x = tf.keras.layers.UpSampling2D(poolsize, name='decoder_upsamp_{}'.format(layerno))(x)<br/>    before_padding_layer = tf.keras.layers.Conv2D(1, 3, activation='relu', padding='same', name='before_padding')<br/>    x = before_padding_layer(x)<br/>    htdiff = 1059 - before_padding_layer.output_shape[1]<br/>    wddiff = 1799 - before_padding_layer.output_shape[2]</span></pre><p id="1297" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在编码器中进行裁剪的地方，现在需要进行零填充:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="965f" class="lk ll iq nh b gy nl nm l nn no">before_padding_layer = tf.keras.layers.Conv2D(1, 3, activation='relu', padding='same', name='before_padding')<br/>    x = before_padding_layer(x)<br/>    htdiff = 1059 - before_padding_layer.output_shape[1]<br/>    wddiff = 1799 - before_padding_layer.output_shape[2]<br/>    decoded = tf.keras.layers.ZeroPadding2D(padding=((htdiff//2,htdiff - htdiff//2),<br/>                                                     (wddiff//2,wddiff - wddiff//2)), name='refc_reconstructed')(x)</span></pre><p id="b260" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，创建自动编码器:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="07ca" class="lk ll iq nh b gy nl nm l nn no">autoencoder = tf.keras.Model(input_img, decoded, name='autoencoder')<br/>autoencoder.compile(optimizer='adam',loss=tf.keras.losses.LogCosh())</span></pre><p id="faf1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为什么是LogCosh？因为我们做的是回归，而LogCosh对异常值的容忍度比均方差高。</p><h2 id="6dc2" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">编写培训师</h2><p id="e13f" class="pw-post-body-paragraph kf kg iq kh b ki md jr kk kl me ju kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">一旦模型写好了，培训师(<a class="ae lb" href="https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/weather_search/wxsearch/train_autoencoder.py" rel="noopener ugc nofollow" target="_blank">完整代码</a>:检查一下)就成了样板。我们所有的图像都是TFRecord，所以我们创建一个TF record数据集，创建模型并调用model.fit。</p><p id="9f46" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">创建TFRecord数据集:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="c2a6" class="lk ll iq nh b gy nl nm l nn no">def parse_tfrecord(example_data):<br/>    parsed = tf.io.parse_single_example(example_data, {<br/>        'size': tf.io.VarLenFeature(tf.int64),<br/>        'ref': tf.io.VarLenFeature(tf.float32),<br/>        'time': tf.io.FixedLenFeature([], tf.string),<br/>        'valid_time': tf.io.FixedLenFeature([], tf.string)<br/>     })<br/>    parsed['size'] = tf.sparse.to_dense(parsed['size'])<br/>    parsed['ref'] = tf.reshape(tf.sparse.to_dense(parsed['ref']), (1059, 1799))/60. # 0 to 1<br/>    return parsed</span><span id="768d" class="lk ll iq nh b gy np nm l nn no">def read_dataset(pattern):<br/>    filenames = tf.io.gfile.glob(pattern)<br/>    ds = tf.data.TFRecordDataset(filenames, compression_type=None, buffer_size=None, num_parallel_reads=None)<br/>    return ds.prefetch(tf.data.experimental.AUTOTUNE).map(parse_tfrecord)</span></pre><p id="ff5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要创建模型并对其进行训练:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="ede3" class="lk ll iq nh b gy nl nm l nn no">def run_job(opts):<br/>    def input_and_label(rec):<br/>        return rec['ref'], rec['ref']<br/>    ds = read_dataset(opts['input']).map(input_and_label).batch(opts['batch_size']).repeat()<br/>    <br/>    checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(opts['job_dir'], 'checkpoints'))<br/>    <br/>    strategy = tf.distribute.MirroredStrategy()<br/>    with strategy.scope():<br/>        autoencoder, error = create_model(opts['num_layers'], opts['pool_size'], opts['num_filters'], opts['num_dense'])<br/>       <br/>        history = autoencoder.fit(ds, steps_per_epoch=opts['num_steps']//opts['num_checkpoints'],<br/>                              epochs=opts['num_checkpoints'], shuffle=True, callbacks=[checkpoint, HptCallback()])<br/>    <br/>        autoencoder.save(os.path.join(opts['job_dir'], 'savedmodel'))</span></pre><p id="e31a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里有几点:</p><ol class=""><li id="12bb" class="mn mo iq kh b ki kj kl km ko mp ks mq kw mr la ms mt mu mv bi translated">我获取TFRecord并返回与输入和标签相同的反射率图像数据。这是因为我们正在进行自动编码。</li><li id="ad46" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">MirroredStrategy将允许我创建一个具有多个GPU的机器，并获得快速的分布式训练。</li><li id="ab16" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">我创建模型检查点(检查点是<a class="ae lb" href="https://www.amazon.com/Machine-Learning-Design-Patterns-Preparation/dp/1098115783" rel="noopener ugc nofollow" target="_blank">机器学习设计模式书</a>中的设计模式之一；这有助于使分布式培训更有弹性)</li><li id="7e5a" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">在fit()方法中，我处理了steps_per_epoch和epoch的数量。这是检查点模式的虚拟纪元变体。再次，阅读这本书，了解为什么这很重要。</li><li id="43ac" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">因为我们正在进行自动编码，所以我不会为验证数据集而烦恼。我们越能接近地表现输入，就越好。</li></ol><h2 id="9a50" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">批量过装</h2><p id="3dc4" class="pw-post-body-paragraph kf kg iq kh b ki md jr kk kl me ju kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">像我一样设计一个模型架构的最佳实践是确保最终的架构足够强大，能够学习我们需要它学习的东西。做到这一点的方法是有用的过度拟合(我们书中的另一个模式)。基本上，取一个非常小的数据集(我用了4张图片)并在这个数据集上过度拟合模型。如果你能让误差变得非常小，那么这个模型就足够强大了。</p><p id="97f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，我是这么做的。我写了我的ML模型，然后我在超参数空间上做了网格搜索，并选择了最小的嵌入大小，这允许我尽可能完美地学习4幅图像(这不会是完美的，因为图像表示将是模糊的)。</p><p id="75a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，还有一个超参数回调:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="34cf" class="lk ll iq nh b gy nl nm l nn no">class HptCallback(tf.keras.callbacks.Callback):<br/>    def __init__(self):<br/>        self.hpt = hypertune.HyperTune()<br/>    <br/>    def on_epoch_end(self, epoch, logs):<br/>        self.hpt.report_hyperparameter_tuning_metric(<br/>            hyperparameter_metric_tag='final_loss',<br/>            metric_value=logs['loss'],   #history.history['loss'][-1],<br/>            global_step=epoch<br/>        )</span></pre><p id="c471" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我现在可以在谷歌云人工智能平台上进行培训:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="963c" class="lk ll iq nh b gy nl nm l nn no">gcloud ai-platform jobs submit training $JOB_NAME \<br/>        --package-path $PACKAGE_PATH \<br/>        --module-name $MODULE_NAME \<br/>        --job-dir gs://${BUCKET}/wxsearch/trained \<br/>        --region $REGION \<br/>        --config hyperparam.yaml \<br/>        --input gs://${BUCKET}/wxsearch/data/2019/tfrecord-00000-* \<br/>        --project ${PROJECT} \<br/>        --batch_size 4 --num_steps 1000 --num_checkpoints 4</span></pre><p id="a627" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果，我发现这个架构已经足够了:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="568d" class="lk ll iq nh b gy nl nm l nn no">--num_layers 4 --pool_size 4 --num_filters 4 --num_dense 50</span></pre><p id="4dc5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">仅仅50个数字代表100万像素！</p><h2 id="aea8" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">训练自动编码器</h2><p id="3c38" class="pw-post-body-paragraph kf kg iq kh b ki md jr kk kl me ju kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">一旦我们决定了小批量超拟合的超参数，我们就可以采用该模型并在整个HRRR 2019数据集上训练它:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="3b29" class="lk ll iq nh b gy nl nm l nn no">gcloud ai-platform jobs submit training $JOB_NAME \<br/>        --package-path $PACKAGE_PATH \<br/>        --module-name $MODULE_NAME \<br/>        --job-dir gs://${BUCKET}/wxsearch/trained \<br/>        --region $REGION \<br/>        --config train.yaml -- \<br/>       <strong class="nh ir"> --input gs://${BUCKET}/wxsearch/data/2019/tfrecord-* \</strong><br/>        --project ${PROJECT} \<br/>        --batch_size 4 --num_steps <strong class="nh ir">50000</strong> --num_checkpoints 10</span></pre><p id="ac9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">train.yaml在哪里:</p><pre class="ld le lf lg gt ng nh ni nj aw nk bi"><span id="3f18" class="lk ll iq nh b gy nl nm l nn no">trainingInput:<br/>  scaleTier: CUSTOM<br/>  masterType: n1-highmem-2<br/>  masterConfig:<br/>    acceleratorConfig:<br/>      count: 2<br/>      type: NVIDIA_TESLA_K80<br/>  runtimeVersion: '2.2'<br/>  pythonVersion: '3.7'<br/>  scheduling:<br/>    maxWaitTime: 3600s</span></pre><p id="b23a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这以0.0023的最终损失结束，实际上甚至比微小数据集上的过度拟合损失更好。</p><h2 id="1791" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">后续步骤:</h2><p id="7b7b" class="pw-post-body-paragraph kf kg iq kh b ki md jr kk kl me ju kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">试用一下(GitHub的所有链接)</p><ol class=""><li id="25a2" class="mn mo iq kh b ki kj kl km ko mp ks mq kw mr la ms mt mu mv bi translated">从HRRR图像创建张量流记录:<a class="ae lb" href="https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/weather_search/01_hrrr_to_tfrecord.sh" rel="noopener ugc nofollow" target="_blank"> hrrr_to_tfrecord.sh </a></li><li id="dfbb" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">[可选] Overfit batch查找最佳超参数集:overfit_batch.sh</li><li id="2654" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">在2019年HRRR数据集上训练自动编码器:<a class="ae lb" href="https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/weather_search/03_train_autoencoder.sh" rel="noopener ugc nofollow" target="_blank"> train_autoencoder </a>。嘘</li><li id="9a53" class="mn mo iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">在我的<a class="ae lb" href="https://lakshmanok.medium.com/compression-search-interpolation-and-clustering-of-images-using-machine-learning-eb65fcf0abbb" rel="noopener">系列的下一篇文章</a>中，我将展示如何采用训练好的模型，并为整个2019数据集创建嵌入。然后做一个嵌入，告诉你如何从中重建HRRR图像。</li></ol></div></div>    
</body>
</html>