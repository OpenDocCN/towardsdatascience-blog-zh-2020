<html>
<head>
<title>What is the K-Nearest Neighbor?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K近邻是什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-the-k-nearest-neighbor-862a6a30e5dc?source=collection_archive---------38-----------------------#2020-10-29">https://towardsdatascience.com/what-is-the-k-nearest-neighbor-862a6a30e5dc?source=collection_archive---------38-----------------------#2020-10-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4be6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python示例简介</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3a2afac0d22ec5f326a8fcec9fe39063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OUQllif9MkAYjrmu"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔恩·泰森在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="c756" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">k-最近邻算法</h1><p id="e64e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">k-最近邻(KNN)是一种容易理解，但基本的和广泛适用的监督机器学习技术。要理解KNN背后的直觉，看看下面的散点图。该图显示了两个任意维度x和y之间的关系。蓝色点代表A组的成员，橙色点代表b组的成员。这将代表KNN的培训数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/4a4d1eefdaa4f8a067bf4a2f69007ce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgtDULs3idqycLse7G8R_Q.png"/></div></div></figure><p id="3404" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在假设一个新的、未分类的数据点出现并绘制到图表上。KNN将基于K个最近点(或最近邻居)对其进行分类，采取多数投票，并据此进行分类。注意K是事先设定好的，代表投票要拿多少分。</p><p id="80b2" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">例如，如果K= 1，KNN将查看最近的数据点，并将新的数据点归类为相同的类别。在下面的例子中,“X”代表分类的新数据点。因为X最接近组B中的已知数据点，所以“X”也将被分类为组B。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/7396af7aa9224c4102b08b126c0fbc1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ODk6sEDhZsK1Vj52e2kFWQ.png"/></div></div></figure><p id="9f74" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在假设K = 3。KNN将查看3个最近的数据点，并投票进行分类。如果两个或两个以上的最近邻属于一个组，则新的数据点按多数分类。</p><p id="85c9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在下面的例子中，新的数据点“X”移动。在最近的3个点中，2个属于A组，1个属于b组。因为大多数点属于A组，所以新的数据点“X”被分类为A组。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/ce6d63078215ee275a2a2a87bc9c02c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xEjZn9lvJ1HMkKcskSihYQ.png"/></div></div></figure><p id="b51a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">如果出现平局(这可能发生在K=2的情况下)，则从K-1个最近的邻居中取得多数。</p><h1 id="0866" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">KNN的优点</h1><ul class=""><li id="eb02" class="mv mw it lt b lu lv lx ly ma mx me my mi mz mm na nb nc nd bi translated"><strong class="lt iu">非参数:</strong> KNN对基础数据不做任何假设。因此，它可以应用于广泛的问题，而不需要担心数据的属性。</li><li id="94fa" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated"><strong class="lt iu">懒惰学习:</strong>算法没有训练阶段。相反，它会在分类时进行计算。这使得KNN成为一种非常动态的机器学习技术，允许添加额外的数据，而无需重新训练它。</li><li id="f085" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated"><strong class="lt iu">高度非线性数据:</strong>因为没有对数据进行假设，也没有计算正式模型，KNN可以很好地预测高度非线性数据。</li><li id="16e4" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated"><strong class="lt iu">多类问题:</strong>不像其他一些算法需要对涉及2个以上类的分类进行调整，KNN可以推广到任意多的类。</li><li id="7c38" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated"><strong class="lt iu">直观:</strong>即使对于非技术观众来说，该算法也相对容易理解和解释。</li></ul><h1 id="8e65" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">KNN的缺点</h1><ul class=""><li id="e001" class="mv mw it lt b lu lv lx ly ma mx me my mi mz mm na nb nc nd bi translated"><strong class="lt iu">内存密集型:</strong>由于一个新的数据点必须与训练数据中的<em class="nj">每隔一个</em>数据点进行比较，KNN经常使用大量的处理能力来进行分类，尤其是在较大的数据集上。</li><li id="2648" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated"><strong class="lt iu">维度的诅咒:</strong>像其他使用距离作为度量的算法一样，KNN很难预测具有大量输入变量的数据。</li><li id="a5ac" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated">对异常值敏感:异常值给KNN带来了一个根本性的问题。通过简单地选择最近的邻居，不管它们有多远，离群值都可能扭曲它的预测。</li><li id="dc12" class="mv mw it lt b lu ne lx nf ma ng me nh mi ni mm na nb nc nd bi translated"><strong class="lt iu">缺失数据:</strong> KNN没有处理缺失数据的方法。如果缺少任何东西，就不能准确预测整个数据点。</li></ul><h1 id="816f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">选择最佳K</h1><p id="a24a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">因为K是唯一要调整的参数，所以应该非常小心地选择一个好的值。一般来说，有两个基本的建议:一个估计和肘方法。</p><p id="d8d1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">作为一个很好的参考点，有时建议将观察次数的平方根作为K。例如，如果将100个观测值输入KNN，K = 10将作为快速估计。请注意，这更多的是一个经验法则，而不是一个严格的方法。</p><p id="ef30" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">然而，更经验性的方法是肘法。基于边际收益递减的原则，这个想法是在测试数据上运行KNN，逐步增加K值，并查看它如何影响模型性能。</p><p id="2a90" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">如果直观地完成，那么在图的“肘”处(用更专业的术语来说是拐点)，K的优化值表示在成本超过收益之前返回最佳性能的点处。下图中的红圈展示了这一原理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/eb5a29b571a30e3a02aa15a0c5083e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tZSzGfCRX2NtaSGGaMcejw.png"/></div></div></figure><p id="5ebc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">然而，真实世界的数据并不总是那么清晰。如果数据变得更嘈杂，在局部最小值处选择最小的可能K是可行的选择。</p><h1 id="98bc" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Python中的演示</h1><p id="08cd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">与许多其他机器学习算法一样，Scikit-Learn模块提供了一个很好的KNN实现。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="c8a1" class="nq la it nm b gy nr ns l nt nu">from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.preprocessing import StandardScaler<br/>import pandas as pd<br/>import numpy as np</span></pre><p id="204d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">除了KNN模块之外，还导入了StandardScaler来标准化数据，并导入了pandas和numpy来处理数据。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="4560" class="nq la it nm b gy nr ns l nt nu"># Store the data in a dictionary<br/>data = {<br/>    "X1": [1,1,3,4,5,2,0,4,0.5,3.3,1.1,4.7,0.2,2,4.5,3.3,2.5],<br/>    "X2": [1,2,4,4,6,1,1,5,0.5,4.2,1.4,5.2,2,0.03,5.1,4.8,2.5],<br/>    "Member": [A,A,B,B,B,A,A,B,A,B,A,B,A,A,B,B,A]<br/>}</span><span id="ec45" class="nq la it nm b gy nv ns l nt nu"># Convert the data into a dataframe<br/>df = pd.DataFrame.from_dict(data)</span></pre><p id="c84c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">接下来，数据集被生成并放入字典中。这组数字实际上是用来生成本文开头的例子的。为了方便起见，字典然后被转换成数据帧。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="913c" class="nq la it nm b gy nr ns l nt nu"># Separate the the independent and dependent variables<br/>features = df_sample.filter(["X1", "X2"])<br/>category = df_sample["Member"]</span><span id="e61e" class="nq la it nm b gy nv ns l nt nu">scaler = StandardScaler()<br/>scaler.fit(features)<br/>scaled_features = scaler.transform(features)</span></pre><p id="f69c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">因为KNN使用距离作为度量单位，并且因为输入不一定使用相同的比例，所以调用标准缩放器来归一化数值数据。这是防止数据中单位偏差的重要步骤。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="1826" class="nq la it nm b gy nr ns l nt nu">k = 5</span><span id="3401" class="nq la it nm b gy nv ns l nt nu">knn = KNeighborsClassifier(n_neighbors=k)<br/>knn.fit(scaled_features, category)</span></pre><p id="f5ef" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">实际拟合KNN的最后一步只是调用函数，并将缩放后的要素和类别用作参数。请注意参数n_neighbors，它表示要使用多少个K近邻。</p><p id="1a9e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">然而，如果应该使用弯头方法，则需要稍微不同的方法。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="340d" class="nq la it nm b gy nr ns l nt nu"># Separate the data into training and test data sets<br/>X_train, X_test, Y_train, Y_test = train_test_split(scaled_features, color_category, test_size=0.30)</span><span id="45ca" class="nq la it nm b gy nv ns l nt nu"># Import Matplotlib for visualization<br/>import matplotlib.pyplot as plt</span><span id="1f41" class="nq la it nm b gy nv ns l nt nu"># Create an empty list to catch the error rate<br/>error_rate = []</span><span id="c6d1" class="nq la it nm b gy nv ns l nt nu"># Iterate through K = 1-20<br/>for i in range(1,20):<br/>    <br/>    knn = KNeighborsClassifier(n_neighbors=i)<br/>    knn.fit(X_train,Y_train)<br/>    pred_i = knn.predict(X_test)<br/>    error_rate.append(np.mean(pred_i != Y_test))</span><span id="389a" class="nq la it nm b gy nv ns l nt nu"># plot the error rate <br/>plt.figure(figsize=(10,6))<br/>plt.plot(range(1,20),error_rate,color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)<br/>plt.title('Error Rate vs. K Value')<br/>plt.xlabel('K')<br/>plt.ylabel('Error Rate')</span></pre><p id="f7e9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">首先，数据被分成训练和测试子集(这应该是标准的过程)。接下来，针对K = 1、K = 2等等的测试数据对模型进行训练和评估，直到K = 20。最后，结果会以图表的形式返回。</p><h1 id="8d69" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="9d8d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">KNN是一种简单但强大的监督机器学习技术。其稳健的方法允许其应用于各种各样的问题。此外，单个参数K使得参数调整相对容易。Python中一个简单易用的实现使得使用KNN只需几行代码。</p></div></div>    
</body>
</html>