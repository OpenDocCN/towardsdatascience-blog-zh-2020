<html>
<head>
<title>How to convert binary files into TensorFlow records</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将二进制文件转换成张量流记录</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-convert-binary-files-into-tensorflow-records-3150d7236341?source=collection_archive---------36-----------------------#2020-10-09">https://towardsdatascience.com/how-to-convert-binary-files-into-tensorflow-records-3150d7236341?source=collection_archive---------36-----------------------#2020-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="64bd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Apache Beam大规模转换自定义二进制文件</h2></div><p id="c1e6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你有JPEG或者PNG的图片，可以使用<a class="ae lb" href="https://www.tensorflow.org/api_docs/python/tf/io/decode_image" rel="noopener ugc nofollow" target="_blank"> tf.io.decode_image </a>直接读入TensorFlow。如果您的数据是某种特定于行业的二进制格式，该怎么办？</p><h2 id="0961" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">为什么HRRR要做张量流记录？</h2><p id="ea15" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">高分辨率快速更新(HRRR)模式是一个数值天气模式。因为当世界各国汇集它们的观测数据时，天气模型工作得最好，所以天气数据的格式由世界气象组织决定，并且极难改变。所以，HRRR的数据是以#@的方式传播的！$@&amp;=称为GRIB的二进制格式。</p><p id="5a1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不管你在哪个行业——制造、发电、制药研究、基因组学、天文学——你可能都有这样的格式。一种现代软件框架都不支持的格式。尽管这篇文章是关于HRRR的，但是这里的技术将适用于您拥有的任何二进制文件。</p><p id="7b54" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">张量流训练最有效的格式是张量流记录。这是一种protobuf格式，使训练程序能够缓冲、预取和并行读取记录。因此，机器学习的第一步是将特定行业的二进制格式文件转换为TensorFlow记录。</p><h2 id="1d22" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">阅读HRRR文件</h2><p id="6f62" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">阅读HRRR·GRIB文件最简单的方法是使用一个名为cfgrib的软件包。这个包将底层数据作为自描述的numpy数组返回，这些数组被打包成一种称为xarray的格式。要使用cfgrib，您需要一个名为libecccodes0的Linux库，它通常不安装在任何Linux发行版上。非常棘手的事情，但底线是我们需要在bog标准的Google计算引擎虚拟机上进行以下设置:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="c1d2" class="lc ld iq mf b gy mj mk l ml mm">sudo apt-get -y --quiet install libeccodes0<br/>python3 -m pip install -q cfgrib xarray</span></pre><p id="4e23" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们安装了这两个，我们可以读取一个GRIB文件，并提取其中一个感兴趣的图像(在一个文件中有许多预测字段)，如下所示:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="5bc1" class="lc ld iq mf b gy mj mk l ml mm">import xarray as xr<br/>ds = xr.open_dataset(FILENAME, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'atmosphere', 'stepType': 'instant'}})<br/>refc = ds.data_vars['refc']<br/>refc.plot()</span></pre><figure class="ma mb mc md gt mo gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/5b3df14dde345da3fcccc628ac1c90c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*4c1jZqq0cCbbQk9iZuLwfw.png"/></div></figure><h2 id="1db8" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">从云存储中读取</h2><p id="cf66" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">不幸的是，我们使用的Python包只适用于本地文件。它不读取云存储。谷歌云存储上有一个HRRR档案，网址如下:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="1ced" class="lc ld iq mf b gy mj mk l ml mm">gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.*</span></pre><p id="10d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">预测文件非常庞大。我们希望从云存储中即时读取数据，这样我们就不必再为永久磁盘付费了。</p><p id="0962" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要创建一个只处理从云存储中读取的本地文件的库，我们可以使用一个简单的技巧——创建一个临时目录，将文件从云存储中复制到本地磁盘，读取文件，并在完成后删除临时目录。这听起来有点拗口，但实际上很容易做到，因为Python的tempfile和TensorFlow的tf.io.gfile模块完成了所有繁重的工作:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="c920" class="lc ld iq mf b gy mj mk l ml mm">import xarray as xr<br/>import tensorflow as tf<br/>import tempfile<br/>import cfgrib</span><span id="fc99" class="lc ld iq mf b gy mr mk l ml mm"><strong class="mf ir">with tempfile.TemporaryDirectory()</strong> as tmpdirname:<br/>    TMPFILE="{}/read_grib".format(tmpdirname)<br/>    <strong class="mf ir">tf.io.gfile.copy</strong>(FILENAME, TMPFILE, overwrite=True)<br/>    <br/>    ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'atmosphere', 'stepType': 'instant'}})<br/>    refc = ds.data_vars['refc']<br/>    refc.plot()</span></pre><h2 id="f08e" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">转换为张量流记录</h2><p id="91ca" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">我们不希望只是读取数据，我们希望将其转换为TensorFlow记录。为此，我们必须创建一个tf.train.Example:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="bb62" class="lc ld iq mf b gy mj mk l ml mm">refc = ds.data_vars['refc']<br/>size = np.array([ds.data_vars['refc'].sizes['y'],<br/>                 ds.data_vars['refc'].sizes['x']])<br/>tfexample = tf.train.Example(features=tf.train.Features(<br/>      feature={<br/>         'size': tf.train.Feature(int64_list=tf.train.Int64List(value=size)),<br/>         'ref': _array_feature(refc.data),<br/>         'time': _string_feature(str(refc.time.data)[:19]),<br/>         'valid_time': _string_feature(str(refc.valid_time.data)[:19])<br/> }))</span></pre><p id="1aa4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TensorFlow记录只能保存bytes、int64、floats类型的要素。所以，我们必须把其他所有东西都转换成这些类型。它也不了解IEEE类型，如-inf或nan。因此，要将一个2D数组转换成一组浮点数，我们必须做:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="f33d" class="lc ld iq mf b gy mj mk l ml mm">def _array_feature(value):<br/>    if isinstance(value, type(tf.constant(0))): # if value is tensor<br/>        value = value.numpy() # get value of tensor<br/> <br/>    value = np.nan_to_num(value.flatten())<br/>    return tf.train.Feature(float_list=tf.train.FloatList(value=value))</span></pre><h2 id="4a08" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">使用Apache Beam缩放它</h2><p id="5462" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">这很好，但它只是一个文件。对于ML训练，我们需要转换大量的文件。我们需要进行大规模的转换。一个好的方法是使用Apache Beam。管道将如下所示:</p><figure class="ma mb mc md gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/2f33f83b54d52b752f8bbb674ec71bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IJNfql8QN8AGtjzpVJQWtA.png"/></div></div></figure><p id="913f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它应该花费27个小时，但我只花了1个小时，因为管道扩展到了许多机器上(在云上，在一台机器上花费27个小时的管道和在27台机器上花费1个小时的管道成本是相同的)。</p><p id="f077" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的代码是<a class="ae lb" href="https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/weather_search/wxsearch/hrrr_to_tfrecord.py" rel="noopener ugc nofollow" target="_blank">这里是</a>，但它的核心是:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="9ffd" class="lc ld iq mf b gy mj mk l ml mm">p | 'hrrr_files' &gt;&gt; beam.Create(<br/>          <strong class="mf ir">generate_filenames</strong>(options['startdate'], options['enddate']))<br/>  | 'create_tfr' &gt;&gt;<br/>          beam.FlatMap(<strong class="mf ir">create_tfrecord</strong>)<br/>  | 'write_tfr' &gt;&gt; <strong class="mf ir">beam.io.tfrecordio.WriteToTFRecord</strong>(<br/>          os.path.join(options['outdir'], 'tfrecord')))</span></pre><p id="b213" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基本上，第一步是生成我想要转换的所有文件的名称(我可以列出GCS目录，但我想每小时对天气预报进行一次采样，因为天气在15分钟内不会发生太大变化):</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="b8fe" class="lc ld iq mf b gy mj mk l ml mm">def generate_filenames(startdate: str, enddate: str):<br/>    start_dt = datetime.strptime(startdate, '%Y%m%d')<br/>    end_dt = datetime.strptime(enddate, '%Y%m%d')<br/>    logging.info('Hourly records from {} to {}'.format(start_dt, end_dt))<br/>    dt = start_dt<br/>    while dt &lt; end_dt:<br/>        # gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.t04z.wrfsfcf00.grib2<br/>        f = '{}/hrrr.{:4}{:02}{:02}/conus/hrrr.t{:02}z.wrfsfcf00.grib2'.format(<br/>                'gs://high-resolution-rapid-refresh',<br/>                dt.year, dt.month, dt.day, dt.hour)<br/>        dt = dt + timedelta(hours=1)<br/>        yield f</span></pre><p id="959d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二步是创建TF示例，最后一步是将示例写到云存储中。</p><p id="b7e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，还记得我们为了让cfgrid在我们的机器上工作所做的所有恶作剧吗？我们必须告诉Beam runner如何在每个worker节点上安装它需要的包。我们通过指定一个<a class="ae lb" href="https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/weather_search/wxsearch/setup.py" rel="noopener ugc nofollow" target="_blank"> setup.py </a>来实现，并在那里声明:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="26e2" class="lc ld iq mf b gy mj mk l ml mm">CUSTOM_COMMANDS = [<br/>    'apt-get update'.split(),<br/>    'apt-get --assume-yes install libeccodes0'.split()<br/>]</span></pre><p id="2794" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="74d0" class="lc ld iq mf b gy mj mk l ml mm">REQUIRED_PACKAGES = [<br/>    'cfgrib',<br/>    'xarray'<br/>]</span></pre><h2 id="e40e" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">在数据流上运行它</h2><p id="4c1d" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">要在Dataflow上运行上述代码，我们只需运行代码。它将负责提交代码，自动缩放工人，并在完成后关闭一切。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="84e7" class="lc ld iq mf b gy mj mk l ml mm">python3 -m wxsearch.hrrr_to_tfrecord -- \<br/>        --startdate 20190101 --enddate 20200101 \<br/>        --outdir gs://{BUCKET}/wxsearch/data/2019 \<br/>        --project {PROJECT}</span></pre><p id="50eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大约60分钟后，您将获得2019年HRRR反射率文件的相应TF记录。</p><h2 id="be12" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">链接</h2><ol class=""><li id="4d23" class="mx my iq kh b ki lv kl lw ko mz ks na kw nb la nc nd ne nf bi translated">进行转换的代码:<a class="ae lb" href="https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/weather_search/wxsearch/hrrr_to_tfrecord.py" rel="noopener ugc nofollow" target="_blank"> hrrr_to_tfrecord.py </a>。</li><li id="a943" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la nc nd ne nf bi translated">在数据流工作器上安装必要软件的<a class="ae lb" href="https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/weather_search/wxsearch/setup.py" rel="noopener ugc nofollow" target="_blank"> setup.py </a>。</li><li id="63ff" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la nc nd ne nf bi translated">阅读本系列接下来的两篇文章:如何<a class="ae lb" rel="noopener" target="_blank" href="/how-to-create-a-concise-image-representation-using-machine-learning-20156c1e0c19">在HRRR数据上训练自动编码器</a>，以及如何<a class="ae lb" href="https://lakshmanok.medium.com/compression-search-interpolation-and-clustering-of-images-using-machine-learning-eb65fcf0abbb" rel="noopener">使用结果嵌入</a>。</li></ol><p id="f6cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽情享受吧！</p></div></div>    
</body>
</html>