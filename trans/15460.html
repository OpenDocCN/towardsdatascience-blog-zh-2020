<html>
<head>
<title>The Linear Regression Equation in a Nutshell</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单地说，线性回归方程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-linear-regression-equation-in-a-nutshell-8df202501255?source=collection_archive---------15-----------------------#2020-10-24">https://towardsdatascience.com/the-linear-regression-equation-in-a-nutshell-8df202501255?source=collection_archive---------15-----------------------#2020-10-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/aa2f1ceb02cb3d40856ca211391ea881.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BxCmIZohJgedkF8wXvM0Tg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">“回归图前的家伙”使用未画的剪贴画|作者图片</p></figure><div class=""/><p id="5bcb" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">回归到底是什么？回归分析是根据其他一些变量来预测某个变量的值或属性。而线性回归是当只有一个变量，你想根据另一个单一的变量来预测。</p><h1 id="94fe" class="la lb jf bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">定义</strong></h1><p id="41dd" class="pw-post-body-paragraph kc kd jf ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">实际上，我在学校学到的简单线性回归的正式定义是，<em class="md">通过一个变量y对另一个变量x的回归，我们指的是y对x的依赖，平均而言。</em>这是你可以写在考卷上的更多答案。回归的概念实际上要简单得多。</p><p id="1a49" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">回归实际上是寻找图表中数据集的趋势。如果画回归线(流行语预警！)通过<a class="ae me" href="https://en.wikipedia.org/wiki/Scatter_plot" rel="noopener ugc nofollow" target="_blank">散点图</a>，只要看着这条线，你就能判断出某个事物的趋势是上升还是下降！！</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/c9e657d55023766b238e283207fe6a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*bmjCvaZ0BwfJh1-Z31GtFA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者的matplotlib散点图|图片</p></figure><p id="c806" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">你应该知道回归分析是计算和制定直线方程的方法(不要担心我们会得到它)，而回归线就是直线本身。而简单回归的方程是一条线的<a class="ae me" href="https://youtu.be/AqFwKecNaTk" rel="noopener ugc nofollow" target="_blank">方程。</a></p><blockquote class="mk"><p id="71ef" class="ml mm jf bd mn mo mp mq mr ms mt kz dk translated"><em class="mu"> Y = mX + b </em></p></blockquote><h1 id="9b45" class="la lb jf bd lc ld le lf lg lh li lj lk ll mv ln lo lp mw lr ls lt mx lv lw lx bi translated"><strong class="ak">直觉</strong></h1><p id="fbcc" class="pw-post-body-paragraph kc kd jf ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">浏览互联网时，你会发现两种直观的线性回归方法。一个是人们会告诉你回归是一种方法，你可以预测一个变量的值，比如说，y，输入x，你可能已经知道了，这没问题！(这是ML的最佳方法)</p><p id="48d9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">另一种方法是我在定义中定义的。想想看，你可以在散点图上画出成千上万条线，但是最符合并且不同于所有其他线的线就是回归线。讲出来在下一段！</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi my"><img src="../Images/b2c11da5428b67ef9244ea20a62bf65d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*SE6wqpCGKljeBY38FbNIXg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">matplotlib散点图中的各种线条|图片由作者提供</p></figure><p id="3bda" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">预测的方法是正确的，但不是100%有保证，这就是为什么在我的教科书中，在定义的末尾写着:“平均而言”。要知道为什么这是真的，你和我必须深入研究均方差。).现在，把它想象成一个函数，它告诉你这条线工作得有多好。均方差的值越低，线条应该越好。</p><p id="7b65" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">均方误差</strong></p><p id="087e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">正如我前面说过的，平方误差是一个函数，可以知道相对于绘制的数据，线中存在多少误差，线越好，mse的值越小。</p><p id="b278" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在让我们学习如何找到mse，这样你就可以自己计算了。从下面的图中，让我们假设蓝线是回归线，然后我们将试图找到该线与图中(yᵢxᵢ)标为红色的点的垂直距离。</p><p id="debe" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">也就是说，我们从Yᵢ点的实际高度中减去xᵢ点，也就是yᵢ点的线的高度，假设值是eᵢ</p><blockquote class="mk"><p id="7b69" class="ml mm jf bd mn mo mp mq mr ms mt kz dk translated">eᵢ = yᵢ - Yᵢ</p></blockquote><figure class="mz na nb nc nd is gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c6c2d6b9b4cbb60de6ba3e678d6d0471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*S86EcJ5FOQqxkxk5glGR4w.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">matplotlib |图像中直线和点的距离(由作者提供)</p></figure><p id="d55c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在以同样的方式，我们把eᵢ的所有值加起来，</p><blockquote class="mk"><p id="f54f" class="ml mm jf bd mn mo mp mq mr ms mt kz dk translated">∑eᵢ = ∑(yᵢ — Yᵢ)</p></blockquote><p id="9b6e" class="pw-post-body-paragraph kc kd jf ke b kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ni kx ky kz ij bi translated">现在你自己想一想，有些点可能在直线上，直线下可能有更多的点，这可能使总和为负，但惯例决定让mse为正值，以便更好地理解。</p><p id="205a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，如果我们使用<a class="ae me" href="https://youtu.be/r6hS_8nm1jM" rel="noopener ugc nofollow" target="_blank">模数</a>使数值为正，那么随着图中更多的数据点，数值将变得太大。所以伟大的头脑决定在相加之前先求平方值。然后在相加之后，他们建议将总和除以数据集的总计数。因此它被称为均方差。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/5ed3a0bd7ccc7cf0ce6fba51a8d9dbcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*IRJwFA1Gb5a1dMb3qgrBfw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">MSE方程|作者方程</p></figure><p id="6cb0" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">线性回归方程</strong></p><p id="0163" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">正如我在直觉部分所说的，均方差(mse)值越低，这条线就越好，被称为最佳拟合。由于mse本身不可能为0(实际上在大多数情况下不可能，但并非所有情况下都是如此)，我们必须通过简化mse方程来找到mse的最小值。</p><p id="0f5e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">简化mse的方程式后，会是这样的，</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/8335af6d8e3c56338f48b622fe23c1b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YLVaDktCMK8Pjbg0BapNJQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者对均方误差方程的扩展</p></figure><p id="4ec7" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">其中n是存在的数据点总数，r是数据集的<a class="ae me" href="https://www.investopedia.com/terms/c/correlation.asp" rel="noopener ugc nofollow" target="_blank">相关性</a>值，sₓ和sᵧ分别是所有y值和x值的标准偏差，x̅ <strong class="ke jg"> ̄ </strong> &amp; y̅分别是x和y值的平均值。</p><p id="31fc" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，由于r总是位于-1到1之间，r可以导致<strong class="ke jg">sᵧ(1-r)</strong>为0，而sᵧ本身不能为0，这一项的帮助并不能真正方便地制定线性回归方程。</p><p id="d664" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，如果<strong class="ke jg"> (msₓ-rsᵧ) = 0，</strong>那么<strong class="ke jg"> m = r(sᵧ/sₓ) </strong>，其中m也可以写成<strong class="ke jg"> mᵧₓ </strong>(听起来像m在x上的y)。<br/>现在，如果<strong class="ke jg"> (y̅ — b — mx̅̄) = 0 </strong>，那么，<strong class="ke jg"> y̅ = b + mx̅ </strong>，或者<strong class="ke jg"> b = y̅ — mᵧₓx̅̄ </strong>，其中b也可以写成bᵧₓ(听起来像b在x上的y)。</p><p id="5ef5" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，如果你把bᵧₓ和mᵧₓ的值放入我们在定义中讨论过的主要线性方程中，它看起来会像这样，</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/b867c7cf487e00b2347aad2058a8391b.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*GtSgPcaVoj-ltpBRjDwfHg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者推导回归方程|方程</p></figure><p id="a626" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">是的，我知道这对于一个方程来说看起来很难看，但是你不需要每次都记住它。请记住，斜率只是相关乘以标准差的比率，截距看起来像旧的线性方程，其中x和Y分别是平均值，x被视为xi，因为Y将是x输入线的未知数。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi my"><img src="../Images/41f2669b758c1af228c687774ba87369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*rUTjak8CBnlOQSDOVJjMHQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">matplotlib |作者图片中的最佳拟合线</p></figure><p id="f335" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是回归线最终外观的可视化表示。如果你输入一个在计算中没有用到的值，你会得到y的预测值，它会像红点一样出现在这条线上。</p><h1 id="5440" class="la lb jf bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">重要提示</strong></h1><p id="c432" class="pw-post-body-paragraph kc kd jf ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">这是大多数回归新手会犯的错误，仔细看我用了<strong class="ke jg"> mᵧₓ </strong>和<strong class="ke jg"> bᵧₓ </strong>这是y在x上而不是x在y上，因为这意味着我们将用x的值作为函数输入来预测y的值。</p><p id="1f13" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">只有在这种情况下，我才把mse的等式表述为<strong class="ke jg"> yᵢ — Yᵢ </strong>。但是，如果你想在某一天根据输入的y来预测x的值，你必须用<strong class="ke jg">xᵢ——xᵢ</strong>来计算mse，因为如果你仔细想想，某个数据点的水平距离与它到直线的垂直距离有很大的不同。</p><p id="c675" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">那么斜率就会变成<strong class="ke jg"> mₓᵧ = r(sᵧ/sₓ) </strong>(仔细看标准差的比值是mᵧₓ值的倒数，现在听起来是x对y)。截距将变成<strong class="ke jg">b =x̅̄——mₓᵧy̅</strong>,最终等式将是这样的，<strong class="ke jg"> X = mₓᵧyᵢ + bₓᵧ </strong></p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi my"><img src="../Images/429144c3e2036097f3661a7ca2eb9797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*qoa67VTDfFfUHtst3ffgCw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">matplotlib | Image中y对x和x对y的回归方程</p></figure><p id="8d9c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这条蓝线是您想要基于x预测y值的函数的方程，绿线是您想要基于y预测x的函数。这证明没有一条回归线是另一条回归线的反函数<a class="ae me" href="https://youtu.be/W84lObmOp8M" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="9120" class="la lb jf bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">结论</h1><p id="9d3d" class="pw-post-body-paragraph kc kd jf ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">线性回归通常用于数据科学家领域，作为一种机器学习模型，主要作为一些分析领域的统计工具。如果你已经仔细阅读，尽管这篇文章有多长，线性回归背后的数学太简单了。在那件事上你必须支持我！</p></div></div>    
</body>
</html>