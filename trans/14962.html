<html>
<head>
<title>Concise Guide To Unsupervised Learning With Clustering!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聚类无监督学习简明指南！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/concise-guide-to-unsupervised-learning-with-clustering-4924cdbb27cb?source=collection_archive---------40-----------------------#2020-10-14">https://towardsdatascience.com/concise-guide-to-unsupervised-learning-with-clustering-4924cdbb27cb?source=collection_archive---------40-----------------------#2020-10-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cc27" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">借助聚类算法详细理解无监督学习的概念。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/21f7986e7fb2abd1316406765db3e07a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yso28Wiw4Ga5IeUe"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">威廉·艾文在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="f905" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习任务通常有一些数据集，其中我们有一些参数，对于那些结果参数，我们有它们各自的输出。从这些数据集中，我们建立的机器学习模型可以预测类似数据的结果。这个过程就是在<strong class="lb iu">监督学习中发生的。</strong></p><p id="1e5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">监督学习的一个例子是用于确定患者是否出现肿瘤。我们有一个大型数据集，其中有一组与他们各自的结果相匹配的患者参数。我们可以假设这是一个简单的分类任务，1代表肿瘤，0代表无肿瘤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/92696adc5f20749c8389c640b5e293c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sUFXO9as_rq7Bbhv"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@tranmautritam?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Tran Mau Tri Tam </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="c7ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，假设我们有一个关于狗和猫的数据集。没有预先训练好的结果让我们来判断他们中的哪一个是猫还是狗。这种具有未标记数据集的问题可以在<strong class="lb iu">无监督学习</strong>的帮助下解决。用技术术语来说，我们可以将<strong class="lb iu">无监督学习</strong>定义为一种机器学习，在没有预先存在的标签和最少人工监督的情况下，在数据集中寻找以前未检测到的模式。<strong class="lb iu">聚类</strong>和<strong class="lb iu">关联</strong>是两种最重要的无监督学习算法。今天，我们将只关注<strong class="lb iu">集群</strong>。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="9d51" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">聚类:</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/db369a5ee88786b35e5e2a22d2265b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/0*DhIsvB-0ZIipyHiJ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="7aad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用某些数据模式，机器学习算法能够找到相似之处，并将这些数据分组。换句话说，<strong class="lb iu">聚类分析</strong>或<strong class="lb iu">聚类</strong>是对一组对象进行分组的任务，使得同一组中的对象(称为<strong class="lb iu">聚类</strong>)彼此比其他组(聚类)中的对象更相似(在某种意义上)。</p><p id="fe91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在聚类中，我们没有任何预测或标记数据。我们得到了一组输入数据点，使用这些数据点，我们需要找到最相似的匹配，并将它们分组到聚类中。聚类算法有广泛的应用，我们将在以后的章节中讨论。</p><p id="b00d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们分析各种可用的聚类算法。我们将讨论三个最流行和最受欢迎的算法技术。我们还将了解用于无监督学习的性能指标，并最终讨论它们在现实世界中的应用。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="7cd4" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">聚类算法:</h1><p id="b5de" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">有许多聚类算法，但今天我们将主要关注三种最流行和最重要的聚类算法。这些聚类算法是—</p><ol class=""><li id="4523" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">基于质心的聚类(K均值聚类)</li><li id="6b64" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">基于连通性的聚类(层次聚类)</li><li id="02a4" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">基于密度的聚类(DBSCAN)</li></ol><p id="09a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将详细分析每一种算法，并理解它们到底是如何工作的。我们也将看看这些算法的优点和局限性。所以，事不宜迟，让我们开始吧！</p><h2 id="63a8" class="np me it bd mf nq nr dn mj ns nt dp mn li nu nv mp lm nw nx mr lq ny nz mt oa bi translated">1.k均值聚类:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/975d7bda28bf03a4c32f6762c8aaf566.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/0*GrvnnVBk352J_Jii.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="b30a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-means聚类算法是执行聚类分析的最流行的方法之一。在K均值聚类中，用于评估的超参数是“K”</p><p id="1ca6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">‘K’</strong>=聚类数。</p><p id="ebb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">聚类的数量将决定将要执行的聚类分类的类型。在上图中，我们可以假设选择的K值为3。K值将决定分离和分组过程中考虑的中心数量。</p><p id="c199" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数K的“正确”值可以用网格搜索或随机搜索等方法来确定。因此，我们可以说，K-means的主要目标是找到最佳质心，并相应地以适合特定数据集的方式对聚类进行分组。</p><p id="dd41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K均值聚类要遵循的步骤—</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="3db5" class="np me it od b gy oh oi l oj ok">1. Initialization: Randomly picking the n points from the dataset and initialize them. Choose the K value as well.<br/>2. Assignment: For each selected point find the nearest centroid values. <br/>3. Update: Re-compute the centroid values and update them accordingly. <br/>4. Repetition: Repeat the step 2 and step 3 until you reach convergence. <br/>5. Termination: Upon reaching convergence terminate the program. </span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/154dd4305d21b6e80f17292265676afd.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/0*6daezixk7s3zxi6R.gif"/></div></figure><p id="3f3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图展示了K-means的收敛过程。</p><p id="962f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优势:</strong></p><ol class=""><li id="e0b8" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">实现和执行相对简单。</li><li id="9692" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">有效且高效地处理大型数据集。</li><li id="c38b" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">保证在某一点上收敛。</li></ol><p id="2e8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">局限性:</strong></p><ol class=""><li id="978d" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">选择最佳超参数“k”可能很困难。</li><li id="56db" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">计算高维数据的问题。</li><li id="bde4" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">在存在异常值或噪声的情况下，更容易出错。</li></ol><h2 id="b3c1" class="np me it bd mf nq nr dn mj ns nt dp mn li nu nv mp lm nw nx mr lq ny nz mt oa bi translated">2.分层聚类:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/c38114edb6b0d5692dff9798d38d4b4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/0*GBy8cqFIc6Q8jG7h.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="46aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于连通性的聚类，也称为层次聚类，其核心思想是对象与附近的对象比与更远的对象更相关。</p><p id="13f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">层次聚类的概念基本上是将相似的事物从较大的块分组到较小的块，反之亦然。当我们看下面两种类型的层次聚类方法时，可以更好地理解这一点:</p><ul class=""><li id="1e7a" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ol nh ni nj bi translated">凝聚聚类—这是一种“自下而上”的方法:每个观察从自己的聚类开始，随着一个观察在层次结构中向上移动，聚类对被合并。</li><li id="9032" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ol nh ni nj bi translated">分裂聚类——这是一种“自上而下”的方法:所有的观察都从一个聚类开始，随着一个聚类向下移动，分裂被递归地执行。</li></ul><p id="a533" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">凝聚聚类通常优于分裂聚类。因此，我们将进一步研究凝聚性集群而不是分裂性集群的分析。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/d4c23bc60c1daa4970a55cd1cec8991f.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/0*FM4SXGdmvwzMBhOn.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="16ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的树状图展示了层次聚类的工作原理，特别是凝聚聚类的工作原理。</p><p id="a48f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来理解聚集成簇过程中所涉及的步骤</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="cb46" class="np me it od b gy oh oi l oj ok">1. Compute the <a class="ae ky" href="https://hlab.stanford.edu/brian/proximity_matrix.html" rel="noopener ugc nofollow" target="_blank">proximity matrix</a> which is basically a matrix containing the closest distance of each of the similarities i.e., the inter/intra cluster distances. <br/>2. Consider each point to be a cluster.<br/>3. Repeat the following step for every point. <br/>4. Merge the two closest points. <br/>5. Update the proximity matrix. <br/>6. Continue until only a single cluster remains. </span></pre><p id="7dab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点:</strong></p><ol class=""><li id="24cd" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">查看树状图更容易判断集群的数量。</li><li id="da8b" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">总体上易于实现。</li></ol><p id="8241" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">限制:</strong></p><ol class=""><li id="eb73" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">对异常值非常敏感。</li><li id="f261" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">不适合较大的数据集。</li><li id="2e74" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">它具有很高的时间复杂度，这对于某些应用来说是不理想的。</li></ol><h2 id="ae39" class="np me it bd mf nq nr dn mj ns nt dp mn li nu nv mp lm nw nx mr lq ny nz mt oa bi translated">3.<strong class="ak"> DBSCAN: </strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a3432a45c691d3c35a81bd38050e3f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/0*2Aqf24Dk0KehW4Ti.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="cd2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DBSCAN代表基于密度的带噪声应用程序空间聚类，并且越来越受欢迎。在DBSCAN中，我们为密度高于数据集其余部分的区域创建聚类。需要分离聚类的稀疏区域中的对象通常被认为是噪声和边界点。</p><p id="db7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DBSCAN在最小点(或MinPts)和ε中使用了两个重要的超参数。在我们查看如何解决这些问题的步骤之前，让我们分析一下这些超参数到底是如何工作的。</p><ul class=""><li id="4fa6" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ol nh ni nj bi translated"><strong class="lb iu"> MinPts: </strong>数据集越大，MinPts的值应该选得越大。minPts必须至少选择3个。</li><li id="9c5e" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ol nh ni nj bi translated"><strong class="lb iu">ε'ϵ':</strong>然后可以通过使用k-距离图来选择ϵ的值，绘制到k = minPts最近邻居的距离。ϵ的好值是图中显示一个像肘形的强弯曲的地方。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/0be6e1ba36b008299c49b38a994d7035.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*JoqS_R3qAh8KtWrj.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:维基百科</p></figure><p id="bd55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于DBSCAN算法的实现，我们需要遵循的逐步过程如下所述:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="ac8f" class="np me it od b gy oh oi l oj ok">1. Find the points in the ε (eps) neighborhood of every point, and identify the core points with more than minPts neighbors.<br/>2. Label each of the selected points as a core point or border point or noise point. This initial labeling is an important step for the overall functionality.<br/>3. Remove all the noise points from your data because sparse regions do not belong to any clusters. <br/>4. for each core point 'p' not assigned to a cluster create a loop as follows - <br/>   a. Create a new cluster with the point 'p'. <br/>   b. Add all points that are density connected to into this newly created cluster. <br/>5. Assign each non-core point to a nearby cluster if the cluster is an ε (eps) neighbor, otherwise assign it to noise. Repeat the procedure until convergence is reached. </span></pre><p id="cd78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点:</strong></p><ol class=""><li id="414a" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">基于密度的聚类方法对噪声和异常值有很强的抵抗力。</li><li id="f10b" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">它通常适用于任何形状，不像前面提到的两种算法在处理非球形(非凸形)形状时有困难。</li><li id="8a40" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">与K-means不同，它们对要设置的聚类数没有特定的要求。</li></ol><p id="2a1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">限制:</strong></p><ol class=""><li id="9f15" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">DBSCAN不能很好地对密度差异较大的数据集进行聚类。</li><li id="ca45" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">它不是完全确定的，并且在高维数据集上容易出错。</li><li id="bf49" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">由于它有两个可变的超参数，因此容易受到它们的变化的影响。</li></ol></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="24a5" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">绩效指标:</h1><p id="0479" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">监督学习中使用的性能指标，如AUC(ROC曲线下面积)或ROC(接收器操作特性)曲线，不适用于无监督学习。因此，为了评估无监督学习的性能度量，我们需要计算一些参数，如类内和类间参数。看看下面的参考图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/47d8e1ac7566cf74716d008bb00f4b6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*DxTNNToe_nVGsbtGONED8Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="fd8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">类内:</strong>属于同一类的两个相似数据点之间的距离。</p><p id="19dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">类间:</strong>属于不同类的两个相异数据点之间的距离。</p><p id="91b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">任何好的聚类算法的主要目标是减少类内距离和最大化类间距离。用于聚类的主要性能指标之一是<strong class="lb iu">邓恩指数</strong>参数。</p><p id="4312" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">邓恩指数:</strong>邓恩指数旨在识别密集且分离良好的集群。它被定义为最小类间距离与最大类内距离之比。对于每个聚类分区，邓恩指数可以通过以下公式计算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/a18bd1d88f54eef589c0bb8852ff1cd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*MaJ2qNZAKMVaSoma5MFuXQ.png"/></div></div></figure><p id="7816" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比率中的第一行试图最小化类间距离，第二部分试图增加类内距离。由于内部标准寻找具有高的组内相似性和低的组间相似性的组，所以产生具有高Dunn指数的组的算法是更理想的。</p><p id="c8fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果有人对这种说法感到困惑，请将邓恩指数“D”视为衡量两个参数最坏情况的参数，因此当“D”较高时，它被视为理想的聚类。也有其他性能指标可供选择，如戴维斯–波尔丁指数，但在大多数情况下，邓恩指数通常更受青睐。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="f7f6" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">聚类的应用:</h1><ol class=""><li id="5564" class="nb nc it lb b lc mw lf mx li or lm os lq ot lu ng nh ni nj bi translated"><strong class="lb iu">数据挖掘:</strong>从现有数据集中提取有用的数据元素。聚类算法可用于选择对任务有用的数据组，其余的数据可以忽略。</li><li id="dfaf" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">模式识别:</strong>我们还可以利用聚类算法找出对象之间的明显模式。<strong class="lb iu">模式识别</strong>是自动识别数据中的模式和规律。</li><li id="dc9d" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">图像分析:</strong>可以将相似性和相似类型的图像分组到一起，得到想要的结果。这方面的一个例子是猫和狗的隔离，这在前面的章节中已经提到过。</li><li id="c123" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">信息检索:</strong>信息检索是从资源集合中获取与信息需求相关的信息系统资源的活动。搜索可以基于全文或其他基于内容的索引。聚类可用于类似的自然语言处理任务，以获得选定的重复模式。</li><li id="102a" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">生物医学应用和生物信息学:</strong>聚类在分析医学数据和扫描以确定模式和匹配的领域中极其有益。这方面的一个例子是<strong class="lb iu"> IMRT分割，</strong>聚类可用于将注量图分成不同的区域，以便在基于MLC的放射治疗中转换成可实施的射野。</li><li id="c6b8" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">异常检测:</strong>聚类是推断模式和检测异常值存在的最佳方式之一，通过将相似的组分组为聚类，同时忽略异常值，即数据集中存在的不必要的噪声信息。</li><li id="ce18" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">机器人学:</strong>机器人学领域以跨学科的方式利用上述所有学科的集群。机器人需要在没有标记数据的情况下自己寻找模式，聚类会有很大帮助。它们还用于机器人情境感知，以跟踪物体和检测传感器数据中的异常值。</li><li id="685a" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">电子商务:</strong>我们要讨论的最后一个但肯定不是最不重要的应用是电子商务。聚类广泛用于市场营销和电子商务，以确定客户在其业务中的规格。这些独特的模式允许这些公司决定向他们特定的客户销售什么产品。</li></ol><p id="ca04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">集群还有很多应用，我强烈推荐大家去看看在集群领域可以利用的各种应用。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="1a36" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">结论:</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/63bd3ccfff6f73c1645d0fb9ca218e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*InRnam31lb8qJdDx.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1c25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">聚类是机器学习的一个极其重要的概念，可以用于各种任务。它在数据挖掘过程和探索性数据分析的初始阶段也非常有用。上图是对数据应用聚类后使用TSNE构建的图表图像。</p><p id="873d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上述三种算法的帮助下，您应该能够非常容易地解决大多数集群任务。提到的技术和应用让我们简要了解为什么聚类算法有助于对未标记数据集进行分类。我希望这篇文章有助于解释这些概念。</p><p id="320c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看看这些你可能感兴趣的最近的文章吧！</p><div class="ov ow gp gr ox oy"><a rel="noopener follow" target="_blank" href="/natural-language-processing-made-simpler-with-4-basic-regular-expression-operators-5002342cbac1"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd iu gy z fp pd fr fs pe fu fw is bi translated">4个基本正则表达式操作符使自然语言处理变得更简单！</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">了解四种基本的常规操作，以清理几乎任何类型的可用数据。</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">towardsdatascience.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm ks oy"/></div></div></a></div><div class="ov ow gp gr ox oy"><a rel="noopener follow" target="_blank" href="/5-best-python-project-ideas-with-full-code-snippets-and-useful-links-d9dc2846a0c5"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd iu gy z fp pd fr fs pe fu fw is bi translated">带有完整代码片段和有用链接的5个最佳Python项目创意！</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">为Python和机器学习创建一份令人敬畏的简历的5个最佳项目想法的代码片段和示例！</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">towardsdatascience.com</p></div></div><div class="ph l"><div class="pn l pj pk pl ph pm ks oy"/></div></div></a></div><div class="ov ow gp gr ox oy"><a rel="noopener follow" target="_blank" href="/artificial-intelligence-is-the-key-to-crack-the-mysteries-of-the-universe-heres-why-56c208d35b62"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd iu gy z fp pd fr fs pe fu fw is bi translated">人工智能是破解宇宙奥秘的关键，下面是原因！</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">人工智能、数据科学和深度学习的工具是否先进到足以破解人类大脑的秘密</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">towardsdatascience.com</p></div></div><div class="ph l"><div class="po l pj pk pl ph pm ks oy"/></div></div></a></div><div class="ov ow gp gr ox oy"><a rel="noopener follow" target="_blank" href="/opencv-complete-beginners-guide-to-master-the-basics-of-computer-vision-with-code-4a1cd0c687f9"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd iu gy z fp pd fr fs pe fu fw is bi translated">OpenCV:用代码掌握计算机视觉基础的完全初学者指南！</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">包含代码的教程，用于掌握计算机视觉的所有重要概念，以及如何使用OpenCV实现它们</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">towardsdatascience.com</p></div></div><div class="ph l"><div class="pp l pj pk pl ph pm ks oy"/></div></div></a></div><div class="ov ow gp gr ox oy"><a rel="noopener follow" target="_blank" href="/lost-in-a-dense-forest-intuition-on-sparsity-in-machine-learning-with-simple-code-2b44ea7b07b0"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd iu gy z fp pd fr fs pe fu fw is bi translated">迷失在密林中:用简单的代码对机器学习中稀疏性的直觉！</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">为什么ML需要稀疏性？理解稀疏性的核心概念。</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">towardsdatascience.com</p></div></div><div class="ph l"><div class="pq l pj pk pl ph pm ks oy"/></div></div></a></div><p id="e97e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谢谢你们坚持到最后。我希望你喜欢读这篇文章。祝大家有美好的一天！</p></div></div>    
</body>
</html>