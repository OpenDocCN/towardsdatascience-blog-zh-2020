<html>
<head>
<title>Clustering using k-Means with implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于k-Means的聚类算法及其实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/clustering-using-k-means-with-implementation-40988620a973?source=collection_archive---------42-----------------------#2020-09-21">https://towardsdatascience.com/clustering-using-k-means-with-implementation-40988620a973?source=collection_archive---------42-----------------------#2020-09-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3fe5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们周围的物体来自自然群体</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c07bd57377fdee131faccf2c7d2e295d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eCk40sElBHbhDBWv"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:<a class="ae kv" href="https://unsplash.com/@nauleyco" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/@nauleyco</a></p></figure><p id="6c84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> C </span>聚类是一种在数据中寻找自然群体的技术。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mb"><img src="../Images/90fbef60a93d4bc7ebac76c9942733bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*2odPUyJEoUi42o7vWvAkNA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:成群的动物(来源:作者)</p></figure><p id="54d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们给一个孩子看上面的图片，他可以识别出有四种动物。他可能不知道他们所有人的名字，但他仍然可以识别出有四种不同的类型，他可以独立完成这项工作，不需要成年人的帮助。<strong class="ky ir">由于我们不需要一个成年人来监督，聚类是一种无人监督的技术</strong>。</p><h1 id="f46f" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">集群的主要动机是什么？</h1><p id="f995" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">这三个动机可以列举如下。</p><p id="23f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">底层结构:</strong>我们可以理解有不同的组，组内的差异较小，组间的差异较小。</p><p id="d07f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">自然分类:</strong>我们知道动物有四种不同的类型或类群。</p><p id="7a80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">总结</strong>:如果我们现在为它们的住所或食物想出一些策略，我不需要为所有的动物考虑，因为一只羊的食物需求与熊猫、长颈鹿或Rhyno不同，但与另一只羊没有太大的不同。当任何公司试图提出一个客户群特定的策略时，这可能是非常重要的。</p><p id="00a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">k-means可以说是最流行的算法，它将对象分成k个组。这有许多应用，因为我们想在数据中找到结构。我们希望将学生、客户、患者、产品、电影、推文、图像等等分组。</p><p id="434b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们看看它是如何工作的。</p><h1 id="feee" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">k-means如何工作</h1><p id="bd39" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">输入:将给出数据和“k”的值</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/8eba81476680ee143e6f716cf8bf796b.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*CQXPlRzvBmZQm5eH78-oLw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据:输入到K- Means(图片:作者)</p></figure><p id="ce32" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤1:从数据中初始化随机的“k”个点作为聚类中心，让我们假设k的值是2，选择第1和第4个观察值作为中心。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/831ad0a888a3cf2c0db02dba16049bbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*9MzEh147Wc9OK2-c6Bb9ng.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">随机选择的K (2)点(来源:作者)</p></figure><p id="51c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤2:对于所有的点，找出距离k个聚类中心的距离。可以使用欧几里德距离。下表用作说明。C1和C2列分别表示离中心的距离。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/99c53895d72cc2591df7a5cd5d9d846d.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*E3dPXt4aK3nJOaRPcdCCjQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">距质心的距离计算(来源:作者)</p></figure><p id="4a33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果对于第二次观测，我们想要确定距质心1的距离，计算将如下所示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/a810da208af281bd5113dc957c7773df.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*NWapMHVCPApsbFNzoUqqpQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">欧几里德距离的例子(来源:作者)</p></figure><ul class=""><li id="57b0" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated">请注意，第一个点距中心1的距离为0，第四个点距第二个聚类的距离为0。原因很简单，这些点被选为质心</li><li id="9c19" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">同样对于第三次观察，两个中心的距离是相同的。</li></ul><p id="6da9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤3:将每个点分配给它最近的聚类中心，然后使用算术方法再次计算聚类中心</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/bec99e57139aae6cdaef75a2bb8fbd20.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*X2uOVYlAMMxEkvCs0vlzFw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分配给最近中心的点(来源:作者)</p></figure><p id="52bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于前两个点，它们被分配给聚类1，第三个点被随机分配，最后四个点被分配给聚类2。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/7d2c3ee7147dfa79ad554d55815df7c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*TcKPV0QIgNy2u9-tCp_Q-w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">集群中心(来源:作者)</p></figure><p id="424c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">重复步骤2和步骤3，直到收敛</p><ul class=""><li id="631c" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated">每次迭代之间聚类中心没有变化</li><li id="2e01" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">集群分配的变化非常小</li></ul><blockquote class="nt nu nv"><p id="53c0" class="kw kx nw ky b kz la jr lb lc ld ju le nx lg lh li ny lk ll lm nz lo lp lq lr ij bi translated">本质上，我们试图找到k个最佳代表点。这些是通过取平均值来选择的。因此，我们试图迭代地找到k个最佳代表点。因此得名k-means。</p></blockquote><h1 id="80cc" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">k-means中的一些问题是什么，如何绕过它们？</h1><p id="7fdf" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">k-means的一些问题是</p><ul class=""><li id="7d2f" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated">由于第一步是随机初始化，这关系到聚类的质量(其中一些可以通过k-means ++来解决，k-means ++递增地选择随机点，确保下一个点仅在远离所选点时被选择)</li><li id="3db7" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">拾取圆形的圆(可以在一定程度上使用距离度量)</li><li id="2413" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">受到异常值的影响(可以删除异常值，然后应用聚类)</li><li id="4730" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">k的值需要由用户给定(讨论了找到相同值的方法)</li></ul><h1 id="e6e0" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">如何求k的最优值</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/8161c6da57d7e3e4b27574bb6ccf96ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KRyhzaktDBryij_hMNMf4Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">WCSS在平方和聚类内(来源:作者)</p></figure><p id="98f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好的聚类是那些点靠近中心的聚类。怎样才能找到亲近？我们可以找到该簇中所有点离簇中心的欧几里德距离。现在我们需要将此扩展到所有集群。这种想法用上面的等式表示，称为平方的组内和。理想情况下，我们希望它很小。极端的情况是，我们可以对每个点有一个聚类，结果WCSS为0。事实证明，随着k的增加，WCSS继续减小。从技术上讲，这被称为单调递减。</p><p id="fb87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们想停下来，当我们在y轴上画WCSS，在x轴上画星团数量的时候，有一个最陡的下降或形成一种肘形。</p><h1 id="2b1b" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">如何应用k-means</h1><p id="6ae3" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">标准化:所有基于距离的算法都需要对数据进行标准化，这样就不会因为值的范围更大而使某个属性变得过于重要。下面给出了代码片段。</p><pre class="kg kh ki kj gt ob oc od oe aw of bi"><span id="a1cc" class="og md iq oc b gy oh oi l oj ok"># Importing the Standard Scaler<br/>from sklearn.preprocessing import StandardScaler <br/>#Intializing the Stnadard Scaler<br/>scaler = StandardScaler().fit(df) <br/>#Standardizing the data<br/>df_scaled = scaler.transform(df)</span></pre><p id="a072" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">集群:下面给出了代码片段</p><pre class="kg kh ki kj gt ob oc od oe aw of bi"><span id="014b" class="og md iq oc b gy oh oi l oj ok">#Imporing the Library<br/>from sklearn.cluster import KMeans<br/># Intialization<br/>kmeans = KMeans(n_clusters = 3, init = 'random', max_iter = 300, n_init = 10, random_state = 0) <br/>#Applying Clustering<br/>y_kmeans = kmeans.fit_predict(df_scaled)</span></pre><p id="92ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些重要参数:</p><ul class=""><li id="9925" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated">n_clusters:簇的数量或k</li><li id="fce7" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">init: Random或kmeans++(我们已经讨论过kmeans++如何提供更好的初始化)</li><li id="bdb6" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">n_init:算法是用这些不同的初始k点随机选择来运行的。最好的还回来了。</li><li id="ba0e" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">最大迭代次数:算法将运行多少次迭代</li><li id="5261" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">tol:代表公差，也用于终止标准。这是一个有趣的概念。如果从一次迭代到另一次迭代，中心的变化很小，那么我们就趋向于收敛。中心是矩阵，我们可以通过检查元素间的欧几里德距离来找出两个矩阵之间的接近度。这被称为弗罗贝纽斯范数。</li></ul><p id="ba53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">确定“k”的值</p><p id="27d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面给出了代码片段。</p><pre class="kg kh ki kj gt ob oc od oe aw of bi"><span id="87dd" class="og md iq oc b gy oh oi l oj ok"># Applying k-means for diffrent value of k and storing the WCSS<br/>from sklearn.cluster import KMeans<br/>wcss = []<br/>for i in range(1, 11):<br/>    kmeans = KMeans(n_clusters = i, init = 'random', max_iter = 300, n_init = 10, random_state = 0)<br/>    kmeans.fit(x_scaled)<br/>    wcss.append(kmeans.inertia_)</span></pre><p id="17a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用于绘制聚类数</p><pre class="kg kh ki kj gt ob oc od oe aw of bi"><span id="3efd" class="og md iq oc b gy oh oi l oj ok">plt.plot(range(1, 11), wcss)<br/>plt.title('The elbow method')<br/>plt.xlabel('Number of clusters')<br/>plt.ylabel('SSE')      #within cluster sum of squares<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/701f2aa72d6dab003b2a165f8f26293f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*gtRKGDLSudv71DlL7JgpoQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:通过肘图得出的聚类数(来源:作者)</p></figure><ul class=""><li id="8f8d" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated">你可以在这里查看我们的完整实现。</li><li id="f152" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">你也可以在这里看一个关于集群<a class="ae kv" href="https://www.youtube.com/watch?v=FFhmNy0W4tE" rel="noopener ugc nofollow" target="_blank">的视频教程</a></li></ul><p id="d46e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong>:</p><p id="c41f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">k-means因其简单、易于实现而一直存在。在任何调查中，它都很容易出现在ML的前10个算法中。有缺点，也有办法使k-means稳健。</p><p id="74b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">参考:</p><p id="726d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]<a class="ae kv" rel="noopener" target="_blank" href="/k-means-clustering-13430ff3461d">https://towardsdatascience . com/k-means-clustering-13430 ff 3461d</a></p></div></div>    
</body>
</html>