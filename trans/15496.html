<html>
<head>
<title>An introduction to possibility theory</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可能性理论导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-possibility-theory-142f99bf1961?source=collection_archive---------21-----------------------#2020-10-25">https://towardsdatascience.com/an-introduction-to-possibility-theory-142f99bf1961?source=collection_archive---------21-----------------------#2020-10-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d5e9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">可能性理论的基本背景</h2></div><p id="af8e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可能性理论是由扎德[1]提出的，并由Dubois和Prade [2]进一步发展，目的是为语言陈述提供一种定义明确和正式的数学表示，允许处理不精确或模糊的信息。例如，根据每个人对廉价的主观定义和上下文，单词<em class="lb"> cheap </em>可以被赋予一大组值。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/efc866a4ce10be2d2b159547e4e7dfc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KuSHEQrNCICpT8bFeHXzzw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><a class="ae ls" href="https://www.researchgate.net/figure/Theories-embedded-in-evidence-theory-Dempster-Shafer-Theory-sometimes-called-Belief_fig2_221912990" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/figure/Theory-embedded-in-evidence-Theory-demps ter-Shafer-Theory-some-called-Belief _ fig 2 _ 221912990</a></p></figure><p id="a438" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可能性值可以解释为事件发生的可行性的<em class="lb">程度。</em></p><p id="2036" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与概率论的一个重要区别是高可能性值是非信息性的，而高概率值是信息性的。事实上，事件<em class="lb"> A </em>发生的可能性非常高，这意味着，无论<em class="lb"> A </em>发生与否，我们都不会感到意外。如果<em class="lb"> A </em>有非常高的概率质量，那么我们会惊讶于<em class="lb"> A </em>没有发生。相反，低可能性和概率值都是信息性的，因为它们都表明<em class="lb">和</em>不太可能发生。</p><p id="14a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可能性理论也与信念函数理论相关，因为可以证明，如果质量函数是一致的(即，它具有嵌套的焦点元素)，那么它与可能性分布是双射对应的。更一般地说，可能性是一类特殊的不精确概率，在这种框架中，概率值只能用两个界限来表示。事实上，可能性框架中事件的不确定性可以通过一对值(可能性和必要性)更好地提升，这对值可以被视为概率界限。</p><p id="5ff4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可能性理论源于模糊集理论。实际上，假设<strong class="kh ir"> T </strong>是为真的事件<em class="lb">b</em>⊆ω的集合，而<strong class="kh ir"> U </strong>是未判定事件(即既非真也非假的事件)的集合。又假设<strong class="kh ir"> T </strong>和<strong class="kh ir"> U </strong>是模糊的，那么必然性是<strong class="kh ir"> T </strong>的隶属函数，可能性是<strong class="kh ir"> T∪U </strong>的隶属函数。</p><h2 id="fd75" class="lt lu iq bd lv lw lx dn ly lz ma dp mb ko mc md me ks mf mg mh kw mi mj mk ml bi translated">可能性分布</h2><p id="886c" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在可能性理论中，可能性分布是最简单的一类对象，它完全捕捉了我们不确定性的所有信息。可能性分布π将论域中的每个元素映射到单位区间[0，1]，其极值对应于不可能和完全可能的状态。由可能性π导出的具有上下界约束的容许概率分布集合用<strong class="kh ir">p</strong>(π)⊆<strong class="kh ir">p</strong>(ω)表示。</p><p id="1473" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果任何状态<em class="lb">c</em><strong class="kh ir"/>∈<strong class="kh ir"/>ω具有等于1的可能性度，那么这个状态(即这个类)是完全可能的，并且按照惯例，可能性分布π被称为归一化的。不精确的概率解释只有在可能性分布是正态的情况下才是有效的，否则我们将得到<em class="lb">P</em>(ω)&lt;1，用于某些概率分布<em class="lb"> p </em>中的<strong class="kh ir"> P </strong> ( <strong class="kh ir"> π </strong>)。</p><p id="651a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在处理可能性分布时，我们可以区分零确定性和完全确定性的两种特殊情况:</p><p id="71e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.完全确定:∃<em class="lb">a</em>∈ω，π( <em class="lb"> a </em> )=1且π( <em class="lb"> b </em> )=0，∀<em class="lb">a</em>≦<em class="lb">b</em>。</p><p id="d4f9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.零确定性(无知):π(<em class="lb">a</em>)=1∀<em class="lb">a</em>∈ω。</p><h2 id="972f" class="lt lu iq bd lv lw lx dn ly lz ma dp mb ko mc md me ks mf mg mh kw mi mj mk ml bi translated">可能性和必要性措施</h2><p id="ca63" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">可能性理论和其他与不精确概率兼容的理论的一个典型方面是存在两种描述不确定性的度量:必要性和可能性。</p><p id="9cb8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个<em class="lb">必要性度量</em>考虑了根据可用信息对每个事件的合理信任度。</p><p id="147c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相应的<em class="lb">可能性度量</em>评估在没有任何矛盾信息的情况下，人们在多大程度上仍然可以说一个事件是可能的。</p><p id="258c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">必要性和可能性测度分别是不精确概率解释中的下概率和上概率。</p><p id="1015" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定ω的子集<em class="lb"> A </em>，可能性度量由下式给出:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/d909afaed25e1b34dc082e7de3748f77.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/1*4Kw-LxjhhriPkLDCKxY8HA.gif"/></div></figure><p id="eb7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着子集<em class="lb"> A </em>的可能性等于该子集中的最大可能性度。因此，可能性度量是最大的:</p><p id="e521" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">π(A∪B)= max(π(A)，π(B))</em>与概率度量相反，概率度量是求和的。</p><p id="0ad7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，该属性说明了这样一个事实，即可能性分布是计算任何子集的可能性度量的足够信息。</p><p id="43b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">必要性度量由下式给出:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/0801f6848a9b8e4aac473766a051ffd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/1*WDoIQdpSs0tSNvTzbOaBiw.gif"/></div></figure><p id="f9d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">必要性度量是这样的:<em class="lb"> N(A ∩ B) =min(N(A)，N(B)) </em>并且，在归一化的情况下，我们有<em class="lb">π(ω)= n(ω)= 1并且π(∅)=n(∅)= 0</em>。</p><h2 id="27ff" class="lt lu iq bd lv lw lx dn ly lz ma dp mb ko mc md me ks mf mg mh kw mi mj mk ml bi translated">概率-可能性转换</h2><p id="2eb8" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">假设我们正在处理客观概率分布，我们的目标是将它们转换成可能性分布。因此，推荐的选择是Dubois和Prade [3]提出的变换，它保留了概率中包含的统计信息。</p><p id="6e1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑ω上的离散概率分布<em class="lb"> p </em>，我们总是可以置换ω的元素的索引，使得概率值的集合以降序排序:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/ce43d39eac4fc698c67ed7825afa9691.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/1*o7Uo6ivNTw-o0sHEy6e29g.gif"/></div></figure><p id="d710" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">转换内容如下:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/21d7c666b0c96ced88e971e6a770c947.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/1*8vQfVzK2BPwuulBTClLXeA.gif"/></div></figure><p id="77c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种转变是可逆的[4](从这个意义上说，<em class="lb"> p </em>可以从π中恢复过来)。它产生一个标准化的可能性分布。如果<em class="lb"> p </em>是均匀的，那么<em class="lb"> p </em>被映射到一个常数π。如果<em class="lb"> p </em>是狄拉克质量，那么<em class="lb"> p </em>映射到自身。它还有三个重要的特性[5]:</p><ol class=""><li id="f776" class="mv mw iq kh b ki kj kl km ko mx ks my kw mz la na nb nc nd bi translated"><strong class="kh ir">一致性</strong>:∀<em class="lb">a</em>⊆ω，π(<em class="lb">a</em>)≥<em class="lb">p</em>(<em class="lb">a</em>)其中π为π跨越的可能性测度。所以π是一个明确定义的上概率。</li><li id="76c0" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated"><strong class="kh ir">偏好保留</strong> : ∀ ( <em class="lb"> a </em>，<em class="lb">b</em>)∈ω，<em class="lb">p</em>(<em class="lb">a</em>)&gt;<em class="lb">p</em>(<em class="lb">b</em>)⇔π(<em class="lb">a</em>)&gt;π(<em class="lb">b</em>)，所以在由ψ编码的偏好之间存在一种形式的兼容性</li><li id="9363" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated"><strong class="kh ir">最大特异性</strong> : π在那些与<em class="lb"> p </em>一致且保持偏好的可能性分布中达到最大特异性。考虑两种可能性分布π1和π2。如果满足以下条件，则称概率分布π1比π2信息量更大:</li></ol><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/7803dbe0bc9d54a4963a53ca624e6186.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/1*GpjjczsKgwJvY3aYSD01Kg.gif"/></div></figure><h1 id="a04b" class="nk lu iq bd lv nl nm nn ly no np nq mb jw nr jx me jz ns ka mh kc nt kd mk nu bi translated">结论</h1><p id="9edc" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">这是关于可能性理论的简要介绍。如上所述，它是用来处理语言陈述中模糊和不精确的信息的。我们在第一部分提供了一个基本背景，然后我们介绍了可能性分布以及从这些分布中计算出来的可能性和必要性度量。然后我们引入了一个概率-可能性转换，它适合客观概率。就我个人而言，我在关于分类器组合的博士论文中使用了这个框架(可能性框架),其中我使用Tnorm函数组合了可能性分布。</p><p id="ff5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae ls" href="https://ori-nuxeo.univ-lille1.fr/nuxeo/site/esupversions/3c84dfb2-c274-442f-b760-1ab5701fa8b5" rel="noopener ugc nofollow" target="_blank">链接</a>到我的博士手稿:</p><p id="7cfe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae ls" href="https://arxiv.org/pdf/1908.06475.pdf" rel="noopener ugc nofollow" target="_blank">将</a>链接到相关论文:</p><h2 id="8119" class="lt lu iq bd lv lw lx dn ly lz ma dp mb ko mc md me ks mf mg mh kw mi mj mk ml bi translated">参考资料:</h2><p id="8455" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">[1]扎德，洛特菲·阿斯克尔。"模糊集是可能性理论的基础."<em class="lb">模糊集与系统</em>1.1(1978):3–28。</p><p id="4c7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]杜布瓦，迪迪埃和亨利·普拉德。可能性理论:一种计算机处理不确定性的方法。斯普林格科学&amp;商业媒体，2012年。</p><p id="1aee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]杜布瓦，迪迪埃和亨利·普拉德。“在几个不确定的证据主体的陈述。模糊信息和决策过程，167-181，古普塔和桑切斯(1982).</p><p id="1d15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4]杜布瓦、迪迪埃和亨利·普拉德。“可能性理论及其应用:我们站在哪里？."<em class="lb">斯普林格计算智能手册</em>。施普林格，柏林，海德堡，2015。31–60.</p><p id="be5f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5] Dubois，Didier等人，“概率-可能性转换，三角模糊集和概率不等式。”<em class="lb">可靠计算</em>10.4(2004):273–297。</p></div></div>    
</body>
</html>