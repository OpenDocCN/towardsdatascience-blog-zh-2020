<html>
<head>
<title>A practical guide to RNN and LSTM in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">喀拉斯RNN和LSTM实用指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-practical-guide-to-rnn-and-lstm-in-keras-980f176271bc?source=collection_archive---------2-----------------------#2020-10-17">https://towardsdatascience.com/a-practical-guide-to-rnn-and-lstm-in-keras-980f176271bc?source=collection_archive---------2-----------------------#2020-10-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ec44045e25ac9c111bb9acb61d3ac6cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l-5RfvUpyUnIcwCk"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@yogidan2012?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">丹尼尔·利维斯·佩鲁西</a>在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="9392" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">介绍</h1><p id="8c15" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在阅读了大量关于循环层的理论文章后，我只想建立我的第一个LSTM模型，并在一些文本上对其进行训练！但是对于我来说，图层暴露参数的庞大列表和图层结构的精致对我来说太复杂了。这意味着我不得不花费大量的时间来研究StackOverflow和API定义，以获得更清晰的图像。这篇文章试图巩固所有的笔记，它可以加速从理论到实践的转变过程。本指南的目标是培养对使用RNN和LSTM等重现图层的实际理解，而不是提供理论理解。为了更深入的理解，我建议<a class="ae kc" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae kc" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">这个</a>，我建议在阅读本文之前先过一遍。如果你准备好了，让我们开始吧！</p><h1 id="9bc1" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">递归神经网络</h1><p id="1515" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">完整的RNN图层在Keras中显示为<code class="fe lz ma mb mc b">SimpleRNN</code>类。与许多文章中建议的体系结构相反，Keras实现非常不同，但是很简单。每个RNN单元接受一个数据输入和一个隐藏状态，从一个时间步骤传递到下一个步骤。RNN细胞看起来像这样，</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi md"><img src="../Images/7a816557a438ef16a5a329d8c5fbbf0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lZ-vhrS9a41yZF_-FYnW8Q.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">Keras中RNN单元实现内部的数据流和隐藏状态。图片作者。</p></figure><p id="92c2" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">RNN单元的完整公式是，</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/e18697ee493d532fb7751d063aa3978d.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*lDt3TfQOjd9f102f5gORgw.png"/></div></figure><p id="2538" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">这里，h{t}和h{t-1}是从时间t和t-1开始的隐藏状态。x{t}是t时刻的输入，y{t}是t时刻的输出，需要注意的是，有两个权重矩阵W{hh}和W{hx}以及一个偏置项b{h}。这些矩阵中的每一个都可以被认为是一个内部1层神经网络，其输出大小在参数<code class="fe lz ma mb mc b">units</code>中定义，偏差也具有相同的大小。y{t}是原始的h{t}，我们不像许多文章建议的那样在这里应用另一个权重矩阵。这表示RNN的一个单个像元，像元的顺序组合(计数等于数据中的时间步长)创建了完整的RNN层。请记住，RNN单元共享相同的权重矩阵和偏差。最后，我们可以如下计算训练RNN层所需的参数数量，</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/5f1af5a7631b4eb458bc72bdf6b44041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*40KFPgVLs3o7JNKm_q3oWA.png"/></div></figure><p id="0b7e" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">请注意，输入在格式上是一个元组(时间步长、要素),参数仅取决于要素，因为我们在每个时间步长上共享相同的权重。这可以通过在Keras中显示RNN的样本模型的概要来检查。</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mp"><img src="../Images/0915a783c2262b9682f0f8af49e46001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HYhPyKYS4NS9Niv38Zcyfw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">检查simple_rnn_2中的参数，它等于我们上面计算的。使总参数计数达到17921的额外的129是由于在RNN之后添加的密集层。</p></figure><p id="a8c3" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">我们还可以获取精确的矩阵并打印出它的名称和形状，</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mq"><img src="../Images/76555ddc78054b6ced39286f311354d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S7J1pMenDxspP8Of-HwWCg.png"/></div></div></figure><p id="87ee" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">要点注意，Keras调用输入权重为<em class="mr">内核</em>，隐藏矩阵为<em class="mr">递归_内核</em>，偏差为<em class="mr">偏差</em>。现在我们来过一遍Keras曝光的参数。虽然完整的清单是由提供的，但我们将简要地看一些相关的清单。</p><ul class=""><li id="a73e" class="ms mt iq ld b le mi li mj lm mu lq mv lu mw ly mx my mz na bi translated">第一个也是最重要的是<code class="fe lz ma mb mc b">units</code>，它等于<em class="mr">内核</em>和<em class="mr"> recurrent_kernel的输出大小。</em>也就是<em class="mr">偏向项的大小</em>项和隐藏项的大小。</li><li id="740a" class="ms mt iq ld b le nb li nc lm nd lq ne lu nf ly mx my mz na bi translated">接下来，我们有<code class="fe lz ma mb mc b">activation</code>在我们的公式中定义了g()函数。默认为“tanh”。</li><li id="67b7" class="ms mt iq ld b le nb li nc lm nd lq ne lu nf ly mx my mz na bi translated">然后我们为<em class="mr">内核、</em>递归_内核和<em class="mr">偏差</em>分别设置了<code class="fe lz ma mb mc b">{*}_initializer</code>、<code class="fe lz ma mb mc b">{*}_regularizer</code>和<code class="fe lz ma mb mc b">{*}_constraint</code>参数。如果您不确定，可以忽略这些，因为默认值已经足够好了。</li><li id="a6ef" class="ms mt iq ld b le nb li nc lm nd lq ne lu nf ly mx my mz na bi translated"><code class="fe lz ma mb mc b">use_bias</code>是打开或关闭偏置项的布尔参数。</li><li id="ed1c" class="ms mt iq ld b le nb li nc lm nd lq ne lu nf ly mx my mz na bi translated"><code class="fe lz ma mb mc b">dropout</code>和<code class="fe lz ma mb mc b">recurrent_dropout</code>分别用于将丢弃概率应用于<em class="mr">内核</em>和<em class="mr">递归_内核</em>。</li><li id="0cb5" class="ms mt iq ld b le nb li nc lm nd lq ne lu nf ly mx my mz na bi translated"><code class="fe lz ma mb mc b">return_sequence</code>是一个布尔参数。当其为“真”时，RNN图层的输出形状为(时间戳，要素)，当其为“假”时，输出仅为(要素)。这意味着，如果它打开，在输出中我们返回所有时间步的y{t}，如果它关闭，我们只返回1 y{t}(这里是从最后一个时间步)。一个额外的警告，不要忘记在添加<code class="fe lz ma mb mc b">Dense</code>层之前，在打开<code class="fe lz ma mb mc b">return_sequence</code>的RNN之后添加一个<code class="fe lz ma mb mc b">TimeDistributed</code>层或<code class="fe lz ma mb mc b">Flatten</code>层。</li><li id="60d6" class="ms mt iq ld b le nb li nc lm nd lq ne lu nf ly mx my mz na bi translated"><code class="fe lz ma mb mc b">go_backwards</code>为布尔类型，当其为“真”时，RNN以相反的顺序处理数据。默认值为“假”</li><li id="d732" class="ms mt iq ld b le nb li nc lm nd lq ne lu nf ly mx my mz na bi translated"><code class="fe lz ma mb mc b">return_state</code>为布尔型，当“真”时，除输出外，还返回最后一个状态。默认值为“False”。</li><li id="c69f" class="ms mt iq ld b le nb li nc lm nd lq ne lu nf ly mx my mz na bi translated"><code class="fe lz ma mb mc b">stateful</code>是一个重要的参数。当设置为“真”时，Keras对相同的样本索引使用相同的批次隐藏状态。这样理解，我们为多个时期训练我们的模型，这就像对完整数据的迭代。1个历元是完整数据的1次传递。现在，每个时期包含多个批次，这些批次又包含多个样本，即单独的数据。通常，在批量运行每个样品后，RNN池的状态会被重置。但是，如果我们以这样的格式准备数据，使得在多个批次中，特定索引处的样本只是同一个句子的扩展，那么我们可以将stateful转换为“True ”,这相当于一次训练所有句子(作为一个样本)。由于内存限制，如果我们不能一次加载完整数据，我们可能会这样做。默认值为“False”。</li></ul><p id="9f50" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">在对RNN有了基本了解之后，让我们来看看RNN经常创作的一个建筑。</p><h1 id="76d1" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">深垂直RNNs</h1><p id="fa7e" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">已经有人建议将多个循环层堆叠在彼此之上，以便<a class="ae kc" href="https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms" rel="noopener ugc nofollow" target="_blank">更好地为多个应用</a>工作。这导致了网状结构，其中水平深度(想象展开的RNN)是由于时间步长，而垂直副本(堆叠)是由于新的RNNs层。这被称为Seq2Seq建模，主要用于语言翻译、实体标记、语音识别等需要输入和输出序列的应用。也就是说，我们也可以在最终应用完全连接的密集层之前堆叠多个rnn，这是输入序列但输出变平的一个示例。一个示例代码是，</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/0ecae9498ac51cbe3113fc04fbf31057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-s8St2v5BqWlUWNY2a_GdQ.png"/></div></div></figure><p id="6303" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">这非常简单，因为我们刚刚在前面的代码中添加了两个新的RNN层。但是请注意，如果我们想在RNN图层上叠加另一个RNN，我们会将<code class="fe lz ma mb mc b">return_sequence</code>设为“真”。这是因为下一个RNN期望时间分布的输入，并且前一个RNN的每个时间步长的输出成为相同时间步长的上RNN的输入。这里，虽然第一RNN的可训练参数保持与之前建议的相同，但是第二和第三RNN具有不同的参数，因为这些RNN的输入大小是128。这使得接下来的两个RNN中的每一个的训练参数等于，</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d34bb18865daf044bb86be08d940c688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*2C8DNTxjG47da3kuc2OaOA.png"/></div></figure><h1 id="4107" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">LSTM</h1><p id="1010" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">继续看LSTMs，上面有一堆非常好的文章，比如<a class="ae kc" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae kc" rel="noopener" target="_blank" href="/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">这个</a>。我建议在继续前进之前先看一看它们。与RNN的问题类似，LSTM的实现与大多数文章中提出的没有什么不同。主要的区别在于，不是连接输入和先前的隐藏状态，而是在将它们传递到LSTM单元中的4个内部神经网络之前，我们具有应用于两者的不同权重矩阵。这意味着我们将所需的矩阵数量增加了一倍(实际上，它将维度增加了一倍，但稍后会有更多的介绍)。与输入相乘的4个矩阵称为<em class="mr">核</em>，与前一隐藏状态相乘的4个称为<em class="mr">递归_核</em>。为了更好地理解这一点，让我们看看公式，</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/300f113d3a714246070c6b8ec9fc6afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*1I99qpDXfsoR-kZ-1CPQUA.png"/></div></figure><p id="154d" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">在这里，如果您观察，我们总共有8个权重矩阵，假设每个矩阵的大小相同，我们可以说，在某种程度上，我们正在进行与RNN相同的操作，但现在增加了4倍。因此，可训练参数的数量现在可以通过下式计算，</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/c6e3cde8f67fbde15555bba43dfcf4f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SIntOm-XHbN6NRD04fhTKg.png"/></div></div></figure><p id="cf71" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">而从RNN切换到LSTM就像替换各自的函数调用一样简单，这可以从下面的代码中看出，</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/b9630502ea9479f8d976479ef025f8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQvc9mdi8DBhL77aeSDKzA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">将lstm_1中提到的参数与我们计算的参数进行匹配。</p></figure><p id="a22f" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">我们可以再次从模型中提取所有的权重，</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/3f7c0f32670b5a9329c8cf58baba5c71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*JEc-9Fl3Fj0N_tk6_RQnvg.png"/></div></figure><p id="c227" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">这里注意，所有4个<em class="mr">内核</em>矩阵和4个<em class="mr">递归_内核</em>矩阵都存储在1个单块矩阵中(串联在列轴上)，因此维数为128*4=512。偏差项也是如此。此外，几乎所有在RNN使用的参数在这里都适用。一个<a class="ae kc" href="https://stackoverflow.com/questions/44947842/can-someone-explain-to-me-the-difference-between-activation-and-recurrent-activa" rel="noopener ugc nofollow" target="_blank">附加参数警告</a>是<code class="fe lz ma mb mc b">recurrent_activation</code>，其默认值为“sigmoid ”,应用于输入、遗忘和输出门，如公式中所示。这留下了实际的<code class="fe lz ma mb mc b">activation</code>，它应用于单元格状态和隐藏状态(默认值为“tanh”)，如上面公式中所建议的。</p><h1 id="cf69" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结论</h1><p id="28f8" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在Keras中，我们试图涵盖一些将理论和实践联系起来所需的基本主题。作为一个包含所有内在细节的完整指南，对于一篇文章来说太多了，我认为有很多资料可以很好地解释这个主题。我真正错过的是一些注释，这些注释将我在文章中看到的公式与Keras中真正实现的内容联系起来，并提供了一些额外的实用细节。希望这有所帮助！</p><p id="17a9" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated">干杯。</p><p id="41e3" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated"><em class="mr">文章中的所有代码已经被</em> <a class="ae kc" href="https://www.kaggle.com/evilmage93/intro-to-recurrent-layers-in-keras" rel="noopener ugc nofollow" target="_blank"> <em class="mr">上传到这里。</em> </a></p><p id="c045" class="pw-post-body-paragraph lb lc iq ld b le mi lg lh li mj lk ll lm mk lo lp lq ml ls lt lu mm lw lx ly ij bi translated"><em class="mr">更多类似的文章请访问我的</em> <a class="ae kc" href="http://mohitmayank.com" rel="noopener ugc nofollow" target="_blank"> <em class="mr">网站</em> </a> <em class="mr">并联系我@</em><a class="ae kc" href="https://www.linkedin.com/in/imohitmayank/" rel="noopener ugc nofollow" target="_blank"><em class="mr">LinkedIn</em></a><em class="mr">。</em></p></div></div>    
</body>
</html>