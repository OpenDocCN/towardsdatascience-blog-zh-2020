<html>
<head>
<title>How to Apply BERT to Arabic and Other Languages</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将BERT应用于阿拉伯语和其他语言</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787?source=collection_archive---------4-----------------------#2020-10-14">https://towardsdatascience.com/how-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787?source=collection_archive---------4-----------------------#2020-10-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5c1a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">多语言(XLM-罗伯塔)与单语(阿拉伯伯特)的方法，以及机器翻译如何帮助。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/10a6e637f38ea05c06b4450a02d8be6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Y3BBk31cNNthiZnn"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">迪米特里B 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="1c7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，Nick和我一直在使用英语编写几乎专门关于NLP应用程序的教程…虽然通用算法和思想可以扩展到所有语言，但是支持英语NLP的大量资源并不能扩展到所有语言。例如，BERT和类似BERT的模型是一个非常强大的工具，但是模型发布几乎总是用英语，可能后面还有中文、俄文或西欧语言的变体。</p><p id="845b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">出于这个原因，我们将关注一个有趣的类伯特模型类别，称为<strong class="ky ir">多语言模型</strong>，它有助于将大型类伯特模型的功能扩展到英语以外的语言。</p><p id="d45c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">克里斯·麦考密克和尼克·瑞安</p><h1 id="197b" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">1.多语言模型</h1><h1 id="b79a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">1.1.多语言模型方法</h1><p id="37b9" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">多语言模型采用一种相当奇怪的方法来处理多种语言…</p><p id="926c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">多语言模型不是单独处理每种语言，而是根据来自混合语言的文本进行预训练。</p><p id="7a92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章和附带的笔记本中，我们将使用一个来自脸书的名为<strong class="ky ir"> XLM-R </strong>的特定多语言模型(“跨语言语言模型—罗伯塔”的简称)。</p><p id="61ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最初的伯特在英语维基百科和BooksCorpus(自助出版书籍的集合)上接受过预训练，而XLM-R在维基百科和来自100种不同语言的通用抓取数据上接受过预训练！不是100种不同的模型在100种不同的语言上接受训练，而是一个单一的伯特型模型，它是在所有这些文本上预先训练好的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/5bc4bfcd87347eac0aa490769aebc246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aiah_VLeN-6Cwa7a"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="ad31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里真的没有任何东西试图刻意区分语言。例如，在XLM-R:</p><ul class=""><li id="98f1" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">有一个单一的、共享的词汇表(包含25万个令牌)来涵盖所有100种语言。</li><li id="6b89" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">输入文本中没有添加特殊的标记来指示它是什么语言。</li><li id="f6fc" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">它不是用“平行数据”(多种语言的同一句话)训练的。</li><li id="cc4f" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">我们没有修改培训目标来鼓励它学习如何翻译。</li></ul><p id="ea33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，XLM-R并没有预测无意义或对其众多输入语言中的任何一种都只有最起码的理解，而是表现得令人惊讶地好，甚至与用单一语言训练的模型相比也是如此！</p><h1 id="5786" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">1.2.跨语言迁移</h1><p id="e3c2" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">如果您的应用程序是用另一种语言编写的(从现在开始，我们将使用阿拉伯语作为例子)，您可以像使用常规BERT一样使用XLM-R。您可以在您的阿拉伯语培训文本上微调XLM-R，然后使用它来用阿拉伯语进行预测。</p><p id="317d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，XLM-R允许你利用另一种更令人惊讶的技术…</p><p id="ac21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设您正试图构建一个模型来自动识别阿拉伯语中令人讨厌的(或“有毒的”)用户评论。已经有一个很棒的数据集叫做“维基百科有毒评论”,大约有225，000条标签评论——只不过都是英文的！</p><p id="91cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你有什么选择？用阿拉伯语收集类似规模的数据集将会非常昂贵。以某种方式应用机器翻译可能很有趣，但有其局限性(我将在后面的部分中更多地讨论翻译)。</p><p id="423b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">XLM-R提供了另一种称为“跨语言迁移”的途径。你可以在英文的维基百科有毒评论数据集<em class="ls">上微调XLM-R，然后将其应用于阿拉伯语评论</em>！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/6afeb57bbc58dfcb834887cf597d65a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*r0oHQ9zaQ8akcq8e"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="c116" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">XLM-R能够将它在英语中学习到的特定任务知识应用到阿拉伯语中，尽管我们从未向它展示过任何阿拉伯语的例子！这是从一种语言到另一种语言的迁移学习的概念——因此，“跨语言迁移”。</p><p id="c0bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章附带的笔记本中，我们将看到纯粹在大约400k <em class="ls">英语</em>样本上训练XLM-R实际上比在(小得多的)阿拉伯数据集上微调“单语”阿拉伯模型产生更好的<em class="ls">结果。</em></p><blockquote class="ng nh ni"><p id="c7a0" class="kw kx ls ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated"><em class="iq">这个令人印象深刻的壮举被称为</em> <strong class="ky ir"> <em class="iq">零距离学习</em> </strong> <em class="iq">或</em> <strong class="ky ir"> <em class="iq">跨语言迁移</em> </strong> <em class="iq">。</em></p></blockquote><h1 id="dcf3" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">1.3.为什么是多语言？</h1><p id="d397" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">多语言模型和跨语言转移是很酷的技巧，但是如果脸书只是为这些不同的语言中的每一种训练和发布一个单独的模型不是更好吗？</p><p id="eeb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是的，这可能会产生最准确的模型——如果网上每种语言的文本都像英语一样多就好了！</p><blockquote class="ng nh ni"><p id="62be" class="kw kx ls ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated"><em class="iq">只对单一语言的文本进行预训练的模型称为</em> <strong class="ky ir"> <em class="iq">单语</em> </strong> <em class="iq">，而对多种语言的文本进行预训练的模型称为</em> <strong class="ky ir"> <em class="iq">多语言</em> </strong> <em class="iq">。</em></p></blockquote><p id="1d80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的柱状图显示了对于一小部分选择的语言，XLM-R的作者能够为预训练收集多少文本数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/eb1055f1b8654dd15ac99ce6f9638e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rs0ZFUirJ-Q23QBC"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片作者，改编自XLM-R <a class="ae kv" href="https://arxiv.org/pdf/1911.02116.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>图1)</p></figure><p id="eecb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，比例是对数的，因此英语数据大约比阿拉伯语或土耳其语多10倍，比意第绪语多1000倍。</p><h1 id="ba05" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">1.4.按资源划分的语言</h1><p id="00f6" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">不同的语言有不同数量的训练数据可用于创建大型的类似BERT的模型。这些被称为<strong class="ky ir">高</strong>、<strong class="ky ir">中</strong>和<strong class="ky ir">低资源</strong>语言。英语、汉语和俄语等高资源语言在网上有大量免费的文本，可以用作训练数据。因此，NLP研究人员主要集中于开发大型语言模型和这些语言的基准。</p><p id="b6ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我改编了XLM-R <a class="ae kv" href="https://arxiv.org/pdf/1911.02116.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的图1中的柱状图。这是他们的完整条形图，显示了他们为100种语言中的88种语言收集的数据量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/93830f18f78cf3b099278bcde02cda07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hoGRQv87gWTV9Mjm"/></div></div></figure><p id="4b2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些语言用两个字母的ISO代码标注——你可以在<a class="ae kv" href="https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes" rel="noopener ugc nofollow" target="_blank">这里</a>的表格中查找。</p><p id="7ca1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是柱状图中的前十个代码(注意，在德语之后还有另外~10种语言有类似的数据量)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/d4c87af509075c776c2f4b43092bad02.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*wXT-_20AVa4wPoIDJbCyMw.png"/></div></figure><p id="4b05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，这个“数据量”的排名与每种语言在互联网上有多少用户的排名并不匹配。在维基百科上查看这个表格。在柱状图中，中文(代码<code class="fe np nq nr ns b">zh</code>)排在第21位，但迄今为止拥有最多的用户(仅次于英语)。</p><p id="b1b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，NLP研究人员对不同语言付出的努力和关注并不遵循柱状图中的排名——否则汉语和法语会排在前5名。</p><blockquote class="ng nh ni"><p id="51d1" class="kw kx ls ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated"><em class="iq">最近有一个名为</em><a class="ae kv" href="https://oscar-corpus.com/" rel="noopener ugc nofollow" target="_blank"><em class="iq">OSCAR</em></a><em class="iq">的项目，它为不同语言的类似BERT的模型提供了大量的预训练文本——如果你正在寻找未标记的文本来使用你的语言进行预训练，那么绝对值得一试！</em></p></blockquote><h1 id="e302" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">1.5.利用机器翻译</h1><p id="8677" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">也有可能涉及“机器翻译”(自动翻译文本的机器学习模型)，以尝试和帮助解决语言资源有限的问题。这里有两种常见的方法。</p><p id="e353" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">方法1——翻译所有内容</strong></p><p id="65ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以完全依赖英文模型，将应用程序中的任何阿拉伯文本翻译成英文。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/bc67811d1ca9ff049d6e4183fe1d702f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w0XUuEF4E8Lgy1bf"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="0cdb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法与单语模型方法具有相同的问题。最好的翻译工具使用机器学习，对于可用的训练数据也有同样的限制。换句话说，针对中低资源语言的翻译工具还不足以成为我们问题的简单解决方案——目前，像XLM-R这样的多语言BERT模型可能是更好的选择。</p><p id="ed21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">方法#2 —增加训练数据</strong></p><p id="5dc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你的任务中已经存在大量带标签的<em class="ls">英语</em>文本，那么你可以将这些带标签的文本翻译成阿拉伯语，并用它来扩充你可用的<em class="ls">阿拉伯语</em>训练数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/dfc56b96c6ebf9bd193952fa7e60ff8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NkRy4LPJXHxb1BaX"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="6326" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您的语言中有一个不错的单语模型，并且有一个大型英语数据集可用于您的任务，那么这是一个很好的技术。我们在一个附带的笔记本中将这种技术应用于阿拉伯语，它的表现优于XLM-R(至少在我们的初始结果中——我们没有执行严格的基准测试)。</p><h1 id="7b14" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">1.6.XLM R词汇</h1><p id="88cf" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">正如你可能想象的那样，为了适应100种不同的语言，XLM-R拥有与最初的BERT非常不同的词汇。</p><p id="2338" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">XLM R的词汇量为250，000个单词，而伯特的词汇量为30，000个单词。</p><p id="aaa1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我在这里发布了一个笔记本<a class="ae kv" href="https://colab.research.google.com/drive/1M7pDk5bbZh_wB4GMtVjDqVG2l9hCK1Wk" rel="noopener ugc nofollow" target="_blank">,在这里我浏览了XLM-R的词汇，以了解它包含的内容，并收集各种统计数据。</a></p><p id="cca1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是一些亮点:</p><ul class=""><li id="5eb2" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">它包含13，828个字符的“字母表”。</li><li id="e631" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">其中62%是全词，38%是子词。</li><li id="f84d" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">为了统计英语单词，我尝试在WordNet(一种综合英语词典)中查找所有完整的单词，找到了大约11，400个英语单词，这只是XLM-R词汇量的5%。</li></ul><h1 id="12bc" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.比较方法</h1><h1 id="5c6d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.1.自然语言推理</h1><p id="5afd" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">评估多语言模型最常用的任务叫做<strong class="ky ir">自然语言推理(NLI) </strong>。原因是有一个优秀的多语言基准数据集叫做<strong class="ky ir"> XNLI </strong>。</p><p id="2787" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在下一节讨论XNLI，但是如果您不熟悉的话，这里有一个对基本NLI任务的解释。</p><p id="f840" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在NLI，我们被给予两个句子:(1)一个“前提”和(2)一个“假设”，并被要求确定是否:</p><ul class=""><li id="361e" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">1逻辑上产生2(这称为“<strong class="ky ir">蕴涵</strong>”)</li><li id="4122" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">2与1矛盾(“矛盾”)</li><li id="482f" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">2对1没有影响(“中性”)</li></ul><p id="090f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是一些例子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/4b55bb0e0e1ae2c83cddbd220d4c8c9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nRqmYP700Qvtl1sBDEWbIA.png"/></div></div></figure><blockquote class="ng nh ni"><p id="6cd1" class="kw kx ls ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated"><em class="iq">据我所知，NLI主要是一个</em>基准测试任务<em class="iq">而不是一个实际应用——它需要模型开发一些复杂的技能，所以我们用它来评估和基准测试像BERT这样的模型。</em></p></blockquote><h1 id="46b8" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.2.MNLI和XNLI概述</h1><p id="6338" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在NLI上对多语言模型的基准测试是通过组合两个名为“MNLI”和“XNLI”的数据集来完成的。</p><p id="c60b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">MNLI将为我们提供大量的<strong class="ky ir">英语</strong>训练实例，以微调XLM-罗伯塔对NLI的总任务。</p><p id="ea3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">XNLI将为我们提供少量不同语言的NLI测试示例<em class="ls"/>。我们将采用我们的XLM-罗伯塔模型(我们将仅对英语MNLI示例进行微调)，并将其应用于XNLI的<strong class="ky ir">阿拉伯语</strong>测试用例。</p><p id="63c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">关于MNLI </strong></p><p id="c684" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://cims.nyu.edu/~sbowman/multinli/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">多体裁自然语言推理</strong> </a> (MultiNLI或MNLI)语料库于2018年发布，是超过40万个<em class="ls">英语</em>句子对的集合，标注有文本蕴涵信息。</p><blockquote class="ng nh ni"><p id="f463" class="kw kx ls ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated"><em class="iq">在MNLI中，‘多’是指多体裁，而不是多语言。很困惑，我知道！它被称为“多体裁”，因为它旨在成为斯坦福NLI语料库(SNLI)的继任者，后者完全由一些取自图像说明的简单句子组成。MNLI通过添加多种更难的文本“体裁”增加了任务的难度，如转录的对话、政府文件、旅游指南等。</em></p></blockquote><p id="b072" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该语料库包含392，000个训练示例，20，000个“开发示例”(开发模型时使用的测试示例)，以及20，000个“测试示例”(报告基准分数的最终测试集)。</p><p id="e52e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是一些随机选择的培训示例</p><pre class="kg kh ki kj gt nw ns nx ny aw nz bi"><span id="ec03" class="oa lu iq ns b gy ob oc l od oe">Premise:<br/>    If I had told you my ideas, the very first time you saw Mr. Alfred<br/>    Inglethorp that astute gentleman would have ”in your so expressive idiom<br/>    ”'smelt a rat'!<br/>Hypothesis:<br/>    In the event that I had revealed my ideas to you, Mr. Alfred would have been<br/>    absolutely oblivious to your knowledge of my ideas.<br/>Label:<br/>    2 (contradiction)</span><span id="1dbe" class="oa lu iq ns b gy of oc l od oe">----------------</span><span id="d8aa" class="oa lu iq ns b gy of oc l od oe">Premise:<br/>    Like federal agencies, the organizations we studied must protect the<br/>    integrity, confidentiality, and availability of the information resources<br/>    they rely on.<br/>Hypothesis:<br/>    Some organizations must protect the confidentiality of information they rely<br/>    on.<br/>Label:<br/>    0 (entailment)</span><span id="f64d" class="oa lu iq ns b gy of oc l od oe">----------------</span><span id="ea36" class="oa lu iq ns b gy of oc l od oe">Premise:<br/>    Well? There was no change of expression in the dark melancholic face.<br/>Hypothesis:<br/>    He just looked at me and said, Well, what is it?<br/>Label:<br/>    0 (entailment)</span><span id="ba54" class="oa lu iq ns b gy of oc l od oe">----------------</span></pre><p id="62d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">关于XNLI </strong></p><p id="a2ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“XNLI”代表跨语言自然语言推理语料库。论文(<a class="ae kv" href="https://arxiv.org/pdf/1809.05053.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>)于2018年9月首次提交给<a class="ae kv" href="https://arxiv.org/abs/1809.05053" rel="noopener ugc nofollow" target="_blank"> arXiv </a>。</p><p id="748d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该数据集由MLNI数据集中已经被人工翻译成14种不同语言(如果包括英语，则总共为15种语言)的示例的较小子集组成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/11ab53a31e09b2813c7324970adfd0d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*GdQitxLLO95zms3iqijCxw.png"/></div></figure><p id="1b63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">XNLI不提供这些不同语言的训练数据，所以它旨在作为我们将在这里采用的跨语言方法的基准。</p><p id="4854" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于每种语言，有5000个测试集句子对和2500个开发集句子对。</p><blockquote class="ng nh ni"><p id="8173" class="kw kx ls ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">NYU大学的Sam Bowman负责MNLI和XNLI数据集。XNLI是与脸书合作完成的。</p></blockquote><p id="0300" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里有几个来自阿拉伯语测试集的随机例子。</p><pre class="kg kh ki kj gt nw ns nx ny aw nz bi"><span id="1fb8" class="oa lu iq ns b gy ob oc l od oe">Premise:<br/>    في المسرحية الاجتماعي كذلك، فإن فرص العمل والتنسيق بين الأدوار المختلفة ربما<br/>    تساعد الأطفال على فهم أوجه التشابه والاختلاف بين الناس في الرغبات والمعتقدات<br/>    والمشاعر.<br/>Hypothesis:<br/>    لا يستطيع الأطفال تعلم اى شئ .<br/>Label:<br/>    2 (contradiction)</span><span id="c6ff" class="oa lu iq ns b gy of oc l od oe">----------------</span><span id="433e" class="oa lu iq ns b gy of oc l od oe">Premise:<br/>    لماذا ، كما كنت  أخبر سيادته هنا ، من فكر مثلك أن وجود الأنسة بيشوب على متن<br/>    السفينة سيجعلنا أمنين ، ليس من أجل أمه ،ذاك النخاس القذر سكت عن ما هو مستحق<br/>    له .<br/>Hypothesis:<br/>    لم أتحدّث إلى سيادته منذ زمن طويل.<br/>Label:<br/>    2 (contradiction)</span><span id="d947" class="oa lu iq ns b gy of oc l od oe">----------------</span><span id="b26b" class="oa lu iq ns b gy of oc l od oe">Premise:<br/>    لقد قذفت إعلان عن كوكاكولا هناك<br/>Hypothesis:<br/>    ضع إعلان مشروب غازي.<br/>Label:<br/>    1 (neutral)</span><span id="032f" class="oa lu iq ns b gy of oc l od oe">----------------</span></pre><h1 id="cfa1" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.3.单语教学法</h1><p id="d0b8" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们为这篇文章创建了两个笔记本——一个用于应用单语模型，另一个用于应用多语言模型(XLM-R)。</p><p id="363f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于单语方法，我使用了一个社区提交的模型，从<a class="ae kv" href="https://huggingface.co/asafaya/bert-base-arabic" rel="noopener ugc nofollow" target="_blank">到</a>。该模型的文档显示，它预先接受了大量阿拉伯文本的训练，并且在过去30天内有很高的下载量(这意味着它是一个受欢迎的选择)。</p><p id="6bba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我用两种不同的方法微调了这个模型。</p><p id="4072" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">方法1——使用一个小的带标签的数据集</strong></p><p id="c16d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用XNLI的小型验证集(2500个人工翻译的阿拉伯语示例)作为我们的训练集。这是一个非常小的训练集，特别是与英语MNLI中的大约40万个例子相比！我认为这种方法与您自己尝试收集带标签的数据集最为相似。</p><p id="7d07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该方法在阿拉伯语XNLI测试集上的准确率为61.0%。这是我们尝试的各种方法的最低分(在后面的部分有一个结果表)。</p><p id="b41b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">方法2——使用机器翻译的示例</strong></p><p id="8e83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">XNLI的作者还为14种非英语语言中的每一种语言提供了大型英语MNLI数据集的机器翻译副本。</p><p id="6b09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将为我们提供充足的训练数据，但想必数据的质量会更低，因为样本是由不完善的机器学习模型而不是人类翻译的。</p><p id="4dd5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法在阿拉伯语XNLI测试集上给了我们73.3%的准确率。</p><h1 id="d75a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.4.多语言方法</h1><p id="097c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">对于多语言方法，我根据完整的英语MNLI训练集对XLM-R进行了微调。</p><p id="1a0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用huggingface/transformers库，应用XLM-R和应用伯特几乎是一样的，你只是使用不同的类名。</p><p id="2fa3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要使用单语方法，您可以用以下代码加载模型和标记器:</p><pre class="kg kh ki kj gt nw ns nx ny aw nz bi"><span id="5908" class="oa lu iq ns b gy ob oc l od oe">from transformers import BertTokenizer<br/>from transformers import BertForSequenceClassification</span><span id="7de3" class="oa lu iq ns b gy of oc l od oe"># Load the tokenizer.<br/>tokenizer = BertTokenizer.from_pretrained("asafaya/bert-base-arabic")</span><span id="bc3a" class="oa lu iq ns b gy of oc l od oe"># Load the model.<br/>model = BertForSequenceClassification.from_pretrained("asafaya/bert-base-arabic", num_labels = 3)</span></pre><p id="9608" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于XLM-R，这就变成了:</p><pre class="kg kh ki kj gt nw ns nx ny aw nz bi"><span id="95c1" class="oa lu iq ns b gy ob oc l od oe">from transformers import XLMRobertaTokenizer<br/>from transformers import XLMRobertaForSequenceClassification</span><span id="ffa5" class="oa lu iq ns b gy of oc l od oe"># Load the tokenizer. <br/>xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base" )</span><span id="5505" class="oa lu iq ns b gy of oc l od oe"># Load the model.<br/>xlmr_model = XLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=3)</span></pre><p id="22df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">学习率</strong></p><p id="c15d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代码的其余部分是相同的。然而，我们确实遇到了参数选择的关键差异…我们发现XLM-R比伯特需要更小的学习速率-我们使用5e-6。当我们尝试2e-5(BERT建议的最小学习率)时，XLM-R训练完全失败了(模型的性能从未比随机猜测有所提高)。注意5e-6是2e-5的四分之一。</p><p id="993d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">跨语言结果</strong></p><p id="fded" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用这种跨语言转换方法，我们在阿拉伯语XNLI测试集上获得了71.6%的准确率。相比之下，对阿拉伯语例子进行微调的单语模型只得到61.0%的分数！</p><p id="73dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">XML-RoBERTa的作者在他们的<a class="ae kv" href="https://arxiv.org/pdf/1911.02116.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中报告了73.8%的阿拉伯语分数，见表1:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/1809329117bfee2edd1d96e28e1fbd3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*y9Z64xRVG6gyA2f5"/></div></div></figure><p id="82b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">表格最下面一行的模型更大，它与BERT-large的规模相匹配。在我们的例子中，我们使用了较小的“基础”尺寸。</p><p id="af5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们较低的精度可能与批量大小、学习速率和过拟合等参数选择有关。</p><h1 id="ea51" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.5.结果摘要</h1><p id="01de" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">同样，我使用这些笔记本的目的是提供工作示例代码；不执行严格的基准测试。为了真正比较这些方法，应该进行更多的超参数调整，并且应该对多次运行的结果进行平均。</p><p id="c3ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是这里是我们用最小的调整得到的结果！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/de057a3e4cfbdf392f730bf463e33e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/0*i41EjSoWpCNeFdPn"/></div></figure><p id="cefe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于表中的第2–4行，您可以通过对阿拉伯语XNLI验证示例进行微调来进一步改进这些结果。(我用XLM-R快速尝试了一下，确认分数上升到了74.2%！)</p><h1 id="c431" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.6.使用哪种方法？</h1><p id="cc01" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">考虑到我更容易用<code class="fe np nq nr ns b">arabic-bert-base</code>获得好的结果，并且知道它需要更少的内存(由于更小的词汇量)，我想在这种情况下我会选择单语模型。</p><p id="3ea2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，这仅仅是因为一个团队预先训练并发布了一个很好的阿拉伯语单语模型！</p><p id="a3bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我原本想用印尼语作为这个项目的示例语言，但是</p><ol class=""><li id="7210" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr oj mx my mz bi translated">印尼语不在15种XNLI语言之列。</li><li id="45f8" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr oj mx my mz bi translated">我发现的最好的印度尼西亚型号，<a class="ae kv" href="https://huggingface.co/cahya/bert-base-indonesian-522M" rel="noopener ugc nofollow" target="_blank">CAH ya/Bert-base-Indonesia-522m</a>，是在相对适度的文本量(~0.5GB)上预先训练的，因此我对它的性能持怀疑态度。</li></ol><p id="0924" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于印度尼西亚人来说，我仍然想尝试这两种方法，但我怀疑XLM-R会走在前面。</p><h1 id="38f4" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">笔记本示例</h1><p id="1e32" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">这篇文章中提到的两个笔记本(一个实现多语言实验，另一个实现单语实验)可以从我的网站<a class="ae kv" href="https://bit.ly/3irTX3y" rel="noopener ugc nofollow" target="_blank">这里</a>购买。我还在YouTube <a class="ae kv" href="https://www.youtube.com/playlist?list=PLam9sigHPGwM27p3FQpLK1nt0eioiM-cq" rel="noopener ugc nofollow" target="_blank">这里</a>提供了这些笔记本的演示。</p></div></div>    
</body>
</html>