<html>
<head>
<title>From Transformers to Performers: Approximating Attention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从变形金刚到表演者:近似注意力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-transformers-to-performers-approximating-attention-69c88af0b11f?source=collection_archive---------9-----------------------#2020-10-29">https://towardsdatascience.com/from-transformers-to-performers-approximating-attention-69c88af0b11f?source=collection_archive---------9-----------------------#2020-10-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/adaa75a118848ff80b05e51fbcb89975.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*hlSgeNZev_WsYLgSoMMmhQ.gif"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><div class=""/><div class=""><h2 id="de7d" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">加速变压器的数学技巧</h2></div><p id="6d10" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">几周前，来自谷歌、剑桥大学、DeepMind和艾伦图灵研究所的研究人员发布了论文<a class="ae lt" href="https://arxiv.org/abs/2009.14794" rel="noopener ugc nofollow" target="_blank">反思表演者的注意力</a>，该论文旨在寻找解决变形金刚中softmax瓶颈问题的解决方案[1]。他们的方法利用了一个聪明的数学技巧，我将在本文中解释。</p><p id="48b7" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">先决条件:</strong></p><ul class=""><li id="fbed" class="lu lv ji kz b la lb ld le lg lw lk lx lo ly ls lz ma mb mc bi translated">变压器的一些知识</li><li id="af18" class="lu lv ji kz b la md ld me lg mf lk mg lo mh ls lz ma mb mc bi translated">核函数</li></ul><p id="5336" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj">涵盖话题:</strong></p><ul class=""><li id="2f1e" class="lu lv ji kz b la lb ld le lg lw lk lx lo ly ls lz ma mb mc bi translated">为什么是变形金刚？</li><li id="33ca" class="lu lv ji kz b la md ld me lg mf lk mg lo mh ls lz ma mb mc bi translated">变压器的问题是</li><li id="9ad3" class="lu lv ji kz b la md ld me lg mf lk mg lo mh ls lz ma mb mc bi translated">绕过softmax瓶颈</li></ul><h1 id="ebb1" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">为什么是变形金刚？</h1><p id="d489" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">从本质上讲，Transformer是一个设计用来高效处理顺序数据的模型，实际上它在自然语言处理(NLP)任务中被大量使用，这些任务需要处理单词/字母序列。与其他顺序模型不同，transformer利用<a class="ae lt" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">注意力机制</a>来并行处理顺序数据(即:不需要一次处理一个单词/输入)[2]。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/812b92ecc4f7023c4060981fed130768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kwSPXlT9VZeGh0s4tgBSMA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="9328" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">如果你不熟悉变形金刚，我推荐你阅读<a class="ae lt" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>——2017年介绍它们的论文，非常容易理解——或者<a class="ae lt" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图解变形金刚</a>以获得对初学者更友好的介绍。[3][4]</p><h1 id="3947" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">变压器的问题是</h1><p id="14e4" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">变形金刚基于注意力，计算如下:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/590f8a0f0dc0f4fa40ca5a1d5752107f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U70prqY0rzEtcNUN0ZDmiQ.png"/></div></div></figure><p id="ba19" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">其中<strong class="kz jj"> Q，K，V </strong>(维度<em class="nl"> L </em> x <em class="nl"> d </em>)是查询、键和值矩阵，<em class="nl"> L </em>是序列的长度，<em class="nl"> d </em>是查询、键和值向量的(任意)维度。</p><p id="4db2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">注意:如果你不知道这是从哪里来的，我强烈推荐阅读前面提到的文章(<a class="ae lt" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>)，尽管对于本文的目的来说，并不严格需要知道什么是查询、键和值，以及为什么注意力是这样计算的(或者它是什么)。</p><p id="6ff4" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">transformer的问题来自softmax函数，让我们看看为什么。</p><h1 id="3672" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">注意的时间复杂性</h1><p id="a043" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">提醒一下，两个维数为<em class="nl"> n </em> x <em class="nl"> m </em>和<em class="nl"> m </em> x <em class="nl"> p </em>的矩阵相乘的时间复杂度为O( <em class="nl"> nmp </em>)。</p><p id="41d2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">如果我们看一下注意力的等式，我们会看到我们在乘以三个矩阵:<strong class="kz jj"> Q </strong>(维度为<em class="nl"> L </em> x <em class="nl"> d </em>)、<strong class="kz jj">k</strong>^t(<em class="nl">d</em>x<em class="nl">l</em>，以及<strong class="kz jj">v</strong>(<em class="nl">l</em>x<em class="nl">d</em>)。我们会得到不同的复杂度，取决于我们相乘的顺序。<br/>暂且忽略softmax和分母sqrt(d)(它只是一个标量)，我们可以看到，通过乘以<strong class="kz jj"> Q K </strong> ^T <strong class="kz jj"> </strong>我们首先获得复杂度O( <em class="nl"> L d </em>)，而如果我们乘以<strong class="kz jj"> K </strong> ^T <strong class="kz jj"> V </strong>我们首先获得复杂度<br/> O( <em class="nl"> d L </em>)。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/a40b68b0fb3ca2f263d404c1d2030e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WsV2MdLLvhFg7_TnqRepMg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="4778" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们更喜欢O( <em class="nl"> d L </em>)，因为<em class="nl"> d </em>是我们可以选择的参数，通常我们可以有<em class="nl"> d &lt; L </em>。然而，我们实际上不能按照这个顺序执行乘法，因为<strong class="kz jj"> Q K </strong> ^T被“卡”在了softmax里面，没有简单的方法把它取出来。这意味着我们不得不处理O( <em class="nl"> L d </em>)的时间复杂度，这是序列长度的二次方(因此处理更长的序列在计算上变得越来越昂贵)。<br/>所以，softmax是变形金刚的瓶颈，我们想找到一种方法来解决这个问题。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/36684b6616c1ff6517b4cb3300420789.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MAcuY1tLvXL4_uySaPNqvA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><h1 id="de9a" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">绕过softmax瓶颈</h1><p id="736e" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">从高层次来看，本文提出的方法非常简单。我们能否找到一种方法来近似softmax，以允许我们选择矩阵的计算顺序？</p><p id="8576" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">本质上，我们能找到一些矩阵<strong class="kz jj">Q’</strong>和<strong class="kz jj">K’</strong>使得</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/de31574b0949db20fb46bd4cff9e09c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjooGh9x8nkQaa3YQ3FCNg.png"/></div></div></figure><p id="0dc3" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">目标很容易理解，但是如何实现的细节有点复杂。</p><p id="e468" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">首先，让我们回忆一下，softmax是一个函数，它给定一个长度为n的向量<strong class="kz jj"> z </strong>，将所有元素归一化为:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/02e50918151574b6de1db02d7ec988f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aVdZ0NwhVZEO82SR1QUUbg.png"/></div></div></figure><p id="2117" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">鉴于此，请注意，我们可以将注意力等式中的softmax改写为:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/728d0a249ed8a727d4cc54b8bf199740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wcjUYpJJRa1ptcg2L9dSSQ.png"/></div></div></figure><p id="9397" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">其中<strong class="kz jj"> A </strong>中的指数是按元素应用的，<strong class="kz jj"> 1 </strong> _L是长度为L的全1向量，<strong class="kz jj"> D </strong>是具有元素<strong class="kz jj"> A1 </strong> _L的对角矩阵。<strong class="kz jj"> D </strong>给出了softmax的分母(实际上，<strong class="kz jj"> A1 </strong> _L只是通过对<em class="nl">的列求和而获得的长度为<em class="nl"> L </em>的向量)</em></p><p id="5d3e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj"> A </strong>，它的元素指数，是这里真正的问题，所以我们的目标是以某种方式分解它。我们可以忽略标量分母sqrt(d ),因为这只是用于规范化，但我们可以等效地规范化查询和键。这意味着我们的目标是找到一些<strong class="kz jj">Q’</strong>和<strong class="kz jj">K’</strong>使得:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/7089c8668170b2cfd9109580cf174d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u8C6MbhS5Kk3g5PLYn1fvQ.png"/></div></div></figure><h1 id="8434" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">通过高斯核寻找Softmax核</h1><p id="af05" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">这就是内核发挥作用的地方。我们知道核是等价于某个特征映射φ的点积的函数:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/4e804392ca1ea9a8b9edffb4f934e873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Pb-Jkx5CIbJCow_Sg6T2g.png"/></div></div></figure><p id="c539" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">通常，给定某个高维特征图φ，我们感兴趣的是找到一个等价函数K，它将允许我们避免在φ的高维空间中进行计算。然而，在我们的例子中，我们实际上要走相反的路:如果我们假设<strong class="kz jj"> A </strong>是一个包含元素<strong class="kz jj"> A </strong> ( <em class="nl"> i，j</em>)=<strong class="kz jj">k</strong>(<strong class="kz jj">q</strong>_ I，<strong class="kz jj">k</strong>_ j)= exp(<strong class="kz jj">q _ I k</strong>_j^t)(其中<strong class="kz jj"> q_i </strong>和<strong class="kz jj">k<strong class="kz jj"/></strong></p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/76aed5e5d58017e10e08830bb302939f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VV1hCp53SKJDmGB1kKgamw.png"/></div></div></figure><p id="9fd1" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">现在，大多数核可以用以下形式的特征图来近似</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/489f6de97e79e64839538b09118464a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gyQGqT6VlxNmW-KxlpY44A.png"/></div></div></figure><p id="b0b7" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">其中<em class="nl"> h </em>和<em class="nl"> f₁,…,f_l </em>是一些确定性函数，<em class="nl"> w₁,…,w_m </em>是从分布<em class="nl"> D </em>中抽取的随机值(所以φ( <strong class="kz jj"> x </strong>是一个具有<em class="nl"> l </em> x <em class="nl"> m </em>元素的向量)。<a class="ae lt" href="https://arxiv.org/pdf/2004.11154.pdf" rel="noopener ugc nofollow" target="_blank">【5】</a></p><p id="ffd6" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">一篇<a class="ae lt" href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf" rel="noopener ugc nofollow" target="_blank">以前的论文</a>已经证明我们可以通过使用配置获得高斯核:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/ac3007027a2ff0a2907e7297a53d9c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T4VXg9IxFbHJl6tick5i8A.png"/></div></div></figure><p id="1a75" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">也就是说，如果我们从均值为0且单位方差的正态分布中画出<em class="nl"> w </em>，我们可以通过使用特征图获得高斯核:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/b8de3c984f8820013192b92f73d17cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hQGZWL96G5Gu8PpmQ4JwTQ.png"/></div></div></figure><p id="7c41" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">注意，具有单位方差的高斯核由下式给出:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/67a7c7e8beeb2e6847d51d7b3c0252f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*La6AyTwVl0rp0YlPXYcyIg.png"/></div></div></figure><p id="34c4" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">现在请记住，我们希望找到一个softmax内核:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/87df0bca606bd7911f7b9f962009f3ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5yZBUHfNIF2YHIPBe2jV3g.png"/></div></div></figure><p id="5a38" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们可以看到Softmax核的结构与高斯核相差不远。事实证明，我们可以利用这种相似性找到softmax内核。事实上，请注意</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/1f97817095de7f837e711628cb1f95de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4EWRusyCObmH4Rkskpt-dw.png"/></div></div></figure><p id="6991" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这意味着我们实际上可以将softmax内核重写为</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/c11f8eb1908df2d7350ec6080ec117e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8nwDZHji8QsC9scWDSEN9w.png"/></div></div></figure><p id="08fc" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">并且我们可以通过将<em class="nl"> h </em>函数从<em class="nl"> h </em> ( <strong class="kz jj"> x </strong> ) = 1改为:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/b768b079f6c5a57c572be53620939b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yATRDcSHF-_QUZEOOE3HQA.png"/></div></div></figure><p id="b53e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这是一个不错的近似值，但它有一些问题。softmax函数总是输出正值，所以<strong class="kz jj"> A </strong>的所有元素都应该是正的。然而，使用这个内核来近似softmax可能会给出一些负值。事实上，由于我们是从均值为0的正态分布中抽取<em class="nl"> w </em>，这些值中的一些将是负的，这反过来意味着<strong class="kz jj"> A </strong>的一些值将是负的。这会导致问题和异常行为。</p><h1 id="3f6e" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">寻找更稳定的Softmax内核</h1><p id="9b57" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">研究人员发现，softmax内核也可以重写为:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/984b8215f58af49eaee4d45c45a538af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ASjx4S9-kECnshC0BqJk5Q.png"/></div></div></figure><p id="bf0a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">(这实际上是softmax内核的证明可以在论文的附录中找到。)</p><p id="ec89" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">因此，我们可以简单地从前面的功能图形式，并设置</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/579887b19ba8c2e14305da4c9edcefbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XX0opQiqZOwZhYWeCbkWhA.png"/></div></div></figure><p id="100d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">通过这样做，我们可以看到所有的值都是正的，因为我们使用了exp，因此解决了我们之前的问题。</p><p id="f071" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">作者还建议了另一个导致相同内核的特性图，如果你感兴趣，我推荐你阅读这篇论文。</p><h1 id="3ede" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">使用softmax内核查找Q '和V '</h1><p id="ab59" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">让我们回顾一下。我们从注意力等式开始</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/cc318e9f59f642100f7a43de5597ff09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-O-ZzG0T16_IccKsuL9Oww.png"/></div></div></figure><p id="ca39" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">发现我们可以把它改写成</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oe"><img src="../Images/d3d3606f01aeccbb6a1a542d423f8d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7KgL4e_-_rapcbAKWcVWhg.png"/></div></div></figure><p id="fc04" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然后我们找到了softmax内核的特性图，我们可以用它来近似矩阵<strong class="kz jj"> A </strong>:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/6e1ed434b2873c790591e39c47fb6789.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jcNHV_HTn3QJydZrcPLFNA.png"/></div></div></figure><p id="c360" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">所以我们现在可以使用特征映射替换<strong class="kz jj"> A </strong>中的元素:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi og"><img src="../Images/9b5fd72112a756726190fa8dd8ba555b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HmgO0t8Htl_Ov-BT8YWD3w.png"/></div></div></figure><p id="aa1e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">注意，我们是从长度为<em class="nl"> L </em>的向量<strong class="kz jj"> q_k </strong>移动到长度为<em class="nl"> m </em>的向量φ( <strong class="kz jj"> q </strong> _i)和φ( <strong class="kz jj"> k </strong> _j)。</p><p id="049b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们现在可以分解<strong class="kz jj"> Q' </strong>和<strong class="kz jj"> K' </strong>中的<strong class="kz jj"> A </strong>，其中<strong class="kz jj"> Q' </strong>和<strong class="kz jj"> K' </strong>的元素为φ( <strong class="kz jj"> q </strong> _i)和φ( <strong class="kz jj"> k </strong> _j)。最后，我们可以自由地改变矩阵乘法的顺序，并将时间复杂度从O( <em class="nl"> L d </em>)降低到O( <em class="nl"> Lmd </em>)，从而获得序列长度的线性复杂度，而不是二次复杂度。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oh"><img src="../Images/74c5d412b1577cf0d2a6b57354027dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dGxlzc_U0ZhOR_SA00LLkQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="c121" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们完了。</p><h1 id="05d3" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">结论和一些结束语</h1><p id="24df" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">本质上，在本文中，作者设法找到了一种使用特征图的点积来近似softmax函数的方法。由于这一点，在变压器中计算注意力的时间复杂度可以从序列长度的平方减少到线性。在处理长序列时，这将显著提高转换器的速度。</p><p id="faa4" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">需要注意一些有趣的事情:</p><ul class=""><li id="ca1d" class="lu lv ji kz b la lb ld le lg lw lk lx lo ly ls lz ma mb mc bi translated">虽然这种方法是在考虑变形器的情况下开发的，但它实际上可以应用于任何需要softmax的模型。看看这在哪里会变得有用将会很有趣。</li><li id="472f" class="lu lv ji kz b la md ld me lg mf lk mg lo mh ls lz ma mb mc bi translated">作者指出，这种方法不仅速度更快，而且内存效率更高。这可以通过查看需要存储的矩阵的维数来看出。</li></ul><h1 id="cd22" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">参考</h1><p id="bee0" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">[1] <a class="ae lt" href="https://arxiv.org/abs/2009.14794" rel="noopener ugc nofollow" target="_blank"> Choromanski等人《与表演者重新思考注意力》，2020年9月30日</a><br/>【2】<a class="ae lt" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">Jay alam mar。《可视化一个神经机器翻译模型(带注意力的Seq2seq模型的力学)》，2018年5月9日</a><br/>【3】<a class="ae lt" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人《注意力就是你所需要的一切》，2017年12月6日</a><br/>【4】<a class="ae lt" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">杰伊·阿拉姆马。《图解变压器》，2018年6月27日</a><br/>【5】<a class="ae lt" href="https://arxiv.org/pdf/2004.11154.pdf" rel="noopener ugc nofollow" target="_blank">刘等，“核逼近的随机特征:算法、理论及超越的综述”，2020年7月4日</a></p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="c08f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nl">觉得这个故事有帮助？考虑</em> <a class="ae lt" href="https://chiaracampagnola.medium.com/membership" rel="noopener"> <em class="nl">订阅</em> </a> <em class="nl">到媒体扶持写手！</em></p></div></div>    
</body>
</html>