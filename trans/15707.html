<html>
<head>
<title>Keyword Extraction with BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用BERT提取关键词</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea?source=collection_archive---------0-----------------------#2020-10-29">https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea?source=collection_archive---------0-----------------------#2020-10-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1d49" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a>，NLP</h2><div class=""/><div class=""><h2 id="720e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一种提取关键词和关键短语的最小方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/dd5f50a86893cbecc5274cb09d7d85fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oqavIaxgEpUJ048z.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://pixabay.com/nl/users/wokandapix-614097/" rel="noopener ugc nofollow" target="_blank"> Wokandapix </a>创作</p></figure><p id="274a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当我们想要从特定文档中理解关键信息时，我们通常转向<strong class="lk jd">关键词提取</strong>。关键词提取是提取与输入文本最相关的单词和短语的自动化过程。</p><p id="2612" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用<a class="ae lh" href="https://github.com/aneesha/RAKE" rel="noopener ugc nofollow" target="_blank">耙</a>和<a class="ae lh" href="https://github.com/LIAAD/yake" rel="noopener ugc nofollow" target="_blank">雅克等方法！我们已经有了易于使用的软件包，可以用来提取关键字和关键短语。然而，这些模型通常基于文本的统计属性工作，而不是基于语义相似性。</a></p><p id="f021" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">伯特进来了。BERT是一个双向转换器模型，它允许我们将短语和文档转换为捕捉其含义的向量。</p><blockquote class="me"><p id="dba7" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">如果我们用BERT代替统计模型会怎么样？</p></blockquote><p id="236d" class="pw-post-body-paragraph li lj it lk b ll mo kd ln lo mp kg lq lr mq lt lu lv mr lx ly lz ms mb mc md im bi translated">尽管有许多优秀的论文和解决方案使用了BERT嵌入(例如，<a class="ae lh" href="https://github.com/pranav-ust/BERT-keyphrase-extraction" rel="noopener ugc nofollow" target="_blank"> 1 </a>、<a class="ae lh" href="https://github.com/ibatra/BERT-Keyword-Extractor" rel="noopener ugc nofollow" target="_blank"> 2 </a>、<a class="ae lh" href="https://www.preprints.org/manuscript/201908.0073/download/final_file" rel="noopener ugc nofollow" target="_blank"> 3 </a>)，但我找不到一个简单易用的基于BERT的解决方案。相反，我决定创建<a class="ae lh" href="https://github.com/MaartenGr/KeyBERT/" rel="noopener ugc nofollow" target="_blank"> KeyBERT </a>一种利用BERT嵌入的简单易用的关键字提取技术。</p><p id="128b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，这篇文章的主要话题将不是使用<a class="ae lh" href="https://github.com/MaartenGr/KeyBERT" rel="noopener ugc nofollow" target="_blank"> KeyBERT </a>而是一个关于如何使用BERT创建你自己的<strong class="lk jd">关键词提取模型</strong>的<strong class="lk jd">教程</strong>。</p><h1 id="8e09" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">1.数据</h1><p id="16a0" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">对于本教程，我们将使用一个关于<strong class="lk jd">监督机器学习</strong>的文档:</p><pre class="ks kt ku kv gt nq nr ns nt aw nu bi"><span id="2108" class="nv mu it nr b gy nw nx l ny nz">doc = """<br/>         Supervised learning is the machine learning task of <br/>         learning a function that maps an input to an output based <br/>         on example input-output pairs.[1] It infers a function <br/>         from labeled training data consisting of a set of <br/>         training examples.[2] In supervised learning, each <br/>         example is a pair consisting of an input object <br/>         (typically a vector) and a desired output value (also <br/>         called the supervisory signal). A supervised learning <br/>         algorithm analyzes the training data and produces an <br/>         inferred function, which can be used for mapping new <br/>         examples. An optimal scenario will allow for the algorithm <br/>         to correctly determine the class labels for unseen <br/>         instances. This requires the learning algorithm to  <br/>         generalize from the training data to unseen situations <br/>         in a 'reasonable' way (see inductive bias).<br/>      """</span></pre><p id="697f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我相信，使用一个读者相当了解的主题的文档有助于你理解产生的关键短语是否有质量。</p><h1 id="3edf" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">2.候选关键词/关键短语</h1><p id="e195" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">我们首先从文档中创建候选关键字或关键短语的列表。虽然很多都集中在名词短语上，但是我们将通过使用Scikit-Learns <code class="fe oa ob oc nr b">CountVectorizer</code>来保持它的简单性。这允许我们指定关键字的长度，并使它们成为关键短语。这也是一个快速删除停用词的好方法。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="a204" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以使用<code class="fe oa ob oc nr b">n_gram_range</code>来改变候选结果的大小。例如，如果我们将它设置为<code class="fe oa ob oc nr b">(3, 3)</code>，那么得到的候选词将是包含<strong class="lk jd"> 3个关键字</strong>的短语。</p><p id="8116" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，变量<code class="fe oa ob oc nr b">candidates</code>只是一个包含候选关键字/关键短语的字符串列表。</p><p id="425b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">注意</strong>:你可以用<code class="fe oa ob oc nr b">n_gram_range</code>创造不同长度的关键短语。然后，您可能不想删除停用词，因为它们会将较长的关键短语连接在一起。</p><h1 id="a65f" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">3.嵌入</h1><p id="9f7b" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">接下来，我们将文档和候选关键字/关键短语都转换成数字数据。我们使用<strong class="lk jd"> BERT </strong>来达到这个目的，因为它在相似性和释义任务上都显示出了很好的结果。</p><p id="178e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">生成BERT嵌入的方法有很多，比如<a class="ae lh" href="https://github.com/flairNLP/" rel="noopener ugc nofollow" target="_blank"> Flair </a>，<a class="ae lh" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"> Hugginface Transformers </a>，现在甚至还有<a class="ae lh" href="https://nightly.spacy.io/" rel="noopener ugc nofollow" target="_blank"> spaCy </a>的3.0版本！然而，我更喜欢使用<code class="fe oa ob oc nr b">sentence-transformers</code>包，因为它允许我快速创建高质量的嵌入，这对于句子和文档级的嵌入非常有效。</p><p id="e27e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们用<code class="fe oa ob oc nr b">pip install sentence-transformers</code>安装包。如果你在安装这个包时遇到问题，那么先安装<a class="ae lh" href="https://pytorch.org/get-started/locally/" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>可能会有帮助。</p><p id="b7fb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们将运行以下代码，将我们的文档和候选项转换为向量:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="9af4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们是<strong class="lk jd"> Distilbert </strong>，因为它在相似性任务中表现出了出色的性能，这就是我们对关键词/关键短语提取的目标！</p><p id="92be" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因为transformer模型有令牌限制，所以在输入大文档时可能会遇到一些错误。在这种情况下，你可以考虑把你的文档分成几个段落，然后把得到的向量放在一起(取平均值)。</p><p id="aec7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">注意</strong>:有很多<a class="ae lh" href="https://www.sbert.net/docs/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">预先训练好的基于BERT的模型</a>可以用于关键词提取。不过，我会建议你使用<code class="fe oa ob oc nr b">distilbert — base-nli-stsb-mean-tokens</code> <strong class="lk jd"> </strong>或<code class="fe oa ob oc nr b">xlm-r-distilroberta-base-paraphase-v1</code> <strong class="lk jd"> </strong>，因为它们分别在<strong class="lk jd">语义相似度</strong>和<strong class="lk jd">释义识别</strong>中表现出色。</p><h1 id="4345" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">4.余弦相似性</h1><p id="64c6" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">在最后一步，我们希望找到与文档最相似的候选项。我们假设与文档最相似的候选项是表示文档的好的关键字/关键短语。</p><p id="5543" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了计算候选项和文档之间的相似度，我们将使用向量之间的<strong class="lk jd">余弦相似度</strong>，因为它在高维度中表现得相当好:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="e5e1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就这样了！我们将与输入文档最相似的前5个候选项作为结果关键字:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/188c702ba848e0ed76e7eae3e8787cac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmO9A99IhDPZjRo4Jpdw1A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由作者提供。</p></figure><p id="d59a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果看起来棒极了！这些术语看起来肯定像是描述了一个关于监督机器学习的文档。</p><p id="f6dc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们看看如果将<code class="fe oa ob oc nr b">n_gram_range</code>改为<code class="fe oa ob oc nr b">(3,3)</code>会发生什么:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/24b2ca70ebdbc12574f949ae867e2022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5DwToI85spdjK4dbvpmehQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由作者提供。</p></figure><p id="3972" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">似乎我们现在得到的是<strong class="lk jd">关键词</strong>而不是<strong class="lk jd">关键词</strong>！这些关键短语本身似乎很好地代表了文档。然而，我不高兴的是，所有的关键短语都如此相似。</p><p id="ba1f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了解决这个问题，让我们看看我们的结果的多样化。</p><h1 id="cdb2" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">5.多样化</h1><p id="57e2" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">返回相似的结果是有原因的…它们最能代表文档！如果我们将关键字/关键短语多样化，那么它们就不太可能很好地代表整个文档。</p><p id="f80a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，我们结果的多样化需要在关键词/关键短语的准确性和多样性之间取得微妙的平衡。</p><p id="4373" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将使用两种算法来使我们的结果多样化:</p><ul class=""><li id="1b63" class="oh oi it lk b ll lm lo lp lr oj lv ok lz ol md om on oo op bi translated">最大和相似度</li><li id="42b0" class="oh oi it lk b ll oq lo or lr os lv ot lz ou md om on oo op bi translated">最大边际关联</li></ul><h2 id="093c" class="nv mu it bd mv ov ow dn mz ox oy dp nd lr oz pa nf lv pb pc nh lz pd pe nj iz bi translated">最大和相似度</h2><p id="1e4a" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">数据对之间的最大总距离定义为数据对之间的距离最大化。在我们的例子中，我们希望最大化候选项与文档的相似性，同时最小化候选项之间的相似性。</p><p id="335a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为此，我们选择前20个关键词/关键短语，并从这20个关键词/关键短语中选择彼此最不相似的5个:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="b2c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们将<strong class="lk jd">设为低</strong> <code class="fe oa ob oc nr b">nr_candidates</code>，那么我们的结果似乎与我们最初的余弦相似度方法非常相似:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/809b4e5b41864db96451f138398ba6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fG-QkmlyF3FGW6AgnzinIw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由作者提供。</p></figure><p id="10dd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，相对较高的<strong class="lk jd"/><code class="fe oa ob oc nr b">nr_candidates</code>将创建更多不同的关键短语:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/0f93b0ae685f0205af30b4cf2ebbdf64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_uMu3fdZifQk2JMxPpTKHA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由作者提供。</p></figure><p id="8fee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如前所述，您必须牢记准确性和多样性之间的权衡。如果你增加了<code class="fe oa ob oc nr b">nr_candidates</code>，那么很有可能你会得到非常多样化的关键词，但是这些关键词并不能很好的代表文档。</p><p id="a4f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我建议你保持<code class="fe oa ob oc nr b">nr_candidates</code>少于你文档中独特单词总数的20%。</p><h2 id="7a03" class="nv mu it bd mv ov ow dn mz ox oy dp nd lr oz pa nf lv pb pc nh lz pd pe nj iz bi translated">最大边际关联</h2><p id="6b0b" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">使我们的结果多样化的最后一个方法是<strong class="lk jd">最大边际相关性</strong> (MMR)。在文本摘要任务中，MMR试图最小化冗余并最大化结果的多样性。幸运的是，一个名为<a class="ae lh" href="https://arxiv.org/pdf/1801.04470.pdf" rel="noopener ugc nofollow" target="_blank">embe beed</a>的关键词提取算法已经实现了一个版本的MMR，允许我们使用它来多样化我们的关键词/关键短语。</p><p id="5c16" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们首先选择与文档最相似的关键字/关键短语。然后，我们迭代地选择既与文档相似又与已经选择的关键词/关键短语不相似的新候选:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="6183" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们设置一个相对较低的多样性，那么我们的结果似乎与我们最初的余弦相似性方法非常相似:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/cb7fe28b929a8aba87be47e9cbd36097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fZb31IpYV3UAc7Jhkbv7xw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由作者提供。</p></figure><p id="c971" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，相对较高的<strong class="lk jd">多样性</strong>分数将创建非常多样化的关键短语:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/ef43758f6f83e7ac6f6aed7e5653dc4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*70FekE90MTceTBr4tIGyYQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由作者提供。</p></figure><h1 id="3d7d" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">感谢您的阅读！</h1><p id="34c2" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">如果你像我一样，对人工智能、数据科学或心理学充满热情，请随时在<a class="ae lh" href="https://www.linkedin.com/in/mgrootendorst/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上添加我，或者在<a class="ae lh" href="https://twitter.com/MaartenGr" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我。</p><p id="2050" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文中的所有示例和代码都可以在这里找到:</p><div class="pi pj gp gr pk pl"><a href="https://github.com/MaartenGr/KeyBERT" rel="noopener  ugc nofollow" target="_blank"><div class="pm ab fo"><div class="pn ab po cl cj pp"><h2 class="bd jd gy z fp pq fr fs pr fu fw jc bi translated">马尔滕格尔/凯伯特</h2><div class="ps l"><h3 class="bd b gy z fp pq fr fs pr fu fw dk translated">KeyBERT是一种简单易用的关键字提取技术，它利用BERT嵌入来创建关键字和…</h3></div><div class="pt l"><p class="bd b dl z fp pq fr fs pr fu fw dk translated">github.com</p></div></div><div class="pu l"><div class="pv l pw px py pu pz lb pl"/></div></div></a></div></div></div>    
</body>
</html>