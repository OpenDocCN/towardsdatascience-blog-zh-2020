<html>
<head>
<title>In-depth analysis of the regularized least-squares algorithm over the empirical risk minimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于经验风险最小化的正则化最小二乘算法的深入分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares-algorithm-over-the-empirical-risk-minimization-729a1433447f?source=collection_archive---------32-----------------------#2020-10-29">https://towardsdatascience.com/in-depth-analysis-of-the-regularized-least-squares-algorithm-over-the-empirical-risk-minimization-729a1433447f?source=collection_archive---------32-----------------------#2020-10-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/b3fc60070349864fe23955343e04396e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lbsllLyOITdwh8YNFEozdw.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片—加拿大桑德湾</p></figure><p id="eff4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">本文将介绍正则化损失最小化(RLM)和经验风险最小化(ERM)的关键概念，并通过MATLAB实现最小二乘算法。然后将使用RLM和机构风险管理获得的模型进行相互比较和讨论。</p><p id="07c8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们将使用多项式曲线拟合问题来预测该数据的最佳多项式。最小二乘算法将使用MATLAB逐步实现。</p><p id="b391" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在这篇文章结束时，你会理解最小二乘算法，并意识到RLM和ERM的优点和缺点。此外，我们将讨论一些关于过拟合和欠拟合的重要概念。</p></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="7eff" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">资料组</h1><p id="82d3" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">我们将使用一个简单的具有N = 100个数据点的输入数据集。这个数据集最初是由Ruth Urner博士在她的一次机器学习课程作业中提出的。在下面的存储库中，你会发现两个TXT文件:<a class="ae mk" href="https://github.com/jaimedantas/least-squares-regresion/blob/main/dataset/dataset1_inputs.txt" rel="noopener ugc nofollow" target="_blank"> dataset1_inputs.txt </a>和<a class="ae mk" href="https://github.com/jaimedantas/least-squares-regresion/blob/main/dataset/dataset1_outputs.txt" rel="noopener ugc nofollow" target="_blank"> dataset1_outputs.txt </a>。</p><div class="ml mm gp gr mn mo"><a href="https://github.com/jaimedantas/least-squares-regression" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd ir gy z fp mt fr fs mu fu fw ip bi translated">jaimedantas/最小二乘回归</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">正则化最小二乘算法用于回归以寻找机器学习模型。这个实现…</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">github.com</p></div></div><div class="mx l"><div class="my l mz na nb mx nc jw mo"/></div></div></a></div><p id="4a9c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这些文件包含输入和输出向量。使用MATLAB，我们将这些数据点绘制成图表。在MATLAB上，我在主页&gt;导入数据中导入它们。然后，我创建了绘制数据点的流程脚本。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="2ddb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">您应该会看到如下所示的图表。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/79a68e18d32e1b60f83fb1f0d0fbc9ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*pK8l0G-SxPh3ajKkhTAlug.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">资料组</p></figure><h1 id="17e0" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">最小平方</h1><p id="43f8" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">最小二乘法用于解决使用平方损失法的线性回归中的ERM问题。在继续讨论实现本身之前，我推荐<a class="ae mk" href="https://en.wikipedia.org/wiki/Least_squares" rel="noopener ugc nofollow" target="_blank">这篇</a>快速文章。</p><h1 id="78f9" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">经验风险最小化(ERM)</h1><p id="d1d7" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">ERM是机器学习中一个广为人知的概念，我建议在开始实际实施之前，先看一下关于ERM的解释。ERM用于对学习算法的性能进行分类，我们可以通过找到一个向量<strong class="ke ir"> w </strong>来解决ERM优化问题，该向量最小化下面的公式【1】。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/7d332a8d3a1e41ba011a918c8a65d089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yk4G0YxtJ4hOUdB_2c3NAQ.png"/></div></div></figure><p id="ffb2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">上式中，<strong class="ke ir"> X </strong>为设计矩阵，<strong class="ke ir"> t </strong>为输出向量。我们想找到多项式拟合问题的最佳拟合。为此，我们将计算W = 1、2、…、30阶多项式，并分析经验平方损耗，以了解哪种多项式阶最适合我们的数据。我们问题的设计矩阵<strong class="ke ir"> X </strong>由下式给出:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/5c298496fd66893ab0053241b6d8ea26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B4-xeI0HAbwkS4rPOhHCTQ.png"/></div></div></figure><p id="c0e6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以通过求解下面的线性方程来找到多项式<strong class="ke ir"> w </strong>。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0dce54b4e8f9cdd8f6ed0d74d9045220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*Rq2KkFVZyqFiBhDH7jrYog.png"/></div></figure><p id="2547" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果我们认为<strong class="ke ir">X’X</strong>是可逆的【1】，上面的方程总是有解的。同样，如果你想知道为什么会这样，你可以阅读<a class="ae mk" href="https://en.wikipedia.org/wiki/Least_squares" rel="noopener ugc nofollow" target="_blank">这篇关于最小二乘法的解释</a>。解决方案由下面的等式给出。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c6516d57a40ac0f72d644ed1ac0214c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*ycAiQcPcCN5PNfzEyXMARw.png"/></div></figure><p id="0076" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这就是MATLAB派上用场的地方。我们需要求解上面的线性方程来找到并测试我们的模型。</p><p id="8280" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了在MATLAB上求解这个方程，我们将通过矩阵相乘和求逆来手动实现运算，而不是使用有效的算法来求解线性方程。需要指出的是，这可能不是理想的方法，因为效率较低。为了计算解决方案，我首先计算W = 1的设计矩阵，并计算第一个多项式。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="e812" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">然后，我创建了一个循环来计算剩余的<strong class="ke ir"> Wᵢ </strong>，并将它们存储在名为<em class="nt">多项式_wi </em>的单元数组中。30个设计矩阵也存储在单元阵列<em class="nt">设计矩阵</em>中。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="739f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，我们需要计算<em class="nt">多项式_wi </em>上所有<strong class="ke ir"> wᵢ </strong>多项式的经验平方损耗。我们的问题的数据点数是N= 100。按照这个脚本，我创建了一个循环来计算每个多项式<strong class="ke ir"> wᵢ </strong>的ERM。我还打印了使我们的经验平方损失最小的多项式的阶。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="4855" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们算法的输出如下图所示。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/db08f4047835d6e409bac34e41c8f7bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xsPUAmMQ0R7FaGYB-WRfTw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><a class="ae mk" href="https://github.com/jaimedantas/least-squares-regresion/blob/main/code/ERM.m" rel="noopener ugc nofollow" target="_blank"> ERM.m </a>脚本的输出</p></figure><p id="bb0d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，21阶多项式的经验平方损失最小。现在，让我们分析下图中所有多项式的经验平方损耗图。对于这个分析，我绘制了从W = 1到W = 30的经验平方损失向量<strong class="ke ir"> E </strong>。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/7a2241f2716e103669855c3eba289a1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JGAfPIsiMKhoq52UTNPhNg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">缩小比例下的经验平方损失</p></figure><p id="374c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">查看ERM的经验平方损失，我们可以看到,<strong class="ke ir">多项式W= 21确实是数据集</strong>的最佳拟合。我们还可以看到，在W = 6之后，经验损失几乎变得稳定，直到W = 21时略有下降，而在W = 23之后，我们的模型开始过度拟合，以至于经验损失飙升。当我们在如下图所示的MATLAB命令窗口中执行命令<strong class="ke ir">【ERM，W】= max(E)</strong>时，我们看到对于订单27，平方损失超过395。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nw"><img src="../Images/cce179319874e5915aa2be996910a77f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gXBWlGPA9HOYPiJz6Lo5fw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">最大经验平方损失</p></figure><p id="9279" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">下图显示了W = 21阶多项式相对于数据集的曲线。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/b8b68314bb470c0abdca15d3f093400b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yNRweKvSegoXN1XCbUttLw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">21阶多项式与数据集</p></figure><p id="3497" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">分析上图中的曲线本身后，我们可以看到，尽管W= 21阶的多项式的经验损失最小，但它过度拟合了数据。因此，仅将经验损失作为选择最佳拟合的唯一指标可能不是最佳方法，因为它会导致我们选择过度拟合模型。</p><h1 id="f324" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">正则化损失最小化(RLM)</h1><p id="3580" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">现在，让我们使用正则化最小二乘多项式回归重复上一步。我建议在阅读这部分之前先看一下<a class="ae mk" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">关于RLM的解释</a>。对于RLM，我们使用正则化子λ来计算向量<strong class="ke ir"> w </strong>。对于正则化最小二乘回归，我们可以使用下面的等式[1]来计算<strong class="ke ir"> w </strong>。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e458c073ca2a1519034558e2610544fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*Yc7zGGNDJGPHogcZQ4yFdQ.png"/></div></figure><p id="a9cd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">注意，我们使用正则化子λ乘以<strong class="ke ir"> X. </strong>阶的单位矩阵</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/5b56ded2f7892da548370b343f0c3d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MU0OREgKHmdhX6mraVnU0w.png"/></div></div></figure><p id="be5d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果我们将<strong class="ke ir"> w </strong>分离出来，我们可以得到下面的线性方程:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/73a59c3c944807d5fcacce2c399b0842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*FNo9g2n0ICfZuO8Kw4qy3g.png"/></div></figure><p id="80dc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">同样，我将在MATLAB上手动实现这些操作来求解这个线性方程。这种情况下的设计矩阵与为ERM定义的相同。</p><h2 id="9f4f" class="ob li iq bd lj oc od dn ln oe of dp lr kn og oh lv kr oi oj lz kv ok ol md om bi translated">正则化子λ的重要性</h2><p id="9514" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">让我们首先分析在我们的机器学习模型的计算中有一个正则化子的后果。为了做到这一点，让我们分析W = 30阶的多项式，看看它在使用不同的λ值时的表现。</p><p id="98ee" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们将对区间λ进行这种分析，其中ln(λ) = -1，-2，…,-30。因此，λ的值将由下式给出</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi on"><img src="../Images/86fb5dc650a0417fa52152c83b985047.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*KCuOKr8npAfr5hy0_It9aA.png"/></div></figure><p id="a8cb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">首先，我计算了30阶多项式的设计矩阵的值。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="657e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">与我在ERM上所做的类似，我创建了一个循环来计算λ的所有值的<strong class="ke ir"> wᵢ </strong>。我还计算了RLM的经验平方损失，这与ERM相同。</p><p id="6b8d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于每次迭代，λ的值被更新。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oo"><img src="../Images/1895bbe10c8bcea1155f0dff2e12b9bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vV0oykB2AvhxGU8VXVgnPg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">带正则项的30阶多项式</p></figure><p id="e9b4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们算法的输出如下图所示。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi op"><img src="../Images/d8085da17c318a116233ece67bd7bc28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5YBxD8NW4hmjuPA_uR8xAw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><a class="ae mk" href="https://github.com/jaimedantas/least-squares-regresion/blob/main/code/RLM.m" rel="noopener ugc nofollow" target="_blank"> RLM.m </a>脚本的输出</p></figure><p id="d0c2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，30阶多项式(其中ln(λ)= 30，即i = 30)最适合此数据。下图显示了带有<strong class="ke ir"> i </strong> <strong class="ke ir"> = 30 </strong>的<strong class="ke ir"> W = 30 </strong>阶多项式对数据集的绘图。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oq"><img src="../Images/89128249798ecde1d2df374c78adf41f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EduLZMljARMqha2-rnJcKQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">i = 30的30阶多项式和数据集</p></figure><p id="84e4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">正如在ERM上看到的，我们的30阶RLM模型和i = 30也过度拟合了数据。</p><h1 id="9a61" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">ERM vs RLM</h1><p id="dbe8" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">现在，让我们比较一下ERM和RLM的结果。当我们分析RLM的等式时，我们可以得出结论，对于λ的大值，即<em class="nt"> i </em>的小值，我们具有较大的经验平方损耗。这意味着对于小值的<em class="nt"> i </em>，我们得到一个<strong class="ke ir">欠拟合</strong>模型。另一方面，对于λ的小值，即<em class="nt"> i </em>的大值，我们得到较小的经验平方损耗。然而，这种行为会导致<strong class="ke ir">过度拟合</strong>模型。我们可以在下图的右图中注意到这一点。此外，λ的大值也有助于减少高阶多项式的过拟合问题。这可以从下图的右图中看出。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi or"><img src="../Images/2f628a737a801c212a1e1ac139dca1d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJIs5W8jiv4_5Xi7iH3wAw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">左边是W = 2(欠拟合)和W = 27(过拟合)阶多项式的ERM，以及i = 2和i = 26的30阶RLM多项式。(剧情来自<a class="ae mk" href="https://github.com/jaimedantas/least-squares-regresion/blob/main/code/RLM_ERM.m" rel="noopener ugc nofollow" target="_blank"> RLM_ERM.m </a>剧本)</p></figure><p id="855b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">谈到EMR曲线(在经验平方损失图中用橙色表示)，我们也有类似的结果。然而，在这种情况下，我们改变多项式的阶数，直到W = 30。对于阶数较小的多项式，我们得到一个具有较大经验平方损失的<strong class="ke ir">欠拟合</strong>模型，而对于阶数较大的多项式(本例中为W &gt; 23 ),我们可以<strong class="ke ir">过拟合</strong>我们的数据，即使我们得到一个较小的经验平方损失。这可以在上图的左图中看到。注意，RLM中的正则化有助于减少较高阶多项式的过度拟合。因此，分析两种解决方案的经验平方损失图对于选择最适合我们数据集的模型至关重要。</p><p id="cf76" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">此外，如果我们分析极限λ→0的正则化最小二乘，即极限i→∞，我们看到RLM方程的正则化项消失，使得RLM与erm相同。当我们分析i= 12的10阶RLM多项式时，这一点得到了证明。由于这种情况下λ值非常小(0.000006144)，我们可以注意到，该多项式的曲线与10阶ERM多项式大致相同。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi os"><img src="../Images/625d3c42a125a8ce877916ae313cdaa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kt3fYfRjQkJkWDqEGW8luQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">10阶ERM多项式和10阶RLM多项式，i = 12。请注意，这些曲线相互重叠。(剧情来自<a class="ae mk" href="https://github.com/jaimedantas/least-squares-regresion/blob/main/code/RLM_ERM.m" rel="noopener ugc nofollow" target="_blank"> RLM_ERM.m </a>脚本)</p></figure><p id="0139" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最后，与W = 30阶的ERM相比，RLM的W = 30阶和i = 30阶的模型具有明显更小的经验平方损失(分别为0.0453和4.7873)。这支持了<strong class="ke ir">正则化有助于减少过拟合</strong>的结论。此外，RLM的W = 30次和i = 30次多项式对于ERM具有大约相同的W = 21次经验平方损失。</p><blockquote class="ot"><p id="3ea5" class="ou ov iq bd ow ox oy oz pa pb pc kz dk translated">毕竟哪种模式最好？</p></blockquote><p id="ca42" class="pw-post-body-paragraph kc kd iq ke b kf pd kh ki kj pe kl km kn pf kp kq kr pg kt ku kv ph kx ky kz ij bi translated">现在我们有了所有这些数据，我们可以为我们的问题选择最好的模型。目标总是找到一个小阶多项式，这样我们就不会过度拟合我们的数据。此外，订单必须足够大，这样我们就不会吃不饱。</p><p id="665e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们绘制并查看订单W= 1、5、10、20、30时的ERM和RLM曲线。这一次，我们将固定正则化因子λ = 0.0025。脚本<a class="ae mk" href="https://github.com/jaimedantas/least-squares-regresion/blob/main/code/Visualization.m" rel="noopener ugc nofollow" target="_blank"> Visualization.m </a>用于绘制两条曲线。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pi"><img src="../Images/b1df27287d9e2a343fa6cb7ba4530a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Mu34evKrnDIY3sQV-YhZA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">左边是RLM和数据集的1、5、10、20和30阶多项式，右边是RLM和数据集的1、5、10、20和30阶多项式</p></figure><p id="2309" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当我们分析EMR和RLM的图表时，我们肯定能注意到它们之间的差异。较大订单的ERM模型具有很大的变化，并且使它们自己过多地适应训练数据。这意味着，随着模型阶数的增加，过度拟合的风险也会增加。当我们研究W = 10阶的多项式时，尤其如此。虽然RLM多项式没有很大的变化和差异，但ERM多项式以一种导致过度拟合的方式适应数据。因此，与RLM相比，欧洲汇率机制的实际损失看起来较小。当我们比较这两个经验损失时，很容易观察到这一点。</p><p id="ccc3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了更好地形象化这种行为，我们可以分析下面的条形图，其中显示了W= 1、5、10、20和30阶多项式的ERM和RLM的经验损耗。当我们查看它时，我们可以证明，除了W= 30多项式，ERM的经验平方损失小于RLM。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pj"><img src="../Images/fbfac26914980666f7e3d50c9b9c83b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBMukqHzhGNMyi8AcmfqPA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">正则项λ = 0.0025的RLM的经验平方损失和W = 1、5、10、20和30的ERM</p></figure><p id="165b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在表示数据时，RML模型中的正则化因子起着重要的作用。<strong class="ke ir">正则化模型对数据集更精确，因为它们不会像ERM </strong>那样过度拟合数据。当我们看到下图时尤其如此。在左侧，我们有不同λ值的RLM的经验平方损耗。注意，当我们降低λ的值时，我们也降低了W &gt; 6的经验损耗。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pk"><img src="../Images/bcac01597b97388acd644e3cf1cd3b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RQiu9OpDDm0uFnJyT63xLw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">在缩小的比例中，左边是具有不同正则化子的RLM的经验平方损失，右边是具有不同正则化子的RLM的10阶多项式。(剧情来自与<a class="ae mk" href="https://github.com/jaimedantas/least-squares-regresion/blob/main/code/RML_For_W_Risk.m" rel="noopener ugc nofollow" target="_blank"> RML_For_W_Risk.m </a>和<a class="ae mk" href="https://github.com/jaimedantas/least-squares-regresion/blob/main/code/RML_For_W_Poly.m" rel="noopener ugc nofollow" target="_blank"> RML_For_W_Poly.m </a>剧本)</p></figure><p id="1795" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在上图的右侧，我们可以看到不同λ值的W= 10阶多项式。随着λ值的减小，我们增加了模型的适应性。因此，非常小的λ值可能导致过拟合，而大的λ值可能导致欠拟合。</p><p id="3ecd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">鉴于这些事实，最适合该数据集的模型是W= 5阶的RML多项式</strong>。多项式如下所示。</p><p id="d7e8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">0.6721-0.5555 x-6.5428 x-0.9711 x+6.5883x⁴+0.7647x⁵</strong></p><p id="44d1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是因为这个多项式阶次小，经验损失小。此外，当我们分析ERM和EML的曲线图时，我们可以看到这个模型没有过度拟合数据。此外，W= 5阶的ERM多项式似乎也符合数据而没有过度拟合。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pl"><img src="../Images/b225c5febf48bc767471fc2c6efeffeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PfThLjX3HnyrAuO6N6GuZA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">λ = 0.0025的W = 5阶多项式</p></figure><h1 id="462c" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">结论</h1><p id="67bf" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">我们看到，使用RML可以降低我们的模型的整体复杂性，并避免过度拟合。然而，由于我们现在有一个新的参数λ要确定，RML模型计算起来更加复杂。</p><p id="faae" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在选择模型时，我们应该在分析中使用的另一个重要特征是经验平方损失曲线。我们还可以得出结论，经验平方损失最小的模型并不总是我们数据的最佳解决方案。此外，在参数λ和我们预测的多项式的阶数之间找到一个平衡在这个分析中是至关重要的。</p><p id="f8eb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">数据欠拟合和过拟合是当今数据科学家面临的一个极其常见的问题，为给定问题找到理想的解决方案需要对可能的模型进行详细分析。</p><p id="1d9f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">感谢阅读！</p><h1 id="0409" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">关于我</h1><p id="370a" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">我是约克大学的一名硕士研究生，骨子里是一名软件工程师。在过去的十年里，我一直在软件开发、云计算和系统工程等领域的几个行业工作。目前，我正在研究云计算和分布式系统。</p><p id="16cc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你愿意，你可以在我的网站上查看我的作品。</p><h1 id="cae8" class="lh li iq bd lj lk nk lm ln lo nl lq lr ls nm lu lv lw nn ly lz ma no mc md me bi translated">参考</h1><p id="63b5" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">[1] Shai Shalev-Shwartz和Ben-David。理解机器学习:从理论到算法。剑桥大学出版社，2014年。DOI:10.1017/CBO9781107298019。网址:<a class="ae mk" href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . huji . AC . il/~ shais/understanding machine learning/understanding-machine-learning-theory-algorithms . pdf</a></p></div></div>    
</body>
</html>