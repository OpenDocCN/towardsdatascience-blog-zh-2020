<html>
<head>
<title>InterpretML: Analysis of SVM and XGBoost models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SVM和XGBoost模型分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretml-analysis-of-svm-and-xgboost-models-e68062f7299f?source=collection_archive---------46-----------------------#2020-10-13">https://towardsdatascience.com/interpretml-analysis-of-svm-and-xgboost-models-e68062f7299f?source=collection_archive---------46-----------------------#2020-10-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b6d6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用微软的MimicExplainer进行回归建模</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7f8feae91cf9f3928bfd81e61a8630db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s2Zkbf1r33ZL5AxW_fF-oA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:图片由<a class="ae ky" href="https://pixabay.com/users/geralt-9301/" rel="noopener ugc nofollow" target="_blank"> geralt </a>从<a class="ae ky" href="https://pixabay.com/illustrations/network-earth-block-chain-globe-3537401/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>拍摄</p></figure><p id="0612" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/interpretml/interpret" rel="noopener ugc nofollow" target="_blank">微软的InterpretML </a>旨在扩展机器学习模型的<strong class="lb iu">可解释性</strong>。换句话说，让那些模型更容易理解，最终便于人类解释。</p><p id="d98b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">微软的<a class="ae ky" href="https://github.com/interpretml/interpret-community" rel="noopener ugc nofollow" target="_blank"> Interpret-Community </a>是这个知识库的扩展，它包含了额外的可解释性技术。</p><p id="fa23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特别是，一个有用的特性是所谓的<strong class="lb iu"> MimicExplainer。</strong>这是一种<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/global.html" rel="noopener ugc nofollow" target="_blank">全局代理模型</a>，允许任何黑盒模型的可解释性。</p><h1 id="da5b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">背景</h1><p id="77a3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在本例中，MimicExplainer用于解释使用SVM(支持向量机)和XGBRegressor (XGBoost用于回归问题)构建的回归模型。</p><p id="02e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，这两个模型的用法如下:</p><ol class=""><li id="da1a" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">SVM用于预测使用特定特征的客户的<strong class="lb iu">平均日费率</strong>，例如他们的原产国、细分市场等。<a class="ae ky" rel="noopener" target="_blank" href="/support-vector-machines-and-regression-analysis-ad5d94ac857f?source=---------23----------------------------">原始调查结果见此处。</a></li><li id="cd08" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">XGBRegressor用作时间序列回归模型，通过将滞后序列与实际序列进行回归来预测每周取消的数量，即序列滞后高达t-5的5个滞后序列用作模型中的特征，以预测时间<strong class="lb iu"> t </strong>的取消值。原始发现可在此获得。</li></ol><p id="9b10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">原始数据来自<a class="ae ky" href="https://www.sciencedirect.com/science/article/pii/S2352340918315191" rel="noopener ugc nofollow" target="_blank"> Antonio、Almeida和Nunes (2019):酒店预订需求数据集</a>。</p><p id="3ee4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了演示MimicExplainer如何工作，我们展示了原始模型和结果，并提供了关于MimicExplainer如何使这些结果更具可解释性的更多信息。</p><h1 id="eb80" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">SVM</h1><p id="c5b5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了预测酒店预订的平均每日价格(或客户平均每日支付的价格)，构建了一个具有以下特征的SVM模型:</p><ul class=""><li id="b0f0" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu ng my mz na bi translated">取消(无论客户是否取消预订)</li><li id="3626" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu ng my mz na bi translated">原产国</li><li id="3f06" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu ng my mz na bi translated">细分市场</li><li id="619d" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu ng my mz na bi translated">存款类型</li><li id="78da" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu ng my mz na bi translated">客户类型</li><li id="cd11" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu ng my mz na bi translated">所需的停车位</li><li id="f13f" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu ng my mz na bi translated">抵达周</li></ul><p id="d4db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型被训练如下:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="9855" class="nm lw it ni b gy nn no l np nq">&gt;&gt;&gt; from sklearn.svm import LinearSVR<br/>&gt;&gt;&gt; svm_reg = LinearSVR(epsilon=1.5)<br/>&gt;&gt;&gt; svm_reg.fit(X_train, y_train)</span><span id="259e" class="nm lw it ni b gy nr no l np nq">LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,<br/>intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000, random_state=None, tol=0.0001, verbose=0)</span><span id="7134" class="nm lw it ni b gy nr no l np nq">&gt;&gt;&gt; predictions = svm_reg.predict(X_val)<br/>&gt;&gt;&gt; predictions</span><span id="ef4d" class="nm lw it ni b gy nr no l np nq">array([100.75090575, 109.08222631,  79.81544167, ...,  94.50700112,<br/>        55.65495607,  65.5248653 ])</span></pre><p id="2a40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当根据测试集验证该模型时，获得了44.6的RMSE(均方根误差)，以及29.5的MAE(平均绝对误差)。平均ADR为105，该模型在估计客户ADR值时显示了一定程度的预测能力。</p><p id="88f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是我们如何使用MimicExplainer来进一步解释这些结果。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="5e9b" class="nm lw it ni b gy nn no l np nq">from interpret.ext.blackbox import MimicExplainer</span><span id="759a" class="nm lw it ni b gy nr no l np nq">from interpret.ext.glassbox import LinearExplainableModel</span><span id="370b" class="nm lw it ni b gy nr no l np nq">explainer = MimicExplainer(svm_reg, <br/>                           X_train, <br/>                           LinearExplainableModel)</span></pre><p id="2c15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MimicExplainer是黑盒模型，而LinearExplainableModel被用作这个黑盒模型的全局代理。</p><p id="8bb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，在运行模型时，如果处理高维数据，即列数超过行数的情况，可以设置<strong class="lb iu"> augment_data = True </strong>。这允许对初始化样本进行过采样。</p><p id="d1b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，假设情况并非如此，即数据集的行数比列数(要素)多得多，则不会调用此功能。</p><p id="2e04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，打印出一个全局解释，其中包括前K个特性及其重要性值:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="a1f0" class="nm lw it ni b gy nn no l np nq">global_explanation = explainer.explain_global(X_val)</span><span id="dcfc" class="nm lw it ni b gy nr no l np nq">sorted_global_importance_values = global_explanation.get_ranked_global_values()</span><span id="9a27" class="nm lw it ni b gy nr no l np nq">sorted_global_importance_names = global_explanation.get_ranked_global_names()<br/>dict(zip(sorted_global_importance_names, sorted_global_importance_values))</span><span id="255d" class="nm lw it ni b gy nr no l np nq">global_explanation.get_feature_importance_dict()</span></pre><p id="bf30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是输出:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="72f5" class="nm lw it ni b gy nn no l np nq">{3: 8.81513709127725,<br/> 7: 4.9616362270740995,<br/> 1: 4.959263897550327,<br/> 6: 2.593931464493208,<br/> 2: 0.9707145503848649,<br/> 5: 0.8455564214901589,<br/> 4: 0.505321879369921,<br/> 0: 0.0}</span></pre><p id="6691" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面可以看出，特征号<strong class="lb iu"> 3 </strong>(细分市场)<strong class="lb iu"> 7 </strong>(抵达周)和<strong class="lb iu"> 1 </strong>(预订取消)是影响客户ADR的最重要特征。</p><p id="53d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，如果我们希望在验证数据中分离出某些观察结果呢？例如，假设我们只想确定选定的几个客户的特征重要性？</p><p id="5a7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个例子。</p><p id="e97d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据验证集，为订购10到15的客户确定重要特性及其值。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="eab5" class="nm lw it ni b gy nn no l np nq">local_explanation = explainer.explain_local(X_val[10:15])</span><span id="b140" class="nm lw it ni b gy nr no l np nq">sorted_local_importance_names = local_explanation.get_ranked_local_names()</span><span id="6654" class="nm lw it ni b gy nr no l np nq">sorted_local_importance_values = local_explanation.get_ranked_local_values()</span></pre><p id="77f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是已确定的重要特性及其价值:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="27ea" class="nm lw it ni b gy nn no l np nq">&gt;&gt;&gt; sorted_local_importance_names</span><span id="dc7e" class="nm lw it ni b gy nr no l np nq">[[1, 3, 2, 6,  5, 4, 0, 7], [7, 3, 2, 6, 5, 4, 1, 0], [6, 5, 4, 1, 0, 2, 7, 3], [6, 3,  5, 4, 1, 0, 2, 7], [3, 7, 6, 5, 4, 1, 0, 2]]</span><span id="dec1" class="nm lw it ni b gy nr no l np nq">&gt;&gt;&gt; sorted_local_importance_values</span><span id="331d" class="nm lw it ni b gy nr no l np nq">[[17.833762274315003,  9.1394041860457, 1.1694515257954607, 0.0, -0.0, -0.0, 0.0,  -1.707920714865971], [9.955928069584559, 9.1394041860457,  1.1694515257954607, 0.0, -0.0, -0.0, 0.0, 0.0], [0.0, -0.0, -0.0, 0.0,  0.0, -0.5708037209239748, -11.288939359236048, -13.709106279068548],  [19.017733248096473, 9.1394041860457, -0.0, -0.0, 0.0, 0.0,  -0.7448292455959183, -5.040448938994693], [9.1394041860457,  6.623399845455836, 0.0, -0.0, -0.0, 0.0, 0.0, -0.9884649801366393]]</span></pre><p id="e90f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这允许客户隔离特性的重要性。例如，功能1(预订取消)是对客户10影响最大的因素，而功能7(抵达周)是对客户11影响最大的因素。</p><h1 id="a83b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">XGBRegressor</h1><p id="4289" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如上所述，最初使用XGBRegressor模型是为了预测有问题的酒店的每周取消数量。</p><p id="2992" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型定义如下:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="0ae7" class="nm lw it ni b gy nn no l np nq">from xgboost import XGBRegressormodel = XGBRegressor(objective='reg:squarederror', n_estimators=1000)<br/>model.fit(X_train, Y_train)</span></pre><p id="3d6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是定义的模型参数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/5a8ec41dbe253a2de3708bb2175afa76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xux_7YSyFWlFDmyv.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><p id="db8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面我们可以看到，在训练XGBRegressor时，有许多模型参数可以修改。但是，在这种情况下，n_estimators被设置为1000。这定义了XGBoost模型中树的数量。目标设置为<a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">‘reg:squarederror’</a>，即平方损失回归，对极值误差的惩罚更重。</p><p id="8232" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">验证集的RMSE为50.14，平均相对误差为38.63，低于验证集的平均值109。</p><p id="fb99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用MimicExplainer，模型结果的可解释性现在可以以与上一个示例类似的方式生成:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="8c30" class="nm lw it ni b gy nn no l np nq">from interpret.ext.blackbox import MimicExplainer</span><span id="7744" class="nm lw it ni b gy nr no l np nq">from interpret.ext.glassbox import LinearExplainableModel</span><span id="3fe3" class="nm lw it ni b gy nr no l np nq">explainer = MimicExplainer(model, <br/>                           X_train, <br/>                           LinearExplainableModel)</span></pre><p id="65b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们可以确定在时间<strong class="lb iu"> t </strong>确定每周酒店取消价值时最重要的特征。然而，假设这是一个时间序列，该模型有效地指示了哪个滞后值在预测特定周的取消时最具信息性，例如，在影响时间<strong class="lb iu"> t </strong>的取消时，最重要的是1周滞后还是3周滞后？</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="20d4" class="nm lw it ni b gy nn no l np nq">global_explanation = explainer.explain_global(X_val)</span><span id="5bde" class="nm lw it ni b gy nr no l np nq">sorted_global_importance_values = global_explanation.get_ranked_global_values()</span><span id="a7ec" class="nm lw it ni b gy nr no l np nq">sorted_global_importance_names = global_explanation.get_ranked_global_names()</span><span id="5194" class="nm lw it ni b gy nr no l np nq">dict(zip(sorted_global_importance_names, sorted_global_importance_values))</span><span id="1e61" class="nm lw it ni b gy nr no l np nq">global_explanation.get_feature_importance_dict()</span></pre><p id="7f4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果如下:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="cdcc" class="nm lw it ni b gy nn no l np nq">{3: 10.580821439553645,<br/> 2: 7.795196757642633,<br/> 1: 4.973377270096975,<br/> 4: 2.329894438847138,<br/> 0: 1.382442979985477}</span></pre><p id="ce72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面可以看出，当预测时间<strong class="lb iu"> t </strong>的每周酒店取消时，特征3 (t-4)的滞后是最重要的。</p><p id="219e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，如果我们希望隔离某些周，该怎么办呢？</p><p id="1293" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">采用验证集，让我们将观察值<strong class="lb iu"> 8 </strong>到<strong class="lb iu"> 13 </strong>中的周分离出来(对应于2017年，第13到18周)。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="38f4" class="nm lw it ni b gy nn no l np nq">local_explanation = explainer.explain_local(X_val[8:13])</span><span id="28ac" class="nm lw it ni b gy nr no l np nq">sorted_local_importance_names = local_explanation.get_ranked_local_names()</span><span id="97dc" class="nm lw it ni b gy nr no l np nq">sorted_local_importance_values = local_explanation.get_ranked_local_values()</span></pre><p id="767e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是研究结果:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="2d2d" class="nm lw it ni b gy nn no l np nq">&gt;&gt;&gt; sorted_local_importance_names<br/>[[4, 0, 2, 1, 3], [3, 4, 1, 0, 2], [3, 4, 1, 2, 0], [3, 2, 0, 4, 1], [2, 3, 4, 0, 1]]</span><span id="21ef" class="nm lw it ni b gy nr no l np nq">&gt;&gt;&gt; sorted_local_importance_values<br/>[[1.2079747395122986, 0.655418467841234, -2.068988871651338,  -5.677678197831921, -16.414222669030814], [5.880283637250306,  1.524120206263528, 1.157876862894971, -1.807324914948728,  -11.282397723111108], [7.472748373413244, 7.056665874410039,  6.562734352772048, 3.8926286204696896, 0.353858053622055],  [35.340881256264645, 4.976559073582604, 2.0627004008640695,  1.1289383728244913, -2.3393838658490202], [23.945342003058602,  5.4821674532095725, 1.287011106200106, -0.7518634651816014,  -2.975249452893382]]</span></pre><p id="772a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以这种方式识别跨时间序列的重要性特征可以潜在地产生关于滞后值在预测跨不同周的取消中的重要性的有用见解。例如，我们可以看到，在预测第13周的取消时，t-5的滞后(特征4)被确定为最重要的特征。</p><p id="9424" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，当预测第18周的取消时，t-3的滞后(特征2)被认为是最重要的。</p><p id="e60d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与使用一刀切的预测模型相反，可解释性可以为特定时间点确定合适的预测方法，反过来又允许在不同时期定制时间序列预测方法。</p><h1 id="3365" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="8851" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这个例子中，我们看到了可解释性如何在分析机器学习输出时产生有价值的见解——当简单地使用黑盒ML模型时，这将更加困难。</p><p id="6e17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，我们已经看到:</p><ul class=""><li id="82b9" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu ng my mz na bi translated">如何将MimicExplainer用作全局代理模型，以允许任何黑盒模型的可解释性</li><li id="0063" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu ng my mz na bi translated">在回归问题中使用MimicExplainer</li><li id="c6f8" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu ng my mz na bi translated">如何解释MimicExplainer的结果并衡量特征重要性</li></ul><p id="58b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢您的宝贵时间，非常感谢您的任何问题或反馈。以上例子的相关Jupyter笔记本可以在<a class="ae ky" href="https://github.com/MGCodesandStats/hotel-cancellations/tree/master/notebooks%20and%20datasets/regression" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="733e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nt">免责声明:本文是在“原样”的基础上编写的，没有担保。本文旨在提供数据科学概念的概述，不应被解释为任何形式的专业建议。调查结果是作者的，与上述任何第三方无关。</em></p></div></div>    
</body>
</html>