<html>
<head>
<title>Gradient Descent With Momentum</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">动量梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-with-momentum-59420f626c8f?source=collection_archive---------5-----------------------#2020-10-03">https://towardsdatascience.com/gradient-descent-with-momentum-59420f626c8f?source=collection_archive---------5-----------------------#2020-10-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="4894" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">梯度下降的问题在于，在时刻(t)的权重更新仅由该时刻的学习速率和梯度控制。它没有考虑遍历成本空间时所采取的过去步骤。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ks"><img src="../Images/c4c347437eb3894d8ceae5d654fd76ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*_2yTRTRx7Gl9ykYP4kVc6g.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="le">作者图片</em></p></figure><p id="9a58" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">会导致以下问题。</p><ol class=""><li id="c799" class="lf lg iq jw b jx jy kb kc kf lh kj li kn lj kr lk ll lm ln bi translated">成本函数在鞍点(平台)的梯度可以忽略或为零，这又导致很小的或没有权重更新。因此，网络变得停滞，学习停止</li><li id="01b6" class="lf lg iq jw b jx lo kb lp kf lq kj lr kn ls kr lk ll lm ln bi translated">即使在小批量模式下运行，梯度下降所遵循的路径也非常不稳定</li></ol><p id="f6db" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">考虑低于成本的表面。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lt"><img src="../Images/36fad62d99de5bc248808c8e5675f938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aw3Wm0KDe_MDwApwNTeqZA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="le">作者图片</em></p></figure><p id="5eda" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们假设所考虑的网络的初始权重对应于点a。在梯度下降的情况下，损失函数沿着斜率AB迅速减小，因为沿着该斜率的梯度很高。但是一旦到达B点，梯度就变得非常低。B附近的权重更新很小。即使在多次迭代之后，成本在停留在梯度最终变为零的点之前移动得非常慢。</p><p id="b079" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这种情况下，理想情况下，成本应该移动到全局最小值点C，但是因为梯度在点B处消失，所以我们陷入了次优解。</p><p id="bf13" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">动量如何解决这个问题？</strong></p><p id="9088" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在，想象你有一个球从a点滚下。球开始慢慢滚下，并聚集了一些动量<strong class="jw ir">越过斜坡AB。当球到达点B时，它已经积累了足够的动量来推动自己穿过平台区域B，并最终沿着斜率BC降落在全局最小值c处。</strong></p><p id="8dd5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">如何将此应用于梯度下降？</strong></p><p id="7c0c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了说明<strong class="jw ir">动量</strong>，我们可以使用过去梯度的移动平均值。在像AB这样梯度高的区域中，权重更新将会很大。因此，在某种程度上，我们通过对这些梯度进行移动平均来收集<strong class="jw ir">动量</strong>。但是这种方法有一个问题，它以相等的权重考虑迭代中的所有梯度。t=0处的梯度与当前迭代t处的梯度具有相等的权重。我们需要使用过去梯度的某种加权平均值，以便给予最近的梯度更多的权重。</p><p id="b844" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这可以通过使用指数移动平均线(EMA)来实现。指数移动平均线是一种移动平均线，它对最近的值赋予更大的权重。</p><p id="3130" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">序列<em class="ly"> Y </em>的均线可以递归计算</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lz"><img src="../Images/640068a2138bec8591284c1914c164eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FNDRYotf1aZNfRylfRyumw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="le">作者图片</em></p></figure><p id="cccd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在哪里</p><ul class=""><li id="681e" class="lf lg iq jw b jx jy kb kc kf lh kj li kn lj kr ma ll lm ln bi translated">系数β表示权重增加的程度，是介于0和1之间的常数平滑因子。较低的β值会更快地忽略旧的观察值。</li><li id="3109" class="lf lg iq jw b jx lo kb lp kf lq kj lr kn ls kr ma ll lm ln bi translated"><em class="ly"> Y(t) </em>是周期<em class="ly"> t </em>的值。</li><li id="0ae5" class="lf lg iq jw b jx lo kb lp kf lq kj lr kn ls kr ma ll lm ln bi translated"><em class="ly"> S(t) </em>是任意时段<em class="ly"> t </em>的均线值。</li></ul><p id="abd7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在我们的梯度序列的情况下，迭代t处的新权重更新方程变成</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/dfbab0a752dd3bc847c4a63e17055fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*pOIJGGuMtDcQDK3ZGFAauQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="le">作者图片</em></p></figure><p id="6d7a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们来分解一下。</p><p id="01ce" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">𝓥 <strong class="jw ir"> (t) </strong>:新的权重更新是在迭代<strong class="jw ir"> t </strong>完成的吗</p><p id="62e9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> β: </strong>动量常数</p><p id="5143" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> 𝛿(t): </strong>是迭代时的梯度<strong class="jw ir"> t </strong></p><p id="3c54" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">假设在第0次迭代t=0时的权重更新为零</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mc"><img src="../Images/ab38c1c3b73f630becbaf063063e1f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-79j71PilAW3R8PZBcTqbg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="le">作者图片</em></p></figure><p id="5cb8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">想想常数β，忽略上式中的(1-β)项。</p><p id="fe6b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">注</strong>:在许多文本中，你可能会发现(1-β)被替换为学习率η。</p><p id="6eb2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果β是0.1呢？</p><p id="a971" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在n=3时；t =3处的梯度将贡献其值的100%，t=2处的梯度将贡献其值的10%，而t=1处的梯度仅贡献其值的1%。</p><p id="c2f7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这里，来自早期梯度的贡献迅速减少。</p><p id="44d8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果β是0.9呢？</p><p id="62eb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在n=3时；t =3处的梯度将贡献其值的100%，t=2处的梯度将贡献其值的90%，t=1处的梯度将贡献其值的81%。</p><p id="fe7f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">从上面我们可以推断，更高的β将容纳更多来自过去的梯度。因此，一般来说，在大多数情况下，β保持在0.9左右。</p><p id="16b9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">注</strong>:权重更新中每个梯度的实际贡献将进一步受制于学习率。</p><p id="46b9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这解决了我们的第一点，当当前时刻的梯度可以忽略或为零时，学习变为零。使用<strong class="jw ir">具有梯度下降的动量</strong>，来自过去的梯度将推动成本进一步围绕鞍点移动。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="6ffe" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在之前显示的成本面中，让我们放大c点。</p><p id="d894" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于梯度下降，如果学习率太小，权重将更新得非常慢，因此即使梯度很高，收敛也需要很多时间。如下图左侧所示。如果学习率太高，成本会在最小值附近波动，如下图右侧所示。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi md"><img src="../Images/9e648b8238d7f00c123180dd4cbb665c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hpywiujXSNBWq49MQwi0Cg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="le">作者图片</em></p></figure><p id="5c8c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">动量是如何解决这个问题的？</p><p id="f476" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们再来看看<strong class="jw ir">动量</strong>的最后一个求和方程。</p><p id="fdc9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">情况1 </strong>:当所有过去的梯度具有相同的符号时</p><p id="62bf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">求和项将变大，我们将在更新权重时采取大的步骤。沿着曲线BC，即使学习率低，沿着曲线的所有梯度将具有相同的方向(符号),从而增加<strong class="jw ir">动量</strong>并加速下降。</p><p id="06eb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">情况2 </strong>:当一些梯度具有+ve符号，而其他梯度具有-ve符号时</p><p id="c757" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">求和项将变小，权重更新将变小。如果学习率很高，围绕谷C的每次迭代的梯度将在+ve和-ve之间改变其符号，并且在几次振荡之后，过去梯度的总和将变小。因此，从那里开始对权重进行小的更新，并抑制振荡。</p><p id="68d6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这在一定程度上解决了我们的第二个问题。<strong class="jw ir">梯度下降带动量</strong>在梯度振荡的方向上迈小步，在过去的梯度具有相同方向(相同符号)的方向上迈大步。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="5d48" class="me mf iq bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated"><strong class="ak">结论</strong></h1><p id="e935" class="pw-post-body-paragraph ju jv iq jw b jx nc jz ka kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr ij bi translated">通过在梯度下降中添加一个<strong class="jw ir">动量</strong>项，即使当前梯度可以忽略或为零，从过去迭代中积累的梯度也将推动成本进一步围绕鞍点移动。</p><p id="943f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">即使梯度下降的<strong class="jw ir">动量</strong>收敛得更好更快，它仍然不能解决所有的问题。首先，超参数η(学习率)必须手动调整。第二，在某些情况下，即使学习率很低，动量项和电流梯度也能单独驱动并引起振荡。</p><p id="f628" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，学习率问题可以通过使用梯度下降的其他变化来进一步解决，如<strong class="jw ir">自适应梯度</strong>和<strong class="jw ir"> RMSprop。</strong>其次，大<strong class="jw ir">动量</strong>问题可以通过使用基于<strong class="jw ir">动量的梯度下降</strong>的变体<strong class="jw ir">内斯特罗夫加速梯度下降</strong>来进一步解决。</p></div></div>    
</body>
</html>