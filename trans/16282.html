<html>
<head>
<title>Getting Started with XGBoost in scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">scikit中的XGBoost入门-学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-xgboost-in-scikit-learn-f69f5f470a97?source=collection_archive---------0-----------------------#2020-11-10">https://towardsdatascience.com/getting-started-with-xgboost-in-scikit-learn-f69f5f470a97?source=collection_archive---------0-----------------------#2020-11-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4d24" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="5a00" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">每一段旅程都从一次助推开始</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/4b29e7d27e01623dbe40eff0fafc249e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ASgi_j7R6YOgDHChINaf4g.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:【https://www.pxfuel.com/en/free-photo-juges T2】</p></figure><p id="bb45" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">这篇文章解释了XGBoost是什么，为什么XGBoost应该是你的首选机器学习算法，以及在Colab或Jupyter笔记本上启动和运行XGBoost所需的代码。假设您对机器学习和Python有基本的了解。</em></p><h2 id="e53e" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">XGBoost是什么</h2><p id="63e9" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在机器学习中，集成模型比单个模型有更高的概率表现更好。集成模型将不同的机器学习模型组合成一个。随机森林是一个流行的集合，它通过装袋取许多决策树的平均值。Bagging是“bootstrap aggregation”的缩写，意思是通过替换(bootstrapping)选择样本，并通过取其平均值进行组合(聚合)。</p><p id="bcfb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">助推是装袋的一种强有力的替代方法。助推器不是聚集预测，而是通过关注单个模型(通常是决策树)哪里出错，将弱学习者变成强学习者。在梯度推进中，单个模型基于残差训练，残差是预测和实际结果之间的差异。梯度提升树不是聚集树，而是在每一轮提升过程中从错误中学习。</p><p id="2d55" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">XGBoost是“极端梯度推进”的缩写。“极致”是指并行计算和缓存感知等速度增强，使XGBoost比传统的梯度提升快大约10倍。此外，XGBoost包括一个独特的分裂查找算法来优化树，以及内置的正则化来减少过度拟合。一般来说，XGBoost是一个更快、更精确的渐变增强版本。</p><p id="946c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">平均而言，boosting比bagging表现得更好，梯度Boosting可以说是最好的Boosting系综。由于XGBoost是梯度增强的高级版本，其结果是无与伦比的，因此它可以说是我们拥有的最好的机器学习组合。</p><h2 id="affc" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">为什么XGBoost应该是您的首选机器学习算法</h2><p id="ca60" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">从2014年的希格斯玻色子Kaggle比赛开始，XGBoost在机器学习领域掀起了风暴，经常在Kaggle比赛中获得一等奖。XGBoost的受欢迎程度飙升，因为在竞争环境中，当从表格数据(行和列的表格)进行预测时，它一直优于可比的机器学习算法。</p><p id="0f9f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从表格数据进行预测时，XGBoost可能是您的最佳起点，原因如下:</p><ul class=""><li id="81b9" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated">XGBoost很容易在scikit-learn中实现。</li><li id="b6b8" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">XGBoost是一个系综，所以比单个模型得分高。</li><li id="75b6" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">XGBoost是正则化的，所以默认模型通常不会过度拟合。</li><li id="2ae4" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">XGBoost非常快(对于系综而言)。</li><li id="5e26" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">XGBoost从错误中学习(梯度提升)。</li><li id="01fe" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">XGBoost有大量用于微调的超参数。</li><li id="df7f" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">XGBoost包括超参数来缩放不平衡的数据和填充空值。</li></ul><p id="fa35" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，您对XGBoost是什么以及为什么XGBoost应该是处理表格数据时的首选机器学习算法有了更好的了解(与神经网络工作得更好的图像或文本等非结构化数据相比)，让我们建立一些模型。</p><h2 id="3aaa" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">安装XGBoost</h2><p id="2812" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">如果您运行的是Colab笔记本，XGBoost是一个选项。如果你在Jupyter笔记本上运行Anaconda，你可能需要先安装它。打开您的终端并运行以下命令来安装XGBoost和Anaconda:</p><p id="e0e2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe nq nr ns nt b">conda install -c conda-forge xgboost</code></p><p id="2ad9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您想要验证安装或您的XGBoost版本，请运行以下命令:</p><p id="67ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe nq nr ns nt b">import xgboost; print(xgboost.__version__)</code></p><p id="25a4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如需更多选项，请查看<a class="ae lh" href="https://xgboost.readthedocs.io/en/release_0.72/build.html" rel="noopener ugc nofollow" target="_blank"> XGBoost安装指南。</a></p><p id="3d8b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来我们拿一些数据来做预测。</p><h2 id="11e0" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">scikit-learn中的XGBRegressor</h2><p id="537d" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">Scikit-learn带有几个内置数据集，您可以访问这些数据集来快速对模型进行评分。以下代码加载scikit-learn糖尿病数据集，该数据集测量一年后疾病的传播程度。更多信息请参见<a class="ae lh" href="https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset" rel="noopener ugc nofollow" target="_blank">sci kit-学习数据集加载页面</a>。</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="d13d" class="mf mg it nt b gy ny nz l oa ob">from sklearn import datasets<br/>X,y = datasets.load_diabetes(return_X_y=True)</span></pre><p id="0c49" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">衡量糖尿病传播程度的指标可能会采用连续值，因此我们需要一个机器学习回归器来进行预测。XGBoost回归器称为<strong class="lk jd"> XGBRegressor </strong>，可按如下方式导入:</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="c267" class="mf mg it nt b gy ny nz l oa ob">from xgboost import XGBRegressor</span></pre><p id="6940" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以使用交叉验证在多个折叠上构建模型并对其评分，这总是一个好主意。使用交叉验证的一个优点是它会为您拆分数据(默认情况下是5次)。首先导入<strong class="lk jd"> cross_val_score </strong>。</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="bc68" class="mf mg it nt b gy ny nz l oa ob">from sklearn.model_selection import cross_val_score</span></pre><p id="894f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要使用XGBoost，只需将XGBRegressor放在cross_val_score中，以及X、y和您首选的回归得分指标。我更喜欢均方根误差，但这需要转换负的均方误差作为一个额外的步骤。</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="6d33" class="mf mg it nt b gy ny nz l oa ob">scores = cross_val_score(XGBRegressor(), X, y, scoring='neg_mean_squared_error')</span></pre><p id="3fac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您得到警告，那是因为XGBoost最近更改了他们默认回归目标的名称，他们希望您知道。要消除警告，请尝试以下方法，结果相同:</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="568f" class="mf mg it nt b gy ny nz l oa ob"><em class="me"># Alternative code to silence potential errors<br/></em>scores = cross_val_score(<strong class="nt jd">XGBRegressor(objective='reg:squarederror')</strong>, X, y, scoring='neg_mean_squared_error')</span></pre><p id="f632" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">求均方根误差，取5个分数的负平方根即可。</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="341e" class="mf mg it nt b gy ny nz l oa ob">(-scores)**0.5<em class="me"> </em></span></pre><p id="b8d1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">回想一下，在Python中，语法x**0.5表示x的1/2次方，也就是平方根。</p><p id="9dd0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我的<a class="ae lh" href="https://colab.research.google.com/drive/1HLZTwEWrzVVhsVZUkAeJcCJ-Ayrx4CHE?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>结果如下。</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="1db2" class="mf mg it nt b gy ny nz l oa ob">array([56.04057166, 56.14039793, 60.3213523 , 59.67532995, 60.7722925 ])</span></pre><p id="c4e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你喜欢一个分数，试试<strong class="lk jd"> scores.mean() </strong>求平均值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/8b1b0b6a5595a8662a26f36350e8ec0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MR-1fjH58_Vp-m_E4EDAOA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">预期的希格斯玻色子衰变。欧洲核子研究中心制作的照片。通用创作许可证。<a class="ae lh" href="https://commons.wikimedia.org/wiki/File:3D_view_of_an_event_recorded_with_the_CMS_detector_in_2012_at_a_proton-proton_centre_of_mass_energy_of_8_TeV.png" rel="noopener ugc nofollow" target="_blank">链接到维基百科。</a></p></figure><h2 id="c681" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">xgb回归码</h2><p id="655e" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">下面是使用scikit-learn中的XGBoost回归器预测糖尿病进展的所有代码。</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="59ee" class="mf mg it nt b gy ny nz l oa ob">from sklearn import datasets<br/>X,y = datasets.load_diabetes(return_X_y=True)<br/>from xgboost import XGBRegressor<br/>from sklearn.model_selection import cross_val_score<br/>scores = cross_val_score(<strong class="nt jd">XGBRegressor(objective='reg:squarederror')</strong>, X, y, scoring='neg_mean_squared_error')<br/>(-scores)**0.5</span></pre><p id="8687" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如你所看到的，由于2019年推出的新scikit-learn包装器，XGBoost与其他scikit-learn机器学习算法的工作原理相同。</p><h2 id="8684" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">scikit-learn中的XGBClassifier</h2><p id="b773" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">接下来，让我们使用类似的步骤构建一个XGBoost分类器并进行评分。</p><p id="6b4c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下url包含可用于预测患者是否患有心脏病的心脏病数据集。</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="1a54" class="mf mg it nt b gy ny nz l oa ob">url = ‘https://media.githubusercontent.com/media/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/master/Chapter02/heart_disease.csv'</span></pre><p id="0f24" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该数据集包含13个预测值列，如胆固醇水平和胸痛。最后一栏标有“目标”，确定患者是否患有心脏病。原始数据集的来源位于<a class="ae lh" href="https://archive.ics.uci.edu/ml/datasets/heart+disease" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库</a>。</p><p id="cc86" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">导入pandas来读取csv链接并将其存储为DataFrame，df。</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="d597" class="mf mg it nt b gy ny nz l oa ob">import pandas as pd<br/>df = pd.read_csv(url)</span></pre><p id="179e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于目标列是最后一列，并且该数据集已经过预清理，因此可以使用索引位置将数据拆分为X和y，如下所示:</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="43d9" class="mf mg it nt b gy ny nz l oa ob">X = df.iloc[:, :-1]<br/>y = df.iloc[:, -1]</span></pre><p id="4aec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，导入XGBClassifier并使用cross_val_score对模型进行评分，将准确性作为默认的评分标准。</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="3675" class="mf mg it nt b gy ny nz l oa ob">from xgboost import XGBClassifier<br/>from sklearn.model_selection import cross_val_score<br/>cross_val_score(XGBClassifier(), X, y)</span></pre><p id="d9a1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下是我从我的<a class="ae lh" href="https://colab.research.google.com/drive/1HLZTwEWrzVVhsVZUkAeJcCJ-Ayrx4CHE?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>中得到的结果。</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="2b12" class="mf mg it nt b gy ny nz l oa ob">array([0.85245902, 0.85245902, 0.7704918 , 0.78333333, 0.76666667])</span></pre><h2 id="567f" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">xgb分类器代码</h2><p id="104e" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">以下是使用scikit中的XGBClassifier预测患者是否患有心脏病的所有代码——了解五个方面:</p><pre class="ks kt ku kv gt nu nt nv nw aw nx bi"><span id="e4eb" class="mf mg it nt b gy ny nz l oa ob">url = 'https://media.githubusercontent.com/media/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/master/Chapter02/heart_disease.csv'</span><span id="5107" class="mf mg it nt b gy od nz l oa ob">import pandas as pd<br/>df = pd.read_csv(url)<br/>X = df.iloc[:, :-1]<br/>y = df.iloc[:, -1]</span><span id="5512" class="mf mg it nt b gy od nz l oa ob">from xgboost import XGBClassifier<br/>from sklearn.model_selection import cross_val_score<br/>cross_val_score(XGBClassifier(), X, y)</span></pre><p id="f039" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您知道如何在scikit中构建XGBoost分类器和回归器并对其评分——轻松学习。</p><h2 id="36ac" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">XGBoost后续步骤</h2><p id="9f18" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">从XGBoost中获得更多需要微调超参数。XGBoost由许多决策树组成，因此除了集成超参数之外，还有决策树超参数可以进行微调。查看这篇<a class="ae lh" href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" rel="noopener ugc nofollow" target="_blank"> Analytics Vidhya文章</a>，以及<a class="ae lh" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">官方XGBoost参数文档</a>开始使用。</p><p id="a9c3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你正在寻找更多的深度，我的书<a class="ae lh" href="https://www.amazon.com/Hands-Gradient-Boosting-XGBoost-scikit-learn/dp/1839218355/ref=sr_1_1?dchild=1&amp;keywords=xgboost&amp;qid=1604947393&amp;sr=8-1" rel="noopener ugc nofollow" target="_blank"> <em class="me">用XGBoost和scikit手动渐变增强-从<a class="ae lh" href="https://www.packtpub.com/product/hands-on-gradient-boosting-with-xgboost-and-scikit-learn/9781839218354" rel="noopener ugc nofollow" target="_blank"> Packt Publishing </a>学习</em> </a> <em class="me"> </em>是一个很好的选择。除了广泛的超参数微调，您将了解XGBoost在机器学习领域的历史背景，XGBoost案例研究的详细信息，如Higgs boson Kaggle竞赛，以及高级主题，如调整备选基础学习器(gblinear，DART，XGBoost随机森林)和为行业部署模型。</p><p id="6796" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">编码快乐！</p><p id="8c6e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">科里·韦德(Corey Wade)是</em> <a class="ae lh" href="http://berkeleycodingacademy.com" rel="noopener ugc nofollow" target="_blank"> <em class="me">伯克利编码学院</em> </a> <em class="me">的创始人和主管，在这里他向来自世界各地的学生教授机器学习。此外，Corey在伯克利高中的独立学习项目中教授数学和编程。著有两本书，</em> <a class="ae lh" href="https://www.amazon.com/Hands-Gradient-Boosting-XGBoost-scikit-learn/dp/1839218355/ref=sr_1_1?dchild=1&amp;keywords=xgboost&amp;qid=1604947393&amp;sr=8-1" rel="noopener ugc nofollow" target="_blank"> <em class="me">用XGBoost和scikit-learn </em> </a> <em class="me">和</em><a class="ae lh" href="https://www.amazon.com/Python-Workshop-Interactive-Approach-Learning/dp/1839218851/ref=sr_1_1_sspa?dchild=1&amp;keywords=The+Python+Workshop&amp;qid=1604971133&amp;sr=8-1-spons&amp;psc=1&amp;spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUE3R1dTUDlVVVVDU1omZW5jcnlwdGVkSWQ9QTA5NjYyNjYxQzdQSDhZTzJUVklUJmVuY3J5cHRlZEFkSWQ9QTAzNTAyNjQyN0E5NUlRNDFNU0NCJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==" rel="noopener ugc nofollow" target="_blank"><em class="me">The Python Workshop</em></a><em class="me">。</em></p><p id="e625" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">本文中的代码可能是直接抄袭了</em> <a class="ae lh" href="https://colab.research.google.com/drive/1HLZTwEWrzVVhsVZUkAeJcCJ-Ayrx4CHE?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <em class="me">科里的Colab笔记本</em> </a> <em class="me">。</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/0406f2d551aac87ca42fc07c3c6f113a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QRMPny9g9mP9ADSvaYzung.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">亚历山大·辛恩摄影:<a class="ae lh" href="https://unsplash.com/photos/KgLtFCgfC28" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/KgLtFCgfC28</a></p></figure></div></div>    
</body>
</html>