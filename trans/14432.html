<html>
<head>
<title>Advanced Ensemble Learning Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高级集成学习技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/advanced-ensemble-learning-techniques-bf755e38cbfb?source=collection_archive---------11-----------------------#2020-10-05">https://towardsdatascience.com/advanced-ensemble-learning-techniques-bf755e38cbfb?source=collection_archive---------11-----------------------#2020-10-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9eaa" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">合奏是一门艺术和科学</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/be7404c1ffff77f595c67b010cb7dcde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3u1z5-XcnWrJu0j7cCcgCg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">杰斯温·托马斯在<a class="ae kv" href="https://unsplash.com/s/photos/ensemble-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="55ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我以前关于集成学习的帖子中，我解释了什么是集成学习，它如何与机器学习中的偏差和方差相关联，以及什么是集成学习的简单技术。如果你没有看过这篇文章，请参考这里的<a class="ae kv" rel="noopener" target="_blank" href="/ensemble-learning-techniques-6346db0c6ef8"/>。</p><p id="cb7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我将介绍集成学习类型，高级集成学习方法——打包、提升、堆叠和与代码样本混合。最后，我将解释使用集成学习的利与弊。</p><h1 id="3b52" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">集成学习类型</h1><p id="f2e6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">集成学习方法可以分为两组:</p><p id="73e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 1。顺序集成方法</strong></p><p id="ccc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种方法中，基础学习者依赖于先前基础学习者的结果。每一个后续的基础模型都会修正它的前任所做的预测，修正其中的错误。因此，可以通过提高先前标签的权重来提高整体性能。</p><p id="33bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 2。并行集成方法</strong></p><p id="fde9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种方法中，基本学习器之间没有依赖性，所有基本学习器并行执行，并且所有基本模型的结果最终被组合(对回归使用平均，对分类问题使用投票)。</p><p id="b722" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并行集成方法分为两类</p><p id="f22a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 1。同类并行集成方法- </strong>在这种方法中，单个机器学习算法被用作基本学习器。</p><p id="3744" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 2。异构并行集成方法- </strong>在该方法中，多个机器学习算法被用作基础学习器。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="4bb7" class="ls lt iq bd lu lv mw lx ly lz mx mb mc jw my jx me jz mz ka mg kc na kd mi mj bi translated">高级集成技术</h1><h1 id="dd23" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">制袋材料</h1><p id="98a5" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Bagging或Bootstrap聚合是一种并行集成学习技术，用于减少最终预测中的方差。</p><p id="ae49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">bagging过程与平均非常相似，唯一的区别是Bagging使用原始数据集的随机子样本来训练相同/多个模型，然后组合预测，而在平均中，使用相同的数据集来训练模型。<strong class="ky ir">因此，该技术被称为自举聚合，因为它结合了自举(或数据采样)和聚合来形成集合模型。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/fa46fa18b49de086d822121baff19bd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*ePJ0VwU3Uz5CU9Y6YjCT6g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="a174" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">装袋有三个步骤-</p><p id="a9b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.用每个样本建立模型(分类器或决策树)。</p><p id="ebfb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.所有基本模型的预测被组合(对于回归问题使用平均或加权平均，对于分类问题使用多数投票)以获得最终结果。</p><p id="f562" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有这三个步骤都可以跨不同的子样本并行化，因此在处理较大的数据集时，训练可以更快地完成。</p><p id="2b3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在bagging中，每个基础模型都在不同的数据子集上进行训练，并且所有结果都进行组合，因此最终模型较少过度拟合，并且方差减少。</p><blockquote class="nc nd ne"><p id="2008" class="kw kx nf ky b kz la jr lb lc ld ju le ng lg lh li nh lk ll lm ni lo lp lq lr ij bi translated">当模型不稳定时，装袋更有用，对于稳定的模型，装袋在提高性能方面没有用处。当模型对训练数据中的小波动不太敏感时，称为稳定模型。</p></blockquote><p id="a02e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">装袋的一些例子是——随机森林、装袋决策树、额外树。sklearn库还提供BaggingClassifier和BaggingRegressor类来创建自己的bagging算法。</strong></p><p id="612a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看下面的例子</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="a921" class="nq lt iq nm b gy nr ns l nt nu">LogisticRegression :::: Mean: 0.7995780505454071 , Std Dev: 0.006888373667690784<br/>Bagging LogisticRegression :::: Mean: 0.8023420359806932 Std Dev: 0.00669463780099821 </span><span id="3c83" class="nq lt iq nm b gy nv ns l nt nu">DecisionTreeClassifier :::: Mean: 0.8119073077604059 , Std Dev: 0.005729415273647502<br/>Bagging DecisionTreeClassifier :::: Mean: 0.849639923635328 Std Dev: 0.0046034229502244905 </span><span id="656d" class="nq lt iq nm b gy nv ns l nt nu">RandomForestClassifier :::: Mean: 0.8489381115139759 , Std Dev: 0.005116577814042257<br/>Bagging RandomForestClassifier :::: Mean: 0.8567037712754901 Std Dev: 0.004468761007278419 </span><span id="dcae" class="nq lt iq nm b gy nv ns l nt nu">ExtraTreesClassifier :::: Mean: 0.8414792383547726 , Std Dev: 0.0064275238258043816<br/>Bagging ExtraTreesClassifier :::: Mean: 0.8511317483045042 Std Dev: 0.004708539080690846 </span><span id="9e1a" class="nq lt iq nm b gy nv ns l nt nu">KNeighborsClassifier :::: Mean: 0.8238853221249702 , Std Dev: 0.006423083088668752<br/>Bagging KNeighborsClassifier :::: Mean: 0.8396364017767104 Std Dev: 0.00599320955270458 </span><span id="fb11" class="nq lt iq nm b gy nv ns l nt nu">VotingClassifier :::: Mean: 0.8462174468641986 Std Dev: 0.006423083088668752</span></pre><p id="2101" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们在示例中看到的，Bagging分类器改善了ML模型的方差并减少了偏差。当使用改进ML模型的平均方差的VotingClassifier时，情况也是如此。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="3f08" class="ls lt iq bd lu lv mw lx ly lz mx mb mc jw my jx me jz mz ka mg kc na kd mi mj bi translated">助推</h1><p id="03e1" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Boosting是一种顺序集成学习技术，用于将弱基础学习者转换为表现更好且偏差更小的强学习者。这里的直觉是，单个模型可能不会在整个数据集上表现得很好，但它们在整个数据集的某个部分上表现得很好。因此，集合中的每个模型实际上都提升了整体性能。</p><blockquote class="nc nd ne"><p id="d4d6" class="kw kx nf ky b kz la jr lb lc ld ju le ng lg lh li nh lk ll lm ni lo lp lq lr ij bi translated">提升是一种迭代方法，它根据先前的分类调整观察值的权重。如果一个观察被错误地分类，那么该观察的权重在下一次迭代中增加。以同样的方式，如果一个观察被正确分类，那么该观察的权重在下一次迭代中被减少。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/1741a5b98aa411ce391a2a4e3a441880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9xT1CmKgxFEhI_2h5eygCg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4087" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">提升用于减少偏差误差，但它也会过度拟合训练数据。这就是为什么参数调整是boosting算法的一个重要部分，以使它们避免过度拟合数据。</p><p id="4f48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Boosting最初是为分类问题设计的，但也扩展到了回归问题。</p><p id="35cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">Boosting算法的一些例子有— AdaBoost、梯度Boosting Machine (GBM)、XGBoost、LightGBM、CatBoost。</strong></p><p id="cbf9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来看一个例子</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="4c7e" class="nq lt iq nm b gy nr ns l nt nu">AdaBoostClassifier :::: Mean: 0.8604337082284473 Std Dev: 0.0032409094349287403<br/>GradientBoostingClassifier :::: Mean: 0.8644262257222698 Std Dev: 0.0032315430892614675<br/>XGBClassifier :::: Mean: 0.8641189579917322 Std Dev: 0.004561102596800773<br/>VotingClassifier :::: Mean: 0.864645581703271 Std Dev: 0.0032985215353102735</span></pre></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="92f8" class="ls lt iq bd lu lv mw lx ly lz mx mb mc jw my jx me jz mz ka mg kc na kd mi mj bi translated">堆垛</h1><blockquote class="nc nd ne"><p id="7b54" class="kw kx nf ky b kz la jr lb lc ld ju le ng lg lh li nh lk ll lm ni lo lp lq lr ij bi translated">堆叠，也称为堆叠泛化，是一种集成学习技术，通过元学习(元分类器或元回归器)结合多种机器学习算法。</p></blockquote><p id="e54b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基础级算法在整个训练数据集上训练，然后元模型在作为特征的来自所有基础级模型的预测上训练。基础模型称为0级模型，结合基础模型预测的元模型称为1级模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a52742c639eb10d48b33ddbb88c5ff15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*acHOIQmK58pjaDq-r4xHOw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="2a51" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1级模型训练数据是通过基本模型的k倍交叉验证准备的，折外预测(用于回归的实数和用于分类的类别标签)用作训练数据集。</p><p id="697f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">0级模型可以是不同范围的算法，也可以是相同的算法(通常它们是不同的)。第一级元模型通常是一个简单的模型，如回归问题的线性回归和分类问题的逻辑回归。</p><p id="f849" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">堆叠方法可以减少基于0级算法的偏差或方差。</p><p id="5400" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">用于堆叠的库有很多，像— StackingClassifier，StackingRegressor，make_classification，make_regression，ML Ensemble，H20。</strong></p><p id="589d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看一个使用StackingClassifier的例子</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="f9a5" class="nq lt iq nm b gy nr ns l nt nu">LogisticRegression :::: Mean: 0.799198344149096 Std Dev: 0.004958323931953346<br/>DecisionTreeClassifier :::: Mean: 0.8130779055654346 Std Dev: 0.008467878845801694<br/>KNeighborsClassifier :::: Mean: 0.8251287819886122 Std Dev: 0.00634438876282278<br/>SVC :::: Mean: 0.8004562250294449 Std Dev: 0.005221775246052317<br/>GaussianNB :::: Mean: 0.7964780515718138 Std Dev: 0.004996489474526567<br/>StackingClassifier :::: Mean: 0.8376917712960184 Std Dev: 0.005593816155570199</span></pre><p id="0e0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们在本例中看到的，我们在级别0中使用了不同的ML模型，并在级别1中使用LogisticRegression将它们与StackingClassifier堆叠在一起，这改善了方差。</p><h2 id="ffbc" class="nq lt iq bd lu ny nz dn ly oa ob dp mc lf oc od me lj oe of mg ln og oh mi oi bi translated"><strong class="ak">多层堆叠</strong></h2><p id="63ba" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">多级堆叠是堆叠的扩展，其中堆叠应用于多个层。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/2264568f060140b3aa7d5040a7d44c68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gu7S_y0lMHxe8vc-IGxNNg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="dce7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，在3级堆叠中，0级是相同的，其中使用k-fold交叉验证来训练不同范围的基础学习者。在级别1中，使用N个这样的元模型，而不是单个元模型。在级别2中，使用从级别1的N个元模型中获得预测的最终元模型。</p><p id="0fee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">增加多个层次既耗费数据(因为需要训练大量数据)，又耗费时间(因为每一层都要增加多个模型)。</strong></p><h1 id="52bf" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">混合</strong></h1><p id="e310" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">混合最常与堆叠互换使用。它几乎类似于堆叠，只有一个不同之处，<strong class="ky ir">堆叠使用训练集的非折叠预测，而混合使用保留(验证)集(训练集的10–20%)来训练下一层。</strong></p><p id="bd5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管混合比堆叠更简单，并且使用的数据更少，但最终模型可能会在维持集上过度拟合。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="73a7" class="ls lt iq bd lu lv mw lx ly lz mx mb mc jw my jx me jz mz ka mg kc na kd mi mj bi translated">集合方法的优势/好处</h1><p id="dd89" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">1.与单个模型相比，集成方法具有更高的预测精度。</p><p id="8f8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.当数据集中既有线性数据又有非线性数据时，集成方法非常有用；可以组合不同的模型来处理这种类型的数据。</p><p id="0ab9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.使用集合方法可以减少偏差/方差，并且大多数情况下，模型不会欠拟合/过拟合。</p><p id="faee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.模型的集合总是更少噪音并且更稳定。</p><h1 id="eac5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">集成学习的缺点</h1><p id="2602" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">1.集合的可解释性较差，集合模型的输出难以预测和解释。因此，合奏的想法很难推销，也很难获得有用的商业见解。</p><p id="4a3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.集成的艺术很难学习，任何错误的选择都会导致比单个模型更低的预测准确性。</p><p id="d305" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.就时间和空间而言，组装是昂贵的。因此，投资回报率可以随着组装而增加。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="b9d7" class="ls lt iq bd lu lv mw lx ly lz mx mb mc jw my jx me jz mz ka mg kc na kd mi mj bi translated">结论</h1><p id="a5a9" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在看了上面的集成学习技术的基础和集成学习的优点/缺点之后，如果使用正确，集成方法对于提高ML模型的整体性能是非常好的，这不会是错误的。当很难依赖一个模型时，集合使生活变得更容易，这就是为什么在大多数ML比赛中，集合方法是获胜者的选择。</p><p id="1c9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然选择正确的合奏方法并使用它们不是一件容易的工作，但这种艺术可以通过经验来学习。本帖中描述的技术通常是集合的可靠来源，但是基于特定的问题/需求，其他变体也是可能的。</p><p id="e17d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要访问高级合奏技术的完整代码，请查看Github <a class="ae kv" href="https://github.com/charumakhijani/Ensemble-Learning/blob/master/EnsembleTechniques1.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="314f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谢谢你的阅读。如果你喜欢这个故事，请喜欢，分享和关注更多这样的内容。如往常一样，请联系我们以获得任何问题/评论/反馈。</p><p id="b2df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">https://github.com/charumakhijani<em class="nf"><br/>LinkedIn:</em><a class="ae kv" href="https://www.linkedin.com/in/charu-makhijani-23b18318/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/charu-makhijani-23b18318/</a></p></div></div>    
</body>
</html>