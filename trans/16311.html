<html>
<head>
<title>Why I use Fastai and you should too.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么我用Fastai，你也应该用。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-i-use-fastai-and-you-should-too-a421f6c99508?source=collection_archive---------29-----------------------#2020-11-10">https://towardsdatascience.com/why-i-use-fastai-and-you-should-too-a421f6c99508?source=collection_archive---------29-----------------------#2020-11-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1f71" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这是一个多部分系列的第1部分:关于我最喜欢的深度学习库fastai，我最喜欢的事情。</h2></div><h1 id="ad78" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">这一集:学习率(LR)</h1><h2 id="e36a" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">fastai之前的LR</h2><p id="8106" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">找到最佳LR的一般共识通常是用不同LR的不同优化器完全训练模型，直到达到期望的度量。最佳LR和优化器的选择取决于它们在选择阶段的最佳组合。这是一种不错的技术，尽管计算量很大。</p><blockquote class="mf mg mh"><p id="ee56" class="lm ln mi lo b lp mj jr lr ls mk ju lu ml mm lw lx mn mo lz ma mp mq mc md me ij bi translated">注:由于我是在深度学习生涯的早期被介绍给fastai的，所以我不太了解在没有fastai的情况下/之前事情是如何完成的，所以如果这有一点不准确，请让我知道，也不要完全相信这一部分。</p></blockquote><h1 id="04d7" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">快速方法</h1><p id="8de7" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">通向LRs的捷径受到了Leslie Smith论文[1]的影响。寻找最佳LR主要有3个组成部分，寻找用于训练的最佳LR(在LR寻找部分中解释)，随着训练的进行，减少LR(在LR退火部分中解释)，以及迁移学习的一些注意事项(在判别LR中解释)和一个周期训练(LR退火的一部分)。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/21419b040febb58c8e408e5b29b81483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5zGROIoKOXwjQ2fy2T7NrQ.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">LR和后果。图片由作者提供。</p></figure><p id="8d69" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated"><strong class="lo ir">我们的学习率应该是多少？<br/> </strong>这是一个要问的重要问题，因为学习率是驱动我们的模型参数达到最优解的因素。太低，学习时间太长。太高的话，模型<em class="mi">甚至</em>都学不会。我们需要一个在一定范围内的学习率，它能以合理的速度使参数收敛。</p><h2 id="45ae" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">LR查找</h2><p id="696d" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">LR find是fastai寻找好的学习率的方法。他们首先选择一个非常低的LR，在这个LR下训练一个小批量，然后计算损耗。以递增的更高LR训练下一个小批量，并且这个过程继续，直到我们达到模型明显偏离的LR。</p><p id="f4de" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">LR是相对于损耗绘制的，我们可以看到下图。存在一个损耗最小的特定LR，在此之后，再增加LR将使损耗恶化。</p><p id="9431" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">选择损耗最低的LR是错误的。我们需要损失减少最快的LR。这也是图表中最陡的部分。Fastai实现了一个函数来查找除以10的最陡和最小LR(这也是一个很好的训练LR)</p><pre class="ms mt mu mv gt nh ni nj nk aw nl bi"><span id="8f31" class="kx kg iq ni b gy nm nn l no np">learn.lr_find() <a class="ae nq" href="https://docs.fast.ai/callback.schedule#Learner.lr_find" rel="noopener ugc nofollow" target="_blank">[docs]</a></span></pre><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/fff88dc1a59eb87ce69f2db6856c7128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*nt29NXRRF-LvpXuCzHqclQ.png"/></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">LR查找。图片来自<a class="ae nq" href="https://github.com/fastai/fastbook" rel="noopener ugc nofollow" target="_blank"> fastiai的fastbook </a>，GPL-3.0许可证</p></figure><blockquote class="mf mg mh"><p id="12e7" class="lm ln mi lo b lp mj jr lr ls mk ju lu ml mm lw lx mn mo lz ma mp mq mc md me ij bi translated">这种方法在计算上更便宜和更快，因为我们只需要1个时期(甚至不需要)来找到最佳LR，而传统上我们将为LR的每个逻辑范围训练1个时期(肯定总共多于1个时期)。</p></blockquote><h2 id="a900" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">低温退火</h2><p id="0379" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">谜题的下一步是LR退火。最初，我们的参数是不完善的。它们与最佳参数相差甚远。但是随着我们训练，参数越来越接近最佳值。</p><p id="9622" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">当我们的参数非常远离最优(开始)时，我们希望在最优的大致方向上采取更大、更不精确的步骤。但是当我们越来越接近时，我们不希望采取大的步骤而意外地超过optima，我们希望采取较小的步骤来获得精确的完美参数集。</p><blockquote class="mf mg mh"><p id="c15c" class="lm ln mi lo b lp mj jr lr ls mk ju lu ml mm lw lx mn mo lz ma mp mq mc md me ij bi translated">这类似于高尔夫球。你不要试图一杆进洞，(你可以，但更多的是运气而不是技巧)，你只是试着在球洞的大致方向上尽可能远地高球。当你靠近时，你改变球杆以获得更高的精确度和控制力，并寻找更小的步伐，每一步都让你更靠近球洞。</p></blockquote><p id="d813" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">这就是为什么在训练开始时，我们想要大的学习率，这促使我们努力快速地接近最佳参数，但当我们接近最佳参数时，我们想要降低学习率。随着我们的损失减少，我们希望采取更小的步骤，因此使用更小的LR。</p><p id="647d" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">这种在训练期间改变LR的过程被称为LR衰减/ LR退火。下图显示了较大的初始步骤与较小的最终步骤的对比。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/b815f690cff9caf6ff072be5680a60d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hsp6I5_ezQUJH6FSdGjgww.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">Lr退火。作者图片</p></figure><h2 id="e63e" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">适合一个周期</h2><p id="5740" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">当前对基于GD(梯度下降)的训练的难度的普遍共识是优化器可能陷入鞍点。这与我们过去认为的主要问题(局部最小值)不同。Leslie Smith在[3]中表明，增加LR有助于避开鞍点并到达损失函数的良好区域，此后，我们再次降低LR，原因在LR退火中解释。然而，Sylvain Gugger <a class="ae nq" href="https://sgugger.github.io/the-1cycle-policy.html" rel="noopener ugc nofollow" target="_blank">在这里</a>解释了拟合一个周期方法的最后一步，即将LR减少到最后几次迭代的最小LR的百分之一。也被称为湮灭。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ns"><img src="../Images/f031c797d216faf1beca7711f60fa412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZzkfkJBMk6pnX5OVYedR-A.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">鞍点。图片由尼科瓜罗——自己的作品，CC由3.0，【https://commons.wikimedia.org/w/index.php?curid=20570051 T2】</p></figure><p id="80b4" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">步骤很简单，选择一个LR，如本文的LR查找部分所述。这是最大可接受的LR。我们还选择一个最小LR，正如Sylvain所建议的，是最大LR的十分之一。现在我们在这个范围之间循环，其中循环长度略小于历元总数。最后，我们将LR降至最小LR的百分之一。</p><pre class="ms mt mu mv gt nh ni nj nk aw nl bi"><span id="6f38" class="kx kg iq ni b gy nm nn l no np">fit_one_cycle<!-- -->(<strong class="ni ir">learn</strong>:<a class="ae nq" href="https://fastai1.fast.ai/basic_train.html#Learner" rel="noopener ugc nofollow" target="_blank">Learner</a>, <strong class="ni ir">cyc_len</strong>:<!-- -->int<!-- -->, <strong class="ni ir">max_lr</strong>:<!-- -->Union<!-- -->[<!-- -->float<!-- -->, <!-- -->Collection<!-- -->[<!-- -->float<!-- -->], <!-- -->slice<!-- -->]=<strong class="ni ir"><em class="mi">slice(None, 0.003, None)</em></strong>, <strong class="ni ir">moms</strong>:<!-- -->Point<!-- -->=<strong class="ni ir"><em class="mi">(0.95, 0.85)</em></strong>, <strong class="ni ir">div_factor</strong>:<!-- -->float<!-- -->=<strong class="ni ir"><em class="mi">25.0</em></strong>, <strong class="ni ir">pct_start</strong>:<!-- -->float<!-- -->=<strong class="ni ir"><em class="mi">0.3</em></strong>, <strong class="ni ir">final_div</strong>:<!-- -->float<!-- -->=<strong class="ni ir"><em class="mi">None</em></strong>, <strong class="ni ir">wd</strong>:<!-- -->float<!-- -->=<strong class="ni ir"><em class="mi">None</em></strong>, <strong class="ni ir">callbacks</strong>:<!-- -->Optional<!-- -->[<!-- -->Collection<!-- -->[<a class="ae nq" href="https://fastai1.fast.ai/callback.html#Callback" rel="noopener ugc nofollow" target="_blank">Callback</a>]]=<strong class="ni ir"><em class="mi">None</em></strong>, <strong class="ni ir">tot_epochs</strong>:<!-- -->int<!-- -->=<strong class="ni ir"><em class="mi">None</em></strong>, <strong class="ni ir">start_epoch</strong>:<!-- -->int<!-- -->=<strong class="ni ir"><em class="mi">None</em></strong>) [<a class="ae nq" href="https://fastai1.fast.ai/train.html#fit_one_cycle" rel="noopener ugc nofollow" target="_blank">docs</a>]</span></pre><p id="88b6" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">安排LR时有两个主要参数可以使用，这就是我们在单周期策略中所做的事情。这些是动量和步长。更多关于LR如何安排的信息和进一步阅读，请点击<a class="ae nq" href="https://fastai1.fast.ai/callbacks.one_cycle.html#The-1cycle-policy" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nt"><img src="../Images/9cd281675dd705780c0adc06f329a511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fmgYmIAcYd18mqX5Oegc3w.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">一个周期左右。图片由fastai课程，Apache-2.0许可。</p></figure><h2 id="8d4b" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">辨别LR(迁移学习期间)</h2><p id="a00f" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">迁移学习是使用为一项任务训练的神经网络，在最少的训练后做不同的任务的过程。这是非常有用的，原因我现在将解释。</p><p id="1766" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">泽勒和弗格斯发表了一篇令人惊叹的论文，名为《可视化和理解卷积网络》[2],他们在论文中展示了神经网络的不同层学习的内容，并将不同层学习的内容可视化。从下图中，我们可以看到第一层识别基本的线条、颜色和颜色渐变。第二层识别更复杂的形状，如边缘和圆形，然后到了第三层，网络开始识别模式。</p><p id="bd75" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">现在，假设你有一个可以识别猫的猫检测器。你想做一个熊探测器。比起随机噪声，猫更像熊，猫探测器的大多数层已经学习了非常有用的参数。所以我们只需要微调模型，改变最后几层。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nu"><img src="../Images/076daab8c10e52239752ffab6b12d1a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9_7xjaHD-5Pad926afsrHA.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">神经网络的不同层次看到了什么。来源:<a class="ae nq" href="https://arxiv.org/pdf/1311.2901.pdf" rel="noopener ugc nofollow" target="_blank">可视化和理解卷积网络</a>。</p></figure><p id="f3d5" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">而fastai非常适合做这件事。fastai迁移学习的方法基本上是两个步骤。</p><ol class=""><li id="65bc" class="nv nw iq lo b lp mj ls mk lc nx lf ny li nz me oa ob oc od bi translated">我们首先冻结我们的模型，这意味着停止对早期层的梯度计算，只训练最后2层。</li><li id="66fe" class="nv nw iq lo b lp oe ls of lc og lf oh li oi me oa ob oc od bi translated">然后我们解冻这个模型，这样渐变就会一直流回来。然而，我们已经知道早期的层不需要大量的学习，因为它们已经学习了重要的参数(像图像的线条这样的公共属性)。因此，我们需要较低的学习率用于早期层，较高的学习率用于后期层。Fastai内置了这一功能。</li></ol><p id="f175" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">用快速人工智能进行迁移学习的4行代码是</p><pre class="ms mt mu mv gt nh ni nj nk aw nl bi"><span id="c6fe" class="kx kg iq ni b gy nm nn l no np">learn = cnn_learner(dls, resnet34, metrics=error_rate) <br/>#import a model of your choice, pretrained on imagenet (default)</span><span id="ecb7" class="kx kg iq ni b gy oj nn l no np">learn.fit_one_cycle(3, 3e-3) <br/>#train the last few layers of model</span><span id="b5f4" class="kx kg iq ni b gy oj nn l no np">learn.unfreeze() <br/>#unfreeze the model, so we can train all layers</span><span id="1719" class="kx kg iq ni b gy oj nn l no np">learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))<br/># use a lr_max, which uses lowest value of LR for first layer<br/># and increments LR for later layers</span></pre><p id="f3c6" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">下图是模型解冻后LR对损失的曲线图。正如我们所看到的，损失并没有急剧下降，这与我们的想法是一致的，即初始层已经学到了有用的信息。因此，即使在开始时损失也很低，并逐渐下降。</p><blockquote class="mf mg mh"><p id="dca5" class="lm ln mi lo b lp mj jr lr ls mk ju lu ml mm lw lx mn mo lz ma mp mq mc md me ij bi translated">选择LR的方法保持不变，最小值/10或最陡下降点</p></blockquote><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/0d5536c27caad8ac5f9fc1967ce538eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*TuCAZJMN-0_GcC7yd8MhnA.png"/></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">微调期间的LR。图片来自<a class="ae nq" href="https://github.com/fastai/fastbook" rel="noopener ugc nofollow" target="_blank"> fastiai的fastbook </a>，GPL-3.0许可证</p></figure><h1 id="4e8e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">参考</h1><p id="468b" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">[1].<a class="ae nq" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Smith%2C+L+N" rel="noopener ugc nofollow" target="_blank"> Leslie N. Smith </a>，神经网络超参数的训练方法:第1部分——学习速率、批量大小、动量和权重衰减(2018 <em class="mi">)，</em> <a class="ae nq" href="https://arxiv.org/abs/1803.09820" rel="noopener ugc nofollow" target="_blank"> arXiv:1803.09820 </a></p><p id="ede1" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">[2].<a class="ae nq" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zeiler%2C+M+D" rel="noopener ugc nofollow" target="_blank">马修·D·泽勒</a>，<a class="ae nq" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fergus%2C+R" rel="noopener ugc nofollow" target="_blank">罗布·弗格斯</a>，可视化和理解卷积网络(2013)，<a class="ae nq" href="https://arxiv.org/abs/1311.2901" rel="noopener ugc nofollow" target="_blank"> arXiv:1311.2901 </a></p><p id="a631" class="pw-post-body-paragraph lm ln iq lo b lp mj jr lr ls mk ju lu lc mm lw lx lf mo lz ma li mq mc md me ij bi translated">[3].<a class="ae nq" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Smith%2C+L+N" rel="noopener ugc nofollow" target="_blank"> Leslie N. Smith </a>，训练神经网络的循环学习率(2017)，<a class="ae nq" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank"> arXiv:1506.01186 </a></p></div></div>    
</body>
</html>