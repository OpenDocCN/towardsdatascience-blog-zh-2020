<html>
<head>
<title>A Simple Reinforcement Learning Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个简单的强化学习问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/apply-reinforcement-learning-to-win-a-free-throw-bet-f555b8adc0de?source=collection_archive---------30-----------------------#2020-09-28">https://towardsdatascience.com/apply-reinforcement-learning-to-win-a-free-throw-bet-f555b8adc0de?source=collection_archive---------30-----------------------#2020-09-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="419c" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="0105" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">通过将一次罚球赌注建模为马尔可夫链并使用值迭代，开始强化学习</h2></div><p id="275d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">与<a class="lk ll ep" href="https://medium.com/u/98505f8c082?source=post_page-----f555b8adc0de--------------------------------" rel="noopener" target="_blank">麦克斯·奇西克</a>合著</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/8cf98199f344324e34dbf86824702bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5dBCuALhs5vODaTx"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">照片由<a class="ae mc" href="https://unsplash.com/@rapiana?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">拉米罗·皮亚诺罗萨</a>在<a class="ae mc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="e13a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="md">本文假设读者熟悉基本的强化学习(RL)概念。这篇</em> <a class="ae mc" rel="noopener" target="_blank" href="/reinforcement-learning-101-e24b50e1d292"> <em class="md">文章</em> </a> <em class="md">提供了一个很好的高层次概述。这里的目的是解决一个简单的问题，以便说明RL问题是如何设置的，这样您就能够开始编写自己的项目了。</em></p><p id="9f45" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你需要在100次罚球中投进90次才能赢得赌注。您可以在一年内无限制尝试，并且您需要指定每次尝试的开始时间。你应该如何决定是继续目前的尝试还是重新开始？在成功完成赌注之前，您预计要打多少杆？你的重置策略对这个数字有什么影响？强化学习可以回答这些问题。</p><p id="c5b9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">根据二项式分布，78%的射手一次尝试成功的概率为0.14%，这相当于365次尝试成功的概率为40%。让我们假设78%是你的命中率，因为这给出了赢得或输掉赌注的合理概率(高于80%几乎保证成功，低于70%几乎保证失败)。</p><p id="8a3f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">方法1:强化学习—马尔可夫链</strong> <br/>如果从一个状态转移到下一个状态的概率完全基于当前状态(即先前的状态不影响概率)，则随机过程是一个<a class="ae mc" href="https://en.wikipedia.org/wiki/Markov_chain" rel="noopener ugc nofollow" target="_blank">马尔可夫链</a>。这个赌注可以被建模为一个马尔可夫链，以投篮次数和失误次数作为状态。</p><p id="27c5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有1001种不同的可能状态(0到10次未命中，0到90次成功)⟹11 * 91 = 1001。因此，赌注可以由1，001×1，001矩阵来表示，其中行表示当前状态，列表示未来状态，元素𝑋𝑖𝑗 {∀ 𝑖,𝑗∈[0,1000]}表示从𝑖状态转移到𝑗.状态的概率</p><p id="8f5f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先，让我们假设你一直投篮直到失败或成功(投中90篮或错过11篮)。然后将矩阵P的状态0到1000定义为[篮子已做，篮子未做]。先列出所有不完整的状态，然后在最后列出代表赢得赌注的状态:<br/> 0，1，…，89 = [0，0]，[0，1]，…，[0，89]；<br/> 90，91，…，179 = [1，0]，[1，1]，…，[1，89]；<br/> ⋮ <br/> 900，901，…，989 = [10，0]，[10，1]，…，[10，89]；<br/> 990，991，…，1000 =[0，90]，[1，90]，…，[10，90]；<br/></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi me"><img src="../Images/c4f161dd3c988225e240f4b1dad97c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9KKAK1gWYhPF6LGFvIU8EQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">概率矩阵P的图解(图片由作者提供)</p></figure><p id="bbd6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">吸收态是一旦进入就不会离开的状态。然后，让矩阵𝑇是矩阵𝑃的子集，只包括跃迁(非吸收)状态。𝑇 = 𝑃₀:₉₉₀,₀:₉₉₀，因为最后十个状态代表成功，所以一旦进入其中一个状态就不会再尝试<em class="md">(这就是我们将最后十个状态设置为成功状态的原因，这样可以很容易地删除它们)。</em></p><p id="339e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">定义矩阵𝐶，使得每个元素𝑋𝑖𝑗都是在进入一个吸收态之前，在任何时间点从𝑖态到𝑗态的预期跃迁数(即使中间进入了其它态)。可以看出，矩阵𝐶计算如下:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/8649bb3ca029f7f4885a93769199fe90.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*LkyTQmfKbr_yJHz7IqkJXA.png"/></div></figure><p id="2682" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">那么第0行的总和就是成功所需的预期总发射数。当拍摄到失败或成功时，预期拍摄到成功的次数为<strong class="kq ja"> 35，418 </strong>。当基于二项式分布重置时(如果当前状态比新尝试成功的可能性低，则重置)，总拍摄次数减少到<strong class="kq ja"> 13，262: </strong></p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="mg mh l"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">使用二项式重置策略射击失败/成功时的预期射击次数</p></figure><p id="deb6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">方法2a:强化学习—值迭代:确定最佳策略<br/> </strong>我们能够直接计算重置策略和预期投篮次数，因为我们知道投篮次数是一个二项分布的随机变量。然而，许多强化学习问题的底层分布是未知的，例如预测库存管理问题的客户订单或预测扑克游戏中对手的行动。因此，我们将使用值迭代来计算最优重置策略，以说明如何处理未知分布。</p><p id="7678" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先，我们定义给定环境中代理可用的状态、动作和奖励:</p><ul class=""><li id="1edc" class="mi mj iq kq b kr ks ku kv kx mk lb ml lf mm lj mn mo mp mq bi translated">定义每个状态对(投篮命中，投篮未中)的奖励为0，除了设置状态中的赌注是赢，[90，0]，[90，1]，…，[90，10]。将获胜州的值设置为100(任意选择)。</li><li id="0b07" class="mi mj iq kq b kr mr ku ms kx mt lb mu lf mv lj mn mo mp mq bi translated">每种状态下可能的2个动作:继续射击或重置赌注。</li></ul><p id="1021" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后将每个状态/动作对的初始值设置为0，并重复迭代每个对，以使用<a class="ae mc" href="https://en.wikipedia.org/wiki/Bellman_equation" rel="noopener ugc nofollow" target="_blank">贝尔曼方程</a>求解更新的值估计，直到连续迭代之间的最大差异变得任意小。<br/> V(s)=R(s，a)+γ÷V(s′)，其中:</p><ul class=""><li id="a03a" class="mi mj iq kq b kr ks ku kv kx mk lb ml lf mm lj mn mo mp mq bi translated">V(s)是当前状态的值</li><li id="a988" class="mi mj iq kq b kr mr ku ms kx mt lb mu lf mv lj mn mo mp mq bi translated">R(s，a)是状态s(射击或重置)下动作a的回报</li><li id="3be6" class="mi mj iq kq b kr mr ku ms kx mt lb mu lf mv lj mn mo mp mq bi translated">γ是贴现因子</li><li id="a77c" class="mi mj iq kq b kr mr ku ms kx mt lb mu lf mv lj mn mo mp mq bi translated">v(s′)是下一个状态的值</li></ul><p id="bb5f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">每个状态都有两个值，一个是继续出手，等于出手后的预期奖励值加上下一个状态的值，每个状态都有一个重置下注的值，这个值简单来说就是等于起始状态的值(因为重置时永远不会有奖励)。国家的价值是这两个价值中最高的。例如，89时的值makes，10次未中(下一次出手的结果不是赢得赌注就是重置赌注)等于:<br/> v[89，10] = max(动作[出手]: 78% * 100 + 22% * 0.98 * v[0，0]，动作[重置]: v[0，0])；<br/> = max(动作[拍摄]:78.0047，动作[复位]:0.0217)；<br/> =动作【拍】:78.0047。</p><p id="d73e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">不出所料，这种状态下的最佳政策是继续当前的尝试。这也证明了状态是如何相互依赖的:v[0，0]依赖于v[89，10]，但是v[89，10]也依赖于v[0，0]，因此，为什么需要继续迭代计算，直到从一次迭代到下一次迭代的值变化几乎为0。</p><p id="bf72" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">值迭代代码:</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="mg mh l"/></div></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/a051245f708c5fc27eed2c62acd7e808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*SOPCzw-PpRVvDSaq4qwTtQ.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者提供的图像:使用值迭代的最佳重置策略(例如，如果第5次失误发生在第30次拍摄时或之前，则尝试重置)</p></figure><p id="2995" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从马尔可夫链代码来看，这种重置策略总共有13，026个预期镜头，与二项式方法相比提高了1.7%。</p><p id="54ea" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">方法2b:模拟——计算预期射击次数</strong></p><p id="f140" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由于分布并不总是已知的，我们还将使用数值迭代确定的最佳策略，通过模拟来计算预期的射击次数。</p><p id="104a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们模拟大量尝试，取成功前的平均出手次数来估计期望值。为此，我们生成一个足够大的0到1之间的均匀随机数列表，低于投篮命中率的值表示投篮命中，高于投篮命中率的值表示投篮不中。然后，我们确定是否达到了代表失败的任何重置阈值。如果没有达到重置阈值，那么至少进行了90次罚球，结果是成功的。然后计算每次尝试成功所需的击球次数，并取这些值的平均值。</p><p id="6c15" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">模拟代码:</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="2753" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">上述代码10次迭代的平均结果是13，145个预期镜头，比上面直接计算的值高0.9%。</p><p id="f2e5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">运行模拟还有一个额外的好处，那就是我们可以估计直到成功的射击次数的分布:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/b3a8c480eeaca1f61028eee27085bc05.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*IC2dzIyhSVrWDIrm9AP1fQ.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">1217次模拟成功尝试的模拟镜头分布(图片由作者提供)</p></figure><p id="b727" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">结论</strong></p><p id="6c03" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们能够找到我们问题的封闭形式的解决方案，因为它是一个马尔可夫链。然而，许多强化学习问题更加复杂，需要迭代方法。我们展示了这样一种方法，值迭代，也可以解决这个问题。策略迭代是本文没有涉及的另一种有用的方法。</p><p id="998b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">感谢您的阅读，并祝您好运解决自己的强化学习问题！</p><p id="c76c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">补充阅读</strong></p><ul class=""><li id="face" class="mi mj iq kq b kr ks ku kv kx mk lb ml lf mm lj mn mo mp mq bi translated"><a class="ae mc" href="https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf" rel="noopener ugc nofollow" target="_blank">https://www . Dartmouth . edu/~ chance/teaching _ AIDS/books _ articles/probability _ book/chapter 11 . pdf</a></li><li id="3bd8" class="mi mj iq kq b kr mr ku ms kx mt lb mu lf mv lj mn mo mp mq bi translated"><a class="ae mc" href="https://www.cis.upenn.edu/~cis519/fall2015/lectures/14_ReinforcementLearning.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cis . upenn . edu/~ cis 519/fall 2015/lections/14 _ reinforcement learning . pdf</a></li></ul></div></div>    
</body>
</html>