<html>
<head>
<title>Recreating Keras code in PyTorch- an introductory tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在PyTorch中重新创建Keras代码——入门教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recreating-keras-code-in-pytorch-an-introductory-tutorial-8db11084c60c?source=collection_archive---------13-----------------------#2020-09-23">https://towardsdatascience.com/recreating-keras-code-in-pytorch-an-introductory-tutorial-8db11084c60c?source=collection_archive---------13-----------------------#2020-09-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1748" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">初学者的深度学习</h2><div class=""/><div class=""><h2 id="572f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">学习在PyTorch中创建神经网络的基础知识</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3dcb57a567fa85988d9df1300622f2de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*k5qgLRy3WIxztEmb.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者于<a class="ae lh" href="https://imgflip.com/i/4fycse" rel="noopener ugc nofollow" target="_blank"> Imgflip </a>创建</p></figure><p id="7ce8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="me">我爱Keras，我说对了！然而……</em></strong></p><p id="ec96" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">作为一名应用数据科学家，没有什么比用三行代码快速构建一个功能性的神经网络更让我高兴的了！然而，随着我开始更深入地钻研神经网络的黑暗网络，我愿意接受Pytorch <em class="me">确实让你对你的网络架构有更大的控制权这一事实。</em></p><p id="163c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">鉴于我们大多数人对Keras都相当熟悉(如果不熟悉，请参见<a class="ae lh" rel="noopener" target="_blank" href="/beginners-guide-to-building-artificial-neural-networks-using-keras-in-python-bdc4989dab00">这里</a>的Keras热情介绍)，学习在Pytorch中创建一个类似的网络(同时学习Pytorch基础知识)一点也不困难。我们开始吧！</p><p id="d92a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">注意:我们不会复制Keras代码，因为我想在这个入门教程中介绍PyTorch的更多特性和功能！</em></p><h1 id="f5de" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">快速回顾一下Keras代码的样子:</h1><p id="0a52" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">下面是创建模型架构、编译模型和最终训练模型的代码片段(来自我之前关于Keras中神经网络的<a class="ae lh" rel="noopener" target="_blank" href="/beginners-guide-to-building-artificial-neural-networks-using-keras-in-python-bdc4989dab00">帖子</a>)。它是一个贷款评估模型，输出贷款应该被接受还是拒绝。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="1dea" class="nh mg it nd b gy ni nj l nk nl"><em class="me"># Model architecture</em><br/>model_m = Sequential([<br/>    Dense(units = 8, input_shape= (2,), activation = 'relu'),<br/>    Dense(units = 16, activation = 'relu'),<br/>    Dense(units = 2, activation = 'softmax') <br/>])</span><span id="8092" class="nh mg it nd b gy nm nj l nk nl"><em class="me"># Model compilation</em><br/>model_m.compile(optimizer= Adam(learning_rate = 0.0001), <br/>              loss = 'sparse_categorical_crossentropy', <br/>              metrics = ['accuracy'] <br/>             )</span><span id="fc3e" class="nh mg it nd b gy nm nj l nk nl"><em class="me"># Model Training and Validation</em><br/>model_m.fit(x = scaled_train_samples_mult, <br/>          y = train_labels, <br/>          batch_size= 10, <br/>          epochs = 30, <br/>          validation_split= 0.1, <br/>          shuffle = True,<br/>          verbose = 2 <br/>         )</span></pre><p id="d16a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">总结一下，我们构建了一个有三个隐层的网络，都是<code class="fe nn no np nd b">Dense</code>。三个隐藏层的激活功能分别是<code class="fe nn no np nd b">relu</code>、<code class="fe nn no np nd b">relu</code>和<code class="fe nn no np nd b">softmax</code>。<code class="fe nn no np nd b">input_shape</code>是一个元组<code class="fe nn no np nd b">(2,0)</code>，意味着我们有两个预测特征。<code class="fe nn no np nd b">Adam</code>优化器和学习率<code class="fe nn no np nd b">lr = 0.0001</code>已用于根据训练数据迭代更新网络权重。我们将在每次迭代中监控的损失是<code class="fe nn no np nd b">sparse_categorical_crossentropy</code>。<code class="fe nn no np nd b">accuracy</code>将用于判断网络的好坏。对于培训，我们将使用30个<code class="fe nn no np nd b">epochs</code>和10个<code class="fe nn no np nd b">batch_size</code>。如果上述任何术语的含义不清楚，请参见此处的<a class="ae lh" rel="noopener" target="_blank" href="/beginners-guide-to-building-artificial-neural-networks-using-keras-in-python-bdc4989dab00">或此处的</a>或<a class="ae lh" rel="noopener" target="_blank" href="/beginners-guide-to-building-convolutional-neural-networks-using-tensorflow-s-keras-api-in-python-6e8035e28238"/>。</p><p id="c70a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">注:如果你想跟随本教程，请查看Github </em>  <em class="me">上的笔记本。</em></p><h1 id="9521" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">让我们从PyTorch开始吧</h1><p id="3404" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">为了与我们之前使用Keras构建的网络保持一定程度的一致性，我将使用相同的数据集，即贷款申请数据集。它看起来是这样的:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/6d770d4d24cdd5cbf77780ab3750d91e.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*UXhluqL-nXx3BsyKWwLeaQ.png"/></div></figure><p id="166f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在对数据集应用一些预处理后，主要是在0-1的范围内缩放所有输入特征(<code class="fe nn no np nd b">age</code>和<code class="fe nn no np nd b">area</code>)并对结果特征(<code class="fe nn no np nd b">application_outcome</code>)进行标签编码，这就是我们最终得到的结果:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/2ceb225c1c26eae578126b9854127260.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*MNFI1xkuC6NKaJuIxzEZig.png"/></div></figure><h2 id="9f78" class="nh mg it bd mh ns nt dn ml nu nv dp mp lr nw nx mr lv ny nz mt lz oa ob mv iz bi translated">在PyTorch中定义数据集</h2><p id="7167" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">我们将使用PyTorch中的<code class="fe nn no np nd b">torch.utils.data.Dataset</code>类为贷款申请数据集定义一个<code class="fe nn no np nd b">Dataset</code>对象。我们将称它为<code class="fe nn no np nd b">CVSDataset</code>。</p><p id="d58c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将在我们的<code class="fe nn no np nd b">CSVDataset</code>类中有四个方法— <code class="fe nn no np nd b">__init__</code>、<code class="fe nn no np nd b">__len__</code>、<code class="fe nn no np nd b">__getitem__</code>和<code class="fe nn no np nd b">get_splits</code>。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="1318" class="nh mg it nd b gy ni nj l nk nl">class CSVDataset(Dataset):<br/>    <br/>    <em class="me"># reading the csv and defining predictor and output columns</em><br/>    def __init__(self):<br/>    <br/>        # store the input and output features<br/>        self.X = df.values[:,:-1] <br/>        self.y = df.values[:,-1] <br/>    <br/>        # ensure all data is numerical - type(float)<br/>        self.X = self.X.astype('float32')<br/>        self.y = self.y.astype('float32')<br/>    <br/>    <em class="me"># number of rows in dataset</em><br/>    def __len__(self):<br/>        return len(self.X)<br/>    <br/>    <em class="me"># get a row at an index</em><br/>    def __getitem__(self, index):<br/>        return [self.X[index], self.y[index]]</span><span id="4280" class="nh mg it nd b gy nm nj l nk nl">    # split into train and testset - using `random_split`<br/>    def get_splits(self, split_ratio = 0.2):<br/>        test_size = round(split_ratio * len(self.X))<br/>        train_size = len(self.X) - test_size<br/>        <br/>        return random_split(self, [train_size, test_size])</span></pre><blockquote class="oc od oe"><p id="4da1" class="li lj me lk b ll lm kd ln lo lp kg lq of ls lt lu og lw lx ly oh ma mb mc md im bi translated">注意:在非常基础的层面上，你为自己的数据集扩展的<code class="fe nn no np nd b">Dataset</code>类应该有<code class="fe nn no np nd b">__init__</code>、<code class="fe nn no np nd b">__len__()</code>和<code class="fe nn no np nd b">__getitem__ </code>方法。</p></blockquote><p id="b994" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们首先在<code class="fe nn no np nd b">__init__()</code>中定义预测器<code class="fe nn no np nd b">X</code>和输出特征<code class="fe nn no np nd b">y</code>，并确保类型是浮点型的，因为神经网络只处理数字数据。</p><p id="38b6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe nn no np nd b">__len__</code>返回数据集中的行数，而<code class="fe nn no np nd b">__getitem__</code>返回数据集中特定索引处的项目。</p><p id="34e1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，我们还定义了一个(可选的)方法<code class="fe nn no np nd b">get_splits</code>，让我们将数据集分成训练集和测试集。我们可以在Sklearn中使用<code class="fe nn no np nd b">train_test_split</code>很好地完成这项工作，但这只是展示如何在PyTorch中使用<code class="fe nn no np nd b">torch.utils.data.random_split</code>完成这项工作的一种方式。</p><h2 id="ed78" class="nh mg it bd mh ns nt dn ml nu nv dp mp lr nw nx mr lv ny nz mt lz oa ob mv iz bi translated">定义模型架构</h2><p id="d665" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">这是我们定义所有隐藏层在我们的网络中想要什么以及这些层如何相互作用的地方。为此，我们将利用<code class="fe nn no np nd b">torch.nn.Module</code>类并扩展它。我们将称呼我们的班级为<code class="fe nn no np nd b">myNeuralNetwork</code>。</p><p id="bff7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它包含定义层的<code class="fe nn no np nd b">__init__</code>方法和解释输入如何通过定义的层向前传播的<code class="fe nn no np nd b">forward</code>方法。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="4469" class="nh mg it nd b gy ni nj l nk nl">class myNeuralNetwork(Module):<br/>      def __init__(self, n_inputs):<br/>        .<br/>        .<br/>        .</span><span id="8c54" class="nh mg it nd b gy nm nj l nk nl">      def forward(self,X):<br/>        .<br/>        .<br/>        .</span></pre><p id="10be" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们先来看一下<code class="fe nn no np nd b">init</code>方法:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="4704" class="nh mg it nd b gy ni nj l nk nl">def __init__(self, n_inputs):<br/>        <br/>        # calling constructor of parent class<br/>        super().__init__()<br/>        <br/>        # defining the inputs to the first hidden layer<br/>        self.hid1 = Linear(n_inputs, 8) <br/>        kaiming_uniform_(self.hid1.weight, nonlinearity='relu')<br/>        self.act1 = ReLU()<br/>        <br/>        # defining the inputs to the second hidden layer<br/>        self.hid2 = Linear(8, 16)<br/>        kaiming_uniform_(self.hid2.weight, nonlinearity='relu')<br/>        self.act2 = ReLU()<br/>        <br/>        # defining the inputs to the third hidden layer<br/>        self.hid3 = Linear(16, 2)<br/>        xavier_uniform_(self.hid3.weight)<br/>        self.act3 = Softmax(dim=1)</span></pre><p id="182f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们首先使用<code class="fe nn no np nd b">super().__init__()</code>调用父类的构造函数。接下来，我们定义三个隐藏层<code class="fe nn no np nd b">hid1</code>、<code class="fe nn no np nd b">hid2</code>和<code class="fe nn no np nd b">hid3</code>，以及它们的权重初始化和激活函数——<code class="fe nn no np nd b">act1</code>、<code class="fe nn no np nd b">act2</code>和<code class="fe nn no np nd b">act3</code>。</p><p id="af8d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您还记得本文开头对Keras模型的总结，我们有三个密集的隐藏层。<strong class="lk jd"> Pytorch相当于Keras致密层的是</strong> <code class="fe nn no np nd b"><strong class="lk jd">Linear</strong></code>。</p><p id="7079" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第一个隐藏线性层<code class="fe nn no np nd b">hid1</code>采用<code class="fe nn no np nd b">n_inputs</code>个输入，输出8个神经元/单元。</p><blockquote class="oc od oe"><p id="4fb5" class="li lj me lk b ll lm kd ln lo lp kg lq of ls lt lu og lw lx ly oh ma mb mc md im bi translated">注意:<code class="fe nn no np nd b">n_inputs</code>粗略地翻译为我们有多少预测列(在我们的例子2中)。</p></blockquote><p id="0679" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第二隐层以8个神经元为输入，输出16个单元。</p><p id="24e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第三个隐藏层将16个神经元作为输入，并产生2个单元作为输出(这是贷款申请为<code class="fe nn no np nd b">approved</code>或<code class="fe nn no np nd b">rejected</code>的概率，因为softmax激活函数在该层中起作用)。</p><p id="56d1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">每个隐藏层的激活函数都存储在<code class="fe nn no np nd b">act1</code>、<code class="fe nn no np nd b">act2</code>和<code class="fe nn no np nd b">act3</code>中。</p><blockquote class="oc od oe"><p id="1db5" class="li lj me lk b ll lm kd ln lo lp kg lq of ls lt lu og lw lx ly oh ma mb mc md im bi translated">注意:Pytorch中激活函数的常见例子有<code class="fe nn no np nd b">ReLu</code>、<code class="fe nn no np nd b">Sigmoid</code>、<code class="fe nn no np nd b">LogSigmoid</code>等。</p></blockquote><p id="8db7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，PyTorch允许您初始化每个隐藏层的权重张量。常见的例子有<code class="fe nn no np nd b">kaiming_uniform</code>、<code class="fe nn no np nd b">xavier_uniform</code>和<code class="fe nn no np nd b">orthogonal</code>。你可以在<a class="ae lh" href="https://pytorch.org/docs/stable/nn.init.html" rel="noopener ugc nofollow" target="_blank">文档</a>页面上了解更多细节。</p><p id="4e4d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们看看<code class="fe nn no np nd b">forward</code>方法:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="e8ba" class="nh mg it nd b gy ni nj l nk nl">def forward(self, X):<br/>        <br/>        #input and act for layer 1<br/>        X = self.hid1(X)<br/>        X = self.act1(X)<br/>        <br/>        #input and act for layer 2<br/>        X = self.hid2(X)<br/>        X = self.act2(X)<br/>        <br/>        #input and act for layer 3<br/>        X = self.hid3(X)<br/>        X = self.act3(X)<br/>        <br/>        return X</span></pre><p id="6f59" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">每层的输入和激活在该函数中定义。总之，输入<code class="fe nn no np nd b">X</code>进入<code class="fe nn no np nd b">hid1</code>，第一激活功能<code class="fe nn no np nd b">act1</code>应用于此。然后输出被传递到<code class="fe nn no np nd b">hid2</code>，在此应用第二激活功能<code class="fe nn no np nd b">act2</code>等等。由于最后一层有一个softmax激活函数，返回的<code class="fe nn no np nd b">X</code>将是一个包含<em class="me"> n </em>个元素的张量，其中<em class="me"> n </em>是输出类的数量(在我们的例子中是两个——批准与拒绝)。</p><blockquote class="oc od oe"><p id="c184" class="li lj me lk b ll lm kd ln lo lp kg lq of ls lt lu og lw lx ly oh ma mb mc md im bi translated">张量只不过是Pytorch自己的Numpy数组。这是一个通用的n维数组，用于任意数值计算。</p></blockquote><h2 id="b78f" class="nh mg it bd mh ns nt dn ml nu nv dp mp lr nw nx mr lv ny nz mt lz oa ob mv iz bi translated">在PyTorch中准备数据</h2><p id="1389" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在我们训练之前，<em class="me">使用<code class="fe nn no np nd b">torch.utils.data.DataLoader</code>加载</em>用于训练的数据是很重要的。你会问，为什么需要这样做？它实际上创建了要作为模型输入发送的批次(针对训练和测试集)。它将行的样本、批量大小以及是否重排行作为输入。</p><p id="07a2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">注:我们在</em> <code class="fe nn no np nd b"><em class="me">test_dl</em></code> <em class="me">的情况下显式设置</em> <code class="fe nn no np nd b"><em class="me">shuffle = False</em></code> <em class="me">是因为我们需要在末尾添加未混淆的标签来绘制混淆矩阵。</em></p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="0fff" class="nh mg it nd b gy ni nj l nk nl"># Preparing the dataset before training the model</span><span id="925d" class="nh mg it nd b gy nm nj l nk nl"># load the dataset<br/>dataset = CSVDataset()</span><span id="420d" class="nh mg it nd b gy nm nj l nk nl"># get the train and test split<br/>train, test = dataset.get_splits()</span><span id="70d6" class="nh mg it nd b gy nm nj l nk nl"># prepare dataloaders - <br/>train_dl = DataLoader(train, batch_size = 32, shuffle = True)<br/>test_dl = DataLoader(test, batch_size= 32, shuffle= False)</span></pre><h2 id="dee6" class="nh mg it bd mh ns nt dn ml nu nv dp mp lr nw nx mr lv ny nz mt lz oa ob mv iz bi translated">训练模型</h2><p id="745c" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">我们首先通过创建<code class="fe nn no np nd b">myNeuralNetwork</code>对象并传递2作为输入来定义模型。这里的2是指输入特征的数量。</p><p id="b8c7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们还将设置<code class="fe nn no np nd b">epochs = 10</code>,这意味着模型将根据所有数据训练<em class="me">10次。</em></p><p id="28d2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于训练过程，我们将使用随机梯度下降(SGD) <code class="fe nn no np nd b">optimizer</code>。</p><blockquote class="oc od oe"><p id="f355" class="li lj me lk b ll lm kd ln lo lp kg lq of ls lt lu og lw lx ly oh ma mb mc md im bi translated">优化器的选择包括<code class="fe nn no np nd b">Adam</code>、<code class="fe nn no np nd b">AdaGrad</code>、<code class="fe nn no np nd b">SparseAdam</code>、<code class="fe nn no np nd b">SGD</code>等，它们的用例可以在<a class="ae lh" href="https://pytorch.org/docs/stable/optim.html" rel="noopener ugc nofollow" target="_blank">文档</a>中找到。</p></blockquote><p id="4e4e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们要在训练时监控<code class="fe nn no np nd b">CrossEntropyLoss</code>。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="e77d" class="nh mg it nd b gy ni nj l nk nl"># define the network<br/>model = myNeuralNetwork(2) # 2 because we only have 2 input features</span><span id="cd13" class="nh mg it nd b gy nm nj l nk nl"># define the number of epochs<br/>epochs = 10</span><span id="37ff" class="nh mg it nd b gy nm nj l nk nl"># define the optimizer - SGD<br/>optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)</span><span id="d06e" class="nh mg it nd b gy nm nj l nk nl"># define the loss function<br/>criterion = CrossEntropyLoss()</span></pre><p id="4774" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了训练网络，我们将使用两个<strong class="lk jd"> <em class="me">用于</em> </strong>循环。以下代码中的外循环是训练历元数所必需的，内循环用于迭代<code class="fe nn no np nd b">train_dl</code>中的样本批次。也就是说，对于<code class="fe nn no np nd b">train_dl</code>中的每一批样本，我们将清除梯度、计算模型输出、计算损耗并进行反向传播以更新权重。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="fe6a" class="nh mg it nd b gy ni nj l nk nl"># iterate through all the epoch<br/>for epoch in range(epochs):<br/>    # go through all the batches generated by dataloader<br/>    for i, (inputs, targets) in enumerate(train_dl):<br/>            # clear the gradients<br/>            optimizer.zero_grad()<br/>            # compute the model output<br/>            yhat = model(inputs)<br/>            # calculate loss<br/>            loss = criterion(yhat, targets.type(torch.LongTensor))<br/>            # credit assignment<br/>            loss.backward()<br/>            # update model weights<br/>            optimizer.step()</span></pre><h2 id="b16b" class="nh mg it bd mh ns nt dn ml nu nv dp mp lr nw nx mr lv ny nz mt lz oa ob mv iz bi translated">评估模型</h2><p id="e845" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">为了评估模型，我们将再次使用<strong class="lk jd"> <em class="me">作为</em> </strong>循环来检查<code class="fe nn no np nd b">test_dl</code>中的所有批次。对于每个样本，我们将把它传递给模型并得到输出，即<code class="fe nn no np nd b">y_pred</code>。由于<code class="fe nn no np nd b">y_pred</code>包含两个概率值(一个用于<code class="fe nn no np nd b">approved</code>，一个用于<code class="fe nn no np nd b">rejected</code>，我们将选择概率最高的索引作为最终输出。</p><p id="4fcb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们将实际输出和预测输出存储在两个列表中— <code class="fe nn no np nd b">actuals</code>和<code class="fe nn no np nd b">predictions</code>。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="8fd5" class="nh mg it nd b gy ni nj l nk nl"># Evaluate the model<br/>predictions, actuals = list(), list()</span><span id="52a5" class="nh mg it nd b gy nm nj l nk nl"># loop over all batches in test set</span><span id="0b56" class="nh mg it nd b gy nm nj l nk nl">for i, (inputs, targets) in enumerate(test_dl):<br/>    # pass input to the model<br/>    y_pred = model(inputs) <br/>    # retrieve the numpy array<br/>    y_pred = y_pred.detach().numpy()<br/>    # pick the index of the highest values<br/>    res = np.argmax(y_pred, axis = 1) <br/>    <br/>    # actual output<br/>    actual = targets.numpy()<br/>    actual = actual.reshape(len(actual), 1)<br/>    <br/>    # store the values in respective lists<br/>    predictions.append(list(res))<br/>    actuals.append(list(actual))<br/>    <br/>actuals = [val for sublist in vstack(list(chain(*actuals))) for val in sublist]<br/>predictions = [val for sublist in vstack(list(chain(*predictions))) for val in sublist]</span></pre><p id="f516" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了打印准确度分数，我们将使用<code class="fe nn no np nd b">sklearn.metrics</code>中的<code class="fe nn no np nd b">accuracy_score</code></p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="0485" class="nh mg it nd b gy ni nj l nk nl"><em class="me"># evaluating how good the model is!</em><br/>from sklearn.metrics import accuracy_score</span><span id="6cea" class="nh mg it nd b gy nm nj l nk nl">print("The accuracy is" , accuracy_score(actuals, predictions))<br/>*********************<br/>The accuracy is 0.7495</span></pre></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="4467" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们成功了！</p><p id="5e7c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们已经使用PyTorch创建了一个基本网络。希望这不是太多的信息超载。如果你想获得全部代码，请查看Github<a class="ae lh" href="https://github.com/V-Sher/pytorch_ANN_example" rel="noopener ugc nofollow" target="_blank">上的笔记本。在下一篇教程中，我们将深入探讨Pytorch提供的一些高级功能。</a></p><p id="13df" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在那之前:)</p><div class="op oq gp gr or os"><a href="https://varshitasher.medium.com/membership" rel="noopener follow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">阅读瓦希塔·谢尔博士的每一家书店(以及媒体上成千上万的其他作家)</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">作为一个媒体会员，你的会员费的一部分给了你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">varshitasher.medium.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg lb os"/></div></div></a></div><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/beginners-guide-to-building-artificial-neural-networks-using-keras-in-python-bdc4989dab00"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">使用Python中的Keras构建人工神经网络的初学者指南</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">创建网络架构、训练、验证和保存模型并使用它进行推理的提示和技巧。</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="ph l pd pe pf pb pg lb os"/></div></div></a></div><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/step-by-step-guide-to-explaining-your-ml-project-during-a-data-science-interview-81dfaaa408bf"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">在数据科学面试中解释你的ML项目的逐步指南。</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">在结尾有一个额外的样本脚本，让你谨慎地展示你的技术技能！</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pi l pd pe pf pb pg lb os"/></div></div></a></div><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">使用Scikit、Pandas和Numpy进行时间序列建模</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">直观地利用季节性来提高模型准确性。</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pj l pd pe pf pb pg lb os"/></div></div></a></div></div></div>    
</body>
</html>