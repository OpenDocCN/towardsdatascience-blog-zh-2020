<html>
<head>
<title>Regression and its variants: Simple, Multiple, LASSO, Ridge and Stepwise Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归及其变体:简单、多重、套索、岭和逐步回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regression-and-its-variants-simple-multiple-lassso-ridge-and-stepwise-regression-9c144fd7a7b2?source=collection_archive---------14-----------------------#2020-11-05">https://towardsdatascience.com/regression-and-its-variants-simple-multiple-lassso-ridge-and-stepwise-regression-9c144fd7a7b2?source=collection_archive---------14-----------------------#2020-11-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d5ed" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">各种回归类型之间的相似性和差异</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9365b1ae5a0ddd0db5d5fd3517170788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VXccRqcXofRB3tWd"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@veselavaclavik?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Vesela Vaclavikova </a>拍摄的照片</p></figure><p id="8df6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设作为一名数据科学家，您想要分析一个学生数据集，该数据集记录了他们的身高和体重。您注意到weight列中缺少一个值。你能预测这个丢失的值吗？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/7be820b03c78256f06430b8cff4651af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*n6G_QgoaX-SA08B2iElcCA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">假设的学生数据集</p></figure><p id="6e4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回归有助于解决这类问题。在现实世界的机器学习应用中，回归模型经常用于根据已知特征预测未知值(例如，根据品牌、型号、里程等确定二手车的价格)。)并通过在未知变量(也称为因变量)和已知特征(也称为自变量)之间建立统计关系来实现。这种关系可以表示为:</p><p id="2420" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lw">因变量= f(自变量1，自变量2 …)</em></p><p id="1410" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种统计关系可以采取许多不同的形式，取决于预测因素的类型、结果和用于建立关系的函数。下面是经常遇到的五种回归模型。我以一种明确地将一种类型与下一种类型联系起来的方式来描述它们，这样就很容易理解它们的相似之处和不同之处。</p><p id="e427" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 1)简单线性回归</strong></p><p id="c184" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顾名思义，简单回归是所有回归中最简单的形式。它只有一个因变量，用一个自变量来解释，两者都用二维散点图表示。上面的例子——身高和体重之间的关系——是简单线性回归的经典例子。简单线性回归模型的数学公式采用以下形式:</p><p id="bcae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lw">体重= B0+B1 *身高+ e </em></p><p id="8f77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中，<em class="lw"> b0 </em> =截距，<em class="lw"> b1 </em> =系数，<em class="lw"> e </em> =误差项</p><p id="2532" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2)多元回归</strong></p><p id="7ef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多元回归类似于简单的线性回归，但在这种情况下，将有多个独立变量，而不是一个。如果我们再次遵循上面的例子，假设体重不仅仅是由身高预测的，而是由<strong class="lb iu"> <em class="lw"> </em> </strong>一个额外的变量——比如年龄——预测的，那么这就是多元回归。在数学公式中，我们只需添加额外的变量:</p><p id="39f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lw">体重= B0+B1 *身高+B2 *年龄+ e </em></p><p id="cddd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3)拉索回归</strong></p><p id="61c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，多元回归比简单线性回归好，对吧？因为有更多的预测因素，所以预测必须更加准确！不完全是，不总是。有时简单的模型比复杂的模型表现得更好。上面的多元回归有2个自变量，还是挺简单的，但是如果数据集中有20个或者200个变量呢？这就是数据科学家需要努力思考的问题:保留多少功能，放弃哪些功能。</p><p id="a842" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在机器学习中，特征选择是消除过拟合的重要步骤，回归中也是如此。所以在LASSO中，如果有太多的特征，有些特征会被完全消除。这是通过将系数设置为零来实现的。这个过程在机器学习vocab中被称为“<strong class="lb iu"> L1 </strong> <strong class="lb iu">正则化</strong>”。</p><p id="e3d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 4)岭回归</strong></p><p id="c3da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在LASSO回归中，我们简化了包含太多特征的回归方程，并通过完全消除其中一些特征来实现这一点。岭回归用于相同的目的，即简化模型，但不是完全消除特征，而是最小化它们的影响。</p><p id="8a10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以在岭回归中，特征系数被缩小到接近零，但不完全是零(这个过程被称为<strong class="lb iu"> L2正则化</strong>)。在机器学习岭回归模型中，称为<em class="lw">λ</em>的超参数用于控制与损失函数相关联的惩罚的权重。</p><p id="2880" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为没有特征被完全消除，所以岭回归不用于特征选择。</p><p id="d12d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 5)逐步回归</strong></p><p id="8d2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们已经研究了各种操纵特征以增强模型性能的方法。有一个古老的，但同样有效的方法来寻找更好的模型，被称为“逐步回归”。</p><p id="2f49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顾名思义，在逐步回归中，您从最简单的模型<em class="lw">开始(如1个因变量和1个自变量)，然后评估其性能。然后，您添加另一个变量，再次评估性能，并比较两个模型，以找到更好的一个。重复该过程，直到找到最佳执行模型。</em></p><p id="92e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">称为<strong class="lb iu">赤池信息准则(AIC) </strong>的估计量用于比较逐步回归中的模型性能。</p><h1 id="bb4f" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">摘要</h1><p id="411e" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">回归量化了因变量和一个或多个自变量之间的关系，有许多不同的方法来建立和量化这种关系。在本文中，我重点介绍了作为一名数据科学家最常遇到的5种回归。有一点要记住，他们之间虽然有区别，但也不是完全不同的阶层。相似之处多于不同之处。因此，当学习回归(或任何建模家族)时，一起学习比一次学习一个更好。</p></div></div>    
</body>
</html>