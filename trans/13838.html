<html>
<head>
<title>Quick Tutorial: Using Bayesian optimization to tune your hyperparameters in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">快速教程:使用贝叶斯优化调整PyTorch中的超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quick-tutorial-using-bayesian-optimization-to-tune-your-hyperparameters-in-pytorch-e9f74fc133c2?source=collection_archive---------11-----------------------#2020-09-23">https://towardsdatascience.com/quick-tutorial-using-bayesian-optimization-to-tune-your-hyperparameters-in-pytorch-e9f74fc133c2?source=collection_archive---------11-----------------------#2020-09-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1442" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">设计神经网络的更快方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6e394190e8aa809c9811932fe96083ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DezRaSxHAujBlrss"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">超参数调音就像调音吉他一样，因为我自己不会调音，所以更愿意使用应用程序。照片由<a class="ae ky" href="https://unsplash.com/@adigold1?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿迪·戈尔茨坦</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="989b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数是模型中决定模型架构、学习速度和范围以及正则化的参数。</p><p id="b716" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">寻找最佳超参数需要一些专业知识和耐心，您经常会发现人们使用网格搜索和随机搜索等令人疲惫的方法来寻找最适合他们问题的超参数。</p><h1 id="b218" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">快速教程</h1><p id="4cce" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我将向您展示如何使用<a class="ae ky" href="https://ax.dev/" rel="noopener ugc nofollow" target="_blank"> Ax </a>在PyTorch中实现贝叶斯优化，以自动找到针对您的神经网络的最佳超参数集。</p><p id="e313" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用迁移学习构建一个简单的CIFAR-10分类器。大部分代码来自CIFAR-10分类器的官方<a class="ae ky" href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" rel="noopener ugc nofollow" target="_blank"> PyTorch初学者教程</a>。</p><p id="cf31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不会深入贝叶斯优化的细节，但你可以在<a class="ae ky" href="https://ax.dev/docs/bayesopt.html" rel="noopener ugc nofollow" target="_blank"> Ax网站</a>上研究该算法，阅读<a class="ae ky" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.467.8687&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>或2012年关于其实际用途的<a class="ae ky" href="https://arxiv.org/abs/1206.2944" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><h1 id="66fb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">首先，像往常一样</h1><p id="7d17" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">使用以下工具安装Ax:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="6e7d" class="mx lw it mt b gy my mz l na nb">pip install ax-platform</span></pre><p id="c10b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">导入所有必需的库:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="330a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下载数据集并构建数据加载器(我建议稍后将训练批量调整为32或64):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="0da9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们通过创建一些辅助函数来看看CIFAR-10数据集:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h1 id="6c71" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">培训和评估职能</h1><p id="b27b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Ax需要一个返回训练模型的函数，以及另一个评估模型并返回性能指标(如准确性或F1分数)的函数。我们在这里只构建训练函数，并使用Ax自己的<code class="fe ne nf ng mt b">evaluate</code> tutorial函数来测试我们的模型性能，它返回准确性。如果你愿意，你可以<a class="ae ky" href="https://ax.dev/api/_modules/ax/utils/tutorials/cnn_utils.html#evaluate" rel="noopener ugc nofollow" target="_blank">看看他们的API </a>来模仿他们的评估函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="0f47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们编写一个<code class="fe ne nf ng mt b">init_net()</code>函数，它初始化模型并返回网络准备训练。这里有很多超参数调优的机会。您会注意到<code class="fe ne nf ng mt b">parameterization</code>参数，它是一个包含超参数的字典。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="0552" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们需要一个<code class="fe ne nf ng mt b">train_evaluate()</code>函数，贝叶斯优化器在每次运行时都会调用它。优化器在<code class="fe ne nf ng mt b">parameterization</code>中生成一组新的超参数，将其传递给这个函数，然后分析返回的评估结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h1 id="8d38" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">优化！</h1><p id="06ec" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在，只需指定想要扫描的超参数，并将其传递给Ax的<code class="fe ne nf ng mt b">optimize()</code>函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><p id="5e44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这确实花了一些时间，但与对所有3个超参数进行简单的网格搜索相比，这算不了什么。让我们来看看结果:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9643" class="mx lw it mt b gy my mz l na nb">results[INFO 09-23 09:30:44] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 5 arms, GPEI for subsequent arms], generated 0 arm(s) so far). Iterations after 5 will take longer to generate due to model-fitting.<br/>[INFO 09-23 09:30:44] ax.service.managed_loop: Started full optimization with 20 steps.<br/>[INFO 09-23 09:30:44] ax.service.managed_loop: Running optimization trial 1...<br/>[INFO 09-23 09:31:55] ax.service.managed_loop: Running optimization trial 2...<br/>[INFO 09-23 09:32:56] ax.service.managed_loop: Running optimization trial 3...</span><span id="fa42" class="mx lw it mt b gy no mz l na nb">...</span><span id="02b3" class="mx lw it mt b gy no mz l na nb">[INFO 09-23 09:52:19] ax.service.managed_loop: Running optimization trial 18...<br/>[INFO 09-23 09:53:20] ax.service.managed_loop: Running optimization trial 19...<br/>[INFO 09-23 09:54:23] ax.service.managed_loop: Running optimization trial 20...</span><span id="8c4e" class="mx lw it mt b gy no mz l na nb">{'lr': 0.000237872310800664, 'batchsize': 117, 'momentum': <!-- -->0.99<!-- -->}<br/>{'accuracy': 0.4912998109307719}<br/>{'accuracy': {'accuracy': 2.2924975426156455e-09}}</span></pre><p id="4b31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当动量为<strong class="lb iu"> 0.99 </strong>且批量为<strong class="lb iu"> 117 </strong>时，我们的最佳学习率似乎是<strong class="lb iu"> 2.37e-4 </strong>。那很好。这里你看到的49.1%的精度并不是模型的最终精度，不用担心！</p><p id="04d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以更进一步，绘制一些图，显示每个时期的精度(随着参数化的改进而改进)，以及优化器使用等高线图作为两个超参数的函数的估计精度。<code class="fe ne nf ng mt b">experiment</code>变量属于<code class="fe ne nf ng mt b">Experiment</code>类型，你一定要<a class="ae ky" href="https://ax.dev/api/core.html#ax.core.experiment.Experiment" rel="noopener ugc nofollow" target="_blank">查看文档</a>看看它提供的所有方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="84d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">渲染的情节易于理解和互动。等高线图中的黑色方块表示实际采样的坐标。</p><div class="kj kk kl km gt ab cb"><figure class="np kn nq nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/3e79430c1dd7c949d00d55d302d09931.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*JrGrXJpLqDmMdexRepj2hw.png"/></div></figure><figure class="np kn nv nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/0b75751ab4f2724a0b63572f51c6049c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*18Dhy-H__2sYvTODJMRSow.png"/></div></figure></div><p id="1282" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，您可以通过简单地运行下面的脚本来获取具有最佳平均精度的参数集(Ax称之为“arm”)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="012c" class="mx lw it mt b gy my mz l na nb">Arm(name=’19_0', parameters={‘lr’: 0.00023787231080066353, ‘batchsize’: 117, ‘momentum’: 0.9914986635285268})</span></pre><p id="ec30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不要害怕调整你想要的任何东西，比如隐藏层数和大小、漏失、激活功能、解冻深度等等。</p><p id="44f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快乐优化！</p></div></div>    
</body>
</html>