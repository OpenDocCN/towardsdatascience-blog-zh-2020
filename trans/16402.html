<html>
<head>
<title>Ultimate Guide to Reinforcement Learning Part 2 — Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习终极指南第2部分——培训</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ultimate-guide-for-ai-game-creation-part-2-training-e252108dfbd1?source=collection_archive---------7-----------------------#2020-11-12">https://towardsdatascience.com/ultimate-guide-for-ai-game-creation-part-2-training-e252108dfbd1?source=collection_archive---------7-----------------------#2020-11-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1a6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个全面的文章系列中，我们将构建自己的环境。稍后，我们将使用强化学习来训练一个神经网络。最后，我们将创建一个视频，展示人工智能在环境中的表现。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/e284bb3a0ce9f55e75e90f5cd4cc9c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a5Vct5PhgsX22n6BJ2HDQQ.png"/></div></div></figure><p id="341c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">环境、培训和展示的完整代码可以在https://github.com/danuo/rocket-meister/的<strong class="jp ir"> GitHub </strong> : <a class="ae kx" href="https://github.com/danuo/rocket-meister/" rel="noopener ugc nofollow" target="_blank">上找到</a></p><h1 id="dd9d" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">我们将涵盖的内容:</h1><h2 id="7f3d" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">第1部分——用Pygame创建一个可玩的环境</h2><p id="b827" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">链接:<a class="ae kx" href="https://medium.com/@d.brummerloh/ultimate-guide-for-reinforced-learning-part-1-creating-a-game-956f1f2b0a91" rel="noopener">https://medium . com/@ d . brummer loh/ultimate-guide-for-reinforced-learning-part-1-creating-a-game-956 f1 F2 b 0 a 91</a></p><ul class=""><li id="c609" class="mn mo iq jp b jq jr ju jv jy mp kc mq kg mr kk ms mt mu mv bi translated">创造一个健身房的环境。Env 子类。</li><li id="bb67" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">通过<code class="fe nb nc nd ne b">step()</code>函数实现环境逻辑。</li><li id="208a" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">用<strong class="jp ir"> Pygame </strong>获取用户输入，使环境适合人类玩。</li><li id="2e26" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">用<strong class="jp ir"> Pygame </strong>实现一个<code class="fe nb nc nd ne b">render()</code>函数来可视化环境状态。</li><li id="d9c1" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">用<strong class="jp ir"> Matplotlib </strong>实现交互级设计。</li></ul><h2 id="a4c7" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">第2部分—开始培训</h2><ul class=""><li id="4a66" class="mn mo iq jp b jq mi ju mj jy nf kc ng kg nh kk ms mt mu mv bi translated">在理解可能性和挑战的同时，定义合适的观察。</li><li id="c30b" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">定义合适的奖励。</li><li id="93cb" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">用<code class="fe nb nc nd ne b">gym</code>环境训练神经网络。</li><li id="f7d5" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">对结果的讨论</li></ul><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="31af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是该系列的第二部分，涵盖了神经网络的训练。在我们开始训练之前，我们必须进一步指定环境和人工智能之间的API。</p><h2 id="6748" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">要求</h2><p id="a591" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">由于我们要训练的模型相对较小，因此可以在合理的时间内(不到一天)在消费级桌面CPU上进行训练。你不需要一个强大的GPU或访问云计算网络。本指南中使用的python包如下所示:</p><pre class="km kn ko kp gt nk ne nl nm aw nn bi"><span id="6392" class="lw kz iq ne b gy no np l nq nr"><strong class="ne ir">Python </strong>3.8.x<br/><strong class="ne ir">ray 1.0</strong><br/><strong class="ne ir">tensorflow </strong>2.3.1<br/><strong class="ne ir">tensorflow-probability</strong> 0.11<br/><strong class="ne ir">gym 0.17.3<br/>pygame 2.0.0</strong></span></pre></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="e686" class="ky kz iq bd la lb nz ld le lf oa lh li lj ob ll lm ln oc lp lq lr od lt lu lv bi translated">观察</h1><p id="f2d3" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">观察是从环境反馈给代理或神经网络的反馈。这真的是唯一的事情，代理可以看到，以推导出它的下一步行动。更重要的是，代理没有记忆。它的决定将完全基于对当前状态的观察。</p><p id="d740" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">定义合适的观察对于获得良好的培训效果至关重要。在我们当前的例子中，定义观察可能是一个微不足道的任务，但是我们将探索几个选项。这可能不是其他机器学习项目的情况，在其他机器学习项目中，开发合适的观测值可能是一项具有挑战性的关键任务。</p><p id="522b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们讨论合适的观测的要求之前，让我们先用最直观的方法来研究:由于火箭不会撞向边界，使用间隙作为观测是有意义的。所以我们计算了各种角度下火箭与环境的距离(-90，-60，...，+90)，如下图所示。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oe"><img src="../Images/403e9523c1ecbf0a76396ab21abd3b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TL1U3ayvyaTKVjtIJ55hsg.jpeg"/></div></div></figure><p id="eb61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">归一化</strong>现在，我们需要确定每个观测值的取值范围是[-1，1]。这个过程称为规范化，不是强制性的。但是，大多数神经网络将受益于标准化值。这是因为大多数神经网络在计算结束时都有一个反正切函数。在这种情况下，归一化值范围在数值上更合适。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi of"><img src="../Images/ad4a7b0cbf871253db3adf15695fe7f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KlBbGAds4FEIibbPxviBhg.png"/></div></div><p class="og oh gj gh gi oi oj bd b be z dk translated">观察的标准化。</p></figure><p id="ae55" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实现标准化的一种方法是应用线性插值。实现这一点的简单方法是使用下面的numpy函数:</p><pre class="km kn ko kp gt nk ne nl nm aw nn bi"><span id="ec1a" class="lw kz iq ne b gy no np l nq nr">obs_norm = np.interp(obs, [0, 1000], [-1, 1])</span></pre><p id="5609" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">观察值的唯一性</strong>用数学术语来说，把模型想象成一个确定性函数<em class="ok"> f </em>，它根据观察值<em class="ok">【o】计算动作<em class="ok">【a】</em>。</em>在本例中，有<em class="ok"> n个</em>观察值和2个动作:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ol"><img src="../Images/8beba314548d83b22803652e51786aa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BeC8zP7oQFxFPPZvqTtkNA.png"/></div></div></figure><p id="027d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里有很多理论要探讨，但重要的含义如下:如果两种不同的情况或状态需要两种不同的行动才能成功，那么它们各自的观察也必须不同。只有当观察结果不同时，代理才能产生两种不同的动作。那么这到底意味着什么呢？让我们来看一个例子:</p><p id="48f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图中显示的两个场景显示火箭处于完全相同的位置。因此，距离[l1，…，l7](观察值)是相同的。然而，左边场景中的火箭速度要高得多。因为速度不是观测的一部分，代理人不知道火箭太快了。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi om"><img src="../Images/225863a114fc5e9d8e1e06d8fe0a1f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TZSOMDj8GZfhfNkc8zfJxw.jpeg"/></div></div><p class="og oh gj gh gi oi oj bd b be z dk translated">具有相同观测值的不同状态(观测值集7)</p></figure><p id="b898" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要执行的适当行动分别是:</p><ul class=""><li id="5f69" class="mn mo iq jp b jq jr ju jv jy mp kc mq kg mr kk ms mt mu mv bi translated">左图:减速，右转。</li><li id="1ede" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">右场景:增加/保持速度，右转。</li></ul><p id="cec3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于对两种情况的观察是相同的，神经网络将不可避免地对两种情况执行相同的动作。因此，它根本无法在两种情况下执行适当的操作，最多只能完成其中一种。因此，单独使用距离作为整个观测值o=[l1，…，l7]不是一个好主意。出于测试的目的，我们将把它作为观测的第一次迭代。</p><p id="755a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">观测值o7的第一次迭代:</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi on"><img src="../Images/2205bc851a4b34a4672880ad18b7f57e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J2s5qgRq6BXbBZBa7iTyhA.jpeg"/></div></div></figure><p id="7271" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们将扩展这个集合。从最近的考虑中，我们已经得出结论，神经网络需要知道火箭的速度。因此，速度大小将起作用:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ol"><img src="../Images/87eaae2dfedc4953ba1c8a311476e902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g_FZTPqCANve-FKVCcQonA.png"/></div></div></figure><p id="d9cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">观测值o8的第二次迭代:</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi on"><img src="../Images/221c090ed9750c0c700cc01a2a8f3c33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tmeRKKbrT-OpEn6Kz26D_Q.jpeg"/></div></div></figure><p id="a56c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，我们可以很容易地想出两个场景，这两个场景需要不同的操作，尽管产生了相同的观察结果。速度转向的方向不能从这组观测值中扣除。因此，移动方向未知，需要另一次观察。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi om"><img src="../Images/bcecf6a2e75cf2de07fef2d609c50e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MieTC8bLwNmdpTrzspDexw.jpeg"/></div></div><p class="og oh gj gh gi oi oj bd b be z dk translated">具有相同观测值的不同状态(观测值集8)</p></figure><p id="0152" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">显然，我们需要使观测范围内的速度有方向。作为一个小注释，简单地分别传递x和y方向的速度是行不通的。火箭的绝对方位也不知道，所以这个问题可以简单地转换成另一个问题。因此，建议将火箭方向与其速度之间的相对角度作为附加观测值:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oe"><img src="../Images/c4692b35f7fcb4685c88391c11e29e55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RClUBeF6QL_il-TfiUWYCw.jpeg"/></div></div></figure><p id="d8af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用角度有点棘手。首先我们需要决定是要有度的工作还是要光芒四射的工作。其次，如果角度不在-180°范围内&lt; α </p><p id="2874" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第三次迭代观测o9: </strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi on"><img src="../Images/76d4817f2c6d18d99e9b141a938da464.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0tG-Q2UE7rCgXNRhUnbkHA.jpeg"/></div></div></figure><p id="cebb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后但同样重要的是，我们将提供某种导航辅助。对于奖励功能，我们稍后将沿着轨迹定义目标，当达到时将获得奖励。下一个目标的方向是垂直于下一个目标的向量，因此指示到所述目标的直接路线。火箭方向和目标向量之间的角度差是第10次也是最后一次观察:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oe"><img src="../Images/2dea86d7321faab8a0ff3e47d35eba1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*agUY9DIwkgRRf6mqW9dtuA.jpeg"/></div></div></figure><p id="141b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第四次迭代观测o10: </strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi on"><img src="../Images/25041d15f6263ecb6e1c2571028f9c7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dlkNnqcMlXZjLhjobG_tzg.jpeg"/></div></div></figure><p id="644b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们评估不同组的观察值之前，请记住，神经网络不知道已定义观察值的含义或上下文。然而，也不尽然。机器学习的目标是找到观察和成功行动之间的数字相关性。对于这一点，数据的上下文无关紧要。</p><h2 id="a58c" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">对观察结果的评估</h2><p id="7978" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">所有四种变体都已经通过用SAC代理进行总共200万步的训练进行了测试。不要担心，我们稍后将经历开始培训的步骤。现在，让我们先看看观测值的选择所导致的后果。通过查看图表，我们可以看到<em class="ok"> o9 </em> &amp; <em class="ok"> o10 </em>比其他两个表现好得多。设置<em class="ok"> o9 </em>产生最好的结果，这是一个小惊喜。然而，我们不应急于下结论，并记住以下几点:首先，200万步并不多。虽然<em class="ok"> o7 </em>和<em class="ok"> o8 </em>的曲线看似收敛，但本试验无法确定<em class="ok"> o9 </em>和<em class="ok"> o10 </em>的最终性能。经过更长时间的训练，<em class="ok"> o10 </em>很有可能超过<em class="ok"> o9 </em>。甚至<em class="ok"> o7 </em>和<em class="ok"> o8 </em>的停滞都不确定。如果这不是一个有趣的项目，更长时间的训练应该被考虑。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi on"><img src="../Images/cb039df617431a439eb03e372083a50b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aGiRHO9MtIAPrEv-mBdjOw.png"/></div></div><p class="og oh gj gh gi oi oj bd b be z dk translated">四组不同观察值之间的性能比较。</p></figure></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="bfab" class="ky kz iq bd la lb nz ld le lf oa lh li lj ob ll lm ln oc lp lq lr od lt lu lv bi translated">奖励函数</h1><p id="b8ec" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">如前所述，奖励功能是环境的一部分，据说在强化学习中被代理人最大化。想出一个好的奖励函数比你想象的要难得多。当最大化奖励函数并不完全符合你实际上想要AI做的事情时，问题就出现了。拥有一个只重视符合你意图的行为的奖励功能，实际上比预期的要困难得多。</p><p id="d8ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Rob Miles [Youtube]有一个非常好的视频，讲述了一个被设计用来收集邮票的人工智能，它可能会在收集邮票的过程中引发一场世界大战。该视频非常有趣，同时也很有见地。</p><p id="4165" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回到奖励函数的定义:假设我们在更高的速度下给予更多的奖励。预测代理最有可能以尽可能高的速度撞上墙并不需要太多的创造力。当然，一个更慢的轨迹，实际上是穿过整个过程，会产生更高的总回报。然而，放弃目前最成功的策略，探索一种完全不同的方法，总是令人乏味的。在数学术语中，给定的高速碰撞进近是所有可能进近空间中的局部最大值。</p><p id="8fbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">摆脱局部极值并进一步优化其策略是智能体的基本能力。但是，不能保证代理真的做到了这一点。因此，明智的做法是不要在奖励函数中包含速度，或者不给它很大的权重，这样就不会产生这种障碍。相反，我们选择设立检查站，在通过时给予奖励。</p><h2 id="2797" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">设置检查点</h2><p id="9303" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">为了计算奖励，我在赛道上一共设置了40个检查站。这是一种跟踪赛道进展的简单方法，也可以计算完成的圈数。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oo"><img src="../Images/f1fb6fdadba4a880aa5e416f7e1058b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrD9PFJBYabE133i7e17Zw.jpeg"/></div></div></figure><p id="4788" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，我们提出了三个变量来推断火箭通过的检查站的回报。变量如下所列，并在之后进行比较:</p><h2 id="eff3" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">变体1:每个关卡的静态奖励</h2><p id="e404" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">静态变量将以固定的数量奖励每个到达的检查点。</p><pre class="km kn ko kp gt nk ne nl nm aw nn bi"><span id="3b32" class="lw kz iq ne b gy no np l nq nr"># for each goal 1 point<br/>reward_total += 1</span></pre><h2 id="a01a" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">变体2:每个关卡的动态奖励</h2><p id="47a2" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">动态变量将奖励每一个达到略低于一点的关卡。到达下一个关卡花费的时间越长，奖励的一分被扣除的就越多。根据所用时间的不同，奖励在1到0.9分之间不等。变量<strong class="jp ir"> steps </strong>保存自达到最后一个目标以来执行的步数。这个变量间接依赖于火箭的速度。</p><pre class="km kn ko kp gt nk ne nl nm aw nn bi"><span id="322e" class="lw kz iq ne b gy no np l nq nr"># for each goal give:<br/>reward_total += max(1, (500 - steps)) / 500</span></pre><h2 id="defd" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">变体3:对每个目标的持续奖励</h2><p id="b675" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">类似于变体1，这种连续变体将奖励每个目标恰好一分。但是，在执行的每一步中，奖励都是连续分配的。如果火箭正好位于目标3和目标4之间，那么到目前为止获得的总奖励是3.5分。后退时，奖励分别减少。这个奖励变量的计算相当繁琐，但是可以看看GitHub库中的代码。</p><h2 id="8c5d" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">比较</h2><p id="3d67" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">正如我们所看到的，在长时间的训练之后，所有的奖励函数都给出了相似的好结果。所有的变化都会产生一个神经网络，它可以相当好地完成课程。这并不奇怪，因为这些奖励函数非常相似。我们可以看到，在很大一部分训练中，有动态奖励的训练有更高的平均奖励。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi on"><img src="../Images/f544564a4e39b770e5e3b7a16241d44d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cuzFQdxh1XeTcPFYImwmCg.png"/></div></div></figure></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="a35f" class="ky kz iq bd la lb nz ld le lf oa lh li lj ob ll lm ln oc lp lq lr od lt lu lv bi translated">培养</h1><p id="4c7c" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">为了在我们的环境中训练神经网络，我们将使用<strong class="jp ir">强化学习(RL)，机器学习的领域之一</strong>。虽然有很多方法可以部署RL，但是我们将使用一个叫做<strong class="jp ir"> Ray </strong>的框架。</p><p id="752b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Ray的核心功能是提供一个多处理框架，允许代码在多个CPU核心甚至多个机器上并行运行。这非常有帮助，因为它使我们能够同时用多个代理/环境来训练我们的神经网络。Ray还包含了一个强化学习库，因此我们可以在很少甚至没有编程的情况下进行训练。我们只需要知道API，不可否认的是，它很少被记录，而且有时很复杂。</p><h2 id="8197" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">在Windows 10、OSX和Linux上安装Ray</h2><p id="c321" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">Ray刚刚发布了1.0版本，终于添加了期待已久的Windows 10支持。现在，您可以通过运行简单的pip安装在任何主要平台上安装Ray:</p><pre class="km kn ko kp gt nk ne nl nm aw nn bi"><span id="f781" class="lw kz iq ne b gy no np l nq nr">pip install --upgrade ray</span></pre><h2 id="2ccd" class="lw kz iq bd la lx ly dn le lz ma dp li jy mb mc lm kc md me lq kg mf mg lu mh bi translated">雷入门</h2><p id="b300" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">我们使用Ray的主要原因是包含了专用于RL的库<strong class="jp ir"> RLlib </strong>。它实现了大量先进的机器学习代理。正如下面的概述所示，大多数代理都支持TensorFlow 2和Pytorch。你想用哪个框架完全取决于你。如果你不确定，就用TensorFlow。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi op"><img src="../Images/3ae436368b5862a84b7953eb6a1e6e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GeBZkE6Pbg8tKKSzYoHu5A.jpeg"/></div></div><p class="og oh gj gh gi oi oj bd b be z dk translated">RLlib提供的机器学习代理。</p></figure><p id="de27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ray的伟大之处在于已经实现了许多训练代理，并且所有的代理都可以以相同的方式使用。这使得我们可以用14个不同的代理来训练网络，只需要在它们之间改变一个字符串。</p><p id="6258" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Ray的文档可能有点令人不知所措，并且由于普遍缺乏示例而令人困惑。尽管如此，这个库非常强大，完全值得学习，因为繁琐的任务可以通过少量的步骤来完成。如果我们使用算法<strong class="jp ir"> PPO </strong>在健身房环境<strong class="jp ir"> CartPole-v0 </strong>上开始训练，我们所要做的就是执行这两行代码:</p><pre class="km kn ko kp gt nk ne nl nm aw nn bi"><span id="3eba" class="lw kz iq ne b gy no np l nq nr">from ray import tune<br/>tune.run('PPO', config={"env": "CartPole-v0"})</span></pre><p id="74f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您遇到一个错误，您很可能会丢失包<code class="fe nb nc nd ne b">tensorflow-probability</code>包。要安装，请运行:</p><pre class="km kn ko kp gt nk ne nl nm aw nn bi"><span id="061a" class="lw kz iq ne b gy no np l nq nr">pip install --upgrade tensorflow-probability</span></pre><p id="5347" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了在一个定制的环境中训练网络(例如，一个不属于<code class="fe nb nc nd ne b">gym</code>包的环境),我们需要在配置字典中修改env关键字。我们可以传递环境类，而不是环境的名称字符串。参考见<strong class="jp ir"> start_ray_training.py: </strong>中的代码</p><p id="b91a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kx" href="https://github.com/danuo/rocket-meister/blob/master/start_ray_training.py" rel="noopener ugc nofollow" target="_blank">https://github . com/danuo/rocket-meister/blob/master/start _ ray _ training . py</a></p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="e8d4" class="ky kz iq bd la lb nz ld le lf oa lh li lj ob ll lm ln oc lp lq lr od lt lu lv bi translated">结果</h1><p id="3bdb" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">最终，9个不同的代理被用来训练不同的神经网络。并不是前面列出的所有代理都可以被使用，因为一些代理经常崩溃，而另一些代理只适合于不连续的动作(记住，我们使用的是连续动作)。以下是培训的结果:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oq"><img src="../Images/caf4f7d9e1fd9545bb89c54c0153331d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BoSjerFR2sM-vYb61FL8Qw.png"/></div></div></figure><p id="17f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们看到一些代理人表现很好，比人类更好地通过了课程。还要记住，所有代理都是未调整的，也就是说，它们使用默认参数运行。当选择其他参数时，它们的性能可能会提高很多。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oq"><img src="../Images/0bb59ce88908e087f7a1700bc52fce14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eWsd3bpe-whxuSmkgz6oVA.png"/></div></div></figure></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="ed99" class="ky kz iq bd la lb nz ld le lf oa lh li lj ob ll lm ln oc lp lq lr od lt lu lv bi translated">结论</h1><p id="04bf" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">还有其他一些因素使得很难在两种药物之间进行直接比较。在这个测试场景中，每个代理接受了总共1500万步的训练。请注意，并非所有代理的计算速度都一样快。如果我每次训练的时间相同，结果可能会不同。此外，对于大多数代理来说，培训可能不一致，因此更长时间的培训可以进一步改进策略。快速学习代理是伟大的，但有人可能会说，长期训练后的最终表现可能更重要。</p><p id="70ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另外请注意，SAC并不像测试中显示的那样优越。SAC代理的得分明显高于其他代理。然而，人们不得不相信，这些有争议的伟大成果是受制于过度拟合。这意味着，代理人实际上记住了轨道，而不是实际学习如何控制火箭。如果环境被改变，代理人被认为是失败的，因为没有在未知环境中机动的一般知识。为了防止过度拟合，训练应该在动态环境中进行，在每次迭代中都会发生变化。rocketmeister环境有一个级别生成器，也许你想尝试一下！有关更多信息，请查看自述文件:</p><div class="or os gp gr ot ou"><a href="https://github.com/danuo/rocket-meister" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd ir gy z fp oz fr fs pa fu fw ip bi translated">达诺/火箭-梅斯特</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">RocketMeister是一个广泛而复杂的健身房环境，用于开发和比较强化学习…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">github.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi kv ou"/></div></div></a></div><p id="7600" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">感谢阅读！</strong></p></div></div>    
</body>
</html>