<html>
<head>
<title>Generating Short Star Wars Text With LSTM’s</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用LSTM的生成简短的星球大战文本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-short-star-wars-text-with-lstms-c7dc65e252c4?source=collection_archive---------42-----------------------#2020-10-04">https://towardsdatascience.com/generating-short-star-wars-text-with-lstms-c7dc65e252c4?source=collection_archive---------42-----------------------#2020-10-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f42e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">“会说话并不会让你变得聪明”——绝地大师魁刚·金。</h2></div><p id="510a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">毫无疑问，我是那些被称为“星球大战迷”的人中的一员。我记得小时候在电视上看《幽灵的威胁》时被它震撼了(我知道，这不是一部伟大的电影)。后来，我看了《T2》中的《克隆人的进攻》和《T4》中的《西斯的复仇》(这是影院上映的最后一部电影)，接着又看了原版三部曲的DVD，那时这些电影还在上映。作为一个成年人，我在右臂上纹了一个Tie战士的纹身，这让我达到了高潮。</p><p id="d0e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对《星球大战》的热情很好地回答了“我应该做什么样的NLP项目？”；我想开发一个完整的<a class="ae lc" href="https://chatbotslife.com/" rel="noopener ugc nofollow" target="_blank">自然语言处理</a>项目，边做边练习一些技能。</p><p id="f1f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个关于LSTM氏症的项目实验的简要文档，以及如何训练一个语言模型来在字符级别上生成特定域内的文本。给定一个种子标题，它通过一个API编写一个简短的描述。该模型是使用Tensorflow从零开始构建的，没有迁移学习。它使用了来自Wookiepedia.com的文本，Fandom.com的团队(负责管理网站)非常友好地允许我使用网络抓取器收集数据并发表这篇文章。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="14b5" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">最终产品</h1><p id="2077" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">下面可以看到工作模型的演示。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/5ccd149ebcc62e87e2e2ebc874a7a8e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aV6pwcww_NXeoxrK.gif"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者图片</p></figure><h1 id="88c2" class="lk ll iq bd lm ln mx lp lq lr my lt lu jw mz jx lw jz na ka ly kc nb kd ma mb bi translated">要求</h1><p id="28bf" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">为了复制这个模型，你需要从<a class="ae lc" href="https://github.com/Pedrohgv/Star-Wars-Text-Generation" rel="noopener ugc nofollow" target="_blank">这里</a>下载代码。然后，安装所有需要的依赖项(强烈建议创建一个新的虚拟环境，并在其上安装所有软件包),如果您使用的是Linux，则:</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="36c1" class="nh ll iq nd b gy ni nj l nk nl">pip install -r requirements-linux.txt</span></pre><p id="7883" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，如果您使用的是Windows，只需使用:</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="be11" class="nh ll iq nd b gy ni nj l nk nl">pip install -r requirements-windows.txt</span></pre><p id="3847" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在安装了所有依赖项之后，您需要经过训练的模型能够生成任何文本。由于GitHub上的大小限制，模型必须从<a class="ae lc" href="https://drive.google.com/drive/folders/1JTzVV8uir74BlqF9HXtYMZdIu_4ZDpBP?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>下载。只需下载整个<em class="lb">模型</em>文件夹，并将其放在<em class="lb">部署</em>文件夹下。最后，激活安装了所有依赖项的环境，并在下载项目的文件夹中打开终端，键入:</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="393e" class="nh ll iq nd b gy ni nj l nk nl">python deploy/deploy.py</span></pre><h1 id="f3af" class="lk ll iq bd lm ln mx lp lq lr my lt lu jw mz jx lw jz na ka ly kc nb kd ma mb bi translated">该项目</h1><p id="78bf" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">下面是所有使用的重要库的列表:</p><ul class=""><li id="1d2f" class="nm nn iq kh b ki kj kl km ko no ks np kw nq la nr ns nt nu bi translated">美丽的声音</li><li id="2318" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">要求</li><li id="3dc5" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">Lxml</li><li id="fefe" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">熊猫</li><li id="fe2c" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">Numpy</li><li id="3424" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">MatplotLib</li><li id="a5fd" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">张量流</li><li id="ae56" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">瓶</li><li id="1108" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">超文本标记语言</li><li id="fd2e" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">半铸钢ˌ钢性铸铁(Cast Semi-Steel)</li></ul><p id="1e58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在将检查项目的所有主要部分。</p><h1 id="3a19" class="lk ll iq bd lm ln mx lp lq lr my lt lu jw mz jx lw jz na ka ly kc nb kd ma mb bi translated">数据处理</h1><p id="d770" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">用于训练模型的文本是使用网络报废从<a class="ae lc" href="https://starwars.fandom.com/wiki/Main_Page" rel="noopener ugc nofollow" target="_blank">伍基人百科网站</a>(一种星球大战维基百科)中挖掘出来的。用于该任务的所有代码都在<a class="ae lc" href="https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/wookiescraper.py" rel="noopener ugc nofollow" target="_blank"> wookiescraper.py </a>文件中，并且创建了一个类<code class="fe oa ob oc nd b">Article</code>来构造文本，每篇文章都包含一个标题、主题描述(这将是文章页面上的第一个简要描述)以及它们所属的类别。提取数据的主要库是beautifulsoup、requests和Pandas(在数据帧中存储文本)。</p><p id="1517" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了列出所有可能的文章，调用了函数<code class="fe oa ob oc nd b">create_complete_database</code>。它创建了一个所有佳能文章的URL列表(在《星球大战》宇宙中，所有在书籍、漫画和视频游戏等替代媒体中产生的故事都被重新启动；旧的故事被贴上“<em class="lb">传说</em>的标签，而仍然是官方的故事和新的故事被认为是<em class="lb">经典</em>。然后，它通过使用<em class="lb"> Article </em>类自己的函数下载并创建列表中的每篇文章。然后创建一个包含所有下载文章的数据框架，并保存在一个<em class="lb">完整的Database.csv </em>文件中。</p><p id="4925" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了给模型提供信息，我们还必须处理获得的数据；文件<a class="ae lc" href="https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/data_processor.py" rel="noopener ugc nofollow" target="_blank"> data_processor.py </a>包含该任务使用的所有代码。函数<code class="fe oa ob oc nd b">clean_data</code>获取文本的数据帧，并通过删除不需要的字符和缩短文本对其进行格式化(这是必须要做的，因为我们正在创建一个在字符级别上工作的模型，该模型将很难从较长的句子中学习模式和上下文)。该文件还包含将给定的文本语料库转换成<em class="lb">单热点</em>向量的函数，反之亦然，以及<em class="lb">数据集生成器</em>函数；函数<code class="fe oa ob oc nd b">build_datasets</code>将构建一个<em class="lb"> training_dataset </em>和一个<em class="lb"> validation_dataset </em>，而不是在训练期间将所有数据加载到内存中，每一个都是Tensorflow <em class="lb"> Dataset </em>对象，它们在训练期间处理数据并将数据作为块馈送到模型中。</p><h1 id="dfa1" class="lk ll iq bd lm ln mx lp lq lr my lt lu jw mz jx lw jz na ka ly kc nb kd ma mb bi translated">模型和培训</h1><p id="aa74" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated"><a class="ae lc" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank"> LSTM </a>是一种递归神经网络细胞，在长数据序列中具有更高的信息保持能力。因为这个特性，它在处理<a class="ae lc" href="https://chatbotslife.com/ultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c" rel="noopener ugc nofollow" target="_blank"> NLP </a>问题时非常有用。对于这个项目，使用了一个<a class="ae lc" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank">序列到序列</a>架构来生成输出句子。在这种方法中，输入字符串(文章的标题)被提供给编码器，编码器按顺序逐个字符地处理数据，并传递包含输入信息的编码向量。然后，解码器将使用此信息再次按顺序逐个字符地生成新的嵌入向量，该向量将进入softmax层以生成概率向量，每个可能的字符一个值。每个输出字符在前一个字符之后生成，总是使用包含编码器生成的输入信息的向量。创建和训练模型的代码在文件<a class="ae lc" href="https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/run.py" rel="noopener ugc nofollow" target="_blank"> run.py </a>中。</p><p id="c34e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在GPU初始化和词汇定义之后，创建了一个<code class="fe oa ob oc nd b">config</code>字典，以使超参数调整更容易:</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="07c7" class="nh ll iq nd b gy ni nj l nk nl"># enable memory growth to be able to work with GPU<br/>GPU = tf.config.experimental.get_visible_devices('GPU')[0]<br/>tf.config.experimental.set_memory_growth(GPU, enable=True)</span><span id="9ef8" class="nh ll iq nd b gy od nj l nk nl"># set tensorflow to work with float64<br/>tf.keras.backend.set_floatx('float64')</span><span id="3ad0" class="nh ll iq nd b gy od nj l nk nl"># the new line character (\n) is the 'end of sentence', therefore there is no need to add a '[STOP]' character<br/>vocab = 'c-y5i8"j\'fk,theqm:/.wnlrdg0u1 v\n4b97)o36z2axs(p'<br/>vocab = list(vocab) + ['[START]']</span><span id="1a63" class="nh ll iq nd b gy od nj l nk nl">config = {  # dictionary that contains the training set up. Will be saved as a JSON file<br/>    'DIM_VOCAB': len(vocab),<br/>    'MAX_LEN_TITLE': MAX_LEN_TITLE,<br/>    'MAX_LEN_TEXT': MAX_LEN_TEXT,<br/>    'DIM_LSTM_LAYER': 512,<br/>    'ENCODER_DEPTH': 2,<br/>    'DECODER_DEPTH': 2,<br/>    'LEARNING_RATE': 0.0005,<br/>    'BATCH_SIZE': 16,<br/>    'EPOCHS': 100,<br/>    'SEED': 1,<br/>    # 'GRAD_VAL_CLIP': 0.5,<br/>    # 'GRAD_NORM_CLIP': 1,<br/>    'DECAY_AT_10_EPOCHS': 0.9,<br/>    'DROPOUT': 0.2,<br/>}</span></pre><p id="da7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了使结果具有可重复性，还将使用种子:</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="3dc9" class="nh ll iq nd b gy ni nj l nk nl">tf.random.set_seed(config['SEED'])</span></pre><p id="bb3f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">之后，最后，采集的数据被加载到数据帧中，进行清理，现在可以设置新的配置选项，如训练/验证分割:</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="2230" class="nh ll iq nd b gy ni nj l nk nl">data = pd.read_csv('Complete Database.csv', index_col=0)</span><span id="1111" class="nh ll iq nd b gy od nj l nk nl">data = clean_data(data)</span><span id="070b" class="nh ll iq nd b gy od nj l nk nl">config['STEPS_PER_EPOCH'] = int((data.shape[0] - 1500) / config['BATCH_SIZE'])<br/>config['VALIDATION_SAMPLES'] = int(<br/>    data.shape[0]) - (config['STEPS_PER_EPOCH'] * config['BATCH_SIZE'])<br/>config['VALIDATION_STEPS'] = int(<br/>    np.floor(config['VALIDATION_SAMPLES'] / config['BATCH_SIZE']))</span></pre><p id="7341" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后定义学习率。在这个项目中，一个指数衰减的学习率被证明可以给出最好的结果:</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="d10b" class="nh ll iq nd b gy ni nj l nk nl"># configures the learning rate to be decayed by the value specified at config['DECAY_AT_10_EPOCHS'] at each 10 epochs, but to that gradually at each epoch<br/>learning_rate = ExponentialDecay(initial_learning_rate=config['LEARNING_RATE'],<br/>                                decay_steps=config['STEPS_PER_EPOCH'],<br/>                                decay_rate=np.power(<br/>                                    config['DECAY_AT_10_EPOCHS'], 1/10),<br/>                                staircase=True)</span></pre><p id="6098" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">加载数据后，现在可以构建训练数据集和验证数据集:</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="c83e" class="nh ll iq nd b gy ni nj l nk nl">training_dataset, validation_dataset = build_datasets(<br/>    data, seed=config['SEED'], validation_samples=config['VALIDATION_SAMPLES'], batch=config['BATCH_SIZE'], vocab=vocab)</span></pre><p id="f608" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以训练后保存模型为目标，将选择一条路径。该文件夹将以通用名称命名，然后更改为模型完成训练的具体时间。此外，词汇和模型配置都将保存为<code class="fe oa ob oc nd b">json</code>文件。</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="7751" class="nh ll iq nd b gy ni nj l nk nl">folder_path = 'Training Logs/Training'  # creates folder to save traning logs<br/>if not os.path.exists(folder_path):<br/>    os.makedirs(folder_path)</span><span id="9859" class="nh ll iq nd b gy od nj l nk nl"># saves the training configuration as a JSON file<br/>with open(folder_path + '/config.json', 'w') as json_file:<br/>    json.dump(config, json_file, indent=4)<br/>    <br/># saves the vocab used as a JSON file<br/>with open(folder_path + '/vocab.json', 'w') as json_file:  <br/>    json.dump(vocab, json_file, indent=4)</span></pre><p id="2eee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了监控模型，使用了一些回调函数(在训练期间以指定的时间间隔调用的函数)。这些函数包含在<a class="ae lc" href="https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/callbacks.py" rel="noopener ugc nofollow" target="_blank"> callbacks.py </a>文件中。创建了一个自定义类<code class="fe oa ob oc nd b">CallbackPlot</code>，以便在整个训练过程中绘制训练误差。Tensorflow回调类<code class="fe oa ob oc nd b">ModelCheckpoint</code>和<code class="fe oa ob oc nd b">CSVLogger</code>的对象也被实例化，以便分别保存训练时的模型和训练日志:</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="d694" class="nh ll iq nd b gy ni nj l nk nl">loss_plot_settings = {'variables': {'loss': 'Training loss',<br/>                                    'val_loss': 'Validation loss'},<br/>                    'title': 'Losses',<br/>                    'ylabel': 'Epoch Loss'}</span><span id="217b" class="nh ll iq nd b gy od nj l nk nl">last_5_plot_settings = {'variables': {'loss': 'Training loss',<br/>                                    'val_loss': 'Validation loss'},<br/>                        'title': 'Losses',<br/>                        'ylabel': 'Epoch Loss',<br/>                        'last_epochs': 5}</span><span id="8274" class="nh ll iq nd b gy od nj l nk nl">plot_callback = CallbackPlot(folder_path=folder_path,<br/>                            plots_settings=[<br/>                                loss_plot_settings, last_5_plot_settings],<br/>                            title='Losses', share_x=False)</span><span id="3c51" class="nh ll iq nd b gy od nj l nk nl">model_checkpoint_callback = ModelCheckpoint(<br/>    filepath=folder_path + '/trained_model.h5')</span><span id="74b2" class="nh ll iq nd b gy od nj l nk nl">csv_logger = CSVLogger(filename=folder_path +<br/>                    '/Training logs.csv', separator=',', append=False)</span></pre><p id="0fe1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，可以构建、编译和训练模型:</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="f6b8" class="nh ll iq nd b gy ni nj l nk nl">###### BUILDS MODEL FROM SCRATCH WITH MULTI LAYER LSTM ###########<br/>tf.keras.backend.clear_session()  # destroys the current graph</span><span id="99d6" class="nh ll iq nd b gy od nj l nk nl">encoder_inputs = Input(shape=(None, config['DIM_VOCAB']), name='encoder_input')<br/>enc_internal_tensor = encoder_inputs</span><span id="1ed7" class="nh ll iq nd b gy od nj l nk nl">for i in range(config['ENCODER_DEPTH']):<br/>    encoder_LSTM = LSTM(units=config['DIM_LSTM_LAYER'],<br/>                        batch_input_shape=(<br/>                            config['BATCH_SIZE'], MAX_LEN_TITLE, enc_internal_tensor.shape[-1]),<br/>                        return_sequences=True, return_state=True,<br/>                        name='encoder_LSTM_' + str(i), dropout=config['DROPOUT'])<br/>    enc_internal_tensor, enc_memory_state, enc_carry_state = encoder_LSTM(<br/>        enc_internal_tensor)  # only the last states are of interest</span><span id="9aa2" class="nh ll iq nd b gy od nj l nk nl">decoder_inputs = Input(shape=(None, config['DIM_VOCAB']), name='decoder_input')<br/>dec_internal_tensor = decoder_inputs</span><span id="3e35" class="nh ll iq nd b gy od nj l nk nl">for i in range(config['DECODER_DEPTH']):<br/>    decoder_LSTM = LSTM(units=config['DIM_LSTM_LAYER'],<br/>                        batch_input_shape=(<br/>                            config['BATCH_SIZE'], MAX_LEN_TEXT, dec_internal_tensor.shape[-1]),<br/>                        return_sequences=True, return_state=True,<br/>                        name='decoder_LSTM_' + str(i), dropout=config['DROPOUT'])  # return_state must be set in order to retrieve the internal states in inference model later<br/>    # every LSTM layer in the decoder model have their states initialized with states from last time step from last LSTM layer in the encoder<br/>    dec_internal_tensor, _, _ = decoder_LSTM(dec_internal_tensor, initial_state=[<br/>                                            enc_memory_state, enc_carry_state])</span><span id="ee34" class="nh ll iq nd b gy od nj l nk nl">decoder_output = dec_internal_tensor</span><span id="c592" class="nh ll iq nd b gy od nj l nk nl">dense = Dense(units=config['DIM_VOCAB'], activation='softmax', name='output')<br/>dense_output = dense(decoder_output)</span><span id="f4d6" class="nh ll iq nd b gy od nj l nk nl">model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=dense_output)</span><span id="febe" class="nh ll iq nd b gy od nj l nk nl">model.compile(optimizer=optimizer, loss='categorical_crossentropy')</span><span id="b2f1" class="nh ll iq nd b gy od nj l nk nl">history = model.fit(x=training_dataset,<br/>                    epochs=config['EPOCHS'],<br/>                    steps_per_epoch=config['STEPS_PER_EPOCH'],<br/>                    callbacks=[plot_callback, csv_logger,<br/>                            model_checkpoint_callback],<br/>                    validation_data=validation_dataset,<br/>                    validation_steps=config['VALIDATION_STEPS'])</span></pre><p id="731b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练后，文件夹将包含与本次训练相关的所有数据，如损失函数图、不同时间步长的误差以及模型本身。</p><pre class="mi mj mk ml gt nc nd ne nf aw ng bi"><span id="c67a" class="nh ll iq nd b gy ni nj l nk nl">model.save(folder_path + '/trained_model.h5', save_format='h5')<br/>model.save_weights(folder_path + '/trained_model_weights.h5')<br/>plot_model(model, to_file=folder_path + '/model_layout.png', show_shapes=True, show_layer_names=True, rankdir='LR')<br/></span><span id="ebee" class="nh ll iq nd b gy od nj l nk nl">timestamp_end = datetime.now().strftime('%d-%b-%y -- %H:%M:%S')</span><span id="51f8" class="nh ll iq nd b gy od nj l nk nl"># renames the training folder with the end-of-training timestamp<br/>root, _ = os.path.split(folder_path)</span><span id="04ac" class="nh ll iq nd b gy od nj l nk nl">timestamp_end = timestamp_end.replace(':', '-')<br/>os.rename(folder_path, root + '/' + 'Training Session - ' + timestamp_end)</span><span id="7ffe" class="nh ll iq nd b gy od nj l nk nl">print("Training Successfully finished.")</span></pre><h1 id="aa6b" class="lk ll iq bd lm ln mx lp lq lr my lt lu jw mz jx lw jz na ka ly kc nb kd ma mb bi translated">部署</h1><p id="a0fa" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">为了服务于该模型，使用Flask包构建了一个简单的接口，其代码可以在<a class="ae lc" href="https://github.com/Pedrohgv/Star-Wars-Text-Generation/tree/master/deploy" rel="noopener ugc nofollow" target="_blank"> deploy </a>文件夹下找到。</p><h1 id="11dd" class="lk ll iq bd lm ln mx lp lq lr my lt lu jw mz jx lw jz na ka ly kc nb kd ma mb bi translated">结论</h1><p id="99a0" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">经过训练后，该模型能够在给定种子字符串的情况下生成句子。下面是生成句子和用于生成句子的种子字符串的一些示例(我将使用我的猫的名字作为示例，但是因为它们已经被称为Luke、Han和Leia，所以它们已经存在于训练数据集中):</p><ul class=""><li id="0663" class="nm nn iq kh b ki kj kl km ko no ks np kw nq la nr ns nt nu bi translated">佩德罗:佩德罗是一名男性人类，在瑞博·l·阿拉内特·奥尼号上担任指挥官。</li><li id="7319" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">佩德罗·恩里克:佩德罗·欧索农是一名人类女性，在银河共和国的加巴尔形态中担任指挥官。</li><li id="4a40" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">奇科是一名塔姆泰德人，作为银河帝国的指挥官为银河帝国服务。</li><li id="f616" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">莎拉:莎拉是一名人类男性，在银河内战期间，他在共和军中担任指挥官。</li></ul><p id="7e4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从上面可以看出，该模型学习了如何合理地形成一些单词，如何确定这些单词的大小，如何正确地结束一个句子，以及如何形成某种上下文。然而，它似乎严重偏向于总是描述在《星球大战》宇宙中的一个派系下服务的人类。这可以用这样一个事实来解释，即模型的架构不是使用单词嵌入构建的(这将允许更复杂的上下文学习)，因为有几个单词是《星球大战》独有的。我对未来项目的一个好主意将是为星球大战宇宙生成一个特定的单词嵌入，然后使用它来生成新的文本。</p></div></div>    
</body>
</html>