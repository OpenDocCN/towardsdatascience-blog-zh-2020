<html>
<head>
<title>A Breakthrough in Deep Image Inpainting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深层图像修复的突破</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0?source=collection_archive---------24-----------------------#2020-10-23">https://towardsdatascience.com/a-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0?source=collection_archive---------24-----------------------#2020-10-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bafc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于上下文关注的生成式图像修复综述</h2></div><p id="4631" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">欢迎回来伙计们！很高兴见到你们:)上一次，我们意识到复制粘贴是如何嵌入CNN进行深度图像修复的。你能领会主旨吗？如果是的话，很好！如果没有，不用担心！今天，我们将深入研究深层图像修复的突破，为此提出了上下文注意。通过使用上下文注意，我们可以有效地从遥远的空间位置借用信息来重建局部缺失的像素。这个想法实际上和复制粘贴差不多。让我们看看他们如何一起做到这一点！</p><h1 id="deca" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">回忆</h1><p id="446d" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">在<a class="ae ly" rel="noopener" target="_blank" href="/how-copy-and-paste-is-embedded-in-cnns-for-image-inpainting-review-shift-net-image-433a2a93c963">我之前的帖子</a>中，我已经介绍了<strong class="kh ir">移动连接层</strong>，其中来自已知区域的<strong class="kh ir"> <em class="lz">特征作为缺失区域内生成特征的参考，以允许我们进一步细化生成的特征，从而获得更好的修复结果</em> </strong>。这里，我们假设所生成的特征是对基本事实的合理估计，并且根据来自已知区域的特征和缺失区域内所生成的特征之间的相似性来确定合适的参考。</p><h1 id="9802" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">动机</h1><p id="17fd" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">对于图像修复任务，<strong class="kh ir"> <em class="lz">细胞神经网络的结构不能有效地模拟缺失区域和由遥远的空间位置</em> </strong>给出的信息之间的长期相关性。如果你熟悉CNN，你应该知道核大小和扩张率控制卷积层的感受野，网络必须越来越深，才能看到整个输入图像。这意味着<strong class="kh ir"> <em class="lz">如果我们想要捕捉图像的上下文，我们必须依赖更深的层，但我们会丢失空间信息，因为更深的层总是具有更小的空间尺寸特征</em> </strong>。因此，我们必须找到一种方法，在不深入网络的情况下，从遥远的空间位置借用信息(即理解图像的上下文)。</p><p id="2bea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你记得什么是扩展卷积(我们在<a class="ae ly" rel="noopener" target="_blank" href="/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df">之前的</a>文章中已经讨论过)，你会知道扩展卷积是一种在早期卷积层增加感受野的方法，而不需要增加额外的参数。然而，<strong class="kh ir"> <em class="lz">扩张卷积有其局限性。它跳过连续的空间位置，以便扩大感受野</em> </strong>。注意，跳过的连续空间位置对于填充缺失区域也是至关重要的。</p><h1 id="29b6" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">介绍</h1><p id="2f34" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">这项工作分享了类似的网络架构，损失函数和相关技术，我们已经涵盖了前<a class="ae ly" rel="noopener" target="_blank" href="/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df">。对于架构，<strong class="kh ir"> <em class="lz">提出的架构由两个发生器网络和两个鉴别器网络</em> </strong>组成。这两个生成器遵循具有扩展卷积的全卷积网络。一个生成器用于粗略重建，另一个用于精细重建。这被称为标准的由粗到细的网络结构。<strong class="kh ir"> <em class="lz">两个鉴别器也同时全局和局部查看完成的图像</em> </strong>。全局鉴别器将整个图像作为输入，而局部鉴别器将填充区域作为输入。</a></p><p id="d656" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于损失函数，简单地说，他们也采用对抗损失(GAN损失)和L1损失(为了像素重建精度)。对于L1损失，他们使用一个<strong class="kh ir"> <em class="lz">空间折扣L1损失</em> </strong>，其中为每个像素差异分配一个权重，该权重基于一个像素到其最近的已知像素的距离。对于GAN损失，他们使用一个<strong class="kh ir"> <em class="lz"> WGAN-GP损失</em> </strong>而不是我们介绍的最标准的对抗性损失。他们声称，这种WGAN对抗损失也是基于L1距离度量的，因此网络更容易训练，并且训练过程更稳定。</p><p id="1ac6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我想重点介绍一下提出的<strong class="kh ir"><em class="lz"/></strong>。因此，我在上面简要介绍了由粗到细的网络架构、WGAN对抗性损耗和加权L1损耗。感兴趣的读者可以参考我之前的帖子和本作的<a class="ae ly" href="https://arxiv.org/pdf/1801.07892.pdf" rel="noopener ugc nofollow" target="_blank">论文了解更多详情。</a></p><h1 id="48f9" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">解决方案(简而言之)</h1><p id="bc79" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated"><strong class="kh ir">提出了<strong class="kh ir"> <em class="lz">上下文注意</em></strong>机制，有效地从遥远的空间位置借用上下文信息来重建缺失像素 </strong>。上下文关注被应用于第二细分网络。第一粗略重建网络负责丢失区域的粗略估计。和前面一样，全局和局部鉴别器被用来鼓励生成像素的更好的局部纹理细节。</p><h1 id="871e" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">贡献</h1><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/14b351213d0668c08e162077c9b4cf8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CpWLaX-KOrj7t1klMt64Lw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图一。在自然场景、人脸和纹理图像上的修复结果的一些例子。图片来自于佳卉等人的<a class="ae ly" href="https://arxiv.org/pdf/1801.07892.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> [1]</p></figure><p id="cb3b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文中最重要的思想是<strong class="kh ir">上下文注意</strong>，它允许我们利用来自遥远空间位置的信息来重建局部缺失像素。第二，<strong class="kh ir"> WGAN对抗损失</strong>和<strong class="kh ir">加权L1损失</strong>的使用提高了训练的稳定性。此外，所提出的修复框架在各种数据集(如自然场景、人脸和纹理)上实现了高质量的修复结果，如上图1所示。</p><h1 id="45af" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">方法</h1><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mq"><img src="../Images/de6536d9082e8b0e9b08ff33d1c8e236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sGAZM3GZMUG0cRRYSZTP0g.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图二。所提出的修复框架的网络结构。图片来自于佳卉等人的<a class="ae ly" href="https://arxiv.org/pdf/1801.07892.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="b99b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图2显示了所提出的修复框架的网络架构。如上所述，它由<strong class="kh ir">两个发生器</strong>和<strong class="kh ir">两个鉴别器</strong>组成。如果你读过我以前的帖子，你会发现这是典型的深层图像修复网络架构。</p><h1 id="1373" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">语境注意</h1><p id="2678" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">这是这篇文章的重点。让我们看看所提出的上下文注意层是如何被设计来借用由遥远空间位置处的已知区域给出的特征信息来在缺失区域内生成特征的。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mr"><img src="../Images/a8fc19792a1df097d1c14f433279e963.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M-3oTs0df0T5_jppYpSJTQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图3。提议的上下文注意层的图示。图片来自于佳卉等人的<a class="ae ly" href="https://arxiv.org/pdf/1801.07892.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>【1】</p></figure><p id="99c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图3以图形方式展示了所提议的上下文关注层。<strong class="kh ir"> <em class="lz">运算可微且全卷积</em> </strong>。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ms"><img src="../Images/3ae5058f3f8ecaaca491045ddea69693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BAqkrfKBYUQU7BeBOCqBQg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图4。上下文注意力层的更具体的例子。图片作者。</p></figure><p id="d893" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图4是提议的上下文注意层的更详细的例子。如图3所示，前景表示缺失区域内生成的特征，而背景表示从已知区域提取的特征。类似于复制粘贴的方法，我们首先要<strong class="kh ir"> <em class="lz">将缺失区域内生成的特征与缺失区域外的特征</em> </strong>进行匹配。</p><p id="0252" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以图4为例，缺失区域内生成的特征大小为64×64×64，假设缺失区域外的特征分为128个大小为64×3×3的小特征块。请注意，本例中特征的通道大小为64。然后，利用128个小特征块和缺失区域内生成的特征进行卷积，得到大小为128×64×64的特征图。在论文中，这种操作被描述为，</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/dc1d46d57f121f4a0dfa14cb21cf5295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*8VETtN5WUC_582oG3iR-NQ.png"/></div></figure><p id="91b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中{ <em class="lz"> f </em> _ <em class="lz"> x </em>，<em class="lz"> y </em> }为前景补片(缺失区域内生成的特征补片)，{<em class="lz">b</em>_<em class="lz">x’</em>，<em class="lz">y’</em>}为背景补片(缺失区域外提取的特征补片)。<em class="lz"> s </em> _ <em class="lz"> x </em>，<em class="lz"> y </em>，<em class="lz">x’</em>，<em class="lz">y’</em>是生成的以缺失区域为中心的面片(<em class="lz"> x </em>，<em class="lz"> y </em>)与以已知区域为中心的已知面片(<em class="lz">x’</em>，<em class="lz">y’</em>)之间的相似度。实际上，这是一个标准的余弦相似性度量过程。</p><p id="c38a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们沿着通道维度观察时，128个元素代表了所有已知补片和缺失区域内特定位置之间的相似性。这反映了128个已知斑块对该位置的贡献。然后，我们沿着通道维度对特征图执行Softmax归一化，如图4中蓝色区域所示。在Softmax归一化之后，沿着通道维度的每个位置的总和应该是1。</p><blockquote class="mu mv mw"><p id="211d" class="kf kg lz kh b ki kj jr kk kl km ju kn mx kp kq kr my kt ku kv mz kx ky kz la ij bi translated">与我在<a class="ae ly" rel="noopener" target="_blank" href="/how-copy-and-paste-is-embedded-in-cnns-for-image-inpainting-review-shift-net-image-433a2a93c963">上一篇文章</a>中提到的移位网相比，你可以看到，这次我们为每个已知特征块分配了一个权重，以表明它对于重建缺失区域内每个特征位置的重要性(<strong class="kh ir">软分配</strong>)，而不是只保留与缺失区域内每个特征位置最相似的已知特征块(<strong class="kh ir">硬分配</strong>)。这也是为什么提出的语境注意是可微的原因。</p></blockquote><p id="9ed8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们使用注意特征图作为输入特征，使用已知补丁作为核，通过反卷积来重建缺失区域内的生成特征。对于对实际实现感兴趣的读者，可以访问他们的<a class="ae ly" href="https://github.com/JiahuiYu/generative_inpainting/tree/v1.0.0" rel="noopener ugc nofollow" target="_blank"> github项目页面</a>了解更多细节。</p><h1 id="213e" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">注意力传播</h1><p id="b8a6" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">注意力传播可以被视为注意力特征图的微调。这里的关键思想是<strong class="kh ir"> <em class="lz">相邻像素通常具有更接近的像素值</em> </strong>。这意味着他们考虑邻域的关注值来调整每个关注分数，</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi na"><img src="../Images/0bc7c5c90d0f89aadd8cf219955f6c22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4zPcwwBsiJJU8O0Jk57n2A.png"/></div></div></figure><p id="7523" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，如果我们考虑左右邻居的关注值，我们可以使用上面列出的等式来更新当前的关注值。注意<em class="lz"> k </em>控制要考虑的邻居数量。</p><p id="f6d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者声称，这可以进一步改善修补结果，并且这也可以通过以单位矩阵作为核的卷积来完成。</p><p id="5237" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于注意机制的另一点是，使用了两种技术来控制提取的已知特征补丁的数量。<strong class="kh ir"> <em class="lz"> i) </em> </strong>提取步幅较大的已知特征面片，减少核的数量。<strong class="kh ir"> <em class="lz"> ii) </em> </strong>操作前对特征图大小进行下采样，得到关注图后再进行上采样。</p><h1 id="0ec6" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">网络注意力</h1><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nb"><img src="../Images/c0d7d4b7d7c1b42641f6eeaa6dc95a66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tpIs6Fs1B23QDYozBSlJHA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图5。在第二细化网络中嵌入上下文注意层的图示。图片来自于佳卉等人的<a class="ae ly" href="https://arxiv.org/pdf/1801.07892.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> [1]</p></figure><p id="3291" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图5显示了作者如何将提出的上下文关注层整合到第二个细化网络中。你可以看到<strong class="kh ir"> <em class="lz">又引入了一个分支来应用上下文关注，然后两个分支被连接起来以获得最终的修复结果</em> </strong>。注意力地图颜色编码用于可视化注意力地图。例如，中间的白色表示像素聚焦于自身，粉色在左下区域，绿色在右上区域，等等。你可以看到这个例子有一个粉红色填充的注意力地图。这意味着填充区域从左下区域借用了很多信息。</p><h1 id="5592" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">实验</h1><p id="2051" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">作者首先与我们之前介绍过的<a class="ae ly" rel="noopener" target="_blank" href="/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df">之前的最新技术</a>进行了比较。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nc"><img src="../Images/529d320e177f43d49bff6e7eeb804819.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dbttFfPFmsZTiIEpYAAD-A.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图6。提议的基线模型与GLCIC的比较[2]。从左至右，输入图像、GLCIC结果和基线结果。图片来自于佳卉等人的论文</p></figure><p id="0ec9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图6显示了所提出的基线模型和之前的最新技术GLCIC [2]的修复结果。提议的基线模型如图2所示，没有提议的上下文注意分支。很明显，基线模型在局部纹理细节方面优于GLCIC。请放大以便看得更清楚。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nd"><img src="../Images/955eedc6af8c18c2cb279737d3f7ffd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zMcnRHcLxAIxWOR1YZtpHg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图7。基线和完整模型修复结果的视觉比较。从左到右，地面实况、输入图像、基线结果、全模型结果、全模型注意力图。图片来自于佳卉等人的<a class="ae ly" href="https://arxiv.org/pdf/1801.07892.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> [1]</p></figure><p id="b2ce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图7显示了在Places2数据集上使用基线模型和完整模型(注意上下文)的定性结果。很明显，完整模型提供了更好的修复结果，具有精细的局部纹理细节。这反映了上下文注意层可以有效地从远处的空间位置借用信息来帮助重建丢失的像素。请放大以获得更好的视图，尤其是注意力地图。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ne"><img src="../Images/23996049ffd90725910b0ea020e87280.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRENr5OdlH-IxWaQiFK22Q.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">表1。Places2数据集上不同方法的定量比较。表由于佳卉等人从他们的<a class="ae ly" href="https://arxiv.org/pdf/1801.07892.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> [1]</p></figure><p id="c4f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">表1列出了一些客观的评估指标供参考。如前所述，这些度量不能完全反映修补结果的质量，因为有许多可能的解决方案来填充缺失区域。你可以看到，建议的完整模型提供了最佳的<em class="lz"> l </em> 1、<em class="lz"> l </em> 2损耗和PSNR。对于电视损耗，PatchMatch提供了更低的电视损耗，因为它直接复制原始图像补丁来填充孔洞。</p><p id="b3f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为参考，建议的全模型有2.9M参数。对于512×512大小的图像，在GPU上每张图像需要0.2秒，在CPU上每张图像需要1.5秒。</p><h1 id="d78b" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">消融研究</h1><p id="a44b" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">注意机制不是一个新的想法，在文献中有几个注意模块。作者做了使用不同注意力模块的实验。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nf"><img src="../Images/e0011416090619782a2b27814f71e7b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uDC1T3ACuuPIYSRLqo_mpA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图8。使用不同注意模块的修复结果。从左到右:输入、使用空间变换网络的结果、使用外观流的结果、以及使用提议的上下文注意的结果。图片来自于佳卉等人的<a class="ae ly" href="https://arxiv.org/pdf/1801.07892.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="910d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者对比了文献中两个著名的注意模块，即空间变换网络[3]和表象流[4]。简单来说，对于外观流，使用卷积层来代替上下文注意层，以直接预测2D像素偏移作为注意。这意味着我们添加了一个卷积层来预测已知像素向缺失像素的偏移。在图8中，您可以看到使用外观流(中间)的结果为不同的测试图像提供了相似的注意力图。这意味着注意力地图对于给予我们想要的“注意力”没有用。您还可以观察到，空间转换器网络(左)无法为图像修复任务提供有意义的注意力图。一个可能的原因是空间变换网络预测全局仿射变换的参数，这不足以帮助填充也需要局部信息的缺失区域。在这里，我没有深入探讨不同的注意力模块。感兴趣的读者可以参考报纸了解更多的细节。</p><p id="901a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">选择GAN损失进行图像修复</strong>。作者实验了不同的GAN损失，例如WGAN损失、典型对抗损失和最小平方GAN。他们凭经验发现，WGAN损失提供了最好的修复结果。</p><p id="b1f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">基本重建损失</strong>。作者在没有L1损失的情况下训练精化网络。他们发现，L1损失是必要的，以确保像素重建精度，即使L1损失使修补结果模糊。因此，L1损失对于确保完整图像的更好的内容结构是至关重要的。</p><p id="a703" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">感性丧失、风格丧失、电视丧失</strong>。我们将很快涵盖感知损失和风格损失。这里一个简单的结论是，这三个损失并没有给修复结果带来明显的改善。因此，他们的模型仅使用加权L1损耗和WGAN损耗来训练。</p><h1 id="3c00" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">结论</h1><p id="2aec" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">显然，本文的核心思想是语境注意机制。上下文关注层嵌入在第二细化网络中。注意，第一粗略重建网络的作用是对缺失区域进行粗略估计。这种估计用于上下文注意层。通过匹配缺失区域内的生成特征和缺失区域外的特征，我们可以知道缺失区域外的所有特征对缺失区域内每个位置的贡献。注意，上下文注意层是可微分的和完全卷积的。利用提出的上下文注意，他们实现了最先进的修复结果。</p><h1 id="631b" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">外卖食品</h1><p id="2acd" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">你可能会发现，我们正在越来越深入到深层图像修复领域。在我之前的文章中，<a class="ae ly" rel="noopener" target="_blank" href="/how-copy-and-paste-is-embedded-in-cnns-for-image-inpainting-review-shift-net-image-433a2a93c963">转移连接层</a>被介绍，它以硬分配的形式将复制粘贴的概念嵌入到CNN中。本文以软分配的形式制定了上下文注意层，使得该层是可微分的，并且可以在不修改梯度计算的情况下端到端地学习。</p><p id="8860" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">希望你能掌握本文提出的情境注意层的关键思想，尤其是它的提法如图3和图4所示。对于想了解更多网络架构和损失函数的读者，请参考<a class="ae ly" href="https://arxiv.org/pdf/1801.07892.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><h1 id="779f" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">下一步是什么？</h1><p id="57e7" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">在以后的文章中，我们将会研究更多的特定任务修复技术。希望我们能一起学习和享受！</p><h1 id="6d9b" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">参考</h1><p id="3b6c" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">[1]，林哲，，沈晓辉，路欣，，<a class="ae ly" href="https://arxiv.org/abs/1801.07892" rel="noopener ugc nofollow" target="_blank"/><em class="lz">Proc .计算机视觉与模式识别</em> ( <em class="lz"> CVPR </em>)，2018。</p><p id="2c94" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]饭冢聪，埃德加·西莫-塞拉，石川宽，“<a class="ae ly" href="http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf" rel="noopener ugc nofollow" target="_blank">全局和局部一致的图像完成</a>，<em class="lz"> ACM Trans .论图形</em>，第36卷，№4，第107条，出版日期:2017年7月。</p><p id="01aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] M. Jaderberg，K. Simonyan，A. Zisserman等人，“<a class="ae ly" href="https://arxiv.org/abs/1506.02025" rel="noopener ugc nofollow" target="_blank">空间变换网络</a>”，载于《神经信息处理系统进展》，2017–2025页，2015年。</p><p id="92c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] T. Zhou，S. Tulsiani，W. Sun，J. Malik和A. A. Efros，"通过外观流的视图合成<a class="ae ly" href="https://arxiv.org/abs/1605.03557" rel="noopener ugc nofollow" target="_blank">，"<em class="lz"> Proc .2016年欧洲计算机视觉会议</em> ( <em class="lz"> ECCV </em>)。</a></p><p id="3f39" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">再次感谢你阅读我的帖子！如果您有任何问题，请随时给我发电子邮件或在这里留言。</p><p id="8391" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实上，我尽量缩短文章的长度，只关注文章的一个关键观点。我认为读者已经从我以前的帖子中了解了深层图像修复的基本知识。顺便说一下，我必须不断提高我的写作技巧，以更有效地表达我对论文的理解。欢迎任何建议。系统学习对我们来说极其重要。非常感谢，下次再见！:)</p></div></div>    
</body>
</html>