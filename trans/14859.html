<html>
<head>
<title>Feature Selection for Machine Learning in Python — Wrapper Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中机器学习的特征选择——包装器方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-for-machine-learning-in-python-wrapper-methods-2b5e27d2db31?source=collection_archive---------12-----------------------#2020-10-13">https://towardsdatascience.com/feature-selection-for-machine-learning-in-python-wrapper-methods-2b5e27d2db31?source=collection_archive---------12-----------------------#2020-10-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7fa0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用最大似然算法选择正确的预测值</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/67c40924cb3eb96429a5b215c559d3de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmoxCWOI7hPPf_rR65ZkVQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由作者根据<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae ky" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Markus Spiske </a>的照片编辑</p></figure><p id="c280" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文的<a class="ae ky" rel="noopener" target="_blank" href="/feature-selection-for-machine-learning-in-python-filter-methods-6071c5d267d5?source=friends_link&amp;sk=6a6258beba174b3c5513f606dfdb9f6b">第一系列</a>中，我们讨论了什么是特性选择，并提供了一些使用统计方法的演练。本文在原文章的基础上，进一步解释了机器学习(ML)的特征选择中的另外两种常见方法，即包装器和嵌入式方法。解释将伴随着用Python编写的示例代码。</p><p id="f2af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">概括地说，特征选择意味着减少用于训练ML模型的预测器的数量。主要目标是提高预测性能的准确性(通过减少冗余预测值的数量)，减少计算时间(预测值越少，计算所需的时间越少)，以及提高模型的可解释性(当预测值数量较少时，更容易研究预测值的相关性)。基于统计技术的过滤方法通常可以独立于用于ML模型的算法来应用。然而，包装器和嵌入式方法通常被“微调”以优化分类器性能，如果目标是客观地为特定的学习算法或模型找到一组理想的预测器，则它们是理想的。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="ba2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">介绍和概念</strong></p><blockquote class="mc md me"><p id="acb9" class="kz la mf lb b lc ld ju le lf lg jx lh mg lj lk ll mh ln lo lp mi lr ls lt lu im bi translated"><strong class="lb iu">包装方法</strong>使用添加和/或移除预测器的程序评估多个模型，以找到最大化模型性能的最佳组合。[1]这些程序通常建立在贪婪搜索技术(或算法)的概念之后。[2]贪婪算法是遵循在每个阶段做出局部最优选择的问题解决启发式算法的任何算法。[3]</p></blockquote><p id="6913" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一般来说，程序的三个方向是可能的:</p><ul class=""><li id="e9b9" class="mj mk it lb b lc ld lf lg li ml lm mm lq mn lu mo mp mq mr bi translated"><strong class="lb iu"> <em class="mf">正向选择</em> </strong> —从一个预测器开始，迭代增加更多预测器。在随后的每次迭代中，基于性能标准添加剩余的最佳原始预测值。</li><li id="6a85" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated"><strong class="lb iu"> <em class="mf">向后消除</em> </strong> —从所有预测值开始，逐一迭代消除。最流行的算法之一是递归特征消除(RFE ),它根据特征重要性排序消除不太重要的预测器。</li><li id="ee36" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated"><strong class="lb iu"> <em class="mf">逐步选择</em> </strong> —双向，基于向前选择和向后消除的组合。它被认为没有前两个过程那么贪婪，因为它确实重新考虑将预测器添加回已经被移除的模型中(反之亦然)。尽管如此，在任何给定的迭代中，考虑因素仍然是基于局部优化的。</li></ul><p id="830d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">Python中的包装方法</strong></p><p id="8bb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python中有两个流行的库可用于执行包装器风格的特性选择——来自<a class="ae ky" href="http://rasbt.github.io/mlxtend/" rel="noopener ugc nofollow" target="_blank"> mlxtend </a>的<a class="ae ky" href="http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/" rel="noopener ugc nofollow" target="_blank">顺序特性选择器</a>和来自<a class="ae ky" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>的<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank">递归特性消除</a>。</p><p id="28a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整的Python代码可以在<a class="ae ky" href="https://github.com/jackty9/Feature_Selection_Wrapper_Methods_in_Python" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。使用的数据是来自Scikit-learn的波士顿房价数据集。</p><ol class=""><li id="40e9" class="mj mk it lb b lc ld lf lg li ml lm mm lq mn lu mx mp mq mr bi translated"><strong class="lb iu">正向选择—来自mlxtend的SFS()</strong></li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/080c3434e06b434bc7eb8e22587fd3ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmAcB4m6VfNKFKLkfgzwvw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">打印输出:5个最重要的特征以基于R平方评分的正向选择方式迭代地添加到子集。</p></figure><p id="289b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nb nc nd ne b">SequentialFeatureSelector()</code>类接受以下主要参数:</p><ul class=""><li id="85a2" class="mj mk it lb b lc ld lf lg li ml lm mm lq mn lu mo mp mq mr bi translated"><code class="fe nb nc nd ne b">LinearRegression()</code>作为特征选择过程的估计器。或者，也可以用其他基于<em class="mf">回归</em>或<em class="mf">分类</em>的算法代替。</li><li id="62a6" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated"><code class="fe nb nc nd ne b">k_features</code>表示要选择的特征数量。出于演示目的，从最初的13个特征中选择了5个特征。可以通过分析不同数量特征的<em class="mf">分数</em>来优化该值。</li><li id="19ce" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated"><code class="fe nb nc nd ne b">forward</code>指示所用包装方法的方向。<code class="fe nb nc nd ne b">forward = True</code>用于<em class="mf">前进选择</em>，而<code class="fe nb nc nd ne b">forward = False</code>用于<em class="mf">后退淘汰</em>。</li><li id="cf86" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated"><code class="fe nb nc nd ne b">Scoring</code>参数指定要使用的<em class="mf">评估标准</em>。对于<em class="mf">回归</em>问题，<code class="fe nb nc nd ne b">r2</code> <em class="mf">得分</em>是默认且唯一实现。但是对于<em class="mf">分类</em>，有准确度、精度、召回率、f1-score等选项。</li><li id="6b30" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated"><code class="fe nb nc nd ne b">cv</code>论证是为了<em class="mf"> k倍交叉验证</em>。默认情况下，它将被设置为5。请记住，大量的交叉验证可能非常耗时且计算量大。</li></ul><p id="1756" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于其他可能的参数，请参考<a class="ae ky" href="http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/#sequentialfeatureselector" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="571c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2。落后淘汰——RFE()来自Sklearn </strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/8f6a83caca6fba01e4c9cbcbf043a4e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ehfdCSP4EWjk7ysyBJ6yLQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">打印输出:根据回归系数，以递归反向消除方式将5个最重要的特征选入子集。由于可用的方法不同，每个迭代的中间评分不是立即可用的。但是，<code class="fe nb nc nd ne b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE.score" rel="noopener ugc nofollow" target="_blank"><strong class="bd ng">score</strong></a></code> (X，y)可以用来输出底层估计量的得分。</p></figure><p id="8449" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nb nc nd ne b"><strong class="lb iu">RFE</strong>(<em class="mf">estimator</em>, <em class="mf">n_features_to_select</em>)</code>是一个代表重复特征消除的类，来源于机器学习算法常用的sklearn库，它接受以下主要参数:</p><ul class=""><li id="786b" class="mj mk it lb b lc ld lf lg li ml lm mm lq mn lu mo mp mq mr bi translated"><code class="fe nb nc nd ne b">estimator</code>(在上面的示例代码中使用<em class="mf">模型</em>传递)作为特征选择过程的对象。对于所示的回归问题，<code class="fe nb nc nd ne b">coef_</code>属性用于确定特征重要性。对于树状算法，使用<code class="fe nb nc nd ne b">feature_importances_</code>属性代替。</li><li id="09d9" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated"><code class="fe nb nc nd ne b">n_features_to_selec</code>指要选择的特征数量。如果<code class="fe nb nc nd ne b">None</code>，则选择一半的特征。</li></ul><p id="8112" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于其他可能的参数，请参考<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="ccb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3。分步选择—mlx tend</strong>中的SFFS()</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/7cb4b88859543b25a71985ed8f659bed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8O_SESCsSwR5TuuR0Zak7Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">打印输出:5个最重要的特征基于R平方评分以逐步方式迭代添加到子集。</p></figure><p id="6cad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似于<em class="mf"> 1中使用的类。正向选择</em>，使用的参数是相同的，除了:</p><ul class=""><li id="407a" class="mj mk it lb b lc ld lf lg li ml lm mm lq mn lu mo mp mq mr bi translated"><code class="fe nb nc nd ne b">floating</code>参数添加了特征的条件排除/包含，以创建双向选择。当<code class="fe nb nc nd ne b">forward = True</code>时，这意味着对于每个正向迭代(在<em class="mf">顺序正向选择</em>中)，它还考虑排除前一次迭代中的特征以优化性能。反之亦然，用于<em class="mf">反向消除</em>将特征包括在现有子集中。</li></ul><p id="25e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">奖励——理想的功能数量是多少？</strong></p><p id="2a09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在最后3个例子中，分配给<code class="fe nb nc nd ne b">k_features</code>的特征数量设置为5。但是我们怎么知道5是不是最好的数字呢？幸运的是，在同一个mlxtend库中，可以使用<code class="fe nb nc nd ne b">plot_sequential_feature_selection</code>类来可视化评分，以使决策更容易。更多详情，请参考<a class="ae ky" href="http://rasbt.github.io/mlxtend/user_guide/plotting/plot_sequential_feature_selection/" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/4c2b8c0b96d12310e930c3829341fdfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YuFcpkg8kIQNMsswQwbuog.png"/></div></div></figure><p id="bf2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如图所示，在5重交叉验证(<code class="fe nb nc nd ne b">cv=5</code>)的情况下，性能在7个特征处达到峰值。不要与性能的负号混淆，<code class="fe nb nc nd ne b">neg_mean_squared_error</code>只是将均方差(MSE)的结果乘以<code class="fe nb nc nd ne b">-1</code>，以遵循<code class="fe nb nc nd ne b">sklearn</code>的惯例。其思想是最小化MSE相当于最大化负MSE。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="86f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">总结</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/b4be0ca88345964020d65336ce9bdd9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B3v-RJiDUduUptTmHiheUA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者自我图解。</p></figure><p id="28ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">过滤器和包装器方法之间的主要区别是对学习算法的依赖。通过观察红框，可以在没有学习算法的先验知识的情况下统计地执行过滤方法。另一方面，包装器方法基于学习算法中使用的估计器迭代地选择特征。它们就像一把双刃剑，所选择的特性将最适合所选择的算法，但可能不太适合另一种算法。包装器方法也更容易过度拟合，因为它涉及用几组特征组合进行训练。因为包装器方法使用交叉验证来选择最佳的特征子集，所以这个过程在计算上是昂贵的。包装方法的优点和缺点可以总结为:</p><p id="fc6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优点:</p><ul class=""><li id="0c7c" class="mj mk it lb b lc ld lf lg li ml lm mm lq mn lu mo mp mq mr bi translated">面向算法，这意味着所选算法通常具有良好的性能</li></ul><p id="da2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缺点:</p><ul class=""><li id="36a7" class="mj mk it lb b lc ld lf lg li ml lm mm lq mn lu mo mp mq mr bi translated">过度装配的风险</li><li id="281b" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">可能计算量很大</li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="7c9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">下一步预期什么</strong></p><p id="dd92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文介绍了特性选择的第二种方法——使用ML算法的包装器方法。在下一篇文章中，我们将研究最后一种方法，也称为嵌入式方法。如果你想了解更多关于特征选择和过滤方法的基础知识，你可以点击<a class="ae ky" rel="noopener" target="_blank" href="/feature-selection-for-machine-learning-in-python-filter-methods-6071c5d267d5">这里</a>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="f2e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参考:</p><p id="8619" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] <a class="ae ky" href="https://www.amazon.de/gp/product/1461468485/ref=as_li_tl?ie=UTF8&amp;camp=1638&amp;creative=6742&amp;creativeASIN=1461468485&amp;linkCode=as2&amp;tag=jackty-21&amp;linkId=af56407a66a11e651fd5e5ddf4933d26" rel="noopener ugc nofollow" target="_blank">应用预测建模</a>，第490页</p><p id="ed6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] <a class="ae ky" href="https://books.google.de/books/about/Feature_Engineering_and_Selection.html?id=q5alDwAAQBAJ&amp;source=kp_book_description&amp;redir_esc=y" rel="noopener ugc nofollow" target="_blank">特征工程与选择</a>，第241页</p><p id="eabb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] <a class="ae ky" href="https://xlinux.nist.gov/dads//HTML/greedyalgo.html" rel="noopener ugc nofollow" target="_blank">美国国家标准与技术研究所(NIST) </a></p></div></div>    
</body>
</html>