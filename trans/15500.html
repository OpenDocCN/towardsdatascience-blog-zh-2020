<html>
<head>
<title>How to code Logistic Regression from scratch with NumPy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用NumPy从头开始编写逻辑回归代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-code-logistic-regression-from-scratch-with-numpy-d33c46d08b7f?source=collection_archive---------25-----------------------#2020-10-25">https://towardsdatascience.com/how-to-code-logistic-regression-from-scratch-with-numpy-d33c46d08b7f?source=collection_archive---------25-----------------------#2020-10-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e46b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习逻辑回归的同时提高你的数字技能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/eb644e0a79d54323587043fc0a0ffc8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*UnP7xmGozSpOZWV91V2EEg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="fc80" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在NumPy中实现逻辑回归的计划是什么？</p><p id="6e2b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们首先考虑我们想要使用的底层数学。</p><p id="c1ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有许多方法来定义一个损失函数，然后为它找到最佳参数，其中，这里我们将在我们的<code class="fe lu lv lw lx b">LogisticRegression</code>类中实现以下3种学习参数的方法:</p><ul class=""><li id="cfa7" class="ly lz it la b lb lc le lf lh ma ll mb lp mc lt md me mf mg bi translated">我们将重写逻辑回归方程，以便将其转化为具有不同标签的最小二乘线性回归问题，然后，我们使用封闭形式的公式来寻找权重:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mh"><img src="../Images/d17359b3d2aab6a284acd57d3f766811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6-_j6UJpFoWVHk443aW8Uw.png"/></div></div></figure><ul class=""><li id="4d1a" class="ly lz it la b lb lc le lf lh ma ll mb lp mc lt md me mf mg bi translated">如上所述，我们将逻辑回归转换为最小二乘线性回归，但我们使用具有以下梯度的随机梯度下降，而不是封闭形式的公式:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mh"><img src="../Images/30b625b115149ef29ec87165a42e97f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ydKJTZyjGIAyLIB77t_98w.png"/></div></div></figure><ul class=""><li id="f38d" class="ly lz it la b lb lc le lf lh ma ll mb lp mc lt md me mf mg bi translated">我们使用最大似然估计(MLE)方法，编写似然函数，对其进行处理，将其重新表述为最小化问题，并应用具有以下梯度的SGD:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/81544ef41c1ad4ab2d5e5326a0d82d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2cjrLFiUapOg1fJvFOAEBg.png"/></div></div></figure><p id="cebb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上述等式中，X是包含行轴上的观察值和列轴上的特征的输入矩阵；y是包含分类标签(0或1)的列向量；f是误差平方和损失函数；h是最大似然法的损失函数。</p><p id="2f13" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要了解以上方法的更多信息，请查阅本文:</p><div class="mj mk gp gr ml mm"><a rel="noopener follow" target="_blank" href="/understanding-logistic-regression-81779525d5c6"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">理解逻辑回归</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">这种方法的数学详细解释</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na ks mm"/></div></div></a></div><p id="ee51" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，这是我们的目标:把上面的方程翻译成代码。为此我们将使用NumPy。</p><p id="3ace" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们计划使用面向对象的方法来实现。我们将用3个公共方法创建一个<code class="fe lu lv lw lx b">LogisticRegression</code>类:<code class="fe lu lv lw lx b">fit()</code>、<code class="fe lu lv lw lx b">predict()</code>和<code class="fe lu lv lw lx b">accuracy()</code>。</p><p id="968d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在fit的参数中，有一个将决定我们的模型如何学习。这个参数被命名为method(不要与作为类的函数的方法相混淆)，它可以将下列字符串作为值:“ols _ solve”(OLS代表普通最小二乘法)、“ols_sgd”和“mle_sgd”。</p><p id="1e8f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了不使<code class="fe lu lv lw lx b">fit()</code>方法太长，我们想将代码分成3个不同的私有方法，每个方法负责一种寻找参数的方法。</p><p id="42b6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用<code class="fe lu lv lw lx b">__ols_solve()</code>私有方法来应用封闭公式。</p><p id="ee37" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这种方法和其他使用OLS方法的方法中，我们将使用常量EPS来确保标签不完全是0或1，而是介于两者之间。这是为了避免在上面的等式中对数的正负无穷大。</p><p id="7135" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<code class="fe lu lv lw lx b">__ols_solve()</code>中，我们首先检查X是否有完整的列秩，以便我们可以应用这个方法。然后我们强制y在EPS和1-EPS之间。<code class="fe lu lv lw lx b">ols_y</code>变量保存普通最小二乘线性回归问题的标签，该问题等价于我们的逻辑回归问题。基本上，我们转换了逻辑回归的标签，使它们符合线性回归方程。之后，我们使用NumPy函数应用封闭形式的公式。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="5523" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于2个基于SGD的算法，将它们作为2个独立的方法是多余的，因为除了计算梯度的部分，它们几乎所有的代码都相同，因为它们有2个不同的梯度公式。</p><p id="3d38" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们要做的是创建一个通用的<code class="fe lu lv lw lx b">__sgd()</code>方法，它不依赖于计算梯度的特定方式。相反，它将期望一个函数作为参数，负责计算<code class="fe lu lv lw lx b">__sgd()</code>方法将使用的梯度。</p><p id="2728" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在此方法中，我们首先将权重初始化为一个随机列向量，其值取自均值为0、标准差为1/(要素数)的正态分布。这个标准差的直觉是，如果我们有更多的特征，那么我们需要更小的权重来收敛(并且不破坏我们的梯度)。然后我们检查所有的数据集，看是否有<code class="fe lu lv lw lx b">iterations</code>次。在每次这样的迭代开始时，我们随机打乱数据集，然后对于每批数据，我们计算梯度并更新权重。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="cdb5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于“ols_sgd”和“mle_sgd ”,我们将创建两个私有方法:<code class="fe lu lv lw lx b">__sse_grad()</code>和<code class="fe lu lv lw lx b">__mle_grad()</code>,它们计算并返回这两种不同技术的梯度。</p><p id="1a1f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于这两种方法，我们使用NumPy简单地应用∇f和∇h的公式。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="9c17" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，当<code class="fe lu lv lw lx b">fit()</code>用<code class="fe lu lv lw lx b">method=‘ols_solve’</code>调用时我们叫<code class="fe lu lv lw lx b">__ols_solve()</code>，当<code class="fe lu lv lw lx b">method=‘ols_sgd’</code>用<code class="fe lu lv lw lx b">grad_fn=self.__sse_grad</code>调用<code class="fe lu lv lw lx b">__sgd()</code>，当<code class="fe lu lv lw lx b">method=’mle_sgd’</code>用<code class="fe lu lv lw lx b">grad_fn=self.__mle_grad</code>调用<code class="fe lu lv lw lx b">__sgd()</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="341d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<code class="fe lu lv lw lx b">predict()</code>中，我们首先通过寻找权重属性(fit方法是创建它的唯一方法)来检查<code class="fe lu lv lw lx b">fit()</code>是否被调用过。然后我们检查输入矩阵x和权重向量的形状是否允许相乘。否则，返回错误消息。如果一切正常，我们做乘法，并通过逻辑函数传递结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="bbbd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<code class="fe lu lv lw lx b">accuracy()</code>中，我们使用上述方法进行预测。然后检查预测的形状是否与真实标签的形状相匹配，否则，我们会显示一条错误消息。之后，我们通过一个简单的规则来确保预测和真实标签的值都是0或1:如果值是&gt; = 0.5，则认为它是1，否则为0。</p><p id="41f5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了计算精度，我们检查y和y_hat之间是否相等。这将返回一个布尔值向量。然后将这些布尔值强制转换为float (False变为0.0，True变为1.0)。那么，精度就是这些值的平均值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="4eba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是<code class="fe lu lv lw lx b">LogisticRegression</code>类的完整代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="9fb7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们想用一些真实世界的数据来测试我们的<code class="fe lu lv lw lx b">LogisticRegression</code>类。为此，我们将使用来自<a class="ae nk" href="https://www.kaggle.com/ronitf/heart-disease-uci" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的心脏病数据集。你可以在Kaggle上阅读关于这个数据集的更多信息，但主要思想是根据其他数据预测“目标”列(如果健康，则为0，如果有心脏病，则为1)。</p><p id="35fd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面的代码展示了我们的<code class="fe lu lv lw lx b">LogisticRegression</code>类的作用(为了避免重复，下面没有显示单元格1&amp;2；如上面的代码片段所示)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="4c5a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如你所看到的，我们能够在训练和测试中获得相当不错的<strong class="la iu"> 80%+ </strong>的准确率。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="3423" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在<a class="ae nk" href="https://www.kaggle.com/dorianlazar/predictions-with-logistic-regression-from-scratch" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上看到完整的笔记本。</p><p id="cafb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nl">我希望这些信息对您有用，感谢您的阅读！</em></p><p id="0c52" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章也贴在我自己的网站<a class="ae nk" href="https://www.nablasquared.com/how-to-implement-logistic-regression-with-numpy/" rel="noopener ugc nofollow" target="_blank">这里</a>。随便看看吧！</p></div></div>    
</body>
</html>