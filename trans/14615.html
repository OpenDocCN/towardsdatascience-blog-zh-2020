<html>
<head>
<title>Emerging problems in machine learning: making AI “good”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中出现的问题:让人工智能变得“好”</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/emerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39?source=collection_archive---------42-----------------------#2020-10-08">https://towardsdatascience.com/emerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39?source=collection_archive---------42-----------------------#2020-10-08</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><h2 id="970c" class="it iu iv bd b dl iw ix iy iz ja jb dk jc translated" aria-label="kicker paragraph"><a class="ae ep" href="https://youtu.be/-rpd1geSNXM" rel="noopener ugc nofollow" target="_blank"> YOUTUBE </a> | <a class="ae ep" href="https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2" rel="noopener ugc nofollow" target="_blank">苹果</a> | <a class="ae ep" href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz" rel="noopener ugc nofollow" target="_blank">谷歌</a> | <a class="ae ep" href="https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU" rel="noopener ugc nofollow" target="_blank"> SPOTIFY </a> | <a class="ae ep" href="https://anchor.fm/towardsdatascience" rel="noopener ugc nofollow" target="_blank">其他</a></h2><div class=""/><div class=""><h2 id="0740" class="pw-subtitle-paragraph kb je iv bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">爱德华·哈里斯在<a class="ae kt" rel="noopener" target="_blank" href="https://towardsdatascience.com/podcast/home"> TDS播客</a></h2></div><figure class="ku kv kw kx gt ky"><div class="bz fp l di"><div class="kz la l"/></div></figure><p id="2223" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated"><em class="lx">编者按:这一集是我们关于数据科学和机器学习新兴问题的播客系列的一部分</em>，<em class="lx">由Jeremie Harris主持。除了主持播客，Jeremie还帮助运营一家名为</em><a class="ae kt" href="http://sharpestminds.com" rel="noopener ugc nofollow" target="_blank"><em class="lx">sharpes minds</em></a><em class="lx">的数据科学导师初创公司。你可以听下面的播客:</em></p><figure class="ku kv kw kx gt ky"><div class="bz fp l di"><div class="ly la l"/></div></figure><p id="4e0c" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我认为很容易忽视机器学习发展的程度和速度。</p><p id="fc7d" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">就在10年前，人们还在说我们正处于人工智能的冬天，深度学习是一个边缘且在很大程度上被忽视的研究领域。从那时起，我们见证了人类历史上最重要的技术革命之一。强化学习允许机器玩游戏，让机器人在现实世界中确定自己的方向，卷积神经网络让计算机比人看得更清楚，变压器架构正在生成大量文本，专家无法将这些文本与人类的书写区分开来。</p><p id="1d31" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这些新工具让我们能够以我们从未想象过的方式来衡量个人的影响力。一个数据科学家可以设计出一种算法，在部署后的几分钟内，它将立即影响数百万甚至数十亿人的生活。</p><p id="4909" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">但是增加单个开发者或单个算法的影响不一定是一件好事。开发人员可能会犯错误，公司可能会有恶意，数据可能会被错误地采样，最终这些问题和许多其他问题可能会导致算法弊大于利。当伤害扩大到数百万或数十亿人时，有可能对人类造成前所未有的巨大伤害。</p><p id="0146" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">虽然我们的技术确实变得更好了，但我们是否已经变得足够聪明来运用它还不太清楚。机器学习社区中的许多人越来越多地意识到，如果我们要能够正确地运用我们的新权力，我们需要大大改善几千年的道德哲学、伦理学甚至形而上学。</p><p id="5d61" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">但在每个人都在不断推出下一个算法、下一个数据集或下一个大架构的背景下，这很难做到。所以我认为至少我们中的一些人花些时间停下来，环顾四周，问一些非常基本的问题是值得的。这一切将走向何方？我们希望我们的技术引领我们走向何方？我们是如何达不到这个目标的？先进的人工智能系统在未来可能会给我们带来什么风险，它们有什么潜力？构建符合人类价值观的道德、安全、可解释和可问责的人工智能意味着什么？</p><p id="f6ae" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这就是今年“走向数据科学”播客的主题。我希望你加入我们的旅程，今天开始采访我的兄弟埃德，他除了作为一个小团队的一部分与我一起工作以建立<a class="ae kt" href="http://sharpestminds.com" rel="noopener ugc nofollow" target="_blank">sharpes minds</a>数据科学导师计划之外，还与我在许多人工智能安全、校准和政策项目上合作。我认为他会是播客新年的最佳嘉宾。</p><p id="a6a7" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我希望你喜欢这一集。</p><p id="997f" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated"><strong class="ld jf">剧集中提到的链接:</strong></p><ul class=""><li id="31bf" class="lz ma iv ld b le lf lh li lk mb lo mc ls md lw me mf mg mh bi translated">如果你有兴趣了解更多关于<a class="ae kt" href="https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/" rel="noopener ugc nofollow" target="_blank">人工智能联盟</a>或<a class="ae kt" href="https://80000hours.org/articles/ai-policy-guide/" rel="noopener ugc nofollow" target="_blank">人工智能政策</a>的职业，80，000小时是一个很好的资源。你可能也有兴趣听听他们与OpenAI的Paul Christiano 的<a class="ae kt" href="https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/" rel="noopener ugc nofollow" target="_blank">惊人播客，其中涵盖了人工智能对齐。</a></li><li id="0db0" class="lz ma iv ld b le mi lh mj lk mk lo ml ls mm lw me mf mg mh bi translated">如果你对人工智能政策感兴趣，你可能想看看GovAI正在做的工作。</li><li id="d85a" class="lz ma iv ld b le mi lh mj lk mk lo ml ls mm lw me mf mg mh bi translated"><a class="ae kt" href="https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence" rel="noopener ugc nofollow" target="_blank">开放慈善项目</a>为有特定项目或研究领域想要探索的人提供资助，这些项目或研究领域与AI安全有关。</li></ul><p id="9e7e" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">你可以<a class="ae kt" href="https://twitter.com/neutronsNeurons" rel="noopener ugc nofollow" target="_blank">在这里</a>的推特上关注艾德，你也可以<a class="ae kt" href="https://twitter.com/jeremiecharris" rel="noopener ugc nofollow" target="_blank">在这里</a>的推特上关注我。</p></div><div class="ab cl mn mo hz mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="io ip iq ir is"><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/e4dc215d6f35f7741c7136b07a27b4f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_o6kgLxZmE8pTg-2cwfkdQ.jpeg"/></div></div></figure><h2 id="4c48" class="nb nc iv bd nd ne nf dn ng nh ni dp nj lk nk nl nm lo nn no np ls nq nr ns jb bi translated">副本</h2><p id="55ab" class="pw-post-body-paragraph lb lc iv ld b le nt kf lg lh nu ki lj lk nv lm ln lo nw lq lr ls nx lu lv lw io bi translated"><br/>嘿，大家好。欢迎回到播客。我希望你一切都好。我的名字当然是杰米。我是“走向数据科学”播客的主持人，也是SharpestMinds数据科学导师计划团队的成员。事实上，上周我们暂停了一下，休息了一下。我们没有发布一集。我想先解释一下我们为什么要暂停，以及我们今后会做些什么。</p><p id="395a" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">首先，暂停的原因是我们正在制作下一季的剧集。这几集将更多地关注围绕我们如何部署人工智能系统以造福人类的问题。随着我们的人工智能系统变得越来越强大，开始问一些问题变得越来越重要，比如我们是否应该实际部署一个人工智能系统来解决这个特殊的问题，或者如果我们开始在这个子领域部署越来越强大的人工智能系统，会出现什么问题？</p><p id="8d74" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>这些都是围绕安全的各种问题，在短期内，我们开始问这样的问题，比如人工智能系统会不会意外地建议一系列可能导致人类伤害或经济损失的行动？一直到更基本的，也许更涉及长期的问题，如RAI系统实际上反映了人类的价值观，将人类塑造成我们想要的形式，我们的AI系统从更存在主义的角度来看实际上是安全的。</p><p id="b806" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">一个任意智能的系统是否有可能对人类自身的持续存在产生影响，这是我们应该尽快考虑的问题。因此，这将是未来围绕道德、人工智能系统中的偏见以及我们是否应该围绕分发部署提出的许多问题的焦点。所以我真的很兴奋能投入到这整个系列中。我们有这么多伟大的嘉宾排队，很难选择一个开始，我最终坐下来，思考我知道谁最适合谈论所有这些话题，不仅仅是人工智能伦理，不仅仅是人工智能偏见，不仅仅是人工智能对齐的技术细节，而是与我一起涵盖所有这些基础的人。</p><p id="62d0" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰瑞米:<br/>我找到了我哥哥埃德，他以前上过播客。他和我是SharpestMinds的联合创始人，他实际上在技术比对工作以及创业工作方面有着丰富的经验，看到了在野外部署的算法，由于他在人工智能安全领域的经验广度和专注深度，我真的认为他是我打开局面的最佳人选。所以我希望你喜欢这次谈话。这将会比我们过去经历的一些更不寻常。但是如果你有任何反馈，请让我知道，我迫不及待地想发布接下来的几集，我们正在这里探索一个真正令人兴奋的主题，与社区分享这些想法并获得你对我们如何做的反馈将是非常棒的。所以事不宜迟，请欣赏这场表演。</p><p id="185a" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">嗨，艾德。感谢你参加我的播客。</p><p id="977b" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>非常感谢你邀请我。我很感激。</p><p id="aa27" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:<br/>哦，是的。完全没问题。天啊，我想我们可以从很多地方入手。人工智能是一个非常大的空间，围绕人工智能应该如何使用，如何保护它的问题开始出现。显然，这是整个系列播客的主题，也是我们接下来几集的重点。我想，也许一开始，我们可以谈论许多不同的事情，但你更关注人工智能对齐的想法，让机器学习模型以不同的形式做我们希望它们做的事情，确保它们的性能本质上对人类没有危险，并且它朝着我们希望的方向优化。我们可以介绍一下这个想法吗，人工智能对齐领域，比如为什么我们应该花费时间和精力来对齐我们的机器学习算法？</p><p id="afcb" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>是啊。这是我最近一直在思考的事情，因为系统变得越来越强大，我们看到了年复一年的进步。系统变得越强大，它们做我们想要的事情就越重要，一致性很重要，因为并不总是很明显系统会做我们想要的事情，也不总是很明显，即使系统开始做看起来像我们想要的事情，它也不会最终做一些非常糟糕的事情。</p><p id="92e2" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>社交媒体订阅就是一个很好的例子。算法让你回到Twitter和所有这些东西。也许你会想，Twitter公司想从你这里得到什么？这是钱，但它是相当良性的，对不对？他们只是想让你多点击，多点赞。在第一个层面上，你想到的是算法会给你更多点击的帖子和推文，所以你会留在那里，看到更多的广告。那是第一关。</p><p id="7247" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>但问题是，如果你训练一个通用系统来做这件事，系统会发现的一件事是，“嘿，我实际上通过对我展示这些推文的人进行测试，我可以发现我可以通过展示某些类型的内容来让人们变得更可预测。”这些算法让我们变得更容易被他们预测的方式之一是内容的政治化。如果我更政治化，我的派系比我的政治立场处于中心时更容易预测。</p><p id="0e30" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>所以实际上正在发生的事情之一是，这些算法纯粹通过试图赚更多钱的行为，实际上将人们推向所有这些不同种类的光谱、不同政治问题和所有这些问题的不同端，纯粹是因为他们喜欢，“嘿，我试图让你成为对我来说更容易预测的东西，而政治上更极端的人更容易预测，因为他们往往有很多相关的观点。”</p><p id="4d44" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>所以这是其中一种方式，实际上很可怕，你可以看到它是如何爬上你的身体的，因为这已经发生了很多年，但是你可以想象这已经对世界产生了影响，随着这些系统变得越来越聪明，我们应该期待更多类似的事情开始发生。</p><p id="d5b6" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:<br/>好的，好的。所以这里的主要问题是，我们的算法有时太聪明了，如果我们不仔细思考我们希望它们为我们创造什么样的世界，它们会找到我们从未想象过的解决方案，或者如果问题是预测我会点击什么，这样我就可以生成更吸引人的内容， 然后，真正改变我的用户心态，使他们更可预测，把政治光谱变成更多的二元，所以这是一个更小的维度问题，更容易降维。 变成了它自己的病理，对吗？我是说，这就是问题的核心，对吧？</p><p id="7a12" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>是啊。这种事情会再次出现在你身上。你可以给一个系统一套指令，你认为这些指令完全是良性的，对你来说完全有意义。好吧。相当良性。只有在事后才意识到，我们可以回顾过去，说，“哦，我的上帝。如果我们当时知道这是我们向世界释放的力量，我们可能会采取不同的做法。”但在当时，不可能预测这将是一个结果。</p><p id="27b4" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>问题是，当你在处理比你聪明的东西时，根据定义，你无法预测它会做什么，因为它比你聪明。如果你比它聪明，你就能预测它，但是因为它比你聪明，它能预测你，而不是相反。</p><p id="ad58" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我想这并不一定…我的意思是，我认为这是当你谈论人工智能时的一个问题，什么是智能？智力是什么意思？我认为这些术语定义不清。从某种意义上来说，很难说Twitter算法比人聪明。其他感官狭义地解释，你可以说它比人更聪明，但肯定有一个阈值，两者开始互动和竞争，一个算法开始超过另一个。Twitter算法开始超过你的人脑，并开始改变你，而不是你改变它，对不对？</p><p id="d462" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>对。我会说，算法目前在狭义上可能比我们更聪明，但我们应该担心它们可能在更普遍的方面变得比我们更聪明。当然，在达到这一点之前，还有其他事情需要关注。</p><p id="5b00" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰瑞米:<br/>好的。所以，是的。因为我认为我们会谈到对一般智能的普遍关注，以及这些越来越先进的系统会走向何方。短期来看，你提到了Twitter。显然，很多人都谈论过人工智能系统中的人工智能伦理和人工智能偏见等问题，对吗？我认为这是今天真正的主题，因为它就在我们身边。我们可以看到系统运行的方式会有让我们吃惊的偏差。在许多情况下，它们似乎反映了世界当前的方式及其当前的故障、故障模式，并且它将倾向于强化这些故障模式，因为它已经根据这些数据进行了训练，并且它将做出反映这些数据的预测。我想你更关心事情的长期方面，但这也可能是长期的问题吗？</p><p id="76cc" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Ed: <br/>某种程度上。因此，关于人工智能中的故障模式和偏见的一个问题是，在某种程度上来说，这是一种正在发生的事情，因为这些系统有点太愚蠢了，它们没有得到足够普遍的训练。当系统过于智能时，未来的风险会越大，最终可能会更大。但是偏见的风险在今天绝对是真实的，而且是当前的。</p><p id="37fd" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>这些偏见风险是你基本上得到的一种涡轮增压版本，当人们构建软件时，他们通常会为自己构建，他们会测试点击量和他们自然期望其他人做的事情。每个人都是以自我为中心的，对吗？所以我们基本上都是为自己构建的，这完全没问题，但当你收集的数据以特定方式出现偏差时，有时会发生什么，就好像这不是算法的错一样。</p><p id="8cee" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>算法只是在给定的数据上运行，它可以完全忘记，“哦，有一群人的名字不是用拉丁字符写的，或者没有特定的肤色或其他什么。”所以你实际上会遇到这样的情况，算法在处理开发者没有想到的情况时非常糟糕，我们给这些系统的能力越大，这些错误的代价就越大。所以我认为这就是人工智能中偏见的现状。</p><p id="e00b" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>对。我的意思是，人们经常谈论数据集偏差。这引起了很多关注。我认为另一个非常重要的方面是特征选择偏差，因为如果你想一想，当我们在环境中导航时，人类会收集关于世界其他地方的特征。这些特征通常是声波之类的东西。它们就像气味一样。它们就像视觉和触觉。这些是进化为我们设计的特征。因为我们通过那个镜头看世界，我们无法注意到某些事情。举例来说，这些东西就像一直在我们身体中穿梭的中微子，我们对它们完全不敏感，但它们却占了我们在任何给定时刻所沐浴的能量的很大一部分，或者说是不可忽略的一部分。</p><p id="91ad" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>所以我认为可以毫不夸张地说，我们的环境大部分是看不见的，甚至是我们原则上可以看见的东西，我们如此短视地专注于我们视野中的一小部分，以至于我们没有吸收我们周围绝大多数的信息。说到我们的机器，我想我们也在做类似的事情。我们选择特征。</p><p id="ee2c" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>例如，如果你要告诉一个信用卡定价算法，比如一个人的姓名、年龄、职业和肤色，你就让它通过某个镜头来看待这个世界，并使它比其他人更容易找到某些相关性，或将复杂性和细微差别纳入一个高级核心应变特征。你认为这也是一个问题吗？还是你认为数据集偏差是一个更大的问题？</p><p id="1178" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>所以我要说的是，特征选择会产生两种偏见。第一个是遗漏偏差，你没有给系统一个有用的特征。你给了系统一个关于结论的非信息特征。第二种更像是标签偏见。因此，如果你给一个系统一个人的肤色来进行信用卡评估，如果你真的在真实的信息数据集上训练这个系统，理论上，肤色不应该成为问题。</p><p id="f951" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>如果你有一个运行在那个系统上的完美的普通智能，它将会忽略肤色，以至于肤色需要被忽略。但问题是，当你在标签上训练这个系统时，这些标签是由那些本身有偏见的人贴上的，我们都有偏见，那么偏见本身就会被诱导到这个系统上。举例来说，这是人们在潜在的算法给出判决时遇到的事情。</p><p id="608e" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>如果结果是之前看到的判决是由…有一个重要的判决样本对谁知道，人的身高、肤色或头发颜色或其他任何东西有偏见，那么这个系统又是垃圾进来，垃圾出去。系统会学习你教给它的东西。所以它会产生相同的偏差。</p><p id="8e1e" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰瑞米:<br/>所以我想这很有趣。我的意思是，我想我看到的偏见数据的情况是其中的一部分。我不认为是这样的……我的意思是，你谈到了一个足够普遍的智能程序，它会查看包括肤色等信息，然后得出正确的结论。我认为事实并非如此。我认为，如果你……除非那个系统能够获得更全面的特征，否则它会倾向于通过特定的镜头短视地看待事物，有时这个镜头，即使数据集没有偏见，即使它检测到世界上正在发生的事情，如果我们谈论肤色或我们谈论性别或其他，肤色和偿还债务的倾向或其他之间存在关联，如果你愿意，这些会反映在设计糟糕的系统的偏见中。</p><p id="fc46" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>但是表面下真正发生的是，如果你把它分开，如果你突然控制其他变量，我们预计……我不知道有没有人真的做到了这一点，但是我个人非常期待肤色会从那个等式中消失，但是只有当你控制这些其他变量时。因此，我认为，只有在一定程度上，你添加到该功能集，你才能真正以这种方式消除系统偏差。</p><p id="ae1a" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated"><br/>是的，这是正确的。我认为这是从另一个角度来看待我所说的，他们的疏忽所造成的罪恶。所以，如果你给了系统足够的基础数据来分解这些变量，最终使这些性别和混杂特征变得不相关，我想，如果你给了系统足够的数据，它会知道这些变量是不相关的。但是，是的，如果你忽略了这些，他们实际上会学习变量，结果会表现出偏差。</p><p id="4b13" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:是的。我认为这一切对我们的自由意志的概念或自由意志的幻想的挑战程度也总是很有趣，因为我也可以想象如果你把这个过程发挥到极限，你继续细化，增加更多的特征，增加更多的特征，最终你的模型变得如此复杂。它可以解释，我的意思是，几乎就像在荒谬的极限中，你大脑中每个神经元的放电模式，然后你可以以超高的确定性预测你的行为，你所有的代理都消失了。我的意思是，随着系统变得越来越复杂，我认为这实际上是我们将要经历的旅程中有趣的一部分。</p><p id="6927" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>我认为这在某种程度上是对的，但这是有限度的。所以这和预测天气是一样的。系统有…即使你没有想到，“哦，在一切之上有量子力学的不确定性，”即使你没有想到这一点，有混沌特性的经典系统有…你不能真正预测它们超过一定数量的时间步骤。它的工作方式是，我想如果我没记错的话，它有点像你的…然而，你开始时的精确度，你后来的精确度下降了一倍，就像时间的平方根。</p><p id="b62a" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>所以基本上在一段时间后，即使你有一个关于事情将如何发生的完全确定的模型，如果你开始时的测量精度基本上随着时间的推移而传播并变得越来越差。因此，非常智能的系统预测未来的能力总是有限的。至少这是我们目前最好的理解。但是你不需要能够预测每一个原子的运动，就能够做很多非常有用的事情。那可能很危险。</p><p id="de4d" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:<br/>是的。我认为实际上…也许补充说明是，在英国已经有一个关于政府试图…而不是政府的故事。对不起，我认为一所大学试图预测学生的考试成绩，因为他们无论出于什么原因都无法预测，我认为这可能与COVID有关。他们实际上不能写测试，对吗？</p><p id="2728" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>所以你可能会说，有些学生对他们的代理机构被剥夺感到不安，这是理所当然的。在这种情况下，算法似乎是非执行性的。它像现在的机器学习模型一样夸大了，但它夸大了它的准确性或淡化了它的不确定性。然而，我认为有一个有趣的问题，“好吧。如果不是这样呢？如果算法是超级执行的，我们会在哪里？”</p><p id="b843" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Ed: <br/>如果你以某种方式去思考，这个测试本身就是对你自己的一个不确定的评价。这是对你自己技能的嘈杂评价，所以如果你在考试中表现不错，你应该更开心。你已经得到了额外的奖励，而如果你只是过了糟糕的一天，睡不好觉，不管是什么，某种混合的东西，你也许你应该理所当然地愤怒，这个测试没有正确地评估你的水平。</p><p id="d657" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Ed: <br/>我认为，如果你有一个已知不确定性的完全无偏的算法，最终这可能更多是一个语义和舒适区的问题，而不是一个真正的问题。所以如果你的算法是无偏的并且校准良好。因为如果它是无偏的，并且校准得很好，那么我至少可以看着我的测试分数或我的预测测试分数，然后说，“嗯，有50%的可能性我应该为此生气，有50%的可能性我应该为此高兴。”另一方面，让计算机来判断我们的未来，我们都会有一种天生的不适。所以我不知道我们作为一个文明会不会克服这一点。</p><p id="401e" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:<br/>对，我想这开始引出了一些我认为更具前瞻性的话题。所以当我们继续开发这项技术时，你之前提到过。这些系统变得越强大，我们就越有必要知道自己到底想要什么。这种能够建立确切知道我们想要什么的系统的想法，能够与他们交流，能够确保他们确实在做我们要求他们做的事情，这就是所谓的人工智能对齐。我想问你，你能不能用自己的话描述一下人工智能的排列，然后你能不能描述一下外部排列和内部排列的问题，这样人们会更熟悉一些。</p><p id="d707" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>对。所以粗略地说，人工智能对齐的问题是让人工智能做我们真正希望它做的事情。描述中有很多细节，甚至还有很大一部分对齐问题没有解决，那就是计算描述中还有多少细节需要计算。但是现在人们倾向于像你说的那样，把排列分成两个子问题。有外部排列和内部排列。</p><p id="afbb" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>所以第一个问题是让系统，让这个大人工智能真正尝试做我们想让它做的事情。你可以这样想，我找到了一盏神灯，我摩擦它，一个精灵就出来了。这不是阿拉丁的精灵。这不像是一个很好的精灵想要帮忙什么的。这是一个精灵，它会尽可能地曲解你的愿望。精灵也比你聪明得多。</p><p id="546a" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>所以你要做的是，你要想办法说出你的愿望，这样你就可以绝对地控制住那个精灵，这样他们就绝对不能以任何有意义的方式曲解你的愿望，他们会说，“啊，我想我要做这件事，因为你让我别无选择。”</p><p id="81a7" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>这很有意思，因为你立即把它框定为一件敌对的事情，我认为这本身就很有意思，但这意味着这台机器、这个人工智能或这个精灵试图曲解我们所说的话，事实并非如此，但它确实看起来至少像是在这个领域与你一起工作过很多东西的人，它确实看起来好像在实践中你必须…这几乎像是防御性驾驶。你必须假设，因为这个东西是如此强大，比你聪明得多，如果你正在构建超级强大，超级智能的人工智能系统，它会找到解决你提出的任何问题的解决方案，这将是非常聪明的，这将涉及到你甚至无法想象的事情。这就是这种敌对暗示的来源吗？</p><p id="ec40" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>在某种程度上，是的。我认为最简单的说法是，曲解一个愿望的方法比正确解释一个愿望的方法多得多，你越聪明，就能想出更多的方法来曲解这个愿望。所以你面对的精灵能够想出很多方法来曲解你的愿望，而你会说，“不，不，不。我希望你在其中一个方面正确地解释它。”</p><p id="680d" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>所以我可以给你一个愿望的例子，实际上可能有助于冲洗出来。</p><p id="0fed" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated"><br/>是的，是的。比方说，我希望…一些简单的事情，比如我希望快乐或者类似的事情。好吧。你希望快乐。如果你面对的是一个真正想帮忙的人，他有一个人的所有限制，他们会问你你想要什么。我会给你煮一杯咖啡，只要能让你开心就好。但是如果你面对一个超级聪明的精灵，精灵首先会说，“好吧。那么，你怎么定义幸福呢？这里有很多模糊之处。是不是我一直都喜欢笑？”</p><p id="eb58" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>好吧，如果这就是精灵对幸福的看法，它会抓住我的脸，让我永远微笑。很好，它完成了它的工作，但我想，“不，不，不。不要这样做。请停止。”但关键是它不听。它不想听。我已经说过了，让我开心。它认为让我开心意味着让我微笑和更多-</p><p id="8006" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">或者任何数量的其他东西，对吗？</p><p id="220d" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>或者任何数量的东西。</p><p id="cb61" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰瑞米:<br/>可能是让你兴奋的药物，可能是任何……是的。</p><p id="1e45" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">是的。然后我会给你注射可卡因，或者我会钻进你的头骨，把你的大脑挖出来，然后把你扔进一大桶内啡肽里。这样就行了。但问题是，一旦你给了精灵第一个指令，从这一点来看，它也有动机阻止这个指令的改变，因为它会…它不想…一旦它有了这个指令，它就有了这个目标，它会尽一切努力来实现这个目标。</p><p id="d84f" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Ed: <br/>降低目标实现几率的一个因素是，如果精灵自己的目标后来被你改变了。所以如果你能告诉精灵，“哦，实际上，不，不，不。不要做那个笑脸的事情。那真的很吓人。请不要那样做。”这就减少了你开心地微笑的机会。</p><p id="c2b9" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>所以精灵，在你告诉它之前停止微笑，但在你告诉它让我微笑之后，就会像这样，“好吧，现在我已经得到了这个指令，我必须防止我的指令被改变，保护我自己免受任何试图改变它的企图，包括刚才让我做这件事的那个人。”所以我最好强迫这家伙闭嘴，或者干脆杀了他，让他笑一笑。开心或者类似的疯狂的事情。这就是我们正在谈论的误解的程度，这就是为什么它是危险的。</p><p id="c023" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>这让我想起了早期的……或者说不是早期。我和电脑的第一次互动，对吧？我想每个人都经历过这一点，当你第一次开始编码时，你会意识到，“伙计，这台机器完全是在带着我。我告诉它做点什么，然后……”我是说，我记得有过这种挫败感。你写一些你认为应该工作的代码。你认为它应该工作，然后你点击运行，然后它中断，并有一个错误信息。你是做什么的？你感到沮丧的不是你自己，而是机器，因为你的大脑在告诉你，“哦，机器搞砸了。我知道我的意思，但机器误解了它。”</p><p id="6f7c" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>我认为，当人们看到人工智能并开始认为它会是默认的积极结果时，也会犯类似的错误。人们在想象的时候会做很多拟人化的事情，比如，“哦，但是它会明白，我不是指用这种可怕的方式来实现我刚刚要求它做的事情。”或者我不会有一个精灵把我从字面上是假设。无论如何，我觉得这确实反映了我开始学习编程时的态度。这是处理这些问题的一种完全可以理解的方式。</p><p id="fec6" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">是的。机器本质上没有同情心，除非我们给它同情，而我们对同情的理解还不足以给机器同情。这是问题的一部分，但这是问题的一部分。</p><p id="b7d0" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">好的，好的。因此，我们已经确定，为了让这个高级人工智能做我们想做的事情，训练它并不是一件小事。我是说，那是另一回事，对吧？当你告诉一台机器我想让他们快乐时，有太多的事情需要考虑。我的意思是，几千年来，哲学家们一直试图弄清楚幸福意味着什么，但没有成功，现在，我们将试图量化这一点，完成不确定性，并将其输入机器。</p><p id="0dfb" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>所以我想这暗示了问题的另一部分。但是假设我们已经过了那个阶段，这就是你所说的，你提到的外部对准问题。那么下一层困难是什么呢？接下来是什么？</p><p id="b978" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>是啊。下一个问题是所谓的内部排列问题。这更像是你所说的精灵可能由许多内部组件组成，而这些内部组件可能会违背精灵的利益。这很难想象，但也许一个思考拟人化的好方法是，你想到一家公司。比方说，你向一家公司发出请求，而不是一个精灵。让我开心，公司会向你收钱，让你开心，诸如此类。</p><p id="a88b" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>所以这家公司很可能有一种病态的倾向，想让你开心什么的，就像Twitter一样。我的意思是，我爱Twitter，但它确实有这个算法问题。但公司有趣的地方在于，公司是由部分与公司目标一致的人组成的。人们一起工作来完成比公司里任何一个人都要多的事情。但是他们并不是都划向同一个方向。</p><p id="a1a6" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>公司内部有些人比其他人更团结。有些人非常相信公司的使命，他们会投入所有需要的时间。他们在同一个方向上雄心勃勃。其他人有各种不同的动机。他们只想在下午5点回家看他们的孩子。这是一个动机。那很好。还有其他的动机，比如有些人只是为了他们自己的野心而工作，他们自己想要晋升的野心。他们对公司自身的成功或结果不感兴趣。而其他人就像是在吃白食。他们真的在犯罪，盗用公司的资金，向黑客出售Twitter账户，或者类似的疯狂行为。</p><p id="2d4d" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">像这样的人将会是非常非常少的一部分。和整个公司本身，该公司的目标是让他们都保持一致，做同样的事情。但是当我们把这个框架移植到人工智能上时，实际上并不清楚移植到什么程度。尚不清楚如何让人工智能将其内部组件排列在一起。</p><p id="ed6c" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Ed: <br/>其实也是一样的问题，基本上就是外对齐问题。这就好像精灵本身包含了一群试图解决子问题的小精灵，但这些小精灵可能会反对主精灵，而主精灵会仔细思考它给小精灵的指令。</p><p id="79c3" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>那么有哪些可能导致这些……顺便说一下，这些小精灵形成的子问题的例子，对吗？我的意思是，在人工智能排列的语言中，这些是台面优化器。他们通常都是这么称呼的。台面优化器有点像优化器中的优化器。所以你有了这个机器学习算法，然后在它里面有一大堆子问题需要解决，以使整个算法工作。</p><p id="5646" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>如果你在做计算机视觉，对吗？计算机视觉整体问题的一部分是，例如，我不知道，识别棱角，对吗？因此，你可能有一个小台面优化器，一个专门研究边缘，另一个专门研究角落，我猜它们在某种程度上可以有自己的生命。至少这是个问题。在某种程度上，他们这样做，他们想以同样的方式保护自己…我猜这就是你所说的外部排列，对吗？以同样的方式作为外部校准代理，不想被重新编程，它想保留其最初的目的。</p><p id="e9e5" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰瑞米:<br/>这些台面优化者不想……一旦他们抓住了他们的子问题，他们就不想改变那个子问题。所以他们拼命想保留自己的结构，对吗？</p><p id="0292" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">是的。举个例子，如果你回到精灵，把你的大脑放在一个装有内啡肽的大桶里，精灵，你给它一个指令，比如也许快乐，精灵就像，“哦，我要把他的大脑放在一个装有内啡肽的大桶里。这就是我要做的。”然后它所做的是分配给它的一个子组件，子问题，“好的。为了实现这一目标，我们必须弄清楚许多关于人类神经化学的东西。所以继续努力吧，我们还有一堆其他的子问题。”</p><p id="ccc1" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">那么也许研究人类神经化学的东西有某种形式的思维过程，“哦，哇，伙计，这是个难题。为了解决这个子问题，我需要更多的计算机，比如比我目前拥有的计算能力更强的计算机。”好的，我将把世界上和太阳系和银河系中的许多原子转换成更多的计算机，以确保我真的真的很好地解决了这个解决人类神经化学的子问题，这样我就可以把这个解决方案向上传递给解决大问题的主优化器。</p><p id="b4e4" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Ed: <br/>哦，就像大的优化器在这里使用计算机，但是我想要那些计算机来解决我的子问题，所以你得到了这种战斗发生。实际上，你可以感觉到类似的事情在你的脑海中发生。所以如果你决定，“嘿，我想吃那边的饼干吗？”这并不像你一生都有一个连贯的损失函数，你试图计算它的期望值。取而代之的是，感觉在你的头脑中有一个完全独立的东西致力于解决一个子问题。</p><p id="8ba1" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>所以你的大脑中有饥饿模块在与你争夺控制权，饥饿模块就像是，是的，去吃饼干，但你有一个更高层次的过程，就像是，“哦，等等。这和我不变胖的人生目标或者你的人生目标有什么关系？”感觉你在和自己的一部分抗争。</p><p id="ca0d" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>对，对。嗯，引入这些优化器后，我们可以做很多事情，但我认为有一件事值得暂停一下，首先，这听起来有点疯狂，这是可以理解的，对吧？我的意思是，我们真的在谈论某种程度上想要接管世界的机器。我认为值得指出的是，至少在某些运营模式下，他们会倾向于…如果他们足够强大，有足够的计算能力，有足够的数据访问权，最终会有这样的野心。</p><p id="9a0d" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰瑞米:<br/>这听起来确实很疯狂，但这是一个非常非常好的既定目标……这就是在这种阶段机器学习模型的工具性目标。这至少是非常严肃的研究人员关注的一个问题。我的意思是，这在联盟社区被认为是一个非常严重的问题。我们能谈谈工具性目标的概念吗？什么是工具性目标？你能定义一下吗？</p><p id="c781" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">是的。所以你之前说的，基本上，我们认为所有足够聪明的系统或者绝大多数足够聪明的系统都会想要，以某种方式，接管世界，这听起来是一个非常极端的说法。原因就在于这种工具性目标和工具性融合的思想。这个想法是，无论你的目标是什么，都有一些特定的资源和特定的行动是值得采取和抓住的。</p><p id="2d3b" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>所以如果你的目标是把我的大脑放进一个大缸里，你看到了，“哦，次优化程序想要更多的计算机来确保它绝对解决问题并把它做好。”那是它的一部分。为了解决一个难题，你需要大量的计算机，当你的能力、你能做的事情没有真正的限制时，你没有理由不把你能得到的整个地球的每一个质量粒子转换成一台计算机。再说一次，在这种东西的概念空间里，没有接近同情的东西。</p><p id="8dac" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:<br/>你当然可以说这是人类正在尝试做的事情。我的意思是，我们正在尽可能多地将世界转化为计算资源，然后我们部署这些资源来解决我们自己的工具性目标。</p><p id="78ec" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>对。即使这样……我们的能力也是有限的，即使我们同情我们正在杀害的动物，我们正在破坏的树木和环境。不是所有人都这样，但是足够多的人这样做了，我们就像，“嘿，伙计们。我们需要自然保护区、国家公园，诸如此类的东西。”这是因为，我们在某种程度上足够重视这些东西，以至于我们不愿意为了得到它们而彻底毁灭这个星球。</p><p id="2dd4" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>但是如果我们的目标函数不同，我们甚至不会在乎那么多。是啊，谁在乎雨林呢？谁在乎这个那个。这也是我们最终要面对的问题。想象一下非洲狮或其他动物面临的困境，这种程度的破坏甚至比我们已经强加给狮子的破坏程度还要严重。无法理解。毫无同情心，完全机械，极具破坏性。</p><p id="e964" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>我的意思是，这是一个关于如何实现回形针优化的著名例子。这个设备被告知，“嘿，做回形针。”没有任何其他背景，它说，“哦，酷。地下有一些铁。你的血液里有些铁。这里有些铁，那里有些铁。它只是从各个地方收集铁，而你制造回形针破坏了这个世界。</p><p id="3571" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">酷。所以我想最后一点，我想，关于工具性融合的想法，有一些工具性目标或许多不同的工具性目标，许多人认为是合理的，机器倾向于优化这些目标，作为优化主要目标函数的副作用。你能列举几个吗？我知道你提到了其中的一些，但也许只是为了让人们能看到一些。</p><p id="d2f7" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>对。从根本上来说，基于我们今天所知道的物理定律，物质和自由能，你想要东西，你想要果汁来运行它。所以换句话说，把物质放在一起建造大量的计算机，这样你就可以尽可能快和有效地思考，并获取你实际运行这些计算机所需的能源。还有其他工具性的目标，比如生存。</p><p id="c6ea" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>一般来说，我可以预测，如果我活下来，我的目标比我死了更有可能实现。大多数情况下。如果我有一套特定的目标，比如如果我继续追求这些目标，这些目标更有可能实现。所以自我保护是这些工具性子目标之一，目标凝聚力或目标保护也是一个工具性子目标。</p><p id="1d38" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Ed: <br/>如果我还活着，但我脑中的目标发生了变化，那么我之前的目标也不太可能实现，因为在我的目标发生变化后，我将不再为之努力。所以我要设法阻止我的目标被改变。</p><p id="2c8b" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>至少如我们所料，我们基本上只有一次机会来设定正确的目标，因为如果我们像这样，“哦，不，不，不，等等，等等，”那么大概系统会像这样，“哦，不，等等，等等。我明白了。我明白了。我已经有了这个目标，我会去完成它。”</p><p id="cbbb" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我认为这在某种程度上反映了…如果我错了，请纠正我。也许你倾向于人工智能结盟辩论中的一个阵营。所以有一个阵营说，“伙计们，我们只有一次机会。我们必须确保我们的目标是正确的。”还有另一个阵营说，“伙计们，我们应该专注于健壮性制作模型，考虑到他们追求的目标的不确定性，并努力做到可修正。”换句话说，在这个意义上，邀请修正和健壮。我只是想指出两者之间的区别，因为它是生态系统的两个分支。这么说公平吗？</p><p id="880e" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">是的。在这些领域，争论开始变得有点技术性。我会说，这两个方向都是可行的，当然，在我们制造出某种超级邪恶的精灵来摧毁世界之前，可能会有一些阶段，我们肯定希望更多地处于学习模式，而不是“啊，我得到了这个完全正确的模式”，这样我们就可以实际上看起来像，“好吧，什么样的系统是相当智能的，也许是危险的，但不一定是瞬间毁灭的？给了他们一套指令后，他们会做什么？有没有什么方法可以让他们接受扳动开关让他们停下来？诸如此类的事情。</p><p id="2f1c" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>爽。好吧。因此，在奠定了这一基础之后，我认为这将会……嗯，对于熟悉人工智能对齐生态系统的人来说，这可能是旧闻，但对于从上一系列中加入我们的人来说，这可能是新东西，在上一系列中，我们正在做更多关于数据科学职业生涯的东西。我想做的一件事是，因为你和我已经谈了很多关于台面优化。我认为梅萨优化者这个概念非常有趣的一点是，它确实给了我们一个非常有趣的视角，关于人类是什么，生命是什么，宇宙到底是什么，我想探索一下，如果你不介意的话，就在这里发表一些我们的私人谈话。这是我想邀请你参加播客的部分原因。你不介意吗？</p><p id="bcf2" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>是的，当然。</p><p id="5c13" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>好的，很好。所以我想也许我在试着想一个问题来开始我们的讨论，但也许我会提供一个框架，你可以从那里开始。因此，我们已经讨论了宇宙中的每一种生物都可以被视为台面优化者的观点。所以本质上正在发生的是宇宙有一大堆原子，它有一大堆光子，粒子各种不同的自由度，它在运行这个实验。</p><p id="3d11" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰瑞米:<br/>这是关于粒子随机重组的训练。所以你会看到粒子之间的相互作用，这种相互作用会随着时间的推移而持续，随着时间的推移，粒子的混杂会趋向于复杂系统的自我组装。无论宇宙的最终状态是什么样的，看起来我们都在朝着越来越复杂的方向发展，以至于人类不会把自己消灭掉，以至于我们实际上生存到了创造一个自我改进的人工智能系统的程度。</p><p id="01c6" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰瑞米:<br/>看起来我们真的是在朝着智能爆炸的方向前进，就像人们所说的那样。在这个框架中，我们在这里与我们周围的所有其他台面优化者竞争。因此，我们想要访问…就像你说的，你已经有了专门了解人类神经化学的台面优化器，这样就可以解决将你的大脑放在一个内啡肽大桶中的整体问题。人类，专注于解决在生存和繁衍中成为真正的好人的问题。也许我会把这个想法放在那里。</p><p id="5ece" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>这是一个很好的开始。我们和世界上所有其他动物和生物一样，我们都在一个进化我们的巨大优化器里，对吗？所以进化是一个大的过程，试图把我们推向遗传适应性。我们的进化方向是相对于人口数量有更多的孩子。增加你在下一代的基因表达。这是我们进化的方向。</p><p id="5844" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>有趣的是，尤其是最近，人类似乎真的没有很好地遵循这个指令。如果我们真的遵循了那个指令，如果我们真的像这样，“好吧。我想要尽可能多的孩子，相比之下……”我们的行为会大不相同。例如，我们不会使用节育措施。我们可能不会使用避孕套。相对于生育率下降之类的事情，我们会有更多的孩子。</p><p id="b19e" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>所以这是出了问题的信号。在缓慢的优化压力和我们正在做和学习做的事情之间有一些不匹配的地方。我们被饥饿、性欲、恐惧、愤怒等欲望所驱使。所有这些都与我们的包容性基因适应性有很长一段时间的关联。这些都是非常复杂的适应。它们进化了数百万年。</p><p id="f086" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>许多与我们无关的动物，可能会像动物一样感到恐惧，诸如此类。所以有些事情正在发生。在某种意义上，进化告诉我们该做什么，而我们自己的内在驱动告诉我们该做什么，两者之间的距离越来越大。我们的内驱力能够以进化的方式来满足这些内驱力，如果它是一个人，就会像这样，“嘿，哇。那是病态的。</p><p id="76b1" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>你们这些家伙在这里做错事了。你对你的饥饿做了太多的优化。你越来越胖，没有孩子。这是什么？你对你的性冲动做了太多的优化，用避孕套做爱，你没有孩子。</p><p id="2514" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>从进化的角度来看，进化就像人类试图获得正确的代码和人工智能，这样它就不会偏离轨道，现在发生的是进化程序让你生一大堆孩子，然后你开始看色情片，而不是变得饥渴，与人做爱和生孩子，进化会说，“哇，哇，哇。我不是这个意思。你不能这样作弊。”</p><p id="8e11" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>这是关键。进化实际上并没有让我们生孩子，而是找到了一种更简单的方法。进化就像，“哦，如果我把这些小致动器放在我的东西上，有机体就会想出如何从那里生孩子。”问题是，当生物体变得非常非常聪明时，它会想出如何在不做进化想要它做的事情的情况下满足这些欲望。</p><p id="2f5b" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>它基本上超越了最初的损失函数。所以你提到进化想要人类做的和人类最终实际做的之间越来越明显。我认为探索日光的来源是值得的，因为我的意思是，如果我错了，请纠正我，基于我们的谈话，我认为我们在这一点上是一致的，但我认为这归结于计算能力。最终，有机体能够运行比进化本身更多的计算。</p><p id="6d51" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>你基本上能够…你有足够的计算能力，自然选择不是改变你的主要因素。现在，改变你的主要是你自己的思维，你自己的认知能力。因此，在人的一生中，如果你像水牛或其他动物一样，你实际上可以改变自己，比进化所允许的要多得多。</p><p id="3425" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>是啊。我认为这与内部和外部反馈信号的密集度和稀疏度有关。有多少个处理步骤-</p><p id="1c51" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">你能详细说明一下吗？这里的密疏是什么意思？</p><p id="28ef" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">所以进化给了你一个非常稀疏的健康信号。它发出一个健身信号，就像是，每20或30年发生一次。你有孩子吗？他们有孩子吗？每二三十年。你在生孩子之前就死了吗？这是一个非常聪明的信号。它不喜欢经常打你的脸，但当它打你的脸时，它会狠狠地打你。然而，你自己的内部过程，让你…在那30年的时间跨度里，你可能会，天啊，我甚至不知道可能，我不知道，你会饿，天知道一天三次，或者在30年里成千上万次。</p><p id="fdfc" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰瑞米:<br/>你撞到了手肘，擦伤了膝盖。</p><p id="c19f" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">是的。就这样，就这样。因此，在进化压力信号的脉冲之间，你有一定的自由度来适应你自己的大脑信号。所以它的工作方式是进化在你自己的信号排列方式上提供反馈压力。所以进化管理着你有多饿之间的平衡。你有多饥渴？你有多生气？你有多开心？所有这些事情。进化在几十年的时间尺度上做到了这一点。但是在你的大脑中，你对这些信号有一套安排，你在内部优化和处理的时间尺度比那要快几万到几百万倍。</p><p id="5d5d" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>结果，你有空间以一种进化无法解释的方式超级关注那些信号。我们的处理能力现在太强了，无法被这些信号所控制。很快我们就有可能直接改变我们自己的DNA，完全脱离我们自己。</p><p id="1b11" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>我们仍然是进化的奴隶，但是从进化的角度来看，这些畸形的病态奴隶不再是进化的直接奴隶，而是进化创造的笨拙的东西的奴隶。</p><p id="812f" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>嗯，你从进化的角度来说很有趣，对吗？但是进化当然是进化产生的东西。我是说，从定义上来说，我们就是目标。所以对我来说，这正是…嗯，基本上，这是外部对齐问题，在这个宇宙中有一些东西正在被优化。这显然不是生物进化。这就是优化的目的。直到今天大约有140亿年的时间。但在过去的几十万年里，人类已经开始分离，随着我们的认知能力在进化想要我们做的事情和你所说的进化给我们反馈的时间线以及我们能够从环境中获得反馈的时间线之间创造了日光，人类开始分离。</p><p id="2e42" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰雷米:<br/>这绝不是过程的结束，对吗？这就是把我们带回人工智能讨论的原因。这里有一个额外的步骤，我们转移基底，不再在生物硬件上运行计算，而是在硅基底而不是细胞上运行计算。在这种情况下，你开始放松许多限制，这些限制使人类与机器相比慢得令人难以置信，我们可能会在更短的时间内完成同样的范式。</p><p id="c0d2" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">是的，没错。所以计算机的一个问题是，是的，它们现在快多了。就计算的连续深度而言，它们似乎比我们快得多。换句话说，我做这个，然后我做那个，然后我做那个。他们能一个接一个做的事情的数量，他们比我们快得多，就其切换基因和测试新生命形式的能力而言，他们比我们快得多。</p><p id="9cf1" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>因此，你可以打个比方，“哦，我造了这个东西，它比我快多了。”我给它这些笨拙的，粗略定义的目标，这些目标一开始和我的目标有很好的关联，但后来它优化，优化，再优化。它变得越来越聪明。而且非常非常快，也许能够以一种我不知道的方式满足我给它的课程目标。我想，“哦，我的上帝。我不是这个意思。停下停下停下。但到那时，我们就不在乎了。进化可能对我们有同样的想法，但我们并不在乎。我们做我们想做的。</p><p id="ea0c" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:<br/>是的。正如相对于人类生活的步伐，进化似乎停滞不前一样，相对于这种人工智能的进化步伐，人类生活似乎也停滞不前。我是说，不管是什么形式。所以我的意思是，我认为这是一个很大的支持你之前的论点，我们有一个机会来做好这件事。我想这也是我们开始关注这个领域，或者人工智能开始兴起的时间领域的一个担忧，越来越多的进展正在取得，越来越多的通用系统正在建立，像GPT3这样的东西，但不仅如此。我们很快就会得到越来越先进的系统。</p><p id="5635" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>我们开始进入这个领域，在这里我们受到想要设计程序的最没有人工智能安全意识的公司的支配。我的意思是，这真的是人工智能政策的领域，你开始说，“好吧，你如何让博弈论在这里工作？”你如何防止一些愚蠢的公司决定说，“哦，我不是特别关心人工智能对齐。我不担心这个。”你如何阻止他们朝这个方向采取一些非常不负责任的行动？我知道这可能不是你的领域，但你对人工智能政策方面的博弈论有什么想法吗？</p><p id="0dc0" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>嗯，是的。因此，从经济角度来看，有很多关于人工智能民主化的讨论，这很好。我认为这在某种程度上是好的。当你开始谈论这些非常，非常大，非常有能力的系统时，它们开始潜在地呈现危险，不仅仅是它们可能接管世界，而且危险在那之前很久就存在了。它们可能会被某些人滥用来发送大量精心制作的垃圾邮件或各种新奇的风险。</p><p id="3f93" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>所以，it的经济学原理是，你越接近垄断，你的业务就有越多的利润。在这里，我谈论它就好像它是一个生意，因为这些模型是作为API和所有这些东西来销售的。</p><p id="1916" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>我们说的利润越多，实际上就是利润越多。</p><p id="32b0" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>这也是我的部分意思。当我说保证金时，保证金可以变成利润。它完全可以盈利，到目前为止，它可以盈利，但也可以安全。可以去安全投资。所以当你是一个垄断者时，假设你收取50%的利润。收入的50%归你所有，你可以用它们做任何你想做的事情。是的，你可以把它们用在你的资产负债表上，比如，“哦，现在，我有更多的钱了。那太好了。”</p><p id="b8e6" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Ed: <br/>但你要做的另一件事是让他们保证你的系统安全，这需要越来越多的投资。拥有一个广泛竞争的环境，销售大量不同的型号，其中一个问题是，当他们相互竞争时，每家公司每种型号，他们竞争他们的利润。在资本主义，这通常被视为好事。这很好，因为这意味着消费者支付的费用减少了。</p><p id="381f" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>如果一家公司很卑鄙或邪恶，我可以换另一家公司，这样对每个人都更便宜。在一个有潜在危险的系统的情况下，你想要的是一个垄断，把所有的利润预算都花在安全上，而不是有太多有限的利润，他们互相竞争，所以他们不能把钱花在安全上。这可能是一个有风险的安排。</p><p id="d297" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:<br/>是的。我看到过关于什么的争论…这叫多极还是单极。基本上，有多少不同的极点，多少不同的公司或组织拥有活跃的人工智能成果或尖端的人工智能成果，以及这如何影响风险格局？我看到有人认为多极可能更可取。我个人倾向于你刚才讨论的方向。我知道我们已经谈了很多了。我认为另一个很有说服力…哦，对不起。你想加点什么吗？</p><p id="f57f" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Ed: <br/>我只是想说，如果你有一个完全致力于安全的组织，资金充足，并且对所有其他公司正在建设的东西有很大的内部透明度，你也许可以实现多极化。这可能是一种可行的方法，但至少在目前的情况下，这似乎很难做到。</p><p id="b3fe" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:是的。我知道开放人工智能，将一项政策工作放在一起研究，基本上就像朝着一种更透明的方式发展组织内部的人工智能。这背后的博弈论很大一部分是信任。如果你是谷歌，你正朝着一个超级强大的AGI努力，这可能是危险的，脸书告诉你，他们也在努力，你们都向对方声称，你在安全方面投入了很多努力，以确保没有可怕的事情发生。</p><p id="89c7" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>你应该在多大程度上相信另一家公司正在发生这种情况，他们不会拿走所有的利润，让你更快地开发产品。</p><p id="e17a" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>这是一个很好的场景。这是两家文化上一致的公司，经常交换员工，基本上位于海湾地区的同一地带。他们在很多方面都是非常相似的公司，当你与来自不同国家的公司打交道时，信任情况会发生什么变化呢？这些公司说的不是同一种语言，从未交换过员工，拥有不同的政府结构和根本不同的价值体系。在这种情况下，建立信任变得非常不同，也更加困难。</p><p id="d620" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:<br/>是的。我的意思是，也许最恰当的类比是，这取决于你怎么理解。因此，人们，尤其是那些主张多极的人一个常见的论点是集中在超级强大的系统和当今系统之间的更中观的领域，他们说，“看，人工智能有被部署到武器系统的风险。”现在，你有多极可能是好的，因为谁知道如果只有一个国家拥有核武器，而没有其他国家可以与它们相互摧毁，从而确保没有人会发射核武器，冷战会如何展开。也许类似的事情也会发生在AI身上。我认为有令人信服的理由说明为什么情况并非如此，或者至少我过去是这样认为的。但是，对不起。</p><p id="ea21" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">埃德:<br/>我认为这基本上是资本主义关于竞争的论点，在我们到目前为止所看到的所有情况下，竞争是一件好事。许多不同的组织拥有相似的技术，这是件好事。他们互相竞争，互相制约，等等。这可能会继续适用于人工智能，直到一定的能力水平，但我怀疑会有一个能力水平，超过这个水平，安全投资变得比能力投资更重要。在这一点上，你基本上需要很大的余量来保证安全。所以某种等同或同构于垄断的安排。</p><p id="89f5" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:是的。我的意思是，当然我们看到桌面生物武器或生物工程设计病原体将很快成为现实。核武器也是一样，对吗？我的意思是你到了一个点，好吧，是时候停止公开发布结果了。我认为开放人工智能本身是他们的功劳，至少从我的角度来看是这样……他们为此打了很多旗号。他们站出来说，“看，随着我们开始接近越来越大的能力，我们将不得不开始把我们的一些工作放在内部，而不是广泛地发布。”</p><p id="4d4f" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Jeremie: <br/>他们已经越来越谨慎了，我们从GPT2那里听说，这是他们朝着那个方向迈出的第一步。现在GPT3完全是内部的。没有一个是公共领域。它不仅仅是一个开源模型。无论如何，观察这个领域如何演变和发展将会很有趣。显然，这个领域的任何决策都有很多争议。我认为，考虑到利害关系，关键的事情之一是真正瞄准那些你我都看到的非常好的政策，因为我们已经与从政策制定者到人工智能比对研究人员的每个人进行了接触。</p><p id="7fba" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰瑞米:<br/>如果你在这个领域不小心，很容易做出弊大于利的举动。所以我想我想做的是以对正在听的人的呼吁来结束这次谈话。如果你对人工智能排列、人工智能安全之类的东西感兴趣。如果你觉得我们在这里讨论的风险很有吸引力，很有意思，那就去看看我们在视频描述中提供的博客文章的一些链接，因为有很多组织你可能想去看看你有什么选择，你可以在哪里做出贡献，但是从什么是好的角度来看事情， 在意想不到的变化下，什么可能真的弊大于利，因为当你谈论尖端技术时，事情可能会以令人惊讶的方式出错。 无论如何，我想说的是，因为我们自己也遇到了一些这样的问题。</p><p id="9231" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>对。我认为这很有道理。这是第一种无害的东西。这是一个希波克拉底誓言，我想在更大的范围内，但我认为这是第一个努力的方向，就像是，“我们如何才能避免不良后果的发生，并让每个人都保持良好的状态？”</p><p id="feb5" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">耶雷米:<br/>太棒了。好吧。艾德，我真的很感激。我想我们要做的是，在所有这些东西的描述中，我也会链接你自己的个人网站。有什么想分享的吗？我想你介意分享你的推特吗，这样人们就可以关注你了？</p><p id="bcb0" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:当然，没错。我是@neutronsNeurons或者神经元中子。我真的忘了。抱歉伙计。</p><p id="7427" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我想…哦，糟了。我们会，链接到它。</p><p id="21e3" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">中子神经元。我是@neutronsNeurons。所以你可以打电话给我，然后在那里关注我。</p><p id="2c1d" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">太棒了。好吧。非常感谢，艾德。我真的很感激。很棒的谈话。</p><p id="c8c7" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">谢谢你邀请我。</p><p id="b87b" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">杰瑞米:<br/>我的荣幸。我们会让它离线。干杯。</p><p id="41d2" class="pw-post-body-paragraph lb lc iv ld b le lf kf lg lh li ki lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">艾德:<br/>干杯。</p></div></div>    
</body>
</html>