<html>
<head>
<title>What is Cross Entropy?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是交叉熵？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-cross-entropy-3bdb04c13616?source=collection_archive---------2-----------------------#2020-11-03">https://towardsdatascience.com/what-is-cross-entropy-3bdb04c13616?source=collection_archive---------2-----------------------#2020-11-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="86e1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">交叉熵的简要说明:什么是交叉熵，它是如何工作的，以及示例代码</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b689594ddc309941e8ca941c468720ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9_RLqHf7IiNTnf72Fw2_eA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从<a class="ae kv" href="https://imgflip.com/memegenerator" rel="noopener ugc nofollow" target="_blank"> ImgFlip </a>生成的图像</p></figure><blockquote class="kw"><p id="7784" class="kx ky iq bd kz la lb lc ld le lf lg dk translated"><em class="lh">交叉熵是分类问题中常用的损失函数。</em></p></blockquote><p id="1b00" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc lg ij bi md translated">几周前，我做了一个相当重大的决定。夜深了，我躺在床上想着一天是怎么过的。因为我一直是一个分析自己选择的人，所以我问了自己两个非常重要的问题。</p><ol class=""><li id="9824" class="mm mn iq lk b ll mo lo mp lr mq lv mr lz ms lg mt mu mv mw bi translated">我应该在睡觉前停止吃薯条吗？它们相当不健康…</li><li id="7ea7" class="mm mn iq lk b ll mx lo my lr mz lv na lz nb lg mt mu mv mw bi translated">我是否按照自己想要的方式度过时间？</li></ol><p id="0cee" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">在考虑了这些想法之后，我意识到这两个问题的答案都是否定的。不，我不应该在睡觉前停止吃薯条。不，我没有用我想要的方式度过我的时间。因此，几周前，我决定尽我所能学习关于深度学习这一新兴领域的一切。我还选择写下我所学到的一切(所以如果你对旅程感兴趣，一定要<a class="ae kv" href="https://medium.com/@anjalibhardwaj2700" rel="noopener">跟随我</a>)。这些文章的灵感来自Udacity的一门课程，名为<a class="ae kv" href="https://www.udacity.com/course/deep-learning-pytorch--ud188" rel="noopener ugc nofollow" target="_blank">用PyTorch进行深度学习</a>。我强烈建议你去看看。</p><p id="1b98" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">如果你是从我的上一篇文章来到这里的，你已经知道我们正在努力寻找数据分类的最佳方法。为了对数据进行最佳分类，我们希望最大化模型的概率，最小化其误差函数。这两件事是反向相关的。交叉熵基于概率和误差来衡量分类模型的性能，其中某个事物的可能性越大，交叉熵就越低。让我们更深入地研究这个问题。</p><h1 id="1342" class="nf ng iq bd nh ni nj nk nl nm nn no np jw nq jx nr jz ns ka nt kc nu kd nv nw bi translated">交叉熵101</h1><p id="55fe" class="pw-post-body-paragraph li lj iq lk b ll nx jr ln lo ny ju lq lr nz lt lu lv oa lx ly lz ob mb mc lg ij bi translated">交叉熵是一个<a class="ae kv" href="https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">损失函数</a>，可以用来量化两个概率分布之间的差异。这可以通过一个例子得到最好的解释。</p><p id="6a55" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">假设，我们有两个模型，A和B，我们想找出哪个模型更好，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/63e60e5cf9c978b174649c7de3959da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zg-mF9K3J2b5z7t76IYvlg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><blockquote class="od oe of"><p id="e0e5" class="li lj og lk b ll mo jr ln lo mp ju lq oh nc lt lu oi nd lx ly oj ne mb mc lg ij bi translated"><em class="iq">注意:数据点附近的数字代表该点是该颜色的概率。例如，模型A中图形顶部的蓝点是蓝色的概率是0.4。</em></p></blockquote><p id="07c0" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">凭直觉，我们知道模型B更好，因为红点在红色分布上，蓝点在蓝色分布上。但是我们如何让一个模型预测这个呢？</p><p id="2649" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">一种方法是将模型A中每个点的概率相乘。这将给出模型的总概率，正如我们从概率中的<a class="ae kv" href="https://www.khanacademy.org/math/ap-statistics/probability-ap/probability-multiplication-rule/a/general-multiplication-rule#:~:text=When%20we%20calculate%20probabilities%20involving,probability%20of%20the%20second%20event." rel="noopener ugc nofollow" target="_blank">一般乘法法则所知。我们可以对模型B做类似的事情，</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/658fcb2b25e59a042736d44236b10bfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5aFTjlbwqL28rHQJMYKMQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6fc2" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">从上图可以看出，最有可能的模型是模型B，因为概率更高。太酷了！我们可以用概率来找出哪个模型更好。</p><p id="df22" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">除了……这个模型有些问题。正如你可能已经猜到的，如果我们有许多数据点，这将导致结果概率非常小。此外，如果我们改变了一个数据点，那么结果概率就会发生巨大变化。</p><p id="749e" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">简而言之，使用产品并不是最好的主意。那么我们该如何解决这个问题呢？一种方法是用求和来代替。如果您记得您的<a class="ae kv" href="https://www.rapidtables.com/math/algebra/Logarithm.html" rel="noopener ugc nofollow" target="_blank">日志规则</a>，有一种方法可以将乘积与总和联系起来，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/b9aebd202a374abc7d2326d904b20f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*XCMPXjvMGh35WE7KPl4qtQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1a17" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">让我们把这个规则应用到我们的概率上，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/af654c7f0705cd7d6588eb3b4d21dd55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d2Fzo094vx6hkvcFTDQoTg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="cb05" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">现在，这几乎看起来不错，但让我们通过使对数为负值来消除负值，让我们也计算一下总数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/128e4e56507cb51c2f9aec59970bcdbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOn-ZNM_kPpjfOSI6X8vrg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="30c6" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">对概率使用负对数就是所谓的交叉熵，其中高数值意味着坏模型，低数值意味着好模型。</p><p id="5136" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">当我们计算每个数据点的对数时，我们实际上得到每个点的误差函数。例如，模型A中点0.2的误差函数为-ln(0.2)，等于1.61。请注意，被错误分类的点具有较大的值，因此具有较大的误差。</p></div><div class="ab cl oo op hu oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="ij ik il im in"><p id="8545" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">所以让我们多理解一点交叉熵。交叉熵真正想说的是，如果你有事件和概率，基于概率事件发生的可能性有多大？如果可能性很大，我们就有一个小的交叉熵，如果可能性不大，我们就有一个大的交叉熵。我们将在一个例子后看到更多</p><p id="4977" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">例如，如果我们假设三扇门后有一份礼物，我们有一张如下所示的桌子，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/aba3f634c6e107a00a5a590ec53618cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JGY4RoOVzxo7hUi9C6-F-A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><blockquote class="od oe of"><p id="d5f6" class="li lj og lk b ll mo jr ln lo mp ju lq oh nc lt lu oi nd lx ly oj ne mb mc lg ij bi translated">注意:有更多的可能性</p></blockquote><p id="ffbd" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">这里我们可以看到，如果交叉熵很大，那么事件发生的概率就很低，反之亦然。</p><p id="e018" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">让我们更深入地看看这个例子。假设，我们取第一种情况，门1是p(gift) = 0.8，门2是p(gift) =0.7，门3是p(gift)=0.1，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/522fed3c30fb1e6f78c6ef36f64325d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cGFROKVmg39lwr-4XAsagA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e2b5" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">请注意，我们将第三扇门描述为1-p，这意味着1减去有礼物的概率。这样会给我们没有礼物的概率。另外，注意y描述了门后有多少礼物。</p><p id="c64d" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">所以交叉熵可以用下面的公式来描述，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/fbd7d2f15d84e296b7f0f3c24c9cba70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A9EeKRtdMcKP3VgCAS1caw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><blockquote class="od oe of"><p id="2e61" class="li lj og lk b ll mo jr ln lo mp ju lq oh nc lt lu oi nd lx ly oj ne mb mc lg ij bi translated">注意:这是描述负对数的另一种方式。</p></blockquote><p id="e9de" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">这个公式计算了上述情况的熵。它计算负对数，也就是交叉熵。</p><p id="9ddd" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">这个函数的有用之处在于它也可以写成这种形式，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/b855636a4ceebd25dd6fb46d886c79ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ko7vOBwdAbIJ0skIqKRdWA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><blockquote class="od oe of"><p id="5321" class="li lj og lk b ll mo jr ln lo mp ju lq oh nc lt lu oi nd lx ly oj ne mb mc lg ij bi translated">注意:这个公式只针对二元交叉熵。如果你对多类交叉熵感兴趣，看看这个视频吧！</p></blockquote><p id="89d7" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">该函数允许两个函数，p(x)和q(x)。我们可以把p(x)描述为期望概率，q(x)描述为实际概率。</p><p id="d729" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">因此，交叉熵公式描述了预测分布与真实分布的接近程度。</p><h2 id="8b06" class="oz ng iq bd nh pa pb dn nl pc pd dp np lr pe pf nr lv pg ph nt lz pi pj nv pk bi translated">结论</h2><p id="b726" class="pw-post-body-paragraph li lj iq lk b ll nx jr ln lo ny ju lq lr nz lt lu lv oa lx ly lz ob mb mc lg ij bi translated">总的来说，正如我们所看到的，交叉熵只是一种度量模型概率的方法。交叉熵是有用的，因为它可以描述模型的可能性以及每个数据点的误差函数。它还可以用来描述与真实结果相比的预测结果。所以希望你开始看到这是多么强大！</p></div><div class="ab cl oo op hu oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="ij ik il im in"><h2 id="d3db" class="oz ng iq bd nh pa pb dn nl pc pd dp np lr pe pf nr lv pg ph nt lz pi pj nv pk bi translated">一些代码</h2><p id="8fb9" class="pw-post-body-paragraph li lj iq lk b ll nx jr ln lo ny ju lq lr nz lt lu lv oa lx ly lz ob mb mc lg ij bi translated">让我们看看如何用python来编写代码！</p><pre class="kg kh ki kj gt pl pm pn po aw pp bi"><span id="fec7" class="oz ng iq pm b gy pq pr l ps pt">import numpy as np</span><span id="ef1f" class="oz ng iq pm b gy pu pr l ps pt"># This function takes as input two lists Y, P,<br/># and returns the float corresponding to their cross-entropy.</span><span id="bc0e" class="oz ng iq pm b gy pu pr l ps pt">def cross_entropy(Y, P):<br/>    Y = np.float_(Y)<br/>    P = np.float_(P)<br/>    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))</span></pre><p id="529d" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated"><em class="og">这段代码直接取自Udacity课程，</em> <a class="ae kv" href="https://classroom.udacity.com/courses/ud188/lessons/b4ca7aaa-b346-43b1-ae7d-20d27b2eab65/concepts/760235e0-a3ec-4e56-8cdb-56d762886690" rel="noopener ugc nofollow" target="_blank"> <em class="og">深度学习用PyTorch。</em> </a></p><p id="3d8c" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">这个简单的代码接受两个输入并返回交叉熵。</p><h1 id="f0a0" class="nf ng iq bd nh ni nj nk nl nm nn no np jw nq jx nr jz ns ka nt kc nu kd nv nw bi translated">关键要点</h1><p id="965e" class="pw-post-body-paragraph li lj iq lk b ll nx jr ln lo ny ju lq lr nz lt lu lv oa lx ly lz ob mb mc lg ij bi translated"><strong class="lk ir">什么是交叉熵？</strong> <br/>交叉熵是一个<a class="ae kv" href="https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">损失函数</a>，用于量化两个概率分布之间的差异。</p><h1 id="f454" class="nf ng iq bd nh ni nj nk nl nm nn no np jw nq jx nr jz ns ka nt kc nu kd nv nw bi translated">Week✨的DL视频</h1><p id="de62" class="pw-post-body-paragraph li lj iq lk b ll nx jr ln lo ny ju lq lr nz lt lu lv oa lx ly lz ob mb mc lg ij bi translated">这个人工智能可以把你的脸变成迪斯尼人物！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pv pw l"/></div></figure><h1 id="1d1f" class="nf ng iq bd nh ni nj nk nl nm nn no np jw nq jx nr jz ns ka nt kc nu kd nv nw bi translated">额外资源/引用</h1><p id="d2a0" class="pw-post-body-paragraph li lj iq lk b ll nx jr ln lo ny ju lq lr nz lt lu lv oa lx ly lz ob mb mc lg ij bi translated"><a class="ae kv" href="https://stackoverflow.com/questions/41990250/what-is-cross-entropy" rel="noopener ugc nofollow" target="_blank">这是一个关于交叉熵的很好的形式。</a></p><p id="2761" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated">关于交叉熵的完美的<a class="ae kv" href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html" rel="noopener ugc nofollow" target="_blank">备忘单</a></p><h1 id="6114" class="nf ng iq bd nh ni nj nk nl nm nn no np jw nq jx nr jz ns ka nt kc nu kd nv nw bi translated">你能在哪里找到我😝</h1><p id="324f" class="pw-post-body-paragraph li lj iq lk b ll nx jr ln lo ny ju lq lr nz lt lu lv oa lx ly lz ob mb mc lg ij bi translated"><a class="ae kv" href="http://www.linkedin.com/in/anjalibhardwaj2700" rel="noopener ugc nofollow" target="_blank">我的LinkedIn！请随时与我联系，我喜欢谈论人工智能！</a></p><p id="7cf6" class="pw-post-body-paragraph li lj iq lk b ll mo jr ln lo mp ju lq lr nc lt lu lv nd lx ly lz ne mb mc lg ij bi translated"><a class="ae kv" href="https://medium.com/@anjalibhardwaj2700" rel="noopener">关注我的中页了解更多</a>！</p></div></div>    
</body>
</html>