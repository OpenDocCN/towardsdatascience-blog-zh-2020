<html>
<head>
<title>Sentence Embeddings and CoreNLP’s Recursive Sentiment Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">句子嵌入与CoreNLP的递归情感模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5?source=collection_archive---------32-----------------------#2020-10-01">https://towardsdatascience.com/sentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5?source=collection_archive---------32-----------------------#2020-10-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="88c0" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="a71e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">理解并实现CoreNLP的情感模型。</h2></div><p id="c333" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你好。几周前，我发布了关于库coreNLP<strong class="kt jd">和更具体的情感分析模型的<strong class="kt jd">系列文章</strong>  <strong class="kt jd">的第一篇<a class="ae ln" rel="noopener" target="_blank" href="/intro-to-stanfords-corenlp-and-java-for-python-programmers-c2586215aab6">。第一篇文章</a></strong>是对Java包及其主要特性的介绍，特别针对像我这样习惯于使用Python的人。正如所承诺的，本系列的第二篇文章将更深入地探讨CoreNLP的情感注释器:为什么它不是你常用的情感分类器，它背后的递归模型，以及如何用一些简单的Java脚本来实现它(你可以在我的github上找到它！).</strong></p><p id="92ea" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我想，在我开始之前，我应该警告读者:⛔️这篇文章很少谈到情感分析<strong class="kt jd"> </strong>和很多关于句子嵌入的⛔️.不要害怕，希望在你阅读的时候这对你有意义！</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/043215b2ca76e3ffb0ddfafd555a444a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UrGLz9FC0OCb0hJs"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated"><a class="ae ln" href="https://unsplash.com/@ninjason?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">梁杰森</a>在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2551" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们从…谷歌一下CoreNLP情绪模型开始！当点击进入coreNLP情感分类器的<a class="ae ln" href="https://nlp.stanford.edu/sentiment/" rel="noopener ugc nofollow" target="_blank">官方页面时，我们发现了以下<strong class="kt jd">描述</strong></a></p><blockquote class="me mf mg"><p id="8472" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">大多数情绪预测系统的工作原理只是孤立地看单词，对正面单词给出正面分，对负面单词给出负面分，然后将这些分相加。那样的话，单词的顺序会被忽略，重要的信息也会丢失。相比之下，我们新的深度学习模型实际上基于句子结构建立了整个句子的表示。它根据单词如何组成更长短语的意思来计算情感。这样模型就不像以前的模型那么容易被忽悠了。</p></blockquote><p id="c706" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">看完这一段，我们已经可以分辨出两件事:</p><ol class=""><li id="0713" class="ml mm it kt b ku kv kx ky la mn le mo li mp lm mq mr ms mt bi translated">这不是一个普通的情绪预测器，而是更有趣的东西(可能更有效！)</li><li id="5a50" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">这个模型和其他情感模型之间的核心区别似乎不是分类器本身，而是输入文本的表示(T2)。</li></ol><p id="d16a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第二点已经揭示了这篇文章的中心主题:<strong class="kt jd">文本表示</strong>。为了深入了解为什么CoreNLP的情感模型如此强大和有效，我们首先需要理解<strong class="kt jd">恰当地表示输入文本的重要性。</strong>而这就是我们下面要讲的！</p><p id="8f1d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我先介绍一下文本表示的复杂性(我称之为<strong class="kt jd">语义建模</strong>)是什么，以及word2vec等知名单词嵌入模型的<strong class="kt jd">局限性</strong> <strong class="kt jd">。然后我将谈论语义组合性的概念以及coreNLP如何使用它来创建一个非常强大的递归情感分析模型。最后，我将给出一个Java中的简单实现<strong class="kt jd">的例子。我们走吧！</strong></strong></p><h2 id="ceed" class="mz na it bd nb nc nd dn ne nf ng dp nh la ni nj nk le nl nm nn li no np nq iz bi translated">文本表示或语义建模</h2><p id="ae32" class="pw-post-body-paragraph kr ks it kt b ku nr kd kw kx ns kg kz la nt lc ld le nu lg lh li nv lk ll lm im bi translated">我喜欢把<strong class="kt jd">语义建模</strong>想成这样:人类可以阅读和理解一系列字母和符号(就像这句话)，然而，ML算法只能理解数字序列。为了让情感分类器或任何其他模型处理文本，必须将文本从人类可读形式翻译成计算机可读形式。</p><p id="4519" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">原来有许多方法来表示文本:作为一个单词包，一个热编码，基于逻辑的框架，嵌入空间上的语义向量…并且重要的是<strong class="kt jd">选择最好的一个</strong>，因为这将直接影响你的模型的性能。想想看:如果模型连输入的文本都不能理解，我们怎么能指望它去分类呢！</p><p id="1a67" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我上面提到的所有语义建模技术中，<strong class="kt jd">语义向量</strong>的使用被认为是NLP文献中<strong class="kt jd">首选文本表示选项</strong>之一。特别是，近年来，单词嵌入已经成为一种非常流行的方法并引起了很多关注。想想<strong class="kt jd"> word2vec </strong>，<strong class="kt jd"> GloVe </strong>，<strong class="kt jd"> FastText </strong> …</p><ul class=""><li id="7056" class="ml mm it kt b ku kv kx ky la mn le mo li mp lm nw mr ms mt bi translated"><strong class="kt jd">单词嵌入及其局限性</strong></li></ul><p id="970c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">单词嵌入是建立在这样一个理念上的:从语境中推断出一个给定单词的意思是可能的<strong class="kt jd"/>(Mitchell and la pata，2010)。</p><p id="135f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这个框架中，<strong class="kt jd">单词</strong> <strong class="kt jd">基本上由向量</strong>来表示，这些向量携带特定单词的潜在语义信息(Socher，2013)。<strong class="kt jd">图1 </strong>表示简单的<strong class="kt jd">语义二维空间</strong>。请注意，相似的单词看起来更接近。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/45f9b89a0ae91371ca68ca922068761c.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/0*5BhHAYpnvX2CK12p"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">图1 (Mitchell和Lapata，2010年，图1，第1390页)语义向量空间中表示的单词。它们之间的接近表明语义相似。</p></figure><p id="0709" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，尽管它们被广泛使用，当我们对计算一个短语或句子的表示感兴趣时，这些模型的局限性变得非常明显。单词嵌入模型<strong class="kt jd">只能孤立地表示单词</strong>并且不能说明它们之间的句法和语法关联。</p><p id="038f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当我们想要表示一个句子，但是我们只有它的单词的嵌入时，更常用的解决方案之一是<strong class="kt jd">平均它的单词向量</strong>以获得一个句子向量。这种方法已经被证明在某些情况下足够有效，但是我个人认为它非常简单，因为句子的特定句法、语法和依赖性都被忽略了。</p><ul class=""><li id="9692" class="ml mm it kt b ku kv kx ky la mn le mo li mp lm nw mr ms mt bi translated"><strong class="kt jd">复合性</strong></li></ul><p id="e375" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">作为一个有语言学和文学背景的人，在做文本分析时考虑句法、语法和词序是我甚至不会质疑的事情！！在亲身经历了单词嵌入的局限性之后，我偶然发现了语义组合的原则，并开始思考将它应用到语义建模任务中会有多酷。这项原则规定:</p><blockquote class="me mf mg"><p id="9ba4" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">“一个(句法上复杂的)整体的意义只是它的(句法)部分的意义以及这些部分组合的方法的函数”(Pelletier，1994年，第11页)。</p></blockquote><p id="8338" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在<strong class="kt jd"> NLP文献</strong>中，弗雷格原理最常见的解释和使用是作为一个理论基础<strong class="kt jd">它规定一个人应该能够根据短语来解释一个完整句子的意思，同样，也应该能够根据单词来解释这些短语。</strong></p><p id="a5a8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，当涉及到语言的表示时，建模者应该意识到<strong class="kt jd">每个句法操作额外隐含了一个语义操作</strong> (Mitchell and Lapata，2010)。Partee (1995，第313页)正式提出了表达两个元素<strong class="kt jd"> u </strong>和<strong class="kt jd"> v </strong>的组合的<strong class="kt jd">公式1 </strong>，其中<strong class="kt jd"> <em class="mh"> f </em> </strong> <em class="mh"> </em>是作用于两个成分的<strong class="kt jd">组合函数</strong>和<em class="mh"> R </em>说明了<strong class="kt jd"> u </strong>和<strong class="kt jd"> v </strong>之间的句法关系(Mitchell and Lapata</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/cbe405836843498db4fa538995b8962f.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/0*l1o0_t4Z0SiaW3ks"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">公式1 (Mitchell和Lapata，2010年，第1393页)</p></figure><p id="2604" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在单词嵌入的基础上，一些作者试图包含不同的组合方法，目的是嵌入语言的各个方面，如词序和句法(Socher，2013)。这方面的一个例子是理查德·索赫尔和他的递归模型。</p><h2 id="ea8c" class="mz na it bd nb nc nd dn ne nf ng dp nh la ni nj nk le nl nm nn li no np nq iz bi translated">用于句子嵌入的递归神经网络</h2><p id="98a6" class="pw-post-body-paragraph kr ks it kt b ku nr kd kw kx ns kg kz la nt lc ld le nu lg lh li nv lk ll lm im bi translated">当我了解到Socher的工作时，我变得非常兴奋，因为他基本上是在试图将这种作曲的想法融入到他的模型中，以构建一种更完整的嵌入句子的形式。为此，他提出了一种基于<strong class="kt jd">递归神经网络(RecNN)的新方法。</strong></p><p id="a442" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该模型基于这样的思想:从更简单的元素(即单词)开始<strong class="kt jd">计算句子嵌入，然后<strong class="kt jd">以自下而上的方式递归使用相同的合成函数</strong>。这种分解句子然后以递归的、自下而上的方式构建句子的方式将允许最终输出更好地捕捉关于句子的语义、句法和情感的复杂信息。我觉得这太酷了！</strong></p><ul class=""><li id="0065" class="ml mm it kt b ku kv kx ky la mn le mo li mp lm nw mr ms mt bi translated"><strong class="kt jd">型号概述</strong></li></ul><p id="7d66" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我将概述一下<strong class="kt jd"> Socher等人(2013) </strong>提出的方法背后的直觉。<strong class="kt jd">图2 </strong>展示了构建递归模型的组合思想的简化版本。在本节中，我将使用这个三元组作为例子。</p><p id="6e0d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">图2将句子及其内部元素描述为解析树的<strong class="kt jd">节点。</strong>这种解析树被称为<strong class="kt jd">二进制</strong>，因为每个父节点只有两个子节点。RecNN模型的基本元素是树的叶子，因此处理从<strong class="kt jd">将句子</strong>分割成单词和<strong class="kt jd">计算每个单词的单词嵌入量</strong>开始。在上面的例子中，第一步是分别计算单词“not”、“very”和“good”的表示法<em class="mh"> a </em>、<em class="mh"> b </em>和<em class="mh"> c </em>。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c82e00376ca2344ac087d512ec4e707b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/0*BQ4bpZlScwlSpDVd"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">图2 (Socher等人，2013年，图4，第4页)</p></figure><p id="7abd" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随后，这些单词表示中的两个将被<strong class="kt jd">配对</strong>，以便计算一个<strong class="kt jd">更高级别的短语表示</strong>。计算将由<strong class="kt jd">合成函数</strong>完成。在图2中，<em class="mh"> p </em> 1是通过将组合函数<em class="mh"> g </em>应用于单词嵌入<em class="mh"> b </em>和<em class="mh"> c </em>来计算的。</p><p id="ffad" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种用于配对节点的组合方法将自底向上递归重复，直到到达树的根。在图2中，根节点是<em class="mh"> p </em> 2，它是通过将组合应用于单词嵌入<em class="mh"> a </em>和短语嵌入<em class="mh"> p </em> 1来计算的。根节点是树中最高的节点，通常<strong class="kt jd">代表完整的句子</strong>。</p><p id="c35a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">值得注意的是，在本例中，组合函数<em class="mh"> g </em> 对于两个组合是相同的。类似地，对于更长的句子，合成函数<strong class="kt jd">将始终保持不变</strong>，并将始终将任何一对向量作为输入。这些<strong class="kt jd">向量</strong>可以表示任何级别的任何节点(例如，单词、子短语、短语)，但是它们<strong class="kt jd">必须总是具有相同的大小</strong>。当组合向量<em class="mh"> b </em>和<em class="mh"> c </em>时，组合<em class="mh"> p </em> 1的输出也将具有与<em class="mh"> b </em>和<em class="mh"> c </em>相同的维数。同样的，组合<em class="mh"> a </em>和<em class="mh"> p </em> 1的输出<em class="mh"> p </em> 2也将具有相同的大小。为了允许递归使用相同的复合函数，这是基本的。</p><ul class=""><li id="9b6c" class="ml mm it kt b ku kv kx ky la mn le mo li mp lm nw mr ms mt bi translated">Sooo，感情在哪里？</li></ul><p id="55ea" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可能想知道这其中的情绪在哪里！</p><p id="2680" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">使用<strong class="kt jd"> softmax分类器</strong>在每个节点</strong>实际预测情感，该分类器使用<strong class="kt jd">节点向量作为输入特征</strong>。这在图2中由从节点<em class="mh"> c </em>、<em class="mh"> p1 </em>和<em class="mh"> p2 </em>出现的彩色圆圈表示。</p><p id="8895" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">此外，情感分类是<strong class="kt jd">多标签</strong>，因此情感得分将在0-4之间变化:0表示非常负面，1表示负面，2表示中性，3表示正面，4表示非常正面。</p><p id="2ebb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在树的根节点预测的<strong class="kt jd">情感将是分配给特定句子的<strong class="kt jd">最终情感</strong>。在图2的例子中，我们可以看到根节点被归类为否定，因此整个句子将是否定的。</strong></p><h2 id="7720" class="mz na it bd nb nc nd dn ne nf ng dp nh la ni nj nk le nl nm nn li no np nq iz bi translated">实施</h2><p id="d9f5" class="pw-post-body-paragraph kr ks it kt b ku nr kd kw kx ns kg kz la nt lc ld le nu lg lh li nv lk ll lm im bi translated">我现在将呈现一个非常简短的<a class="ae ln" rel="noopener" target="_blank" href="/intro-to-stanfords-corenlp-and-java-for-python-programmers-c2586215aab6?source=your_stories_page-------------------------------------">前一篇文章</a>的脚本扩展，以便通过情感分类器运行一些输入文本，并获得一些关于预测输出的指标。</p><p id="ead2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第一步是将<em class="mh">解析</em>和<em class="mh">情感</em>包含在我们的注释器列表中(我们需要解析来运行情感分析)。</p><pre class="lp lq lr ls gt oa ob oc od aw oe bi"><span id="82ae" class="mz na it ob b gy of og l oh oi">// set the list of annotators to run<br/>props.<strong class="ob jd">setProperty</strong>("annotators", "tokenize,ssplit,pos,lemma,ner,depparse,parse,sentiment");</span></pre><p id="22ae" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这样，我们知道输入文本现在将通过情感预测器，因此我们只需检索结果。我们首先想知道特定句子的最终情感得分是多少(在根节点的预测)。</p><pre class="lp lq lr ls gt oa ob oc od aw oe bi"><span id="5748" class="mz na it ob b gy of og l oh oi"><strong class="ob jd">Tree</strong> tree = sentence.<strong class="ob jd">sentimentTree</strong>();</span><span id="7522" class="mz na it ob b gy oj og l oh oi">//get overall score<br/><strong class="ob jd">int</strong> sentimentScore = RNNCoreAnnotations.<strong class="ob jd">getPredictedClass</strong>(tree);</span><span id="4652" class="mz na it ob b gy oj og l oh oi">//print score to terminal <br/>System.out.println("Final score " + sentimentScore );</span></pre><p id="5a24" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们这样做是为了将最终得分保存在一个名为sentimentScore的变量中。这个数字总是0、1、2、3或4。</p><p id="2f84" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">此外，我们想知道预测者分配一个句子属于每个类别的概率是多少。我们通过以下方式获得此类信息:</p><pre class="lp lq lr ls gt oa ob oc od aw oe bi"><span id="c156" class="mz na it ob b gy of og l oh oi"><strong class="ob jd">SimpleMatrix</strong> simpleMatrix = RNNCoreAnnotations.<strong class="ob jd">getPredictions</strong>(tree);</span><span id="51c4" class="mz na it ob b gy oj og l oh oi">//Gets probability for each sentiment using the elements of the sentiment matrix</span><span id="8e38" class="mz na it ob b gy oj og l oh oi"><strong class="ob jd">float</strong> veryneg = (float)Math.round((simpleMatrix.<strong class="ob jd">get</strong>(0)*100d));<br/><strong class="ob jd">float</strong> neg = (float)Math.round((simpleMatrix.<strong class="ob jd">get</strong>(1)*100d));<br/><strong class="ob jd">float</strong> neutral = (float)Math.round((simpleMatrix.<strong class="ob jd">get</strong>(2)*100d));<br/><strong class="ob jd">float</strong> pos = (float)Math.round((simpleMatrix.<strong class="ob jd">get</strong>(3)*100d));<br/><strong class="ob jd">float</strong> verypos = (float)Math.round((simpleMatrix.<strong class="ob jd">get</strong>(4)*100d));</span></pre><p id="9c97" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">概率将存储在变量<em class="mh"> veryneg </em>、<em class="mh"> neg </em>、<em class="mh">neary</em>、<em class="mh"> pos </em>和<em class="mh"> verypos </em>中。</p><p id="2873" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在让我们运行整个文件<em class="mh">coreNLP _ pipeline 3 _ LBP . Java</em>来获得一个示例输出。我们将使用以下文本作为输入，以便观察预测中的变化:“<em class="mh">这是一个可怕的句子。我太喜欢这句话了！这是正常的一句话</em>”。该文本保存为<em class="mh"> coreNLPinput_2.txt </em>。使用以下命令运行脚本:</p><pre class="lp lq lr ls gt oa ob oc od aw oe bi"><span id="4043" class="mz na it ob b gy of og l oh oi">java -cp "*" coreNLP_pipeline3_LBP.java</span></pre><p id="a33e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一手牌的结果将打印在终端上，如下图所示。我们可以观察到分配的分数(“<em class="mh">最终分数</em>”)对句子有意义:否定、肯定和中性。我们也看到概率是一致的，加起来是100。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ok"><img src="../Images/a7ec9c4ede9d00a6c7dedde1b0b3ed65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2lzq6rV2MwqgrsjWjv0Tsw.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">终端输出</p></figure><p id="93f7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">此外，所有结果都被打印到一个. txt文档<em class="mh"> coreNLP_output2.txt </em>中，使用下面的命令可以很容易地将该文档作为DataFrame导入python。结果数据帧将有13列:' par_id '，' sent_id '，' words '，' lemmas '，' posTags '，' nerTags '，' depParse '，' perspective '，' veryneg '，' neg '，' neu '，' pos '和' verypos '。</p><pre class="lp lq lr ls gt oa ob oc od aw oe bi"><span id="1a92" class="mz na it ob b gy of og l oh oi">import pandas as pd<br/>df = pd.read_csv('coreNLP_output.txt', delimiter=';',header=0)</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ol"><img src="../Images/45ada68dc281dbdb1261be5935fb4c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rG02B6-iARPmkjjfVnsjeA.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">从输出创建的数据帧。txt文件</p></figure><h2 id="dc03" class="mz na it bd nb nc nd dn ne nf ng dp nh la ni nj nk le nl nm nn li no np nq iz bi translated">下次……</h2><p id="59c8" class="pw-post-body-paragraph kr ks it kt b ku nr kd kw kx ns kg kz la nt lc ld le nu lg lh li nv lk ll lm im bi translated">暂时就这样吧！希望您喜欢它，并且像我第一次看到这个模型时一样，对在句子向量中包含语法感到兴奋！我觉得对于像我这样的文学专业学生来说，这是一个非常令人满意的模式，因为它建立在实际的语言学基础上。</p><p id="0bf6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">下次</strong>我们将继续讨论句子嵌入！我们将介绍如何从coreNLP注释对象中提取它们，将它们与其他更基本的句子嵌入进行比较，并使用一些特征约简和可视化方法探索它们的信息性。我们还将进一步使用这些向量来计算更全面的文档嵌入，以便在文档级别执行情感分析！✌🏻</p><p id="ec5b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">GitHub:【https://github.com/laurabravopriegue/coreNLP_tutorial T4】</p><h2 id="0dd6" class="mz na it bd nb nc nd dn ne nf ng dp nh la ni nj nk le nl nm nn li no np nq iz bi translated">文献学</h2><p id="48e6" class="pw-post-body-paragraph kr ks it kt b ku nr kd kw kx ns kg kz la nt lc ld le nu lg lh li nv lk ll lm im bi translated">弗雷格，g .，1980年。算术的基础:对数字概念的逻辑数学探究。西北大学出版社。</p><p id="f8a8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">米切尔，j .和拉帕塔，m .，2010年。语义分布模型中的合成。<em class="mh">认知科学</em>，<em class="mh"> 34 </em> (8)，第1388–1429页。可从以下网址获得:<a class="ae ln" href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01106.x" rel="noopener ugc nofollow" target="_blank">https://online library . Wiley . com/doi/full/10.1111/j . 1551-6709 . 2010 . 01106 . x</a></p><p id="cd2d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">帕蒂，b，1995年。词汇语义和组合性。<em class="mh">认知科学的邀请:语言</em>，<em class="mh"> 1 </em>，第311–360页。</p><p id="4d16" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">罗瑟尔，曼宁，C.D .和ng，纽约，2010年12月。用递归神经网络学习连续短语表示和句法分析。在<em class="mh">NIPS-2010深度学习和无监督特征学习研讨会的会议录</em>(2010卷，第1–9页)。可在:<a class="ae ln" href="https://nlp.stanford.edu/pubs/2010SocherManningNg.pdf" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/pubs/2010SocherManningNg.pdf</a></p><p id="020a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Socher，Lin c . c .，Manning c .和ng，A.Y .，2011年。用递归神经网络解析自然场景和自然语言。在<em class="mh">第28届机器学习国际会议(ICML-11)的会议记录中</em>(第129-136页)。可在:<a class="ae ln" href="https://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf" rel="noopener ugc nofollow" target="_blank">https://NLP . Stanford . edu/pubs/SocherLinNgManning _ icml 2011 . pdf</a>查阅</p><p id="a414" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Socher，b . Hu val，c . d . Manning和a . y . Ng，2012年7月。通过递归矩阵向量空间的语义合成。在<em class="mh">2012年自然语言处理和计算自然语言学习经验方法联合会议记录</em>(第1201-1211页)。计算语言学协会。</p><p id="c2d2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Socher，r .，Perelygin，a .，Wu，j .，Chuang，j .，Manning，C.D .，ng，a .，Potts，c .，2013年10月。情感树库语义合成的递归深度模型。在<em class="mh">2013年自然语言处理经验方法会议记录</em>(第1631-1642页)。可在:<a class="ae ln" href="https://www.aclweb.org/anthology/D13-1170" rel="noopener ugc nofollow" target="_blank">https://www.aclweb.org/anthology/D13-1170</a></p><div class="om on gp gr oo op"><a href="https://stanfordnlp.github.io/CoreNLP/" rel="noopener  ugc nofollow" target="_blank"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">概观</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">高性能人类语言分析工具，现在带有Python中的原生深度学习模块，可用于许多…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">stanfordnlp.github.io</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd ly op"/></div></div></a></div><div class="om on gp gr oo op"><a rel="noopener follow" target="_blank" href="/introduction-to-word-embedding-and-word2vec-652d0c2060fa"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">单词嵌入和Word2Vec简介</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">单词嵌入是最流行的文档词汇表示之一。它能够捕捉…的上下文</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="pe l pa pb pc oy pd ly op"/></div></div></a></div><div class="om on gp gr oo op"><a href="https://nlp.stanford.edu/sentiment/" rel="noopener  ugc nofollow" target="_blank"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">感人至深:用于情感分析的深度学习</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">这个网站提供了预测电影评论情绪的现场演示。大多数情绪预测系统都有效…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">nlp.stanford.edu</p></div></div><div class="oy l"><div class="pf l pa pb pc oy pd ly op"/></div></div></a></div></div></div>    
</body>
</html>