<html>
<head>
<title>Data Lake Change Data Capture (CDC) using Apache Hudi on Amazon EMR — Part 2—Process</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Amazon EMR上使用Apache胡迪进行数据湖变更数据捕获(CDC)——第2部分——流程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-lake-change-data-capture-cdc-using-apache-hudi-on-amazon-emr-part-2-process-65e4662d7b4b?source=collection_archive---------7-----------------------#2020-10-22">https://towardsdatascience.com/data-lake-change-data-capture-cdc-using-apache-hudi-on-amazon-emr-part-2-process-65e4662d7b4b?source=collection_archive---------7-----------------------#2020-10-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5aab" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Amazon EMR上的Apache胡迪轻松处理从数据库到数据湖的数据变化</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c5e06ca98371ae11038c31e5590deabf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUUhW4NCBqWHmZh3-SrCAA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2654130" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae ky" href="https://pixabay.com/users/absolutvision-6158753/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2654130" rel="noopener ugc nofollow" target="_blank"> Gino Crescoli </a>拍摄</p></figure><p id="07b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下面的前一篇文章中，我们讨论了如何使用亚马逊数据库迁移服务(DMS)无缝地<strong class="lb iu">收集</strong> CDC数据。</p><p id="b41c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/data-lake-change-data-capture-cdc-using-amazon-database-migration-service-part-1-capture-b43c3422aad4">https://towards data science . com/data-lake-change-data-capture-CDC-using-Amazon-database-migration-service-part-1-capture-b43c 3422 aad 4</a></p><p id="5699" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一篇文章将展示如何<strong class="lb iu">处理</strong> CDC数据，以便在数据湖中实现数据库的接近实时的表示。我们将使用Apache胡迪和Amazon EMR的联合力量来执行此操作。Apache胡迪是一个开源数据管理框架，用于简化近乎实时的增量数据处理。</p><p id="8aab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将通过创建一个新的EMR集群来启动该流程</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="fddd" class="ma mb it lw b gy mc md l me mf">$ aws emr create-cluster --auto-scaling-role EMR_AutoScaling_DefaultRole --applications Name=Spark Name=Hive --ebs-root-volume-size 10 --ec2-attributes '{"KeyName":"roopikadf","InstanceProfile":"EMR_EC2_DefaultRole","SubnetId":"subnet-097e5d6e","EmrManagedSlaveSecurityGroup":"sg-088d03d676ac73013","EmrManagedMasterSecurityGroup":"sg-062368f478fb07c11"}' --service-role EMR_DefaultRole --release-label emr-6.0.0 --name 'Training' --instance-groups '[{"InstanceCount":3,"EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"SizeInGB":32,"VolumeType":"gp2"},"VolumesPerInstance":2}]},"InstanceGroupType":"CORE","InstanceType":"m5.xlarge","Name":"Core - 2"},{"InstanceCount":1,"EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"SizeInGB":32,"VolumeType":"gp2"},"VolumesPerInstance":2}]},"InstanceGroupType":"MASTER","InstanceType":"m5.xlarge","Name":"Master - 1"}]' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region us-east-1 --bootstrap-actions Path=s3://aws-analytics-course/job/energy/emr.sh,Name=InstallPythonLibs</span></pre><p id="803d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建EMR集群后，使用SSH登录到主节点，并发出以下命令。这些命令将把Apache胡迪JAR文件复制到S3。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c092" class="ma mb it lw b gy mc md l me mf">$ aws s3 cp /usr/lib/hudi/hudi-spark-bundle.jar s3://aws-analytics-course/hudi/jar/   upload: ../../usr/lib/hudi/hudi-spark-bundle.jar to s3://aws-analytics-course/hudi/jar/hudi-spark-bundle.jar</span><span id="a745" class="ma mb it lw b gy mg md l me mf">$ aws s3 cp /usr/lib/spark/external/lib/spark-avro.jar s3://aws-analytics-course/hudi/jar/<br/>upload: ../../usr/lib/spark/external/lib/spark-avro.jar to s3://aws-analytics-course/hudi/jar/spark-avro.jar</span><span id="fab7" class="ma mb it lw b gy mg md l me mf">$ aws s3 ls s3://aws-analytics-course/hudi/jar/<br/>2020-10-21 17:00:41   23214176 hudi-spark-bundle.jar<br/>2020-10-21 17:00:56     101212 spark-avro.jar</span></pre><p id="58ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在创建一个新的EMR笔记本并上传到以下位置。上传<strong class="lb iu">胡迪/hudi.ipynb </strong></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0947" class="ma mb it lw b gy mc md l me mf">$ git clone https://github.com/mkukreja1/blogs.git</span></pre><p id="34e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用在上一步中上传到S3的胡迪JAR文件创建一个Spark会话。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="297f" class="ma mb it lw b gy mc md l me mf">from pyspark.sql import SparkSession<br/>import pyspark<br/>from pyspark.sql.types import StructType, StructField, IntegerType, StringType, array, ArrayType, DateType, DecimalType<br/>from pyspark.sql.functions import *<br/>from pyspark.sql.functions import concat, lit, col</span><span id="eb72" class="ma mb it lw b gy mg md l me mf">spark = pyspark.sql.SparkSession.builder.appName("Product_Price_Tracking") \<br/>     <strong class="lw iu">.config("spark.jars", "s3://aws-analytics-course/hudi/jar/hudi-spark-bundle.jar,s3://aws-analytics-course/hudi/jar/spark-avro.jar") </strong>\<br/>     .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \<br/>     .config("spark.sql.hive.convertMetastoreParquet", "false") \<br/>     .getOrCreate()</span></pre><p id="d496" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们读取疾病控制中心的文件。我们将从读取<strong class="lb iu">完整的</strong>加载文件开始。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d8bf" class="ma mb it lw b gy mc md l me mf">TABLE_NAME = "coal_prod"<br/>S3_RAW_DATA = "s3://aws-analytics-course/raw/dms/fossil/coal_prod/LOAD00000001.csv"<br/>S3_HUDI_DATA = "s3://aws-analytics-course/hudi/data/coal_prod"</span><span id="802c" class="ma mb it lw b gy mg md l me mf">coal_prod_schema = StructType([StructField("Mode", StringType()),<br/>                               StructField("Entity", StringType()),<br/>                               StructField("Code", StringType()),<br/>                               StructField("Year", IntegerType()),<br/>                               StructField("Production", DecimalType(10,2)),<br/>                               StructField("Consumption", DecimalType(10,2))<br/>                               ])<br/>df_coal_prod = spark.read.csv(S3_RAW_DATA, header=False, schema=coal_prod_schema)</span><span id="9895" class="ma mb it lw b gy mg md l me mf">df_coal_prod.show(5)</span><span id="cd3d" class="ma mb it lw b gy mg md l me mf">+----+-----------+----+----+----------+-----------+<br/>|Mode|     Entity|Code|Year|Production|Consumption|<br/>+----+-----------+----+----+----------+-----------+<br/>|   I|Afghanistan| AFG|1949|      0.04|       0.00|<br/>|   I|Afghanistan| AFG|1950|      0.11|       0.00|<br/>|   I|Afghanistan| AFG|1951|      0.12|       0.00|<br/>|   I|Afghanistan| AFG|1952|      0.14|       0.00|<br/>|   I|Afghanistan| AFG|1953|      0.13|       0.00|<br/>+----+-----------+----+----+----------+-----------+<br/>only showing top 5 rows</span></pre><p id="48c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Apache胡迪需要一个主键来唯一标识每个记录。通常，顺序生成的主键最适合此目的。然而我们的桌子没有。为了解决这个问题，让我们通过使用实体和年份列的组合来生成一个PK。下面的<strong class="lb iu">键</strong>列将被用作主键。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0ea8" class="ma mb it lw b gy mc md l me mf">df_coal_prod=df_coal_prod.select("*", concat(col("Entity"),lit(""),col("Year")).alias("key"))<br/>df_coal_prod_f=df_coal_prod.drop(df_coal_prod.Mode)<br/>df_coal_prod_f.show(5)</span><span id="75dc" class="ma mb it lw b gy mg md l me mf">+-----------+----+----+----------+-----------+---------------+<br/>|     Entity|Code|Year|Production|Consumption|            <strong class="lw iu">key</strong>|<br/>+-----------+----+----+----------+-----------+---------------+<br/>|Afghanistan| AFG|1949|      0.04|       0.00|Afghanistan1949|<br/>|Afghanistan| AFG|1950|      0.11|       0.00|Afghanistan1950|<br/>|Afghanistan| AFG|1951|      0.12|       0.00|Afghanistan1951|<br/>|Afghanistan| AFG|1952|      0.14|       0.00|Afghanistan1952|<br/>|Afghanistan| AFG|1953|      0.13|       0.00|Afghanistan1953|<br/>+-----------+----+----+----------+-----------+---------------+<br/>only showing top 5 rows</span></pre><p id="1479" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在准备以胡迪格式保存数据。由于这是我们第一次保存该表，我们将使用"<strong class="lb iu"> bulk_insert </strong>"操作和<strong class="lb iu">模式=覆盖</strong>。还要注意，我们使用“<strong class="lb iu">键</strong>列作为<strong class="lb iu">记录键</strong>。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9a82" class="ma mb it lw b gy mc md l me mf">df_coal_prod_f.write.format("org.apache.hudi") \<br/>            .option("hoodie.table.name", TABLE_NAME) \<br/>            .option("hoodie.datasource.write.storage.type", "COPY_ON_WRITE") \<br/>            <strong class="lw iu">.option("hoodie.datasource.write.operation", "bulk_insert") \<br/>            .option("hoodie.datasource.write.recordkey.field","key") </strong>\<br/>            .option("hoodie.datasource.write.precombine.field", "key") \<br/>            .mode("<strong class="lw iu">overwrite</strong>") \<br/>            .save(S3_HUDI_DATA)</span></pre><p id="859e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以读取新创建的胡迪表。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8e70" class="ma mb it lw b gy mc md l me mf">df_final = spark.read.format("org.apache.hudi")\<br/>          .load("s3://aws-analytics-course/hudi/data/coal_prod/default/*.parquet")<br/>df_final.registerTempTable("coal_prod")<br/>spark.sql("select count(*) from coal_prod").show(5)<br/>spark.sql("select * from coal_prod where key='India2013'").show(5)</span><span id="22a8" class="ma mb it lw b gy mg md l me mf">+--------+<br/>|count(1)|<br/>+--------+<br/>|    <strong class="lw iu">6282</strong>|<br/>+--------+<br/><br/>+-------------------+--------------------+------------------+----------------------+--------------------+------+----+----+----------+-----------+---------+<br/>|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|Entity|Code|Year|Production|Consumption|      key|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+------+----+----+----------+-----------+---------+<br/>|     20201021215857|20201021215857_54...|         India2013|               default|8fae00ae-34e7-45e...| India| IND|2013|   <strong class="lw iu">2841.01</strong>|       <strong class="lw iu">0.00</strong>|India2013|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+------+----+----+----------+-----------+---------+</span></pre><p id="6928" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我们有来自满载的<strong class="lb iu"> 6282 </strong>行和2013年关键<strong class="lb iu">印度的数据。</strong>该密钥将在下一次操作中更新，因此记录历史非常重要。我们现在将读取增量数据。</p><p id="a0bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">增量数据带有<strong class="lb iu"> 4 </strong>行，插入<strong class="lb iu"> 2 </strong>行，更新<strong class="lb iu">一</strong>行，删除和<strong class="lb iu">一</strong>行。我们将首先处理插入和更新的行。注意下面的<strong class="lb iu">(“模式输入(' U '，' I ')”)</strong>的过滤器。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="f9ed" class="ma mb it lw b gy mc md l me mf">S3_INCR_RAW_DATA = "s3://aws-analytics-course/raw/dms/fossil/coal_prod/20200808-*.csv"<br/>df_coal_prod_incr = spark.read.csv(S3_INCR_RAW_DATA, header=False, schema=coal_prod_schema)<br/><strong class="lw iu">df_coal_prod_incr_u_i=df_coal_prod_incr.filter("Mode IN ('U', 'I')")</strong><br/>df_coal_prod_incr_u_i=df_coal_prod_incr_u_i.select("*", concat(col("Entity"),lit(""),col("Year")).alias("key"))<br/>df_coal_prod_incr_u_i.show(5)</span><span id="b642" class="ma mb it lw b gy mg md l me mf">df_coal_prod_incr_u_i_f=df_coal_prod_incr_u_i.drop(df_coal_prod_incr_u_i.Mode)<br/>df_coal_prod_incr_u_i_f.show()</span><span id="b9f9" class="ma mb it lw b gy mg md l me mf">+----+------+----+----+----------+-----------+---------+<br/>|Mode|Entity|Code|Year|Production|Consumption|      key|<br/>+----+------+----+----+----------+-----------+---------+<br/>|   I| India| IND|2015|   4056.33|       0.00|India2015|<br/>|   I| India| IND|2016|   4890.45|       0.00|India2016|<br/>|   U| India| IND|2013|   2845.66|     145.66|India2013|<br/>+----+------+----+----+----------+-----------+---------+<br/><br/>+------+----+----+----------+-----------+---------+<br/>|Entity|Code|Year|Production|Consumption|      key|<br/>+------+----+----+----------+-----------+---------+<br/>| India| IND|2015|   4056.33|       0.00|India2015|<br/>| India| IND|2016|   4890.45|       0.00|India2016|<br/>| India| IND|2013|   2845.66|     145.66|India2013|<br/>+------+----+----+----------+-----------+---------+</span></pre><p id="f453" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在准备对增量数据执行胡迪<strong class="lb iu">上插</strong>操作。由于这个表已经存在，这次我们将使用<strong class="lb iu">追加</strong>选项。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="ed03" class="ma mb it lw b gy mc md l me mf">df_coal_prod_incr_u_i_f.write.format("org.apache.hudi") \<br/>            .option("hoodie.table.name", TABLE_NAME) \<br/>            .option("hoodie.datasource.write.storage.type", "COPY_ON_WRITE") \<br/>            <strong class="lw iu">.option("hoodie.datasource.write.operation", "upsert")</strong> \<br/>            .option("hoodie.upsert.shuffle.parallelism", 20) \<br/>            .option("hoodie.datasource.write.recordkey.field","key") \<br/>            .option("hoodie.datasource.write.precombine.field", "key") \<br/>            .mode("<strong class="lw iu">append</strong>") \<br/>            .save(S3_HUDI_DATA)</span></pre><p id="7034" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">检查基础数据。请注意，已经添加了2个新行，因此表计数已经从6282增加到6284。另请注意，2013年印度的关键<strong class="lb iu">行现已更新为生产&amp;消耗列。</strong></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="1eff" class="ma mb it lw b gy mc md l me mf">df_final = spark.read.format("org.apache.hudi")\<br/>          .load("s3://aws-analytics-course/hudi/data/coal_prod/default/*.parquet")<br/>df_final.registerTempTable("coal_prod")<br/>spark.sql("select count(*) from coal_prod").show(5)<br/>spark.sql("select * from coal_prod where key='India2013'").show(5)</span><span id="cada" class="ma mb it lw b gy mg md l me mf">+--------+<br/>|count(1)|<br/>+--------+<br/>|    <strong class="lw iu">6284</strong>|<br/>+--------+<br/><br/>+-------------------+--------------------+------------------+----------------------+--------------------+------+----+----+----------+-----------+---------+<br/>|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|Entity|Code|Year|<strong class="lw iu">Production</strong>|<strong class="lw iu">Consumption</strong>|      key|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+------+----+----+----------+-----------+---------+<br/>|     20201021220359|20201021220359_0_...|         India2013|               default|8fae00ae-34e7-45e...| India| IND|2013|   <strong class="lw iu">2845.66</strong>|     <strong class="lw iu">145.66</strong>|India2013|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+------+----+----+----------+-----------+---------+</span></pre><p id="116e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们要处理删除了行的<strong class="lb iu">行。</strong></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="de65" class="ma mb it lw b gy mc md l me mf">df_coal_prod_incr_d=df_coal_prod_incr.filter<strong class="lw iu">("Mode IN ('D')")</strong><br/>df_coal_prod_incr_d=df_coal_prod_incr_d.select("*", concat(col("Entity"),lit(""),col("Year")).alias("key"))<br/>df_coal_prod_incr_d_f=df_coal_prod_incr_d.drop(df_coal_prod_incr_u_i.Mode)<br/>df_coal_prod_incr_d_f.show()</span><span id="50b2" class="ma mb it lw b gy mg md l me mf">+------+----+----+----------+-----------+---------+<br/>|Entity|Code|Year|Production|Consumption|      key|<br/>+------+----+----+----------+-----------+---------+<br/>| India| IND|2010|   2710.54|       0.00|India2010|<br/>+------+----+----+----------+-----------+---------+</span></pre><p id="e8d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过胡迪<strong class="lb iu"> Upsert </strong>操作来实现这一点，但是需要使用额外的选项来删除<strong class="lb iu">hoodie . data source . write . payload . class = org . Apache . hudi . emptyhoodierecordpayload</strong></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9345" class="ma mb it lw b gy mc md l me mf">df_coal_prod_incr_d_f.write.format("org.apache.hudi") \<br/>            .option("hoodie.table.name", TABLE_NAME) \<br/>            .option("hoodie.datasource.write.storage.type", "COPY_ON_WRITE") \<br/>            .option("hoodie.datasource.write.operation", "upsert") \<br/>            .option("hoodie.upsert.shuffle.parallelism", 20) \<br/>            .option("hoodie.datasource.write.recordkey.field","key") \<br/>            .option("hoodie.datasource.write.precombine.field", "key") \<br/>           <strong class="lw iu"> .option("hoodie.datasource.write.payload.class", "org.apache.hudi.EmptyHoodieRecordPayload") \</strong><br/>            .mode("append") \<br/>            .save(S3_HUDI_DATA)</span></pre><p id="ed2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以检查结果。由于删除了一行，计数从6284下降到6283。此外，对已删除行的查询返回空值。一切都按预期进行。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a266" class="ma mb it lw b gy mc md l me mf">df_final = spark.read.format("org.apache.hudi")\<br/>          .load("s3://aws-analytics-course/hudi/data/coal_prod/default/*.parquet")<br/>df_final.registerTempTable("coal_prod")<br/>spark.sql("select count(*) from coal_prod").show(5)<br/>spark.sql("select * from coal_prod where key='India2010'").show(5)</span><span id="911d" class="ma mb it lw b gy mg md l me mf">+--------+<br/>|count(1)|<br/>+--------+<br/>|    <strong class="lw iu">6283</strong>|<br/>+--------+<br/><br/>+-------------------+--------------------+------------------+----------------------+-----------------+------+----+----+----------+-----------+---+<br/>|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name|Entity|Code|Year|Production|Consumption|key|<br/>+-------------------+--------------------+------------------+----------------------+-----------------+------+----+----+----------+-----------+---+<br/>+-------------------+--------------------+------------------+----------------------+-----------------+------+----+----+----------+-----------+---+</span></pre><p id="6dd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文中使用的所有代码都可以在下面的链接中找到:</p><div class="mh mi gp gr mj mk"><a href="https://github.com/mkukreja1/blogs/tree/master/dms" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd iu gy z fp mp fr fs mq fu fw is bi translated">mkukreja 1/博客</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">github.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my ks mk"/></div></div></a></div><p id="9c6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章是有帮助的。<strong class="lb iu"> CDC使用亚马逊数据库迁移服务</strong>是由<a class="ae ky" href="http://www.datafence.com" rel="noopener ugc nofollow" target="_blank"> Datafence Cloud Academy </a>提供的AWS大数据分析课程的一部分。课程是周末自己在网上教的。</p></div></div>    
</body>
</html>