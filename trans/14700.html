<html>
<head>
<title>Deep Convolutional Vs Wasserstein Generative Adversarial Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度卷积Vs Wasserstein生成对抗网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-convolutional-vs-wasserstein-generative-adversarial-network-183fbcfdce1f?source=collection_archive---------20-----------------------#2020-10-10">https://towardsdatascience.com/deep-convolutional-vs-wasserstein-generative-adversarial-network-183fbcfdce1f?source=collection_archive---------20-----------------------#2020-10-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4f5e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解b/w DCGAN和WGAN的差异。使用TensorFlow 2.x实现WGAN</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/da37a34e0d6edc56c6048e00d517c978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AJrLFnuCAD8dE1p1Bg0dbg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">丁满·克劳斯在<a class="ae kv" href="https://unsplash.com/s/photos/art?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="bb24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将尝试了解两种基本GAN类型(即DCGAN和WGAN)的区别，还将了解它们的区别以及WGAN在TensorFlow 2.x中的实现。我使用TensorFlow的官方教程代码DCGAN作为本教程的基础代码，并针对WGAN对其进行了修改。你可以在这里找到<a class="ae kv" href="https://www.tensorflow.org/tutorials/generative/dcgan" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="82d7" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">DCGAN</h2><ol class=""><li id="3394" class="ml mm iq ky b kz mn lc mo lf mp lj mq ln mr lr ms mt mu mv bi translated">DCGAN像其他GAN一样由两个神经网络组成，即一个生成器和一个鉴别器。</li><li id="7282" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">生成器将随机噪声作为输入，并输出生成的假图像。</li><li id="aca1" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">鉴别器将真实和伪造图像作为输入，并输出值b/w 0和1，即图像真实或伪造的置信度。</li><li id="c022" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">DCGAN使用二进制交叉熵作为损失函数。</li><li id="0723" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">生成器看不到真实图像，仅通过来自鉴别器的反馈进行学习。</li><li id="88e4" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">生成器的目标是通过生成逼真的假图像来欺骗鉴别器。而鉴别器的目标是正确识别真实和伪造的图像。</li></ol><h2 id="b5f5" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">DCGAN中的问题</h2><p id="5868" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">DCGAN中出现的一些问题是由于使用了二进制交叉熵损失，如下所示。</p><ol class=""><li id="bc8d" class="ml mm iq ky b kz la lc ld lf ne lj nf ln ng lr ms mt mu mv bi translated"><strong class="ky ir">模式崩溃</strong>:这是一个术语(在GAN的上下文中)，用来定义GAN无法生成不同的类镜像。例如:当用MNIST数据集训练时，GAN可能只能生成一种类型的数，而不是所有10种，即它可能只生成“2”或一些其他数。你也可以举一个例子，甘只能够产生一个品种的狗，如哈士奇，但甘训练所有品种的狗。</li><li id="e146" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated"><strong class="ky ir">消失梯度:</strong>由于鉴别器的置信值是只能为b/w 0和1的单个值，并且目标是获得尽可能接近1的值，因此计算的梯度接近于零，并且因此，生成器不能获得太多信息并且不能学习。因此，这可能导致一个强鉴别器，这将导致一个差的发生器。</li></ol><h2 id="11b5" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">解决办法</h2><p id="b7ff" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">上述问题的一个解决方案是使用接近推土机距离的Wasserstein损失(EMD是从一个分布到另一个分布所需的努力量。在我们的情况下，我们希望使生成的图像分布等于真实的图像分布)。WGAN利用了Wasserstein损耗，所以我们现在来谈谈WGAN。</p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><h1 id="d492" class="no lt iq bd lu np nq nr lx ns nt nu ma jw nv jx md jz nw ka mg kc nx kd mj ny bi translated">WGAN</h1><p id="603b" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">如上所述，WGAN利用了Wasserstein损耗，因此让我们了解发生器和鉴别器的目的。</p><h2 id="5c74" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">瓦瑟斯坦损失{ E(d(R)) — E(d(F)) }</h2><ol class=""><li id="7c6d" class="ml mm iq ky b kz mn lc mo lf mp lj mq ln mr lr ms mt mu mv bi translated">因此，我们的损失是鉴别器输出对真实图像的b/w期望值与鉴别器输出对所产生的伪图像的期望值之差。</li><li id="50ac" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">鉴别器的目标是最大化这个差异，而生成器的目标是最小化这个差异。</li><li id="1b81" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated"><strong class="ky ir"> <em class="nz">注</em> </strong> <em class="nz"> : </em> <strong class="ky ir"> <em class="nz">为了使用Wasserstein损耗，我们的鉴别器需要是1-L (1-Lipschitz)连续的，即梯度的范数在每个点上必须至多为1。</em>T9】</strong></li></ol><p id="ed12" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来看一种加强1-1连续性的方法。</p><h2 id="a799" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">梯度惩罚</h2><p id="2083" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">梯度惩罚用于加强1-L连续性，并作为鉴别器梯度的正则化添加到损失中。以下是计算梯度损失的步骤。</p><ol class=""><li id="049f" class="ml mm iq ky b kz la lc ld lf ne lj nf ln ng lr ms mt mu mv bi translated">通过<strong class="ky ir">(real _ image * epsilon+fake _ image *(1—epsilon))</strong>从真实和虚假图像计算插值图像。</li><li id="5c28" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">然后，计算鉴频器输出相对于插值图像的梯度。之后，计算梯度的范数。</li><li id="3a8c" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">然后，惩罚被计算为(norm-1)的平方的<strong class="ky ir">平均值，因为我们希望范数接近1。</strong></li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h2 id="390b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">生成器和鉴别器的目标</h2><p id="a85a" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">因此，发生器的最终目标是增加鉴频器假输出的平均值。鉴别器的目标是加权惩罚的w损失。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h2 id="1244" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">训练WGAN</h2><p id="619a" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">训练WGAN时，鉴别器每步训练多次，而生成器每步训练一次。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h2 id="d4d1" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">结论</h2><p id="3903" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated"><em class="nz"> WGAN导致稳定的训练，并解决了我们在DCGAN中面临的模式崩溃和消失梯度的问题。</em></p><p id="44b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nz">但这一切都是有代价的，即WGAN在训练时比DCGAN慢。</em></p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><p id="7675" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在这里找到这篇文章的完整代码。敬请关注即将发布的文章，我们将在TensorFlow 2中实现一些有条件和可控的gan。</p><p id="3bb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，本文到此结束。谢谢你的阅读，希望你喜欢并且能够理解我想要解释的内容。希望你阅读我即将发表的文章。哈里奥姆…🙏</p><h1 id="8150" class="no lt iq bd lu np oc nr lx ns od nu ma jw oe jx md jz of ka mg kc og kd mj ny bi translated">参考</h1><div class="oh oi gp gr oj ok"><a href="https://www.coursera.org/specializations/generative-adversarial-networks-gans" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">生成对抗网络</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">由DeepLearning.AI提供关于GANs生成对抗网络(GANs)是强大的机器学习模型…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">www.coursera.org</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy kp ok"/></div></div></a></div><div class="oh oi gp gr oj ok"><a href="https://keras.io/examples/generative/wgan_gp/" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">Keras文档:WGAN-GP覆盖` Model.train_step '</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">作者:A_K_Nain创建日期:2020/05/9最近修改时间:2020/05/9内容简介:用Wasserstein实现甘…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">keras.io</p></div></div><div class="ot l"><div class="oz l ov ow ox ot oy kp ok"/></div></div></a></div></div></div>    
</body>
</html>