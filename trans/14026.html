<html>
<head>
<title>Reinforcement Learning frameworks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a?source=collection_archive---------16-----------------------#2020-09-27">https://towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a?source=collection_archive---------16-----------------------#2020-09-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6df6" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/deep-r-l-explained" rel="noopener" target="_blank">深度强化学习讲解— 20 </a></h2><div class=""/><div class=""><h2 id="1f8e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated"><strong class="ak">在Ray框架上使用</strong> RLlib解决CartPole环境</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fa0f6596bcb8f7d913d0517d5a751d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDhcpkgdzkisARJU93i5xw.png"/></div></div></figure><p id="f06a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">欢迎来到这第20篇文章，它总结了<em class="lz"> " </em> <a class="ae ma" href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener ugc nofollow" target="_blank"> <em class="lz">深度强化学习讲解</em> </a> <em class="lz"> " </em>系列，该系列提供了一种实用的方法来开始令人兴奋的深度强化学习世界。</p><p id="79a9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">到目前为止，在<a class="ae ma" rel="noopener" target="_blank" href="/policy-gradient-methods-104c783251e0">之前的帖子</a>中，我们已经看到了相对容易编程的RL算法集的基本表示(尽管我们跳过了几个)。但是从现在开始，我们需要考虑RL算法的规模和复杂性。在这种情况下，从头开始编写强化学习实现可能会变得单调乏味，并且存在很高的编程错误风险。</p><p id="d620" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了解决这个问题，RL社区开始构建框架和库来简化RL算法的开发，既通过创建新的部分，又特别是通过涉及各种算法组件的组合。在本帖中，我们将对那些<strong class="lf jd">强化学习框架</strong>做一个大致的介绍，并使用<a class="ae ma" href="http://rllib.io/" rel="noopener ugc nofollow" target="_blank"><strong class="lf jd">rl lib</strong></a>(Python中的一个开源库)基于<a class="ae ma" href="https://ray.io/" rel="noopener ugc nofollow" target="_blank"> Ray </a>框架解决前面的CartPole问题。</p><blockquote class="mb mc md"><p id="75c1" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><a class="ae ma" href="https://medium.com/aprendizaje-por-refuerzo/11-frameworks-de-aprendizaje-por-refuerzo-ray-rllib-1329e93f14ee" rel="noopener">本出版物的西班牙语版本</a></p></blockquote><div class="mh mi gp gr mj mk"><a href="https://medium.com/aprendizaje-por-refuerzo/11-frameworks-de-aprendizaje-por-refuerzo-ray-rllib-1329e93f14ee" rel="noopener follow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">11.难民保护框架:Ray+RLlib</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">请访问第11页的自由介绍</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">medium.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="e0fd" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">强化学习框架</h1><h2 id="04d9" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">动机</h2><p id="922c" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">但是在继续之前，作为一个激励的例子，让我们记住在<a class="ae ma" rel="noopener" target="_blank" href="/policy-gradient-methods-104c783251e0">之前的帖子</a>中，我们提出了<a class="ae ma" href="http://incompleteideas.net/williams-92.pdf" rel="noopener ugc nofollow" target="_blank">加强</a>及其局限性。研究界创造了许多训练算法来解决它:<a class="ae ma" href="https://docs.ray.io/en/latest/rllib-toc.html#algorithms" rel="noopener ugc nofollow" target="_blank">、A3C、DDPG、TD3、SAC、PPO </a>等等。但是从头开始编写这些算法比REINFORCE更复杂。此外，你在这个领域参与得越多，你就越会意识到你在一遍又一遍地写同样的代码。</p><p id="4c46" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">与深度学习等其他领域相比，强化学习的实际应用相对年轻，在深度学习中，TensorFlow、PyTorch或MXnet等成熟的框架简化了DL实践者的生活。然而，RL框架的出现已经开始，现在我们可以从几个项目中进行选择，这些项目极大地促进了高级RL方法的使用。</p><p id="f3b4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在介绍这些RL框架之前，让我们先了解一下它们的背景。</p><h2 id="a909" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">从互动中学习，而不是从例子中学习</h2><p id="d1fc" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">在过去的几年中，模式识别一直是深度学习社区中许多工作和讨论的焦点。我们正在使用强大的超级计算机处理大型标记数据集(专家为训练集提供输出)，并应用基于梯度的方法在这些数据集中找到模式，这些模式可用于预测或试图找到数据内部的结构。</p><p id="2a6a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这与我们对世界的知识的重要部分是通过互动获得的事实形成对比，没有外部老师告诉我们我们采取的每一个行动的结果会是什么。人类能够从互动和经验中发现新问题的解决方案，通过积极探索获得关于世界的知识。</p><p id="233d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于这个原因，当前的方法将通过深度强化学习(DRL)的镜头研究从模拟环境的交互中学习的问题，这是一种从交互中进行目标导向学习的计算方法，不依赖于专家的监督。即，强化学习代理必须与环境交互以生成其自己的训练数据。</p><p id="7c09" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这激发了与一个环境的多个实例的并行交互，从而更快地产生更多可以学习的经验。这导致了越来越大规模的分布式和并行系统在RL训练中的广泛使用。这带来了许多工程和算法上的挑战，我们正在谈论的这些框架可以解决这些挑战。</p><h2 id="7909" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">开源拯救世界</h2><p id="4fda" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">近年来，TensorFlow或PyTorch ( <a class="ae ma" rel="noopener" target="_blank" href="/tensorflow-or-pytorch-146f5397278a">我们在本博客</a>中广泛讨论了这两种框架)等框架已经出现，有助于将模式识别转化为商品，使深度学习更容易被实践者尝试和使用。</p><p id="cad4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">类似的模式也开始在强化学习领域上演。我们开始看到许多开源库和工具的出现，通过帮助创建新的部分(而不是从头开始编写)，以及最重要的，涉及各种预构建算法组件的组合来解决这一问题。因此，这些强化学习框架通过创建RL算法核心组件的高级抽象来帮助工程师。总之，这使得代码更容易开发，阅读起来更舒服，并提高了效率。</p><p id="4f09" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">接下来，我提供了一个最流行的RL框架列表。我认为读者会从使用已经建立的框架或库的代码中受益。在写这篇文章的时候，我可以提到最重要的几个(我肯定我漏掉了其中的一些):</p><ul class=""><li id="20d7" class="oh oi it lf b lg lh lj lk lm oj lq ok lu ol ly om on oo op bi translated"><a class="ae ma" href="https://github.com/keras-rl/keras-rl" rel="noopener ugc nofollow" target="_blank"> Keras-RL </a></li><li id="50ca" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://github.com/NervanaSystems/coach" rel="noopener ugc nofollow" target="_blank"> RL蔻驰</a></li><li id="5b54" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://github.com/facebookresearch/ReAgent" rel="noopener ugc nofollow" target="_blank">试剂</a></li><li id="e550" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://ray.readthedocs.io/en/master/rllib.html" rel="noopener ugc nofollow" target="_blank">射线+RLlib </a></li><li id="f04d" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://github.com/google/dopamine" rel="noopener ugc nofollow" target="_blank">多巴胺</a></li><li id="aa68" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://github.com/tensorforce/tensorforce" rel="noopener ugc nofollow" target="_blank">张量力</a></li><li id="dea9" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://github.com/rlgraph/rlgraph" rel="noopener ugc nofollow" target="_blank"> RLgraph </a></li><li id="473d" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://github.com/rlworkgroup/garage" rel="noopener ugc nofollow" target="_blank">车库</a></li><li id="2dfa" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://github.com/VinF/deer" rel="noopener ugc nofollow" target="_blank">鹿</a></li><li id="7a67" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://github.com/deepmind/acme" rel="noopener ugc nofollow" target="_blank">极致</a></li><li id="64f1" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://github.com/openai/baselines" rel="noopener ugc nofollow" target="_blank">基线</a></li><li id="2e12" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://github.com/pfnet/pfrl" rel="noopener ugc nofollow" target="_blank"> PFRL </a></li><li id="3e94" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://github.com/DLR-RM/stable-baselines3" rel="noopener ugc nofollow" target="_blank">稳定基线</a></li></ul><p id="4597" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">决定使用这里列出的RL框架中的哪一个，取决于您的偏好以及您想用它做什么。读者可以通过链接了解更多信息。</p><p id="68f9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们在研究中心使用<a class="ae ma" href="https://github.com/deepmind/acme" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> Acme </strong> </a>进行研究。但是为了描述这些环境中的一个，以便读者可以看到它们提供的可能性，我个人选择了基于<a class="ae ma" href="https://ray.io" rel="noopener ugc nofollow" target="_blank"> Ray </a>的<a class="ae ma" href="https://docs.ray.io/en/master/rllib.html" rel="noopener ugc nofollow" target="_blank"> RLlib </a>，原因我将在下面解释。</p><h1 id="b23f" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">RLlib:使用Ray的可扩展强化学习</h1><p id="287a" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">我们使用<a class="ae ma" href="https://github.com/deepmind/acme" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> Acme </strong> </a>在我们的研究中心做研究。但是为了描述这些环境中的一个，以便读者可以看到它们提供的可能性，我选择了基于<a class="ae ma" href="https://ray.io" rel="noopener ugc nofollow" target="_blank"> Ray </a>的<a class="ae ma" href="https://docs.ray.io/en/master/rllib.html" rel="noopener ugc nofollow" target="_blank"> RLlib </a>，原因我将在下面解释。</p><h2 id="5926" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">计算需求的增长</h2><p id="ed67" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">深度强化学习算法涉及大量模拟，这给深度学习本身的计算复杂性增加了另一个倍增因素。大多数情况下，这是我们在本系列中尚未看到的算法所需要的，例如分布式行动者-批评家方法或多代理方法等。</p><p id="ca35" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">但是，即使找到最佳模型，也常常需要超参数调整和在各种超参数设置中搜索；这可能很昂贵。所有这些都需要由基于异构服务器的分布式系统(具有多核CPU和硬件加速器，如GPU或TPU)的超级计算机提供高计算能力。</p><p id="8bc4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">两年前，当我作为一名作者在Medium上首次亮相时，我已经在文章“<a class="ae ma" rel="noopener" target="_blank" href="/supercomputing-the-heart-of-deep-learning-and-artificial-intelligence-49218c6bdee5">超级计算</a>”中解释了这种类型的基础设施是什么样的。在巴塞罗那，我们现在有一台超级计算机，名为Marenostrum 4，计算能力为13 Petaflops。</p><blockquote class="mb mc md"><p id="deed" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><a class="ae ma" href="https://www.bsc.es" rel="noopener ugc nofollow" target="_blank">巴塞罗那超级计算中心</a>明年将托管一台新的超级计算机Marenostrum 5，它将把计算能力提高x17倍。</p></blockquote><p id="a0a5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">目前的超级计算机MareNostrum 4分为两个不同的硬件模块:一个通用模块和一个基于IBM系统的模块，专为深度学习和人工智能应用而设计。</p><p id="50fd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在硬件方面，Marenostrum的这一部分包括一个54节点集群，基于IBM Power 9和NVIDIA V100，采用Linux操作系统，通过Infiniband网络以每秒100千兆位的速度互连。每个节点都配备了2个IBM POWER9处理器，每个处理器有20个物理核心和512GB内存。这些POWER9处理器中的每一个都连接到两个16GB内存的NVIDIA V100(Volta)GPU，每个节点共有4个GPU。</p><p id="2c4d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如何有效管理这种硬件结构？</p><h2 id="7482" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">系统软件堆栈</h2><p id="3119" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">用分布式和并行系统加速强化学习在管理程序执行的并行化和分布方面引入了几个挑战。为了解决这种日益增长的复杂性，已经开始提出新的软件层，我们在现有的软件层上进行堆叠，试图在逻辑上保持系统的分层<strong class="lf jd">软件栈</strong>的不同组件的分离</p><p id="5f6e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于这个关键的抽象，我们可以专注于不同的软件组件，今天的超级计算机合并，以执行复杂的任务。我想提一下<a class="ae ma" href="https://dl.acm.org/doi/book/10.5555/554797" rel="noopener ugc nofollow" target="_blank">丹尼尔·希利斯</a>，他是开发并行连接机器的思维机器公司的联合创始人，他说<em class="lz">抽象的层次结构是我们理解复杂系统最重要的工具，因为它让我们一次专注于问题的一个方面。</em></p><p id="94f7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我选择的框架RLlib就是这种情况，它遵循分而治之的理念，对软件堆栈进行分层设计。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/146a83ba478f12ef675393d8b839f5c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*u5R7-jCyQQL4pV55T4gpiA.png"/></div><p class="ow ox gj gh gi oy oz bd b be z dk translated">RLlib的软件栈(来源:<a class="ae ma" href="https://docs.ray.io/en/latest/rllib.html" rel="noopener ugc nofollow" target="_blank"> docs.ray.io </a>)</p></figure><p id="d84b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">允许这种功能抽象的这种抽象层次结构是基本的，因为它将让我们操纵信息而不用担心它的底层表示。丹尼尔·希利斯说<em class="lz">一旦我们知道如何完成一个给定的功能，我们就可以把这个机制放在一个“积木”的“黑匣子”里，不再去想它。积木体现的功能可以反复使用，不用参考里面的细节。</em></p><h2 id="bca9" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">光线</h2><p id="8271" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">简而言之，并行和分布式计算是强化学习算法的主要内容。我们需要利用多核和加速器(在多台机器上)来加速RL应用，Python的<a class="ae ma" href="https://docs.python.org/2/library/multiprocessing.html" rel="noopener ugc nofollow" target="_blank">多处理模块</a>不是解决方案。一些RL框架，像<a class="ae ma" href="https://ray.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> Ray </strong> </a>可以很好地处理这个挑战。</p><p id="cfc7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在<a class="ae ma" href="https://docs.ray.io/en/latest/" rel="noopener ugc nofollow" target="_blank">官方项目页面</a>上，Ray被定义为构建和运行分布式应用的快速简单框架:</p><ol class=""><li id="f6d7" class="oh oi it lf b lg lh lj lk lm oj lq ok lu ol ly pa on oo op bi translated">为构建和运行分布式应用程序提供简单的原语。</li><li id="b24a" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly pa on oo op bi translated">支持最终用户并行处理单个机器代码，几乎不需要修改代码。</li><li id="c409" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly pa on oo op bi translated">包括应用程序、库和工具的大型生态系统，以支持复杂的应用程序。</li></ol><p id="4c78" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Ray Core为应用程序构建提供了简单的原语。在Ray Core之上，除了<a class="ae ma" href="https://docs.ray.io/en/latest/rllib.html#rllib-index" rel="noopener ugc nofollow" target="_blank"> RLlib </a>之外，还有其他解决机器学习问题的库:<a class="ae ma" href="https://docs.ray.io/en/latest/tune/index.html" rel="noopener ugc nofollow" target="_blank"> Tune(可扩展超参数调优</a>)、<a class="ae ma" href="https://docs.ray.io/en/latest/raysgd/raysgd.html#sgd-index" rel="noopener ugc nofollow" target="_blank"> RaySGD(分布式训练包装器)</a>、<a class="ae ma" href="https://docs.ray.io/en/latest/serve/index.html#rayserve" rel="noopener ugc nofollow" target="_blank"> Ray Serve(可扩展可编程服务</a>)。</p><h2 id="5389" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">RLlib</h2><p id="62cb" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated"><a class="ae ma" href="https://docs.ray.io/en/latest/rllib.html#rllib-index" rel="noopener ugc nofollow" target="_blank"> RLlib </a>是一个用于强化学习的开源库，它为各种应用程序提供了高可扩展性和统一的API。RLlib原生支持TensorFlow、TensorFlow Eager和PyTorch，但它的大部分内部是框架不可知的。</p><p id="2866" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">目前，这个库已经有了大量的文档(<a class="ae ma" href="https://docs.ray.io/en/latest/rllib-toc.html" rel="noopener ugc nofollow" target="_blank"> API文档</a>)，除了允许创建<a class="ae ma" href="https://docs.ray.io/en/latest/rllib-concepts.html" rel="noopener ugc nofollow" target="_blank">自定义算法</a>之外，还提供了大量的<a class="ae ma" href="https://docs.ray.io/en/latest/rllib-toc.html#algorithms" rel="noopener ugc nofollow" target="_blank">内置算法</a>。</p><p id="87ce" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">RLlib中的关键概念是策略、样本和训练器。简而言之，<strong class="lf jd">策略</strong>是定义代理在环境中如何行为的Python类。RLlib中的所有数据交换都是以<strong class="lf jd">样本</strong>批次的形式进行的，这些样本对轨迹的一个或多个片段进行编码。<strong class="lf jd">训练器</strong>是将上述组件放在一起的样板类，管理算法配置、优化器、训练指标、执行并行组件的工作流等。</p><p id="d505" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在本系列的后面，当我们在分布式和多代理算法方面取得更多进展时，我们将更详细地介绍RLlib的这些关键组件。</p><h2 id="edde" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">TensorFlow或PyTorch</h2><p id="502a" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">在之前的一篇帖子中，<a class="ae ma" rel="noopener" target="_blank" href="/tensorflow-vs-pytorch-the-battle-continues-9dcd34bb47d4"> TensorFlow vs. PyTorch:战斗仍在继续</a>，我展示了深度学习重量级公司TensorFlow和PyTorch之间的战斗正在全面展开。而在这方面，RLlib采取的选项，允许用户在TensorFlow和PyTorch之间无缝切换，用于他们的强化学习工作，似乎也非常合适。</p><p id="1c1e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了允许用户在RLlib中作为后端在TensorFlow和PyTorch之间轻松切换，RLlib包含了“框架”<a class="ae ma" href="https://docs.ray.io/en/master/rllib-training.html#common-parameters" rel="noopener ugc nofollow" target="_blank">训练器配置</a>。例如，要切换到PyTorch版本的算法，我们可以指定<code class="fe pb pc pd pe b">{"framework":"torch"}</code>。在内部，这告诉RLlib尝试对算法使用策略的torch版本(查看<a class="ae ma" href="https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/ppo_tf_policy.py" rel="noopener ugc nofollow" target="_blank"> PPOTFPolicy </a>与<a class="ae ma" href="https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/ppo_torch_policy.py" rel="noopener ugc nofollow" target="_blank"> PPOTorchPolicy </a>的示例)。</p><h1 id="3520" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">用RLlib解决磁极环境问题</h1><p id="e28c" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">现在，我们将展示一个玩具示例来帮助您入门，并向您展示如何使用RLlib使用PPO算法解决OpenAI Gym的<a class="ae ma" rel="noopener" target="_blank" href="/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4"> Cartpole <em class="lz"> </em>环境</a>。PPO是解决增强的局限性的建议之一，由John Schulman等人(2017)在OpenAI的论文“<a class="ae ma" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">近似策略优化算法</a>”中介绍。但是读者可以使用本节中提出的代码来测试任何已经在这个框架中编程的算法。</p><p id="6dc7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这篇文章的<a class="ae ma" href="https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_20_RLlib.ipynb" rel="noopener ugc nofollow" target="_blank">完整代码可以在GitHub </a>上找到，并且<a class="ae ma" href="https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_20_RLlib.ipynb" rel="noopener ugc nofollow" target="_blank">可以使用这个链接</a>作为一个Colab google笔记本运行。</p><blockquote class="mb mc md"><p id="7213" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">警告:鉴于我们正在Colab中执行我们的示例，我们需要在安装ray包并卸载pyarrow后重新启动运行时。</p></blockquote><p id="4e61" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">您可以通过<code class="fe pb pc pd pe b">ray.rllib.agents</code>访问各种算法。<a class="ae ma" href="https://github.com/ray-project/ray/tree/master/rllib/agents" rel="noopener ugc nofollow" target="_blank">在这里</a>，您可以在PyTorch和Tensorflow中找到一长串不同的实现来开始使用。</p><p id="f81c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果您想使用PPO，您可以运行以下代码:</p><pre class="ks kt ku kv gt pf pe pg ph aw pi bi"><span id="b9e5" class="nr na it pe b gy pj pk l pl pm">import ray<br/>from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG</span><span id="8e5c" class="nr na it pe b gy pn pk l pl pm">ray.init()</span></pre><p id="d043" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><code class="fe pb pc pd pe b">ray.init()</code>命令启动所有相关的光线进程。这必须在我们实例化任何RL代理之前完成，例如我们示例中的<code class="fe pb pc pd pe b">PPOTrainer</code>对象:</p><pre class="ks kt ku kv gt pf pe pg ph aw pi bi"><span id="a721" class="nr na it pe b gy pj pk l pl pm">config = DEFAULT_CONFIG.copy()<br/>config["num_gpus"] = 1 # in order to use the GPU<br/>agent = PPOTrainer(config, 'CartPole-v0')</span></pre><p id="8027" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以在一个<code class="fe pb pc pd pe b">config</code>对象中传递许多指定网络和训练过程应该如何配置的超参数。更改超参数就像将它们作为字典传递给<code class="fe pb pc pd pe b">config</code>参数一样简单。查看可用内容的一个快速方法是调用<code class="fe pb pc pd pe b">trainer.config</code>来打印出适用于您选择的算法的选项:</p><pre class="ks kt ku kv gt pf pe pg ph aw pi bi"><span id="3167" class="nr na it pe b gy pj pk l pl pm">print(DEFAULT_CONFIG)</span><span id="d59a" class="nr na it pe b gy pn pk l pl pm">{<br/>‘num_workers’: 2, <br/>‘num_envs_per_worker’: 1, <br/>‘rollout_fragment_length’: 200, <br/>‘num_gpus’: 0, <br/>‘train_batch_size’: 4000, </span><span id="09ec" class="nr na it pe b gy pn pk l pl pm">.<br/>.<br/>.</span><span id="0dc4" class="nr na it pe b gy pn pk l pl pm">}</span></pre><p id="69e0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一旦我们指定了我们的配置，调用我们的<code class="fe pb pc pd pe b">trainer</code>对象上的<code class="fe pb pc pd pe b">train()</code>方法将更新并发送输出到一个名为<code class="fe pb pc pd pe b">results</code>的新字典。</p><pre class="ks kt ku kv gt pf pe pg ph aw pi bi"><span id="a305" class="nr na it pe b gy pj pk l pl pm">result = agent.train()</span></pre><p id="fff8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所有算法都遵循相同的基本结构，从小写缩写到大写缩写，后跟<code class="fe pb pc pd pe b">Trainer</code>。例如，如果您想尝试DQN，您可以拨打:</p><pre class="ks kt ku kv gt pf pe pg ph aw pi bi"><span id="6b5f" class="nr na it pe b gy pj pk l pl pm">from ray.rllib.agents.dqn import DQNTrainer, DEFAULT_CONFIG<br/>agent = <strong class="pe jd">DQNTrainer</strong>(config=DEFAULT_CONFIG, env='CartPole-v0')</span></pre><p id="7497" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">从一个训练有素的代理程序计算动作的最简单方法是使用<code class="fe pb pc pd pe b">trainer.compute_action()</code>:</p><pre class="ks kt ku kv gt pf pe pg ph aw pi bi"><span id="44f9" class="nr na it pe b gy pj pk l pl pm">action=agent.compute_action(state)</span></pre><p id="5eb6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该方法在将观察结果传递给代理策略之前对其进行预处理和过滤。下面是一个如何观察使用<code class="fe pb pc pd pe b">compute_action()</code>的代理的简单示例:</p><pre class="ks kt ku kv gt pf pe pg ph aw pi bi"><span id="aa81" class="nr na it pe b gy pj pk l pl pm">def watch_agent(env):<br/>   state = env.reset()<br/>   rewards = []<br/>   img = plt.imshow(env.render(mode=’rgb_array’))<br/>   for t in range(2000):<br/>       action=agent.compute_action(state)</span><span id="d352" class="nr na it pe b gy pn pk l pl pm">       img.set_data(env.render(mode=’rgb_array’))<br/>       plt.axis(‘off’)<br/>       display.display(plt.gcf())<br/>       display.clear_output(wait=True)<br/> <br/>       state, reward, done, _ = env.step(action)<br/>       rewards.append(reward)<br/>       if done:<br/>          print(“Reward:”, sum([r for r in rewards]))<br/>          break<br/>       env.close()</span></pre><p id="32a9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">使用<code class="fe pb pc pd pe b">watch_agent</code>函数，我们可以比较代理在被训练运行多个更新之前和之后的行为，为给定的数字调用<code class="fe pb pc pd pe b">train()</code>方法:</p><pre class="ks kt ku kv gt pf pe pg ph aw pi bi"><span id="088f" class="nr na it pe b gy pj pk l pl pm">for i in range(10):<br/>   result = agent.train()<br/>   print(f'Mean reward: {result["episode_reward_mean"]:4.1f}')</span></pre><p id="b85e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后一行代码显示了我们如何监控包含在方法<code class="fe pb pc pd pe b">train()</code>的返回中的训练循环打印信息。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi po"><img src="../Images/b9c090398c0bee8019e9f142464c5cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*stBWj1KUV23u60nRkVyndA.png"/></div></div><p class="ow ox gj gh gi oy oz bd b be z dk translated">训练前</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pp"><img src="../Images/88c09b29cf1a541352bcc7f418294d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-UM3E9i0A61mINcMnHHlJA.png"/></div></div><p class="ow ox gj gh gi oy oz bd b be z dk translated">训练后</p></figure><p id="35be" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是一个简单算法的玩具实现，非常简单地展示了这个框架。RLlib框架的实际价值在于它在执行固有并行的大型基础设施中的使用，同时，从头开始编写代码的复杂算法是完全不可行的。</p><p id="2ee1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">正如我所说的，在看了上面提到的所有其他框架之后，我选择了RLlib。原因是多样的；有些已经在本帖中介绍过了。给我加上那个；相关的是，它已经被包括在主要的云提供商中，如<a class="ae ma" href="https://docs.aws.amazon.com/sagemaker/latest/dg/reinforcement-learning.html" rel="noopener ugc nofollow" target="_blank"> AWS </a>和<a class="ae ma" href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-reinforcement-learning" rel="noopener ugc nofollow" target="_blank"> AzureML </a>。或者有一个像<a class="ae ma" href="https://anyscale.com/" rel="noopener ugc nofollow" target="_blank"> ANYSCALE </a>这样的推动公司已经筹集了<a class="ae ma" href="https://anyscale.com/blog/page/3/" rel="noopener ugc nofollow" target="_blank">2000万</a>并组织了<a class="ae ma" href="https://events.linuxfoundation.org/ray-summit/?utm_source=ion&amp;utm_medium=blog&amp;utm_campaign=ray_summit" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">射线峰会</strong> </a> <strong class="lf jd">会议</strong>，该会议将于本周(9月30日至10月1日)在网上举行，并有伟大的演讲者(如我们的朋友<a class="ae ma" href="https://scholar.google.com/citations?user=NkzyCvUAAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank">Oriol Vinyals</a>；-).也许可以添加更多的背景细节，但对我来说，与上述原因同样重要的是，有来自加州大学伯克利分校的伟大研究人员<a class="ae ma" href="https://arxiv.org/abs/1712.09381" rel="noopener ugc nofollow" target="_blank">参与其中，包括有远见的</a><a class="ae ma" href="https://people.eecs.berkeley.edu/~istoica/" rel="noopener ugc nofollow" target="_blank">扬·斯托伊察</a>，我在Spark的问题上见过他，他们显然是对的！</p><h1 id="e653" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">从这里去哪里</h1><p id="5d83" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">我希望这些帖子能够鼓励读者了解这项令人兴奋的技术，它是人工智能领域最新突破性进展的真正推动者。我们真的没有看到太多，只是最基本的，让你继续自己发现和享受这个神话般的世界。以下参考资料可能有用。</p><h2 id="8a04" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">书</h2><ul class=""><li id="6e46" class="oh oi it lf b lg oc lj od lm pq lq pr lu ps ly om on oo op bi translated">理查德·萨顿和安德鲁·巴尔托。<a class="ae ma" href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition" rel="noopener ugc nofollow" target="_blank"> <em class="lz">强化学习:导论</em> </a>，麻省理工出版社，2018。<a class="ae ma" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank">【incompleteideas.net在线】</a>。Python代码的相关独立<a class="ae ma" href="https://github.com/Pulkit-Khandelwal/Reinforcement-Learning-Notebooks" rel="noopener ugc nofollow" target="_blank">回购。</a></li><li id="e355" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated">米格尔·莫拉莱斯。<a class="ae ma" href="https://www.manning.com/books/grokking-deep-reinforcement-learning" rel="noopener ugc nofollow" target="_blank"> <em class="lz">摸索深度强化学习</em> </a>。曼宁，2020。</li><li id="d799" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated">亚历山大·宰和布兰登·布朗。<a class="ae ma" href="https://www.manning.com/books/deep-reinforcement-learning-in-action" rel="noopener ugc nofollow" target="_blank"> <em class="lz">深度强化学习在行动</em> </a>。曼宁，2020。</li><li id="a1b8" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated">马克西姆·拉潘。<a class="ae ma" href="https://www.packtpub.com/data/deep-reinforcement-learning-hands-on-second-edition" rel="noopener ugc nofollow" target="_blank"> <em class="lz">深度强化学习</em> </a>。帕克特出版有限公司，第二版，2020年。</li><li id="8132" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated">迈克尔·兰哈姆。<a class="ae ma" href="https://www.packtpub.com/product/hands-on-reinforcement-learning-for-games/9781839214936" rel="noopener ugc nofollow" target="_blank"> <em class="lz">游戏动手强化学习</em> </a>。帕克特出版社，2020年。</li><li id="b67e" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated">菲尔·温德。<a class="ae ma" href="https://www.oreilly.com/library/view/reinforcement-learning/9781492072386/" rel="noopener ugc nofollow" target="_blank"> <em class="lz">强化学习、工业应用和智能代理</em> </a>。奥瑞丽媒体公司，2020年。</li><li id="ee9d" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated">Sudharsan Ravichandiran。<a class="ae ma" href="https://www.packtpub.com/product/deep-reinforcement-learning-with-python-second-edition/9781839210686" rel="noopener ugc nofollow" target="_blank"> <em class="lz">深度强化学习用Python </em> </a>。帕克特出版有限公司，第二版，2020年。</li></ul><h2 id="3d4f" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">在线资源</h2><ul class=""><li id="87b2" class="oh oi it lf b lg oc lj od lm pq lq pr lu ps ly om on oo op bi translated"><a class="ae ma" href="https://www.davidsilver.uk/teaching/" rel="noopener ugc nofollow" target="_blank">大卫·西尔弗在RL上的UCL课程</a></li><li id="dd58" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated">由谢尔盖·莱文创作的加州大学伯克利分校CS 285</li><li id="7801" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated">哈多·范·哈瑟尔特的《UCL》</li><li id="680d" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="http://web.stanford.edu/class/cs234/index.html" rel="noopener ugc nofollow" target="_blank">斯坦福CS234，作者艾玛·布伦斯基尔</a></li><li id="8e61" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://www.cs.upc.edu/~mmartin/url-RL.html" rel="noopener ugc nofollow" target="_blank">马里奥·马丁的UPC巴塞罗那理工学院</a></li><li id="cfaa" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/schedule.html" rel="noopener ugc nofollow" target="_blank">滑铁卢大学CS 885，作者帕斯卡·普帕特</a></li><li id="d029" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated">彼得·阿比尔等人的伯克利DRL训练营</li><li id="9d98" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://telecombcn-dl.github.io/drl-2020/" rel="noopener ugc nofollow" target="_blank">Xavier giróI-Nieto等人的UPC巴塞罗那技术公司</a></li><li id="4030" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated">Josh Achiam的OpenAI</li><li id="5d1c" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html" rel="noopener ugc nofollow" target="_blank">莉莲翁:一(长)窥强化学习</a></li><li id="c930" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://paperswithcode.com/methods/area/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">证件代码</a></li><li id="2ced" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://www.alexirpan.com/2018/02/14/rl-hard.html" rel="noopener ugc nofollow" target="_blank"> Alex Irpan:有点见地:深度强化学习还没起作用</a></li><li id="bb79" class="oh oi it lf b lg oq lj or lm os lq ot lu ou ly om on oo op bi translated"><a class="ae ma" href="https://karpathy.github.io/2016/05/31/rl/" rel="noopener ugc nofollow" target="_blank"> Andrej Karpathy博客:从像素进行深度强化学习</a></li></ul></div><div class="ab cl pt pu hx pv" role="separator"><span class="pw bw bk px py pz"/><span class="pw bw bk px py pz"/><span class="pw bw bk px py"/></div><div class="im in io ip iq"><h1 id="950f" class="mz na it bd nb nc qa ne nf ng qb ni nj ki qc kj nl kl qd km nn ko qe kp np nq bi translated">深度强化学习讲解系列</h1><p id="c09a" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated"><strong class="lf jd">由</strong> <a class="ae ma" href="https://www.upc.edu/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> UPC巴塞罗那理工</strong> </a> <strong class="lf jd">和</strong> <a class="ae ma" href="https://www.bsc.es/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">巴塞罗那超级计算中心</strong> </a></p><p id="4cf4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一个轻松的介绍性<a class="ae ma" href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener ugc nofollow" target="_blank">系列</a>以一种实用的方式逐渐向读者介绍这项令人兴奋的技术，它是人工智能领域最新突破性进展的真正推动者。</p><div class="mh mi gp gr mj mk"><a href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">深度强化学习解释-乔迪托雷斯。人工智能</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">本系列的内容</h3></div></div><div class="mt l"><div class="qf l mv mw mx mt my lb mk"/></div></div></a></div><h2 id="89c0" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">关于这个系列</h2><p id="87a4" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">我在五月份开始写这个系列，那是在巴塞罗那的封锁期。老实说，由于封锁，在业余时间写这些帖子帮助了我<a class="ae ma" href="https://twitter.com/hashtag/StayAtHome?src=hashtag_click" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> #StayAtHome </strong> </a>。感谢您当年阅读这份刊物；它证明了我所做的努力。</p><p id="d9d0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">免责声明</strong> —这些帖子是在巴塞罗纳封锁期间写的，目的是分散个人注意力和传播科学知识，以防对某人有所帮助，但不是为了成为DRL地区的学术参考文献。如果读者需要更严谨的文档，本系列的最后一篇文章提供了大量的学术资源和书籍供读者参考。作者意识到这一系列的帖子可能包含一些错误，如果目的是一个学术文件，则需要对英文文本进行修订以改进它。但是，尽管作者想提高内容的数量和质量，他的职业承诺并没有留给他这样做的自由时间。然而，作者同意提炼所有那些读者可以尽快报告的错误。</p><h2 id="a1dc" class="nr na it bd nb ns nt dn nf nu nv dp nj lm nw nx nl lq ny nz nn lu oa ob np iz bi translated">我们在DRL的研究</h2><p id="4c63" class="pw-post-body-paragraph ld le it lf b lg oc kd li lj od kg ll lm oe lo lp lq of ls lt lu og lw lx ly im bi translated">我们在<a class="ae ma" href="https://www.upc.edu/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> UPC巴塞罗那理工</strong> </a>和<a class="ae ma" href="https://www.bsc.es/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">巴塞罗那超级计算中心</strong> </a>的课题组正在做这个课题的研究。我们在这一领域的最新论文是在第37届国际机器学习会议(ICML2020) 上发表的“<a class="ae ma" href="https://arxiv.org/pdf/2002.03647.pdf" rel="noopener ugc nofollow" target="_blank">探索、发现和学习:状态覆盖技能的无监督发现</a>”。提出了一种新的强化学习无监督技能发现范式。是我们博士生之一的<a class="ae ma" href="https://twitter.com/vcampos7" rel="noopener ugc nofollow" target="_blank"> @vcampos7 </a>和<a class="ae ma" href="https://twitter.com/DocXavi" rel="noopener ugc nofollow" target="_blank"> @DocXavi </a>共同建议的最后一篇投稿。本文由来自<a class="ae ma" href="https://einstein.ai/" rel="noopener ugc nofollow" target="_blank"> Salesforce Research </a>的<a class="ae ma" href="https://twitter.com/alexrtrott" rel="noopener ugc nofollow" target="_blank"> @alexrtrott </a>、<a class="ae ma" href="https://twitter.com/CaimingXiong" rel="noopener ugc nofollow" target="_blank">@蔡明雄</a>、<a class="ae ma" href="https://twitter.com/RichardSocher" rel="noopener ugc nofollow" target="_blank"> @RichardSocher </a>共同撰写。</p></div></div>    
</body>
</html>