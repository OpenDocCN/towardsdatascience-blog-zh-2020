<html>
<head>
<title>An Introduction to Building Pipelines and Using Grid Searches in Scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Scikit-learn中构建管道和使用网格搜索的介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-building-pipelines-and-using-grid-searches-in-scikit-learn-92ea72f9b5b7?source=collection_archive---------19-----------------------#2020-11-02">https://towardsdatascience.com/an-introduction-to-building-pipelines-and-using-grid-searches-in-scikit-learn-92ea72f9b5b7?source=collection_archive---------19-----------------------#2020-11-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/bb8d63fd79ac46b5de8110aa546c74b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JxzAjC5M5xEK4c1h-LYxXA.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片鸣谢:https://unsplash.com/photos/adJNLL2CLM4阿林-安德森<a class="ae kc" href="https://unsplash.com/photos/adJNLL2CLM4" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="a290" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">构建最基本的机器学习模型也需要几个步骤。必须选择特征，需要标准化数据，必须确定要使用的估计器的类型，然后使其适合训练数据。一旦我们有了一个工作模型，下一步就是寻找和优化参数。</p><p id="1135" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">模型涉及两类参数:<br/> i) <strong class="kf ir">模型参数</strong>:模型内部的配置变量，可以从数据中估计出来；以及，<br/> ii) <strong class="kf ir">模型超参数</strong>:模型外部的配置变量，无法从数据中估计。<a class="ae kc" href="https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/" rel="noopener ugc nofollow" target="_blank">【1】</a><br/>在整个建模过程中，有许多步骤需要提供超参数。这些参数必须由外部提供，调整它们是开发模型的一个重要部分。这通常被称为超参数调整，涉及数据科学家优化这些参数以提高性能。</p><p id="c46a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">建立模型是一个迭代的过程。正如人们可以想象的那样，这个过程很容易变得乏味、笨拙和容易出错。幸运的是，Scikit-learn有一套很好的工具来解决这个问题:pipeline和gridsearch。本文的目标是演示这些工具的用法。在我们继续之前，重要的是要提到本文的其余部分不是回归分析的练习。这里的分析工作只是展示Scikit-learn工具的一个工具。说完了，让我们开始吧。</p><p id="02b4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像往常一样，我们从导入必要的库开始。在我们的模型中，我们将构建一个管道，为此我们需要Scikit-learn中的<code class="fe lb lc ld le b">sklearn.pipeline</code>模块中的<code class="fe lb lc ld le b">make_pipeline</code>方法。<code class="fe lb lc ld le b">GridSearchCV</code>，我们将使用的另一个方法，来自同一个Scikit-learn库的<code class="fe lb lc ld le b">model_selection</code>模块。我们将使用Seaborn附带的企鹅数据集。</p><pre class="lf lg lh li gt lj le lk ll aw lm bi"><span id="8677" class="ln lo iq le b gy lp lq l lr ls">#import libraries<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns;<br/>import matplotlib.pyplot as plt<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.pipeline import make_pipeline<br/>from sklearn.feature_selection import SelectKBest, f_regression<br/>from sklearn.model_selection import train_test_split,GridSearchCV<br/>from sklearn.linear_model import Ridge, Lasso</span><span id="18d2" class="ln lo iq le b gy lt lq l lr ls">#loading dataset<br/>penguins=sns.load_dataset('penguins')</span><span id="82cd" class="ln lo iq le b gy lt lq l lr ls">#looking at a snapshot of newly loaded data<br/>penguins.info()</span><span id="096c" class="ln lo iq le b gy lt lq l lr ls">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 344 entries, 0 to 343<br/>Data columns (total 7 columns):<br/> #   Column             Non-Null Count  Dtype  <br/>---  ------             --------------  -----  <br/> 0   species            344 non-null    object <br/> 1   island             344 non-null    object <br/> 2   bill_length_mm     342 non-null    float64<br/> 3   bill_depth_mm      342 non-null    float64<br/> 4   flipper_length_mm  342 non-null    float64<br/> 5   body_mass_g        342 non-null    float64<br/> 6   sex                333 non-null    object <br/>dtypes: float64(4), object(3)<br/>memory usage: 18.9+ KB</span></pre><p id="4eae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了说明构建管道的过程，我将使用回归来估计身体质量，这是一个连续的数字变量。让我们选择3列数字数据(<code class="fe lb lc ld le b">bill length</code>、<code class="fe lb lc ld le b">bill depth</code>、<code class="fe lb lc ld le b">flipper length</code>)和2列分类数据(<code class="fe lb lc ld le b">sex</code>、<code class="fe lb lc ld le b">species</code>)作为特征。<code class="fe lb lc ld le b">sex</code>列有一些值为空的行，我们将删除它们。</p><pre class="lf lg lh li gt lj le lk ll aw lm bi"><span id="9dca" class="ln lo iq le b gy lp lq l lr ls">#dropping rows with NA or NaN values<br/>penguins.dropna(inplace=True)</span></pre><p id="6bf1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们创建一个特征矩阵(<strong class="kf ir"> X </strong>)和一个带有目标变量的向量(<strong class="kf ir"> y </strong>)。由于<code class="fe lb lc ld le b">sex</code>是一个分类变量，我们还需要在将矩阵分成训练集和测试集之前对它们进行虚拟化。</p><pre class="lf lg lh li gt lj le lk ll aw lm bi"><span id="67f3" class="ln lo iq le b gy lp lq l lr ls">#creating a feature matrix X<br/>X=penguins.drop(['island','body_mass_g'],axis=1)</span><span id="c4cc" class="ln lo iq le b gy lt lq l lr ls">#Quick look at columns with categorical variable<br/>X[['species','sex']]</span></pre><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="ab gu cl lu"><img src="../Images/22acdac23891d69ce6056f636eb7b6fc.png" data-original-src="https://miro.medium.com/v2/format:webp/1*QcWk-ITFHZQYETSCnViojg.png"/></div></figure><pre class="lf lg lh li gt lj le lk ll aw lm bi"><span id="645f" class="ln lo iq le b gy lp lq l lr ls">#Let's pass 'species' column to OneHotEncoder<br/>X=pd.get_dummies(data=X,columns=['species','sex'],drop_first=True)</span><span id="cc48" class="ln lo iq le b gy lt lq l lr ls">y=penguins['body_mass_g']</span><span id="6e5a" class="ln lo iq le b gy lt lq l lr ls">#Split data into training and test sets<br/>X_train,X_test,y_train,y_test=train_test_split(X,y)</span><span id="4330" class="ln lo iq le b gy lt lq l lr ls">#taking a quick look at the newly created training set<br/>X_train.head(2)</span></pre><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="ab gu cl lu"><img src="../Images/2df2a955ecbad80da58e60d93225980c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*k6oX5alU6WpdtFiyNtXraw.png"/></div></figure><p id="500b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们终于准备好建立一个管道，并建立一个模型。Scikit-learn中的<code class="fe lb lc ld le b">pipeline</code>模块有一个<code class="fe lb lc ld le b">make-pipeline</code>方法。第一步是实例化该方法。为此，我们按顺序向它传递我们希望输入数据经历的步骤。实例化的管道就像任何其他Scikit-learn估计器一样工作。下面的代码是构建新管道的示例。我们用一个不太有创意的名字命名新创建的管道:pipe。</p><pre class="lf lg lh li gt lj le lk ll aw lm bi"><span id="3e0d" class="ln lo iq le b gy lp lq l lr ls">#Setting up a pipeline<br/>pipe=make_pipeline(StandardScaler(),SelectKBest(f_regression),Ridge())</span></pre><p id="8832" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">指定为<code class="fe lb lc ld le b">make_pipeline</code>的参数的方法，从左到右依次是:<br/>I)<code class="fe lb lc ld le b">StandardScaler()</code>-&gt;<code class="fe lb lc ld le b">train_test_split</code>之前的数据我们还没有标准化。因此，在对传入的数据执行任何其他转换之前，需要对其进行标准化。<br/> ii) <code class="fe lb lc ld le b">SelectKBest()</code> - &gt;这个方法来自Scikit-learn的<code class="fe lb lc ld le b">feature_selection</code>模块。它根据指定的评分函数(在本例中为<code class="fe lb lc ld le b">f_regression)</code>)选择最佳特征。特征的数量由参数<code class="fe lb lc ld le b">k</code>的值指定。即使在选定的特性中，我们也希望改变提供给模型的最终特性集，并找到性能最佳的特性。我们可以用<code class="fe lb lc ld le b">GridSearchCV</code>方法做到这一点，我很快就会回来。<br/> iii) <code class="fe lb lc ld le b">Ridge()</code> - &gt;这是一个执行实际回归的估计器。该方法的名称是指Tikhonov正则化，通常称为岭回归，用于减少多重共线性的影响。像前面讨论的参数k一样，我们想要测试岭回归的各种参数的几个不同值。我们这样做是网格搜索的一部分，这将在下面讨论。</p><p id="5901" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的管道现在可以安装了。正如我前面提到的，实例化管道的行为就像任何其他估计器一样。它可以接受作为管道一部分的每个方法的参数。获取管道可以接受的参数列表的快速方法如下所示:</p><pre class="lf lg lh li gt lj le lk ll aw lm bi"><span id="7d01" class="ln lo iq le b gy lp lq l lr ls">#Looking up parameters that can be passed to the pipeline<br/>pipe.get_params().keys()</span><span id="ca08" class="ln lo iq le b gy lt lq l lr ls">Output:<br/>dict_keys(['memory', 'steps', 'verbose', 'standardscaler', 'selectkbest', 'ridge', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'selectkbest__k', 'selectkbest__score_func', 'ridge__alpha', 'ridge__copy_X', 'ridge__fit_intercept', 'ridge__max_iter', 'ridge__normalize', 'ridge__random_state', 'ridge__solver', 'ridge__tol'])</span></pre><p id="d86e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">网格搜索使我们能够搜索上面列出的每个参数的指定值。我们通过向<code class="fe lb lc ld le b">GridSearchCV</code>传递一个字典来实现这一点，该字典将参数名作为键，并将值列表作为这些参数的参数。在这个例子中，我调用这个字典<code class="fe lb lc ld le b">params</code>并将其传递给<code class="fe lb lc ld le b">GridSearchCV</code>。一旦安装好，<code class="fe lb lc ld le b">GridSearchCV</code>实例<code class="fe lb lc ld le b">gs</code>，就像任何其他估计器一样工作。</p><p id="a823" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为旁注，我想强调一个可选但非常有用的参数:n_jobs。它告诉sci kit-了解并行运行多少个作业。将其设置为-1相当于指示Scikit-learn使用所有可用的处理器。现在，大多数CPU都有不止一个内核。如果您有一个四核处理器，使用所有4核而不是1核可以使处理速度明显加快。</p><pre class="lf lg lh li gt lj le lk ll aw lm bi"><span id="55e2" class="ln lo iq le b gy lp lq l lr ls">#putting together a parameter grid to search over using grid search<br/>params={<br/>    'selectkbest__k':[1,2,3,4,5,6],<br/>    'ridge__fit_intercept':[True,False],<br/>    'ridge__alpha':[5,10],<br/>    'ridge__solver':[ 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag',<br/>'saga']<br/>}</span><span id="03e4" class="ln lo iq le b gy lt lq l lr ls">#setting up the grid search<br/>gs=GridSearchCV(pipe,params,n_jobs=-1,cv=5)</span><span id="17db" class="ln lo iq le b gy lt lq l lr ls">#fitting gs to training data<br/>gs.fit(X_train, y_train)</span></pre><p id="1e98" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GridSearchCV不是搜索所选参数的所有排列，而是对训练数据进行交叉验证。默认值是5倍，但是我们可以使用参数<code class="fe lb lc ld le b">cv</code>指定任何其他数值。属性<code class="fe lb lc ld le b">cv_results_</code>包括每次交叉验证运行的详细结果，并提供大量数据，可用于确定新开发模型的拟合和稳健性。性能最佳的参数排列的细节由<code class="fe lb lc ld le b">best_params_</code>属性提供。</p><pre class="lf lg lh li gt lj le lk ll aw lm bi"><span id="2eff" class="ln lo iq le b gy lp lq l lr ls">#building a dataframe from cross-validation data<br/>df_cv_scores=pd.DataFrame(gs.cv_results_).sort_values(by='rank_test_score')</span><span id="aa61" class="ln lo iq le b gy lt lq l lr ls">#selecting specific columns to create a view<br/>df_cv_scores[['params','split0_test_score', 'split1_test_score', 'split2_test_score',\<br/>       'split3_test_score', 'split4_test_score', 'mean_test_score',\<br/>       'std_test_score', 'rank_test_score']].head()</span></pre><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="ab gu cl lu"><img src="../Images/c89307741ecb82b9952d4dde42d8a328.png" data-original-src="https://miro.medium.com/v2/format:webp/1*HhyNB2B7OSq4aYRx9iSYcA.png"/></div></figure><pre class="lf lg lh li gt lj le lk ll aw lm bi"><span id="3268" class="ln lo iq le b gy lp lq l lr ls">#checking the selected permutation of parameters<br/>gs.best_params_</span><span id="b098" class="ln lo iq le b gy lt lq l lr ls">Output:<br/>{'ridge__alpha': 5,<br/> 'ridge__fit_intercept': True,<br/> 'ridge__solver': 'sag',<br/> 'selectkbest__k': 6}</span></pre><p id="bf13" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们可以通过将其特征矩阵传递给<code class="fe lb lc ld le b">gs</code>来预测测试集的目标值。预测值可以与实际目标值进行比较，以可视化和传达模型的性能。</p><pre class="lf lg lh li gt lj le lk ll aw lm bi"><span id="41a3" class="ln lo iq le b gy lp lq l lr ls">#checking how well the model does on the holdout-set<br/>gs.score(X_test,y_test)</span><span id="74db" class="ln lo iq le b gy lt lq l lr ls">Output:<br/>0.8707008994531131</span><span id="90d1" class="ln lo iq le b gy lt lq l lr ls">#plotting predicted body weights vs actual body weights of penguins<br/>y_preds=gs.predict(X_test)<br/>plt.scatter(y_test,y_preds);</span></pre><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/6d4d49df90262291f7d80141f1294e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*JgaCi9QkPkAqZ643GLVbmw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">企鹅的预测体重与实际体重(克)</p></figure><p id="f1e5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个非常简单的模型。但是，即使在这种简单程度上，管道和网格搜索的有效性也是显而易见的。也许，不太明显的是，在建立管道并将其放入网格搜索时，批判性思维的价值。这方面的一个例子是特征选择。假设我们要在一个分类问题中选择k个特征。在网格搜索之外，我们可以随机选择k个特征，而不需要考虑过程。然而，在网格搜索中进行特征选择迫使人们考虑可用的方法/算法、它们适用的问题以及它们的优点和缺点。因此，可以说，使用管道和网格搜索可以使一个人成为更好的数据科学家。这些技术的一个注意事项是，要构建的模型数量随着每个额外参数值的增加而成倍增加。因此，与其在大量可能的参数值中进行网格搜索，不如每次在每个参数的少量值中进行搜索，然后改变范围，重新进行网格搜索。一旦进入期望的范围，我们就可以放大以找到最佳值。如果使用得当，在处理未知的复杂系统时，它们是真正的“救命稻草”。如果你还没有用过它们，我强烈推荐它们。</p></div></div>    
</body>
</html>