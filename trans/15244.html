<html>
<head>
<title>How to build an encoder decoder translation model using LSTM with Python and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何通过Python和Keras使用LSTM构建一个编码器解码器翻译模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b?source=collection_archive---------4-----------------------#2020-10-20">https://towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b?source=collection_archive---------4-----------------------#2020-10-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3ca8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">按照这个逐步指南建立一个编码器解码器模型，并创建自己的翻译模型</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/189ec4595ea277aefe6a38d65c1a836e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gRPLlwKQEQX4Vbkj9G88tQ.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">由<a class="ae kz" href="https://unsplash.com/@lazycreekimages?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">迈克尔·泽兹奇</a>在<a class="ae kz" href="https://unsplash.com/@lazycreekimages?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="50e8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="lw">先决条件:理解本文之前关于</em><a class="ae kz" href="https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnn-c2374305a630" rel="noopener"><em class="lw">【RNN】</em></a><em class="lw"/><a class="ae kz" rel="noopener" target="_blank" href="/what-is-an-encoder-decoder-model-86b3d57c5e1a"><em class="lw">编码器解码器</em> </a> <em class="lw">的知识是有价值的。</em></p><p id="3735" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">本文是关于如何使用Python和Keras开发编码器/解码器模型的实用指南，更准确地说是Seq2Seq。在上一个教程中，我们开发了一个类似于下图的多对多翻译模型:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi lx"><img src="../Images/4aa271a733e6d6078422d36290833e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GTMiqDZGc4yzF6xZF366ZA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">多对多RNN。作者提供的图片</p></figure><p id="4655" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这种结构有一个<strong class="lc iu">重要的限制，</strong>序列长度。正如我们在图中看到的，<strong class="lc iu">输入序列和输出序列必须具有相同的长度。如果我们需要不同的长度怎么办？例如，具有不同序列长度的模型是接收单词序列并输出数字的情感分析，或者输入是图像而输出是单词序列的图像字幕模型。</strong></p><p id="d38c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果我们想要开发输入和输出长度不同的模型，我们需要开发一个编码器解码器模型。通过本教程，我们将看到如何开发模型，将其应用于翻译练习。模型的表示如下所示。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ly"><img src="../Images/146590d3fcbcedbf3ca145b5c30daa5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NhzTb2Wcp4rQThL3J_TzgA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">编码器解码器结构。作者图片</p></figure><p id="4be9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">我们将模型分成两部分，</strong>首先，我们有一个<strong class="lc iu">编码器</strong>，它输入西班牙语句子并产生一个隐藏向量。编码器由一个嵌入层和一个递归神经网络(RNN)构成，嵌入层将单词转换为矢量，递归神经网络计算隐藏状态，这里我们将使用长短期记忆(LSTM)层。</p><p id="8d34" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后编码器的输出将被用作<strong class="lc iu">解码器</strong>的输入。对于解码器，我们将再次使用LSTM层，以及预测英语单词的密集层。</p><h1 id="185e" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">亲自动手</h1><p id="a123" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">样本数据可以在<a class="ae kz" href="http://www.manythings.org/bilingual/" rel="noopener ugc nofollow" target="_blank">manythings.org</a>下载，来自<a class="ae kz" href="https://tatoeba.org/spa" rel="noopener ugc nofollow" target="_blank">塔图埃巴</a>。它由你需要的语言中的句子对组成。在我们的例子中，我们将使用<a class="ae kz" href="http://www.manythings.org/anki/spa-eng.zip" rel="noopener ugc nofollow" target="_blank">西班牙语-英语组合</a>。</p><p id="9d7b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了建立模型，我们需要做的第一件事是预处理数据，并获得西班牙语和英语句子的最大长度。</p><h2 id="f1b3" class="mw ma it bd mb mx my dn mf mz na dp mj lj nb nc ml ln nd ne mn lr nf ng mp nh bi translated">1-预处理</h2><p id="65da" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated"><em class="lw">先决条件:理解Keras中的类“tokenizer”和“pad_sequences”。如果你想详细回顾它们，我们在</em> <a class="ae kz" rel="noopener" target="_blank" href="/how-to-build-a-translation-pipeline-with-rnn-and-keras-57c1cf4a8a7"> <em class="lw">之前的教程中已经讨论过这个主题。</em>T11】</a></p><p id="4e67" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">首先，我们将导入库，然后读取下载的数据。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="7dd9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一旦我们读取了数据，我们将保留第一个例子，以便更快地训练。如果我们想开发一个更高性能的模型，我们需要使用完整的数据集。然后，我们必须通过删除大写字母和标点符号来清理数据。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="6f35" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">接下来，我们对句子进行分词并分析数据。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="e3e1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">创建完函数后，我们可以进行预处理:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="54f8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">上面的代码打印出以下结果</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/bc4b9f37f9723ba4c0135570e0b44127.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*rwQES7EeECdafuJl9ft5Yg.png"/></div></figure><p id="b8f2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">根据之前的代码，西班牙语句子的最大长度为12个单词，英语句子的最大长度为6个单词。在这里，我们可以看到使用编码器/解码器模型的优势，以前我们有处理等长句子的限制，所以我们需要对多达12个英文句子应用填充，<strong class="lc iu">现在是一半。</strong>因此，更重要的是，它还减少了LSTM时间步长的数量，从而降低了计算需求和复杂性。</p><p id="b546" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们应用填充以使每种语言中句子的最大长度相等。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="e25b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在我们已经准备好数据，让我们建立模型。</p><h1 id="e4a3" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">双模型开发</h1><p id="afd7" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">在接下来的部分中，我们将创建模型，并解释我们在python代码中添加的每一层。</p><h2 id="e7ce" class="mw ma it bd mb mx my dn mf mz na dp mj lj nb nc ml ln nd ne mn lr nf ng mp nh bi translated">2.1-编码器</h2><p id="6c7c" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">正如我们在模型图像中看到的，要定义的第一层是嵌入层。为此，我们首先必须添加一个<strong class="lc iu">输入层，</strong>这里要考虑的唯一参数是<strong class="lc iu">“形状”，</strong>这是西班牙语句子的最大长度<strong class="lc iu">，在我们的例子中是12。</strong></p><p id="1a7d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后我们将它连接到<a class="ae kz" rel="noopener" target="_blank" href="/introduction-to-word-embedding-and-word2vec-652d0c2060fa">嵌入层</a>，这里要考虑的参数是<strong class="lc iu">‘input _ dim’</strong>，它是西班牙语词汇的<strong class="lc iu">长度，以及<strong class="lc iu">‘output _ dim’</strong>，它是嵌入向量</strong>的<strong class="lc iu">形状。该层会将任何西班牙语单词转换为输出维度形状的矢量。</strong></p><p id="63b6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这背后的概念是以空间表示的形式提取单词的含义，其中每个维度将是定义单词的特征。例如，世界“sol”将被转换成形状为128的矢量。输出维度越高，从每个单词中提取的语义就越多，但所需的计算和处理时间也就越长。<strong class="lc iu">需要在速度和性能之间找到平衡。</strong></p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="941a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">接下来我们将添加大小为64的LSTM图层</strong>。尽管LSTM的每个时间步都输出一个隐藏向量，我们将注意力集中在最后一个上，因此参数<strong class="lc iu"> return_sequences为‘False’。</strong>我们将看到解码器在return_sequences=True的情况下LSTM层是如何工作的。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/80418f61a1748e5489b65241b7d8f0ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*7eMDChm9Q-VR3ZpMWY7N8w.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">当return_sequences为“假”时，输出是最后一个隐藏状态。作者图片</p></figure><h2 id="d38f" class="mw ma it bd mb mx my dn mf mz na dp mj lj nb nc ml ln nd ne mn lr nf ng mp nh bi translated">2.2-解码器</h2><p id="ea7e" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">编码器层的输出将是最后一个时间步长的隐藏状态。然后我们需要将这个向量输入解码器。让我们更精确地看看解码器部分，了解它是如何工作的。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nm"><img src="../Images/a46a468816652f1f78cdb83c1bb5d692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n6AK9f09yZQdvC2c7P5LNQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">解码器在每个时间步长接收相同的输入。作者图片</p></figure><p id="bbb2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">正如我们在图像中看到的，隐藏向量重复了n次，因此LSTM <strong class="lc iu">的每个时间步长都接收到相同的向量</strong>。为了让每个时间步都有相同的矢量，我们需要使用层RepeatVector，顾名思义，它的作用是重复它接收的矢量，我们需要定义的唯一参数是n，即重复的次数。这个数等于解码器部分的时间步长数，换句话说，等于最大英语句子长度6。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="4014" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一旦我们准备好输入，我们将继续与解码器。这也是用LSTM层构建的，不同的是参数<strong class="lc iu"> return_sequences，在这种情况下为“真”。</strong>这个参数是干什么用的？在编码器部分，我们期望在最后一个时间步只有一个向量，忽略所有其他的，这里<strong class="lc iu">我们期望在每个时间步</strong>有一个输出向量，这样密集层可以进行预测。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nn"><img src="../Images/2fe31be18c63e0e11b6fa5bd35b7bf7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uvAcuxfQb_xMgJJTG1V2AQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">解码器:LSTM计算密集层的输入。作者图片</p></figure><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="0580" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们还有最后一步，预测翻译的单词。为此，我们需要使用一个密集层。我们需要定义的参数是单位的数量，这个单位的数量是输出向量的形状，它需要与英语词汇的长度相同。为什么？<strong class="lc iu">向量将是所有接近零的值，除了接近1 </strong>的一个单位。然后，我们需要将输出1的单元的索引映射到一个字典中，我们将每个单元映射到一个单词。例如，如果输入是单词“sol ”,输出是一个向量，其中所有都是零，那么单元472是1，我们将这个索引映射到包含英语单词的字典，并且我们得到值“sun”。</p><p id="6298" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们刚刚看到了如何应用密集层并预测一个单词，但我们如何对整个句子进行预测呢？因为我们使用return_sequence=True，LSTM层在每个时间步输出一个向量，所以我们需要在每个时间步应用前面解释的密集层，一次预测一个单词。为了做到这一点，Keras开发了一个名为时间分布的特定层，<strong class="lc iu">它将相同的密集层应用于每个时间步。</strong></p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="01f7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最后，我们堆叠这些层来创建模型并添加功能损失。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="7553" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一旦我们定义了模型，我们只需要训练它。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="b259" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当模型被训练后，我们可以进行第一次翻译。您还会发现将密集层的输出与英语词汇进行映射的函数“logits_to_sentence”。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ni nj l"/></div></figure><h2 id="44e8" class="mw ma it bd mb mx my dn mf mz na dp mj lj nb nc ml ln nd ne mn lr nf ng mp nh bi translated">结论</h2><p id="7eff" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">编码器解码器结构允许<strong class="lc iu">不同的输入和输出序列长度</strong>。首先，我们使用一个<strong class="lc iu">嵌入层来创建单词</strong>的空间表示，并将其输入输出隐藏向量的LSTM层，因为<strong class="lc iu">我们只关注最后一个时间步骤</strong>的输出，我们使用return_sequences=False。</p><p id="00a8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这个输出向量需要重复与解码器部分的时间步长相同的次数，为此我们使用RepeatVector层。解码器将使用LSTM图层和参数return_sequences=True构建，因此时间步长的每个输出都由密集图层使用。</p><p id="7479" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">尽管这个模型已经是对之前教程的一个很好的改进，我们仍然可以提高精确度。我们可以增加模型中LSTM层的数量，而不是编码器中只有一层，解码器中只有一层。我们也可以使用预先训练的嵌入层，如word2vec或Glove。最后，我们可以使用注意机制，这是自然语言处理领域的主要改进之一。我们将在下一个教程中介绍这个概念。</p><p id="3216" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"> <em class="lw">附录:不使用重复矢量的编码器-解码器</em> </strong></p><p id="8c6e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在本教程中，我们已经看到了如何使用RepeatVector层构建一个编码器解码器。还有第二个选项，我们使用模型的输出作为下一个时间步的输入，而不是重复隐藏向量<strong class="lc iu">，正如我们在这个图像中看到的。</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi no"><img src="../Images/76add741a0161d71bc13acab913605ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*L242c6zp0sPxs1CeH7nRzg.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="69f9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">实现这个模型的代码可以在<a class="ae kz" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank"> Keras文档</a>中找到，它需要对Keras库有更深的理解，开发也相当复杂。</p></div></div>    
</body>
</html>