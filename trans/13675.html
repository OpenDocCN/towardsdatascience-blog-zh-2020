<html>
<head>
<title>Monocular Bird’s-Eye-View Semantic Segmentation for Autonomous Driving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向自动驾驶的单目鸟瞰语义分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/monocular-birds-eye-view-semantic-segmentation-for-autonomous-driving-ee2f771afb59?source=collection_archive---------2-----------------------#2020-09-20">https://towardsdatascience.com/monocular-birds-eye-view-semantic-segmentation-for-autonomous-driving-ee2f771afb59?source=collection_archive---------2-----------------------#2020-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a097" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">2020年BEV语义分割综述</h2></div><p id="5ff9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更新:</p><ul class=""><li id="0724" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">添加BEV feat缝线，2021/01/31</li><li id="d737" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">添加PYVA，2021/10/01</li><li id="a3fe" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">添加全景BEV，2021/10/04</li><li id="acfb" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">TODO:添加<a class="ae ls" href="https://arxiv.org/abs/2006.11436" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> BEV-Seg </strong> </a>，<a class="ae ls" href="https://arxiv.org/abs/2103.01100" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> CaDDN </strong> </a>，<a class="ae ls" href="https://arxiv.org/abs/2104.10490" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> FIERY </strong> </a>，<a class="ae ls" href="https://arxiv.org/abs/2107.06307" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">HDPE pnet</strong></a><strong class="kk iu">。</strong></li></ul><p id="a0c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我还写了一篇关于BEV物体检测的<a class="ae ls" rel="noopener" target="_blank" href="/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944">更新博文</a>，尤其是关于变形金刚。</p><div class="lt lu gp gr lv lw"><a rel="noopener follow" target="_blank" href="/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944"><div class="lx ab fo"><div class="ly ab lz cl cj ma"><h2 class="bd iu gy z fp mb fr fs mc fu fw is bi translated">自动驾驶中使用变压器的单目BEV感知</h2><div class="md l"><h3 class="bd b gy z fp mb fr fs mc fu fw dk translated">截至2021年末的学术文献和行业实践综述</h3></div><div class="me l"><p class="bd b dl z fp mb fr fs mc fu fw dk translated">towardsdatascience.com</p></div></div><div class="mf l"><div class="mg l mh mi mj mf mk ml lw"/></div></div></a></div></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><p id="54cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自动驾驶需要自我车辆周围环境的准确表示。环境包括静态元素，如道路布局和车道结构，也包括动态元素，如其他汽车、行人和其他类型的道路使用者。<strong class="kk iu">静态</strong>元素可由包含车道等级信息的高清地图捕捉。</p><p id="9a6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有两种类型的映射方法，离线和在线。离线映射以及深度学习在离线映射中的应用，请参考<a class="ae ls" rel="noopener" target="_blank" href="/deep-learning-in-mapping-for-autonomous-driving-9e33ee951a44">我之前的帖子</a>。在没有地图支持或无人驾驶汽车从未去过的地方，在线地图会很有用。对于在线绘图，一种传统的方法是SLAM(同步定位和绘图),它依赖于对一系列图像的几何特征的检测和匹配，或者利用物体的<a class="ae ls" rel="noopener" target="_blank" href="/monocular-dynamic-object-slam-in-autonomous-driving-f12249052bf1">附加概念的变形。</a></p><div class="lt lu gp gr lv lw"><a rel="noopener follow" target="_blank" href="/deep-learning-in-mapping-for-autonomous-driving-9e33ee951a44"><div class="lx ab fo"><div class="ly ab lz cl cj ma"><h2 class="bd iu gy z fp mb fr fs mc fu fw is bi translated">自动驾驶地图中的深度学习</h2><div class="md l"><h3 class="bd b gy z fp mb fr fs mc fu fw dk translated">截至2020年的文献综述</h3></div><div class="me l"><p class="bd b dl z fp mb fr fs mc fu fw dk translated">towardsdatascience.com</p></div></div><div class="mf l"><div class="mt l mh mi mj mf mk ml lw"/></div></div></a></div><p id="42db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章将关注另一种在线制图的方式——鸟瞰(BEV)语义分割。与SLAM相比，SLAM需要来自同一移动摄像机的一系列图像，BEV语义分割基于多个摄像机同时观察车辆的不同方向捕获的图像。因此，与SLAM相比，它能够从一次性收集的数据中生成更多有用的信息。此外，当ego汽车静止或缓慢移动时，BEV语义分割仍将工作，而SLAM将表现不佳或失败。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mu"><img src="../Images/c012e76c1dc3c6ba7013df818c4358b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VT6wNZAIbzyqcJEqgBr6_A.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">BEV语义分段与SLAM的输入差异(<em class="nj">图片由本文作者</em>提供)</p></figure><h1 id="eb38" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">为什么选择BEV语义地图？</h1><p id="fefe" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated">在一个典型的自动驾驶堆栈中，<strong class="kk iu">行为预测和规划</strong>通常以这种<strong class="kk iu">自上而下的视图(或鸟瞰图，BEV) </strong>完成，因为高度信息不太重要，自动驾驶汽车需要的大部分信息可以方便地用BEV表示。这个BEV空间可以被不严格地称为3D空间。(例如，BEV空间中的对象检测通常被称为3D定位，以区别于成熟的3D对象检测。)</p><p id="a17d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，标准做法是将高清地图光栅化为BEV图像，并在行为预测规划中与动态对象检测相结合。最近探索这一策略的研究包括<a class="ae ls" href="http://www.cs.toronto.edu/~wenjie/papers/intentnet_corl18.pdf" rel="noopener ugc nofollow" target="_blank"> IntentNet </a>(优步·ATG，2018年)<a class="ae ls" href="https://arxiv.org/pdf/1812.03079.pdf" rel="noopener ugc nofollow" target="_blank">司机网</a> (Waymo，2019年)<a class="ae ls" href="https://arxiv.org/abs/1906.08945" rel="noopener ugc nofollow" target="_blank">交通规则</a> (Zoox，2019年)<a class="ae ls" href="https://arxiv.org/abs/2006.14480" rel="noopener ugc nofollow" target="_blank"> Lyft预测数据集</a> (Lyft，2020年)，等等。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oh"><img src="../Images/e456629e8f16e2b3e4ba60c868c3bc51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XU3W-Y5UoRu0o0L4lhEqKA.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">最近的研究将高清地图渲染为BEV语义地图(<em class="nj">图片由本文作者编辑，</em>来源于参考出版物)</p></figure><p id="d675" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">诸如对象检测和语义分割的传统计算机视觉任务涉及在与输入图像相同的坐标框架中进行估计。因此，自动驾驶的<strong class="kk iu">感知</strong>堆栈通常发生在与车载摄像头图像相同的空间——透视视图空间<strong class="kk iu">。</strong></p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oi"><img src="../Images/d5ae20fa710cfcba3aac66cf68d3d332.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cF8eonw7_slYr0hxwjVhPQ.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">感知发生在图像空间(左:<a class="ae ls" href="https://arxiv.org/abs/1511.00561" rel="noopener ugc nofollow" target="_blank"> SegNet </a>)而规划发生在BEV空间(右:<a class="ae ls" href="http://www.cs.toronto.edu/~wenjie/papers/cvpr19/nmp.pdf" rel="noopener ugc nofollow" target="_blank"> NMP </a> ) ( <a class="ae ls" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="291f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感知中使用的表示与预测和规划等下游任务之间的差距通常在<strong class="kk iu">传感器融合</strong>堆栈中弥合，该堆栈通常在雷达或激光雷达等主动传感器的帮助下，将透视空间中的2D观测提升到3D或BEV。也就是说，使用BEV表示对跨模态感知是有益的。首先，它是可解释的，并有助于调试每种传感模态的固有故障模式。它也很容易扩展到其他新的形式，并简化了后期融合的任务。此外，如上所述，这种表示中的感知结果可以很容易地被预测和计划堆栈使用。</p><h1 id="8348" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">将透视RGB图像提升到BEV</h1><p id="a94b" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated">来自主动传感器(如雷达或激光雷达)的数据有助于BEV表示，因为测量在3D中是固有的度量。然而，由于全景相机传感器的普遍存在和低成本，具有语义意义的BEV图像的生成最近吸引了很多关注。</p><p id="e364" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章的标题中，“单目”是指管道的输入是从单目RGB相机获得的图像，没有显式的深度信息。自主车辆上捕获的单目RGB图像是3D空间的透视投影，将2D透视观察提升到3D的逆问题是固有的不适定问题。</p><h2 id="4473" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated">挑战，IPM及其他</h2><p id="fc3b" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated">BEV语义分割的一个明显挑战是<strong class="kk iu">视图转换</strong>。为了正确地恢复3D空间的BEV表示，该算法必须利用硬的(但可能有噪声的)几何先验，例如相机的内在和外在先验，以及软的先验，例如道路布局的知识集和常识(车辆在BEV中不重叠，等等)。传统上，<a class="ae ls" href="https://csyhhu.github.io/2015/07/09/IPM/" rel="noopener ugc nofollow" target="_blank">逆透视映射(IPM) </a>一直是该任务的常用方法，假设平地假设和固定摄像机外部。但是，当相机外部条件变化时，这项任务不适用于不平坦的表面或崎岖不平的道路。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ov"><img src="../Images/a497153cc8e035efb8ebdef58b22ac19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8kpo0VFdlGmoq6Bt.jpg"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">IPM会在汽车等3D物体周围产生伪像(<a class="ae ls" href="https://csyhhu.github.io/2015/07/09/IPM/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="9fa7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个挑战在于为这样的任务收集<strong class="kk iu">数据和注释</strong>。一种方法是让无人机一直跟随自动驾驶汽车(类似于<a class="ae ls" href="https://www.youtube.com/watch?v=HPWGFzqd7pI" rel="noopener ugc nofollow" target="_blank"> MobileEye的CES 2020 talk </a>)，然后要求人类标注语义分段。这种方法显然不具有实用性和可扩展性。许多研究依赖于合成数据或不成对的地图数据来训练提升算法。</p><p id="7076" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在接下来的会议中，我将回顾该领域的最新进展，并强调其共性。根据使用的监控信号，这些研究可以大致分为两种类型。第一类研究借助模拟进行间接监管，第二类研究直接利用最近发布的多模态数据集进行直接监管。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="2810" class="nk nl it bd nm nn ow np nq nr ox nt nu jz oy ka nw kc oz kd ny kf pa kg oa ob bi translated">模拟和语义分割</h1><p id="d33b" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated">该领域的开创性研究使用模拟来生成必要的数据和注释，以将透视图像提升到BEV中。为了弥合模拟到现实(sim2real)领域的差距，许多人使用语义分割作为中间表示。</p><h2 id="72c5" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated">VPN(查看解析器网络，RAL 2020)</h2><p id="cd0c" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/abs/1906.03560" rel="noopener ugc nofollow" target="_blank"> VPN(用于感知周围环境的跨视角语义分割)</a>是最早探索BEV语义分割的作品之一，并将其称为“跨视角语义分割”。视图解析网络(VPN)使用视图转换器模块来模拟从透视图到BEV的转换。这个模块被实现为一个多层感知器(MLP ),它将2D物理范围扩展到一个1D向量，然后在其上执行完全连接的操作。换句话说，它忽略了强几何先验，而是纯粹采用数据驱动的方法来学习透视到BEV扭曲。这种扭曲是特定于摄像机的，每个摄像机必须学习一个网络。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pb"><img src="../Images/bea98b6d7ba82db059e2b45af9170860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bw1JqGE1QEVDFwofn8ixEA.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">视图解析网络的整体管道(<a class="ae ls" href="https://arxiv.org/abs/1906.03560" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="afcc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">VPN使用合成数据(用<a class="ae ls" href="https://carla.org/" rel="noopener ugc nofollow" target="_blank"> CARLA </a>生成)和对抗性损失进行训练期间的域适应。此外，它使用语义遮罩作为中间表示，没有照片级的纹理间隙。</p><blockquote class="pc pd pe"><p id="e493" class="ki kj pf kk b kl km ju kn ko kp jx kq pg ks kt ku ph kw kx ky pi la lb lc ld im bi translated">视图转换器模块的输入和输出具有相同的大小。该论文提到，这使得它很容易插入到其他架构中。在我看来，这实际上是完全没有必要的，因为透视图和BEV本质上是不同的空间，因此不需要强制相同的像素格式，甚至输入和输出之间的纵横比也不需要。代码可在<a class="ae ls" href="https://view-parsing-network.github.io/" rel="noopener ugc nofollow" target="_blank"> github </a>上获得。</p></blockquote><h2 id="0861" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated">渔网(CVPR 2020)</h2><p id="1a95" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/abs/2006.09917" rel="noopener ugc nofollow" target="_blank">渔网</a>将激光雷达、雷达和相机融合转换成BEV空间中的单一统一表示。这种表示使得跨不同模态执行后期融合更加容易。视图转换模块(视觉路径中的紫色块)类似于基于MLP的VPN。视图变换网络的输入是一系列图像，但它们只是在通道维度上连接起来并馈入网络，而不是利用RNN结构。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pj"><img src="../Images/c70bf10ce1774dc12454f9786fc2f129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*32WSYgtsJ-bg97SgVUPUdQ.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">渔网使用BEV表示来促进跨传感器模态的后期融合(<a class="ae ls" href="https://arxiv.org/abs/2006.09917" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="10ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">地面实况生成使用激光雷达中的3D注记，主要关注动态对象，如车辆和VRU(易受影响的道路使用者，如行人和骑自行车的人)。其余的都由一个背景类表示。</p><blockquote class="pc pd pe"><p id="2728" class="ki kj pf kk b kl km ju kn ko kp jx kq pg ks kt ku ph kw kx ky pi la lb lc ld im bi translated">BEV语义网格的分辨率为10厘米和20厘米/像素。这比<a class="ae ls" rel="noopener" target="_blank" href="/deep-learning-in-mapping-for-autonomous-driving-9e33ee951a44">离线映射</a>中使用的典型值4或5厘米/像素要粗糙得多。遵循<a class="ae ls" href="https://arxiv.org/abs/1906.03560" rel="noopener ugc nofollow" target="_blank"> VPN </a>的惯例，图像的尺寸与192 x 320的输出分辨率相匹配。CVPR 2020的演讲可以在Youtube上找到。</p></blockquote><h2 id="19a0" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated">ICRA 2019年奥运会</h2><p id="698d" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/abs/1804.02176" rel="noopener ugc nofollow" target="_blank"> VED(具有卷积变分编码器-解码器网络的单目语义占据网格映射</a>)开发了用于语义占据网格映射预测的变分编码器-解码器(VED)架构。它对驾驶场景的前视视觉信息进行编码，并随后将其解码为BEV语义占用网格。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pk"><img src="../Images/15f1cd8590a90e50ef0c4cd6cd447fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9NlV-3qLFknmjkfufX9Ww.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">用于VED中视图变换的可变编码器-解码器网络(<a class="ae ls" href="https://arxiv.org/abs/1804.02176" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="1da2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此地面实况是使用城市景观数据集中立体匹配的视差图生成的。这个过程可能会有噪声，这实际上促使使用VED和从潜在空间采样，以使模型对不完美的GT具有鲁棒性。然而，由于是VAE，它通常不会产生尖锐的边缘，这可能是由于高斯先验和均方误差。</p><blockquote class="pc pd pe"><p id="de13" class="ki kj pf kk b kl km ju kn ko kp jx kq pg ks kt ku ph kw kx ky pi la lb lc ld im bi translated">输入图像和输出图像分别为256×512和64×64。<a class="ae ls" href="https://arxiv.org/abs/1804.02176" rel="noopener ugc nofollow" target="_blank"> VED </a>利用了vanilla SegNet(传统语义分割的相对强大的基线)的架构，并引入了一个1 x2池层，以适应输入和输出的不同纵横比。</p></blockquote><h2 id="dd31" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated">学习观察周围的物体(ECCV 2018)</h2><p id="6839" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/abs/1803.10870" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">学习环视物体</strong>寻找室外场景的俯视图表示</a>在BEV中产生遮挡区域的幻觉，并利用模拟和地图数据来帮助。</p><blockquote class="pc pd pe"><p id="6b30" class="ki kj pf kk b kl km ju kn ko kp jx kq pg ks kt ku ph kw kx ky pi la lb lc ld im bi translated">我个人认为，这是BEV语义分割领域的一篇开创性论文，但它似乎没有受到太多关注。也许它需要一个朗朗上口的名字？</p></blockquote><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pl"><img src="../Images/d16c2d7d9698a2f096cdd3cc048ca50c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xWhfdFo_5natJc3tQSyuyA.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">学习环视物体的整体流程(<a class="ae ls" href="https://arxiv.org/abs/1803.10870" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="0258" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过逐像素深度预测和投影到BEV来完成视图变换。这部分克服了BEV空间中缺乏训练数据的问题。这也在后面的工作中完成，如下面审查的<a class="ae ls" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank">提升、拍打、射击(ECCV 2020) </a>。</p><p id="a869" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">论文用来学习幻觉(预测遮挡部分)的技巧相当惊人。对于GT深度很难找到的动态对象，我们过滤掉损失。随机屏蔽图像块，并要求模型产生幻觉。用亏损作为监督信号。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pm"><img src="../Images/9e4644d5813e598f1d4d71e31423dbb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qM5TvtSNwaNDxzKTBEjwfw.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">忽略动态区域，但明确删除静态区域以直接监控幻觉(<a class="ae ls" href="https://arxiv.org/abs/1803.10870" rel="noopener ugc nofollow" target="_blank">源</a>)</p></figure><p id="36ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于在BEV空间中很难获得显式成对的监督，因此本文使用对抗性损失来指导具有模拟和<a class="ae ls" href="https://www.openstreetmap.org/about" rel="noopener ugc nofollow" target="_blank"> OpenStreetMap </a>数据的学习，以确保生成的道路布局看起来像真实的道路布局。这个技巧也用在后面的作品中，如<a class="ae ls" href="https://arxiv.org/abs/2002.08394" rel="noopener ugc nofollow" target="_blank">monola layout(WACV 2020)</a>。</p><p id="7045" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它在图像空间中使用一个CNN进行深度和语义预测，将预测提升到3D空间并在BEV中渲染，最后在BEV空间中使用另一个CNN进行细化。BEV中的这个细化模块还被用在很多其他作品中，如Cam2BEV (ITSC 2020)和<a class="ae ls" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank"> Lift，Splat，Shoot (ECCV 2020) </a>。</p><h2 id="cd61" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated">Cam2BEV (ITSC 2020)</h2><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/a1c7d98ab047acf993929b69623223b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*-q3Jq9B4Ba6B8puc.gif"/></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">在Cam2BEV中输入语义分割图像和预测的BEV图(<a class="ae ls" href="https://github.com/ika-rwth-aachen/Cam2BEV" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="4b2b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/abs/2005.04078" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> Cam2BEV </strong>(一种Sim2Real深度学习方法，用于将来自多个车载摄像头的图像转换为鸟瞰图中语义分割的图像)</a>使用具有IPM的空间转换器模块将透视特征转换到BEV空间。神经网络架构接收由不同摄像机捕获的四幅图像，并在将它们连接在一起之前对每幅图像应用IPM变换。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi po"><img src="../Images/80730820da79dd3e505cf0e4cc9e86b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kmu9eLVAkcVM3EZlQbCtnQ.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">Cam2BEV使用确定性IPM来转换中间特征图(<a class="ae ls" href="https://arxiv.org/abs/2005.04078" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="7fa5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Cam2BEV使用从<a class="ae ls" href="https://www.mscsoftware.com/product/virtual-test-drive" rel="noopener ugc nofollow" target="_blank"> VTD </a>(虚拟试驾)模拟环境生成的合成数据。它采用了四个语义分割图像，重点放在提升过程中，避免了处理sim2real域的空白。</p><p id="b59e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Cam2BEV具有相当集中的范围和许多设计选择，这使得它非常实用。首先，它只在语义空间起作用，因此避免了sim2real domain gap的问题。它有一个预处理阶段，故意掩盖遮挡区域，以避免对遮挡进行推理，并可以使问题更容易处理。为了简化提升过程，它还将“单应图像”作为输入，该单应图像由语义分割结果的IPM生成并连接成360 deg图像。因此，Cam2BEV的主要目标是推理BEV中3D对象的物理范围，其在单应图像中可能被拉长。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pp"><img src="../Images/86b260afbcbe3904b230dc5498d61950.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wkZcWCifqml5K8q9_I_CYQ.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">预处理后的单应图像和目标GT供神经网络在Cam2BEV中预测(<a class="ae ls" href="https://arxiv.org/abs/2005.04078" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><blockquote class="pc pd pe"><p id="b486" class="ki kj pf kk b kl km ju kn ko kp jx kq pg ks kt ku ph kw kx ky pi la lb lc ld im bi translated">Cam2BEV的目标是校正IPM，但在某种意义上，IPM会扭曲3D对象，如不在路面上的汽车。然而，它仍然不能处理不平坦的路面或行驶过程中的倾斜度变化。Cam2BEV的输入输出都是256x512像素。代码可在<a class="ae ls" href="https://github.com/ika-rwth-aachen/Cam2BEV" rel="noopener ugc nofollow" target="_blank"> github </a>中获得。它还提供了一个很好的IPM基线实现。</p></blockquote></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="b48a" class="nk nl it bd nm nn ow np nq nr ox nt nu jz oy ka nw kc oz kd ny kf pa kg oa ob bi translated">你所需要的是(多模态)数据集</h1><p id="3a8d" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated">最近发布的许多多模态数据集(<a class="ae ls" href="https://self-driving.lyft.com/level5/data/" rel="noopener ugc nofollow" target="_blank"> Lyft </a>、<a class="ae ls" href="https://www.nuscenes.org/" rel="noopener ugc nofollow" target="_blank"> Nuscenes </a>、<a class="ae ls" href="https://www.argoverse.org/" rel="noopener ugc nofollow" target="_blank"> Argoverse </a>等)使得直接监督单目BEV语义分割任务成为可能。这些数据集不仅提供3D对象检测信息，还提供高清地图以及定位信息，以在高清地图上的每个时间戳精确定位ego车辆。</p><p id="dda7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">BEV分割任务有两个部分，即(动态)对象分割任务和(静态)道路布局分割任务。对于对象分割，3D边界框被光栅化到BEV图像中以生成注释。对于静态道路布局，基于所提供的定位结果将地图转换到ego车辆框架中，并光栅化为BEV注释。</p><h2 id="9333" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated">单层布局(WACV 2020)</h2><p id="9d19" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/abs/2002.08394" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">monola layout</strong>:单幅图像的Amodal场景布局</a>重点是将单个摄像机提升到语义BEV空间。本文的重点是研究造成遮挡区域的模型完成情况。似乎受到了<a class="ae ls" href="https://arxiv.org/abs/1803.10870" rel="noopener ugc nofollow" target="_blank">学习观察周围物体的严重影响(ECCV 2018) </a> <strong class="kk iu">。</strong></p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pq"><img src="../Images/9ef1d07892af8f0ef9e55a529c754afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6LyBSAb4_dc8zzArL_Chvw.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">MonoLayout在KITTI(左)和<a class="ae ls" href="https://www.argoverse.org/" rel="noopener ugc nofollow" target="_blank"> Argoverse </a>(右)的闭塞区域产生幻觉(<a class="ae ls" href="https://arxiv.org/abs/2002.08394" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="5101" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">视图变换通过编码器-解码器结构来执行，并且潜在特征被称为“共享上下文”。两个解码器用于分别解码静态和动态类。作者还报告了在消融研究中使用组合解码器处理静态和动态对象的负面结果。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pr"><img src="../Images/cf99f5fc3d134be409c87c64c009c3cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SVwI-riwZCcI078ZrG5cTw.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">MonoLayout的编解码设计(<a class="ae ls" href="https://arxiv.org/abs/2002.08394" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="a5d0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然高清地图groundtruth在<a class="ae ls" href="https://www.argoverse.org/" rel="noopener ugc nofollow" target="_blank"> Argoverse </a>数据集中可用，但MonoLayout选择仅将其用于评估，而不是用于训练(事后诸葛亮还是有意的设计选择？).对于训练，MonoLayout使用<strong class="kk iu">时间传感器融合</strong>过程，通过将整个视频中的2D语义分割结果与定位信息聚合来生成弱背景真相。它使用monodepth2将RGB像素提升为点云。它还会丢弃距离自我车5米远的任何东西，因为它们可能会很吵。为了鼓励网络输出可以想象的场景布局，MonoLayout使用了对抗性特征学习(类似于在<a class="ae ls" href="https://arxiv.org/abs/1803.10870" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">学习环视物体</strong> </a>中使用的)。先验数据分布从OpenStreetMap获得。</p><blockquote class="pc pd pe"><p id="cce6" class="ki kj pf kk b kl km ju kn ko kp jx kq pg ks kt ku ph kw kx ky pi la lb lc ld im bi translated">MonoLayout的空间分辨率为30厘米/像素，因此128 x 128的输出相当于BEV空间中的40米x 40米。代码可在<a class="ae ls" href="https://github.com/hbutsuak95/monolayout" rel="noopener ugc nofollow" target="_blank"> github </a>中获得。</p></blockquote><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="ps pt l"/></div></figure><h2 id="34f3" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated">PyrOccNet (CVPR 2020)</h2><p id="ddf4" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> PyrOccNet </strong>:使用金字塔占据网络从图像预测语义图表示</a>从单目图像预测BEV语义图，并使用贝叶斯过滤将它们融合成连贯视图。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pu"><img src="../Images/e07444bf94ee7130c38e053595b98f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1yQHfSv_SEygeWCynK_Rw.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">PyrOccNet在NuScenes数据集上的演示结果(<a class="ae ls" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="7fbc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PyrOccNet中视图转换的核心组件是通过<strong class="kk iu">密集转换器</strong>模块执行的(<em class="pf">注意，该转换器不是基于注意力的！</em>)。它似乎受到了来自同一作者的<a class="ae ls" href="https://arxiv.org/abs/1811.08188" rel="noopener ugc nofollow" target="_blank"> OFT (BMVC 2019) </a>的极大启发。OFT沿着投射回3D空间的光线在像素位置均匀地涂抹特征，这非常类似于<a class="ae ls" href="https://en.wikipedia.org/wiki/Tomographic_reconstruction" rel="noopener ugc nofollow" target="_blank">计算断层摄影</a>中使用的背投算法。PyrOccNet中的密集变压器模块通过使用FC层沿深度轴扩展而更进一步。实际上，在BEV空间中，有多个密集的变压器以不同的比例工作，集中在不同的距离范围。</p><blockquote class="pc pd pe"><p id="8e79" class="ki kj pf kk b kl km ju kn ko kp jx kq pg ks kt ku ph kw kx ky pi la lb lc ld im bi translated">密集的变压器层的灵感来自于这样的观察:虽然网络需要大量的垂直上下文来将特征映射到鸟瞰图(由于遮挡、缺乏深度信息和未知的地面拓扑)，但是在水平方向上，BEV位置和图像位置之间的关系可以使用简单的相机几何结构来建立。—来自<a class="ae ls" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank"> PyrOccNet </a> paper</p></blockquote><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pv"><img src="../Images/debab8d8c89f75d86b245dedbb19ab83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wNjUfeWBwMioxPfrpXdS9w.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">密集变压器的PyrOccNet架构图(<a class="ae ls" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="6746" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练数据来自于<a class="ae ls" href="https://www.argoverse.org/" rel="noopener ugc nofollow" target="_blank"> Argoverse数据集</a>和<a class="ae ls" href="https://www.nuscenes.org/" rel="noopener ugc nofollow" target="_blank"> nuScenes数据集</a>的多模态数据集，其中既有地图数据，也有3D物体检测地面真实</p><p id="c29c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PyrOccNet使用贝叶斯过滤以连贯的方式融合跨多个摄像机和跨时间的信息。它从<a class="ae ls" href="https://en.wikipedia.org/wiki/Occupancy_grid_mapping" rel="noopener ugc nofollow" target="_blank">二元贝叶斯占用网格</a>的旧思想中汲取灵感，并增强网络输出的可解释性。时间融合结果非常类似于映射过程，并且非常类似于<a class="ae ls" href="https://arxiv.org/abs/2002.08394" rel="noopener ugc nofollow" target="_blank">monola layout</a>中用于生成弱GT的“时间传感器融合”过程。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pw"><img src="../Images/7f420abc46a49e8bd25c5ea6ba991084.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L3FuvSS23zFrl3L3Q9YPPQ.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">仅静态类的PyrOccNet的时间融合结果(<a class="ae ls" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><blockquote class="pc pd pe"><p id="1249" class="ki kj pf kk b kl km ju kn ko kp jx kq pg ks kt ku ph kw kx ky pi la lb lc ld im bi translated">PyrOccNet的空间分辨率为25厘米/像素，因此200 x 200的输出相当于BEV空间中的50米x 50米。代码将在<a class="ae ls" href="https://github.com/tom-roddick/mono-semantic-maps" rel="noopener ugc nofollow" target="_blank"> github </a>中提供。</p></blockquote><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="ps pt l"/></div></figure><h2 id="eecc" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated">举起，拍打，射击(ECCV 2020)</h2><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ov"><img src="../Images/baf8c75916c9cf0761e2890019007276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MvQIYn1VyZVdoUQ8.gif"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">nuScenes数据集上的演示(<a class="ae ls" href="https://nv-tlabs.github.io/lift-splat-shoot/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="191e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">提升、拍打、拍摄</strong>:通过隐式反投影到3D对来自任意相机装备的图像进行编码</a>为视图变换执行密集的逐像素深度估计。它首先使用每个摄像机的CNN来执行概率性的像素深度预测，以将每个透视图像提升到3D点云中，然后使用摄像机extrinsics来绘制BEV。最后，使用BEV CNN来改进预测。“拍摄”部分意味着路径规划，将被跳过，因为它超出了本文的范围。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi px"><img src="../Images/9cfbb3f30713dbbb44bcf4a3649e9341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gnKSinu6ZNDCnk3J.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">提升和飞溅管道(左)和概率深度预测(右)(<a class="ae ls" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="045a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过预测RGB图像中像素的深度分布，提出了概率3D提升。某种程度上统一了<a class="ae ls" href="https://arxiv.org/abs/1812.07179" rel="noopener ugc nofollow" target="_blank">伪激光雷达</a> (CVPR 2019)的一热提升和<a class="ae ls" href="https://arxiv.org/abs/1811.08188" rel="noopener ugc nofollow" target="_blank"> OFT </a> (BMVC 2019)的统一提升。其实这种“软”预测是<a class="ae ls" href="https://arxiv.org/abs/2006.12057" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">可微分渲染</strong> </a>中常用的一招。<a class="ae ls" href="https://arxiv.org/abs/2004.03080" rel="noopener ugc nofollow" target="_blank">伪激光雷达v3 </a> (CVPR 2020)的并发工作也使用了这种软光栅化技巧，使深度提升和投影变得可区分。</p><p id="b63c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练数据来自于<a class="ae ls" href="https://self-driving.lyft.com/level5/data/" rel="noopener ugc nofollow" target="_blank"> Lyft数据集</a>和<a class="ae ls" href="https://www.nuscenes.org/" rel="noopener ugc nofollow" target="_blank"> nuScenes数据集</a>的多模态数据集，其中既有地图数据，也有3D物体检测地面真实。</p><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="ps pt l"/></div></figure><blockquote class="pc pd pe"><p id="4131" class="ki kj pf kk b kl km ju kn ko kp jx kq pg ks kt ku ph kw kx ky pi la lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank"> Lift-Splat-Shoot </a>输入分辨率为128x352，BEV网格为200x200，分辨率为0.5 m/pixel = 100m x 100m。代码可在<a class="ae ls" href="https://github.com/nv-tlabs/lift-splat-shoot" rel="noopener ugc nofollow" target="_blank"> github </a>中找到。</p></blockquote><h2 id="df27" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated">BEV特征拼接</h2><p id="3d83" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated">PyrOccNet和<a class="ae ls" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank">Lift-Splat-shot</a>都专注于将来自多个相机的同步图像拼接成一个连贯的360度视图。<strong class="kk iu"> BEV特征拼接</strong> ( <a class="ae ls" href="https://arxiv.org/abs/2012.03040" rel="noopener ugc nofollow" target="_blank">使用机载单目摄像机理解鸟瞰语义高清地图</a>)将单目视频(具有估计的自我姿态)融合成连贯的前视。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi py"><img src="../Images/b75c46090b76240031fbcf8d4aee8204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V74Hx7upGL-1glngX0W93Q.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated"><a class="ae ls" href="https://arxiv.org/abs/2012.03040" rel="noopener ugc nofollow" target="_blank"> BEV特征拼接</a>使用BEV时间聚合将单目视频转换成BEV图</p></figure><p id="d8a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/abs/2012.03040" rel="noopener ugc nofollow" target="_blank"> BEV特征拼接</a>取单目视频(一系列图像)作为模型的输入。为了融合来自多个帧的信息，它引入了BEV时间聚合模块。该模块首先投影中间特征图，然后将具有自我姿态(从里程计管道估计)的特征图聚集到连贯和扩展的BEV空间中。此外，每个图像帧的中间特征在摄像机空间中用重新投影的BEV地面真实来监督。</p><blockquote class="pc pd pe"><p id="424f" class="ki kj pf kk b kl km ju kn ko kp jx kq pg ks kt ku ph kw kx ky pi la lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/abs/2012.03040" rel="noopener ugc nofollow" target="_blank"> BEV特征拼接</a>具有200x200像素的BEV网格，分辨率为0.25 m/pixel = 50m x 50m。</p></blockquote><h2 id="c6cd" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated"><strong class="ak"> PYVA </strong>:用心投射你的观点(CVPR 2021)</h2><p id="8dd7" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated">由于卷积层的局部受限感受野，CNN很难直接拟合视图投影模型。因此，大多数基于CNN的方法明确地将视图变换编码到CNN架构中(具有相机外切的特征地图的单应变换等)。</p><p id="b157" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一方面，由于全局注意机制，变形金刚更适合做这项工作。<strong class="kk iu"> PYVA ( </strong></p><p id="f9a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae ls" href="https://arxiv.org/abs/1906.03560" rel="noopener ugc nofollow" target="_blank"> VPN </a>之后，PYVA使用MLP将投影图像特征提升到BEV中。然后，它使用交叉注意力转换器来进一步细化提升。提升后的BEV特征X’用作查询，图像特征X(或循环重投影的图像特征X”)用作键和值。</p><p id="861a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用交叉注意力转换器模块背后的思想是直观的:<em class="pf">对于查询中的每个像素(BEV特征)，网络应该关注键(图像特征)中的哪个像素</em>？不幸的是，本文没有展示transformer模块内部的一些注意力地图的例子，这些例子可以很好地说明这种直觉。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pz"><img src="../Images/9e70d4d5749592d360d31d29688e41f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Wpc18ZsLb8d3wG6nQiiuQ.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated"><a class="ae ls" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Projecting_Your_View_Attentively_Monocular_Road_Scene_Layout_Estimation_via_CVPR_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank"> PYVA </a>使用交叉视图转换器模块将前视图提升到BEV中</p></figure><p id="4f21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PYVA遵循<a class="ae ls" href="https://arxiv.org/abs/2002.08394" rel="noopener ugc nofollow" target="_blank">monola layout</a>的思想，利用对抗性训练损失，促使解码器输出更合理的道路布局和车辆位置。PYVA更进一步，不仅鼓励网络输出合理的车辆位置，还输出道路和车辆之间合理的<strong class="kk iu">关系</strong>，因为道路布局在预测车辆位置和方向之前提供了有用的信息(例如，汽车更有可能停在路边)。</p><h2 id="8eb0" class="oj nl it bd nm ok ol dn nq om on dp nu kr oo op nw kv oq or ny kz os ot oa ou bi translated">全景BEV</h2><p id="845b" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated">全景BEV ( <a class="ae ls" href="https://arxiv.org/abs/2108.03227" rel="noopener ugc nofollow" target="_blank">使用单目正视图图像的鸟瞰全景分割</a>)正确地指出实例的概念对下游是至关重要的。<strong class="kk iu"> FIERY </strong>将语义分割思想扩展到实例分割，<strong class="kk iu">全景BEV </strong>更进一步，在BEV空间进行全景分割。</p><p id="90e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Panoptic BEV使用带有BiFPN(如EfficientDet)颈部的ResNet主干。在每个级别(P2-P5)，图像特征通过密集转换器模块被投射到BEV中(<em class="pf">注意，这个转换器不是基于注意力的！</em>)。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qa"><img src="../Images/a71496432424a96e1267c564689dd127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CagVp4PC5yYJUefD.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">全景BEV的网络架构(transformer her不是基于注意力的)</p></figure><p id="e630" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在每个密集变压器中，有一个明显的垂直变压器和一个平面变压器。首先，输入图像被二进制语义分割成垂直掩码和水平掩码。被垂直屏蔽屏蔽的图像被馈送到垂直变换器。<strong class="kk iu">垂直</strong>转换器使用体积晶格来模拟中间3D空间，然后将其展平以生成垂直BEV特征。这种立式变压器与<a class="ae ls" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank">升降拍击</a>的变压器非常相似。<strong class="kk iu">扁平</strong>变压器使用IPM，后接误差校正模块(ECM ),以产生扁平BEV特征。这很大程度上遵循了<a class="ae ls" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank"> PyrOccNet </a>的思路。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qb"><img src="../Images/c1fc7ded3cba4d847e59309df959072b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*94iio2gYHfR7rVRE.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">密集变压器提升图像到BEV，并有一个垂直变压器和平面变压器</p></figure><p id="3a01" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过将IPM的先验信息注入到总升降管道中，整个设计似乎是建立在<a class="ae ls" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank">升降拍击</a>和<a class="ae ls" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank"> PyrOccNet </a>之上。模块设计看起来确实很合理，但它们感觉太手工制作和复杂，一旦模型可以访问足够的数据，可能不会导致最佳性能。</p><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="ps pt l"/></div></figure><blockquote class="pc pd pe"><p id="25db" class="ki kj pf kk b kl km ju kn ko kp jx kq pg ks kt ku ph kw kx ky pi la lb lc ld im bi translated">从将视图转换模块命名为“密集转换器”和引用“垂直特征”来看，<a class="ae ls" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank"> PyrOccNet </a>的影响也很明显。从演示视频看，结果很有希望。将在<a class="ae ls" href="https://github.com/robot-learning-freiburg/PanopticBEV" rel="noopener ugc nofollow" target="_blank"> Github </a>上发布的代码。</p></blockquote></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="53a6" class="nk nl it bd nm nn ow np nq nr ox nt nu jz oy ka nw kc oz kd ny kf pa kg oa ob bi translated">局限性和未来方向</h1><p id="205a" class="pw-post-body-paragraph ki kj it kk b kl oc ju kn ko od jx kq kr oe kt ku kv of kx ky kz og lb lc ld im bi translated">尽管在BEV语义分割方面已经取得了很大进展，但是在它被广泛部署到生产系统之前，我认为还存在几个关键的差距。</p><p id="8629" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，对于动态参与者来说，还没有<strong class="kk iu">实例</strong>的概念。这使得在行为预测中很难利用动态对象的先验知识。例如，汽车遵循某种运动模型(如自行车模型)，未来轨迹的模式有限，而行人往往具有更多随机运动。许多现有的方法倾向于在语义分割结果中将多个汽车连接成一个连续的区域。</p><p id="452b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">动态语义类不能被重用，并且在很大程度上是“一次性的”信息，而BEV图像中的静态语义类(例如道路布局和道路上的标记)可以被视为“<strong class="kk iu">在线地图</strong>，并且应该被收获和回收。如何聚合多个时间戳上的BEV语义分段来估计一个<strong class="kk iu">更好的映射</strong>是另一个需要回答的关键问题。<a class="ae ls" href="https://arxiv.org/abs/2002.08394" rel="noopener ugc nofollow" target="_blank">monola layout</a>和<a class="ae ls" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank"> PyrOccNet </a>中的时间传感器融合方法可能有用，但需要针对SLAM等传统方法进行基准测试。</p><p id="b11f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如何将线上的逐像素语义图转换成<strong class="kk iu">轻量级结构化图</strong>以备将来重用。为了不浪费宝贵的车载地图周期，在线地图必须转换成某种格式，以便ego汽车或其他汽车在未来可以有效地利用。</p><h1 id="3768" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">外卖食品</h1><ul class=""><li id="7db6" class="le lf it kk b kl oc ko od kr qc kv qd kz qe ld lj lk ll lm bi translated"><strong class="kk iu">视图变换</strong>:现有的许多工作忽略了相机外部的强几何先验信息。应该避免这种情况。<a class="ae ls" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank"> PyrOccNet </a>和Lift-Splat-shot看起来方向正确。</li><li id="a85b" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">数据和监管:</strong>2020年之前的大部分研究都是基于仿真数据，并使用语义分割作为中间表示，以弥合sim2real领域的差距。最近的工作利用多模态数据集对任务进行直接监督，取得了非常有希望的结果。</li><li id="68df" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">我确实觉得BEV空间中的感知是感知的未来，尤其是借助<strong class="kk iu">可微分渲染</strong>，可以将视图变换实现为可微分模块，并插入端到端模型，直接将透视图像提升到BEV空间中。</li></ul><h1 id="c1ce" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">参考</h1><ul class=""><li id="4665" class="le lf it kk b kl oc ko od kr qc kv qd kz qe ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/1906.03560" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> VPN </strong>:感知周围环境的跨视角语义分割</a>，RAL 2020</li><li id="e341" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/1804.02176" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> VED </strong>:卷积变分编解码网络的单目语义占据网格映射</a>，ICRA 2019</li><li id="356d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2005.04078" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> Cam2BEV </strong>:一种模拟现实深度学习方法，用于将来自多个车载摄像头的图像转换为鸟瞰图中的语义分割图像</a>，ITSC 2020</li><li id="deac" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2006.11436" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> BEV-Seg </strong>:利用几何和语义点云进行鸟瞰语义分割</a>，CVPR 2020研讨会</li><li id="94af" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/1803.10870" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">学习环视物体</strong>寻找户外场景的俯视图</a>，ECCV 2018</li><li id="7f6c" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2006.09917" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">渔网</strong>:网格中语义热图的未来推断</a>，CVPR 2020</li><li id="365a" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2002.08394" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">monola layout</strong>:单幅图像的模拟场景布局</a>，WACV 2020</li><li id="8f2c" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"/></a></li><li id="7a28" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank"/></li><li id="4ef5" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://www.reddit.com/r/SelfDrivingCars/comments/gxq56t/recreating_the_tesla_autopilots_birds_eye_view/" rel="noopener ugc nofollow" target="_blank">https://www . Reddit . com/r/self driving cars/comments/gxq56t/recreating _ the _ Tesla _ autopilots _ birds _ eye _ view/</a>和github 上的<a class="ae ls" href="https://github.com/MankaranSingh/Auto-Birds-Eye" rel="noopener ugc nofollow" target="_blank">代码</a></li><li id="3572" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://medium.com/asap-report/from-semantic-segmentation-to-semantic-birds-eye-view-in-the-carla-simulator-1e636741af3f" rel="noopener">https://medium . com/asap-report/from-semantic-segmentation-to-semantic-birds-eye-view-in-the-the-Carla-simulator-1e 636741 af 3f</a></li><li id="0c25" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2006.14480" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">一千零一个小时</strong>:自驾运动预测数据集</a>，Lyft预测数据集</li><li id="8b50" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/pdf/1812.03079.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">司机网</strong>:模仿最好综合最差</a>学车，RSS 2019</li><li id="a6be" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/1906.08945" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">道路规则</strong>:用语义交互卷积模型预测驾驶行为</a>，CVPR 2019</li><li id="2721" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="http://www.cs.toronto.edu/~wenjie/papers/intentnet_corl18.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> IntentNet </strong>:学习从原始传感器数据预测意图</a>，CoRL 2018</li><li id="017d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/1811.08188" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> OFT </strong>:单目三维物体检测的正交特征变换</a>，BMVC 2019</li><li id="d565" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/1812.07179" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">来自视觉深度估计的伪激光雷达</strong>:填补自动驾驶3D物体检测的空白</a>，CVPR 2019</li><li id="8842" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2004.03080" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">伪激光雷达v3 </strong>:基于图像的三维目标探测端到端伪激光雷达</a>，CVPR 2020</li><li id="7617" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2012.03040" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">BEV-Feat-Stitching</strong>:使用机载单目摄像机理解鸟瞰语义高清地图</a>，Arxiv 2021/01</li><li id="3dfb" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Projecting_Your_View_Attentively_Monocular_Road_Scene_Layout_Estimation_via_CVPR_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> PYVA </strong>:用心投射你的视角:通过交叉视角变换的单目道路场景布局估计</a>，CVPR 2021</li><li id="d1b9" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2108.03227" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"/>:使用单目正视图图像的鸟瞰全景分割</a>，Arxiv 2021/08</li><li id="2c8c" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2103.01100" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> CaDDN </strong>:单目3D物体检测的分类深度分布网络</a>，CVPR 2021口述</li><li id="d151" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2104.10490" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">火热</strong>:从环绕单目摄像机鸟瞰未来实例预测</a>，ICCV 2021</li><li id="e825" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2006.11436" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> BEV-Seg </strong>:利用几何和语义点云进行鸟瞰语义分割</a>，CVPR 2020研讨会</li><li id="50bc" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae ls" href="https://arxiv.org/abs/2107.06307" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">高清地图网</strong>:在线高清地图构建与评估框架</a>，CVPR 2021研讨会</li></ul></div></div>    
</body>
</html>