<html>
<head>
<title>A Step-by-Step Tutorial for Conducting Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">进行情感分析的分步指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-step-by-step-tutorial-for-conducting-sentiment-analysis-9d1a054818b6?source=collection_archive---------18-----------------------#2020-10-02">https://towardsdatascience.com/a-step-by-step-tutorial-for-conducting-sentiment-analysis-9d1a054818b6?source=collection_archive---------18-----------------------#2020-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6e489ac4f309dfff8cf1c56bd20f44c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b_eUpCfZgiphxDNzpfpSGg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由<a class="ae jg" href="https://unsplash.com/s/photos/transformation?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae jg" href="https://unsplash.com/@grimstad?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">hkon grim stad</a>拍摄的照片</p></figure><div class=""/><div class=""><h2 id="4362" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">第2部分:用TFIDF矢量器转换文本数据</h2></div><p id="09e5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我的<a class="ae jg" href="https://medium.com/@zzhu17/a-step-by-step-tutorial-for-conducting-sentiment-analysis-a7190a444366" rel="noopener">上一篇文章</a>中，我讨论了进行情感分析的第一步，即预处理文本数据。这个过程包括标记化、去除停用词和词条化。在本文中，我将讨论将“干净的”文本数据转换成稀疏矩阵的过程。具体来说，我将通过简单的例子讨论不同矢量器的使用。</p><p id="5ab4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们进入更多的技术之前，我想介绍两个在文本分析中广泛使用的术语。对于我们想要分析的文本数据集合，我们称之为<em class="lu">语料库</em>。一个语料库包含几个观察结果，如新闻文章、顾客评论等。这些观察结果中的每一个都被称为<em class="lu">文档</em>。从现在开始我将使用这两个术语。</p><p id="5d23" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">转换步骤的作用是搭建一座桥梁，连接文本数据中携带的信息和机器学习模型。对于情感分析，要对每个文档进行情感预测，机器学习模型需要学习文档中每个唯一单词的情感得分，以及每个单词在那里出现的次数。例如，如果我们想要对某个产品的客户评论进行情感分析，在训练模型之后，机器学习模型更有可能从负面评论中提取像“糟糕”、“不满意”这样的词，而从正面评论中获得像“棒极了”、“棒极了”这样的词。</p><p id="8ea5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">面对有监督的机器学习问题，为了训练模型，我们需要指定特征和目标值。情感分析是在解决一个分类问题，大多数情况下是一个二元分类问题，目标值定义为正和负。用于模型的特征是来自矢量器的转换的文本数据。不同的矢量器构造的特征也不同。在Scikit Learn中，有三个矢量器，CountVectorizer、TFIDFVectorizer和HashingVectorizer。我们先来讨论一下CountVectorizer。</p><p id="85b5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">计数矢量器</strong></p><p id="5413" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">CountVectorizer使用单词包方法，该方法忽略文本结构，只从单词计数中提取信息。它会将每个文档转换成一个向量。向量的输入是这个文档中每个唯一单词的出现次数。当语料库中有m个文档，并且所有m个文档中有n个唯一单词时，CountVectorizer会将文本数据转换为m*n稀疏矩阵。以下示例显示了计数矢量器的用法:</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lv"><img src="../Images/e292c191345f9dffecdac9738300e5cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uPm8nMfL6sS-h1SGI6yHfQ.png"/></div></div></figure><p id="c74b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">CountVectorizer获取文档列表，并通过两步生成稀疏矩阵:拟合和转换。在拟合过程中，矢量器读入文档列表，计算语料库中唯一单词的数量，并为每个单词分配一个索引。对于上面的例子，我们可以看到这两个文档有六个不同的单词，我们根据字母顺序给它们分配了一个索引。请注意，您可以在这里指定停用字词来排除无用的字词。您可以使用默认列表，也可以自定义列表。或者如果已经对文本数据进行了预处理，就可以通过这一步。</p><p id="9ace" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一步是转换拟合的数据。CountVectorizer将计算每个文档中每个唯一单词的出现次数。这里我有两个文档和六个唯一的单词，因此我们将得到一个如上所示的2*6矩阵。为了更好地理解矩阵的元素，这里我有一个图表:</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ma"><img src="../Images/71ed5847ed842967b0889e8b15244538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*amonFSyYdSnm3avTrHy9Sw.png"/></div></div></figure><p id="b902" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，行id与每个文档相对应，列id在匹配过程中跟随唯一单词的索引。例如，单词“day”在两个文档中都出现了，所以第一列输入是(1，1)。如果某个单词没有出现在文档中，则该单词在该文档行中的输入将为0。随着文档数量的增加，矩阵变成稀疏矩阵，因为矩阵中会有更多的0。</p><p id="2539" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">tfidf矢量器</strong></p><p id="4346" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一个更广泛使用的矢量器是TFIDFVectorizer，TFIDF是术语频率，逆文档频率的缩写。除了每个文档中的字数，TFIDF还包括该单词在其他文档中的出现次数。具体来说，TFIDF的计算公式如下:</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/170a46b88c481cf8d59281cba53cca5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*OVJ9r3zskAKK8QukTFX-uw.png"/></div></figure><p id="e79a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中t_i，wj是单词wj在文档I中出现的频率。通过检查等式，可以清楚地看到，第一项是计算术语频率，第二项是计算逆文档频率。第一项是评估单词wj在文档I中出现了多少次，用文档I的长度归一化。较高的词频率指示较高的TFIDF值，表明单词wj通过出现显著的次数而在文档I中扮演非常重要的角色。但是如果wj也出现在I之外的很多其他文档中，wj的作用就会减弱，也就是说它是这个题目的常用词。这个过程被第二项捕获，第二项是wj出现的文档数除以文档总数的倒数。综合两种效果，文档I中TFIDF值高的一个词wj，意味着wj在文档I中出现多次，在其他文档中只出现很少。</p><p id="eb17" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用前一个示例的TFIDF，区别如下:</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mc"><img src="../Images/ec6b04f65b449b4cf4b9ae259511c18e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yI0vqqsUx1d0rgXzQuJuYw.png"/></div></div></figure><p id="11dd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到每个元素的值都变小了，但是矩阵的形状还是一样的。</p><p id="0dd7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">哈希矢量器</strong></p><p id="c32d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一种常用的矢量器叫做哈希矢量器。它通常在处理大型数据集时使用。使用<a class="ae jg" href="https://en.wikipedia.org/wiki/Feature_hashing" rel="noopener ugc nofollow" target="_blank">特征散列</a>，散列矢量器是内存高效的，并确保大型数据集的更好的模型性能。在这篇文章里我不会讲太多细节，但是你可以在这里查阅更多信息<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="e9b4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">附加功能输入</strong></p><p id="37b9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除了指定和定制停用词，我们还可以定制矢量器中的标记化功能。正如我在<a class="ae jg" href="https://medium.com/@zzhu17/a-step-by-step-tutorial-for-conducting-sentiment-analysis-a7190a444366" rel="noopener">上一篇文章</a>中所讨论的，在这里包含定制的tokenize函数会减慢矢量化过程。</p><p id="d417" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在前面的例子中，我们正在构建只有单个单词的稀疏矩阵，我们可以通过包含二元模型来增加特征的数量。我们可以通过在函数中添加<em class="lu"> ngram_range </em>来在函数中指定它。这里有一个例子:</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi md"><img src="../Images/84573a5a2025a5d5f92e7f465f7d9482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hIsE_hm4WLbMHgyqjEh3jQ.png"/></div></div></figure><p id="dafd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过包含bigram，特性的数量从6个增加到11个。有时，当我们在文档中有“不错”这样的词时，包含bigram会提高模型性能。</p><p id="5b53" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您还可以在矢量函数中指定<em class="lu"> min_df </em>和<em class="lu"> max_df </em>。通过指定一个单词在不同的文档中出现多少次才能被认为是一个特征，我们过滤掉在语料库中不太常见的单词。此外，当设置一个单词在不同文档中出现的次数限制(max_df)时，我们忽略了太常见的内容，比如停用词。在不同场景中定制矢量器函数输入应该会提高模型性能。</p><p id="9d68" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">了解矢量器的定制选项非常有用。更多选择，你可以访问每个矢量器的<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> sklearn文档</a>。为了确保最佳的模型性能，我们可以使用GridSearchCV来调整变压器的超参数。在我的<a class="ae jg" href="https://medium.com/@zzhu17/a-step-by-step-tutorial-for-conducting-sentiment-analysis-cf3e995e3171" rel="noopener">下一篇文章</a>中，我将讨论在我的项目中应用TFIDF的更多细节，并构造估计器。</p><p id="1439" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您的阅读！这是我所有博客帖子的列表。如果你感兴趣的话，可以去看看！</p><div class="is it gp gr iu me"><a href="https://zzhu17.medium.com/my-blog-posts-gallery-ac6e01fe5cc3" rel="noopener follow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd jk gy z fp mj fr fs mk fu fw ji bi translated">我的博客文章库</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">我快乐的地方</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">zzhu17.medium.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms ja me"/></div></div></a></div><div class="is it gp gr iu me"><a href="https://zzhu17.medium.com/membership" rel="noopener follow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd jk gy z fp mj fr fs mk fu fw ji bi translated">阅读朱(以及媒体上成千上万的其他作家)的每一个故事</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">zzhu17.medium.com</p></div></div><div class="mn l"><div class="mt l mp mq mr mn ms ja me"/></div></div></a></div></div></div>    
</body>
</html>