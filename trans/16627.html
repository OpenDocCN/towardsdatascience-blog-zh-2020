<html>
<head>
<title>Regularization &amp; Dropout in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的正规化和退出</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regularization-dropout-in-deep-learning-5198c2bf6107?source=collection_archive---------28-----------------------#2020-11-16">https://towardsdatascience.com/regularization-dropout-in-deep-learning-5198c2bf6107?source=collection_archive---------28-----------------------#2020-11-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b64d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">编码，这样你就能理解了</h2></div><p id="8219" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di">在</span>上一篇<a class="ae ln" rel="noopener" target="_blank" href="/code-a-deep-neural-network-a5fd26ec41c4">帖子</a>中，我们已经编码了一个深度密集的神经网络，但是为了拥有一个更好、更完整的神经网络，我们需要它更加健壮，能够抵抗过拟合。你可能听说过，深度神经网络中常用的方法是正则化和丢弃。在本文中，我们将一起理解这两种方法，并用python实现它们。</p><p id="c670" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(我们将在下面直接使用上一篇文章中创建的函数，如果你对一些代码感到困惑，你可能需要查看上一篇<a class="ae ln" rel="noopener" target="_blank" href="/code-a-deep-neural-network-a5fd26ec41c4">文章</a>)</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/350a74a4006da17bdb7072cf31c4223f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DRvwfMEgJZh0Wp7U"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated"><a class="ae ln" href="https://unsplash.com/@fabioha?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">法比奥</a>在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="698c" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">正规化</h1><p id="3bb8" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">正则化通过在损失函数的末尾增加一个额外的惩罚项来帮助防止模型过度拟合。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nb"><img src="../Images/7104c055b062e6296507a65089582d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3xdEOa4XCcBHzYO2LFqeg.png"/></div></div></figure><p id="baeb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe nc nd ne nf b">m</code>是批量大小。所示的正则化称为<code class="fe nc nd ne nf b">L2 regularization</code>，而<code class="fe nc nd ne nf b">L2</code>将平方应用于权重，<code class="fe nc nd ne nf b">L1 regularization</code>应用绝对值，其形式为|W|。</p><p id="b380" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当有太多的重量或重量变得太大时，附加的额外项会扩大损失，并且可调因子λ强调了我们想要对重量进行多少惩罚。</p><h2 id="3433" class="ng mf it bd mg nh ni dn mk nj nk dp mo kr nl nm mq kv nn no ms kz np nq mu nr bi translated">1.为什么惩罚重量有助于防止过度拟合？</h2><blockquote class="ns nt nu"><p id="6652" class="ki kj nv kk b kl km ju kn ko kp jx kq nw ks kt ku nx kw kx ky ny la lb lc ld im bi translated">直观的理解是，在最小化新的损失函数的过程中，一些权重将降低到接近零，使得相应的神经元对我们的结果具有非常小的影响，就好像我们在具有更少神经元的更小的神经网络上训练一样。</p></blockquote><h2 id="a0a4" class="ng mf it bd mg nh ni dn mk nj nk dp mo kr nl nm mq kv nn no ms kz np nq mu nr bi translated">向前</h2><p id="064f" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">在正演过程中，我们只需要改变损失函数。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h2 id="df4a" class="ng mf it bd mg nh ni dn mk nj nk dp mo kr nl nm mq kv nn no ms kz np nq mu nr bi translated">向后的</h2><p id="576a" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated"><code class="fe nc nd ne nf b">L2 regularization</code>的后向传播实际上是直接向前的，我们只需要加上L2项的梯度。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ob"><img src="../Images/7f683e7e0414971a5802db67c833df8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qi8lSlurM0hYOYmkm0gGhw.png"/></div></div></figure><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h2 id="bcfd" class="ng mf it bd mg nh ni dn mk nj nk dp mo kr nl nm mq kv nn no ms kz np nq mu nr bi translated">培养</h2><p id="7071" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">像往常一样，我们在一个二元分类案例上测试我们的模型，并比较有正则化和没有正则化的模型。</p><p id="c618" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">没有正则化的模型</strong></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oc"><img src="../Images/1838c2fb875c2470fd8db296e586904a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*akH4M5XZJtDVNT-op04bxA.png"/></div></div></figure><p id="6e51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">正规化模型</strong></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi od"><img src="../Images/6a9c095c10680cac76522b44e95c3b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wbvx-pjffAr3Ss3yew7pUQ.png"/></div></div></figure><p id="df22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实际上，当我们让<code class="fe nc nd ne nf b">iteration</code>上升时，模型会继续过拟合，这导致除法运算中的错误，怀疑在前进过程中，结果<code class="fe nc nd ne nf b">A</code>太接近0。</p><p id="ecc7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相比之下，正则化模型不会过拟合。有关完整的实施和培训流程，请查看我的<a class="ae ln" href="https://github.com/MJeremy2017/deep-learning/tree/main/regularization" rel="noopener ugc nofollow" target="_blank"> Github Repo </a>。</p><h1 id="31a4" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">拒绝传统社会的人</h1><p id="ea84" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">Dropout通过随机关闭一些输出单元来防止过拟合。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oe"><img src="../Images/21baf1fce3fc560c20831cce35432d47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2uIGSALpIRNxE8rU5FUtng.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">* *[来源]* *:<a class="ae ln" href="https://github.com/enggen/Deep-Learning-Coursera" rel="noopener ugc nofollow" target="_blank">https://github.com/enggen/Deep-Learning-Coursera</a></p></figure><p id="5775" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上述过程中，在每次迭代中，层[2]上的一些单元将被随机静音，这意味着在正向过程中工作的神经元将减少，从而简化了神经网络的整体结构。</p><p id="d08b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同时，经过训练的模型将更加健壮，因为该模型不再依赖于任何特定的神经元(因为它们可能在此过程中被抑制)，所有其他神经元都需要在训练中学习。</p><h2 id="055f" class="ng mf it bd mg nh ni dn mk nj nk dp mo kr nl nm mq kv nn no ms kz np nq mu nr bi translated">向前</h2><p id="ba7c" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">你可以把辍学看作是在前进的过程中增加了额外的一层。</p><p id="517d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在前面的课程中，我们有如下的正向方程式:</p><p id="3ec6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">无脱落</strong></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/dfa0f7a8fa4fe068c5def4efc838781a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*1I14uEoTtTAPq3k1RnXSGA.png"/></div></figure><p id="112f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe nc nd ne nf b">g</code>是激活函数。现在辍学的一个额外的层是适用于A^[l].</p><p id="47ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">辍学</strong></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/28af1148fa8df2d3ba5bc8592c5d86fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*azW6Hc9ZILZ_7XzX0F9MDg.png"/></div></figure><p id="6eb0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe nc nd ne nf b">D</code>是漏失层。辍学层的关键因素是<code class="fe nc nd ne nf b">keep_prob</code>参数，它指定了保持每个单位的概率。假设<code class="fe nc nd ne nf b">keep_prob = 0.8</code>，我们有80%的机会保持每个输出单元不变，20%的机会将它们设置为0。</p><p id="5783" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个实现将会给结果<code class="fe nc nd ne nf b">A</code>添加一个额外的遮罩。假设我们有一个包含如下四个元素的输出A^{[l]}，</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/fd40af9ac87f92415e0d42b54ce73044.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*kcRNYHiLVc7Sa1j2GZR8EQ.png"/></div></figure><p id="601d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们想让第三个单元静音，同时保留其余的单元，我们需要的是一个相同形状的矩阵，并做如下的元素乘法，</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oi"><img src="../Images/385d9c127b46eb24bc8ad5233835834f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T4TnqimoZCFi3FiXy2Iygg.png"/></div></div></figure><h2 id="1ff1" class="ng mf it bd mg nh ni dn mk nj nk dp mo kr nl nm mq kv nn no ms kz np nq mu nr bi translated">向前</h2><p id="a8d1" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">下面有些模块是预导入的，要查看完整代码，请去我的<a class="ae ln" href="https://github.com/MJeremy2017/deep-learning/tree/main/dropout" rel="noopener ugc nofollow" target="_blank"> Github Repo </a>。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="5915" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里我们将<code class="fe nc nd ne nf b">D</code>初始化为与<code class="fe nc nd ne nf b">A's</code>相同的形状，并基于<code class="fe nc nd ne nf b">keep_prob</code>将其转换为0和1矩阵。</p><p id="ce28" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">注意，退学后，成绩</strong> <code class="fe nc nd ne nf b"><strong class="kk iu">A</strong></code> <strong class="kk iu">需要重新标度！</strong>因为一些神经元在此过程中被抑制，相应地，左侧神经元需要被增强以匹配期望值。</p><h2 id="247d" class="ng mf it bd mg nh ni dn mk nj nk dp mo kr nl nm mq kv nn no ms kz np nq mu nr bi translated">向后的</h2><p id="3c25" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">反向过程是将同一个函数<code class="fe nc nd ne nf b">D</code>屏蔽到对应的<code class="fe nc nd ne nf b">dA</code>。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="0764" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">反向传播等式与我们在深度密集网络实施中介绍的等式相同。唯一的区别在于矩阵<code class="fe nc nd ne nf b">D</code>。除了最后一层，所有其他有缺失的层将应用相应的蒙版<code class="fe nc nd ne nf b">D</code>到<code class="fe nc nd ne nf b">dA</code>。</p><p id="22f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">注意，在反向传播中，</strong> <code class="fe nc nd ne nf b"><strong class="kk iu">dA</strong></code> <strong class="kk iu">也需要重新缩放。</strong></p><p id="f528" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">培训和评估部分与辍学，如果你有兴趣，请检查我的Github上面的链接。</p><h1 id="2074" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">结论</h1><p id="041e" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">正则化和丢失都是广泛采用的防止过拟合的方法，正则化是通过在损失函数的末端增加一个额外的惩罚项来实现的，而丢失是通过在正向过程中随机静音一些神经元来实现的，以使网络更加简洁。</p></div></div>    
</body>
</html>