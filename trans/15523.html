<html>
<head>
<title>Train a regression model using a decision tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用决策树训练回归模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/train-a-regression-model-using-a-decision-tree-70012c22bcc1?source=collection_archive---------13-----------------------#2020-10-26">https://towardsdatascience.com/train-a-regression-model-using-a-decision-tree-70012c22bcc1?source=collection_archive---------13-----------------------#2020-10-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2ce5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对于复杂的非线性数据</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5da6655a72f4ef001e319cdbdce48f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-NBGDOOVlNzOL882kI4FSg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">作者图片</strong></p></figure><p id="5170" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">决策树</strong>是一种非参数监督学习方法，能够发现数据中复杂的非线性关系。他们可以执行分类和回归任务。但是在本文中，我们只关注带有回归任务的决策树。为此，等价的Scikit-learn类是<strong class="lb iu"><em class="lv">DecisionTreeRegressor</em></strong>。</p><p id="f3ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将从讨论如何用决策树训练、可视化和预测回归任务开始。我们还将讨论如何正则化决策树中的超参数。这样可以避免过拟合的问题。最后，我们将讨论决策树的一些优点和缺点。</p><h2 id="d39d" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">代码约定</h2><p id="32f3" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我们使用以下代码惯例来导入必要的库并设置打印样式。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="4cc9" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">目标受众</h2><p id="3152" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我假设您对决策树中使用的术语及其幕后工作原理有基本的了解。在本教程中，将重点介绍模型超参数调整技术，如<em class="lv">k</em>-折叠交叉验证。</p><h1 id="cf9f" class="mw lx it bd ly mx my mz mb na nb nc me jz nd ka mh kc ne kd mk kf nf kg mn ng bi translated">问题陈述</h1><p id="2f11" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我们打算在流行的加州住房数据集(<a class="ae nh" href="https://drive.google.com/file/d/1Kees3lk-Zo7AsrYz7svcj8Hnbr6gHok6/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">Cali _ housing . CSV</strong></a>)中为非线性特征<strong class="lb iu">经度</strong>和<strong class="lb iu"> MedHouseVal </strong>(房屋中值)构建一个模型。</p><p id="256c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看数据集的前几行。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/127f579b1b4569aeb960ecf0d7f90b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*X4RAIkewNZaOWardM1ukvw.png"/></div></figure><p id="2636" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个数据集有20640个观察值！</p><h1 id="fd01" class="mw lx it bd ly mx my mz mb na nb nc me jz nd ka mh kc ne kd mk kf nf kg mn ng bi translated">可视化数据</h1><p id="83dc" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">为了查看上述两个特征之间的关系，我们使用seaborn创建了以下散点图。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9485c9ca0ef4995a6f741fe81a7c3180.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*BYr-cdGcjCbTT_caXSllHw.png"/></div></figure><p id="e60b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您在散点图中看到的，这两个特征之间存在复杂的非线性关系。决策树回归是一种强大的模型，能够在数据中发现这种复杂的非线性关系。</p><h1 id="e2f7" class="mw lx it bd ly mx my mz mb na nb nc me jz nd ka mh kc ne kd mk kf nf kg mn ng bi translated">建立模型</h1><p id="78dd" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">让我们使用sci kit-learn decision tree regressor类来训练我们的模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="7f1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们想象一下我们的模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/81f149601c950aa341b4592d79abe328.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*1DP5SaVQszSJs6ugaSBZPA.png"/></div></figure><p id="a41a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用<strong class="lb iu"> <em class="lv"> Graphviz </em> </strong>来可视化这个模型的树形图。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/d8e5ecee51c49bb4d43245254a53e2a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KG5xYueTFdHSdLgJnXOwxw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5d2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的树中有4个叶节点。这是因为我们设置了<strong class="lb iu"> max_depth=2 </strong>。叶节点的数量相当于<strong class="lb iu"> 2^max_depth </strong>。超参数<strong class="lb iu"> max_depth </strong>控制分支的复杂度。</p><p id="9d74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu"> max_depth=2 </strong>的这种情况下，模型不太符合训练数据。这就是所谓的<strong class="lb iu"> <em class="lv">欠配</em> </strong>的问题。</p><p id="e6be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用<strong class="lb iu"> max_depth=15 </strong>创建一个不同的模型。通过重复相同的步骤，我们可以创建以下模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/aa798b3c89c26f0d65c4cab9a41ab493.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*LlizXqB7QBVKdHipQRKzDA.png"/></div></figure><p id="e95a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们不能可视化树形图，因为它有32768 (2 ⁵)个叶节点！在这种情况下，当<strong class="lb iu"> max_depth=15 </strong>时，模型非常适合训练数据，但是它不能推广到新的输入数据。这就是所谓的<strong class="lb iu"> <em class="lv">过拟合</em> </strong>的问题。在这里，模型已经适应了数据的噪声，它试图记住数据，而不是学习任何类型的模式。</p><p id="ec19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，当我们创建最佳模型时，我们应该避免欠拟合和过拟合的情况。</p><p id="a0f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，超参数<strong class="lb iu"> max_depth </strong>的最佳值是多少呢？找出<strong class="lb iu"> max_depth </strong>的最佳值(不能太小也不能太大)称为<strong class="lb iu"> <em class="lv">超参数调整</em> </strong>。</p><h1 id="68ff" class="mw lx it bd ly mx my mz mb na nb nc me jz nd ka mh kc ne kd mk kf nf kg mn ng bi translated">决策树回归的超参数调整</h1><p id="92d6" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">主要有两种方法。</p><ol class=""><li id="56ec" class="nl nm it lb b lc ld lf lg li nn lm no lq np lu nq nr ns nt bi translated">使用Scikit-learn<strong class="lb iu">train _ test _ split()</strong>函数</li><li id="9d46" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">使用<em class="lv"> k </em>折叠交叉验证</li></ol><h2 id="31cf" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">使用Scikit-learn<strong class="ak">train _ test _ split()</strong>函数</h2><p id="f4d4" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">这是一个实现起来非常简单的方法，但也是一个非常有效的方法。你需要做的就是将原始数据集拆分成两部分，分别叫做<strong class="lb iu"> <em class="lv">训练集</em> </strong>和<strong class="lb iu"> <em class="lv">测试集</em> </strong>。我们可以使用sci kit-learn<strong class="lb iu"><em class="lv">train _ test _ split()</em></strong>函数轻松做到这一点。输入是<strong class="lb iu">特征矩阵— X </strong>和<strong class="lb iu">目标向量— y </strong>。我们通常会保留大约10%-30%的测试数据。超参数<strong class="lb iu"> <em class="lv"> random_state </em> </strong>接受整数。通过指定这一点，我们可以确保在不同的执行中有相同的分割。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="d4c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们用<strong class="lb iu"> X_train </strong>、<strong class="lb iu"> y_train </strong>训练模型，用<strong class="lb iu"> X_test </strong>、<strong class="lb iu"> y_test </strong>测试模型。这是针对从1到20范围内的<strong class="lb iu"> <em class="lv"> max_depth </em> </strong>超参数的不同值进行的，并绘制测试误差与训练误差。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/83c66004aed566a31d3e226e5a0cf0f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*aaZLlKgBE8pAFkvsEdwNwg.png"/></div></figure><p id="147b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在树深度= 7的点上，测试误差开始增加，尽管训练误差持续减小。从该图中，我们可以确定<strong class="lb iu"> <em class="lv"> max_depth </em> </strong>超参数的最佳值为7。</p><h2 id="c225" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">使用k倍交叉验证</h2><p id="7b19" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">一种更有前途的调整模型超参数的方法是使用<em class="lv"> k </em>折叠交叉验证。通过使用这种方法，您可以同时调整多个超参数。这里，在训练集上进行训练，之后在验证集上进行评估，并且可以在测试集上进行最终评估。然而，将原始数据集划分为3个集合(训练、验证和测试)大大减少了训练过程的可用数据。</p><p id="2f3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个解决方案，我们使用一个叫做<strong class="lb iu"> <em class="lv"> k </em>的过程——折叠交叉验证</strong>其中<em class="lv"> k </em>是折叠的次数(通常是5或10)。在<em class="lv">k</em>-折叠交叉验证中，</p><ul class=""><li id="1d55" class="nl nm it lb b lc ld lf lg li nn lm no lq np lu nz nr ns nt bi translated">我们首先使用<strong class="lb iu"> train_test_split() </strong>函数将原始数据集分为训练集和测试集。</li><li id="5c27" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated">列车组进一步分为<em class="lv">k</em>-折叠数。</li><li id="6bae" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated">使用褶皱的<em class="lv">k1</em>训练模型，并在剩余的褶皱上进行验证。</li><li id="2258" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated">该过程进行<em class="lv"> k </em>次，并且在每次执行时报告性能测量。然后取平均值。</li><li id="991c" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated">找到参数后，对测试集进行最终评估。</li></ul><p id="d9a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图说明了<em class="lv"> k </em> -fold交叉验证程序。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/da0e8634c3de72e25ea0140cdf396135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*26cMt9Tgm58CiDNLg7sDPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae nh" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn官网</a></p></figure><p id="ccce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以使用<strong class="lb iu">网格搜索</strong>方法和<em class="lv"> k </em>折叠交叉验证来调整超参数。等效的Scikit-learn函数是<strong class="lb iu"> <em class="lv"> GridSearchCV </em> </strong>。它为指定的<em class="lv"> k </em>折叠数找到所有超参数组合。</p><p id="c020" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设您想在DecisionTreeRegressor中为以下两个超参数从所有超参数组合中找出最佳组合。</p><ul class=""><li id="2b37" class="nl nm it lb b lc ld lf lg li nn lm no lq np lu nz nr ns nt bi translated"><strong class="lb iu">最大深度:</strong>1–10(10个不同的值)</li><li id="a82e" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated"><strong class="lb iu"> min_samples_split: </strong> 10，20，30，40，50 (5个不同的值)</li></ul><p id="d824" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图显示了超参数空间。如果取一个点(一个组合)，<strong class="lb iu"> <em class="lv"> GridSearchCV </em> </strong>函数会搜索该组合，并使用这些值以及<em class="lv"> k </em>折叠交叉验证来训练模型。同样，它搜索所有的组合(这里是10 x 5= 50！).所以，它执行50 x <em class="lv"> k </em>次！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/c6001be72307a8ea69dfc5bb9ea17474.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*07S_j_jYsJD02HIxY341mA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/69d6233da311243838f34009e8cb16e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*gyx2e4z9s4O1o8DcIuC29Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最佳值</p></figure><p id="6035" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以使用这些最佳值创建最佳模型。它避免了过拟合和欠拟合的情况。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0f872caa7618c0f8e8224f2e48c693a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*zcaULDqO56KHILjeQ-MnSw.png"/></div></figure><h2 id="415d" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">决策树的优势</h2><ul class=""><li id="8a11" class="nl nm it lb b lc mp lf mq li od lm oe lq of lu nz nr ns nt bi translated">不需要特征缩放</li><li id="87c1" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated">可用于非线性数据</li><li id="a8ba" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated">非参数:数据中很少的潜在假设</li><li id="7f71" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated">可用于回归和分类</li><li id="9bad" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated">易于想象</li><li id="3263" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated">容易理解</li></ul><h2 id="af28" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">决策树的缺点</h2><ul class=""><li id="81f7" class="nl nm it lb b lc mp lf mq li od lm oe lq of lu nz nr ns nt bi translated">决策树训练在计算上是昂贵的，尤其是当通过<em class="lv"> k </em>折叠交叉验证调整模型超参数时。</li><li id="a51f" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated">数据的微小变化会导致决策树结构的巨大变化。</li></ul><p id="0941" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本教程由<a class="ae nh" href="https://www.linkedin.com/in/rukshan-manorathna-700a3916b/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">鲁克山·普拉莫迪塔</em></a><em class="lv"/><a class="ae nh" href="https://medium.com/data-science-365" rel="noopener">数据科学365博客</a>作者设计创作。</p><h2 id="afc1" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">本教程中使用的技术</h2><ul class=""><li id="0cad" class="nl nm it lb b lc mp lf mq li od lm oe lq of lu nz nr ns nt bi translated"><strong class="lb iu"> Python </strong>(高级编程语言)</li><li id="6a6d" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated"><strong class="lb iu"> numPy </strong>(数字Python库)</li><li id="1e15" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated"><strong class="lb iu">熊猫</strong> (Python数据分析和操作库)</li><li id="f3a1" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated"><strong class="lb iu"> matplotlib </strong> (Python数据可视化库)</li><li id="46e1" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated"><strong class="lb iu"> seaborn </strong> (Python高级数据可视化库)</li><li id="473d" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated"><strong class="lb iu"> Scikit-learn </strong> (Python机器学习库)</li><li id="a3d6" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nz nr ns nt bi translated"><strong class="lb iu"> Jupyter笔记本</strong>(集成开发环境)</li></ul><h2 id="572e" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">本教程中使用的机器学习</h2><ul class=""><li id="4c8a" class="nl nm it lb b lc mp lf mq li od lm oe lq of lu nz nr ns nt bi translated"><strong class="lb iu">决策树回归</strong></li></ul><p id="3da7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">2020–10–26</p></div></div>    
</body>
</html>