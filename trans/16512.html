<html>
<head>
<title>Spark vs Pandas, part 4— Recommendations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">星火大战熊猫，第4部分—建议</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spark-vs-pandas-part-4-recommendations-35fc554573d5?source=collection_archive---------21-----------------------#2020-11-14">https://towardsdatascience.com/spark-vs-pandas-part-4-recommendations-35fc554573d5?source=collection_archive---------21-----------------------#2020-11-14</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="8217" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">为什么星火和熊猫都不比对方强。或者:总是为正确的工作选择正确的工具。</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/e2151dcea21267adabbb1055cee167ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Vodusai3UccTl1ul"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">塞萨尔·卡利瓦里诺·阿拉贡在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="d346" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">最初我想写一篇文章来公平地比较熊猫和火花，但它继续增长，直到我决定把它分开。这是小编的第二部。</p><ul class=""><li id="e10a" class="lw lx iu lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated"><a class="ae kz" rel="noopener" target="_blank" href="/spark-vs-pandas-part-1-pandas-10d768b979f5">星火大战熊猫，第一部——熊猫</a></li><li id="c3ab" class="lw lx iu lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><a class="ae kz" rel="noopener" target="_blank" href="/spark-vs-pandas-part-2-spark-c57f8ea3a781">星火大战熊猫，第二部——星火</a></li><li id="074a" class="lw lx iu lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><a class="ae kz" rel="noopener" target="_blank" href="/spark-vs-pandas-part-3-scala-vs-python-7b267b130158">星火大战熊猫，第三部分——语言</a></li><li id="8964" class="lw lx iu lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">星火大战熊猫，第4部分—推荐</li></ul><h2 id="0762" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">期待什么</h2><p id="4b0e" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">本系列的最后一部分将为您提供一些建议，告诉您如何在实现给定任务的两种技术之间进行选择。</p></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><h1 id="759c" class="np ml iu bd mm nq nr ns mp nt nu nv ms ka nw kb mv kd nx ke my kg ny kh nb nz bi translated">什么时候更喜欢熊猫而不是星火</h1><p id="7bf1" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">在详细分析了两个竞争者Pandas和Spark之后，我们现在可以总结两者的优势和劣势，并提供何时使用什么的指示。</p><p id="7a80" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">先说熊猫吧。</p><h2 id="d66d" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">强项</h2><p id="35f4" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">熊猫很容易使用，你可以找到很多有价值的信息和在线资源。只要数据量不太大，Pandas执行所有it操作的速度都相当快。它很好地集成到一个完整的数字、统计和机器学习库生态系统中，如SciKit Learn、Tensorflow等。</p><h2 id="bdc1" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">弱点</h2><p id="6aba" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">熊猫一点也没有伸缩性。它不能利用多个CPU，并且整个数据集需要放入本地机器的RAM中。像Dask这样的一些项目试图解决这些缺点，但那是另一回事。</p><p id="9078" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">Python作为一种语言，由于是动态类型的，所以有点弱。编写健壮的代码比静态编译语言更难。</p><h2 id="c2d3" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">结论</h2><p id="0600" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">因为它的简单性、灵活性和可用性，我总是用Pandas进行数据探索和实验——只要数据适合内存。具体到ML项目，我不会三思而行，从熊猫开始，因为所有强大的库，都与熊猫很好地集成在一起。即使最终的数据量可能太大，熊猫和它的朋友仍然是一个足够简单和灵活的工具，可以用完整数据集的子集进行第一次实验。</p><p id="cd60" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">另一方面，现在我在使用Pandas进行生产工作负载之前会三思，因为Python作为一种动态类型语言，其正确性保证较弱。但是由于许多重要的ML库的可用性，Python和Pandas也在生产中占有一席之地。(可惜)。</p><h1 id="ddf0" class="np ml iu bd mm nq oa ns mp nt ob nv ms ka oc kb mv kd od ke my kg oe kh nb nz bi translated">什么时候更喜欢星火而不是熊猫</h1><p id="3fed" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Spark在熊猫有一些弱点的许多领域大放异彩，我们将在下面看到。</p><h2 id="2adb" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">强项</h2><p id="378c" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Spark可以很好地扩展——包括CPU数量、机器数量以及最重要的数据量。除了时间之外，您可以用有限的资源处理多少数据并没有真正的限制。</p><p id="459d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">由于各种数据源和接收器都有大量的连接器可用，Spark非常适合集成来自不同来源的数据。</p><p id="bd35" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">最后，依靠Scala作为静态类型和编译语言，Spark代码通常比Python代码具有更高的内在健壮性。这使得Spark和Scala成为非常好的产品候选。</p><h2 id="2931" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">弱点</h2><p id="99fb" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Spark是为海量数据而生的——尽管它比它的老祖先Hadoop快得多，但在小数据集上仍然经常较慢，对于Pandas来说不到一秒钟。</p><p id="5fee" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">Spark提供了一些ML算法，但是你可能永远也不会得到像Python那样丰富的宇宙。</p><p id="1493" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">需要记住的一点是，Spark被设计为在机器集群中运行的关系代数——但是关系代数的处理原子(连接、投影、过滤、聚合、简单转换等)不同于矩阵代数的处理原子(矩阵乘法、分解等),后者是大多数ML算法所需要的。当你花些时间观察Spark 中ML算法的<a class="ae kz" href="https://github.com/apache/spark/tree/master/mllib/src/main/scala/org/apache/spark/ml" rel="noopener ugc nofollow" target="_blank">实现时，你会发现开发人员不得不将数值问题转化为map/reduce问题，以便进行分布式处理。虽然这是可能的，但它当然比使用分布式矩阵代数要困难得多，这反过来也解释了ML领域中新功能Spark的缓慢开发。</a></p><h2 id="4e82" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">结论</h2><p id="962e" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Spark非常适合典型的ETL/ELT工作负载，但由于可用算法的数量有限，它只是<em class="of">我的</em>机器学习项目的第二选择。如果你有大量的数据，子采样是行不通的，Spark仍然是一个不错的选择。</p><h1 id="76e8" class="np ml iu bd mm nq oa ns mp nt ob nv ms ka oc kb mv kd od ke my kg oe kh nb nz bi translated">星火和熊猫的结合</h1><p id="ed36" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">到目前为止，我的观点是在给定的任务中使用<em class="of">或者</em>熊猫<em class="of">或者</em> Spark，但是不将它们组合在一个应用程序中。但有时候这个世界会有一些礼物送给你，这次是<em class="of"> PySpark </em>。尽管PySpark主要是Spark的Python包装器，但它包含了对在Spark 中集成Pandas代码的<em class="of">的支持。</em></p><p id="295d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">Spark内部直接支持Pandas的主要驱动力是，即使是Spark开发人员也明白Spark不能也不应该试图在某些场景中取代Pandas，尤其是在项目的ML部分。相反，Spark将其开发重点放在整合像Pandas甚至Tensorflow这样的框架上。</p><p id="d20b" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">Spark基本上提供了两个级别的熊猫集成:</p><h2 id="2fd4" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">转换数据帧</h2><p id="068c" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Pandas的第一个也是最简单直接的集成是Spark数据帧和Pandas数据帧之间的转换能力。这允许开发人员使用两种框架并在它们之间切换。但是要注意，Spark不会神奇地消除Pandas的限制:当将Spark数据帧转换成Pandas数据帧时，整个数据集再次需要适合本地机器的RAM。</p><p id="959e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">虽然这种限制对于某些场景来说可能是一个障碍，但在其他场景中是可以接受的，在这些场景中，您使用Spark来减少数据量(通过采样或聚合)，然后使用Pandas及其朋友(SciKit Learn等)来继续处理较小的数据集。</p><h2 id="3170" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">嵌入熊猫</h2><p id="2535" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">正如我们所见，简单地在Spark数据帧和Pandas数据帧之间来回切换是不可取的，但幸运的是，在PySpark应用程序中使用Pandas有一种更好的方法:</p><p id="5d45" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在2.3.0版本中，Apache Spark引入了所谓的<a class="ae kz" href="https://spark.apache.org/docs/3.0.1/sql-pyspark-pandas-with-arrow.html#pandas-udfs-aka-vectorized-udfs" rel="noopener ugc nofollow" target="_blank"> <em class="of"> Pandas用户定义函数</em></a>(UDF)以及已经存在的Python UDFs。在2.3.0版本之前，编写应该由Apache Spark在所有执行器上并行执行的定制Python代码的唯一方法是编写一个包含所需逻辑的小Python函数，并将该函数包装到Python UDF中。然后Spark将对每条记录执行UDF。</p><p id="3892" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">虽然听起来不错，但这种方法是出了名的慢。Spark中的Python UDFs是通过在Spark(位于一个JVM进程中)和多个Python进程之间交换数据来实现的，然后为每条记录调用用户定义的Python函数。这种方法包含两个重要的瓶颈:首先，数据交换涉及CPU密集型的数据序列化和反序列化步骤，其次，为每个单独的记录调用Python函数非常慢。</p><p id="1422" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">为了改善这种情况，Spark实现了一个新的API来创建Pandas UDFs，以提供显著的性能提升。使用Pandas UDFs，您现在可以提供Python函数，这些函数不再对单个记录起作用，而是转换Pandas数据帧或包含要转换的多批记录的系列。此外，通过使用<a class="ae kz" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Arrow </a> Spark不再需要执行昂贵的序列化/反序列化。相反，Spark使用共享内存或直接传递内存块，不进行任何转换，在JVM和Python之间交换数据。</p><p id="7f65" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这种结合将把来自两个世界的特性引入到一个应用程序中:通过使用Pandas，您有了更大程度的灵活性，并且通过在Spark中嵌入Pandas代码，它可以在集群中的多台机器上并行执行。</p></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><h1 id="52a2" class="np ml iu bd mm nq nr ns mp nt nu nv ms ka nw kb mv kd nx ke my kg ny kh nb nz bi translated">结论</h1><p id="5c22" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">不要试图用Spark代替熊猫，它们是互补的，各有利弊。</p><p id="38ad" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">使用熊猫还是Spark取决于你的用例。对于大多数机器学习任务，你可能最终会使用熊猫，即使你用Spark做预处理。但是对于复杂的数据工程任务，通常也需要扩展到大量的数据，我强烈推荐使用Spark和Scala(不要害怕Scala——从项目的角度来看，投资Scala是有回报的)。</p></div></div>    
</body>
</html>