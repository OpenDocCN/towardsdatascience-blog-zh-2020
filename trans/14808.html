<html>
<head>
<title>Instance-level Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实例级识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/instance-level-recognition-6afa229e2151?source=collection_archive---------40-----------------------#2020-10-12">https://towardsdatascience.com/instance-level-recognition-6afa229e2151?source=collection_archive---------40-----------------------#2020-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3c3a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">实例级识别的简介、挑战和最新获奖解决方案。</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/0e110f89fc98008cda77c1b6ed5307fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R4lmpKnfORcqE1uvOp02cA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">实例级识别，按作者分类的图像</p></figure><p id="4c10" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇博客中，我将介绍实例级识别、用例、挑战、当前可用的数据集，以及这些挑战/数据集的最新成果(最近的获奖解决方案)。</p><h1 id="4357" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">介绍</h1><p id="ae65" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><strong class="ky ir">I</strong>instance<strong class="ky ir">L</strong>level<strong class="ky ir">R</strong>ecognition(ILR)，是一个视觉识别任务，用来识别一个对象的特定实例，而不仅仅是对象类。</p><p id="023d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如上图所示，painting是一个对象类，达芬奇的“蒙娜丽莎”是该绘画的一个实例。同样，印度的泰姬陵也是物体类建筑的一个实例。</p><h1 id="62c3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">用例</h1><ul class=""><li id="2773" class="mp mq iq ky b kz mk lc ml lf mr lj ms ln mt lr mu mv mw mx bi translated"><a class="ae my" href="https://www.kaggle.com/c/landmark-recognition-2020" rel="noopener ugc nofollow" target="_blank">地标识别</a>:识别图像中的地标。</li><li id="0bfc" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated"><a class="ae my" href="https://www.kaggle.com/c/landmark-retrieval-2020" rel="noopener ugc nofollow" target="_blank">地标检索</a>:从大规模数据库中检索相关地标图像。</li><li id="c760" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">艺术品识别:识别图像中的艺术品。</li><li id="742f" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">产品检索:从大规模数据库中检索相关产品图片。</li></ul><h1 id="2fd7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">挑战</h1><ul class=""><li id="7932" class="mp mq iq ky b kz mk lc ml lf mr lj ms ln mt lr mu mv mw mx bi translated"><strong class="ky ir">大规模:</strong>识别任务的大多数当前技术水平的结果是在非常有限的类别上测量的，例如<a class="ae my" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>中的大约1000个图像类别，<a class="ae my" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO </a>中的大约80个类别。但像地标检索和识别这样的用例有20万多个类别，例如在<a class="ae my" href="https://www.kaggle.com/c/landmark-recognition-2020" rel="noopener ugc nofollow" target="_blank">谷歌地标数据集V2 (GLDv2) </a>中，亚马逊上有10万多个产品类别。</li><li id="9a17" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated"><a class="ae my" href="https://en.wikipedia.org/wiki/Long_tail" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">长尾</strong> </a> <strong class="ky ir"> : </strong>很少热门的地方有超过1000+的图片但是很多不太知名的地方在<a class="ae my" href="https://www.kaggle.com/c/landmark-recognition-2020" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> GLDv2 </strong> </a> <strong class="ky ir">中图片少于5张。</strong></li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ne"><img src="../Images/df62b7cb72515e4c724255e19518664a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aC5zPseREbjkF5WLng2EnQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><a class="ae my" href="https://github.com/cvdfoundation/google-landmark" rel="noopener ugc nofollow" target="_blank">谷歌地标数据集v2</a><strong class="bd nf">【gld v2】</strong>类别分布，图片来自<a class="ae my" href="https://arxiv.org/pdf/2004.01804.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2004.01804.pdf</a></p></figure><ul class=""><li id="487b" class="mp mq iq ky b kz la lc ld lf ng lj nh ln ni lr mu mv mw mx bi translated"><strong class="ky ir">类内可变性:</strong>地标大多分布在广阔的区域内，并且具有非常高的类内可变性，如下图所示。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nj"><img src="../Images/04c9cc89ae3328ea33cb291327f1aaa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aeDQnU_vQMjR0Vf4ZGd6aA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片来自<a class="ae my" href="https://github.com/cvdfoundation/google-landmark" rel="noopener ugc nofollow" target="_blank">谷歌地标数据集v2 </a> <strong class="bd nf"> (GLDv2) </strong></p></figure><ul class=""><li id="f56c" class="mp mq iq ky b kz la lc ld lf ng lj nh ln ni lr mu mv mw mx bi translated"><strong class="ky ir">嘈杂的标签:</strong>机器学习模型的成功依赖于高质量的带标签训练数据，因为标签错误的存在会大大降低模型的性能。这些噪声标签如下图所示，不幸的是，噪声标签是大型训练集的一部分，需要额外的学习步骤。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3bab3f90a9fc2dad91de40b1792cbd52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*Zbz1rloJaGZMPkOp_Rdjkw.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来自AliProducts的嘈杂标签。图片来自<a class="ae my" href="https://arxiv.org/pdf/2008.11586.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2008.11586.pdf</a></p></figure><h1 id="84f1" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据集</h1><ul class=""><li id="5ce6" class="mp mq iq ky b kz mk lc ml lf mr lj ms ln mt lr mu mv mw mx bi translated"><a class="ae my" href="https://arxiv.org/pdf/2004.01804.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">谷歌地标数据集V2 (GLDV2) </strong> </a> <strong class="ky ir"> : </strong>谷歌地标数据集v2是人工和自然地标领域大规模、细粒度实例识别和图像检索的新基准。下图显示了地标数据集的所有细节</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nl"><img src="../Images/aadad588688fe014d6aa4844da1471bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*09OHPcf5cA5QPbGqjmW-XQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片来自<a class="ae my" href="https://arxiv.org/pdf/2004.01804.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2004.01804.pdf</a></p></figure><ul class=""><li id="9d2b" class="mp mq iq ky b kz la lc ld lf ng lj nh ln ni lr mu mv mw mx bi translated"><a class="ae my" href="https://retailvisionworkshop.github.io/recognition_challenge_2020/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> CVPR 2020阿里产品挑战赛</strong> </a> <strong class="ky ir"> : </strong>阿里产品挑战赛是一个大规模、嘈杂、细粒度的产品数据集，包含约50K个类别，约3M张图片。该数据集是作为研究世界领先的电子商务公司遇到的商品图像识别问题的竞赛而提出的。本次比赛是零售视觉研讨会<a class="ae my" href="https://retailvisionworkshop.github.io/" rel="noopener ugc nofollow" target="_blank">的一部分，零售视觉CVPR 2020研讨会</a>在<a class="ae my" href="http://cvpr2020.thecvf.com/" rel="noopener ugc nofollow" target="_blank"> CVPR 2020 </a>举行。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nm"><img src="../Images/033c3dc14c3f4c8ddcd3b1325dd32ceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aBlYU4lBbUh6Xtcf.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片来自<a class="ae my" href="https://retailvisionworkshop.github.io/recognition_challenge_2020/" rel="noopener ugc nofollow" target="_blank">https://retail vision workshop . github . io/recognition _ challenge _ 2020/</a></p></figure><ul class=""><li id="8ca9" class="mp mq iq ky b kz la lc ld lf ng lj nh ln ni lr mu mv mw mx bi translated"><a class="ae my" href="https://drive.google.com/file/d/1SgSbVnvXLHKEhAjdlmQbwMWiC9HLl0DN/view" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">亚马逊产品挑战赛</strong> </a> <strong class="ky ir"> : </strong>亚马逊产品数据集在<a class="ae my" href="https://ilr-workshop.github.io/ECCVW2020/" rel="noopener ugc nofollow" target="_blank"> ILR ECCVW 2020 </a> workshop公布，将于<strong class="ky ir"> 2021 Q1 </strong>发布。由于产品目录图像是由专业人员拍摄的，而查询图像通常是由手机摄像头拍摄的，并且光照条件差，背景杂乱，因此数据集中的图像存在域不匹配。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nn"><img src="../Images/d1733cb19e3ec26ef7039a2c3d976003.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5qlUmgXzsJu38uenF71_qw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片来自<a class="ae my" href="https://drive.google.com/file/d/1SgSbVnvXLHKEhAjdlmQbwMWiC9HLl0DN/view" rel="noopener ugc nofollow" target="_blank">https://drive . Google . com/file/d/1 sgsvnvxlhkehajdlmlqbwmwic 9 hll 0 dn/view</a></p></figure><h1 id="04d3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结果</h1><p id="01af" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">下面我将讨论地标识别和产品识别的最新获奖解决方案。</p><h1 id="cbab" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">谷歌地标识别</strong></h1><p id="bae7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在<a class="ae my" href="https://www.kaggle.com/c/landmark-recognition-2020/" rel="noopener ugc nofollow" target="_blank">地标识别挑战赛</a>中，任务是从图像中预测地标标签，自2018年起每年都进行。下图显示了本次比赛的<a class="ae my" href="https://www.kaggle.com/c/landmark-recognition-2020/overview/evaluation" rel="noopener ugc nofollow" target="_blank">全球平均精度</a>的进度。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi no"><img src="../Images/a467837df34104d8c68928702994a3ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*ivxuqUdEyyZC6rzedrv4Tw.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">作者图片</p></figure><h2 id="2384" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated"><a class="ae my" href="https://www.kaggle.com/c/landmark-recognition-2020/discussion/187821" rel="noopener ugc nofollow" target="_blank"> 2020年解决方案</a></h2><h2 id="27a3" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated"><strong class="ak">建筑</strong></h2><p id="86df" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">一个集合了7个全局描述符的模型(<a class="ae my" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"> SeResNext101 </a>，<a class="ae my" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNet-B3 </a>，<a class="ae my" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank"> ResNet152 </a>，Res2Net101)。下图显示了SeResNext101主干的设置，对于其他主干，我们也遵循类似的架构。每个主干网络使用<a class="ae my" href="https://arxiv.org/pdf/1711.02512.pdf" rel="noopener ugc nofollow" target="_blank">广义平均(GeM)池</a>聚合，然后是线性颈(Linear neck)【512，<a class="ae my" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank"> BatchNorm1D </a>，<a class="ae my" href="https://arxiv.org/pdf/1502.01852v1.pdf" rel="noopener ugc nofollow" target="_blank"> PReLU </a>，最后馈入<a class="ae my" href="https://arxiv.org/pdf/1801.07698.pdf" rel="noopener ugc nofollow" target="_blank">弧形边缘头</a>。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ob"><img src="../Images/acf342c495d916e944de8a8a02e69fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MZ7G2_TzdaCQIQmw"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">SEResNext101 backbone的设置，图片来自<a class="ae my" href="https://www.kaggle.com/c/landmark-recognition-2020/discussion/187821" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/landmark-recognition-2020/discussion/187821</a></p></figure><p id="b474" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae my" rel="noopener" target="_blank" href="/an-overview-of-resnet-and-its-variants-5281e2f56035"> ResNet变体</a> (152 &amp; 101)都依赖于识别跳过一层或多层的快捷连接来解决渐变消失的问题。<a class="ae my" href="https://arxiv.org/pdf/1709.01507.pdf" rel="noopener ugc nofollow" target="_blank">T5】SeResNeXt</a>是ResNeXt的变种，是一个<a class="ae my" rel="noopener" target="_blank" href="/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202">初始网</a>的快捷连接，<strong class="ky ir"> Se </strong>是指ResNeXt中增加的<a class="ae my" rel="noopener" target="_blank" href="/squeeze-and-excitation-networks-9ef5e71eacd7">挤压和激励</a>模块。Se 网络通过自适应调整特征图的权重来改善信道相关性。<a class="ae my" href="https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html" rel="noopener ugc nofollow" target="_blank"> EfficientNet </a>是一种先进的图像分类网络，它依靠自动机器学习来找出最佳的基础网络和高效的复合缩放，以根据可用的计算资源来实现改进的结果。</p><p id="a6b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae my" href="https://arxiv.org/pdf/1711.02512.pdf" rel="noopener ugc nofollow" target="_blank">颈网络中的广义均值池(<strong class="ky ir"> GeM </strong> ) </a>计算张量中每个通道的广义均值。如果p𝑘 → <strong class="ky ir"> ∞，GeM </strong>表现为最大池，p𝑘 → 1，表现为平均池。随着p𝑘的增加，汇集的特征地图的对比度增加并聚焦于图像的显著特征。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oc"><img src="../Images/e1151904766aaafd08d3cd846ba97cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2YRPCCoP0x_oJetJmYc2TQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">广义平均池方程。图片来自<a class="ae my" href="https://arxiv.org/pdf/1711.02512.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1711.02512.pdf</a></p></figure><p id="d7e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae my" href="https://arxiv.org/pdf/1502.01852v1.pdf" rel="noopener ugc nofollow" target="_blank"> PReLU </a> : PReLU是leaky ReLU的推广，用于解决当数据没有归一化或网络权重没有正确初始化时出现的神经元死亡问题。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi od"><img src="../Images/4082b5242a7b655b30cc3fe89769d2af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8LFS-2-7qZETKciFivttmQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">ReLU(左)和PReLU(右)。对于PReLU，自适应地学习负部分的系数。图片来自<a class="ae my" href="https://arxiv.org/pdf/1502.01852v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1502.01852v1.pdf</a></p></figure><p id="72a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae my" href="https://arxiv.org/pdf/1801.07698.pdf" rel="noopener ugc nofollow" target="_blank"> Arc margin </a>对softmax loss进行了改进，通过在半径为s的超球面上分布学习嵌入，加强了类内变化的相似性和类间的多样性。下面是MxNet上ArcFace loss的伪代码。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oe"><img src="../Images/ccda6eff411c7fd231dfc5c28b865fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HPG-NJQ5rpI97BcyEs96vw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片来自https://arxiv.org/pdf/1801.07698.pdf<a class="ae my" href="https://arxiv.org/pdf/1801.07698.pdf" rel="noopener ugc nofollow" target="_blank"/></p></figure><h2 id="c3b3" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated">培养</h2><ul class=""><li id="cfd8" class="mp mq iq ky b kz mk lc ml lf mr lj ms ln mt lr mu mv mw mx bi translated">使用<a class="ae my" href="https://github.com/albumentations-team/albumentations" rel="noopener ugc nofollow" target="_blank">相册</a>在不同的图像比例【448x448，568x568，600x600，512x512】下训练模型。</li><li id="06f0" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">使用<a class="ae my" href="https://arxiv.org/pdf/1704.00109.pdf" rel="noopener ugc nofollow" target="_blank">余弦退火调度程序对每个模型进行10个时期的训练。</a></li><li id="2934" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">2019年比赛的测试集与标签一起发布，用作验证。</li></ul><h2 id="8b7f" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated"><strong class="ak">重新排名</strong></h2><p id="ef41" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">作为后处理步骤，进行重新排序以惩罚非界标图像，从而改善<a class="ae my" href="https://www.kaggle.com/c/landmark-recognition-2020/overview/evaluation" rel="noopener ugc nofollow" target="_blank">间隙度量</a>。</p><ul class=""><li id="b632" class="mp mq iq ky b kz la lc ld lf ng lj nh ln ni lr mu mv mw mx bi translated">测试:排行榜测试集。</li><li id="0b9b" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">训练:确定标签和置信度候选图像。</li><li id="28c9" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">非界标:来自<a class="ae my" href="https://www.kaggle.com/c/landmark-recognition-2020/data" rel="noopener ugc nofollow" target="_blank"> GLDv2 </a>测试集的无界标图像。</li></ul><h2 id="c8aa" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated">重新排序步骤:</h2><ol class=""><li id="e097" class="mp mq iq ky b kz mk lc ml lf mr lj ms ln mt lr of mv mw mx bi translated">计算测试图像和训练图像之间的余弦相似度<strong class="ky ir"> (A) </strong></li><li id="bd09" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr of mv mw mx bi translated">计算训练和非标志图像之间的平均(前5或前10)余弦相似性。<strong class="ky ir"> (B) </strong></li><li id="99ef" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr of mv mw mx bi translated">计算Ai，j — Bj</li><li id="e907" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr of mv mw mx bi translated">对同一标签的置信度求和，选取最高的。</li></ol><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi og"><img src="../Images/a0279d488a3ca2695914c8aff2f32de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*POep-81lsESpSm9r"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">后处理重新排序。图片来自<a class="ae my" href="https://www.kaggle.com/c/landmark-recognition-2020/discussion/187821" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/landmark-recognition-2020/discussion/187821</a></p></figure><p id="2453" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述解决方案最重要的一点是使用2019年的竞赛测试集作为推理后处理重新排名的验证，并导致排行榜最高得分<strong class="ky ir"> 0.6598 </strong>，比2019年的结果好<strong class="ky ir">~ 1.75倍</strong>。</p><h1 id="52cf" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak"> CVPR 2020阿里产品挑战赛</strong></h1><h2 id="ac1e" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated"><a class="ae my" href="https://trax-geometry.s3.amazonaws.com/cvpr_challenge/recognition_challenge_technical_reports/1st__Winner+Solution+for+AliProducts+Challenge+Large-scale+Product+Recognition.pdf" rel="noopener ugc nofollow" target="_blank">赢家方案</a></h2><h2 id="d436" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated">体系结构</h2><p id="cb83" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">骨干网(<a class="ae my" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNet-B3 </a>、<a class="ae my" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNet-B </a> 4、<a class="ae my" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank"> ResNet50 </a>、<a class="ae my" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"> SeResNext50 </a>、<a class="ae my" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"> SeResNext101 </a>)用<a class="ae my" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Destruction_and_Construction_Learning_for_Fine-Grained_Image_Recognition_CVPR_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">破坏与构造学习(DCL) </a>、<a class="ae my" href="https://arxiv.org/abs/2003.14142" rel="noopener ugc nofollow" target="_blank">查看对象(LIO) </a>方法进行微调。模型平均用于集成所有微调模型，实现<strong class="ky ir"> 6.27% </strong>的前1位错误率。</p><p id="53e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae my" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Destruction_and_Construction_Learning_for_Fine-Grained_Image_Recognition_CVPR_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">如下图所示的DCL </a>通过洗牌局部区域来学习局部区别区域和特征，从而增强细粒度识别。为了防止网络学习有噪声的模式，提出了一个对抗性的对等体来拒绝区域混淆机制(RCM)引起的不相关的模式。更多详情请查看<a class="ae my" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Destruction_and_Construction_Learning_for_Fine-Grained_Image_Recognition_CVPR_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">文件</a>。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oh"><img src="../Images/a9ace593a84b2b4e761fdc843e819f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zL0QuxIyjldKxGGx3Q4Zeg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">DCL网络，图片来自<a class="ae my" href="https://trax-geometry.s3.amazonaws.com/cvpr_challenge/RetailVision_ChallengeTalk1.mp4" rel="noopener ugc nofollow" target="_blank">https://trax-geometry . S3 . amazonaws . com/cvpr _ challenge/retail vision _ challenge talk 1 . MP4</a></p></figure><p id="c9b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae my" href="https://arxiv.org/pdf/2003.14142.pdf" rel="noopener ugc nofollow" target="_blank"> LIO </a>如下图所示，模型结构采用<a class="ae my" href="https://www.fast.ai/2020/01/13/self_supervised/" rel="noopener ugc nofollow" target="_blank">自监督学习</a>。对象范围学习帮助主干网络区分前景和背景。使用自我监督的空间上下文学习加强了主干网络的结构信息。更多细节请查看<a class="ae my" href="https://arxiv.org/pdf/2003.14142.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oi"><img src="../Images/b60907aa386356d6263594c2847bc4cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SgRvLRmAuFmRX-52RhJQUg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">探究对象(LIO)框架。图片来自<a class="ae my" href="https://arxiv.org/pdf/2003.14142.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.14142.pdf</a></p></figure><h2 id="7d9d" class="np lt iq bd lu nq nr dn ly ns nt dp mc lf nu nv me lj nw nx mg ln ny nz mi oa bi translated">预处理</h2><p id="648c" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">所有图像的大小都调整为256x256，然后随机裁剪为224x224用于训练，中间裁剪为224x224用于测试。使用以下工具扩充训练数据</p><ul class=""><li id="26b7" class="mp mq iq ky b kz la lc ld lf ng lj nh ln ni lr mu mv mw mx bi translated"><a class="ae my" href="https://github.com/kakaobrain/fast-autoaugment" rel="noopener ugc nofollow" target="_blank">自动增强</a></li><li id="48b6" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated"><a class="ae my" href="https://github.com/uoguelph-mlrg/Cutout" rel="noopener ugc nofollow" target="_blank">断流器</a></li></ul><p id="7b8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">培训</strong></p><p id="0099" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有模型都由一个<a class="ae my" href="https://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent" rel="noopener ugc nofollow" target="_blank"> SGD优化器</a>训练，该优化器具有手动学习率衰减功能。</p><ol class=""><li id="f8d9" class="mp mq iq ky b kz la lc ld lf ng lj nh ln ni lr of mv mw mx bi translated">所有经过基本培训的骨干网络都达到了<strong class="ky ir">20–25%</strong>的最高错误率。</li><li id="4f93" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr of mv mw mx bi translated">所有的主干网络都通过平衡训练进行了微调，实现了<strong class="ky ir">9–12%</strong>的顶级错误率。如果一个类别中的图像数量少于30，则平衡训练集也包括来自验证的所有图像。</li><li id="b7c1" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr of mv mw mx bi translated">在更高分辨率的图像(448x448)上使用<a class="ae my" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Destruction_and_Construction_Learning_for_Fine-Grained_Image_Recognition_CVPR_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank"> DCL </a>进一步微调所有主干，进一步将错误率降低<strong class="ky ir">1–2%</strong>。</li><li id="d634" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr of mv mw mx bi translated">如下图所示，使用精度损失对所有网络进行了进一步微调，针对前1位错误率进行了优化，将错误率降低了<strong class="ky ir">~ 0.2–0.5%</strong>。</li></ol><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/1582e062879988c296425ad1226c40c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*cVnBGoujRPaZqAm9gRU5RA.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">精度损失，图片来自<a class="ae my" href="https://trax-geometry.s3.amazonaws.com/cvpr_challenge/recognition_challenge_technical_reports/1st__Winner+Solution+for+AliProducts+Challenge+Large-scale+Product+Recognition.pdf" rel="noopener ugc nofollow" target="_blank">https://trax-geometry . S3 . Amazon AWS . com/cvpr _ Challenge/recognition _ Challenge _ technical _ reports/1st _ _ Winner+Solution+for+AliProducts+Challenge+大规模+Product+Recognition.pdf </a></p></figure><pre class="kh ki kj kk gt ok ol om on aw oo bi"><span id="de37" class="np lt iq ol b gy op oq l or os">def <strong class="ol ir">acc_loss</strong>(y_true, y_pred):<br/>    tp = (y_pred, y_true).sum(1)<br/>    fp = ((1-y_true)*y_pred).sum(1)<br/>    acc = tp/(tp+fp)<br/>    return 1 - acc.mean()</span></pre><p id="a5fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">低于11个模型用于计算最终概率</p><ul class=""><li id="b35a" class="mp mq iq ky b kz la lc ld lf ng lj nh ln ni lr mu mv mw mx bi translated">平衡微调resnet50、seresnext50、seresnext101、efficientnet-b3、efficientnet-b4</li><li id="54a4" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated"><a class="ae my" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Destruction_and_Construction_Learning_for_Fine-Grained_Image_Recognition_CVPR_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank"> DCL </a>微调resnet50，seresnext50</li><li id="5964" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">精确损耗微调resnet50、seresnext50、efficientnet-b3</li><li id="6e54" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated"><a class="ae my" href="https://arxiv.org/abs/2003.14142" rel="noopener ugc nofollow" target="_blank"> LIO </a>微调resnet50</li></ul><h1 id="6dc9" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">结论</strong></h1><p id="1cc1" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">实例级识别将揭示深度学习技术在电子商务、旅游、媒体和娱乐、农业等领域的语义图像分类/检索的真正潜力。高效的实例级解决方案的一些主要构件是</p><ul class=""><li id="0eb2" class="mp mq iq ky b kz la lc ld lf ng lj nh ln ni lr mu mv mw mx bi translated">主干网络选择(剩余、挤压和激励、高效网络)</li><li id="dd4a" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">数据扩充(缓冲、自动扩充、剪切等)。</li><li id="fbd3" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">损失函数(ArcFace，AccuracyLoss)。</li><li id="a417" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">多尺度处理。</li><li id="b6a9" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">微调和后期处理。</li></ul><p id="adb1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢您阅读这篇文章，我希望这对您有所帮助。如果你有，请在你最喜欢的社交媒体上分享，这样其他人也可以找到它。此外，如果有不清楚或不正确的地方，请在评论区告诉我。</p><h1 id="ff63" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><ol class=""><li id="851a" class="mp mq iq ky b kz mk lc ml lf mr lj ms ln mt lr of mv mw mx bi translated"><a class="ae my" href="https://ilr-workshop.github.io/ECCVW2020/" rel="noopener ugc nofollow" target="_blank">https://ilr-workshop.github.io/ECCVW2020/</a></li><li id="7b17" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr of mv mw mx bi translated"><a class="ae my" href="https://ai.googleblog.com/2020/09/advancing-instance-level-recognition.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2020/09/advancing-instance-level-recognition . html</a></li><li id="d04b" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr of mv mw mx bi translated"><a class="ae my" href="https://drive.google.com/file/d/1f9VZW1gtxAohL_ApA-qdZkAklAB5cZ6t/view" rel="noopener ugc nofollow" target="_blank">https://drive . Google . com/file/d/1 F9 vzw 1 gtx aohl _ ApA-qdzkaklab 5 cz 6t/view</a></li><li id="2bc5" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr of mv mw mx bi translated"><a class="ae my" href="https://www.kaggle.com/c/landmark-recognition-2020/discussion/187821" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/landmark-recognition-2020/discussion/187821</a></li><li id="802d" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr of mv mw mx bi translated"><a class="ae my" href="https://trax-geometry.s3.amazonaws.com/cvpr_challenge/recognition_challenge_technical_reports/1st__Winner+Solution+for+AliProducts+Challenge+Large-scale+Product+Recognition.pdf" rel="noopener ugc nofollow" target="_blank">https://trax-geometry . S3 . Amazon AWS . com/cvpr _ Challenge/recognition _ Challenge _ technical _ reports/1st _ _ Winner+Solution+for+AliProducts+Challenge+Large-scale+Product+recognition . pdf</a></li></ol></div></div>    
</body>
</html>