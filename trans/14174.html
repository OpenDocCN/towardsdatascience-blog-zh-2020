<html>
<head>
<title>Optimizing Hyperparameters the right Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">以正确的方式优化超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimizing-hyperparameters-the-right-way-3c9cafc279cc?source=collection_archive---------10-----------------------#2020-09-30">https://towardsdatascience.com/optimizing-hyperparameters-the-right-way-3c9cafc279cc?source=collection_archive---------10-----------------------#2020-09-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e7c7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Python中的skopt通过<em class="kf">贝叶斯优化</em>有效地探索参数搜索。TL；我的超参数总是比你的好。</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi kg"><img src="../Images/e0c77e9d54edf862871826b3374d29d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*rSHVzXO7oLBFPpnh.jpg"/></div><p class="ko kp gj gh gi kq kr bd b be z dk translated"><em class="kf">高效探索问题空间的广阔峡谷—Photo by</em><a class="ae ks" href="https://unsplash.com/@fineas_anton?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"><em class="kf">fine as Anton</em></a><em class="kf">on</em><a class="ae ks" href="https://unsplash.com/s/photos/balance-landscape?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"><em class="kf">Unsplash</em></a></p></figure><p id="4ea3" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">在这篇文章中，我们将使用多个优化器构建一个机器学习管道，并使用贝叶斯优化的力量来为我们所有的参数获得<strong class="kv ir">最佳配置。我们所需要的就是sklearn <a class="ae ks" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html" rel="noopener ugc nofollow" target="_blank">管道</a>和<a class="ae ks" href="https://scikit-optimize.github.io/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Skopt </a>。<br/>你可以用你喜欢的ML机型，只要有sklearn的包装(看你XGBoost或者NGBoost)。</strong></p><h1 id="2fcf" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">关于超参数</h1><p id="6b3a" class="pw-post-body-paragraph kt ku iq kv b kw mh jr ky kz mi ju lb lc mj le lf lg mk li lj lk ml lm ln lo ij bi translated">寻找能够解决问题的最佳模型的关键点是<em class="mm">而不是</em>仅仅是模型。在给定数据集的情况下，我们需要<strong class="kv ir">找到最佳参数</strong>来使我们的模型以最佳方式工作。这被称为寻找或搜索超参数。<br/>例如，我们想在实践中实现一个<a class="ae ks" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">随机森林</a>，它的文档声明:</p><pre class="kh ki kj kk gt mn mo mp mq aw mr bi"><span id="4a53" class="ms lq iq mo b gy mt mu l mv mw">class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, ...</span></pre><p id="fa40" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">所有这些参数都可以探索。这可以包括从1到10.000的森林中所有可能的估计数(n_estimators)，您可以尝试使用<strong class="kv ir"> <em class="mm"> {"gini "，" entropy"} </em> </strong>进行分割，或者您的树的最大深度是另一个整数，还有许多许多选项。<strong class="kv ir">这些<strong class="kv ir">参数中的每一个</strong>都会影响你的模型</strong>的性能，最糟糕的是大多数时候<strong class="kv ir">当你开始处理一个新的问题集时，你不知道正确的配置</strong>。</p><h1 id="ece8" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">手提钻又名网格搜索</h1><p id="df80" class="pw-post-body-paragraph kt ku iq kv b kw mh jr ky kz mi ju lb lc mj le lf lg mk li lj lk ml lm ln lo ij bi translated">寻找最佳配置的强力方法是执行网格搜索，例如使用sklearn的<a class="ae ks" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> GridSearchCV </a>。这意味着您要在模型上尝试所有可能的参数组合。<br/>从好的方面来看，你可能会找到想要的价值观。问题是<strong class="kv ir">运行时间是可怕的</strong>，并且总的来说<strong class="kv ir">网格搜索不能很好地扩展</strong>。对于您想要尝试的每个新参数，您也将测试所有其他先前指定的参数。这种方法非常缺乏信息，感觉就像在我们的问题空间中盲目地测试所有可能的按钮和配置。与此相反的是我们对超参数的理解:</p><blockquote class="mx my mz"><p id="a5b4" class="kt ku mm kv b kw kx jr ky kz la ju lb na ld le lf nb lh li lj nc ll lm ln lo ij bi translated">我们有一种<strong class="kv ir">直觉，一些参数的选择比其他的更能提供信息。</strong></p></blockquote><p id="d17d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">一些参数更有意义，一旦我们知道一些参数范围比其他范围更好。此外，一旦我们知道什么方向起作用，我们就不必在错误的领域探索所有的价值。</p><p id="a5b1" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">我们不需要测试所有的参数——尤其是那些我们知道相差很远的参数。<br/>朝正确方向的一步是像<a class="ae ks" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank"> RandomizedSearchCV </a>一样的随机搜索，我们在朝正确方向移动的同时随机选取参数。</p><h1 id="7d5a" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">更好的贝叶斯搜索</h1><p id="a8fa" class="pw-post-body-paragraph kt ku iq kv b kw mh jr ky kz mi ju lb lc mj le lf lg mk li lj lk ml lm ln lo ij bi translated">我们选择的工具是<a class="ae ks" href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html" rel="noopener ugc nofollow" target="_blank"> BayesSearchCV </a>。这种方法使用逐步贝叶斯优化来探索问题空间中最有希望的超参数。<br/>简而言之，贝叶斯优化在大型问题空间中找到目标函数的最小值，并且非常适用于连续值。为此，它对目标函数使用高斯过程回归。一个彻底的数学介绍可以在<a class="ae ks" href="https://arxiv.org/pdf/1807.02811.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>中找到。在我们的例子中，<strong class="kv ir">目标函数</strong>是在给定我们指定的模型参数的情况下，达到<strong class="kv ir">最佳模型输出。<br/>贝叶斯优化方法的好处是，我们可以给出更大范围的可能值<strong class="kv ir"/>，因为随着时间的推移，我们会自动探索最有希望的区域，并丢弃不太有希望的区域。简单的网格搜索需要很长时间来愚蠢地探索所有可能的值。<br/>由于我们的行动更加有效，我们可以有更大的活动空间。我们来看一个例子<em class="mm">。</em></strong></p><h1 id="d771" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">问题集</h1><p id="94fc" class="pw-post-body-paragraph kt ku iq kv b kw mh jr ky kz mi ju lb lc mj le lf lg mk li lj lk ml lm ln lo ij bi translated">今天我们使用来自sklearn 的<a class="ae ks" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html" rel="noopener ugc nofollow" target="_blank">糖尿病数据集，以方便使用。<br/>这省去了我们加载和清理数据的麻烦，而且这些特性已经被整齐地编码了。</a></p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/999e5e4a09a84a2da4abc769d02ff77b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b8OmjXq-EHTzMozz.png"/></div></div></figure><p id="7ac6" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">我们有一组编码栏，包括年龄、性别、体重指数、血压和血清值，用数字编码。我们的目标值是疾病进展的量度。<br/>我们可以在s柱(血清值)上看到一些相互作用，表明某种程度的相关性。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ni"><img src="../Images/0030678d3be1a4aaf3b060b7945f590e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nvuoUW1YZ81N_Lt0.png"/></div></div></figure><p id="b0b6" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">为了构建我们的管道，我们首先将数据集以80:20的比例分别分为训练和测试。</p><pre class="kh ki kj kk gt mn mo mp mq aw mr bi"><span id="3b38" class="ms lq iq mo b gy mt mu l mv mw">X_train, X_test, y_train, y_test = train_test_split(diabetes_df.drop(columns="target"), diabetes_df.target, test_size=0.2, random_state=21)</span></pre><h1 id="e5fc" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">修建管道</h1><p id="6953" class="pw-post-body-paragraph kt ku iq kv b kw mh jr ky kz mi ju lb lc mj le lf lg mk li lj lk ml lm ln lo ij bi translated">我们需要三个元素来构建管道:(1)要优化的模型，(sklearn管道对象，以及(skopt优化过程。</p><p id="8c51" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">首先，我们选择<strong class="kv ir">两个助推模型</strong> : AdaBoost和GradientBoosted回归器，对于每个模型，我们<strong class="kv ir">在关键超参数</strong>上定义一个搜索空间。来自sklearn库深度的任何其他回归器都可以，但boosting可能会为您赢得下一次黑客马拉松(…现在是2015年，不是吗？)<br/>搜索空间是一个带有<br/>键值对的字典:= { ' model _ _<em class="mm">parameter</em>':skopt . space . object }。<br/>对于每个参数，我们在我们想要的范围内从skopt库中设置一个<a class="ae ks" href="https://scikit-optimize.github.io/stable/modules/space.html#space" rel="noopener ugc nofollow" target="_blank">空间</a>。分类值也通过将它们作为字符串列表传递来包含(参见下面的分类):</p><pre class="kh ki kj kk gt mn mo mp mq aw mr bi"><span id="c2fe" class="ms lq iq mo b gy mt mu l mv mw">ada_search = {<br/>    'model': [AdaBoostRegressor()],<br/>    'model__learning_rate': Real(0.005, 0.9, prior="log-uniform"),<br/>    'model__n_estimators': Integer(1, 1000),<br/>    'model__loss': Categorical(['linear', 'square', 'exponential'])<br/>}</span><span id="14a6" class="ms lq iq mo b gy nj mu l mv mw">gb_search = {<br/>    'model': [GradientBoostingRegressor()],<br/>    'model__learning_rate': Real(0.005, 0.9, prior="log-uniform"),<br/>    'model__n_estimators': Integer(1, 1000),<br/>    'model__loss': Categorical(['ls', 'lad', 'quantile'])<br/>}</span></pre><p id="a372" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">其次，我们通过另一个模型选择回归模型，这是我们的管道元素，其中两个优化器(adaboost和gradientboost)一起进行选择:</p><pre class="kh ki kj kk gt mn mo mp mq aw mr bi"><span id="e10d" class="ms lq iq mo b gy mt mu l mv mw">pipe = Pipeline([('model', GradientBoostingRegressor())])</span></pre><p id="fc7b" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">第三，我们优化我们的搜索空间。为此我们调用BayesSearchCV。我们还指定优化器应该如何调用我们的搜索空间。在我们的例子中是100次调用。然后我们用一个简单的目的来安装管道。fit() 命令:</p><pre class="kh ki kj kk gt mn mo mp mq aw mr bi"><span id="0176" class="ms lq iq mo b gy mt mu l mv mw">opt = BayesSearchCV(<br/>    pipe,<br/>    [(ada_search, 100), (gb_search, 100)],<br/>    cv=5<br/>)</span><span id="a61d" class="ms lq iq mo b gy nj mu l mv mw">opt.fit(X_train, y_train)</span></pre><p id="f36b" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">拟合完成后，我们可以要求找到最佳的参数。这包括对看不见的测试数据使用分数函数。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nk"><img src="../Images/46dba98dcf3af190083867d49da80544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*995Q-BI7DWhmfd0aJokSWg.png"/></div></div></figure><p id="0ac6" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">我们可以看到验证和测试分数以及最佳拟合的参数。具有最佳结果的模型是AdaBoost模型，具有线性损失和259个估计器，学习率为0.064。干净利落。<br/>从输出中我们还可以看到，优化器使用了一个GP进行底层优化:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nl"><img src="../Images/cb18b8d3c1fc122c7106fe304d4a6791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YjUFpThDvVI0YLnw.png"/></div></div></figure><h1 id="a125" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">GP最小化</h1><p id="dc54" class="pw-post-body-paragraph kt ku iq kv b kw mh jr ky kz mi ju lb lc mj le lf lg mk li lj lk ml lm ln lo ij bi translated">为了便于说明，我们也可以专门调用GP最小化，我们将使用AdaBoost回归器，只关注数值超参数空间。</p><pre class="kh ki kj kk gt mn mo mp mq aw mr bi"><span id="dbe4" class="ms lq iq mo b gy mt mu l mv mw">ada = AdaBoostRegressor(random_state=21)</span><span id="76b5" class="ms lq iq mo b gy nj mu l mv mw"># numerical space<br/>space  = [Real(0.005, 0.9, "log-uniform", name='learning_rate'),<br/>          Integer(1, 1000, name="n_estimators")]</span></pre><p id="8803" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">这里的主要变化是我们必须<strong class="kv ir">定义一个目标函数</strong>来优化。为了查看模型在参数空间上的表现，我们使用了named_args装饰器。</p><pre class="kh ki kj kk gt mn mo mp mq aw mr bi"><span id="6be7" class="ms lq iq mo b gy mt mu l mv mw">@use_named_args(space)<br/>def objective(**params):<br/>    ada.set_params(**params)<br/>    return -np.mean(cross_val_score(ada, X_train, y_train, cv=5, n_jobs=-1,<br/>                                    scoring="neg_mean_absolute_error"))</span></pre><p id="2e0d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">因此，我们的优化是使用<a class="ae ks" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html" rel="noopener ugc nofollow" target="_blank">负平均绝对误差</a>从交叉验证拟合中获得的负平均分数。其他评分功能可能更适合你。<br/>然后，我们使用GP最小化来拟合回归变量的最佳参数。<code class="fe nm nn no mo b">gp_minimize(objective, space, n_calls=100, random_state=21)</code></p><h1 id="7b0a" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">可视化问题空间—优化后</h1><p id="481c" class="pw-post-body-paragraph kt ku iq kv b kw mh jr ky kz mi ju lb lc mj le lf lg mk li lj lk ml lm ln lo ij bi translated">直接使用GP优化允许我们在最小化过程中绘制收敛图。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi np"><img src="../Images/855530e52060098b1ffbedaf560d3c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/0*6sYK0AePg8mHWaZn.png"/></div><p class="ko kp gj gh gi kq kr bd b be z dk translated"><em class="kf">寻找AdaBoost回归器相对于数据集中目标列的最佳超参数时GP最小化的收敛性。</em></p></figure><p id="ab14" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">我们可以看到，在大约40次迭代之后，已经达到了函数值的最小值。<br/>最后一个优秀的特性是<strong class="kv ir">可视化探索的问题空间</strong>。因为我们在这一点上只使用了数字输入，所以我们可以通过使用skopt绘制来评估问题空间，如下所示:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/5cdfab5852a0512236c06677be393f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/0*Zo9x7FAQ6EbRF7Di.png"/></div><p class="ko kp gj gh gi kq kr bd b be z dk translated"><em class="kf">将问题空间上调用的learning_rate和n_estimator值可视化为直方图。问题空间显示为散点图，使得每个GP运行(100次中的一次)揭示该空间中的一个函数值。最小值由散点图中的红星标出。</em></p></figure><p id="2698" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">从上图中我们可以看到，在我们的学习率谱的低端进行了大量的探索，经过尝试和测试的估计数的峰值是200和800以上。<br/>散点图描绘了我们试图优化的问题空间有多困难——最小值标有红星。每个GP-minimize运行发现一个函数值，给定它作为输入的参数。虽然我们已经达到收敛，但在最小值周围的区域中没有很多值是明显的，并且可以考虑增加评估的数量。</p><p id="1145" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">既然我们有了给定参数空间的最佳模型，我们就可以相应地实现该模型，并进行具体的分析。</p><h1 id="73e8" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">结论</h1><p id="a48d" class="pw-post-body-paragraph kt ku iq kv b kw mh jr ky kz mi ju lb lc mj le lf lg mk li lj lk ml lm ln lo ij bi translated">我们已经表明，我们可以有效地探索可能的模型参数的巨大空间，节省时间和计算资源。我们通过将对超参数的搜索公式化为目标函数来做到这一点，我们使用贝叶斯优化来为该目标函数找到最优值。没有必要通过尝试每一个参数来测试无用的参数。如你所见，skopt是一个在优化领域提供了很多东西的库，我鼓励你在日常的ML工程中使用它们。</p><p id="058f" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">探索愉快！</p><p id="b27f" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">完整的代码可以在笔记本上找到<a class="ae ks" href="https://github.com/RMichae1/PyroStudies/blob/master/Bayesian_Optimization.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>:</p><div class="nr ns gp gr nt nu"><a href="https://github.com/RMichae1/PyroStudies/blob/master/Bayesian_Optimization.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd ir gy z fp nz fr fs oa fu fw ip bi translated">RMI chae 1/烟火研究</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">github.com</p></div></div><div class="od l"><div class="oe l of og oh od oi km nu"/></div></div></a></div><h1 id="dfb9" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">参考</h1><ol class=""><li id="9c0c" class="oj ok iq kv b kw mh kz mi lc ol lg om lk on lo oo op oq or bi translated">马克·克拉森，<a class="ae ks" href="https://arxiv.org/search/cs?searchtype=author&amp;query=De+Moor%2C+B" rel="noopener ugc nofollow" target="_blank">巴特·德·穆尔</a>。<a class="ae ks" href="https://arxiv.org/pdf/1502.02127.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mm">机器学习中的超参数搜索</em> </a>。arXiv上2015。</li><li id="82b3" class="oj ok iq kv b kw os kz ot lc ou lg ov lk ow lo oo op oq or bi translated">彼得·弗雷泽。<a class="ae ks" href="https://arxiv.org/pdf/1807.02811.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mm">贝叶斯优化教程</em> </a>。arXiv上2018。</li><li id="f18a" class="oj ok iq kv b kw os kz ot lc ou lg ov lk ow lo oo op oq or bi translated">sci kit-优化贡献者(BSD许可证)。<a class="ae ks" href="https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html" rel="noopener ugc nofollow" target="_blank"><em class="mm">Scikit-学习超参数搜索包装器</em> </a>。</li></ol></div></div>    
</body>
</html>