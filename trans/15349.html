<html>
<head>
<title>Spark vs Pandas, part 2 — Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">星火大战熊猫，第二部分——星火</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spark-vs-pandas-part-2-spark-c57f8ea3a781?source=collection_archive---------1-----------------------#2020-10-22">https://towardsdatascience.com/spark-vs-pandas-part-2-spark-c57f8ea3a781?source=collection_archive---------1-----------------------#2020-10-22</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="d5f6" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">通过Spark扩展推动极限</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/b03ce3ad6160179db9bedd1aca1e446c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EdQC3xnI8nIkWMCn"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">保罗·卡莫纳在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e253" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">最初我想写一篇文章来公平地比较熊猫和火花，但它继续增长，直到我决定把它分开。这是小编的第二部。</p><ul class=""><li id="e10a" class="lw lx iu lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated"><a class="ae kz" rel="noopener" target="_blank" href="/spark-vs-pandas-part-1-pandas-10d768b979f5">星火大战熊猫，第一部——熊猫</a></li><li id="c3ab" class="lw lx iu lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">星火大战熊猫，第二部分——星火</li><li id="074a" class="lw lx iu lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">星火大战熊猫，第三部分——语言</li><li id="8964" class="lw lx iu lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">星火大战熊猫，第四部分——枪战与推荐</li></ul><h2 id="0762" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">期待什么</h2><p id="4b0e" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">第二部分描述了Apache Spark。作为大数据工程系列的一部分，我已经写了一篇关于Spark 的<a class="ae kz" rel="noopener" target="_blank" href="/big-data-engineering-apache-spark-d67be2d9b76f">不同的文章，但这次我将更多地关注与熊猫的不同之处。</a></p></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><h1 id="b931" class="np ml iu bd mm nq nr ns mp nt nu nv ms ka nw kb mv kd nx ke my kg ny kh nb nz bi translated">什么是阿帕奇火花？</h1><p id="d9f5" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Apache Spark<a class="ae kz" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">是一个用Scala编写的大数据处理框架，目标是Java虚拟机，但它也为Java、Python和r提供语言绑定。Spark的诞生可能与Pandas非常不同，因为Spark最初主要解决有效处理大量数据的挑战，这些数据不再适合单台计算机的内存(甚至不适合整个计算集群的内存总量)。</a></p><p id="8a45" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">可以说Spark是通过提供更简单同时更强大的编程模型来取代Hadoop MapReduce的。Spark非常成功地完成了这项任务，因为我认为没有项目会在今天之前开始编写新的Hadoop MapReduce作业，显然会转而使用Spark。</p><h2 id="fb9b" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">火花数据模型</h2><p id="ff3e" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">最初，Spark只提供了一个名为RDDs的API(现在称为低级API ),它要求开发人员将他们的数据建模为类。几年后，Spark由<em class="oa"> DataFrame </em> API扩展，该API吸取了Pandas和R的许多好想法，现在是首选的API(以及数据集，但我将在讨论中省略它们)。</p><p id="74ae" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">与Pandas相似，Spark数据帧建立在列和行的概念上，列集合隐式定义了一个在所有行之间共享的<em class="oa">模式</em>。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ob"><img src="../Images/1b526814e64b9f105772a8efa285a772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pAxTcMLdKds3J473ltkLsg.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">多列Spark数据框架</p></figure><p id="c93a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">与Pandas不同，Spark数据帧的模式定义还规定了可以存储在每行中的每列的数据类型。这一点与经典数据库非常相似，其中每一列都有一个固定的数据类型，并在所有记录上强制执行(较新的NoSQL数据库可能更灵活，但这并不意味着类型强制执行不好或过时)。您可以轻松地显示给定数据帧的模式，以便检查列及其数据类型:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oc"><img src="../Images/eb361b0a896b1dafddc0966ee8c2657a.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*2MLjoAdeCOP7WeMxmotyiQ.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">上述数据框架的模式</p></figure><p id="a54e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">与Pandas相反，Spark不支持任何索引来有效地访问数据帧中的单个行。Spark通过蛮力解决了所有或多或少受益于索引的任务——因为所有转换总是在所有记录上执行，Spark将根据需要动态地重新组织数据。</p><p id="4ea6" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">一般来说，Spark中的<em class="oa">列</em>和<em class="oa">行</em>不像熊猫那样可以互换。缺乏正交性的原因是Spark被设计成根据行数而不是列数来缩放数据。Spark可以轻松处理数十亿行，但是列的数量应该总是有限的(几百或几千)。</p><p id="9cea" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">因此，当我们试图用开盘、收盘、盘低、盘高和成交量属性来模拟股票价格时，我们需要在Spark中采用与Pandas不同的方法。我们没有对不同的股票使用具有不同列的宽数据框架，而是使用更加<em class="oa">规范化的</em>(在某种意义上说是<a class="ae kz" href="https://en.wikipedia.org/wiki/Database_normalization" rel="noopener ugc nofollow" target="_blank">数据库规范化</a>)方法，其中每一行都由其维度<code class="fe od oe of og b">date</code>和<code class="fe od oe of og b">asset</code>唯一标识，并包含指标<code class="fe od oe of og b">close</code>、<code class="fe od oe of og b">high</code>、<code class="fe od oe of og b">low</code>和<code class="fe od oe of og b">open</code>。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oh"><img src="../Images/0fb17f6187995cc076375eb78c447673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZWDKMac67fzecvyPY0AfQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">包含股票编号的数据帧</p></figure><p id="430d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">尽管Apache Spark的数据模型不如Pandas的灵活，但这并不一定是件坏事。随着Sparks专注于实现关系代数(下面将详细介绍),这些限制自然会出现，并且严格数据类型的实施有助于您更早地发现错误。</p><p id="6ebe" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">由于Spark不支持索引，所以它也不支持像Pandas这样的嵌套索引。相反，Spark为深度嵌套的数据结构提供了很好的支持，就像在JSON文档或Avro消息中发现的那样。这些类型的数据经常在应用程序之间的内部通信协议中使用，Sparks对它们的全面支持强调了它作为数据处理工具的重要性。<em class="oa">嵌套结构</em>被实现为复杂的数据类型，如结构、数组和映射，它们又可以包含这些复杂的数据类型。</p><p id="4f80" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">作为一个例子，我给你看一些Twitter数据，这些数据在互联网档案馆公开。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oi"><img src="../Images/6283dbd6b506c0a2afb9148a39568528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YavvK2e4U5yUrbLE4a7jRw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">以表格形式显示的嵌套数据</p></figure><p id="2715" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这种表格表示并没有真正显示推文的复杂本质。查看该模式可以了解所有细节(尽管下面只显示了一个小节):</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oj"><img src="../Images/910140f6ab18631ed32627e7a0ad6012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m_RrT2urV_Hcq7DES_Icww.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">twitter数据集完整模式的一部分</p></figure><p id="c2d1" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这种对复杂和深度嵌套模式的支持是Spark不同于Pandas的地方，Pandas只能处理纯表格数据。由于这种类型的数据自然会出现在许多领域，所以知道Spark可以成为处理这些数据的工具是件好事。关于如何处理这类数据的建议可能是另一篇文章的有趣主题。</p><h1 id="f4e3" class="np ml iu bd mm nq ok ns mp nt ol nv ms ka om kb mv kd on ke my kg oo kh nb nz bi translated">火花的灵活性</h1><p id="4714" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Apache Spark还提供了广泛的转换，实现了传统数据库(MySQL、Oracle、DB2、MS SQL等)中的完整关系代数。这意味着您可以像在SQL的<code class="fe od oe of og b">SELECT</code>语句中一样执行任何转换。</p><p id="f8df" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">下面的Spark/Scala示例遵循第一篇文章中熊猫示例的思路。请注意Spark是如何在其所有方法中使用SQL的措辞(而不是语法)的。</p><h2 id="365c" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">预测</h2><p id="876f" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">可能最简单的转换之一是投影，它只是用现有列的子集创建一个新的DataFrame。这个操作叫做<em class="oa">投影</em>，因为它类似于高维空间到低维空间的数学投影(例如3d到2d)。具体来说，投影减少了维数，并且它是<a class="ae kz" href="https://en.wikipedia.org/wiki/Idempotence" rel="noopener ugc nofollow" target="_blank">幂等的</a>，即，对结果再次执行相同的投影将不会再改变数据。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj op"><img src="../Images/0a50ccb5a0887b6174b978a97e86a563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L_NVk20zuR6MdT4VBxZDGg.png"/></div></div></figure><p id="13b3" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">SQL中的投影是一个非常简单的<code class="fe od oe of og b">SELECT</code>语句，包含所有可用列的子集。注意Spark方法的名字<code class="fe od oe of og b">select</code>是如何反映这种等价的。</p><h2 id="61b5" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">过滤</h2><p id="3192" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">过滤中的下一个简单转换，它只选择可用行的子集。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oq"><img src="../Images/af283338125b4f06039aa2da5d16dc00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldFRZ-NWjm19bfAmciUlkw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">选择21岁以上的人。</p></figure><p id="b97d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">SQL中的过滤通常在<code class="fe od oe of og b">WHERE</code>子句中执行。再次注意，Spark选择使用SQL术语<code class="fe od oe of og b">where</code>，尽管Scala用户更喜欢<code class="fe od oe of og b">filter</code>——实际上你也可以使用等效的方法<code class="fe od oe of og b">filter</code>来代替。</p><h2 id="d8ee" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">连接</h2><p id="3157" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">连接是关系数据库中的基本操作——没有它们，术语<em class="oa">关系</em>就没有什么意义了。</p><p id="216f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">对于一个小的演示，我首先加载第二个数据帧，其中包含一些人居住的城市的名称:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj or"><img src="../Images/9037ced45b067a26676bc1753f61e66f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XMpTM6oxG3ri7NWIsLfTdA.png"/></div></div></figure><p id="c8a7" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在我们可以执行连接操作了:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj os"><img src="../Images/d9425bd36b63e0f84f47770ca99215f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PW8Z4lRpED6uPogDa755JQ.png"/></div></div></figure><p id="4e3a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在SQL中，连接操作是通过作为<code class="fe od oe of og b">SELECT</code>语句一部分的<code class="fe od oe of og b">JOIN</code>子句来执行的。</p><p id="2a26" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">注意，与Pandas不同，Spark不需要对任何一个数据帧进行特殊的索引(如果您还记得的话，Spark不支持索引的概念)。</p><p id="6226" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">Spark不再依赖可用的索引，而是将数据重新组织，作为实现高效并行和分布式连接操作的一部分。这种重组将<em class="oa">在一个集群的所有机器之间重排</em>数据，这意味着两个数据帧的所有记录都将被重新分配，使得具有匹配连接键的记录被发送到同一台机器。一旦这个洗牌阶段完成，就可以在所有机器上使用<a class="ae kz" href="https://en.wikipedia.org/wiki/Sort-merge_join" rel="noopener ugc nofollow" target="_blank">排序-合并连接</a>独立并行执行连接。</p><h1 id="d271" class="np ml iu bd mm nq ok ns mp nt ol nv ms ka om kb mv kd on ke my kg oo kh nb nz bi translated">串联</h1><p id="3345" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Spark还支持多个数据帧的连接，但只是垂直连接(即从第二个数据帧添加相同列数的行)。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ot"><img src="../Images/c879a5219a10c7f9bbb4ef53f2ed8f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ggC_9XPQBfXjNHSah6i-3Q.png"/></div></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ot"><img src="../Images/e31eca43895b3afd88d5794c68d88a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HJKmStpZ9GAYXpb5QuOHIg.png"/></div></div></figure><p id="220a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在SQL中，使用<code class="fe od oe of og b">UNION</code>可以很容易地完成垂直连接。</p><p id="545b" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">如果您需要两个数据帧的水平连接(这对Pandas来说很容易)，您必须使用join操作。在某些情况下，这可能很困难(甚至不可能)，特别是在没有自然连接键的情况下。</p><h2 id="ba02" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">聚集</h2><p id="4496" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Spark也很好地支持简单的汇总。以下示例计算了我们的<code class="fe od oe of og b">persons</code>数据框架中所有列的最小值、最大值和平均值:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ou"><img src="../Images/4394026b8952b7af40036a275490c450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R2n8IZj7HwO4Nj5oK7ooVA.png"/></div></div></figure><p id="95ab" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">同样，Spark明显模仿了SQL，在SQL中，通过在<code class="fe od oe of og b">SELECT</code>语句中使用聚合函数(如<code class="fe od oe of og b">SUM</code>、<code class="fe od oe of og b">MIN</code>、<code class="fe od oe of og b">AVG</code>等)可以获得相同的结果。</p><p id="9f77" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">与Spark相比，Pandas还能够对数据帧的所有列执行行聚合。这在Spark中不可能直接实现，您需要手动合计所有列(这不是一个聚合)。请记住，Spark是根据行数而不是列数来扩展的，并且行和列不像在Pandas中那样可以互换。</p><h2 id="709c" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">分组聚合</h2><p id="141a" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">像传统数据库和熊猫一样，Spark也支持分组聚合。例如，<code class="fe od oe of og b">persons</code>数据框的平均年龄和性别身高可计算如下:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ov"><img src="../Images/e0a916f3b897fd87caa211b56482e8ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DT8eTZZJ9KgkMT5E8cwwbw.png"/></div></div></figure><p id="a510" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">SQL还通过<code class="fe od oe of og b">GROUP BY</code>子句和<code class="fe od oe of og b">SELECT</code>语句中的聚合函数支持分组聚合。</p><h2 id="0289" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">重塑</h2><p id="78d6" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">本系列的第一篇文章提供了一个小节，介绍了Pandas为改造桌子提供的灵活而强大的可能性。使用Pandas，您可以轻松地将一个表分解成更小的子表(水平和垂直)，然后将它们连接在一起。对于Pandas，你可以用一个手指来转置整个数据帧。</p><p id="7921" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">Spark不提供这些操作，因为它们不太适合概念数据模型，在概念数据模型中，数据帧有一组固定的列，行数可能是未知的，甚至是无限的(在流式应用程序中，当新行进入系统时，它会不断地处理新行)。</p><h2 id="3408" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">数据源</h2><p id="a0c6" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Spark支持CSV、JSON、Parquet和ORC文件开箱即用。由于Spark实际上是为了处理大量数据，所以您不会找到对处理Excel文件的直接支持(“如果它不再适合Excel，它一定是大数据”)。除了传统文件之外，Spark还可以轻松访问SQL数据库，并且有大量的连接器可用于所有其他类型的数据库系统(Cassandra、MongoDB、HBase等等)。</p><p id="9d18" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">当在“本地”模式下工作而不在集群上分配工作时，您可以很好地处理本地文件系统上的文件。但是，当您在集群中工作时，必须能够从集群中的所有机器访问数据。这可以通过使用一个共享的数据库来实现，这个数据库可以通过网络访问，或者通过使用一个共享的网络文件系统来实现(就像HDFS或者S3一样——但是<a class="ae kz" href="https://en.wikipedia.org/wiki/Network_File_System" rel="noopener ugc nofollow" target="_blank"> NFS </a>也可以做到这一点，尽管可能带宽有限)。</p><h2 id="1142" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">限制</h2><p id="0937" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">正如我已经多次指出的，Spark和Pandas的数据模型之间的一个重要区别是Spark中的列和行缺乏可互换性。这意味着您不能简单地在Apache Spark中执行<em class="oa">转置</em>操作，也不能直接沿着列(只能沿着行)执行聚合。</p><h2 id="1936" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">结论</h2><p id="1581" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">有了所有这些操作，Spark可以被理解为一个处理外部数据的<em class="oa">关系执行引擎</em>。Spark优雅地选择了SQL中使用的措辞和术语，因此大多数SQL用户将很快发现如何使用Spark执行特定的任务。</p><p id="6d27" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">但是Sparks design已经在概念层面上对可能的转换类型施加了一些限制。Spark总是假设一组固定的列和一个可能未知甚至无限数量的行。这种设计是Sparks扩展能力的基础，与熊猫形成鲜明对比。</p><h1 id="3679" class="np ml iu bd mm nq ok ns mp nt ol nv ms ka om kb mv kd on ke my kg oo kh nb nz bi translated">火花运行时间特征</h1><p id="9e0e" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">到目前为止，Pandas似乎是更好的解决方案，因为它提供了更多的灵活性，并且与整个Python数据科学生态系统很好地集成在一起。但是现在当我们看Apache Spark的引擎盖下时，这种印象将会改变。</p><h2 id="f5b4" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated"><strong class="ak">运行时平台</strong></h2><p id="84ac" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Spark是用面向Java虚拟机(JVM)的编程语言<a class="ae kz" href="https://www.scala-lang.org/" rel="noopener ugc nofollow" target="_blank"> Scala </a>实现的。与Python相反，Scala是一种编译过的静态类型语言，这两个方面通常有助于计算机生成(快得多)的代码。Spark不依赖于优化的低级C/C++代码，相反，所有代码在执行过程中都由Java <a class="ae kz" href="https://en.wikipedia.org/wiki/Just-in-time_compilation" rel="noopener ugc nofollow" target="_blank">实时(JIT)编译器</a>进行了优化。</p><p id="191f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我敢说(但许多人会不同意)，与高度优化的(但特定于硬件的)C/C++甚至汇编代码相比，Java平台作为一个整体可能无法发挥硬件的最佳性能。但是性能通常不仅仅是“足够好”，而且肯定比纯解释代码(即没有优化低级代码的Python)好至少一个数量级。术语<em class="oa">快</em>和<em class="oa">慢</em>应该总是与特定的替代物联系在一起使用，在这种情况下就是Python。</p><h2 id="f2e8" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated"><strong class="ak">执行模式</strong></h2><p id="a2cc" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">与Pandas相反，Spark使用一个<em class="oa">延迟执行</em>模型。这意味着当您对数据帧进行某种转换时，数据不会立即得到处理。相反，您的转换被记录在一个<em class="oa">逻辑执行计划</em>中，它本质上是一个图，其中节点表示操作(比如读取数据或应用转换)。然后，当需要所有转换的结果时，Spark将开始工作——例如，当您想要在控制台上显示一些记录，或者当您将结果写入文件时。</p><p id="9cf8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">正如在专门讨论Pandas的文章中已经提到的，这种懒惰执行模型具有巨大的优势，它为Spark提供了在执行之前优化整个计划的能力，而不是盲目地遵循开发人员指定的步骤。内部火花使用以下阶段:</p><ol class=""><li id="123b" class="lw lx iu lc b ld le lg lh lj ly ln lz lr ma lv ow mc md me bi translated">从开发人员指定的转换中创建一个<em class="oa">逻辑执行计划</em></li><li id="2643" class="lw lx iu lc b ld mf lg mg lj mh ln mi lr mj lv ow mc md me bi translated">然后，导出一个<em class="oa">分析执行计划</em>，其中检查并解析所有列名和对数据源的外部引用</li><li id="3c8f" class="lw lx iu lc b ld mf lg mg lj mh ln mi lr mj lv ow mc md me bi translated">然后，通过反复应用可用的优化策略，该计划被转换成<em class="oa">优化的执行计划</em>。例如，过滤操作尽可能地靠近数据源，以便尽可能早地减少记录数量。但是还有更多的优化。</li><li id="0948" class="lw lx iu lc b ld mf lg mg lj mh ln mi lr mj lv ow mc md me bi translated">最后，上一步的结果被转换成一个<em class="oa">物理执行计划</em>，其中转换被一起流水线化成所谓的<em class="oa">阶段</em>。</li></ol><p id="023f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">为了完整起见，物理执行计划随后被沿着数据分割成所谓的<em class="oa">任务</em>，这些任务随后可以由集群中的机器并行执行和分配。</p><p id="7e21" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这种通用方法可能与所有关系数据库都非常相似，它们也在执行SQL查询之前用相似的策略对其进行优化。但是，正如我们在下一段中看到的，延迟执行不仅仅是优化。</p><h2 id="dc7d" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated"><strong class="ak">处理扩展性</strong></h2><p id="f8d7" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Spark本质上是多线程的，可以利用机器的所有内核。此外，Spark从一开始就被设计为在可能有数百台机器和数千个内核的大型集群中执行工作。</p><p id="91c8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">通过将工作总量分解为单个任务，然后可以并行独立处理(只要每个任务的输入数据可用)，Spark可以非常有效地利用可用的集群资源。在不浪费大量资源(CPU能力、RAM和磁盘存储)的情况下，在集群中实现一个<em class="oa">渴望</em>执行模型要困难得多。</p><h2 id="ffbe" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated"><strong class="ak">数据可扩展性</strong></h2><p id="256b" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">Spark还可以很好地处理海量数据。它不仅可以通过集群中的多台机器进行扩展，而且<em class="oa">将中间结果溢出到磁盘的能力</em>是Spark设计的核心。因此，Spark几乎不受主内存总量的限制，而只受可用磁盘空间总量的限制。</p><p id="c82a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">重要的是要理解，通过延迟执行计划，总的工作数据集永远不会在任何时间点完全具体化到RAM中。相反，所有的数据(包括输入数据和中间结果)被分割成小块，它们被独立处理，甚至结果最终被存储在小块中，这些数据永远不需要一次放入RAM。</p><p id="7820" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">如您所见，Spark的灵活性稍小，但它的可伸缩性超过了它，无论是从计算能力还是从数据大小来看都是如此。Spark是为熊猫以外的不同类型的问题而制造的。</p><h2 id="5a2e" class="mk ml iu bd mm mn mo dn mp mq mr dp ms lj mt mu mv ln mw mx my lr mz na nb nc bi translated">结论</h2><p id="96cc" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">就数据和处理能力而言，几乎无限的可扩展性使Spark成为一个<em class="oa">分布式并行关系执行引擎。</em></p></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><h1 id="8537" class="np ml iu bd mm nq nr ns mp nt nu nv ms ka nw kb mv kd nx ke my kg ny kh nb nz bi translated">结论</h1><p id="96e8" class="pw-post-body-paragraph la lb iu lc b ld nd jv lf lg ne jy li lj nf ll lm ln ng lp lq lr nh lt lu lv in bi translated">像熊猫一样，Spark是一个非常通用的处理大量数据的工具。虽然Pandas在重塑功能方面超过了Spark，但Spark擅长处理非常庞大的数据集，除了RAM之外，它还利用磁盘空间，并扩展到一个集群中的多个CPU核心、多个进程和多台机器。</p><p id="9bca" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">只要您的数据有一个固定的模式(即每天不添加新列)，Spark就能够处理它，即使它包含数千亿行。这种可扩展性以及几乎任何存储系统(即远程文件系统、数据库等)的连接器的可用性使Spark成为大数据工程和数据集成任务的绝佳工具。</p><p id="edc8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">有了这第二部分，我们现在应该对熊猫和Spark的重点有个大概的了解。但是在我们最终给出何时使用什么的建议之前，我们还应该检查编程语言和两种框架所处的生态系统。这将是本系列下一部分的主题。</p></div></div>    
</body>
</html>