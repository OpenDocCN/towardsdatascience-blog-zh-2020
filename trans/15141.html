<html>
<head>
<title>Audio Classification with PyTorch’s Ecosystem Tools</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorch的生态系统工具进行音频分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/audio-classification-with-pytorchs-ecosystem-tools-5de2b66e640c?source=collection_archive---------20-----------------------#2020-10-18">https://towardsdatascience.com/audio-classification-with-pytorchs-ecosystem-tools-5de2b66e640c?source=collection_archive---------20-----------------------#2020-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2912" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">PYTORCH生态系统</h2><div class=""/><div class=""><h2 id="23f6" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">torchaudio和Allegro列车介绍</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/9a5099a6a219184353125eca28a1db8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DQwMDJk5K94PgFrD.jpg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">torchaudio和Allegro列车的音频分类</p></figure><p id="b55c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">音频信号无处不在。因此，对各种场景的音频分类越来越感兴趣，从听力受损者的火警检测，通过用于维护目的的引擎声音分析，到婴儿监控。虽然音频信号本质上是时间性的，但在许多情况下，可以利用图像分类领域的最新进展，并使用流行的高性能卷积神经网络进行音频分类。在这篇博文中，我们将通过使用将音频信号转换到频域的流行方法来演示这样一个例子。</p><p id="7c4d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这篇博文是关于如何利用PyTorch的生态系统工具轻松启动ML/DL项目的系列文章的第三篇。之前的博文主要关注<a class="ae ma" href="https://allegro.ai/blog/ml-dl-engineering-made-easy-with-pytorch-and-allegro-trains/?utm_source=pytorch_blog&amp;utm_medium=referral&amp;utm_campaign=trains_c&amp;utm_content=audioclass" rel="noopener ugc nofollow" target="_blank">图像分类</a>和<a class="ae ma" href="https://allegro.ai/blog/accelerate-hyperparameter-optimization-with-pytorchs-ecosystem-tools/?utm_source=pytorch_blog&amp;utm_medium=referral&amp;utm_campaign=trains_c&amp;utm_content=audioclass" rel="noopener ugc nofollow" target="_blank">超参数优化</a>。在这篇博文中，我们将展示如何使用<a class="ae ma" href="https://pytorch.org/audio/" rel="noopener ugc nofollow" target="_blank"> Torchaudio </a>和<a class="ae ma" href="https://allegro.ai/trains-open-source/?utm_source=pytorch_blog&amp;utm_medium=referral&amp;utm_campaign=trains_c&amp;utm_content=audioclass" rel="noopener ugc nofollow" target="_blank"> Allegro Trains </a>实现简单高效的音频分类。</p><h1 id="c3e2" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">基于卷积神经网络的音频分类</h1><p id="c198" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">近年来，卷积神经网络(CNN)已被证明在图像分类任务中非常有效，这导致了各种架构的设计，如Inception、ResNet、ResNext、Mobilenet等。这些CNN在图像分类任务上实现了最先进的结果，并提供了各种现成的经过预先训练的骨干。因此，如果我们能够将音频分类任务转移到图像领域，我们将能够利用这些丰富的主干来满足我们的需求。</p><p id="cd14" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如前所述，我们希望将音频信号转换成图像，而不是直接使用声音文件作为振幅与时间信号。以下预处理是使用<a class="ae ma" href="http://t.allegro.ai/code_pytorch_audioclassification" rel="noopener ugc nofollow" target="_blank">这个脚本</a>在<a class="ae ma" href="https://pytorch.org/audio/datasets.html#yesno" rel="noopener ugc nofollow" target="_blank"> YesNo </a>数据集上完成的，该数据集包含在<a class="ae ma" href="https://pytorch.org/audio/datasets.html" rel="noopener ugc nofollow" target="_blank"> torchaudio内置数据集</a>中。</p><p id="6639" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">作为预处理的第一阶段，我们将:</p><ul class=""><li id="718e" class="my mz iq lg b lh li lk ll ln na lr nb lv nc lz nd ne nf ng bi translated"><strong class="lg ja">使用torchaudio读取音频文件</strong></li><li id="501c" class="my mz iq lg b lh nh lk ni ln nj lr nk lv nl lz nd ne nf ng bi translated"><strong class="lg ja">以固定的采样率对音频信号进行重新采样</strong> —这将确保我们将使用的所有信号都具有相同的采样率。理论上，采样信号可以代表的最大频率略低于采样率的一半(称为<a class="ae ma" href="https://en.wikipedia.org/wiki/Nyquist_frequency" rel="noopener ugc nofollow" target="_blank">奈奎斯特频率</a>)。由于20 kHz是人类通常能听到的最高频率，44100 Hz的采样率被认为是最受欢迎的选择。然而，在许多情况下，为了减少每个音频文件的数据量，移除较高的频率被认为是合理的。因此，对于低比特率的MP3文件，20050 Hz的采样率已经相当普遍。在我们的例子中，我们将使用这个采样率。</li><li id="c6bb" class="my mz iq lg b lh nh lk ni ln nj lr nk lv nl lz nd ne nf ng bi translated"><strong class="lg ja">创建单声道音频信号</strong> —为简单起见，我们将确保我们使用的所有信号都有相同数量的声道。</li></ul><p id="57b5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">这种预处理的代码如下:</strong></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="ac9d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">生成的matplotlib图如下所示:</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/34d9c473e24a7308bee00ffeecd590bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/0*5vw114F2DAoiCBrN"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来自YESNO数据集的音频信号时间序列</p></figure><p id="accc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在是时候将这个时序信号转换到图像域了。我们将通过将其转换为频谱图来实现，频谱图是信号频谱随时间变化的直观表示。为此，我们将使用对数标度的mel谱图。mel频谱图是一种频谱图，其中频率被转换为mel标度，Mel标度考虑了这样一个事实，即人类在较低频率下比在较高频率下更善于检测差异。mel音阶会转换频率，使音高距离相等的人听起来距离相等。</p><p id="3d74" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因此，让我们使用torchaudio转换，并将以下代码行添加到我们的代码片段中:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6040" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在，音频文件被表示为二维频谱图图像:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi np"><img src="../Images/1aa9e1dce3accabc1cabc6acebf8bc2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rgr2ujrVnNe3V0sC"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/3a9eb27ff405055767575ae0f5579568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EFpHctq7GYZuWOfS"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Mel光谱图(上图)及其对数标度版本(下图)</p></figure><p id="c8dc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这正是我们想要实现的。音频分类问题现在转化为图像分类问题。</p><h1 id="5d72" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">使用Allegro-Trains、torchaudio和torchvision进行音频分类</h1><p id="2f45" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">Pytorch的生态系统包括各种开源工具，可以启动我们的音频分类项目，并帮助我们管理和支持它。在这篇博客中，我们将使用其中的三种工具:</p><ul class=""><li id="a8f3" class="my mz iq lg b lh li lk ll ln na lr nb lv nc lz nd ne nf ng bi translated"><a class="ae ma" href="https://allegro.ai/trains-open-source/?utm_source=pytorch_blog&amp;utm_medium=referral&amp;utm_campaign=trains_c&amp;utm_content=audioclass" rel="noopener ugc nofollow" target="_blank"> Allegro Trains </a>是一款开源的机器学习和深度学习实验管理器和MLOps解决方案。它提高了人工智能团队的效率和生产力，以及本地和云GPU的利用率。Allegro Trains帮助研究人员和开发人员零集成地管理复杂的机器学习项目。</li><li id="4c19" class="my mz iq lg b lh nh lk ni ln nj lr nk lv nl lz nd ne nf ng bi translated"><a class="ae ma" href="https://pytorch.org/audio/" rel="noopener ugc nofollow" target="_blank"> Torchaudio </a>是一个由I/O函数、流行数据集和常见音频转换组成的包。</li><li id="a3c8" class="my mz iq lg b lh nh lk ni ln nj lr nk lv nl lz nd ne nf ng bi translated"><a class="ae ma" href="https://pytorch.org/docs/stable/torchvision/index.html" rel="noopener ugc nofollow" target="_blank"> Torchvision </a>是一个由流行的数据集、模型架构和计算机视觉的通用图像转换组成的包。</li></ul><p id="96c8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了简单起见，我们不会在这个博客中解释如何安装一个<a class="ae ma" href="http://t.allegro.ai/git_trains_server_p1" rel="noopener ugc nofollow" target="_blank"> Trains-server </a>。因此，实验将被记录在Allegro Trains <a class="ae ma" href="http://t.allegro.ai/demo_pytorch_audioclassification" rel="noopener ugc nofollow" target="_blank">演示服务器</a>上。有关如何部署自托管Trains服务器的更多信息，请参见Allegro Trains <a class="ae ma" href="https://allegro.ai/docs/deploying_trains/trains_deploy_overview/?utm_source=pytorch_blog&amp;utm_medium=referral&amp;utm_campaign=trains_c&amp;utm_content=audioclass" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="8fe9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">出于本博客的目的，我们将使用<a class="ae ma" href="https://urbansounddataset.weebly.com/urbansound8k.html" rel="noopener ugc nofollow" target="_blank"> UrbanSound8K </a>数据集，该数据集包含来自10个类别的8732个带标签的声音摘录(&lt; =4s)，包括狗叫、警笛和街头音乐。我们将使用预训练的ResNet模型来分类这些音频文件。</p><p id="496f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">我们将从初始化Allegro Trains来跟踪我们所做的一切开始:</strong></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6c6d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">接下来，我们将确保代码中没有隐藏“神奇数字”,并且所有脚本参数都反映在实验管理器web应用程序中。当编写python脚本时，可以使用流行的argparse包，Allegro Trains会自动拾取它。在我们编写Jupyter笔记本示例时，我们将定义一个配置字典，并将其连接到Allegro Trains任务对象:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="c539" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在是时候定义我们的PyTorch数据集对象了。这个对象应该包括数据加载和数据预处理。数据集元数据的加载在类的构造函数中完成，并基于UrbanSound8K数据集结构进行配置。因此，它将如下所示:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6a8e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">接下来我们要做的是定义Dataset类的__ <em class="nr"> getitem__ </em>方法。如前所述，我们希望这部分执行几个预处理步骤:</p><ol class=""><li id="c6d6" class="my mz iq lg b lh li lk ll ln na lr nb lv nc lz ns ne nf ng bi translated">加载音频文件</li><li id="0214" class="my mz iq lg b lh nh lk ni ln nj lr nk lv nl lz ns ne nf ng bi translated">将其重新采样到预配置的采样速率，注意这里这样做是为了简单起见。对音频文件进行重新采样是一项耗时的功能，会显著降低训练速度，并导致GPU利用率下降。建议在训练周期之前对所有文件执行该预处理功能。</li><li id="127e" class="my mz iq lg b lh nh lk ni ln nj lr nk lv nl lz ns ne nf ng bi translated">将其转换成单声道音频信号</li><li id="7150" class="my mz iq lg b lh nh lk ni ln nj lr nk lv nl lz ns ne nf ng bi translated">将其转换成Mel频谱图信号</li></ol><p id="2636" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">除此之外，我们希望所有转换后的信号具有相同的形状。因此，我们将把所有Mel光谱图裁剪到一个预先配置的长度，并把比这个长度短的光谱图补零。结果应该是这样的:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="7f5d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在最精彩的部分来了。由于我们已经将问题从音频域转换到图像域，所以我们不需要担心定义模型。我们可以使用Torchvision自带的内置模型之一。我们决定选择有效且健壮的ResNet模型。由于数据集很小，我们希望降低过度拟合的风险，因此我们将使用小而有效的ResNet18模型。除了我们刚刚采取的这个伟大的捷径，Torchvision使我们能够加载在Imagenet上预先训练的模型，因此训练将更短，更有效。</p><p id="80d5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们需要做的就是使模型的输入层和输出层适应我们的数据。这很容易做到，如下所示:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="311b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">就是这样！我们可以开始训练我们的模型了。在这篇博客中，我们不会讨论培训和评估循环的结构。它们非常简单明了——你可以在<a class="ae ma" href="http://t.allegro.ai/UrbanSound8" rel="noopener ugc nofollow" target="_blank">完整笔记本</a>中查找它们。我们只需注意，在训练和评估过程中，我们确保将音频信号、标量(损耗、精度)和频谱图报告给PyTorch的内置TensorBoard类，以便进行调试。Allegro Trains会自动拾取发送到TensorBoard的所有报告，并将它们记录在web应用程序中您的实验下。</p><p id="f1fe" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们剩下要做的就是使用<a class="ae ma" href="http://t.allegro.ai/git_trains_agent_p1" rel="noopener ugc nofollow" target="_blank">火车代理</a>在本地或远程机器上执行我们的Jupyter笔记本，并在<a class="ae ma" href="http://t.allegro.ai/demo_pytorch_audioclassification" rel="noopener ugc nofollow" target="_blank"> Allegro Trains的web应用</a>上观看我们的培训进度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/04add7f03bd0de915b900ccbc60a8c6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1LepgrK4PgZ8ZvUn"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Allegro Trains webapp中标量报告的快照</p></figure><p id="30c5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因为我们确保每隔几个迭代就报告调试数据，所以我们可以检查Allegro Trains webapp中的调试样本部分，并确保输入模型的数据是有意义的。我们可以听原始音频信号或检查频谱图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/e5381bf10da0df79190cacc460308ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sCzgFzDsVh6UdPWF"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Mel光谱图调试报告的快照</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/b62608a046915cc79d7fbf2ac1270546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*L8bCsLJwtc-oWSeD"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">音频调试样本的快照</p></figure><h1 id="6f70" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">摘要</h1><p id="5043" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">自动音频分类是一个不断发展的研究领域，包括语音、音乐和环境声音等领域。这一新兴领域可以极大地受益于丰富的经验和为计算机视觉任务开发的各种工具。因此，利用PyTorch生态系统开源工具可以推动您的音频分类机器学习项目。您不仅可以享受一套免费的开源生产力工具，还可以通过将信号从时域转换到频域，使用一套强大且经过验证的预训练计算机视觉模型。</p><p id="bc7c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在本教程中，我们演示了如何使用Tochaudio、Torchvision和Allegro Trains来完成简单有效的音频分类任务。通过零集成工作和零成本，您可以获得一个通用的培训和评估脚本。要了解更多信息，请参考<a class="ae ma" href="https://allegro.ai/docs/?utm_source=pytorch_blog&amp;utm_medium=referral&amp;utm_campaign=trains_c&amp;utm_content=audioclass" rel="noopener ugc nofollow" target="_blank">快板列车文件</a>、<a class="ae ma" href="https://pytorch.org/audio" rel="noopener ugc nofollow" target="_blank">火炬广播文件</a>和<a class="ae ma" href="https://pytorch.org/docs/stable/torchvision/index.html" rel="noopener ugc nofollow" target="_blank">火炬广播文件</a>。</p><p id="f084" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在本系列的下一篇博文中，我们将展示如何使用PyTorch生态系统轻松创建机器学习管道。这对于我们有重复任务序列的情况非常有效。比如预处理和训练，就像我们在这篇博文开头提到的。</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="8a09" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="nr">原载于2020年10月18日</em><a class="ae ma" href="https://allegro.ai/blog/audio-classification-with-pytorchs-ecosystem-tools/" rel="noopener ugc nofollow" target="_blank"><em class="nr">https://allegro . ai</em></a><em class="nr">。</em></p></div></div>    
</body>
</html>