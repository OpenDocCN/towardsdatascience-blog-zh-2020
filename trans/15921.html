<html>
<head>
<title>Math behind the assumptions of linear regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归假设背后的数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/math-behind-assumptions-of-linear-regression-451532318e8b?source=collection_archive---------34-----------------------#2020-11-02">https://towardsdatascience.com/math-behind-assumptions-of-linear-regression-451532318e8b?source=collection_archive---------34-----------------------#2020-11-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/34de63b356740385c55414ed18c25999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*STJezHLqg7tYaIcY"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">杰斯温·托马斯在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="de3c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在对他们的数据应用线性回归时，必须记住某些假设。当我开始阅读线性回归时，我无法理解这些假设背后的必要性。我们在线性回归中所做的就是最小化预测目标变量(<strong class="kf ir"> ŷ </strong>)和原始目标变量(<strong class="kf ir"> y </strong>)之间的差的平方和，以找到最佳拟合线或平面。在解决这个问题时，为什么下面的假设会出现？为什么要素之间的多重共线性是一个问题。为什么我们要检查残差的分布？为什么我们必须检查观测值之间的自相关性？在理解线性回归的同时，所有这些问题不断出现。所以，我决定在这上面多花些时间，是的，我得到了上述问题的答案。</p><p id="8a5b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇博客中，我将通过研究用于解决这个优化问题的数学推导和概念，来解释这些线性回归假设背后的基本原理。</p><p id="5dda" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">琐碎的假设:</strong></p><ol class=""><li id="2ea9" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated"><strong class="kf ir">特征和目标变量之间的线性:</strong></li></ol><p id="2105" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑一下这个表达</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/ac5a86433ea43292be6ba232335ef100.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*giuTyPmFU6zhWIvYfk9ynA@2x.png"/></div></figure><p id="75ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，如果m1和m2都是实数，那么我们可以说x1和x2与目标变量(y)线性相关。如果m1和m2不是实数，那么我们可以说x1和x2与目标变量y没有线性关系。</p><blockquote class="lp lq lr"><p id="91aa" class="kd ke ls kf b kg kh ki kj kk kl km kn lt kp kq kr lu kt ku kv lv kx ky kz la ij bi translated">例如，如果m1 = <strong class="kf ir"> √ </strong> x1和m2 = <strong class="kf ir"> √ </strong> x2，那么y = x1**3/2 + x2**3/2，这里x1和x2都不线性依赖于y。</p></blockquote><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/a843d79dd20c23ce4469a6af66354ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*Ex4IGDK2ePuIDgiDsbiJWQ@2x.png"/></div></figure><p id="b159" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> 2。同方差</strong>:同方差意味着从数据中提取的样本的方差应该保持不变，也就是说，我们需要我们的数据点(观察值)更接近我们预测的平面(如果在空间上绘制的话)。</p><p id="7941" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下图中，您可以看到在第一种情况下方差保持不变(同方差行为)，而在第二种情况下方差保持不变(异方差行为)。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lx"><img src="../Images/de5817d93f06106aee1be5f94b9bf6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xMn5bs5Pfk73yH5TfNmww.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">同性恋行为</p></figure><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ly"><img src="../Images/113726a65aa1995a35e5be482c46fa68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EIVLUfmLNY9D2IoiPkUakQ.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">异方差行为</p></figure><p id="32f2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> 3。没有异常值</strong>:因为我们正在处理欧几里德距离，任何异常值都会显著影响我们预测的回归平面。去除异常值，然后执行任何机器学习算法(不仅仅是线性回归)总是更好。</p><p id="f7ba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">非平凡的假设</strong>:</p><p id="e508" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了理解其他假设背后的原因，有必要进行一些推导。</p><p id="334f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">选择成本函数:</strong></p><p id="410f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">选择<strong class="kf ir"><em class="ls">【OLS】</em></strong>作为我们的代价函数是有代价的。为了最终确定OLS作为我们的成本函数，我们必须处理某些假设。下面的推导将使事情更加清楚。</p><p id="2bb6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">任何线性方程都可以写成这样的形式，</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lz"><img src="../Images/ecedac79b1c430d14cb60d319f68a9c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oe51DlbsiYnJ75HGGdIGIg@2x.png"/></div></div></figure><p id="d58b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">引入术语<strong class="kf ir"> <em class="ls"> ε(i) </em> </strong>是为了捕捉给定θ(<strong class="kf ir"><em class="ls">θ</em></strong>)引起的未建模效应和误差。换句话说，它可以被称为误差项或残差。</p><p id="fc0c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们假设<strong class="kf ir"> <em class="ls"> ε </em> </strong>服从高斯分布(均值为零且有一些方差)<strong class="kf ir"><em class="ls">ε(I)’</em></strong><em class="ls">s</em>独立同相关((i.i.d .)。</p><p id="4930" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">页（page的缩写）D.F的<strong class="kf ir"> <em class="ls"> ε(i) </em> </strong>可以写成</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ma"><img src="../Images/14023dc3afdd7a3eebcb143375e2441f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l5NWVNJxqJKuFGNECtxpRQ@2x.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mb"> <em class="mc">的概率密度函数ε(i) </em> </strong></p></figure><p id="6de9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也可以写成，</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi md"><img src="../Images/64837e414a889c1035be64df012d7342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3MHGQCyDzhiUZOxT8cqwkg@2x.png"/></div></div></figure><p id="027e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的目标是在给定数据的情况下获得最高概率的<strong class="kf ir"><em class="ls">y(I)</em></strong>，也就是说，我们需要最大化每个点的pdf的可能性。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi me"><img src="../Images/d49dabcbb76b2821da9442e6cf1c935a.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*7nak_76BIBl-Phrp28aA6Q.png"/></div></figure><p id="e0be" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们知道对数是一个单调函数。所以，<strong class="kf ir"><em class="ls">【L(θ)</em></strong>在<strong class="kf ir"> <em class="ls"> log(L(θ)) </em> </strong>最大时最大。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/518c2699fca955e99b8635779b3a72d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*mnRtvpwcY9WweC8RL9QAQg@2x.png"/></div></figure><p id="4f28" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">利用对数属性展开<strong class="kf ir"> <em class="ls"> log(L(θ)) </em> </strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/dabd49e40756d9888432ac4c26d02078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*UuoLickDS_waVQStAr0wpg@2x.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来源:<a class="ae kc" href="http://cs229.stanford.edu/notes2020spring/cs229-notes1.pdf" rel="noopener ugc nofollow" target="_blank">http://cs229.stanford.edu/notes2020spring/cs229-notes1.pdf</a>，第10-12页</p></figure><p id="48ee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="ls"> l(θ) </em> </strong>在下式最小时最大。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mh"><img src="../Images/8801db24c414975084b5298c1294853a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LaLMN15M1LDsqTLNkjBKtw.png"/></div></div></figure><p id="c2bf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个表达式就是我们的OLS(普通最小二乘)成本函数。</p><p id="5168" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注:这是证明选择OLS作为成本函数的一种方法。可能有其他方法可以推断出相同的最终结果。</p><p id="e7b5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">我们选择OLS作为成本函数的假设:</strong></p><p id="96e1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上述推导中，我们假设残差(<strong class="kf ir"> <em class="ls"> ε </em> </strong>)具有正态分布，因此残差具有<strong class="kf ir">正态性。</strong></p><p id="e0f8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还有，正如我们假设<strong class="kf ir"> <em class="ls"> ε(i)的</em> </strong>是独立同相关的((i.i.d)，残差也应该是独立同分布的。这基本上意味着，残差和时间之间不应该有任何关系。= &gt; <strong class="kf ir">无自相关。</strong></p><p id="b2bc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">最小化成本函数:</strong>现在我们知道了我们的成本函数，我们的下一个目标就是最小化它。有多种方法可以最小化它。我们可以使用迭代算法，如梯度下降，牛顿法等。或者我们可以使用线性代数技术来寻找最优θ。与其他算法相比，后一种算法花费的计算时间少得多，收敛速度也快得多。</p><p id="6968" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">使用线性代数最小化成本函数:</strong></p><p id="83a8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在矩阵形式中，成本函数可以写成</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/814973aae888eba887e56808506f765e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*ultCd7s5q27vefpp0TpfKw@2x.png"/></div></figure><p id="c731" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">应用适当的技术后，表达式最终简化为</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/9f7064b3ad2706043918345d85b47ef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*nsU4iPqKYISnmwck280xTg@2x.png"/></div></figure><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/f0419bceb5452f2d326d97507617a255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*muXgKQe9nNyCzMVILhm-Lg@2x.png"/></div></figure><p id="2b29" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">推论:</strong></p><p id="caa8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">显然，从上面的表达式可以看出，如果xtx可逆，则θ(<strong class="kf ir"><em class="ls">θ</em></strong>)存在。为了使<strong class="kf ir"> X </strong> T <strong class="kf ir"> X </strong>可逆，X应该具有线性独立的列。为了有独立的列，X不应该有重复的列，因此<strong class="kf ir">没有多重共线性</strong>。</p><blockquote class="lp lq lr"><p id="a991" class="kd ke ls kf b kg kh ki kj kk kl km kn lt kp kq kr lu kt ku kv lv kx ky kz la ij bi translated">Sklearn的线性回归使用LAPACK(用FORTRAN和C语言编写)进行矩阵的逆运算(XTX)。它根据LAPACK中使用的驱动程序，使用LQ或QR或奇异值分解来求逆。矩阵分解技术用于寻找XTX的逆矩阵，即使XTX不是满秩矩阵<strong class="kf ir">，即</strong> X没有独立列。</p></blockquote><p id="93ea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们来看一个解释这一点的例子</p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="8533" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们从sklearn的数据集模块加载一个玩具数据集</p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="925e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">定义x，y，然后添加虚拟列(重复列)来检查其性能</p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="df7c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行sklearn的线性回归</p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="5eb8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如您所看到的，即使在添加了重复的列之后，它仍然工作得很好。</p><p id="9066" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们实现一个定制的线性回归模型</p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="cfa0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，由于<strong class="kf ir"> xTx </strong>不是奇异矩阵，所以它不能执行<strong class="kf ir">逆</strong>运算。</p><p id="940c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">结论</strong></p><p id="436a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇博客中，我们已经通过一些推导看到了线性回归假设背后的原因。我们也看到了为线性回归实现定制代码的缺点，而如果从sklearn的库中取出，它工作得很好，因为它使用了高级矩阵技术。</p><p id="1add" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">希望这篇博客能帮助你更好地理解线性回归。</p><p id="d015" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参考文献:</strong></p><p id="dd84" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">cs-229课程讲义二，<a class="ae kc" href="http://cs229.stanford.edu/syllabus-spring2019.html" rel="noopener ugc nofollow" target="_blank">http://cs229.stanford.edu/syllabus-spring2019.html</a></p><div class="mn mo gp gr mp mq"><a href="https://www.netlib.org/lapack/lug/node27.html" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">线性最小二乘(LLS)问题</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">下一页:广义线性最小二乘法上一页:驱动程序上一页:线性方程内容索引…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">www.netlib.org</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne jw mq"/></div></div></a></div><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="nf mm l"/></div></figure><p id="a0a3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请点击这里查看我的其他博客:</p><div class="mn mo gp gr mp mq"><a rel="noopener follow" target="_blank" href="/convolutional-neural-networks-cnns-a-practical-perspective-c7b3b2091aa8"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">卷积神经网络(CNN)——实用的观点</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">深入了解卷积神经网络的主要操作，并使用Mel光谱图对心跳进行分类…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="ng l nb nc nd mz ne jw mq"/></div></div></a></div><div class="mn mo gp gr mp mq"><a rel="noopener follow" target="_blank" href="/end-to-end-case-study-bike-sharing-demand-dataset-53201926c8db"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">端到端案例研究:自行车共享需求数据集</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">大家好！！</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="nh l nb nc nd mz ne jw mq"/></div></div></a></div></div></div>    
</body>
</html>