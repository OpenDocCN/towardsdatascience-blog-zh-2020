<html>
<head>
<title>Xception: Implementing from scratch using Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">例外:使用Tensorflow从头实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/xception-from-scratch-using-tensorflow-even-better-than-inception-940fb231ced9?source=collection_archive---------6-----------------------#2020-10-22">https://towardsdatascience.com/xception-from-scratch-using-tensorflow-even-better-than-inception-940fb231ced9?source=collection_archive---------6-----------------------#2020-10-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="13e5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">甚至比《盗梦空间》还要好</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/52b1eadf8ff694d5450fceb2ba27154d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t6qfo9ucYza_lbLfg5-p_w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。Xception架构(来源:图片来自原论文)</p></figure><p id="02fd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">卷积神经网络(CNN)已经走过了漫长的道路，从LeNet-style、AlexNet、VGG模型(使用简单的卷积层堆栈用于特征提取，最大池层用于空间子采样，一个接一个地堆叠)到Inception和ResNet网络(在每层中使用跳过连接和多个卷积和最大池块)。自从引入以来，计算机视觉中最好的网络之一就是盗梦空间网络。Inception模型使用一堆模块，每个模块包含一堆特征提取器，这允许它们用更少的参数学习更丰富的表示。</p><p id="fb58" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例外纸—<a class="ae lu" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1610.02357</a></p><p id="2418" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们在图1中看到的，异常模块有3个主要部分。入口流、中间流(重复8次)和出口流。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/0cfa170018bd6c9143ad18de44fe6793.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*SIFn9-WKGtivHCr4J63crg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。Xception架构的入口流(来源:图片来自原论文)</p></figure><p id="5c31" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">入口流具有两个卷积层块，后跟一个ReLU激活。该图还详细提到了过滤器的数量、过滤器大小(内核大小)和步长。</p><p id="b250" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有各种可分离的卷积层。还有最大池层。当步幅不等于1时，也提到步幅。也有跳过连接，我们使用“添加”来合并两个张量。它还显示了每个流中输入张量的形状。例如，我们从299x299x3的图像大小开始，在进入流之后，我们得到19x19x728的图像大小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/98847ad21bd14049be93f0ddf967ea98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*XbkrLxJ9G-jmJ2OWixBEGg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。Xception架构的中间和出口流程(来源:图片来自原论文)</p></figure><p id="8d0b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">类似地，对于中间流和出口流，该图清楚地解释了图像大小、各种层、过滤器的数量、过滤器的形状、池的类型、重复的数量以及最后添加完全连接的层的选项。</p><p id="be0a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，所有卷积和可分离卷积层都要进行批量归一化。</p><h2 id="b1e8" class="lx ly it bd lz ma mb dn mc md me dp mf lh mg mh mi ll mj mk ml lp mm mn mo mp bi translated">可分离卷积层</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/ead99aada838d85a567c14bd77955da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DfE-E_4TqPKbn-5J9EDHww.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4。可分离卷积层(来源:图片由作者创建)</p></figure><blockquote class="mr ms mt"><p id="5e2f" class="ky kz mu la b lb lc ju ld le lf jx lg mv li lj lk mw lm ln lo mx lq lr ls lt im bi translated">可分离卷积包括首先执行深度方向的空间卷积(其分别作用于每个输入声道),然后执行混合所得输出声道的点方向卷积。-来自Keras文档</p></blockquote><p id="448c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我们有一个大小为(K，K，3)的输入张量。k是空间维度，3是特征图/通道的数量。正如我们从上面的Keras文档中看到的，首先我们需要对每个输入通道分别实施深度方向的空间卷积。所以我们用K，K，1——图像/张量的第一通道。假设我们使用大小为3x3x1的过滤器。并且该滤波器应用于输入张量的所有三个通道。因为有3个通道，所以我们得到的维数是3x3x1x3。这在图4的深度方向卷积部分中示出。</p><p id="fbf4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这之后，将所有3个输出集合在一起，我们获得大小为(L，L，3)的张量。L的维数可以与K相同，也可以不同，这取决于先前卷积中使用的步长和填充。</p><p id="a81e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后应用逐点卷积。该滤波器的尺寸为1x1x3 (3个通道)。过滤器的数量可以是我们想要的任何数量。假设我们使用64个过滤器。所以总尺寸是1x1x3x64。最后，我们得到一个大小为LxLx64的输出张量。这显示在图4的逐点卷积部分。</p><p id="0b22" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">为什么可分卷积比正规卷积好？</strong></p><p id="bda0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们在输入张量上使用正常卷积，并且我们使用3x3x3的滤波器/核大小(核大小— (3，3)和3个特征图)。我们需要的过滤器总数是64个。所以一共3x3x3x64。</p><p id="69f4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相反，在可分离卷积中，我们首先在深度方向卷积中使用3x3x1x3，在点方向卷积中使用1x1x3x64。</p><p id="38af" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">区别在于过滤器的维度。</p><p id="3831" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">传统卷积层= 3 x3x 3 x 64 = 1728</strong></p><p id="36df" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">可分离卷积层=(3 x3x 1 x 3)+(1 x1 x3 x 64)= 27+192 = 219</strong></p><p id="8cda" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们所见，无论是在计算成本还是在内存方面，可分离卷积层都比传统卷积层更具优势。主要区别是在正常卷积中，我们是<strong class="la iu"> </strong>多次变换图像。而每一次变换都要用掉3 x3x 3 x 64 = 1728次<strong class="la iu"> </strong>乘法。在可分离卷积中，我们只对图像进行一次变换——在深度方向卷积中。然后，我们把变换后的图像简单地拉长到64通道。不必一次又一次地变换图像，我们可以节省计算能力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/eedf2318e70a25ea3bfb7a4708890cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*wxQfICjA3psSlY-Wb0l2KQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5。ImageNet上的异常性能vs Inception(来源:图片来自原始论文)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/e3844a14a0c59194f351f4e6e18af7da.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*Te2pBG7KXNfgRQsjoQTuuA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6。JFT数据集上的异常性能与初始(来源:图片来自原始论文)</p></figure><p id="4881" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">算法:</strong></p><ol class=""><li id="9032" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated">导入所有必要的层</li><li id="a0b6" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">为以下各项编写所有必要的函数:</li></ol><p id="66d6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">a.conv-巴特诺姆块体</p><p id="beb7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">b.可分离Conv- BatchNorm块</p><p id="c43e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.为3个流中的每一个写一个函数——入口、中间和出口</p><p id="3701" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.使用这些函数构建完整的模型</p><h1 id="5509" class="no ly it bd lz np nq nr mc ns nt nu mf jz nv ka mi kc nw kd ml kf nx kg mo ny bi translated">使用张量流创建异常</h1><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="e357" class="lx ly it oa b gy oe of l og oh"><em class="mu">#import necessary libraries</em><br/><br/><strong class="oa iu">import</strong> <strong class="oa iu">tensorflow</strong> <strong class="oa iu">as</strong> <strong class="oa iu">tf</strong><br/><strong class="oa iu">from</strong> <strong class="oa iu">tensorflow.keras.layers</strong> <strong class="oa iu">import</strong> Input,Dense,Conv2D,Add<br/><strong class="oa iu">from</strong> <strong class="oa iu">tensorflow.keras.layers</strong> <strong class="oa iu">import</strong> SeparableConv2D,ReLU<br/><strong class="oa iu">from</strong> <strong class="oa iu">tensorflow.keras.layers</strong> <strong class="oa iu">import</strong> BatchNormalization,MaxPool2D<br/><strong class="oa iu">from</strong> <strong class="oa iu">tensorflow.keras.layers</strong> <strong class="oa iu">import</strong> GlobalAvgPool2D<br/><strong class="oa iu">from</strong> <strong class="oa iu">tensorflow.keras</strong> <strong class="oa iu">import</strong> Model</span></pre><p id="0836" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">创建conv-巴特诺姆块:</strong></p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="0382" class="lx ly it oa b gy oe of l og oh"><em class="mu"># creating the Conv-Batch Norm block</em><br/><br/><strong class="oa iu">def</strong> conv_bn(x, filters, kernel_size, strides=1):<br/>    <br/>    x = Conv2D(filters=filters, <br/>               kernel_size = kernel_size, <br/>               strides=strides, <br/>               padding = 'same', <br/>               use_bias = <strong class="oa iu">False</strong>)(x)<br/>    x = BatchNormalization()(x)</span><span id="b408" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu">return</strong> x</span></pre><p id="0447" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Conv批处理范数块将张量——x、滤波器数量——滤波器、卷积层的内核大小——内核大小、卷积层的步距——步距作为输入。然后我们对x应用一个卷积层，然后应用批量归一化。我们添加use_bias = False，这样最终模型的参数数量，将与原始论文的参数数量相同。</p><p id="924b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">创建SeparableConv- BatchNorm块:</strong></p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="fa9e" class="lx ly it oa b gy oe of l og oh"><em class="mu"># creating separableConv-Batch Norm block</em><br/><br/><strong class="oa iu">def</strong> sep_bn(x, filters, kernel_size, strides=1):<br/>    <br/>    x = SeparableConv2D(filters=filters, <br/>                        kernel_size = kernel_size, <br/>                        strides=strides, <br/>                        padding = 'same', <br/>                        use_bias = <strong class="oa iu">False</strong>)(x)<br/>    x = BatchNormalization()(x)</span><span id="4463" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu">return</strong> x</span></pre><p id="6738" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与Conv批处理范数模块结构相似，只是我们使用了SeparableConv2D而不是Conv2D。</p><p id="01dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">入口、中间和出口流量功能:</strong></p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="dbe8" class="lx ly it oa b gy oe of l og oh"><em class="mu"># entry flow</em><br/><br/><strong class="oa iu">def</strong> entry_flow(x):<br/>    <br/>    x = conv_bn(x, filters =32, kernel_size =3, strides=2)<br/>    x = ReLU()(x)<br/>    x = conv_bn(x, filters =64, kernel_size =3, strides=1)<br/>    tensor = ReLU()(x)<br/>    <br/>    x = sep_bn(tensor, filters = 128, kernel_size =3)<br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters = 128, kernel_size =3)<br/>    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)<br/>    <br/>    tensor = conv_bn(tensor, filters=128, kernel_size = 1,strides=2)<br/>    x = Add()([tensor,x])<br/>    <br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters =256, kernel_size=3)<br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters =256, kernel_size=3)<br/>    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)<br/>    <br/>    tensor = conv_bn(tensor, filters=256, kernel_size = 1,strides=2)<br/>    x = Add()([tensor,x])<br/>    <br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters =728, kernel_size=3)<br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters =728, kernel_size=3)<br/>    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)<br/>    <br/>    tensor = conv_bn(tensor, filters=728, kernel_size = 1,strides=2)<br/>    x = Add()([tensor,x])</span><span id="113e" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu">return</strong> x</span></pre><p id="d3b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里我们只需遵循图2。它从分别具有32和64个滤波器的两个Conv层开始。每次激活后都有一个ReLU激活。</p><p id="7602" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后是一个跳过连接，这是通过使用Add完成的。</p><p id="a284" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在每个skip连接块中，有两个可分离的Conv层，后跟MaxPooling。跳跃连接本身具有跨度为2的1x1的Conv层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3c7e3d424f2a04d6ae0f0b7199b0d1bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*CvQDeL7DhGyY2m2TDmSOgA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7。中流量(来源:图片来自原论文)</p></figure><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="6c7d" class="lx ly it oa b gy oe of l og oh"><em class="mu"># middle flow</em><br/><br/><strong class="oa iu">def</strong> middle_flow(tensor):<br/>    <br/>    <strong class="oa iu">for</strong> _ <strong class="oa iu">in</strong> range(8):<br/>        x = ReLU()(tensor)<br/>        x = sep_bn(x, filters = 728, kernel_size = 3)<br/>        x = ReLU()(x)<br/>        x = sep_bn(x, filters = 728, kernel_size = 3)<br/>        x = ReLU()(x)<br/>        x = sep_bn(x, filters = 728, kernel_size = 3)<br/>        x = ReLU()(x)<br/>        tensor = Add()([tensor,x])<br/>        <br/>    <strong class="oa iu">return</strong> tensor</span></pre><p id="49c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">中间的流程遵循图7所示的步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/b35c12988de28c608e9c08f14b3e99c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*L9DzRc-LJuA9h65waZSe0Q.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8。退出流程(来源:图片来自原纸)</p></figure><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="f944" class="lx ly it oa b gy oe of l og oh"><em class="mu"># exit flow</em><br/><br/><strong class="oa iu">def</strong> exit_flow(tensor):<br/>    <br/>    x = ReLU()(tensor)<br/>    x = sep_bn(x, filters = 728,  kernel_size=3)<br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters = 1024,  kernel_size=3)<br/>    x = MaxPool2D(pool_size = 3, strides = 2, padding ='same')(x)<br/>    <br/>    tensor = conv_bn(tensor, filters =1024, kernel_size=1, strides =2)<br/>    x = Add()([tensor,x])<br/>    <br/>    x = sep_bn(x, filters = 1536,  kernel_size=3)<br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters = 2048,  kernel_size=3)<br/>    x = GlobalAvgPool2D()(x)<br/>    <br/>    x = Dense (units = 1000, activation = 'softmax')(x)<br/>    <br/>    <strong class="oa iu">return</strong> x</span></pre><p id="fba3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">退出流程遵循图8所示的步骤。</p><p id="4683" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">创建异常模型:</strong></p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="9ae8" class="lx ly it oa b gy oe of l og oh"><em class="mu"># model code</em><br/><br/>input = Input(shape = (299,299,3))<br/>x = entry_flow(input)<br/>x = middle_flow(x)<br/>output = exit_flow(x)<br/><br/>model = Model (inputs=input, outputs=output)<br/>model.summary()</span></pre><p id="6959" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出片段:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/6768534fef649e3ee021e34af02d0fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*90fz3gznbJBYjTxl3cQL6A.png"/></div></div></figure><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="775e" class="lx ly it oa b gy oe of l og oh"><strong class="oa iu">from</strong> <strong class="oa iu">tensorflow.python.keras.utils.vis_utils</strong> <strong class="oa iu">import</strong> model_to_dot<br/><strong class="oa iu">from</strong> <strong class="oa iu">IPython.display</strong> <strong class="oa iu">import</strong> SVG<br/><strong class="oa iu">import</strong> <strong class="oa iu">pydot</strong><br/><strong class="oa iu">import</strong> <strong class="oa iu">graphviz</strong><br/><br/>SVG(model_to_dot(model, show_shapes=<strong class="oa iu">True</strong>, show_layer_names=<strong class="oa iu">True</strong>, rankdir='TB',expand_nested=<strong class="oa iu">False</strong>, dpi=60, subgraph=<strong class="oa iu">False</strong>).create(prog='dot',format='svg'))</span></pre><p id="d7a7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出片段:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/04720ef9eac198c38c5711da3f0910d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOho4t05ClS-DYMe-OwLkQ.png"/></div></div></figure><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="3afd" class="lx ly it oa b gy oe of l og oh"><strong class="oa iu">import</strong> <strong class="oa iu">numpy</strong> <strong class="oa iu">as</strong> <strong class="oa iu">np</strong> <br/><strong class="oa iu">import</strong> <strong class="oa iu">tensorflow.keras.backend</strong> <strong class="oa iu">as</strong> <strong class="oa iu">K</strong> </span><span id="41cd" class="lx ly it oa b gy oi of l og oh">np.sum([K.count_params(p) <strong class="oa iu">for</strong> p <strong class="oa iu">in</strong> model.trainable_weights])</span></pre><p id="c2ad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:22855952</p><p id="4dfc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上述代码显示了可训练参数的数量。</p><p id="0a1d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">使用Tensorflow从头开始创建异常模型的完整代码:</strong></p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="04ea" class="lx ly it oa b gy oe of l og oh"><strong class="oa iu"><em class="mu">#import necessary libraries</em></strong><br/><br/><strong class="oa iu">import</strong> <strong class="oa iu">tensorflow</strong> <strong class="oa iu">as</strong> <strong class="oa iu">tf</strong><br/><strong class="oa iu">from</strong> <strong class="oa iu">tensorflow.keras.layers</strong> <strong class="oa iu">import</strong> Input,Dense,Conv2D,Add<br/><strong class="oa iu">from</strong> <strong class="oa iu">tensorflow.keras.layers</strong> <strong class="oa iu">import</strong> SeparableConv2D,ReLU<br/><strong class="oa iu">from</strong> <strong class="oa iu">tensorflow.keras.layers</strong> <strong class="oa iu">import</strong> BatchNormalization,MaxPool2D<br/><strong class="oa iu">from</strong> <strong class="oa iu">tensorflow.keras.layers</strong> <strong class="oa iu">import</strong> GlobalAvgPool2D<br/><strong class="oa iu">from</strong> <strong class="oa iu">tensorflow.keras</strong> <strong class="oa iu">import</strong> Model</span><span id="be6d" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu"><em class="mu"># creating the Conv-Batch Norm block</em></strong><br/><br/><strong class="oa iu">def</strong> conv_bn(x, filters, kernel_size, strides=1):<br/>    <br/>    x = Conv2D(filters=filters, <br/>               kernel_size = kernel_size, <br/>               strides=strides, <br/>               padding = 'same', <br/>               use_bias = <strong class="oa iu">False</strong>)(x)<br/>    x = BatchNormalization()(x)</span><span id="c219" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu">return</strong> x</span><span id="73f4" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu"><em class="mu"># creating separableConv-Batch Norm block</em></strong><br/><br/><strong class="oa iu">def</strong> sep_bn(x, filters, kernel_size, strides=1):<br/>    <br/>    x = SeparableConv2D(filters=filters, <br/>                        kernel_size = kernel_size, <br/>                        strides=strides, <br/>                        padding = 'same', <br/>                        use_bias = <strong class="oa iu">False</strong>)(x)<br/>    x = BatchNormalization()(x)</span><span id="53a0" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu">return</strong> x</span><span id="2538" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu"><em class="mu"># entry flow</em></strong><br/><br/><strong class="oa iu">def</strong> entry_flow(x):<br/>    <br/>    x = conv_bn(x, filters =32, kernel_size =3, strides=2)<br/>    x = ReLU()(x)<br/>    x = conv_bn(x, filters =64, kernel_size =3, strides=1)<br/>    tensor = ReLU()(x)<br/>    <br/>    x = sep_bn(tensor, filters = 128, kernel_size =3)<br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters = 128, kernel_size =3)<br/>    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)<br/>    <br/>    tensor = conv_bn(tensor, filters=128, kernel_size = 1,strides=2)<br/>    x = Add()([tensor,x])<br/>    <br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters =256, kernel_size=3)<br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters =256, kernel_size=3)<br/>    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)<br/>    <br/>    tensor = conv_bn(tensor, filters=256, kernel_size = 1,strides=2)<br/>    x = Add()([tensor,x])<br/>    <br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters =728, kernel_size=3)<br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters =728, kernel_size=3)<br/>    x = MaxPool2D(pool_size=3, strides=2, padding = 'same')(x)<br/>    <br/>    tensor = conv_bn(tensor, filters=728, kernel_size = 1,strides=2)<br/>    x = Add()([tensor,x])</span><span id="6353" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu">return</strong> x</span><span id="fcd0" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu"><em class="mu"># middle flow</em></strong><br/><br/><strong class="oa iu">def</strong> middle_flow(tensor):<br/>    <br/>    <strong class="oa iu">for</strong> _ <strong class="oa iu">in</strong> range(8):<br/>        x = ReLU()(tensor)<br/>        x = sep_bn(x, filters = 728, kernel_size = 3)<br/>        x = ReLU()(x)<br/>        x = sep_bn(x, filters = 728, kernel_size = 3)<br/>        x = ReLU()(x)<br/>        x = sep_bn(x, filters = 728, kernel_size = 3)<br/>        x = ReLU()(x)<br/>        tensor = Add()([tensor,x])<br/>        <br/>    <strong class="oa iu">return</strong> tensor</span><span id="b1ca" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu"><em class="mu"># exit flow</em></strong><br/><br/><strong class="oa iu">def</strong> exit_flow(tensor):<br/>    <br/>    x = ReLU()(tensor)<br/>    x = sep_bn(x, filters = 728,  kernel_size=3)<br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters = 1024,  kernel_size=3)<br/>    x = MaxPool2D(pool_size = 3, strides = 2, padding ='same')(x)<br/>    <br/>    tensor = conv_bn(tensor, filters =1024, kernel_size=1, strides =2)<br/>    x = Add()([tensor,x])<br/>    <br/>    x = sep_bn(x, filters = 1536,  kernel_size=3)<br/>    x = ReLU()(x)<br/>    x = sep_bn(x, filters = 2048,  kernel_size=3)<br/>    x = GlobalAvgPool2D()(x)<br/>    <br/>    x = Dense (units = 1000, activation = 'softmax')(x)<br/>    <br/>    <strong class="oa iu">return</strong> x</span><span id="29c4" class="lx ly it oa b gy oi of l og oh"><strong class="oa iu"><em class="mu"># model code</em></strong><br/><br/>input = Input(shape = (299,299,3))<br/>x = entry_flow(input)<br/>x = middle_flow(x)<br/>output = exit_flow(x)<br/><br/>model = Model (inputs=input, outputs=output)<br/>model.summary()</span></pre><p id="fb29" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">结论:</strong></p><p id="573f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如图5和图6所示，与ImageNet数据集相比，Xception架构在JFT数据集上显示出比Inception网络更好的性能提升。《例外》的作者认为，这是因为《盗梦空间》被设计成专注于ImageNet，因此可能过度适合特定的任务。另一方面，这两种架构都没有针对JFT数据集进行过调整。</p><p id="b8e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，Inception有大约2360万个参数，而Xception有2280万个参数。</p><p id="64a9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如图1所示，本文中很容易解释Xception架构，这使得使用TensorFlow实现网络架构变得非常容易。</p><p id="26a7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">参考文献:</strong></p><ol class=""><li id="a1e0" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated">Franç ois Chollet，Xception:深度可分卷积深度学习，<a class="ae lu" href="https://arxiv.org/abs/1610.02357v3" rel="noopener ugc nofollow" target="_blank">arXiv:1610.02357 v3</a><strong class="la iu">【cs。CV]，2017 </strong></li></ol></div></div>    
</body>
</html>