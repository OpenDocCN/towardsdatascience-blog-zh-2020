<html>
<head>
<title>Cheat sheet for implementing 7 methods for selecting the optimal number of clusters in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Python中实现选择最佳集群数量的7种方法的备忘单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad?source=collection_archive---------1-----------------------#2020-10-25">https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad?source=collection_archive---------1-----------------------#2020-10-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1488" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于多个聚类验证指标(如间隙统计、轮廓系数、Calinski-Harabasz指数等)选择最佳聚类数。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ae6ffb081a6db340df98572b344b4446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z9yw7gLfrSAK2yqd"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Mehrshad Rajabi 在<a class="ae kv" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="f813" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">细分为检查有意义的细分提供了一个数据驱动的角度，高管可以使用它来采取有针对性的行动和改善业务成果。许多高管冒着基于过度概括做出决策的风险，因为他们利用一刀切的方法来评估他们的商业生态系统。然而，分段通过提供多个有意义的透镜来分解数据并采取行动，从而改善了决策。</p><p id="b712" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在试图对客户或产品进行细分时面临的最令人困惑的问题之一是选择理想的细分数量。这是K均值、凝聚聚类和GMM聚类等多种聚类算法的关键参数。除非我们的数据只有2或3维，否则不可能直观地理解数据中存在的聚类。而在大多数实际应用中，我们会有3个以上的维度。本博客将帮助读者理解并快速实现选择最佳集群数量的最流行技术:</p><ol class=""><li id="3826" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">差距统计</li><li id="bf39" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">肘法</li><li id="4f1a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">轮廓系数</li><li id="726c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">卡林斯基-哈拉巴斯指数</li><li id="6a10" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">戴维斯-波尔丁指数</li><li id="8412" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">系统树图</li><li id="f3c1" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">贝叶斯信息准则(BIC)</li></ol><p id="73fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个练习中，我们将使用一家为孕妇提供服装的在线商店的点击流<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/clickstream+data+for+online+shopping" rel="noopener ugc nofollow" target="_blank">数据</a>。它有从2008年4月到2008年8月的数据，包括产品类别、图片在网页上的位置、IP地址的来源国以及产品的美元价格等变量。在选择最佳的聚类数之前，我们需要准备数据进行分割。</p><p id="01f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mg">我建议您在继续下一步之前，阅读下面的文章，深入了解为分段准备数据的不同步骤:</em></p><p id="4205" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/one-hot-encoding-standardization-pca-data-preparation-steps-for-segmentation-in-python-24d07671cf0b"> <em class="mg"> One Hot编码、标准化、PCA:python中分割的数据准备</em> </a></p><h1 id="292a" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated"><strong class="ak">差距统计</strong></h1><p id="8169" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">差距统计是由斯坦福大学的研究人员<a class="ae kv" href="http://www.stanford.edu/~hastie/Papers/gap.pdf" rel="noopener ugc nofollow" target="_blank">蒂布拉尼、瓦尔特和哈斯蒂在他们2001年的论文</a>中提出的。他们的方法背后的想法是找到一种方法来比较聚类紧密度与数据的零引用分布，即没有明显聚类的分布。他们对最佳聚类数的估计是原始数据上的聚类紧密度落在该参考曲线之下最远的值。此信息包含在以下差距统计公式中:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/869ad0bdc1d58a4c3ed10f2d6b3063a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/0*GONXmtupcYOGHgYE"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1b8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中，Wk是基于类内误差平方和(WSS)的聚类紧密度的度量:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/d495447213077f988f3a25dbf062b36e.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/0*MJFy4qeBiH5BRn6m"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="73ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">误差平方和的类内计算由<code class="fe ng nh ni nj b">KMeans</code>函数的<code class="fe ng nh ni nj b">inertia_</code>属性进行，如下所示:</p><ul class=""><li id="e535" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nk ly lz ma bi translated">每个点到聚类中心的距离的平方(误差平方)</li><li id="019e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nk ly lz ma bi translated">WSS分数是所有点的这些平方误差的总和</li></ul><p id="43a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在python中计算k均值聚类的间隙统计包括以下步骤:</p><ul class=""><li id="a7ca" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nk ly lz ma bi translated">将观察到的数据在不同数量的聚类上进行聚类，并计算我们的聚类的紧密度</li><li id="4fca" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nk ly lz ma bi translated">生成参考数据集，并用不同数量的聚类对每个数据集进行聚类。参考数据集是使用<code class="fe ng nh ni nj b">random_sample</code>函数根据“连续均匀”分布创建的。</li><li id="31ef" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nk ly lz ma bi translated">计算参考数据集上聚类的平均紧密度</li><li id="330f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nk ly lz ma bi translated">根据参考数据和原始数据的聚类紧密度差异计算间隙统计数据</li></ul><pre class="kg kh ki kj gt nl nj nm nn aw no bi"><span id="08bb" class="np mi iq nj b gy nq nr l ns nt"># Gap Statistic for K means</span><span id="51ec" class="np mi iq nj b gy nu nr l ns nt">def optimalK(data, nrefs=3, maxClusters=15):<br/>    """<br/>    Calculates KMeans optimal K using Gap Statistic <br/>    Params:<br/>        data: ndarry of shape (n_samples, n_features)<br/>        nrefs: number of sample reference datasets to create<br/>        maxClusters: Maximum number of clusters to test for<br/>    Returns: (gaps, optimalK)<br/>    """<br/>    gaps = np.zeros((len(range(1, maxClusters)),))<br/>    resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})<br/>    for gap_index, k in enumerate(range(1, maxClusters)):</span><span id="90b4" class="np mi iq nj b gy nu nr l ns nt"># Holder for reference dispersion results<br/>        refDisps = np.zeros(nrefs)</span><span id="fcfb" class="np mi iq nj b gy nu nr l ns nt"># For n references, generate random sample and perform kmeans getting resulting dispersion of each loop<br/>        for i in range(nrefs):<br/>            <br/>            # Create new random reference set<br/>            randomReference = np.random.random_sample(size=data.shape)<br/>            <br/>            # Fit to it<br/>            km = KMeans(k)<br/>            km.fit(randomReference)<br/>            <br/>            refDisp = km.inertia_<br/>            refDisps[i] = refDisp</span><span id="d931" class="np mi iq nj b gy nu nr l ns nt"># Fit cluster to original data and create dispersion<br/>        km = KMeans(k)<br/>        km.fit(data)<br/>        <br/>        origDisp = km.inertia_</span><span id="56df" class="np mi iq nj b gy nu nr l ns nt"># Calculate gap statistic<br/>        gap = np.log(np.mean(refDisps)) - np.log(origDisp)</span><span id="1fa6" class="np mi iq nj b gy nu nr l ns nt"># Assign this loop's gap statistic to gaps<br/>        gaps[gap_index] = gap<br/>        <br/>        resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)</span><span id="06e3" class="np mi iq nj b gy nu nr l ns nt">return (gaps.argmax() + 1, resultsdf)</span><span id="fa7c" class="np mi iq nj b gy nu nr l ns nt">score_g, df = optimalK(cluster_df, nrefs=5, maxClusters=30)</span><span id="095f" class="np mi iq nj b gy nu nr l ns nt">plt.plot(df['clusterCount'], df['gap'], linestyle='--', marker='o', color='b');<br/>plt.xlabel('K');<br/>plt.ylabel('Gap Statistic');<br/>plt.title('Gap Statistic vs. K');</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c851f666b916422968d6ea80fb454c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*iviLx37L1laZx6XNe0dxpQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:不同聚类值的差距统计(图片由作者提供)</p></figure><p id="468d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如图1所示，29个聚类的差距统计最大，因此，我们可以选择29个聚类作为K均值。</p><h1 id="fddc" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">肘法</h1><p id="8b57" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">这是确定最佳聚类数的最常用方法。<em class="mg"> </em>该方法基于计算不同数量的组(k)的组内误差平方和(WSS ),并选择WSS变化首先开始减小的k。</p><p id="fbd8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">肘形法背后的思想是，对于少量的聚类，所解释的变化迅速变化，然后它变慢，导致曲线中形成肘形。拐点是我们可以用于聚类算法的聚类数。关于这种方法的更多细节可以在袁春辉和杨海涛的论文中找到。</p><p id="b818" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用<a class="ae kv" href="https://www.scikit-yb.org/en/latest/" rel="noopener ugc nofollow" target="_blank"> YellowBrick </a>库，它可以用几行代码实现elbow方法。它是Scikit-Learn的包装器，并且有一些很酷的机器学习可视化！</p><pre class="kg kh ki kj gt nl nj nm nn aw no bi"><span id="2683" class="np mi iq nj b gy nq nr l ns nt"># Elbow Method for K means</span><span id="3fd5" class="np mi iq nj b gy nu nr l ns nt"># Import ElbowVisualizer<br/>from yellowbrick.cluster import KElbowVisualizer<br/>model = KMeans()<br/># k is range of number of clusters.<br/>visualizer = KElbowVisualizer(model, k=(2,30), timings= True)<br/>visualizer.fit(cluster_df)        # Fit data to visualizer<br/>visualizer.show()        # Finalize and render figure</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c9c02b207192cf8aebd6cf9aa1681225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*d-4y0RU1xv2tnU8cINWYvA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:肘法结果(图片由作者提供)</p></figure><p id="1000" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于介于2到30之间的聚类值范围，<code class="fe ng nh ni nj b">KElbowVisualizer</code>函数适合于<code class="fe ng nh ni nj b">KMeans</code>模型。如图2所示，拐点是用8个集群实现的，它由函数本身突出显示。该函数还通过绿线告知我们为不同数量的聚类绘制模型需要多少时间。</p><h1 id="9553" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">轮廓系数</h1><p id="e7e1" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">点<em class="mg"> i </em>的轮廓系数定义如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/331271a992d756bd1f149abb94ba7615.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*IU-aQ4RoUFrGiPWr.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0306" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<em class="mg"> b(i) </em>是点<em class="mg"> i </em>到任何其他聚类中所有点的最小平均距离，而<em class="mg"> a(i) </em>是<em class="mg"> i </em>到其聚类中所有点的平均距离。例如，如果我们只有3个聚类A、B和C，并且I属于聚类C，那么通过测量<em class="mg"> i </em>到聚类A中每一点的平均距离，即I到聚类B中每一点的平均距离，并取最小结果值来计算<em class="mg"> b(i) </em>。数据集的轮廓系数是单个点的轮廓系数的平均值。</p><p id="7bc3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">轮廓系数告诉我们各个点是否被正确地分配到它们的簇中。使用轮廓系数时，我们可以使用以下经验法则:</p><ol class=""><li id="0e4b" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><em class="mg"> S(i) </em>接近0表示该点在两个聚类之间</li><li id="42e7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如果它更接近-1，那么我们最好将它分配给其他集群</li><li id="3580" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如果<em class="mg"> S(i) </em>接近1，则该点属于“正确的”聚类</li></ol><p id="4506" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于这种方法的更多细节，请参考N. Kaoungku，K. Suksut，R. Chanklan和K. Kerdprasop的这篇2018年<a class="ae kv" href="https://www.researchgate.net/publication/323588077_The_silhouette_width_criterion_for_clustering_and_association_mining_to_select_image_features" rel="noopener ugc nofollow" target="_blank">论文</a>。我们将使用<code class="fe ng nh ni nj b">KElbowVisualizer</code>函数来实现K均值聚类算法的轮廓系数:</p><pre class="kg kh ki kj gt nl nj nm nn aw no bi"><span id="fb01" class="np mi iq nj b gy nq nr l ns nt"># Silhouette Score for K means</span><span id="7a94" class="np mi iq nj b gy nu nr l ns nt"># Import ElbowVisualizer<br/>from yellowbrick.cluster import KElbowVisualizer<br/>model = KMeans()<br/># k is range of number of clusters.<br/>visualizer = KElbowVisualizer(model, k=(2,30),metric='silhouette', timings= True)<br/>visualizer.fit(cluster_df)        # Fit the data to the visualizer<br/>visualizer.show()        # Finalize and render the figure</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/309068a7dffd7cef9f23e9e241293e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*W1geZm5TEVwkYms--7UI2g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:剪影评分结果(作者图片)</p></figure><p id="85e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于轮廓得分的最佳聚类数是4。</p><h1 id="ba4e" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">卡林斯基-哈拉巴斯指数</h1><p id="10d6" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">Calinski-Harabasz指数是基于这样一种想法，即(1)本身非常紧凑的聚类和(2)彼此间隔良好的聚类是好的聚类。该指数是通过将单个对象到其聚类中心的距离的平方和的方差除以聚类中心之间的距离的平方和来计算的。卡林斯基-哈拉巴斯指数值越高，聚类模型越好。卡林斯基-哈拉巴斯指数的公式定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/f505b7103f8e1ab4c4dc6b91344613df.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*VqHwJDaqc4OxTJlR2mnlEA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="fe41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中k是聚类数，n是数据中的记录数，BCSM(聚类间散布矩阵)计算聚类间的分离度，WCSM(聚类内散布矩阵)计算聚类内的紧密度。</p><p id="144b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ng nh ni nj b">KElbowVisualizer</code>函数还能够计算卡林斯基-哈拉巴斯指数:</p><pre class="kg kh ki kj gt nl nj nm nn aw no bi"><span id="3e5a" class="np mi iq nj b gy nq nr l ns nt"># Calinski Harabasz Score for K means</span><span id="5a83" class="np mi iq nj b gy nu nr l ns nt"># Import ElbowVisualizer<br/>from yellowbrick.cluster import KElbowVisualizer<br/>model = KMeans()<br/># k is range of number of clusters.<br/>visualizer = KElbowVisualizer(model, k=(2,30),metric='calinski_harabasz', timings= True)<br/>visualizer.fit(cluster_df)        # Fit the data to the visualizer<br/>visualizer.show()        # Finalize and render the figure</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0b7a048a602e0001becebe096831b946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*EBMg66qvuP4MyrfVePWrEg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:卡林斯基哈拉巴斯指数(图片由作者提供)</p></figure><p id="13d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如图4所示，对于K均值聚类算法，当聚类数为2时，Calinski Harabasz指数最大。要更深入地了解这个指数，请参考和徐玉生的<a class="ae kv" href="https://iopscience.iop.org/article/10.1088/1757-899X/569/5/052024/pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><h1 id="483c" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated"><strong class="ak">戴维斯-波尔丁指数</strong></h1><p id="b440" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">戴维斯-波尔丁(DB)指数定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6fd6535c2958aa792b292ca83050b34d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*FNPys53FwVb1jL-m.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="7993" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<em class="mg"> n </em>是聚类的计数，而<em class="mg"> σi </em>是聚类<em class="mg"> i </em>中所有点距聚类中心<em class="mg"> ci </em>的平均距离。</p><p id="37f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像剪影系数和Calinski-Harabasz指数一样，DB指数同时捕获了聚类的分离度和紧密度。这是因为测量的“最大值”语句重复选择平均点距离其中心最远的值和中心最接近的值。但与剪影系数和Calinski-Harabasz指数不同，随着DB指数的下降，聚类得到改善。</p><pre class="kg kh ki kj gt nl nj nm nn aw no bi"><span id="b14d" class="np mi iq nj b gy nq nr l ns nt"># Davies Bouldin score for K means</span><span id="7ccd" class="np mi iq nj b gy nu nr l ns nt">from sklearn.metrics import davies_bouldin_score</span><span id="c8da" class="np mi iq nj b gy nu nr l ns nt">def get_kmeans_score(data, center):<br/>    '''<br/>    returns the kmeans score regarding Davies Bouldin for points to centers<br/>    INPUT:<br/>        data - the dataset you want to fit kmeans to<br/>        center - the number of centers you want (the k value)<br/>    OUTPUT:<br/>        score - the Davies Bouldin score for the kmeans model fit to the data<br/>    '''<br/>    #instantiate kmeans<br/>    kmeans = KMeans(n_clusters=center)</span><span id="5329" class="np mi iq nj b gy nu nr l ns nt"># Then fit the model to your data using the fit method<br/>    model = kmeans.fit_predict(cluster_df)<br/>    <br/>    # Calculate Davies Bouldin score</span><span id="2c1a" class="np mi iq nj b gy nu nr l ns nt">score = davies_bouldin_score(cluster_df, model)<br/>    <br/>    return score</span><span id="05b4" class="np mi iq nj b gy nu nr l ns nt">scores = []<br/>centers = list(range(2,30))</span><span id="b946" class="np mi iq nj b gy nu nr l ns nt">for center in centers:<br/>    scores.append(get_kmeans_score(cluster_df, center))<br/>    <br/>plt.plot(centers, scores, linestyle='--', marker='o', color='b');<br/>plt.xlabel('K');<br/>plt.ylabel('Davies Bouldin score');<br/>plt.title('Davies Bouldin score vs. K');</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8df0fb62ae16631101c69a1712766623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*Jc-Zz8uP9YzLxoLysGRhag.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:戴维斯·波尔丁得分(图片由作者提供)</p></figure><p id="2e22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如图5所示，Davies Bouldin分数在4个聚类中最小，可以考虑用于k均值算法。关于DB分数的更多细节可以在斯洛博丹·佩特罗维奇的一篇论文中找到。</p><h1 id="0900" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">系统树图</h1><p id="a46e" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">这种技术是特定于凝聚层次聚类方法的。聚类的凝聚层次方法首先将每个点视为一个单独的聚类，然后开始根据点和聚类的距离以层次方式将点连接到聚类。在另一篇博客中，我们将重点介绍这种方法的细节。为了获得分层聚类的最佳聚类数，我们使用了一个树形图，这是一个显示聚类合并或分裂顺序的树形图。</p><p id="97ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果两个聚类合并，树状图会将它们连接成一个图，连接的高度就是这些聚类之间的距离。我们将使用<code class="fe ng nh ni nj b">scipy</code>库中的<code class="fe ng nh ni nj b">dendogram</code>函数来绘制图表。</p><pre class="kg kh ki kj gt nl nj nm nn aw no bi"><span id="e92e" class="np mi iq nj b gy nq nr l ns nt"># Dendogram for Heirarchical Clustering<br/>import scipy.cluster.hierarchy as shc<br/>from matplotlib import pyplot<br/>pyplot.figure(figsize=(10, 7))  <br/>pyplot.title("Dendrograms")  <br/>dend = shc.dendrogram(shc.linkage(cluster_df, method='ward'))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/2174566ceebe381f715da616a035925a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*hvajB98_TCUcgLyGp9Jkxg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6:树状图(图片由作者提供)</p></figure><p id="9a46" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如图6所示，我们可以根据树状图的层次结构选择最佳的集群数量。正如其他聚类验证指标所强调的，4个聚类也可以被考虑用于聚集层次结构。</p><h1 id="d944" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">贝叶斯信息准则</h1><p id="a883" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">贝叶斯信息准则(BIC)评分是一种对使用最大似然估计框架的模型进行评分的方法。BIC统计数据的计算方法如下:</p><p id="0e1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mg"> BIC = (k*ln(n)) — (2ln(L)) </em></p><p id="3889" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mg">其中L是模型的似然函数的最大值，k是参数的数量，n是记录的数量</em></p><p id="5144" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BIC分数越低，模型越好。我们可以将BIC分数用于聚类的高斯混合建模方法。我们将在单独的博客中讨论这个模型的细节，但这里需要注意的关键是，在这个模型中，我们需要选择聚类的数量以及协方差的类型。我们尝试了参数的各种组合，并选择了具有最低BIC分数的模型。</p><pre class="kg kh ki kj gt nl nj nm nn aw no bi"><span id="9918" class="np mi iq nj b gy nq nr l ns nt"># BIC for GMM</span><span id="e550" class="np mi iq nj b gy nu nr l ns nt">from sklearn.mixture import GaussianMixture<br/>n_components = range(1, 30)<br/>covariance_type = ['spherical', 'tied', 'diag', 'full']<br/>score=[]<br/>for cov in covariance_type:<br/>    for n_comp in n_components:<br/>        gmm=GaussianMixture(n_components=n_comp,covariance_type=cov)<br/>        gmm.fit(cluster_df)<br/>        score.append((cov,n_comp,gmm.bic(cluster_df)))<br/>score</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/602dbde3c01a0d2975b8e95576ecd0c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*HZEQdmzbO4yo74ImrPI05A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">BIC评分(图片由作者提供)</p></figure><p id="6498" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong></p><p id="1cf7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们学习了7种不同的方法来为不同的聚类算法选择最佳的聚类数。</p><p id="534d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">具体来说，我们了解到:</p><ul class=""><li id="807f" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nk ly lz ma bi translated">如何计算用于选择最佳集群数量的各种指标</li><li id="5844" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nk ly lz ma bi translated">使用这些指标选择最佳集群数量的经验法则</li><li id="3cfd" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nk ly lz ma bi translated">如何在python中实现集群验证方法</li><li id="b89b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nk ly lz ma bi translated">如何解释这些方法的结果</li></ul><p id="0c40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，给定用于选择最佳集群数量的多个指标，我们可以将各种指标的平均值/中值/众数作为最佳集群数量。</p><p id="3c6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你对这个博客有什么问题或建议吗？请随时留言。</p><h1 id="3c31" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">感谢您的阅读！</h1><p id="a69f" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">如果你和我一样，对人工智能、数据科学或经济学充满热情，请随时添加/关注我的<a class="ae kv" href="http://www.linkedin.com/in/indraneel-dutta-baruah-ds" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae kv" href="https://github.com/IDB-FOR-DATASCIENCE" rel="noopener ugc nofollow" target="_blank"> Github </a>和<a class="ae kv" href="https://medium.com/@indraneeldb1993ds" rel="noopener"> Medium </a>。</p><h1 id="840f" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">参考</h1><ol class=""><li id="77f6" class="ls lt iq ky b kz mz lc na lf oe lj of ln og lr lx ly lz ma bi translated">Tibshirani R，Walther G和Hastie T，通过间隙统计估计数据集中的聚类数，统计学家杂志。社会主义者B (2001) 63，第二部分，第411-423页</li><li id="cb7b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">袁C，杨H，K-Means聚类算法的K值选取方法研究，，2019 . 6 . 18</li><li id="96ea" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">Kaoungku和Suksut，k .和Chanklan，r .和Kerdprasop，k .和Kerdprasop，Nittaya。(2018).用于聚类和关联挖掘选择图像特征的轮廓宽度准则。国际机器学习和计算杂志。8.69–73.10.18178/ijmlc.2018.8.1.665</li><li id="4470" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">王X &amp;徐Y，基于剪影指数和Calinski-Harabasz指数的聚类验证改进指数，2019 IOP Conf。爵士。:板牙。Sci。英语。569 052024</li><li id="b666" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">佩特罗维奇S,“剪影指数”和“戴维斯-波尔丁指数”在标记IDS聚类中的比较</li></ol></div></div>    
</body>
</html>