<html>
<head>
<title>Understanding Markov Decision Process: The Framework Behind Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解马尔可夫决策过程:强化学习背后的框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-markov-decision-process-the-framework-behind-reinforcement-learning-4b5166f3c5b4?source=collection_archive---------40-----------------------#2020-10-28">https://towardsdatascience.com/understanding-markov-decision-process-the-framework-behind-reinforcement-learning-4b5166f3c5b4?source=collection_archive---------40-----------------------#2020-10-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1566" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解强化学习中马尔可夫决策过程的概念</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f43c4ff562b57f91e6d2af84a89af64c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uuXWQmRlWn5Apqmj"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Alexander Schimmeck 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="bfdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过AWS和Jakarta机器学习，以有趣的方式继续我们了解强化学习的旅程。我们已经在第一篇文章中讨论了AWS Deep Racer和强化学习(RL)的基础知识。对于没有读过这篇文章或者想重读这篇文章的你，不要着急。您可以在这里再次找到它:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/aws-deepracer-the-fun-way-of-learning-reinforcement-learning-c961cde9ce8b"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">AWS DeepRacer:强化学习的有趣方式</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">欢迎来到AWS DeepRacer之旅</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="0c5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，在这篇文章中，我们将讨论如何制定RL问题。要做到这一点，我们需要理解马尔可夫决策过程或众所周知的MDP。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/2e53eff74a5e5b2d73bdc5ab243ccc77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q2aGqZkrl78pm02PR-Xo0g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">典型的强化学习过程。作者图片</p></figure><p id="b3bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MDP是一个框架，可以用来制定RL问题的数学。几乎所有的RL问题都可以建模为具有状态、动作、转移概率和回报函数的MDP。</p><p id="9d14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，我们为什么需要关心MDP呢？因为有了MDP，一个代理人可以得到一个最优的策略，随着时间的推移得到最大的回报，我们将得到最优的结果。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="4a83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，我们开始吧。为了更好地了解MDP，我们需要先了解MDP的组成部分。</p><h1 id="a128" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">马尔可夫性质</h1><blockquote class="nn"><p id="9524" class="no np it bd nq nr ns nt nu nv nw lu dk translated">未来只取决于<em class="nx">现在</em>而不是过去。</p></blockquote><p id="0ded" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li oa lk ll lm ob lo lp lq oc ls lt lu im bi translated">这句话概括了马尔可夫性质的原理。另一方面，术语“马尔可夫性质”是指概率论和统计学中随机(或随机确定)过程的无记忆性质。</p><p id="e252" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举个例子，假设你拥有一家餐馆，管理原材料库存。您每周检查库存，并使用结果来订购下周的原材料。这个条件意味着你只考虑本周的库存来预测下周的需求，而不考虑上周的库存水平。</p><h1 id="8196" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">马尔可夫链</h1><p id="08ea" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">马尔可夫链由遵循马尔可夫性质的状态序列组成。这个马尔可夫链实际上是一个概率模型，它依赖于当前状态来预测下一个状态。</p><p id="fe13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解马尔可夫性质和马尔可夫链，我们将以天气预报为例。如果当前状态是多云，那么下一个状态可能是下雨或刮风。在第一个状态到下一个状态的中间，有一个概率我们称之为跃迁概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/fca746ffa50ef8de89a6734df8632021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qHTBVXS9HLNqwSWeUezYMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">天气链。作者图片</p></figure><p id="f700" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图来看，当前状态是阴天的时候，下一个状态70%会下雨，或者30%会刮风。如果当前状态是多风的，那么下一个状态有100%的可能性是多雨的。那么当状态是阴雨的时候，有80%的概率下一个状态会保持阴雨，有20%会变成多云。我们可以将这些状态和转移概率显示为如下表格或矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/d5b9d6fa817415661038e71b2870c05c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BmRZ_pBg3-LBN3Y-uC22zw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">转移概率表和矩阵</p></figure><p id="a5ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为总结，我们可以说马尔可夫链由一组状态及其转移概率组成。</p><h1 id="a622" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">马尔可夫奖励过程</h1><p id="fc1c" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">马尔可夫奖励过程(MRP)是马尔可夫链的扩展，增加了一个奖励函数。所以，它由状态，转移概率和奖励函数组成。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/4b6cd6f17e25446ef8b2411cf6a45d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GUvOCVVk83YX5_wQ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">杰里米·蔡在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1cb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个奖励函数给出了我们从每个状态得到的奖励。这个函数将告诉我们在阴天状态下我们获得的奖励，在刮风和下雨状态下我们获得的奖励。这种奖励也可以是正值或负值。</p><h1 id="428e" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">马尔可夫决策过程</h1><p id="a765" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">至此，我们已经看到了关于马尔可夫性质、马尔可夫链和马尔可夫报酬过程。这些成为马尔可夫决策过程的基础(MDP)。在马尔可夫决策过程中，我们有行动作为马尔可夫奖励过程的补充。</p><p id="e732" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来描述一下这个MDP，他是一个矿工，他想在格子迷宫中得到一颗钻石。在这种情况下，矿工可以在网格内移动以获取钻石。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/844e759026b1cdf331f4fd6614193c47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*t1y2Suk3Ntb5kLk6vonRXA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">钻石猎人迷宫。作者图片</p></figure><p id="cdb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，我们可以描述MDP包括:</p><ul class=""><li id="9bd5" class="om on it lb b lc ld lf lg li oo lm op lq oq lu or os ot ou bi translated">一组状态<strong class="lb iu"> s ∈ S </strong>。这些状态代表了世界上所有可能的构型。在这里，我们可以将状态定义为机器人可以移动到的网格。</li><li id="3bd1" class="om on it lb b lc ov lf ow li ox lm oy lq oz lu or os ot ou bi translated">一个转移函数<strong class="lb iu"> T(s，a，s’)</strong>。这代表了MDP的不确定性。给定当前位置，转移函数可以是向下移动80%，向右移动5%，向上移动15%。</li><li id="a841" class="om on it lb b lc ov lf ow li ox lm oy lq oz lu or os ot ou bi translated">一个奖励函数<strong class="lb iu"> R(s，a，s’)</strong>。这个函数显示了每一步获得了多少奖励。代理人的目标是最大化奖励的总和。一般来说，在每一步都会有一个小的负面奖励来鼓励快速解决问题，而在实现目标时会有大的奖励，或者在终止状态或进入像僵尸和火这样的障碍时会有大的负面奖励。</li><li id="4e26" class="om on it lb b lc ov lf ow li ox lm oy lq oz lu or os ot ou bi translated">一组动作<strong class="lb iu"> a ∈ A </strong>。代理可以采取的所有可能操作的集合。在这个例子中，动作是向上、向下、向左和向右。</li></ul><p id="c4c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在MDP，我们还需要理解几个术语。</p><h2 id="00e6" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">行为空间</h2><p id="3d36" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">行动空间是一个行动者为达到目标可以采取的行动。在我们的miner示例中，动作空间向上、向下、向左和向右移动。这种行动空间可以分为两种类型。第一个是离散动作空间，第二个是连续动作空间。</p><h2 id="269c" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">政策</h2><p id="6368" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">在MDP，代理行为被定义为一种策略。该策略告诉代理在每种状态下要执行的操作。开始时，我们需要初始化一个随机策略。然后，代理将继续学习，最优策略从迭代中产生，其中代理在每个状态中执行良好的动作，这使得累积奖励最大化。</p><p id="0cbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该策略可分为两种类型:</p><ul class=""><li id="6708" class="om on it lb b lc ld lf lg li oo lm op lq oq lu or os ot ou bi translated"><strong class="lb iu">确定性策略</strong>。给定特定的状态和时间，策略告诉代理执行一个特定的操作。</li><li id="a182" class="om on it lb b lc ov lf ow li ox lm oy lq oz lu or os ot ou bi translated"><strong class="lb iu">随机政策</strong>。这并不直接针对某个特定的动作。相反，它将状态映射到动作空间上的概率分布。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/75fd6f6c2436904d71d9b710ad1152eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K1BGNiXEAE06QTBTzcnzHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">确定性和随机性策略的例子。作者图片</p></figure><h2 id="d138" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">插曲</h2><p id="de73" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">一个情节是代理从初始状态到最终状态或终止状态所采取的动作。例如，从当前状态进入最终获得钻石的旅程。矿工运动是D &gt; A &gt; B &gt; C &gt; F &gt; I。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/09442250e12603b05c6f8ea13f6e22bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*nBROlKtCFq5ElbVIQM3syA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">插曲插图。作者图片</p></figure><h2 id="853d" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">返回</h2><p id="0995" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">在强化学习中，我们的目标是<strong class="lb iu">最大化一集的累积回报</strong>。这个奖励是代理收到的奖励的总和，而不是代理从当前状态收到的奖励(即时奖励)。这种累积奖励也称为<strong class="lb iu">回报</strong>。</p><p id="3cc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代理可以通过在每个状态下执行正确的动作来最大化回报。它将在最优策略的指导下实现这一正确的动作。</p><h2 id="73c9" class="pa mw it bd mx pb pc dn nb pd pe dp nf li pf pg nh lm ph pi nj lq pj pk nl pl bi translated">贴现因素</h2><p id="acec" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">在奖励最大化的过程中，我们需要考虑当下和未来奖励的重要性。因此，<strong class="lb iu">贴现因子</strong>开始起作用。这个贴现因子决定了我们对未来奖励和眼前奖励的重视程度。</p><p id="7363" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">折扣因子的值范围从0到1。较小的值(接近0)更重视眼前的回报，而不是未来的回报。另一方面，高价值(接近1)给予未来回报比眼前回报更大的重要性。</p><p id="db3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在比赛中，我们的主要目标是完成一圈。那么我们需要给予未来的回报比眼前的回报更重要。所以，我们需要使用一个接近1的折现因子。</p><h1 id="a28a" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">结论</h1><p id="c2e5" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">恭喜你！！</p><p id="782c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">至此，我们已经涵盖了什么是马尔可夫性质、马尔可夫链、马尔可夫报酬过程和马尔可夫决策过程。这包括MDP的重要术语，如行动空间、政策、情节、回报和折扣系数。</p><p id="90a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，强化学习可以被表示为具有状态、动作、转移概率和奖励函数的MDP。</p><h1 id="2b2f" class="mv mw it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">文献学</h1><div class="lv lw gp gr lx ly"><a href="https://inst.eecs.berkeley.edu/~cs188/sp20/" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">CS 188:人工智能导论，2020年春季</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">本课程将介绍智能计算机系统设计的基本思想和技术。一个…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">inst.eecs.berkeley.edu</p></div></div><div class="mh l"><div class="po l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">强化学习导论:马尔可夫决策过程</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">在一个典型的强化学习(RL)问题中，有一个学习者和一个被称为代理的决策者</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="pp l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a href="https://en.wikipedia.org/wiki/Markov_property" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">马尔可夫性质</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">在概率论和统计学中，术语马尔可夫性是指随机变量的无记忆性。</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">en.wikipedia.org</p></div></div><div class="mh l"><div class="pq l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a href="https://en.wikipedia.org/wiki/Markov_chain" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">马尔可夫链</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">马尔可夫链是描述一系列可能事件的随机模型，其中每个事件的概率…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">en.wikipedia.org。</p></div></div><div class="mh l"><div class="pr l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">马尔可夫决策过程</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">在数学中，马尔可夫决策过程(MDP)是离散时间随机控制过程。它提供了一个…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">en.wikipedia.org</p></div></div><div class="mh l"><div class="ps l mj mk ml mh mm ks ly"/></div></div></a></div></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="1982" class="mv mw it bd mx my pt na nb nc pu ne nf jz pv ka nh kc pw kd nj kf px kg nl nm bi translated">关于作者</h1><p id="03d8" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">Bima是一名数据科学家，他总是渴望扩展自己的知识和技能。他毕业于万隆技术学院和新南威尔士大学，分别是采矿工程师。然后他通过HardvardX、IBM、Udacity等的各种在线课程开始了他的数据科学之旅。目前，他正与DANA Indonesia一起在印度尼西亚建立一个无现金社会。</p><p id="15de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有任何疑问或需要讨论的话题，请通过<a class="ae ky" href="https://www.linkedin.com/in/bpratama/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系Bima。</p></div></div>    
</body>
</html>