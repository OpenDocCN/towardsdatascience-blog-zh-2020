<html>
<head>
<title>Paper Summary &amp; Implementation: Contrastive Learning of General-Purpose Audio Representations.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文摘要与实施:通用音频表征的对比学习。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-summary-implementation-contrastive-learning-of-general-purpose-audio-representations-f4e3cc06fcf7?source=collection_archive---------25-----------------------#2020-10-31">https://towardsdatascience.com/paper-summary-implementation-contrastive-learning-of-general-purpose-audio-representations-f4e3cc06fcf7?source=collection_archive---------25-----------------------#2020-10-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5c99" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">PyTorch实施的COLA培训计划。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/03eb20422217af139b5c8056d04e998c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XeBtgO9B4iZYjNvwqo5HuA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">赫拉尔·莫夫西斯扬在<a class="ae kv" href="https://unsplash.com/s/photos/audio?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="cf8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章是一个简短的总结和实施以下文章的步骤:</p><ul class=""><li id="27b9" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/abs/2010.10915" rel="noopener ugc nofollow" target="_blank">通用音频表示的学习</a></li></ul><p id="8e8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文的目的是使用区分性预训练来学习自我监督的通用音频表示。作者训练了一个2D CNN EfficientNet-B0来将梅尔频谱图转换成1D-512向量。这些表征然后被转移到其他任务，如说话人识别或鸟鸣检测。</p><p id="7132" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DPT背后的基本思想是定义一个锚元素、一个积极元素和一个或多个干扰元素。然后训练一个模型来匹配锚和正面的例子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/54726d0da94deda2996cf6b66a5686a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*BsHigYU_qjPpQvq1k09k1A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">DPT —作者提供的图像</p></figure><p id="f34c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用DPT的一种这样的方式是使用三元组损失以及余弦相似性度量来训练模型，例如余弦(F(P)，F(A))比余弦(F(D)，F(A))高得多。这将使锚的潜在空间中的表征更接近于积极的例子，而不是干扰物。上面链接的论文的作者使用这种方法作为基线来表明他们的方法<strong class="ky ir"> COLA </strong>效果更好。</p><h2 id="830c" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">可乐</h2><p id="b136" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">这种方法适用于音频领域。对于每个音频剪辑，作者挑选一个片段作为锚，另一个片段作为正例，对于这些样本中的每一个(锚，正)，他们挑选训练批次中的其他样本作为干扰物。这是一个好主意，原因有二:</p><ul class=""><li id="c5b5" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">有多个干扰物，这使得训练任务更加困难，迫使模型学习更有意义的表示来解决它。</li><li id="1699" class="ls lt iq ky b kz na lc nb lf nc lj nd ln ne lr lx ly lz ma bi translated">干扰项从批次中的其他样本中重用，这降低了生成它们的IO、计算和内存成本。</li></ul><p id="9a63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">COLA还使用双线性相似度，这是直接从数据中学习的。作者表明双线性相似性比余弦好得多，相比之下，在下游任务上给出了额外的<strong class="ky ir"> 7% </strong>平均准确度。</p><p id="50d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在计算锚和其他例子之间的相似性之后，相似性值被用于交叉熵损失，该损失测量模型在干扰物中识别正面例子的能力(论文中的等式2)。</p><h2 id="6134" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">可乐评价</h2><p id="030b" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated"><strong class="ky ir">线性模型评估</strong></p><p id="7854" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">COLA用于在AudioSet上训练EfficientNet-B0，AudioSet是一个数据集，包含来自YouTube的大约100万个音频剪辑。由该模型生成的特征向量然后被用于在广泛的下游任务上训练线性分类器。模型学习的表示越好，当用作执行监督任务的线性模型的输入时，其性能就越好。作者发现，在下游任务中，COLA比其他方法(如三重损失)多了20%的平均准确率(论文的表2)</p><p id="3e4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">微调评估</strong></p><p id="f98c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">测试这种方法的另一种方法是在下游任务中微调模型。这使得作者可以将使用COLA预先训练的模型与从头开始训练的模型进行比较。他们的结果显示，预训练的模型比从零开始训练的模型平均高出大约<strong class="ky ir"> 1.2% </strong>。</p><h2 id="3a03" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">PyTorch实现</h2><p id="775b" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">这种方法很容易在PyTorch Lightning中实现。<br/>编码器可以定义为:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="d4d1" class="mc md iq ng b gy nk nl l nm nn"><strong class="ng ir">class </strong>Encoder(torch.nn.Module):<br/>    <strong class="ng ir">def </strong>__init__(self, drop_connect_rate=0.1):<br/>        super(Encoder, self).__init__()<br/><br/>        self.cnn1 = torch.nn.Conv2d(1, 3, kernel_size=3)<br/>        self.efficientnet = EfficientNet.from_name(<br/>            <strong class="ng ir">"efficientnet-b0"</strong>, include_top=<strong class="ng ir">False</strong>, drop_connect_rate=drop_connect_rate<br/>        )<br/><br/>    <strong class="ng ir">def </strong>forward(self, x):<br/>        x = x.unsqueeze(1)<br/><br/>        x = self.cnn1(x)<br/>        x = self.efficientnet(x)<br/><br/>        y = x.squeeze(3).squeeze(2)<br/><br/>        <strong class="ng ir">return </strong>y</span></pre><p id="32fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后可乐训练可以定义为:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="cb0e" class="mc md iq ng b gy nk nl l nm nn"><strong class="ng ir">class </strong>Cola(pl.LightningModule):<br/>    <strong class="ng ir">def </strong>__init__(self, p=0.1):<br/>        super().__init__()<br/>        self.save_hyperparameters()<br/><br/>        self.p = p<br/><br/>        self.do = torch.nn.Dropout(p=self.p)<br/><br/>        self.encoder = Encoder(drop_connect_rate=p)<br/><br/>        self.g = torch.nn.Linear(1280, 512)<br/>        self.layer_norm = torch.nn.LayerNorm(normalized_shape=512)<br/>        self.linear = torch.nn.Linear(512, 512, bias=<strong class="ng ir">False</strong>)<br/><br/>    <strong class="ng ir">def </strong>forward(self, x):<br/>        x1, x2 = x<br/><br/>        x1 = self.do(self.encoder(x1))<br/>        x1 = self.do(self.g(x1))<br/>        x1 = self.do(torch.tanh(self.layer_norm(x1)))<br/><br/>        x2 = self.do(self.encoder(x2))<br/>        x2 = self.do(self.g(x2))<br/>        x2 = self.do(torch.tanh(self.layer_norm(x2)))<br/><br/>        x1 = self.linear(x1)<br/><br/>        <strong class="ng ir">return </strong>x1, x2<br/><br/>    <strong class="ng ir">def </strong>training_step(self, x, batch_idx):<br/>        x1, x2 = self(x)<br/><br/>        y = torch.arange(x1.size(0), device=x1.device)<br/><br/>        y_hat = torch.mm(x1, x2.t())<br/><br/>        loss = F.cross_entropy(y_hat, y)<br/><br/>        _, predicted = torch.max(y_hat, 1)<br/>        acc = (predicted == y).double().mean()<br/><br/>        self.log(<strong class="ng ir">"train_loss"</strong>, loss)<br/>        self.log(<strong class="ng ir">"train_acc"</strong>, acc)<br/><br/>        <strong class="ng ir">return </strong>loss</span></pre><p id="78a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我没有计算资源来复制论文中的实验，所以我试着在小得多的规模上做类似的事情。我在<a class="ae kv" href="https://github.com/mdeff/fma" rel="noopener ugc nofollow" target="_blank"> FMA大号</a>(没有标签)上使用可乐预训练了一个模型几个时代，然后微调了应用于FMA小号的音乐流派检测。</p><p id="05eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">FMA·斯莫尔的结果如下:</p><ul class=""><li id="3951" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">随机猜测:12.5%</li><li id="82db" class="ls lt iq ky b kz na lc nb lf nc lj nd ln ne lr lx ly lz ma bi translated">从零开始培训:51.1%</li><li id="34c2" class="ls lt iq ky b kz na lc nb lf nc lj nd ln ne lr lx ly lz ma bi translated">使用可乐预先训练:<strong class="ky ir"> 54.3% </strong></li></ul><h2 id="78d5" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">结论</h2><p id="a240" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">论文<a class="ae kv" href="https://arxiv.org/abs/2010.10915" rel="noopener ugc nofollow" target="_blank">通用音频表示的学习</a>介绍了COLA预训练方法，该方法实现了一些伟大的想法，使自我监督训练更加有效，如使用批量样本作为干扰物和双线性相似性度量。这种方法可用于提高下游监督音频任务的性能。</p><p id="47ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代码:<a class="ae kv" href="https://github.com/CVxTz/COLA_pytorch" rel="noopener ugc nofollow" target="_blank">https://github.com/CVxTz/COLA_pytorch</a></p></div></div>    
</body>
</html>