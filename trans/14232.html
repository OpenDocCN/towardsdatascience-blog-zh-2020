<html>
<head>
<title>Hyperparameter Tuning to Reduce Overfitting — LightGBM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调整减少过度拟合— LightGBM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-to-reduce-overfitting-lightgbm-5eb81a0b464e?source=collection_archive---------7-----------------------#2020-10-01">https://towardsdatascience.com/hyperparameter-tuning-to-reduce-overfitting-lightgbm-5eb81a0b464e?source=collection_archive---------7-----------------------#2020-10-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9b41" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用例子演示</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ebf158d967f60b3c9d9bf205108e7e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SmWJPvCF1Y5OMz_Wvxe5Vg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安德烈斯·达利蒙提在<a class="ae ky" href="https://unsplash.com/s/photos/cockpit?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="d85d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对大量数据的轻松访问和高计算能力使得设计复杂的机器学习算法成为可能。随着模型复杂性的增加，训练模型所需的数据量也会增加。</p><p id="0e11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据不是模型性能的唯一因素。复杂模型有许多超参数，需要正确地调整或调整，以便充分利用它们。</p><p id="288e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，XGBoost和LightGBM的性能高度依赖于超参数调优。这就像以每小时50英里的速度驾驶法拉利，在不仔细调整超参数的情况下实现这些算法。</p><p id="62c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本帖中，我们将实验LightGBM的性能如何根据超参数值而变化。重点是有助于推广模型的参数，从而降低过度拟合的风险。</p><p id="8804" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从导入库开始。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="3299" class="ma mb it lw b gy mc md l me mf">import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>import lightgbm as lgb</span></pre><p id="f5b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集包含60 k个观测值、99个数字特征和一个目标变量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/58028a7a522de6002101b34a42854c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*u07h-mSFCI2ZFzcruhzYgw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="3c11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目标变量包含9个值，这使其成为多类分类任务。</p><p id="05b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的重点是超参数调优，因此我们将跳过数据争论部分。下面的代码块将数据集分为训练和测试子集，并将它们转换为适合LightGBM的格式。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="e96c" class="ma mb it lw b gy mc md l me mf">X = df.drop('target', axis=1)<br/>y = df['target']</span><span id="df33" class="ma mb it lw b gy mh md l me mf">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=42)</span><span id="c5b7" class="ma mb it lw b gy mh md l me mf">lgb_train = lgb.Dataset(X_train, y_train)<br/>lgb_test = lgb.Dataset(X_test, y_test)</span></pre><p id="bae7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将从一组新的基本超参数开始，并逐步引入新的超参数。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="40bf" class="ma mb it lw b gy mc md l me mf">params = {<br/>'boosting_type': 'gbdt',<br/>'objective': 'multiclass',<br/>'metric': 'multi_logloss',<br/>'num_class':9<br/>}</span></pre><p id="1175" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以训练模型，并根据指定的评估指标查看结果。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d10c" class="ma mb it lw b gy mc md l me mf">gbm = lgb.train(<br/>params,<br/>lgb_train,<br/>num_boost_round=500,<br/>valid_sets=[lgb_train, lgb_test],<br/>early_stopping_rounds=10<br/>)</span></pre><p id="8f21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">评价标准是多级测井曲线损失。这是训练集和验证集的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/2dbde0ee3d7400206fb5efef5332191d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*3uDqWqxmCQdF7Wg_2rQNPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="26df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">助推轮数设置为500，但出现了提前停止。如果在指定的回合数内成绩没有提高，early_stopping_rounds将停止训练。</p><p id="4d5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为在训练集和验证集的损失之间存在显著的差异，所以该模型似乎高度过度适合训练集。</p><p id="42a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">min_data_in_leaf参数是一种减少过度拟合的方法。它要求每片叶子都有指定数量的观察值，这样模型才不会变得太具体。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="e2be" class="ma mb it lw b gy mc md l me mf">'min_data_in_leaf':300 #added to params dict</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/c09f38088e196a8a3c983de13ef5c31d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*F_yL65gqySWFuYIzTUyEcQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="ef14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">验证损失几乎相同，但差异变小，这意味着过度拟合的程度降低。</p><p id="22c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">防止模型过于具体的另一个参数是<strong class="lb iu"> feature_fraction </strong>，它表示在每次迭代中随机选择的特征的比率。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="1689" class="ma mb it lw b gy mc md l me mf">'feature_fraction':0.8 #added to params dict</span></pre><p id="5439" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在模型在每次迭代中使用80%的特性。这是结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/b9f323e45fb52251d674c7672f9fb796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*3m0kvAVAKGNWs0v_KujQgw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="145a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">过度拟合进一步减少。</p><p id="9524" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Bagging_fraction允许在每次迭代中使用随机选择的行样本。它类似于feature_fraction，但用于行。bagging_freq指定更新所选行的迭代频率。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="027a" class="ma mb it lw b gy mc md l me mf">#added to params dict<br/>'bagging_fraction':0.8,<br/>'bagging_freq':10</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/49abbdafe2bfd40da768fe0856118a3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*p8YY-VJA7ZgIubrn3CY0oA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="87d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练损失和验证损失之间的差异正在减小，这表明我们在正确的轨道上。</p><p id="e8fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LightGBM是一种使用boosting技术组合决策树的集成方法。单个树的复杂性也是过度拟合的决定因素。它可以通过max_depth和num_leaves参数来控制。max_depth决定了一棵树的最大深度，而num_leaves限制了一棵树可以拥有的最大叶子数。因为LightGBM适应逐叶的树生长，所以一起调整这两个参数很重要。</p><p id="38bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个重要的参数是learning_rate。较小的学习速率通常更好，但它会导致模型学习更慢。</p><p id="ba1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以添加一个正则项作为超参数。LightGBM支持L1和L2正则化。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="cefe" class="ma mb it lw b gy mc md l me mf">#added to params dict<br/>'max_depth':8,<br/>'num_leaves':70,<br/>'learning_rate':0.04</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/89d70bca720f3d3c8a4daf13b415727d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*lEpEkHN-kRE8qIduQ4qhyQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="894a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们进一步降低了训练和验证损失之间的差异，这意味着更少的过度拟合。</p><p id="498c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">迭代次数也是模型训练的一个重要因素。更多的迭代导致模型学习更多，因此模型在一定数量的迭代之后开始过度拟合。</p><p id="d97b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能需要花费大量时间来调优超参数。最终，你会创造出你自己的方法或策略来加速调整的过程。</p><p id="2c90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有很多超参数。有些在准确性和速度方面更重要。其中一些主要用于防止过度拟合。</p><p id="c4eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">交叉验证也可以用来减少过度拟合。它允许在训练集和验证集中使用每个数据点。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="bc6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们只关注减少过度拟合。然而，如果精度或损失不令人满意，消除过拟合没有多大关系。您还可以调整超参数以在一定程度上提高精度。提高模型性能的一些方法有:</p><ul class=""><li id="63bb" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">特征工程</li><li id="f53a" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">特征抽出</li><li id="ed3a" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">集成多个模型</li></ul><p id="99af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>