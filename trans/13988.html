<html>
<head>
<title>Training Better Deep Learning Models for Structured Data using Semi-supervised Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用半监督学习为结构化数据训练更好的深度学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-better-deep-learning-models-for-structured-data-using-semi-supervised-learning-8acc3b536319?source=collection_archive---------16-----------------------#2020-09-26">https://towardsdatascience.com/training-better-deep-learning-models-for-structured-data-using-semi-supervised-learning-8acc3b536319?source=collection_archive---------16-----------------------#2020-09-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4366" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用未标记的样本来提高神经网络的性能。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/efc8f839ac35ff484cf63dfa9cd827e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5dX6O-2dcV7pk22T39jlg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@dtopkin1?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">戴恩·托普金</a>在<a class="ae kv" href="https://unsplash.com/s/photos/structure?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="aba3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">众所周知，深度学习在应用于文本、音频或图像等非结构化数据时效果很好，但在应用于结构化或表格数据时，有时会落后于梯度增强等其他机器学习方法。<br/>在这篇文章中，我们将使用半监督学习来提高深度神经模型在低数据区应用于结构化数据时的性能。我们将展示，通过使用无监督的预训练，我们可以使神经模型的性能优于梯度推进。</p><p id="fa5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章基于两篇论文:</p><ul class=""><li id="a466" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1810.11921.pdf" rel="noopener ugc nofollow" target="_blank"> AutoInt:通过自关注神经网络的自动特征交互学习</a></li><li id="749c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1908.07442.pdf" rel="noopener ugc nofollow" target="_blank"> TabNet:专注的可解释表格学习</a></li></ul><p id="4acc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们实现了一个深度神经架构，类似于AutoInt论文中介绍的内容，我们使用多头自我关注和特征嵌入。预训练部分摘自Tabnet论文。</p><h1 id="759f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">方法描述:</h1><p id="c4a7" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们将研究结构化数据，即可以写成包含列(数字、分类、序数)和行的表格的数据。我们还假设我们有大量的未标记样本可以用于预训练，少量的标记样本可以用于监督学习。在接下来的实验中，我们将模拟这个设置来绘制学习曲线，并在使用不同大小的标记集时评估该方法。</p><h2 id="87ee" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">数据准备:</h2><p id="cf60" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">让我们用一个例子来描述我们如何在将数据输入神经网络之前准备数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/0b91da755f8dd9411717848edc38beb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*Oh0p8rDfHvFhtAdIlZaUeg.png"/></div></figure><p id="aa7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本例中，我们有三个样本和三个特征{ <strong class="ky ir"> F1 </strong>、<strong class="ky ir"> F2 </strong>、<strong class="ky ir"> F3 </strong> }，以及一个目标。<strong class="ky ir"> F1 </strong>为分类特征，而<strong class="ky ir"> F2 </strong>和<strong class="ky ir"> F3 </strong>为数值特征。<br/>我们将为<strong class="ky ir"> F1 </strong>的每个模态<strong class="ky ir"> X </strong>创建一个新特征<strong class="ky ir"> F1_X </strong>，如果<strong class="ky ir"> F1 </strong> == <strong class="ky ir"> X </strong>为1，否则为0。</p><p id="62a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">转换后的样本将被写入一组(<strong class="ky ir">特征名</strong>，<strong class="ky ir">特征值</strong>)。</p><p id="cfb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如:<br/>第一个样本- &gt; {( <strong class="ky ir"> F1_A </strong>，1)、(<strong class="ky ir"> F2 </strong>，0.3)、(<strong class="ky ir"> F3 【T10，1.3)} <br/>第二个样本- &gt; {( <strong class="ky ir"> F1_B </strong>，1)、(<strong class="ky ir"> F2 </strong>，0.4)、(<strong class="ky ir"> F3 </strong>，0.9)} <br/>第三个样本- &gt;</strong></p><p id="f64e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特征名称将被送入嵌入层，然后与特征值相乘。</p><h2 id="8092" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">型号:</h2><p id="29d2" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">这里使用的模型是一系列多头注意力块和逐点前馈层。在训练时，我们也使用注意力集中跳过连接。多头注意力块允许我们对特征之间可能存在的交互进行建模，而注意力集中跳过连接允许我们从特征嵌入集合中获得单个向量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/6e9bde3e4dbc52638b8d1a8ef2129ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*JjiIj2DHgh-787hdLYA8VA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">简化模型-作者图片</p></figure><p id="7080" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">预训练:</strong></p><p id="6c1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在预训练步骤中，我们使用完整的未标记数据集，我们输入特征的损坏版本，并训练模型来预测未损坏的特征，类似于在去噪自动编码器中所做的。</p><p id="ddd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">监督训练:</strong></p><p id="3bf9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练的监督部分，我们在编码器部分和输出之间添加跳过连接，并尝试预测目标。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/febf23cfd7c324ae3f7764cf5ea0d26b.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*oOKzy5ZPIj4G--VISPGpBA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">简化模型-作者图片</p></figure><h2 id="0854" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">实验:</h2><p id="1960" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在下面的实验中，我们将使用四个数据集，两个用于回归，两个用于分类。</p><ul class=""><li id="384e" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">Sarco :约有50k个样本，21个特征，7个连续目标。</li><li id="d4d9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity" rel="noopener ugc nofollow" target="_blank">在线新闻</a>:约有40k个样本，61个特征，1个连续目标。</li><li id="e31e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://www.kaggle.com/uciml/adult-census-income/" rel="noopener ugc nofollow" target="_blank">成人普查</a>:约有40k个样本，15个特征，1个二元目标。</li><li id="ed03" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://www.kaggle.com/uciml/forest-cover-type-dataset" rel="noopener ugc nofollow" target="_blank">森林覆盖</a>:约有50万个样本，54个特征，1个分类目标。</li></ul><p id="9af4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将比较预先训练的神经模型和从零开始训练的神经模型，我们将关注低数据状态下的性能，这意味着几百到几千个标记样本。我们还将与一种流行的梯度增强实现进行比较，这种实现叫做<a class="ae kv" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> lightgbm </a>。</p><h2 id="2edb" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">森林覆盖:</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/1fad0093a9557a20ed191bf4052a0386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*P3XBmgB3slOkZzSZgcZbwg.png"/></div></figure><h2 id="ff07" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">成人人口普查:</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/9fe8ff007c25a5d11bcf43050858149a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*mHfRax4OuYDL8Zf7wLAQWg.png"/></div></figure><p id="95ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个数据集，我们可以看到，如果训练集小于2000，预训练是非常有效的。</p><h2 id="8a5a" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">在线新闻:</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/edc6e2074072d1359c7bdd7a168fa1af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*XOCQ1Ja6hKfCgfubX9h4zw.png"/></div></figure><p id="4ecf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于在线新闻数据集，我们可以看到预训练神经网络非常有效，甚至超过了所有样本大小500或更大的梯度提升。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e1083c4fe6202f84ac5f47e3d1b1b230.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*bGE6mcTkDyhlEDjCqnujsw.png"/></div></figure><p id="dca3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于Sarco数据集，我们可以看到预训练神经网络非常有效，甚至超过了所有样本大小的梯度提升。</p><h2 id="b265" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">附注:重现结果的代码</h2><p id="8c72" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">重现结果的代码可从这里获得:<a class="ae kv" href="https://github.com/CVxTz/DeepTabular" rel="noopener ugc nofollow" target="_blank">https://github.com/CVxTz/DeepTabular</a></p><p id="a767" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用它你可以很容易地训练一个分类或回归模型-&gt;</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="d9c8" class="nd mh iq nu b gy ny nz l oa ob"><strong class="nu ir">import </strong>pandas <strong class="nu ir">as </strong>pd<br/><strong class="nu ir">from </strong>sklearn.model_selection <strong class="nu ir">import </strong>train_test_split</span><span id="75f0" class="nd mh iq nu b gy oc nz l oa ob"><strong class="nu ir">from </strong>deeptabular.deeptabular <strong class="nu ir">import </strong>DeepTabularClassifier</span><span id="2a28" class="nd mh iq nu b gy oc nz l oa ob"><strong class="nu ir">if </strong>__name__ == <strong class="nu ir">"__main__"</strong>:<br/>    data = pd.read_csv(<strong class="nu ir">"../data/census/adult.csv"</strong>)</span><span id="59cf" class="nd mh iq nu b gy oc nz l oa ob">    train, test = train_test_split(data, test_size=0.2, random_state=1337)</span><span id="f1b7" class="nd mh iq nu b gy oc nz l oa ob">    target = <strong class="nu ir">"income"</strong></span><span id="6f4e" class="nd mh iq nu b gy oc nz l oa ob"><strong class="nu ir">    </strong>num_cols = [<strong class="nu ir">"age"</strong>, <strong class="nu ir">"fnlwgt"</strong>, <strong class="nu ir">"capital.gain"</strong>, <strong class="nu ir">"capital.loss"</strong>, <strong class="nu ir">"hours.per.week"</strong>]<br/>    cat_cols = [<br/>        <strong class="nu ir">"workclass"</strong>,<br/>        <strong class="nu ir">"education"</strong>,<br/>        <strong class="nu ir">"education.num"</strong>,<br/>        <strong class="nu ir">"marital.status"</strong>,<br/>        <strong class="nu ir">"occupation"</strong>,<br/>        <strong class="nu ir">"relationship"</strong>,<br/>        <strong class="nu ir">"race"</strong>,<br/>        <strong class="nu ir">"sex"</strong>,<br/>        <strong class="nu ir">"native.country"</strong>,<br/>    ]</span><span id="20ed" class="nd mh iq nu b gy oc nz l oa ob">    <strong class="nu ir">for </strong>k <strong class="nu ir">in </strong>num_cols:<br/>        mean = train[k].mean()<br/>        std = train[k].std()<br/>        train[k] = (train[k] - mean) / std<br/>        test[k] = (test[k] - mean) / std</span><span id="ac3e" class="nd mh iq nu b gy oc nz l oa ob">    train[target] = train[target].map({<strong class="nu ir">"&lt;=50K"</strong>: 0, <strong class="nu ir">"&gt;50K"</strong>: 1})<br/>    test[target] = test[target].map({<strong class="nu ir">"&lt;=50K"</strong>: 0, <strong class="nu ir">"&gt;50K"</strong>: 1})</span><span id="9f73" class="nd mh iq nu b gy oc nz l oa ob">    classifier = DeepTabularClassifier(<br/>        num_layers=10, cat_cols=cat_cols, num_cols=num_cols, n_targets=1,<br/>    )</span><span id="a0b5" class="nd mh iq nu b gy oc nz l oa ob">    classifier.fit(train, target_col=target, epochs=128)</span><span id="67c8" class="nd mh iq nu b gy oc nz l oa ob">    pred = classifier.predict(test)</span><span id="9f59" class="nd mh iq nu b gy oc nz l oa ob">    classifier.save_config(<strong class="nu ir">"census_config.json"</strong>)<br/>    classifier.save_weigts(<strong class="nu ir">"census_weights.h5"</strong>)</span><span id="4fcc" class="nd mh iq nu b gy oc nz l oa ob">    new_classifier = DeepTabularClassifier()</span><span id="4b69" class="nd mh iq nu b gy oc nz l oa ob">    new_classifier.load_config(<strong class="nu ir">"census_config.json"</strong>)<br/>    new_classifier.load_weights(<strong class="nu ir">"census_weights.h5"</strong>)</span><span id="b782" class="nd mh iq nu b gy oc nz l oa ob">    new_pred = new_classifier.predict(test)</span></pre><h1 id="b47e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论:</h1><p id="8f34" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">已知无监督预训练可以提高计算机视觉或自然语言领域中神经网络的性能。在这篇文章中，我们证明了它在应用于结构化数据时也可以工作，使其与其他机器学习方法(如低数据区的梯度推进)相竞争。</p></div></div>    
</body>
</html>