<html>
<head>
<title>TOP2VEC: New way of topic modelling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TOP2VEC:主题建模的新方式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/top2vec-new-way-of-topic-modelling-bea165eeac4a?source=collection_archive---------9-----------------------#2020-10-11">https://towardsdatascience.com/top2vec-new-way-of-topic-modelling-bea165eeac4a?source=collection_archive---------9-----------------------#2020-10-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3e9d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">几年前，从成千上万没有标注的自由文本文档中提取主题/话题/概念是非常困难的。最好和简单的方法是让一些人坐下来，浏览每篇文章，理解和注释主题。事实上，这很费时间，而且容易受到我们人类主观感知的影响。</p><p id="f53e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尽管在过去已经进行了许多尝试，使用像pLSA这样的简单算法来处理这种无监督学习问题并提取主题，但是LDA彻底改变了该领域，它通过在主题-单词和文档-主题分布之前添加Dirichlet来实现这一奇迹。多年来，LDA曾经是主题建模问题最受欢迎的算法。然而，这些模型的目标是找到可以用来以最小的误差重新创建原始文档的主题。</p><p id="f33e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这两种算法都落后的几个领域:</p><ol class=""><li id="4afa" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">假设主题的数量是已知的，理想情况下是未知的，并且成为模型的超参数，以获得更好的连贯性和迷惑性</li><li id="36f9" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">删除停用词是必要的，否则主题词分布将会受到停用词的高度污染</li><li id="0412" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">致力于文档的BOW(单词包)表示，忽略语义。像词汇化、词干化这样的预处理步骤可能有助于但无助于理解<em class="kz">边</em>和<em class="kz">边</em>的意思相同</li></ol><p id="bf07" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么，Top2Vec是如何工作的呢？？让我们一层一层地解码，并尝试在高层次上构建它的框架</p><p id="2b52" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在幕后，Top2Vec利用Doc2vec首先生成一个语义空间(语义空间是一个空间空间，其中向量之间的距离是语义相似性的指标)。</p><p id="bf8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你关注NLP研究，那么你一定读过doc2vec算法，它是对word2vec的高级修改，用于创建文档/句子/段落嵌入。与word2vec不同，doc2vec还在训练阶段将段落向量(可以将其视为正在学习的另一个单词向量)与单词向量结合在一起。和word2vec一样，do2vec也有两种训练方式:</p><ol class=""><li id="9027" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">具有分布式存储器(DM)段落向量:给定段落向量和上下文向量，预测目标单词</li><li id="975c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">分布式单词包(DBOW):给定段落向量，预测上下文单词</li></ol><p id="9d32" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于更好的性能和模型的简单性，Top2Vec使用了Doc2Vec的DBOW版本。</p><p id="7be8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个由单词和文档向量组成的语义空间是主题的连续表示，不像LDA，主题是从离散空间中采样的。现在以这个共同嵌入的单词和文档语义空间为例，文档高度集中的密集区域可以被认为具有相似的主题，并且可以由附近的嵌入单词来最好地表示。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi la"><img src="../Images/e7a7c34180e56ab6caf4f2f53686aa17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*tUxlygypdYg5s2SSsye-ng.png"/></div><p class="li lj gj gh gi lk ll bd b be z dk translated">图一。(来源于[1])]具有文档(紫色气泡)和单词(绿色气泡)的联合嵌入的语义空间的图示</p></figure><p id="b7f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了找到文档向量的密集区域来识别主题向量，首先想到的选择是利用一些基于密度的聚类算法(HDBSCAN似乎是理想的)。但是直接使用这些算法的问题是，我们处理的不是二维空间，而是它的多维稀疏空间和维数灾难。所以要克服这个问题，可以先进行降维，然后用HDBSCAN进行聚类。为了降低文档向量的维数，使用了一致流形逼近和投影降维(UMAP ),因为它在保持局部和全局结构方面是很好的。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/18458e002b941a137d9e44408d6b1348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ibU2ZXo3DPPnGFkBIVCEgg.png"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated">图二。(来源于[1])识别文档密集区域</p></figure><p id="02fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，UMAP和HDBSCAN帮助我们识别文档密集的聚类，主题向量可以简单地计算为这些聚类的质心。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/dd5ea399395eb3db90ec7e3e537d9131.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*FnSzyJsX3xnjPUq2pnsS-g.png"/></div><p class="li lj gj gh gi lk ll bd b be z dk translated">图三。(来源于[1])密集文档聚类的质心是主题向量</p></figure><p id="1b41" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为我们已经在单个语义空间中联合训练了文档和词向量。因此，一旦我们确定了主题向量，就很容易找到附近最能代表该主题的词向量，因此我们可以获得每个主题的词分布。这样，我们就可以为body中的每个文档生成主题</p><p id="3787" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于Top2Vec给了我们一个语义空间中主题的连续表示，这也允许我们将主题的数量减少到任何期望的数量。这是通过取最小主题的主题向量和它的最近主题向量的加权算术平均值来完成的，每个主题向量都根据它们的主题大小来加权。每次合并后，都会重新计算每个主题的主题大小(即主题所属文档的数量)</p><p id="d8f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">在使用Top2Vec之前我们需要去掉停用词吗？</strong></p><p id="ad5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不，因为这样的停用词将出现在几乎所有的文档中，因此它们将与所有主题等距，而不会作为离任何主题最近的词出现。</p><p id="357f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇论文的作者也为这个模型提供了开源的API(【https://github.com/ddangelov/Top2Vec】T2),并且只需要很少的编码工作就可以很好地完成。我不会进入这一部分，让你来做实验</p><p id="b539" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参考资料:</p><p id="0ce7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[1]https://arxiv.org/pdf/2008.09470.pdf<a class="ae ls" href="https://arxiv.org/pdf/2008.09470.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="f2fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]<a class="ae ls" href="https://arxiv.org/pdf/1405.4053.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1405.4053.pdf</a></p></div></div>    
</body>
</html>