<html>
<head>
<title>Finding Duplicate Quora Questions Using Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习发现重复的Quora问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/finding-duplicate-quora-questions-using-machine-learning-249475b7d84d?source=collection_archive---------26-----------------------#2020-09-20">https://towardsdatascience.com/finding-duplicate-quora-questions-using-machine-learning-249475b7d84d?source=collection_archive---------26-----------------------#2020-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/d3da6d75f334aec14a38c1e73f1453f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iHaj_W0daBxns4aa"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">埃文·丹尼斯在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="69c3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Quora是一个令人惊叹的平台，互联网公司可以在这里提问、回答、关注和编辑问题。这使人们能够相互学习，更好地了解世界。每个月大约有1亿人访问Quora，所以很多人问类似措辞的问题也就不足为奇了。quora让它的关注者为同一个问题写一个答案并不是更好的一面。因此，如果有一个系统能够检测到一个新问题与已经回答的问题相似，那就更好了。</p><p id="afd9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以我们的问题陈述是<strong class="ki iu">预测一对问题是否重复</strong>。我们将使用各种机器学习技术来解决这个问题。这个博客不是一个完整的代码演练，但是我将解释我用来解决这个问题的各种方法。你可以从我的<a class="ae kf" href="https://github.com/arunm8489/Quora-question-pair-similarity" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> Github库看看我的代码。</strong>T9】</a></p><h2 id="cd3a" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">一些业务限制</h2><ul class=""><li id="d018" class="lx ly it ki b kj lz kn ma kr mb kv mc kz md ld me mf mg mh bi translated"><strong class="ki iu">错误分类的成本可能非常高</strong>。也就是说，如果一个用户问了一个特定的问题，而我们提供了其他的答案，那就不好了。会影响生意的。这是最重要的约束。</li><li id="538b" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated">我们想要一对问题被复制的概率，这样你就可以选择任何选择的阈值。因此，根据使用情况，我们可以改变它。</li><li id="a571" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated">我们没有任何延迟要求。</li><li id="51c2" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated">可解释性是部分重要的。即我们不想让用户知道为什么一对问题是重复的。但是如果我们知道会更好。</li></ul><h2 id="5d43" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">绩效指标</h2><p id="aad7" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">这里我们有一个<strong class="ki iu">二元分类</strong>任务，我们想预测一对问题是否重复。我们将使用<strong class="ki iu">日志损失</strong>作为衡量标准。有意义因为我们预测的是概率值，所以使用对数损失作为度量是有意义的。这是我们的主要KPI(关键绩效指标)。我们还将使用混淆矩阵来衡量绩效。</p><blockquote class="mq mr ms"><p id="64e9" class="kg kh mt ki b kj kk kl km kn ko kp kq mu ks kt ku mv kw kx ky mw la lb lc ld im bi translated">对数损失只不过是可能性乘积对数的负值</p></blockquote></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="b095" class="ne lf it bd lg nf ng nh lj ni nj nk lm nl nm nn lp no np nq ls nr ns nt lv nu bi translated">探索性数据分析</h1><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/5edf6cd2b5fd2da5dad663843dff094b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mFzrUz-4kiCFJnMS0sNNqg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">数据</p></figure><p id="8da7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们有大约404290个数据点和6列。这6列/特征是:</p><ul class=""><li id="d4d5" class="lx ly it ki b kj kk kn ko kr oa kv ob kz oc ld me mf mg mh bi translated">id:问题对的唯一id</li><li id="cc59" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated">qid1:第一题的id。</li><li id="e7b3" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated">qid2:第二个问题id</li><li id="90cd" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated">问题1:第一个问题</li><li id="2b35" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated">问题2:第二个问题</li><li id="e6fa" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated">is_duplicate:两者是否重复。</li></ul><p id="5e08" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当最初接近时，我检查丢失值。我发现3行中有缺失值(在问题1和问题2中)。所以我放弃了那几行。接下来，我检查任何重复的行。但是没有这样的争吵。</p><h2 id="3dc8" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">目标变量分析</h2><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/f362cccb695f4941f4c9fd291c42c9be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*M2l-ZNf0PiPT0VjZGZgD_A.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">目标分布</p></figure><p id="a19b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">很明显，我们有不平衡的数据，重复问题的数量超过了非重复问题。</p><h2 id="73dc" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">问题分析</h2><p id="60b9" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">在分析了这些问题后，我得出了以下结论:</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/3afaca3b7dfc913464faf9f535851edf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*7Fa2hfqczXfQd2Fak4XmGw.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">独特问题与重复问题</p></figure><pre class="nw nx ny nz gt of og oh oi aw oj bi"><span id="a8aa" class="le lf it og b gy ok ol l om on">Total number of unique questions is 537929<br/>Number of questions that repeated more than 1 time is 111778 which is 20.779322178205675%<br/>The maximum number of times a question occured is 157</span></pre><p id="5e6a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之后，我试着绘制了问题出现次数的直方图和问题数量的日志。我们可以看到，大多数问题的出现频率大约为&lt; 60. We can see there is 1 question that occurs 167 times, 1 question that occurs 120 times 1 question that occurs 80 times, and so on.</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oo"><img src="../Images/674f067a840114656b33d823fb361b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0lYsVFCjexTadrmj-sCTQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">log histogram of question appearances</p></figure><p id="382e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Now we have a broad understanding of data. Next, I created some basic feature features before preprocessing the data.</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="ec3a" class="ne lf it bd lg nf ng nh lj ni nj nk lm nl nm nn lp no np nq ls nr ns nt lv nu bi translated">Basic Feature Engineering</h1><p id="1293" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">I created the following features:</p><ul class=""><li id="c45d" class="lx ly it ki b kj kk kn ko kr oa kv ob kz oc ld me mf mg mh bi translated"><strong class="ki iu">freq _ qid 1</strong>= qid 1的频率。</li><li id="8683" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu">freq _ qid 2</strong>= qid 2的频率，问题1出现的次数。</li><li id="0e28" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu">Q1 len</strong>= Q1的长度</li><li id="0b64" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu">q2len</strong>= Q2的长度</li><li id="5d6b" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu"> q1_n_words </strong> =问题1的字数</li><li id="8c63" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu"> q2_n_words </strong> =问题2的字数</li><li id="123f" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu"> word_Common </strong> =(问题1和问题2中常见唯一词的个数)。</li><li id="21fe" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu"> word_Total </strong> =(问题1总字数+问题2总字数)</li><li id="5ee4" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu">word _ share</strong>=(word _ common)/(word _ Total)</li><li id="0dfa" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu">freq _ Q1+freq _ Q2</strong>= qid 1和qid2的频率总和</li><li id="aa94" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu">freq _ Q1-freq _ Q2</strong>= qid 1和qid2的绝对频率差</li></ul><h2 id="1d46" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">工程特征分析</h2><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi op"><img src="../Images/c15c9925c194b40619d2b3efce14764f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OR5rCS45jAHaq-0xmplf_A.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">相似和不相似问题中的单词共享</p></figure><p id="552a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，随着“份额”这个词的增加，出现类似问题的可能性越来越大。我们知道，对于pdf来说，它们重叠的越多，区分类别的信息就越少。从直方图中，我们可以看到单词share具有一些区分相似和不相似类的信息。</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oq"><img src="../Images/0eb7b0c38f78f4f5dbf5cb87c9ea6bf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1AJft-vl5dIdVFHZj0iz0g.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">相似和不相似问题中的常用词</p></figure><p id="c60f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，common_words没有足够的信息来区分类别。重复和非重复问题的word_common的历史图重叠。没有太多的信息可以检索，因为大多数的pdf文件是重叠的。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="ce9c" class="ne lf it bd lg nf ng nh lj ni nj nk lm nl nm nn lp no np nq ls nr ns nt lv nu bi translated">高级特征工程</h1><p id="d7c1" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">现在，我们将使用现有数据创建一些高级功能。在此之前，我们将清理我们的文本数据。作为文本预处理的一部分，我已经删除了停用词、标点符号、特殊字符如“₹","$","€”等，并且我还应用了词干来获得更多的概括。接下来，我设计了以下功能。</p><blockquote class="mq mr ms"><p id="e696" class="kg kh mt ki b kj kk kl km kn ko kp kq mu ks kt ku mv kw kx ky mw la lb lc ld im bi translated">注意:在下面的特征中，<strong class="ki iu"> tocken </strong>表示通过拆分文本获得的单词，<strong class="ki iu">单词</strong>表示不是停用词的单词。</p></blockquote><ul class=""><li id="5a9f" class="lx ly it ki b kj kk kn ko kr oa kv ob kz oc ld me mf mg mh bi translated"><strong class="ki iu">CWC _ min</strong>:Q1和Q2常用字数与最小字数长度的比值<br/> <em class="mt"> cwc_min =常用字数/ (min(len(q1_words)，len(q2_words)) </em></li><li id="e5b1" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu">CWC _ max</strong>:common _ word _ count与Q1和Q2最大字数长度的比值<br/><em class="mt">CWC _ max = common _ word _ count/(max(len(Q1 _ words)，len(q2_words)) </em></li><li id="8e11" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu">CSC _ min</strong>:common _ stop _ count与Q1和Q2停车计数最小长度之比<br/><em class="mt">CSC _ min = common _ stop _ count/(min(len(Q1 _ stops)，len(q2_stops)) </em></li><li id="2d88" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu">CSC _ max</strong>:common _ stop _ count与Q1和Q2 <br/>停车计数最大长度的比值<em class="mt">CSC _ max = common _ stop _ count/(max(len(Q1 _ stops)，len(q2_stops)) </em></li><li id="cfbc" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu">CTC _ min</strong>:common _ token _ count与Q1和Q2令牌计数的最小长度之比<br/><em class="mt">CTC _ min = common _ token _ count/(min(len(Q1 _ tokens)，len(q2_tokens)) </em></li><li id="40a3" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu">CTC _ max</strong>:common _ token _ count与Q1和Q2令牌计数最大长度之比<br/><em class="mt">CTC _ max = common _ token _ count/(max(len(Q1 _ tokens)，len(q2_tokens)) </em></li><li id="f339" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu"> last_word_eq </strong>:检查两个问题的最后一个单词是否相等<br/><em class="mt">last _ word _ eq = int(Q1 _ tokens[-1]= = Q2 _ tokens[-1])</em></li><li id="586a" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu"> first_word_eq </strong>:检查两个问题的首字是否相等<br/><em class="mt">First _ word _ eq = int(Q1 _ tokens[0]= = Q2 _ tokens[0])</em></li><li id="516f" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu"> abs_len_diff </strong> : Abs。长度差<br/><em class="mt">ABS _ len _ diff = ABS(len(Q1 _ tokens)—len(Q2 _ tokens))</em></li><li id="8713" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="ki iu"> mean_len </strong>:两个问题的平均令牌长度<br/><em class="mt">mean _ len =(len(Q1 _ tokens)+len(Q2 _ tokens))/2</em></li><li id="f5c8" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated">有趣的部分来了。模糊率取决于Levenshtein距离。直观地说，如果从一个实例变成另一个实例所需的相应编辑很大，模糊比率就会很小。也就是说，大多数相似词的模糊率是相似的。</li></ul><pre class="nw nx ny nz gt of og oh oi aw oj bi"><span id="6613" class="le lf it og b gy ok ol l om on">eg: s1 = “mumbai is a great place”   s2 = "mumbai is a nice place"<br/>fuzz ratio = 91</span></pre><ul class=""><li id="2285" class="lx ly it ki b kj kk kn ko kr oa kv ob kz oc ld me mf mg mh bi translated">模糊比率在某些情况下不能解决问题。</li></ul><pre class="nw nx ny nz gt of og oh oi aw oj bi"><span id="a73c" class="le lf it og b gy ok ol l om on">fuzz.ratio("YANKEES", "NEW YORK YANKEES") ⇒ 60<br/>fuzz.ratio("NEW YORK METS", "NEW YORK YANKEES") ⇒ 75</span></pre><p id="9461" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">s1和s2的意思相同。但是它们的起毛率可以更小。所以我们会发现部分句子的比率会很高。在这种情况下，它被称为模糊部分比率。</p><pre class="nw nx ny nz gt of og oh oi aw oj bi"><span id="3831" class="le lf it og b gy ok ol l om on">fuzz.partial_ratio("YANKEES", "NEW YORK YANKEES") ⇒ 60</span></pre><ul class=""><li id="05d8" class="lx ly it ki b kj kk kn ko kr oa kv ob kz oc ld me mf mg mh bi translated"><strong class="ki iu"> token_sort_ratio </strong>:在其他一些情况下，甚至fuzz partial ratio也会失败。</li></ul><p id="f267" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如:</p><pre class="nw nx ny nz gt of og oh oi aw oj bi"><span id="27bd" class="le lf it og b gy ok ol l om on">fuzz.partial_ratio("MI vs RCB","RCB vs MI") <!-- -->⇒ 72</span></pre><p id="76d6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实际上这两个句子意思相同。但是模糊率给出了较低的结果。因此，更好的方法是对令牌进行排序，然后应用模糊比率。</p><pre class="nw nx ny nz gt of og oh oi aw oj bi"><span id="ce43" class="le lf it og b gy ok ol l om on">fuzz.token_sort_ratio("MI vs RCB","RCB vs MI") <!-- -->⇒ 100</span></pre><ul class=""><li id="a0dd" class="lx ly it ki b kj kk kn ko kr oa kv ob kz oc ld me mf mg mh bi translated">token_set_ratio :还有另一种类型的模糊比率，即使在上述所有方法都失败的情况下，它也会有所帮助。就是令牌集比率。</li></ul><p id="041c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，我们必须首先找到以下内容:</p><p id="49bd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">t0 -&gt;找出sentance1和sentance2的交集词并排序。</p><p id="9bf7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">t1-&gt; t0+sentance 1中的其余令牌</p><p id="bd45" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">T2-&gt; t0+sentance 2中的剩余令牌</p><p id="8cef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mt">tocken _ set _ ratio = max(fuzz _ ratio(to，t1)，fuzz_ratio(t1，T2)，fuzz_ratio(t0，t2)) </em></p><ul class=""><li id="d50d" class="lx ly it ki b kj kk kn ko kr oa kv ob kz oc ld me mf mg mh bi translated"><strong class="ki iu">longest _ substr _ Ratio</strong>:Q1和Q2令牌计数中最长公共子串长度与最小长度之比。</li></ul><pre class="nw nx ny nz gt of og oh oi aw oj bi"><span id="fd17" class="le lf it og b gy ok ol l om on">s1-&gt; hai, today is a good day<br/>s2-&gt; No, today is a bad day</span></pre><p id="2355" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里最长的公共子串是“今天是一个”。所以我们有longest _ substring _ ratio = 3/min(6，6)= 0.5<br/><em class="mt">longest _ substr _ ratio = len(最长公共子串)/ (min(len(q1_tokens)，len(q2_tokens)) </em></p><h2 id="3cfe" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">更多数据分析</h2><p id="589f" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">现在我们将绘制重复句和非重复句的词云。为了更好地理解，我在删除了停用词后绘制了它。</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi or"><img src="../Images/a74830fe0019c5ea6e8598db4473df4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iZhlK3GhE5iYQQW4h8MzMg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">重复句子的词云</p></figure><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi os"><img src="../Images/b86c785edab3857eda97b9dab18c4ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FX3zJyE1RjEoWdjhJKdExw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">非重复句子的词云</p></figure><p id="b271" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词越大，语料库中的重复频率就越大。我们可以看到，像唐纳德·特朗普、卢比、最佳方式这样的词在重复句中经常重复，而像差异、印度、使用等这样的词在非重复句中经常重复。</p><h2 id="b59d" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">' ctc_min '，' cwc_min '，' csc_min '，' token_sort_ratio '的配对图</h2><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ot"><img src="../Images/e5d9f9513889af1b8caa99a63410d249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5AVD9bWQ3SsdMSSDyExHPg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">配对图</p></figure><p id="338d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从pair图中，我们可以看到，在区分重复和非重复句子的所有特征中，都有一些有用的信息。其中token sort ratio和ctc min做得更好。</p><h2 id="09ca" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">问题之间字数的绝对差异</h2><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ou"><img src="../Images/dfcec52e9ef4607072f75c80c6b583e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Sb0JjUtxasoh4V0PQ2FMw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">问题之间字数的绝对差异</p></figure><p id="c706" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，大多数问题的区别就在于一个词。差别巨大的问题只有极少数。</p><h2 id="3fd2" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">TSNE可视化</h2><p id="46ec" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">接下来，我尝试对数据进行低维可视化。我随机采样了5000个数据点，并使用TSNE进行低维可视化。我只使用了我们最近设计的功能来观察它对分析的影响。我们看到，在一些地区，阶级和阶层划分得很清楚。因此，我们可以说，现在我们的模型中有很多信息来执行良好的分类。</p><p id="05eb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意:你总是可以试验更多的数据点和困惑(如果你有足够的计算能力)，因为它会给出更多的信息。</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/2fde73fa6763f0a2f6e9de3739903944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*zJJ7Fl7ytnx9Chgy5-S6mA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">TSNE可视化</p></figure><h2 id="ea63" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">列车测试分离</h2><p id="1b07" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">我们做了70:30的分割。也就是说，我们把70%的数据点用于训练，剩下的30%用于测试。</p><h2 id="febd" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">向量化文本数据</h2><p id="b2a7" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">在使用数据创建模型之前，我们必须对文本数据进行矢量化。为此，我们使用了两种方法</p><ul class=""><li id="b2f7" class="lx ly it ki b kj kk kn ko kr oa kv ob kz oc ld me mf mg mh bi translated">TFIDF矢量化</li><li id="88f9" class="lx ly it ki b kj mi kn mj kr mk kv ml kz mm ld me mf mg mh bi translated">TFIDF加权手套矢量化</li></ul><p id="3737" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在将这些矢量与我们创建的初始特征合并后，我们保存了单独的文件。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="d71e" class="ne lf it bd lg nf ng nh lj ni nj nk lm nl nm nn lp no np nq ls nr ns nt lv nu bi translated">机器学习模型</h1><p id="766e" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">让我们进入这个博客最有趣的部分——创建机器学习模型。现在我们有两个数据帧用于训练——一个使用tfidf，另一个使用tfidf加权手套向量。</p><h2 id="4aa8" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">逻辑回归</h2><p id="aa62" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated"><strong class="ki iu"> TFIDF功能:</strong></p><p id="0037" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在对TFIDF数据进行训练逻辑回归时，我们最终得到训练的对数损失约为<strong class="ki iu"> 0.43，测试的对数损失约为</strong>0.53。我们的混淆精度和召回矩阵如下所示:</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ow"><img src="../Images/88c24b0c4e6497f76acfc410a3abe4cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-5dGl1exYumgtPfhazruQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">混淆矩阵/精确矩阵/召回矩阵</p></figure><p id="f8cf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个人都必须知道如何解释正常的混淆矩阵。我不想对它进行解释。让我们看看如何解释精度矩阵。上图中，第二个是精度矩阵。</p><p id="1dff" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直观上，精度意味着在所有预测为正的点中，有多少实际上是正的。这里我们可以看到，所有预测为1类的标签中，有24.6%属于2类，剩下的75.4 %属于1类。类似地，对于预测为第2类的所有点，69.2%属于第2类，30.02%属于第1类。这里，类1的精度是0.754，类2的精度是0.30。</p><p id="28f1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的第三个矩阵是回忆矩阵。直观回忆是指，在属于一类的所有点中，有多少是正确预测的。在召回矩阵中，在属于类别1的所有标签中，86.5%被预测为类别1，13.5%被预测为类别2。同样，在所有属于class2的原始点中，59.9%属于class2，其余属于class1。</p><p id="58ee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> TFIDF加重手套:</strong></p><p id="f42f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在超参数调整之后，我们最终得到的对数损失大约是测试的0.39和训练的0.38。</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ox"><img src="../Images/d2b5045a2d4afb80eb9f026f22b019a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KS-psRDGnp7ZBNxNHqV_-g.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">混淆矩阵/精确矩阵/召回矩阵</p></figure><p id="f15e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从两个召回矩阵中，我们可以看到召回值较低。让我们看看它如何执行线性SVM。</p><h2 id="20f0" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">线性SVM</h2><h2 id="1600" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">TFIDF功能:</h2><p id="658d" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">列车日志损失:0.3851</p><p id="1557" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">测试日志损失:0.3942</p><p id="18b6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">混淆矩阵:</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oy"><img src="../Images/e085b9ad8ec9324e5d2bd157ec617cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iUFCjzFTBw-_PdVbfElYJw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">混淆矩阵/精确矩阵/召回矩阵</p></figure><p id="aca3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> TFIDF加重手套:</strong></p><p id="3014" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">列车日志损失:0.3876</p><p id="9e81" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">测试日志损失:0.395</p><p id="fe39" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">混淆矩阵:</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ox"><img src="../Images/32f98da225c4199cc065d0eb596d2a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rWTxqHvprBkmIQaUY2UL7w.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">混淆矩阵/精确矩阵/召回矩阵</p></figure><p id="9763" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，无论是逻辑回归还是线性SVM，我们的回忆值都很低。但是这两个模型都没有过度拟合。所以我在这里想到了一个偏高的问题。如果我们选择一些提升方法，我们可以克服这一点，可以获得更好的召回值。出于这个目的，我在tfidf和tfidf加重手套功能上试用了XGBoost。</p><h2 id="bf21" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">XGBOOST</h2><p id="a9e9" class="pw-post-body-paragraph kg kh it ki b kj lz kl km kn ma kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated"><strong class="ki iu"> TFIDF功能:</strong></p><p id="5ced" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">列车日志损失:0.457</p><p id="ba09" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">测试日志损失:0.516</p><p id="9760" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">混淆矩阵:</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oz"><img src="../Images/0fd6a57c7e1522f64be142d0bf01dbdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jk9UHzYd3GFuJd-a8PQf5g.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">混淆矩阵/精确矩阵/召回矩阵</p></figure><p id="6388" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> TFIDF加重手套特征:</strong></p><p id="a8a1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">列车日志损失:0.183</p><p id="360d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">测试日志损失:0.32</p><p id="472e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">混淆矩阵:</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oz"><img src="../Images/2a11252ee72ed916e71276a4890521af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*43UXmtzuFAlJEARtRyNKJA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">混淆矩阵/精确矩阵/召回矩阵</p></figure><p id="ae5c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，使用Xgboost后，两个类的召回率都略有提高。在所有这些xgboost手套向量表现更好。</p><figure class="nw nx ny nz gt ju gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/6a98a04786abbd2a738484279620c270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*tOu2rfZIA4docS2ckUi_yA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">结果</p></figure><p id="65bd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要查看完整的代码，你可以访问我的<a class="ae kf" href="https://github.com/arunm8489/Quora-question-pair-similarity" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h2 id="716a" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">参考</h2><ul class=""><li id="9cdd" class="lx ly it ki b kj lz kn ma kr mb kv mc kz md ld me mf mg mh bi translated">应用课程</li></ul></div></div>    
</body>
</html>