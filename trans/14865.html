<html>
<head>
<title>Data Lake Change Data Capture (CDC) using Amazon Database Migration Service — Part 1 —Capture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用亚马逊数据库迁移服务的数据湖变化数据捕获(CDC)——第1部分——捕获</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-lake-change-data-capture-cdc-using-amazon-database-migration-service-part-1-capture-b43c3422aad4?source=collection_archive---------18-----------------------#2020-10-13">https://towardsdatascience.com/data-lake-change-data-capture-cdc-using-amazon-database-migration-service-part-1-capture-b43c3422aad4?source=collection_archive---------18-----------------------#2020-10-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f2cc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Amazon Database Migration Service(DMS ),轻松捕获随时间推移从数据库到数据湖的数据更改</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c5e06ca98371ae11038c31e5590deabf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUUhW4NCBqWHmZh3-SrCAA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2654130" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae ky" href="https://pixabay.com/users/absolutvision-6158753/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2654130" rel="noopener ugc nofollow" target="_blank"> Gino Crescoli </a>拍摄</p></figure><p id="615d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我过去10年在大数据和分析领域的经历中，我逐渐意识到捕获和处理变化数据集一直是一个具有挑战性的领域。这些年来，我看到了疾控中心是如何发展的。让我带你走过这段旅程:</p><p id="6854" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2011–2013年—对许多人来说，Hadoop是主要的数据分析平台。通常，Sqoop用于将数据从给定的数据库传输到HDFS。这对于满表负载非常有效。Sqoop incremental可以捕获<em class="lv">插入</em>以及<em class="lv">。</em></p><p id="465a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">但是CDC不仅仅是关于插页。我的<em class="lv">更新</em>和<em class="lv">删除</em>在哪里？</strong></p><p id="f5ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2016年—我们创建了一个策略，使用数据库表上的触发器捕获更新和删除，并将更改写入影子表。一旦捕获到发生更改的数据，我们将使用Sqoop将数据传输到HDFS。这种方法需要修改数据库，所以我们的很多客户都反对。</p><p id="ca0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2015–2016年——名为<strong class="lb iu"> Debezium </strong>的新开源项目的使用越来越多。此后的几年里，我们非常有效地使用了这个CDC工具。最初，Debezium只支持有限数量的数据库，但这足以覆盖我们的大部分用例。</p><p id="3562" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Debezium能够查询数据库二进制日志并提取更改。它以JSON文档的形式将每个变化发布给Kafka。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/668b47552a6ecd37ea4b6cf69319a767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*97esWsHv5jVrZMrc"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像—在图像之前和之后记录</p></figure><p id="a4a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2016年—现在—对于AWS云部署，我们通常使用亚马逊数据库迁移服务(DMS)。DMS可以从内部服务器或RDS读取变更数据集，并将其发布到许多目的地，包括S3、红移、Kafka和Elasticsearch等。</p><p id="2a34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我向您展示如何创建一个示例CDC管道。我们将首先在AWS上创建一个RDS数据库，创建一个示例数据库，最后设置Amazon DMS来执行到S3的变更数据捕获。</p><p id="c742" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从下载一个样本数据文件开始</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="ce52" class="mc md it ly b gy me mf l mg mh">$ git clone https://github.com/mkukreja1/blogs.git</span></pre><p id="8ccb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建RDS安全组并打开入口</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="25ee" class="mc md it ly b gy me mf l mg mh">$ aws ec2 delete-security-group --group-name "RDS Security Group"</span><span id="f0ba" class="mc md it ly b gy mi mf l mg mh">$ RDS_GROUP=` aws ec2 create-security-group --description sg-rds --group-name "RDS Security Group" | grep GroupId | sed 's/"GroupId"://' |  sed 's/"//g' |  sed 's/,//g'`;echo $RDS_GROUP</span><span id="9e9a" class="mc md it ly b gy mi mf l mg mh">$ aws ec2 authorize-security-group-ingress --group-id $RDS_GROUP  --protocol tcp --port 3306 --cidr <strong class="ly iu">0.0.0.0/0</strong></span><span id="8af1" class="mc md it ly b gy mi mf l mg mh"><em class="lv"># For security reasons you may want the replace </em><strong class="ly iu"><em class="lv">0.0.0.0/0 </em></strong><em class="lv">with your web facing IP. This will limit traffic originating from your IP address only. </em></span></pre><p id="399f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将创建一个MySQL数据库。该数据库将用作CDC的来源。首先创建一个RDS参数组。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="38c1" class="mc md it ly b gy me mf l mg mh">$ aws rds delete-db-parameter-group --db-parameter-group-name rds-mysql</span><span id="8f02" class="mc md it ly b gy mi mf l mg mh">$ PG_ARN=`aws rds create-db-parameter-group --db-parameter-group-name rds-mysql --db-parameter-group-family MySQL5.7 --description "RDS Group" | grep DBParameterGroupArn | sed -e 's/"//g' -e 's/,//g'  -e 's/DBParameterGroupArn//g' -e 's/: //g' `;echo $PG_ARN</span><span id="151d" class="mc md it ly b gy mi mf l mg mh">$ aws rds modify-db-parameter-group --db-parameter-group-name rds-mysql --parameters "ParameterName=binlog_format, ParameterValue=ROW,ApplyMethod=immediate" "ParameterName=binlog_checksum,ParameterValue=None,ApplyMethod=immediate"</span></pre><p id="076b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建RDS实例</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="0ebc" class="mc md it ly b gy me mf l mg mh">$ aws rds delete-db-instance --db-instance-identifier fossil --skip-final-snapshot</span><span id="1e6a" class="mc md it ly b gy mi mf l mg mh">$ aws rds create-db-instance --db-instance-identifier fossil --db-instance-class db.t2.micro --engine mysql --region us-east-1 --output text --master-username admin --master-user-password admin123 --allocated-storage 20 --vpc-security-group-ids $RDS_GROUP --db-parameter-group-name rds-mysql --option-group-name default:mysql-5-7 --engine-version 5.7.30</span><span id="4150" class="mc md it ly b gy mi mf l mg mh"><strong class="ly iu">-- Wait for 5-10 minutes after this step</strong></span></pre><p id="4501" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">安装一个MySQL客户端并测试与MySQL的连接。将数据文件导入新创建的数据库。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="4d59" class="mc md it ly b gy me mf l mg mh">$ sudo yum -y install mysql</span><span id="9690" class="mc md it ly b gy mi mf l mg mh">$ RDS_ENDPOINT=`aws rds describe-db-instances --db-instance-identifier fossil | grep "Address" | sed 's/.*://'   | sed 's/"//g'    | sed 's/,//g'`;echo $RDS_ENDPOINT</span><span id="a9f4" class="mc md it ly b gy mi mf l mg mh">$ mysql -uadmin -padmin123 -h $RDS_ENDPOINT -e "DROP DATABASE IF EXISTS fossil;CREATE DATABASE fossil;grant REPLICATION CLIENT on *.* to admin;grant REPLICATION SLAVE on *.* to admin;"</span><span id="9f4f" class="mc md it ly b gy mi mf l mg mh">$ mysql -uadmin -padmin123 -h $RDS_ENDPOINT fossil &lt; blogs/dms/energy.sql</span><span id="bedc" class="mc md it ly b gy mi mf l mg mh">$ mysql -uadmin -padmin123 -h $RDS_ENDPOINT -e "use fossil;select count(*) from coal_prod"</span></pre><p id="7bb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">配置DMS。首先创建DMS复制实例。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="0893" class="mc md it ly b gy me mf l mg mh">$ SG_RDS=`aws ec2 describe-security-groups --group-names "RDS Security Group" | grep GroupId | sed -e 's/"//g' -e 's/,//g'  -e 's/GroupId//g' -e 's/: //g' `;echo $SG_RDS</span><span id="6a86" class="mc md it ly b gy mi mf l mg mh">$ aws dms create-replication-instance --replication-instance-identifier rds-s3-dms --replication-instance-class dms.t2.micro --no-publicly-accessible --vpc-security-group-ids $SG_RDS</span><span id="feed" class="mc md it ly b gy mi mf l mg mh">$ REP_ARN=`aws dms describe-replication-instances | grep ReplicationInstanceArn | sed -e 's/"//g' -e 's/,//g'  -e 's/ReplicationInstanceArn//g' -e 's/: //g' `;echo $REP_ARN</span><span id="63f4" class="mc md it ly b gy mi mf l mg mh"><strong class="ly iu"># wait 5 minutes for the above to finish</strong></span></pre><p id="a3fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建DMS源端点。在这种情况下，上面创建的RDS实例将充当源。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="75ff" class="mc md it ly b gy me mf l mg mh">$ DMS_END_SOURCE=`aws dms create-endpoint --endpoint-identifier rds-end --endpoint-type source --server-name $RDS_ENDPOINT --engine-name mysql --username admin --password admin123 --port 3306 --database-name fossil | grep EndpointArn | sed -e 's/"//g' -e 's/,//g'  -e 's/EndpointArn//g' -e 's/: //g' `;echo $DMS_END_SOURCE</span></pre><p id="f8f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测试DMS源端点连接。只有在成功的情况下才能继续。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="1f3b" class="mc md it ly b gy me mf l mg mh">$ aws dms test-connection --replication-instance-arn $REP_ARN --endpoint-arn $DMS_END_SOURCE</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/248f10266de35048cbae53fdcd7f25fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KN6b3oT8wzuOgb_N"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像— DMS源端点</p></figure><p id="5ba5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建有权访问S3的DMS角色。我们将使用该角色来定义DMS目的地端点。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="1051" class="mc md it ly b gy me mf l mg mh">$ aws iam detach-role-policy --role-name dms-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess</span><span id="e0ea" class="mc md it ly b gy mi mf l mg mh">$ aws iam delete-role --role-name dms-role</span><span id="485c" class="mc md it ly b gy mi mf l mg mh">$ DMS_ROLE=`aws iam create-role --role-name dms-role --assume-role-policy-document file://blogs/dms/policy.json | grep Arn | sed -e 's/"//g' -e 's/,//g'  -e 's/Arn//g' -e 's/ //g' -e 's/://' `;echo $DMS_ROLE</span><span id="c17f" class="mc md it ly b gy mi mf l mg mh">$ aws iam attach-role-policy --role-name dms-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess</span><span id="fe26" class="mc md it ly b gy mi mf l mg mh"><strong class="ly iu"># you ay want to tighten up the above policy to limit access to specific buckets only</strong></span><span id="e0ad" class="mc md it ly b gy mi mf l mg mh">$ aws iam create-role --role-name dms-vpc-role --assume-role-policy-document file://blogs/dms/policy.json</span><span id="0874" class="mc md it ly b gy mi mf l mg mh">$ aws iam attach-role-policy --role-name dms-vpc-role --policy-arn arn:aws:iam::aws:policy/service-role/AmazonDMSVPCManagementRole</span></pre><p id="8273" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建DMS目标端点</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="a9bb" class="mc md it ly b gy me mf l mg mh">$ S3_JSON="{\"ServiceAccessRoleArn\": \"$DMS_ROLE\",\"BucketFolder\": \"raw/dms\",\"BucketName\": \"aws-analytics-course\",\"DataFormat\": \"csv\", \"IncludeOpForFullLoad\": true }";echo $S3_JSON &gt;s3.json;cat s3.json</span><span id="cd69" class="mc md it ly b gy mi mf l mg mh">$ DMS_END_DEST=`aws dms create-endpoint --endpoint-identifier s3-end --engine-name s3 --endpoint-type target --s3-settings file://s3.json | grep EndpointArn | sed -e 's/"//g' -e 's/,//g'  -e 's/EndpointArn//g' -e 's/: //g' `;echo $DMS_END_DEST</span><span id="27ce" class="mc md it ly b gy mi mf l mg mh">$ aws dms test-connection --replication-instance-arn $REP_ARN --endpoint-arn $DMS_END_DEST</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/04e054d120b39331f57c7683a47731b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*g0op2km6Qt6xaZpw"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像— DMS目标端点</p></figure><p id="4c75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建并运行DMS任务。在第一次运行时，该任务将从源端点的所有表中获取全部数据，并将数据复制到目的端点。之后，复制实例跟踪源端点上的更改，并迅速将它们传递到目标。在这个过程中，复制实例维护每个表的日志。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="48f1" class="mc md it ly b gy me mf l mg mh">$ aws dms create-replication-task     --replication-task-identifier rdstos3     --source-endpoint-arn $DMS_END_SOURCE     --target-endpoint-arn $DMS_END_DEST     --replication-instance-arn $REP_ARN     <strong class="ly iu">--migration-type full-load-and-cdc </strong>    --table-mappings file://blogs/dms/table-mappings.json</span><span id="7b99" class="mc md it ly b gy mi mf l mg mh">$ TASK_ARN=` aws dms describe-replication-tasks | grep ReplicationTaskArn | sed -e 's/"//g' -e 's/,//g'  -e 's/ReplicationTaskArn//g' -e 's/ //g' -e 's/://' `;echo $TASK_ARN</span><span id="9e28" class="mc md it ly b gy mi mf l mg mh">$ aws dms start-replication-task --replication-task-arn $TASK_ARN  --start-replication-task-type reload-target</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/848937194c2d3dc25d5121db5d6f94d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*dbzbE3zLRYMS9_B-"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像—复制任务的状态</p></figure><p id="2771" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦DMS作业运行，我们就可以检查S3上满负载的数据</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="fe71" class="mc md it ly b gy me mf l mg mh">$ aws s3 ls aws-analytics-course/raw/dms/</span><span id="24e1" class="mc md it ly b gy mi mf l mg mh">$ aws s3 ls aws-analytics-course/raw/dms/<br/>                           PRE fossil/</span><span id="93a6" class="mc md it ly b gy mi mf l mg mh">$ aws s3 ls aws-analytics-course/raw/dms/fossil/<br/>                           PRE coal_prod/<br/>                           PRE fossil_capita/<br/>                           PRE gas_prod/<br/>                           PRE oil_prod/</span><span id="ef12" class="mc md it ly b gy mi mf l mg mh">$ aws s3 ls aws-analytics-course/raw/dms/fossil/coal_prod/<br/>2020-07-13 18:08:09     326026 LOAD00000001.csv</span></pre><p id="106a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，每条记录都标记了DML操作，在本例中，所有行都标记了Insert (I ),因为这是第一次将数据从源装载到目标。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/b6c2de7ac2ea5ca19ed0468c8ffcb725.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*3cIlLAIWgZZZGlutno3I4w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5d71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们对源数据库执行更多的DML操作——插入、更新和删除。从几个插页开始。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="3334" class="mc md it ly b gy me mf l mg mh">$ mysql -uadmin -padmin123 -h $RDS_ENDPOINT fossil -e "INSERT INTO fossil.coal_prod VALUES('India', 'IND', 2015, 4056.33, 0.00);INSERT INTO fossil.coal_prod VALUES('India', 'IND', 2016, 4890.45, 0.00)"</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mm"><img src="../Images/c458ac63a4675202b6b687fd3122f254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xH6lsu0HQScllyTCGud_oQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c58a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，S3的文件将新插入的行标记为“I”。现在让我们发送一个更新。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="5b97" class="mc md it ly b gy me mf l mg mh">$ mysql -uadmin -padmin123 -h $RDS_ENDPOINT fossil -e "UPDATE fossil.coal_prod SET Production=2845.66, consumption=145.66 WHERE Entity='India' AND Year=2013"</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/f9cd492a0edd25d30ee006b7c1a077a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qSUe3vlrXC704CFzO0xBw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b662" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，S3的文件将新插入的行标记为“U”。最后发个删除。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="c9f3" class="mc md it ly b gy me mf l mg mh">$ mysql -uadmin -padmin123 -h $RDS_ENDPOINT fossil -e "DELETE FROM fossil.coal_prod WHERE Entity='India' AND Year=2010"</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/c3df2a1050661180f86569d2b4ba2a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9mbgkSLEIh6KQWoATrrxVg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="178d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，S3的文件将新插入的行标记为“D”。</p><blockquote class="mp"><p id="f390" class="mq mr it bd ms mt mu mv mw mx my lu dk translated">有什么大惊小怪的？毕竟它只是插入、更新和删除。</p></blockquote><p id="b26f" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">既然我们已经有了连续复制，源端和目标端将保持同步。在本文的第2部分，我将向您展示如何使用Apache胡迪将CDC接收到数据湖中。</p><p id="31ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您将本文用于测试/教育，不要忘记在完成后清理您的AWS资源。如果在生产中使用，DMS资源将永久部署。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="2718" class="mc md it ly b gy me mf l mg mh">$ aws dms stop-replication-task --replication-task-arn $TASK_ARN</span><span id="f541" class="mc md it ly b gy mi mf l mg mh">$ aws dms delete-replication-task --replication-task-arn $TASK_ARN</span><span id="4bc3" class="mc md it ly b gy mi mf l mg mh">$ aws dms delete-endpoint --endpoint-arn $DMS_END_SOURCE</span><span id="743f" class="mc md it ly b gy mi mf l mg mh">$ aws dms delete-endpoint --endpoint-arn $DMS_END_DEST</span><span id="771b" class="mc md it ly b gy mi mf l mg mh">$ aws dms delete-replication-instance --replication-instance-arn $REP_ARN</span><span id="1ec3" class="mc md it ly b gy mi mf l mg mh">$ aws rds delete-db-instance --db-instance-identifier fossil --skip-final-snapshot</span></pre><p id="8253" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文中使用的所有代码都可以在下面的链接中找到:</p><div class="ne nf gp gr ng nh"><a href="https://github.com/mkukreja1/blogs/tree/master/dms" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">mkukreja 1/博客</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">github.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ks nh"/></div></div></a></div><p id="9c6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章是有帮助的。<strong class="lb iu"> CDC使用亚马逊数据库迁移服务</strong>是由<a class="ae ky" href="http://www.datafence.com" rel="noopener ugc nofollow" target="_blank"> Datafence云学院</a>提供的AWS大数据分析课程的一部分。课程是周末自己在网上教的。</p></div></div>    
</body>
</html>