<html>
<head>
<title>Beyond Grid Search: Hypercharge Hyperparameter Tuning for XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超越网格搜索:XGBoost的超荷超参数调优</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d?source=collection_archive---------3-----------------------#2020-10-30">https://towardsdatascience.com/beyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d?source=collection_archive---------3-----------------------#2020-10-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4d28" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Hyperopt、Optuna和Ray Tune加速机器学习超参数优化</h2></div><p id="e8a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">机器学习模型超参数的贝叶斯优化比网格搜索更快更好地工作。以下是我们如何加快超参数调优的方法:1)使用Hyperopt和Optuna进行贝叶斯优化，运行于……2)使用<a class="ae lb" href="https://ray.io/" rel="noopener ugc nofollow" target="_blank"> Ray </a>分布式机器学习框架，使用<a class="ae lb" href="https://medium.com/riselab/cutting-edge-hyperparameter-tuning-with-ray-tune-be6c0447afdf" rel="noopener">统一Ray调优API来实现许多超参数搜索算法</a>和提前停止调度程序，以及……3)使用云实例的分布式集群来实现更快的调优。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/e6e8d67c567adad372bcf10dcf7b8a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xfnh-biDrMCECEO37qecKQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者图片</p></figure><h2 id="e531" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">大纲:</h2><ol class=""><li id="536e" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la ms mt mu mv bi translated">结果</li><li id="d79e" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">超参数调整概述</li><li id="ec5b" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">贝叶斯优化</li><li id="a4e6" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">提前停止</li><li id="9ce2" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">实施细节</li><li id="0b3a" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">基线线性回归</li><li id="b86d" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">L1和L2正则化线性回归</li><li id="470e" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">带GridSearchCV的ElasticNet</li><li id="c022" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">XGBoost:超参数子集上的序贯网格搜索</li><li id="0f2f" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">XGBoost: Hyperopt和Optuna搜索算法</li><li id="bcb7" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">LightGBM: Hyperopt和Optuna搜索算法</li><li id="800b" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">光线簇上的XGBoost</li><li id="9ef6" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">光线簇上的LightGBM</li><li id="8c1f" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">结束语</li></ol><h2 id="3376" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">1.结果</h2><p id="fbb1" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">底线在前面:这里是艾姆斯住房数据集的结果，预测爱荷华州的房价:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="ne nf l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">使用各种超参数优化方法的XGB和LightGBM的RMSE和搜索时间</p></figure><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="ne nf l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">基线线性模型的RMSE和拟合时间</p></figure><h2 id="847e" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">基线线性模型</h2><p id="f29b" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated"><em class="ng">单实例的时间是在一个12线程的本地桌面上，与EC2 4xlarge相当。集群的时间在m5.large x 32 (1个头节点+ 31个工作节点)上。</em></p><ul class=""><li id="0187" class="ml mm iq kh b ki kj kl km ko nh ks ni kw nj la nk mt mu mv bi translated">与网格搜索相比，我们在本地使用Hyperopt和Optuna时获得了很大的加速。顺序搜索执行了大约261次尝试，因此XGB/Optuna搜索在一半的时间内执行了大约3倍的尝试，并得到了类似的结果。</li><li id="e1a7" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">与具有12个线程的本地桌面相比，32个实例(64个线程)的集群提供了适度的RMSE改进。我试图这样设置，这样我们就可以在RMSE和本地的Hyperopt/Optuna之间获得一些改进(我们在2048次试验中做到了这一点)，并在训练时间上获得一些加速(我们在64线程中没有得到这一点)。它在不到两倍的时间内进行了两倍的试验。比较是不完美的，本地桌面与AWS，在本地运行Ray 1.0，在集群上运行1.1，不同的试验次数(更好的超参数配置不会提前停止，需要更长的训练时间)。但重点是看看在实践中，利用集群与本地台式机或笔记本电脑相比，可以获得什么样的改进。底线是，32节点群集带来了适度的好处。</li><li id="99bf" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">RMSEs在所有方面都是相似的。XGB与2048年的审判是最好的一个小幅度的助推模型。</li><li id="3966" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">在RMSE或运行时，LightGBM并没有提供对XGBoost的改进。以我的经验来看，LightGBM往往更快，所以你可以在给定的时间内进行更多的训练和调优。但是我们在这里看不到。XGB可能会更好地与ASHA早期停止互动。</li><li id="3308" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">远视眼和眼的RMSE相似。Optuna的速度始终更快(LGBM/cluster可提高35%)。</li></ul><p id="3fb3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在几秒钟内，我们简单的ElasticNet基线产生的结果比boosting稍好。这可能是因为我们的特征工程是密集的，并且被设计成适合线性模型。未显示，SVR和KernelRidge优于ElasticNet，并且集合改进了所有单个算法。</p><p id="1eb9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的笔记本在<a class="ae lb" href="https://github.com/druce/iowa/blob/master/hyperparameter_optimization.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上。</p><h2 id="7749" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">2.超参数调整概述</h2><p id="2425" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">(如果你不是数据科学家忍者，这里有一些脉络。如果你是，你可以安全地跳到<em class="ng">贝叶斯优化</em>部分和下面的实现。)</p><p id="c02a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">任何足够先进的机器学习模型都与魔术难以区分，任何足够先进的机器学习模型都需要良好的调整。</p><p id="fa04" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">后退一步，下面是一个典型的建模工作流程:</p><ul class=""><li id="f146" class="ml mm iq kh b ki kj kl km ko nh ks ni kw nj la nk mt mu mv bi translated">探索性数据分析:理解你的数据。</li><li id="f3c7" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">特征工程和特征选择:清理、转换和工程最佳可能的特征</li><li id="ca7f" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">建模:模型选择和超参数调整以确定最佳的模型架构，以及集成以组合多个模型</li><li id="36c1" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">评估:描述样本外误差及其预期分布。</li></ul><p id="3b66" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了最小化样本外误差，您可以最小化来自<em class="ng">偏差</em>的误差，这意味着模型对数据中的信号不够敏感，以及<em class="ng">方差</em>，这意味着模型对特定于训练数据的信号过于敏感，不会泛化样本外。建模是90%的数据准备，另一半是寻找<a class="ae lb" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">最佳偏差-方差权衡</a>。</p><p id="9441" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">超参数帮助您调整偏差-方差权衡。对于预测泰坦尼克号存活率的简单逻辑回归，正则化参数允许您通过惩罚对任何单个特征的敏感性来控制过度拟合。对于进行机器翻译的大规模神经网络，除了正则化之外，层、单元、激活函数的数量和类型都是超参数。我们使用<a class="ae lb" href="https://machinelearningmastery.com/k-fold-cross-validation/" rel="noopener ugc nofollow" target="_blank"> <em class="ng"> k重交叉验证</em> </a>选择最佳超参数；这就是我们所说的超参数调谐。</p><p id="75ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在这篇文章中使用的回归算法是XGBoost和LightGBM，它们是<em class="ng">梯度推进</em>的变体。梯度推进是一种通常涉及决策树的集成方法。决策树构建规则，例如，如果乘客是头等舱的女性，他们可能在泰坦尼克号沉没后幸存下来。树是强大的，但是一个具有你所有特征的单一深度决策树将倾向于过度拟合训练数据。一种<em class="ng">随机森林</em>算法基于观察和特征的随机子集构建许多决策树，然后进行投票(<em class="ng"> bagging </em>)。由<em class="ng">弱学习者</em>投票的结果比在所有数据行和所有特征列上训练以生成单个强学习者更少过度拟合，并且在样本外执行得更好。随机森林超参数包括树的数量、树的深度以及每棵树应该使用多少个特征和观测值。</p><p id="8371" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ng">即</em> bagging，<em class="ng"> boosting </em>使用许多串联的学习器，而不是聚集许多并行工作的独立学习器:</p><ul class=""><li id="6682" class="ml mm iq kh b ki kj kl km ko nh ks ni kw nj la nk mt mu mv bi translated">从一个简单的估计开始，如中位数或基本利率。</li><li id="d635" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">将树拟合到该预测中的<em class="ng">误差</em>。</li><li id="0c96" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">如果你能<em class="ng">预测</em>误差，你就能<em class="ng">为其调整</em>并改善预测。调整预测不是一直到树预测，而是基于<em class="ng">学习率</em>(超参数)的一部分。</li><li id="8af5" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">将另一棵树拟合到更新预测中的误差，并基于学习率进一步调整预测。</li><li id="260f" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">迭代地继续减少特定数量的增强回合的误差(另一个超参数)。</li><li id="a872" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">最终估计值是初始预测加上所有预测的必要调整的总和(通过学习率加权)。</li></ul><p id="7753" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">学习率执行与随机森林中的投票类似的功能，在某种意义上，没有单个决策树确定太多的最终估计。这种“群体智慧”的方法有助于防止过度拟合。</p><p id="b5e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">梯度推进是对传统结构化表格数据进行回归和分类的当前技术状态(与图像/视频/自然语言处理等结构化程度较低的数据相反，在这些数据中，深度学习、<em class="ng">即</em>深度神经网络是当前技术状态)。</p><p id="cc7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像<a class="ae lb" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>、<a class="ae lb" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>和<a class="ae lb" href="https://catboost.ai/docs/concepts/python-reference_parameters-list.html" rel="noopener ugc nofollow" target="_blank"> CatBoost </a>这样的梯度增强算法有非常大量的超参数，调整是使用它们的重要部分。</p><p id="fb8f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些是<a class="ae lb" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" rel="noopener ugc nofollow" target="_blank">超参数调谐的主要方法</a>:</p><ul class=""><li id="55b8" class="ml mm iq kh b ki kj kl km ko nh ks ni kw nj la nk mt mu mv bi translated"><em class="ng">网格搜索</em>:给定每个超参数的有限组离散值，彻底交叉验证所有组合。</li><li id="632a" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng">随机搜索</em>:给定每个超参数的离散或连续分布，从联合分布中随机抽样。一般来说<a class="ae lb" href="https://dl.acm.org/doi/10.5555/2188385.2188395" rel="noopener ugc nofollow" target="_blank">比穷举网格搜索更有效。</a></li><li id="1e1b" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng">贝叶斯优化</em>:类似随机搜索的采样，但是根据先前搜索的结果，更新你采样的搜索空间。</li><li id="c6d3" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng">基于梯度的优化</em>:尝试估计交叉验证指标相对于超参数的梯度，并提升/降低梯度。</li><li id="4e7d" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng">进化优化</em>:对搜索空间进行采样，丢弃度量差的组合，基于成功的组合遗传进化出新的组合。</li><li id="d033" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng">基于群体的训练</em>:一种在训练的同时进行超参数优化的方法。</li></ul><p id="8814" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们将重点讨论使用Hyperopt和Optuna的贝叶斯优化。</p><h2 id="a7dd" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">3.贝叶斯优化</h2><p id="6eee" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">什么是贝叶斯优化？当我们执行网格搜索时，搜索空间是一个<a class="ae lb" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank">先验</a>:我们相信最好的超参数向量在这个搜索空间中。并且<em class="ng">先验地</em>也许每个超参数组合有相等的概率成为最佳组合(均匀分布)。所以我们把它们都试了一遍，然后选出最好的一个。</p><p id="4c66" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也许我们可以进行两次网格搜索。在一个宽的、粗略间隔的网格上进行初始搜索之后，我们在第一遍的最佳度量周围的更小区域中用更精细间隔的网格进行更深入的探索。在贝叶斯术语中，我们<em class="ng">更新了我们之前的</em>。</p><p id="fbc9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">贝叶斯优化从随机抽样开始，<em class="ng">例如</em> 30个组合，并使用<a class="ae lb" href="https://machinelearningmastery.com/k-fold-cross-validation/" rel="noopener ugc nofollow" target="_blank"> <em class="ng"> k倍交叉验证</em> </a>为30个随机抽样的组合中的每一个计算交叉验证度量。然后，该算法更新它从中采样的分布，以便它更有可能对类似于好的度量的组合进行采样，而不太可能对类似于差的度量的组合进行采样。当它继续采样时，它会根据找到的指标继续更新采样的搜索分布。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/e6e8d67c567adad372bcf10dcf7b8a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xfnh-biDrMCECEO37qecKQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者图片</p></figure><p id="39da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好的度量标准通常不是均匀分布的。如果在高斯分布或我们可以建模的任何分布中发现它们彼此接近，那么贝叶斯优化可以利用潜在的模式，并且可能比网格搜索或朴素随机搜索更有效。</p><p id="f6f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="http://hyperopt.github.io/hyperopt/" rel="noopener ugc nofollow" target="_blank"> HyperOpt </a>是由<a class="ae lb" href="https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf" rel="noopener ugc nofollow" target="_blank"> James Bergstra等人</a>提出的贝叶斯优化算法，参见Subir Mansukhani 的这篇<a class="ae lb" href="https://blog.dominodatalab.com/hyperopt-bayesian-hyperparameter-optimization/" rel="noopener ugc nofollow" target="_blank">优秀博文。</a></p><p id="45e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://optuna.org/" rel="noopener ugc nofollow" target="_blank"> Optuna </a>是由<a class="ae lb" href="https://arxiv.org/abs/1907.10902" rel="noopener ugc nofollow" target="_blank"> Takuya Akiba等人</a>提出的贝叶斯优化算法，参见这篇由Crissman Loomis 撰写的<a class="ae lb" href="https://medium.com/optuna/using-optuna-to-optimize-xgboost-hyperparameters-63bfcdfd3407" rel="noopener">优秀博文。</a></p><h2 id="a8a8" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">4.提前停止</h2><p id="5033" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">如果在评估超参数组合时，评估度量在训练中没有改善，或者改善的速度不够快，不足以击败我们迄今为止的最好成绩，我们可以在对其进行完全训练之前丢弃该组合。<em class="ng">尽早停止</em>不成功的训练跑可以提高我们搜索的速度和效率。</p><p id="cbd6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">XGBoost和LightGBM有助于提供提前停止回调来检查训练进度并提前停止训练试验(<a class="ae lb" href="https://xgboost.readthedocs.io/en/latest/python/callbacks.html" rel="noopener ugc nofollow" target="_blank">XGBoost</a>；<a class="ae lb" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.early_stopping.html#lightgbm.early_stopping" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>)。Hyperopt、Optuna和Ray使用这些回调来快速停止糟糕的试验并提高性能。</p><p id="fc2f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本帖中，我们将使用<a class="ae lb" href="https://arxiv.org/abs/1810.05934" rel="noopener ugc nofollow" target="_blank">异步连续减半算法(ASHA) </a>进行提前停止，如<a class="ae lb" href="https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/" rel="noopener ugc nofollow" target="_blank">博文</a>中所述。</p><h2 id="84c2" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">延伸阅读:</h2><ul class=""><li id="d7a1" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la nk mt mu mv bi translated"><a class="ae lb" href="https://arxiv.org/abs/2003.05689" rel="noopener ugc nofollow" target="_blank">超参数优化:算法与应用综述</a>佟宇，朱红(2020)</li><li id="5897" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><a class="ae lb" href="https://arxiv.org/abs/1502.02127v2" rel="noopener ugc nofollow" target="_blank">机器学习中的超参数搜索</a>，Marc Claesen，Bart De Moor (2015)</li><li id="6ef4" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><a class="ae lb" href="https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1" rel="noopener ugc nofollow" target="_blank">超参数优化</a>，马蒂亚斯·福雷尔，弗兰克·哈特(2019)</li></ul><h2 id="ec19" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">5.实施细节</h2><p id="d130" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">我们使用来自艾姆斯住宅数据集的数据。原始数据集有79个原始特征。我们将使用的数据有100个特征，其中相当数量的特征工程来自我自己在建模中的尝试<a class="ae lb" href="https://github.com/druce/iowa" rel="noopener ugc nofollow" target="_blank">，当我将它提交给Kaggle时，它位于前5%左右。我们对销售价格的日志进行建模，并使用RMSE作为模型选择的度量标准。为了更容易理解，我们将RMSE转换回原始美元单位。</a></p><p id="4ad3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用4种回归算法:</p><ul class=""><li id="9dcf" class="ml mm iq kh b ki kj kl km ko nh ks ni kw nj la nk mt mu mv bi translated"><em class="ng">线性回归</em>:无超参数的基线</li><li id="f127" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng">elastic net</em>:L1和L2正则化的线性回归(2个超参数)。</li><li id="c199" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng"> XGBoost </em></li><li id="d481" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng"> LightGBM </em></li></ul><p id="7da6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用5种方法:</p><ul class=""><li id="7b3e" class="ml mm iq kh b ki kj kl km ko nh ks ni kw nj la nk mt mu mv bi translated"><em class="ng">原生CV </em>:在sklearn中，如果一个算法<em class="ng"> xxx </em>有超参数，那么它通常会有一个<em class="ng"> xxxCV </em>版本，比如ElasticNetCV，它使用指定的kfolds对超参数迭代器执行自动网格搜索。</li><li id="3ca8" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng"> GridSearchCV </em>:抽象网格搜索，可以环绕任何sklearn算法，在指定的kfolds上运行多线程试验。</li><li id="39b3" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng">手动顺序网格搜索</em>:我们通常如何用XGBoost实现网格搜索，XGBoost与GridSearchCV配合得不是很好，并且有太多的超参数需要一次调优。</li><li id="28e9" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng">本地桌面上的光线调整</em>:ASHA提前停止的远视和Optuna。</li><li id="1ae9" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><em class="ng">AWS集群上的光线调节</em>:额外扩展以在集群中的许多实例上运行单个超参数优化任务。</li></ul><h2 id="c777" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">6.基线线性回归</h2><ul class=""><li id="667c" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la nk mt mu mv bi translated">每次运行使用相同的kfold，这样RMSE度量的变化就不是由于kfold的变化造成的。</li><li id="dc5c" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">我们拟合了对数响应，因此为了便于解释，我们将误差转换回美元单位。</li><li id="55c7" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><code class="fe nl nm nn no b">sklearn.model_selection.cross_val_score</code>进行评估</li><li id="0d6d" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">墙壁时间的魔法</li><li id="4187" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><code class="fe nl nm nn no b">n_jobs=-1</code>使用所有可用的CPU内核并行运行折叠。</li><li id="b82d" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">注意墙壁时间&lt; 1 second and RMSE of 18192.</li><li id="6162" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">Full notebooks are on <a class="ae lb" href="https://github.com/druce/iowa/blob/master/hyperparameter_optimization.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</li></ul><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="b256" class="ls lt iq no b gy nt nu l nv nw">%%time<br/><br/># always use same RANDOM_STATE k-folds for comparability between tests, reproducibility<br/>RANDOMSTATE = 42<br/>np.random.seed(RANDOMSTATE)<br/><br/>kfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOMSTATE)<br/><br/>MEAN_RESPONSE=df[response].mean()<br/>def cv_to_raw(cv_val, mean_response=MEAN_RESPONSE):<br/>    """convert log1p rmse to underlying SalePrice error"""<br/>    # MEAN_RESPONSE assumes folds have same mean response, which is true in expectation but not in each fold<br/>    # we can also pass the mean response for each fold<br/>    # but we're really just looking to consistently convert the log value to a more meaningful unit<br/>    return np.expm1(mean_response+cv_val) - np.expm1(mean_response)<br/>    <br/>lr = LinearRegression()<br/># compute CV metric for each fold<br/>scores = -cross_val_score(lr, df[predictors], df[response],<br/>                          scoring="neg_root_mean_squared_error",<br/>                          cv=kfolds,<br/>                          n_jobs=-1)<br/>raw_scores = [cv_to_raw(x) for x in scores]<br/>print("Raw CV RMSE %.0f (STD %.0f)" % (np.mean(raw_scores), np.std(raw_scores)))</span></pre><p id="9d17" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nl nm nn no b">Raw CV RMSE 18192 (STD 1839)</code></p><p id="d7d8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nl nm nn no b">Wall time: 65.4 ms</code></p><h2 id="0ae4" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">7.弹性电视</h2><ul class=""><li id="6918" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la nk mt mu mv bi translated">ElasticNet是使用L1和L2 <a class="ae lb" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">正则化</a> (2个超参数)的线性回归。</li><li id="0e7e" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">当我们使用正则化时，我们需要缩放我们的数据，以便系数惩罚对各要素产生类似的影响。我们使用带有鲁棒定标器的流水线来定标。</li><li id="ef1a" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">拟合模型并从拟合的模型中提取超参数。</li><li id="e2d2" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">然后我们用报告的超参数做<code class="fe nl nm nn no b">cross_val_score</code>(似乎没有办法在不重新调整的情况下从拟合的模型中提取分数)</li><li id="3bd7" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">详细输出报告130个任务，对于10倍的全网格搜索，我们预计13x9x10=1170。显然是一个聪明的优化。</li><li id="ec1b" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">请注意，与没有正则化的线性回归相比，RMSE略有降低。</li></ul><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="fa6f" class="ls lt iq no b gy nt nu l nv nw">elasticnetcv = make_pipeline(RobustScaler(),<br/>                             ElasticNetCV(max_iter=100000, <br/>                                          l1_ratio=[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],<br/>                                          alphas=np.logspace(-4, -2, 9),<br/>                                          cv=kfolds,<br/>                                          n_jobs=-1,<br/>                                          verbose=1,<br/>                                         ))<br/><br/>#train and get hyperparams<br/>elasticnetcv.fit(df[predictors], df[response])<br/>l1_ratio = elasticnetcv._final_estimator.l1_ratio_<br/>alpha = elasticnetcv._final_estimator.alpha_<br/>print('l1_ratio', l1_ratio)<br/>print('alpha', alpha)<br/><br/># evaluate using kfolds on full dataset<br/># I don't see API to get CV error from elasticnetcv, so we use cross_val_score<br/>elasticnet = ElasticNet(alpha=alpha,<br/>                        l1_ratio=l1_ratio,<br/>                        max_iter=10000)<br/><br/>scores = -cross_val_score(elasticnet, df[predictors], df[response],<br/>                          scoring="neg_root_mean_squared_error",<br/>                          cv=kfolds,<br/>                          n_jobs=-1)<br/>raw_scores = [cv_to_raw(x) for x in scores]<br/>print()<br/>print("Log1p CV RMSE %.04f (STD %.04f)" % (np.mean(scores), np.std(scores)))<br/>print("Raw CV RMSE %.0f (STD %.0f)" % (np.mean(raw_scores), np.std(raw_scores)))</span><span id="ee71" class="ls lt iq no b gy nx nu l nv nw">l1_ratio 0.01<br/>alpha 0.0031622776601683794<br/><br/>Log1p CV RMSE 0.1030 (STD 0.0109)<br/>Raw CV RMSE 18061 (STD 2008)<br/>CPU times: user 5.93 s, sys: 3.67 s, total: 9.6 s<br/>Wall time: 1.61 s</span></pre><h2 id="656c" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">8.GridSearchCV</h2><ul class=""><li id="8cfc" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la nk mt mu mv bi translated">相同的结果，运行速度稍慢。</li><li id="9fbf" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">GridSearchCV详细输出显示1170个作业，这是预期的13x9x10个作业。</li></ul><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="61cb" class="ls lt iq no b gy nt nu l nv nw">gs = make_pipeline(RobustScaler(),<br/>                   GridSearchCV(ElasticNet(max_iter=100000),<br/>                                param_grid={'l1_ratio': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],<br/>                                            'alpha': np.logspace(-4, -2, 9),<br/>                                           },<br/>                                scoring='neg_root_mean_squared_error',<br/>                                refit=True,<br/>                                cv=kfolds,<br/>                                n_jobs=-1,<br/>                                verbose=1<br/>                               ))<br/><br/># do cv using kfolds on full dataset<br/>gs.fit(df[predictors], df[response])<br/>print('best params', gs._final_estimator.best_params_)<br/>print('best score', -gs._final_estimator.best_score_)<br/>l1_ratio = gs._final_estimator.best_params_['l1_ratio']<br/>alpha = gs._final_estimator.best_params_['alpha']<br/><br/># eval similarly to before<br/>elasticnet = ElasticNet(alpha=alpha,<br/>                        l1_ratio=l1_ratio,<br/>                        max_iter=100000)<br/>print(elasticnet)<br/><br/>scores = -cross_val_score(elasticnet, df[predictors], df[response],<br/>                          scoring="neg_root_mean_squared_error",<br/>                          cv=kfolds,<br/>                          n_jobs=-1)<br/>raw_scores = [cv_to_raw(x) for x in scores]<br/>print()<br/>print("Log1p CV RMSE %.06f (STD %.04f)" % (np.mean(scores), np.std(scores)))<br/>print("Raw CV RMSE %.0f (STD %.0f)" % (np.mean(raw_scores), np.std(raw_scores)))</span><span id="fc9d" class="ls lt iq no b gy nx nu l nv nw">best params {'alpha': 0.0031622776601683794, 'l1_ratio': 0.01}<br/>best score 0.10247177583755482<br/>ElasticNet(alpha=0.0031622776601683794, l1_ratio=0.01, max_iter=100000)<br/><br/>Log1p CV RMSE 0.103003 (STD 0.0109)<br/>Raw CV RMSE 18061 (STD 2008)<br/><br/>Wall time: 5 s</span></pre><h2 id="dafa" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">9.使用顺序网格搜索的XGBoost</h2><p id="4028" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated"><em class="ng">应该</em>可以使用GridSearchCV和XGBoost。但是当我们也尝试使用早期停止时，XGBoost需要一个eval集。好，我们可以给它一个从GridSearchCV得到的静态评估集。现在，GridSearchCV在训练集中进行k重交叉验证，但XGBoost使用单独的专用评估集来提前停止。这有点像弗兰肯斯坦方法论。如果您真的感兴趣，请查看<a class="ae lb" href="https://github.com/druce/iowa/blob/master/hyperparameter_optimization.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>了解使用XGBoost和早期停止的GridSearchCV尝试。</p><p id="00ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相反，我们编写自己的网格搜索，为XGBoost提供每个CV折叠的正确保持集:</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="ab30" class="ls lt iq no b gy nt nu l nv nw">EARLY_STOPPING_ROUNDS=100  # stop if no improvement after 100 rounds<br/><br/>def my_cv(df, predictors, response, kfolds, regressor, verbose=False):<br/>    """Roll our own CV <br/>    train each kfold with early stopping<br/>    return average metric, sd over kfolds, average best round"""<br/>    metrics = []<br/>    best_iterations = []<br/><br/>    for train_fold, cv_fold in kfolds.split(df): <br/>        fold_X_train=df[predictors].values[train_fold]<br/>        fold_y_train=df[response].values[train_fold]<br/>        fold_X_test=df[predictors].values[cv_fold]<br/>        fold_y_test=df[response].values[cv_fold]<br/>        regressor.fit(fold_X_train, fold_y_train,<br/>                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,<br/>                      eval_set=[(fold_X_test, fold_y_test)],<br/>                      eval_metric='rmse',<br/>                      verbose=verbose<br/>                     )<br/>        y_pred_test=regressor.predict(fold_X_test)<br/>        metrics.append(np.sqrt(mean_squared_error(fold_y_test, y_pred_test)))<br/>        best_iterations.append(regressor.best_iteration)<br/>    return np.average(metrics), np.std(metrics), np.average(best_iterations)</span></pre><p id="652f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">XGBoost有许多调优参数，因此穷举网格搜索的组合数量不合理。取而代之的是，我们使用网格搜索和提前停止来依次调优精简集。</p><p id="1979" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是调优XGBoost的典型网格搜索方法:</p><h2 id="5647" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">XGBoost调优方法</h2><ul class=""><li id="dc19" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la nk mt mu mv bi translated">设置初始的一组启动参数。</li><li id="8427" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">按顺序调整各组之间交互不太多的超参数，以减少测试的组合数量。</li><li id="fc58" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">先调好<code class="fe nl nm nn no b">max_depth</code>。</li><li id="7606" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">然后调<code class="fe nl nm nn no b">subsample</code>、<code class="fe nl nm nn no b">colsample_bytree</code>、<code class="fe nl nm nn no b">colsample_bylevel</code>。</li><li id="44c5" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">最后，调整<code class="fe nl nm nn no b">learning rate</code>:较低的学习率将需要更多的助推轮(<code class="fe nl nm nn no b">n_estimators</code>)。</li><li id="f901" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">对每个超参数组合进行10重交叉验证。选择超参数以最小化kfolds上的平均RMSE。</li><li id="6833" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">如果100轮后没有改善，使用XGboost早期停止停止每一轮的训练。</li><li id="8a0c" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">在调优和选择最佳超参数后，使用跨xval kfolds的平均增强回合，在完整数据集上重新训练和评估，而不提前停止。</li><li id="9b63" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">如前所述，我们使用XGBoost sklearn API，并使用我们自己的网格搜索，它理解使用k-folds而不是GridSearchCV的早期停止。(另一种方法是使用原生xgboost。理解早期停止但不使用sklearn API的cv(使用DMatrix，而不是numpy数组或dataframe))</li><li id="89a9" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">我们编写一个助手函数<code class="fe nl nm nn no b">cv_over_param_dict</code>，它获取一个<code class="fe nl nm nn no b">param_dict</code>字典列表，对所有字典进行测试，并返回最佳的<code class="fe nl nm nn no b">param_dict</code>字典和一个结果数据帧。</li><li id="54c5" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">我们运行<code class="fe nl nm nn no b">cv_over_param_dict</code> 3次，在3轮调优中进行3次网格搜索。</li></ul><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="b9d3" class="ls lt iq no b gy nt nu l nv nw">BOOST_ROUNDS=50000   # we use early stopping so make this arbitrarily high<br/><br/>def cv_over_param_dict(df, param_dict, predictors, response, kfolds, verbose=False):<br/>    """given a list of dictionaries of xgb params<br/>    run my_cv on params, store result in array<br/>    return updated param_dict, results dataframe<br/>    """<br/>    start_time = datetime.now()<br/>    print("%-20s %s" % ("Start Time", start_time))<br/><br/>    results = []<br/><br/>    for i, d in enumerate(param_dict):<br/>        xgb = XGBRegressor(<br/>            objective='reg:squarederror',<br/>            n_estimators=BOOST_ROUNDS,<br/>            random_state=RANDOMSTATE,    <br/>            verbosity=1,<br/>            n_jobs=-1,<br/>            booster='gbtree',   <br/>            **d<br/>        )    <br/><br/>        metric_rmse, metric_std, best_iteration = my_cv(df, predictors, response, kfolds, xgb, verbose=False)    <br/>        results.append([metric_rmse, metric_std, best_iteration, d])<br/>    <br/>        print("%s %3d result mean: %.6f std: %.6f, iter: %.2f" % (datetime.strftime(datetime.now(), "%T"), i, metric_rmse, metric_std, best_iteration))<br/>        <br/>    end_time = datetime.now()<br/>    print("%-20s %s" % ("Start Time", start_time))<br/>    print("%-20s %s" % ("End Time", end_time))<br/>    print(str(timedelta(seconds=(end_time-start_time).seconds)))<br/>    <br/>    results_df = pd.DataFrame(results, columns=['rmse', 'std', 'best_iter', 'param_dict']).sort_values('rmse')<br/>    display(results_df.head())<br/>    <br/>    best_params = results_df.iloc[0]['param_dict']<br/>    return best_params, results_df<br/><br/># initial hyperparams<br/>current_params = {<br/>    'max_depth': 5,<br/>    'colsample_bytree': 0.5,<br/>    'colsample_bylevel': 0.5,<br/>    'subsample': 0.5,<br/>    'learning_rate': 0.01,<br/>}<br/><br/>##################################################<br/># round 1: tune depth<br/>##################################################<br/>max_depths = list(range(2,8))<br/>grid_search_dicts = [{'max_depth': md} for md in max_depths]<br/># merge into full param dicts<br/>full_search_dicts = [{**current_params, **d} for d in grid_search_dicts]<br/><br/># cv and get best params<br/>current_params, results_df = cv_over_param_dict(df, full_search_dicts, predictors, response, kfolds)<br/><br/>##################################################<br/># round 2: tune subsample, colsample_bytree, colsample_bylevel<br/>##################################################<br/># subsamples = np.linspace(0.01, 1.0, 10)<br/># colsample_bytrees = np.linspace(0.1, 1.0, 10)<br/># colsample_bylevel = np.linspace(0.1, 1.0, 10)<br/># narrower search<br/>subsamples = np.linspace(0.25, 0.75, 11)<br/>colsample_bytrees = np.linspace(0.1, 0.3, 5)<br/>colsample_bylevel = np.linspace(0.1, 0.3, 5)<br/># subsamples = np.linspace(0.4, 0.9, 11)<br/># colsample_bytrees = np.linspace(0.05, 0.25, 5)<br/><br/>grid_search_dicts = [dict(zip(['subsample', 'colsample_bytree', 'colsample_bylevel'], [a, b, c])) <br/>                     for a,b,c in product(subsamples, colsample_bytrees, colsample_bylevel)]<br/># merge into full param dicts<br/>full_search_dicts = [{**current_params, **d} for d in grid_search_dicts]<br/># cv and get best params<br/>current_params, results_df = cv_over_param_dict(df, full_search_dicts, predictors, response, kfolds)<br/><br/># round 3: learning rate<br/>learning_rates = np.logspace(-3, -1, 5)<br/>grid_search_dicts = [{'learning_rate': lr} for lr in learning_rates]<br/># merge into full param dicts<br/>full_search_dicts = [{**current_params, **d} for d in grid_search_dicts]<br/><br/># cv and get best params<br/>current_params, results_df = cv_over_param_dict(df, full_search_dicts, predictors, response, kfolds, verbose=False)</span></pre><p id="bee9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总训练持续时间(3次迭代的时间总和)为1:24:22。这个时间可能被低估了，因为这个搜索空间是基于以前的经验。</p><p id="d4ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们使用最佳超参数进行改装，并评估:</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="88f0" class="ls lt iq no b gy nt nu l nv nw">xgb = XGBRegressor(<br/>    objective='reg:squarederror',<br/>    n_estimators=3438,<br/>    random_state=RANDOMSTATE,    <br/>    verbosity=1,<br/>    n_jobs=-1,<br/>    booster='gbtree',   <br/>    **current_params<br/>)    <br/><br/>print(xgb)<br/><br/>scores = -cross_val_score(xgb, df[predictors], df[response],<br/>                          scoring="neg_root_mean_squared_error",<br/>                          cv=kfolds,<br/>                          n_jobs=-1)<br/><br/>raw_scores = [cv_to_raw(x) for x in scores]<br/>print()<br/>print("Log1p CV RMSE %.06f (STD %.04f)" % (np.mean(scores), np.std(scores)))<br/>print("Raw CV RMSE %.0f (STD %.0f)" % (np.mean(raw_scores), np.std(raw_scores)))</span></pre><p id="f5bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果基本上与线性回归匹配，但不如ElasticNet好。</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="293a" class="ls lt iq no b gy nt nu l nv nw">Raw CV RMSE 18193 (STD 2461)</span></pre><h2 id="09c4" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">10.XGBoost带有远视、Optuna和光线</h2><p id="e035" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">使用Hyperopt运行光线调整作业的步骤如下:</p><ol class=""><li id="13ed" class="ml mm iq kh b ki kj kl km ko nh ks ni kw nj la ms mt mu mv bi translated">建立一个光线搜索空间作为配置字典。</li><li id="534d" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">将训练循环重构为一个函数，该函数将配置字典作为参数，并调用<code class="fe nl nm nn no b">tune.report(rmse=rmse)</code>来优化像RMSE这样的指标。</li><li id="b2f3" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">用<code class="fe nl nm nn no b">config</code>和一个指定采样次数的<code class="fe nl nm nn no b">num_samples</code>参数调用<code class="fe nl nm nn no b">ray.tune</code>。</li></ol><p id="7aa3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设置光线搜索空间:</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="15d1" class="ls lt iq no b gy nt nu l nv nw">xgb_tune_kwargs = {<br/>    "n_estimators": tune.loguniform(100, 10000),<br/>    "max_depth": tune.randint(0, 5),<br/>    "subsample": tune.quniform(0.25, 0.75, 0.01),<br/>    "colsample_bytree": tune.quniform(0.05, 0.5, 0.01),<br/>    "colsample_bylevel": tune.quniform(0.05, 0.5, 0.01),    <br/>    "learning_rate": tune.quniform(-3.0, -1.0, 0.5),  # powers of 10<br/>}<br/><br/>xgb_tune_params = [k for k in xgb_tune_kwargs.keys() if k != 'wandb']<br/>xgb_tune_params</span></pre><p id="15cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设置培训功能。注意，一些搜索算法期望所有的超参数都是浮点数，并且一些搜索区间从0开始。所以我们根据需要转换参数。</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="d5f8" class="ls lt iq no b gy nt nu l nv nw">def my_xgb(config):<br/><br/>    # fix these configs to match calling convention<br/>    # search wants to pass in floats but xgb wants ints<br/>    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int<br/>    # hyperopt needs left to start at 0 but we want to start at 2    <br/>    config['max_depth'] = int(config['max_depth']) + 2<br/>    config['learning_rate'] = 10 ** config['learning_rate']<br/>    <br/>    xgb = XGBRegressor(<br/>        objective='reg:squarederror',<br/>        n_jobs=1,<br/>        random_state=RANDOMSTATE,<br/>        booster='gbtree',   <br/>        scale_pos_weight=1, <br/>        **config,<br/>    )<br/>    scores = -cross_val_score(xgb, df[predictors], df[response],<br/>                                      scoring="neg_root_mean_squared_error",<br/>                                      cv=kfolds)<br/>    rmse = np.mean(scores)<br/>    tune.report(rmse=rmse)<br/>    <br/>    return {"rmse": rmse}</span></pre><p id="1f11" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行光线调节:</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="c0e4" class="ls lt iq no b gy nt nu l nv nw">algo = HyperOptSearch(random_state_seed=RANDOMSTATE)<br/># ASHA<br/>scheduler = AsyncHyperBandScheduler()<br/><br/>analysis = tune.run(my_xgb,<br/>                    num_samples=NUM_SAMPLES,<br/>                    config=xgb_tune_kwargs,                    <br/>                    name="hyperopt_xgb",<br/>                    metric="rmse",<br/>                    mode="min",<br/>                    search_alg=algo,<br/>                    scheduler=scheduler,<br/>                    verbose=1,<br/>                   )</span></pre><p id="3ad5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">提取最佳超参数，并使用它们评估模型:</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="ce81" class="ls lt iq no b gy nt nu l nv nw"># results dataframe sorted by best metric<br/>param_cols = ['config.' + k for k in xgb_tune_params]<br/>analysis_results_df = analysis.results_df[['rmse', 'date', 'time_this_iter_s'] + param_cols].sort_values('rmse')<br/><br/># extract top row<br/>best_config = {z: analysis_results_df.iloc[0]['config.' + z] for z in xgb_tune_params}<br/><br/>xgb = XGBRegressor(<br/>    objective='reg:squarederror',<br/>    random_state=RANDOMSTATE,    <br/>    verbosity=1,<br/>    n_jobs=-1,<br/>    **best_config<br/>)<br/>print(xgb)<br/><br/>scores = -cross_val_score(xgb, df[predictors], df[response],<br/>                          scoring="neg_root_mean_squared_error",<br/>                          cv=kfolds)<br/><br/>raw_scores = [cv_to_raw(x) for x in scores]<br/>print()<br/>print("Log1p CV RMSE %.06f (STD %.04f)" % (np.mean(scores), np.std(scores)))<br/>print("Raw CV RMSE %.0f (STD %.0f)" % (np.mean(raw_scores), np.std(raw_scores)))</span></pre><p id="d3f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于NUM_SAMPLES=1024，我们得到:</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="6b8b" class="ls lt iq no b gy nt nu l nv nw">Raw CV RMSE 18309 (STD 2428)</span></pre><p id="9cb8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以简单地将远视换成Optuna:</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="e71f" class="ls lt iq no b gy nt nu l nv nw">algo = OptunaSearch()</span></pre><p id="1e48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于NUM_SAMPLES=1024，我们得到:</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="b391" class="ls lt iq no b gy nt nu l nv nw">Raw CV RMSE 18325 (STD 2473)</span></pre><h2 id="094c" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">11.配有远视和光学镜的LightGBM</h2><p id="342b" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">我们也可以很容易地用LightGBM替换XGBoost。</p><ol class=""><li id="488e" class="ml mm iq kh b ki kj kl km ko nh ks ni kw nj la ms mt mu mv bi translated">使用<a class="ae lb" href="https://sites.google.com/view/lauraepp/parameters" rel="noopener ugc nofollow" target="_blank"> LightGBM等价物</a>更新搜索空间。</li></ol><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="2cf3" class="ls lt iq no b gy nt nu l nv nw">lgbm_tune_kwargs = {<br/>       "n_estimators": tune.loguniform(100, 10000),<br/>       "max_depth": tune.randint(0, 5),<br/>       'num_leaves': tune.quniform(1, 10, 1.0),               # xgb max_leaves<br/>       "bagging_fraction": tune.quniform(0.5, 0.8, 0.01),    # xgb subsample<br/>       "feature_fraction": tune.quniform(0.05, 0.5, 0.01),   # xgb colsample_bytree<br/>       "learning_rate": tune.quniform(-3.0, -1.0, 0.5),<br/>   }</span></pre><ol class=""><li id="785c" class="ml mm iq kh b ki kj kl km ko nh ks ni kw nj la ms mt mu mv bi translated">更新培训功能:</li></ol><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="62eb" class="ls lt iq no b gy nt nu l nv nw">def my_lgbm(config):<br/>    <br/>    # fix these configs <br/>    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int<br/>    config['num_leaves'] = int(2**config['num_leaves'])<br/>    config['learning_rate'] = 10**config['learning_rate']<br/>    <br/>    lgbm = LGBMRegressor(objective='regression',<br/>                         max_bin=200,<br/>                         feature_fraction_seed=7,<br/>                         min_data_in_leaf=2,<br/>                         verbose=-1,<br/>                         n_jobs=1,<br/>                         # these are specified to suppress warnings<br/>                         colsample_bytree=None,<br/>                         min_child_samples=None,<br/>                         subsample=None,<br/>                         **config,<br/>                         )<br/>    <br/>    scores = -cross_val_score(lgbm, df[predictors], df[response],<br/>                              scoring="neg_root_mean_squared_error",<br/>                              cv=kfolds)<br/>    rmse=np.mean(scores)  <br/>    tune.report(rmse=rmse)<br/>    <br/>    return {'rmse': np.mean(scores)}</span></pre><p id="f1d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和以前一样运行，用my_lgbm代替my_xgb。LGBM的结果:(样本数=1024):</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="bad4" class="ls lt iq no b gy nt nu l nv nw">Raw CV RMSE 18615 (STD 2356)</span></pre><p id="b74c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将远视换成Optuna:</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="4283" class="ls lt iq no b gy nt nu l nv nw">Raw CV RMSE 18614 (STD 2423)</span></pre><h2 id="c3ff" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">12.光线簇上的XGBoost</h2><p id="9104" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">Ray是一个分布式框架。我们可以使用具有一个<em class="ng">头节点</em>和许多<em class="ng">工作节点</em>的集群在许多实例上运行光线调整作业。</p><p id="0f3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">启动Ray非常简单。在头节点上，我们运行<code class="fe nl nm nn no b">ray start</code>。在每个worker节点上，我们使用head节点的地址运行<code class="fe nl nm nn no b">ray start --address x.x.x.x</code>。然后在python中我们调用<code class="fe nl nm nn no b">ray.init()</code>来连接头节点。其他一切都像以前一样进行，头节点使用集群中的所有实例运行试验，并将结果存储在Redis中。</p><p id="ab7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更复杂的地方是指定所有AWS细节、实例类型、区域、子网等。</p><ul class=""><li id="2920" class="ml mm iq kh b ki kj kl km ko nh ks ni kw nj la nk mt mu mv bi translated">集群在<code class="fe nl nm nn no b">ray1.1.yaml</code>中定义。(到目前为止，在这个笔记本中，我们一直使用当前的产品ray 1.0，但是我很难让一个集群运行ray 1.0，所以我切换到了dev nightly。YMMV。)</li><li id="7eb7" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated"><code class="fe nl nm nn no b">boto3</code>和AWS CLI配置的凭证用于产生实例，因此<a class="ae lb" href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html" rel="noopener ugc nofollow" target="_blank">安装和配置AWS CLI </a></li><li id="1f14" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">编辑<code class="fe nl nm nn no b">ray1.1.yaml</code>文件，至少包含您的AWS区域和可用性区域。Imageid可能因地区而异，搜索当前深度学习AMI (Ubuntu 18.04)。你可能不需要指定子网，我有一个问题，一个不可访问的子网，当我让雷默认子网，可能是错误的默认。</li><li id="8785" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">为了获得这些变量，将最新的深度学习AMI (Ubuntu 18.04)当前版本35.0启动到您最喜欢的地区/区域的一个小实例中</li><li id="101f" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">测试它的工作情况</li><li id="8877" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">请注意这4个变量:区域、可用性区域、子网、AMI imageid</li><li id="b326" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">终止实例并使用您的区域、可用性区域、AMI imageid、可选子网编辑<code class="fe nl nm nn no b">ray1.1.yaml</code></li><li id="c9d6" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">建议您创建自己的映像，预安装所有更新和要求，并指定其AMI imageid，而不是使用通用映像并在启动时安装所有内容。</li><li id="b6ed" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">运行集群:<code class="fe nl nm nn no b">ray up ray1.1.yaml</code></li><li id="1a00" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">使用指定的AMI创建head实例。</li><li id="2937" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">安装Ray和相关需求，包括来自<code class="fe nl nm nn no b">requirements.txt</code>的XGBoost</li><li id="fec5" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">从GitHub克隆druce/iowa repo</li><li id="2753" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">根据自动扩展参数启动工作节点(目前我们固定节点数量，因为我们没有对集群自动扩展所需的时间进行基准测试)</li><li id="da32" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">集群启动后，您可以检查AWS控制台，并注意到启动了几个实例。</li><li id="3d75" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">检查<code class="fe nl nm nn no b">ray monitor ray1.1.yaml</code>是否有任何错误信息</li><li id="ff57" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">在带有端口转发的集群上运行Jupyter<code class="fe nl nm nn no b">ray exec ray1.1.yaml --port-forward=8899 'jupyter notebook --port=8899'</code></li><li id="3979" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">在启动时打印在控制台上的生成的URL上打开笔记本<em class="ng">例如</em> <a class="ae lb" href="http://localhost:8899/?token=5f46d4355ae7174524ba71f30ef3f0633a20b19a204b93b4" rel="noopener ugc nofollow" target="_blank"> http://localhost:8899/？token = 5f 46d 4355 AE 7174524 ba 71 f 30 ef 3 f 0633 a 20 b 19 a 204 b 93 b 4</a></li><li id="f964" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">您可以使用<code class="fe nl nm nn no b">ray attach /Users/drucev/projects/iowa/ray1.1.yaml</code>在集群的头节点上运行终端</li><li id="a836" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">您可以使用IP地址和生成的私有密钥<code class="fe nl nm nn no b">ssh -o IdentitiesOnly=yes -i ~/.ssh/ray-autoscaler_1_us-east-1.pem ubuntu@54.161.200.54</code>进行ssh</li><li id="1275" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">用<br/> <code class="fe nl nm nn no b">ray dashboard ray1.1.yaml</code>运行端口转发到Ray dashboard，然后打开<a class="ae lb" href="http://localhost:8265/" rel="noopener ugc nofollow" target="_blank"> http://localhost:8265/ </a></li><li id="a078" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">确保在Jupyter中选择默认内核，以便在正确的conda环境中运行所有安装</li><li id="1309" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">确保使用启动消息中给出的ray.init()命令。<code class="fe nl nm nn no b">ray.init(address='localhost:6379', _redis_password='5241590000000000')</code></li><li id="7e9c" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">该集群将产生AWS费用，因此<code class="fe nl nm nn no b">ray down ray1.1.yaml</code>完成后</li><li id="7258" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">请参见<a class="ae lb" href="https://github.com/druce/iowa/blob/master/hyperparameter_optimization_cluster.ipynb" rel="noopener ugc nofollow" target="_blank">hyperparameter _ optimization _ cluster . ipynb</a>，它是独立的，因此每台笔记本电脑都可以在有/没有集群设置的情况下端到端运行</li><li id="a84b" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nk mt mu mv bi translated">参见<a class="ae lb" href="https://docs.ray.io/en/latest/cluster/launcher.html" rel="noopener ugc nofollow" target="_blank">光线文档</a>了解更多关于光线簇的信息。</li></ul><p id="07a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了连接到集群而不是在本地运行Ray Tune之外，无需对代码进行任何其他更改即可在集群上运行</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="28b5" class="ls lt iq no b gy nt nu l nv nw">analysis = tune.run(my_xgb,<br/>                    num_samples=NUM_SAMPLES,<br/>                    config=xgb_tune_kwargs,                    <br/>                    name="hyperopt_xgb",<br/>                    metric="rmse",<br/>                    mode="min",<br/>                    search_alg=algo,<br/>                    scheduler=scheduler,<br/>                    # add this because distributed jobs occasionally error out<br/>                    raise_on_failed_trial=False, # otherwise no reults df returned if any trial error           <br/>                    verbose=1,<br/>                   )</span></pre><p id="cd92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">XGBM在集群上的结果(2048个样本，集群是32个m5 .大型实例):</p><p id="33b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">远视(时间1:30:58)</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="a39a" class="ls lt iq no b gy nt nu l nv nw">Raw CV RMSE 18030 (STD 2356)</span></pre><p id="e8f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">奥普图纳(时间1时29分57秒)</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="6384" class="ls lt iq no b gy nt nu l nv nw">Raw CV RMSE 18028 (STD 2353)</span></pre><h2 id="2a78" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">13.光线簇上的LightGBM</h2><p id="6df1" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">类似地，对于LightGBM:</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="80da" class="ls lt iq no b gy nt nu l nv nw">analysis = tune.run(my_lgbm,<br/>                    num_samples=NUM_SAMPLES,<br/>                    config = lgbm_tune_kwargs,<br/>                    name="hyperopt_lgbm",<br/>                    metric="rmse",<br/>                    mode="min",<br/>                    search_alg=algo,<br/>                    scheduler=scheduler,<br/>                    raise_on_failed_trial=False, # otherwise no reults df returned if any trial error                                                            <br/>                    verbose=1,<br/>                   )</span></pre><p id="9c50" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">集群上LightGBM的结果(2048个样本，集群是32个m5 .大型实例):</p><p id="d281" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">远视(时间:1:05:19):</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="9f26" class="ls lt iq no b gy nt nu l nv nw">Raw CV RMSE 18459 (STD 2511)</span></pre><p id="7803" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Optuna(时间0:48:16):</p><pre class="ld le lf lg gt np no nq nr aw ns bi"><span id="e80c" class="ls lt iq no b gy nt nu l nv nw">Raw CV RMSE 18458 (STD 2511)</span></pre><h2 id="863e" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">14.结束语</h2><p id="77e3" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">在我应用它们的每一个案例中，在我使用网格搜索方法找到的最佳指标中，Hyperopt和Optuna至少给了我一个小的改进。与顺序调优相比，贝叶斯优化的手动过程更少，调优速度更快。这是火了就忘了。</p><p id="1ea0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">射线调谐是超参数调谐的必由之路吗？暂时的，是的。Ray提供了底层ML ( <em class="ng">例如</em> XGBoost)、贝叶斯搜索(<em class="ng">例如</em>hyperpt)和早期停止(ASHA)之间的集成。它允许我们轻松地交换搜索算法。</p><p id="bd41" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae lb" href="https://docs.ray.io/en/master/tune/api_docs/suggestion.html" rel="noopener ugc nofollow" target="_blank"> Ray docs </a>中还有其他可供选择的搜索算法，但这些似乎是最流行的，我还没有让其他的运行。如果过了一段时间后，我发现我总是使用<em class="ng">，例如</em> Hyperopt，而从不使用集群，我可能会使用没有Ray的本机Hyperopt/XGBoost集成，以访问任何本机Hyperopt功能，因为它是堆栈中的一项少技术。</p><p id="6f6d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">集群？大多数时候，我没有必要，成本增加了，并没有看到预期的大幅度加速。我只在32个实例的集群上看到了大约2倍的加速。在设置测试时，我预计会有不到4倍的加速，这说明了线性扩展的不足。我试过的最长的一次运行，有4096个样本，在桌面上运行了一整夜。对于这个数据集，我的MacBook Pro w/16线程和12线程的台式机以及GPU已经足够强大了。尽管如此，在后面的口袋里有集群选项还是很有用的。在生产中，用<em class="ng">部署，例如</em> Terraform，Kubernetes可能比Ray原生YAML集群配置文件更标准，更易维护。如果你想大规模训练大数据，你需要真正了解并简化你的管道。</p><p id="ed9f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我感到惊讶的是，ElasticNet，<em class="ng">，即</em>正则化线性回归，在这个数据集上的表现略好于boosting。我大量设计了一些特性，使得线性方法工作得很好。使用Lasso/ElasticNet选择预测因子，我使用log和Box-Cox变换迫使预测因子遵循最小二乘假设。但是，boosting仍然被认为是表格数据的黄金标准。</p><p id="a5e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这可能会验证对机器学习的<a class="ae lb" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3624052" rel="noopener ugc nofollow" target="_blank">批评之一</a>，即最强大的机器学习方法不一定总是收敛到最佳解决方案。如果您有一个线性加噪声的基本事实，一个复杂的XGBoost或神经网络算法应该任意接近封闭形式的最优解，但可能永远不会精确匹配最优解。XGBoost回归是分段常数，复杂的神经网络易受随机梯度下降的影响。我以为任意接近意味着几乎无法区分。但显然情况并非总是如此。</p><p id="c13b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">具有L1 + L2正则化加上梯度下降和超参数优化的ElasticNet仍然是机器学习。它只是更好地匹配这个问题的ML的一种形式。在数据集与OLS假设不匹配的现实世界中，梯度推进通常表现得非常好。即使在这个数据集上，为线性模型的成功而设计的SVR和KernelRidge也比ElasticNet(未显示)表现得更好，并且将ElasticNet与XGBoost、LightGBM、SVR集成，神经网络的效果最好。</p><p id="efc5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">套用Casey Stengel的话，聪明的特征工程总是胜过聪明的模型算法，反之亦然。但是提高你的超参数总是会提高你的结果。贝叶斯优化可以被认为是一种最佳实践。</p><p id="c732" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，完整的代码在<a class="ae lb" href="https://github.com/druce/iowa" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上</p></div><div class="ab cl ny nz hu oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="ij ik il im in"><p id="162e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">单独调整停止回合会更合理。仅仅平均kfolds的最佳停止时间是有问题的。在真实的场景中，我们应该保留一个维持测试集。我们应该在完整的训练数据集(而不是kfolds)上重新训练，并尽早停止，以获得最佳的强化轮数。然后，我们应该使用所有交叉验证的参数在测试集中测量RMSE，包括预期的OOS RMSE的助推轮数。然而，为了比较调谐方法，CV误差是可以的。我们只是想看看如何使用CV进行模型决策，而不是太担心泛化错误。人们甚至可以说它给超参数选择的比较增加了一点噪声。但是在实践中，测试集将是正确的方法。这不会直接改变结论，我也不会重新做所有的事情，但是如果我要重新开始，我会那样做。</p><p id="94eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是没有意义的。</p></div></div>    
</body>
</html>