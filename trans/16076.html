<html>
<head>
<title>The math behind Gradient Descent and Backpropagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降和反向传播背后的数学原理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5?source=collection_archive---------16-----------------------#2020-11-05">https://towardsdatascience.com/the-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5?source=collection_archive---------16-----------------------#2020-11-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9ccf" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="7fea" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用Deeplearning4J的Java代码示例</h2></div><p id="1bf6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">本文介绍了两种算法:梯度下降法和反向传播法。我给出了它们如何工作的直觉，也给出了它们背后的数学原理的详细介绍。在本文的最后，我用Deeplearning4J给出了这个算法的Java实现。然而，它的目的不是介绍Deeplearning4J框架，而是实现文章中提出的所有概念。读完这篇文章后，你应该对这两种算法的工作原理和工作方式有一个清晰的理解，并且能够自己实现它们。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/72de5cbfe2bfa3211543f2da7ca63340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p1xxJXu39kedVekNFvoAyQ.jpeg"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="294d" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated"><strong class="ak">梯度下降</strong></h1><p id="05ea" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">通常情况下，我们希望最小化一个函数，比如说C。一种方法是计算导数，并尝试找到C的极值点。如果C的变量数量很少，这是一种可行的解决方案，但如果C的变量数量很多，就像机器学习算法经常遇到的情况一样，这就不再是一种实用的解决方案，我们需要另一种方法。</p><p id="6004" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们假设我们的函数如图1所示，一个随机点被选为红点。我们的目标是达到该函数的最小值或底部。那么，从我们最初的位置，我们应该去哪个方向才能到达谷底？直觉上，方向是指向最陡下方的方向。如果我们往那个方向走一点点，然后环顾四周，再次找到最陡的方向，再往那个方向走一点点，重复这个过程很多次，我们最终会到达谷底。但是怎么才能找到最陡下降的方向呢？</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f72222ed715d850c8ec3700c89f5b71c.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*HVGr7X2uucX5jOLsMguwng.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">图1: f(x，y)=x +y(图片由作者提供)</p></figure><h2 id="1fca" class="nf mi iq bd mj ng nh dn mn ni nj dp mr kx nk nl mt lb nm nn mv lf no np mx iw bi translated">方向导数</h2><p id="b43d" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">为了找到我们想要的方向，方向导数的概念将是有帮助的。让我们假设我们的函数C有两个变量<em class="nq"> x </em>和<em class="nq"> y </em>，我们想知道C在方向上移动时的瞬时变化率</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c514047c840aa75aafd6638cbb16908f.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*AkUZx0XmBkOYJCrLoA5Y8g.png"/></div></figure><p id="6df2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">C沿<em class="nq"> x </em>轴的瞬时变化率为</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/a9b54fb080e5e3ef854670a9f661880f.png" data-original-src="https://miro.medium.com/v2/resize:fit:72/format:webp/1*e8pTSGBiEGSdZfht0btA2g.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">C关于x的偏导数</p></figure><p id="9d28" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于y也是一样，向量v可以认为是x方向的v₁单位和y方向的v₂单位。因此，C沿矢量<em class="nq"> v </em>方向的方向导数为</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nt"><img src="../Images/b8a9f90319d7dd95b13975591bae3df6.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*-TwA4bCRVhi208rHzXVH9w.png"/></div></div></figure><p id="4c82" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">或者更简洁地说:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/5eceb80ffa6c88e6374a40f719eb471f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*zW1j3Cvvr4-VVlbRyEFNqg.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">C在v方向上的方向导数</p></figure><p id="a7eb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在哪里</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/6e1224c5895a1ee1be8412cb5223e1e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*0gJ569-cp-sVBGepHRN2wQ.png"/></div></figure><p id="d97e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，这不是C在v方向上的斜率。为了得到斜率，向量v的范数必须为1，因为v可以按比例放大，这将反映在等式(1)的值中。</p><p id="b306" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们回到我们最初的问题:如果我们在点w=(a，b)，向量v是什么使它指向最陡下降方向？当向v方向移动时，C的瞬时变化率必须是负的，绝对值最大，这是最陡的下降。所以，问题归结为找到:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nw"><img src="../Images/7e187db6e3deacb83e6ad5834b8b13ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OCchgH9FyUrWhk0fIBnMXQ.png"/></div></div></figure><p id="0d2b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们有两个向量∇ C(a，b)和v之间的点积，这是通过将向量v在∇ C(a，b)上的投影长度乘以∇ C(a，b)的长度来计算的(如果投影指向∇c(a b的相反方向，则它的长度为负值)。那么，向量v应该是多少才能使乘积最小呢？如果你向各个方向摆动v，很容易看出，当v指向与∇C(a相同的方向时，v的投影最大，而当v指向与∇C(a, b).相反的方向时，乘积最小所以，最陡下降的方向与∇C(a的方向相反。这就是我们要找的矢量v！</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6de67221036d37257f37c309737228a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*oZmnEXDidZ5wCc7A29opvw.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">图2最陡下降方向(图片由作者提供)</p></figure><p id="a7f7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了接近最小值，剩下要做的就是更新我们的位置直到收敛，如下所示:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b7b147d03a1037acdfa3d738aebd1c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*R9FaWie-nSxVckIrxuuMtw.png"/></div></figure><p id="5718" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中<strong class="kq ja"> η </strong>称为<strong class="kq ja">学习率</strong>。这就是<strong class="kq ja">梯度下降</strong>算法。这不仅适用于两个变量的函数，也适用于任意数量的变量。为了使梯度下降正确工作，必须仔细选择η。如果η太大，更新w时我们采取的步骤将会很大，我们可能会“错过”最小值。另一方面，如果η太小，梯度下降算法将运行缓慢。梯度下降算法的一个局限性是它容易陷入局部极小值，但许多机器学习成本函数是凸的，不存在局部极小值。</p><h2 id="cc7f" class="nf mi iq bd mj ng nh dn mn ni nj dp mr kx nk nl mt lb nm nn mv lf no np mx iw bi translated">批量梯度下降</h2><p id="94b7" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">在机器学习中，使用成本函数来测量模型的准确性。其中一个函数是<strong class="kq ja">均方误差</strong>或<strong class="kq ja"> MSE </strong>:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/eb9b3a17ffd60645079e9c4c0bb4798f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*9MUs6Rd2SEPG4iOm7mZZ6A.png"/></div></figure><p id="f990" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在此等式中，w和b表示模型中所有权重和偏差的集合，m是训练观测值的数量，ŷ(x是模型输出的值，y(x)是x的真实预测值。c是平方项的和，我们希望最小化其值，即C(w，b) ≈ 0。</p><p id="b5de" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了最小化C(w，b)的值，我们可以应用梯度下降算法。为此，我们可以从w和b的一些初始值开始，然后重复更新它们，直到它有希望收敛到最小化C(w，b)的值，如上面等式(3)中所述。然而，这种方法有一个缺点。在每次迭代中，我们必须计算梯度∇C.，但是为了计算∇C，我们必须计算每个训练样本的梯度∇Cₓ，然后对它们进行平均。当有大量训练观测值时，算法可能会很慢。为了加快速度，可以使用另一种叫做随机梯度下降的方法。</p><h2 id="b8ae" class="nf mi iq bd mj ng nh dn mn ni nj dp mr kx nk nl mt lb nm nn mv lf no np mx iw bi translated">随机梯度下降</h2><p id="e8a2" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">随机梯度下降的思想是一次仅使用一个训练观察来更新参数值。它的工作原理如下:</p><ol class=""><li id="60de" class="oa ob iq kq b kr ks ku kv kx oc lb od lf oe lj of og oh oi bi translated">随机“洗牌”数据集</li><li id="331a" class="oa ob iq kq b kr oj ku ok kx ol lb om lf on lj of og oh oi bi translated">对于i =1…m <br/> (w，b) ← (w，b)——η∇c x(I)(w，b)其中(w，b)是包含所有权重和偏差值的向量</li></ol><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/563891d2d7ca9245fca545d663b49901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*tvUssUuPktWM83barrU2ng.png"/></div></figure><p id="c93b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此算法不会扫描所有的训练观察值来更新模型参数，但它会尝试一次适合一个训练示例。随机梯度下降可能不会收敛到全局最小值，但它可以足够接近，是一个很好的近似。</p><p id="e813" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一种方法是不使用一个训练示例(如在随机梯度下降中)或所有训练示例(如在批量梯度下降中)，而是使用一些中间数量的观察值。这叫做<strong class="kq ja"> <em class="nq">小批量梯度下降</em> </strong>。</p><p id="4013" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设我们随机选择了k个训练样本，x，x … xᵏ.这是小批量的。有了足够大的小批量，小批量中梯度的平均值将接近整组训练样本的平均值，即:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi op"><img src="../Images/f0727c62a258c44cba2063e6c30c3e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kWFQzFopVxKeyRx6bmypNA.png"/></div></div></figure><p id="07d8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该算法的工作原理是随机选取一个小批量，并使用它们更新参数:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi oq"><img src="../Images/c426fdbaaf8cb433943d2cc3c94f9c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hfN6QbgmC2XY0CawHpI4Xw.png"/></div></div></figure><p id="6e5e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">重复这一过程，直到使用了所有的训练样本，从而完成了一个<em class="nq">时期</em>的训练。之后，我们可以按照相同的程序开始新的训练周期。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="0403" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">神经网络</h1><p id="da6a" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">分类问题的一种常见方法是使用sigmoid函数。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi or"><img src="../Images/5abf13cd4e362ed3f050169bb421f244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*AV6BcGW9PBpePIS4vhiyTA.png"/></div></figure><p id="3f2e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它具有每个输入的权重和总体偏差b。此函数的一个非常好的特性是σ(wᵗ x + b)取值于区间(0，1)内，因此当其输出大于0.5时，我们可以将输入分类为属于类1或类0。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi os"><img src="../Images/173282f37ef1b39a67846d06ed6a8b30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eKCLwLuWPl7kcc81Omb1sQ.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">图3 Sigmoid函数(图片由作者提供)</p></figure><p id="9605" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，如果我们想要使用包括所有二次项的三个特征，我们必须计算</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/2a94fe77ac273622a7b26c39c9261b44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*xlmlVQYrCTz_ALL1X5iFKg.png"/></div></figure><p id="03d3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它有六个特点。在这里，我们采用所有两个元素的特征组合。更一般地，为了计算多项式项数，我们可以使用重复的组合函数:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/70b082d906d9ab4768c757eb544b3db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/format:webp/1*HLWNJSsrh6vDhVOcrVmGLA.png"/></div></figure><p id="17b8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以，如果我们的输入是一张100x100像素的黑白图片，那么特征大小最终将是(100+2–1)！/2!(100–1)=5050.显然，这是不切实际的，我们需要另一种方法。</p><p id="8c42" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当创建具有许多特征的复杂模型时，神经网络是另一种选择。神经网络的体系结构如下所示:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ov"><img src="../Images/61b11eb09fb3c9b93f44b5e08e5b25d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P0wtYG0ApvOsVGXRI7k7lA.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">图4神经网络(图片作者提供)</p></figure><p id="fcd6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">神经网络有许多层，每层有一群神经元。第一层称为<strong class="kq ja"> <em class="nq">输入层</em> </strong>，该层内的神经元称为输入神经元。最后一层称为<strong class="kq ja"> <em class="nq">输出层</em> </strong>和神经元输出神经元。中间的图层称为<strong class="kq ja"> <em class="nq">隐藏图层</em> </strong>。一层的输出被用作下一层的输入。这些网络被称为<strong class="kq ja"> <em class="nq">前馈神经网络</em> </strong>。</p><p id="b8dc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">乙状结肠神经元的工作方式如下。神经元的每个输入x1、x2 …xn具有相应的权重w1、w2 …wn。神经元也有一个总偏差b。神经元的输出是σ(w T x + b ),这是神经元的<em class="nq">激活</em>。该输出值将作为下一层的输入。</p><p id="771f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">权重被表示为<em class="nq"> wˡⱼₖ </em>，意味着从层<em class="nq"> (l-1) </em>中的<em class="nq"> kᵗʰ </em>神经元到层<em class="nq"> l </em>中的<em class="nq"> jᵗʰ </em>神经元的权重。b <em class="nq"> ˡⱼ </em>是<em class="nq"> jᵗʰ </em>神经元在<em class="nq"> lᵗʰ </em>层的偏置。激活<em class="nq"> lᵗʰ </em>层中的<em class="nq"> jᵗʰ </em>神经元是一个<em class="nq"> ˡⱼ </em>。对于对应于最后一层的值，使用上标L。</p><p id="e57e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例如，在对图片进行分类的问题中，输入层可以由灰度值在0.0和1.0之间的所有像素组成。如图4所示，为了计算第2层中最后一个神经元的激活值，我们计算:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ow"><img src="../Images/7bae35c70977f95ef36534da7684562d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5e0xYto5zBSlIkFTyfidkA.png"/></div></div></figure><p id="01b6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们如何训练一个神经网络？我们可以使用梯度下降来最小化成本函数。但是为了执行梯度下降，我们必须计算∂C/∂w和∂C/∂b.。接下来，给出了计算这些偏导数的算法。</p><h1 id="c241" class="mh mi iq bd mj mk ox mm mn mo oy mq mr kf oz kg mt ki pa kj mv kl pb km mx my bi translated">反向传播</h1><p id="cb22" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated"><em class="nq">反向传播</em>是用于计算成本函数梯度的算法，即偏导数∂C/∂ <em class="nq"> wˡⱼₖ和</em> ∂C/∂b <em class="nq"> ˡⱼ.</em>为了定义成本函数，我们可以使用等式(4):</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/adea6233d6b862ccb055343763eae4e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*LwVRKTCUkJ-cG_gYPvepLA.png"/></div></figure><p id="6b81" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中第二项是输入x的激活值向量，我们知道</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/6b2b8245814e4200cc063e5342659137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*jHLLaY7o1vfVRRJWp6pkjQ.png"/></div></figure><p id="e810" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中总和超过了<em class="nq">(1)ᵗʰ</em>层中的k个神经元。</p><p id="025d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以定义一个<em class="nq">权重矩阵</em> Wˡ，它们是连接到<em class="nq"> lᵗʰ </em>层的权重，而权重<em class="nq"> Wˡⱼₖ </em>对应于wˡ中具有行j和列k的条目。类似地，我们可以定义一个包含层l中所有偏差的偏差向量bˡ，以及激活向量:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/307060ed43310876829adbe89d5111a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*3CVcZFEaytzGj2InKBn1rg.png"/></div></figure><p id="0083" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">计算中间值也很有用</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/91bc20e1b8a52682ae8524a47cdf2704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*FbnngaHotoyMDR4h0oFuig.png"/></div></figure><p id="12f4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">或者以矢量化的形式</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/48e8c789b6af802febeb82b99c11ef17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*hhopxrL5psdRH3cXHRuU9g.png"/></div></figure><p id="f303" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">很明显，</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/f95cc127ae62ea19a79eae978ffc7815.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*8wraGJLMF7kjD5QLGdNwFQ.png"/></div></figure><p id="4b07" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们还将l层神经元j的误差定义为δ <em class="nq"> ˡⱼ </em>。我们从梯度下降算法中知道，当偏导数为0或接近0时，我们接近最小值，否则斜率越陡，我们就越不正确。考虑到这一点，我们可以定义</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/c05ad48ef2a4597017c657bbf10e831c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*DOccz5z8lSPaccVgvX-kzA.png"/></div></figure><p id="fe94" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有了所有这些工具，我们可以将输出层中的误差定义如下:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/118cc7366f8a2c2092dd68fa3de3e5e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*Ul6KomQeRq7RAsLgh077gg.png"/></div></figure><p id="43c2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在哪里</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/5d45d37e05473e797dc840991ad66e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*k45B7JCDkKXYLBXfgi1MQw.png"/></div></figure><p id="117e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果我们使用等式(9)中定义的二次函数作为一个例子，那么</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/c089be8acc9d2832e98e42bc036351f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*7bfABB1pR4R1FTn0WKJczg.png"/></div></figure><p id="a42d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以将∇ₐC定义为包含c相对于最后一层中的每个激活的偏导数的向量，然后我们可以写出(16)的矢量化形式:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/de8638e3c88150f2a83207ee1f7cdac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*e_2OC0eR_1ApNf98thsbdw.png"/></div></figure><p id="1863" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在让我们尝试计算任何神经元的误差。特别</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/849944ec540330f23de2bfe76a247519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*Ukg8ESnj9-jA_IdbMprNag.png"/></div></figure><p id="9eb4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/ed432c9819dde19e0da2cc10d99c79b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*Lr721zQAXz7ToNCoYSJIFQ.png"/></div></figure><p id="4b5f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，我们有</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi po"><img src="../Images/591a61ad9412a05c5d754bf6905b50c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*AkjsPncIpnlxrpXG6fsggw.png"/></div></figure><p id="f22a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">代入等式(18 ),我们得到:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/2850c8d3dc7f703619b5f6393b3f9028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*x-xgXRgnBQRt3qQmPbD7RA.png"/></div></figure><p id="2870" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">或者矢量化的形式:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/d8b23caecf5a82bd5d2c71c044795e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*6TdmGnfA7oXOsIT1gfkK2g.png"/></div></figure><p id="807a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，使用这个公式，如果我们知道层<em class="nq"> l </em>的δ，我们可以计算<em class="nq"> l-1，l-2 </em>等的δ。那就是我们<em class="nq">反向传播</em>的错误，因此算法的名字。我们已经在等式(17)中看到了如何计算δ L。</p><p id="6a1e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在让我们试着计算∂C/∂b <em class="nq"> ˡⱼ </em></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/2507a88785bd65583db25fcff34d5d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*sUbypgxTLHHZXiHa-zc1Qw.png"/></div></figure><p id="c4ca" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第二个等式由等式(15)和等式(12)得出。等式(23)表明:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/d0d4bcad99bf8212023c50ce61aa15e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/format:webp/1*nVZmZ6RqdcLuNTlnLERkRQ.png"/></div></figure><p id="e643" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这是个好消息，因为我们已经看到了如何计算任意l的δ <em class="nq"> ˡⱼ</em></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/9426f14af5ea9b8f7adb185268212dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*r8PA6X9mld76XQ_78dk1pQ.png"/></div></figure><p id="bcad" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">等式(23)和(24)示出了如何计算成本函数的梯度。</p><h2 id="e175" class="nf mi iq bd mj ng nh dn mn ni nj dp mr kx nk nl mt lb nm nn mv lf no np mx iw bi translated">交叉熵</h2><p id="d495" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">在上一节中，我描述了使用二次成本函数(9)的反向传播算法。用于分类问题的另一个成本函数是<strong class="kq ja"> <em class="nq">交叉熵</em> </strong>函数。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi pt"><img src="../Images/92d852c09d614358ae8df72ca802a19b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d6uBLizxaoA5Nz0jepOyJQ.png"/></div></div></figure><p id="8e25" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们把这个函数分解成几个部分，看看它为什么有意义。如果y=1，则第二项抵消，ln <em class="nq">和a </em>剩余。如果我们看这个函数的图形，我们看到当<em class="nq"> a </em>接近1时，函数的值接近0，当<em class="nq"> a </em>越接近0时，函数趋于无穷大。也就是说，a 越接近y的真实值，成本就越小。类似地，对于y=0。剩下的一项是ln(1 a)。当<em class="nq"> a </em>接近1时，函数值接近无穷大，a越接近0，函数值越接近0。交叉熵函数中j上的和意味着输出层中所有神经元上的和。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi pu"><img src="../Images/36300e83b7e0604f147d93ef1ad27396.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kEvk4mRdAJDGMY4kV3TFRw.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure><p id="ab7c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在实践中，交叉熵成本似乎是最常用的，而不是二次成本函数。我们一会儿就会明白为什么会这样。我们前面介绍过反向传播算法。对于这个新功能，它有什么变化？等式(16)为我们提供了一种计算输出层误差的方法，我们还看到了二次成本函数的情况。现在让我们计算交叉熵函数。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi pv"><img src="../Images/7343aca87f94552241007a43beeacddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3JRUSuA-R50WeWfhybdtLQ.png"/></div></div></figure><p id="65a3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">似乎与二次函数的唯一区别是，我们去掉了等式(16)中的第二项，即sigma质数。这有多重要？如果我们观察图3，我们可以注意到，当sigmoid函数接近0或1时，图形变平，因此σ'(x)非常接近0。例如，如果输出层神经元将输出非常接近0的值，则σ’将接近0。但是如果真值是1，δ的值接近0将使得相应权重的偏导数非常小，因此学习将非常慢，并且将需要多次迭代。但是当使用交叉熵函数时，σ'项消失了，学习可以更快。计算相对于权重和偏差的偏导数的方式与上一节中描述的方式相同，唯一的区别是计算等式(26)的方式。</p><p id="e431" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">正规化</strong></p><p id="8d84" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正则化是克服过度拟合的一种方法。实现正则化的技巧有很多，但这里我只描述<strong class="kq ja"><em class="nq">【L2】</em></strong>。L2正则化的思想是通过增加正则化项来惩罚大的权值。然后，正则化的交叉熵变成:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi pw"><img src="../Images/473a3a483bf618c7c6740f2c6d8e924e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nyAYY3Pp2Gqvrw8RkNHC5A.png"/></div></div></figure><p id="737b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正则化参数是α。正则化项中不包括偏差，因此偏差的偏导数不变，梯度下降更新规则也不变:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi px"><img src="../Images/0efbc1784c5aa3d72b5a96213346442f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*bJAB8UFvqdL3e6KZZSqqxg.png"/></div></figure><p id="88b7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">权重的更新规则变为:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi py"><img src="../Images/b17a18149dd08b7e8de9ed340de03906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*SyvHGffyOVPIdrJZKYNxhg.png"/></div></figure><p id="5246" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你可以注意到，第一项缩小了权重。这也叫做<em class="nq">重量衰减。</em></p><h2 id="d4fd" class="nf mi iq bd mj ng nh dn mn ni nj dp mr kx nk nl mt lb nm nn mv lf no np mx iw bi translated">代码示例</h2><p id="46fe" class="pw-post-body-paragraph ko kp iq kq b kr mz ka kt ku na kd kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">使用上述概念，我们将尝试使用MNIST手写数字数据库来识别手写数字。</p><p id="d163" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">完整的代码可以在<a class="ae pz" href="https://github.com/enghinomer/deep-learning/tree/master/NN" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="a6b4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先，指定层大小来构建网络</p><pre class="ll lm ln lo gt qa qb qc qd aw qe bi"><span id="134f" class="nf mi iq qb b gy qf qg l qh qi"><em class="nq">/**<br/> * The constructor takes as input and array of integers<br/> * representing the number of nodes in each layer<br/> * */<br/></em>public Network(int[] layerSizes) {<br/>    this.layerSizes = Arrays.<em class="nq">copyOf</em>(layerSizes, layerSizes.length);<br/>    this.layers = layerSizes.length;<br/><br/>    //initialise biases<br/>    for (int i = 1; i&lt;layerSizes.length; i++) {<br/>        biases.add(Nd4j.<em class="nq">randn</em>(layerSizes[i], 1));<br/>    }<br/><br/>    //initialise weights<br/>    for (int i = 1; i&lt;layerSizes.length; i++) {<br/>        weights.add(Nd4j.<em class="nq">randn</em>(layerSizes[i], layerSizes[i-1]));<br/>    }<br/>}</span></pre><p id="6945" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了训练网络，我实现了随机梯度下降算法。</p><pre class="ll lm ln lo gt qa qb qc qd aw qe bi"><span id="e11a" class="nf mi iq qb b gy qf qg l qh qi"><em class="nq">/**<br/> * Performs mini-batch gradient descent to train the network. If test data is provided<br/> * it will print the performance of the network at each epoch<br/> * </em><strong class="qb ja"><em class="nq">@param </em></strong><em class="nq">trainingData data to train the network<br/> * </em><strong class="qb ja"><em class="nq">@param </em></strong><em class="nq">epochs number of epochs used to train the network<br/> * </em><strong class="qb ja"><em class="nq">@param </em></strong><em class="nq">batchSize the size of batch<br/> * </em><strong class="qb ja"><em class="nq">@param </em></strong><em class="nq">eta the learning rate<br/> * </em><strong class="qb ja"><em class="nq">@param </em></strong><em class="nq">testData data to test the network<br/> * */<br/></em>public void SGD(DataSet trainingData, int epochs, int batchSize, double eta, DataSet testData) {<br/>    int testSize=0;<br/>    if (testData != null) {<br/>        testSize = testData.numExamples();<br/>    }<br/>    int trainingSize = trainingData.numExamples();<br/>    for (int i=0; i&lt;epochs; i++) {<br/>        trainingData.shuffle();<br/>        for(int j=0; j&lt;trainingSize; j+=batchSize) {<br/>            DataSet miniBatch = trainingData<br/>           .getRange(j,j+batchSize&lt;trainingSize ? j+batchSize :    trainingSize-1);<br/>            this.updateMiniBatch(miniBatch, eta);<br/>        }<br/>        if (testData != null) {<br/>            System.<em class="nq">out</em>.printf("Epoch %s: %d / %d ", i, this.evaluate(testData), testSize);<br/>            System.<em class="nq">out</em>.println();<br/>        }<br/>    }<br/>}<br/><br/><em class="nq">/**<br/> * Updates the weights un biases of the network using backpropagation for a single mini-batch<br/> * </em><strong class="qb ja"><em class="nq">@param </em></strong><em class="nq">miniBatch the mini batch used to train the network<br/> * </em><strong class="qb ja"><em class="nq">@param </em></strong><em class="nq">eta the learning rate<br/> * */<br/></em>public void updateMiniBatch(DataSet miniBatch, double eta) {<br/>    INDArray [] gradientBatchB = new INDArray[layers];<br/>    INDArray [] gradientBatchW = new INDArray[layers];<br/>    for (int i=0; i &lt; this.biases.size(); i++) {<br/>        gradientBatchB[i+1] = Nd4j.<em class="nq">zeros</em>(this.biases.get(i).shape());<br/>    }<br/>    for (int i=0; i &lt; this.weights.size(); i++) {<br/>        gradientBatchW[i+1] = Nd4j.<em class="nq">zeros</em>(this.weights.get(i).shape());<br/>    }<br/>    List&lt;INDArray[]&gt; result;<br/>    for(DataSet batch : miniBatch) {<br/>        result = this.backpropagation(batch.getFeatures(), batch.getLabels());<br/>        for(int i=1; i&lt;layers; i++) {<br/>            gradientBatchB[i] = gradientBatchB[i]<br/>.add(result.get(0)[i]);<br/>            gradientBatchW[i] = gradientBatchW[i]<br/>.add(result.get(1)[i]);<br/>        }<br/>    }<br/>    for (int i=0; i&lt;this.biases.size(); i++) {<br/>        INDArray b = this.biases.get(i).sub(gradientBatchB[i+1]<br/>.mul(eta/miniBatch.numExamples()));<br/>        this.biases.set(i, b);<br/>        INDArray w = this.weights.get(i).sub(gradientBatchW[i+1]<br/>.mul(eta/miniBatch.numExamples()));<br/>        this.weights.set(i, w);<br/>    }<br/>}</span></pre><p id="a167" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">SGD方法基本上是从训练集中取批，调用updateMiniBatch方法。在这种方法中，调用反向传播方法，它返回为该小批量计算的梯度，偏差和权重数组根据随机梯度下降部分给出的规则进行更新。</p><p id="1c8d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">反向传播方法如下计算上述方程:</p><pre class="ll lm ln lo gt qa qb qc qd aw qe bi"><span id="8d8d" class="nf mi iq qb b gy qf qg l qh qi">//feedforward<br/>INDArray activation = x;<br/>INDArray [] activations = new INDArray[layers];<br/>INDArray [] zs = new INDArray[layers];<br/>activations[0] = x;<br/>INDArray z;<br/>for (int i=1; i&lt;layers; i++) {<br/>    z = this.weights<br/>.get(i-1).mmul(activation).add(this.biases.get(i-1));<br/>    zs[i] = z;<br/>    activation = sigmoid(z);<br/>    activations[i] = activation;<br/>}</span></pre><p id="eb06" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里计算等式(13)和(14)，然后使用(22)计算δ项。有了deltas，我们可以用(23)和(24)计算偏导数。代码如下:</p><pre class="ll lm ln lo gt qa qb qc qd aw qe bi"><span id="ac9b" class="nf mi iq qb b gy qf qg l qh qi">//back pass<br/>INDArray sp;<br/>INDArray delta = costDerivative(activations[layers-1], y).mul(sigmoidPrime(zs[layers-1]));<br/>gradientB[layers - 1] = delta;<br/>gradientW[layers - 1] = delta.mul(activations[layers2].transpose());<br/>for (int i=2; i&lt;layers; i++) {<br/>    z = zs[layers-i];<br/>    sp = sigmoidPrime(z);<br/>    delta = (this.weights<br/>.get(layers - i).transpose().mmul(delta)).mul(sp);<br/>    gradientB[layers - i] = delta;<br/>    gradientW[layers - i] = delta<br/>.mmul(activations[layers - i - 1].transpose());<br/>}</span></pre><p id="8b6b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在本文中，我们看到了梯度下降和反向传播是如何工作的。我介绍了描述它们的数学概念和这些算法的实现。您可以使用代码并尝试调整参数，看看您的分类有多准确。</p></div></div>    
</body>
</html>