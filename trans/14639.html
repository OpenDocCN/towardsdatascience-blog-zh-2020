<html>
<head>
<title>A Practical Introduction to Keras Callbacks in TensorFlow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 2中Keras回调的实用介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-practical-introduction-to-keras-callbacks-in-tensorflow-2-705d0c584966?source=collection_archive---------14-----------------------#2020-10-09">https://towardsdatascience.com/a-practical-introduction-to-keras-callbacks-in-tensorflow-2-705d0c584966?source=collection_archive---------14-----------------------#2020-10-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5e93" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最常用的Keras回调API教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/db5f9329b3abced60dc93e2c215012e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ofBnYxi466DsUsyjIvn0vw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者使用<a class="ae ky" href="http://www.canva.com" rel="noopener ugc nofollow" target="_blank">www.canva.com</a>制作的图像</p></figure><p id="31cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当训练机器学习模型时，我们希望能够监控模型性能，并根据这些性能指标执行某些操作。这就是Keras回调的原因。</p><p id="3531" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回调是对象TensorFlow和Keras的一种重要类型，它们被设计成能够在训练运行中的某些点上监视度量中的性能，并执行一些可能依赖于那些度量值中的性能的动作。</p><p id="4eee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将借助一些例子来探索以下流行的Keras回调API。</p><ol class=""><li id="089c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><code class="fe me mf mg mh b">EarlyStopping</code>:为提前停止而设计的回调。</li><li id="2d40" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><code class="fe me mf mg mh b">CSVLogger</code>:回调将epoch结果流至CSV文件。</li><li id="ac04" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><code class="fe me mf mg mh b">ModelCheckpoint</code>:训练时保存Keras模型或模型权重的回调</li><li id="ae5c" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><code class="fe me mf mg mh b">ReduceLROnPlateau</code>:当指标停止改善时，回调以降低学习率。</li><li id="20cf" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><code class="fe me mf mg mh b">LearningRateScheduler</code>:对<a class="ae ky" rel="noopener" target="_blank" href="/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c"> <strong class="lb iu">学习率计划表</strong> </a>的回调。</li><li id="b3ab" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><code class="fe me mf mg mh b">LambdaCallback</code>:动态创建定制回调的回调。</li></ol><p id="6820" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/007-keras-callback/keras-callbacks.ipynb" rel="noopener ugc nofollow" target="_blank">我的Github repo </a>获取源代码。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="9c9b" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">1.提前停止</h1><p id="5628" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated"><code class="fe me mf mg mh b">EarlyStopping</code>是为<a class="ae ky" rel="noopener" target="_blank" href="/a-practical-introduction-to-early-stopping-in-machine-learning-550ac88bc8fd">提前停止</a>设计的内置回调。首先，让我们导入它并创建一个提前停止对象:</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="59f4" class="nv mv it mh b gy nw nx l ny nz">from tensorflow.keras.callbacks import <strong class="mh iu">EarlyStopping</strong></span><span id="d089" class="nv mv it mh b gy oa nx l ny nz"><strong class="mh iu">early_stopping = EarlyStopping()</strong></span></pre><p id="acad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">EarlyStopping()</code>有一些选项，默认情况下:</p><ul class=""><li id="820e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated"><code class="fe me mf mg mh b">monitor='val_loss'</code>:使用验证损失作为绩效衡量标准，终止培训。</li><li id="c80d" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">patience=0</code>:是没有改善的时期数。值<code class="fe me mf mg mh b">0</code>意味着一旦性能测量从一个时期到下一个时期变得更差，就终止训练。</li></ul><p id="960d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们只需要将回调对象传递给<code class="fe me mf mg mh b">model.fit()</code>方法。</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="e9f2" class="nv mv it mh b gy nw nx l ny nz">history = model.fit(<br/>    X_train, <br/>    y_train, <br/>    epochs=50, <br/>    validation_split=0.20, <br/>    batch_size=64, <br/>    verbose=2,<br/>    <strong class="mh iu">callbacks=[early_stopping]</strong><br/>)</span></pre><p id="a2e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以看到,<code class="fe me mf mg mh b">early_stopping</code>在一个列表中被传递给了<code class="fe me mf mg mh b">callbacks</code>参数。这是一个列表，因为在实践中，我们可能会为执行不同的任务传递许多回调，例如，调试和<a class="ae ky" rel="noopener" target="_blank" href="/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c">学习速率时间表</a>。</p><p id="79df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过执行该语句，您应该得到如下所示的输出:</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="8405" class="nv mv it mh b gy nw nx l ny nz">Train on 8000 samples, validate on 2000 samples<br/>Epoch 1/50<br/>8000/8000 - 6s - loss: 1.5632 - accuracy: 0.5504 - val_loss: 1.1315 - val_accuracy: 0.6605<br/>......<br/>......<br/>Epoch 10/50<br/>8000/8000 - 2s - loss: 0.5283 - accuracy: 0.8213 - val_loss: 0.5539 - val_accuracy: 0.8170<br/>Epoch 11/50<br/>8000/8000 - 2s - loss: 0.5141 - accuracy: 0.8281 - <strong class="mh iu">val_loss: 0.5644</strong> - val_accuracy: 0.7990</span></pre><p id="9d20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于<code class="fe me mf mg mh b">val_loss</code>值的增加，训练在时期11终止，这正是条件<code class="fe me mf mg mh b">monitor='val_loss'</code>和<code class="fe me mf mg mh b">patience=0</code>。</p><p id="1504" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看一个剧情往往更方便，我们来运行<code class="fe me mf mg mh b">plot_metric(history, 'loss')</code>来一个清晰的画面。在下图中，验证损失显示为橙色，很明显，验证误差在第11时段增加。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/c13f8d4808c815bbb698acc83d321ceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RZVx9yEAXtOxtif9pFYiSg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者制作(请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/007-keras-callback/keras-callbacks.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)</p></figure><h2 id="e775" class="nv mv it bd mw od oe dn na of og dp ne li oh oi ng lm oj ok ni lq ol om nk on bi translated">争论</h2><p id="86f7" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">除了我们之前提到的选项<code class="fe me mf mg mh b">monitor</code>和<code class="fe me mf mg mh b">patience</code>之外，另外两个选项<code class="fe me mf mg mh b">min_delta</code>和<code class="fe me mf mg mh b">mode</code>可能会经常使用。</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="a026" class="nv mv it mh b gy nw nx l ny nz">EarlyStopping(<br/>    <!-- -->monitor='val_loss', <br/>    patience=0, <br/>    min_delta=0, <br/>    mode='auto'<br/>)</span></pre><ul class=""><li id="6fcf" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated"><code class="fe me mf mg mh b">monitor='val_loss'</code>:使用验证损失作为绩效衡量标准，终止培训。</li><li id="765d" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">patience=0</code>:无改善的时期数。值<code class="fe me mf mg mh b">0</code>意味着一旦性能测量从一个时期到下一个时期变得更差，就终止训练。</li><li id="9b46" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b"><strong class="lb iu">min_delta</strong></code>:符合改善条件的监控量的最小变化，即小于<code class="fe me mf mg mh b">min_delta</code>的绝对变化，将被视为无改善。</li><li id="7b19" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b"><strong class="lb iu">mode='auto'</strong></code>:应为<code class="fe me mf mg mh b">auto</code>、<code class="fe me mf mg mh b">min</code>或<code class="fe me mf mg mh b">max</code>中的一种。在<code class="fe me mf mg mh b">'min'</code>模式下，当监控量停止下降时，训练将停止；在<code class="fe me mf mg mh b">'max'</code>模式下，当监控的数量停止增加时，它将停止；在<code class="fe me mf mg mh b">'auto'</code>模式下，方向自动从监控量的名称推断出来。</li></ul><p id="35d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个定制的提前停止的例子:</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="bcdc" class="nv mv it mh b gy nw nx l ny nz">custom_early_stopping = EarlyStopping(<br/>    <strong class="mh iu">monitor='val_accuracy', </strong><br/>    <strong class="mh iu">patience=3,</strong> <br/>    <strong class="mh iu">min_delta=0.001,</strong> <br/>    <strong class="mh iu">mode='max'</strong><br/>)</span></pre><p id="3695" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">monitor='val_accuracy'</code>使用<strong class="lb iu">验证准确度</strong>作为绩效衡量标准来终止培训。<code class="fe me mf mg mh b">patience=3</code>表示训练在3个周期后终止，没有改善。<code class="fe me mf mg mh b">min_delta=0.001</code>表示验证准确度必须至少提高0.001才能算作改进。<code class="fe me mf mg mh b">mode='max'</code>表示当监控的数量停止增加时，它将停止。</p><p id="8deb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们继续，使用定制的提前停止运行它，并绘制精度图。</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="6f42" class="nv mv it mh b gy nw nx l ny nz">Train on 8000 samples, validate on 2000 samples<br/>Epoch 1/50<br/>......<br/>Epoch 12/50<br/>8000/8000 - 2s - loss: 0.5043 - accuracy: 0.8290 - val_loss: 0.5311 - val_accuracy: 0.8250<br/>Epoch 13/50<br/>8000/8000 - 3s - loss: 0.4936 - accuracy: 0.8332 - val_loss: 0.5310 - <strong class="mh iu">val_accuracy: 0.8155</strong><br/>Epoch 14/50<br/>8000/8000 - 2s - loss: 0.4835 - accuracy: 0.8353 - val_loss: 0.5157 - <strong class="mh iu">val_accuracy: 0.8245</strong><br/>Epoch 15/50<br/>8000/8000 - 2s - loss: 0.4757 - accuracy: 0.8397 - val_loss: 0.5299 - <strong class="mh iu">val_accuracy: 0.8060</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/577d82851af17b6e3c46bd2ea802fd7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bg6f_f4Ky2b2tZJowhYng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者制作(请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/007-keras-callback/keras-callbacks.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)</p></figure><p id="5334" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一次，训练在时期15终止，因为有3个时期在验证准确性上没有改进(它必须≥ 0.001才能算作改进)。</p><p id="d13b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关提前停止的更多信息，请查看这篇文章:</p><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/a-practical-introduction-to-early-stopping-in-machine-learning-550ac88bc8fd"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">机器学习中早期停止的实用介绍</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">使用Keras和TensorFlow 2.0添加和自定义提前停止的分步教程</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg ks os"/></div></div></a></div><h1 id="8d33" class="mu mv it bd mw mx ph mz na nb pi nd ne jz pj ka ng kc pk kd ni kf pl kg nk nl bi translated">2.CSVLogger</h1><p id="6481" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated"><code class="fe me mf mg mh b">CSVLogger</code>是一个回调函数，将epoch结果转换成一个CSV文件。首先，让我们导入它并创建一个<code class="fe me mf mg mh b">CSVLogger</code>对象:</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="06f8" class="nv mv it mh b gy nw nx l ny nz">from tensorflow.keras.callbacks import <strong class="mh iu">CSVLogger</strong></span><span id="66c1" class="nv mv it mh b gy oa nx l ny nz"><strong class="mh iu">csv_log = CSVLogger("results.csv")</strong></span></pre><p id="9e59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们只需要将<code class="fe me mf mg mh b"><strong class="lb iu">csv_log</strong></code>对象传递给<code class="fe me mf mg mh b">model.fit()</code>方法。</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="87b4" class="nv mv it mh b gy nw nx l ny nz">history_csv_logger = model.fit(<br/>    X_train, <br/>    y_train, <br/>    epochs=10, <br/>    validation_split=0.20, <br/>    batch_size=64, <br/>    verbose=2,<br/>    <strong class="mh iu">callbacks=[csv_log]</strong><br/>)</span></pre><p id="bc90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">培训完成后，我们可以查看CSV文件中的信息</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="9b7a" class="nv mv it mh b gy nw nx l ny nz">import pandas as pd</span><span id="65db" class="nv mv it mh b gy oa nx l ny nz"><strong class="mh iu">pd.read_csv("results.csv", index_col='epoch')</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/526b3537f8a0838307b60e8c0c7abf20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jJL5OIuMVYWOJzvKnJbe-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者制作(请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/007-keras-callback/keras-callbacks.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)</p></figure><h2 id="ecce" class="nv mv it bd mw od oe dn na of og dp ne li oh oi ng lm oj ok ni lq ol om nk on bi translated">争论</h2><p id="4cfe" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">除了强制参数<code class="fe me mf mg mh b">filename</code>之外，其他两个选项<code class="fe me mf mg mh b">separator</code>和<code class="fe me mf mg mh b">append</code>可能会经常使用。</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="4977" class="nv mv it mh b gy nw nx l ny nz">CSVLogger(filename, separator=',', append=False)</span></pre><ul class=""><li id="042d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated"><code class="fe me mf mg mh b">filename</code>:CSV文件的文件名。</li><li id="4d2b" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b"><strong class="lb iu">separator</strong></code>:用于分隔CSV文件中元素的字符串。</li><li id="8edf" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b"><strong class="lb iu">append</strong></code>:布尔型，默认为<code class="fe me mf mg mh b">False</code>，<code class="fe me mf mg mh b">True</code>如果文件存在则追加(对继续培训有用)。<code class="fe me mf mg mh b">False</code>:覆盖现有文件。</li></ul><h1 id="2940" class="mu mv it bd mw mx ph mz na nb pi nd ne jz pj ka ng kc pk kd ni kf pl kg nk nl bi translated">3.模型检查点</h1><p id="037c" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated"><code class="fe me mf mg mh b">ModelCheckpoint</code>是一个回调，用于在训练期间保存Keras模型或模型权重，以便稍后可以加载模型或权重，以从保存的状态继续训练。</p><p id="7132" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们导入它并创建一个<code class="fe me mf mg mh b">ModelCheckpoint</code>对象:</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="840f" class="nv mv it mh b gy nw nx l ny nz">from tensorflow.keras.callbacks import <strong class="mh iu">ModelCheckpoint</strong></span><span id="05fe" class="nv mv it mh b gy oa nx l ny nz">checkpoint_path = 'model_checkpoints/'</span><span id="7289" class="nv mv it mh b gy oa nx l ny nz">checkpoint = ModelCheckpoint(<br/>    <strong class="mh iu">filepath=checkpoint_path,</strong><br/>    <strong class="mh iu">save_freq='epoch',</strong><br/>    <strong class="mh iu">save_weights_only=True,</strong><br/>    verbose=1<br/>)</span></pre><p id="6604" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们将<code class="fe me mf mg mh b">checkpoint</code>对象传递给<code class="fe me mf mg mh b">model.fit()</code>方法进行训练。</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="f953" class="nv mv it mh b gy nw nx l ny nz">history_checkpoint = model.fit(<br/>    X_train, <br/>    y_train, <br/>    epochs=10, <br/>    validation_split=0.20, <br/>    batch_size=64, <br/>    verbose=2,<br/>    <strong class="mh iu">callbacks=[checkpoint]</strong><br/>)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/1b00f013302773a26ca17369165e2ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oa_6pbmHCSfLpDbbvZFT9w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者制作(请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/007-keras-callback/keras-callbacks.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)</p></figure><p id="17cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在培训过程中，我们应该能够看到上面的打印输出。</p><p id="54dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练完成后，我们可以通过运行以下命令来获得测试准确性:</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="946b" class="nv mv it mh b gy nw nx l ny nz">&gt;&gt;&gt; get_test_accuracy(model, X_test, y_test)<br/><strong class="mh iu">accuracy: 0.779</strong></span></pre><h2 id="c52d" class="nv mv it bd mw od oe dn na of og dp ne li oh oi ng lm oj ok ni lq ol om nk on bi translated">装载重量</h2><p id="ed4f" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">让我们创建一个新模型<code class="fe me mf mg mh b">new_model</code>来演示负重是如何工作的。通过运行<code class="fe me mf mg mh b">get_test_accuracy(new_model, X_test, y_test)</code>，我们得到了一个没有加载任何训练过的砝码的模型的测试精度<strong class="lb iu"> 0.086 </strong>。</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="065a" class="nv mv it mh b gy nw nx l ny nz"># Create a new model<br/>&gt;&gt;&gt; new_model = create_model()</span><span id="2bae" class="nv mv it mh b gy oa nx l ny nz"># Without loading weight<br/>&gt;&gt;&gt; get_test_accuracy(new_model, X_test, y_test)<br/><strong class="mh iu">accuracy: 0.086</strong></span></pre><p id="13d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们用<code class="fe me mf mg mh b">load_weights('model_checkpoints/')</code>加载砝码，再次得到它的测试精度。这一次，我们应该能够看到与我们训练模型<code class="fe me mf mg mh b">model</code>相同的准确性。</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="4f0c" class="nv mv it mh b gy nw nx l ny nz"># Load weights<br/>&gt;&gt;&gt; new_model.load_weights('model_checkpoints/')<br/>&gt;&gt;&gt; get_test_accuracy(new_model, X_test, y_test)<br/><strong class="mh iu">accuracy: 0.779</strong></span></pre><h2 id="1145" class="nv mv it bd mw od oe dn na of og dp ne li oh oi ng lm oj ok ni lq ol om nk on bi translated">争论</h2><p id="93e3" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">下面是使用<code class="fe me mf mg mh b">ModelCheckpoint</code>回调时你应该知道的常用参数</p><ul class=""><li id="c31a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated"><code class="fe me mf mg mh b">filepath</code> : string或<code class="fe me mf mg mh b">PathLike</code>，保存模型文件的路径。<code class="fe me mf mg mh b">filepath</code>可以包含命名的格式化选项，例如，如果<code class="fe me mf mg mh b">filepath</code>是<code class="fe me mf mg mh b">weights.{epoch:02d}-{val_loss:.2f}</code>，那么模型检查点将与纪元编号和验证损失一起保存在文件名中。</li><li id="74f3" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">save_freq</code> : <code class="fe me mf mg mh b">'epoch'</code>或整数。使用<code class="fe me mf mg mh b">'epoch'</code>时，回调在每个时期后保存模型。当使用integer时，回调在这许多批的末尾保存模型。</li><li id="0ba7" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">save_weights_only</code>:如果<code class="fe me mf mg mh b">True</code>，则只保存模型的权重(<code class="fe me mf mg mh b">model.save_weights(filepath)</code>)，否则保存整个模型(<code class="fe me mf mg mh b">model.save(filepath)</code>)。</li></ul><h1 id="501e" class="mu mv it bd mw mx ph mz na nb pi nd ne jz pj ka ng kc pk kd ni kf pl kg nk nl bi translated">4.ReduceLROnPlateau</h1><p id="30a7" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated"><code class="fe me mf mg mh b">ReduceLROnPlateau</code>是一种回调，用于在指标停止改善时降低学习率。该回调监控一个量，如果在一个<code class="fe me mf mg mh b">patience</code>数量的周期内没有看到改善，则学习率减少<code class="fe me mf mg mh b">factor</code>值(<code class="fe me mf mg mh b">new_lr = lr * factor</code>)。让我们借助一个例子来看看这是如何工作的。</p><p id="7878" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们导入它并创建一个<code class="fe me mf mg mh b">ReduceLROnPlateau</code>对象:</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="ca20" class="nv mv it mh b gy nw nx l ny nz">from tensorflow.keras.callbacks import ReduceLROnPlateau</span><span id="2e92" class="nv mv it mh b gy oa nx l ny nz">reduce_lr = ReduceLROnPlateau(<br/>    <strong class="mh iu">monitor='val_loss', <br/>    factor=0.2,   <br/>    patience=2, <br/>    min_lr=0.001,</strong><br/>    verbose=2<br/>)</span></pre><p id="2552" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">monitor='val_loss'</code>使用<strong class="lb iu">验证损失</strong>作为降低学习率的绩效衡量标准。<code class="fe me mf mg mh b">patience=2</code>表示学习率在2个周期后降低，但没有改善。<code class="fe me mf mg mh b">min_delta=0.001</code>表示验证损失必须至少改善0.001才能算作改善。<code class="fe me mf mg mh b">factor=0.2</code>表示新的学习率将随着<code class="fe me mf mg mh b">new_lr = lr * factor</code>减少。</p><p id="9c9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用<code class="fe me mf mg mh b">reduce_lr</code>回调来训练模型</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="c006" class="nv mv it mh b gy nw nx l ny nz">history_reduce_lr = model.fit(<br/>    X_train, <br/>    y_train, <br/>    epochs=50, <br/>    validation_split=0.20, <br/>    batch_size=64, <br/>    verbose=2,<br/>    <strong class="mh iu">callbacks=[reduce_lr]</strong><br/>)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/e9fd48628203c3c6990cea69be696f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c7eP2LNdoStm1OmiA3I00w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者制作(请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/007-keras-callback/keras-callbacks.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a></p></figure><p id="3b8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该得到如上所示的输出。在上面的输出中，<code class="fe me mf mg mh b">ReduceLROnPlateau</code>回调已经在时间点30和37被触发。</p><p id="333b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们画出学习率和损失的图表，以便清楚地了解情况。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/81f0be8b558c20164eb6e2838f6b7c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L6Dq6AgpWwjRNzmJ8EsEVA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者制作(请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/007-keras-callback/keras-callbacks.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/68d5dd777d352bae51021e1ec2812d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNjjKHZm3I-o6N14efojdg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者制作(请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/007-keras-callback/keras-callbacks.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)</p></figure><h2 id="e001" class="nv mv it bd mw od oe dn na of og dp ne li oh oi ng lm oj ok ni lq ol om nk on bi translated">争论</h2><p id="da3e" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">下面是使用<code class="fe me mf mg mh b">ReduceLROnPlateau</code>回调时你应该知道的常用参数</p><ul class=""><li id="9521" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated"><code class="fe me mf mg mh b">monitor='val_loss'</code>:使用验证损失作为性能指标，降低学习率。</li><li id="5e8b" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">factor</code>:学习率降低的系数。<code class="fe me mf mg mh b">new_lr = lr * factor</code>。</li><li id="ed27" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">patience</code>:无改善的时期数。</li><li id="d91e" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">min_delta</code>:符合改善条件的监控量的最小变化，即小于<code class="fe me mf mg mh b">min_delta</code>的绝对变化，将被视为无改善。</li><li id="cbdb" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">mode='auto'</code>:应为<code class="fe me mf mg mh b">auto</code>、<code class="fe me mf mg mh b">min</code>或<code class="fe me mf mg mh b">max</code>中的一种。在<code class="fe me mf mg mh b">'min'</code>模式下，当监控量停止下降时，学习率将会降低；在<code class="fe me mf mg mh b">'max'</code>模式中，当监控的数量停止增加时，学习率将降低；在<code class="fe me mf mg mh b">'auto'</code>模式下，方向由监控量的名称自动推断。</li><li id="b5d2" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">min_lr</code>:学习率的下限。</li></ul><h1 id="6d95" class="mu mv it bd mw mx ph mz na nb pi nd ne jz pj ka ng kc pk kd ni kf pl kg nk nl bi translated">5.学习率计划程序</h1><p id="c41b" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">在训练神经网络时，随着训练的进行降低学习速率通常是有用的。这可以通过使用<strong class="lb iu">学习速率表</strong>或<strong class="lb iu">自适应学习速率</strong>来完成。<code class="fe me mf mg mh b">LearningRateScheduler</code>是<strong class="lb iu">学习率计划</strong>的内置回调。</p><p id="2e6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关学习费率表的更多详细信息，请查看以下文章:</p><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">实践中的学习率计划:以Keras和TensorFlow 2.0为例</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">训练一个神经网络的一个痛苦的事情是我们必须处理的超参数的绝对数量…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pq l pd pe pf pb pg ks os"/></div></div></a></div><p id="12f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们借助一个例子来看看<code class="fe me mf mg mh b">LearningRateScheduler</code>是如何工作的。</p><p id="afba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们导入它并创建一个调度函数:</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="22a3" class="nv mv it mh b gy nw nx l ny nz">from tensorflow.keras.callbacks import LearningRateScheduler</span><span id="4916" class="nv mv it mh b gy oa nx l ny nz">def <strong class="mh iu">lr_decay</strong>(epoch, lr):<br/>    if epoch != 0 and epoch % 5 == 0:<br/>        return lr * 0.2<br/>    return lr</span></pre><p id="a8e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">lr_decay()</code>取2个参数<code class="fe me mf mg mh b">epoch</code>(当前纪元)&amp; <code class="fe me mf mg mh b">lr</code>(当前学习率)，返回一个新的学习率。我们的<code class="fe me mf mg mh b">lr_decay()</code>函数将每5个历元将学习率降低0.2倍。</p><p id="3b4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用<code class="fe me mf mg mh b">reduce_lr</code>回调来训练模型</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="9683" class="nv mv it mh b gy nw nx l ny nz">history_lr_schedule = model.fit(<br/>    X_train, <br/>    y_train, <br/>    epochs=20, <br/>    validation_split=0.20, <br/>    batch_size=64, <br/>    verbose=2,<br/>    callbacks=[<strong class="mh iu">LearningRateScheduler(lr_decay, verbose=1)</strong>]<br/>)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/71873ed98e417f60fd845a2a598d22c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FGj2NTam5F0FpIGig3390A.png"/></div></div></figure><p id="201f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该得到如上所示的输出。下面是学习率的曲线图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/19b5ef49d04e918f3ed6cffb1055a480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*25y3NoZsE8nsNh4Xne4LBQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者制作(请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/007-keras-callback/keras-callbacks.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)</p></figure><h1 id="dce1" class="mu mv it bd mw mx ph mz na nb pi nd ne jz pj ka ng kc pk kd ni kf pl kg nk nl bi translated">6.LambdaCallback</h1><p id="e02b" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">另一个有用的回调是<code class="fe me mf mg mh b">LambdaCallback</code>。它类似于<code class="fe me mf mg mh b">Callback</code>，允许我们动态构建定制回调。</p><p id="0606" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">LambdaCallback</code>由下面的匿名函数构成，这些函数将在适当的时候被调用。</p><ul class=""><li id="4554" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated"><code class="fe me mf mg mh b">on_epoch_begin</code>:在每个纪元开始时调用。</li><li id="056a" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">on_epoch_end</code>:在每个历元结束时调用。</li><li id="b10e" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">on_batch_begin</code>:在每批开始时调用。</li><li id="080d" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">on_batch_end</code>:每批结束时调用。</li><li id="2002" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">on_train_begin</code>:模型训练开始时调用。</li><li id="08f3" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">on_train_end</code>:模型训练结束时调用。</li></ul><p id="58e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，回调需要位置参数，如下所示:</p><ul class=""><li id="5ee7" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated"><code class="fe me mf mg mh b">on_epoch_begin</code>和<code class="fe me mf mg mh b">on_epoch_end</code>需要两个位置参数:<code class="fe me mf mg mh b">epoch</code>、<code class="fe me mf mg mh b">logs</code></li><li id="77fa" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">on_batch_begin</code>和<code class="fe me mf mg mh b">on_batch_end</code>期待两个位置论元:<code class="fe me mf mg mh b">batch</code>、<code class="fe me mf mg mh b">logs</code></li><li id="c523" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">on_train_begin</code>和<code class="fe me mf mg mh b">on_train_end</code>期待一个位置自变量:<code class="fe me mf mg mh b">logs</code></li></ul><p id="ca28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们借助一个例子来看看<code class="fe me mf mg mh b">LambdaCallback</code>是如何工作的。</p><p id="9b37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们导入它并创建3个不同的<code class="fe me mf mg mh b">LambdaCallback</code>:</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="6a84" class="nv mv it mh b gy nw nx l ny nz">from tensorflow.keras.callbacks import LambdaCallback</span><span id="efc4" class="nv mv it mh b gy oa nx l ny nz"><strong class="mh iu">epoch_callback</strong> = LambdaCallback(<br/>    on_epoch_begin=lambda epoch,logs: print('Starting Epoch {}!'.format(epoch+1))<br/>)</span><span id="1ff6" class="nv mv it mh b gy oa nx l ny nz"><strong class="mh iu">batch_loss_callback</strong> = LambdaCallback(<br/>    on_batch_end=lambda batch,logs: print('\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))<br/>)</span><span id="fe98" class="nv mv it mh b gy oa nx l ny nz"><strong class="mh iu">train_finish_callback</strong> = LambdaCallback(<br/>    on_train_end=lambda logs: print('Training finished!')<br/>)</span></pre><p id="216c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用上面的回调来训练模型</p><pre class="kj kk kl km gt nr mh ns nt aw nu bi"><span id="a417" class="nv mv it mh b gy nw nx l ny nz">history_lambda_callback = model.fit(<br/>    X_train, <br/>    y_train,<br/>    epochs=2,                  # change epoch to 2 for demo purpose <br/>    validation_split=0.20, <br/>    batch_size=2000,           # change to 2000 for demo purpose<br/>    verbose=False,<br/>    callbacks=[<strong class="mh iu">epoch_callback, batch_loss_callback, train_finish_callback</strong>]<br/>)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/d1d4f4e03bb04357b4119ed8ee5183ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*jELAnERl8nAcXUeMVvHhQg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者制作(请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/007-keras-callback/keras-callbacks.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a></p></figure><p id="42d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该得到如上所示的输出。这非常有用</p><h1 id="7fdd" class="mu mv it bd mw mx ph mz na nb pi nd ne jz pj ka ng kc pk kd ni kf pl kg nk nl bi translated">其他回调函数</h1><p id="029e" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">除了上述函数，在深度学习项目中，您可能会遇到或希望使用其他回调函数:</p><ul class=""><li id="a59a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated"><code class="fe me mf mg mh b">Callback</code>:这是用于构建自定义回调的基类。它与<code class="fe me mf mg mh b">LambdaCallback</code>相似，但功能更强大。我们将为此再进行一次讨论。</li><li id="c635" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">TensorBoard</code>:一个回调为TensorBoard写日志，tensor board是TensorFlow优秀的可视化工具。</li><li id="396d" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><code class="fe me mf mg mh b">RemoteMonitor</code>:用于将事件流式传输到服务器的回调。</li></ul><h1 id="c8bb" class="mu mv it bd mw mx ph mz na nb pi nd ne jz pj ka ng kc pk kd ni kf pl kg nk nl bi translated">好了</h1><p id="b176" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">感谢阅读。本文涵盖了最受欢迎的Keras回调。</p><p id="561e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/007-keras-callback/keras-callbacks.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>获取源代码。</p><p id="e555" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对机器学习的实用方面感兴趣，请继续关注。</p><h2 id="e14c" class="nv mv it bd mw od oe dn na of og dp ne li oh oi ng lm oj ok ni lq ol om nk on bi translated">您可能对我的其他TensorFlow文章感兴趣:</h2><ul class=""><li id="2f37" class="lv lw it lb b lc nm lf nn li pu lm pv lq pw lu ob mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c">实践中的学习率计划</a></li><li id="6166" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/the-googles-7-steps-of-machine-learning-in-practice-a-tensorflow-example-for-structured-data-96ccbb707d77">谷歌机器学习实践的7个步骤:结构化数据的TensorFlow示例</a></li><li id="02fd" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/3-ways-to-create-a-machine-learning-model-with-keras-and-tensorflow-2-0-de09323af4d3">用Keras和TensorFlow 2.0创建机器学习模型的3种方法</a></li><li id="67be" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/batch-normalization-in-practice-an-example-with-keras-and-tensorflow-2-0-b1ec28bde96f">批量规范化实践:以Keras和TensorFlow 2.0为例</a></li><li id="7352" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ob mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/a-practical-introduction-to-early-stopping-in-machine-learning-550ac88bc8fd">实践中的提前停止:以Keras和TensorFlow为例</a></li></ul><p id="cf0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更多可以从我的<a class="ae ky" href="https://github.com/BindiChen/machine-learning" rel="noopener ugc nofollow" target="_blank"> Github </a>中找到</p></div></div>    
</body>
</html>