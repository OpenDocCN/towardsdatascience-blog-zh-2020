<html>
<head>
<title>Introduction to Federated Learning and Challenges</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">联合学习简介和挑战</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-federated-learning-and-challenges-ea7e02f260ca?source=collection_archive---------6-----------------------#2020-10-18">https://towardsdatascience.com/introduction-to-federated-learning-and-challenges-ea7e02f260ca?source=collection_archive---------6-----------------------#2020-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4472" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">联邦学习和挑战简介</h2></div><p id="0847" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一代人工智能建立在围绕<strong class="kh ir">“数据隐私”</strong>的核心理念之上。当数据隐私是一个主要问题，并且我们不信任任何隐瞒我们数据的人时，我们可以转向联合学习，通过私人构建智能系统来构建保护隐私的人工智能。</p><p id="2f91" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">联邦学习就是将计算转移到数据上。全球共享模型被购买到数据所在的位置，例如智能手机。通过将模型移动到设备上，我们可以将模型作为一个整体进行集体训练。</p><p id="a45d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了这个概念，任何人都可以直接或间接地在他们的设备上参与联合学习，例如，智能手机和物联网设备等边缘设备可以受益于设备上的数据，而数据永远不会离开设备，特别是对于计算受限的设备，通信是较小设备的瓶颈。</p><p id="73d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从构建任何智能系统同时保护任何个人隐私的角度来看，将计算转移到数据的概念是一个强大的概念。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/a1dd309656fcab72d5522377ad817abf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*s5KeyAlPcsnvSuU5fLCP0A.png"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图一。您的手机会根据您的使用情况在本地对型号进行个性化设置A)。许多用户的更新被聚集B)以形成对共享模型的一致变更C)之后重复该过程。(图由<a class="ae ln" href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a>提供)</p></figure><p id="8897" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">联合学习也分为三类，如“<strong class="kh ir">、</strong>、<strong class="kh ir">、</strong>、<strong class="kh ir">、</strong>。</p><p id="8c3e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">水平联合学习</strong>在所有设备上使用具有相同特征空间的数据集，这意味着客户端A和客户端B具有如下A)所示的相同特征集。<strong class="kh ir">垂直联合学习</strong>使用不同特征空间的不同数据集来联合训练如下b)所示的全局模型。例如，客户端A (Amazon)拥有关于客户在Amazon上购买的电影的信息，客户端B (IMDB)拥有关于客户的电影评论的信息，使用来自不同域的这两组数据集可以更好地为客户服务，使用电影评论信息(IMDB)向在Amazon上浏览电影的客户提供更好的电影推荐。最后，<strong class="kh ir">联合迁移学习</strong>是与预训练模型一起使用的垂直联合学习，该预训练模型在类似的数据集上被训练以解决不同的问题。联合迁移学习的一个这样的例子是训练个性化模型，例如针对用户过去的浏览行为的电影推荐。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/62cabce1cadae17c66bf34638bf902a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*Tyfm2oQJcvdCkL_dCgfdow.png"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图二。联合学习的分类。a)横向学习，b)纵向学习，c)迁移学习(图由<a class="ae ln" href="https://www.researchgate.net/figure/Categorization-of-Federated-Learning-a-Horizontal-federated-learning-b-Vertical_fig1_339582159" rel="noopener ugc nofollow" target="_blank">链接</a></p></figure><h1 id="81ca" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">联合学习是如何工作的</h1><p id="cc8b" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">联邦学习围绕着名为“<strong class="kh ir">、FedAvg</strong>”[3]的联邦平均算法。FedAvg是Google [3]为解决联邦学习问题制定的第一个标准联邦学习算法。此后，FedAvg算法的许多变种如“<strong class="kh ir"> FedProx </strong>”、“<strong class="kh ir"> FedMa </strong>”、“<strong class="kh ir"> FedOpt </strong>”、“<strong class="kh ir">脚手架</strong>等..已经被开发来解决[2]中的许多联合学习问题。</p><p id="d686" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面描述了FedAvg算法如何在较高层次上工作。</p><p id="22d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每一轮FedAvg中，目标是最小化全局模型<strong class="kh ir"> <em class="mm"> w </em> </strong>的目标，其中<strong class="kh ir"> <em class="mm"> </em> </strong>只是本地设备损耗的加权平均之和。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/adb4d81a7004eae95433559f29228133.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*wtp-TRPLD6R03KzyIleyaA.png"/></div></figure><ol class=""><li id="c4d8" class="mo mp iq kh b ki kj kl km ko mq ks mr kw ms la mt mu mv mw bi translated">随机采样客户端/设备的子集。</li><li id="649f" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">服务器向每个客户端广播其全局模型。</li><li id="2042" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">并行地，客户端对它们自己的损失函数运行<strong class="kh ir">随机梯度下降(SGD) </strong>，并将结果模型发送到服务器进行聚合。</li><li id="3f45" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">然后，服务器将其全局模型更新为这些局部模型的平均值。</li><li id="e77d" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated">然后对于<strong class="kh ir"> <em class="mm"> n </em> </strong>这样的通信循环重复该过程。</li></ol><h1 id="ac4e" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">联合学习的挑战</h1><p id="0394" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">这种有用的技术带来了大量需要解决的挑战[2，4]。我认为需要解决的一些最重要的挑战是从[4]中挑选出来的以下四点(该列表绝非详尽无遗)。</p><ol class=""><li id="8d26" class="mo mp iq kh b ki kj kl km ko mq ks mr kw ms la mt mu mv mw bi translated"><strong class="kh ir">噪声和准确性之间的权衡:</strong>使用<strong class="kh ir">差分隐私(DP) </strong>，我们可以将噪声添加到数据中，以增强隐私保护。然而，与<strong class="kh ir"> DP </strong>我们牺牲了模型的性能。因此，需要在添加适量噪声和不损害模型性能之间进行权衡。</li><li id="d120" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">系统和统计异构:</strong>在异构设备上进行培训是一项挑战，无论设备类型如何，确保在所有设备上有效地扩展联合学习非常重要。统计信息的不相似性是指一个设备不能导出全局统计模式，因此一个设备的总体、样本或结果与其他设备的不同。</li><li id="aa1c" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">通信瓶颈:</strong>将模型带到设备的通信成本应该适度低，因为它可能会影响FL环境，其中一个设备可能会由于通信瓶颈而瘫痪，这反过来会拖延联合训练过程。有几项工作致力于解决通信瓶颈，如丢弃掉队者(未能在指定时间窗口内计算训练的设备)，以及建模压缩/量化以降低带宽成本。</li><li id="793a" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">中毒:</strong>中毒有两种形式，<strong class="kh ir"> 1。数据中毒:</strong>在联合训练过程中，多个客户端可以通过贡献它们的设备上训练数据来参与，并且很难检测/防止恶意客户端发送恶意/伪造数据来毒害训练过程，这反过来毒害模型。<strong class="kh ir"> 2。模型中毒</strong>，与数据中毒相反，恶意客户端在将收到的模型发送回中央服务器进行聚合之前，通过篡改其梯度/参数来修改模型，因此，在聚合过程中，全局模型可能会因无效梯度而严重中毒。</li><li id="6c91" class="mo mp iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">效率和隐私之间的权衡:</strong>使用<strong class="kh ir">安全多计算(SMPC)和差分隐私(DP) </strong>提高了联邦学习中的隐私保护能力，然而，这种保护伴随着成本和效率之间的权衡。使用<strong class="kh ir"> SMPC </strong>，客户端将在发送回中央服务器之前加密模型的参数，因此加密需要额外的计算资源，这将损害训练模型的效率。使用<strong class="kh ir"> DP </strong>，噪声被添加到模型和数据中，因此损失了一些精度。因此，在联邦学习中，在<strong class="kh ir"> SMPC </strong>和<strong class="kh ir"> DP </strong>之间找到一个合适的平衡是一个公开的挑战。</li></ol><h1 id="f714" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">结论</h1><p id="1b85" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">联邦学习仍然是一个相对较新的领域，有很多研究机会可以让保护隐私的人工智能变得更好。这包括诸如<strong class="kh ir">系统异构</strong>、<strong class="kh ir">统计异构</strong>、<strong class="kh ir">隐私问题、</strong>和<strong class="kh ir">沟通效率等挑战。</strong>。这带来了联合学习中的许多开放问题，在联合学习被业界广泛采用之前，这些问题需要作为一个整体来解决。</p><h1 id="c98f" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated"><strong class="ak">参考文献:</strong></h1><p id="d4c3" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">[1] <a class="ae ln" href="http://ai.googleblog.com/2017/04/federated-learning-collaborative.html" rel="noopener ugc nofollow" target="_blank">联合学习:没有集中训练数据的协同机器学习</a></p><p id="b193" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] <a class="ae ln" href="https://arxiv.org/abs/1912.04977" rel="noopener ugc nofollow" target="_blank">联邦学习的进展和公开问题</a></p><p id="08d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] <a class="ae ln" href="https://arxiv.org/pdf/1602.05629.pdf" rel="noopener ugc nofollow" target="_blank">从分散数据进行深度网络的通信高效学习</a></p><p id="2fb5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] <a class="ae ln" href="https://www.sciencedirect.com/science/article/abs/pii/S0167739X20329848" rel="noopener ugc nofollow" target="_blank">一项关于联合学习的安全性和隐私性的调查</a></p></div></div>    
</body>
</html>