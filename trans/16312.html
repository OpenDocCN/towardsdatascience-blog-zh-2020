<html>
<head>
<title>A simple SVM based implementation of semi-supervised learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个简单的基于SVM的半监督学习实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-simple-svm-based-implementation-of-semi-supervised-learning-f44eafb0a970?source=collection_archive---------30-----------------------#2020-11-10">https://towardsdatascience.com/a-simple-svm-based-implementation-of-semi-supervised-learning-f44eafb0a970?source=collection_archive---------30-----------------------#2020-11-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6d4c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">扫清半监督学习的迷雾</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8e5174d62d42f1731dc3e309d149cbce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OAvfscCltMVg5yrR"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:-<a class="ae kv" href="https://unsplash.com/photos/GhFn3mddePk" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/GhFn3mddePk</a></p></figure><p id="3bfc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">我们</span>都遇到过作为一种机器学习问题的半监督学习。但这是一个没有被很好理解的概念。最好通过弄脏我们的手来理解这一点，而这正是我们正在带来的。</p><p id="16b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用例很简单，我们有一些数据，可能有100个观察值，其中30个是有标签的(有监督的)，其余的是无标签的(无监督的)，我们正在解决一个分类问题。我想大家都会同意，如果我们可以使用所有100个观察值，那么通过对30个案例或观察值进行训练所能达到的性能会更低，不幸的是，其中70个没有标记。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/f4d1669f69c09d7bcf2b42f10ce6175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*Va3RZ9tPKRTmV932Wnvj2w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:无监督学习场景(来源:作者)</p></figure><p id="9a47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本质上这是一个二元分类问题。这两个类别由蓝色三角形和红色圆圈表示。灰色菱形表示未标记的数据。</p><p id="bf19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的第一个策略是</p><p id="3d97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤1:在标记的数据上建立一个分类器(常规的东西)</p><p id="d128" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二步:用这个来预测未标记的数据。<strong class="ky ir">然而，除了预测，你还要检查你的信心水平。</strong></p><p id="ea32" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第三步:将这些观察结果添加到你有一定把握的训练数据中。这些被称为<strong class="ky ir">伪标记</strong>与标记数据相对照。</p><p id="225c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤4:使用这个<strong class="ky ir">扩充数据集</strong>，现在用于训练，并使用这个模型。</p><p id="5303" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们使用非监督数据来增加监督学习的训练数据，这介于两者之间，因此称为半监督。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/80adb1c101c9581bc517318f603c28a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*1JmNnvBVFYF-elmdHloI3g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:扩展数据集</p></figure><p id="ce12" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上图中，说明了训练数据的扩展。对于我们有信心的观察结果，我们使用了伪标签，而对于我们没有信心的观察结果，我们仍然保持不加标签。</p><p id="dd7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们现在跳到代码</p><p id="a1ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们正在创建一个虚拟场景，其中我们将数据集(wine)分为train(已标记)、unl(未标记)和test。</p><pre class="kg kh ki kj gt md me mf mg aw mh bi"><span id="eaa2" class="mi mj iq me b gy mk ml l mm mn">X_train, X_test, y_train, y_test = train_test_split(<br/>    X, y, test_size=0.3, random_state=1)<br/>X_train, X_unl, y_train, y_unl = train_test_split(<br/>    X_train, y_train, test_size=0.7, random_state=1)</span></pre><p id="416d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里分别是训练、测试和未标记的形状。请注意，我们不会考虑unl部分的标签信息，因此将其视为未标记</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/1a401807380fa075b1271e165cabc80e.png" data-original-src="https://miro.medium.com/v2/resize:fit:244/format:webp/1*oz2fh7y4Ecs4YyOsAY4ySQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据集的形状(图片来源:作者)</p></figure><p id="a494" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们简单地在标记的部分进行训练，它刚刚走了19行。</p><pre class="kg kh ki kj gt md me mf mg aw mh bi"><span id="d1bf" class="mi mj iq me b gy mk ml l mm mn">clf = svm.SVC(kernel='linear', probability=True,C=1).fit(X_train, y_train)<br/>clf.score(X_test, y_test)</span></pre><p id="37a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">获得的精度如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/ebf35f71dd9fbc8765cd0014064c2a21.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/format:webp/1*SKlZwfJmFBFP4QEXoY8sCw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">初始分类准确性(来源:作者)</p></figure><p id="7a55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们对未标记的数据进行预测</p><pre class="kg kh ki kj gt md me mf mg aw mh bi"><span id="8665" class="mi mj iq me b gy mk ml l mm mn">df = pd.DataFrame(clp, columns = ['C1Prob', 'C2Prob','C3Prob']) <br/>df['lab']=lab<br/>df['actual']=y_unl<br/>df['max']=df[["C1Prob", "C2Prob","C3Prob"]].max(axis=1)</span></pre><p id="c0af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可能已经注意到了，我们使用了预测概率，它预测了类别概率，而不是标签，这实际上会帮助我们找到有把握的猜测</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/9d67ce9316d62ac842a0f06ff86bec71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*lfM8mgaoc1BiF6ErTM76zA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对预测的信心(来源:作者)</p></figure><p id="f972" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们只看预测概率之间的绝对差异</p><ul class=""><li id="3f30" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">当这三个类别的概率相等时，则所有类别的概率大约为0.33</li><li id="0744" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">任何大于这个值的值都显示出一定的可信度</li></ul><p id="b0b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本质上，我们希望在训练中添加这些概率差异较大的观察值。代码如下</p><p id="b2f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图给出了置信度的分布，以最有可能的类别的概率表示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/97a3bb626803b6b4ab7fb70b20416990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*JgNitXjOXUBc_BQ2LMAVzA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对预测的信心(图片来源:作者)</p></figure><p id="d3b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们运行一个循环，对于最可能类的不同阈值，将数据添加到观察值中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/1452609dadfdc2612450f974b6945b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*sfkxuFqQZFiL47PW4fOkLg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同阈值下的准确性(来源:-作者)</p></figure><ul class=""><li id="dabd" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">当最可能的类的概率大于0.4且高达0.55时，情况会有所改善</li><li id="89e0" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">在那之后，几乎没有任何改善，因为在那些我们超级自信的人身上，这并没有增加知识，因为观察结果与标记非常相似。</li></ul><p id="aa74" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">尾注:</strong></p><p id="07de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我们已经展示了如何使用SVM来采用简单的半监督策略。这种技术可以很容易地扩展到其他分类器。影响将取决于类的重叠或区分程度、要素的信息量等等。</p><p id="8ffc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">参考资料:</p><p id="721a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]<a class="ae kv" href="https://www.kaggle.com/saptarsi/a-simple-semi-supervised-strategy-based-on-svm" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/saptarsi/a-simple-semi-supervised-strategy-based-on-SVM</a></p><p id="f0b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]<a class="ae kv" rel="noopener" target="_blank" href="/supervised-learning-but-a-lot-better-semi-supervised-learning-a42dff534781">https://towards data science . com/supervised-learning-but-a-lot-better-semi-supervised-learning-a 42 dff 534781</a></p></div></div>    
</body>
</html>