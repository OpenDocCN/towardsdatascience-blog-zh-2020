<html>
<head>
<title>How to Serve Different Model Versions using TensorFlow Serving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用TensorFlow服务不同的模型版本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-serve-different-model-versions-using-tensorflow-serving-de65312e58f7?source=collection_archive---------24-----------------------#2020-10-19">https://towardsdatascience.com/how-to-serve-different-model-versions-using-tensorflow-serving-de65312e58f7?source=collection_archive---------24-----------------------#2020-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="698b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解不同的配置设置，以便使用TensorFlow Serving管理不同的模型和模型的不同版本</h2></div><p id="ae86" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">本文解释了如何使用配置文件在TensorFlow服务中管理多个模型和同一模型的多个版本，并简要了解批处理。</em>T3】</strong></p><h2 id="552d" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">先决条件:</h2><p id="edea" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated"><a class="ae md" rel="noopener" target="_blank" href="/deploying-a-tensorflow-model-to-production-made-easy-4736b2437103"> <strong class="kk iu">将TensorFlow模型部署到生产制造Eas </strong> </a> <strong class="kk iu"> y </strong></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi me"><img src="../Images/5eae3be5400d5fed7b17451fb104a31e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*eb8w6sU35rI1VE0CnRcfyQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">照片由<a class="ae md" href="https://unsplash.com/@loverna?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Loverna在<a class="ae md" href="https://unsplash.com/s/photos/serving-tea?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的旅程</a>拍摄</p></figure><p id="f5aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您拥有不同架构的TensorFlow深度学习模型，或者已经使用不同的超参数训练了您的模型，并且希望在本地或生产中测试它们。最简单的方法是使用一个<strong class="kk iu">模型服务器配置文件来服务模型。</strong></p><blockquote class="mq"><p id="5b00" class="mr ms it bd mt mu mv mw mx my mz ld dk translated">模型服务器配置文件是一个协议缓冲文件(protobuf ),它是一种与语言无关、与平台无关的可扩展的、简单而快速的序列化结构数据的方法。</p></blockquote><p id="3c20" class="pw-post-body-paragraph ki kj it kk b kl na ju kn ko nb jx kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated"><strong class="kk iu"> <em class="le">如何创建模型配置文件？</em>T19】</strong></p><p id="aa87" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是一个样本模型配置文件，它将服务于磁盘上所有版本的“MNIST”模型。</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="5855" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">model_config_list {<br/> config {<br/>    name: 'mnist'<br/>    base_path: '/models/mnist/'<br/>    model_platform: 'tensorflow'<br/>    model_version_policy: {all: {}}<br/>        }<br/>}</strong></span></pre><p id="fcb3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个ModelConfig指定一个模型，该模型使用以下参数。</p><p id="9d9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">名字- </strong>一个可服务的模型名</p><p id="8654" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> base_path - </strong>指定查找servable版本的路径。</p><p id="6540" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">模型_平台- </strong>开发模型的平台。</p><p id="b5ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> model_version_policy - </strong>模型的版本策略指示加载哪个版本的模型并提供给客户端。默认情况下，将提供模型的最新版本，并且可以通过更改<strong class="kk iu"> model_version_policy </strong>字段来覆盖。</p><p id="b29f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">如何加载模型配置文件？</em> </strong></p><p id="852d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">点击阅读如何在Windows 10上加载TensorFlow服务Docker容器<a class="ae md" rel="noopener" target="_blank" href="/deploying-a-tensorflow-model-to-production-made-easy-4736b2437103"/></p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="d90f" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">docker run -p 8501:8501 --mount type=bind,source=C:\TF_serv\TF_model,target=/models/mnist -e MODEL_NAME=mnist -t tensorflow/serving</strong></span></pre><p id="98bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">列出所有正在运行的Docker容器</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="f1a2" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">docker container list</strong><br/></span></pre><p id="528c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="8ad8" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">docker ps</strong></span></pre><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi no"><img src="../Images/451f531e6b41ee7c03bb835bdf8c4d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cd1y5K2LLAHR39mxLuf3bw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">以红色突出显示的容器Id和容器名称</p></figure><p id="fd73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将models.config文件从Docker映像的源复制到目标</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="5064" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">docker cp \TF_serv\TF_model\models.config <em class="le">ba978f5a9475</em>:/models/mnist</strong></span></pre><p id="42e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将文件复制到docker容器时，提供容器Id，如上所示。</p><p id="38d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用Docker容器名称或容器Id停止Docker容器</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="a733" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">docker stop <em class="le">ba978f5a9475</em></strong></span></pre><p id="1f35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，使用选项加载模型配置文件</p><p id="a842" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nt nu nv ng b"><strong class="kk iu">--model_config_file_poll_wait_seconds</strong></code> flag指示服务器每60秒在使用<code class="fe nt nu nv ng b"><strong class="kk iu">--model_config_file</strong></code>指定的路径检查一次新的配置文件。</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="9bd5" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">docker run <br/>-p 8501:8501 <br/>--mount type=bind,source=C:\TF_serv\TF_model,target=/models/mnist <br/>-e MODEL_NAME=mnist <br/>-t tensorflow/serving <br/>--model_config_file_poll_wait_seconds=60 <br/>--model_config_file=/models/mnist/models.config</strong></span></pre><p id="c9dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">如何使用models.config文件提供的模型使用模型的具体版本进行推理？</em> </strong></p><p id="fe29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用REST API进行推理请求</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="04f0" class="lf lg it ng b gy nk nl l nm nn">json_response = requests.post('<a class="ae md" href="http://localhost:8501/v1/models/mnist/versions/5:predict'" rel="noopener ugc nofollow" target="_blank"><strong class="ng iu">http://localhost:8501/v1/models/mnist/versions/5</strong>:<strong class="ng iu">predict'</strong></a>, data=data, headers=headers)</span></pre><p id="ba71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为上面指定的models.config文件将加载磁盘上的所有版本；我已经加载了版本5。</p><p id="e74d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面显示了调用模型的特定版本进行推理的一般方法。</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="eb7b" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">/v1/models/<em class="le">&lt;model name&gt;</em>/versions/<em class="le">&lt;version number&gt;</em></strong></span></pre><p id="8458" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">我如何覆盖并指定一个要加载和服务的模型的特定版本？</em>T11】</strong></p><p id="cb9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您需要使用<strong class="kk iu">指定的</strong>标签和<strong class="kk iu">型号版本政策</strong>中的<strong class="kk iu">版本号</strong></p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="d2de" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">model_config_list {<br/> config<br/> {<br/>    name: 'mnist'<br/>    base_path: '/models/mnist/'<br/>    model_platform: 'tensorflow'<br/>    model_version_policy{<br/>                        specific{<br/>                           versions:2<br/>                                }<br/>                         }<br/> }<br/>}</strong></span></pre><p id="7b82" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了上面的变更models.config文件，客户端现在可以只为版本2调用模型的predict方法。</p><p id="c1c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">我可以同时服务同一型号的多个版本吗？</em> </strong></p><p id="c8e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您还可以同时提供同一型号的多个版本。在下面的例子中，我们同时为MNIST模型提供版本2和版本3。</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="33af" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">model_config_list {<br/> config<br/> {<br/>    name: 'mnist'<br/>    base_path: '/models/mnist/'<br/>    model_platform: 'tensorflow'<br/>    model_version_policy{<br/>                        specific{<br/>                           versions:2<br/>                           versions:3<br/>                              }<br/>                         }<br/> }<br/>}</strong></span></pre><p id="303f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当您有一个较新版本的模型，并希望将一些用户转移到一个较新的版本，并让大多数客户端使用稳定的较旧版本的模型时，这是很有帮助的。这允许您完成A/B测试。</p><p id="5642" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">我能否为多个模型提供服务，如定制模型和转移学习模型？</em>T25】</strong></p><p id="886d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要为多个模型提供服务器，请更新models.config文件，如下所示</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="8d57" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">model_config_list {<br/>  config{<br/>         name: 'mnist'<br/>         base_path: '/models/mnist/'<br/>         model_platform: 'tensorflow'<br/>         model_version_policy{<br/>             specific{<br/>                      versions:2<br/>                      versions:3<br/>                      }</strong></span><span id="c8ef" class="lf lg it ng b gy nw nl l nm nn"><strong class="ng iu">                    }<br/>       }<br/>  config{<br/>        name: 'Inception_1'<br/>        base_path: '/models/inception_1/'<br/>        model_platform: 'tensorflow'<br/>        }<br/>}</strong></span></pre><p id="daa5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TensorFlow服务允许批量处理多个请求吗？T29】</p><p id="0220" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TensorFlow服务允许两种不同形式的批处理。</p><ul class=""><li id="3457" class="nx ny it kk b kl km ko kp kr nz kv oa kz ob ld oc od oe of bi translated"><strong class="kk iu">批量处理单个模型推理请求，TensorFlow serving等待预定时间，然后对该时间段内到达的所有请求进行推理</strong></li><li id="b6bd" class="nx ny it kk b kl og ko oh kr oi kv oj kz ok ld oc od oe of bi translated"><strong class="kk iu">单个客户端可以向TensorFlow服务器发送批量请求。</strong></li></ul><p id="622b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">批处理要求所有请求使用相同版本的模型。</p><p id="d277" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">批处理允许更高的资源利用率和更高的吞吐量。</p><p id="7bec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要启用批处理，将<code class="fe nt nu nv ng b"><strong class="kk iu">--enable_batching</strong></code>标志指定为真，并使用<code class="fe nt nu nv ng b"><strong class="kk iu">--batching_parameters_file</strong></code>标志设置包含批处理参数的配置文件。</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="de48" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">docker run -p 8501:8501 --mount type=bind,source=C:\TF_serv\TF_model,target=/models/mnist <br/>-e MODEL_NAME=mnist <br/>-t tensorflow/serving <br/>--model_config_file_poll_wait_seconds=60  <br/>--model_config_file=/models/mnist/models.config <br/>--enable_batching=true <br/>--batching_parameters_file=/models/mnist/batch.config</strong></span></pre><p id="599c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">批处理配置文件是一个. protobuf文件</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="64ac" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">max_batch_size { value: 128 }<br/>batch_timeout_micros { value: 0 }<br/>max_enqueued_batches { value: 1000000 }<br/>num_batch_threads { value: 8 }</strong></span></pre><p id="b2ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> max_batch_size </strong>:允许您指定任何批次的最大大小。如果您指定一个大于<strong class="kk iu"> max_batch_size，</strong>的批处理，那么您将得到一个错误。该参数决定吞吐量/延迟的权衡，并确保不超过资源限制。</p><p id="784c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> batch_timeout_micros </strong>:这是服务器重试批量请求的最大时间。参数值以微秒为单位指定。</p><p id="6572" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> num_batch_threads </strong>:通过指定要同时处理的最大批处理数量来定义并行度。</p><p id="9395" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> max_enqueued_batches </strong>:它有助于通过拒绝需要很长时间才能得到服务的请求来限制批处理队列，并避免构建大量积压的请求。</p><p id="2994" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我在有批处理和没有批处理的情况下运行下面的代码时，批处理让我的结果快了2到3倍。</p><pre class="mf mg mh mi gt nf ng nh ni aw nj bi"><span id="aee1" class="lf lg it ng b gy nk nl l nm nn"><strong class="ng iu">import time</strong><br/>#Build the batched data for making iferences for 100 images<br/><strong class="ng iu">data = json.dumps({"signature_name": "serving_default", "instances": test_images[:100].tolist()})</strong><br/><br/><strong class="ng iu">st=time.perf_counter()<br/>headers = {"content-type": "application/json"}<br/>json_response = requests.post('</strong><a class="ae md" href="http://localhost:8501/v1/models/mnist/labels/stable:predict'" rel="noopener ugc nofollow" target="_blank"><strong class="ng iu">http://localhost:8501/v1/models/mnist/labels/2:predict'</strong></a><strong class="ng iu">, data=data, headers=headers)</strong></span><span id="1def" class="lf lg it ng b gy nw nl l nm nn"><strong class="ng iu">predictions = json.loads(json_response.text)['predictions']<br/>end_t= time.perf_counter()<br/>print(end_t-st)</strong></span></pre><h2 id="f691" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">结论:</h2><p id="e423" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">TensorFlow服务通过使用模型服务器配置提供服务不同模型或同一模型的不同版本的简单方法，使生产部署变得更加容易。</p><p id="10b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">批处理允许您对同一客户端或不同客户端的多个请求进行批处理，从而优化硬件加速器资源并提供更好的吞吐量。</p><h2 id="9e51" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">参考资料:</h2><p id="dc59" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated"><a class="ae md" href="https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md#batch-scheduling-parameters-and-tuning" rel="noopener ugc nofollow" target="_blank">https://github . com/tensor flow/serving/blob/master/tensor flow _ serving/batching/readme . MD # batch-scheduling-parameters-and-tuning</a></p><p id="602a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae md" href="https://www.tensorflow.org/tfx/serving/serving_config" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tfx/serving/serving_config</a></p></div></div>    
</body>
</html>