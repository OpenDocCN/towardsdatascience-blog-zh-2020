<html>
<head>
<title>XGBoost vs LightGBM on a High Dimensional Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高维数据集上的XGBoost与LightGBM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/xgboost-vs-lightgbm-on-a-high-dimensional-dataset-bbd5b5174274?source=collection_archive---------25-----------------------#2020-09-29">https://towardsdatascience.com/xgboost-vs-lightgbm-on-a-high-dimensional-dataset-bbd5b5174274?source=collection_archive---------25-----------------------#2020-09-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="38c0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">速度和性能的比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e9cf41d7ef1ca4fc7538a3f52912b657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yg7lLOo6SW-L6nrfmQynOQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔希·卡拉布雷斯在<a class="ae ky" href="https://unsplash.com/s/photos/speed?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="59b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我最近完成了一个多类分类问题，作为一个数据科学家职位的家庭作业。这是比较梯度推进决策树的两种最新实现XGBoost和LightGBM的好机会。</p><p id="61d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两种算法都非常强大，在性能最好的机器学习模型中非常突出。</p><p id="7f44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该数据集包含超过6万个观测值和103个数值特征。目标变量包含9个不同的类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/1cc2a354b94e31d49cb6d9eecb27c3c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*xPadEsE8-JrxbigE-zqPxQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="98b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于这篇文章的重点是XGBoost和LightGBM的比较，所以我将跳过探索性的数据分析和数据争论部分。我已经设法消除了11个特征和大约2000个异常值。</p><p id="df19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得注意的是，数据争论过程中应用的所有技术都是在训练集和测试集分离后完成的。否则，我们将不得不处理数据泄露，这是机器学习中的一个严重问题。</p><p id="6b8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征空间是高度稀疏的(即特征大部分由零组成)。下面是显示特征稀疏率的直方图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/1171bef1c63a1bb6206f1e8abb265f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*ZTdm2jRDxfMjYtxmuHk1BQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="e076" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated"><strong class="ak"> LightGBM </strong></h1><p id="e5db" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">由微软研究人员创建的LightGBM是梯度推进决策树(GBDT)的一种实现。在LightGBM之前，GBDT before的现有实现会随着实例或特性数量的增加而变慢。LightGBM旨在解决这个效率问题，尤其是对于大型数据集。</p><p id="b661" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LightGBM使用两种技术来解决大型数据集的效率问题:</p><ul class=""><li id="07b7" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">梯度单侧采样</li><li id="7e73" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">EFB(独家功能捆绑)</li></ul><p id="e305" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解更多，我有一篇关于LightGBM如何使用这些技术的独立文章:</p><div class="np nq gp gr nr ns"><a rel="noopener follow" target="_blank" href="/understanding-the-lightgbm-772ca08aabfa"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">了解光GBM</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">是什么让它更快更高效</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">towardsdatascience.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og ks ns"/></div></div></a></div></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="984d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们回到我们的实现。LightGBM要求数据采用特定的格式，因此我们使用Dataset函数。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="e736" class="om mf it oi b gy on oo l op oq">import lightgbm as lgb</span><span id="99e8" class="om mf it oi b gy or oo l op oq">lgb_train = lgb.Dataset(X_train, y_train)<br/>lgb_test = lgb.Dataset(X_test, y_test)</span></pre><p id="9e86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数在LightGBM和XGBoost的性能中起着关键作用。您可能需要花费大量时间来调优超参数。最终，你会创造出你自己的方法或策略来加速调整的过程。</p><p id="188b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有很多超参数。有些在准确性和速度方面更重要。其中一些主要用于防止过度拟合。</p><p id="bb92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数可以作为字典传递给模型。这些超参数给了我最好的参数。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="7b0e" class="om mf it oi b gy on oo l op oq">params = {<br/>'boosting_type': 'gbdt',<br/>'objective': 'multiclass',<br/>'metric': 'multi_logloss',<br/>'num_class':9,<br/>'max_depth':9,<br/>'num_leaves': 100,<br/>'min_data_in_leaf':300,<br/>'learning_rate': 0.03,<br/>'feature_fraction': 0.7,<br/>'bagging_fraction':0.8,<br/>'bagging_freq':10,<br/>'lambda_l1': 1,<br/>'verbose': 0<br/>}</span></pre><p id="8077" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">没有严格的规则来定义最佳超参数值。这更像是一个受过教育的试错过程。如果你知道一个特定的超参数是做什么的，调优过程将比尝试随机值更有效。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="7d12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">超参数</strong></p><ul class=""><li id="d1e5" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">Max_depth:单棵树的最大深度。</li><li id="ba2e" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">Num_leaves:它控制一棵树的叶子数量。LightGBM使用逐叶的树生长算法，因此num_leaves是控制树复杂度的主要参数。</li><li id="038d" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">Min_data_in_leaf:它表示一片叶子上所需的最小样本数(即观察值),这对控制过拟合非常重要。</li><li id="7177" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">Feature_fraction:在每个节点随机选择的要素的比率。</li><li id="b3e5" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">Bagging_fraction和bagging_freq也有助于避免过度拟合。</li></ul><p id="17b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LightGBM超参数的完整列表可以在<a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="3fae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以训练模型了。下面的代码块使用定型集对模型进行定型，并在定型集和验证集上评估模型的性能。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="2a24" class="om mf it oi b gy on oo l op oq">%%timeit</span><span id="96f7" class="om mf it oi b gy or oo l op oq">gbm = lgb.train(params, lgb_train, num_boost_round=700,<br/>valid_sets=[lgb_train, lgb_test], early_stopping_rounds=10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/dee50584b44936d0d542bdfffc5279ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*hFfdm9z2ygXk2lxpKoKsYg.png"/></div></figure><p id="dcd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练集和验证集的对数损失分别为0.383和0.418。执行训练平均需要2分26秒。</p><p id="a7cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以在这些模型上实现非常低的损耗，但这会导致过度拟合。更重要的是在训练和测试精度方面有一个平衡的模型。此外，任务的目标是实现0.41的对数损失。</p><p id="50e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个很好的实践，摆弄超参数值，看看它们对精度和过度拟合的影响。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="ed99" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated"><strong class="ak"> XGBoost </strong></h1><p id="02bc" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated"><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>是GBDT算法的一个实现。它的效率如此之高，以至于统治了Kaggle上的一些主要比赛。</p><p id="4594" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于XGBoost，数据集也应该以特定的方式进行格式化。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="1561" class="om mf it oi b gy on oo l op oq">dtrain = xgb.DMatrix(X_train, y_train)<br/>dtest = xgb.DMatrix(X_test, y_test)</span></pre><p id="22e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XGBoost还有许多超参数，需要正确地进行调整，以便创建一个健壮而准确的模型。这里是我发现的超参数值，可以达到令人满意的结果，同时还可以最小化过度拟合。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="9a63" class="om mf it oi b gy on oo l op oq">import xgboost as xgb</span><span id="5d0a" class="om mf it oi b gy or oo l op oq">params_xgb = {<br/>'boosting_type': 'dart',<br/>'objective':'multi:softmax',<br/>'num_class':9,<br/>'max_depth':7,<br/>'min_child_weight':20,<br/>'gamma':1,<br/>'subsample':0.8,<br/>'colsample_bytree':0.7,<br/>'tree_method':'hist',<br/>'eval_metric':'mlogloss',<br/>'eta':0.04,<br/>'alpha': 1,<br/>'verbose': 2<br/>}</span></pre></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="7980" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">超参数</strong></p><ul class=""><li id="0298" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">Max_depth:一棵树的最大深度。</li><li id="9a90" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">Min_child_weight:一个孩子所需的实例权重的最小总和。将它保持在较高的位置可以防止孩子过于具体，从而有助于避免过度适应。</li><li id="48e4" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">Gamma:在树的叶节点上进行进一步划分所需的最小损失减少。同样，游戏越大，模型越不可能过度拟合。</li><li id="3fb4" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">子样本:在生长树之前随机选择的行的比率。子样也可以用来避免过度拟合。</li><li id="65fe" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">Eta:学习率。保持较高的值会使模型快速学习，但同时也会增加过度拟合的机会。</li><li id="0309" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">α:L1正则化项。</li></ul><p id="87a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XGBoost的整个超参数列表可以在<a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="712e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以训练我们的模型了。我们传递参数、训练集和要运行的回合数。您可以增加回合数并获得更好的精度，但总是有过度拟合的风险。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="d036" class="om mf it oi b gy on oo l op oq">%%timeit</span><span id="9457" class="om mf it oi b gy or oo l op oq">bst = xgb.train(<br/>params_xgb, dtrain , 700, <br/>evals=[(dtrain,'eval'),(dtest, 'eval')], <br/>verbose_eval=True<br/>)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/dcdf7689f25bdd571fb1e94f626d4dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*eX6GXJ8tkz2Z6B18Auii9w.png"/></div></figure><p id="d965" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练集和验证集的对数损失分别为0.369和0.415。执行训练平均需要3分52秒。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="8400" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated"><strong class="ak">结论</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/dee50584b44936d0d542bdfffc5279ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*hFfdm9z2ygXk2lxpKoKsYg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">LightGBM</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/dcdf7689f25bdd571fb1e94f626d4dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*eX6GXJ8tkz2Z6B18Auii9w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">XGBoost</p></figure><p id="265c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">损失非常接近，因此我们可以得出结论，就准确性而言，这些模型在具有所选超参数值的数据集上表现大致相同。</p><p id="71f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在速度方面，LightGBM比XGBoost快40%。</p><p id="fe9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>