<html>
<head>
<title>Machine Learning Algorithms from Start to Finish in Python: Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中从头到尾的机器学习算法:逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318?source=collection_archive---------31-----------------------#2020-10-23">https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318?source=collection_archive---------31-----------------------#2020-10-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="da0f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">探索基本分类模型背后的真实情况，并使用Python从头构建一个分类器。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1902b147f53f4f9146ea66c6f4d143b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eOqrMO8fZJvx-Fieaa4udA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@drew_beamer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">德鲁·比默</a>在<a class="ae ky" href="https://unsplash.com/s/photos/split?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4c4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于任何即将到来的数据科学家或机器学习实践者来说，逻辑回归本质上是一个必须知道的知识。这很可能是人们遇到的第一个分类模型。但是，问题是，<strong class="lb iu"> <em class="lv">怎么</em> </strong> <em class="lv">真的管用吗？</em><strong class="lb iu"><em class="lv"/></strong><em class="lv">它有什么作用？</em> <strong class="lb iu"> <em class="lv">为什么</em> </strong> <em class="lv">是用来分类的？</em>在这篇文章中，我希望能回答所有这些问题，当你读完这篇文章时，你将会:</p><ol class=""><li id="428b" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">学习如何用简单的语言解释逻辑回归模型</li><li id="79b9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">了解逻辑回归的数学形式</li><li id="7567" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">使用Python从头实现逻辑回归</li></ol><p id="96e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，准备好迎接前方的狂野冒险吧，伙伴！</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mr"><img src="../Images/438241464c10e7c4a75fc56f13254af2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Wg-a7IVSAqE3lh90NYRQQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">塞巴斯蒂安·赫尔曼在<a class="ae ky" href="https://unsplash.com/s/photos/handshake?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="595e" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">逻辑回归模型解释了</h1><p id="2585" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">逻辑回归是一种统计模型，它使用逻辑函数来预测某个实例属于某个特定类的概率。如果估计的概率大于50%，则模型预测该实例属于正类(1)。如果不超过50%，那么模型预测属于负类。逻辑回归用于以下情况</p><ul class=""><li id="8aaf" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu np mc md me bi translated">一个病人到底有没有乳腺癌？</li><li id="a9d0" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">学生能通过考试吗？</li><li id="c20a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">给定的图像是否合适？</li><li id="3fef" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">这个信用卡交易是诈骗交易还是正常交易？</li></ul><p id="a9ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这些都是可以使用逻辑回归的例子。嗯，太好了！现在你知道如何定义什么是逻辑回归，并解释它做什么。但问题是，<em class="lv">如何运作，循序渐进</em>？为了解释这一点，我们将比较逻辑回归和线性回归！</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="6b56" class="nq mt it bd mu nr ns dn my nt nu dp nc li nv nw ne lm nx ny ng lq nz oa ni ob bi translated">通过线性回归理解逻辑回归</h2><p id="5d30" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">在我的<a class="ae ky" rel="noopener" target="_blank" href="/machine-learning-algorithms-from-start-to-finish-in-python-linear-regression-aa8c1d6b1169">上一篇文章</a>中，我解释了线性回归；它如何工作，做什么以及如何实现。在许多方面，逻辑回归与线性回归非常相似。因此，让我们简要总结一下线性回归的作用:</p><ol class=""><li id="78e8" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">线性趋势线或超平面适合数据</li><li id="3833" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">计算点之间的距离(图上的红点是点，绿线是距离)，然后平方，然后求和(值被平方以确保负值不会产生不正确的值并妨碍计算)。这是算法的误差，或者更好地称为<em class="lv">残差。</em></li></ol><p id="536d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.然后，我们使用一种<em class="lv">优化算法</em>(稍后将详细介绍)来“改变”该算法，使其能够基于<em class="lv">成本函数</em>更好地拟合数据</p><p id="3b32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.重复步骤2 + 3，直到我们达到理想的输出，或者我们的误差接近0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/3bae4d17db5db2dfb44d962b27da88e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*dnFljsilN6-wpo0orHg3Bw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="3093" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在逻辑回归中，情况非常相似，但也有一些不同:</p><ol class=""><li id="615e" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">逻辑回归预测一个<em class="lv">离散值</em> (0或1)，而线性回归用于预测<em class="lv">连续值</em> (245.6，89.6等..)</li><li id="45b8" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">它不是拟合数据的趋势线，而是拟合数据的S形曲线，称为<em class="lv">逻辑函数。</em>这条曲线的范围从0到1，告诉你一个类为正(1)或负(0)的概率。</li><li id="edc7" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">该模型的残差计算不适用于逻辑回归，这一点我们将在以后研究逻辑回归的细节时发现。因此，它也不使用与线性回归相同的<em class="lv">成本函数</em>。</li></ol></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="8654" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，现在你知道什么是逻辑回归了(用外行人的话来说)，这是对它的功能和操作步骤的高度概括。但我肯定你有一些问题，比如:</p><ul class=""><li id="9635" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu np mc md me bi translated">为什么Logistic回归<em class="lv">叫</em> Logistic <em class="lv">回归</em>，而不是Logistic <em class="lv">分类？</em></li><li id="f896" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">什么是<em class="lv">成本函数？</em>为什么使用线性回归和逻辑回归时会有所不同？</li><li id="0a7c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">什么是<em class="lv">优化算法</em>？</li></ul><p id="6beb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我向你保证，现在，你所寻求的一切都会得到满足！</p><h1 id="89a5" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">在我们继续之前，清理一些基本的统计数据</h1><blockquote class="od oe of"><p id="3d77" class="kz la lv lb b lc ld ju le lf lg jx lh og lj lk ll oh ln lo lp oi lr ls lt lu im bi translated">注意:现在，这些概念可能看起来不相关，但请耐心听我说，因为这些概念构成了逻辑回归的基础，将有助于您更好地理解算法。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/889e9736bd4a39b7dc7b671782399ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QBGNUsHl6A1jsd38tC3evg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·斯皮斯克在<a class="ae ky" href="https://unsplash.com/s/photos/basketball?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><ul class=""><li id="622c" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu np mc md me bi translated"><strong class="lb iu">几率:</strong>某事发生与某事未发生的比率。例如，让我们假设洛杉矶湖人队打了13场比赛，赢了5场，输了8场。那么，湖人赢得下一场比赛的几率将是发生的事情(湖人赢)和没有发生的事情(湖人输)的比率。所以，在这个例子中，他们赢的几率是5/8。</li><li id="e4cd" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">概率:某件事情发生与所有可能发生的事情的比率。回到我们湖人的例子，湖人赢得下一场比赛的概率是发生的事情(湖人获胜)与可能发生的事情(湖人输赢)的比率。所以，概率是5/13。</li></ul><p id="6ae2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经看到了一种计算赔率的方法，但是我们也可以使用下面的公式根据概率计算赔率:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/adbc587cffdaabd63f7aaf794fab8625.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*k-mBzcbYa-4rZLdOcO8NIg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="a828" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中p =某事发生的概率。为了验证这一点，让我们试着用这个公式看看我们是否能得到湖人获胜的几率:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/de127434238f28bf91f7c505ee2d0796.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*9xdhLQeB7xWPKPaPKpoamQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="83a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">果然，我们得到了同样的结果！</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="83ed" class="ms mt it bd mu mv om mx my mz on nb nc jz oo ka ne kc op kd ng kf oq kg ni nj bi translated">赔率的问题</h1><p id="4aca" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">回到我们的例子，让我们假设湖人队经历了一个糟糕的赛季(显然不是这样)，在20场比赛中，他们只赢了1场。所以湖人获胜的几率是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/7511f4592695e63ff6ab780ba00ab626.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*4C1Aoh0P2s_ahvKBt3ajjw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="b0ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果他们在整个赛季中打得比T8更差，并且在100场比赛中赢了2场，那么他们获胜的几率将是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/9371efb120b2d10644b08f9450b29535.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*XNJNl83-kuKmK7s_KAAsEQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="4699" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以做一个简单的观察:他们打得越差，他们的胜算就越接近于0。具体来说，当他们赢的几率为<em class="lv">时，那么几率将在0和1之间。</em></p><p id="1f2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们来看反面。如果湖人打20场比赛，赢19场，那么他们的胜算是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/8b7ad87dcd77365b0eb53d2b3d41f3fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*Fv6P5RUmssKQw5II-upKAw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="2359" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果他们玩200场游戏，赢了194场，那么他们赢的几率是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/fc9df88677698b189e2855a8cef63bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*UNh_RzrSnA2wJILXJorL4w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="1533" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，当湖人获胜的几率是1时，他们可以一直到无穷大。</p><p id="553e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，这里有一个问题。湖人获胜的几率从0到1不等，但是他们获胜的几率从1到无穷大不等。这种不对称使得很难比较湖人获胜的可能性。如果我们有一个函数能让一切都对称就好了…</p><h1 id="1452" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">介绍Logit函数</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/df1e09ffd7fc5f3ba9cfa59096dbaef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*36KSCTp-yhYAJuiG1agvgw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Eduardo Flores 在<a class="ae ky" href="https://unsplash.com/s/photos/symmetrical?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="2771" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，这个函数确实存在！它被称为概率的<em class="lv">对数函数、</em>或<em class="lv">对数。</em>本质上，它输出了赔率的…日志！</p><p id="d657" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们再次用湖人队的例子来证明这一点(对不起！).如果湖人要打7场比赛，并且只赢一场，他们获胜的几率将会是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/3bb10003d8cbe07c1651194ef2921f52.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*4O_NlrxhywE272qsuq84gQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="6b9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果他们赢了6场比赛，只输了1场:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/0677581b4a702b805fe5460efb72318e.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*7uSx7MQ4db8864d-sAbnaA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="7767" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用对数(赔率)，离原点的距离等于1比6，或6比1。这个logit函数对于理解是至关重要的，因为它构成了逻辑回归的基础。如果你仍然不确定什么是赔率和日志(赔率)，看看Statquest的这个伟大的<a class="ae ky" href="https://www.youtube.com/watch?v=ARfXDSkQf1Y&amp;t=315s" rel="noopener ugc nofollow" target="_blank">视频</a>。</p><h1 id="72ff" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">逻辑回归背后的“回归”</h1><p id="6ef4" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">好了，伙计们，我正式向你们扔炸弹了:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/d5413eab0aa8277da6db8ded68ff656a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRiUKYGt_fw6NpLKbO6VSA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@jens_johnsson?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Jens Johnsson </a>在<a class="ae ky" href="https://unsplash.com/s/photos/explosion?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="01f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逻辑回归(就其本身而言)不是一种分类算法。它实际上是一个<em class="lv">回归</em>模型(因此得名Logistic <em class="lv">回归</em>)！怎么会？好了，事不宜迟，让我们深入研究一下逻辑回归模型。</p><h1 id="5b38" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">深入研究逻辑回归</h1><p id="d14f" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">逻辑回归实际上是广义线性模型(GLM)的一部分，该模型最初是由约翰·内尔德和罗伯特·威德伯恩创建的。线性回归的响应值来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank"><em class="lv"/></a>，而逻辑回归的响应值来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Binomial_distribution" rel="noopener ugc nofollow" target="_blank"> <em class="lv">二项分布</em> </a>(值为0和1)。</p><p id="644a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逻辑回归是一种特殊类型的GLM，它可以通过使用<em class="lv">链接函数</em>使与响应变量相关联，并允许每个测量值的方差大小是其预测值的函数，从而推广线性回归。(<a class="ae ky" href="https://en.wikipedia.org/wiki/Generalized_linear_model" rel="noopener ugc nofollow" target="_blank">来源</a>)。</p><p id="585f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基本上，逻辑回归是一种GLM，通过使用logit链接函数输出值0和1，并可以使用特殊的<em class="lv">成本函数</em>来计算模型的方差。</p><p id="592a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，你一定对这一切是如何运作的感到难以置信，但是现在跟着我，它很快就会有意义。</p><h1 id="8799" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">逻辑回归如何工作第1部分:理论</h1><p id="e511" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">正如我们之前讨论的，理论上，线性回归的y轴值(或目标值)可以是从-无穷大到+无穷大范围内的任何数字。</p><p id="c80b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，在逻辑回归中，y轴值的范围仅在0和1之间。</p><p id="5b46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，为了解决这个问题，y轴值从X发生的概率转换成…某事发生的log(odds)！现在，值的范围可以从无穷大到+无穷大，就像线性回归一样。我们使用之前讨论过的logit函数来完成这个转换。</p><p id="3083" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，换句话说，我们把S形曲线转换成直线。这意味着，虽然我们仍然在逻辑回归中使用S形曲线，但系数是根据对数(赔率)计算的。</p><p id="0d31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这之后，算法本质上是线性模型；我们有了直线趋势线，然后我们使用<em class="lv">优化算法</em>以及用于逻辑回归的<em class="lv">修改成本函数</em>(稍后将详细介绍)<em class="lv">将其拟合到数据上。</em>一旦我们做出了预测，我们就使用一个链接函数将我们的预测“翻译”或反转，其中最常见的一个称为<em class="lv"> sigmoid函数，</em>回到一个概率。</p><p id="e66c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，现在你对逻辑回归的工作原理有了一点点的了解。让我们用一个数学例子来巩固你的知识。</p><h1 id="9324" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">逻辑回归如何工作第一部分:数学</h1><p id="741e" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">假设我们有一个具有两个特征的模型，<strong class="lb iu"> X </strong> 1 + <strong class="lb iu"> X </strong> 2，以及单二项式响应变量<strong class="lb iu"> Y </strong>，我们将其表示为p = <strong class="lb iu"> P </strong> (Y=1):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/3c5ec6f58f045f3557a93591a2ff4702.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*5X1T15pCtrx34Fz0cbBTFQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="d435" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Y=1的事件概率的对数可以表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/ad7cb9cc2945941141f10c5c92516f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*zuGhny--qtgDHDRWkHrS8A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="6ca8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过对对数赔率求幂来恢复赔率:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/5e37f5092b82d332eab4d8bb85d2c860.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*7tVCw-BH4EujrDK_EQURmA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="c8ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过简单的代数运算，<strong class="lb iu"> Y </strong> =1的概率为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/d5e05a6f1902549df385cbc9d46587c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*Ce2uK6Wn2NwVoj-1t-f4eA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="d0e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个公式本质上是将我们的对数优势预测转化为概率的链接函数。该函数也称为<em class="lv"> sigmoid函数</em>，也可以表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/d7cdc2e682bc55a2cc5b3a158412c72e.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/format:webp/1*FV6N4icRe1YJ1jvG2eCIeA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="758a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可以直观地看做一条S形曲线:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/e1d9c383bb0c14c104fa316ddf1f0e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*PaEjawWyMpwVttNjwswDnA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="0a75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们可以清楚地看到如何计算给定实例的概率对数，或者给定权重的情况下<strong class="lb iu"> Y </strong> =0的概率。使逻辑回归成为分类模型的部分是用于分割预测概率的阈值。没有这一点，逻辑回归是真正的回归模型。</p><h1 id="28ca" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">该算法如何工作的逐步指南</h1><ol class=""><li id="a4f7" class="lw lx it lb b lc nk lf nl li pe lm pf lq pg lu mb mc md me bi translated">随机初始化模型的参数(或系数)</li><li id="d8b3" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">将sigmoid函数应用于线性模型的预测</li><li id="6fb9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">通过使用优化算法更新参数，以更好地拟合数据。</li><li id="55c6" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">计算误差。</li><li id="88d6" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">对任一<em class="lv"> n </em>次迭代重复2-4，直到达到期望的结果或模型的计算误差接近或等于0。</li></ol></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="e9bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以现在你应该对逻辑回归的工作原理有了很好的理解。但我相信您可能仍有疑问，例如:</p><ol class=""><li id="f783" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">嘿，你从没解释过什么是优化算法！</li><li id="1a3b" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">伙计，什么是成本函数？</li><li id="ce95" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">等等，是线性模型吧？那么我们需要特征尺度吗？</li></ol><p id="4cd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，你抓到我了。所以，在我们开始编码之前，让我用通俗的语言解释一下这些概念。</p><h1 id="04ea" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">成本函数</h1><p id="f845" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">成本函数本质上是一个衡量损失的公式，或模型的“成本”。如果你曾经参加过任何Kaggle比赛，你可能会遇到一些。一些常见的包括:</p><ul class=""><li id="147e" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu np mc md me bi translated">均方误差</li><li id="cb6c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">均方根误差</li><li id="3a30" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">绝对平均误差</li><li id="47a0" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">原木损失</li></ul><p id="d2a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，有必要强调一下线性回归和逻辑回归之间的一个重要区别:</p><ul class=""><li id="237e" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu np mc md me bi translated">线性回归基本上是通过计算<em class="lv">残差</em>(每个点离假设有多远)来计算成本。这可以在下图中看到，因为绿线从下图中的点分叉:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/3bae4d17db5db2dfb44d962b27da88e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*dnFljsilN6-wpo0orHg3Bw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">维基百科</a>提供。</p></figure><p id="645f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意来自每个数据点的绿线。这是模型的<em class="lv">残差</em>，用于计算线性回归模型的成本。</p><ul class=""><li id="ebb7" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu np mc md me bi translated">当我先前说概率被转换成对数比值时，我没有提到一种情况；当X发生的概率为1时。在这种情况下，我们会得到以下输出:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/79e567b1b41aa15534713a409ea036f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*v85-59QX_dKWraOrRCJ3Bw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="3914" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们不能计算残差，因为点和假设之间的距离可能是无穷大。这就是为什么我们使用一个特殊的成本函数叫做<em class="lv">对数损失:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/bc57c856d4b21c70a2873f2c1e755673.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*DxrWJJWFF-B7tgbI0kWu4w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="2689" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个函数是从所谓的<em class="lv">最大似然估计</em>中得到的。我知道这篇文章的长度，所以如果你想得到更多的信息，我推荐你看一下这个<a class="ae ky" href="https://www.youtube.com/watch?v=BfKanl1aSG0&amp;frags=pl%2Cwn" rel="noopener ugc nofollow" target="_blank">视频</a>来获得更多关于这个功能的直觉。</p><p id="a101" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些函数对于模型训练和开发是必不可少的，因为它们回答了“<em class="lv">的基本问题:我的模型预测新实例有多好？</em>”。请记住这一点，因为这与我们的下一个主题有关。</p><h1 id="dd36" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">优化算法</h1><p id="5724" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">优化通常被定义为改进某样东西的过程，以使其发挥最大潜力。这也适用于机器学习。在ML的世界中，优化本质上是试图为某个数据集找到最佳的参数组合。这本质上是机器学习的“学习”部分。</p><p id="08e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然存在许多优化算法，但我将讨论其中最常见的两种:梯度下降和正规方程。</p><h1 id="29d2" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">梯度下降</h1><p id="cfee" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">梯度下降是一种优化算法，旨在找到一个函数的最小值。它通过在斜率的负方向迭代地采取步骤来实现这个目标。在我们的例子中，梯度下降通过移动函数切线的斜率来不断更新权重。好极了，听起来很棒。请说英语。:)</p><h1 id="c79a" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">梯度下降的一个具体例子</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/28a760ba9328be9e6fa65e8ca8f36b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5alB1wTkrZiAgNfb.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卢卡斯·克拉拉在<a class="ae ky" href="https://unsplash.com/s/photos/mountain?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="31ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更好地说明梯度下降，让我们通过一个简单的例子。想象一个人在山顶，他/她想到达山下。他们可以做的是环顾四周，看看他们应该朝哪个方向迈一步，以便更快地下来。然后，他们可能会朝那个方向迈出第<em class="lv">步</em>，现在他们离目标更近了。然而，他们下来时必须小心，因为他们可能会在某个点被<em class="lv">卡住</em>，所以我们必须确保<em class="lv">相应地选择我们的步长。</em></p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="a86c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，梯度下降的目标是最小化一个函数。在我们的案例中，是为了最小化我们模型的成本。这是通过找到函数的切线并向那个方向移动来实现的。算法的“<em class="lv">步骤</em>的大小由所谓的<em class="lv">学习率来定义。</em>这基本上控制了我们向下移动的距离。有了这个参数，我们必须小心两种情况:</p><ol class=""><li id="11cb" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">学习率太大，算法可能不收敛(达到最小值)并在最小值附近跳动，但永远不会收敛</li><li id="3730" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">学习率太小，算法将花费太长时间达到最小值，还可能“卡”在次优点。</li></ol><p id="83ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还有一个参数来控制算法在数据集上迭代的次数。</p><p id="31f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从视觉上看，该算法会做这样的事情:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/fb87d4eac1fc94d8a38163c3655b82db.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*MMPjjNCYasazpfju.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="0eff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为这种算法对机器学习来说非常重要，所以让我们回顾一下它的作用:</p><ol class=""><li id="3668" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">随机初始化权重。这被称为(你猜对了)<em class="lv">随机初始化</em></li><li id="5663" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">然后，该模型使用这些随机权重进行预测。</li><li id="1e96" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">模型的预测通过成本函数进行评估。</li><li id="0c64" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">然后，模型运行梯度下降，通过找到函数的切线，然后在切线的斜率中采取一个步骤</li><li id="01c5" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">该过程重复N次迭代，或者如果满足标准。</li></ol><h1 id="da61" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">梯度下降的优点和缺点</h1><h1 id="e017" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">优势:</h1><ol class=""><li id="8147" class="lw lx it lb b lc nk lf nl li pe lm pf lq pg lu mb mc md me bi translated">很可能将成本函数降低到全局最小值(非常接近或= 0)</li><li id="71a9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">最有效的优化算法之一</li></ol><h1 id="671f" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">缺点:</h1><ol class=""><li id="a2fa" class="lw lx it lb b lc nk lf nl li pe lm pf lq pg lu mb mc md me bi translated">在大型数据集上可能会很慢，因为它使用整个数据集来计算函数切线的梯度</li><li id="608e" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">容易陷入次优点(或局部最小值)</li><li id="569d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">用户必须手动选择学习速率和迭代次数，这可能很耗时</li></ol><p id="e8cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然已经介绍了梯度下降，那就来介绍法线方程吧。</p><h1 id="75f7" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">正态方程</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/ff2c46f811a9c0b7aecf287b7493938a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l1YMfCnbVYWGxJ8e6FFMCg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@saffu?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">萨夫</a>在<a class="ae ky" href="https://unsplash.com/s/photos/fast?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="c612" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们回到我们的例子，而不是采取步骤反复下山，我们将能够立即到达底部。法线方程就是这种情况。它利用线性代数来产生权重，可以在很短的时间内产生与梯度下降一样好的结果。</p><h1 id="6635" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">法线方程的优点和缺点</h1><h1 id="44cb" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">优势:</h1><ol class=""><li id="4ed5" class="lw lx it lb b lc nk lf nl li pe lm pf lq pg lu mb mc md me bi translated">不需要选择学习率或迭代次数</li><li id="136a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">极快</li></ol><h1 id="9b7f" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">缺点:</h1><ol class=""><li id="2a4e" class="lw lx it lb b lc nk lf nl li pe lm pf lq pg lu mb mc md me bi translated">无法很好地扩展到大型数据集</li><li id="2e4f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">倾向于产生良好的权重，但不是最佳的权重</li></ol></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="9b69" class="ms mt it bd mu mv om mx my mz on nb nc jz oo ka ne kc op kd ng kf oq kg ni nj bi translated">特征缩放</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/df70c43d1cca9082203e6ebff6945fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5c3uceUXV4DQdWIirmKBSw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com/s/photos/scaling?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae ky" href="https://unsplash.com/@calum_mac?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Calum MacAulay </a>拍摄的照片</p></figure><p id="c68b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是许多机器学习算法的重要预处理步骤，尤其是那些使用距离度量和计算的算法(如线性回归、梯度下降，当然还有逻辑回归，因为它确实是一个回归模型！).它基本上缩放了我们的特征，使它们在相似的范围内。把它想象成一栋房子，一栋房子的比例模型。两者的<strong class="lb iu">形状</strong>相同(都是房子)，但是<strong class="lb iu">大小</strong>不同(5m！= 500m)。我们这样做的原因如下:</p><ol class=""><li id="5d6f" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">它加速了算法</li><li id="5f14" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">有些算法对规模很敏感。换句话说，如果特征具有不同的比例，则具有较高量值的特征有可能被赋予较高的权重。这将影响机器学习算法的性能，显然，我们不希望我们的算法偏向一个特征。</li></ol><p id="8604" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了证明这一点，让我们假设我们有三个特性，分别命名为A、B和C:</p><ul class=""><li id="8a83" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu np mc md me bi translated">缩放前AB的距离= &gt;</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/8087f6cc2abd294a82f3088b7a1fe07a.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/0*0sSt44YMiK2cGEEo.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya </a>拍摄</p></figure><p id="21c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缩放前BC的距离= &gt;</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/b463310ee6269375634ac41abaa97578.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/0*WMuece8KCEhsLJMm.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya拍摄</a></p></figure><p id="8024" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缩放后AB的距离= &gt;</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/ebe477f56e3ede62e2aef68382cd59b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/0*a_p08JiLMrKVNNK8.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya拍摄</a></p></figure><p id="5264" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缩放后BC的距离= &gt;</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/25a56951a3d06291e53e267b9d161911.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/0*Bt4Ohc9M1hWm2ibS.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya拍摄</a></p></figure><p id="ed68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以清楚地看到，这些特性比缩放之前更具可比性，也更公正。如果你想要一个关于特性缩放的很棒的教程，请查看这篇由Analytics Vidhya撰写的<a class="ae ky" href="http://photo%20by%20analytics%20vidhya/" rel="noopener ugc nofollow" target="_blank">博客文章</a>。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="caba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">哇！你的大脑一定充满了大量的信息！因此，我建议在我们从头开始实际编写算法之前，休息一下，四处逛逛，享受生活，做些伸展运动！</p><h1 id="7320" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">从头开始编码逻辑回归</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/045be922706e2fbd1fa1c565ba5a6482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HF9kNOZL66CDkWay.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯里德在<a class="ae ky" href="https://unsplash.com/s/photos/code?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="6106" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，现在是你一直在等待的时刻；实施！事不宜迟，我们开始吧！</p><blockquote class="od oe of"><p id="d910" class="kz la lv lb b lc ld ju le lf lg jx lh og lj lk ll oh ln lo lp oi lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">注</em> </strong> <em class="it">:所有代码可以从</em> <a class="ae ky" href="https://github.com/Vagif12/ML-Algorithms-From-Scratch/blob/main/Logistic%20Regression%20from%20Scratch.py" rel="noopener ugc nofollow" target="_blank"> <em class="it">这个</em> </a> <em class="it"> Github repo下载。但是，我建议您在这样做之前先跟随教程，因为这样您会对您实际编码的内容有更好的理解！</em></p></blockquote><p id="33f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们做一些基本的导入:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="87cc" class="nq mt it ps b gy pw px l py pz">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.datasets import load_breast_cancer</span></pre><p id="e5cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用numpy进行数学计算，使用matplotlib绘制图表，使用来自scikit-learn的乳腺癌数据集。</p><p id="4a0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们加载数据并定义我们的特征和标签:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="fbd9" class="nq mt it ps b gy pw px l py pz"># Load and split data<br/>data = load_boston()<br/>X,y = data['data'],data['target']</span></pre><p id="7777" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们创建一个自定义的train_test_split函数，将我们的数据分成一个训练集和测试集:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="247a" class="nq mt it ps b gy pw px l py pz"># Custom train test split<br/>def train_test_divide(X,y,test_size=0.3,random_state=42):<br/>    np.random.seed(random_state)<br/>    train_size = 1 - test_size<br/>    arr_rand = np.random.rand(X.shape[0])<br/>    split = arr_rand &lt; np.percentile(arr_rand,(100*train_size))<br/>    <br/>    X_train = X[split]<br/>    y_train = y[split]<br/>    X_test =  X[~split]<br/>    y_test = y[~split]<br/>    <br/>    return X_train, X_test, y_train, y_test</span><span id="6e51" class="nq mt it ps b gy qa px l py pz">X_train,X_test,y_train,y_test = train_test_divide(X,y,test_size=0.3,random_state=42)</span></pre><p id="f563" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基本上，我们只是</p><ol class=""><li id="fbb8" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">进入测试尺寸。</li><li id="f44e" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">设置随机种子以确保我们的结果是可重复的。</li><li id="ba2c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">基于测试集大小获得训练集大小</li><li id="44fc" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">从我们的特征中随机选取样本</li><li id="f4ea" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">将随机选择的实例分成训练集和测试集</li></ol><p id="5cfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了制作干净、可重复和高效的代码，以及坚持软件开发实践，我们将创建一个逻辑回归类:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="cd90" class="nq mt it ps b gy pw px l py pz">class LogReg:<br/>    def __init__(self,X,y):<br/>        self.X = X<br/>        self.y = y<br/>        self.m = len(y)<br/>        self.bgd = False</span></pre><p id="01d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们添加链接函数作为该类的方法:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="b443" class="nq mt it ps b gy pw px l py pz">def sigmoid(self,z):<br/>        return 1/ (1 + np.exp(-z))</span></pre><p id="2067" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们使用上述公式实现我们的成本函数:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="3718" class="nq mt it ps b gy pw px l py pz">def cost_function(self,X,y):<br/>        h = self.sigmoid(X.dot(self.thetas.T))<br/>        m = len(y)<br/>        J = (1/m) * (-y.dot(h) - (1-y).dot(np.log(1-h)))<br/>        return J</span></pre><ul class=""><li id="0ac9" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu np mc md me bi translated">h =假设(我们的推断或预测)</li><li id="60c9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">m =训练集中的实例数量</li><li id="9a11" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">J =我们的成本函数</li></ul><p id="f97d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还将添加一个插入截取项的方法:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="e896" class="nq mt it ps b gy pw px l py pz">def add_intercept_term(self,X):<br/>        X = np.insert(X,0,np.ones(X.shape[0:1]),axis=1).copy()<br/>        return X</span></pre><ul class=""><li id="ac5f" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu np mc md me bi translated">这基本上是在我们的特性的开头插入一列1。</li><li id="0332" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">如果我们不加上这一点，那么我们将迫使超平面通过原点，导致它倾斜相当大，因此不能正确拟合数据。</li></ul><p id="6158" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将扩展我们的功能:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/26d96b2ddf62c858fb9ac721efa739c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*T10bazlvaD_24fsIiEGy0w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="4701" class="nq mt it ps b gy pw px l py pz">def feature_scale(self,X):<br/>        X = (X - X.mean()) / (X.std())<br/>        return X</span></pre><p id="c56d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将添加一个随机初始化模型参数的方法:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="89ab" class="nq mt it ps b gy pw px l py pz">def initialise_thetas(self):<br/>        np.random.seed(42)<br/>        self.thetas = np.random.rand(self.X.shape[1])</span></pre><p id="c5d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们将使用下面的公式实现法线方程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/160f6185319522e85a16384ae15afa4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*qYzKW9aVF1gfZcTi5GHdlQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="1c1d" class="nq mt it ps b gy pw px l py pz">def normal_equation(self):<br/>        A = np.linalg.inv(np.dot(self.X.T,self.X))<br/>        B = np.dot(self.X.T,self.y)<br/>        thetas = np.dot(A,B)<br/>        return thetas</span></pre><p id="0db2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，我们将算法分为3个部分:</p><ol class=""><li id="d03b" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">我们得到X转置和X的点积的倒数</li><li id="27fc" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们得到重量和标签的点积</li><li id="cd2a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们得到两个计算值的点积</li></ol><p id="9c17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，这就是正规方程！不算太差！现在，我们将使用以下公式实现批量梯度下降:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/195940eb654e803600cc15934e906165.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/0*pwT0lSPtqvbbFX01.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="d320" class="nq mt it ps b gy pw px l py pz">def batch_gradient_descent(self,alpha,n_iterations):<br/>        self.cost_history = [0] * (n_iterations)<br/>        self.n_iterations = n_iterations<br/>        <br/>        for i in range(n_iterations):<br/>            h = self.sigmoid(np.dot(self.X,self.thetas.T))<br/>            gradient = alpha * (1/self.m) * (h - self.y).dot(self.X)<br/>            self.thetas = self.thetas - gradient<br/>            self.cost_history[i] = self.cost_function(self.X,self.y)<br/>        <br/>        return self.thetas</span></pre><p id="b508" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们执行以下操作:</p><ol class=""><li id="114f" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">我们接受alpha，或者学习率，和迭代次数</li><li id="e9b7" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们创建一个列表来存储我们的成本函数历史，以便稍后绘制成线图</li><li id="09f7" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们遍历数据集n次迭代，</li><li id="ee7f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们获得预测，并计算梯度(函数切线的斜率)。这被称为h(x)</li><li id="8dbc" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们通过从实际值中减去我们的预测值并乘以每个特征来更新权重，以沿梯度向下移动</li><li id="b08f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们使用自定义的日志损失函数记录这些值。</li><li id="b6bd" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">重复，完成后，返回我们的最终优化参数。</li></ol><p id="93bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们创建一个拟合函数来拟合我们的数据:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="c08f" class="nq mt it ps b gy pw px l py pz">def fit(self,bgd=False,alpha=0.4,n_iterations=2000):<br/>        self.X = self.feature_scale(self.X)<br/>        if bgd == False:<br/>            self.add_intercept_term(self.X)<br/>            self.thetas = self.normal_equation()<br/>        else:<br/>            self.bgd = True<br/>            self.add_intercept_term(self.X)<br/>            self.initialise_thetas()<br/>            <br/>            self.thetas = self.batch_gradient_descent(alpha,n_iterations)</span></pre><p id="3bc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还将创建一个plot_function方法来显示成本函数在多个时期内的大小:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="e5af" class="nq mt it ps b gy pw px l py pz">def plot_cost_function(self):<br/>        <br/>        if self.bgd == True:<br/>            plt.plot(range((self.n_iterations)),self.cost_history)<br/>            plt.xlabel('No. of iterations')<br/>            plt.ylabel('Cost Function')<br/>            plt.title('Gradient Descent Cost Function Line Plot')<br/>            plt.show()<br/>        else:<br/>            print('Batch Gradient Descent was not used!') </span></pre><p id="d566" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将创建一个用于推理的预测函数，将阈值设置为0.5:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="6d56" class="nq mt it ps b gy pw px l py pz">def predict(self,X_test):<br/>        self.X_test = X_test.copy()<br/>        self.X_test = self.feature_scale(self.X_test)<br/>        h = self.sigmoid(np.dot(self.X_test,self.thetas.T))<br/>        predictions = (h &gt;= 0.5).astype(int)<br/>        return predictions</span></pre><p id="e540" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该类的完整代码如下所示:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="ec83" class="nq mt it ps b gy pw px l py pz">class LogReg:<br/>    def __init__(self,X,y):<br/>        self.X = X<br/>        self.y = y<br/>        self.m = len(y)<br/>        self.bgd = False<br/>        <br/>        <br/>    def sigmoid(self,z):<br/>        return 1/ (1 + np.exp(-z))<br/>    <br/>    def cost_function(self,X,y):<br/>        h = self.sigmoid(X.dot(self.thetas.T))<br/>        m = len(y)<br/>        J = (1/m) * (-y.dot(h) - (1-y).dot(np.log(1-h)))<br/>        return J</span><span id="742f" class="nq mt it ps b gy qa px l py pz">def add_intercept_term(self,X):<br/>        X = np.insert(X,0,np.ones(X.shape[0:1]),axis=1).copy()<br/>        return X<br/>        <br/>    def feature_scale(self,X):<br/>        X = (X - X.mean()) / (X.std())<br/>        return X<br/>    <br/>    def initialise_thetas(self):<br/>        np.random.seed(42)<br/>        self.thetas = np.random.rand(self.X.shape[1])</span><span id="5eea" class="nq mt it ps b gy qa px l py pz">def normal_equation(self):<br/>        A = np.linalg.inv(np.dot(self.X.T,self.X))<br/>        B = np.dot(self.X.T,self.y)<br/>        thetas = np.dot(A,B)<br/>        return thetas<br/>    <br/>    def batch_gradient_descent(self,alpha,n_iterations):<br/>        self.cost_history = [0] * (n_iterations)<br/>        self.n_iterations = n_iterations<br/>        <br/>        for i in range(n_iterations):<br/>            h = self.sigmoid(np.dot(self.X,self.thetas.T))<br/>            gradient = alpha * (1/self.m) * (h - self.y).dot(self.X)<br/>            self.thetas = self.thetas - gradient<br/>            self.cost_history[i] = self.cost_function(self.X,self.y)<br/>        <br/>        return self.thetas<br/>    <br/>    <br/>    def fit(self,bgd=False,alpha=0.4,n_iterations=2000):<br/>        self.X = self.feature_scale(self.X)<br/>        if bgd == False:<br/>            self.add_intercept_term(self.X)<br/>            self.thetas = self.normal_equation()<br/>        else:<br/>            self.bgd = True<br/>            self.add_intercept_term(self.X)<br/>            self.initialise_thetas()<br/>            <br/>            self.thetas = self.batch_gradient_descent(alpha,n_iterations)<br/>            <br/>    def plot_cost_function(self):<br/>        <br/>        if self.bgd == True:<br/>            plt.plot(range((self.n_iterations)),self.cost_history)<br/>            plt.xlabel('No. of iterations')<br/>            plt.ylabel('Cost Function')<br/>            plt.title('Gradient Descent Cost Function Line Plot')<br/>            plt.show()<br/>        else:<br/>            print('Batch Gradient Descent was not used!')<br/>            <br/>    def predict(self,X_test):<br/>        self.X_test = X_test.copy()<br/>        self.X_test = self.feature_scale(self.X_test)<br/>        h = self.sigmoid(np.dot(self.X_test,self.thetas.T))<br/>        predictions = (h &gt;= 0.5).astype(int)<br/>        return predictions</span></pre><p id="217d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们使用正规方程调用新创建的类:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="0f8b" class="nq mt it ps b gy pw px l py pz">log_reg_norm = LogReg(X_train,y_train)<br/>log_reg_norm.fit(bgd=False)</span></pre><p id="ee93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并评估:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="e223" class="nq mt it ps b gy pw px l py pz">accuracy = round((log_reg_norm.predict(X_test) == y_test).mean(),2)<br/>accuracy</span><span id="039c" class="nq mt it ps b gy qa px l py pz">OUT:<br/>0.82</span></pre><p id="e0c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，通过梯度下降，我们还可以绘制成本函数:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="7803" class="nq mt it ps b gy pw px l py pz">log_reg_bgd = LogReg(X_train,y_train)<br/>log_reg_bgd.fit(bgd=True)<br/>log_reg_bgd.plot_cost_function()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/08043a2f4d26cde003c821fa2252789a.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*ebBovFBu0NuMyVaY7Gh4zg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="ea2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们来评估一下:</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="461e" class="nq mt it ps b gy pw px l py pz">accuracy = round((log_reg_bgd.predict(X_test) == y_test).mean(),2)<br/>accuracy</span></pre><p id="007d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，现在你已经从零开始成功实现了逻辑回归！</p><h2 id="14bc" class="nq mt it bd mu nr ns dn my nt nu dp nc li nv nw ne lm nx ny ng lq nz oa ni ob bi translated">有些家庭作业要你做:</h2><ul class=""><li id="d183" class="lw lx it lb b lc nk lf nl li pe lm pf lq pg lu np mc md me bi translated">我们可以看到，在优化方面，批量梯度下降是明显的赢家。但是，您可以尝试用更少的迭代再次运行它，因为它似乎已经收敛了。有什么区别？</li><li id="42de" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">尝试跳过特征缩放。这对你的成绩有影响吗？</li><li id="3302" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu np mc md me bi translated">试着省去截距项。这有多大的影响？</li></ul></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="6dd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在过去的几个月里，我真的很喜欢写博客，我很感激所有关注我的人，他们总是对我的工作表示赞赏。因此，我要感谢你在百忙之中抽出时间来阅读这篇文章，我希望继续写出更多有趣的文章来与世界分享。敬请期待，祝你玩得愉快！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qf"><img src="../Images/7046934b63372ac4b3e9cd2f68b97958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Da87Siay-BrXeF8GcvqgsQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">蕾妮·费希尔在<a class="ae ky" href="https://unsplash.com/s/photos/goodbye?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div></div>    
</body>
</html>