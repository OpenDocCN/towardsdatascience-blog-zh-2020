<html>
<head>
<title>ONNX: Preventing Framework Lock in</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ONNX:防止框架锁定</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/onnx-preventing-framework-lock-in-9a798fb34c92?source=collection_archive---------27-----------------------#2020-10-28">https://towardsdatascience.com/onnx-preventing-framework-lock-in-9a798fb34c92?source=collection_archive---------27-----------------------#2020-10-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dbf6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">介绍如何使用ONNX标准实现深度学习框架之间的互操作性。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/34db356e9381ec149b4909b5bd343ddc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K5Ki9DFrqPLQdVVA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">里克·梅森在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a9a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇博客中，我们将看到什么是<strong class="lb iu"> ONNX </strong>标准，它的组成部分以及如何在不同的<em class="lv">深度学习框架</em>之间进行互操作。本博客将讨论以下部分:</p><ul class=""><li id="f1bb" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><strong class="lb iu">简介</strong></li><li id="c441" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">ONNX是什么？</strong></li><li id="c78f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">什么是ONNX运行时？</strong></li><li id="97db" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">互操作性:从PyTorch到其他框架</strong></li></ul><p id="1fbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以让我们开始吧！</p><h1 id="9346" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">介绍</h1><p id="93d2" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated"><em class="lv"> PyTorch </em>、<em class="lv"> Tensorflow </em>、<em class="lv"> Caffe2 </em>、<em class="lv"> MXNet </em>等，只是今天开发<em class="lv">深度学习模型</em>最流行的一些框架。尽管这些框架之间的共同点是<em class="lv">深度学习模型</em>的培训和调整，但“<em class="lv">它们如何做</em>”以及它们针对的领域是主要的区别点。一些框架更加面向研究(如<em class="lv"> PyTorch </em>)，而另一些则主要用于设备部署(如<em class="lv"> Tensorflow </em>)，同样，一些框架的设计架构基于静态图(如<em class="lv"> Tensorflow </em>和<em class="lv"> Caffe2 </em>)，而其他框架则基于动态图(如<em class="lv"> PyTorch </em>)。</p><p id="87f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，这些框架中的每一个都提供了不同于其他框架的优势，然而，我们如何将这些优势联系起来呢？我们如何互操作不同的框架？我们如何用框架“<em class="lv"> x </em>”优化一个模型，并在为框架“<em class="lv"> y </em>”优化的架构中部署它？嗯，这种类型的互操作性的实现要感谢<strong class="lb iu"> ONNX </strong>标准和<strong class="lb iu"> ONNX运行时</strong>(我们将在后面看到)。在图1中，描述了由<strong class="lb iu"> ONNX </strong>和<strong class="lb iu"> ONNX运行时解决的问题。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/114f1a350f2f8fc1b04030877faeddbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L6n8otv7SZ-HJJtLJOUOAQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。ONNX和ONNX运行时解决的问题|作者图片|取自<a class="ae ky" href="https://www.flaticon.com/home" rel="noopener ugc nofollow" target="_blank">平面图标</a>的图标</p></figure><p id="0761" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ONNX已经开始打破框架和硬件架构之间的依赖。<strong class="lb iu"> ONNX </strong>寻求成为不同<em class="lv">深度学习框架</em>之间<em class="lv">可移植性和互操作性</em>的默认标准。那么，让我们更详细地看看什么是<strong class="lb iu"> ONNX </strong>和什么是<strong class="lb iu"> ONNX运行时</strong>。</p><h1 id="90d2" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">ONNX是什么？</h1><p id="629e" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated"><strong class="lb iu"> ONNX </strong>是首字母缩写，代表<strong class="lb iu"> <em class="lv">开放式神经网络交换</em> </strong>。它指的是促进<em class="lv">深度学习框架之间互操作性的标准模型。ONNX 标准始于2017年，由微软、脸书和亚马逊三大巨头发起。基本想法是提出一个标准，允许已经众所周知的深度学习框架之间的可移植性和互操作性。</em></p><blockquote class="ni nj nk"><p id="30e1" class="kz la lv lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated"><a class="ae ky" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank">开放神经网络交换(ONNX) </a>是一个开放的生态系统，使人工智能开发者能够随着项目的发展选择正确的工具[1]。</p></blockquote><p id="9f9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> ONNX </strong>已经开始简化<em class="lv">机器学习模型</em>从研究到生产的生命周期，因为一些框架更适合原型化和优化模型，而其他框架则提供加速部署到不同设备的工具，这是很常见的。因此，需要特别提及的是，目前<strong class="lb iu"> ONNX </strong>已经具备支持推理的能力，即我们可以在框架“<em class="lv"> a </em>中训练一个模型，在框架“<em class="lv"> b </em>中进行推理。图2提供了对<em class="lv">框架互操作性的可视化描述。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/f322174a44fbe53f274566c04d2875fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3N6uPaLNEYDjtWBW1vdNoQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。ONNX互操作性|作者图片|取自原始来源的徽标</p></figure><blockquote class="ni nj nk"><p id="a1e7" class="kz la lv lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">ONNX规范针对以下三个组件来实现互操作性:<br/> 1 .一种可扩展计算图模型的定义。<br/> 2。标准数据类型的定义。<br/> 3。内置运算符的定义。</p></blockquote><p id="5295" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，已经有几个框架集成了一个扩展，能够在<strong class="lb iu"> ONNX </strong>规范下导出模型，同样，还没有集成导出模块的框架利用了工作良好的包装器。</p><p id="c29d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，到目前为止，我们已经知道为什么框架之间的<em class="lv">互操作性</em>需要一个标准，同样我们也已经知道<strong class="lb iu"> ONNX </strong>在这个生态系统中的参与情况，现在我们必须知道"<strong class="lb iu"> <em class="lv"> ONNX Runtime </em> </strong>"是什么，以及它在行业中产生的巨大影响，所以让我们开始吧！</p><h1 id="f403" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">什么是ONNX运行时？</h1><p id="eaea" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated"><strong class="lb iu"> ONNX Runtime </strong>是一个多平台加速器，专注于训练和模型推理，与最常见的<em class="lv">机器学习&amp;深度学习框架</em>【2】兼容。换句话说，<strong class="lb iu"> ONNX运行时</strong>就是<strong class="lb iu"> ONNX </strong>标准的实现。</p><p id="5198" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> ONNX运行时</strong>的出现是因为需要一个接口来加速不同硬件架构中的推理。在<strong class="lb iu"> ONNX运行时</strong>之前，将主要针对<em class="lv">基于CUDA的</em>架构优化的模型部署到<em class="lv">基于NUPHAR </em>、<em class="lv"> nGraph </em>、<em class="lv"> OpenVINO </em>的架构等是非常昂贵的。换句话说，在框架和硬件架构之间存在着一种依赖关系，模型就是为这种依赖关系而优化的。有了<strong class="lb iu"> ONNX </strong>标准和<strong class="lb iu"> ONNX运行时</strong>加速器，框架和硬件架构之间的互操作性的大门就敞开了。</p><p id="413e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ONNX运行时的一些主要优势是:</p><ul class=""><li id="b59d" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">推理性能提高，推理时间大大减少。</li><li id="eae7" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">减少培训时间</li><li id="4262" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">用Python开发和训练模型，并在基于C、C ++或Java的应用程序中部署。</li></ul><p id="6d04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很好，现在我们知道了<strong class="lb iu"> ONNX </strong>和<strong class="lb iu"> ONNX运行时</strong>在<em class="lv">互操作性和可移植性</em>方面的影响，让我们来看一个例子！</p><h1 id="8a38" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">互操作性:从PyTorch到其他框架</h1><p id="e5bf" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在下面的例子中，我们将演示如何使用<strong class="lb iu"> ONNX </strong>标准，以便能够在不同的<em class="lv">深度学习框架</em>之间进行互操作。</p><p id="c016" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">示例的架构给出如下，我们将在<strong class="lb iu"> PyTorch </strong>中训练一个分类器，然后我们将使用这个训练好的模型在<strong class="lb iu"> Tensorflow </strong>、<strong class="lb iu"> Caffe2 </strong>和<strong class="lb iu"> ONNX运行时</strong>中进行推理。该示例的架构如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/968fde86be4b3ec825bf6692edd7dfe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gtkM1GT26IdlMZ-QW-BQdg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。示例架构|作者图片|取自<a class="ae ky" href="https://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank">平面图标</a>的图标</p></figure><p id="b10d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">开始吧！</p><blockquote class="ni nj nk"><p id="d870" class="kz la lv lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">如果你想看看完整的代码，这是实现:<a class="ae ky" href="https://github.com/FernandoLpz/ONNX-PyTorch-TF-Caffe2" rel="noopener ugc nofollow" target="_blank">https://github.com/FernandoLpz/ONNX-PyTorch-TF-Caffe2</a>。随意克隆或者叉！</p></blockquote><p id="a5a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将创建<em class="lv">通用数据</em>，其想法是创建一个虚拟模型，因此让我们定义以下生成器，它将返回训练和测试数据:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段1。数据生成程序</p></figure><p id="f1aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们定义我们的虚拟模型的结构以及正向函数。我们基本上定义了两个线性层，这就够了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段2。PyTorch模型定义</p></figure><p id="c391" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完美！到目前为止，我们已经有了通用数据以及我们的虚拟模型，是时候训练模型了！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段3。PyTorch培训模式</p></figure><p id="5d36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所看到的，训练阶段非常简单，不需要太多的解释。那么让我们继续下面的，是时候将我们的模型导出到<strong class="lb iu"> ONNX </strong>标准了，因为这个<strong class="lb iu"> PyTorch </strong>已经为我们提供了一个扩展来导出<strong class="lb iu"> ONNX格式的模型</strong>，让我们看看我们是怎么做的！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段4。导出到ONNX</p></figure><p id="75eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们先来看第4行的if-else语句。我们正在定义一个"<em class="lv">虚拟输入</em>，因为<strong class="lb iu"> ONNX </strong>需要遍历由<strong class="lb iu"> PyTorch </strong>定义的整个图形，这样<strong class="lb iu"> ONNX </strong>将负责跟踪每个图形实例中定义的每个层和参数。在这种情况下，我们使用变量<em class="lv">定义一个通用输入，或者在它的情况下，我们将虚拟输入定义为训练中使用的真实输入。</em></p><p id="c90a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">稍后，在第9行和第10行中，我们定义了我们将分配给图中每个层的名称，这些名称将由<strong class="lb iu"> ONNX </strong>生成，否则<strong class="lb iu"> ONNX </strong>将使用通用名称。</p><p id="e556" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，在第13行中，我们使用了<strong class="lb iu"> PyTorch </strong>的<strong class="lb iu"> ONNX </strong>扩展，我们将训练好的模型、虚拟输入和分配给图中每个元素的名称作为参数传递。查看第22行中传递的参数很重要，因为推理张量在维度大小上可能有一些变化(通常是指批量大小的维度)，我们将这样的维度定义为"<em class="lv"> dynamic </em>"，因此在推理时，我们可以传递任何批量大小，而不是训练原始模型时使用的那个。</p><p id="91c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嗯，到目前为止，我们已经在<strong class="lb iu"> PyTorch </strong>中训练了一个模型，并保存在<strong class="lb iu"> ONNX </strong>标准下，现在我们看看如何用不同的框架加载这个模型来进行推理。</p><h2 id="b37f" class="ns ml it bd mm nt nu dn mq nv nw dp mu li nx ny mw lm nz oa my lq ob oc na od bi translated">ONNX运行时推理</h2><p id="fc52" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">为了用<strong class="lb iu"> ONNX运行时</strong>执行推理，我们需要导入<em class="lv">ONNX运行时</em>库，然后我们只需要用<em class="lv"> onnx模块</em>加载<em class="lv"> onnx模型</em>并生成预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段5。ONNX运行时推理</p></figure><h2 id="069e" class="ns ml it bd mm nt nu dn mq nv nw dp mu li nx ny mw lm nz oa my lq ob oc na od bi translated">咖啡2推断</h2><p id="1836" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">为了使用<strong class="lb iu"> caffe2 </strong>框架进行预测，我们需要为作为后端工作的<em class="lv"> onnx </em>导入<em class="lv"> caffe2扩展</em>(类似于tensorflow中的会话)，然后我们将能够进行预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段6。咖啡2推断</p></figure><h2 id="23d8" class="ns ml it bd mm nt nu dn mq nv nw dp mu li nx ny mw lm nz oa my lq ob oc na od bi translated">张量流推理</h2><p id="b0cf" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">要使用<strong class="lb iu"> Tensorflow </strong>进行预测，需要使用<em class="lv"> onnx_tf </em>模块，该模块提供一个包装器(模拟会话)，以便进行预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段7。张量流推理</p></figure><p id="b420" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">恭喜你！我们到达了博客的结尾。</p><blockquote class="ni nj nk"><p id="b926" class="kz la lv lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">如果你想看看完整的代码，这是https://github.com/FernandoLpz/ONNX-PyTorch-TF-Caffe2的实现:<a class="ae ky" href="https://github.com/FernandoLpz/ONNX-PyTorch-TF-Caffe2" rel="noopener ugc nofollow" target="_blank"/>。随意克隆或者叉！</p></blockquote><h1 id="9acf" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">结论</h1><p id="3f6d" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在这篇博客中，我们解释了<strong class="lb iu"> ONNX </strong>标准产生的必要性。我们也提出了<strong class="lb iu"> ONNX </strong>如何帮助防止<em class="lv">框架锁定</em>的想法。另一方面，我们解释了<strong class="lb iu"> ONNX </strong>标准与<strong class="lb iu"> ONNX </strong>运行时如何让我们减少<em class="lv">深度学习生命周期</em>中的时间，因为它加快了培训阶段和部署阶段之间的联系。<br/>最后，我们看到了一个例子，使用<strong class="lb iu"> ONNX </strong>，我们可以在给定的框架中训练一个模型，并在其他框架中执行推理。</p><h1 id="4aec" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">参考</h1><p id="d08c" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">[1]https://github.com/onnx/onnx<a class="ae ky" href="https://github.com/onnx/onnx" rel="noopener ugc nofollow" target="_blank"/></p><p id="05e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://microsoft.github.io/onnxruntime/docs/" rel="noopener ugc nofollow" target="_blank">https://microsoft.github.io/onnxruntime/docs/</a></p></div></div>    
</body>
</html>