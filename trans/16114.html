<html>
<head>
<title>Fake News Classification with Recurrent Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于递归卷积神经网络的假新闻分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fake-news-classification-with-recurrent-convolutional-neural-networks-4a081ff69f1a?source=collection_archive---------13-----------------------#2020-11-06">https://towardsdatascience.com/fake-news-classification-with-recurrent-convolutional-neural-networks-4a081ff69f1a?source=collection_archive---------13-----------------------#2020-11-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/39a3ba3b58d8be503d456d4c043e27a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m9d7IXkza5VEYjsJU9FL2w.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">马库斯·温克勒在<a class="ae kf" href="https://unsplash.com/s/photos/news?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="dccf" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="6c95" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">假新闻是一个在过去几年里引起广泛关注的话题，这是有充分理由的。随着社交媒体变得普及，通过传播错误信息来影响数百万人变得更加容易。作为人类，我们经常无法识别我们读到的新闻是真是假。<a class="ae kf" href="https://arxiv.org/abs/1708.07104" rel="noopener ugc nofollow" target="_blank">密执安大学的一项研究发现，人类参与者只能在70%的时候发现假新闻。但是神经网络能做得更好吗？请继续阅读，寻找答案。</a></p><p id="a3ae" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">本文的目标是回答以下问题:</p><ul class=""><li id="2d2f" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated"><strong class="lg iu">什么样的话题或关键词在真实新闻和假新闻中频繁出现？</strong></li><li id="a7fd" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated"><strong class="lg iu">如何利用深度神经网络识别假新闻故事？</strong></li></ul><h1 id="28eb" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">导入基本库</h1><p id="a847" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">而我下面导入的大多数库都是常用的(NumPy，Pandas，Matplotlib等。)，我还利用了以下有用的库:</p><ul class=""><li id="76ee" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated"><a class="ae kf" href="https://github.com/nalepae/pandarallel" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">Pandarallel</strong></a><strong class="lg iu"/>是一个很有帮助的库，可以并行运行对Pandas数据帧的操作，并实时监控每个worker的进度。</li><li id="c5b2" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated"><a class="ae kf" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">Spacy</strong></a><strong class="lg iu"/>是一个用于高级自然语言处理的库。它附带了英语、西班牙语和德语等语言的语言模型。在这个项目中，我安装并导入了英语语言模型<em class="mv"> en_core_web_md. </em></li></ul><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="8c8a" class="nf kh it nb b gy ng nh l ni nj">import numpy as np<br/>import pandas as pd<br/>from pandarallel import pandarallel<br/>pandarallel.initialize(progress_bar=True, use_memory_fs=False, )<br/>import spacy<br/>import en_core_web_md<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/>%matplotlib inline</span></pre><h1 id="169b" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">数据集</h1><p id="58c3" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我在这个项目中使用的数据集包含从下面列出的多个Kaggle新闻数据集中选择和聚合的数据:</p><ul class=""><li id="64c3" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated"><a class="ae kf" href="https://www.kaggle.com/mrisdal/fake-news" rel="noopener ugc nofollow" target="_blank">面对假新闻</a></li><li id="d665" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated"><a class="ae kf" href="https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset" rel="noopener ugc nofollow" target="_blank">真假新闻数据集</a></li><li id="7ca0" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated"><a class="ae kf" href="https://www.kaggle.com/ruchi798/source-based-news-classification" rel="noopener ugc nofollow" target="_blank">基于来源的假新闻分类</a></li><li id="d92d" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated"><a class="ae kf" href="https://www.kaggle.com/snapcrack/all-the-news" rel="noopener ugc nofollow" target="_blank">所有新闻:来自15家美国出版物的143，000篇文章</a></li></ul><p id="92fc" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如下面Pandas代码的输出所示，数据集大约有74，000行，有三列:新闻文章的<strong class="lg iu">标题</strong>，新闻文章的<strong class="lg iu">文本</strong>，以及一个<strong class="lg iu">二进制</strong> <strong class="lg iu">标签</strong>，指示新闻是真是假。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="f904" class="nf kh it nb b gy ng nh l ni nj">data = pd.read_csv('./data/combined_news_data.csv')<br/>data.dropna(inplace=True)<br/>data.info()</span><span id="4d57" class="nf kh it nb b gy nk nh l ni nj">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 74012 entries, 0 to 74783<br/>Data columns (total 3 columns):<br/> #   Column  Non-Null Count  Dtype <br/>---  ------  --------------  ----- <br/> 0   title   74012 non-null  object<br/> 1   text    74012 non-null  object<br/> 2   label   74012 non-null  int64 <br/>dtypes: int64(1), object(2)<br/>memory usage: 2.3+ MB</span></pre><h1 id="e4d6" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">探索性数据分析</h1><h2 id="1547" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated">伪造和真实新闻文章的分发</h2><p id="8c2f" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如下面使用Seaborn生成的图所示，数据集的虚假和真实新闻文章分布大致均匀，这对于二进制分类任务来说是最佳的。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="3165" class="nf kh it nb b gy ng nh l ni nj">sns.set(rc={'figure.figsize':(11.7,8.27)})<br/>sns.countplot(data['label'])</span></pre><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nw"><img src="../Images/e46a272742b84ccc0b1272401e6b2f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eX7r_5tZ5n1xbAWpa1h4OA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">真假新闻文章的分发。</p></figure><h2 id="01de" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated">文章长度分布(字数)</h2><p id="a368" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们还可以使用下面的代码检查新闻文章的文章长度分布，该代码创建了一个列，计算每篇文章的字数，并使用Seaborn的<a class="ae kf" href="https://seaborn.pydata.org/generated/seaborn.displot.html#seaborn.displot" rel="noopener ugc nofollow" target="_blank"> distplot </a>函数显示文章长度的分布。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="a2b6" class="nf kh it nb b gy ng nh l ni nj">data['length'] = data['text'].apply(lambda x: len(x.split(' ')))<br/>sns.distplot(data['length'])</span></pre><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/814f24af017576c0deff06f2af581cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m2Hy3y5TUffBthNv3jPABA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">字数分布。</p></figure><p id="a7be" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">使用Pandas的<a class="ae kf" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html" rel="noopener ugc nofollow" target="_blank"> describe </a>函数仔细查看这个分布，会产生以下输出。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="5421" class="nf kh it nb b gy ng nh l ni nj">data['length'].describe()</span><span id="1f79" class="nf kh it nb b gy nk nh l ni nj">count    74012.000000<br/>mean       549.869251<br/>std        629.223073<br/>min          1.000000<br/>25%        235.000000<br/>50%        404.000000<br/>75%        672.000000<br/>max      24234.000000<br/>Name: length, dtype: float64</span></pre><p id="82d6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">平均文章长度约为550字，平均文章长度为404字。这种分布是右偏的，75%的文章字数在672字以下，而最长的文章显然是超过24000字的异常值。为了建立一个模型，我们可以通过仅使用每篇文章中的前500个左右的单词来确定它是否是假新闻，从而获得令人满意的结果。</p><h1 id="4f48" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">数据准备</h1><h2 id="22f0" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated">预处理文本数据</h2><p id="86c9" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为大多数自然语言处理任务准备数据的第一步是预处理文本数据。对于这个任务，我在下面定义的<strong class="lg iu">预处理器</strong>函数中执行了以下预处理步骤:</p><ul class=""><li id="e72a" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated"><strong class="lg iu">使用正则表达式删除不需要的字符</strong>，如标点符号、HTML标签和表情符号。</li><li id="8d7a" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated"><strong class="lg iu">删除停用词</strong>(在英语中非常常见的词，通常不需要用于文本分类)。</li><li id="a617" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated"><strong class="lg iu">词条化</strong>，这是将一个单词简化为其词条或词典形式的过程。比如<em class="mv">跑</em>这个词就是<em class="mv">跑</em>、<em class="mv">跑</em>和<em class="mv">跑这几个词的引理。</em></li></ul><p id="e912" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我使用Python的regex库从文本数据中移除不需要的字符，并使用Spacy的中型英语语言模型(en_core_web_md)来执行停用词移除和词汇化。为了加速这个昂贵的文本处理函数的计算过程，我使用了Pandarallel的<strong class="lg iu"> parallel_apply </strong>函数，它在四个内核上并行执行过程。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="5902" class="nf kh it nb b gy ng nh l ni nj">import re <br/>from spacy.lang.en.stop_words import STOP_WORDS<br/><br/>nlp = en_core_web_md.load()<br/><br/>def preprocessor(text):<br/>    text = re.sub('&lt;[^&gt;]*&gt;', '', text)<br/>    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text)<br/>    text = re.sub('[\W]+', ' ', text.lower()) + ''.join(emoticons).replace('-', '')<br/>    doc = nlp(text)<br/>    text = ' '.join([token.lemma_ for token in doc if token.text not in STOP_WORDS])<br/>    return text</span><span id="9621" class="nf kh it nb b gy nk nh l ni nj">X = data['text'].parallel_apply(preprocessor)<br/>y = data['label']<br/><br/>data_processed = pd.DataFrame({'title': data['title'], 'text': X, 'label': y})</span></pre><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ny"><img src="../Images/35a190ec739d6a1fc2c683be7bbd2a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vZhb1Mtg548xBziIDVs1Qg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">parallel_apply函数显示的进度条输出。</p></figure><h1 id="97b5" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">基于潜在狄利克雷分配的主题建模</h1><p id="4119" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在对文本数据进行预处理后，我能够使用潜在狄利克雷分配(LDA)来比较真实和虚假新闻文章中的主题和最重要的术语。LDA是一种基于以下假设的无监督主题建模技术:</p><ul class=""><li id="d13c" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated">每个文档(在本例中是每篇新闻文章)都是一个由单词组成的<em class="mv">包</em>，这意味着在提取主题时不考虑文档中单词的顺序。</li><li id="29b9" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated">每个文档都有一个主题分布，每个主题都由词的分布来定义。</li><li id="3d11" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated">所有文档中都有<em class="mv"> k </em>主题。参数<em class="mv"> k </em>是为算法预先指定的。</li><li id="49b0" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated">包含属于特定主题的单词的文档的概率可以建模为<a class="ae kf" href="https://en.wikipedia.org/wiki/Dirichlet_distribution" rel="noopener ugc nofollow" target="_blank">狄利克雷分布</a>。</li></ul><p id="4ae8" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在其最简单的形式中，LDA算法对文档集合中的每个文档<em class="mv"> D </em>遵循以下步骤:</p><ol class=""><li id="b68a" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb nz mn mo mp bi translated">通过根据狄利克雷分布给每个单词分配一个主题，在文档<em class="mv"> D </em>中分配每个<em class="mv"> k </em>主题。</li><li id="7144" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb nz mn mo mp bi translated">对于<em class="mv"> D </em>中的每个单词，假设它的主题是错误的，但是每隔一个单词被分配正确的主题。</li><li id="3153" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb nz mn mo mp bi translated">根据以下因素为该单词分配属于每个主题的概率:<br/> -文档中的主题<em class="mv"> D <br/> - </em>该单词在所有文档中被分配给每个主题的次数。</li><li id="f9cb" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb nz mn mo mp bi translated">对所有文档重复步骤1-4。</li></ol><p id="715c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">关于LDA更详细但更容易理解的概述，请查看埃德温·陈的博客中的页面。</p><p id="0fc8" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我使用Scikit-learn的LDA模块来执行主题建模，并使用一个名为<a class="ae kf" href="https://github.com/bmabey/pyLDAvis" rel="noopener ugc nofollow" target="_blank"> pyLDAvis </a>的有用的Python库来创建真实和虚假新闻的主题模型的交互式可视化。下面给出了这项任务所需的导入。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="6edf" class="nf kh it nb b gy ng nh l ni nj">from sklearn.decomposition import LatentDirichletAllocation<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.manifold import TSNE<br/>from sklearn.pipeline import Pipeline<br/>import pyLDAvis.sklearn</span></pre><h2 id="e7fd" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated">真正的新闻</h2><p id="5620" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">下面给出的代码使用pyLDAvis对经过预处理的具有十个不同主题的真实新闻文章执行主题建模，然后创建一个交互式可视化，在二维空间中显示每个主题。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="f80e" class="nf kh it nb b gy ng nh l ni nj">real_news = data_processed[data_processed['label'] == 1]<br/><br/>num_topics = 10<br/>num_features=5000<br/><br/>vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=num_features, stop_words='english')<br/>lda = LatentDirichletAllocation(n_components=num_topics,<br/>                                max_iter=5, <br/>                                learning_method='online', <br/>                                learning_offset=50.,<br/>                                random_state=0)<br/><br/>lda_pipeline = Pipeline([('vectorizer', vectorizer), ('lda', lda)])<br/>lda_pipeline.fit(real_news['text'])</span><span id="f489" class="nf kh it nb b gy nk nh l ni nj">pyLDAvis.enable_notebook()<br/>data_vectorized = vectorizer.fit_transform(data_processed['text'])<br/>dash = pyLDAvis.sklearn.prepare(lda_pipeline.steps[1][1], data_vectorized, vectorizer, mds='tsne')</span><span id="d3f5" class="nf kh it nb b gy nk nh l ni nj">pyLDAvis.save_html(dash, 'real_news_lda.html')</span></pre><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oa"><img src="../Images/9228f369ec38ce9212ca20e75079f464.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3asgYkLdEMFm3GvRhazuyQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">真实新闻数据的LDA可视化。</p></figure><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ob"><img src="../Images/38e94a13794e5208b9e7d1a005258395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IsJcFuW27oJyx_muPg7x5Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">真实新闻数据中最大话题的热门术语。</p></figure><p id="61e9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">上面的可视化允许用户查看十个提取主题中每个主题的相对大小，同时显示每个主题的最相关术语。你可以点击查看完整的交互式可视化<a class="ae kf" href="https://www.amolmavuduru.me/real_news_lda" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="48f5" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated"><strong class="ak">假新闻</strong></h2><p id="ae1b" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">下面给出的代码复制了前面假新闻文章的步骤，以产生类似的交互式可视化效果。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="28c2" class="nf kh it nb b gy ng nh l ni nj">fake_news = data_processed[data_processed['label'] == 0]<br/><br/>num_topics = 10<br/>num_features=5000<br/><br/>vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=num_features, stop_words='english')<br/>lda = LatentDirichletAllocation(n_components=num_topics,<br/>                                max_iter=5, <br/>                                learning_method='online', <br/>                                learning_offset=50.,<br/>                                random_state=0)<br/><br/>lda_pipeline = Pipeline([('vectorizer', vectorizer), ('lda', lda)])<br/>lda_pipeline.fit(fake_news['text'])<br/><br/>pyLDAvis.enable_notebook()<br/>data_vectorized = vectorizer.fit_transform(data_processed['text'])<br/>dash = pyLDAvis.sklearn.prepare(lda_pipeline.steps[1][1], data_vectorized, vectorizer, mds='tsne')</span><span id="ff5e" class="nf kh it nb b gy nk nh l ni nj">pyLDAvis.save_html(dash, 'fake_news_lda.html')</span></pre><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/a3c8fc7b7ca08802f26fdd90e830e656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s9xPyYq78EWsHiffVnbg_A.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">假新闻数据的LDA可视化。</p></figure><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/1795e78bc768bdff65ed8f9b25fda580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9o1ECYG82IetCUFlJL_BSw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">假新闻数据中最大话题的热门词汇。</p></figure><p id="6d8e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">你可以在这里查看完整的互动可视化<a class="ae kf" href="https://www.amolmavuduru.me/fake_news_lda" rel="noopener ugc nofollow" target="_blank"/>。基于真实和虚假新闻的主题模型可视化，很明显，与真实新闻相比，虚假新闻通常涉及不同的主题。基于可视化和一些话题关键词，如<em class="mv">叛国</em>、<em class="mv">违规</em>、<em class="mv">可悲</em>、<em class="mv">狂奔</em>、<em class="mv"> </em>和<em class="mv">暴力</em>，假新闻似乎一般涵盖更具争议性的话题，如所谓的政治丑闻和阴谋论。</p><h1 id="0ea5" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">定义和训练模型</h1><p id="0c63" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我为这个任务设计的深度学习模型是一个递归卷积神经网络模型，由几种不同类型的顺序操作和层组成:</p><ol class=""><li id="b75e" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb nz mn mo mp bi translated">标记器用于将每篇文章转换成索引单词(标记)的向量。</li><li id="eeb3" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb nz mn mo mp bi translated">单词嵌入层，其为每个唯一单词学习具有<em class="mv"> m </em>维的嵌入向量，并将该嵌入应用于每个新闻文章中的前<em class="mv"> n </em>个单词，生成<em class="mv"> m </em> x <em class="mv"> n </em>矩阵。</li><li id="e7cb" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb nz mn mo mp bi translated">1D卷积和最大池层。</li><li id="92d4" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb nz mn mo mp bi translated">LSTM层，然后是辍学层。</li><li id="762e" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb nz mn mo mp bi translated">最终完全连接的层。</li></ol><p id="8048" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这些组件将在下面更详细地解释。</p><h2 id="3eeb" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated"><strong class="ak">分词器</strong></h2><p id="073a" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">标记器用于将每篇新闻文章分割成一个连续单词的向量，随后通过为每个单词分配一个唯一的整数索引，将该向量转换成整数向量。下图用一个简单的句子演示了这个过程。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/c23ca617c1d1c97aeabf324e5e1c11d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YFOfAoj-afvBOwXPCvry0Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">由记号赋予器执行的步骤。(图片由作者提供)</p></figure><h2 id="16e5" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated"><strong class="ak">字嵌入图层</strong></h2><p id="8362" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">单词嵌入是单词的可学习矢量表示，表示单词相对于其他单词的意思。深度学习方法可以从文本集合中学习单词嵌入，使得具有相似嵌入向量的单词倾向于具有相似的含义或表示相似的概念。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/3446833fc31f213a1a65f5df23e2df93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EAvwSSPTVaMWoG-pNxklXg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">一个句子的单词嵌入，每个单词有5维向量。(图片由作者提供)</p></figure><h2 id="a3dd" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated"><strong class="ak"> 1D卷积和最大池层</strong></h2><p id="f14f" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这些组件是递归卷积神经网络的<em class="mv">卷积</em>部分。如果你学过计算机视觉，你可能熟悉2D卷积和池层的图像数据操作。然而，对于文本数据，我们需要使用1D卷积和池层。1D卷积层具有一系列内核，这些内核是低维向量，在计算点积以产生输出向量时，这些内核会在输入向量上递增滑动。在下面的例子中，具有大小为2的核的1D卷积运算被应用于具有5个元素的输入向量。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/26da7ee2545b08e698adb3cacaa341f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WKmgGrCKjwroqdBiwZTDiQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">1D卷积运算的例子。(图片由作者提供)</p></figure><p id="6c35" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">与1D卷积层一样，1D最大池层也对向量进行操作，但通过从输入的局部区域中选择最大值来减小输入的大小。在下面的示例中，池大小为2的max-pooling操作应用于具有6个元素的vector。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/8cea1e97a213cfca0344f3d567f6650f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*9h2QItj0mUUGjLcbxx2FkA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">池大小为2的1D最大池操作示例。(图片由作者提供)</p></figure><h2 id="7b71" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated"><strong class="ak"> LSTMs </strong></h2><p id="7759" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">LSTM(长短期记忆)单元形成递归卷积神经网络的<em class="mv">递归</em>部分。LSTMs通常用于涉及序列数据的任务，例如时间序列预测和文本分类。我不会深入探究LSTMs背后的数学背景，因为该主题超出了本文的范围，但本质上，LSTM是神经网络中的一个单元，能够长时间记住重要信息，并在不再相关时忘记信息(因此得名，长短期记忆)。一个LSTM单元由三个门组成:</p><ul class=""><li id="729d" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated">接收输入值的<strong class="lg iu">输入门</strong>。</li><li id="0290" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated">一个<strong class="lg iu">遗忘门</strong>，它决定在培训期间获得的过去信息中有多少应该被记住。</li><li id="2919" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated">产生输出值的<strong class="lg iu">输出门</strong>。</li></ul><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/82279c16873fdd343f6a45c53a32621f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s20yBdJPsc9KcN1QYJ4SHA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">LSTM单位图。(图片由作者提供)</p></figure><p id="f22a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">LSTMs选择性记忆信息的能力在诸如假新闻分类的文本分类问题中是有用的，因为新闻文章开头的信息可能仍然与文章中间或接近结尾的内容相关。</p><h2 id="acb5" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated"><strong class="ak">全连通层</strong></h2><p id="e8ca" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这个模型的最后一部分只是一个完全连接的层，你可以在一个“香草”神经网络中找到。该层接收来自最后一个LSTM层的输出，并计算向量值的加权和，对该和应用sigmoid激活以产生最终输出-0和1之间的值，对应于文章是真实新闻的概率。</p><h2 id="66ea" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated"><strong class="ak">综合考虑</strong></h2><p id="d551" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我在下面创建的类是为定制和封装一个包含上述所有组件的模型而设计的。此类表示一个管道，它可以直接适合预处理的文本数据，而不必事先执行标记化和单词索引等步骤。LSTM _文本_分类器类扩展了Scikit-learn中的<a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator" rel="noopener ugc nofollow" target="_blank"> BaseEstimator </a>和<a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html" rel="noopener ugc nofollow" target="_blank"> ClassifierMixin </a>类，使其行为类似于Scikit-learn估计器。</p><figure class="mw mx my mz gt ju"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="3358" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">使用这个类，我在下面的代码中创建了一个包含以下组件的模型:</p><ul class=""><li id="2d29" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated">一个<strong class="lg iu">单词嵌入层</strong>，它为每个单词学习一个<strong class="lg iu"> 64维嵌入向量</strong>，并从新闻文章的<strong class="lg iu">前512个单词</strong>中聚合这些向量，为每个输入文章生成一个<strong class="lg iu">512×64嵌入矩阵</strong>。</li><li id="34f2" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated"><strong class="lg iu">三个卷积层</strong>，具有<strong class="lg iu"> 128个卷积滤波器</strong>和<strong class="lg iu">内核大小为5 </strong>，每个卷积层之后是<strong class="lg iu">最大池层</strong>。</li><li id="b2bf" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated"><strong class="lg iu">具有<strong class="lg iu"> 128个神经元</strong>的两个LSTM层</strong>，每个层之后是<strong class="lg iu">退出层</strong>，退出率为<strong class="lg iu">10%</strong>。</li><li id="98dc" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated">网络末端的<strong class="lg iu">全连接层</strong>，带有<strong class="lg iu"> sigmoid激活</strong>，输出从0到1的单个值，指示文章成为真实新闻的<strong class="lg iu">概率。</strong></li></ul><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="4f2c" class="nf kh it nb b gy ng nh l ni nj">lstm_classifier = LSTM_Text_Classifier(embedding_vector_length=64, max_seq_length=512, dropout=0.1, lstm_layers=[128, 128], batch_size=256, num_epochs=5, use_hash=False,<br/>conv_params={'filters': 128, <br/>             'kernel_size': 5, <br/>             'pool_size': 2,<br/>             'n_layers': 3})</span></pre><p id="27f0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">下面的可视化让我们很好地了解了这个递归卷积网络的模型架构。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/b1cdeba6a3bd05b941681d76ea72c474.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*qi_BytNtP3qLENzdm9XByw.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">具有单词嵌入层的递归卷积神经网络结构。(图片由作者提供)</p></figure><h2 id="e585" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated">培训、验证和测试分离</h2><p id="2a5c" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了有效地评估此模型的性能，有必要将数据分成单独的训练集、验证集和测试集。根据下面的代码，<strong class="lg iu">30%的数据用于测试</strong>，剩下的70%<strong class="lg iu">14%(70的20%)用于验证</strong>，剩下的<strong class="lg iu">56%用于训练</strong>。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="2c5b" class="nf kh it nb b gy ng nh l ni nj">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)<br/>X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)</span></pre><h2 id="e48a" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated">模特培训</h2><p id="e565" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在定义了这个复杂的模型之后，我能够在训练集上训练它，同时在验证集上监控它的性能。该模型经过三个时期的训练，并在第二个训练时期结束时根据下面的代码和输出达到其峰值验证性能。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="b1dc" class="nf kh it nb b gy ng nh l ni nj"><br/>lstm_classifier.fit(X_train, y_train, validation_data=(X_valid, y_valid))</span><span id="9992" class="nf kh it nb b gy nk nh l ni nj">Fitting Tokenizer...<br/>Model: "sequential_4"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_4 (Embedding)      (None, 512, 64)           13169920  <br/>_________________________________________________________________<br/>conv1d_10 (Conv1D)           (None, 512, 256)          82176     <br/>_________________________________________________________________<br/>max_pooling1d_10 (MaxPooling (None, 256, 256)          0         <br/>_________________________________________________________________<br/>conv1d_11 (Conv1D)           (None, 256, 512)          655872    <br/>_________________________________________________________________<br/>max_pooling1d_11 (MaxPooling (None, 128, 512)          0         <br/>_________________________________________________________________<br/>conv1d_12 (Conv1D)           (None, 128, 768)          1966848   <br/>_________________________________________________________________<br/>max_pooling1d_12 (MaxPooling (None, 64, 768)           0         <br/>_________________________________________________________________<br/>lstm_7 (LSTM)                (None, 64, 128)           459264    <br/>_________________________________________________________________<br/>dropout_7 (Dropout)          (None, 64, 128)           0         <br/>_________________________________________________________________<br/>lstm_8 (LSTM)                (None, 128)               131584    <br/>_________________________________________________________________<br/>dropout_8 (Dropout)          (None, 128)               0         <br/>_________________________________________________________________<br/>dense_4 (Dense)              (None, 1)                 129       <br/>=================================================================<br/>Total params: 16,465,793<br/>Trainable params: 16,465,793<br/>Non-trainable params: 0<br/>_________________________________________________________________<br/>None<br/>Fitting model...<br/><br/><br/><br/>Train on 41446 samples, validate on 10362 samples<br/>Epoch 1/5<br/>41446/41446 [==============================] - 43s 1ms/step - loss: 0.2858 - accuracy: 0.8648 - val_loss: 0.1433 - val_accuracy: 0.9505<br/>Epoch 2/5<br/>41446/41446 [==============================] - 42s 1ms/step - loss: 0.0806 - accuracy: 0.9715 - val_loss: 0.1192 - val_accuracy: 0.9543<br/>Epoch 3/5<br/>41446/41446 [==============================] - 43s 1ms/step - loss: 0.0381 - accuracy: 0.9881 - val_loss: 0.1470 - val_accuracy: 0.9527<br/>Epoch 00003: early stopping</span></pre><h2 id="a332" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated">验证结果</h2><p id="e51c" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">虽然准确度是分类的有用度量，但它不能告诉我们该模型在检测每个类别方面表现如何。下面提供的代码为验证数据集上的模型预测计算了<strong class="lg iu">混淆矩阵</strong>和<strong class="lg iu">分类报告</strong>，以更好地描述模型的性能。混淆矩阵以下列格式提供分类统计数据:</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/e6a2d08abe70b6d6ebd280e7eb2f5039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BefgkrW5BXKp7y-wLIcjEQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">如何解读一个混淆矩阵？(图片由作者提供)</p></figure><p id="db9a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">每个类别的分类报告提供了以下附加指标:</p><ol class=""><li id="38e6" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb nz mn mo mp bi translated">精度-类被正确预测的次数除以模型预测该类的总次数。</li><li id="30eb" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb nz mn mo mp bi translated">召回—正确预测类别的次数除以测试数据中带有该类别标签的样本总数。</li><li id="4094" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb nz mn mo mp bi translated">F1分数——精确度和召回率的调和平均值。</li></ol><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="3d6f" class="nf kh it nb b gy ng nh l ni nj">lstm_classifier.load_model('best_model')</span><span id="9c9d" class="nf kh it nb b gy nk nh l ni nj">from sklearn.metrics import confusion_matrix, classification_report<br/><br/>y_pred = lstm_classifier.predict_classes(X_valid)<br/>print(confusion_matrix(y_valid, y_pred))<br/>print(classification_report(y_valid, y_pred, digits=4))</span><span id="433e" class="nf kh it nb b gy nk nh l ni nj">[[4910  204]<br/> [ 271 4977]]<br/>              precision    recall  f1-score   support<br/><br/>           0     0.9477    0.9601    0.9539      5114<br/>           1     0.9606    0.9484    0.9545      5248<br/><br/>    accuracy                         0.9542     10362<br/>   macro avg     0.9542    0.9542    0.9542     10362<br/>weighted avg     0.9542    0.9542    0.9542     10362</span></pre><p id="b35c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">基于上述结果，我们可以清楚地看到，该模型在正确检测假新闻方面几乎与正确检测真实新闻一样好，并在验证数据上实现了95.42%的<strong class="lg iu">总体准确率</strong>，这是非常令人印象深刻的。根据混淆矩阵，<strong class="lg iu">只有271篇文章被误归类为假新闻</strong>和<strong class="lg iu">只有204篇文章被误归类为真新闻</strong>。</p><h2 id="b093" class="nf kh it bd ki nl nm dn km nn no dp kq lp np nq ku lt nr ns ky lx nt nu lc nv bi translated">测试结果</h2><p id="270e" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">虽然验证结果可以为我们提供模型在看不见的数据上的性能的一些指示，但是在模型训练过程中根本没有触及的测试集提供了模型性能的最佳客观和统计上正确的度量。下面的代码为测试集生成一个分类报告。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="12ff" class="nf kh it nb b gy ng nh l ni nj">from sklearn.metrics import accuracy_score<br/><br/>y_pred_test = lstm_classifier.predict_classes(X_test)<br/>print(classification_report(y_test, y_pred_test))</span><span id="7738" class="nf kh it nb b gy nk nh l ni nj">               precision    recall  f1-score   support<br/><br/>           0       0.94      0.95      0.95     11143<br/>           1       0.95      0.94      0.95     11061<br/><br/>    accuracy                           0.95     22204<br/>   macro avg       0.95      0.95      0.95     22204<br/>weighted avg       0.95      0.95      0.95     22204</span></pre><p id="477b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">基于上面的输出，该模型在测试集上实现了与其在验证集上的性能相似的性能水平。该模型以95%的准确率对测试集中的新闻文章进行分类。与人类只能在70%的时间里发现假新闻的研究相比，这些结果是有希望的，并证明了一个经过训练的神经网络可能比人类读者在过滤假新闻方面做得更好。</p><h1 id="c740" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><ul class=""><li id="1247" class="mh mi it lg b lh li ll lm lp on lt oo lx op mb mm mn mo mp bi translated">基于LDA可视化，我们可以看到真实和虚假新闻的主题和相关关键词有不同的分布。</li><li id="6dd1" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated">这个项目中使用的递归卷积神经网络能够在测试数据上以95%的准确率区分真实和虚假的新闻文章，这表明神经网络可能比人类读者更好地检测假新闻。</li></ul><p id="ca57" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">您可以在GitHub 上查看Jupyter笔记本和本文的代码。</p><h1 id="2ea3" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">来源</h1><ol class=""><li id="9f54" class="mh mi it lg b lh li ll lm lp on lt oo lx op mb nz mn mo mp bi translated">动词 （verb的缩写）佩雷斯-罗萨斯，b .克莱因伯格，a .勒费夫尔，r .米哈尔恰，<a class="ae kf" href="https://arxiv.org/abs/1708.07104" rel="noopener ugc nofollow" target="_blank">假新闻自动检测</a> <strong class="lg iu">，</strong> (2018)，arXiv.org</li><li id="7031" class="mh mi it lg b lh mq ll mr lp ms lt mt lx mu mb nz mn mo mp bi translated">A.Bharadwaj，B. Ashar，P. Barbhaya，R. Bhatia，Z. Shaikh，<a class="ae kf" href="http://www.ijirset.com/upload/2020/june/115_4_Source.PDF" rel="noopener ugc nofollow" target="_blank">利用机器学习进行基于来源的假新闻分类</a>，(2020)，《国际科学、工程和技术创新研究杂志》</li></ol></div></div>    
</body>
</html>