<html>
<head>
<title>AlphaZero, a novel Reinforcement Learning Algorithm, in JavaScript</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一种新的强化学习算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/alphazero-a-novel-reinforcement-learning-algorithm-deployed-in-javascript-56018503ad18?source=collection_archive---------8-----------------------#2020-11-08">https://towardsdatascience.com/alphazero-a-novel-reinforcement-learning-algorithm-deployed-in-javascript-56018503ad18?source=collection_archive---------8-----------------------#2020-11-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6c51" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解并实现AlphaZero，完全用JavaScript实现！</h2></div><p id="a3dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇博文中，你将了解并实现<a class="ae le" href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go" rel="noopener ugc nofollow" target="_blank"> AlphaZero </a>，这是一种令人兴奋的新颖的强化学习算法，用于在<a class="ae le" href="https://en.wikipedia.org/wiki/Go_(game)" rel="noopener ugc nofollow" target="_blank">围棋</a>和国际象棋等游戏中击败世界冠军。您将使用它来掌握一个钢笔和铅笔游戏(<a class="ae le" href="https://en.wikipedia.org/wiki/Dots_and_Boxes" rel="noopener ugc nofollow" target="_blank">点和方框</a>)，并将其部署到一个web应用程序中，完全用JavaScript编写。</p><p id="9960" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaZero的关键和最令人兴奋的方面是它能够在不依赖外部知识的情况下，在棋盘游戏中获得超人的行为。AlphaZero通过与自己对弈(自我对弈)和从那些经历中学习来学习掌握游戏。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/4467a83935c08159870d1f335a3f006e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*FMIJr962t3YfgcndOrH76A.gif"/></div></div></figure><p id="055b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将利用<a class="ae le" href="https://github.com/suragnair/alpha-zero-general" rel="noopener ugc nofollow" target="_blank"> Github </a>中可用的<a class="ae le" href="https://github.com/suragnair" rel="noopener ugc nofollow" target="_blank"> Surag Nair </a>的AlphaZero的“简化的、高度灵活的、有注释的、易于理解的实现”Python版本。</p><p id="162b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在这里玩游戏<a class="ae le" href="https://carlos-aguayo.github.io/alphazero/" rel="noopener ugc nofollow" target="_blank"/>。WebApp和JavaScript实现在这里<a class="ae le" href="https://github.com/carlos-aguayo/carlos-aguayo.github.io/tree/master/alphazero" rel="noopener ugc nofollow" target="_blank">可用</a>。这段代码是从这个Python <a class="ae le" href="https://github.com/carlos-aguayo/alpha-zero-general/tree/dotsandboxes/dotsandboxes" rel="noopener ugc nofollow" target="_blank">实现</a>移植过来的。</p><p id="c23e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://carlos-aguayo.github.io/alphazero/" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">https://carlos-aguayo.github.io/alphazero/</strong></a></p><p id="52f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaZero是由<em class="lr"> Silver，David等人在论文中描述的“</em> <a class="ae le" href="https://www.nature.com/articles/nature24270" rel="noopener ugc nofollow" target="_blank"> <em class="lr">掌握没有人类知识的围棋游戏</em></a><em class="lr">”nature 550.7676(2017):354–359</em>。</p><h1 id="8c7c" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">点和盒子游戏</strong></h1><p id="1af4" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">《点和盒子》是一个初级的儿童游戏，有着惊人的复杂程度。</p><p id="0875" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两名玩家轮流在两个相邻的点之间放置一条水平线或垂直线。完成1×1方块第四边的玩家得一分，并获得另一轮机会。棋盘满了，游戏结束，得分最多的玩家获胜。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mp"><img src="../Images/4b51a5455d132290837eacc61e5b128e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HIQFy-K6cD8FnckciKNOfA.gif"/></div></div></figure><h1 id="2e69" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">人工智能和桌游</strong></h1><p id="9a64" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">我们已经思考了很长时间，如果机器可以证明智能。如何验证机器展现智能行为的能力？一种典型的方式是玩棋盘游戏，如国际象棋，并试图获得超人的行为来击败世界冠军。</p><p id="9b9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1957年，<a class="ae le" href="https://www.aaas.org/birth-modern-computing" rel="noopener ugc nofollow" target="_blank">希尔伯特·西蒙预言计算机系统将在十年内击败国际象棋世界冠军i </a>。它花了更长的时间，但在1997年5月的<a class="ae le" href="https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov#1997_rematch" rel="noopener ugc nofollow" target="_blank">，一台计算机打败了国际象棋世界冠军加里·卡斯帕罗夫</a>。</p><p id="41aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管这个里程碑意义非凡，但人们可能会争论这个计算机系统是否是“智能的”。</p><p id="0dc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些计算机系统由三部分组成:</p><ol class=""><li id="a1ac" class="mq mr it kk b kl km ko kp kr ms kv mt kz mu ld mv mw mx my bi translated">自定义的<a class="ae le" href="https://en.wikipedia.org/wiki/Evaluation_function" rel="noopener ugc nofollow" target="_blank">评估函数</a>。</li><li id="540d" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">博弈树搜索算法。</li><li id="b22e" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">非常强大的硬件。</li></ol><p id="c164" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">评估函数将棋盘作为输入，并返回棋盘的“值”。高值表示当前玩家处于非常有利的位置。例如，棋手将要将死的棋盘配置将具有非常高的值。</p><p id="d39e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">博弈树搜索算法(如<a class="ae le" href="https://en.wikipedia.org/wiki/Minimax" rel="noopener ugc nofollow" target="_blank"> Minimax </a>)搜索<strong class="kk iu">所有</strong>可能的走法，寻找一个能保证有高价值的棋盘配置的路径。通过不访问那些我们知道找不到更好价值的配置，可以提高搜索效率。这就是<a class="ae le" href="https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning" rel="noopener ugc nofollow" target="_blank">Alpha–beta修剪</a>的作用。</p><p id="109d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，添加非常强大的硬件，你将拥有一台能够击败世界冠军的机器。</p><p id="3832" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有什么条件？<em class="lr">有经验的玩家</em> <a class="ae le" href="https://www.nytimes.com/1997/05/18/nyregion/what-deep-blue-learned-in-chess-school.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">手动工艺</em> </a> <em class="lr">这些评价函数</em>。这些系统将依靠<a class="ae le" href="https://en.wikipedia.org/wiki/Chess_opening" rel="noopener ugc nofollow" target="_blank">开局</a>书来指示最著名的棋步。中级游戏<a class="ae le" href="https://en.wikipedia.org/wiki/Chess_middlegame" rel="noopener ugc nofollow" target="_blank">将使用通过研究大师的游戏而创建的评估函数。然后大师们会进一步微调它们。</a></p><p id="67f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，我们可以为点和盒子游戏创建一个评估函数。一个合理而直接的评估函数是比较分数。分数中的正delta越高，棋盘游戏越有利。在大多数情况下，这是可行的。然而，在点和盒子游戏中，就像在许多棋盘游戏中一样，最好的行动可能包括牺牲短期利益以获得更好的长期收益。在点和盒子游戏中，有时最好不要在盒子中得分，以避免获得另一轮机会，相反，要迫使对方移动。然后，我们必须调整我们的评估函数，以考虑越来越多的复杂场景！</p><p id="c80c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">击败卡斯帕罗夫的<a class="ae le" href="https://www.sciencedirect.com/science/article/pii/S0004370201001291/pdf" rel="noopener ugc nofollow" target="_blank">评价函数</a>有8000个特征！这些功能中的大部分都是手工创建和调整的！</p><p id="9cda" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，在不贬低像击败国际象棋世界冠军这样的重要里程碑的情况下，一个不可取的方面是需要顶级玩家定义这些计算机的行为，并手动调整如此多的变量。</p><h1 id="94a2" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">alpha zero是什么，为什么这么激动人心？</strong></h1><p id="1947" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">AlphaZero是第一个能够在国际象棋或围棋等游戏中获得超人行为的计算机系统，它击败了世界冠军，仅依靠游戏规则而没有人类领域的知识。</p><p id="7d12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果只给定游戏规则，AlphaZero将开始自相残杀。一点一点地学习策略和技术，相对快速地获得超人的行为。</p><p id="32c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">像深蓝这样的系统需要国际象棋专家的帮助，而AlphaZero会通过与自己对弈变得强大。此外，AlphaZero不仅在国际象棋上表现出超人的力量，在围棋上也是如此。围棋对电脑来说是一个更复杂的游戏，因为它的<a class="ae le" href="https://en.wikipedia.org/wiki/Go_(game)#Software_players" rel="noopener ugc nofollow" target="_blank">更大的游戏空间和其他因素</a>。</p><p id="20b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然人类从数千年的数百万场游戏中积累了像T4围棋和国际象棋这样的游戏知识，但AlphaZero，一种只使用游戏规则的简单算法，在几天内重新发现了这些知识和新策略。</p><p id="0c29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">甚至还有一部关于它的电影<a class="ae le" href="https://www.youtube.com/watch?v=WXuK6gekU1Y" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="11ad" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">仅通过自我游戏，AlphaZero将如何学习？</h1><p id="bf2d" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">回想一下，像DeepBlue这样的系统依赖于人定义的“<a class="ae le" href="https://en.wikipedia.org/wiki/Evaluation_function" rel="noopener ugc nofollow" target="_blank">评估函数</a>，它将电路板状态作为输入，并输出状态的“值”。</p><p id="f172" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如今，深度学习模型非常容易将图像作为输入，并将其分类为狗或猫。这个想法是将棋盘作为深度学习模型的输入，并训练它预测棋盘是赢还是输的配置。</p><p id="6771" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是训练机器学习，需要数据，大量的数据。你从哪里得到游戏的数据集？很简单，我们让计算机自己玩，生成一组游戏，并从中制作一个数据集。</p><h1 id="52fd" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak"> AlphaZero训练算法</strong></h1><p id="72b7" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">算法很简单:</p><ol class=""><li id="766b" class="mq mr it kk b kl km ko kp kr ms kv mt kz mu ld mv mw mx my bi translated">让电脑和自己玩几局游戏，记录棋盘的每一步棋。一旦我们知道了结果，在游戏结束时用给定的结果更新所有的棋盘，“赢”或“输”。然后，我们建立了一个数据集，提供给神经网络(NN ),并开始学习给定的电路板配置是赢还是输。</li><li id="14f2" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">克隆神经网络。使用上一步中生成的数据集训练克隆。</li><li id="36fd" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">让克隆神经网络和原始神经网络相互对抗。</li><li id="6119" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">挑一个神经网络赢的，丢掉另一个。</li><li id="e626" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">转到步骤1。</li></ol><p id="f47f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就像魔术一样，经过多次迭代，你就有了世界级的模型。这个模型仅用了<a class="ae le" href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go" rel="noopener ugc nofollow" target="_blank"> 4个小时</a>就成功超越了最强的电脑象棋程序！</p><h1 id="028e" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">AlphaZero组件</h1><p id="c801" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">AlphaZero有两个组成部分。我们已经讨论过第一个，也就是神经网络。第二个是“蒙特卡罗树搜索”或MCTS。</p><ol class=""><li id="f365" class="mq mr it kk b kl km ko kp kr ms kv mt kz mu ld mv mw mx my bi translated"><strong class="kk iu">神经网络(NN) </strong>。将棋盘配置作为输入，输出棋盘的值，以及所有可能走法的概率分布。</li><li id="98e4" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated"><strong class="kk iu">蒙特卡罗树搜索(MCTS) </strong>。理想情况下，神经网络足以选择下一步行动。我们仍然希望看到尽可能多的董事会配置，并确保我们确实选择了最好的行动。像极小极大一样，MTCS是一种算法，它将帮助我们了解董事会的配置。不像极小极大，MTCS将有助于有效地探索游戏树。</li></ol><h1 id="9d6e" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">让我们深入细节，确定下一步行动</strong></h1><p id="a32d" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">跟随AlphaZero更容易，首先看它在决定下一步棋(竞技模式)时的行为，然后再看训练套路。</p><p id="1f6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经网络已经很擅长对事物进行分类，比如猫和狗。这里的想法很简单，神经网络可以学习将棋盘配置分类为赢与输吗？更具体地说，神经网络将预测表示赢与输的概率的值。此外，它将输出所有可能移动的概率分布，表示我们下一步应该探索哪个动作。</p><p id="e09d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经网络将游戏状态作为输入，输出一个值和一个概率分布。具体对于点和方框来说，游戏状态由三个元素表示:首先，一条线是否已经玩过，使用0和1的数组表示穿过点的每条线。如果一个玩家已经给这条线着色，则为1，否则为0。第二，如果当前移动是通过，第三，分数。我们可以用这三个元素来表示所有需要的信息，以确定董事会的价值并预测其下一步行动。</p><p id="18a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来分析一下下面的博弈，该轮到蓝玩家了。蓝有两个选择，走上面的路输了，或者走下面的路赢了。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ne"><img src="../Images/b6aabd4b19190679159c8d1ba8d921d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1bDomZTCmyCb9bd3Ud3vUA.jpeg"/></div></div></figure><p id="8f4d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果蓝色先玩23然后玩21，红色赢。相反，如果蓝色先玩23再玩9，蓝色就赢了。如果AlphaZero要出蓝色，它如何找到获胜的一步？</p><p id="14aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以用这个<a class="ae le" href="https://colab.research.google.com/github/carlos-aguayo/carlos-aguayo.github.io/blob/master/alphazero/notebooks/Get%20probabilities%20and%20values.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>重现下面看到的结果。</p><p id="45f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们将棋盘配置输入神经网络，我们将得到一个数组，其中包含每个动作移动概率:</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="f845" class="nk lt it ng b gy nl nm l nn no">move_probability[0]: 9.060527501880689e-12<br/>move_probability[1]: 3.9901679182996475e-10<br/>move_probability[2]: 3.0028431828490586e-15<br/>move_probability[3]: 7.959351400188552e-09<br/>move_probability[4]: 5.271672681717021e-11<br/>move_probability[5]: 4.101417122592821e-12<br/>move_probability[6]: 1.2123925357696643e-16<br/>move_probability[7]: 6.445387395019553e-23<br/>move_probability[8]: 2.8522254313207743e-22<br/><strong class="ng iu">move_probability[9]: 0.0002768792328424752</strong><br/>move_probability[10]: 1.179791128073232e-13<br/>move_probability[11]: 5.543385303737047e-13<br/>move_probability[12]: 3.2618200407341646e-07<br/>move_probability[13]: 4.302984970292259e-14<br/>move_probability[14]: 2.7477634988877216e-16<br/>move_probability[15]: 1.3767548163795204e-14<br/>move_probability[16]: 8.998188305575638e-11<br/>move_probability[17]: 7.494002147723222e-07<br/>move_probability[18]: 8.540691764924446e-11<br/>move_probability[19]: 9.55116696843561e-09<br/>move_probability[20]: 4.6348909953086714e-12<br/><strong class="ng iu">move_probability[21]: 0.46076449751853943</strong><br/>move_probability[22]: 2.179317506813483e-20<br/><strong class="ng iu">move_probability[23]: 0.5389575362205505</strong><br/>move_probability[24]: 5.8165523789057046e-15</span></pre><p id="70a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还获得了板配置的值:</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="f951" class="nk lt it ng b gy nl nm l nn no">-0.99761635</span></pre><p id="8c9d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在这里找到生成这些值的代码。</p><p id="b5db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些输出中有一些有趣的东西:</p><ol class=""><li id="a11d" class="mq mr it kk b kl km ko kp kr ms kv mt kz mu ld mv mw mx my bi translated">在8个可能的移动中，最有可能的移动是23、21和9。如果NN打23或21，他得一分。23是赢棋，其值(0.53)略高于21的值(0.46)。</li><li id="d031" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">神经网络将输出概率，即使是无效的移动。遵循游戏规则并确保我们不玩无效的动作是代码的工作。</li><li id="8550" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">该值为-0.99。这表明AlphaZero认为它已经输掉了这场比赛。该值的范围从-1(失败)到1(成功)。这个值应该更接近于1(赢)而不是-1(输)，因为我们知道这是一个赢的状态。也许我们需要更多的训练回合来达到这样一种状态，即AlphaZero将学习这种板配置的正确值。</li></ol><p id="8a4a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用神经网络的输出来决定我们的下一步很有诱惑力。</p><p id="e73f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在桌游中(在生活中！)，玩家在决定自己的下一步棋时，通常会看到许多“前面的棋”。这里也没什么不同。我们使用神经网络预测来选择下一步要探索的状态，并忽略那些低价值的状态。像Minimax一样，传统的人工智能博弈树搜索算法效率低下，因为它们必须在进入下一步之前分析每一步棋。即使有很小的分支因素的游戏也使他们的游戏空间难以处理。<a class="ae le" href="https://en.wikipedia.org/wiki/Branching_factor" rel="noopener ugc nofollow" target="_blank">分支因子</a>是可能的移动次数。随着游戏的进行，可能的移动次数会发生变化。如果是这样，你计算一个平均分支因子。<a class="ae le" href="https://www.wired.com/2014/05/the-world-of-computer-go/" rel="noopener ugc nofollow" target="_blank">国际象棋的平均分支因子是35。围棋是250。</a></p><p id="6228" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着，在国际象棋中，仅仅走了两步，就有1，225 (35)种可能的棋盘布局，而在围棋中，可能是62，500 (250)。在点和框中，对于一个3x3的游戏，初始分支因子是24，并且随着每一步棋而减少(除非是一次通过)。所以在中局中，当分支因子为15时，仅三步棋就有2730(15 * 14 * 13)种棋盘布局。</p><p id="4b14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相反，NN会引导我们，告诉我们应该探索哪里，避免被许多无用的路径淹没。NN告诉我们23和21是很强的移动。</p><p id="5524" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是蒙特卡洛树搜索的工作。</p><h1 id="79d0" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">蒙特卡罗树搜索(MCTS) </strong></h1><p id="beb9" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">神经网络已经给了我们下一步行动的指示。蒙特卡洛树搜索算法将帮助我们遍历节点，选择下一步行动。</p><p id="a1c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看看这个<a class="ae le" href="https://www.nature.com/articles/nature24270/figures/2" rel="noopener ugc nofollow" target="_blank">链接</a>中这篇论文对蒙特卡罗树搜索的图形描述。</p><p id="8b5a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">MCTS的工作方式是，对于给定的电路板，它将运行N次模拟。n是我们模型的一个参数。在这些模拟的最后，下一步将是最受欢迎的。你可以跟随代码<a class="ae le" href="https://github.com/suragnair/alpha-zero-general/blob/5156c7fd1d2f3e5fefe732a4b2e0ffc5b272f819/MCTS.py#L37-L48" rel="noopener ugc nofollow" target="_blank">到这里</a>。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="np nq l"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">运行n MCTS模拟</p></figure><p id="967b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">MCTS模拟包括遍历游戏树，从当前棋盘开始，通过选择具有最高“置信上限(UCB)”值(定义如下)的节点，直到我们到达我们以前没有访问过的游戏状态，这被称为“叶子”。这就是本文所称的A部分“选择”。</p><p id="e9b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">UCB是什么？就是Q(s，a) + U(s，a)。其中“s”是状态，“a”是动作。Q(s，a)是我们期望从动作“a”之后的状态“s”中得到的期望值，与Q-Learning中的值相同。请记住，在这种情况下，该值的范围将从-1(输)到1(赢)。<em class="lr"> U </em> ( <em class="lr"> s </em>，<em class="lr">a</em>)∧<em class="lr">P</em>(<em class="lr">s</em>，<em class="lr">a</em>)/(1+<em class="lr">N</em>(<em class="lr">s</em>，<em class="lr"> a </em>)。这意味着U与P和N成比例，其中P(s，a)是元组(s，a)的先验概率，即NN返回的值，N(s，a)是我们访问状态s并采取行动a的次数。</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="e8ed" class="nk lt it ng b gy nl nm l nn no"># Upper Confidence Bound<br/>ucb = Qsa[(s,a)] + Ps[s,a] * sqrt(Ns[s]) / (1 + Nsa[(s,a)]</span></pre><p id="016d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">UCB的要点是最初偏好具有高先验概率(P)和低访问计数(N)的动作，但是渐近地偏好具有高动作值(Q)的动作。</p><p id="c2db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以沿着<a class="ae le" href="https://github.com/suragnair/alpha-zero-general/blob/5156c7fd1d2f3e5fefe732a4b2e0ffc5b272f819/MCTS.py#L108-L125" rel="noopener ugc nofollow" target="_blank">这里的</a>看代码。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="np nq l"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">A部分—选择置信上限最高的行动(UCB)</p></figure><p id="6021" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦找到一片叶子，我们就用神经网络来搜索这块板子。这就是本文所说的B部分，“扩展和评估”。你可以跟随代码<a class="ae le" href="https://github.com/suragnair/alpha-zero-general/blob/5156c7fd1d2f3e5fefe732a4b2e0ffc5b272f819/MCTS.py#L83-L102" rel="noopener ugc nofollow" target="_blank">到这里</a>。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="np nq l"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">B部分—扩展和评估</p></figure><p id="ba95" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们将传播神经网络返回的值。这就是论文所说的C部分“备份”。你可以跟随代码<a class="ae le" href="https://github.com/suragnair/alpha-zero-general/blob/5156c7fd1d2f3e5fefe732a4b2e0ffc5b272f819/MCTS.py#L125-L136" rel="noopener ugc nofollow" target="_blank">到这里</a>。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="np nq l"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">C部分—备份</p></figure><p id="a808" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">决定下一步棋</strong></p><p id="4b1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来看看AlphaZero是如何针对上面提到的状态确定下一步行动的。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ne"><img src="../Images/b6aabd4b19190679159c8d1ba8d921d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1bDomZTCmyCb9bd3Ud3vUA.jpeg"/></div></div></figure><p id="c827" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaZero将运行50次蒙特卡罗树搜索模拟。</p><p id="9910" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以使用这个<a class="ae le" href="https://colab.research.google.com/github/carlos-aguayo/carlos-aguayo.github.io/blob/master/alphazero/notebooks/Get%20probabilities%20and%20values.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>重现下面看到的结果。</p><p id="a345" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些是每次迭代遵循的路径:</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="7e26" class="nk lt it ng b gy nl nm l nn no">Simulation #1 -&gt; Expand root node<br/><strong class="ng iu">Simulation #2 -&gt; 23</strong><br/>Simulation #3 -&gt; 21<br/>Simulation #4 -&gt; 9<br/>Simulation #5 -&gt; 17<br/>Simulation #6 -&gt; 12<br/>Simulation #7 -&gt; 19<br/>Simulation #8 -&gt; 3<br/>Simulation #9 -&gt; 18<br/><strong class="ng iu">Simulation #10 -&gt; 23,24</strong><br/>Simulation #11 -&gt; 21,24<br/><strong class="ng iu">Simulation #12 -&gt; 23,24,21</strong><br/>Simulation #13 -&gt; 21,24,23,24<br/><strong class="ng iu">Simulation #14 -&gt; 23,24,9</strong><br/><strong class="ng iu">Simulation #15 -&gt; 23,24,17</strong><br/>Simulation #16 -&gt; 21,24,9<br/><strong class="ng iu">Simulation #17 -&gt; 23,24,12</strong><br/><strong class="ng iu">Simulation #18 -&gt; 23,24,18</strong><br/>Simulation #19 -&gt; 21,24,17<br/><strong class="ng iu">Simulation #20 -&gt; 23,24,21,24,9</strong><br/>Simulation #21 -&gt; 21,24,19<br/><strong class="ng iu">Simulation #22 -&gt; 23,24,3</strong><br/>Simulation #23 -&gt; 21,24,18<br/><strong class="ng iu">Simulation #24 -&gt; 23,24,19</strong><br/>Simulation #25 -&gt; 21,24,23,24,17<br/><strong class="ng iu">Simulation #26 -&gt; 23,24,21,24,18</strong><br/><strong class="ng iu">Simulation #27 -&gt; 23,24,21,24,3</strong><br/>Simulation #28 -&gt; 21,24,3<br/><strong class="ng iu">Simulation #29 -&gt; 23,24,21,24,19</strong><br/>Simulation #30 -&gt; 21,24,12<br/><strong class="ng iu">Simulation #31 -&gt; 23,24,21,24,9,24</strong><br/>Simulation #32 -&gt; 21,24,23,24,12<br/><strong class="ng iu">Simulation #33 -&gt; 23,24,21,24,9,24,18</strong><br/>Simulation #34 -&gt; 21,24,23,24,9,24,17<br/><strong class="ng iu">Simulation #35 -&gt; 23,24,21,24,9,24,12</strong><br/><strong class="ng iu">Simulation #36 -&gt; 23,24,21,24,9,24,3</strong><br/>Simulation #37 -&gt; 21,24,23,24,9,24,19<br/><strong class="ng iu">Simulation #38 -&gt; 23,24,21,24,9,24,18,17</strong><br/>Simulation #39 -&gt; 21,24,23,24,9,24,18,17,24<br/><strong class="ng iu">Simulation #40 -&gt; 23,24,21,24,9,24,18,17,24,19</strong><br/>Simulation #41 -&gt; 21,24,23,24,9,24,18,17,24,19,24<br/><strong class="ng iu">Simulation #42 -&gt; 23,24,9,21</strong><br/><strong class="ng iu">Simulation #43 -&gt; 23,24,9,18</strong><br/><strong class="ng iu">Simulation #44 -&gt; 23,24,9,17</strong><br/><strong class="ng iu">Simulation #45 -&gt; 23,24,9,19</strong><br/><strong class="ng iu">Simulation #46 -&gt; 23,24,9,12</strong><br/><strong class="ng iu">Simulation #47 -&gt; 23,24,9,21,24</strong><br/><strong class="ng iu">Simulation #48 -&gt; 23,24,9,3</strong><br/><strong class="ng iu">Simulation #49 -&gt; 23,24,9,21,24,18</strong><br/><strong class="ng iu">Simulation #50 -&gt; 23,24,9,21,24,17</strong></span></pre><p id="aded" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的意思是:在第一个模拟中，我们以前没有见过那个板，因此它是一个“叶”节点，并“扩展”它。扩展意味着它用神经网络评估该板。</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="d929" class="nk lt it ng b gy nl nm l nn no">Simulation #1 -&gt; Expand root node</span></pre><p id="f037" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在第二个模拟中，我们已经扩展了根节点，因此它不再是“叶子”，因此我们可以搜索具有最高UCB的节点</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="c187" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">UCB最高的动作是23。它进入那个状态，因为它以前没有见过那个状态，因此它是一个叶节点，它展开它，第二个模拟完成。</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="5f56" class="nk lt it ng b gy nl nm l nn no">Simulation #2 -&gt; 23</span></pre><p id="c0a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，神经网络给出23的损失值。我们之前讨论过这个。神经网络可以使用更多的训练来了解它确实是一个糟糕的状态。但是现在，这仍然有效，我们将在下面看到。</p><p id="e06e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于接下来的模拟，每次，具有最高UCB的选项是剩余的选项。这是因为在访问每个动作后，它发现它的值很低，因此UCB也很低。</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="4d0b" class="nk lt it ng b gy nl nm l nn no">Simulation #3 -&gt; 21<br/>Simulation #4 -&gt; 9<br/>Simulation #5 -&gt; 17<br/>Simulation #6 -&gt; 12<br/>Simulation #7 -&gt; 19<br/>Simulation #8 -&gt; 3<br/>Simulation #9 -&gt; 18</span></pre><p id="9b93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在接下来的模拟中，一个令人兴奋的模式开始浮现。记住中奖顺序是23，24(跳过)，9。</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="2734" class="nk lt it ng b gy nl nm l nn no">Simulation #10 -&gt; 23,24<br/>Simulation #11 -&gt; 21,24<br/>Simulation #12 -&gt; 23,24,21<br/>Simulation #13 -&gt; 21,24,23,24<br/>Simulation #14 -&gt; 23,24,9<br/>Simulation #15 -&gt; 23,24,17<br/>Simulation #16 -&gt; 21,24,9<br/>Simulation #17 -&gt; 23,24,12<br/>Simulation #18 -&gt; 23,24,18<br/>Simulation #19 -&gt; 21,24,17<br/>Simulation #20 -&gt; 23,24,21,24,9<br/>Simulation #21 -&gt; 21,24,19<br/>Simulation #22 -&gt; 23,24,3<br/>Simulation #23 -&gt; 21,24,18<br/>Simulation #24 -&gt; 23,24,19</span></pre><p id="d0af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在模拟10到24中，MCTS已经将其注意力集中在节点21和23上。这是有意义的，因为这两条路中的任何一条，我们都得了一分。</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="f2de" class="nk lt it ng b gy nl nm l nn no">Simulation #33 -&gt; 23,24,21,24,9,24,18<br/>Simulation #34 -&gt; 21,24,23,24,9,24,17<br/>Simulation #35 -&gt; 23,24,21,24,9,24,12<br/>Simulation #36 -&gt; 23,24,21,24,9,24,3<br/>Simulation #37 -&gt; 21,24,23,24,9,24,19<br/>Simulation #38 -&gt; 23,24,21,24,9,24,18,17<br/>Simulation #39 -&gt; 21,24,23,24,9,24,18,17,24<br/>Simulation #40 -&gt; 23,24,21,24,9,24,18,17,24,19<br/>Simulation #41 -&gt; 21,24,23,24,9,24,18,17,24,19,24</span></pre><p id="53b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后在模拟33到41中，它开始深入失败的组合。注意一个有趣的事情。尽管深入，它永远不会到达游戏的结尾，因为仍然有可玩的移动。</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="5452" class="nk lt it ng b gy nl nm l nn no">Simulation #42 -&gt; 23,24,9,21<br/>Simulation #43 -&gt; 23,24,9,18<br/>Simulation #44 -&gt; 23,24,9,17<br/>Simulation #45 -&gt; 23,24,9,19<br/>Simulation #46 -&gt; 23,24,9,12<br/>Simulation #47 -&gt; 23,24,9,21,24<br/>Simulation #48 -&gt; 23,24,9,3<br/>Simulation #49 -&gt; 23,24,9,21,24,18<br/>Simulation #50 -&gt; 23,24,9,21,24,17</span></pre><p id="334b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，在模拟42至50中，在NN的帮助下，它认识到23、24、21或21、24、23不是好的选项，并完全集中在获胜序列23、24、9上。</p><p id="bfb3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">50次模拟后，我们的时间到了，我们需要选择一步棋。MCTS选择了我们去过最多的地方。以下是我们每次移动的次数(在第一次行动中):</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="c503" class="nk lt it ng b gy nl nm l nn no">counts[3] = 1<br/>counts[9] = 1<br/>counts[12] = 1<br/>counts[17] = 1<br/>counts[18] = 1<br/>counts[19] = 1<br/>counts[21] = 15<br/>counts[23] = 28</span></pre><p id="226d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在前10个模拟中，动作3、9、12、17、18和19仅被访问一次。然后MCTS把注意力集中在第21步和第23步。最近9次模拟访问行动23。鉴于我们访问23号行动最多，达28次，MCTS将其作为下一步行动。</p><p id="2413" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有哪些外卖？</p><ol class=""><li id="2ad9" class="mq mr it kk b kl km ko kp kr ms kv mt kz mu ld mv mw mx my bi translated">通过每次模拟，MCTS将依靠神经网络，使用累积值(Q)、神经网络给出的移动先验概率(P)以及它访问节点的频率的组合，来跟随看起来最有希望的路径。或者换句话说，具有最高UCB的路径。</li><li id="62cc" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">在每次模拟期间，MCTS将尽可能深入，直到它到达一个它以前没有见过的板配置，它将依赖神经网络来评估板，以告诉板有多好。</li></ol><p id="af3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们将它与经典方法进行比较，如使用带有Alpha-Beta修剪和评估函数的Minimax，我们可以说:</p><ol class=""><li id="1c81" class="mq mr it kk b kl km ko kp kr ms kv mt kz mu ld mv mw mx my bi translated">在Minimax中，深度是由设计者建立的参数。它会深入到那个深度，然后使用评估函数。如果没有Alpha-Beta修剪，它将不得不访问给定深度的所有节点，效率非常低。在上面的场景中，还剩8步棋，深度为3，这意味着评估336个棋盘。在MTCS，有50个模拟，我们只需要评估50个板，并设法深入得多。</li><li id="3dc4" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">阿尔法-贝塔剪枝会帮助我们减少336这个数字。然而，它不允许我们走一条智能的道路。</li><li id="5e66" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">我们依靠神经网络随时告诉我们电路板的“价值”,而不是使用人类定义的评估函数。</li><li id="0565" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">有趣的是，神经网络在最初的移动中没有预测到正确的值。然而，当我们深入游戏而没有一直到游戏结束时，它有正确的预测。</li><li id="fe86" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">最后，注意AlphaZero的优雅和简单。而在Alpha-Beta剪枝中，您必须跟踪Alpha和Beta参数才能知道在哪里剪枝，并且人工定义的评估函数可能很难使用。MCTS和NN让一切变得优雅而简单。你可以用JavaScript实现这一切！</li></ol><h1 id="2af0" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">训练神经网络</strong></h1><p id="44a7" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">我们遗漏了最后一个关键方面。我们如何训练神经网络？</p><p id="eb06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">幸运的是这很简单。我们之前提到过这些步骤:</p><ol class=""><li id="5d02" class="mq mr it kk b kl km ko kp kr ms kv mt kz mu ld mv mw mx my bi translated">让电脑在“训练模式”下和自己玩几局，记录棋盘的一举一动。一旦我们知道了结果，在游戏结束时用给定的结果更新所有的棋盘，“赢”或“输”。然后，我们建立了一个数据集，提供给神经网络(NN ),并开始学习给定的电路板配置是赢还是输。</li><li id="2a65" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">克隆神经网络。使用上一步中生成的数据集训练克隆。</li><li id="e389" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">让克隆神经网络和原始神经网络相互对抗。</li><li id="9eca" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">挑一个神经网络赢的，丢掉另一个。</li><li id="9d2c" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated">转到步骤1。</li></ol><p id="02a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">玩“训练模式”是什么意思？差别很小。当在“竞争模式”下玩时，我们总是选择访问量最高的棋步。而在“训练模式”中，对于游戏开始时的一定数量的移动，计数成为我们的概率分布并鼓励探索。例如，假设有3个可能的操作，访问次数为[2，2，4]。在竞争模式下，我们总是选择第三个动作，因为它有最高的访问计数。但是在训练模式中，[2，2，4]变成了概率分布，2+2+4 = 8，因此[2/8，2/8，4/8]或[0.25，0.25，0.5]。换句话说，我们50%的时间选择第三个动作，而25%的时间探索第一个和第二个动作。</p><p id="5033" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们用一个简单的井字游戏来描述数据集。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nv"><img src="../Images/f072c50f80f6d310b61548e7b5bdbee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0YkTsvYhsti6s_sjNDn7wg.png"/></div></div></figure><p id="2969" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的游戏中，第一个玩家，Xs获胜。</p><p id="cbf4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以这样配置棋盘的状态:如果方块还没有下完，则为0；如果第一个玩家下了方块，则为1；如果第二个玩家下了方块，则为-1。</p><p id="1de1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，董事会如下:</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="44c3" class="nk lt it ng b gy nl nm l nn no">0 0 0    1 0 0     1 0 0     1 0 0     1 0 0     1 0 0<br/>0 0 0 -&gt; 0 0 0 -&gt; -1 0 0 -&gt; -1 1 0 -&gt; -1 1 0 -&gt; -1 1 0<br/>0 0 0    0 0 0     0 0 0     0 0 0    -1 0 0    -1 0 1</span></pre><p id="29ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者:</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="ebe8" class="nk lt it ng b gy nl nm l nn no">[0, 0, 0, 0, 0, 0, 0, 0, 0]<br/>[1, 0, 0, 0, 0, 0, 0, 0, 0]<br/>[1, 0, 0,-1, 0, 0, 0, 0, 0]<br/>[1, 0, 0,-1, 1, 0, 0, 0, 0]<br/>[1, 0, 0,-1, 1, 0,-1, 0, 0]<br/>[1, 0, 0,-1, 1, 0,-1, 0, 1]</span></pre><p id="d667" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们必须做两件事。首先，更新每个棋盘，就好像轮到玩家1了。我们总是从第一个玩家的角度向神经网络提供训练数据。在TicTacToe，这很简单。每次轮到第二个玩家时，我们可以翻转棋子，变成玩家1的视角。</p><p id="f785" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们从这里开始:</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="cb7c" class="nk lt it ng b gy nl nm l nn no">[0, 0, 0, 0, 0, 0, 0, 0, 0]  # Player 1 turn<br/><strong class="ng iu">[1, 0, 0, 0, 0, 0, 0, 0, 0]  # Player 2 turn </strong><br/>[1, 0, 0,-1, 0, 0, 0, 0, 0]  # Player 1 turn<br/><strong class="ng iu">[1, 0, 0,-1, 1, 0, 0, 0, 0]  # Player 2 turn</strong><br/>[1, 0, 0,-1, 1, 0,-1, 0, 0]  # Player 1 turn<br/><strong class="ng iu">[1, 0, 0,-1, 1, 0,-1, 0, 1]  # Player 2 turn</strong></span></pre><p id="1cc6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对此:</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="0d13" class="nk lt it ng b gy nl nm l nn no">[ 0, 0, 0, 0, 0, 0, 0, 0, 0]  # Player 1 turn<br/><strong class="ng iu">[-1, 0, 0, 0, 0, 0, 0, 0, 0]  # Player 1 turn </strong><br/>[ 1, 0, 0,-1, 0, 0, 0, 0, 0]  # Player 1 turn<br/><strong class="ng iu">[-1, 0, 0, 1,-1, 0, 0, 0, 0]  # Player 1 turn</strong><br/>[ 1, 0, 0,-1, 1, 0,-1, 0, 0]  # Player 1 turn<br/><strong class="ng iu">[-1, 0, 0, 1,-1, 0, 1, 0,-1]  # Player 1 turn</strong></span></pre><p id="f036" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二，我们追加游戏结果。如果一号玩家赢了，我们就用“1”，如果输了，就用“0”。</p><pre class="lg lh li lj gt nf ng nh ni aw nj bi"><span id="e3b5" class="nk lt it ng b gy nl nm l nn no">[ 0, 0, 0, 0, 0, 0, 0, 0, 0, <strong class="ng iu">1</strong>]  # Winning board<br/>[-1, 0, 0, 0, 0, 0, 0, 0, 0, <strong class="ng iu">0</strong>]  # Losing board<br/>[ 1, 0, 0,-1, 0, 0, 0, 0, 0, <strong class="ng iu">1</strong>]  # Winning board<br/>[-1, 0, 0, 1,-1, 0, 0, 0, 0, <strong class="ng iu">0</strong>]  # Losing board<br/>[ 1, 0, 0,-1, 1, 0,-1, 0, 0, <strong class="ng iu">1</strong>]  # Winning board<br/>[-1, 0, 0, 1,-1, 0, 1, 0,-1, <strong class="ng iu">0</strong>]  # Losing board</span></pre><p id="5119" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据集开始有一些形状了！正如你从上面看到的，我们可以把它输入神经网络来学习棋盘上的输赢值。</p><p id="979c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不过，我们忽略了概率。那些是从哪里来的？请记住，在训练模式下，我们仍然会在每一步运行MCTS模拟。与我们记录棋盘配置的方式相同，我们将记录概率。</p><p id="7085" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们克隆神经网络，并使用该数据集训练克隆，预计它会随着使用新数据而稍微变强。我们通过让它与原来的NN竞争来验证它确实更强。如果它赢了55%以上的时间，我们认为它更强，并放弃原来的，克隆成为我们新的神经网络。</p><p id="dc61" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们不断重复这个过程，神经网络会不断变强。</p><p id="6df8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以从论文<a class="ae le" href="https://www.nature.com/articles/nature24270/figures/1" rel="noopener ugc nofollow" target="_blank">这里</a>看到图表。</p><p id="feca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为什么数据集有更强的动作可以学习？通过学习MCTS生成的概率，他们通常会选择比神经网络给出的原始移动概率更强的移动。我们通过让MCTS深入许多路径来提供神经网络知识！</p><p id="042e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用这个<a class="ae le" href="https://colab.research.google.com/github/carlos-aguayo/alpha-zero-general/blob/dotsandboxes/dotsandboxes/Train%20Dots%20and%20Boxes%20using%20AlphaZero%20General.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>训练一个点和方框模型。</p><p id="3b19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">将其部署到WebApp中</strong></p><p id="ce76" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">几个月前，我发表了一篇<a class="ae le" rel="noopener" target="_blank" href="/deploying-a-simple-machine-learning-model-into-a-webapp-using-tensorflow-js-3609c297fb04">博文</a>，带你完成将Keras或Tensorflow模型转换成供Tensorflow.js使用的JavaScript的过程。我们将转换我们在Keras中训练的模型，以便在Tensorflow.js中使用。</p><p id="8ee8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个<a class="ae le" href="https://colab.research.google.com/github/carlos-aguayo/alpha-zero-general/blob/dotsandboxes/dotsandboxes/Convert%20Keras%20Model%20for%20use%20with%20Tensorflow.js.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>将预先训练的点和方框模型转换成Tensorflow.js模型。</p><p id="5bd9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们这样做了，我们就可以轻松地用JavaScript加载模型，如这里的<a class="ae le" href="https://github.com/carlos-aguayo/carlos-aguayo.github.io/blob/50de28a29d198df07eed60478fcd0e6de8d5913a/alphazero/index.html#L138" rel="noopener ugc nofollow" target="_blank">所示。</a></p><h1 id="b1b7" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">结论</strong></h1><p id="eb0a" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">在这篇博文中，你了解了AlphaZero，这是一种新颖的强化学习算法，可以在双人零和棋盘游戏中击败世界冠军。</p><p id="f9be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您了解了它如何沿着神经网络使用蒙特卡罗树搜索来找到最佳的下一步行动。你也学会了如何训练这个神经网络。</p><p id="49c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这篇博文，并发现它很有价值。如果是，<a class="ae le" href="https://twitter.com/carlosaguayo81" rel="noopener ugc nofollow" target="_blank">伸出</a>！</p><p id="6db8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我是<a class="ae le" href="https://www.appian.com/" rel="noopener ugc nofollow" target="_blank"> Appian </a>的<a class="ae le" href="https://www.linkedin.com/in/carlosaguayo" rel="noopener ugc nofollow" target="_blank">软件开发高级总监兼机器学习工程师</a>。我在Appian工作了15年，我一直很开心。这个疫情并没有让我们放慢脚步，我们正在<a class="ae le" href="https://www.appian.com/careers/" rel="noopener ugc nofollow" target="_blank">招聘</a>！给我发一条<a class="ae le" href="https://twitter.com/carlosaguayo81" rel="noopener ugc nofollow" target="_blank">消息</a>如果你想知道我们是如何制作软件的，请取悦我们的客户，玩得开心！</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nw"><img src="../Images/4687c51cb91d34ff0549a500c6f09c62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T5AhtjSUat_6VbkN5bsquw.png"/></div></div></figure></div></div>    
</body>
</html>