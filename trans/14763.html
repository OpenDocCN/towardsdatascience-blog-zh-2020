<html>
<head>
<title>Multi-Perspective Neural Networks — An unsupervised deep metric learning algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多视角神经网络——一种无监督的深度度量学习算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-perspective-neural-networks-an-unsupervised-deep-metric-learning-algorithm-54c4a7c93e3?source=collection_archive---------39-----------------------#2020-10-11">https://towardsdatascience.com/multi-perspective-neural-networks-an-unsupervised-deep-metric-learning-algorithm-54c4a7c93e3?source=collection_archive---------39-----------------------#2020-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fc59" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种学习生成嵌入的无监督深度度量学习算法。</h2></div><p id="76c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">链接:</em> </strong> <a class="ae lf" href="https://www.youtube.com/watch?v=zIReaxDhfcQ" rel="noopener ugc nofollow" target="_blank">演示视频</a> &amp; <a class="ae lf" href="https://github.com/evrimozmermer/filter_distinguisher" rel="noopener ugc nofollow" target="_blank"> Github资源库</a></p><p id="a32e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">想象你是一个刚刚睁开眼睛看世界的婴儿。想象一下缺乏意识，想象一下什么都不知道。你连怎么看东西都不知道，更别说怎么识别东西了。现在，回到你的意识中来。你可以看到，你可以认出你周围的任何物体，这些物体甚至会让你想起你的记忆。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/ddd4c7285e5ec8747ff96d52305d245d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZO0ITiOMwKgNBcOl"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">克里斯蒂安·鲍文在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="7423" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，你是如何获得这些技能的呢？这是多视角神经网络(MPNN)的起点。</p><p id="2236" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我之前的文章中，我已经谈到了一种无监督学习算法，它可以学习创建有意义的特征作为输出的过滤器。帖子链接:<a class="ae lf" rel="noopener" target="_blank" href="/filter-learning-with-unsupervised-learning-6e72fd5057a9">无监督学习的学习过滤器</a>。这篇文章是上一篇文章的延续。在这篇文章中将有三个部分，如逻辑和哲学，代码，测试。</p><h2 id="6953" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">逻辑和哲学</h2><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mp"><img src="../Images/a8a23d83d3cfdfdf775017ca0f1f5402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SuG0ya-OxQd22RD9"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">由<a class="ae lf" href="https://unsplash.com/@serjosoza?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> sergio souza </a>在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="3ace" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该算法基本上学会了将卷积层的输出彼此分解。当我们对图像应用卷积运算时，我们得到NxRxC矩阵，其中N是滤波器的数量，R和C是行大小和列大小。在MPNNs中，卷积运算的输出中的N个层试图彼此分解。当最大相似性值低于预定阈值时，该级别的学习过程完成。之后，对下一级重复这一过程。最后，神经网络生成具有有意义特征的嵌入。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/86b019525da9e9648e419dfd08153130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*nWJm7Cw1jVvRujvIYT5TGA.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">显示分解管道的图表(按作者)</p></figure><p id="9627" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我开始思考这次行动的哲学。我想到了与这个层分解操作相匹配的概念。我一直在寻找的概念是<strong class="kk iu"> <em class="le">视角。</em> </strong>人类试图学会用不同的视角看待事件、物体和其他一切，以便他们能够分析正在发生的事情。这不仅发生在高级抽象中，也发生在低级学习中。MPNN试图做的是将这种视角生成应用于视觉中的低级学习过程。</p><h2 id="05c8" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">密码</h2><p id="9381" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">我在以前的帖子中分享了代码。但是，我已经做了一些改变，所以我想在这个帖子里再次分享。完整的代码仍然可以在<a class="ae lf" href="https://github.com/evrimozmermer/filter_distinguisher" rel="noopener ugc nofollow" target="_blank">的GitHub库</a>中找到。</p><p id="15cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个单独的卷积图层类。</p><pre class="lh li lj lk gt mw mx my mz aw na bi"><span id="7c1a" class="lw lx it mx b gy nb nc l nd ne">class ConvLayer(torch.nn.Module):<br/>    def __init__(self, in_channels, out_channels, kernel_size, stride):<br/>        super(ConvLayer, self).__init__()<br/>        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)</span><span id="2af2" class="lw lx it mx b gy nf nc l nd ne">def forward(self, x):<br/>        out = self.conv2d(x)<br/>        return out</span></pre><p id="b28c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个单一的卷积级类。级别是分解发生的地方。</p><pre class="lh li lj lk gt mw mx my mz aw na bi"><span id="dc7b" class="lw lx it mx b gy nb nc l nd ne">class SSNet(torch.nn.Module):<br/>    def __init__(self,in_filters, out_filters):<br/>        super(SSNet, self).__init__()<br/>        self.conv1 = ConvLayer(in_filters, 64, kernel_size = 5, stride = 1)<br/>        self.conv2 = ConvLayer(64, out_filters, kernel_size = 1, stride = 1)<br/>        self.pool = nn.AvgPool2d(2, stride=2)<br/>        self.relu = torch.nn.ReLU()<br/>        <br/>    def forward(self, x):<br/>        out = self.pool(self.conv2(self.relu(self.conv1(x))))<br/>        return out</span></pre><p id="b04d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">包含几级SSNet类的MPNN类。</p><pre class="lh li lj lk gt mw mx my mz aw na bi"><span id="75ac" class="lw lx it mx b gy nb nc l nd ne">class SSNetMultiple(torch.nn.Module):<br/>    def __init__(self,levels = 5):<br/>        super(SSNetMultiple, self).__init__()<br/>        self.children = []<br/>        for cnt in range(levels):<br/>            if cnt == 0:<br/>                in_filters, out_filters = 3,16<br/>            elif cnt == levels-1:<br/>                in_filters, out_filters = 16,16<br/>            else:<br/>                in_filters, out_filters = 16,16<br/>            self.children.append(SSNet(in_filters, out_filters))<br/>        <br/>        self.main = nn.Sequential(*self.children)<br/>        <br/>    def forward(self, x, queue = 1):<br/>        outs = [x]<br/>        for cnt,child in enumerate(self.main):<br/>            if cnt&lt;queue:<br/>                outs.append(child(outs[-1]))<br/>        return outs[-1]</span></pre><p id="8a94" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">规范化操作。如果我们想要获得相似性值1作为最大相似性，归一化是必要的。</p><pre class="lh li lj lk gt mw mx my mz aw na bi"><span id="19c0" class="lw lx it mx b gy nb nc l nd ne">def normalize(vector):<br/>    norm = vector.norm(p=2, dim=0, keepdim=True)<br/>    vector_normalized = vector.div(norm.expand_as(vector))<br/>    return vector_normalized</span></pre><p id="79e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相似性函数用于提取和组合层的相似性，以便我们可以计算损失。</p><pre class="lh li lj lk gt mw mx my mz aw na bi"><span id="b2ce" class="lw lx it mx b gy nb nc l nd ne">def sim_func(layers):<br/>    combinations = list(itertools.combinations(np.arange(0,layers.shape[1]), 2))<br/>    similarity_vector = torch.empty(len(combinations))<br/>    for cnt,comb in enumerate(combinations):<br/>        first = layers[0][comb[0]].flatten()<br/>        second = layers[0][comb[1]].flatten()<br/>        first_norm = normalize(first)<br/>        second_norm = normalize(second)<br/>        similarity_vector[cnt] = torch.matmul(first_norm,second_norm.T)<br/>    return similarity_vector</span></pre><p id="a2c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用分解层数定义MPNN实例。</p><pre class="lh li lj lk gt mw mx my mz aw na bi"><span id="0421" class="lw lx it mx b gy nb nc l nd ne">model = SSNetMultiple(levels = 4)</span></pre><p id="e33b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于数据集，我在上一篇文章中使用了MNIST。这一次，我们将使用从YouTube下载的视频。</p><p id="84d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">视频可在链接中找到:<a class="ae lf" href="https://www.youtube.com/watch?v=axqSGXU5qIw" rel="noopener ugc nofollow" target="_blank">西班牙伊比沙岛的诱惑</a></p><p id="22fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了从视频中捕捉帧，我使用了OpenCV。我们需要捕捉帧，应用中心裁剪，调整大小，并转换到PyTorch张量。</p><pre class="lh li lj lk gt mw mx my mz aw na bi"><span id="54c5" class="lw lx it mx b gy nb nc l nd ne">def cam_to_tensor(cam):<br/>    if cam.isOpened():<br/>        ret, frame_ = cam.read()<br/>    else:<br/>        cam.release()<br/>        cam = cv2.VideoCapture(video_source)<br/>        ret, frame_ = cam.read()<br/>    frame = cv2.cvtColor(frame_, cv2.COLOR_BGR2RGB)<br/>    frame_pil = Image.fromarray(frame)<br/>    image = transform(frame_pil)<br/>    return image, frame_, cam</span><span id="5224" class="lw lx it mx b gy nf nc l nd ne">transform=transforms.Compose([<br/>                            transforms.CenterCrop((360,360)),<br/>                            transforms.Resize((224,224)),<br/>                            transforms.ToTensor()<br/>                            ])</span></pre><p id="9430" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，所有的训练脚本。对于评论行，请检查我在以前的职位解释。链接可以在这篇文章的顶部找到。</p><p id="5193" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们从视频中捕捉一帧。然后，我们准备好要提供给模型的帧。之后，模型训练分解的第一级。当最大相似值低于0.3时，我们开始训练下一级，以此类推。别忘了，视频中捕捉到的画面是城市旅游的场景。</p><pre class="lh li lj lk gt mw mx my mz aw na bi"><span id="0d5d" class="lw lx it mx b gy nb nc l nd ne">lr = 0.02<br/>optimizer = optim.SGD(model.parameters(), lr=lr)<br/>lossfunc = nn.MSELoss()</span><span id="e143" class="lw lx it mx b gy nf nc l nd ne">video_source = "./videoplayback.mp4"<br/>cam = cv2.VideoCapture(video_source)</span><span id="08df" class="lw lx it mx b gy nf nc l nd ne">loss_obs = 0<br/>epoch = 0</span><span id="7e4e" class="lw lx it mx b gy nf nc l nd ne">while epoch&lt;4:<br/>#    if epoch&gt;0:<br/>#        for cc,param in enumerate(model.main[epoch-1].parameters()):<br/>#            print(epoch-1,"grad is deactivated")<br/>#            param.requires_grad = True<br/>    for cnt in range(0,120000):<br/>        image, _, cam = cam_to_tensor(cam)<br/>        <br/>        optimizer.zero_grad()<br/>        out = model(image.unsqueeze(0), queue = epoch+1)<br/>        sim_vec = sim_func(out)<br/>        loss = lossfunc(sim_vec, torch.zeros(sim_vec.shape))<br/>        loss_obs_ = torch.max(torch.abs(sim_vec-torch.zeros(sim_vec.shape)))<br/>        loss_obs += loss_obs_<br/>        loss.backward()<br/>        optimizer.step()<br/>        print("Epoch: {}\tSample: {}\tLoss: {}\tLR: {}".format(epoch,cnt,loss_obs_,optimizer.param_groups[0]["lr"]))</span><span id="a81e" class="lw lx it mx b gy nf nc l nd ne">if cnt%20 == 0 and cnt!=0:<br/>            loss_obs = loss_obs/20<br/>            print("Epoch: {}\tSample: {}\tLoss: {}\tLR: {}".format(epoch,cnt,loss_obs,optimizer.param_groups[0]["lr"]))<br/>            if loss_obs&lt;0.30:<br/>                epoch += 1<br/>                break<br/>            loss_obs = 0</span></pre><h2 id="48c8" class="lw lx it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">试验</h2><p id="7237" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">我们的模特学会了欣赏城市风景。我们如何观察它学到了什么？我们可以捕捉随机帧，并将这些帧与后面的帧进行比较。通过这样做，我们可以观察帧的相似性值，并看到帧中每个场景出现的效果。例如:如果有一个特定的对象，如人、窗户等。在锚定帧中，并且如果在随后的帧中存在该对象，即使场景已经改变，并且如果相似性值有点高，那么我们理解模型可以提取特征来帮助它识别场景中的该对象。</p><pre class="lh li lj lk gt mw mx my mz aw na bi"><span id="54c1" class="lw lx it mx b gy nb nc l nd ne">def generate_embedding(model,cam,queue = 3):<br/>    image, frame, _ = cam_to_tensor(cam)<br/>    embedding = model(image.unsqueeze(0), queue = queue).flatten()<br/>    return embedding, frame</span><span id="42f1" class="lw lx it mx b gy nf nc l nd ne">def compare_samples(e1,e2):<br/>    first_norm = normalize(e1.flatten())<br/>    second_norm = normalize(e2.flatten())<br/>    return torch.matmul(first_norm,second_norm.T).detach().numpy()</span><span id="7522" class="lw lx it mx b gy nf nc l nd ne">embedding_list = []<br/>def compare_continuous(model,cam,queue):<br/>    min_sim = 1<br/>    max_diff = 0<br/>    <br/>    font                   = cv2.FONT_HERSHEY_SIMPLEX<br/>    bottomLeftCornerOfText = (10,100)<br/>    fontScale              = 1<br/>    fontColor              = (255,255,255)<br/>    lineType               = 2<br/>    <br/>    last_sim_list = []<br/>    cnt_f = 0<br/>    while True:<br/>        if cnt_f%300==0:<br/>            e1, f1 = generate_embedding(model,cam,queue = queue)<br/>            cv2.imshow('frame 1', f1)<br/>        <br/>        e2, f2 = generate_embedding(model,cam,queue = queue)<br/>        embedding_list.append(e2.detach().numpy())<br/>        embedding_list_np = np.array(embedding_list)<br/>        std = np.std(embedding_list_np, axis=0)<br/>        pca_idx = std.argsort()[-64:][::-1]<br/>        <br/>        e1_pca = e1[pca_idx.tolist()]<br/>        e2_pca = e2[pca_idx.tolist()]<br/>        <br/>        sim = compare_samples(e1_pca,e2_pca)<br/>        print(sim)<br/>        <br/>        cv2.putText(f2,'Similarity: {}'.format(sim), <br/>            bottomLeftCornerOfText, <br/>            font, <br/>            fontScale,<br/>            fontColor,<br/>            lineType)<br/>        cv2.imshow('frame 2', f2)<br/>        if cv2.waitKey(25) &amp; 0xFF == ord('q'):<br/>            break<br/>        <br/>        cnt_f += 1</span></pre><p id="18c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还有播放按钮。</p><pre class="lh li lj lk gt mw mx my mz aw na bi"><span id="8e14" class="lw lx it mx b gy nb nc l nd ne">compare_continuous(model,cam,queue=5)</span></pre><p id="85db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">演示视频可以在<a class="ae lf" href="https://www.youtube.com/watch?v=zIReaxDhfcQ" rel="noopener ugc nofollow" target="_blank">链接</a>中找到。</p></div></div>    
</body>
</html>