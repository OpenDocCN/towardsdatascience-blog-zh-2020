<html>
<head>
<title>Exploiting the differences between model training and prediction.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用模型训练和预测之间的差异。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploiting-the-differences-between-model-training-and-prediction-40f087e52923?source=collection_archive---------39-----------------------#2020-10-21">https://towardsdatascience.com/exploiting-the-differences-between-model-training-and-prediction-40f087e52923?source=collection_archive---------39-----------------------#2020-10-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/ddb2e92c11d7561595a104161c242329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYInpEeKSqcFK_lt8JVKWw.png"/></div></div></figure><div class=""/><div class=""><h2 id="f74e" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">减少内存占用，提高部署模型的速度和可移植性。</h2></div><blockquote class="kq kr ks"><p id="e5ea" class="kt ku kv kw b kx ky kc kz la lb kf lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这篇文章的代码可以在这里找到<a class="ae lq" href="https://github.com/scailable/sclbl-tutorials/tree/master/sclbl-train-vs-deploy" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><p id="efab" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lr le lf lg ls li lj lk lt lm ln lo lp ij bi translated">最近几个月，我们已经帮助许多公司在各种环境中部署他们的AI / ML模型。我们为医疗保健领域的模型部署做出了贡献，在过去的几个月里，我们已经帮助几家公司将经过训练的模型迁移到不同类型的物联网设备上。特别是在后一种情况下，要求通常很严格:计算周期的数量和可用内存通常都是有限的。</p><p id="026b" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lr le lf lg ls li lj lk lt lm ln lo lp ij bi translated">在这篇文章中，我澄清了我们如何确保使用标准ML库训练的模型，如<a class="ae lq" href="https://pytorch.org" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>、<a class="ae lq" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>和<a class="ae lq" href="https://www.tensorflow.org" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>可以在各种边缘设备上有效地部署<em class="kv"/>。为了使事情变得具体，我们将检查一个简单的逻辑回归模型的训练和部署。然而，我们在这里讨论的大部分直接转移到更复杂的模型。</p><h1 id="b84e" class="lu lv jb bd lw lx ly lz ma mb mc md me kh mf ki mg kk mh kl mi kn mj ko mk ml bi translated">模特培训</h1><p id="7537" class="pw-post-body-paragraph kt ku jb kw b kx mm kc kz la mn kf lc lr mo lf lg ls mp lj lk lt mq ln lo lp ij bi translated">为了说明模型训练和部署之间的区别，让我们从模拟一些数据开始。下面的代码根据下面的简单模型生成1000个观察值:</p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mr"><img src="../Images/2f0bd869843669f2cba264fa26b8b32a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qZ_q1M5wD0idgHRX0DVCVQ.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">(一个简单的逻辑回归模型用作本例的DGP)</p></figure><pre class="ms mt mu mv gt na nb nc nd aw ne bi"><span id="ab4d" class="nf lv jb nb b gy ng nh l ni nj">import numpy as np<br/>np.random.seed(66)  # Set seed for replication</span><span id="8800" class="nf lv jb nb b gy nk nh l ni nj"># Simulate Data Generating Process<br/>n = 1000  # 1000 observations<br/>x1 = np.random.uniform(-2,2,n)  # x_1 &amp; x_2 between -2 and 2<br/>x2 = np.random.uniform(-2,2,n)<br/>p = 1 / (1 + np.exp( -1*(.75 + 1.5*x1 - .5*x2) ))  # Implement DGP</span><span id="83ea" class="nf lv jb nb b gy nk nh l ni nj">y = np.random.binomial(1, p, n)  # Draw outcomes</span><span id="7d5e" class="nf lv jb nb b gy nk nh l ni nj"># Create dataset and print first few lines:<br/>data = np.column_stack((x1,x2,y))<br/>print(data[:10])</span></pre><p id="41a4" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lr le lf lg ls li lj lk lt lm ln lo lp ij bi translated">生成数据后，我们可以专注于拟合模型。我们简单地使用<code class="fe nl nm nn nb b">sklearn</code>的<code class="fe nl nm nn nb b">LogisticRegression()</code>函数来实现:</p><pre class="ms mt mu mv gt na nb nc nd aw ne bi"><span id="9f48" class="nf lv jb nb b gy ng nh l ni nj">from sklearn.linear_model import LogisticRegression<br/>mod = LogisticRegression().fit(data[:,[0,1]], np.ravel(data[:,[2]]))</span></pre><h2 id="9986" class="nf lv jb bd lw no np dn ma nq nr dp me lr ns nt mg ls nu nv mi lt nw nx mk ny bi translated">近距离观察</h2><p id="5e90" class="pw-post-body-paragraph kt ku jb kw b kx mm kc kz la mn kf lc lr mo lf lg ls mp lj lk lt mq ln lo lp ij bi translated">在这一点上，有必要暂停一下，简要地考虑一下引擎盖下正在发生的事情。逻辑回归模型，就像许多其他有趣的ML模型一样，被反复训练<em class="kv"/>。为了训练这个模型，<code class="fe nl nm nn nb b">sklearn</code>(或者任何其他提供类似功能的包)必须实现几个功能:</p><ol class=""><li id="66ff" class="nz oa jb kw b kx ky la lb lr ob ls oc lt od lp oe of og oh bi translated">某种分数函数表示模型的适合度。这可能是一个误差函数，或最大似然函数。</li><li id="cb5a" class="nz oa jb kw b kx oi la oj lr ok ls ol lt om lp oe of og oh bi translated">从一次迭代到下一次迭代更新拟合模型参数的函数。</li></ol><p id="a91a" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lr le lf lg ls li lj lk lt lm ln lo lp ij bi translated">训练过程将有效地重复使用这两个函数:最初，模型的参数是随机实例化的。接下来，检查模型的分数。如果分数被认为是不够的(通常是因为它与前一次迭代相比有所改进)，则更新模型参数并重复该过程。</p><p id="5ae8" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lr le lf lg ls li lj lk lt lm ln lo lp ij bi translated">即使对于这个简单的模型，<code class="fe nl nm nn nb b">sklearn</code>也需要多次遍历数据集。下面的代码给出了迭代次数(对于这个种子选择是7次):</p><pre class="ms mt mu mv gt na nb nc nd aw ne bi"><span id="3712" class="nf lv jb nb b gy ng nh l ni nj"># Print the number of iterations<br/>print(f'The number of iterations is: {mod.n_iter_}.')</span></pre><p id="4767" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lr le lf lg ls li lj lk lt lm ln lo lp ij bi translated">因此，<strong class="kw jc">为了训练一个模型</strong>，我们需要访问数据，几个效用函数，并且我们需要多次迭代/遍历数据集。一般来说，这种训练过程在计算上要求很高，这解释了为什么对于复杂的模型，我们求助于并行计算和GPU或NPU加速来在合理的时间内完成它。然而，幸运的是，当训练模型时，我们使用的各种ML库抽象掉了这样做所需的相当复杂的逻辑。</p><h1 id="318c" class="lu lv jb bd lw lx ly lz ma mb mc md me kh mf ki mg kk mh kl mi kn mj ko mk ml bi translated">生成预测</h1><p id="3295" class="pw-post-body-paragraph kt ku jb kw b kx mm kc kz la mn kf lc lr mo lf lg ls mp lj lk lt mq ln lo lp ij bi translated">将这与从已经拟合的模型中生成<em class="kv">预测</em>进行比较(通常称为<em class="kv">推论</em>，但我发现后一个术语令人困惑，因为它在统计学中的用法不同，所以我坚持使用预测)。在拟合模型时，在这种情况下，实际上我们需要生成预测的只是逻辑回归函数(与我们在上面的示例中用于生成数据的数学函数相同)和拟合模型的三个参数。这些很容易检索:</p><pre class="ms mt mu mv gt na nb nc nd aw ne bi"><span id="5da5" class="nf lv jb nb b gy ng nh l ni nj">b = np.concatenate((mod.intercept_, mod.coef_.flatten()))<br/>print(b)</span></pre><p id="2425" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lr le lf lg ls li lj lk lt lm ln lo lp ij bi translated">并且参数最终与我们用于数据生成的值相对接近:<code class="fe nl nm nn nb b">[0.84576563 1.39541631 -0.47393112]</code>。</p><p id="ffa2" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lr le lf lg ls li lj lk lt lm ln lo lp ij bi translated">此外，在大多数部署情况下，我们通常只使用一个输入来评估模型:在本例中，是一个长度为2的数字向量。所以真的，如果我们想要部署一个模型，我们不需要拟合函数，我们不需要数据，我们不需要迭代。<strong class="kw jc">为了生成预测</strong>，我们只需要简单有效地实现相关的数学函数。</p><h1 id="1223" class="lu lv jb bd lw lx ly lz ma mb mc md me kh mf ki mg kk mh kl mi kn mj ko mk ml bi translated">利用训练和预测之间的差异进行(边缘)部署</h1><p id="f6c0" class="pw-post-body-paragraph kt ku jb kw b kx mm kc kz la mn kf lc lr mo lf lg ls mp lj lk lt mq ln lo lp ij bi translated">“那又怎么样？”你可能会问。当现代模型训练工具抽象掉所有这些细节时，为什么还要关心训练和预测中涉及的本质细节呢？嗯，因为当你希望你的模型被有效地<em class="kv">部署的时候，例如当你需要它们在小型设备上快速运行的时候，你最好利用上面描述的差异。</em></p><p id="0d99" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lr le lf lg ls li lj lk lt lm ln lo lp ij bi translated">为了便于讨论，请对比以下两种模型部署方法(即，将训练好的模型投入生产，以便您可以使用其预测):</p><ol class=""><li id="352b" class="nz oa jb kw b kx ky la lb lr ob ls oc lt od lp oe of og oh bi translated"><strong class="kw jc">sklearn作为REST服务的docker容器部署:</strong>这种方法简单且常用:我们启动一个Docker映像，其中包含python堆栈和用于训练的工具:在上面的示例逻辑回归模型中，<code class="fe nl nm nn nb b">sklearn</code>。接下来，我们创建一个REST端点，它使用拟合模型的<code class="fe nl nm nn nb b">mod.predict()</code>函数来生成结果。</li><li id="5471" class="nz oa jb kw b kx oi la oj lr ok ls ol lt om lp oe of og oh bi translated"><strong class="kw jc">可伸缩WebAssembly部署:</strong>最近，但一点也不困难的是将fitted模型转换为WebAssembly(使用类似于<a class="ae lq" href="https://www.scailable.net" rel="noopener ugc nofollow" target="_blank">可伸缩</a>提供的服务)，并部署。WASM二进制只包含在最小WebAssembly运行时预测所需的逻辑。(自动生成的)二进制文件将只包含必要的逻辑函数(在这种情况下)和估计的参数。二进制文件可能部署在服务器上(因此类似地通过REST调用使用)，但是，使用<a class="ae lq" href="https://github.com/scailable/sclbl-webnode" rel="noopener ugc nofollow" target="_blank">各种可用的运行时</a>，它也可以运行在几乎任何边缘设备上。</li></ol><p id="aadd" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lr le lf lg ls li lj lk lt lm ln lo lp ij bi translated">显然，第一个部署过程接近数据科学家的“我们所知道的”。直接使用我们熟悉的工具工作是很好的。在许多方面，它是可行的:我们可以通过调用REST端点来生成预测。第二种解决方案与我们的标准实践相去甚远，并且对于模型训练毫无用处(也就是说，没有“用于训练模型的WebAssembly包……”，如果那句话有任何意义的话)。但是，我们仍然认为应该优先选择它:第二种设置利用了训练和预测之间的差异，在几个方面使模型部署更好:</p><ul class=""><li id="a883" class="nz oa jb kw b kx ky la lb lr ob ls oc lt od lp on of og oh bi translated"><strong class="kw jc">内存占用:</strong>上面两个选项中的第一个将需要<em class="kv">至少</em> 75Mb的容器(让容器变得那么小<a class="ae lq" href="https://jilongliao.com/2018/08/09/Reduce-Docker-Image-Size/" rel="noopener ugc nofollow" target="_blank">需要大量的工程设计</a>，更常见的是容器的大小接近1Gb)。在这种情况下，存储的模型本身很小(~2Kb)，因此容器包含了部署的最大内存块(注意，对于大型神经网络来说，这可能不是真的)。相反，WebAssembly运行时可以减少到小于64 <strong class="kw jc"> Kb </strong>。无可否认，WebAssembly二进制文件本身比存储的<code class="fe nl nm nn nb b">sklearn</code>模型大(~50kb)，但是它现在包含了生成预测所需的全部内容<em class="kv">。因此，第一个部署选项至少需要75Mb，而第二个部署选项需要不到. 1Mb。</em></li><li id="14af" class="nz oa jb kw b kx oi la oj lr ok ls ol lt om lp on of og oh bi translated"><strong class="kw jc">速度:</strong>与高效的WebAssembly部署相比，消耗运行在Docker容器中的REST端点(它启动了训练所需的所有东西)在执行时间方面并不占优势。<a class="ae lq" href="https://www.scailable.net/demo/bench/" rel="noopener ugc nofollow" target="_blank">这里</a>是各种模型的一些速度比较，但是，不用说，利用训练和预测之间的差异，仅仅将预测的基本需求投入生产，会将这些预测生成的速度提高一个数量级。</li></ul><p id="a55f" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lr le lf lg ls li lj lk lt lm ln lo lp ij bi translated">因此，内存占用更少，执行速度更快。这很好，有几个原因:一个原因是，我们可能希望为<a class="ae lq" href="https://unfccc.int/process-and-meetings/the-paris-agreement/the-paris-agreement" rel="noopener ugc nofollow" target="_blank">巴黎协定</a>做出贡献，从而有效地部署模型，而不会在每次生成预测时浪费能量。但是，小的占用空间和快速的执行也很有吸引力，因为这正是我们在将模型投入生产时所需要的:祝您在<a class="ae lq" href="https://en.wikipedia.org/wiki/ESP32" rel="noopener ugc nofollow" target="_blank"> ESP32 MCU板</a>上部署Docker容器好运。有了WebAssembly，这简直是小菜一碟。</p><h1 id="3e2e" class="lu lv jb bd lw lx ly lz ma mb mc md me kh mf ki mg kk mh kl mi kn mj ko mk ml bi translated">放弃</h1><p id="cb7d" class="pw-post-body-paragraph kt ku jb kw b kx mm kc kz la mn kf lc lr mo lf lg ls mp lj lk lt mq ln lo lp ij bi translated"><em class="kv">值得注意的是我自己的参与:我是Jheronimus数据科学院</em> <a class="ae lq" href="https://www.jads.nl" rel="noopener ugc nofollow" target="_blank"> <em class="kv">的数据科学教授，也是</em></a><a class="ae lq" href="https://www.scailable.net" rel="noopener ugc nofollow" target="_blank"><em class="kv">Scailable</em></a><em class="kv">的联合创始人之一。因此，毫无疑问，我对Scailable有既得利益；我有兴趣让它成长，这样我们就可以最终将人工智能投入生产并兑现它的承诺。这里表达的观点是我自己的。</em></p></div></div>    
</body>
</html>