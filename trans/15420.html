<html>
<head>
<title>Working With The Lambda Layer in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Keras中使用Lambda图层</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/working-with-the-lambda-layer-in-keras-cfbaffdfc4c9?source=collection_archive---------27-----------------------#2020-10-23">https://towardsdatascience.com/working-with-the-lambda-layer-in-keras-cfbaffdfc4c9?source=collection_archive---------27-----------------------#2020-10-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="fd4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本教程中，我们将介绍如何使用Keras中的Lambda层来构建、保存和加载对数据执行自定义操作的模型。</p><p id="7e8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Keras是一个流行且易于使用的库，用于构建深度学习模型。它支持所有已知类型的层:输入层、密集层、卷积层、转置卷积层、整形层、归一化层、下降层、展平层和激活层。每一层都对数据执行特定的操作。</p><p id="dc12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">也就是说，您可能希望对未应用于任何现有图层的数据执行操作，然后这些现有图层类型将无法满足您的任务。举一个简单的例子，假设您需要一个层来执行在模型架构的给定点添加固定数字的操作。因为没有现有的层可以做到这一点，所以您可以自己构建一个。</p><p id="1a34" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本教程中，我们将讨论在Keras中使用<code class="fe kl km kn ko b">Lambda</code>层。这允许您指定要作为函数应用的操作。我们还将看到在构建具有lambda层的模型时如何调试Keras加载特性。</p><p id="7b46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本教程涵盖的部分如下:</p><ul class=""><li id="867a" class="kp kq iq jp b jq jr ju jv jy kr kc ks kg kt kk ku kv kw kx bi translated">使用<code class="fe kl km kn ko b">Functional API</code>构建Keras模型</li><li id="411d" class="kp kq iq jp b jq ky ju kz jy la kc lb kg lc kk ku kv kw kx bi translated">添加一个<code class="fe kl km kn ko b">Lambda</code>层</li><li id="3fcf" class="kp kq iq jp b jq ky ju kz jy la kc lb kg lc kk ku kv kw kx bi translated">将多个张量传递给λ层</li><li id="aabb" class="kp kq iq jp b jq ky ju kz jy la kc lb kg lc kk ku kv kw kx bi translated">保存和加载带有lambda层的模型</li><li id="aacb" class="kp kq iq jp b jq ky ju kz jy la kc lb kg lc kk ku kv kw kx bi translated">加载带有lambda层的模型时解决系统错误</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/32ee5b2042d6e1f064fc2bf6cacf0821.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G6yS3IeYiHq2ZN-jO4IsyA.jpeg"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">图片来自Unsplash由<a class="ae lt" href="https://unsplash.com/@martinsanchez" rel="noopener ugc nofollow" target="_blank">马丁·桑切斯</a>:<a class="ae lt" href="https://unsplash.com/photos/NfLZeAN7I6s" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/NfLZeAN7I6s</a></p></figure><h1 id="9580" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">使用<code class="fe kl km kn ko b">Functional API</code>构建Keras模型</h1><p id="e27c" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">有三种不同的API可用于在Keras中构建模型:</p><ol class=""><li id="8273" class="kp kq iq jp b jq jr ju jv jy kr kc ks kg kt kk mx kv kw kx bi translated">顺序API</li><li id="a377" class="kp kq iq jp b jq ky ju kz jy la kc lb kg lc kk mx kv kw kx bi translated">功能API</li><li id="b8a6" class="kp kq iq jp b jq ky ju kz jy la kc lb kg lc kk mx kv kw kx bi translated">模型子类化API</li></ol><p id="20b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以在本帖中找到关于这些的更多信息，但是在本教程中，我们将重点关注使用Keras <code class="fe kl km kn ko b">Functional API</code>来构建一个定制模型。由于我们想专注于我们的架构，我们将只使用一个简单的问题示例，并建立一个模型来识别MNIST数据集中的图像。</p><p id="a905" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要在Keras中构建模型，您需要将层堆叠在另一层之上。这些层在<code class="fe kl km kn ko b">keras.layers</code>模块中可用(在下面导入)。模块名由<code class="fe kl km kn ko b">tensorflow</code>前置，因为我们使用TensorFlow作为Keras的后端。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="d501" class="nc lv iq ko b gy nd ne l nf ng">import tensorflow.keras.layers</span></pre><p id="2571" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要创建的第一层是<code class="fe kl km kn ko b">Input</code>层。这是使用<code class="fe kl km kn ko b">tensorflow.keras.layers.Input()</code>类创建的。传递给这个类的构造函数的必要参数之一是<code class="fe kl km kn ko b">shape</code>参数，它指定了将用于训练的数据中每个样本的形状。在本教程中，我们将只使用密集层，因此输入应该是1-D矢量。因此，<code class="fe kl km kn ko b">shape</code>参数被赋予一个只有一个值的元组(如下所示)。值为784，因为MNIST数据集中每个影像的大小为28 x 28 = 784。可选的<code class="fe kl km kn ko b">name</code>参数指定该层的名称。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="f9d6" class="nc lv iq ko b gy nd ne l nf ng">input_layer = tensorflow.keras.layers.Input(shape=(784), name="input_layer")</span></pre><p id="31e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一层是根据下面的代码使用<code class="fe kl km kn ko b">Dense</code>类创建的密集层。它接受一个名为<code class="fe kl km kn ko b">units</code>的参数来指定该层中神经元的数量。请注意该图层是如何通过在括号中指定该图层的名称来连接到输入图层的。这是因为函数式API中的层实例可在张量上调用，并且也返回张量。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="7a23" class="nc lv iq ko b gy nd ne l nf ng">dense_layer_1 = tensorflow.keras.layers.Dense(units=500, name="dense_layer_1")(input_layer)</span></pre><p id="6fd2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在密集层之后，根据下一行，使用<code class="fe kl km kn ko b">ReLU</code>类创建一个激活层。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="2faa" class="nc lv iq ko b gy nd ne l nf ng">activ_layer_1 = tensorflow.keras.layers.ReLU(name="activ_layer_1")(dense_layer_1)</span></pre><p id="17f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据下面的代码行，添加了另外两个dense-ReLu层。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="44ba" class="nc lv iq ko b gy nd ne l nf ng">dense_layer_2 = tensorflow.keras.layers.Dense(units=250, name="dense_layer_2")(activ_layer_1)<br/>activ_layer_2 = tensorflow.keras.layers.ReLU(name="relu_layer_2")(dense_layer_2)</span><span id="805f" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_3 = tensorflow.keras.layers.Dense(units=20, name="dense_layer_3")(activ_layer_2)<br/>activ_layer_3 = tensorflow.keras.layers.ReLU(name="relu_layer_3")(dense_layer_3)</span></pre><p id="8c51" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一行根据MNIST数据集中的类数量将最后一个图层添加到网络架构中。因为MNIST数据集包括10个类(每个数字对应一个类)，所以此图层中使用的单位数为10。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="3092" class="nc lv iq ko b gy nd ne l nf ng">dense_layer_4 = tensorflow.keras.layers.Dense(units=10, name="dense_layer_4")(activ_layer_3)</span></pre><p id="6bb4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了返回每个类的分数，根据下一行在前一密集层之后添加一个<code class="fe kl km kn ko b">softmax</code>层。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="e72e" class="nc lv iq ko b gy nd ne l nf ng">output_layer = tensorflow.keras.layers.Softmax(name="output_layer")(dense_layer_4)</span></pre><p id="2bd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在已经连接了层，但是模型还没有创建。为了构建一个模型，我们现在必须使用<code class="fe kl km kn ko b">Model</code>类，如下所示。它接受的前两个参数代表输入和输出层。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="702a" class="nc lv iq ko b gy nd ne l nf ng">model = tensorflow.keras.models.Model(input_layer, output_layer, name="model")</span></pre><p id="fce1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在加载数据集和训练模型之前，我们必须使用<code class="fe kl km kn ko b">compile()</code>方法编译模型。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="700d" class="nc lv iq ko b gy nd ne l nf ng">model.compile(optimizer=tensorflow.keras.optimizers.Adam(lr=0.0005), loss="categorical_crossentropy")</span></pre><p id="e32a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用<code class="fe kl km kn ko b">model.summary()</code>我们可以看到模型架构的概述。输入层接受一个形状张量(None，784 ),这意味着每个样本必须被整形为一个784元素的向量。输出<code class="fe kl km kn ko b">Softmax</code>图层返回10个数字，每个数字都是该类MNIST数据集的分数。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="0874" class="nc lv iq ko b gy nd ne l nf ng"><strong class="ko ir">_________________________________________________________________</strong><br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_layer (InputLayer)     [(None, 784)]             0         <br/><strong class="ko ir">_________________________________________________________________</strong><br/>dense<em class="ni">_layer_</em>1 (Dense)        (None, 500)               392500    <br/><strong class="ko ir">_________________________________________________________________</strong><br/>relu<em class="ni">_layer_</em>1 (ReLU)          (None, 500)               0         <br/><strong class="ko ir">_________________________________________________________________</strong><br/>dense<em class="ni">_layer_</em>2 (Dense)        (None, 250)               125250    <br/><strong class="ko ir">_________________________________________________________________</strong><br/>relu<em class="ni">_layer_</em>2 (ReLU)          (None, 250)               0         <br/><strong class="ko ir">_________________________________________________________________</strong><br/>dense<em class="ni">_layer_</em>3 (Dense)        (None, 20)                12550     <br/><strong class="ko ir">_________________________________________________________________</strong><br/>relu<em class="ni">_layer_</em>3 (ReLU)          (None, 20)                0         <br/><strong class="ko ir">_________________________________________________________________</strong><br/>dense<em class="ni">_layer_</em>4 (Dense)        (None, 10)                510       <br/><strong class="ko ir">_________________________________________________________________</strong><br/>output_layer (Softmax)       (None, 10)                0         <br/>=================================================================<br/>Total params: 530,810<br/>Trainable params: 530,810<br/>Non-trainable params: 0<br/><strong class="ko ir">_________________________________________________________________</strong></span></pre><p id="d39b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们已经构建并编译了模型，让我们看看数据集是如何准备的。首先，我们将从<code class="fe kl km kn ko b">keras.datasets</code>模块加载MNIST，将它们的数据类型更改为<code class="fe kl km kn ko b">float64</code>，因为这使得训练网络比将其值留在0-255范围内更容易，最后重新调整，使每个样本都是784个元素的向量。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="f513" class="nc lv iq ko b gy nd ne l nf ng">(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()</span><span id="4eb6" class="nc lv iq ko b gy nh ne l nf ng">x_train = x_train.astype(numpy.float64) / 255.0<br/>x_test = x_test.astype(numpy.float64) / 255.0</span><span id="51e4" class="nc lv iq ko b gy nh ne l nf ng">x_train = x_train.reshape((x_train.shape[0], numpy.prod(x_train.shape[1:])))<br/>x_test = x_test.reshape((x_test.shape[0], numpy.prod(x_test.shape[1:])))</span></pre><p id="d881" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为在<code class="fe kl km kn ko b">compile()</code>方法中使用的损失函数是<code class="fe kl km kn ko b">categorical_crossentropy</code>，样本的标签应该根据下一个代码进行热编码。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="f642" class="nc lv iq ko b gy nd ne l nf ng">y_test = tensorflow.keras.utils.to_categorical(y_test)<br/>y_train = tensorflow.keras.utils.to_categorical(y_train)</span></pre><p id="c84d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，模型训练开始使用<code class="fe kl km kn ko b">fit()</code>方法。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="5840" class="nc lv iq ko b gy nd ne l nf ng">model.fit(x_train, y_train, epochs=20, batch_size=256, validation_data=(x_test, y_test))</span></pre><p id="6daa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">至此，我们已经使用已经存在的层类型创建了模型架构。下一节讨论使用<code class="fe kl km kn ko b">Lambda</code>层构建定制操作。</p><h1 id="5b7e" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">使用λ层</h1><p id="68cd" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">假设在名为<code class="fe kl km kn ko b">dense_layer_3</code>的稠密层之后，我们想要对张量进行某种操作，比如给每个元素加值2。我们如何做到这一点？现有的层都没有这样做，所以我们必须自己建立一个新的层。幸运的是，<code class="fe kl km kn ko b">Lambda</code>层正是为了这个目的而存在的。大家讨论一下怎么用吧。</p><p id="beba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先构建将执行所需操作的函数。在这种情况下，名为<code class="fe kl km kn ko b">custom_layer</code>的函数创建如下。它只接受输入张量并返回另一个张量作为输出。如果不止一个张量被传递给函数，那么它们将作为一个列表被传递。</p><p id="bf56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个例子中，只有一个张量作为输入，输入张量中的每个元素加2。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="390d" class="nc lv iq ko b gy nd ne l nf ng">def custom_layer(tensor):<br/>    return tensor + 2</span></pre><p id="a0a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在构建了定义操作的函数之后，接下来我们需要使用下一行中定义的<code class="fe kl km kn ko b">Lambda</code>类创建lambda层。在这种情况下，只有一个张量被提供给<code class="fe kl km kn ko b">custom_layer</code>函数，因为lambda层可在名为<code class="fe kl km kn ko b">dense_layer_3</code>的稠密层返回的单个张量上调用。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="1a7a" class="nc lv iq ko b gy nd ne l nf ng">lambda_layer = tensorflow.keras.layers.Lambda(custom_layer, name="lambda_layer")(dense_layer_3)</span></pre><p id="ee83" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是使用lambda层后构建完整网络的代码。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="7dcd" class="nc lv iq ko b gy nd ne l nf ng">input_layer = tensorflow.keras.layers.Input(shape=(784), name="input_layer")</span><span id="ae6c" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_1 = tensorflow.keras.layers.Dense(units=500, name="dense_layer_1")(input_layer)<br/>activ_layer_1 = tensorflow.keras.layers.ReLU(name="relu_layer_1")(dense_layer_1)</span><span id="338e" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_2 = tensorflow.keras.layers.Dense(units=250, name="dense_layer_2")(activ_layer_1)<br/>activ_layer_2 = tensorflow.keras.layers.ReLU(name="relu_layer_2")(dense_layer_2)</span><span id="9268" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_3 = tensorflow.keras.layers.Dense(units=20, name="dense_layer_3")(activ_layer_2)</span><span id="1e8d" class="nc lv iq ko b gy nh ne l nf ng">def custom_layer(tensor):<br/>    return tensor + 2</span><span id="5f15" class="nc lv iq ko b gy nh ne l nf ng">lambda_layer = tensorflow.keras.layers.Lambda(custom_layer, name="lambda_layer")(dense_layer_3)</span><span id="930e" class="nc lv iq ko b gy nh ne l nf ng">activ_layer_3 = tensorflow.keras.layers.ReLU(name="relu_layer_3")(lambda_layer)</span><span id="8a23" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_4 = tensorflow.keras.layers.Dense(units=10, name="dense_layer_4")(activ_layer_3)<br/>output_layer = tensorflow.keras.layers.Softmax(name="output_layer")(dense_layer_4)</span><span id="f860" class="nc lv iq ko b gy nh ne l nf ng">model = tensorflow.keras.models.Model(input_layer, output_layer, name="model")</span></pre><p id="bb97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了查看馈送到lambda层之前和之后的张量，我们将创建两个新模型，除了上一个模型。我们将这些称为<code class="fe kl km kn ko b">before_lambda_model</code>和<code class="fe kl km kn ko b">after_lambda_model</code>。两种模型都使用输入层作为输入，但输出层不同。<code class="fe kl km kn ko b">before_lambda_model</code>模型返回<code class="fe kl km kn ko b">dense_layer_3</code>的输出，T3是正好存在于lambda层之前的层。<code class="fe kl km kn ko b">after_lambda_model</code>模型的输出是来自名为<code class="fe kl km kn ko b">lambda_layer</code>的λ层的输出。这样做，我们可以看到应用lambda层之前的输入和之后的输出。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="319d" class="nc lv iq ko b gy nd ne l nf ng">before_lambda_model = tensorflow.keras.models.Model(input_layer, dense_layer_3, name="before_lambda_model")<br/>after_lambda_model = tensorflow.keras.models.Model(input_layer, lambda_layer, name="after_lambda_model")</span></pre><p id="4532" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面列出了构建和训练整个网络的完整代码。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="2503" class="nc lv iq ko b gy nd ne l nf ng">import tensorflow.keras.layers<br/>import tensorflow.keras.models<br/>import tensorflow.keras.optimizers<br/>import tensorflow.keras.datasets<br/>import tensorflow.keras.utils<br/>import tensorflow.keras.backend<br/>import numpy</span><span id="18b1" class="nc lv iq ko b gy nh ne l nf ng">input_layer = tensorflow.keras.layers.Input(shape=(784), name="input_layer")</span><span id="b81e" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_1 = tensorflow.keras.layers.Dense(units=500, name="dense_layer_1")(input_layer)<br/>activ_layer_1 = tensorflow.keras.layers.ReLU(name="relu_layer_1")(dense_layer_1)</span><span id="582d" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_2 = tensorflow.keras.layers.Dense(units=250, name="dense_layer_2")(activ_layer_1)<br/>activ_layer_2 = tensorflow.keras.layers.ReLU(name="relu_layer_2")(dense_layer_2)</span><span id="8498" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_3 = tensorflow.keras.layers.Dense(units=20, name="dense_layer_3")(activ_layer_2)</span><span id="49ea" class="nc lv iq ko b gy nh ne l nf ng">before_lambda_model = tensorflow.keras.models.Model(input_layer, dense_layer_3, name="before_lambda_model")</span><span id="83d5" class="nc lv iq ko b gy nh ne l nf ng">def custom_layer(tensor):<br/>    return tensor + 2</span><span id="1815" class="nc lv iq ko b gy nh ne l nf ng">lambda_layer = tensorflow.keras.layers.Lambda(custom_layer, name="lambda_layer")(dense_layer_3)<br/>after_lambda_model = tensorflow.keras.models.Model(input_layer, lambda_layer, name="after_lambda_model")</span><span id="6ae1" class="nc lv iq ko b gy nh ne l nf ng">activ_layer_3 = tensorflow.keras.layers.ReLU(name="relu_layer_3")(lambda_layer)</span><span id="e166" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_4 = tensorflow.keras.layers.Dense(units=10, name="dense_layer_4")(activ_layer_3)<br/>output_layer = tensorflow.keras.layers.Softmax(name="output_layer")(dense_layer_4)</span><span id="14a0" class="nc lv iq ko b gy nh ne l nf ng">model = tensorflow.keras.models.Model(input_layer, output_layer, name="model")</span><span id="a895" class="nc lv iq ko b gy nh ne l nf ng">model.compile(optimizer=tensorflow.keras.optimizers.Adam(lr=0.0005), loss="categorical_crossentropy")<br/>model.summary()</span><span id="d482" class="nc lv iq ko b gy nh ne l nf ng">(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()</span><span id="9349" class="nc lv iq ko b gy nh ne l nf ng">x_train = x_train.astype(numpy.float64) / 255.0<br/>x_test = x_test.astype(numpy.float64) / 255.0</span><span id="12dd" class="nc lv iq ko b gy nh ne l nf ng">x_train = x_train.reshape((x_train.shape[0], numpy.prod(x_train.shape[1:])))<br/>x_test = x_test.reshape((x_test.shape[0], numpy.prod(x_test.shape[1:])))</span><span id="011a" class="nc lv iq ko b gy nh ne l nf ng">y_test = tensorflow.keras.utils.to_categorical(y_test)<br/>y_train = tensorflow.keras.utils.to_categorical(y_train)</span><span id="a0f1" class="nc lv iq ko b gy nh ne l nf ng">model.fit(x_train, y_train, epochs=20, batch_size=256, validation_data=(x_test, y_test))</span></pre><p id="52d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，您不必编译或训练这两个新创建的模型，因为它们的层实际上是从存在于<code class="fe kl km kn ko b">model</code>变量中的主模型中重用的。在这个模型被训练之后，我们可以使用<code class="fe kl km kn ko b">predict()</code>方法返回<code class="fe kl km kn ko b">before_lambda_model</code>和<code class="fe kl km kn ko b">after_lambda_model</code>模型的输出，看看lambda层的结果如何。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="f0d7" class="nc lv iq ko b gy nd ne l nf ng">p = model.predict(x_train)</span><span id="15cc" class="nc lv iq ko b gy nh ne l nf ng">m1 = before_lambda_model.predict(x_train)<br/>m2 = after_lambda_model.predict(x_train)</span></pre><p id="0452" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一段代码只打印前两个样本的输出。可以看到，从<code class="fe kl km kn ko b">m2</code>数组返回的每个元素实际上都是<code class="fe kl km kn ko b">m1</code>加2后的结果。这正是我们在自定义lambda层中应用的操作。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="fe1f" class="nc lv iq ko b gy nd ne l nf ng">print(m1[0, :])<br/>print(m2[0, :])</span><span id="e9af" class="nc lv iq ko b gy nh ne l nf ng">[ 14.420735    8.872794   25.369402    1.4622561   5.672293    2.5202641 -14.753801   -3.8822086  -1.0581762  -6.4336205  13.342142   -3.0627508  -5.694006   -6.557313   -1.6567478  -3.8457105  11.891999   20.581928   2.669979   -8.092522 ]<br/>[ 16.420734    10.872794    27.369402     3.462256     7.672293<br/>   4.520264   -12.753801    -1.8822086    0.94182384  -4.4336205<br/>  15.342142    -1.0627508   -3.694006    -4.557313     0.34325218<br/>  -1.8457105   13.891999    22.581928     4.669979    -6.0925217 ]</span></pre><p id="fcfd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本节中，lambda层用于对单个输入张量进行运算。在下一节中，我们将看到如何将两个输入张量传递给这一层。</p><h1 id="2c79" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">将多个张量传递给λ层</h1><p id="c40f" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">假设我们想要做一个依赖于名为<code class="fe kl km kn ko b">dense_layer_3</code>和<code class="fe kl km kn ko b">relu_layer_3</code>的两层的操作。在这种情况下，我们必须调用lambda层，同时传递两个张量。这可以简单地通过创建一个包含所有这些张量的列表来完成，如下一行所示。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="acd5" class="nc lv iq ko b gy nd ne l nf ng">lambda_layer = tensorflow.keras.layers.Lambda(custom_layer, name="lambda_layer")([dense_layer_3, activ_layer_3])</span></pre><p id="932f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个列表被传递给<code class="fe kl km kn ko b">custom_layer()</code>函数，我们可以简单地根据下一个代码获取各个层。它只是把这两层加在一起。Keras中实际上有一个名为<code class="fe kl km kn ko b">Add</code>的层，可以用来添加两层或更多层，但是我们只是介绍如果有另一个Keras不支持的操作，你可以自己怎么做。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="764c" class="nc lv iq ko b gy nd ne l nf ng">def custom_layer(tensor):<br/>    tensor1 = tensor[0]<br/>    tensor2 = tensor[1]<br/>    return tensor1 + tensor2</span></pre><p id="bb7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来的代码构建了三个模型:两个用于捕获从<code class="fe kl km kn ko b">dense_layer_3</code>和<code class="fe kl km kn ko b">activ_layer_3</code>传递到lambda层的输出，另一个用于捕获lambda层本身的输出。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="6ae6" class="nc lv iq ko b gy nd ne l nf ng">before_lambda_model1 = tensorflow.keras.models.Model(input_layer, dense_layer_3, name="before_lambda_model1")<br/>before_lambda_model2 = tensorflow.keras.models.Model(input_layer, activ_layer_3, name="before_lambda_model2")</span><span id="56ee" class="nc lv iq ko b gy nh ne l nf ng">lambda_layer = tensorflow.keras.layers.Lambda(custom_layer, name="lambda_layer")([dense_layer_3, activ_layer_3])<br/>after_lambda_model = tensorflow.keras.models.Model(input_layer, lambda_layer, name="after_lambda_model")</span></pre><p id="e8d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了查看来自<code class="fe kl km kn ko b">dense_layer_3</code>、<code class="fe kl km kn ko b">activ_layer_3</code>和<code class="fe kl km kn ko b">lambda_layer</code>层的输出，下一段代码预测它们的输出并打印出来。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="19f5" class="nc lv iq ko b gy nd ne l nf ng">m1 = before_lambda_model1.predict(x_train)<br/>m2 = before_lambda_model2.predict(x_train)<br/>m3 = after_lambda_model.predict(x_train)</span><span id="0061" class="nc lv iq ko b gy nh ne l nf ng">print(m1[0, :])<br/>print(m2[0, :])<br/>print(m3[0, :])<!-- --> </span><span id="40ec" class="nc lv iq ko b gy nh ne l nf ng">[ 1.773366   -3.4378722   0.22042789 11.220362    3.4020965  14.487111  4.239182   -6.8589864  -6.428128   -5.477719   -8.799093    7.264849 17.503246   -6.809489   -6.846208   16.094025   24.483786   -7.084775 17.341183   20.311539  ]<br/>[ 1.773366    0.          0.22042789 11.220362    3.4020965  14.487111  4.239182    0.          0.          0.          0.          7.264849 17.503246    0.          0.         16.094025   24.483786    0. 17.341183   20.311539  ]<br/>[ 3.546732   -3.4378722   0.44085577 22.440723    6.804193   28.974222  8.478364   -6.8589864  -6.428128   -5.477719   -8.799093   14.529698 35.006493   -6.809489   -6.846208   32.18805    48.96757    -7.084775 34.682365   40.623077  ]</span></pre><p id="5e27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用lambda层现在很清楚了。下一节将讨论如何保存和加载使用lambda层的模型。</p><h1 id="959c" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">保存和加载带有Lambda层的模型</h1><p id="d2c1" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">为了保存一个模型(不管它是否使用lambda层),使用了<code class="fe kl km kn ko b">save()</code>方法。假设我们只对保存主模型感兴趣，下面是保存它的代码行。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="8fb0" class="nc lv iq ko b gy nd ne l nf ng">model.save("model.h5")</span></pre><p id="cff3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们还可以使用<code class="fe kl km kn ko b">load_model()</code>方法加载保存的模型，如下一行所示。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="0166" class="nc lv iq ko b gy nd ne l nf ng">loaded_model = tensorflow.keras.models.load_model("model.h5")</span></pre><p id="d2d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望模型能够成功加载。不幸的是，Keras中的一些问题可能会导致在加载带有lambda层的模型时出现<code class="fe kl km kn ko b">SystemError: unknown opcode</code>。这可能是由于使用Python版本构建模型并在另一个版本中使用它。我们将在下一节讨论解决方案。</p><h1 id="1712" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">加载带有Lambda层的模型时解决系统错误</h1><p id="29e6" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">为了解决这个问题，我们不打算以上面讨论的方式保存模型。相反，我们将使用<code class="fe kl km kn ko b">save_weights()</code>方法保存模型权重。</p><p id="f56d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们只保留了重量。模型架构呢？将使用代码重新创建模型架构。为什么不将模型架构保存为JSON文件，然后再次加载呢？原因是加载架构后错误仍然存在。</p><p id="a198" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总之，经过训练的模型权重将被保存，模型架构将使用代码被复制，并且最终权重将被加载到该架构中。</p><p id="0013" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以使用下一行保存模型的权重。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="3394" class="nc lv iq ko b gy nd ne l nf ng">model.save_weights('model_weights.h5')</span></pre><p id="6200" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是复制模型架构的代码。<code class="fe kl km kn ko b">model</code>将不会被训练，但保存的重量将再次分配给它。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="dc00" class="nc lv iq ko b gy nd ne l nf ng">input_layer = tensorflow.keras.layers.Input(shape=(784), name="input_layer")</span><span id="e57e" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_1 = tensorflow.keras.layers.Dense(units=500, name="dense_layer_1")(input_layer)<br/>activ_layer_1 = tensorflow.keras.layers.ReLU(name="relu_layer_1")(dense_layer_1)</span><span id="a512" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_2 = tensorflow.keras.layers.Dense(units=250, name="dense_layer_2")(activ_layer_1)<br/>activ_layer_2 = tensorflow.keras.layers.ReLU(name="relu_layer_2")(dense_layer_2)</span><span id="6b0a" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_3 = tensorflow.keras.layers.Dense(units=20, name="dense_layer_3")(activ_layer_2)<br/>activ_layer_3 = tensorflow.keras.layers.ReLU(name="relu_layer_3")(dense_layer_3)</span><span id="acc0" class="nc lv iq ko b gy nh ne l nf ng">def custom_layer(tensor):<br/>    tensor1 = tensor[0]<br/>    tensor2 = tensor[1]</span><span id="8903" class="nc lv iq ko b gy nh ne l nf ng">    epsilon = tensorflow.keras.backend.random_normal(shape=tensorflow.keras.backend.shape(tensor1), mean=0.0, stddev=1.0)<br/>    random_sample = tensor1 + tensorflow.keras.backend.exp(tensor2/2) * epsilon<br/>    return random_sample</span><span id="d816" class="nc lv iq ko b gy nh ne l nf ng">lambda_layer = tensorflow.keras.layers.Lambda(custom_layer, name="lambda_layer")([dense_layer_3, activ_layer_3])</span><span id="62ab" class="nc lv iq ko b gy nh ne l nf ng">dense_layer_4 = tensorflow.keras.layers.Dense(units=10, name="dense_layer_4")(lambda_layer)<br/>after_lambda_model = tensorflow.keras.models.Model(input_layer, dense_layer_4, name="after_lambda_model")</span><span id="3f84" class="nc lv iq ko b gy nh ne l nf ng">output_layer = tensorflow.keras.layers.Softmax(name="output_layer")(dense_layer_4)</span><span id="f1be" class="nc lv iq ko b gy nh ne l nf ng">model = tensorflow.keras.models.Model(input_layer, output_layer, name="model")</span><span id="d5c6" class="nc lv iq ko b gy nh ne l nf ng">model.compile(optimizer=tensorflow.keras.optimizers.Adam(lr=0.0005), loss="categorical_crossentropy")</span></pre><p id="3abf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是如何使用<code class="fe kl km kn ko b">load_weights()</code>方法加载保存的权重，并将其分配给复制的架构。</p><pre class="le lf lg lh gt my ko mz na aw nb bi"><span id="0a4c" class="nc lv iq ko b gy nd ne l nf ng">model.load_weights('model_weights.h5')</span></pre></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="d636" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">本文原载于</strong> <a class="ae lt" href="https://blog.paperspace.com/working-with-the-lambda-layer-in-keras" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> Paperspace博客</strong> </a> <strong class="jp ir">。你可以在渐变</strong>  <strong class="jp ir">上免费运行我的教程的代码</strong> <a class="ae lt" href="https://gradient.paperspace.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">。</strong></a></p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="407b" class="lu lv iq bd lw lx nq lz ma mb nr md me mf ns mh mi mj nt ml mm mn nu mp mq mr bi translated">结论</h1><p id="5d0a" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">本教程讨论了使用<code class="fe kl km kn ko b">Lambda</code>层创建自定义层，该层执行Keras中预定义层不支持的操作。<code class="fe kl km kn ko b">Lambda</code>类的构造函数接受一个指定层如何工作的函数，该函数接受调用层的张量。在函数内部，您可以执行任何想要的操作，然后返回修改后的张量。</p><p id="0492" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尽管Keras在加载使用lambda层的模型时存在问题，但我们也看到了如何通过保存训练好的模型权重、使用代码重新生成模型架构，并将权重加载到该架构中来简单地解决这个问题。</p></div></div>    
</body>
</html>