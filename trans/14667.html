<html>
<head>
<title>Machine Learning Algorithms from Start to Finish in Python: KNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从头到尾的机器学习算法:KNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-knn-7a92cce47986?source=collection_archive---------42-----------------------#2020-10-09">https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-knn-7a92cce47986?source=collection_archive---------42-----------------------#2020-10-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/bf5dd3689f87d4e840d3304414c12677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5x_tZX-3lfawIy_5ZsDCug.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">布莱克·惠勒在<a class="ae kf" href="https://unsplash.com/s/photos/neighborhood?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="f5d7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习从业者工具箱中的一个基本算法必须是K个最近邻(或简称为KNN)。虽然它可能是最简单的算法之一，但它也是一种非常强大的算法，并在许多现实应用中使用。在本教程中，我将执行以下操作:</p><ul class=""><li id="46a7" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">解释KNN算法及其工作原理</li><li id="1d20" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">使用Python从头开始实现它</li></ul><p id="752e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，事不宜迟，让我们开始这场机器学习派对吧！</p></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="23a2" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">k最近的邻居解释说</h1><p id="7b32" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">这是一种常见的机器学习算法，可用于分类和回归。该算法采取的步骤如下:</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/f91ff5b85558ee3a6c8cf599c0342eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*LnX1aZocxa_CRMwbPsxsbg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来自<a class="ae kf" href="https://en.wikipedia.org/wiki/File:KnnClassification.svg" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><ol class=""><li id="d15f" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld nh lk ll lm bi translated">首先，它将一组特征和标签作为输入，使其成为一种<strong class="ki iu">监督学习算法。</strong></li><li id="b693" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nh lk ll lm bi translated">除了特性和标签，它还接受一个<strong class="ki iu">n _ neighbors</strong>参数。</li><li id="7410" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nh lk ll lm bi translated">为了预测新样本的类别，它执行以下操作:</li></ol><ul class=""><li id="4c6d" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">测量新样本和第<strong class="ki iu"> N </strong>个最近样本之间的距离(由<strong class="ki iu">N _ neighbors</strong>参数指定)</li><li id="2570" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">基于其最近的邻居，它将基于多数投票对样本进行分类。具体地，如果有3个邻居，其中2个邻居属于A类，1个邻居属于B类，那么它将把新样本分类为A类</li></ul></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><p id="0e37" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了填补你的知识空白，让我们来看一个具体的例子。</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/b21faf9e63c3641b9ab5b88178d8d81a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lu2r5yKHKGarVwEBiWiJ8w.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://unsplash.com/@uvesanchez?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">乌韦·桑切斯</a>在<a class="ae kf" href="https://unsplash.com/s/photos/concrete?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ed14" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们假设以下情况:</p><ul class=""><li id="1517" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">你试图根据血型、身体质量指数等来预测一个病人是否患有心脏病。</li><li id="4ead" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">您已经为训练和验证分离了数据集</li><li id="d12a" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">你已经<strong class="ki iu">缩放了</strong>你的特征(我稍后会讨论这一点)</li><li id="db58" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">你已经有了fir KNN算法，并且已经将<strong class="ki iu">n _ neighbors</strong>参数设置为3</li></ul><p id="8acb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您要向实例提供验证集，将会发生以下情况:</p><ol class=""><li id="8751" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld nh lk ll lm bi translated">该算法将使用<strong class="ki iu">距离度量</strong>来测量新实例和其他训练集实例之间的距离(我稍后也会讨论这一点！)</li><li id="f75c" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nh lk ll lm bi translated">基于<strong class="ki iu">n _ neighbors</strong>参数(在本例中，该参数被设置为3)，它将使用与新样本最接近的三个例子来对新点进行分类</li><li id="55ca" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nh lk ll lm bi translated">然后它将进行投票，哪个类拥有最多的票数就是新样本所属的类。具体来说:</li></ol><ul class=""><li id="31fa" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">如果两个最接近的类别被标记为1(患有心脏病)，一个被标记为0(没有心脏病)，则新样本将被预测为在类别1中</li></ul><p id="2f91" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.对测试集中的所有实例重复这个过程</p></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="9ae2" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">清理一些KNN行话</h1><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/6b873a517d1395191dcede4bdc887b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RXedIAvqj07JUBHiGCUDXg.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://unsplash.com/@ninjason?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">梁杰森</a>在<a class="ae kf" href="https://unsplash.com/s/photos/language?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h2 id="7f4c" class="nk ma it bd mb nl nm dn mf nn no dp mj kr np nq mn kv nr ns mr kz nt nu mv nv bi translated">距离度量</h2><p id="c017" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">这是用于计算两点之间距离的度量。它通常是欧几里德距离，由以下公式给出:</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/bec33d0e94a4c6dbc13a07eb372943a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*F507EfvD_1GAA42iLcCANw.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来自<a class="ae kf" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><h2 id="e566" class="nk ma it bd mb nl nm dn mf nn no dp mj kr np nq mn kv nr ns mr kz nt nu mv nv bi translated"><strong class="ak">特征缩放</strong></h2><p id="4bfb" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">这是许多机器学习算法的重要预处理步骤，尤其是那些使用距离度量和计算的算法(如我们的朋友KNN)。它基本上缩放了我们的特征，使它们在相似的范围内。把它想象成一栋房子，一栋房子的比例模型。两者的<strong class="ki iu">形状</strong>相同(都是房子)，但是<strong class="ki iu">大小</strong>不同(5m！= 500m)。我们这样做的原因如下:</p><ol class=""><li id="99dc" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld nh lk ll lm bi translated">它加速了算法</li><li id="deb2" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nh lk ll lm bi translated">有些算法对规模很敏感。换句话说，如果特征具有不同的比例，则具有较高量值的特征有可能被赋予较高的权重。这将影响机器学习算法的性能，显然，我们不希望我们的算法偏向一个特征。</li></ol><p id="a60f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了证明这一点，让我们假设我们有三个特性，分别命名为A、B和C:</p><ul class=""><li id="61aa" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">缩放前AB的距离= &gt;</li></ul><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/55aefa4ef9f89fa5535a5dcfd47d5705.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/0*CKXBV_mva31U1qW5.gif"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya拍摄</a></p></figure><ul class=""><li id="3d52" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">缩放前BC的距离= &gt;</li></ul><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/2a9f36a74c689cf5a0deee3e76c75f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/0*kucNCcMCcJzfpOU5.gif"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya拍摄</a></p></figure><ul class=""><li id="bb65" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">缩放后AB的距离= &gt;</li></ul><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/2f081c12f9eef3636054c55e761d73a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/0*mTGCfJekCEOrJmaz.gif"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya </a>拍摄</p></figure><ul class=""><li id="1b7e" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">缩放后BC的距离= &gt;</li></ul><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/685dc063b9729be904a58ad9f2df546c.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/0*vf51YKUVBWbv72cY.gif"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya </a>拍摄</p></figure><p id="029e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以清楚地看到，这些特性比缩放之前更具可比性，也更公正。如果你想要一个关于功能缩放的很棒的教程，请查看这个由分析Vidhya撰写的<a class="ae kf" href="http://Photo by Analytics Vidhya" rel="noopener ugc nofollow" target="_blank">博客</a>。</p><h2 id="e784" class="nk ma it bd mb nl nm dn mf nn no dp mj kr np nq mn kv nr ns mr kz nt nu mv nv bi translated">选择n_neighbors参数</h2><p id="99d8" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">虽然没有预先确定的方法来选择最佳数量的邻居，但确实存在一些方法。一种流行的方法叫做<em class="ob">肘方法，</em>运行一个循环，将数据拟合到一系列相邻值。如果您要根据邻居的数量绘制误差图，您的图将如下所示:</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/2a870ae010c84b252cc430bb521b1863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*bu_lVTBu8tLbfcDmwt8obQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者照片</p></figure><p id="e7c1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，最佳点将是弯头快速减少其误差的点(因此在我们的例子中，可能是5或6)。这种方法的问题如下:</p><ul class=""><li id="b2af" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">过度适应测试集是可能的，所以要确保交叉验证。</li><li id="67c0" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">有时，K的最佳数量可能不是很清楚，因此可能很难判断什么是真正的最佳数量</li></ul><p id="f540" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在实现KNN时，一个很好的技巧是将K设置为奇数。这样，就不需要处理票数相等的情况。</p></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="ea2c" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">从技术角度了解代码</h1><p id="29aa" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">知道如何使用像scikit这样的ML库是很好的——学习编码算法，但是真正能提升你的ML技能的是学习如何从头开始构建算法。因此，我们将这样做；从头开始创建KNNClassifier！</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/afe2c3d9a67abf2b2975e7ebbb984939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eE6Js6xF-0kulG-yZ3oFJw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://unsplash.com/@redcharlie?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> redcharlie </a>在<a class="ae kf" href="https://unsplash.com/s/photos/neighbours?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><blockquote class="oe of og"><p id="9f60" class="kg kh ob ki b kj kk kl km kn ko kp kq oh ks kt ku oi kw kx ky oj la lb lc ld im bi translated">注意:代码的链接可以在<a class="ae kf" href="https://github.com/Vagif12/ML-Algorithms-From-Scratch/blob/main/KNN%20From%20Scratch.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到，但是我建议你在检查代码之前浏览一下博客文章，以完全理解正在发生的事情。</p></blockquote><p id="6064" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们导入我们的库:</p><pre class="nd ne nf ng gt ok ol om on aw oo bi"><span id="1be0" class="nk ma it ol b gy op oq l or os">import numpy as np<br/>from sklearn.datasets import load_iris<br/>from sklearn.model_selection import train_test_split<br/>from collections import Counter</span><span id="268e" class="nk ma it ol b gy ot oq l or os">iris = load_iris()<br/>X, y = iris.data, iris.target</span></pre><p id="99fe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是的，我们导入scikit-learn的唯一原因是使用iris数据集并分割数据。除此之外，我们使用普通的数字和集合！</p><p id="ac58" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们创建我们的训练和测试集:</p><pre class="nd ne nf ng gt ok ol om on aw oo bi"><span id="2684" class="nk ma it ol b gy op oq l or os">X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1810)</span></pre><p id="d88e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里没什么不寻常的，所以让我们快速前进。</p><p id="ff09" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我提到特征缩放是KNN的一个重要的预处理步骤。然而，我们的数据已经在一个相似的范围内，所以我们可以跳过这一步。然而，在现实世界的数据中，我们很少能如此幸运。</p><h2 id="54e4" class="nk ma it bd mb nl nm dn mf nn no dp mj kr np nq mn kv nr ns mr kz nt nu mv nv bi translated">使用面向对象的方法编码算法</h2><p id="86c7" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">干净和可重用的代码对任何数据科学家或机器学习工程师来说都是关键。因此，为了遵循软件工程原则，我将创建一个KNN类，使代码可重用且保持原样。</p><p id="4783" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们定义类名，并传入一些参数。即，</p><ul class=""><li id="51b0" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">x(特性)</li><li id="2112" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">y(标签向量)</li><li id="42a7" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">n_neighbors(我们希望的邻居数量)</li></ul><pre class="nd ne nf ng gt ok ol om on aw oo bi"><span id="0eeb" class="nk ma it ol b gy op oq l or os">class KNN:<br/>    def __init__(self,X,y,n_neighbors=3):<br/>        self.X = X<br/>        self.y = y<br/>        self.n_neighbors = n_neighbors</span></pre><p id="fe22" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们将上面的欧几里德距离公式转换成代码，并使其成为类的方法:</p><pre class="nd ne nf ng gt ok ol om on aw oo bi"><span id="ea31" class="nk ma it ol b gy op oq l or os">def euclidean(self,x1,x2):<br/>        return np.sqrt(np.sum((x1 - x2) ** 2))</span></pre><p id="85da" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在是举重。我将首先向您展示代码，然后解释发生了什么:</p><pre class="nd ne nf ng gt ok ol om on aw oo bi"><span id="b4a8" class="nk ma it ol b gy op oq l or os">def fit_knn(self,X_test):<br/>        distances = [self.euclidean(X_test,x) for x in X_train]<br/>        k_nearest = np.argsort(distances)[:self.n_neighbors]<br/>        k_nearest_labels = [y_train[i] for i in k_nearest]<br/>        <br/>        most_common = Counter(k_nearest_labels).most_common(1)[0][0]<br/>        return most_common</span></pre><p id="6221" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先创建一个名为fit_knn的方法，该方法可以将knn拟合到数据中！更具体地说，正在开展以下工作:</p><ol class=""><li id="5871" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld nh lk ll lm bi translated">测量测试集中的每个数据点到训练集中的数据点之间的距离</li><li id="bb78" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nh lk ll lm bi translated">我们得到K个最近的点(K是我们的邻居数量的参数，在我们的例子中是3)</li><li id="b614" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nh lk ll lm bi translated">之后，代码返回我们发现最接近新测试集实例的最近邻居的标签。</li><li id="00e9" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nh lk ll lm bi translated">最常见的类由方法计数并返回</li></ol><p id="5f93" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，最重要的是，我们做出预测:</p><pre class="nd ne nf ng gt ok ol om on aw oo bi"><span id="9f12" class="nk ma it ol b gy op oq l or os">knn = KNN(X_train,y_train)</span><span id="f7a9" class="nk ma it ol b gy ot oq l or os">preds = knn.predict(X_test)</span></pre><p id="2eb2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们评估我们的模型，看看它对新样本的分类效果如何:</p><pre class="nd ne nf ng gt ok ol om on aw oo bi"><span id="b055" class="nk ma it ol b gy op oq l or os">accuracy = (preds == y_test).mean()</span><span id="2dde" class="nk ma it ol b gy ot oq l or os">OUT: 1.0</span></pre><p id="6faf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，完整的代码如下:</p><pre class="nd ne nf ng gt ok ol om on aw oo bi"><span id="d239" class="nk ma it ol b gy op oq l or os">import numpy as np<br/>from collections import Counter<br/>from sklearn.datasets import load_iris</span><span id="21c4" class="nk ma it ol b gy ot oq l or os">iris = load_iris()<br/>X, y = iris.data, iris.target</span><span id="3818" class="nk ma it ol b gy ot oq l or os">X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1810)</span><span id="e0fa" class="nk ma it ol b gy ot oq l or os">class KNN:<br/>    def __init__(self,X,y,n_neighbors=3):<br/>        self.X = X<br/>        self.y = y<br/>        self.n_neighbors = n_neighbors<br/>        <br/>    def euclidean(self,x1,x2):<br/>        return np.sqrt(np.sum((x1 - x2) ** 2))<br/>    <br/>    def fit_knn(self,X_test):<br/>        distances = [self.euclidean(X_test,x) for x in X_train]<br/>        k_nearest = np.argsort(distances)[:self.n_neighbors]<br/>        k_nearest_labels = [y_train[i] for i in k_nearest]<br/>        <br/>        most_common = Counter(k_nearest_labels).most_common(1)[0][0]<br/>        return most_common<br/>    <br/>    def predict(self,X_test):<br/>        preds = [self.fit_knn(x) for x in X_test]<br/>        return preds</span><span id="e57a" class="nk ma it ol b gy ot oq l or os">knn = KNN(X_train,y_train)<br/>preds = knn.predict(X_test)</span><span id="851a" class="nk ma it ol b gy ot oq l or os">accuracy = (preds == y_test).mean()</span></pre></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="648a" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">要做的其他任务:</h1><ol class=""><li id="d374" class="le lf it ki b kj mx kn my kr ou kv ov kz ow ld nh lk ll lm bi translated">尝试使用n_neighbors参数。什么变化？</li><li id="d244" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nh lk ll lm bi translated">尝试缩放您的要素，看看结果是否不同。好点了吗？或者更糟？</li><li id="d9ff" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nh lk ll lm bi translated">尝试将模型拟合到不同的数据集。看看改变K如何改变结果</li><li id="d3e3" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nh lk ll lm bi translated">看看您是否可以自己实现肘方法来确定最佳邻居数量</li></ol></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><p id="b6eb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">非常感谢您阅读本文。我希望你学到了新的东西，或者至少刷新了你的记忆。现在，那是很久了！所以，我对你的最后一个愿望是，休息一下，休息一下，享受生活！</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ox"><img src="../Images/5a960cfb7cfecb22bbbd10a641517f70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*98rMg3QH0CFXOEUVLN3GjQ.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">简·廷伯格在<a class="ae kf" href="https://unsplash.com/s/photos/goodbye?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div></div>    
</body>
</html>