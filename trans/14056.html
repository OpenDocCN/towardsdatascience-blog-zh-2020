<html>
<head>
<title>An Intuitive Guide to LSTMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTMs直观指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-guide-to-lstms-e26f06273ad3?source=collection_archive---------46-----------------------#2020-09-27">https://towardsdatascience.com/an-intuitive-guide-to-lstms-e26f06273ad3?source=collection_archive---------46-----------------------#2020-09-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1179" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从零开始创造它们</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a21768ce11a787d26e544c9d96705034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6OVV5rtaPqG4Vfezy67Vpg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">布伦特·德·兰特在<a class="ae ky" href="https://unsplash.com/s/photos/building?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="9802" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LSTMs是机器学习中最重要的突破之一；赋予机器学习算法回忆过去信息的能力，允许实现时间模式。理解一个概念比从头开始创造一个概念更好吗？</p><h1 id="b120" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是LSTMs？</h1><h2 id="49a9" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">宽泛:</h2><p id="b1ba" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">LSTM代表长期短期记忆，表示其利用过去信息进行预测的能力。LSTM背后的机制非常简单。LSTMs具有来自不同时间步长的不同处理信息源作为网络的输入，而不是用于数据传播的单一前馈过程，因此能够访问数据内与时间相关的模式。</p><p id="0e01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LSTM架构不仅仅由一个神经网络组成，而是由至少三个同时训练的神经网络组成的电池。此外，LSTM体系结构还包含一些门，这些门为神经网络的最终预测赋予某些数据更高的权重。</p><h2 id="578d" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated">详细信息:</h2><p id="d2c5" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">以下是LSTM传播一条输入数据所采取的步骤的详细列表:</p><ol class=""><li id="1241" class="ne nf it lb b lc ld lf lg li ng lm nh lq ni lu nj nk nl nm bi translated">输入数据提供给三个不同的神经网络，即遗忘网络、选择网络和细胞状态网络</li><li id="206e" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">输入数据通过这些神经网络传播，给出输出</li><li id="f054" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">适当的激活函数用于每个输出，以防止消失或爆炸梯度。</li><li id="8660" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">对于遗忘门，数据然后与从上一时间步收集的可能性交叉相乘</li><li id="5e99" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">将为收集的可能性交叉相乘的值存储为收集的可能性</li><li id="e468" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">将该值乘以选择神经网络的预测，以给出LSTM的预测</li></ol><p id="c10d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个复杂的过程，但理解每一步是至关重要的，以便正确地应用导数。</p><h1 id="77a8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">代码:</h1><p id="c448" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">现在，您已经很好地理解了LSTM应该如何工作(理论上)，我们可以继续实际的程序。</p><h1 id="10dc" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">代码:</h1><p id="72a9" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated"><strong class="lb iu">第一步|设置:</strong></p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="a2ae" class="mn lw it nt b gy nx ny l nz oa">import numpy <br/>from matplotlib import pyplot as plt<br/>def sigmoid(x):<br/>    return 1/(1+np.exp(-x))def sigmoid_p(x):<br/>    return sigmoid(x)*(1 -sigmoid(x))def relu(x):<br/>    return np.maximum(x, 0)def relu_p(x):<br/>    return np.heaviside(x, 0)def tanh(x):<br/>    return np.tanh(x)def tanh_p(x):<br/>    return 1.0 - np.tanh(x)**2def deriv_func(z,function):<br/>    if function == sigmoid:<br/>        return sigmoid_p(z)<br/>    elif function == relu:<br/>        return relu_p(z)<br/>    elif function == tanh:<br/>        return tanh_p(z)</span></pre><p id="1b1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这段代码中，我安装了程序的唯一依赖项，Numpy用于矩阵乘法和数组操作，不同的激活函数和它们的导数用于神经网络的灵活性</p><p id="de88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第二步| LSTM框架:</strong></p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="9bc8" class="mn lw it nt b gy nx ny l nz oa">class LSTM:<br/>    def __init__(self,network):<br/>        <br/>        def plus_gate(x,y):<br/>            return np.array(x) + np.array(y)<br/>            <br/>        def multiply_gate(x,y):<br/>            return np.array(x) * np.array(y)<br/>            <br/>        class NeuralNetwork:<br/>            def __init__(self,network):<br/>                self.weights = []<br/>                self.activations = []<br/>                for layer in network:<br/>                    input_size = layer[0]<br/>                    output_size = layer[1]<br/>                    activation = layer[2]<br/>                    index = network.index(layer)<br/>                    if layer[3] == 'RNN':<br/>                        increment = network[-1][1]<br/>                    else:<br/>                        increment = 0<br/>                    self.weights.append(np.random.randn(input_size+increment,output_size))<br/>                    self.activations.append(activation)</span></pre><p id="2135" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一部分，我初始化了LSTM类，并定义了其中的关键组件。两个门和神经网络组件的框架。</p><p id="d331" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第三步|神经网络功能:</strong></p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="edac" class="mn lw it nt b gy nx ny l nz oa">def propagate(self,data):<br/>                input_data = data<br/>                Zs = []<br/>                As = []<br/>                for i in range(len(self.weights)):<br/>                    z = np.dot(input_data,self.weights[i])<br/>                    if self.activations[i]:<br/>                        a = self.activations[i](z)<br/>                    else:<br/>                        a = z<br/>                    As.append(a)<br/>                    Zs.append(z)<br/>                    input_data = a<br/>                return As,Zs<br/>            <br/>            def network_train(self, As,Zs,learning_rate,input_data,extended_gradient):<br/>                As.insert(0,input_data)<br/>                g_wm = [0] * len(self.weights)<br/>                for z in range(len(g_wm)):<br/>                    a_1 = As[z].T<br/>                    pre_req = extended_gradient<br/>                    z_index = 0<br/>                    weight_index = 0for i in range(0,z*-1 + len(network)):<br/>                        if i % 2 == 0:<br/>                            z_index -= 1<br/>                            if self.activations[z]:<br/>                                pre_req = pre_req * deriv_func(Zs[z_index],self.activations[z])<br/>                            else:<br/>                                pre_req = pre_req * Zs[z_index]<br/>                        else:<br/>                            weight_index -= 1<br/>                            pre_req = np.dot(pre_req,self.weights[weight_index].T)<br/>                    a_1 = np.reshape(a_1,(a_1.shape[0],1))<br/>                    pre_req = np.reshape(pre_req,(pre_req.shape[0],1))<br/>                    pre_req = np.dot(a_1,pre_req.T)<br/>                    g_wm[z] = pre_req<br/>                for i in range(len(self.weights)):<br/>                    self.weights[i] += g_wm[i]*learning_rate</span></pre><p id="7691" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该部分包含网络的定义，以传播数据并计算每个相应权重的梯度，能够引入外部梯度以将权重直接链接到损失函数。</p><p id="638c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤4|创建组件:</strong></p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="5e46" class="mn lw it nt b gy nx ny l nz oa">self.plus_gate = plus_gate<br/>self.multiply_gate = multiply_gate<br/>self.recurrent_nn = NeuralNetwork(network)<br/>self.forget_nn = NeuralNetwork(network)<br/>self.select_nn = NeuralNetwork(network)</span></pre><p id="6e58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这个框架，我只需要创建LSTM运行所需的每个组件。</p><p id="682c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤5|定义结构:</strong></p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="8ae6" class="mn lw it nt b gy nx ny l nz oa">def cell_state(self,input_data,memo,select):<br/>        global rnn_As,rnn_Zs<br/>        rnn_As,rnn_Zs = lstm.recurrent_nn.propagate(input_data)<br/>        yhat_plus = tanh(rnn_As[-1])<br/>        plus = self.plus_gate(yhat_plus,memo)<br/>        collect_poss = plus<br/>        yhat_mult = tanh(plus)<br/>        mult = self.multiply_gate(yhat_mult,select)<br/>        pred = mult<br/>        return pred,collect_poss<br/>        <br/>def forget_gate(self,input_data,colposs):<br/>        global forget_As,forget_Zs<br/>        forget_As,forget_Zs = lstm.forget_nn.propagate(input_data)<br/>        yhat_mult = sigmoid(forget_As[-1])<br/>        mult = self.multiply_gate(colposs,yhat_mult)<br/>        memo = mult<br/>        return memodef select_gate(self,input_data):<br/>        global select_As,select_Zs<br/>        select_As,select_Zs = lstm.select_nn.propagate(input_data)<br/>        yhat_mult = sigmoid(select_As[-1])<br/>        select = yhat_mult<br/>        return select</span></pre><p id="4fe4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的部分中，我定义了网络的三个主要架构元素，添加了全局变量以便于在培训期间访问。</p><p id="e2f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤6|定义传播:</strong></p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="a3d3" class="mn lw it nt b gy nx ny l nz oa">def propagate(self,X,network):<br/>        colposs = 1<br/>        As = []<br/>        for i in range(len(X)):<br/>            input_data = X[i]<br/>            if i == 0:<br/>                increment = network[-1][1]<br/>                input_data = list(input_data) + [0 for _ in range(increment)]<br/>            else:<br/>                input_data = list(input_data) + list(pred)<br/>            input_data = np.array(input_data)<br/>            memory = self.forget_gate(input_data,colposs)<br/>            select = self.select_gate(input_data)<br/>            pred,colposs = self.cell_state(input_data,memory,select)<br/>            As.append(pred)<br/>        return As</span></pre><p id="b442" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，它就像浏览图表并按正确的顺序排列组件一样简单。比较图表和代码:这会让你更好地理解LSTMs是如何工作的。</p><p id="7dec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第七步|训练:</strong></p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="5158" class="mn lw it nt b gy nx ny l nz oa">def train(self,X,y,network,iterations,learning_rate):<br/>        colposs = 1<br/>        loss_record = []<br/>        for _ in range(iterations):<br/>            for i in range(len(X)):<br/>                input_data = X[i]<br/>                if i == 0:<br/>                    increment = network[-1][1]<br/>                    input_data = list(input_data) + [0 for _ in range(increment)]<br/>                else:<br/>                    input_data = list(input_data) + list(pred)<br/>                input_data = np.array(input_data)<br/>                memory = self.forget_gate(input_data,colposs)<br/>                select = self.select_gate(input_data)<br/>                pred,colposs = self.cell_state(input_data,memory,select)loss = sum(np.square(y[i]-pred).flatten())gloss_pred = (y[i]-pred)*2<br/>                gpred_gcolposs = selectgpred_select = colposs<br/>                gloss_select = gloss_pred * gpred_selectgpred_forget = select*sigmoid_p(colposs)*colposs<br/>                gloss_forget = gloss_pred * gpred_forgetgpred_rnn = select*sigmoid_p(colposs)<br/>                gloss_rnn = gloss_pred*gpred_rnnself.recurrent_nn.network_train(rnn_As,rnn_Zs,learning_rate,input_data,gloss_rnn)<br/>                self.forget_nn.network_train(forget_As,forget_Zs,learning_rate,input_data,gloss_forget)<br/>                self.select_nn.network_train(select_As,select_Zs,learning_rate,input_data,gloss_select)<br/>            As = self.propagate(X,network)<br/>            loss = sum(np.square(y[i]-pred))<br/>            loss_record.append(loss)<br/>        return loss_record</span></pre><p id="0d71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">继续前进！我已经检查了96%的代码。解释每一个导数要花很长时间，所以我简单解释一下。画一条从最终预测到你想要计算偏导数的项的路径，把所有垂直于路径的项加到方程中。逐元素相乘。使用这些作为扩展梯度来训练嵌套神经网络内的权重。</p><p id="72e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第八步|运行程序:</strong></p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="dd14" class="mn lw it nt b gy nx ny l nz oa">X = np.array([[0,1,1],[1,1,0],[1,0,1]])<br/>y = np.array([0,1,1])<br/>network = [[3,5,sigmoid,'RNN'],[5,5,sigmoid,'Dense'],[5,1,sigmoid,'Dense']]<br/>lstm = LSTM(network)<br/>loss_record = lstm.train(X,y,network,5000,0.1)<br/>plt.plot(loss_record)</span></pre><p id="0dd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该脚本的最后一段应该使用嵌套神经网络执行程序，以具有在网络变量下定义的架构。您可以在这里更改数字，但要确保矩阵是对齐的。</p><p id="f3ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">完整源代码:</strong></p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="de14" class="mn lw it nt b gy nx ny l nz oa">import numpy <br/>from matplotlib import pyplot as pltdef sigmoid(x):<br/>    return 1/(1+np.exp(-x))def sigmoid_p(x):<br/>    return sigmoid(x)*(1 -sigmoid(x))def relu(x):<br/>    return np.maximum(x, 0)def relu_p(x):<br/>    return np.heaviside(x, 0)def tanh(x):<br/>    return np.tanh(x)def tanh_p(x):<br/>    return 1.0 - np.tanh(x)**2def deriv_func(z,function):<br/>    if function == sigmoid:<br/>        return sigmoid_p(z)<br/>    elif function == relu:<br/>        return relu_p(z)<br/>    elif function == tanh:<br/>        return tanh_p(z)class LSTM:<br/>    def __init__(self,network):<br/>        <br/>        def plus_gate(x,y):<br/>            return np.array(x) + np.array(y)<br/>            <br/>        def multiply_gate(x,y):<br/>            return np.array(x) * np.array(y)<br/>            <br/>        class NeuralNetwork:<br/>            def __init__(self,network):<br/>                self.weights = []<br/>                self.activations = []<br/>                for layer in network:<br/>                    input_size = layer[0]<br/>                    output_size = layer[1]<br/>                    activation = layer[2]<br/>                    index = network.index(layer)<br/>                    if layer[3] == 'RNN':<br/>                        increment = network[-1][1]<br/>                    else:<br/>                        increment = 0<br/>                    self.weights.append(np.random.randn(input_size+increment,output_size))<br/>                    self.activations.append(activation)def propagate(self,data):<br/>                input_data = data<br/>                Zs = []<br/>                As = []<br/>                for i in range(len(self.weights)):<br/>                    z = np.dot(input_data,self.weights[i])<br/>                    if self.activations[i]:<br/>                        a = self.activations[i](z)<br/>                    else:<br/>                        a = z<br/>                    As.append(a)<br/>                    Zs.append(z)<br/>                    input_data = a<br/>                return As,Zs<br/>            <br/>            def network_train(self, As,Zs,learning_rate,input_data,extended_gradient):<br/>                As.insert(0,input_data)<br/>                g_wm = [0] * len(self.weights)<br/>                for z in range(len(g_wm)):<br/>                    a_1 = As[z].T<br/>                    pre_req = extended_gradient<br/>                    z_index = 0<br/>                    weight_index = 0for i in range(0,z*-1 + len(network)):<br/>                        if i % 2 == 0:<br/>                            z_index -= 1<br/>                            if self.activations[z]:<br/>                                pre_req = pre_req * deriv_func(Zs[z_index],self.activations[z])<br/>                            else:<br/>                                pre_req = pre_req * Zs[z_index]<br/>                        else:<br/>                            weight_index -= 1<br/>                            pre_req = np.dot(pre_req,self.weights[weight_index].T)<br/>                    a_1 = np.reshape(a_1,(a_1.shape[0],1))<br/>                    pre_req = np.reshape(pre_req,(pre_req.shape[0],1))<br/>                    pre_req = np.dot(a_1,pre_req.T)<br/>                    g_wm[z] = pre_req<br/>                for i in range(len(self.weights)):<br/>                    self.weights[i] += g_wm[i]*learning_rate<br/>        <br/>        self.plus_gate = plus_gate<br/>        self.multiply_gate = multiply_gate<br/>        self.recurrent_nn = NeuralNetwork(network)<br/>        self.forget_nn = NeuralNetwork(network)<br/>        self.select_nn = NeuralNetwork(network)<br/>        <br/>    def cell_state(self,input_data,memo,select):<br/>            global rnn_As,rnn_Zs<br/>            rnn_As,rnn_Zs = lstm.recurrent_nn.propagate(input_data)<br/>            yhat_plus = tanh(rnn_As[-1])<br/>            plus = self.plus_gate(yhat_plus,memo)<br/>            collect_poss = plus<br/>            yhat_mult = tanh(plus)<br/>            mult = self.multiply_gate(yhat_mult,select)<br/>            pred = mult<br/>            return pred,collect_poss<br/>        <br/>    def forget_gate(self,input_data,colposs):<br/>        global forget_As,forget_Zs<br/>        forget_As,forget_Zs = lstm.forget_nn.propagate(input_data)<br/>        yhat_mult = sigmoid(forget_As[-1])<br/>        mult = self.multiply_gate(colposs,yhat_mult)<br/>        memo = mult<br/>        return memodef select_gate(self,input_data):<br/>        global select_As,select_Zs<br/>        select_As,select_Zs = lstm.select_nn.propagate(input_data)<br/>        yhat_mult = sigmoid(select_As[-1])<br/>        select = yhat_mult<br/>        return select<br/>        <br/>    def propagate(self,X,network):<br/>        colposs = 1<br/>        As = []<br/>        for i in range(len(X)):<br/>            input_data = X[i]<br/>            if i == 0:<br/>                increment = network[-1][1]<br/>                input_data = list(input_data) + [0 for _ in range(increment)]<br/>            else:<br/>                input_data = list(input_data) + list(pred)<br/>            input_data = np.array(input_data)<br/>            memory = self.forget_gate(input_data,colposs)<br/>            select = self.select_gate(input_data)<br/>            pred,colposs = self.cell_state(input_data,memory,select)<br/>            As.append(pred)<br/>        return As<br/>    <br/>    def train(self,X,y,network,iterations,learning_rate):<br/>        colposs = 1<br/>        loss_record = []<br/>        for _ in range(iterations):<br/>            for i in range(len(X)):<br/>                input_data = X[i]<br/>                if i == 0:<br/>                    increment = network[-1][1]<br/>                    input_data = list(input_data) + [0 for _ in range(increment)]<br/>                else:<br/>                    input_data = list(input_data) + list(pred)<br/>                input_data = np.array(input_data)<br/>                memory = self.forget_gate(input_data,colposs)<br/>                select = self.select_gate(input_data)<br/>                pred,colposs = self.cell_state(input_data,memory,select)loss = sum(np.square(y[i]-pred).flatten())gloss_pred = (y[i]-pred)*2<br/>                gpred_gcolposs = selectgpred_select = colposs<br/>                gloss_select = gloss_pred * gpred_selectgpred_forget = select*sigmoid_p(colposs)*colposs<br/>                gloss_forget = gloss_pred * gpred_forgetgpred_rnn = select*sigmoid_p(colposs)<br/>                gloss_rnn = gloss_pred*gpred_rnnself.recurrent_nn.network_train(rnn_As,rnn_Zs,learning_rate,input_data,gloss_rnn)<br/>                self.forget_nn.network_train(forget_As,forget_Zs,learning_rate,input_data,gloss_forget)<br/>                self.select_nn.network_train(select_As,select_Zs,learning_rate,input_data,gloss_select)<br/>            As = self.propagate(X,network)<br/>            loss = sum(np.square(y[i]-pred))<br/>            loss_record.append(loss)<br/>        return loss_record   <br/>    <br/>X = np.array([[0,1,1],[1,1,0],[1,0,1]])<br/>y = np.array([0,1,1])<br/>network = [[3,5,sigmoid,'RNN'],[5,5,sigmoid,'Dense'],[5,1,sigmoid,'Dense']]<br/>lstm = LSTM(network)<br/>loss_record = lstm.train(X,y,network,5000,0.1)<br/>plt.plot(loss_record)</span></pre><h1 id="b785" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">我的链接:</h1><p id="a665" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">如果你想看更多我的内容，点击这个<a class="ae ky" href="https://linktr.ee/victorsi" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">链接</strong> </a>。</p></div></div>    
</body>
</html>