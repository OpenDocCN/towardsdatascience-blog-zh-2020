<html>
<head>
<title>Working with NLP datasets in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Python中使用NLP数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/working-with-nlp-datasets-in-python-7030a179e9fa?source=collection_archive---------35-----------------------#2020-10-19">https://towardsdatascience.com/working-with-nlp-datasets-in-python-7030a179e9fa?source=collection_archive---------35-----------------------#2020-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7d2c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">教程:将新的HuggingFace数据集库与TensorFlow数据集库和其他选项进行比较</h2></div><p id="26ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在深度学习领域，数据集是每个项目必不可少的一部分。为了训练一个可以处理新情况的神经网络，人们必须使用一个代表世界即将到来的场景的数据集。在动物图像上训练的图像分类模型在汽车分类任务上表现不好。</p><p id="6f16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了训练最佳模型，研究人员还使用公共数据集作为模型性能的基准。我个人认为易于使用的公共基准是帮助促进研究过程的最有用的工具之一。这方面的一个很好的例子是代码为的<a class="ae lb" href="http://paperswithcode.com" rel="noopener ugc nofollow" target="_blank">论文和最先进的图表。</a></p><p id="d8c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一个很棒的工具是现成的数据集库。在这篇文章中，我将以IMBD情感分析数据集为例，回顾新的<a class="ae lb" href="https://huggingface.co/docs/datasets/index.html" rel="noopener ugc nofollow" target="_blank"> HuggingFace数据集</a>库，并使用Keras biLSTM网络将其与<a class="ae lb" href="https://www.tensorflow.org/datasets/catalog/overview?hl=en" rel="noopener ugc nofollow" target="_blank"> TensorFlow数据集</a>库进行比较。这个故事也可以作为使用这些库的教程。</p><p id="2ec5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有代码都可以在Google Colab上找到。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/a641dc3a85cc0eecee3221dc45d2a301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNBcU5LF-cX3enRrrCU2jw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><a class="ae lb" href="https://paperswithcode.com/sota/sentiment-analysis-on-imdb" rel="noopener ugc nofollow" target="_blank"> IMDb纸上情绪分析图代码</a></p></figure><h1 id="4489" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">原始数据集出版物</h1><p id="d313" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">当有人发布一个新的数据集库时，最直接的做法就是在研究团队的网页中进行分享。例如，IMDB情感分析数据集由斯坦福大学的研究人员团队发布，并可在他们自己的网页上获得:<a class="ae lb" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">大型电影评论数据集</a>。在科学出版物的情况下，它通常带有一篇发表的文章:例如，见Maas等人[1]。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mp"><img src="../Images/3fcdfe28471911adf4dfefc7b51b7bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MYm6cPEp4UQUVl_j823WQg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><a class="ae lb" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">IMDB情感数据集的原始发布页面</a></p></figure><p id="26f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我认为，这有两个主要问题:1)很难找到，尤其是如果你是一个早期携带者科学家；2)没有存储数据的标准化格式，使用新的数据集必须有特定的预处理步骤。</p><h1 id="14e9" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">在线数据集集合</h1><p id="0e96" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">要使数据集可访问，不仅要使它可用，还要确保用户能找到它。谷歌意识到了它的重要性，他们在datasetsearch.research.google.com为数据集开发了一个搜索平台。然而，搜索IMDB大型电影评论情感数据集<a class="ae lb" href="https://datasetsearch.research.google.com/search?query=IMDB%20Large%20Movie%20Reviews%20Sentiment%20Dataset&amp;docid=exXzLiK1WI6YWyHzAAAAAA%3D%3D" rel="noopener ugc nofollow" target="_blank">，结果</a>不包括该研究的原始网页。浏览谷歌数据集搜索结果，你会发现<a class="ae lb" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>是最大的在线公共数据集集合之一。</p><h2 id="fe27" class="mq lt iq bd lu mr ms dn ly mt mu dp mc ko mv mw me ks mx my mg kw mz na mi nb bi translated">卡格尔</h2><p id="01e4" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">Kaggle是世界上最大的在线机器学习社区，拥有各种竞赛任务、数据集集合和讨论主题。如果你从未听说过Kaggle，但对深度学习感兴趣，我强烈建议你去看一看。在Kaggle中，任何人都可以上传新的数据集(限制为10GB )，社区可以根据数据集的文档、机器可读性和代码示例的存在对数据集进行评级。</p><p id="ba48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Kaggle 上的<a class="ae lb" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank"> IMDB情感数据集有8.2分和164个公共笔记本示例来开始使用它。用户可以阅读数据集的文档，并在下载之前进行预览。</a></p><p id="e3eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">值得注意的是，该数据集不包括数据的原始分割。这无助于模型的再现性，除非构建者描述它们的分裂函数。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nc"><img src="../Images/7e644bb7e97c4db0e859a5a5bb08dde4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yxGYo1FJIbvCr5B__EwHoA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><a class="ae lb" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank">ka ggle上的IMDB数据集</a></p></figure><p id="ba43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用Kaggle数据集时，最重要的预防措施是1)确保您使用准确的数据集，因为许多用户共享数据集的更改/改进版本，2)确保您拥有使用它的许可证，并且正确的人对此负责。Kaggle上的许多数据集不是由原始创建者共享的。</p><h1 id="2c31" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据集库</h1><p id="7773" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">虽然数据集集合的主要原因是将所有数据集存储在一个地方，但数据集库侧重于随时可用的可访问性和性能。</p><p id="3917" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">机器学习库通常带有一些数据集示例。<a class="ae lb" href="https://scikit-learn.org/stable/datasets/index.html" rel="noopener ugc nofollow" target="_blank">这里的</a>是Scikit-learn数据集的列表。我选择IMDB数据集，因为这是Keras 中包含的唯一文本数据集。</p><p id="0e5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TensorFlow团队专门为数据集开发了一个包。它包括几个数据集，并与TensorFlow和Keras神经网络兼容。</p><p id="c366" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下文中，我将比较TensorFlow数据集库和新的<a class="ae lb" href="https://huggingface.co/docs/datasets/index.html" rel="noopener ugc nofollow" target="_blank"> HuggingFace数据集</a>库，重点关注NLP问题。</p><h2 id="6334" class="mq lt iq bd lu mr ms dn ly mt mu dp mc ko mv mw me ks mx my mg kw mz na mi nb bi translated">公共数据集</h2><p id="653e" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">目前，TensorFlow数据集列出了来自机器学习各个领域的155个条目，而HuggingFace数据集包含165个专注于自然语言处理的条目。以下是共享相同名称的数据集列表(39):</p><p id="2734" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nd ne nf ng b">tfds_list = tfds.list_builders(); hfds_list = datasets.list_datasets(); list(set(tfds_list).intersection(set(hfds_list)))</code></p><p id="44ce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nd ne nf ng b">[‘xnli’, ‘multi_news’, ‘multi_nli_mismatch’, ‘wikihow’, ‘squad’, ‘xsum’, ‘super_glue’, ‘cos_e’, ‘newsroom’, ‘lm1b’, ‘eraser_multi_rc’, ‘aeslc’, ‘civil_comments’, ‘gap’, ‘cfq’, ‘gigaword’, ‘esnli’, ‘multi_nli’, ‘scan’, ‘librispeech_lm’, ‘opinosis’, ‘snli’, ‘reddit_tifu’, ‘wikipedia’, ‘scicite’, ‘tiny_shakespeare’, ‘scientific_papers’, ‘qa4mre’, ‘c4’, ‘definite_pronoun_resolution’, ‘flores’, ‘math_dataset’, ‘trivia_qa’, ‘para_crawl’, ‘movie_rationales’, ‘natural_questions’, ‘billsum’, ‘cnn_dailymail’, ‘glue’]</code></p><p id="ac55" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，IMDB数据集不在列表中！在TensorFlow数据集中，它被命名为<code class="fe nd ne nf ng b">imdb_reviews</code>，而HuggingFace数据集将其称为<code class="fe nd ne nf ng b">imdb</code>数据集。我认为这是非常不幸的，图书馆的建设者应该努力保持相同的名字。</p><h2 id="d816" class="mq lt iq bd lu mr ms dn ly mt mu dp mc ko mv mw me ks mx my mg kw mz na mi nb bi translated">数据集描述</h2><p id="193f" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">HuggingFace数据集有一个<a class="ae lb" href="https://huggingface.co/nlp/viewer/?dataset=imdb" rel="noopener ugc nofollow" target="_blank">数据集查看器站点</a>，在此呈现数据集的样本。该网站显示了数据的分割，原始网站的链接，引用和例子。除此之外，他们还有另一个<a class="ae lb" href="https://huggingface.co/datasets/imdb" rel="noopener ugc nofollow" target="_blank">数据集描述站点</a>，其中显示了导入使用和相关模型。</p><p id="fcb0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TensorFlow数据集有一个单独的<a class="ae lb" href="https://www.tensorflow.org/datasets/catalog/imdb_reviews" rel="noopener ugc nofollow" target="_blank">数据集描述站点</a>，在这里可以获得前面提到的元数据信息。与HugginFace网站相比，TensorFlow提供了多种下载选项:纯文本和编码数字单词令牌。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nh"><img src="../Images/6ef6e43028bfcd2a8844f1e7c399e5d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sI3nJEfg0r7rVR2kG4COyg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">HuggingFace(上图)和TensorFlow(下图)IMDb数据集描述页面</p></figure><h1 id="d86d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">情感分析的Keras示例</h1><p id="161d" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在IMDB数据集的<a class="ae lb" href="https://keras.io/api/datasets/imdb/" rel="noopener ugc nofollow" target="_blank"> Keras版本</a>中，纯文本已经过预处理。我使用相同的处理步骤来说明其他库的用例。步骤如下:</p><ol class=""><li id="b913" class="ni nj iq kh b ki kj kl km ko nk ks nl kw nm la nn no np nq bi translated">加载数据集</li><li id="0d5e" class="ni nj iq kh b ki nr kl ns ko nt ks nu kw nv la nn no np nq bi translated">将纯文本标记化并编码</li><li id="ec22" class="ni nj iq kh b ki nr kl ns ko nt ks nu kw nv la nn no np nq bi translated">截断长示例</li><li id="4675" class="ni nj iq kh b ki nr kl ns ko nt ks nu kw nv la nn no np nq bi translated">Pad简短示例</li><li id="9e8a" class="ni nj iq kh b ki nr kl ns ko nt ks nu kw nv la nn no np nq bi translated">洗牌，批量处理数据</li><li id="e0bf" class="ni nj iq kh b ki nr kl ns ko nt ks nu kw nv la nn no np nq bi translated">为每个数据集拟合相同的模型</li></ol><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">最初由fchollet 构建Keras模型<a class="ae lb" href="https://keras.io/examples/nlp/bidirectional_lstm_imdb/" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="fa7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Keras网络将期望200个令牌长整数向量，词汇表为[0，20000]。词汇表中的单词基于数据集的词频。该网络由用于输入读取的<code class="fe nd ne nf ng b">Input</code>层、用于将单词表示从整数投影到128维向量空间的<code class="fe nd ne nf ng b">Embedding</code>层、两个双向<code class="fe nd ne nf ng b">LSTM</code>层和用于匹配输出维度的<code class="fe nd ne nf ng b">Dense</code>层组成。</p><p id="73d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于数据已经过预处理，最初的步骤只有几行:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">最初由fchollet 预处理Keras数据<a class="ae lb" href="https://keras.io/examples/nlp/bidirectional_lstm_imdb/" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="7a2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和训练线:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">使用最初由fchollet 提供的Keras数据<a class="ae lb" href="https://keras.io/examples/nlp/bidirectional_lstm_imdb/" rel="noopener ugc nofollow" target="_blank">训练模型</a></p></figure><p id="f902" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在Colab计算机上训练的历元在CPU上是450秒长，在GPU上是55秒长，在测试数据上的最终精度是0.8447。</p><h1 id="0fac" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">用于情感分析的TensorFlow数据集</h1><p id="67d8" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">TensorFlow提供了非常好的教程，它们很详细，但阅读起来篇幅相对较短。如果你想了解更多，从这里开始。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">从tensorflow_datasets加载IMDB数据集</p></figure><p id="71a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个参数按名称指定数据集。接下来，<code class="fe nd ne nf ng b">split</code>参数告诉库应该包含哪些数据分割。也可以是分成的百分比:<code class="fe nd ne nf ng b">train[:10%]</code>。<code class="fe nd ne nf ng b">as_supervised</code>参数指定了格式，它允许Keras模型从TensorFlow数据集进行训练。<code class="fe nd ne nf ng b">with_info</code>向列表中添加一个新的返回值，它包含来自数据的各种信息。我最喜欢的是显示原始数据构建团队的信用的引文。</p><p id="dab3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我认为TensorFlow数据集库最强大的工具是，您不必一次加载全部数据，而只需分批加载。不幸的是，为了建立基于词频的词汇表，我们必须在训练前加载数据。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">张量流数据集的符号化和截断</p></figure><p id="5b4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">记号赋予器的构建基于<a class="ae lb" href="https://www.tensorflow.org/tutorials/load_data/text" rel="noopener ugc nofollow" target="_blank">本教程</a>。但是，我添加了一个<code class="fe nd ne nf ng b">Counter</code>来统计这些词在训练数据集中的出现频率。代码首先用空格分隔句子。然后，<code class="fe nd ne nf ng b">Counter</code>统计词频。为了匹配Keras模型的词汇大小，计数器只保留顶部的<code class="fe nd ne nf ng b">max_features-2</code>。额外的两个标记是填充标记(<code class="fe nd ne nf ng b">0</code>)和不在词汇表中的标记(OOV)，用于不包括在最常用列表中的单词(<code class="fe nd ne nf ng b">max_features-1</code>)。接下来的几行代码构建了一个编码器，以确保词汇表中的每个单词都有唯一的整数值。<code class="fe nd ne nf ng b">encode_map_fn</code>函数将编码器包装在TensorFlow函数中，以便数据集对象可以使用它。这段代码还包含了<code class="fe nd ne nf ng b">truncate </code>函数:它通过截掉句子的结尾来确保句子的长度不会太长。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">TensorFlow数据集管道</p></figure><p id="9e04" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代码的下一部分为处理步骤构建管道。首先，它读取<code class="fe nd ne nf ng b">(sentence,label)</code>对，然后将其编码为<code class="fe nd ne nf ng b">(integer_list,label)</code>对，并截断长句(列表)。如果数据可以存储在内存中，那么<code class="fe nd ne nf ng b">cache</code>就可以加快工作速度。<code class="fe nd ne nf ng b">shuffle</code>部分只对训练数据是必要的:<code class="fe nd ne nf ng b">buffer_size=1024</code>参数指定程序在数据的较小窗口上随机化，而不是一次全部随机化。如果数据太大而无法放入存储器，这是有用的，但是，只有当此<code class="fe nd ne nf ng b">buffer_size</code>大于数据样本数时，才能实现真正的随机。流水线的<code class="fe nd ne nf ng b">padded_batch</code>步骤将数据分成32个一组，并将较短的句子填充到200个标记中。在这个步骤之后，输入形状是<code class="fe nd ne nf ng b">(32,200)</code>，输出是<code class="fe nd ne nf ng b">(32,1)</code>。最后，<code class="fe nd ne nf ng b">prefetch</code>步骤与多处理一起工作:当模型在一个批次上训练时，算法在下一个批次中加载，因此当模型完成前一个批次时，它们将准备好。</p><p id="e6b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，数据已准备好训练模型:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="f7e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个历元在GPU上较慢(82s)，但在将处理后的数据加载到缓存后，历元持续时间与Keras(55s)相似。测试数据的最终精度为0.8014。如果您运行代码，您可以看到Keras fit verbose无法猜测第一个epoch的持续时间:管道在不知道其最终长度的情况下读取数据！</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/356a311be6ad24f67d02d647abc5c548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*4BtCyiZ7k7hIS2l68vxLbw.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">一次读取一批数据(在CPU上)，而不知道数据的结尾</p></figure><h1 id="3ec7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">基于拥抱人脸数据集的情感分析</h1><p id="5b01" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">首先，我想谈谈HugginFace软件包的名称:该公司有3个pip软件包，<code class="fe nd ne nf ng b">transformers</code>、<code class="fe nd ne nf ng b">tokenizers </code>和<code class="fe nd ne nf ng b">datasets</code>。虽然我理解保护这些短名称的PR值，并且可能transformers和tokenizers是它们中的第一个，但我不喜欢它们的名称，因为它可能会令人困惑。例如，如果我使用TensorFlow数据集库和HuggingFace数据集库，如果一个人的名字是Datasets，我无法确定哪个是它。我想如果Tensorflow用名字命名他们的库(像<code class="fe nd ne nf ng b">tensorflow_datasets</code>)，如果HuggingFace这样做就好了。</p><p id="9a90" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其次，与<code class="fe nd ne nf ng b">tokenizers </code>和<code class="fe nd ne nf ng b">datasets</code>一起工作，我不得不注意到，虽然<code class="fe nd ne nf ng b">transformers </code>和<code class="fe nd ne nf ng b">datasets </code>有很好的文档，但<code class="fe nd ne nf ng b">tokenizers </code>图书馆缺乏。此外，在按照文档构建这个示例的过程中，我遇到了一个问题——这个问题在6月份被报告给了他们。</p><p id="09e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">HuggingFace数据集可以构建一个类似于TensorFlow的管道来处理大数据。在这个实验中，我没有使用它，如果你需要，请阅读教程！</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">HuggingFace数据集读取IMDB数据集</p></figure><p id="36ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据加载的工作方式与前一个类似。HuggingFace库可以处理百分比和张量流。Dataset对象将数据信息作为引用信息等属性。</p><h2 id="ffdb" class="mq lt iq bd lu mr ms dn ly mt mu dp mc ko mv mw me ks mx my mg kw mz na mi nb bi translated">标记化者</h2><p id="6188" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">HuggingFace库的一个强大功能来了:由于该公司专注于自然语言处理，它比TensorFlow有更多更适合该领域的功能。如果我们想要使用一个特定的transformer模型，我们可以从相应的包中导入它的tokenizer。或者我们可以从头开始训练一个新的。我在以前的一篇文章中谈到了记号赋予者之间的区别，<a class="ae lb" rel="noopener" target="_blank" href="/comparing-transformer-tokenizers-686307856955">如果你感兴趣，可以在这里</a>阅读。</p><p id="a818" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个实验中，我基于训练数据构建了一个单词块[2]标记器。这就是著名的BERT模型[3]使用的记号化器。此外，我还展示了如何使用前一部分的词汇表作为标记器的数据来实现相同的功能。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">使用训练数据构建单词块[2]—<a class="ae lb" href="https://github.com/huggingface/tokenizers/tree/91f602f744f7fee72f2f5fa8d6c7c48bb1d72d3b/bindings/python" rel="noopener ugc nofollow" target="_blank">基于此通过HuggingFace </a></p></figure><p id="385e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个代码示例展示了如何基于标记器实现构建一个单词块。不幸的是，训练器只处理文件，因此我不得不临时保存IMDB数据集的纯文本。词汇表的大小可通过<code class="fe nd ne nf ng b">train</code>功能定制。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">构建频率列表标记器</p></figure><p id="b624" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了用最常用的单词构建标记器，应该更新词汇表。唯一的诀窍是跟踪特殊的令牌。要查看tokenizer内部的数据，一种可能的方法是将其保存到一个JSON文件中:它是可读的，并且包含所有需要的信息。</p><p id="d9a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果使用句子<em class="nz">“这是一个藏身之处”，区别就很明显了</em>。第一个版本分割<em class="nz">藏身处</em>并识别“.”字符，但第二种方法将整个单词作为标记，但不包括标点字符。缺省情况下，记号赋予器使这个数据小写，我在以前的版本中没有使用这个步骤。</p><p id="c706" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nd ne nf ng b">WordPiece: [‘this’, ‘is’, ‘a’, ‘hide’, ‘##out’, ‘.’]</code></p><p id="9a21" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nd ne nf ng b">From Vocab: [‘this’, ‘is’, ‘a’, ‘hideout’, ‘[UNK]’]</code></p><p id="44b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了确保我没有使用相同的令牌，我在两次运行之间调用了<code class="fe nd ne nf ng b"><a class="ae lb" href="https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.cleanup_cache_files" rel="noopener ugc nofollow" target="_blank">datasets.Dataset.cleanup_cache_files()</a></code> <strong class="kh ir"> </strong>。</p><h2 id="9ced" class="mq lt iq bd lu mr ms dn ly mt mu dp mc ko mv mw me ks mx my mg kw mz na mi nb bi translated">为Keras训练格式化为张量流数据集</h2><p id="2acb" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">实现数据集教程中的示例，我们可以将数据加载到TensorFlow数据集格式，并用它训练Keras模型。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">将人脸数据集绑定到张量流数据集— <a class="ae lb" href="https://huggingface.co/docs/datasets/quicktour.html" rel="noopener ugc nofollow" target="_blank">基于本教程</a></p></figure><p id="3454" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这段代码类似于HuggingFace教程中的代码。唯一的区别在于使用了不同的记号赋予器。本教程使用来自<code class="fe nd ne nf ng b">transformers</code>库的BERT模型的标记器，而我使用来自<code class="fe nd ne nf ng b">tokenizers</code>库的<code class="fe nd ne nf ng b">BertWordPieceTokenizer</code>。不幸的是，来自同一家公司不同库中的这两个逻辑上相似的类并不完全兼容。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nw nx l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">HuggingFace数据的管道和培训</p></figure><p id="fcb5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一步几乎与使用张量流数据的步骤相同。唯一不同的是<code class="fe nd ne nf ng b">shuffle</code>缓冲器。HuggingFace数据集的IMDB数据库中的样本按标签排序。在这种情况下，这不是一个问题，但它禁用了TensorFlow的功能，这些功能允许一次只加载部分数据。如果我们只用一个小窗口混洗这些数据，在几乎所有的情况下这个窗口只包含一个标签值。希望其他数据集不会出现这种情况。</p><p id="c886" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">两个模型的最终精度相似:0.8241和0.8224。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oa"><img src="../Images/9ee10d34aec537faf5a52cc8a4455ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NyT2xZPFl98ymTOZjLQE9A.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">托比亚斯·菲舍尔在<a class="ae lb" href="https://unsplash.com/s/photos/database?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="5e35" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">摘要</h1><p id="8e13" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在这个故事中，我展示了TensorFlow和HuggingFace的数据集库的使用。我谈到了为什么我认为建立数据集集合对研究领域很重要。总的来说，我认为专注于NLP问题的HuggingFace将是该领域的一个伟大推动者。该库已经拥有比TensorFlow更多的NLP数据集。我认为他们与TensorFlow(以及PyTorch)紧密合作以确保两个库的每个特性都能被恰当地利用是很重要的。</p><h1 id="d569" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="79de" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">[1]安德鲁·马斯、雷蒙德·戴利、彼得·范、黄丹、安德鲁·吴和克里斯托弗·波茨。(2011).<a class="ae lb" href="http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf" rel="noopener ugc nofollow" target="_blank">学习用于情感分析的词向量。</a> <em class="nz">计算语言学协会第49届年会(ACL 2011)。</em></p><p id="8121" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]吴，m .舒斯特，陈，z .乐，Q. V .，m .马切里，w .，… &amp;克林纳，J. (2016)。<a class="ae lb" href="https://arxiv.org/abs/1609.08144" rel="noopener ugc nofollow" target="_blank">谷歌的神经机器翻译系统:弥合人类和机器翻译之间的鸿沟。</a>T6】arXiv预印本arXiv:1609.08144 。</p><p id="6787" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] Devlin，j .，Chang，M. W .，Lee，k .，&amp; Toutanova，K. (2018年)。<a class="ae lb" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> Bert:用于语言理解的深度双向转换器的预训练。</a> <em class="nz"> arXiv预印本arXiv:1810.04805 </em>。</p></div></div>    
</body>
</html>