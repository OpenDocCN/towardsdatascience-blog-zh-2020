<html>
<head>
<title>Code a Deep Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编写一个深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/code-a-deep-neural-network-a5fd26ec41c4?source=collection_archive---------28-----------------------#2020-11-06">https://towardsdatascience.com/code-a-deep-neural-network-a5fd26ec41c4?source=collection_archive---------28-----------------------#2020-11-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="57de" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="43dc" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用Python构建深度神经网络的实践操作</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1dece46bf7fe9343f91b5b06d9f05324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YkvbcWSrDAFURBff"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@the_roaming_platypus?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> timJ </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="a6f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di">在</span>上一篇<a class="ae lh" rel="noopener" target="_blank" href="/building-a-shallow-neural-network-a4e2728441e0">帖子</a>中，我们用python构建了一个具有基本功能的单隐层神经网络。为了推广和增强我们的网络，在这篇文章中，我们将建立一个n层神经网络来执行二进制分类任务，其中n是可定制的(建议重温我上次对神经网络的介绍，因为这里不会重复理论的基础)。</p><blockquote class="mn mo mp"><p id="1884" class="li lj mq lk b ll lm kd ln lo lp kg lq mr ls lt lu ms lw lx ly mt ma mb mc md im bi translated">所有图片都是我自己创作的，引用的图片都是添加的。</p></blockquote><h1 id="6bf8" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">权重初始化</h1><p id="35df" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">首先，需要为不同的层初始化权重。请注意，通常情况下，输入不被视为图层，但输出被视为图层。</p><p id="89d9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">(详细的培训和测试流程，请点击<a class="ae lh" href="https://github.com/MJeremy2017/deep-learning/tree/main/deep-neural-network" rel="noopener ugc nofollow" target="_blank">这里</a></p><p id="902e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们了解到，对于<code class="fe nr ns nt nu b">lth</code>层:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4f088d5fe91036e243942515ae7a3f3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*MeFRNSaK-rdojCjGceHxzA.png"/></div></figure><p id="1100" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中n^[0]等于数字输入特征。</p><p id="e88e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">考虑下面的神经网络:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/ce89b601902b10d5e8b55f942d740459.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NZfo1oKBPVsxAKcX2GOtuQ.png"/></div></div></figure><p id="3242" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这种情况下，第一个W^[1会有形状<code class="fe nr ns nt nu b">(4, 2)</code>，第二个W^[2会有形状<code class="fe nr ns nt nu b">(1, 4)</code>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="81d3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果输入要求是一个列表，对于上面的情况，输入层应该是</p><pre class="ks kt ku kv gt nz nu oa ob aw oc bi"><span id="fb83" class="od mv it nu b gy oe of l og oh">[2, 4, 1]</span></pre><p id="cd8c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">并且我们的初始化权重需要足够小，以便在反向传播过程中梯度会很大，并且学习会更快。</p><h1 id="fd58" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">正向传播</h1><p id="120e" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">转发过程将是直截了当的，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/b81f6ee12b3fa4e79d4804803b1771de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*SfXXKWqfLTUvU-ofOjwghw.png"/></div></figure><p id="27cb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<code class="fe nr ns nt nu b">l</code>是<code class="fe nr ns nt nu b">lth</code>层，而<code class="fe nr ns nt nu b">g(x)</code>是激活函数。这里我们将使用2个不同激活函数:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="0c5f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以上所有函数都适用于矩阵。</p><p id="0085" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正向传播将遵循上面的等式。注意，在我们的实现中，除了最后一层我们使用<code class="fe nr ns nt nu b">sigmoid</code>激活，其余的我们使用<code class="fe nr ns nt nu b">relu</code>激活函数。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h1 id="5c65" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">价值函数</h1><p id="ad2d" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">我们仍然认为这是一个二元分类，一批的成本是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/9ba1ce646363daab216674c6d63a3db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4QFGw11H2pRW1FHKSMduiQ.png"/></div></div></figure><p id="3bd8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<code class="fe nr ns nt nu b">a</code>为预测值，<code class="fe nr ns nt nu b">y</code>为实际值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h1 id="63a8" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">反向传播</h1><p id="b22f" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">既然我们的正向过程已经完成，为了让我们的模型通过迭代得到改进，让我们来看看反向传播的关键。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/a292ae637399cfe7a6fd8e4805e3e5fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-9Zq67cZh6HNSEGJR992Sg.png"/></div></div></figure><p id="5741" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[ <strong class="lk jd">来源</strong>:https://github.com/enggen/Deep-Learning-Coursera]</p><p id="6a74" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">可以以递归方式计算反向梯度:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/78e3a8b7a13bec7744f63bb97933a718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qio8YDtPDmZqJ52m_nh5vA.png"/></div></div></figure><p id="fd50" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，需要实现<code class="fe nr ns nt nu b">sigmoid</code>和<code class="fe nr ns nt nu b">relu</code>的导数。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="51d2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">出于对称的原因，这里两个函数具有相同的输入，尽管这不是必需的。</p><p id="d889" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据上面的等式，我们实现了反向传播。注意，除了最后一层使用了<code class="fe nr ns nt nu b">sigmoid</code>函数，其余的我们都应用<code class="fe nr ns nt nu b">relu</code>导数来得到梯度。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="3b8a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在给定梯度，我们的权重更新如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/2e82ac7779c87bb78dcf6bb921a08da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*ZuEixKoDkLrYXJE6v3hlhA.png"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h1 id="0d36" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">应用于数据集</h1><p id="857e" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">让我们将我们的模型应用于创建的包含200个要素的数据集。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="3349" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在让我们训练我们的模型。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="afef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里我们有一个有200个输入特征的3层神经网络。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/028723540ec0dd19154ac62a895a2894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AjrGWfHoj_WXXT6dzdubhA.png"/></div></div></figure><p id="c18c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要了解详细的训练过程，请查看我的<a class="ae lh" href="https://github.com/MJeremy2017/deep-learning/tree/main/deep-neural-network" rel="noopener ugc nofollow" target="_blank"> github </a>。</p></div></div>    
</body>
</html>