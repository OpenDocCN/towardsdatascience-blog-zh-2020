<html>
<head>
<title>Why VAE are likelihood-based generative models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么VAE是基于可能性的生成模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-vae-are-likelihood-based-generative-models-2670dd81a40?source=collection_archive---------12-----------------------#2020-10-25">https://towardsdatascience.com/why-vae-are-likelihood-based-generative-models-2670dd81a40?source=collection_archive---------12-----------------------#2020-10-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ed9a1a92496eb22b09f0a94ba6ea037d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lBXgASr1SBP7XV9N"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">罗马法师在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="d644" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">生成模型是学习从现有数据生成真实数据样本的强大工具</p><p id="9ba3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们必须决定使用什么具体方法来实现生成模型时，我们必须处理泛化与可解释性的权衡:简单的方法是可解释的，但它们的泛化能力明显低于像(大)神经网络这样的方法，神经网络已经显示出惊人的能力，但它们被认为是黑盒，因为我们仍然缺少一个合理的理论。</p><p id="d8d6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，从更抽象的角度来看这个领域，我们有一个很好的理论，它可以帮助我们理解一些重要的特征，并提供重要的见解，当我们必须监控这种模型的训练时，这将是有用的</p><p id="23f1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将在这里集中讨论这个问题</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="8e19" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">最大似然估计</h1><p id="ac27" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">最大似然估计是参数估计的基本工具</p><p id="202d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从贝叶斯定理</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/d1ee9588634c92d5a0496a4e874188ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/0*o1auQBmC0E8imEDQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">贝叶斯定理，来自https://www.saedsayad.com/naive_bayesian.htm<a class="ae kc" href="https://www.saedsayad.com/naive_bayesian.htm" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="c51d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们看到可能性被标识为P(X|C ),但是它是什么意思呢？</p><p id="f114" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">严格来说，它代表一个条件概率，因此给定C为真，X的概率是多少</p><p id="5e27" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是当C是一个分布时会发生什么呢？</p><p id="34cb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">P(X|C)仍然是概率，因此它的输出在[0，1]中吗？</p><p id="234b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个问题的答案是否定的:P(X|C)的输出恰好是非负的(事实上，贝叶斯公式中的所有其他项都是非负的，并且后验概率一定是非负的，因为它是一个概率),但它不是一个概率，因此它可以大于1</p><p id="d14d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么这个值如何转化为后验概率呢？</p><p id="20ee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是贝叶斯公式中其他两项的工作:先验和证据(分母),虽然前者来自假设(因此在计算方面不存在问题),但后者需要计算，这实际上通常非常困难，因为它涉及非常大空间上的棘手积分。</p><p id="1f10" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">无论如何，如果我们专注于可能性，我们就不必担心处理棘手的积分</p><p id="af51" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想法是使用似然性作为两个分布X和C之间的相似性度量，并在其上定义一个优化问题</p><p id="fc94" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是请注意，可能性不是距离，因为它不是对称的:所以P(X|C)不是P(C|X)</p><p id="5ae2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要理解什么样的顺序对你有效，只要想想你想要什么样的后验概率:可能性的顺序是相反的。</p><p id="2a8d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是哪两个发行版？</p><p id="a83d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为我们的目标是在数据集D上训练生成模型G(X ),那么我们将有</p><p id="a09f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">- D:训练集中数据的分布</p><p id="dc50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">- G(X):生成数据的分布</p><p id="a26e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着将训练转换为以最大化P(D | G(X))为目标的优化问题</p><p id="cc5c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实际上，我们将使用本质上是参数函数的神经网络来实现G(X ),我们称之为G(X；因此，问题的最终目标是找到最大化数据集和生成数据之间的似然性的θ值</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/a32270c28ec1d225c34c5de25fb8a0a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*IusskVYd3aVfHb3G7mRsEg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">生成模型学习的最大似然法</p></figure><h1 id="b130" class="li lj iq bd lk ll mr ln lo lp ms lr ls lt mt lv lw lx mu lz ma mb mv md me mf bi translated">生成模型</h1><p id="a8fd" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">因此，虽然现在应该清楚我们的生成模型是参数化的，以及我们使用什么框架来估计它的参数，但我们还没有定义什么是X</p><p id="e6b0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个很好的观点，因为生成模型本身不需要额外的输入X，它应该像一个黑盒</p><ul class=""><li id="2f72" class="mw mx iq kf b kg kh kk kl ko my ks mz kw na la nb nc nd ne bi translated">按下按钮，我们就可以从中获取样本</li><li id="f512" class="mw mx iq kf b kg nf kk ng ko nh ks ni kw nj la nb nc nd ne bi translated">并允许我们改变一些设置，因为它们通过微分与结果有某种联系</li></ul><p id="76c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是实际上，如果我们使用一个神经网络，那么它就是一个完全确定性的架构:它没有任何新颖性，给定相同的输入和相同的参数化，那么它总是返回相同的输出</p><p id="749d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么我们如何增加新鲜感呢？</p><p id="cffa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此我们需要一个额外的输入</p><p id="306b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们终于能够将它与标题联系起来，并开始处理最初的问题，所以让我们介绍一下变分自动编码器</p><h1 id="16e7" class="li lj iq bd lk ll mr ln lo lp ms lr ls lt mt lv lw lx mu lz ma mb mv md me mf bi translated">VAE</h1><p id="b625" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">关于VAE的详细介绍，我推荐这篇来自TDS的文章</p><div class="nk nl gp gr nm nn"><a rel="noopener follow" target="_blank" href="/understanding-variational-autoencoders-vaes-f70510919f73"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd ir gy z fp ns fr fs nt fu fw ip bi translated">了解变分自动编码器(VAEs)</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">逐步建立导致VAEs的推理。</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob jw nn"/></div></div></a></div><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oc"><img src="../Images/717d41162fadef9bbb992fdc8dd3a714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RMktMUnc3BtixJad.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来自<a class="ae kc" rel="noopener" target="_blank" href="/understanding-variational-autoencoders-vaes-f70510919f73"><strong class="bd od">https://towardsdatascience . com/understanding-variable-auto encoders-vaes-f 70510919 f 73</strong></a></p></figure><p id="34df" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，VAE的生成部分是解码器，它本质上是一个将潜在空间中的点映射到可观察空间中的点的函数，用于图像生成</p><p id="4bce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">-潜在空间作为N维欧几里得空间，所以潜在码本质上是N个实元素的向量</p><p id="0783" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">-作为W x H x 3张量空间的可观察空间，用于W x H大小的RGB图像</p><p id="d735" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，从统计的角度来看，解码器得到输入分布Z，并将其转换为输出分布X，因此G(Z) -&gt; X和Z的每个样本Z属于潜在空间，X的每个样本X属于可观察空间</p><p id="02f3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个公式给出了在最大似然估计框架中使用的第二个术语:发生器分布，所以让我们用正确的符号重写这个公式</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/471d8c7891a4e12d1696106dfc93f9a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*DjfN0zTHxYN_2LF0exne2g.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">MLE解码器培训</p></figure><p id="dd3f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以开始训练解码器了吗？</p><p id="9728" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从技术上来说，是的，我们可以开始随机采样发电机潜在空间，并从数据集中随机选取图像，尝试调整θ，以对齐2个分布</p><p id="e1fe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它永远不会起作用，因为不仅潜在代码和期望输出之间没有关系，而且它还会随着时间而改变。</p><p id="5986" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，让我们至少将潜在空间代码与数据集中的每个样本相关联:随机选择，但至少随时间保持不变。</p><p id="6440" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，解码器将最好地完全过拟合训练集，但是将具有零泛化能力，因此完全无用。</p><p id="e022" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们缺少的组件是编码器，它与解码器的功能相反:从可见空间映射到潜在空间。</p><p id="685b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们用F(X) -&gt; Z来表示它，让我们假设用神经网络来实现它，还有另一个参数函数</p><p id="0d2c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理论上，它应该增加参数空间大小，但是实际上编码器和解码器共享它们的权重(在某种意义上，它们的参数化是另一个的转置),所以让我们用θ来标识它们两者的参数化</p><p id="4fcb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">加上这个就结束了循环，允许我们从图像生成代码，从代码生成图像。</p><p id="bb7a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，从分布角度来看，我们通过对数据集进行采样获得分布X，该分布由编码器转换为分布，该分布由解码器转换为分布X’，因此，通过比较X和X’，可以在MLE框架中进行训练，我们可以表示如下</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/eda2c31d359c76edfe70ed3b505e9088.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*OYOZdQ06QszmJvd_jjeqzQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">全自动编码器的MLE训练</p></figure><h1 id="54d5" class="li lj iq bd lk ll mr ln lo lp ms lr ls lt mt lv lw lx mu lz ma mb mv md me mf bi translated">结论</h1><p id="8c7a" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">在这里，我们已经详细地展示了自动编码器如何在最大似然估计框架中被训练，以及为什么它们被称为基于似然的生成模型</p></div></div>    
</body>
</html>