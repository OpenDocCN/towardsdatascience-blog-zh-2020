<html>
<head>
<title>Beginners Guide to PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beginners-guide-to-pyspark-bbe3b553b79f?source=collection_archive---------1-----------------------#2020-10-21">https://towardsdatascience.com/beginners-guide-to-pyspark-bbe3b553b79f?source=collection_archive---------1-----------------------#2020-10-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d6d1" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">PySpark教程</h2><div class=""/><div class=""><h2 id="7dbf" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">第1章:使用美国股票价格数据介绍PySpark</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/2e59612c3c9ea0689b00941e06cddd5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OfEl7BueS90S3eBC"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">卢克·切瑟在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e3c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> PySpark </strong>是<strong class="lk jd"> Apache Spark </strong>的API，Apache Spark 是一个开源的分布式处理系统，用于大数据处理，最初是在加州大学伯克利分校用Scala编程语言开发的。Spark拥有Scala、Java、Python和R的开发API，并支持跨多种工作负载的代码重用——批处理、交互式查询、实时分析、机器学习和图形处理。它利用内存缓存和优化的查询执行，针对任何大小的数据进行快速分析查询。它没有自己的文件系统，如Hadoop HDFS，它支持大多数流行的文件系统，如Hadoop分布式文件系统(HDFS)，HBase，Cassandra，亚马逊S3，亚马逊红移，Couchbase，等等</p><p id="a2d6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用Apache Spark的优势:</p><ul class=""><li id="22d6" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">它在内存中运行程序比Hadoop MapReduce快100倍，在磁盘上快10倍。<strong class="lk jd"> </strong>它声明是因为它在worker节点的主存中进行处理，防止不必要的I/O操作。</li><li id="3bb6" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">它是用户友好的，因为它有用流行语言编写的API，这使你的开发人员很容易，因为他们将分布式处理的复杂性隐藏在简单的高级操作符后面，大大降低了所需的代码量。</li><li id="69eb" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">它可以通过Mesos、Hadoop via Yarn或Spark自己的集群管理器进行部署。</li><li id="a779" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">实时计算和低延迟，因为内存计算。</li></ul><p id="5169" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章中我们看到</p><ol class=""><li id="40d0" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md ms mk ml mm bi translated"><strong class="lk jd">在Google Colab中设置环境</strong></li><li id="4b6b" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ms mk ml mm bi translated"><strong class="lk jd">星火会</strong></li><li id="553c" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ms mk ml mm bi translated"><strong class="lk jd">读取数据</strong></li><li id="c772" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ms mk ml mm bi translated"><strong class="lk jd">使用Spark模式构建数据</strong></li><li id="31cb" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ms mk ml mm bi translated"><strong class="lk jd">检查数据的不同方法</strong></li><li id="b0a0" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ms mk ml mm bi translated"><strong class="lk jd">列操纵</strong></li><li id="3a67" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ms mk ml mm bi translated"><strong class="lk jd">处理缺失值</strong></li><li id="cf0e" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ms mk ml mm bi translated"><strong class="lk jd">查询数据</strong></li><li id="3f35" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ms mk ml mm bi translated"><strong class="lk jd">数据可视化</strong></li><li id="343a" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ms mk ml mm bi translated"><strong class="lk jd">将数据写入/保存到文件</strong></li><li id="d5b6" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ms mk ml mm bi translated"><strong class="lk jd">结论</strong>和</li><li id="fb38" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ms mk ml mm bi translated"><strong class="lk jd">参考文献</strong></li></ol><p id="f256" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们开始吧，✨</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi na"><img src="../Images/b875b2bd56fb5f7863ba2c3ab681b941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*scGlvmas7nXc3RKl"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@franki?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">弗兰基·查马基</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="c571" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">在Google Colab中设置环境</h1><p id="4fcf" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">为了在本地机器上运行pyspark，我们需要Java和其他软件。因此，我们不用繁重的安装程序，而是使用Google Colaboratory，它有更好的硬件规格，还带有广泛的数据科学和机器学习库。我们需要安装<strong class="lk jd"> pyspark </strong>和<strong class="lk jd"> Py4J </strong>包。Py4J使运行在python解释器中的Python程序能够动态访问Java虚拟机中的Java对象。安装上述软件包的命令是</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="a8d0" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">火花会议</h1><p id="e2a1" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated"><strong class="lk jd"> SparkSession </strong>已经成为PySpark的一个入口点，因为在2.0版本之前<strong class="lk jd"> SparkContext </strong>被用作一个入口点。<strong class="lk jd"> SparkSession </strong>是底层PySpark功能的入口点，用于以编程方式创建PySpark RDD、数据帧和数据集。它可以在replace with SQLContext、HiveContext和2.0之前定义的其他上下文中使用。您还应该知道，SparkSession在内部使用SparkSession提供的配置创建SparkConfig和SparkContext。可以使用<code class="fe oa ob oc od b">SparkSession.builder</code>构建器模式创建SparkSession。</p><h2 id="de41" class="oe nc it bd nd of og dn nh oh oi dp nl lr oj ok nn lv ol om np lz on oo nr iz bi translated">创建SparkSession</h2><p id="3824" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">要创建SparkSession，您需要使用构建器模式方法<code class="fe oa ob oc od b">builder()</code></p><ul class=""><li id="fb04" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><code class="fe oa ob oc od b">getOrCreate()</code> —该方法返回一个已经存在的SparkSession如果不存在，它将创建一个新的SparkSession。</li><li id="57cd" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><code class="fe oa ob oc od b">master()</code>–如果您在集群上运行它，您需要使用您的主名称作为参数。通常，它可能是<code class="fe oa ob oc od b"><a class="ae lh" href="https://sparkbyexamples.com/hadoop/how-yarn-works/" rel="noopener ugc nofollow" target="_blank">yarn</a></code> <a class="ae lh" href="https://sparkbyexamples.com/hadoop/how-yarn-works/" rel="noopener ugc nofollow" target="_blank"> </a>或<code class="fe oa ob oc od b">mesos</code>，这取决于您的集群设置，并且在独立模式下运行时也使用<code class="fe oa ob oc od b">local[X]</code>。<code class="fe oa ob oc od b">X</code>应该是一个大于0的整数值，表示使用RDD、数据帧和数据集时应该创建多少个分区。理想情况下，值<code class="fe oa ob oc od b">X</code>应该是CPU内核的数量。</li><li id="19ea" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><code class="fe oa ob oc od b">appName()</code>该方法用于设置你的应用程序的名称。</li><li id="21d9" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><code class="fe oa ob oc od b">getOrCreate()</code>该方法返回一个现有的SparkSession，如果它存在，否则它创建一个新的SparkSession。</li></ul><p id="7579" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">创建<strong class="lk jd"> SparkSession </strong>的示例</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="95bb" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">读取数据</h1><p id="b480" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">pyspark可以读取各种文件格式的数据，如逗号分隔值(CSV)、JavaScript对象符号(JSON)、Parquet等。以下是从不同文件格式读取数据的示例:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="d662" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">使用Spark模式构建数据</h1><p id="bd44" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">让我们来看看Kaggle数据集中2019年1月至2020年7月的美国股票价格数据。</p><div class="op oq gp gr or os"><a href="https://www.kaggle.com/dinnymathew/usstockprices" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">美国股票价格</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">2019年1月至2020年7月的标准普尔500指数数据</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">www.kaggle.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg lb os"/></div></div></a></div><p id="f1ca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">读取CSV文件格式数据的代码。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="7847" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们使用<code class="fe oa ob oc od b">PrintSchema</code>方法来查看数据的模式。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/dab785f71a456203af19ff7e8b54d2dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*KZ3Q8hIrOf37WRuRc9Us-Q.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">数据的模式</p></figure><p id="0a89" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> Spark schema </strong>是DataFrame或Dataset的结构，我们可以使用<strong class="lk jd"> StructType </strong>类来定义它，该类是定义列名(字符串)、列类型(数据类型)、可空列(布尔)和元数据(元数据)的<strong class="lk jd"> StructField </strong>的集合。spark从数据中推断模式，但是有时推断的数据类型可能不正确，或者我们可能需要定义自己的列名和数据类型，特别是在处理非结构化和半结构化数据时。</p><p id="06fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们看看如何使用它来组织我们的数据:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="9286" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的代码显示了如何使用<code class="fe oa ob oc od b">StructType</code>和<code class="fe oa ob oc od b">StructField</code>创建结构。然后将创建的结构传递给模式参数，同时使用<code class="fe oa ob oc od b">spark.read.csv()</code>读取数据。让我们看看结构化数据的模式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/d598a7ff627f3427d61dc4a688af060a.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*nwagsS9yvzxtPbvlpaw68g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">结构化后的数据模式—按作者分类的图像</p></figure><h1 id="af4e" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">检查数据的不同方法</h1><p id="15f5" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">有各种方法用于检查数据。它们是schema、dtypes、show、head、first、take、description、columns、count、distinct、printSchema。我们用一个例子来看看他们方法的解释。</p><ul class=""><li id="7a6f" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd"> schema() </strong>:这个方法返回数据的模式(dataframe)。以下示例显示了w.r.t美国股票价格数据。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><ul class=""><li id="1949" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd"> dtypes </strong>:返回包含列名及其数据类型的元组列表。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><ul class=""><li id="42d5" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd"> head(n) </strong>:返回<strong class="lk jd"> n </strong>行列表。这里有一个例子:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><ul class=""><li id="ba5c" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd"> show() </strong>:默认显示前20行，并以数字为参数显示数据的行数。下面是一个例子:show(5)。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/a3f86bae07888c2c3f45ed82620be819.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NU62F4CMdqqOfzExLMmEqg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">美国股票价格数据的前五行—作者图片</p></figure><ul class=""><li id="87dc" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd"> first() </strong>:返回数据的第一行。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><ul class=""><li id="cb6a" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd"> take(n) </strong>:返回数据的前<strong class="lk jd"> n </strong>行。</li><li id="f0e5" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd"> describe() </strong>:计算数据类型为数值的列的汇总统计数据。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><ul class=""><li id="b6c2" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd">列</strong>:返回包含数据列名的列表。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><ul class=""><li id="5970" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd"> count() </strong>:返回数据中行数的计数。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><ul class=""><li id="6550" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd"> distinct() </strong>:返回数据中不同的行数。</li><li id="2450" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd"> printSchema() </strong>:显示数据的模式。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="f093" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">列操作</h1><p id="7686" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">让我们看看用于添加、更新、删除数据列的不同方法。</p><ol class=""><li id="86c1" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md ms mk ml mm bi translated"><strong class="lk jd">添加列</strong>:使用<code class="fe oa ob oc od b">withColumn</code>该方法采用两个参数列名和数据向现有数据添加新列。请参见下面的示例:</li></ol><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/57a0a6fa33f7fd28e8c2d6a76726d367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*--ok8I-ZgWnUpIGyHOcLBQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">添加列后的数据—按作者分类的图像</p></figure><p id="5941" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">2.<strong class="lk jd">更新列</strong>:使用<code class="fe oa ob oc od b">withColumnRenamed</code>来重命名现有的列，T1带有参数现有的列名和新的列名。请参见下面的示例:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/a87d27b34cba60649491c34f35212cd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ddyy3wecyGQJVzgXZiPlwQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">更新后的数据-按作者分类的图像</p></figure><p id="8ef4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">3.<strong class="lk jd">删除列</strong>:使用<code class="fe oa ob oc od b">drop</code>获取列名并返回数据的方法。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/67de05d7b924bc7a2d772da4329610d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n0N3tv59ImZoa5RG-NdTSw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">删除列后的数据-按作者分类的图像</p></figure><h1 id="c101" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">处理缺失值</h1><p id="6d37" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">在处理实时数据时，我们经常会遇到缺失值。这些缺失值被编码为<strong class="lk jd"> NaN </strong>、空格和占位符。有各种处理缺失值的技术，其中一些比较流行的是:</p><ul class=""><li id="d6b1" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd">删除</strong>:删除任意一列中缺失值的行。</li><li id="96cf" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd">用平均值/中值估算</strong>:用相应列的平均值/中值替换缺失值。它简单、快速，并且适用于小型数值数据集。</li><li id="579c" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd">使用最频繁值估算</strong>:顾名思义，使用该列中最频繁的值来替换该列中缺失的值。这适用于分类特征，但也可能会在数据中引入偏差。</li><li id="ca77" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd">使用KNN估算</strong>:<em class="pn">K</em>-最近邻是一种分类算法，它使用不同的距离度量来使用特征相似性，例如欧几里德距离、马哈拉诺比斯距离、曼哈顿距离、闵可夫斯基距离和汉明距离，用于任何新的数据点。与上述根据数据集估算缺失值的方法相比，这种方法非常有效，而且计算量大，对异常值敏感。</li></ul><p id="1ac9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们看看如何使用PySpark来处理缺失值:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="23bf" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">查询数据</h1><p id="f040" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">PySpark和PySpark SQL提供了广泛的方法和函数来轻松查询数据。以下是几种最常用的方法:</p><ul class=""><li id="d906" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd">选择</strong></li><li id="4916" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd">过滤器</strong></li><li id="c196" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd">与</strong>之间</li><li id="74a9" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd">当</strong></li><li id="b46b" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd">喜欢</strong></li><li id="9278" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd">分组依据</strong></li><li id="f6b0" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd">聚合</strong></li></ul><h2 id="3eb0" class="oe nc it bd nd of og dn nh oh oi dp nl lr oj ok nn lv ol om np lz on oo nr iz bi translated">挑选</h2><p id="17bc" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">它用于使用列名选择单个或多个列。这里有一个简单的例子:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><div class="ks kt ku kv gt ab cb"><figure class="po kw pp pq pr ps pt paragraph-image"><img src="../Images/5447ee339857d2aca12d4d8f7f16a077.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*Mccpf5BivXHS7s_mq0LC0Q.png"/></figure><figure class="po kw pu pq pr ps pt paragraph-image"><img src="../Images/3e32690001bab61fa7b2afe6efeba0fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*8aXyNtDWvpzvCNakD7v8nQ.png"/><p class="ld le gj gh gi lf lg bd b be z dk pv di pw px translated">选择提取单列或多列数据的操作-按作者排序的图像</p></figure></div><h2 id="85b9" class="oe nc it bd nd of og dn nh oh oi dp nl lr oj ok nn lv ol om np lz on oo nr iz bi translated">过滤器</h2><p id="80f7" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">根据给定的条件筛选数据，也可以使用AND(&amp;)、OR(|)和NOT(~)运算符给出多个条件。下面是获取2020年1月股票价格数据的示例。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi py"><img src="../Images/ec1dd31e76ce20e5e5a5ab00de5c6f77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k_Ta5z9Y0iy82hHpzOimPQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用过滤器获取2020年1月的股票价格数据—图片由作者提供</p></figure><h2 id="ad8b" class="oe nc it bd nd of og dn nh oh oi dp nl lr oj ok nn lv ol om np lz on oo nr iz bi translated">在...之间</h2><p id="bce3" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">如果between方法中传递的值。让我们看一个获取数据的例子，其中调整后的值在100和500之间。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pz"><img src="../Images/a1c8b911b1d21375c59b24e6c30c28cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Va_xuneQa1tDjGb7iJMmA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用Between获取数据—按作者分类的图像</p></figure><h2 id="4bb8" class="oe nc it bd nd of og dn nh oh oi dp nl lr oj ok nn lv ol om np lz on oo nr iz bi translated">当...的时候</h2><p id="02c5" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">根据给定的条件，它返回0或1，下面的例子显示了当调整后的价格大于等于200时，如何选择股票的开盘价和收盘价。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/6ce8c9b1ecf7dcb2096a454c014d0e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*y4uRFkOJuGY-kIbS0EsKCg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用何时获取数据-按作者分类的图像</p></figure><h2 id="591b" class="oe nc it bd nd of og dn nh oh oi dp nl lr oj ok nn lv ol om np lz on oo nr iz bi translated">喜欢</h2><p id="6665" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">它类似于SQL中的like操作符，下面的例子显示了使用“rlike”提取以M或C开头的扇区名。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/e25c92bd8bc42d859cf7b5122e065295.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*M_9xmH0LxfpSw9APYwRfzw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">输出—按作者分类的图像</p></figure><h2 id="33b6" class="oe nc it bd nd of og dn nh oh oi dp nl lr oj ok nn lv ol om np lz on oo nr iz bi translated">古尔比</h2><p id="8e7d" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">该名称本身说明了它根据给定的列名对数据进行分组，并且可以执行不同的操作，如sum、mean、min、max等。下面的示例说明了如何获得与行业相关的平均开盘、收盘和调整后的股票价格。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qc"><img src="../Images/8a193ab979310106b4bd1ec9aadeeacc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cWETY3zjKiSmo0Z2yYvmIQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">按示例分组—按作者分组的图像</p></figure><h2 id="2566" class="oe nc it bd nd of og dn nh oh oi dp nl lr oj ok nn lv ol om np lz on oo nr iz bi translated">聚合</h2><p id="55fa" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">PySpark在DataFrame API中提供了内置的标准聚合函数定义，当我们需要对数据列进行聚合操作时，这些函数就派上了用场。聚合函数对一组行进行操作，并为每个组计算一个返回值。以下示例显示了如何显示最小值、最大值和平均值；从2019年1月到2020年1月与行业相关的开盘、收盘和调整后的股价。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qd"><img src="../Images/518004125b9e0fe0523abd4482a8f966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V-6kw1kgqnf_bAzOCcLlng.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">聚合示例—按作者分类的图像</p></figure><h1 id="4ab5" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">数据可视化</h1><p id="ed06" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">我们将利用matplotlib和pandas来可视化数据，toPandas()方法用于将数据转换为pandas dataframe。使用dataframe，我们利用plot()方法来可视化数据。下面的代码显示了如何显示该板块的平均开盘价、收盘价和调整后股价的条形图。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qe"><img src="../Images/e0f22662481a7d296795e40a2c7e3e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yTuakatg9NRhj5wUSOC0tg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">平均开盘价、收盘价和调整后的股票价格(相对于行业)——图片由作者提供</p></figure><p id="f390" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">类似地，让我们来看看各行业的平均开盘价、收盘价和调整后价格。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qf"><img src="../Images/939199b1f8e71d8d0aa1a301f8bb5d28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H-e1lAlOWIpn9eo3-2zhGw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">行业的平均开盘价、收盘价和调整后股价-图片由作者提供</p></figure><p id="e6e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们来看看科技股平均开盘、收盘和调整后股价的时序图。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qg"><img src="../Images/e6748fed7a1545ec87543034f3c7d594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ob8uQzbpNhL8lmMUlfk9Lw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">技术行业的时间序列图—作者图片</p></figure><h1 id="2bf6" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">将数据写入/保存到文件</h1><p id="b23d" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">“write.save()”方法用于以不同的格式保存数据，如CSV、JSVON、Parquet、e.t.c。让我们看看如何以不同的文件格式保存数据。我们可以使用' select()'方法保存整个数据和选定的数据。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="d310" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">结论</h1><p id="5295" class="pw-post-body-paragraph li lj it lk b ll nt kd ln lo nu kg lq lr nv lt lu lv nw lx ly lz nx mb mc md im bi translated">PySpark 是数据科学家学习的好语言，因为它支持可扩展分析和ML管道。如果您已经熟悉Python、SQL和Pandas，那么PySpark是一个很好的起点。</p><p id="e737" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文展示了如何使用PySpark执行各种各样的操作，从读取文件到将细节写入文件。它还涵盖了使用matplotlib可视化洞察的基本可视化技术。此外，谷歌协作笔记本是开始学习PySpark的好方法，无需安装必要的软件。查看有助于更轻松、更快速地学习PySpark的参考资料。</p><blockquote class="qh qi qj"><p id="bd0a" class="li lj pn lk b ll lm kd ln lo lp kg lq qk ls lt lu ql lw lx ly qm ma mb mc md im bi translated">通过使用下面的colab notebook和GitHub，您可以随意访问/使用我在文章中写的代码。</p><p id="e44a" class="li lj pn lk b ll lm kd ln lo lp kg lq qk ls lt lu ql lw lx ly qm ma mb mc md im bi translated">快乐学习✨</p></blockquote><div class="op oq gp gr or os"><a href="https://colab.research.google.com/drive/1Iwn8bmZjzjzO5_Li2ZFF_zf_b1td44Eg?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">PySpark初学者指南</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">第1章:使用美国股票价格数据介绍PySpark</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">colab.research.google.com</p></div></div><div class="pb l"><div class="qn l pd pe pf pb pg lb os"/></div></div></a></div><div class="op oq gp gr or os"><a href="https://github.com/syamkakarla98/Beginners_Guide_to_PySpark" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">syamkakarla 98/初学者指南</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">第1章:使用股票价格数据介绍PySpark</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">github.com</p></div></div><div class="pb l"><div class="qo l pd pe pf pb pg lb os"/></div></div></a></div><h1 id="7ef1" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">参考</h1><div class="op oq gp gr or os"><a href="https://www.oreilly.com/library/view/mastering-big-data/9781838640583/" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">使用PySpark掌握大数据分析</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">使用PySpark的强大功能有效地将高级分析应用于大数据集关于此视频解决您的大数据…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">www.oreilly.com</p></div></div><div class="pb l"><div class="qp l pd pe pf pb pg lb os"/></div></div></a></div><div class="op oq gp gr or os"><a href="https://spark.apache.org/docs/latest/api/python/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">欢迎使用Spark Python API文档！— PySpark 3.0.1文档</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">Spark功能的主要入口点。弹性分布式数据集(RDD)，Spark中的基本抽象。主…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">spark.apache.org</p></div></div><div class="pb l"><div class="qq l pd pe pf pb pg lb os"/></div></div></a></div><div class="op oq gp gr or os"><a href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">pyspark.sql模块— PySpark 2.4.0文档</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">Spark SQL和DataFrames的重要类别:在…中使用结构化数据(行和列)的入口点</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">spark.apache.org</p></div></div></div></a></div></div></div>    
</body>
</html>