<html>
<head>
<title>Integrating Prefect &amp; Databricks to Manage your Spark Jobs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成Prefect和Databricks来管理您的Spark作业</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tutorial-integrating-prefect-databricks-af426d8edf5c?source=collection_archive---------20-----------------------#2020-11-03">https://towardsdatascience.com/tutorial-integrating-prefect-databricks-af426d8edf5c?source=collection_archive---------20-----------------------#2020-11-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/ff8f9c092f7e2a545147670f6a1ce16c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qChT6ti4eKEGpSYotQ1mg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">Justin Jairam的照片来自<a class="ae kf" href="https://www.instagram.com/jusspreme/?hl=en" rel="noopener ugc nofollow" target="_blank"> @jusspreme </a>(经允许)</p></figure><p id="28d0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Prefect是一个工作流管理系统，使用户能够轻松地使用数据应用程序并添加重试、日志记录、动态映射、缓存、故障通知、调度等功能，所有这些都使用功能性Python API。Prefect允许用户将他们现有的代码转换成DAG(有向无环图),并且依赖关系已经确定[1]。它简化了ETL管道和依赖项的创建，并使用户能够严格地关注应用程序代码而不是管道代码(看着你的气流)。Prefect甚至可以创建分布式管道来并行化您的数据应用程序。</p><p id="ce23" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Databricks的核心是一个PaaS(平台即服务),它可以提供完全托管的Spark集群、交互式和协作式笔记本电脑(类似于Jupyter)、生产流水线调度程序以及为基于Spark的应用提供支持的平台。它集成在Azure和AWS生态系统中，使处理大数据变得简单。Databricks使用户能够在其管理的Spark集群上运行定制的Spark应用程序。它甚至允许用户将他们的笔记本电脑安排为Spark作业。它完全简化了大数据开发和围绕它的ETL过程。</p><p id="4269" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Databricks已经成为一个不可或缺的大数据ETL工具，一个我每天在工作中使用的工具，所以我为Prefect项目做了一个<a class="ae kf" href="https://github.com/PrefectHQ/prefect/pull/3247" rel="noopener ugc nofollow" target="_blank">贡献</a>，使用户能够将Databricks作业与Prefect集成。在本教程中，我们将讨论这一点——如何将正在运行的数据块笔记本和Spark作业整合到您的完美流程中。</p><h1 id="4bb0" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">先决条件</h1><p id="7733" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这个帖子不需要任何先验知识，但是推荐一个免费的<a class="ae kf" href="https://www.prefect.io/" rel="noopener ugc nofollow" target="_blank">府尹</a>账号来实现这个例子。虽然这篇文章会涉及到级长的基础知识，但它不是一个深入的级长教程。</p><h1 id="eef4" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">完美基础</h1><h1 id="eda4" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">任务</h1><p id="bcfe" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">Prefect中的任务相当于数据管道中的一个步骤。它就像应用程序或脚本中的Python函数一样简单。对任务的简单或复杂程度没有限制。也就是说，最好遵循编码最佳实践并开发您的功能，这样它们只做一件事。级长自己推荐这个。</p><blockquote class="mh mi mj"><p id="3f51" class="kg kh mk ki b kj kk kl km kn ko kp kq ml ks kt ku mm kw kx ky mn la lb lc ld im bi translated">一般来说，我们鼓励小任务而不是大任务，每个任务应该执行工作流程中一个独立的逻辑步骤，但不能更多。[2]</p></blockquote><p id="f7ca" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过保持小任务，您将最大限度地利用Prefect的引擎，例如高效的状态检查点。</p><h1 id="d5fb" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">流</h1><p id="3a5a" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">流程是将所有任务及其依赖关系联系在一起的东西。它描述了任务、它们的顺序和数据流之间的依赖关系。流将任务集合在一起，并使其成为一个管道，从而完善您的数据应用程序。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/abbcfeb3dbae3c1460cc9afdba9dc264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HdjvGZkJXZ2VfgblsMDGrQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">完美的流动可视化(图片由作者提供)</p></figure><h1 id="fa96" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">Prefect中的本机数据块集成</h1><p id="138c" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我通过实现任务<code class="fe mt mu mv mw b">DatabricksRunNow</code> &amp; <code class="fe mt mu mv mw b">DatabricksRunSubmit</code>来实现perfect和Databricks之间的无缝集成，从而为perfect项目做出了贡献。通过这些任务，用户可以从外部触发一个已定义的Databricks作业或一个jar、Python脚本或笔记本的单次运行。一旦任务被执行，它就使用Databricks本地API调用来运行笔记本或Spark作业。当任务运行时，它将继续轮询运行的当前状态，直到任务完成。一旦任务完成，如果成功，它将允许下游任务运行。</p><h1 id="34e9" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">使用数据块任务创建流</h1><p id="f7a0" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在开始编写任何代码之前，我们必须创建一个完美的秘密来存储我们的数据块连接字符串。从你的提督云账户，点击左侧菜单中的<code class="fe mt mu mv mw b">Team</code>，进入<code class="fe mt mu mv mw b">Secrets</code>部分。这个部分是你管理完美流程的所有秘密的地方。</p><p id="77e8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要生成Databricks连接字符串，您需要Databricks实例的主机名以及Databricks帐户的PAT。要创建数据块PAT，请遵循数据块<a class="ae kf" href="https://docs.databricks.com/dev-tools/api/latest/authentication.html" rel="noopener ugc nofollow" target="_blank">文档</a>中的步骤。连接字符串必须是有效的JSON对象。秘密的标题必须是<code class="fe mt mu mv mw b">DATABRICKS_CONNECTION_STRING</code>。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mx"><img src="../Images/9eca8a9cb36d7703de353a1ea9fadeb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3jG_oVLzKLu_6Bu84atRHA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">数据块连接字符串的完美秘密(图片由作者提供)</p></figure><h1 id="e9e9" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">创建任务</h1><p id="bcad" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">让我们从定义一些运行Databricks笔记本和Spark作业所需的常见任务开始我们的流程。</p><pre class="mp mq mr ms gt my mw mz na aw nb bi"><span id="da17" class="nc lf it mw b gy nd ne l nf ng">from prefect import task, Flow<br/>from prefect.tasks.databricks.databricks_submitjob import (<br/>    DatabricksRunNow,<br/>    DatabricksSubmitRun,<br/>)<br/>from prefect.tasks.secrets.base import PrefectSecret</span><span id="dcda" class="nc lf it mw b gy nh ne l nf ng">conn <strong class="mw iu">=</strong> PrefectSecret("DATABRICKS_CONNECTION_STRING")<br/><em class="mk"># Initialize Databricks task class as a template<br/># We will then use the task function to pass in unique config options &amp; params<br/></em>RunNow <strong class="mw iu">=</strong> DatabricksRunNow(conn)<br/>SubmitRun <strong class="mw iu">=</strong> DatabricksSubmitRun(conn)</span></pre><p id="fb07" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们定义了两个任务对象<code class="fe mt mu mv mw b">RunNow</code>和<code class="fe mt mu mv mw b">SubmitRun</code>，作为模板来运行我们的数据块作业。我们可以通过不同的配置重用这些相同的任务，从而轻松创建新的数据块作业。让我们创建一些助手任务来动态创建我们的作业的配置。</p><pre class="mp mq mr ms gt my mw mz na aw nb bi"><span id="29d0" class="nc lf it mw b gy nd ne l nf ng"><strong class="mw iu">@</strong>task<br/><strong class="mw iu">def</strong> <strong class="mw iu">get_submit_config</strong>(python_params: list):<br/>    """<br/>    SubmitRun config template for the DatabricksSubmitRun task,</span><span id="6430" class="nc lf it mw b gy nh ne l nf ng">    Spark Python Task params must be passed as a list.<br/>    """<br/>    <strong class="mw iu">return</strong> {<br/>        "run_name": "MyDatabricksJob",<br/>        "new_cluster": {<br/>          "spark_version": "7.3.x-scala2.12",<br/>          "node_type_id": "r3.xlarge",<br/>          "aws_attributes": {<br/>            "availability": "ON_DEMAND"<br/>          },<br/>          "num_workers": 10<br/>        },<br/>        "spark_python_task": {<br/>            "python_file": "/Users/ashton/databricks_task/main.py",<br/>            "parameters": python_params,<br/>        },<br/>    }<br/></span><span id="2c9f" class="nc lf it mw b gy nh ne l nf ng"><strong class="mw iu">@</strong>task<br/><strong class="mw iu">def</strong> <strong class="mw iu">get_run_now_config</strong>(notebook_params: dict):<br/>    """<br/>    RunNow config template for the DatabricksSubmitRun task,</span><span id="b18e" class="nc lf it mw b gy nh ne l nf ng">    Notebook Task params must be passed as a dictionary.<br/>    """<br/>    <strong class="mw iu">return</strong> {"job_id": 42, "notebook_params": notebook_params}</span></pre><p id="0731" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe mt mu mv mw b">get_submit_config</code>任务允许我们动态地将参数传递给DBFS (Databricks文件系统)上的Python脚本，并返回一个配置来运行一次性Databricks作业。您可以通过在Databricks作业配置中创建更多映射到配置选项的参数来增加灵活性。<code class="fe mt mu mv mw b">get_run_now_config</code>执行相同的任务，除了它为<code class="fe mt mu mv mw b">DatabricksRunNow</code>任务返回一个配置来运行一个预配置的数据块笔记本任务。<code class="fe mt mu mv mw b">get_run_now_config</code>和<code class="fe mt mu mv mw b">get_submit_config</code>的模式分别匹配<a class="ae kf" href="https://docs.databricks.com/dev-tools/api/latest/jobs.html#run-now" rel="noopener ugc nofollow" target="_blank">现在运行</a>和<a class="ae kf" href="https://docs.databricks.com/dev-tools/api/latest/jobs.html#runs-submit" rel="noopener ugc nofollow" target="_blank">运行提交</a> API。</p><blockquote class="mh mi mj"><p id="cfd0" class="kg kh mk ki b kj kk kl km kn ko kp kq ml ks kt ku mm kw kx ky mn la lb lc ld im bi translated">Python文件参数必须作为列表传递，而笔记本参数必须作为字典传递。</p></blockquote><p id="bda8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们创建一个可以运行我们的任务的流。</p><h1 id="4437" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">创造流动</h1><p id="3839" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们将创建一个流程，在Databricks上运行一个预配置的笔记本作业，然后是两个后续的Python脚本作业。</p><pre class="mp mq mr ms gt my mw mz na aw nb bi"><span id="0d29" class="nc lf it mw b gy nd ne l nf ng"><strong class="mw iu">with</strong> Flow("Databricks-Tasks", schedule<strong class="mw iu">=</strong>None) <strong class="mw iu">as</strong> flow:</span><span id="16c3" class="nc lf it mw b gy nh ne l nf ng">    run_now_config <strong class="mw iu">=</strong> get_run_now_config({"param1": "value"})<br/>    submit_config_a <strong class="mw iu">=</strong> get_submit_config(["param1"])<br/>    submit_config_b <strong class="mw iu">=</strong> get_submit_config(["param2"])</span><span id="272f" class="nc lf it mw b gy nh ne l nf ng">    run_now_task <strong class="mw iu">=</strong> RunNow(json<strong class="mw iu">=</strong>run_now_config)</span><span id="2822" class="nc lf it mw b gy nh ne l nf ng">    submit_task_a <strong class="mw iu">=</strong> SubmitRun(json<strong class="mw iu">=</strong>submit_config_a)</span><span id="42f7" class="nc lf it mw b gy nh ne l nf ng">    submit_task_b <strong class="mw iu">=</strong> SubmitRun(json<strong class="mw iu">=</strong>submit_config_b)</span><span id="ec99" class="nc lf it mw b gy nh ne l nf ng">    <em class="mk"># Since Databricks tasks don't return any data dependencies we can leverage,<br/></em>    <em class="mk"># we have to define the dependencies between Databricks tasks themselves<br/></em>    flow.add_edge(run_now_task, submit_task_a)<br/>    flow.add_edge(submit_task_a, submit_task_b)</span></pre><p id="c8d1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先需要通过使用我们的<code class="fe mt mu mv mw b">get_run_now_config</code>和<code class="fe mt mu mv mw b">get_submit_config</code>任务来创建数据块作业配置。通过<code class="fe mt mu mv mw b">json</code>参数将立即运行配置传递给<code class="fe mt mu mv mw b">RunNow</code>任务，并将提交运行配置传递给<code class="fe mt mu mv mw b">SubmitRun</code>任务。<code class="fe mt mu mv mw b">json</code>参数接受一个与上面提到的<code class="fe mt mu mv mw b">Run Now</code>和<code class="fe mt mu mv mw b">Submit Run</code>API匹配的字典。为了运行更多的数据块作业，我们实例化我们创建的<code class="fe mt mu mv mw b">RunNow</code>或<code class="fe mt mu mv mw b">SubmitRun</code>模板，并传入一个新的json作业配置。</p><p id="0f9e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Prefect flow的一个令人惊叹的特性是，它可以从您的任务中自动构建一个DAG。它将任务输入视为数据依赖，并由此推断出在其他任务运行之前需要完成哪些任务。例如，由于我们的<code class="fe mt mu mv mw b">run_now_task</code>有输入<code class="fe mt mu mv mw b">run_now_config</code>，流程构建DAG，知道<code class="fe mt mu mv mw b">get_run_now_config</code>任务必须在<code class="fe mt mu mv mw b">run_now_task</code>之前运行。</p><p id="a4eb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有些任务不返回可用作下游任务输入的数据。例如，数据块任务只返回一个作业ID。我们仍然可以通过使用<code class="fe mt mu mv mw b">.add_edge</code>函数来定义流程的任务间依赖关系。这将增加任务之间的相关性，这些相关性不会用作后续下游任务的输入。例如，<code class="fe mt mu mv mw b">flow.add_edge(run_now_task, submit_task_a)</code>表示<code class="fe mt mu mv mw b">submit_task_a</code>是<code class="fe mt mu mv mw b">run_now_task</code>的下游任务，在<code class="fe mt mu mv mw b">run_now_task</code>完成之前<code class="fe mt mu mv mw b">submit_task_a</code>不能运行。通过将边添加到剩余的数据块任务中，我们得到了最终的流程，您也可以在Prefect schematics选项卡中查看。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/afbae5ac61ff60245f0bed868b5f7aba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wx9tijPPOa7kG3QGtV2Quw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">我们流程的DAG(图片由作者提供)</p></figure><p id="f677" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了运行流，我们调用流对象的<code class="fe mt mu mv mw b">.run()</code>方法— <code class="fe mt mu mv mw b">flow.run()</code>。最终的流程如下所示:</p><pre class="mp mq mr ms gt my mw mz na aw nb bi"><span id="0f10" class="nc lf it mw b gy nd ne l nf ng">from prefect import task, Flow<br/>from prefect.tasks.databricks.databricks_submitjob import (<br/>    DatabricksRunNow,<br/>    DatabricksSubmitRun,<br/>)<br/>from prefect.tasks.secrets.base import PrefectSecret<br/></span><span id="e3da" class="nc lf it mw b gy nh ne l nf ng"><strong class="mw iu">@</strong>task<br/><strong class="mw iu">def</strong> <strong class="mw iu">get_submit_config</strong>(python_params: list):<br/>    """<br/>    SubmitRun config template for the DatabricksSubmitRun task,</span><span id="667f" class="nc lf it mw b gy nh ne l nf ng">    Spark Python Task params must be passed as a list.<br/>    """<br/>    <strong class="mw iu">return</strong> {<br/>        "run_name": "MyDatabricksJob",<br/>        "new_cluster": {<br/>          "spark_version": "7.3.x-scala2.12",<br/>          "node_type_id": "r3.xlarge",<br/>          "aws_attributes": {<br/>            "availability": "ON_DEMAND"<br/>          },<br/>          "num_workers": 10<br/>        },<br/>        "spark_python_task": {<br/>            "python_file": "/Users/ashton/databricks_task/main.py",<br/>            "parameters": python_params,<br/>        },<br/>    }<br/></span><span id="0dff" class="nc lf it mw b gy nh ne l nf ng"><strong class="mw iu">@</strong>task<br/><strong class="mw iu">def</strong> <strong class="mw iu">get_run_now_config</strong>(notebook_params: dict):<br/>    """<br/>    RunNow config template for the DatabricksSubmitRun task,</span><span id="4d4a" class="nc lf it mw b gy nh ne l nf ng">    Notebook Task params must be passed as a dictionary.<br/>    """<br/>    <strong class="mw iu">return</strong> {"job_id": 42, "notebook_params": notebook_params}<br/></span><span id="3f66" class="nc lf it mw b gy nh ne l nf ng">conn <strong class="mw iu">=</strong> PrefectSecret("DATABRICKS_CONNECTION_STRING")<br/><em class="mk"># Initialize Databricks task class as a template<br/># We will then use the task function to pass in unique config options &amp; params<br/></em>RunNow <strong class="mw iu">=</strong> DatabricksRunNow(conn)<br/>SubmitRun <strong class="mw iu">=</strong> DatabricksSubmitRun(conn)</span><span id="920c" class="nc lf it mw b gy nh ne l nf ng"><strong class="mw iu">with</strong> Flow("Databricks-Tasks", schedule<strong class="mw iu">=</strong>None) <strong class="mw iu">as</strong> flow:</span><span id="4365" class="nc lf it mw b gy nh ne l nf ng">    run_now_config <strong class="mw iu">=</strong> get_run_now_config({"param1": "value"})<br/>    submit_config_a <strong class="mw iu">=</strong> get_submit_config(["param1"])<br/>    submit_config_b <strong class="mw iu">=</strong> get_submit_config(["param2"])</span><span id="2206" class="nc lf it mw b gy nh ne l nf ng">    run_now_task <strong class="mw iu">=</strong> RunNow(json<strong class="mw iu">=</strong>run_now_config)</span><span id="0c15" class="nc lf it mw b gy nh ne l nf ng">    submit_task_a <strong class="mw iu">=</strong> SubmitRun(json<strong class="mw iu">=</strong>submit_config_a)</span><span id="5f4c" class="nc lf it mw b gy nh ne l nf ng">    submit_task_b <strong class="mw iu">=</strong> SubmitRun(json<strong class="mw iu">=</strong>submit_config_b)</span><span id="88e2" class="nc lf it mw b gy nh ne l nf ng">    <em class="mk"># Since Databricks tasks don't return any data dependencies we can leverage,<br/></em>    <em class="mk"># we have to define the dependencies between Databricks tasks themselves<br/></em>    flow.add_edge(run_now_task, submit_task_a)<br/>    flow.add_edge(submit_task_a, submit_task_b)</span><span id="01dc" class="nc lf it mw b gy nh ne l nf ng">flow.run()<br/><em class="mk"># flow.register("YOUR_PROJECT") to register your flow on the UI</em></span></pre><h1 id="f259" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结论</h1><p id="9ebb" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">现在，作为ETL流程的一部分，您已经掌握了运行Databricks笔记本和Spark作业所需的所有知识。要了解更多关于Prefect和Databricks作业的信息，我推荐阅读它们的文档，在这里找到<a class="ae kf" href="https://docs.prefect.io/core/" rel="noopener ugc nofollow" target="_blank">在这里找到</a>和在这里找到<a class="ae kf" href="https://docs.databricks.com/dev-tools/api/latest/jobs.html" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="2e46" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">反馈</h1><p id="d7d8" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">一如既往，我鼓励对我的帖子的任何反馈。如果你有任何问题或者需要任何帮助，你可以给我发邮件到sidhuashton@gmail.com或者在帖子上留言。</p><p id="10a5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你也可以通过Twitter联系我并关注我，地址是<a class="ae kf" href="https://twitter.com/ashtonasidhu" rel="noopener ugc nofollow" target="_blank"> @ashtonasidhu </a>。</p><h1 id="7473" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">参考</h1><ol class=""><li id="b5d5" class="nj nk it ki b kj mc kn md kr nl kv nm kz nn ld no np nq nr bi translated">https://docs.prefect.io/core/,提督文件</li><li id="4474" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated"><a class="ae kf" href="https://docs.prefect.io/core/getting_started/first-steps.html," rel="noopener ugc nofollow" target="_blank"> https://docs .提督. io/core/Getting _ Started/first-steps . html，</a>提督入门</li></ol></div></div>    
</body>
</html>