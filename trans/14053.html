<html>
<head>
<title>Building the Ultimate AI Agent for Doom using Duelling Double Deep Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用决斗式双深度Q学习构建终极AI智能体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f?source=collection_archive---------43-----------------------#2020-09-27">https://towardsdatascience.com/building-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f?source=collection_archive---------43-----------------------#2020-09-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f481" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Pytorch中强化学习的实现。</h2></div><h1 id="1159" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">简介</strong></h1><p id="56db" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在过去的几篇文章中，我们已经<a class="ae lw" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830" rel="noopener">讨论了</a>和<a class="ae lw" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">为VizDoom环境实现了</a>各种价值学习架构，并检验了它们在最大化回报方面的表现。简而言之，这些包括:</p><ul class=""><li id="f164" class="lx ly it lc b ld lz lg ma lj mb ln mc lr md lv me mf mg mh bi translated">深度Q学习 (DQN)</li><li id="e07e" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated"><a class="ae lw" rel="noopener" target="_blank" href="/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4">双深度Q学习</a> (DDQN)</li><li id="0d15" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated"><a class="ae lw" rel="noopener" target="_blank" href="/building-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f">决斗深度Q学习</a> (DuelDQN)</li></ul><p id="f7df" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">总的来说，vanilla Deep Q-learning是一种高度灵活和响应性的在线强化学习方法，它利用快速的场景内更新来估计环境中的状态-动作(Q)值，以便最大化回报。Q-learning可以被认为是一种策略外的TD方法，其中该算法旨在选择独立于当前遵循的策略的最高值的状态-动作对，并且已经与OpenAI Atari健身房环境的许多原始突破相关联。</p><p id="75d8" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">相比之下，双重深度Q学习改进通过使用双重网络设置将动作选择与Q值目标计算分离，解决了在DQN观察到的状态-动作值高估的问题，这是之前在训练中观察到的普遍问题。类似地，due Deep Q-learning通过将单个输出状态-动作流分成单独的价值和优势流，提高了模型在训练期间的概括能力，允许代理学习专注于单独的目标以提高性能。</p><p id="effd" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">在我们的DQN系列文章的最后一篇中，我们将把迄今为止我们所学的所有内容结合到一个单一的复合方法中——决斗双深度Q学习(DuelDDQN)。通过结合我们以前模型的所有优势，我们将致力于进一步提高我们的代理在VizDoom环境中的收敛性。</p><h1 id="33dd" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">实施</strong></h1><p id="98b3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">我们将在与上一篇文章<em class="mq">保卫防线</em>相同的多目标条件下，在相同的VizDoomgym场景中实现我们的方法。</strong>环境的一些特征包括:</p><ul class=""><li id="844d" class="lx ly it lc b ld lz lg ma lj mb ln mc lr md lv me mf mg mh bi translated">一个3的动作空间:开火，左转，右转。不允许扫射。</li><li id="3a60" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">向玩家发射火球的棕色怪物，命中率为100%。</li><li id="7077" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">试图以之字形靠近来咬玩家的粉红色怪物。</li><li id="affb" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">重生的怪物可以承受更多伤害。</li><li id="49d7" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">杀死一个怪物+1点。</li><li id="2fae" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">-死了得1分。</li></ul><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/bd2d054f1275a6dc1a0ace9ea9b80b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/0*MV4aGLCMBxhJAkBj.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">“防线方案”的初始状态</p></figure><p id="f4ec" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">回想一下，在<a class="ae lw" rel="noopener" target="_blank" href="/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2">我们最初的DQN实现</a>中，我们已经利用了两个并发网络——一个用于行动选择的评估网络，以及一个定期更新的目标网络，以确保生成的TD目标是固定的。我们可以利用这个现有的设置来构建我们的DuelDDQN架构，而无需初始化更多的网络。</p><p id="3e84" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">请注意，由于两个网络定期更新彼此的权重，因此这两个模型仍然是部分耦合的，但重要的是，动作选择和Q值评估是由在特定时间步长不共享同一组a权重的独立网络完成的。</p><p id="18af" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">我们的Google协作实现是利用Pytorch用Python编写的，可以在<a class="ae lw" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> GradientCrescent Github上找到。</a>我们的方法基于泰伯优秀强化学习<a class="ae lw" href="https://www.manning.com/livevideo/reinforcement-learning-in-motion" rel="noopener ugc nofollow" target="_blank">课程</a>中详述的方法。由于我们的DDQN实现类似于我们之前的普通DQN实现，所以整个高级工作流是共享的，这里不再重复<a class="ae lw" rel="noopener" target="_blank" href="/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2"/>。</p><p id="966c" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">让我们从导入所有必需的包开始，包括OpenAI和Vizdoomgym环境。我们还将安装火炬视觉所需的AV包，我们将使用它进行可视化。请注意，安装完成后必须重新启动运行时。</p><pre class="ms mt mu mv gt nd ne nf ng aw nh bi"><span id="cb2b" class="ni kj it ne b gy nj nk l nl nm">#Visualization cobe for running within Colab<br/>!sudo apt-get update<br/>!sudo apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev libopenal-dev timidity libwildmidi-dev unzip# Boost libraries</span><span id="f4dc" class="ni kj it ne b gy nn nk l nl nm">!sudo apt-get install libboost-all-dev# Lua binding dependencies<br/>!apt-get install liblua5.1-dev<br/>!sudo apt-get install cmake libboost-all-dev libgtk2.0-dev libsdl2-dev python-numpy git<br/>!git clone <a class="ae lw" href="https://github.com/shakenes/vizdoomgym.git" rel="noopener ugc nofollow" target="_blank">https://github.com/shakenes/vizdoomgym.git</a><br/>!python3 -m pip install -e vizdoomgym/</span><span id="6333" class="ni kj it ne b gy nn nk l nl nm">!pip install av</span></pre><p id="fbb1" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">接下来，我们初始化我们的环境场景，检查观察空间和动作空间，并可视化我们的环境。</p><pre class="ms mt mu mv gt nd ne nf ng aw nh bi"><span id="d4ff" class="ni kj it ne b gy nj nk l nl nm">import gym<br/>import vizdoomgymenv = gym.make('VizdoomDefendLine-v0')<br/>n_outputs = env.action_space.n<br/>print(n_outputs)observation = env.reset()<br/>import matplotlib.pyplot as plt</span><span id="233b" class="ni kj it ne b gy nn nk l nl nm">for i in range(22):<br/>  <br/>  if i &gt; 20:<br/>    print(observation.shape)<br/>    plt.imshow(observation)<br/>    plt.show()<br/>observation, _, _, _ = env.step(1)</span></pre><p id="0d13" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">接下来，我们将定义预处理包装器。这些类继承自OpenAI gym基类，覆盖了它们的方法和变量，以便隐式地提供所有必要的预处理。我们将开始定义一个包装器来重复许多帧的每个动作，并执行元素方式的最大值以增加任何动作的强度。您会注意到一些三级参数，如<em class="mq"> fire_first </em>和<em class="mq">no _ ops</em>——这些是特定于环境的，在Vizdoomgym中对我们没有影响。</p><pre class="ms mt mu mv gt nd ne nf ng aw nh bi"><span id="ed6e" class="ni kj it ne b gy nj nk l nl nm">class RepeatActionAndMaxFrame(gym.Wrapper):<br/>  #input: environment, repeat<br/>  #init frame buffer as an array of zeros in shape 2 x the obs space<br/>    def __init__(self, env=None, repeat=4, clip_reward=False, no_ops=0,<br/>                 fire_first=False):<br/>        super(RepeatActionAndMaxFrame, self).__init__(env)<br/>        self.repeat = repeat<br/>        self.shape = env.observation_space.low.shape<br/>        self.frame_buffer = np.zeros_like((2, self.shape))<br/>        self.clip_reward = clip_reward<br/>        self.no_ops = no_ops<br/>        self.fire_first = fire_first<br/>  def step(self, action):<br/>        t_reward = 0.0<br/>        done = False<br/>        for i in range(self.repeat):<br/>            obs, reward, done, info = self.env.step(action)<br/>            if self.clip_reward:<br/>                reward = np.clip(np.array([reward]), -1, 1)[0]<br/>            t_reward += reward<br/>            idx = i % 2<br/>            self.frame_buffer[idx] = obs<br/>            if done:<br/>                break<br/>        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])<br/>        return max_frame, t_reward, done, info<br/>  def reset(self):<br/>        obs = self.env.reset()<br/>        no_ops = np.random.randint(self.no_ops)+1 if self.no_ops &gt; 0    else 0<br/>        for _ in range(no_ops):<br/>            _, _, done, _ = self.env.step(0)<br/>            if done:<br/>                self.env.reset()<br/>        <br/>        if self.fire_first:<br/>            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'<br/>            obs, _, _, _ = self.env.step(1)<br/>        self.frame_buffer = np.zeros_like((2,self.shape))<br/>        self.frame_buffer[0] = obs<br/>    return obs</span></pre><p id="b116" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">接下来，我们为我们的观察定义预处理函数。我们将使我们的环境对称，将它转换到标准化的盒子空间，将通道整数交换到张量的前面，并将其从原始(320，480)分辨率调整到(84，84)区域。我们也将我们的环境灰度化，并通过除以一个常数来归一化整个图像。</p><pre class="ms mt mu mv gt nd ne nf ng aw nh bi"><span id="ab82" class="ni kj it ne b gy nj nk l nl nm">class PreprocessFrame(gym.ObservationWrapper):<br/>  #set shape by swapping channels axis<br/> #set observation space to new shape using gym.spaces.Box (0 to 1.0)<br/>    def __init__(self, shape, env=None):<br/>        super(PreprocessFrame, self).__init__(env)<br/>        self.shape = (shape[2], shape[0], shape[1])<br/>        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,<br/>                                    shape=self.shape, dtype=np.float32)<br/>   def observation(self, obs):<br/>        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)<br/>        resized_screen = cv2.resize(new_frame, self.shape[1:],<br/>                                    interpolation=cv2.INTER_AREA)<br/>        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)<br/>        new_obs = new_obs / 255.0<br/>   return new_obs</span></pre><p id="67e2" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">接下来，我们创建一个包装器来处理帧堆叠。这里的目标是通过将几个帧堆叠在一起作为单个批次，帮助从堆叠帧中捕捉运动和方向。这样，我们可以捕捉环境中元素的位置、平移、速度和加速度。通过堆叠，我们的输入采用(4，84，84，1)的形状。</p><pre class="ms mt mu mv gt nd ne nf ng aw nh bi"><span id="74fa" class="ni kj it ne b gy nj nk l nl nm">class StackFrames(gym.ObservationWrapper):<br/>  #init the new obs space (gym.spaces.Box) low &amp; high bounds as repeat of n_steps. These should have been defined for vizdooom<br/>  <br/>  #Create a return a stack of observations<br/>    def __init__(self, env, repeat):<br/>        super(StackFrames, self).__init__(env)<br/>        self.observation_space = gym.spaces.Box( env.observation_space.low.repeat(repeat, axis=0),<br/>                              env.observation_space.high.repeat(repeat, axis=0),<br/>                            dtype=np.float32)<br/>        self.stack = collections.deque(maxlen=repeat)<br/>    def reset(self):<br/>        self.stack.clear()<br/>        observation = self.env.reset()<br/>        for _ in range(self.stack.maxlen):<br/>            self.stack.append(observation)<br/>        return  np.array(self.stack).reshape(self.observation_space.low.shape)<br/>    def observation(self, observation):<br/>        self.stack.append(observation)<br/>    return np.array(self.stack).reshape(self.observation_space.low.shape)</span></pre><p id="2e2b" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">最后，在返回最终环境供使用之前，我们将所有的包装器绑定到一个单独的<em class="mq"> make_env() </em>方法中。</p><pre class="ms mt mu mv gt nd ne nf ng aw nh bi"><span id="6ea2" class="ni kj it ne b gy nj nk l nl nm">def make_env(env_name, shape=(84,84,1), repeat=4, clip_rewards=False,<br/>             no_ops=0, fire_first=False):<br/>    env = gym.make(env_name)<br/>    env = PreprocessFrame(shape, env)<br/>    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards, no_ops, fire_first)<br/>    <br/>    env = StackFrames(env, repeat)<br/>    return env</span></pre><p id="cd3e" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">接下来，让我们定义模型中的决斗部分，一个深度Q网络，具有两个输出流。这基本上是一个三层卷积网络，它采用预处理的输入观测值，将生成的展平输出馈送到一个全连接层，然后将输出分成价值流(单节点输出)和优势流(节点输出对应于环境中的动作数量)。</p><p id="4e27" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">请注意，这里没有激活层，因为激活层的存在会导致二进制输出分布。我们的损失是我们当前状态-动作的估计Q值和我们预测的状态-动作值的平方差。然后，我们附上RMSProp优化器，以尽量减少我们在培训期间的损失。</p><pre class="ms mt mu mv gt nd ne nf ng aw nh bi"><span id="d4a2" class="ni kj it ne b gy nj nk l nl nm">import os<br/>import torch as T<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>import numpy as np<br/>class DeepQNetwork(nn.Module):<br/>    def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):<br/>        super(DeepQNetwork, self).__init__()<br/>        self.checkpoint_dir = chkpt_dir<br/>        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)<br/>        self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)<br/>        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)<br/>        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)<br/>        fc_input_dims = self.calculate_conv_output_dims(input_dims)<br/>        self.fc1 = nn.Linear(fc_input_dims,1024)<br/>        self.fc2 = nn.Linear(1024, 512)<br/>        #Here we split the linear layer into the State and Advantage streams<br/>        self.V = nn.Linear(512, 1)<br/>        self.A = nn.Linear(512, n_actions)<br/>        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)<br/>        self.loss = nn.MSELoss()<br/>        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')<br/>        self.to(self.device)<br/>    def calculate_conv_output_dims(self, input_dims):<br/>        state = T.zeros(1, *input_dims)<br/>        dims = self.conv1(state)<br/>        dims = self.conv2(dims)<br/>        dims = self.conv3(dims)<br/>        return int(np.prod(dims.size()))<br/>    def forward(self, state):<br/>        conv1 = F.relu(self.conv1(state))<br/>        conv2 = F.relu(self.conv2(conv1))<br/>        conv3 = F.relu(self.conv3(conv2))<br/>        # conv3 shape is BS x n_filters x H x W<br/>        conv_state = conv3.view(conv3.size()[0], -1)<br/>        # conv_state shape is BS x (n_filters * H * W)<br/>        flat1 = F.relu(self.fc1(conv_state))<br/>        flat2 = F.relu(self.fc2(flat1))<br/>        V = self.V(flat2)<br/>        A = self.A(flat2)<br/>        return V, A<br/>     def save_checkpoint(self):<br/>        print('... saving checkpoint ...')<br/>        T.save(self.state_dict(), self.checkpoint_file)<br/>     def load_checkpoint(self):<br/>        print('... loading checkpoint ...')<br/>        self.load_state_dict(T.load(self.checkpoint_file))</span></pre><p id="7f65" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">接下来，我们将定义我们的代理，它遵循我们之前的<a class="ae lw" rel="noopener" target="_blank" href="/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4"> DDQN实现</a>和<a class="ae lw" rel="noopener" target="_blank" href="/building-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f"> DuelDQN实现</a>。我们的代理正在使用一个勘探率递减的ε贪婪策略，以便随着时间的推移最大化开发。为了学会预测使我们的累积奖励最大化的状态-行动-值，我们的代理人将使用通过抽样存储的记忆获得的贴现的未来奖励。</p><p id="309f" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">您会注意到，作为代理的一部分，我们初始化了DQN的两个副本，并使用方法将原始网络的权重参数复制到目标网络中。虽然我们的常规方法利用这种设置来生成固定的TD目标，但我们的DuelDDQN方法将扩展到这一点:</p><ul class=""><li id="b208" class="lx ly it lc b ld lz lg ma lj mb ln mc lr md lv me mf mg mh bi translated">从重放存储器中检索状态、动作、奖励和下一状态(sar)。</li><li id="2dbf" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">评估网络用于生成当前状态的优势(<em class="mq"> A_s </em>)和状态(<em class="mq"> V_s </em>)值。它还用于为下一个状态生成相应的值。</li><li id="db08" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">目标网络用于创建下一个状态的优势(<em class="mq"> A_s_ </em>)和状态(<em class="mq"> V_s_ </em>)值。</li><li id="1ce8" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">当前状态的预测Q值(<em class="mq"> q_pred </em>)是通过对当前状态的优势和状态值求和，并使用评估网络的输出流减去用于归一化的当前状态优势值的平均值而生成的。使用评估网络的输出流，下一状态re的评估Q值(<em class="mq"> q_eval </em>)也以类似的方式创建。使用<em class="mq"> argmax() </em>函数获得下一个状态的最大可能性动作。请注意，<em class="mq"> q_eval </em>在这里的作用只是关注于动作选择，与明确地参与目标计算无关。</li><li id="29ff" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">使用目标网络的输出流，以类似的方式为下一状态(<em class="mq"> q_next </em>)创建目标Q值。<em class="mq"> q_next </em>的作用是专注于目标计算，不参与动作选择。</li><li id="8c97" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">通过评估网络识别的<em class="mq"> max_actions </em>将当前状态中的回报与从下一状态的目标网络导出的Q值相结合，计算当前状态的TD-target。</li><li id="24fd" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">通过将TD目标与当前状态Q值进行比较来计算损失函数，然后将其用于训练网络。</li></ul><pre class="ms mt mu mv gt nd ne nf ng aw nh bi"><span id="a2ce" class="ni kj it ne b gy nj nk l nl nm">import numpy as np<br/>import torch as T<br/>#from deep_q_network import DeepQNetwork<br/>#from replay_memory import ReplayBuffer</span><span id="7ea6" class="ni kj it ne b gy nn nk l nl nm">#Combining the value stream splitting of a DuelDQN with the update approach of a DDQN<br/>class DuelDDQNAgent(object):<br/>    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,<br/>                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,<br/>                 replace=1000, algo=None, env_name=None, chkpt_dir='tmp/dqn'):<br/>        self.gamma = gamma<br/>        self.epsilon = epsilon<br/>        self.lr = lr<br/>        self.n_actions = n_actions<br/>        self.input_dims = input_dims<br/>        self.batch_size = batch_size<br/>        self.eps_min = eps_min<br/>        self.eps_dec = eps_dec<br/>        self.replace_target_cnt = replace<br/>        self.algo = algo<br/>        self.env_name = env_name<br/>        self.chkpt_dir = chkpt_dir<br/>        self.action_space = [i for i in range(n_actions)]<br/>        self.learn_step_counter = 0</span><span id="4a8f" class="ni kj it ne b gy nn nk l nl nm">self.memory = ReplayBuffer(mem_size, input_dims, n_actions)</span><span id="6d4b" class="ni kj it ne b gy nn nk l nl nm">self.q_eval = DeepQNetwork(self.lr, self.n_actions,<br/>                                    input_dims=self.input_dims,<br/>                                    name=self.env_name+'_'+self.algo+'_q_eval',<br/>                                    chkpt_dir=self.chkpt_dir)</span><span id="fe7d" class="ni kj it ne b gy nn nk l nl nm">self.q_next = DeepQNetwork(self.lr, self.n_actions,<br/>                                    input_dims=self.input_dims,<br/>                                    name=self.env_name+'_'+self.algo+'_q_next',<br/>                                    chkpt_dir=self.chkpt_dir)</span><span id="b7a9" class="ni kj it ne b gy nn nk l nl nm">#Epsilon greedy action selection<br/>    def choose_action(self, observation):<br/>        if np.random.random() &gt; self.epsilon:<br/>          # Add dimension to observation to match input_dims x batch_size by placing in list, then converting to tensor<br/>            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)<br/>            #As our forward function now has both state and advantage, fetch latter for actio selection<br/>            _, advantage = self.q_eval.forward(state)<br/>            action = T.argmax(advantage).item()<br/>        else:<br/>            action = np.random.choice(self.action_space)</span><span id="f869" class="ni kj it ne b gy nn nk l nl nm">return action</span><span id="056b" class="ni kj it ne b gy nn nk l nl nm">def store_transition(self, state, action, reward, state_, done):<br/>        self.memory.store_transition(state, action, reward, state_, done)</span><span id="31fa" class="ni kj it ne b gy nn nk l nl nm">def sample_memory(self):<br/>        state, action, reward, new_state, done = \<br/>                                self.memory.sample_buffer(self.batch_size)</span><span id="5bc4" class="ni kj it ne b gy nn nk l nl nm">states = T.tensor(state).to(self.q_eval.device)<br/>        rewards = T.tensor(reward).to(self.q_eval.device)<br/>        dones = T.tensor(done).to(self.q_eval.device)<br/>        actions = T.tensor(action).to(self.q_eval.device)<br/>        states_ = T.tensor(new_state).to(self.q_eval.device)</span><span id="ee51" class="ni kj it ne b gy nn nk l nl nm">return states, actions, rewards, states_, dones</span><span id="bf81" class="ni kj it ne b gy nn nk l nl nm">def replace_target_network(self):<br/>        if self.learn_step_counter % self.replace_target_cnt == 0:<br/>            self.q_next.load_state_dict(self.q_eval.state_dict())</span><span id="073f" class="ni kj it ne b gy nn nk l nl nm">def decrement_epsilon(self):<br/>        self.epsilon = self.epsilon - self.eps_dec \<br/>                           if self.epsilon &gt; self.eps_min else self.eps_min</span><span id="eadf" class="ni kj it ne b gy nn nk l nl nm">def save_models(self):<br/>        self.q_eval.save_checkpoint()<br/>        self.q_next.save_checkpoint()</span><span id="294b" class="ni kj it ne b gy nn nk l nl nm">def load_models(self):<br/>        self.q_eval.load_checkpoint()<br/>        self.q_next.load_checkpoint()<br/>    #Make sure you understand this line by line<br/>    #For DDQN main difference is here - Consult the lecture to gain a stronger understanding of why things differe here<br/>    def learn(self):</span><span id="aec1" class="ni kj it ne b gy nn nk l nl nm">#First check if memory is even big enough<br/>        if self.memory.mem_cntr &lt; self.batch_size:<br/>            return</span><span id="b3b8" class="ni kj it ne b gy nn nk l nl nm">self.q_eval.optimizer.zero_grad()</span><span id="0b2a" class="ni kj it ne b gy nn nk l nl nm">#Replace target network if appropriate<br/>        self.replace_target_network()</span><span id="980c" class="ni kj it ne b gy nn nk l nl nm">states, actions, rewards, states_, dones = self.sample_memory()<br/>        #Fetch states and advantage actions for current state using eval network<br/>        #Also fetch the same for next state using target network<br/>        V_s, A_s = self.q_eval.forward(states)<br/>        V_s_, A_s_ = self.q_next.forward(states_)<br/>        <br/>        #Eval network calculation of next state V and A<br/>        V_s_eval, A_s_eval = self.q_eval.forward(states_)</span><span id="3f07" class="ni kj it ne b gy nn nk l nl nm">#Indices for matrix multiplication<br/>        indices = np.arange(self.batch_size)</span><span id="cfe3" class="ni kj it ne b gy nn nk l nl nm">#Calculate current state Q-values and next state max Q-value by aggregation, subtracting constant advantage mean<br/>        #Along first dimension, which is action dimension, keeping original matrix dimensions<br/>        <br/>        #recall [indices,actions] is used to maintain array shape of (batch_size) instead of (batch_size,actions)<br/>        #Essentilly by adding a a batch index to our vector array we ensure that calculated Q_pred is not tabular, but applicable for a batch update<br/>        q_pred = T.add(V_s,<br/>                        (A_s - A_s.mean(dim=1, keepdim=True)))[indices, actions]<br/>        #For q_next, fetch max along the action dimension. 0th element, because max returns a tuple,<br/>        #of which 0th position are values and 1st position the indices.<br/>        q_next = T.add(V_s_,<br/>                        (A_s_ - A_s_.mean(dim=1, keepdim=True)))<br/>        <br/>        #QEval q-values for DDQN<br/>        q_eval = T.add(V_s_eval, (A_s_eval - A_s_eval.mean(dim=1,keepdim=True)))<br/>        max_actions = T.argmax(q_eval, dim=1)<br/>        q_next[dones] = 0.0<br/>        #Build your target using the current state reward and q_next, DDQN setup<br/>        q_target = rewards + self.gamma*q_next[indices, max_actions]</span><span id="1d4a" class="ni kj it ne b gy nn nk l nl nm">loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)<br/>        loss.backward()<br/>        self.q_eval.optimizer.step()<br/>        self.learn_step_counter += 1</span><span id="8114" class="ni kj it ne b gy nn nk l nl nm">self.decrement_epsilon()</span></pre><p id="4656" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">定义了所有支持代码后，让我们运行主训练循环。我们已经在最初的总结中定义了大部分，但是让我们为后代回忆一下。</p><ul class=""><li id="0d81" class="lx ly it lc b ld lz lg ma lj mb ln mc lr md lv me mf mg mh bi translated">对于训练集的每一步，在使用ε-贪婪策略选择下一个动作之前，我们将输入图像堆栈输入到我们的网络中，以生成可用动作的概率分布。</li><li id="953e" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">然后，我们将它输入到网络中，获取下一个状态和相应奖励的信息，并将其存储到我们的缓冲区中。我们更新我们的堆栈，并通过一些预定义的步骤重复这一过程。</li><li id="4100" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">在一集的结尾，我们将下一个状态输入到我们的网络中，以便获得下一个动作。我们还通过对当前奖励进行贴现来计算下一个奖励。</li><li id="373d" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">我们使用去耦网络通过上述Q学习更新函数生成我们的目标y值，并训练我们的actor网络。</li><li id="764f" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">通过最小化训练损失，我们更新网络权重参数，以便为下一个策略输出改进的状态-动作值。</li><li id="eb5b" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">我们通过跟踪模型的平均得分(在100个训练步骤中测量)来评估模型。</li></ul><pre class="ms mt mu mv gt nd ne nf ng aw nh bi"><span id="560c" class="ni kj it ne b gy nj nk l nl nm"># Main thread</span><span id="96c7" class="ni kj it ne b gy nn nk l nl nm">env = make_env('DefendTheLine-v0')<br/>best_score = -np.inf<br/>load_checkpoint = False<br/>n_games = 20000</span><span id="3b85" class="ni kj it ne b gy nn nk l nl nm">agent = DuelDDQNAgent(gamma=0.99, epsilon=1.0, lr=0.001,input_dims=(env.observation_space.shape),n_actions=env.action_space.n, mem_size=5000, eps_min=0.15,batch_size=32, replace=1000, eps_dec=1e-5,chkpt_dir='/content/', algo='DuelDDQNAgent',env_name='vizdoogym')</span><span id="9658" class="ni kj it ne b gy nn nk l nl nm">if load_checkpoint:<br/>  agent.load_models()</span><span id="d8d6" class="ni kj it ne b gy nn nk l nl nm">fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) +'_'+ str(n_games) + 'games'<br/>figure_file = 'plots/' + fname + '.png'</span><span id="3810" class="ni kj it ne b gy nn nk l nl nm">n_steps = 0<br/>scores, eps_history, steps_array = [], [], []</span><span id="5797" class="ni kj it ne b gy nn nk l nl nm">for i in range(n_games):<br/>  done = False<br/>  observation = env.reset()</span><span id="8537" class="ni kj it ne b gy nn nk l nl nm">score = 0<br/>  while not done:<br/>    action = agent.choose_action(observation)<br/>    observation_, reward, done, info = env.step(action)<br/>    score += reward</span><span id="76e4" class="ni kj it ne b gy nn nk l nl nm">if not load_checkpoint:<br/>      agent.store_transition(observation, action,reward, observation_, int(done))<br/>      agent.learn()<br/>    observation = observation_<br/>    n_steps += 1</span><span id="a052" class="ni kj it ne b gy nn nk l nl nm">scores.append(score)<br/>  steps_array.append(n_steps)</span><span id="4923" class="ni kj it ne b gy nn nk l nl nm">avg_score = np.mean(scores[-100:])</span><span id="02f3" class="ni kj it ne b gy nn nk l nl nm">if avg_score &gt; best_score:<br/>    best_score = avg_score<br/>      <br/>    <br/>    print('Checkpoint saved at episode ', i)<br/>    agent.save_models()</span><span id="08a4" class="ni kj it ne b gy nn nk l nl nm">print('Episode: ', i,'Score: ', score,' Average score: %.2f' % avg_score, 'Best average: %.2f' % best_score,'Epsilon: %.2f' % agent.epsilon, 'Steps:', n_steps)</span><span id="dc4b" class="ni kj it ne b gy nn nk l nl nm">eps_history.append(agent.epsilon)<br/>  if load_checkpoint and n_steps &gt;= 18000:<br/>    break</span><span id="ef5c" class="ni kj it ne b gy nn nk l nl nm">#x = [i+1 for i in range(len(scores))]</span></pre><p id="a1f1" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">我们绘制了500集和1000集代理人的平均得分和我们的epsilon值。可以观察到，DuelDDQN模型的收敛性明显优于基线DQN和DDQN模型的收敛性，甚至稍微改善了DuelDQN模型的性能。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c7cff45063e9ea1d536255d337d0bea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*KrebkGY1iehDC0W8y5E_ZA.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">500集后我们经纪人的奖励分配。</p></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/b5fc528348fbf3c22e91b3e9831b2c85.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*G-6ZH7sMP1cMQ9ifVkt_NQ.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">1000集后我们经纪人的奖励分配。</p></figure><p id="5d5a" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">我们可以想象我们的代理人在500集以下的表现。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nq nr l"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">500集的代理性能。</p></figure><p id="d8f1" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">回想一下，我们以前的代理遭受了陷入局部最小值的特殊问题，依赖于怪物之间的友好射击作为主要的得分策略，而不是代理的主动性。我们的DuelDDQN代理没有什么不同，比以前的模型更快地达到这个最小值。解决这个问题要么需要改变环境(用扫射代替转弯可能是一种选择)，加速学习以避免模式崩溃(如通过动量)，要么通过奖励工程——例如，根据生存的持续时间或怪物从代理人的武器中受到的原始伤害增加相应的奖励。</p><p id="923c" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">那么，通过我们的系列课程，我们学到了什么？</p><ul class=""><li id="5561" class="lx ly it lc b ld lz lg ma lj mb ln mc lr md lv me mf mg mh bi translated">像DQN这样的价值学习架构为RL代理开发提供了一个很好的基线。</li><li id="e129" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">这种架构的性能可以通过对学习过程的设计进行小的修改来提高。</li><li id="a170" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">鼓励模型探索，特别是在高水平的收敛上，是学习过程的一把双刃剑。</li><li id="bc17" class="lx ly it lc b ld mi lg mj lj mk ln ml lr mm lv me mf mg mh bi translated">就实现目标而言，一个模型的表现只能和它的回报函数的设计一样好，这必须通过观察仔细分析或推断。</li></ul><p id="cd6b" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">这就结束了决斗双深度Q学习的实现，以及我们关于DQN的系列。在我们的下一篇文章中，我们将暂时停止强化学习，尝试一些特定的生成图像处理技术。</p><p id="c3e4" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">我们希望你喜欢这篇文章，并希望你查看GradientCrescent上的许多其他文章，涵盖人工智能的应用和理论方面。为了保持对<a class="ae lw" href="https://medium.com/@adrianitsaxu" rel="noopener"> GradientCrescent </a>的最新更新，请考虑关注该出版物并关注我们的<a class="ae lw" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> Github </a>资源库</p><h1 id="fff1" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">来源</h1><p id="8301" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">萨顿等人。al，“强化学习”</p><p id="f390" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">塔博尔，“运动中的强化学习”</p><p id="6835" class="pw-post-body-paragraph la lb it lc b ld lz ju lf lg ma jx li lj mn ll lm ln mo lp lq lr mp lt lu lv im bi translated">西蒙尼尼，<a class="ae lw" href="https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/" rel="noopener ugc nofollow" target="_blank">“深度Q学习的改进* </a></p></div></div>    
</body>
</html>