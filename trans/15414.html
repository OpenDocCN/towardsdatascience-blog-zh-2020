<html>
<head>
<title>Evolution of Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理的发展</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evolution-of-natural-language-processing-8e4532211cfe?source=collection_archive---------21-----------------------#2020-10-23">https://towardsdatascience.com/evolution-of-natural-language-processing-8e4532211cfe?source=collection_archive---------21-----------------------#2020-10-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="cb7f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="0233" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">对过去十年自然语言处理的直观视觉解释</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e1e4f508afc914d298af941053ea3743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*30QL2zsxvANkOpvT"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae lh" href="https://unsplash.com/@owlixir?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Thyla Jane </a>拍摄的照片</p></figure><p id="41be" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">你所需要的只是一个关心。这是2017年一篇论文的名称，该论文将注意力作为一种独立的学习模型进行了介绍——这是我们现在在自然语言处理(NLP)领域占据主导地位的先驱。</p><p id="5489" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">变形金刚是自然语言处理领域的新锐，它们可能看起来有些抽象——但是当我们回顾自然语言处理领域过去十年的发展时，它们开始变得有意义了。</p><p id="e097" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将讨论这些发展，看看它们是如何导致今天使用的变压器的。本文并不假设您已经理解了这些概念——我们将建立一个直观的理解，而不会过于专业。</p><p id="3fc1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将涵盖:</p><pre class="ks kt ku kv gt mn mo mp mq aw mr bi"><span id="6bbd" class="ms mt it mo b gy mu mv l mw mx"><strong class="mo jd">Natural Language Neural Nets</strong><br/>  - Recurrence<br/>  - Vanishing Gradients<br/>  - Long-Short Term Memory<br/>  - Attention</span><span id="282d" class="ms mt it mo b gy my mv l mw mx"><strong class="mo jd">Attention is All You Need</strong><br/>  - Self-Attention<br/>  - Multi-Head Attention<br/>  - Positional Encoding<br/>  - Transformers</span></pre></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="07e6" class="ng mt it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">自然语言神经网络</h1><p id="d387" class="pw-post-body-paragraph li lj it lk b ll nx kd ln lo ny kg lq lr nz lt lu lv oa lx ly lz ob mb mc md im bi translated">随着Mikolov等人在2013年发表的介绍word2vec的论文[2]，NLP真正爆发了。这引入了一种通过使用单词向量来表示单词之间的相似性和关系的方法。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/0f20e61c3beefc0dd6886345b21795b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z8PxJ6AN9nseUrGxmwjKPg.png"/></div></div></figure><p id="d436" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些初始单词向量包含50-100个值的维度。这些向量的编码机制意味着相似的单词将被分组在一起(周一、周二等)，向量空间上的计算可以产生真正深刻的关系。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/2c3b520ca13e5cbcef86a47395f70a56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mVbIoksbDtsKWsLWlkoSAA.png"/></div></div></figure><p id="2044" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一个众所周知的例子是，取<em class="oe">国王</em>的向量，减去向量<em class="oe">男人</em>，加上向量<em class="oe">女人</em>，导致最近的数据点是<em class="oe">王后</em>。</p><h2 id="d385" class="ms mt it bd nh of og dn nl oh oi dp np lr oj ok nr lv ol om nt lz on oo nv iz bi translated">重现</h2><p id="27b8" class="pw-post-body-paragraph li lj it lk b ll nx kd ln lo ny kg lq lr nz lt lu lv oa lx ly lz ob mb mc md im bi translated">在NLP的繁荣时期，递归神经网络(RNN)迅速成为大多数语言应用程序的最爱。rnn非常适合语言，这要感谢它们的<em class="oe">重现</em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/ffdc41815ccf897895804b80ce57cf07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Y-Oxwp3wEvhPX2XSx-Gog.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">递归神经网络单元将消耗第一个时间步<strong class="bd oq"><em class="or"/></strong>，将其输出状态传递给下一个时间步<strong class="bd oq">【快速】</strong>——该递归过程持续特定长度的时间步(序列长度)。</p></figure><p id="cf0d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种循环允许神经网络考虑单词的顺序及其对前后单词的影响，从而更好地表达人类语言的细微差别。</p><p id="3d81" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然我们直到2013年才看到它们的普遍使用，但RNNs的概念和方法在20世纪80年代的几篇论文中都有讨论[2]，[3]。</p><h2 id="5aca" class="ms mt it bd nh of og dn nl oh oi dp np lr oj ok nr lv ol om nt lz on oo nv iz bi translated">消失渐变</h2><p id="37a3" class="pw-post-body-paragraph li lj it lk b ll nx kd ln lo ny kg lq lr nz lt lu lv oa lx ly lz ob mb mc md im bi translated">rnn带来了他们的问题，主要是消失梯度问题。这些网络的循环意味着它们本质上是非常深的网络，具有许多包含输入数据和神经元权重之间的运算的点。</p><p id="db05" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当计算网络的误差并使用它来更新网络权重时，我们一个接一个地通过网络更新权重。</p><p id="f631" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果更新梯度是一个小数字，我们乘以一个越来越小的数字——这意味着整个网络要么需要很长时间来训练，要么根本不起作用。</p><p id="e4c6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一方面，如果我们的权重循环值太高，我们就会遭遇爆炸梯度问题。这里，网络权重将振荡，而不学习任何有意义的表示。</p><h2 id="bf4d" class="ms mt it bd nh of og dn nl oh oi dp np lr oj ok nr lv ol om nt lz on oo nv iz bi translated">长短期记忆</h2><p id="c060" class="pw-post-body-paragraph li lj it lk b ll nx kd ln lo ny kg lq lr nz lt lu lv oa lx ly lz ob mb mc md im bi translated">随着长短期记忆(LSTM)单元的引入，消失梯度问题得到了解决。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/ecd3238019df86fde5a75c7215195932.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kT1D0oH-OxcsUj0r6CiScA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">LSTM单位引入了一种更稳定的信息通道——细胞状态，如上图黑色所示。</p></figure><p id="ca28" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">LSTMs引入了一个额外的信息流，沿着时间状态链，通过“门”控制最少数量的转换。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/994aaef3270772182252370d4fca4b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y7dDgK9FhuEt6069qtJMAQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">单元状态允许信息以较少的转换从早期状态传递到后期状态。</p></figure><p id="744b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这允许通过允许来自序列中更早的信息被保留并应用于序列中更晚的状态来学习长期依赖性。</p><h2 id="864c" class="ms mt it bd nh of og dn nl oh oi dp np lr oj ok nr lv ol om nt lz on oo nv iz bi translated">注意力</h2><p id="989a" class="pw-post-body-paragraph li lj it lk b ll nx kd ln lo ny kg lq lr nz lt lu lv oa lx ly lz ob mb mc md im bi translated">很快，循环的编码器-解码器模型被附加的隐藏状态和神经网络层所补充——这些产生了注意力机制。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/9c08341714a1720fa683a6527b7e48a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2RoY3m1ikaq35ysXU-S5w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">注意编解码器LSTMs。</p></figure><p id="b168" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">添加编码器-解码器网络允许模型的输出层不仅接收RNN单元的最终状态，而且还接收来自输入层的每个状态的信息，从而创建一种“注意”机制。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/d17ef08f242ae2fcb46142c28afb4f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LQP1tXf-boH83vGb8WhG_A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">英法翻译任务中编码和解码神经元间的注意。图像源[3]。</p></figure><p id="9f68" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用这种方法，我们发现编码器和解码器状态之间的相似性将导致更高的权重，产生类似于上面法语翻译图像的结果。</p><p id="5e53" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于这种编码器-解码器实现，在关注操作中使用了三个张量，即查询、关键字和值。在每个时间步长，从解码器的隐藏状态中提取查询，评估该查询和键值张量之间的对齐，以产生上下文向量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/2f39e6554ca926f6e6704a5ac20b897a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Jy-6XkhyhVSjaMdYn7H3A.png"/></div></div></figure><p id="ec26" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，上下文向量被传回解码器，用于产生该时间步长的预测。对于解码器空间中的每个时间步长，重复该过程。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="6130" class="ng mt it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">你需要的只是关注</h1><p id="0cd0" class="pw-post-body-paragraph li lj it lk b ll nx kd ln lo ny kg lq lr nz lt lu lv oa lx ly lz ob mb mc md im bi translated">正如我们在简介中所说，这一切都始于2017年的“你只需要关注”论文[5]。你可能已经猜到了，这篇论文介绍了我们不需要在注意力旁边使用这些复杂的卷积或递归神经网络的想法——事实上，注意力就是你所需要的全部。</p><h2 id="a704" class="ms mt it bd nh of og dn nl oh oi dp np lr oj ok nr lv ol om nt lz on oo nv iz bi translated">自我关注</h2><p id="279d" class="pw-post-body-paragraph li lj it lk b ll nx kd ln lo ny kg lq lr nz lt lu lv oa lx ly lz ob mb mc md im bi translated">自我关注是实现这一功能的关键因素。这意味着之前的查询来自输出解码器，而现在是直接从输入值以及T2 K ey和T4 V T5值产生。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/84e07f08ae1583941b5dae17a6e40afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lArKjROg3KFuaO_sNZGX5g.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用两个略有不同的短语自我关注。通过将<strong class="bd oq">累</strong>(左)改为<strong class="bd oq">宽</strong>(右)，注意力从<strong class="bd oq">动物</strong>转移到<strong class="bd oq">街道</strong>。来源[3]。</p></figure><p id="255a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因为<strong class="lk jd"> Q </strong> uery、<strong class="lk jd"> K </strong> ey和<strong class="lk jd">V</strong>value都是由输入产生的，所以我们能够对同一输入序列的不同部分之间的对齐进行编码。如果我们取上面的图像，我们可以看到，将最后一个词从<strong class="lk jd">累</strong>改为<strong class="lk jd">宽</strong>将注意力焦点从<strong class="lk jd">动物</strong>转移到<strong class="lk jd">街道</strong>。</p><p id="44cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这允许注意力机制对输入数据中所有单词之间的关系进行编码。</p><h2 id="fed2" class="ms mt it bd nh of og dn nl oh oi dp np lr oj ok nr lv ol om nt lz on oo nv iz bi translated">多头注意力</h2><p id="3c91" class="pw-post-body-paragraph li lj it lk b ll nx kd ln lo ny kg lq lr nz lt lu lv oa lx ly lz ob mb mc md im bi translated">注意力机制的下一个重大变化是增加了多个<em class="oe">注意力头</em>——本质上是并行执行的许多自我注意力操作，并用不同的权重进行初始化。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/f1b3e9e88f3f826d7724aeb8bdfc2667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dOMEPuuHm0iDWP1reEf95w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">多头注意是指并行处理多个注意‘头’。这些多个头的输出被连接在一起。</p></figure><p id="2856" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在没有多头关注的情况下，A. Vaswani等人的transformer模型实际上比它的许多前辈表现得更差[5]。</p><p id="0bb8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">并行机制允许模型表示相同序列的几个<em class="oe">子空间</em>。这些不同程度的注意力然后被一个线性单元连接和处理。</p><h2 id="9668" class="ms mt it bd nh of og dn nl oh oi dp np lr oj ok nr lv ol om nt lz on oo nv iz bi translated">位置编码</h2><p id="b91f" class="pw-post-body-paragraph li lj it lk b ll nx kd ln lo ny kg lq lr nz lt lu lv oa lx ly lz ob mb mc md im bi translated">变压器模型的输入不像RNNs那样是顺序的。在过去，正是这种顺序操作允许我们考虑单词的位置和顺序。</p><p id="c3bf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了保持单词的位置信息，在进入注意机制之前，向单词嵌入添加位置编码。</p><p id="6d1e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">《注意力就是你所需要的》一文中采用的方法是为嵌入维中的每一维生成不同的正弦函数。</p><p id="e22e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">还记得我们之前说过word2vec引入了用50到100维的向量来表示单词的概念吗？这里，在Vaswani等人的论文中，他们使用了相同的概念，但是表示了单词的位置。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/8e50fe657f116838a65c9ef7a8e1deac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*YqVm4d_OmlE-J17r4i2yIg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">交替位置编码值。使用字位置<strong class="bd oq"> pos </strong>，嵌入尺寸<strong class="bd oq"> i </strong>，嵌入尺寸数量<strong class="bd oq"> d_model </strong>。</p></figure><p id="5518" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是，这次不是使用ML模型计算矢量值，而是使用修改后的正弦函数计算矢量值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/53541ca27e5499b02dc3e239e561f10f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*guZylOVMGI5W4n68mxxNsQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">正弦后接余弦函数。这种正弦-余弦-正弦的交替模式对于嵌入索引中的每个增量持续。</p></figure><p id="e5d7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">向量的每个索引被分配一个交替的正弦-余弦-正弦函数(索引0是正弦，索引1是余弦)。接下来，随着索引值从零向d(嵌入维数)增加，正弦函数的频率降低。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/ab1c17d80417d13525f15f2946e8e625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SnrzoDFNyeINfDBileb2wA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用总嵌入维数为20的前五个嵌入指数的正弦函数。嵌入索引位置显示在图例中。</p></figure><p id="0861" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以从上面得到同样难以控制的正弦曲线，加上在A. Vaswani等人的研究中使用的512嵌入维数。用纸将这些绘制到<em class="oe">更容易理解的</em>热图上:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/e9130abccab894041d596aacb229950c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_0otFcN2cN-VWJLfiOJzw.png"/></div></div></figure><p id="87e8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以在较低的嵌入维度中看到较高的频率(左图)，它随着嵌入维度的增加而降低。在第24维附近，频率降低了很多，以至于我们在剩余的(交替的)正弦余弦波中不再看到任何变化。</p><p id="e5c7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些位置编码然后被添加到单词嵌入中。</p><p id="02f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="oe">顺便提一下，这意味着单词嵌入维度和位置编码维度必须匹配。</em></p><h2 id="a47c" class="ms mt it bd nh of og dn nl oh oi dp np lr oj ok nr lv ol om nt lz on oo nv iz bi translated">变形金刚</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/9946a5946fb5541fe5956206c288d727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iZXsdeCUa3htLzpIcDlSmA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">A. Vaswani等人的变压器架构。改编自同一篇论文。</p></figure><p id="bdb9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意力模型的这些变化产生了世界上第一个变压器。</p><p id="e0fb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了已经讨论过的单词嵌入、位置编码和多头自我关注操作，这个模型相当容易理解。</p><p id="ca3d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们有加法和归一化层，我们简单地将两个矩阵相加，然后归一化它们。还有普通的前馈神经网络。</p><p id="3e88" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们将张量输出到线性层。这是一个完全连接的神经网络，它映射到一个logits向量——一个大向量，其中每个索引映射到一个特定的单词，其中包含的值是每个相应单词的概率。</p><p id="813a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，softmax函数输出最高概率索引，该索引映射到我们的最高概率单词。</p><p id="0fcb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是全部(我承认，这是很多)。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="38d1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">NLP中目前最先进的技术仍然使用变压器，尽管有一些奇怪而奇妙的修改和添加。尽管如此，核心概念仍然是相同的，即使像GPT-3和伯特模型。</p><p id="0b36" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我相信，NLP的未来将和过去一样多样化，我们将在未来几年看到一些真正迷人的、坦率地说是改变世界的进步——这是一个非常令人兴奋的领域。</p><p id="19f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望这篇文章能够帮助您更好地理解transformer模型的基础知识，以及它们为什么如此强大。如果你想了解更多，我在<a class="ae lh" href="https://www.youtube.com/channel/UCv83tO5cePwHMt1952IVVHw" rel="noopener ugc nofollow" target="_blank"> YouTube上发布了编程/ML视频</a>！</p><p id="0c46" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您有任何问题、想法或建议，请通过<a class="ae lh" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中联系我们。</p><p id="1417" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">感谢阅读！</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="a8ad" class="ng mt it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">参考</h1><p id="3992" class="pw-post-body-paragraph li lj it lk b ll nx kd ln lo ny kg lq lr nz lt lu lv oa lx ly lz ob mb mc md im bi translated">[1] T. Mikolov等人，<a class="ae lh" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的有效估计</a> (2013)，ICLR</p><p id="9170" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2] D. Rumelhart等人，<a class="ae lh" href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf" rel="noopener ugc nofollow" target="_blank">通过错误传播学习内部表征</a> (1985)，ICS 8504</p><p id="6036" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3] M. Jordan，<a class="ae lh" href="https://cseweb.ucsd.edu/~gary/258/jordan-tr.pdf" rel="noopener ugc nofollow" target="_blank">串行顺序:并行分布式处理方法</a> (1986)，ICS 8604</p><p id="e06f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[4] D. Bahdanau等人，<a class="ae lh" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">通过联合学习对齐和翻译的神经机器翻译</a> (2015)，ICLR</p><p id="04d0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[5] A. Vaswani等人，<a class="ae lh" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a> (2017)，NeurIPS</p><p id="9e15" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://www.udemy.com/course/nlp-with-transformers/?couponCode=MEDIUM" rel="noopener ugc nofollow" target="_blank">🤖带变压器的NLP课程</a></p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="b243" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="oe">*所有图片均由作者提供，除非另有说明</em></p></div></div>    
</body>
</html>