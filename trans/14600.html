<html>
<head>
<title>Background removal with U²-Net</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用U -Net去除背景</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/background-removal-with-u%C2%B2-net-2819b8e77078?source=collection_archive---------27-----------------------#2020-10-08">https://towardsdatascience.com/background-removal-with-u%C2%B2-net-2819b8e77078?source=collection_archive---------27-----------------------#2020-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="adae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">去除图像的背景是许多应用程序的老问题。在我的使用中，我需要在各种环境下擦除一张居中家具照片的背景，同时保持物体本身完整。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/95fd1118e7692f8aa2f0c69e63cfbb97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQ8u8XK441DoHJ2WsMANmA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在这里，我们想保留两把椅子，同时删除灰色背景。</p></figure><p id="3f2d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然在大多数情况下，这项任务可以通过经典的计算机视觉算法来实现，如图像阈值处理(例如使用<a class="ae le" href="https://docs.opencv.org/master/d7/d4d/tutorial_py_thresholding.html" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"/></a><strong class="js iu">【1】</strong>)，但如果没有特定的预处理或后处理，一些图像可能会非常困难。如果对象具有与背景非常相似的颜色，那么由于边缘或阴影较弱，要找到一个清晰的轮廓会非常困难。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/597ae3d4323766dc63bd3b9da6a9c79c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*P0FtvRK1TldRvybIyZJM8w.jpeg"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">边缘和阴影较弱的图像示例。</p></figure><p id="72a0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">相反，如果背景是不同于对象的特征颜色，则很容易构造遮罩来去除背景。这就是<strong class="js iu">色度键控</strong>背后的原理，使用绿色和蓝色屏幕擦除背景，并用另一个场景替换，这是娱乐业广泛使用的效果。</p><p id="b240" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我没有使用绿色屏幕的奢侈，但对我来说幸运的是，由于过去十年深度学习的最新进展，这个问题正在被专门为这项任务设计的新模型再次探索。</p><p id="71ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与这个问题相关的具体子任务叫做<strong class="js iu">显著性物体检测。</strong></p><p id="591e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">听起来不错，但是显著性到底是什么意思呢？</p><p id="0e8b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里是维基百科对<strong class="js iu">显著图的定义:</strong></p><blockquote class="lg lh li"><p id="3254" class="jq jr lj js b jt ju jv jw jx jy jz ka lk kc kd ke ll kg kh ki lm kk kl km kn im bi translated">在计算机视觉中，显著图是显示每个像素的独特质量的图像。显著图的目标是简化和/或改变图像的表示，使其更有意义和更易于分析。</p></blockquote><p id="1441" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通俗地说，<strong class="js iu">显著性</strong>就是从图像的其余部分中脱颖而出的能力。显著图允许将图像的重要部分(通常是前景中的对象)与其余部分(背景)区分开来。</p><p id="5ce5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">非常适合我们的使用案例！</p><p id="d116" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为我想快速解决这个问题，因此制作自己的数据集是毫无疑问的，所以我寻找一个预训练的模型，它无需进一步微调就能满足我的需要。</p><p id="ef76" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最近针对这项任务发布了许多不同的模型，我选择了<a class="ae le" href="https://github.com/NathanUA/U-2-Net" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">U-Net</strong></a><strong class="js iu">【2】</strong>，因为根据基准测试，它表现很好，并且代码很容易修改，可以根据我自己的图像进行推断。</p><p id="42e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个名字来源于众所周知的<strong class="js iu"> Unet[3] </strong>，它借用了这个名字的一般架构。我不会深入探讨这个问题，但这两个网络都是“U形”,具有一系列卷积和下采样层，向下到一个低点，在该点开始一系列卷积和上采样层，直到原始输入形状。</p><p id="a950" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae le" href="https://github.com/NathanUA/U-2-Net" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> U -Net </strong> </a> <strong class="js iu"> </strong>的主要区别在于“层”本身就是带有向下和向上采样层的“T8】U-结构。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ln"><img src="../Images/6508069baefcef607d4d440fe05dd029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*12xajBgmftF7y3A6vJFSdg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Unet(左)与U -Net(右)的网络架构</p></figure><p id="bec2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，在对原始代码做了一些轻微的修改后，我们可以直接从模型中可视化显著性图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e004f8a4959d7a14ec14d9125049d734.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qWq6NYHSqf8vQf0aR7-oew.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">unet2预测的掩码</p></figure><p id="f0f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每个单独的像素是一个介于0和1之间的浮点数，这意味着这个显著图处理透明度！</p><p id="42b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">没有任何进一步的修改，我们可以通过使用<strong class="js iu"> numpy </strong>的简单矩阵乘法，将<strong class="js iu">显著图</strong>作为遮罩应用于我们的图像。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/47115f255232cbaeed9ec54ea707ca52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BMC9xtWWe9cnvAog8C8fQA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在应用显著图作为掩模之后</p></figure><p id="8ec4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模型表现很好。我们仍然可以透过椅子看到一点地板，但在我们的情况下这不是问题。总的来说，效果非常好，但可能会留下一些伪像，尤其是像这把椅子这样有“空隙”的物体。</p><p id="1093" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个结果是在没有对图像进行任何预处理或后处理的情况下获得的。例如，您可以将较高的值向上舍入为1，将较低的值向下舍入以避免图像周围的“模糊”，或者只是锐化图像。</p><p id="ceeb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该型号还配备了另一种更小的架构(<strong class="js iu"> u2netp </strong>)，与原始的<strong class="js iu"> 176 mb </strong>相比，重量仅为<strong class="js iu"> 4.7 mb </strong>以获得类似的结果<strong class="js iu"> </strong>！</p><p id="9c57" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我们想要将我们的模型部署到一些基于云的平台上，而这些平台可能对模型大小有一些限制时，这就变得很方便了。例如<strong class="js iu"> Amazon Lambda，</strong>它允许你运行一个无限可扩展的API，而不需要建立你自己的服务器，它对内存使用和磁盘空间有很大的限制。</p><p id="68f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是不同设置下的图像样本的结果:</p><div class="kp kq kr ks gt ab cb"><figure class="lo kt lp lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/982de702f57a8d8d8b51e96ea930c2d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*YEFUHYlsfBYP2N3YFF6pEA.jpeg"/></div></figure><figure class="lo kt lp lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/199c14ca5130f6dca1af869cf9e0e035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*cxtJJWhYMhGDz94dentbvA.png"/></div></figure></div><div class="ab cb"><figure class="lo kt lp lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/42b9a4292c087a0477b315e149dc1d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*acU5rBu9Pv9Ks09Nnwwxhg.jpeg"/></div></figure><figure class="lo kt lp lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/83462d7be7b1907dc9b22174fa30008a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*I1e6620asXn5qb5vj1TLoQ.png"/></div></figure></div><div class="ab cb"><figure class="lo kt lp lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/b2bf1f2a521aa8d64536558f921c48dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*KOEaGmNl1Y3-4g2jDpcU1w.jpeg"/></div></figure><figure class="lo kt lp lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/db9825d132fdfe50f16ed049cff3fe69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ZhIsCMxxh1IXSATaQus85g.jpeg"/></div></figure></div><div class="ab cb"><figure class="lo kt lp lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/4c7e9fb7588c368b13298b7f3bc23292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*nI8ib6uV9DNcnnoHijiJ_w.jpeg"/></div></figure><figure class="lo kt lp lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/dacc9beb3c5d4d13c72497fe45124057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*QC736Gx4KsSEDlygnQ_wMg.png"/></div></figure></div><p id="906f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">来源:</strong></p><p id="4632" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[1]<a class="ae le" href="https://docs.opencv.org/master/d7/d4d/tutorial_py_thresholding.html" rel="noopener ugc nofollow" target="_blank">https://docs . opencv . org/master/D7/d4d/tutorial _ py _ thresholding . html</a></p><p id="1664" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2] <a class="ae le" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320320302077?dgcid=author" rel="noopener ugc nofollow" target="_blank"> U2-Net:使用嵌套的U结构进行更深入的显著对象检测</a></p><p id="1f03" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3] <a class="ae le" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> U-Net:用于生物医学图像分割的卷积网络</a></p></div></div>    
</body>
</html>