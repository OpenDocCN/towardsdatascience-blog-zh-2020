<html>
<head>
<title>From Decision Trees and Random Forests to Gradient Boosting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从决策树和随机森林到梯度推进</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3?source=collection_archive---------26-----------------------#2020-11-16">https://towardsdatascience.com/from-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3?source=collection_archive---------26-----------------------#2020-11-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="18cb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在垃圾邮件数据集上回顾机器学习中的关键概念</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2e1d2b7b6de4387fd264cd614cf811e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kWLv9a5vdRzbWeniriKPyw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:图片引用:Brett Jordan在Unsplash上的照片。</p></figure><p id="45ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我们希望对分类问题执行监督学习，以确定传入的电子邮件是垃圾邮件还是非垃圾邮件。<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/spambase" rel="noopener ugc nofollow" target="_blank"> <em class="ls">垃圾邮件数据集</em> </a>由4601封电子邮件组成，每封都被标记为真实(或非垃圾邮件)(0)或垃圾邮件(1)。该数据还包含大量预测值(57)，每个预测值或者是字符数，或者是某个单词或符号的出现频率。在这篇短文中，我们将简要介绍基于树的分类的主要概念，并比较和对比最流行的方法。</p><p id="ccbc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" href="https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.info.txt" rel="noopener ugc nofollow" target="_blank">统计学习的要素，第二版</a>中详细介绍了这个数据集和几个工作示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/bf4462a9656970254cfbaaa42a2ddd3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*-Zgaw7f-MJmqHSIJYAg9Rw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:统计学习的要素第二版。图片引用:<a class="ae kv" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~hastie/ElemStatLearn/</a></p></figure><h1 id="7056" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">数据:</h1><p id="ca5e" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">垃圾邮件数据集由4601个观察值组成，响应为<strong class="ky ir"> Y </strong>，不是垃圾邮件(0)也不是垃圾邮件(1)。该数据还包括57个预测值<strong class="ky ir"> x </strong>。每个预测值反映了电子邮件中某个单词或字母或某个符号的频率。例如大写字母的总数，符号“$”的频率等。看一看原始数据的样本。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/3331cf26b012275d94217e7e246672ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pfF9vjLpC5Jd_q-Q9Ki_pA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:垃圾邮件数据集的前15个观察值和前几列。图片来自作者。</p></figure><p id="9bcb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，预测值非常小，似乎有许多零数据点。在拟合模型来帮助处理这个问题之前，我们对预测值进行对数变换。我们将ε0.1添加到我们的日志中，以确保我们不会试图取0的日志。</p><p id="405a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还将数据集随机分为训练集和测试集。稍后我们将使用测试集来评估我们的模型性能。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h1 id="0063" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak">决策树:</strong></h1><p id="994d" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">用于分类的决策树是最古老和最简单的机器学习算法之一。由于直观的解释，它们很受欢迎。然而，在实践中，他们倾向于<em class="ls">过度拟合</em>训练数据，这导致在试图进行预测时表现不佳(也称为高方差问题)。r允许我们很容易地用<em class="ls"> rpart </em>函数来拟合决策树。有几种方法来决定什么是“好”或“最好”的树，但通常我们使用<a class="ae kv" href="https://www.geeksforgeeks.org/gini-impurity-and-entropy-in-decision-tree-ml/" rel="noopener ugc nofollow" target="_blank"> <em class="ls">基尼杂质</em> </a> <em class="ls">。理想情况下，一个终端节点应该只包含一个类。混合得越多，就越不“纯粹”,树在新数据上的表现就越差。</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="2235" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该函数构建了几棵树，在每个点上，我们根据预测值向左或向右移动。我们可以看到，在我们的训练数据中,“$”似乎是确定电子邮件是否为垃圾邮件的最重要特征。根据' $ '的不同，我们要么向左，在这一点上检查' remove '，要么向右，在这一点上查看' hp '。不管我们的路径如何，我们最终会到达一个终端节点，在那里我们的电子邮件被分类为垃圾邮件(1)或非垃圾邮件(0)。在我们的根节点，我们有100%的训练样本。根据log的值($+0.1)，我们要么向左(76%的情况下发生)，要么向右(24%的情况下发生)。中间的数字代表那个节点的‘杂质’。我们的根节点有大约40%的杂质，因为我们的训练数据包含大约40%的垃圾邮件。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/c92b6c39fbf9d9624579cf9a7336485d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZaHz9DkPkEGaI5uEUxn0w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:适合垃圾邮件数据的简单决策树。符合对数(预测值+0.1)。在每次分割时，我们查看预测器来确定我们下一步要去哪一侧。作者使用r。</p></figure><p id="81e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们将模型拟合到我们的训练数据，我们就可以看到它在我们的测试数据上的表现。这是通过把我们的新例子放入树中并遵循指示的路径来完成的。为了评估性能，我们检查混淆矩阵。我们获得了相当高的91%的测试准确度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/b887c49d178085c470c4581c269d055c.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*fh8JmohkQrDYQPyxI1p43g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:我们混淆矩阵的总结。我们使用一个简单的决策树获得了低于91%的准确率。图片来自作者。</p></figure><h1 id="9925" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">随机森林和装袋:</h1><p id="b92d" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">在决策树中，我们的算法构建了几棵树，并选择了一棵我们认为是“最好”的树。然后，我们用这一棵树对新邮件进行预测。<em class="ls"> Brieman </em>通过引入两个关键概念，对这些简单模型进行了大幅改进。首先是<strong class="ky ir">装袋或引导聚集</strong>，最后是<strong class="ky ir">随机森林</strong>。这些组合了许多(数百或数千)树，我们从我们的观察和预测中随机抽取样本来形成新的树。</p><p id="d5c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林通过对多棵树进行平均，极大地减少了过度拟合的可能性(即减少了方差)，这有时被称为<em class="ls">群体智慧。</em></p><p id="f2ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Hastie在一次演讲中描述了以下内容(参考资料链接):</p><ul class=""><li id="66ad" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr nb nc nd ne bi translated">Bagging (Breiman，1996):“<em class="ls">将许多大树拟合到训练数据的自助重采样版本，并通过多数投票进行分类。”</em></li><li id="b4ff" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated"><em class="ls"/>(布雷曼1999) <em class="ls">:“更好的装袋版本”</em></li></ul><p id="457c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本质上，随机森林利用套袋形成数百棵树。由于bagging的定义排除了一些模型中的某些观察值，我们可以使用不包含某个观察值的树来获得“超出bagging误差”的估计值。我们还可以通过观察哪些预测因子在许多树中始终是重要的，来获得对最重要特征的估计。注意，自举聚合通常会导致预测值的去相关。因此，如果我们有一些基本相同的预测器，随机森林应该能够梳理出这些预测器，并且只保留一个副本。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/4954e473df840db1db8d779e2f9c5041.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*TvlCYW8wP4n1OYs_jk4W3w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6:使用randomForest函数的测试数据的混淆矩阵。图片来自作者。</p></figure><p id="8195" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用R中的randomForest包来拟合一个模型。我们还可以保留我们的树中包含最多的变量，这允许我们可视化“变量重要性”。我们将准确率从不到91%提高到了94.6%。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/c19e19c1108bb8739ce5c1a69836c42e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*sf9hR_SKtYXM9TL5OZ9eww.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7:可变的重要性。统计学习的要素，第二版。<a class="ae kv" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~hastie/ElemStatLearn/</a></p></figure><p id="67db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林的一个伟大之处在于，它们还允许我们保留预测者的“相对重要性”。图7总结了在确定垃圾邮件时哪些符号/单词是重要的，哪些是不重要的。</p><h1 id="7412" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">梯度提升:</h1><ul class=""><li id="3520" class="mw mx iq ky b kz mm lc mn lf nm lj nn ln no lr nb nc nd ne bi translated">Boosting (Freund &amp; Shapire，1996): <em class="ls">“使许多大树或小树适合训练数据的重新加权版本。按加权多数票分类。”——</em>哈斯蒂。</li></ul><p id="47c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">梯度推进是对随机森林的一种改进(如果你调整好超参数的话)。现在，我们不再仅仅拟合引导聚合树，而是考虑中间树的表现。使用这些中间树，我们调整未来树的权重(我们对某些树的权重大于其他树)。这可以认为是<em class="ls">重复拟合我们模型的残差。</em>我们从一个基本模型开始，看看它如何预测新的观察结果。残差是我们在预测方面做得不好的地方，所以我们将我们的下一棵树拟合到这些残差，以此类推，以说明我们做得不好的地方(我们也不使用bagging，而是用替换进行采样)。梯度推进也倾向于创建许多“浅”树。也就是说，它们包含一个根节点和少量的后续分割。通常，他们只包含一个单一的分裂，这些被称为“树桩”或“弱学习者”。</p><ul class=""><li id="09b2" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr nb nc nd ne bi translated"><em class="ls">“梯度提升继承了树的所有好的特征(变量选择、缺失数据、混合预测器)，对弱的特征进行了改进，比如预测性能。”——</em>特雷弗<em class="ls"> </em>哈斯蒂</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/92501aac2dadfa2a51e1ab4685f8f968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqxTMiMpGGrUGKwOvj_tZg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8:说明装袋的图像。图片引用:<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg" rel="noopener ugc nofollow" target="_blank">知识共享。</a></p></figure><p id="1fe6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图8示出了上面讨论的装袋。装袋可以被视为一种并行工作的算法。也就是说，装袋是通过选择多个同时替换的器械包来完成的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/5ca1a958c0f2d30767f406ad9ecc2c68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V-NAC2LVwUFBUAzCXK7Otg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9:展示了一般的增强方法。<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:Ensemble_Boosting.svg" rel="noopener ugc nofollow" target="_blank">图片引用:知识共享。</a></p></figure><p id="c0d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">装袋和助推是不一样的。如图8所示，Bagging是并行完成的，用于形成几组观察值来构建模型。Boosting实际上使用中间树来调整计算最终预测时使用的权重。如图9所示，boosting不是并行完成的，它会随着进程更新权重。</p><p id="6389" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用R包“gbm”对数据集进行梯度增强。使用梯度增强进行拟合需要</p><p id="2cb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">超参数n.trees、interaction.depth和shrinkage可以使用网格搜索和交叉验证来调整。我们将相互作用深度设置为6，这允许六阶相互作用。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/84971ff6e59745b85675059b189427e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*cZzp3H6QeIawWCVuHW9agg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9:我们使用梯度增强获得了最佳精度，精度超过95%。图片来自作者。</p></figure><p id="c56f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经将测试数据的准确率提高到了95%以上！</p><h1 id="22ce" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">总结:</h1><p id="81f7" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">基于树的分类仍然是当今使用的最流行的算法之一，主要是因为它易于解释并且能够处理大量的预测值。决策树是基于树的方法中最基本的，并且可以容易地解释，但是非常容易在训练数据上过度拟合。幸运的是，这些年来已经有了一些改进来解决简单决策树所面临的弱点。随机森林利用Bagging (Bootstrap Aggregation)来获得许多树，并使用群体的<em class="ls">智慧来获得较低的方差预测。提升树通过考虑中间树的表现进一步改进了以前的算法，并调整树以在它们以前表现不佳的地方表现得更好(并对强大的树加权更多)。所有这些技术和方法都很容易在R、Python和其他编码语言中使用。尤其是梯度推进仍然是机器学习竞赛中最受欢迎的算法之一，例如Kaggle上举办的那些竞赛。</em></p><p id="fcd5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这篇文章提供了这些基本ML技术的广泛概述和简单示例。感谢阅读！</p><h1 id="c896" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">代码:</h1><ul class=""><li id="b648" class="mw mx iq ky b kz mm lc mn lf nm lj nn ln no lr nb nc nd ne bi translated">【https://github.com/Robby955/spam T2】号</li></ul><h1 id="c5ee" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">来源:</h1><p id="2557" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">[1]特雷弗·哈斯蒂的演讲，<em class="ls">助推</em>，<a class="ae kv" href="https://www.cc.gatech.edu/~hic/CS7616/pdf/lecture5.pdf" rel="noopener ugc nofollow" target="_blank">https://www.cc.gatech.edu/~hic/CS7616/pdf/lecture5.pdf</a></p><p id="6c8d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]利奥·布雷曼(1996年)。打包预测值。<em class="ls">机器学习</em></p><p id="bd9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]利奥·布雷曼(2001年)。随机森林。<em class="ls">机器学习</em></p><p id="d597" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4] <a class="ae kv" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">哈斯蒂，提比拉尼，弗里德曼(2009)。统计学习的要素2。</a></p><p id="defa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5]j . h .弗里德曼(2002年)。随机梯度增强。<em class="ls">计算统计&amp;数据分析</em>，<em class="ls"> 38 </em> (4)，367–378。</p><p id="a98a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[6] Liaw，a .，&amp; Wiener，M. (2002年)。randomForest分类和回归。<em class="ls"> R新闻</em>，<em class="ls"> 2 </em> (3)，18–22。</p><p id="ee8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[7]杰努尔、波吉、J. M .和图洛-马洛特(2010年)。使用随机森林的变量选择。<em class="ls">模式识别字母</em>，<em class="ls"> 31 </em> (14)，2225–2236。</p></div></div>    
</body>
</html>