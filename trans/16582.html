<html>
<head>
<title>Image Upscaling with Javascript</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Javascript进行图像放大</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-upscaling-with-javascript-836445267496?source=collection_archive---------48-----------------------#2020-11-15">https://towardsdatascience.com/image-upscaling-with-javascript-836445267496?source=collection_archive---------48-----------------------#2020-11-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5b4c" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><p id="eb06" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">我最近发布了一个工具，</em><a class="ae kv" href="https://github.com/thekevinscott/UpscalerJS" rel="noopener ugc nofollow" target="_blank"><em class="ku">upscaler js</em></a><em class="ku">，它可以用Javascript在你的浏览器中放大图片，并将图片尺寸缩小1/16。它的设计是与模型无关的——你可以即插即用任何可以转换为Tensorflow.js的训练过的模型</em></p><p id="697d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">在这篇文章中，我想展示我认为的浏览器中神经网络的一个杀手级用例，以及我如何发现这项研究，将其转换为Javascript，以及未来改进它的方法。</em></p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi kw"><img src="../Images/e52ceb5a824a38c3af0034880911bd33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A6TO2F4kBWXpinu6.gif"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated"><a class="ae kv" href="https://upscaler.ai" rel="noopener ugc nofollow" target="_blank"> https://upscaler.ai </a> //作者动画</p></figure></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><p id="34bb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">假设你在做一个电子商务平台。你的用户上传产品照片来销售。</p><p id="f561" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">你设计了一个看起来很棒的网站，用来突出你的用户手工制作的美丽和奇妙的产品。只有一个问题——一旦你启动，你发现你的用户正在上传小的像素化的图片，突然之间你美丽的网站看起来不那么漂亮了。</p><p id="93c7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">(我是从经验中得出的结论——这种情况在我身上发生过不止一次。)</p><p id="9a30" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">你可以回头向用户唠叨要更好的图像——有时这是可行的。但通常情况下，他们所能得到的只有他们提供的图片。也许图像是从pdf截屏的，或者也许图像是旧的，用户没有更好的。即使他们有更好的图像，让你的用户回去帮你修改他们的图像也是一件费力的事情，即使这是为了他们的利益。</p><p id="b23e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">有没有我们可以探索的技术解决方案？当然有，它叫做<strong class="jy ja">超分辨率</strong>。</p><h2 id="28b5" class="lt lu iq bd lv lw lx dn ly lz ma dp mb kh mc md me kl mf mg mh kp mi mj mk iw bi translated">你所说的“超级分辨率”是什么？</h2><p id="8759" class="pw-post-body-paragraph jw jx iq jy b jz ml kb kc kd mm kf kg kh mn kj kk kl mo kn ko kp mp kr ks kt ij bi translated">假设有人上传了一张150像素的照片到我们的电子商务网站:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/9cc95879f60ca697199f7d736f08db77.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/0*XzG3_YhBbMMybj8b.jpg"/></div><p class="li lj gj gh gi lk ll bd b be z dk translated">merec0的快乐狗</p></figure><p id="70a7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们想在我们的主页上展示这张图片，因为它是一只漂亮的狗，但我们的设计要求图片为300像素。我们能做什么？如果我们将每个像素加倍，我们会得到看起来像素化的更大图像:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/28c9c090172be3d84c09afc65a6c2643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*OO557fn0pzmhJDxx.png"/></div><p class="li lj gj gh gi lk ll bd b be z dk translated">通过复制像素放大图像</p></figure><p id="5970" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">你必须努力在浏览器中实现像素化的外观;默认情况下，大多数浏览器会对图像应用某种缩放算法，通常是<a class="ae kv" href="https://en.wikipedia.org/wiki/Bicubic_interpolation" rel="noopener ugc nofollow" target="_blank">双三次插值</a>，如下所示:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/05d66e0973c5c5893b49105cb7556340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*HRN4pNaLecW6OJkd.png"/></div><p class="li lj gj gh gi lk ll bd b be z dk translated">使用双三次插值放大图像。</p></figure><p id="1d95" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">使用双三次插值放大的图像看起来肯定没有第一幅图像像素化，我敢打赌大多数人会觉得它更有美感，但它很模糊，没有人会误认为是高分辨率图像。</p><p id="9f70" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">超分辨率是一种机器学习技术，用于从较低分辨率的图像重建较高分辨率的图像。</strong>你可以把这个过程想象成在图像中绘制新像素，实现比双三次插值算法更高的保真度。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/7119c01660b05c1c3a443548686fb73e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*O8p7bDpDKiX_FHCm.png"/></div><p class="li lj gj gh gi lk ll bd b be z dk translated">使用GAN放大图像。</p></figure><p id="6b40" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">有<a class="ae kv" href="https://arxiv.org/abs/1902.06068" rel="noopener ugc nofollow" target="_blank">许多不同的方法</a>可以用来实现超分辨率，也有一些很棒的博客文章描述了基本的理论<a class="ae kv" rel="noopener" target="_blank" href="/an-evolution-in-single-image-super-resolution-using-deep-learning-66f0adfb2d6b">这里</a>和<a class="ae kv" href="https://medium.com/beyondminds/an-introduction-to-super-resolution-using-deep-learning-f60aff9a499d" rel="noopener">这里</a>。</p><figure class="kx ky kz la gt lb"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="ed5e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">超分辨率<a class="ae kv" href="https://letsenhance.io/" rel="noopener ugc nofollow" target="_blank">通常使用Python </a>在后端实现。以这种方式建造它有很好的理由。在后台运行它可以让你接触到强大的硬件，而这些硬件可以让你使用最新、最精确的模型。如果获得最高分辨率的图像很重要，后端部署是一个不错的选择。此外，许多用例是“一次缩放，经常显示”——如果放大一幅图像需要更长的时间，没什么大不了的。</p><p id="2858" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">另一方面，在后端实现这一点也有缺点。一个是即时反馈——你需要将图像上传到服务器，进行处理，然后发送回来，这可能需要一段时间，取决于你的用户连接和你的模型大小。这可能不是小事，特别是因为如此多的前沿实现处于最前沿，具有不稳定的依赖性和不断变化的需求。而且，如果您的部署需要GPU，这可能会成为一个昂贵的提议，并且难以扩展。</p><p id="a750" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">事实证明，我们可以在浏览器中运行，这样做有一些明显的好处。</p><p id="5708" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">第一，部署问题？完全消失了。用Javascript运行神经网络意味着不需要安装任何东西，不需要配置任何GPU——tensor flow . js会处理所有这些事情。模型直接在用户的浏览器中运行。</p><p id="757d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">其次，你的用户会看到更多即时反馈。特别是对于带宽可能较慢的连接，如电话，直接在设备上执行推理可以省去昂贵的往返步骤。</p><p id="a630" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">第三，也是我认为最有说服力的论点——你可以提供更小的图片。有时候，<em class="ku">小得多</em>的图像。例如，上面的那些图片？300px是724kb。150px版本？是<em class="ku"> 9kb </em>。这是一张原始文件大小的百分之六的图像<em class="ku">。这是一个巨大的减少！</em></p><p id="ca96" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">当然，在浏览器中运行有一些明显的缺点。最大的问题是你受到用户硬件的限制。这体现在两个方面。一个是，如果你想部署最新最棒的型号，你可能就要倒霉了。特别是如果他们是GPU饥饿，他们只是可能无法在浏览器中运行。近年来，包括苹果和谷歌<a class="ae kv" href="https://heartbeat.fritz.ai/hardware-acceleration-for-machine-learning-on-apple-and-android-f3e6ca85bda6" rel="noopener ugc nofollow" target="_blank">在内的硬件制造商已经投入了巨额资金来提高他们的设备上芯片的性能</a>，特别关注提高在设备上运行神经网络的能力。好消息是，年复一年，这项技术的性能会越来越好；坏消息是，对于使用旧设备的用户来说，性能差异将变得更加显著。如果您想要跨平台的一致体验，服务器端解决方案可能是更好的选择。</p><p id="1000" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">最终，尽管精确的权衡将取决于用例，但Javascript绝对是考虑该技术应用的一个有价值的竞争者。让我们看看如何评估那里有什么，看看什么对我们的目的有用。</p><h2 id="f5c1" class="lt lu iq bd lv lw lx dn ly lz ma dp mb kh mc md me kl mf mg mh kp mi mj mk iw bi translated">通过小道消息听到的</h2><p id="9c62" class="pw-post-body-paragraph jw jx iq jy b jz ml kb kc kd mm kf kg kh mn kj kk kl mo kn ko kp mp kr ks kt ij bi translated">如果你来自Javascript世界，你脑海中的一个问题是——你最初是怎么听说这项研究的？</p><p id="a38e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">大多数前沿的机器学习研究都张贴在arxiv.org的网站上，在那里可以免费搜索和下载PDF格式的研究成果。这是学术研究，论文可能倾向于理论和数学，很难理解。这可以吓跑很多人——一开始当然是把我吓跑了。</p><p id="607a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我不想低估充分理解研究的重要性——深入理解理论通常可以带来与你的领域相关的新见解和发展——但你不一定需要对使用它的技术有深刻的理解。特别是如果你专注于推理，就像我们在这种情况下，你可以依靠他人来评估研究，以及实现训练代码，在某些情况下，提供训练模型。</p><p id="54a0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">有一个网站就是这么做的，叫做<a class="ae kv" href="https://paperswithcode.com/" rel="noopener ugc nofollow" target="_blank">论文，代码</a>:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi mu"><img src="../Images/e85e79e33d08b7c18ac04df1c16b43c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fR8xan3votlrJUx7.png"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated"><a class="ae kv" href="https://paperswithcode.com" rel="noopener ugc nofollow" target="_blank">https://paperswithcode.com</a>截图</p></figure><p id="023c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">研究按主题领域分类，并根据其相对于公认指标的表现进行排名。<a class="ae kv" href="https://paperswithcode.com/task/image-super-resolution" rel="noopener ugc nofollow" target="_blank">甚至有一个专门针对这个领域的特定类别</a>。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi mv"><img src="../Images/f95b08d1d91ee0dfd421d44755cac713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Y999auCDVn2pdxg3.png"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated">https://paperswithcode.com的<a class="ae kv" href="https://paperswithcode.com" rel="noopener ugc nofollow" target="_blank">截图</a></p></figure><p id="59ee" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">您可以根据标准数据集查看每个实现的性能，并查看如何根据不同的指标对它们进行测量。PSNR和SSIM是测量超分辨率任务性能的两种常用方法；<a class="ae kv" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio" rel="noopener ugc nofollow" target="_blank"> PSNR </a>可以测量噪声，<a class="ae kv" href="https://en.wikipedia.org/wiki/Structural_similarity" rel="noopener ugc nofollow" target="_blank"> SSIM </a>测量两幅图像的相似度。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi mw"><img src="../Images/733062e82b9c84b41b3a67326dce4f35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ct-alxqNvApziLaD.jpeg"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated"><a class="ae kv" href="https://medium.com/@datamonsters/a-quick-overview-of-methods-to-measure-the-similarity-between-images-f907166694ee" rel="noopener">来自“图像间相似性度量方法快速概述”</a></p></figure><p id="b8f7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">指标可能有点棘手。你可以在上面的图像中看到，相同的PSNR分数可以有完全不同的SSIM分数，具有相应不同的视觉表现。</p><p id="96db" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">PSNR和SSIM都是衡量一幅图像彼此差异的标准，但都不能代替人类的评价。作为人类，我们感知图像的方式与计算机不同。比如说，饱和度不同但也更清晰的一组像素可能导致较低的度量分数，但从人的角度来看更美观的分数。</p><blockquote class="mx my mz"><p id="7750" class="jw jx ku jy b jz ka kb kc kd ke kf kg na ki kj kk nb km kn ko nc kq kr ks kt ij bi translated">SR算法通常由几种广泛使用的失真度量来评估，例如PSNR和SSIM。然而，这些度量从根本上与人类观察者的主观评价不一致。感知质量评估采用非参考指标，包括马评分和NIQE，两者均用于计算SR挑战赛中的感知指数。在最近的一项研究中，Blau等人发现失真和感知质量是相互矛盾的。— <a class="ae kv" href="https://arxiv.org/pdf/1809.00219v2.pdf" rel="noopener ugc nofollow" target="_blank">王等</a></p></blockquote><p id="31ca" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">除了判断模型准确性的主观性，准确性不是我们最关心的还有其他原因。记住，我们的最终目标是一个用Javascript运行的模型。考虑以下因素也很重要:</p><ul class=""><li id="c8e1" class="nd ne iq jy b jz ka kd ke kh nf kl ng kp nh kt ni nj nk nl bi translated"><strong class="jy ja">一篇好论文</strong>。我们想要一个健康的建筑。我们可能需要对基础理论有所了解，所以论文清晰易懂、严谨是很重要的；一篇论文被引用的频率也可以很好地反映其整体质量。</li><li id="a00e" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt ni nj nk nl bi translated"><strong class="jy ja">性能良好</strong>。速度和准确性一样重要。运行一分钟的模型不适合浏览器。</li><li id="9198" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt ni nj nk nl bi translated"><strong class="jy ja">可保存，可转换</strong>。实现的模型必须与Javascript兼容。我们将很快触及细节，但最重要的是坚持Tensorflow实现，因为Tensorflow.js是在浏览器中进行机器学习的主要方式，所以Pytorch实现是不可能的。</li></ul><p id="583e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我最终选择了<a class="ae kv" href="https://paperswithcode.com/paper/esrgan-enhanced-super-resolution-generative" rel="noopener ugc nofollow" target="_blank"> ESRGAN </a>。</p><p id="7e03" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我开始看按分数排序的论文。一些得分较高的实现要么没有链接的代码实现，要么代码实现完全在Pytorch中。(并不是所有的代码实现都会在paperswithcode.com上展示，所以自己去谷歌一下是个好主意。)</p><p id="3a0d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">ESRGAN在指标上排名很高，并在Tensorflow中实现了很多。这篇论文本身相当清晰易懂。ESRGAN基于以前的架构，<a class="ae kv" href="https://arxiv.org/pdf/1609.04802.pdf" rel="noopener ugc nofollow" target="_blank"> SRGAN </a>，它本身是一个健壮的架构，但ESRGAN进行了许多改进，包括改进了生成器的构建模块，改进了预测图像显示逼真程度的鉴别器，以及更有效的感知损失。</p><p id="b073" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在我能找到的实现中，我觉得有三个满足了我的标准，看起来代码质量不错，文档也不错。</p><ul class=""><li id="a5f5" class="nd ne iq jy b jz ka kd ke kh nf kl ng kp nh kt ni nj nk nl bi translated"><a class="ae kv" href="https://github.com/idealo/image-super-resolution" rel="noopener ugc nofollow" target="_blank">idealo/图像超分辨率</a></li><li id="33ca" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt ni nj nk nl bi translated"><a class="ae kv" href="https://github.com/krasserm/super-resolution" rel="noopener ugc nofollow" target="_blank">krasserm/超分辨率</a></li><li id="bb93" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt ni nj nk nl bi translated"><a class="ae kv" href="https://github.com/peteryuX/esrgan-tf2" rel="noopener ugc nofollow" target="_blank"> peteryuX/esrgan-tf2 </a></li></ul><p id="9ee9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如果不下载并运行代码，很难确定一个实现是否适合。如果你习惯于安装一个<code class="fe nr ns nt nu b">npm</code>库并直接进入，请做好准备:使用机器学习代码通常是一种沮丧的练习。解决依赖性挑战、环境问题和内存瓶颈可能会将评估变成一件多天的事情。</p><p id="5a02" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">出于这个原因，包含<code class="fe nr ns nt nu b">Dockerfile</code> s或Google Colab链接的回购通常是一个非常好的迹象。这也是一个好迹象，当作者包括预训练的重量，以及这些重量是如何训练的文件。如果您能够直接跳到推论，它有助于更快地评估模型；同样，关于如何训练这些权重的信息使您能够测试自己的实现，这为您提供了一个坚实的基准。在回购中忽略这些不会破坏交易，但会让你的日子更难过。</p><p id="b9ab" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">不管作者是否提供Dockerfile，我通常会建立自己的<em class="ku">docker file，因为当我探索repo时，我会安装自己的依赖项并编写探索性代码，我希望能够以可复制的方式运行这些代码。几乎在每一种情况下，当我玩了几个星期的机器学习代码后，当我回来时，我会遇到一些深奥的错误，这些错误是由一些包过时或升级引起的。固定您的版本，从一开始就获得一个可复制的环境！</em></p><p id="0b59" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我最终选定了由<a class="ae kv" href="https://github.com/idealo/image-super-resolution" rel="noopener ugc nofollow" target="_blank"> idealo </a>实现。代码易于阅读，提供了预先训练的模型，作者<a class="ae kv" href="https://medium.com/idealo-tech-blog/a-deep-learning-based-magnifying-glass-dae1f565c359" rel="noopener">提供了他们探索太空之旅的精彩记录</a>。然而，真正的关键是我只需要做一些修改就可以将RDN模型转换成Javascript。转换RRDN模型有点棘手——稍后会详细介绍。</p><h2 id="0583" class="lt lu iq bd lv lw lx dn ly lz ma dp mb kh mc md me kl mf mg mh kp mi mj mk iw bi translated">转换为Javascript</h2><p id="6ec6" class="pw-post-body-paragraph jw jx iq jy b jz ml kb kc kd mm kf kg kh mn kj kk kl mo kn ko kp mp kr ks kt ij bi translated">Tensorflow.js提供了一个方便的命令行工具，用于将模型转换为Javascript，称为TFJS converter。您可以使用类似下面的内容来转换模型:</p><pre class="kx ky kz la gt nv nu nw nx aw ny bi"><span id="f43b" class="lt lu iq nu b gy nz oa l ob oc">tensorflowjs_converter --input_format=keras --output_format=tfjs_layers_model ./rdn-model.h5 rdn-tfjs</span></pre><p id="685c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae kv" href="https://colab.research.google.com/drive/1WmTHfcNiEWVta5B5AJ5V0dnrQg-JXH06#scrollTo=oMMODAFu05Rc" rel="noopener ugc nofollow" target="_blank">我已经整合了一个谷歌实验室来证明这一点。</a></p><p id="a792" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">为了将模型干净地转换成Javascript，需要注意一些事情:</p><ol class=""><li id="9628" class="nd ne iq jy b jz ka kd ke kh nf kl ng kp nh kt od nj nk nl bi translated">模型必须以Keras或与Tensorflow转换器兼容的其他格式保存。此外，确保将<em class="ku">模型</em>转换成Javascript，而不仅仅是权重。如果是后者，您可能会收到一个含糊不清的错误，没有任何关于正在发生什么的指导。</li><li id="0b69" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt od nj nk nl bi translated">变量不能引用<code class="fe nr ns nt nu b">self</code>——这让我对idealo的实现产生了误解。(<a class="ae kv" href="https://github.com/idealo/image-super-resolution/issues/114#issuecomment-605067405" rel="noopener ugc nofollow" target="_blank">参考本Github问题</a>、<a class="ae kv" href="https://github.com/idealo/image-super-resolution/pull/137" rel="noopener ugc nofollow" target="_blank">或我的PR </a>，寻求解决方案)</li><li id="b9a3" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt od nj nk nl bi translated">所有张量运算都必须用Javascript实现。除了试错法，我不知道还有什么更好的方法来检查这个问题(也就是转换模型，看看它是否能运行)。</li><li id="7629" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt od nj nk nl bi translated">如果实现了自定义层，则必须用Javascript重新实现。例如，<a class="ae kv" href="https://github.com/idealo/image-super-resolution/pull/137/files#diff-33e903d44b1c48dec8eabcca53955976R191" rel="noopener ugc nofollow" target="_blank">RRDN模型的定制层必须重新实现，以便干净地保存</a>。在这篇文章的后面，我将讨论如何处理自定义层。</li><li id="dd38" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt od nj nk nl bi translated">在Tensorflow.js转换器的输出中，我必须手动将模型的<code class="fe nr ns nt nu b">class_name</code>从<code class="fe nr ns nt nu b">Functional</code>更改为<code class="fe nr ns nt nu b">Model</code>。(<a class="ae kv" href="https://colab.research.google.com/drive/1WmTHfcNiEWVta5B5AJ5V0dnrQg-JXH06#scrollTo=oMMODAFu05Rc&amp;line=4&amp;uniqifier=1" rel="noopener ugc nofollow" target="_blank">在Google Colab中，这是实现这个</a>的单元格。)不知道为什么会这样，也不知道是不是bug欢迎评论！</li><li id="3359" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt od nj nk nl bi translated">图像的任何预处理和后处理都需要用Javascript再现。</li></ol><h2 id="16ad" class="lt lu iq bd lv lw lx dn ly lz ma dp mb kh mc md me kl mf mg mh kp mi mj mk iw bi translated">拉你的体重，好神经元</h2><p id="faa9" class="pw-post-body-paragraph jw jx iq jy b jz ml kb kc kd mm kf kg kh mn kj kk kl mo kn ko kp mp kr ks kt ij bi translated">性能是基于浏览器的应用程序的关键。更瘦的模特表现更好。</p><p id="ef8d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">有两种方法可以提高Javascript的性能。</p><p id="3d3b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">首先，我们可以<strong class="jy ja">量化</strong>我们的模型。<a class="ae kv" href="https://github.com/tensorflow/tfjs-examples/tree/master/quantization" rel="noopener ugc nofollow" target="_blank">量化模型意味着降低模型权重的精度</a>。这可能会导致较低的准确性，但是可以显著减小模型的大小(并且具有使模型比gzip更可压缩的附带好处)。</p><p id="62d5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们可以在tensorflow js转换器中直接量化:</p><pre class="kx ky kz la gt nv nu nw nx aw ny bi"><span id="ac40" class="lt lu iq nu b gy nz oa l ob oc">tensorflowjs_converter \<br/>   --input_format tfjs_layers_model \<br/>   --output_format tfjs_layers_model \<br/>   --quantize_uint8 \<br/>   original_model/model.json<br/>   quantized_model/</span></pre><p id="ca03" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在这种特定情况下，最大量化量<code class="fe nr ns nt nu b">uint8</code>，对最终模型的性能没有显著影响。</p><p id="573f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">第二，我们可以<strong class="jy ja">修剪</strong>我们的模型。修剪是我们在训练中剔除表现不佳的权重的过程。我还没有亲自探索过这条途径，但是如果你感兴趣的话<a class="ae kv" href="https://www.tensorflow.org/model_optimization/guide/pruning" rel="noopener ugc nofollow" target="_blank">你可以在这里阅读更多相关内容</a>。对于在前端挤出额外的性能来说，这无疑是一个很有前途的策略。</p><h2 id="a600" class="lt lu iq bd lv lw lx dn ly lz ma dp mb kh mc md me kl mf mg mh kp mi mj mk iw bi translated">浏览器中的推理—给我看看代码！</h2><p id="d613" class="pw-post-body-paragraph jw jx iq jy b jz ml kb kc kd mm kf kg kh mn kj kk kl mo kn ko kp mp kr ks kt ij bi translated">我们已经转换并量化了RDN模型。成功！现在，我们如何让它在浏览器中运行？我们可以用以下内容加载我们的模型:</p><pre class="kx ky kz la gt nv nu nw nx aw ny bi"><span id="29be" class="lt lu iq nu b gy nz oa l ob oc">import * as tf from "@tensorflow/tfjs";<br/>tf.loadLayersModel("./rdn-tfjs/model.json");</span></pre><p id="460f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">确保你加载的是<code class="fe nr ns nt nu b">model.json</code>、<em class="ku">而不是</em>、<code class="fe nr ns nt nu b">bin</code>文件。</p><p id="85c3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">然后，我们可以得到作为张量<em class="ku">的图像</em>:</p><pre class="kx ky kz la gt nv nu nw nx aw ny bi"><span id="72a9" class="lt lu iq nu b gy nz oa l ob oc">const img = new Image();<br/>img.crossOrigin = "anonymous";<br/>img.src = "your-image";<br/>img.onload = () =&gt; {<br/>  const tensor = tf.browser.fromPixels(img).expandDims(0);<br/>};</span></pre><p id="fe79" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">张量是几乎所有神经网络中使用的一种数字数据结构，你可以在这里阅读更多关于它们的内容。</p><p id="29c5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在上面的代码中需要注意两件事:</p><ol class=""><li id="b3ba" class="nd ne iq jy b jz ka kd ke kh nf kl ng kp nh kt od nj nk nl bi translated">如果你正在处理来自其他领域的图像，你可能会遇到CORS问题。将<code class="fe nr ns nt nu b">crossOrigin</code>设置为<code class="fe nr ns nt nu b">anonymous</code>会有所帮助。</li><li id="cda6" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt od nj nk nl bi translated">你需要调用<code class="fe nr ns nt nu b">expandDims</code>从你的图像中获取的张量。你需要给你的模型传递一个四维张量；要了解更多原因，你可以点击这里查看我关于图像分类的文章。</li></ol><p id="79f1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在我们有了一个张量，我们可以在模型中运行它:</p><pre class="kx ky kz la gt nv nu nw nx aw ny bi"><span id="992e" class="lt lu iq nu b gy nz oa l ob oc">import tensorAsBase64 from 'tensor-as-base64';<br/>const prediction = model.predict(tensor).squeeze();</span></pre><p id="d44b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">还有维奥拉。你有一个升级的张量，准备在你的浏览器中显示！</p><p id="4bf7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae kv" href="https://codesandbox.io/s/upscaling-1-0xmbx?file=/src/index.js" rel="noopener ugc nofollow" target="_blank">下面是CodeSandbox </a>上所有代码实现:</p><figure class="kx ky kz la gt lb"><div class="bz fp l di"><div class="oe mt l"/></div></figure><p id="0f2a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这仍然需要一段时间——在我的例子中，大约2.5秒——这对于生产来说是不可接受的。此外，它还有一个令人讨厌的副作用，就是在工作时冻结了用户界面。让我们来看一些不同的提高性能的策略。</p><h2 id="d34a" class="lt lu iq bd lv lw lx dn ly lz ma dp mb kh mc md me kl mf mg mh kp mi mj mk iw bi translated">热身</h2><p id="7c3e" class="pw-post-body-paragraph jw jx iq jy b jz ml kb kc kd mm kf kg kh mn kj kk kl mo kn ko kp mp kr ks kt ij bi translated">Tensorflow.js中神经网络的初始调用将花费很长时间，但是随后的<em class="ku">调用将会快得多。</em></p><blockquote class="mx my mz"><p id="5e4a" class="jw jx ku jy b jz ka kb kc kd ke kf kg na ki kj kk nb km kn ko nc kq kr ks kt ij bi translated">TensorFlow.js通过运行WebGL着色器程序在GPU上执行操作。当用户要求执行操作时，这些着色器被缓慢地组装和编译。着色器的编译发生在CPU的主线程上，可能会很慢。TensorFlow.js将自动缓存已编译的着色器，从而使第二次调用具有相同形状的输入和输出张量的相同操作的速度快得多。— <a class="ae kv" href="https://www.tensorflow.org/js/guide/platform_environment#shader_compilation_texture_uploads" rel="noopener ugc nofollow" target="_blank"> Tensorflow.js文档</a></p></blockquote><p id="c28a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们可以利用这一点，通过传递一个虚拟张量来“预热”我们的模型。这里有一些你可以使用的代码(<a class="ae kv" href="https://codesandbox.io/s/upscaling-2-warm-up-vjllx?file=/src/index.js" rel="noopener ugc nofollow" target="_blank">查看CodeSandbox </a>上的代码):</p><pre class="kx ky kz la gt nv nu nw nx aw ny bi"><span id="9720" class="lt lu iq nu b gy nz oa l ob oc">const dummyTensor = tf.zeros([1, img.height, img.width, 3]);<br/>model.predict(dummyTensor);</span></pre><p id="856e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在这种情况下，我的推理时间下降到150毫秒。好多了！然而，这只有在张量大小完全匹配的情况下才有效。我们显然不能依赖这个——用户可以上传任何大小和比例的照片。此外，当模型运行其预测时，用户界面上仍有明显的滞后。</p><p id="b4d4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们先试着解决第二个问题。如果我们把计算从主线程转移到一个网络工作者身上会怎么样？</p><h2 id="00d3" class="lt lu iq bd lv lw lx dn ly lz ma dp mb kh mc md me kl mf mg mh kp mi mj mk iw bi translated">网络工作者</h2><p id="fb74" class="pw-post-body-paragraph jw jx iq jy b jz ml kb kc kd mm kf kg kh mn kj kk kl mo kn ko kp mp kr ks kt ij bi translated"><a class="ae kv" href="https://codesandbox.io/s/github/thekevinscott/upscalerjs/tree/master/examples/webworker" rel="noopener ugc nofollow" target="_blank">这里有一个CodeSandbox链接，演示了Web Workers </a>的使用。(这个例子使用了UpscalerJS，而不是手工写出TFJS代码，但是概念是一样的。)</p><figure class="kx ky kz la gt lb"><div class="bz fp l di"><div class="oe mt l"/></div></figure><p id="ea9e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">将代码转移到web worker可以让我们将处理从主线程中转移出来，从而让我们以更流畅的速度运行动画。然而，它不是万灵药；动画中仍有一些起伏。我相信这种波动来自GPU本身锁定线程，这在旧设备上比新设备上表现得更糟。网络工作者绝对有帮助，但他们不能完全解决问题。</p><h2 id="9a80" class="lt lu iq bd lv lw lx dn ly lz ma dp mb kh mc md me kl mf mg mh kp mi mj mk iw bi translated">将图像分割成块</h2><p id="179d" class="pw-post-body-paragraph jw jx iq jy b jz ml kb kc kd mm kf kg kh mn kj kk kl mo kn ko kp mp kr ks kt ij bi translated">如果我们不是一次处理完整的图像，而是将图像细分为多个部分分别处理，会怎么样？</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi of"><img src="../Images/4ac6421a684990bd226d679b5dfd72d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xBCAMFB_5p6-mK9T.gif"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated">作者制作的动画</p></figure><p id="ca3d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如果我们将图像细分成几个部分，我们可以将一个长任务分成4个任务，每个任务之后我们可以释放UI线程:</p><pre class="kx ky kz la gt nv nu nw nx aw ny bi"><span id="0586" class="lt lu iq nu b gy nz oa l ob oc">const tensor = tf.browser.fromPixels(img);<br/>const [height, width] = tensor.shape;<br/>for (let i = 0; i &lt; 2; i++) {<br/>  for (let j = 0; j &lt; 2; j++) {<br/>    const slicedTensor = tensor.slice(<br/>      [(i * height) / 2, (j * width) / 2],<br/>      [height / 2, width / 2]<br/>    );<br/>    const prediction = model.predict(slicedTensor.expandDims(0)).squeeze();<br/>  }<br/>}</span></pre><p id="5022" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae kv" href="https://codesandbox.io/s/upscaling-patch-size-demonstration-pdki6?file=/src/index.js" rel="noopener ugc nofollow" target="_blank">这里有一个CodeSandbox链接演示这个</a>。</p><p id="e126" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这极大地提高了我们代码的响应能力，但现在又出现了一个新问题:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi og"><img src="../Images/9ed83881fe9f6e46479fbb272f865fe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b_UnjCuTOjdL8lTM.gif"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated">作者制作的动画</p></figure><p id="b467" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这些放大的图像往往在边缘有伪像。这是许多放大算法中固有的一个相当常见的问题，但它通常不是一个问题，除非你密切关注放大图像的边缘。然而，在这种情况下——因为我们将许多图像拼接成一幅图像——问题就更加明显了。</p><p id="95c6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">修复方法是给我们的每个图像切片添加<em class="ku">填充</em>——类似这样:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/0d4b16aa58c95051d951167205807ee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/0*Rzi64lgn84v7_Llk.gif"/></div><p class="li lj gj gh gi lk ll bd b be z dk translated">作者制作的动画</p></figure><p id="4f36" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">然后我们可以切掉多余的像素，组合成一幅没有任何伪像的图像。<a class="ae kv" href="https://codesandbox.io/s/upscaling-3-patch-sizes-8h6pt?file=/src/utils.ts" rel="noopener ugc nofollow" target="_blank">这里有一个代码沙箱，演示了端到端的</a>。</p><p id="c1f5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">最好的一点是，只要你把补丁的大小设置得足够小——比你期望得到的最小图像小——你就能得到大小一致的图像。还记得我们在预热部分提到的要求图像大小一致以获得速度提升的好处吗？这个解决方案双管齐下！</p><h2 id="cee6" class="lt lu iq bd lv lw lx dn ly lz ma dp mb kh mc md me kl mf mg mh kp mi mj mk iw bi translated">RRDN和寻找定制层</h2><p id="6dd3" class="pw-post-body-paragraph jw jx iq jy b jz ml kb kc kd mm kf kg kh mn kj kk kl mo kn ko kp mp kr ks kt ij bi translated">到目前为止，我们一直在处理RDN模型。RRDN模型是更强大的版本，它依赖于定制层，需要用Javascript重新实现。</p><p id="a525" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我在Tensorflow.js中没有找到大量关于自定义图层的文档。<a class="ae kv" href="https://www.tensorflow.org/js/guide/models_and_layers#custom_layers" rel="noopener ugc nofollow" target="_blank">有官方文档</a>，还有蔡的<a class="ae kv" href="https://gist.github.com/caisq/33ed021e0c7b9d0e728cb1dce399527d" rel="noopener ugc nofollow" target="_blank">这个要点</a>，这是我能找到的大部分内容。</p><p id="6986" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae kv" href="https://github.com/idealo/image-super-resolution/blob/master/ISR/models/rrdn.py#L191" rel="noopener ugc nofollow" target="_blank">Python中的两个自定义层定义为</a>:</p><pre class="kx ky kz la gt nv nu nw nx aw ny bi"><span id="2853" class="lt lu iq nu b gy nz oa l ob oc">class PixelShuffle(tf.keras.layers.Layer):<br/>    def __init__(self, scale, *args, **kwargs):<br/>        super(PixelShuffle, self).__init__(*args, **kwargs)<br/>        self.scale = scale<br/><br/>    def call(self, x):<br/>        return tf.nn.depth_to_space(x, block_size=self.scale, data_format='NHWC')<br/><br/>    def get_config(self):<br/>        config = super().get_config().copy()<br/>        config.update({<br/>            'scale': self.scale,<br/>        })<br/>        return config<br/><br/>class MultiplyBeta(tf.keras.layers.Layer):<br/>    def __init__(self, beta, *args, **kwargs):<br/>        super(MultiplyBeta, self).__init__(*args, **kwargs)<br/>        self.beta = beta<br/><br/>    def call(self, x, **kwargs):<br/>        return x * self.beta<br/><br/>    def get_config(self):<br/>        config = super().get_config().copy()<br/>        config.update({<br/>            'beta': self.beta,<br/>        })<br/>        return config</span></pre><p id="d5a1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在Javascript中，这些看起来像:</p><pre class="kx ky kz la gt nv nu nw nx aw ny bi"><span id="4dd7" class="lt lu iq nu b gy nz oa l ob oc">class MultiplyBeta extends tf.layers.Layer {<br/>  beta: number;<br/><br/>  constructor() {<br/>    super({});<br/>    this.beta = BETA;<br/>  }<br/><br/>  call(inputs: Inputs) {<br/>    return tf.mul(getInput(inputs), this.beta);<br/>  }<br/><br/>  static className = 'MultiplyBeta';<br/>}<br/><br/>class PixelShuffle extends tf.layers.Layer {<br/>  scale: number;<br/><br/>  constructor() {<br/>    super({});<br/>    this.scale = SCALE;<br/>  }<br/><br/>  computeOutputShape(inputShape: number[]) {<br/>    return [inputShape[0], inputShape[1], inputShape[2], 3];<br/>  }<br/><br/>  call(inputs: Inputs) {<br/>    return tf.depthToSpace(getInput(inputs), this.scale, 'NHWC');<br/>  }<br/><br/>  static className = 'PixelShuffle';<br/>}</span></pre><p id="5af0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">您还需要显式注册每个自定义层:</p><pre class="kx ky kz la gt nv nu nw nx aw ny bi"><span id="5744" class="lt lu iq nu b gy nz oa l ob oc">tf.serialization.registerClass(MultiplyBeta);<br/>tf.serialization.registerClass(PixelShuffle);</span></pre><p id="8de9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这里需要指出一些事情:</p><ul class=""><li id="75bd" class="nd ne iq jy b jz ka kd ke kh nf kl ng kp nh kt ni nj nk nl bi translated">确保您在层上定义了一个静态的<code class="fe nr ns nt nu b">className</code>，它与层的名称完全匹配</li><li id="fe2b" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt ni nj nk nl bi translated"><code class="fe nr ns nt nu b">call</code>是您进行大量计算的地方。</li><li id="621f" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt ni nj nk nl bi translated"><code class="fe nr ns nt nu b">computeOutputShape</code>我<em class="ku">相信</em>你只需要定义它是否不同，这个函数被调用来告诉TFJS你的输出张量的形状</li><li id="c2ec" class="nd ne iq jy b jz nm kd nn kh no kl np kp nq kt ni nj nk nl bi translated">你可能需要把函数调用从Python翻译成Javascript例如，Python中的<code class="fe nr ns nt nu b">tf.nn.depth_to_space</code>变成了Javascript中的<code class="fe nr ns nt nu b">tf.depthToSpace</code></li></ul><h2 id="1e7e" class="lt lu iq bd lv lw lx dn ly lz ma dp mb kh mc md me kl mf mg mh kp mi mj mk iw bi translated">训练您的模型</h2><p id="464a" class="pw-post-body-paragraph jw jx iq jy b jz ml kb kc kd mm kf kg kh mn kj kk kl mo kn ko kp mp kr ks kt ij bi translated">超分辨率技术的一个挑战是它们的规模是固定的。</p><p id="4e87" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这意味着，一个经过训练可以将图像放大2倍的模型，将无法放大3倍或4倍。它只能将图像放大到2倍。</p><p id="1ed0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">要改变规模，你需要从头开始训练一个模型。你可以想象，支持不同的规模可以大大增加你必须做的训练量。</p><p id="405d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">此外，有一些迹象表明，针对特定数据集的进一步培训会产生与您的领域相关的特定益处。</p><blockquote class="mx my mz"><p id="6314" class="jw jx ku jy b jz ka kb kc kd ke kf kg na ki kj kk nb km kn ko nc kq kr ks kt ij bi translated">首先，我们表明，较大的数据集导致更好的面向PSNR的方法的性能。我们使用一个大模型，其中23个残差中残差块(RRDB)放置在上采样层之前，随后是两个卷积层用于重建…一个广泛使用的训练数据集是DIV2K，包含800个图像。我们还探索了其他具有更多样化场景的数据集——Flickr 2K数据集，由Flickr网站上收集的2650张2K高分辨率图像组成。据观察，具有DIV2K和Flickr2K的合并数据集，即DF2K数据集，提高了PSNR性能。</p></blockquote><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oi"><img src="../Images/02945d3dce481a3401a586a936b53841.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MphoShbJwDUdHkT-.png"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated"><a class="ae kv" href="https://arxiv.org/pdf/1809.00219v2.pdf" rel="noopener ugc nofollow" target="_blank">图片来自ESRGAN paper </a></p></figure><p id="1149" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">针对特定领域的数据集进行训练可能会提高准确性。</p><p id="08e0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">去年，我花了一些时间和<a class="ae kv" href="https://ai.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html" rel="noopener ugc nofollow" target="_blank"> RAISR </a>一起工作。该论文中的一个关键见解是，压缩低分辨率图像会产生一个更具弹性的模型，能够更好地处理伪像，而锐化高分辨率图像会产生更具美感的放大图像(代价是相对于度量标准而言性能更差)。我怀疑——虽然我不确定——类似的技术可能在这里的训练中产生类似的好处，我目前正在试验寻找答案。</p><h2 id="ed58" class="lt lu iq bd lv lw lx dn ly lz ma dp mb kh mc md me kl mf mg mh kp mi mj mk iw bi translated">升级。射流研究…</h2><p id="449a" class="pw-post-body-paragraph jw jx iq jy b jz ml kb kc kd mm kf kg kh mn kj kk kl mo kn ko kp mp kr ks kt ij bi translated">我已经将所有这些打包成一个名为<a class="ae kv" href="https://github.com/thekevinscott/UpscalerJS" rel="noopener ugc nofollow" target="_blank"> Upscaler.js </a>的npm模型。</p><p id="f574" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">它不知道正在使用的升级模型，这意味着在未来，我将能够改进模型，并可能引入适应各种用例(人脸、插图)的模型。我目前通过JS CDNs服务模型，并期待在未来增加更多的模型。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi kw"><img src="../Images/a5743dea39b13241b891c04b9506a8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xUpSpVd3qVmqVagt.gif"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated">作者制作的动画</p></figure><p id="b31d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我认为有很多改进的机会，特别是在性能方面，但坦白地说，我很高兴这是可能的。</p><p id="7752" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">想象一下能够将此应用于视频流。想象一下，如果您能以普通视频流的形式提供文件大小为6%的视频，会怎么样？我们还没有到那一步——我们必须将工作速度提高10倍才能处理实时视频——但是离<em class="ku">已经不远了。想想真的很令人兴奋！</em></p></div></div>    
</body>
</html>