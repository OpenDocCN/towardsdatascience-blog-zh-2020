<html>
<head>
<title>Advanced Spark Tuning, Optimization, and Performance Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">先进的火花调谐、优化和性能技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/advanced-spark-tuning-optimization-and-performance-techniques-54f858c92e?source=collection_archive---------16-----------------------#2020-11-09">https://towardsdatascience.com/advanced-spark-tuning-optimization-and-performance-techniques-54f858c92e?source=collection_archive---------16-----------------------#2020-11-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="52a9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Apache Spark调优提示和技巧</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9fa526a6f982531e6397073f25d84a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pJPj-Ru5LipI0CBw"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@chuttersnap?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> CHUTTERSNAP </a>拍摄</p></figure><h1 id="6d1e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="5b56" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu"> <em class="mn"> Apache Spark </em> </strong>是一个分布式计算大数据分析框架，旨在跨一个机器集群转换、设计和处理海量数据(想想太字节和太字节)。它有许多用于特定任务的嵌入式组件，包括Spark SQL的结构化数据框架和结构化流API，这两者都将在本博客中讨论。Spark面临的一个挑战是向数据湖追加新数据，从而在写入时产生<em class="mn">‘小而倾斜的文件’</em>。完全解决这些挑战可能会很棘手，因此会对用户执行额外的下游Spark层、数据科学分析和使用<em class="mn">“小而扭曲的文件”</em>的SQL查询产生负面影响。相当新的框架<strong class="lt iu"> <em class="mn">三角洲湖泊</em> </strong>和<strong class="lt iu"> <em class="mn">阿帕奇胡迪</em> </strong>有助于解决这些问题。</p><p id="9710" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">然而，在这篇博客中，我将使用原生的<em class="mn"> Scala </em> API向你展示1。)如何在Spark <em class="mn">结构化流</em>作业中包含一个瞬态计时器，用于自动终止周期性数据处理添加新源数据，以及2 .)如何控制Spark作业产生的输出文件数量和分区大小。问题解决#1功能避免了总是为长期运行(有时空闲)<em class="mn">、【24/7】、</em>集群(即在<em class="mn"> Amazon EMR </em>中)付费。例如，对于只处理新的可用源数据(即在<em class="mn">亚马逊S3 </em>)而言，短期流作业是一个可靠的选择，这些数据没有一致的节奏到达；也许每小时都会有小批量的登陆。问题解决#2功能对于提高下游流程(如下一层Spark作业、SQL查询、数据科学分析和整体数据湖元数据管理)的I/O性能非常重要。</p><p id="75a0" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu"> <em class="mn">免责声明:本博客中使用的公共数据集包含非常小的数据量，仅用于演示目的。这些Spark技术最适用于真实世界的大数据量(即万亿字节&amp;千兆字节)。因此，相应地调整Spark clusters &amp;应用的规模、配置和调优。</em> </strong></p><h1 id="c070" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">示例1:火花流瞬态终止定时器</h1><p id="f499" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1a。)首先，让我们查看一些示例文件，并为从存储在<em class="mn">dbfs:/data bricks-datasets/structured-streaming/events/</em>的<em class="mn">data bricks Community Edition</em>中检索的公共物联网设备事件数据集定义模式。</p><p id="d759" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><code class="fe mt mu mv mw b">ls /blogs/source/devices.json/</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/926114db6672d16eda4bf66860cb626c.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/0*KOwdMpn58uIlWubh.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c514" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><code class="fe mt mu mv mw b">head /blogs/source/devices.json/file-0.json/</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/ebaeae56ce296c2b9ed6a80049ec730c.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/0*AqJ2rKd1DEvrs_83.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="5862" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">1b。)接下来，我们将把数据集作为定义了模式的流数据帧来读取，并包括函数参数:</p><ul class=""><li id="6fc9" class="nb nc it lt b lu mo lx mp ma nd me ne mi nf mm ng nh ni nj bi translated"><em class="mn"> maxFilesPerTrigger </em>(每个触发器读取的最大文件数)</li><li id="a01c" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><em class="mn">基本路径</em>(数据源位置)</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="5a2c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">1c。)现在，我们以<code class="fe mt mu mv mw b">parquet</code>文件接收器格式和<code class="fe mt mu mv mw b">append</code>模式执行流式查询，以确保仅周期性增量写入新数据，并包括函数参数:</p><ul class=""><li id="ab23" class="nb nc it lt b lu mo lx mp ma nd me ne mi nf mm ng nh ni nj bi translated"><em class="mn">测向</em>(源数据帧)</li><li id="85ee" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><em class="mn">重新分区</em>(每次触发时持续输出分区的数量)</li><li id="342d" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><em class="mn">检查点路径</em>(恢复检查点位置)</li><li id="7ec9" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><em class="mn">触发</em>(触发间隔处理时间)</li><li id="c7dc" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><em class="mn">目标路径</em>(数据目标位置)</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="ac6f" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">1d。)Scala sleep函数(以毫秒为单位)将用于在优雅的瞬态计时器上关闭流作业。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="4e77" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">1e。)最后，流式作业Spark会话将在定时器到期后执行，从而终止短期应用程序。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="00f9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">1f。)将函数应用于Scala值，并根据需要设置额外的Spark属性:</p><ul class=""><li id="011c" class="nb nc it lt b lu mo lx mp ma nd me ne mi nf mm ng nh ni nj bi translated"><code class="fe mt mu mv mw b">spark.sql.session.timeZone</code>(设置为<em class="mn"> UTC </em>以避免时间戳和时区不匹配问题)</li><li id="0a24" class="nb nc it lt b lu nk lx nl ma nm me nn mi no mm ng nh ni nj bi translated"><code class="fe mt mu mv mw b">spark.sql.shuffle.partitions</code>(设置在<em class="mn">宽“洗牌”转换</em>上创建的期望分区数量；价值因以下因素而异:1 .数据量&amp;结构，2。集群硬件&amp;分区大小，3。可用内核，4个。应用程序的意图)</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="b4bc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">1g。)查看作业的输出位置</p><p id="da0a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><code class="fe mt mu mv mw b">ls /blogs/target/devices.parquet/</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/8312473f9d7fb453c7f6ef7dc5470b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Si_9V3INdLpM2ceu.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="45d3" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">总之，流作业将只从源json位置向目标parquet位置连续处理、转换和附加微批未处理的数据。定时器超时(例如:5分钟)后，Spark应用程序正常关闭。对于Spark应用程序部署，最佳实践包括用包含作为命令行参数的<code class="fe mt mu mv mw b">args: Array[String]</code>的<code class="fe mt mu mv mw b">main()</code>方法定义Scala <code class="fe mt mu mv mw b">object</code>。然后创建一个所需的目录结构，用一个<code class="fe mt mu mv mw b">build.sbt</code>(库依赖)文件编译<code class="fe mt mu mv mw b">&lt;appName&gt;.scala</code>(应用程序代码)文件，所有这些都通过<em class="mn"> SBT </em>构建工具创建一个<em class="mn"> JAR </em>文件，该文件将用于通过<code class="fe mt mu mv mw b">spark-submit</code>运行应用程序。</p><p id="5809" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">下面是官方的<a class="ae ky" href="https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> Apache Spark文档</strong> </a>解释步骤。</p><p id="ed5b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在AWS中，通过<em class="mn"> Amazon EMR </em>您可以提交应用程序作为作业步骤，并在所有步骤完成时自动终止集群的基础设施。这可以通过像<em class="mn"> AWS步骤功能</em>、<em class="mn"> AWS Lambda </em>和<em class="mn"> Amazon CloudWatch </em>这样的服务来完全编排、自动化和安排。</p><p id="d9e1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">有时，流作业的输出文件大小会相当<em class="mn">【倾斜】</em>，这是由于源数据的到达节奏不规则，以及始终将其与流作业的触发器同步的时间挑战。示例2将有助于解决和优化<em class="mn">“小而歪斜的文件”</em>的困境。</p><p id="020a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">到下一个例子…</p><h1 id="ea2f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">示例2: Spark重新分区文件大小优化</h1><p id="513f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">2a。)首先，让我们查看一些样本文件，并读取我们的公共航空公司输入数据集(从存储在<em class="mn">dbfs:/data bricks-datasets/airlines/</em>的<em class="mn">data bricks Community Edition</em>中检索，并转换为用于演示目的的小拼花文件)并确定数据帧中的分区数量。</p><p id="fc43" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><code class="fe mt mu mv mw b">ls /blogs/source/airlines.parquet/</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/f19a13759b07b9042f2f11981721d368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wRIH9Sr6Y_XGblRF.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1515" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><code class="fe mt mu mv mw b">display(df)</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/b9618d7877ca631bbf311253bdafcc05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*i-R9o-rUj8UUMxy8.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="79c6" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">2b。)为了计算所需的输出分区(文件)大小，您需要通过将输入数据帧保存在内存中来估计其大小(即兆字节)。这可以通过执行<code class="fe mt mu mv mw b">df.cache()</code>或<code class="fe mt mu mv mw b">df.persist()</code>预先确定，调用类似<code class="fe mt mu mv mw b">df.count()</code>或<code class="fe mt mu mv mw b">df.foreach(x =&gt; println(x))</code>的动作来缓存整个数据帧，然后在<em class="mn">存储</em>选项卡下的<em class="mn"> Spark UI </em>中搜索数据帧的RAM大小。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="c27d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">2c。)Spark属性<code class="fe mt mu mv mw b">spark.default.parallelism</code>可以帮助确定数据帧的初始划分，也可以用于增加Spark并行性。通常，建议将该参数设置为集群中可用核心的数量乘以2或3。比如在<em class="mn"> Databricks社区版</em>中<code class="fe mt mu mv mw b">spark.default.parallelism</code>只有8个(<em class="mn">本地模式</em>单机，1个火花执行器，共8个内核)。对于真实的场景，我建议您避免在运行时或在笔记本中设置这个应用程序参数。在<em class="mn"> Amazon EMR </em>中，您可以在创建Spark集群的基础设施时附加一个配置文件，从而使用这个公式<code class="fe mt mu mv mw b">spark.default.parallelism = spark.executor.instances * spark.executors.cores * 2 (or 3)</code>实现更多的并行性。回顾一下，<code class="fe mt mu mv mw b">spark.executor.instances</code>属性是跨工作节点的JVM容器的总数。每个执行器都有一个通过<code class="fe mt mu mv mw b">spark.executor.cores</code>属性设置的通用固定数量的分配内部内核。</p><p id="b8f8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><em class="mn">【核心】</em>也被称为<em class="mn">【插槽】</em>或<em class="mn">【线程】</em>，负责并行执行Spark <em class="mn">【任务】</em>，这些任务被映射到Spark <em class="mn">【分区】</em>，也被称为<em class="mn">【文件中的数据块】</em>。</p><p id="d6a3" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这里是官方的<a class="ae ky" href="https://spark.apache.org/docs/latest/configuration.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> Apache Spark文档</strong> </a>解释了许多属性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="e7d0" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">2d。)新数据帧的分区值将取决于哪个整数值更大:<em class="mn">(默认并行度乘以乘数)</em>或<em class="mn">(大约。数据帧内存大小除以近似值。</em>所需分区大小。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="3a91" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">2e。)为了进行演示，缓存的数据帧大约为3，000 mb，所需的分区大小为128 mb。在本例中，计算出的分区大小<em class="mn"> (3，000除以128=~23) </em>大于默认的并行度乘数<em class="mn"> (8乘以2=16) </em>，因此选择值23作为重新分区的数据帧的新分区计数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="6a8d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">2f。)最后，我们查看一些示例输出分区，可以看到正好有23个文件(<em class="mn"> part-00000 </em>到<em class="mn"> part-00022 </em>)，每个文件的大小大约为127 mb (~127，000，000字节=~127 mb)，这接近于设置的128 mb目标大小，并且在优化的50到200 mb建议范围内。在所有分区中使用相同的优化文件大小解决了损害数据湖管理、存储成本和分析I/O性能的<em class="mn">“小且倾斜的文件”</em>问题。替代方法还包括按列对数据进行分区。例如，文件夹层级(即<em class="mn">年/月/日)</em>每天包含1个合并分区。具体的最佳实践会有所不同，取决于用例需求、数据量和数据结构。</p><p id="e61b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><code class="fe mt mu mv mw b">ls /blogs/optimized/airlines.parquet/</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/c0f1936395a1e703d9d07bce8283d004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Hylm3y0Kbk4AXOwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="61dd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="e84f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">从长远来看，希望您能看到像<code class="fe mt mu mv mw b">spark.sql.shuffle.partitions</code>和<code class="fe mt mu mv mw b">spark.default.parallelism</code>这样的Spark属性对您的Spark应用程序的性能有重大影响。在跨许多Spark工作节点处理大型数据集时，对这些Spark属性进行相应的调整以优化输出数量和分区大小至关重要。</p><p id="162b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">总之，在构建高可用性和容错数据湖、弹性机器学习管道、经济高效的云计算和存储节省，以及用于生成可重用的精选特征工程库的最佳I/O时，这些Spark技术在许多场合对我有用。但是，它们可能是也可能不是Spark社区中的官方最佳实践。好处可能取决于您的使用案例。此外，探索这些不同类型的调优、优化和性能技术具有巨大的价值，将帮助您更好地理解Spark的内部。对于持续学习、解决现实世界的问题和交付解决方案来说，创造力是开源软件和云计算最棒的事情之一。感谢你阅读这篇博客。</p></div></div>    
</body>
</html>