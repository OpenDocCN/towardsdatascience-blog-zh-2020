<html>
<head>
<title>Pyspark Data Manipulation Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pyspark数据操作教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-data-manipulation-tutorial-8c62652f35fa?source=collection_archive---------13-----------------------#2020-11-09">https://towardsdatascience.com/pyspark-data-manipulation-tutorial-8c62652f35fa?source=collection_archive---------13-----------------------#2020-11-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ad0e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="60f7" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">绝对火花初学者入门</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e7cf53ecafd51108ff5c6447ce612e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pMNPyuLRe327-I2d0vOJ8A.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片由<a class="ae le" href="https://pixabay.com/photos/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=839831" rel="noopener ugc nofollow" target="_blank">免费提供-照片</a>来自<a class="ae le" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=839831" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><h2 id="fe36" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">这个教程适合你吗？</h2><p id="66e3" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">本教程是为有一些Python经验的数据人员编写的，他们绝对是Spark初学者。它将帮助您安装Pyspark并启动您的第一个脚本。您将了解弹性分布式数据集(rdd)和数据帧，它们是Pyspark中的主要数据结构。我们讨论一些基本的概念，因为我认为它们会让你以后不会感到困惑和调试。您将学习数据转换以及从文件或数据库中读取数据。</p><h2 id="8995" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">为什么火花</h2><p id="5367" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">学习Spark的主要原因是，您将编写可以在大型集群中运行并处理大数据的代码。本教程只讨论Python API py Spark，但是你应该知道Spark APIs支持4种语言:Java、Scala和R以及Python。由于Spark core是用Java和Scala编程的，所以那些API是最完整和最有原生感觉的。</p><p id="76a8" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">Pyspark的优势在于Python已经有了许多数据科学的库，您可以将它们插入到管道中。再加上Python摇滚的事实！！！能让Pyspark真正有生产力。例如，如果你喜欢熊猫，你知道你可以用一个方法调用将Pyspark数据帧转换成熊猫数据帧。然后你可以对数据做一些事情，并用matplotlib绘制出来。</p><p id="925b" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">随着时间的推移，你可能会发现Pyspark几乎和pandas或sklearn一样强大和直观，并在你的大部分工作中使用它。试试Pyspark吧，它可能会成为你职业生涯中的下一件大事。</p><h1 id="bbd6" class="my lg iq bd lh mz na nb lk nc nd ne ln kf nf kg lr ki ng kj lv kl nh km lz ni bi translated">Pyspark快速启动</h1><p id="857f" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">有很多关于如何创建Spark集群、配置Pyspark向它们提交脚本等等的文章。所有这些都是在Spark上进行高性能计算所需要的。然而，在大多数公司中，他们会有数据或基础架构工程师来维护集群，而您只需编写和运行脚本。因此，您可能永远不会安装Spark集群，但是如果您需要，您可以在问题出现时处理它。OTOH，如果我告诉你，只需一个命令，你就可以在几分钟内开始工作，那会怎么样？</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="5bc0" class="lf lg iq nk b gy no np l nq nr">pip install pyspark</span></pre><p id="2541" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">这将在您的机器上下载并安装Pyspark，这样您就可以学习其中的诀窍，然后，当您得到一个集群时，使用它将会相对容易。</p><p id="0a77" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">为了支持本教程，我创建了这个GitHub库:</p><div class="ns nt gp gr nu nv"><a href="https://github.com/ArmandoRiveroPi/pyspark_tutorial" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ja gy z fp oa fr fs ob fu fw iz bi translated">ArmandoRiveroPi/py spark _教程</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">pyspark GitHub中的数据操作和机器学习介绍是超过5000万开发人员的家园，他们致力于…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">github.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj ky nv"/></div></div></a></div><p id="434a" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">示例中的大部分代码最好放在<a class="ae le" href="https://github.com/ArmandoRiveroPi/pyspark_tutorial/blob/master/tutorial_part_1_data_wrangling.py" rel="noopener ugc nofollow" target="_blank">tutorial _ part _ 1 _ data _ wrangling . py</a>文件中。</p><p id="ae4b" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">在加快速度之前，先给你点提示。我发现如果我不在主Python脚本的开头设置一些环境变量，Pyspark会抛出错误。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="1e05" class="lf lg iq nk b gy no np l nq nr">import sys, os<br/>environment = ['PYSPARK_PYTHON', 'PYSPARK_DRIVER_PYTHON']<br/><em class="ok">for </em>var <em class="ok">in </em>environment:<br/>    os.environ[var] = sys.executable</span></pre><p id="f711" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">之后，您可以创建spark会话</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="7d1c" class="lf lg iq nk b gy no np l nq nr"><em class="ok">from </em>pyspark.sql <em class="ok">import </em>SparkSession<br/>session = SparkSession.builder.getOrCreate()</span></pre><p id="b6d2" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">相信我，这是你开始工作所需要的。</p><h1 id="4d69" class="my lg iq bd lh mz na nb lk nc nd ne ln kf nf kg lr ki ng kj lv kl nh km lz ni bi translated">基本概念</h1><h2 id="53fd" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">火花建筑</h2><p id="9ae3" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">好了，我们可以进入正题了。首先，你必须知道Spark是复杂的。它不仅包含大量用于执行许多数据处理任务的代码，而且在执行分布在数百台机器上的代码时，它有望高效无误。在开始的时候，你真的不需要这种复杂性，你需要的是把事情抽象出来，忘记那些具体的细节。不过，重要的是司机和工人之间的区别。驱动程序是Python (single)流程，您在其中为工人发布订单。工人是听从司机命令的火花过程。您可能会认为这些工人“在云中”(Spark集群)。Spark假设大数据将分布在工人中，这些工人有足够的内存和处理能力来处理这些数据。预计驱动程序没有足够的资源来保存这么多的数据。这就是为什么你需要明确地说你什么时候想要移动数据到驱动程序。我们一会儿会谈到这一点，但要注意，在实际应用中，你应该非常小心你带给驱动程序的数据量。</p><p id="b09a" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><strong class="mc ja"> RDDs </strong></p><p id="67b1" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">弹性分布式数据集是Spark中最重要的数据结构之一，是数据帧的基础。您可以将它们视为“分布式”阵列。在许多方面，它们的行为类似于列表，一些细节我们将在下面讨论。</p><p id="02e3" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">那么，如何创建RDD呢？最直接的方法是“并行化”Python数组。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="2f8d" class="lf lg iq nk b gy no np l nq nr">rdd = session.sparkContext.parallelize([1,2,3])</span></pre><p id="9cf6" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">要开始与您的RDD互动，请尝试以下操作:</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="5185" class="lf lg iq nk b gy no np l nq nr">rdd.take(num=2)</span></pre><p id="ca84" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">这将为驾驶员带来RDD的前两个值。<em class="ok">计数</em>方法将返回RDD的长度</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="6470" class="lf lg iq nk b gy no np l nq nr">rdd.count()</span></pre><p id="7238" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">如果你想把所有的RDD数据作为一个数组发送给驱动程序，你可以使用<em class="ok"> collect </em></p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="5d25" class="lf lg iq nk b gy no np l nq nr">rdd.collect()</span></pre><p id="1a30" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">但是要小心，正如我们之前说过的，在实际应用中，这可能会使驱动程序崩溃，因为RDD的大小可能是千兆字节。一般来说，更喜欢使用<em class="ok"> take </em>传递您想要的行数，只有当您确定RDD或数据帧不太大时才调用<em class="ok"> collect </em>。</p><h2 id="79c8" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">数据帧</h2><p id="1b7b" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">如果你了解熊猫或R数据帧，你会非常清楚Spark数据帧代表什么。它们用命名列表示表格(矩阵)数据。在内部，它们是通过行对象的RDD实现的，这有点类似于名为tuple的Python。数据框架的最大优势在于，它们使您能够将SQL思维付诸实践。我们稍后将讨论数据帧操作，但是让我们开始创建一个数据帧，以便您可以使用它。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="4f82" class="lf lg iq nk b gy no np l nq nr">df = session.createDataFrame(<br/>  [[1,2,3], [4,5,6]], [‘column1’, ‘column2’, ‘column3’]<br/>)</span></pre><p id="c7e8" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">首先是数据矩阵，然后是列名。</p><p id="8cf3" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">可以像RDD案一样，尝试<em class="ok">拿</em>、<em class="ok">数</em>、<em class="ok">收</em>的方法；take and collect将为您提供一个行对象列表。但是对我来说，最友好的显示方式应该是<em class="ok"> show </em>:</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="2e2d" class="lf lg iq nk b gy no np l nq nr">df.show(n=3)</span></pre><p id="e287" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">它将用前n行打印数据帧的表格表示。</p><h2 id="3b81" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">不变</h2><p id="4652" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">Python列表的一个关键区别是rdd(以及数据帧)是不可变的。在并发应用程序和函数式语言中，经常需要不可变数据。</p><p id="186f" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">让我们讨论一下Python中不变性的含义。假设你这样做:</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="504f" class="lf lg iq nk b gy no np l nq nr">a = list(range(10))<br/>a.append(11)</span></pre><p id="9310" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">在这里，解释器首先创建一个名称“a ”,它指向一个列表对象，在第二行中，它修改了同一个对象。然而，当你这样做的时候</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="e934" class="lf lg iq nk b gy no np l nq nr">st = “my string”<br/>st += “ is pretty”</span></pre><p id="0850" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">解释器首先创建指向字符串对象的名称“st”，然后创建一个带有“my string is pretty”的全新字符串对象(在内存中的不同位置)，并将名称“<strong class="mc ja">ST”</strong>指向该新对象。发生这种情况是因为字符串是不可变的，你不能就地修改它们。</p><p id="79c9" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">同样，rdd和数据帧也不能就地修改，所以当您这样做时</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="a21f" class="lf lg iq nk b gy no np l nq nr">my_rdd.map(<em class="ok">lambda</em> <em class="ok">x</em>: x*100)</span></pre><p id="49c7" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">我的_rdd不变。你需要做什么</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="395b" class="lf lg iq nk b gy no np l nq nr">my_rdd = my_rdd.map(<em class="ok">lambda</em> <em class="ok">x</em>: x*100)</span></pre><p id="4f5c" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">以获得指向转换后的rdd对象的名称“my_rdd”。</p><h2 id="7253" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated"><strong class="ak">转换和动作</strong></h2><p id="bb38" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">与普通Python相比，一个更令人震惊的区别可能会让您在开始时感到困惑。有时您会注意到一个非常繁重的操作会立即发生。但是后来你做了一些小事情(比如打印RDD的第一个值),这似乎要花很长时间。</p><p id="9976" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">让我试着把这个问题简化很多来解释。在Spark中，<strong class="mc ja">变换</strong>和<strong class="mc ja">动作</strong>是有区别的。当你改变一个数据帧时，那是一个<strong class="mc ja">转换</strong>，然而，当你实际使用数据时(例如df.show(1))，那是一个<strong class="mc ja">动作</strong>。转换是延迟加载的，当你调用它们时它们不会运行。当您通过操作使用它们的结果时，它们就会被执行。然后，所有需要的转换将被一起计划、优化和运行。所以当你看到看起来瞬时的操作时，即使它们很重，那也是因为它们是转换，它们将在以后运行。</p><h1 id="63d6" class="my lg iq bd lh mz na nb lk nc nd ne ln kf nf kg lr ki ng kj lv kl nh km lz ni bi translated">数据帧操作</h1><h2 id="0655" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">用户定义函数</h2><p id="d508" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">用户定义的函数允许您使用Python代码对数据帧像元进行操作。你创建一个常规的Python函数，把它包装在一个UDF对象中，并把它传递给Spark，它会让你的函数在所有的工作器中可用，并安排它的执行来转换数据。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="b09e" class="lf lg iq nk b gy no np l nq nr"><em class="ok">import </em>pyspark.sql.functions <em class="ok">as </em>funcs<br/><em class="ok">import </em>pyspark.sql.types <em class="ok">as </em>types</span><span id="e6a6" class="lf lg iq nk b gy ol np l nq nr">def multiply_by_ten(number):<br/>    return number*10.0</span><span id="56cf" class="lf lg iq nk b gy ol np l nq nr">multiply_udf = funcs.udf(multiply_by_ten, types.DoubleType())</span><span id="af3a" class="lf lg iq nk b gy ol np l nq nr">transformed_df = df.withColumn(<br/>    'multiplied', multiply_udf('column1')<br/>)<br/>transformed_df.show()</span></pre><p id="27ea" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">首先你创建一个Python函数，它可以是一个对象中的方法，也是一个函数。然后创建一个UDF对象。恼人的部分是您需要定义输出类型，这是我们在Python中不太习惯的。要真正有效地使用UDF，你需要学习这些类型，特别是复合映射类型(如字典)和数组类型(如列表)。这样做的好处是，您可以将这个UDF传递给dataframe，告诉它将对哪一列进行操作，这样您就可以在不离开旧Python的舒适环境的情况下完成奇妙的事情。</p><p id="8a5c" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">然而，UDF的一个主要限制是，尽管<a class="ae le" href="https://stackoverflow.com/questions/42540169/pyspark-pass-multiple-columns-in-udf" rel="noopener ugc nofollow" target="_blank">它们可以接受几列作为输入</a>，但是它们不能整体改变行。如果您想要处理整行，您将需要RDD地图。</p><h2 id="c07d" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">RDD制图</h2><p id="fdd9" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">这很像使用UDF，你也传递一个常规的Python函数。但是在这种情况下，该函数将接收一个完整的行对象，而不是列值。预计它也将返回一整行。这将赋予您对行的最终权力，但有几个注意事项。首先:Row对象是不可变，所以您需要创建一个全新的行并返回它。第二:你需要将数据帧转换成RDD，然后再转换回来。幸运的是，这些问题都不难克服。</p><p id="3c98" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">让我向您展示一个函数，它将对数转换您的数据帧中的所有列。作为一个不错的尝试，它还将每个列名转换为“log(column_name)”。我发现最简单的方法是用row.asDict()将行放入字典。这个函数有点Python的魔力，比如字典理解和关键字参数打包(双星形)。希望你能全部理解。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="6f13" class="lf lg iq nk b gy no np l nq nr"><em class="ok">import </em>pyspark.sql.types <em class="ok">as </em>types</span><span id="7571" class="lf lg iq nk b gy ol np l nq nr"><em class="ok">def </em>take_log_in_all_columns(row: types.Row):<br/>     old_row = row.asDict()<br/>     new_row = {f'log({column_name})': math.log(value) <br/>                <em class="ok">for </em>column_name, value <em class="ok">in </em>old_row.items()}<br/>     <em class="ok">return </em>types.Row(**new_row)</span></pre><p id="55c7" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">这本身不会做任何事情。你需要执行地图。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="1286" class="lf lg iq nk b gy no np l nq nr">logarithmic_dataframe = df.rdd.map(take_log_in_all_columns).toDF()</span></pre><p id="efaa" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">您会注意到这是一个链式方法调用。首先调用rdd，它将为您提供存储数据帧行的底层RDD。然后在这个RDD上应用map，在这里传递函数。要关闭，您可以调用toDF()，它将行的RDD转换为数据帧。进一步讨论见<a class="ae le" href="https://stackoverflow.com/questions/39699107/spark-rdd-to-dataframe-python" rel="noopener ugc nofollow" target="_blank">这个栈溢出问题</a>。</p><h2 id="1330" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">SQL操作</h2><p id="5f0b" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">由于数据帧表示表，自然地，它们被赋予了类似SQL的操作。为了让你兴奋，我将只提到其中的几个，但是你可以期待找到几乎同构的功能。调用<em class="ok"> select </em>将返回一个只有一些原始列的数据帧。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="64a0" class="lf lg iq nk b gy no np l nq nr">df.select('column1', 'column2')</span></pre><p id="784f" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">对<em class="ok">的调用，其中</em>将返回一个数据帧，其中只有列1的值为3的行。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="a341" class="lf lg iq nk b gy no np l nq nr">df.where('column1 = 3')</span></pre><p id="61ab" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">这个对<em class="ok"> join </em>的调用将返回一个dataframe，也就是，嗯…，一个df和df1通过column1的连接，就像SQL中的内部连接一样。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="d4b9" class="lf lg iq nk b gy no np l nq nr">df.join(df1, [‘column1’], how=’inner’)</span></pre><p id="9088" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">如果您需要执行右或左连接，在我们的例子中，df就像左边的表，而df1就是右边的表。外部连接也是可能的。</p><p id="7e0c" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">但是除此之外，在Spark中，您可以更直接地执行SQL。您可以从数据帧中创建时态视图</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="4d9a" class="lf lg iq nk b gy no np l nq nr">df.createOrReplaceTempView(“table1”)</span></pre><p id="8c6d" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">然后在视图上执行查询</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="6d81" class="lf lg iq nk b gy no np l nq nr">df2 = session.sql("SELECT column1 AS f1, column2 as f2 from table1")</span></pre><p id="80c3" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">这些查询将返回一个具有相应列名和值的新数据帧。</p><h2 id="8b33" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">数据帧列操作</h2><p id="014a" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">列抽象支持像算术这样的直接操作，例如，假设您想将列1相加，再加上列2和列3的乘积。那你可以</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="e12e" class="lf lg iq nk b gy no np l nq nr">df3 = df.withColumn(<br/>    'derived_column', df['column1'] + df['column2'] * df['column3']<br/>)</span></pre><h2 id="9bcc" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated"><strong class="ak">聚合和快速统计</strong></h2><p id="6133" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">对于这个例子，我们需要从一个CSV文件中读取。我从<a class="ae le" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/adult</a>下载了成人数据文件</p><p id="478f" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">我将文件移动到我的<a class="ae le" href="https://github.com/ArmandoRiveroPi/pyspark_tutorial" rel="noopener ugc nofollow" target="_blank">存储库</a>中的<em class="ok">数据</em>文件夹，并将其重命名为成人.数据. csv。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="a8fc" class="lf lg iq nk b gy no np l nq nr">ADULT_COLUMN_NAMES = [<br/>     "age",<br/>     "workclass",<br/>     "fnlwgt",<br/>     "education",<br/>     "education_num",<br/>     "marital_status",<br/>     "occupation",<br/>     "relationship",<br/>     "race",<br/>     "sex",<br/>     "capital_gain",<br/>     "capital_loss",<br/>     "hours_per_week",<br/>     "native_country",<br/>     "income"<br/> ]</span></pre><p id="88ec" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">读取文件后，我使用这个列表设置列名，因为文件没有标题。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="0990" class="lf lg iq nk b gy no np l nq nr">csv_df = session.read.csv(<br/>     'data/adult.data.csv', header=<em class="ok">False</em>, inferSchema=<em class="ok">True</em><br/> )</span><span id="3dbf" class="lf lg iq nk b gy ol np l nq nr"><em class="ok">for </em>new_col, old_col <em class="ok">in zip</em>(ADULT_COLUMN_NAMES, csv_df.columns):<br/>     csv_df = csv_df.withColumnRenamed(old_col, new_col)</span></pre><p id="b5f0" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">之后，您可以尝试下面的一些快速描述性统计</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="51ae" class="lf lg iq nk b gy no np l nq nr">csv_df.describe().show()</span></pre><p id="db01" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">若要获取聚合，请将groupBy与agg方法一起使用。例如，下面将为您提供一个数据框架，其中包含按年龄组划分的平均工作时间和标准偏差。我们也按年龄对数据帧进行排序。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="d263" class="lf lg iq nk b gy no np l nq nr">work_hours_df = csv_df.groupBy(<br/>    'age'<br/>).agg(<br/>    funcs.avg('hours_per_week'),<br/>    funcs.stddev_samp('hours_per_week')<br/>).sort('age')</span></pre><h1 id="b98a" class="my lg iq bd lh mz na nb lk nc nd ne ln kf nf kg lr ki ng kj lv kl nh km lz ni bi translated">连接到数据库</h1><p id="19e9" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">您很可能将数据保存在关系数据库中，由MySQL或PostgreSQL之类的RDBMS处理。如果是这样的话，不要担心，Spark有办法与多种数据存储进行交互。我敢打赌，你可以通过谷歌搜索到你可能有的大多数IO需求。</p><p id="20cc" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">要连接到RDBMS，您需要一个JDBC驱动程序，这是一个Spark可以用来与数据库对话的jar文件。我将向您展示一个PostgreSQL示例。</p><p id="1555" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">首先你需要下载<a class="ae le" href="https://jdbc.postgresql.org/download.html" rel="noopener ugc nofollow" target="_blank">驱动</a>。</p><p id="df91" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">在我的GitHub repo中，我把JDBC驱动程序放在bin文件夹中，但是你可以在任何路径中找到它。此外，您应该知道本例中的代码位于repo中的另一个<a class="ae le" href="https://github.com/ArmandoRiveroPi/pyspark_tutorial/blob/master/tutorial_part_1_reading_from_database.py" rel="noopener ugc nofollow" target="_blank">文件</a>中。我必须这样做，因为与其他文件不同，这个文件会抛出错误，因为数据库凭证当然是假的。您需要用您的数据库来替换它们以使其运行。</p><p id="81b2" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">现在，您将通过一个额外的步骤启动Pyspark:</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="48d3" class="lf lg iq nk b gy no np l nq nr">session = SparkSession.builder.config(<br/>    'spark.jars', 'bin/postgresql-42.2.16.jar'<br/>).config(<br/>    'spark.driver.extraClassPath', 'bin/postgresql-42.2.16.jar'<br/>).getOrCreate()</span></pre><p id="ef0e" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">这将创建一个可识别驱动程序的会话。然后，您可以像这样从数据库中读取(用您的真实配置替换假配置):</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="d3cf" class="lf lg iq nk b gy no np l nq nr">url = f"jdbc:postgresql://your_host_ip:5432/your_database"<br/>properties = {'user': 'your_user', 'password': 'your_password'}<br/><em class="ok"># read from a table into a dataframe<br/></em>df = session.read.jdbc(<br/>    url=url, table='your_table_name', properties=properties<br/>)</span></pre><p id="39b1" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">然后，您可以以任何方式创建转换后的数据帧，并将数据写回数据库(可能在不同的表中)。</p><pre class="kp kq kr ks gt nj nk nl nm aw nn bi"><span id="8d3f" class="lf lg iq nk b gy no np l nq nr">transformed_df.write.jdbc(<br/>    url=url, table='new_table', mode='append', properties=properties<br/>)</span></pre><p id="1af2" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">根据文档，写入模式有:</p><ul class=""><li id="0305" class="om on iq mc b md mt mg mu lo oo ls op lw oq ms or os ot ou bi translated">append:将此数据帧的内容追加到现有数据中。</li><li id="232c" class="om on iq mc b md ov mg ow lo ox ls oy lw oz ms or os ot ou bi translated">覆盖:覆盖现有数据。</li><li id="b859" class="om on iq mc b md ov mg ow lo ox ls oy lw oz ms or os ot ou bi translated">忽略:如果数据已经存在，则忽略此操作。</li><li id="563d" class="om on iq mc b md ov mg ow lo ox ls oy lw oz ms or os ot ou bi translated">error或“errorifexists”(默认情况):如果数据已经存在，则抛出异常。</li></ul><h1 id="8f79" class="my lg iq bd lh mz na nb lk nc nd ne ln kf nf kg lr ki ng kj lv kl nh km lz ni bi translated">结论</h1><p id="948f" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">我希望你受到鼓励，马上开始学习Pyspark。我的意思是，这将需要几周的时间来提高效率，这取决于你每天能花多少时间，但之后你就可以用Pyspark来执行大量的数据处理了。万一你不能选择你在工作中使用的，至少你可以在你的空闲时间学习它。我的介绍几乎没有触及表面，主要是希望给你一个诱人的咬。比熊猫或者sklearn难不了多少。</p><p id="a1d4" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">熟练掌握Spark，它可以为您打开大数据的大门。</p><p id="54ce" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">第2部分将介绍基本的分类和回归。</p><h1 id="beaf" class="my lg iq bd lh mz na nb lk nc nd ne ln kf nf kg lr ki ng kj lv kl nh km lz ni bi translated">进一步阅读</h1><p id="1d8d" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated"><em class="ok"> PySpark食谱</em>作者Raju Kumar Mishra。Apress，2018。</p><p id="6a56" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">官方文件</p><p id="c977" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">【https://spark.apache.org/docs/latest/api/python/ T4】</p></div></div>    
</body>
</html>