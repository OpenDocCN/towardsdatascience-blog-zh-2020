<html>
<head>
<title>Model Sub-Classing and Custom Training Loop from Scratch in TensorFlow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 2中从头开始的模型子分类和自定义训练循环</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e?source=collection_archive---------0-----------------------#2020-11-08">https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e?source=collection_archive---------0-----------------------#2020-11-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9588" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对TF.Keras中模型子类化和自定义训练循环的各种组件的简单介绍。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/214dfa680313cb466617e543d70d1566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lkP8y-x7n6DXli_S6-4I4g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae kv" href="https://unsplash.com/t/nature?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@nima_sarram?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">尼玛·萨拉姆</a>拍摄</p></figure><p id="58b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将尝试从<strong class="ky ir"> TensorFlow 2 </strong>中的开始理解<strong class="ky ir">模型子类API </strong>和<strong class="ky ir">自定义训练循环。这可能不是一个初学者或高级介绍，但目的是获得他们都是什么粗略的直觉。本帖分为三个部分:</strong></p><ol class=""><li id="d784" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">tensor flow 2中的可比建模策略</strong></li><li id="c790" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">用模型子类化API建立一个初始网络</strong></li><li id="f449" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">端到端培训，从头开始定制培训循环</strong></li></ol><p id="b6e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，首先，我们将了解使用TensorFlow 2定义模型的几种方法以及它们之间的区别。接下来，我们将看到使用TF 2中引入的模型子类化API来构建一个复杂的神经架构是多么的可行。然后我们将实现一个定制的训练循环，从头开始端到端地训练这些子类化模型。我们还将在我们的定制训练循环中使用Tensorboard来跟踪每一批的模型性能。我们还将看到如何在训练后保存和加载模型。最后，我们将通过混淆矩阵和分类报告等来衡量模型的性能。</p><h1 id="15da" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">TensorFlow 2中的可比建模策略</h1><p id="e307" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在<strong class="ky ir">中<em class="nd"> TF。Keras </em> </strong>我们基本上有三种方式可以定义一个神经网络，即</p><ul class=""><li id="a3cb" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr ne ly lz ma bi translated"><strong class="ky ir">顺序API </strong></li><li id="3a06" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr ne ly lz ma bi translated"><strong class="ky ir">功能API </strong></li><li id="e831" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr ne ly lz ma bi translated"><strong class="ky ir">模型子类化API </strong></li></ul><p id="ed9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中，<strong class="ky ir">顺序API </strong>是最容易实现的方法，但有一定的局限性。例如，使用这个API，我们不能创建一个与另一个层共享特性信息的模型，除了它的后续层。此外，多输入和多输出也不可能实现。在这一点上，<strong class="ky ir">功能API </strong>确实很好地解决了这些问题。像Inception或ResNet这样的模型在函数式API中实现是可行的。但深度学习研究人员通常希望对网络和训练管道的每个细微差别有更多的控制，这正是<strong class="ky ir">模型子类化API </strong>所服务的。模型子类化是一种完全可定制的方式，以面向对象的方式实现定制设计的深度神经网络的前馈机制。</p><p id="5252" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们使用这三个API创建一个非常基本的神经网络。这将是相同的神经架构，并会看到什么是实现差异。这当然不会展示全部潜力，尤其是对于<strong class="ky ir">功能</strong>和<strong class="ky ir">模型子类API </strong>。架构将如下所示:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="1277" class="nk mh iq ng b gy nl nm l nn no">Input - &gt; Conv - &gt; MaxPool - &gt; BN - &gt; Conv -&gt; BN - &gt; Droput - &gt; GAP -&gt; Dense</span></pre><p id="ef36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很简单。如上所述，让我们分别用顺序、功能和模型子类创建神经网络。</p><h2 id="521a" class="nk mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">顺序API</h2><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="921f" class="od mh iq ng b be oe of l og no"># declare input shape <br/>seq_model = tf.keras.Sequential()<br/>seq_model.add(tf.keras.Input(shape=imput_dim))<br/><br/># Block 1<br/>seq_model.add(tf.keras.layers.Conv2D(32, 3, strides=2, activation="relu"))<br/>seq_model.add(tf.keras.layers.MaxPooling2D(3))<br/>seq_model.add(tf.keras.layers.BatchNormalization())<br/><br/># Block 2<br/>seq_model.add(tf.keras.layers.Conv2D(64, 3, activation="relu"))<br/>seq_model.add(tf.keras.layers.BatchNormalization())<br/>seq_model.add(tf.keras.layers.Dropout(0.3))<br/><br/># Now that we apply global max pooling.<br/>seq_model.add(tf.keras.layers.GlobalMaxPooling2D())<br/><br/># Finally, we add a classification layer.<br/>seq_model.add(tf.keras.layers.Dense(output_dim))</span></pre><h2 id="c480" class="nk mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">功能API</h2><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="f1e5" class="od mh iq ng b be oe of l og no"># declare input shape <br/>input = tf.keras.Input(shape=(imput_dim))<br/><br/># Block 1<br/>x = tf.keras.layers.Conv2D(32, 3, strides=2, activation="relu")(input)<br/>x = tf.keras.layers.MaxPooling2D(3)(x)<br/>x = tf.keras.layers.BatchNormalization()(x)<br/><br/># Block 2<br/>x = tf.keras.layers.Conv2D(64, 3, activation="relu")(x)<br/>x = tf.keras.layers.BatchNormalization()(x)<br/>x = tf.keras.layers.Dropout(0.3)(x)<br/><br/># Now that we apply global max pooling.<br/>gap = tf.keras.layers.GlobalMaxPooling2D()(x)<br/><br/># Finally, we add a classification layer.<br/>output = tf.keras.layers.Dense(output_dim)(gap)<br/><br/># bind all<br/>func_model = tf.keras.Model(input, output)</span></pre><h2 id="5709" class="nk mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">模型子类API</h2><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="f56b" class="od mh iq ng b be oe of l og no">class ModelSubClassing(tf.keras.Model):<br/>    def __init__(self, num_classes):<br/>        super(ModelSubClassing, self).__init__()<br/>        # define all layers in init<br/>        # Layer of Block 1<br/>        self.conv1 = tf.keras.layers.Conv2D(<br/>                          32, 3, strides=2, activation="relu"<br/>                     )<br/>        self.max1  = tf.keras.layers.MaxPooling2D(3)<br/>        self.bn1   = tf.keras.layers.BatchNormalization()<br/><br/>        # Layer of Block 2<br/>        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation="relu")<br/>        self.bn2   = tf.keras.layers.BatchNormalization()<br/>        self.drop  = tf.keras.layers.Dropout(0.3)<br/><br/>        # GAP, followed by Classifier<br/>        self.gap   = tf.keras.layers.GlobalAveragePooling2D()<br/>        self.dense = tf.keras.layers.Dense(num_classes)<br/><br/><br/>    def call(self, input_tensor, training=False):<br/>        # forward pass: block 1 <br/>        x = self.conv1(input_tensor)<br/>        x = self.max1(x)<br/>        x = self.bn1(x)<br/><br/>        # forward pass: block 2 <br/>        x = self.conv2(x)<br/>        x = self.bn2(x)<br/><br/>        # droput followed by gap and classifier<br/>        x = self.drop(x)<br/>        x = self.gap(x)<br/>        return self.dense(x)</span></pre><p id="4a4a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<strong class="ky ir">模型子类</strong>中有两个最重要的函数<strong class="ky ir"> <em class="nd"> __init__ </em> </strong>和<strong class="ky ir"> <em class="nd">调用</em> </strong>。基本上，我们将在<strong class="ky ir"> <em class="nd"> __init__ </em> </strong>方法中定义所有可训练的<strong class="ky ir"> <em class="nd"> tf.keras </em> </strong>层或自定义实现的层，并在用于执行前向传播的<strong class="ky ir"> <em class="nd"> call </em> </strong>方法中根据我们的网络设计调用这些层。(反正和PyTorch里用来建模型的<strong class="ky ir"> <em class="nd"> forward </em> </strong>方法挺像的。)</p><p id="3f66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们在MNIST数据集上运行这些模型。我们将从<strong class="ky ir"><em class="nd">TF . keras . datasets</em></strong><em class="nd">加载。</em>然而，输入图像是<strong class="ky ir"> 28 </strong>乘<strong class="ky ir"> 28 </strong>并且是灰度形状。我们将重复这个轴三次，这样如果需要的话，我们就可以用预训练的重量进行可行的实验。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="da70" class="od mh iq ng b be oe of l og no">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()<br/><br/># x_train.shape, y_train.shape: (60000, 28, 28) (60000,)<br/># x_test.shape,  y_test.shape : (10000, 28, 28) (10000,)<br/><br/># train set / data <br/>x_train = np.expand_dims(x_train, axis=-1)<br/>x_train = np.repeat(x_train, 3, axis=-1)<br/>x_train = x_train.astype('float32') / 255<br/># train set / target <br/>y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)<br/><br/><br/># validation set / data <br/>x_test = np.expand_dims(x_test, axis=-1)<br/>x_test = np.repeat(x_test, 3, axis=-1)<br/>x_test = x_test.astype('float32') / 255<br/># validation set / target <br/>y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)<br/><br/><br/># ------------------------------------------------------------------<br/># compile <br/>print('Sequential API')<br/>seq_model.compile(<br/>          loss      = tf.keras.losses.CategoricalCrossentropy(),<br/>          metrics   = tf.keras.metrics.CategoricalAccuracy(),<br/>          optimizer = tf.keras.optimizers.Adam())<br/># fit <br/>seq_model.fit(x_train, y_train, batch_size=128, epochs=1)<br/><br/><br/># compile <br/>print('\nFunctional API')<br/>func_model.compile(<br/>          loss      = tf.keras.losses.CategoricalCrossentropy(),<br/>          metrics   = tf.keras.metrics.CategoricalAccuracy(),<br/>          optimizer = tf.keras.optimizers.Adam())<br/># fit <br/>func_model.fit(x_train, y_train, batch_size=128, epochs=1)<br/><br/><br/># compile <br/>print('\nModel Sub-Classing API')<br/>sub_classing_model = ModelSubClassing(10)<br/>sub_classing_model.compile(<br/>          loss      = tf.keras.losses.CategoricalCrossentropy(),<br/>          metrics   = tf.keras.metrics.CategoricalAccuracy(),<br/>          optimizer = tf.keras.optimizers.Adam())<br/># fit <br/>sub_classing_model.fit(x_train, y_train, batch_size=128, epochs=1);</span></pre><p id="9127" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="4db8" class="nk mh iq ng b gy nl nm l nn no">Sequential API<br/>469/469 [==============================] - 2s 3ms/step - loss: 7.5747 - categorical_accuracy: 0.2516<br/><br/>Functional API<br/>469/469 [==============================] - 2s 3ms/step - loss: 8.1335 - categorical_accuracy: 0.2368<br/><br/>Model Sub-Classing API<br/>469/469 [==============================] - 2s 3ms/step - loss: 5.2695 - categorical_accuracy: 0.1731</span></pre><h1 id="2a43" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">用模型子类API构建一个初始网络</h1><p id="889e" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">TF中的核心数据结构。Keras是<strong class="ky ir">层</strong>和<strong class="ky ir">模型</strong>类。一层封装了<strong class="ky ir"> <em class="nd">状态</em> </strong>(权重)和从输入到输出的转换，即<strong class="ky ir"> <em class="nd">调用</em> </strong>方法，用于定义正向传递。然而，这些层也是递归可组合的。这意味着如果我们将一个<strong class="ky ir"><em class="nd">TF . keras . layers . layer</em></strong>实例指定为另一个<strong class="ky ir"><em class="nd">TF . keras . layer</em></strong>的属性，外层将开始跟踪内层的权重矩阵。因此，每一层将跟踪其子层的权重，包括可训练的和不可训练的。当我们需要构建这样一个更高级别的抽象层时，就需要这样的功能。</p><p id="10f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这一部分中，我们将通过对层和模型类进行子类化来构建一个小型的初始模型。请看下图。这是一个小<strong class="ky ir">盗梦空间</strong>网络<a class="ae kv" href="https://arxiv.org/pdf/1611.03530.pdf" rel="noopener ugc nofollow" target="_blank"> src </a>。如果仔细观察，我们会发现它主要由三个特殊模块组成，即:</p><ol class=""><li id="625a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir"> <em class="nd"> Conv模块</em> </strong></li><li id="05d9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> <em class="nd">盗梦模块</em> </strong></li><li id="40ad" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> <em class="nd">下采样模块</em> </strong></li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/fb05c1f5b8956aa6a7248cdc85b30ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CAFo0A-z6w7xn8Le"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一个小型的盗梦空间网络，<a class="ae kv" href="https://arxiv.org/pdf/1611.03530.pdf" rel="noopener ugc nofollow" target="_blank"> src </a>。</p></figure><h2 id="1aee" class="nk mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">Conv模块</h2><p id="ab75" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">从图中我们可以看到，它由一个卷积网络、一个批量归一化和一个relu激活组成。同样，它用<strong class="ky ir"> <em class="nd"> K x K </em> </strong>滤镜和<strong class="ky ir"> <em class="nd"> S x S </em> </strong>步距生成<strong class="ky ir"> <em class="nd"> C </em> </strong>倍特征图。现在，如果我们简单地采用顺序建模方法，效率会非常低，因为我们将在整个网络中多次重复使用该模块。因此，定义一个功能块将是足够简单有效的。但是这一次，我们会更喜欢<em class="nd">层子类</em>更<em class="nd">python化</em>更高效。为此，我们将创建一个将继承<strong class="ky ir"><em class="nd">TF . keras . layers . layer</em></strong>类的类对象。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="c467" class="od mh iq ng b be oe of l og no">class ConvModule(tf.keras.layers.Layer):<br/>     def __init__(<br/>        self, kernel_num, kernel_size, strides, padding='same'<br/>    ):<br/>        super(ConvModule, self).__init__()<br/>        # conv layer<br/>        self.conv = tf.keras.layers.Conv2D(<br/>                    kernel_num, <br/>                    kernel_size=kernel_size, <br/>                    strides=strides, <br/>                    padding=padding<br/>                )<br/>        # batch norm layer<br/>         self.bn   = tf.keras.layers.BatchNormalization()<br/><br/><br/>      def call(self, input_tensor, training=False):<br/>        x = self.conv(input_tensor)<br/>        x = self.bn(x, training=training)<br/>        x = tf.nn.relu(x)<br/>        return x</span></pre><p id="4100" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们还可以初始化该类的对象，并查看以下属性。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="84c4" class="nk mh iq ng b gy nl nm l nn no">cm = ConvModule(96, (3,3), (1,1))<br/>y = cm(tf.ones(shape=(2,32,32,3))) # first call to the `cm` will create weights<br/><br/>print("weights:", len(cm.weights))<br/>print("trainable weights:", len(cm.trainable_weights))<br/><br/><br/># output<br/>weights: 6<br/>trainable weights: 4</span></pre><h2 id="ba9b" class="nk mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">初始模块</h2><p id="d7a0" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">接下来是初始模块。根据上图，它由两个<strong class="ky ir"> <em class="nd">卷积模块</em> </strong> <em class="nd"> </em>组成，然后合并在一起。现在我们知道要合并，这里我们需要确保输出特征图的尺寸(<strong class="ky ir"> <em class="nd">高度</em> </strong>和<strong class="ky ir"> <em class="nd">宽度</em> </strong>)需要相同。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="d39e" class="od mh iq ng b be oe of l og no">class InceptionModule(tf.keras.layers.Layer):<br/>    def __init__(self, kernel_size1x1, kernel_size3x3):<br/>        super(InceptionModule, self).__init__()<br/>        # two conv modules: they will take same input tensor <br/>        self.conv1 = ConvModule(<br/>                        kernel_size1x1, kernel_size=(1,1), strides=(1,1)<br/>                      )<br/>        self.conv2 = ConvModule(<br/>                        kernel_size3x3, kernel_size=(3,3), strides=(1,1)<br/>                      )<br/>        self.cat   = tf.keras.layers.Concatenate()<br/><br/><br/>    def call(self, input_tensor, training=False):<br/>        x_1x1 = self.conv1(input_tensor)<br/>        x_3x3 = self.conv2(input_tensor)<br/>        x = self.cat([x_1x1, x_3x3])<br/>        return x </span></pre><p id="7d00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里你可能会注意到，我们现在根据网络(图)对两个卷积层的确切的<strong class="ky ir"> <em class="nd">内核大小</em> </strong>和<strong class="ky ir"> <em class="nd">步长</em> </strong>数进行了硬编码。同样在<strong class="ky ir"><em class="nd">conv module</em></strong><em class="nd">，</em>中我们已经将<strong class="ky ir"> <em class="nd"> padding </em> </strong>设置为“<em class="nd">same”</em>，这样两者(<strong class="ky ir"> <em class="nd"> self.conv1 </em> </strong>和<strong class="ky ir"> <em class="nd"> self.conv2 </em> </strong>)的特征图的尺寸将是相同的；这是将它们连接到末尾所需要的。</p><p id="b496" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，在这个模块中，两个变量作为占位符执行，<strong class="ky ir"><em class="nd">kernel _ size1x 1</em></strong>和<strong class="ky ir"><em class="nd">kernel _ size3x 3</em></strong><em class="nd">。</em>这是为了当然的目的。因为在整个模型的不同阶段，我们需要不同数量的特征地图。如果我们查看模型的图表，我们会看到<strong class="ky ir"><em class="nd">inception module</em></strong>在模型的不同阶段采用不同数量的过滤器。</p><h2 id="10bd" class="nk mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">下采样模块</h2><p id="c95d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">最后是下采样模块。下采样的主要直觉是，我们希望获得更相关的特征信息，这些信息高度代表模型的输入。因为它倾向于移除不需要的特征，以便模型可以集中于最相关的特征。有许多方法可以降低特征图(或输入)的维数。例如:使用<strong class="ky ir"> <em class="nd">跨步</em> 2 </strong>或使用常规<strong class="ky ir"> <em class="nd">拼成</em> </strong>操作。池化操作有很多种，分别是:<strong class="ky ir"> MaxPooling、AveragePooling、GlobalAveragePooling </strong>。</p><p id="23b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从图中，我们可以看到下采样模块包含一个卷积层和一个最大池层，它们后来合并在一起。现在，如果我们仔细观察该图(右上角)，我们会看到卷积层采用了一个<strong class="ky ir"> 3 </strong> x <strong class="ky ir"> 3 </strong>大小的滤波器，步长为<strong class="ky ir"> 2 </strong> x <strong class="ky ir"> 2 </strong>。而池层(此处<em class="nd"> MaxPooling </em>)取池大小<strong class="ky ir"> 3 </strong> x <strong class="ky ir"> 3 </strong>步长<strong class="ky ir"> 2 </strong> x <strong class="ky ir"> 2 </strong>。然而，公平地说，我们也需要确保来自它们中每一个的维度应该是相同的，以便最终合并。现在，如果我们还记得，当我们设计<strong class="ky ir"><em class="nd"/></strong>conv module时，我们特意将<strong class="ky ir"> <em class="nd"> padding </em> </strong>参数的值设置为`<em class="nd"> same </em>'。但在这种情况下，我们需要将其设置为`<em class="nd">有效</em>`。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="9cdc" class="od mh iq ng b be oe of l og no">class DownsampleModule(tf.keras.layers.Layer):<br/>    def __init__(self, kernel_size):<br/>        super(DownsampleModule, self).__init__()<br/>        # conv layer<br/>        self.conv3 = ConvModule(<br/>                  kernel_size, <br/>                  kernel_size=(3,3), <br/>                  strides=(2,2), <br/>                  padding="valid"<br/>              ) <br/><br/>        # pooling layer <br/>        self.pool  = tf.keras.layers.MaxPooling2D(<br/>                  pool_size=(3, 3), <br/>                  strides=(2,2)<br/>              )<br/>        self.cat   = tf.keras.layers.Concatenate()<br/><br/><br/> def call(self, input_tensor, training=False):<br/>        # forward pass <br/>        conv_x = self.conv3(input_tensor, training=training)<br/>        pool_x = self.pool(input_tensor)<br/><br/>        # merged<br/>        return self.cat([conv_x, pool_x])</span></pre><h2 id="8ef4" class="nk mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">模型类:图层包含</h2><p id="8dbc" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">一般来说，我们使用<strong class="ky ir"> <em class="nd">层</em> </strong>类来定义内部计算模块，并将使用<strong class="ky ir"> <em class="nd">模型</em> </strong>类来定义外部模型，实际上就是我们将要训练的对象。在我们的例子中，在一个<strong class="ky ir"> <em class="nd">初始</em> </strong>模型中，我们定义了三个计算模块:<em class="nd"> Conv模块、初始模块</em>和<em class="nd">下采样模块</em>。这些是通过子类化<strong class="ky ir"> <em class="nd">层</em> </strong>类创建的。所以接下来，我们将使用<strong class="ky ir"> <em class="nd">模型</em> </strong>类来包含这些计算模块，以便创建整个<strong class="ky ir"> <em class="nd">初始</em> </strong>网络。通常情况下，<strong class="ky ir"> <em class="nd">模型</em> </strong>类具有与<strong class="ky ir"> <em class="nd">层</em> </strong>相同的API，但具有一些额外的功能。</p><p id="3d26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与<strong class="ky ir"> <em class="nd">层</em> </strong>类相同，我们将<em class="nd">初始化<strong class="ky ir"> <em class="nd">模型</em> </strong>类的<em class="nd"> init </em>方法内的计算块</em>如下:</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="4597" class="od mh iq ng b be oe of l og no"># the first conv module<br/>self.conv_block = ConvModule(96, (3,3), (1,1))<br/><br/># 2 inception module and 1 downsample module<br/>self.inception_block1  = InceptionModule(32, 32)<br/>self.inception_block2  = InceptionModule(32, 48)<br/>self.downsample_block1 = DownsampleModule(80)<br/>  <br/># 4 inception module and 1 downsample module<br/>self.inception_block3  = InceptionModule(112, 48)<br/>self.inception_block4  = InceptionModule(96, 64)<br/>self.inception_block5  = InceptionModule(80, 80)<br/>self.inception_block6  = InceptionModule(48, 96)<br/>self.downsample_block2 = DownsampleModule(96)<br/><br/># 2 inception module <br/>self.inception_block7 = InceptionModule(176, 160)<br/>self.inception_block8 = InceptionModule(176, 160)<br/><br/># average pooling<br/>self.avg_pool = tf.keras.layers.AveragePooling2D((7,7))</span></pre><p id="e3b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个计算模块的<strong class="ky ir">滤波器数量</strong>的数量根据模型的设计进行设置(也可在下图中看到)。在草签完所有的积木后，我们将根据设计(图)把它们连接起来。这里是使用<strong class="ky ir">模型</strong>子类的完整<strong class="ky ir">初始</strong>网络:</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="a68a" class="od mh iq ng b be oe of l og no">class MiniInception(tf.keras.Model):<br/>    def __init__(self, num_classes=10):<br/>        super(MiniInception, self).__init__()<br/><br/>        # the first conv module<br/>        self.conv_block = ConvModule(96, (3,3), (1,1))<br/><br/>        # 2 inception module and 1 downsample module<br/>        self.inception_block1  = InceptionModule(32, 32)<br/>        self.inception_block2  = InceptionModule(32, 48)<br/>        self.downsample_block1 = DownsampleModule(80)<br/>  <br/>        # 4 inception module and 1 downsample module<br/>        self.inception_block3  = InceptionModule(112, 48)<br/>        self.inception_block4  = InceptionModule(96, 64)<br/>        self.inception_block5  = InceptionModule(80, 80)<br/>        self.inception_block6  = InceptionModule(48, 96)<br/>        self.downsample_block2 = DownsampleModule(96)<br/><br/>        # 2 inception module <br/>        self.inception_block7 = InceptionModule(176, 160)<br/>        self.inception_block8 = InceptionModule(176, 160)<br/><br/>        # average pooling<br/>        self.avg_pool = tf.keras.layers.AveragePooling2D((7,7))<br/><br/>        # model tail<br/>        self.flat      = tf.keras.layers.Flatten()<br/>        self.classfier = tf.keras.layers.Dense(<br/>                                num_classes, activation='softmax'<br/>                            )<br/><br/>    def call(self, input_tensor, training=False, **kwargs):<br/>        # forward pass <br/>        x = self.conv_block(input_tensor)<br/>        x = self.inception_block1(x)<br/>        x = self.inception_block2(x)<br/>        x = self.downsample_block1(x)<br/><br/>        x = self.inception_block3(x)<br/>        x = self.inception_block4(x)<br/>        x = self.inception_block5(x)<br/>        x = self.inception_block6(x)<br/>        x = self.downsample_block2(x)<br/><br/>        x = self.inception_block7(x)<br/>        x = self.inception_block8(x)<br/>        x = self.avg_pool(x)<br/><br/>        x = self.flat(x)<br/>        return self.classfier(x)<br/><br/><br/>    def build_graph(self, raw_shape):<br/>        x = tf.keras.layers.Input(shape=raw_shape)<br/>        return Model(inputs=[x], outputs=self.call(x))</span></pre><p id="f1d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可能注意到了，除了<strong class="ky ir"> <em class="nd"> __init__ </em> </strong>和<strong class="ky ir"> call </strong>方法之外，我们还定义了一个自定义方法<strong class="ky ir"> <em class="nd"> build_graph </em> </strong> <em class="nd">。</em>我们使用它作为辅助功能，方便地绘制模型摘要信息。请查看<a class="ae kv" href="https://github.com/tensorflow/tensorflow/issues/31647#issuecomment-692586409" rel="noopener ugc nofollow" target="_blank">本讨论</a>了解更多详情。无论如何，让我们看看模型的总结。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="3cda" class="nk mh iq ng b gy nl nm l nn no">raw_input = (32, 32, 3)<br/><br/># init model object<br/>cm = MiniInception()<br/><br/># The first call to the `cm` will create the weights<br/>y = cm(tf.ones(shape=(0,*raw_input))) <br/><br/># print summary<br/>cm.build_graph(raw_input).summary()<br/><br/># ---------------------------------------------------------------------<br/><br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_6 (InputLayer)         [(None, 32, 32, 3)]       0         <br/>_________________________________________________________________<br/>conv_module_329 (ConvModule) (None, 32, 32, 96)        3072      <br/>_________________________________________________________________<br/>inception_module_136 (Incept (None, 32, 32, 64)        31040     <br/>_________________________________________________________________<br/>inception_module_137 (Incept (None, 32, 32, 80)        30096     <br/>_________________________________________________________________<br/>downsample_module_34 (Downsa (None, 15, 15, 160)       58000     <br/>_________________________________________________________________<br/>inception_module_138 (Incept (None, 15, 15, 160)       87840     <br/>_________________________________________________________________<br/>inception_module_139 (Incept (None, 15, 15, 160)       108320    <br/>_________________________________________________________________<br/>inception_module_140 (Incept (None, 15, 15, 160)       128800    <br/>_________________________________________________________________<br/>inception_module_141 (Incept (None, 15, 15, 144)       146640    <br/>_________________________________________________________________<br/>downsample_module_35 (Downsa (None, 7, 7, 240)         124896    <br/>_________________________________________________________________<br/>inception_module_142 (Incept (None, 7, 7, 336)         389520    <br/>_________________________________________________________________<br/>inception_module_143 (Incept (None, 7, 7, 336)         544656    <br/>_________________________________________________________________<br/>average_pooling2d_17 (Averag (None, 1, 1, 336)         0         <br/>_________________________________________________________________<br/>flatten_13 (Flatten)         (None, 336)               0         <br/>_________________________________________________________________<br/>dense_17 (Dense)             (None, 10)                3370      <br/>=================================================================<br/>Total params: 1,656,250<br/>Trainable params: 1,652,826<br/>Non-trainable params: 3,424</span></pre><p id="9830" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，通过模型子类化构建整个<strong class="ky ir"> Inception </strong>模型就完成了。</p><h1 id="ed7e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">从头开始定制培训循环的端到端培训</h1><p id="7020" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">现在我们已经建立了一个复杂的网络，是时候让它忙着学习一些东西了。我们现在可以简单地通过使用<strong class="ky ir"> <em class="nd">编译</em> </strong>和<strong class="ky ir"> <em class="nd">拟合</em> </strong>来轻松训练模型。但是这里我们将从头开始看一个定制的训练循环。这个功能是<strong class="ky ir"> <em class="nd"> TensorFlow 2 </em> </strong>中新引入的。请注意，这个功能相对来说有点复杂，更适合深度学习研究者。</p><h2 id="e782" class="nk mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">数据集</h2><p id="c08b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">出于演示目的，我们将使用<a class="ae kv" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 </a>数据集。先准备一下吧。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="4b5f" class="od mh iq ng b be oe of l og no">(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()<br/>print(x_train.shape, y_train.shape) # (50000, 32, 32, 3) (50000, 1)<br/>print(x_test.shape, y_test.shape)   # (10000, 32, 32, 3) (10000, 1)<br/><br/># train set / data <br/>x_train = x_train.astype('float32') / 255<br/><br/># validation set / data <br/>x_test = x_test.astype('float32') / 255<br/><br/># target / class name<br/>class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',<br/>               'dog', 'frog', 'horse', 'ship', 'truck']<br/><br/><br/>plt.figure(figsize=(10,10))<br/>for i in range(25):<br/>    plt.subplot(5,5,i+1)<br/>    plt.grid(False)<br/>    plt.imshow(x_train[i], cmap=plt.cm.binary)<br/>    plt.xlabel(class_names[y_train[i][0]])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/b609d6f457c73dc5bcd4ff20ab26ef58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/0*fCK0y7HDpWtHTgqV"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自cifar-10的样本</p></figure><p id="763a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们将<em class="nd">类向量(</em> <strong class="ky ir"> <em class="nd"> y_train，y_test </em> </strong> <em class="nd"> ) </em>转换为<em class="nd">多类矩阵</em>。我们还将使用<strong class="ky ir"> <em class="nd"> tf.data </em> </strong> API来获得更好、更高效的输入管道。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="8bdc" class="od mh iq ng b be oe of l og no"># train set / target <br/>y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)<br/># validation set / target <br/>y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)<br/><br/><br/># Prepare the training dataset.<br/>train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))<br/>train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)<br/><br/><br/># Prepare the validation dataset.<br/>val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))<br/>val_dataset = val_dataset.batch(batch_size)</span></pre><p id="c83f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们快速检查一下<em class="nd">标签转换</em>和<em class="nd">输入限幅</em>后的数据形状:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="0498" class="nk mh iq ng b gy nl nm l nn no">for i, (x, y) in enumerate(train_dataset):<br/>    print(x.shape, y.shape)<br/>    <br/>    if i == 2:<br/>        break<br/><br/><br/>for i, (x, y) in enumerate(val_dataset):<br/>    print(x.shape, y.shape)<br/>    <br/>    if i == 2:<br/>       break<br/># output <br/><br/><br/>(64, 32, 32, 3) (64, 10)<br/>(64, 32, 32, 3) (64, 10)<br/>(64, 32, 32, 3) (64, 10)<br/><br/><br/>(64, 32, 32, 3) (64, 10)<br/>(64, 32, 32, 3) (64, 10)<br/>(64, 32, 32, 3) (64, 10)</span></pre><p id="bf55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止一切顺利。我们有一个输入形状<strong class="ky ir">32<em class="nd">x</em>32<em class="nd">x</em>3</strong>和总共<strong class="ky ir"> 10 </strong> <em class="nd"> </em>类来分类。然而，将<em class="nd">测试集</em>作为<em class="nd">验证集</em>并不理想，但出于演示目的，我们不考虑<strong class="ky ir"><em class="nd">train _ test _ split</em></strong>方法。现在，让我们看看Tensorflow 2中的自定义培训管道由哪些部分组成。</p><h2 id="4486" class="nk mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">培训机制</h2><p id="316d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在<em class="nd"> TF。Keras </em>，我们有方便的训练和评估循环，<strong class="ky ir"> <em class="nd">适合</em> </strong>，<strong class="ky ir"> <em class="nd">评估</em> </strong>。但是我们也可以利用对培训和评估过程的低层次控制。在这种情况下，我们需要从头开始编写我们自己的训练和评估循环。以下是食谱:</p><ol class=""><li id="9aa8" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">我们为循环打开一个<strong class="ky ir">,它将迭代历元的数量。</strong></li><li id="4c9d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">对于每个历元，我们为循环打开另一个<strong class="ky ir">，它将批量迭代数据集(<strong class="ky ir"> <em class="nd"> x </em>，<em class="nd"> y </em> </strong>)。</strong></li><li id="2266" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">对于每一批，我们打开<strong class="ky ir"><em class="nd">gradient tape()</em></strong><em class="nd"/>范围。</li><li id="0584" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">在这个范围内，我们称这个模型为<strong class="ky ir">正向传递</strong>，并计算<strong class="ky ir">损失</strong>。</li><li id="4f79" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">在这个范围之外，我们检索关于<strong class="ky ir">损失</strong>的模型的权重的<strong class="ky ir">梯度。</strong></li><li id="c6ab" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">接下来，我们使用优化器<strong class="ky ir">基于梯度更新模型的权重</strong>。</li></ol><p id="8426" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">TensorFlow提供了<strong class="ky ir"> <em class="nd"> tf。GradientTape() </em> </strong>用于自动微分的API，即计算计算相对于某些输入的梯度。</p><p id="eaa7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是其操作过程的简短演示。这里我们有一些输入(<strong class="ky ir"> <em class="nd"> x </em> </strong>)和可训练参数(<strong class="ky ir"> <em class="nd"> w，b </em> </strong>)。<strong class="ky ir">内<em class="nd"> tf。GradientTape() </em> </strong>范围，输出(<strong class="ky ir"><em class="nd">【y】</em></strong>，基本上会是模型输出)，损耗进行测量。在这个范围之外，我们检索权重参数相对于损失的梯度。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="6f2f" class="od mh iq ng b be oe of l og no"># x:input, w,b: trainable param - x*w + b<br/>w = tf.Variable(tf.random.normal((3, 2)), name='w')<br/>b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')<br/>x = [[1., 2., 3.]]<br/><br/><br/># Open a GradientTape to record the operations run<br/># during the forward pass, which enables auto-differentiation.<br/>with tf.GradientTape(persistent=True) as tape:<br/>    y = x @ w + b # output from the forward pass (for the actual model)<br/>    <br/>    # Compute the loss value for this minibatch.<br/>    loss = tf.reduce_mean(y**2)<br/><br/><br/># Calculate gradients with respect to every trainable variable<br/>grad = tape.gradient(loss, [w, b])<br/>grad<br/><br/># output <br/>[<br/>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=<br/> array([[ -5.2607636,   1.4286567],<br/>        [-10.521527 ,   2.8573134],<br/>        [-15.782291 ,   4.28597  ]], dtype=float32)&gt;,<br/><br/>&lt;tf.Tensor: shape=(2,), dtype=float32, <br/>numpy=array([-5.2607636,  1.4286567], dtype=float32)&gt;<br/>]</span></pre><p id="c4ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们相应地实现定制的培训方案。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="b21b" class="od mh iq ng b be oe of l og no">for epoch in range(epochs): # &lt;----- start for loop, step 1<br/><br/>  # &lt;-------- start for loop, step 2<br/>  # Iterate over the batches of the dataset.<br/>  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):<br/><br/>    # &lt;-------- start gradient tape scope, step 3<br/>    # Open a GradientTape to record the operations run<br/>    # during the forward pass, which enables auto-differentiation.<br/>    with tf.GradientTape() as tape:<br/><br/>       # Run the forward pass of the layer.<br/>       # The operations that the layer applies<br/>       # to its inputs are going to be recorded<br/>       # on the GradientTape.<br/>       logits = model(x_batch_train, training=True) &lt;- step 4<br/><br/>       # Compute the loss value for this minibatch.<br/>       loss_value = loss_fn(y_batch_train, logits)  &lt;- step 4 <br/><br/><br/>    # compute the gradient of weights w.r.t. loss  &lt;-------- step 5<br/>    # Use the gradient tape to automatically retrieve<br/>    # the gradients of the trainable variables with respect to the loss.<br/>    grads = tape.gradient(loss_value, model.trainable_weights)<br/><br/>    # update the weight based on gradient  &lt;---------- step 6<br/>    # Run one step of gradient descent by updating<br/>    # the value of the variables to minimize the loss.<br/>    optimizer.apply_gradients(zip(grads, model.trainable_weights))</span></pre><p id="09ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">太好了。然而，我们仍然没有讨论如何添加指标来监控这个定制的训练循环。显然，我们也可以在训练循环中使用内置指标甚至自定义指标。在训练循环中添加指标相当简单，流程如下:</p><ol class=""><li id="e3cd" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">每批后调用<strong class="ky ir"><em class="nd">metric . update _ state()</em></strong></li><li id="f910" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">当我们需要显示度量的当前值时，调用<strong class="ky ir"><em class="nd">metric . result()</em></strong></li><li id="7587" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">调用<strong class="ky ir"><em class="nd">metric . reset _ States()</em></strong></li></ol><p id="963a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里还有一件事要考虑。TensorFlow 2.0中的默认运行时是急切执行。上面的训练循环正在急切地执行。但是如果我们想要进行图形编译，我们可以用<strong class="ky ir"><em class="nd">@ TF . function</em></strong>decorator将任何函数编译成静态图。这也加快了训练的速度。下面是用<strong class="ky ir"><em class="nd">@ TF . function</em></strong>decorator对训练和评估函数的设置。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="55d6" class="od mh iq ng b be oe of l og no">@tf.function<br/>def train_step(x, y):<br/>   '''<br/>   input: x, y &lt;- typically batches <br/>   return: loss value<br/>   '''<br/><br/>    # start the scope of gradient <br/>    with tf.GradientTape() as tape:<br/>        logits = model(x, training=True) # forward pass<br/>        train_loss_value = loss_fn(y, logits) # compute loss <br/><br/>    # compute gradient <br/>    grads = tape.gradient(train_loss_value, model.trainable_weights)<br/><br/>    # update weights<br/>    optimizer.apply_gradients(zip(grads, model.trainable_weights))<br/><br/>    # update metrics<br/>    train_acc_metric.update_state(y, logits)<br/>    <br/>    return train_loss_value<br/><br/><br/>@tf.function<br/>def test_step(x, y):<br/>   '''<br/>   input: x, y &lt;- typically batches <br/>   return: loss value<br/>   '''<br/>    # forward pass, no backprop, inference mode <br/>    val_logits = model(x, training=False) <br/><br/>    # Compute the loss value <br/>    val_loss_value = loss_fn(y, val_logits)<br/><br/>    # Update val metrics<br/>    val_acc_metric.update_state(y, val_logits)<br/><br/>    return val_loss_value</span></pre><p id="1492" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们看到了<strong class="ky ir"><em class="nd">metrics . update _ state()</em></strong><em class="nd">的用法。</em>这些函数返回到训练循环，在这里我们将设置显示日志消息、<strong class="ky ir"><em class="nd">【metric . result()</em></strong><em class="nd">、</em>以及重置指标、<strong class="ky ir"><em class="nd">metric . reset _ States()</em></strong><em class="nd">。</em></p><p id="f3ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们最不喜欢设置的东西，TensorBoard。有一些很棒的功能可以利用，如:<a class="ae kv" href="https://www.tensorflow.org/tensorboard/image_summaries" rel="noopener ugc nofollow" target="_blank">显示每批样本+混淆矩阵</a>，超参数调整，嵌入投影仪，模型图等。目前，我们将只关注记录it上的培训指标。很简单，但我们将把它集成到自定义培训循环中。所以，我们不能使用<strong class="ky ir"><em class="nd">TF . keras . callbacks . tensor board</em></strong>而是需要使用<em class="nd"> TensorFlow汇总API </em>。<strong class="ky ir"> <em class="nd"> tf.summary </em> </strong>模块提供了在TensorBoard上写入汇总数据的API。我们希望在每个批处理操作之后写入日志记录状态，以获得更多详细信息。否则，我们可能更喜欢在每个时期结束时。让我们创建一个保存<strong class="ky ir"> <em class="nd">事件</em> </strong>消息的目录。在工作目录中，创建<strong class="ky ir"> <em class="nd">日志/训练</em> </strong>和<strong class="ky ir"> <em class="nd">日志/测试</em> </strong> <em class="nd">。</em>下面是完整的培训管道。我们建议首先通读代码，以便了解整个培训流程。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="0033" class="od mh iq ng b be oe of l og no"># Instantiate an optimizer to train the model.<br/>optimizer = tf.keras.optimizers.Adam()<br/><br/># Instantiate a loss function<br/>loss_fn = tf.keras.losses.CategoricalCrossentropy()<br/><br/># Prepare the metrics.<br/>train_acc_metric = tf.keras.metrics.CategoricalAccuracy()<br/>val_acc_metric   = tf.keras.metrics.CategoricalAccuracy()<br/><br/># tensorboard writer <br/>train_writer = tf.summary.create_file_writer('logs/train/')<br/>test_writer  = tf.summary.create_file_writer('logs/test/')<br/><br/><br/>@tf.function<br/>def train_step(step, x, y):<br/>   '''<br/>   input: x, y &lt;- typically batches <br/>   input: step &lt;- batch step<br/>   return: loss value<br/>   '''<br/><br/>    # start the scope of gradient <br/>    with tf.GradientTape() as tape:<br/>        logits = model(x, training=True) # forward pass<br/>        train_loss_value = loss_fn(y, logits) # compute loss <br/><br/>    # compute gradient <br/>    grads = tape.gradient(train_loss_value, model.trainable_weights)<br/><br/>    # update weights<br/>    optimizer.apply_gradients(zip(grads, model.trainable_weights))<br/><br/>    # update metrics<br/>    train_acc_metric.update_state(y, logits)<br/>    <br/>    # write training loss and accuracy to the tensorboard<br/>    with train_writer.as_default():<br/>        tf.summary.scalar('loss', train_loss_value, step=step)<br/>        tf.summary.scalar(<br/>            'accuracy', train_acc_metric.result(), step=step<br/>        ) <br/>    return train_loss_value<br/><br/><br/>@tf.function<br/>def test_step(step, x, y):<br/>   '''<br/>   input: x, y &lt;- typically batches <br/>   input: step &lt;- batch step<br/>   return: loss value<br/>   '''<br/>    # forward pass, no backprop, inference mode <br/>    val_logits = model(x, training=False) <br/><br/>    # Compute the loss value <br/>    val_loss_value = loss_fn(y, val_logits)<br/><br/>    # Update val metrics<br/>    val_acc_metric.update_state(y, val_logits)<br/>    <br/>    # write test loss and accuracy to the tensorboard<br/>    with test_writer.as_default():<br/>        tf.summary.scalar('val loss', val_loss_value, step=step)<br/>        tf.summary.scalar(<br/>            'val accuracy', val_acc_metric.result(), step=step<br/>        ) <br/>    return val_loss_value<br/><br/><br/># custom training loop <br/>for epoch in range(epochs):<br/>    t = time.time()<br/>    # batch training <br/><br/>    # Iterate over the batches of the train dataset.<br/>    for train_batch_step, (x_batch_train, \<br/>                           y_batch_train) in enumerate(train_dataset):<br/>        train_batch_step = tf.convert_to_tensor(<br/>                                train_batch_step, dtype=tf.int64<br/>                           )<br/>        train_loss_value = train_step(<br/>                                train_batch_step, <br/>                                x_batch_train, y_batch_train<br/>                           )<br/><br/>    # evaluation on validation set <br/>    # Run a validation loop at the end of each epoch.<br/>    for test_batch_step, (x_batch_val, \<br/>                          y_batch_val) in enumerate(val_dataset):<br/>        test_batch_step = tf.convert_to_tensor(<br/>                               test_batch_step, dtype=tf.int64<br/>                          )<br/>        val_loss_value = test_step(<br/>                                test_batch_step, x_batch_val, y_batch_val<br/>                          )<br/><br/><br/>    template = '<br/>        ETA: {} - epoch: {} loss: {}  acc: {} val loss: {} val acc: {}\n<br/>    '<br/>    print(template.format(<br/>        round((time.time() - t)/60, 2), epoch + 1,<br/>        train_loss_value, float(train_acc_metric.result()),<br/>        val_loss_value, float(val_acc_metric.result())<br/>    ))<br/>        <br/>    # Reset metrics at the end of each epoch<br/>    train_acc_metric.reset_states()<br/>    val_acc_metric.reset_states()</span></pre><p id="4ca3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">瞧啊。我们用RTX 2070在本地系统中运行代码。通过启用<strong class="ky ir"><em class="nd"/></strong>，我们能够将<em class="nd">批次大小</em>增加到<strong class="ky ir"> 256 </strong>。以下是日志输出:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="41c1" class="nk mh iq ng b gy nl nm l nn no">ETA: 0.78 - epoch: 1 loss: 0.7587890625  acc: 0.5794399976730347 val loss: 3.173828125 val acc: 0.10159999877214432<br/><br/>ETA: 0.29 - epoch: 2 loss: 0.63232421875  acc: 0.7421200275421143 val loss: 1.0126953125 val acc: 0.5756999850273132<br/><br/>ETA: 0.32 - epoch: 3 loss: 0.453369140625  acc: 0.8073400259017944 val loss: 0.7734375 val acc: 0.7243000268936157<br/><br/>ETA: 0.33 - epoch: 4 loss: 0.474365234375  acc: 0.8501200079917908 val loss: 0.64111328125 val acc: 0.7628999948501587<br/><br/>..<br/>..<br/><br/><br/>ETA: 0.35 - epoch: 17 loss: 0.0443115234375  acc: 0.9857199788093567 val loss: 1.8603515625 val acc: 0.7465000152587891<br/><br/>ETA: 0.68 - epoch: 18 loss: 0.01328277587890625  acc: 0.9839400053024292 val loss: 0.65380859375 val acc: 0.7875999808311462<br/><br/>ETA: 0.53 - epoch: 19 loss: 0.035552978515625  acc: 0.9851599931716919 val loss: 1.0849609375 val acc: 0.7432000041007996<br/><br/>ETA: 0.4 - epoch: 20 loss: 0.04217529296875  acc: 0.9877399802207947 val loss: 3.078125 val acc: 0.7224000096321106</span></pre><p id="0f68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">过度拟合！但是现在没关系。为此，我们只需要考虑一些细节，如<strong class="ky ir">图像增强</strong>、<strong class="ky ir">学习速率表</strong>等。在工作目录中，对实时tensorboard运行以下命令。在下面的命令中，<strong class="ky ir"> <em class="nd"> logs </em> </strong>是我们手动创建的保存事件日志的文件夹名。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="3f31" class="nk mh iq ng b gy nl nm l nn no">tensorboard --logdir logs</span></pre><h2 id="b3b8" class="nk mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">保存并加载</h2><p id="d1e7" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">根据我们使用的API，有各种各样的<a class="ae kv" href="https://www.tensorflow.org/guide/keras/save_and_serialize" rel="noopener ugc nofollow" target="_blank">方法来保存</a> <em class="nd"> TensorFlow </em>模型。模型子类化中的模型<strong class="ky ir"> <em class="nd">保存</em> </strong>和<strong class="ky ir"> <em class="nd">重新加载</em> </strong>与<em class="nd">顺序</em>或<em class="nd">功能API </em>中的不一样。它需要一些特别的关注。目前有两种格式存储模型:<a class="ae kv" href="https://www.tensorflow.org/guide/saved_model" rel="noopener ugc nofollow" target="_blank"> <em class="nd"> SaveModel </em> </a>和<a class="ae kv" href="https://www.tensorflow.org/tutorials/keras/save_and_load" rel="noopener ugc nofollow" target="_blank"> HDF5 </a>。来自官方文件:</p><blockquote class="oj ok ol"><p id="114a" class="kw kx nd ky b kz la jr lb lc ld ju le om lg lh li on lk ll lm oo lo lp lq lr ij bi translated">HDF5和SavedModel的关键区别在于，HDF5使用对象配置来保存模型架构，而SavedModel保存执行图。因此，SavedModels能够保存自定义对象，如子类模型和自定义层，而不需要原始代码。</p></blockquote><p id="313d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，看起来<em class="nd"> SavedModels </em>能够保存我们的定制子类模型。但是如果我们想要<em class="nd"> HDF5 </em>格式的自定义子类模型呢？根据医生的说法。我们也能做到，但是我们需要一些额外的东西。我们必须在我们的对象中定义<strong class="ky ir"> <em class="nd"> get_config </em> </strong>方法。并且在加载模型时还需要将对象传递给<strong class="ky ir"><em class="nd">custom _ object</em></strong>参数。此参数必须是字典映射:<em class="nd">TF . keras . models . load _ model(path，custom _ objects = { ' custom layer ':custom layer })。</em>然而，似乎我们现在不能使用HDF5，因为我们没有在自定义对象中使用<strong class="ky ir"> <em class="nd"> get_config </em> </strong>方法。然而，在自定义对象中定义这个函数实际上是一个很好的做法。如果需要的话，这将允许我们稍后容易地更新计算。</p><p id="24d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是现在，让我们保存模型并用<em class="nd"> SavedModel </em>格式重新加载它。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="8fd5" class="od mh iq ng b be oe of l og no">model.save('net', save_format='tf')</span></pre><p id="fd30" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">之后，它会在工作目录下创建一个名为<em class="nd"> net </em>的新文件夹。它将包含<strong class="ky ir"> <em class="nd">资产</em> </strong>，<strong class="ky ir"><em class="nd">saved _ model . Pb</em></strong>，<strong class="ky ir"> <em class="nd">变量</em> </strong> <em class="nd">。</em>模型架构和训练配置，包括优化器、损失和指标，存储在<em class="nd"> saved_model.pb </em>中。权重保存在<em class="nd">变量</em>目录中。</p><p id="6176" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当保存模型及其层时，<em class="nd"> SavedModel </em>格式存储类名、<strong class="ky ir">调用</strong>函数、损失和权重(以及配置，如果实现的话)。<strong class="ky ir">调用</strong>函数定义模型/层的计算图。在没有模型/层配置的情况下，<strong class="ky ir">调用</strong>函数用于创建一个像原始模型一样存在的模型，该模型可以被<em class="nd">训练</em>，<em class="nd">评估</em>，并用于<em class="nd">推理</em>。稍后，为了重新加载保存的模型，我们将执行以下操作:</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="86f8" class="od mh iq ng b be oe of l og no">new_model = tf.keras.models.load_model("net", compile=False)</span></pre><p id="40d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">设置<em class="nd"/><strong class="ky ir"><em class="nd">compile = False</em></strong>是可选的，我这样做是为了避免警告日志。此外，由于我们正在进行自定义循环训练，我们不需要任何编译。</p><p id="54f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，我们已经讨论了保存整个模型(<em class="nd">计算图</em>和<em class="nd">参数</em>)。但是，如果我们只想保存<strong class="ky ir">训练重量</strong>并在需要时重新加载重量。是的，我们也能做到。简单地说，</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="2149" class="od mh iq ng b be oe of l og no">model.save_weights('net.h5')</span></pre><p id="1966" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将减轻我们模型的重量。现在，当需要重新加载时，有一点需要记住。在尝试加载权重之前，我们需要调用<strong class="ky ir"> <em class="nd">构建</em> </strong>方法。它主要初始化子类模型中的层，以便可以构建计算图。例如，如果我们尝试如下:</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="5a33" class="od mh iq ng b be oe of l og no">new_model = MiniInception()<br/>new_model.load_weights('net.h5')<br/><br/>--------------------------------------<br/>ValueError: Unable to load weights saved in HDF5 format <br/>into a subclassed Model which has not created its variables yet. <br/>Call the Model first, then load the weights.</span></pre><p id="4679" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要解决这个问题，我们可以做如下工作:</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="cd8c" class="od mh iq ng b be oe of l og no">new_model = MiniInception()<br/>new_model.build((None, *x_train.shape[1:])) # or .build((x_train.shape))<br/>new_model.load_weights('net.h5')</span></pre><p id="463a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它将成功加载。这里有一篇关于在TF中保存和序列化模型的很棒的文章。弗朗索瓦·乔莱的作品，必读。</p><h2 id="cd43" class="nk mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">评估和预测</h2><p id="3faf" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">尽管不是必须的，让我们通过测量模型性能来结束。<strong class="ky ir"> CIFAR-10 </strong>级标号图如下:<strong class="ky ir"> 0: </strong>飞机<em class="nd">，</em> <strong class="ky ir"> 1: </strong>汽车<em class="nd">，</em> <strong class="ky ir"> 2 <em class="nd"> : </em> </strong>鸟<em class="nd">，</em> <strong class="ky ir"> 3 <em class="nd"> : </em> </strong>猫<em class="nd">，</em> <strong class="ky ir"> 4 <em class="nd"> : </em> </strong>先找<strong class="ky ir">分类报告</strong>吧。</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="7a68" class="od mh iq ng b be oe of l og no"><br/>Y_pred = model.predict(x_test, verbose=1)<br/>y_pred = np.argmax(Y_pred, axis=1)<br/><br/>target_names = [<br/>      'airplane', 'automobile', 'bird', 'cat', 'deer',<br/>      'dog', 'frog', 'horse', 'ship', 'truck'<br/>]<br/><br/><br/>classification_report(<br/>      np.argmax(y_test, axis=1), <br/>      y_pred,target_names=target_names<br/>)<br/>#--------------------------------------------------------------------<br/><br/>          precision    recall  f1-score   support<br/><br/>    airplane       0.81      0.79      0.80      1000<br/>  automobile       0.76      0.95      0.85      1000<br/>        bird       0.78      0.65      0.71      1000<br/>         cat       0.36      0.92      0.51      1000<br/>        deer       0.94      0.49      0.65      1000<br/>         dog       0.87      0.46      0.60      1000<br/>        frog       0.97      0.52      0.68      1000<br/>       horse       0.89      0.69      0.78      1000<br/>        ship       0.83      0.90      0.86      1000<br/>       truck       0.90      0.81      0.85      1000<br/><br/>    accuracy                           0.72     10000<br/>   macro avg       0.81      0.72      0.73     10000<br/>weighted avg       0.81      0.72      0.73     10000</span></pre><p id="fcf7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，多级<strong class="ky ir"> ROC AUC </strong>得分:</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="78f4" class="od mh iq ng b be oe of l og no"><br/>def multiclass_roc_auc_score(<br/>      y_test, y_pred, average="macro"<br/>):<br/>    lb = LabelBinarizer()<br/>    lb.fit(y_test)<br/>    y_test = lb.transform(y_test)<br/>    y_pred = lb.transform(y_pred)<br/>    return roc_auc_score(y_test, y_pred, average=average)<br/><br/><br/>multiclass_roc_auc_score(y_test,y_pred)<br/># ---------------------------------------------------<br/>#output: 0.8436111111111112</span></pre><p id="f363" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">混淆矩阵:</strong></p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="c0b9" class="od mh iq ng b be oe of l og no">Y_pred = model.predict(x_test, verbose=2)<br/>y_pred = np.argmax(Y_pred, axis=1)<br/><br/>cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)<br/>cm = pd.DataFrame(cm, range(10),range(10))<br/>plt.figure(figsize = (10,10))<br/><br/>sns.heatmap(cm, annot=True, annot_kws={"size": 12}) # font size<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/f59eb69509be512deba8f5aab1bb6573.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xDayHx6xyoGJHQMN"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">cifar_10混淆矩阵</p></figure><p id="0529" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">对新样本的预测/推断</strong>:</p><pre class="kg kh ki kj gt nf ng oa bn ob oc bi"><span id="690d" class="od mh iq ng b be oe of l og no"><br/>target_names = [<br/>      'airplane', 'automobile', 'bird', 'cat', 'deer',<br/>      'dog', 'frog', 'horse', 'ship', 'truck'<br/>]<br/><br/># Give the link of the image here to test <br/>test_image1 = image.load_img(image_file, target_size=(32,32))<br/>test_image  = image.img_to_array(test_image1) <br/>test_image  = np.expand_dims(test_image, axis =0) <br/>prediction  = model.predict(test_image)[0] <br/>label_index = np.argmax(prediction)<br/>target_names[label_index]</span></pre><h1 id="7970" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">尾注</h1><p id="011e" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">这件事到此为止。非常感谢你阅读这篇文章，希望你们喜欢。文章有点长，这里简单总结一下。</p><p id="4fca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们先对比一下<strong class="ky ir"> TF。Keras </strong>建模API。接下来，我们使用<strong class="ky ir"> <em class="nd">模型子类</em> </strong> API来逐步构建一个小型<strong class="ky ir">初始</strong>网络。然后我们用<strong class="ky ir"> GradientTape </strong>来看看TensorFlow 2中新引入的自定义循环训练的流程。我们还端到端地训练了子类化的<strong class="ky ir"> Inception </strong>模型。最后，我们讨论自定义模型保存和重新加载，然后测量这些经过训练的模型的性能。</p><h1 id="8a70" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">有趣的阅读</h1><ol class=""><li id="577e" class="ls lt iq ky b kz my lc mz lf oq lj or ln os lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/67591848/9215780" rel="noopener ugc nofollow" target="_blank">py torch中的亲笔签名和TensorFlow 2中的gradient tape</a></li><li id="e96e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/66849164/9215780" rel="noopener ugc nofollow" target="_blank">tensor flow 2[Keras]中的多输入多输出建模</a></li><li id="bfcb" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/67851641/9215780" rel="noopener ugc nofollow" target="_blank">选择损失和指标</a></li><li id="62ec" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/67744905/9215780" rel="noopener ugc nofollow" target="_blank">在喀拉斯重装最佳关卡</a></li><li id="b8d9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/66627036/9215780" rel="noopener ugc nofollow" target="_blank"> Keras致密层vs PyTorch线性层</a></li><li id="5027" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/66880334/9215780" rel="noopener ugc nofollow" target="_blank"> Keras建模API与顺序API </a></li><li id="f0ab" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/67238117/9215780" rel="noopener ugc nofollow" target="_blank"> Keras模型。预测比NumPy慢！</a></li><li id="79a7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/66189774/9215780" rel="noopener ugc nofollow" target="_blank">在Keras中实现grad cam</a></li><li id="752d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/68297796/9215780" rel="noopener ugc nofollow" target="_blank">LSTM/GRU Keras关注层</a></li><li id="4d76" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/66238161/9215780" rel="noopener ugc nofollow" target="_blank"> TensorFlow数据API vs Numpy数组</a></li><li id="0f7a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/67467084/9215780" rel="noopener ugc nofollow" target="_blank">神经网络&amp;二元分类指导</a></li><li id="6ae7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/66881336/9215780" rel="noopener ugc nofollow" target="_blank">用tf进行量化感知训练。梯度胶带</a></li><li id="552c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/a/66486573/9215780" rel="noopener ugc nofollow" target="_blank">Keras中self.add_loss函数的用途</a></li></ol></div></div>    
</body>
</html>