<html>
<head>
<title>A Guide to Word Embedding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-word-embeddings-8a23817ab60f?source=collection_archive---------20-----------------------#2020-10-26">https://towardsdatascience.com/a-guide-to-word-embeddings-8a23817ab60f?source=collection_archive---------20-----------------------#2020-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5c7c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这是什么？它怎么会比单词袋模型更有用呢？</h2></div><p id="b0f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">阅读、理解、交流并最终产生新内容是我们所有人都会做的事情，不管我们在职业生活中是谁。</p><p id="b896" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当要从给定的文本体中提取有用的特征时，所涉及的过程与连续整数向量相比有着根本的不同。这是因为一个句子或一段文本中的信息是以结构化序列编码的，单词的语义位置传达了文本的意思。</p><p id="0996" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，对数据的适当表示以及保留文本的上下文含义的双重要求使我了解并实现了两种不同的NLP模型来完成文本分类的任务。</p><blockquote class="lb"><p id="8f2d" class="lc ld iq bd le lf lg lh li lj lk la dk translated">单词嵌入是文本中单个单词的密集表示，考虑了上下文和该单个单词出现的其他周围单词。</p><p id="1eab" class="lc ld iq bd le lf lg lh li lj lk la dk translated">可以选择这个实值向量的维数，并且比简单的单词袋模型更有效地捕捉单词之间的语义关系。</p></blockquote><figure class="lm ln lo lp lq lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ll"><img src="../Images/7c2f3bd16d002bdb60a87968cfab891b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SYiW1MUZul1NvL1kc1RxwQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">单词之间的线性关系。图片来自developers.google.com</p></figure><blockquote class="lb"><p id="6117" class="lc ld iq bd le lf mc md me mf mg la dk translated">简而言之，拥有相似含义或经常在相似上下文中一起出现的单词将具有相似的向量表示，这取决于这些单词在含义上的“接近”或“远离”。</p></blockquote><p id="c9ee" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在这篇文章中，我将探索两个单词嵌入——</p><blockquote class="mm mn mo"><p id="1637" class="kf kg mp kh b ki kj jr kk kl km ju kn mq kp kq kr mr kt ku kv ms kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq"> 1。训练我们自己嵌入</em> </strong></p><p id="2dcc" class="kf kg mp kh b ki kj jr kk kl km ju kn mq kp kq kr mr kt ku kv ms kx ky kz la ij bi translated"><strong class="kh ir">2<em class="iq">。预先训练好的手套字嵌入</em> </strong></p></blockquote></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="6478" class="na nb iq bd nc nd ne nf ng nh ni nj nk jw nl jx nm jz nn ka no kc np kd nq nr bi translated">数据集—</h1><p id="18a1" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">对于本案例研究，我们将使用Kaggle中的<a class="ae nx" href="https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">堆栈溢出数据集</strong> </a> <strong class="kh ir"> </strong>。该数据集包含用户在网站上提出的60，000个问题，主要任务是将所提问题的质量分为3类。</p><p id="e59b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们看看这个多类NLP项目的实际模型本身。</p><p id="37ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，在开始之前，请确保您已经安装了这些包/库。</p><pre class="ny nz oa ob gt oc od oe of aw og bi"><span id="4834" class="oh nb iq od b gy oi oj l ok ol">pip install gensim            # For NLP preprocessing tasks</span><span id="a640" class="oh nb iq od b gy om oj l ok ol">pip install keras             # For the Embedding layer</span></pre><h1 id="4529" class="na nb iq bd nc nd on nf ng nh oo nj nk jw op jx nm jz oq ka no kc or kd nq nr bi translated">1.训练单词嵌入—</h1><p id="4c44" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">如果您想跳过解释，请在此 访问第一个型号的完整代码<a class="ae nx" href="https://github.com/shraddha-an/nlp/blob/main/word_embedding_classification.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">。</strong></a></p><h2 id="b760" class="oh nb iq bd nc os ot dn ng ou ov dp nk ko ow ox nm ks oy oz no kw pa pb nq pc bi translated">1)数据预处理—</h2><p id="e8b5" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">在第一个模型中，我们将训练一个神经网络从我们的文本语料库中学习嵌入。具体来说，我们将使用Keras库向神经网络中的嵌入层提供单词标记及其索引。</p><p id="658b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在训练我们的网络之前，必须确定一些关键参数。这些因素包括词汇表的大小或语料库中独特单词的数量以及嵌入向量的大小。</p><p id="a359" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下载zip中提供了2个<a class="ae nx" href="https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">数据集</strong> </a>用于训练和测试。我们现在将导入它们，只保留问题和质量列进行分析。</p><p id="7154" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我还更改了列名并定义了一个函数<strong class="kh ir"> <em class="mp"> text_clean </em> </strong>来清理问题。</p><figure class="ny nz oa ob gt lr"><div class="bz fp l di"><div class="pd pe l"/></div></figure><p id="67b6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您查看原始数据集，您会发现HTML标签中包含的问题如下所示，<p> …..<em class="mp">问题</em> &lt; /p &gt;。而且还有href，https等词。，遍布整个文本，所以我要确保从文本中删除这两组不需要的字符。</p></p><p id="f09a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Gensim的<strong class="kh ir"><em class="mp">simple _ preprocess</em></strong>方法返回一个去掉了重音符号的小写记号列表。</p><p id="d336" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里使用apply方法将通过预处理函数迭代运行每个观察/行，并在移动到下一行之前返回输出。继续将文本预处理功能应用于训练和测试数据集。</p><p id="a767" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于因变量向量中有3个类别，我们将应用一键编码并初始化一些参数以备后用。</p><h2 id="5d0e" class="oh nb iq bd nc os ot dn ng ou ov dp nk ko ow ox nm ks oy oz no kw pa pb nq pc bi translated">2)标记化—</h2><p id="7e02" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">接下来，我们将使用Keras Tokenizer类将仍然由单词组成的问题转换为一个数组，该数组表示单词及其索引。</p><p id="b55a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们首先必须使用<strong class="kh ir"> fit_on_texts </strong>方法，从数据集中出现的单词中构建一个索引词汇表。</p><p id="297f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在构建了词汇表之后，我们使用<strong class="kh ir"> text_to_sequences </strong>方法将句子转换成代表单词的数字列表。</p><p id="af53" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> pad_sequences </strong>函数确保所有观察值长度相同，设置为任意数字或数据集中最长问题的长度。</p><p id="6be0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们之前初始化的<strong class="kh ir"> vocab_size </strong>参数仅仅是我们的独特单词词汇表的大小(用于学习和索引)。</p><figure class="ny nz oa ob gt lr"><div class="bz fp l di"><div class="pd pe l"/></div></figure><h2 id="0635" class="oh nb iq bd nc os ot dn ng ou ov dp nk ko ow ox nm ks oy oz no kw pa pb nq pc bi translated">3)训练嵌入层—</h2><p id="fdb9" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">最后，在这一部分中，我们将构建和训练我们的模型，该模型由两个主要层组成，一个嵌入层将从上面准备的训练文档中学习，另一个密集输出层用于实现分类任务。</p><p id="c86d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嵌入层将在训练时学习单词表示和神经网络，并需要大量的文本数据来提供准确的预测。在我们的例子中，45，000次训练观察足以有效地学习语料库并对所提问题的质量进行分类。正如我们将从指标中看到的。</p><figure class="ny nz oa ob gt lr"><div class="bz fp l di"><div class="pd pe l"/></div></figure><h2 id="3e94" class="oh nb iq bd nc os ot dn ng ou ov dp nk ko ow ox nm ks oy oz no kw pa pb nq pc bi translated">4)评估和度量图—</h2><p id="b4bf" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">剩下的就是评估我们的模型的性能，并绘制图表来查看模型的准确性和损失度量如何随时代而变化。</p><p id="25c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们模型的性能指标显示在下面的屏幕截图中。</p><figure class="ny nz oa ob gt lr gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/d6a77705f1e30b6464b81ab4c2e7f36b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*vXO5M8oDGW7CdgzdmqzSDw.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">绩效指标。</p></figure><p id="58fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其代码如下所示。</p><figure class="ny nz oa ob gt lr"><div class="bz fp l di"><div class="pd pe l"/></div></figure><p id="177a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是准确度在训练中如何提高的…</p><figure class="ny nz oa ob gt lr gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/2d0108487980dfa3176e99dfaee9bf05.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*p58dKwSWFyJ4SZ0xNHlezw.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">超过20个训练时期的准确度</p></figure><p id="351e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">…损失在20个时期内下降。</p><figure class="ny nz oa ob gt lr gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/8fe327af59f93ec60ad3fe4e6cbec515.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*jt15Dk4F8zxEYrgjXxwMlA.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">超过20个训练时期的损失。</p></figure><h1 id="e96c" class="na nb iq bd nc nd on nf ng nh oo nj nk jw op jx nm jz oq ka no kc or kd nq nr bi translated">2.预先训练的手套单词嵌入——</h1><p id="9255" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">完整代码<a class="ae nx" href="https://github.com/shraddha-an/nlp/blob/main/pretrained_glove_classification.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">这里</strong> </a>，如果你只是想运行模型。</p><p id="973c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代替训练你自己的嵌入，另一个选择是使用预先训练的单词嵌入，像GloVe或Word2Vec。在这一部分，我们将使用在Wikipedia + Gigaword 5上训练的手套单词嵌入；从<a class="ae nx" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">这里</strong> </a> <strong class="kh ir">下载。</strong></p><h2 id="5e87" class="oh nb iq bd nc os ot dn ng ou ov dp nk ko ow ox nm ks oy oz no kw pa pb nq pc bi translated">I)选择预训练的单词嵌入，如果—</h2><p id="fad7" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">你的数据集是由更“通用”的语言组成的，而且你一开始就没有那么大的数据集。</p><p id="603c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于这些嵌入已经在来自不同来源的大量单词上进行了训练，如果你的数据也被一般化，预训练模型可能会做得很好。</p><p id="011f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，通过预先训练的嵌入，您将节省时间和计算资源。</p><h2 id="d363" class="oh nb iq bd nc os ot dn ng ou ov dp nk ko ow ox nm ks oy oz no kw pa pb nq pc bi translated">ii)选择训练自己的嵌入，如果—</h2><p id="3a74" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">你的数据(和项目)是基于一个利基行业，如医药、金融或任何其他非通用和高度特定的领域。</p><p id="ae10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，一般的单词嵌入表示可能不适合您，并且一些单词可能从预先训练的嵌入中完全丢失。</p><p id="c380" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，需要大量的数据来确保正在学习的单词嵌入能够恰当地表示不同的单词以及它们之间的语义关系，这是您的领域所特有的。</p><p id="1cf9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，遍历您的语料库并构建单词嵌入需要大量的计算资源。</p><blockquote class="lb"><p id="1fa0" class="lc ld iq bd le lf lg lh li lj lk la dk translated">最终，选择从您已获得的数据中训练您自己的嵌入还是使用预训练的嵌入将归结为您独特的项目环境。</p></blockquote><p id="04fc" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">显然，您仍然可以试验这两种模型，并选择一个提供更好的准确性，但上面的指南是一个简化的指南，以帮助您做出决定。</p><h1 id="7e1c" class="na nb iq bd nc nd on nf ng nh oo nj nk jw op jx nm jz oq ka no kc or kd nq nr bi translated">这个过程—</h1><p id="d0e1" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">所需的大部分步骤已经在前面的部分中完成，只需要做一些调整。</p><p id="d15b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们只需要建立一个单词和它们的向量的嵌入矩阵，然后用它来设置嵌入层的权重。</p><p id="5cbd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，如果你正在跟随这个教程<em class="mp">(你是吗？)</em>，保持预处理、标记化和填充步骤不变。</p><p id="68d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们导入了原始数据集并运行了前面的文本清理步骤，我们将运行下面的代码来构建嵌入矩阵。</p><p id="7259" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">决定嵌入的维数(50，100，200 ),并将其名称包含在下面的path变量中。</p><figure class="ny nz oa ob gt lr"><div class="bz fp l di"><div class="pd pe l"/></div></figure><p id="c1b6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">应该稍微修改构建和训练嵌入层和神经网络的代码，以允许嵌入矩阵用作嵌入层中的权重。</p><figure class="ny nz oa ob gt lr"><div class="bz fp l di"><div class="pd pe l"/></div></figure><p id="febc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是我们预训练模型的测试集的性能指标。</p><figure class="ny nz oa ob gt lr gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/a6e20a371d8dd3dcc403eb740f4036cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*gSdhSdV8iW0x8nik0KOGNQ.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">预训练手套模型的准确性和损失度量。</p></figure><h1 id="6dfe" class="na nb iq bd nc nd on nf ng nh oo nj nk jw op jx nm jz oq ka no kc or kd nq nr bi translated">结论:</h1><p id="0d4e" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">从两个模型的性能指标来看，训练嵌入层似乎更适合此数据集。</p><h2 id="96e0" class="oh nb iq bd nc os ot dn ng ou ov dp nk ko ow ox nm ks oy oz no kw pa pb nq pc bi translated">一些原因可能是—</h2><p id="0458" class="pw-post-body-paragraph kf kg iq kh b ki ns jr kk kl nt ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">1)大多数关于栈溢出的问题都与IT和编程有关，也就是说，一个从定制嵌入中获益更多的利基领域。</p><p id="7f9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2)45，000个样本的大型训练数据集为我们的嵌入层提供了良好的学习场景。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="ed78" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">希望你觉得这个教程很有帮助，并且能够理解训练你自己的单词嵌入背后的概念。</p><p id="2451" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一如既往，任何改进建议都是有益的，也是受欢迎的。</p><p id="314f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将在下面留下一些链接供进一步阅读，因为这绝对是一个高级的主题，你应该进一步练习才能很好地理解。</p><p id="b837" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢您的阅读，我们将在下一篇文章中再见。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="ac2e" class="oh nb iq bd nc os ot dn ng ou ov dp nk ko ow ox nm ks oy oz no kw pa pb nq pc bi translated">更多资源:</h2><div class="pj pk gp gr pl pm"><a rel="noopener follow" target="_blank" href="/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd ir gy z fp pr fr fs ps fu fw ip bi translated">数学之光ML:理解手套嵌入的直观指南</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">理解GloVe和Keras实现背后的理论！</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">towardsdatascience.com</p></div></div><div class="pv l"><div class="pw l px py pz pv qa lw pm"/></div></div></a></div><div class="pj pk gp gr pl pm"><a href="https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd ir gy z fp pr fr fs ps fu fw ip bi translated">词嵌入及其与分布式语义模型的联系综述</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">近年来，无监督的学习单词嵌入在许多NLP任务中取得了巨大的成功。以至于…</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">aylien.com</p></div></div><div class="pv l"><div class="qb l px py pz pv qa lw pm"/></div></div></a></div><div class="pj pk gp gr pl pm"><a href="https://pureai.com/articles/2020/01/06/neural-word-embeddings.aspx" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd ir gy z fp pr fr fs ps fu fw ip bi translated">理解神经单词嵌入——纯人工智能</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">微软研究院的数据科学家解释了单词嵌入是如何在自然语言处理中使用的——这是一个…</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">pureai.com</p></div></div><div class="pv l"><div class="qc l px py pz pv qa lw pm"/></div></div></a></div></div></div>    
</body>
</html>