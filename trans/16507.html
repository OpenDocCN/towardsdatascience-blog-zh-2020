<html>
<head>
<title>Guide to Custom Recurrent Modeling in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras中的自定义递归建模指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/guide-to-custom-recurrent-modeling-in-keras-29027e3f8465?source=collection_archive---------16-----------------------#2020-11-14">https://towardsdatascience.com/guide-to-custom-recurrent-modeling-in-keras-29027e3f8465?source=collection_archive---------16-----------------------#2020-11-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="aab3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">探索由基本循环层组成的12种不同排列，实验性地回答3个关于模型构建的有趣问题！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a93c49604fad9461606887cd0b30c986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*J3Brg2V_ZXd6V9eQ"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">弗兰基·查马基在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="210f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="6132" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">递归神经操作的初始层次通常从LSTM、GRU和RNN开始。但是随着任务复杂性的增加，我们应该使用更复杂的模型。也就是说，在直接转向不同的、相对复杂的模型(如注意力或变形金刚)之前，我们应该先问一个简单的问题——我们还能快速处理基本的循环层吗？在这篇文章中，我将关注同样的哲学——我们应该先尝试简单的解决方案，然后再尝试更复杂的解决方案。在接下来的部分中，我们将探索相同的一组旧的循环层(尽管有一些有趣的安排)，以获得更好的数据推断。总共，我们将讨论12个这样的安排(包括原始设置)。最后，我们将在文本情感检测任务上测试我们的所有模型，以比较它们的性能。我们开始吧！</p><h1 id="f5e6" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">先决条件</h1><p id="a390" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在继续之前，让我们讨论几个我们应该知道的主题，以便完全理解这篇文章。还有一点，我交替使用排列和模型，因为所有讨论的模型不过是基本循环层的组合，它充当最小的构建块。</p><h1 id="f922" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">递归层输出类型</h1><p id="e5d4" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">任何递归层都可以选择返回两种类型的输出— (1)最后状态输出(1个输出)和(2)所有状态输出(N个输出)。实际上，第二种类型更通用，因为它也包含最后的状态输出(即，第一种类型)，但通常，流行的深度学习库的实现提供了返回两种类型输出的选项。对于Keras，默认为第一种类型，您可以设置参数<code class="fe mk ml mm mn b">return_sequence=True</code>转换为第二种类型。请注意，这里的“状态”是指递归图层的隐藏状态，N个输出通常是数据集中的时间步长数。为了了解更多关于循环层和隐藏状态的内部情况，我推荐这篇<a class="ae kv" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">优秀文章</a>。现在回到主题，有趣的是，虽然第二种类型包含更多的信息(这通常是一件好事)，但它也可能令人不知所措，因为我们不知道该如何处理它。由于大多数下游应用程序仍然需要一个输出，我们最终将所有状态的输出合并为一个最终输出(可能的技术—使用<code class="fe mk ml mm mn b">AveragePooling</code>或<code class="fe mk ml mm mn b">MaxPooling</code>)。在某种程度上，LSTM或GRU也是如此，他们使用前一个州的输出(和当前输入)来创建下一个州的输出。那么有趣的问题是，“什么更好——信任LSTM/GRU来巩固国家产出还是应用我们自己的逻辑？”。我们将在后面的章节中尝试回答这个问题，现在，让我们继续。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/ed17305335b6bd246613f355f16e489b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZ8kAWZ1hGAzcsUWhGwzTw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">循环层中的变化与输出数量有关。(作者)</p></figure><h1 id="b7ec" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">堆叠循环层</h1><p id="c14e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">递归层接受顺序输入并处理它们以返回一个或多个输出(状态向量)。现在，由于输出(如果我们返回所有状态的输出)也遵循顺序感，它们可以被认为是一些转换的原始输入，并可以被传递到LSTM/GRU的另一层进行进一步处理。这被称为堆叠，这里的主要直觉是，就像普通的深度神经网络，你可以添加更密集的层，或者CNN，你可以一个接一个地添加多个卷积层，在循环分层中，你可以将多个层一个接一个地堆叠起来。理想情况下，层数越多，可调参数越多，学习能力越强。但是要小心添加太多的栈，因为这可能会导致过度拟合(对于太简单的学习需求来说，网络太复杂)。我们在这里可以问的主要问题是“增加堆叠层会带来更好的性能吗？”。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/0fb993ba55055b1cf3f60209aaca049b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q4gVlkXhqFJw1gcWa3cFfg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">2级堆叠循环模型，其中每一级都有不同的循环层(不同的权重)</p></figure><h1 id="56f9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">双向循环层</h1><p id="864b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一个有趣的安排是，当您有两个递归层(它们没有堆叠)时，在一个层中，数据从左到右传递用于训练，而在另一个层中，这个方向相反。这是双向递归层，直觉是，与仅具有前向训练和左上下文的正常层相比，在准备状态向量输出时具有左和右上下文可以导致更高的性能。通常，对于您期望完整数据(双向上下文)存在的任务，这是正确的，因此这对于情绪检测是有效的，因为我们有完整的句子字符串可用，但对于时间序列预测是不正确的，因为如果我们想要今天的值(比如温度)，我们没有未来日期的数据。由于双向系统使用2个循环层，我们将其与堆叠架构进行比较，并询问“哪一个更好——2层堆叠循环层还是双向循环层？”。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/36a3a563800438f2b6c8363206bca7c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VAQMWkwu43tdRdFbVAVo3w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有两个不同循环层的双向层。</p></figure><h1 id="7b02" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">方法学</h1><p id="3781" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">接下来，让我们尝试使用我们知道的模型(基本循环层)和上面学到的技术来定义和分组我们可以创建的不同排列。在更高的层次，我们把所有的安排分成两大类—</p><ol class=""><li id="fe38" class="mr ms iq lq b lr mt lu mu lx mv mb mw mf mx mj my mz na nb bi translated"><strong class="lq ir">单栈(SS) </strong>:其中我们只使用一个循环层</li><li id="fec8" class="mr ms iq lq b lr nc lu nd lx ne mb nf mf ng mj my mz na nb bi translated"><strong class="lq ir">多栈(MS) </strong>:其中我们使用了多个循环层。为了保持分析简单，我将只使用2层安排。</li></ol><p id="3805" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">此外，由于rnn已经<a class="ae kv" href="https://stats.stackexchange.com/questions/222584/difference-between-feedback-rnn-and-lstm-gru#:~:text=We%20can%20say%20that%2C%20when,Inputs%20as%20per%20trained%20Weights.&amp;text=So%2C%20LSTM%20gives%20us%20the,more%20Complexity%20and%20Operating%20Cost." rel="noopener ugc nofollow" target="_blank">让位于“更好”</a>的循环层，我们在分析中将只考虑LSTM和GRU。现在，在SS的情况下，我们可以使用LSTM或GRU，我们也可以通过只取1个输出或N个输出来增加味道。这给了我们可能想要考虑的不同安排。接下来，在<strong class="lq ir"> MS </strong>(本文只有2个)的情况下，让我们首先从1个和N个输出开始。对于每个这样的设置，我们可以使用两层堆叠或双向LSTM和GRU。这给了我们<code class="fe mk ml mm mn b">2x2x2=8</code>不同的安排。所有12种排列组合如下所示。注意，我说的“返回序列错误”是指1个输出，因为你只返回一个输出，而不是完整的序列。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/20acd2527405efe7b72b0e0f413a3097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2j1-R6ocxTCDoWuW5mY2cg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">循环层排列的分离。</p></figure><h1 id="9a53" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">密码</h1><p id="0ed4" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在让我们看看如何在Keras中编码不同的安排。为了简洁起见，我将只展示LSTM递归层的代码。转移到GRU就像在代码中用GRU替换LSTM一样简单(不过别忘了导入GRU)。我也将只涵盖4个不同的品种，因为休息可以很容易地建立后，小的修改。LSTM RSF市入门，即单一堆叠-返回序列错误-LSTM图层😉</p><p id="153c" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated"><strong class="lq ir"> SS_RSF_LSTM </strong></p><pre class="kg kh ki kj gt nl mn nm nn aw no bi"><span id="3f17" class="np kx iq mn b gy nq nr l ns nt"># import <br/>from tensorflow.keras import layers<br/>from tensorflow import keras<br/># model<br/>inputs = keras.Input(shape=(99, )) # input layer - shape should be defined by user.<br/>embedding = layers.Embedding(num_words, 64)(inputs) # embedding layer<br/>rl = layers.LSTM(128)(embedding) # our LSTM layer - default return sequence is False<br/>dense = layers.Dense(64)(rl) # a dense layer on the output of LSTM<br/>output = layers.Dense(1, activation='sigmoid')(dense) # final layer which gives classification result<br/>self.model = keras.Model(inputs=inputs, outputs=output, name="SS_RSF_LSTM") # <!-- -->stitching<!-- --> everything together</span></pre><p id="c591" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">需要注意的事情很少，</p><ul class=""><li id="9c2d" class="mr ms iq lq b lr mt lu mu lx mv mb mw mf mx mj nu mz na nb bi translated">代码是根据情感分类任务创建的(下一节)。因此，最终的输出层的大小为1，具有sigmoid激活以返回概率作为输出。</li><li id="d1c1" class="mr ms iq lq b lr nc lu nd lx ne mb nf mf ng mj nu mz na nb bi translated">我们定义一个输入，其中我们说shape=99，即我们在数据中预期的时间步长。这里指的是每个句子的字数。</li><li id="c88b" class="mr ms iq lq b lr nc lu nd lx ne mb nf mf ng mj nu mz na nb bi translated">我们使用嵌入层将单词转换成向量表示。我们可以使用现成的单词嵌入，如Glove或Word2Vec，但是我们初始化了一个新的，专门为这个任务训练的单词嵌入。每个向量的大小是64，<code class="fe mk ml mm mn b">num_words</code>是我们正在使用的数据集的词汇大小。</li><li id="1c1e" class="mr ms iq lq b lr nc lu nd lx ne mb nf mf ng mj nu mz na nb bi translated">我们使用一个状态输出大小为128的LSTM层。注意，由于默认返回序列为假，所以我们只得到一个输出，即LSTM的最后一个状态。</li><li id="ada5" class="mr ms iq lq b lr nc lu nd lx ne mb nf mf ng mj nu mz na nb bi translated">我们将最后一个状态输出与大小=64的密集层连接起来。这用于增强LSTM输出的复杂阈值处理。</li></ul><p id="d716" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated"><strong class="lq ir"> SS_RST_LSTM </strong></p><p id="4ce7" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">这里唯一的区别是，我们从LSTM返回所有州的输出。我们可以这样做:</p><pre class="kg kh ki kj gt nl mn nm nn aw no bi"><span id="9a94" class="np kx iq mn b gy nq nr l ns nt">inputs = keras.Input(shape=(99, ))<br/>embedding = layers.Embedding(num_words, 64)(inputs)<br/>rl = layers.LSTM(128, return_sequences=True)(embedding)<br/>avg = tf.keras.layers.AveragePooling1D(pool_size=99)(rl)<br/>dense = layers.Dense(64)(avg)<br/>output = layers.Dense(1, activation='sigmoid')(dense)<br/>self.model = keras.Model(inputs=inputs, outputs=output, name="SS_RST_LSTM")</span></pre><p id="90fe" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">注意事项:</p><ul class=""><li id="44eb" class="mr ms iq lq b lr mt lu mu lx mv mb mw mf mx mj nu mz na nb bi translated">第一个变化是在LSTM定义中增加了<code class="fe mk ml mm mn b">return_sequence=True</code>。</li><li id="b64f" class="mr ms iq lq b lr nc lu nd lx ne mb nf mf ng mj nu mz na nb bi translated">接下来，我们需要将99x128矩阵合并成1x128大小的向量。虽然有很多方法可以做到这一点，但我们将选择<code class="fe mk ml mm mn b">AveragePooling1D</code>，它简单地取每99个时间步长向量的平均值来返回1个向量输出。这里的<code class="fe mk ml mm mn b">pool_size</code>是指我们取每99个时间步的平均值。</li></ul><p id="e172" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated"><strong class="lq ir">MS _ RSF _比尔斯特姆</strong></p><p id="22e8" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">要有一个双向层，我们需要做的就是在LSTM之上添加一个<code class="fe mk ml mm mn b">Bidirectional</code>函数。</p><pre class="kg kh ki kj gt nl mn nm nn aw no bi"><span id="9893" class="np kx iq mn b gy nq nr l ns nt">inputs = keras.Input(shape=(99, ))<br/>embedding = layers.Embedding(num_words, 64)(inputs)<br/>rl = layers.Bidirectional(layers.LSTM(128))(embedding)<br/>dense = layers.Dense(64)(rl)<br/>output = layers.Dense(1, activation='sigmoid')(dense)<br/>self.model = keras.Model(inputs=inputs, outputs=output, name="MS_RSF_biLSTM")</span></pre><p id="78c1" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">注意要移动到<strong class="lq ir">MS _ RST _比尔斯特姆</strong>你需要做两件事——(1)在LSTM层内添加<code class="fe mk ml mm mn b">return_sequence=True</code>，以及(2)在双向LSTM层后添加<code class="fe mk ml mm mn b">AveragePooling1D</code>逻辑，如上所述。</p><p id="a731" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated"><strong class="lq ir">RST LSTM女士</strong></p><pre class="kg kh ki kj gt nl mn nm nn aw no bi"><span id="625b" class="np kx iq mn b gy nq nr l ns nt">inputs = keras.Input(shape=(99, ))<br/>embedding = layers.Embedding(num_words, 64)(inputs)<br/>rl = layers.LSTM(128, return_sequences=True)(embedding)<br/>rl = layers.LSTM(128, return_sequences=True)(rl)<br/>avg = tf.keras.layers.AveragePooling1D(pool_size=99)(rl)<br/>dense = layers.Dense(64)(avg)<br/>output = layers.Dense(1, activation='sigmoid')(dense)<br/>self.model = keras.Model(inputs=inputs, outputs=output, name="MS_RST_LSTM")</span></pre><p id="e93f" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">注意事项:</p><ul class=""><li id="307e" class="mr ms iq lq b lr mt lu mu lx mv mb mw mf mx mj nu mz na nb bi translated">要堆叠多个LSTM，所有较低的lstm都必须有<code class="fe mk ml mm mn b">return_sequence=True</code>，因为它们将作为输入馈送到下一个LSTM。</li><li id="6e1a" class="mr ms iq lq b lr nc lu nd lx ne mb nf mf ng mj nu mz na nb bi translated">对于最顶端的LSTM，它的用户的选择。正如我们看到的一个<em class="nv"> RST </em>的例子，代码返回所有的状态序列，然后进行平均。</li><li id="1c48" class="mr ms iq lq b lr nc lu nd lx ne mb nf mf ng mj nu mz na nb bi translated">要以这种方式堆叠更多的层，我们需要做的就是一次又一次地复制粘贴<code class="fe mk ml mm mn b">rl = layers.LSTM(128, return_sequences=True)(rl)</code>行。这里我们有两条这样的线，我们有两层堆叠的LSTM。</li></ul><p id="dfac" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">按照上面分享的提示，剩下的安排可以很容易地编码。我把它作为一个练习，并在这里分享完整的<a class="ae kv" href="https://gist.github.com/imohitmayank/757a2d878a1510180f134a8c7f45d6dc" rel="noopener ugc nofollow" target="_blank">代码</a>以供参考。</p><h1 id="f44f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">实验</h1><h2 id="d9e1" class="np kx iq bd ky nw nx dn lc ny nz dp lg lx oa ob li mb oc od lk mf oe of lm og bi translated">资料组</h2><p id="2877" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了测试我们的模型，我选择了<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb" rel="noopener ugc nofollow" target="_blank"> IMDB情感分类数据集</a>，其中包含25，000条高度极性的电影评论，其二元情感由标签0和1表示。为了在我的笔记本电脑上更方便，我进一步整理了数据，只考虑了频率最高的5000个单词，并截断了超过100个单词的句子。由于数据API是由Keras公开的，所有这些都可以由，</p><pre class="kg kh ki kj gt nl mn nm nn aw no bi"><span id="016e" class="np kx iq mn b gy nq nr l ns nt"># import<br/>import tensorflow as tf<br/># set parameters<br/>num_words = 5000<br/>maxlen = 100<br/># fetch data<br/>(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=num_words, <br/>                                                                        maxlen=maxlen)</span></pre><p id="5b3a" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">此外，Keras已经将单词转换成整数表示形式，从而更容易输入到我们的模型中。剩下的就是确保所有的句子大小相同，因为可能有一些句子的大小小于<code class="fe mk ml mm mn b">maxlen</code>。这可以通过用一个虚拟数字(比如0)填充较小的句子来实现。你可以在短句的左边或右边添加填充，我选择了左边。这样做的代码是，</p><pre class="kg kh ki kj gt nl mn nm nn aw no bi"><span id="5fe9" class="np kx iq mn b gy nq nr l ns nt">## perform padding<br/>x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, padding="pre")<br/>x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, padding="pre")</span></pre><h2 id="1d54" class="np kx iq bd ky nw nx dn lc ny nz dp lg lx oa ob li mb oc od lk mf oe of lm og bi translated">结果</h2><p id="6b04" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了比较所有模型的性能得分，我们将根据准备好的数据和报告准确性，对每个模型进行指定次数的训练。为了处理训练中的变化，我们将多次重复训练过程(运行),并报告每次运行中观察到的准确度分数的变化以及最高和最低分数。最后，为了确保随机权重初始化不会引入模型性能的变化，我们将为每次运行使用相同的初始权重。然后，引入的主要差异在于模型拟合及其内部训练验证数据分割。</p><p id="c30b" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">每个实验(针对一个模型)运行5次，每次运行5个时期。总共，我们运行了12个这样的实验(对于我们所有的模型)。综合业绩报告卡如下所示，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/9681b1bbe99430c93ba8531648029259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F8oHqEFoDNXPm6jvLfzuAQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">看看LSTM RST的获胜者，这比LSTM RSF的简单SS(香草LSTM)要好得多。</p></figure><p id="ce1a" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">现在，让我们试着回答我们之前提出的问题，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/efe6806e09bef69577ccf4ad07325106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7D7IX6ib64kQKPKm24IbGg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">报告精确度差异的问题相关模型比较表(右侧模型减去左侧模型)。对于每个问题，更多的绿色表示是，更多的红色表示不是。</p></figure><ul class=""><li id="fc5f" class="mr ms iq lq b lr mt lu mu lx mv mb mw mf mx mj nu mz na nb bi translated">Q1:不同的经常性产出类型有关系吗？ —简答:有！为了理解其中的原因，让我们将每一款<em class="nv"> RSF </em>车型与其对应的RST车型进行比较。所有这些可能的比较和差异(右减去左)如上所示。很明显，4/6的模型表现更好。事实上，我们最好的模型是MS_RST_LSTM，它比它的RSF版本有超过3%的改进。</li><li id="99cc" class="mr ms iq lq b lr nc lu nd lx ne mb nf mf ng mj nu mz na nb bi translated"><strong class="lq ir"> Q2:堆叠多层很重要吗？</strong> —又来了！见第二个比较表，所有正差异。增加更多的层会产生额外的可调参数，从而获得更大的学习能力。但是，这并不意味着你可以堆叠100层，并期望它比以前工作得更好——在某一点上，它会收敛，即增加更多的层不会有帮助。但是堆叠几层确实有帮助，如图所示。</li><li id="d815" class="mr ms iq lq b lr nc lu nd lx ne mb nf mf ng mj nu mz na nb bi translated"><strong class="lq ir"> Q3:双向分层在对抗正常堆叠时表现更好吗？</strong>——有趣的是没有！3/4倍堆叠层性能更好。这可能是因为多个堆栈层具有更多训练参数，因此对数据的推断更好。</li></ul><h1 id="f9d8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="a9db" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">让我从一个免责声明开始——虽然我(或者更确切地说是实验结果)说一种安排比另一种安排更好，但我并不是暗示你应该只训练“好”的模型，而忽略其他的。请记住，每个任务及其数据都是不同的，可能会导致与上述报告不同的结果。以双向为例，通常，我发现双向与堆叠层竞争，在某些情况下甚至表现更好。这与我们之前看到的形成了对比，这证明了每个数据都是不同的。也就是说，这篇文章的目的有两个——( 1)展示仅使用基本的递归层就可以实现的不同种类的分层排列，以及(2)基于实验来帮助排列优先级，而不是忽略它们。所以下次你有一个文本分类项目，你可以从MS_RST_LSTM模型开始，用时间(如果你有的话)在别人身上做实验。</p><p id="d22b" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">干杯！</p></div><div class="ab cl oj ok hu ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="ij ik il im in"><p id="36b2" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">在LinkedIn上与我联系，或者在我的网站上阅读更多这样的文章。</p></div></div>    
</body>
</html>