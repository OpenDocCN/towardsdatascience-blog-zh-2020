<html>
<head>
<title>Introduction to Message Passing Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">消息传递神经网络简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-message-passing-neural-networks-e670dc103a87?source=collection_archive---------5-----------------------#2020-10-05">https://towardsdatascience.com/introduction-to-message-passing-neural-networks-e670dc103a87?source=collection_archive---------5-----------------------#2020-10-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="619d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在图表上发送信息的神经网络</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/07e23eb3517f5d5973ca96860ba59251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zcZeI_u6Ea2hFNdS"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@ryuldavidson?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Ryul Davidson </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="420d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">相关文章</h1><ul class=""><li id="a86e" class="lr ls it lt b lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/feature-extraction-for-graphs-625f4c5fb8cd">图的特征提取</a></li><li id="de54" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/machine-learning-tasks-on-graphs-7bc8f175119a">图上的机器学习任务</a></li><li id="0259" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/towards-explainable-graph-neural-networks-45f5e3912dd0">面向可解释图的神经网络</a></li><li id="cb64" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/top-10-learning-resources-for-graph-neural-networks-f24d4eb2cc2b">图形神经网络的10大学习资源</a></li></ul></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="70c6" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi nk translated"><span class="l nl nm nn bm no np nq nr ns di"> W </span>欢迎来到图形神经网络的世界，在这里我们在图形上构建深度学习模型。你可能认为这很简单。毕竟，难道我们不能重用使用正常数据的模型吗？</p><p id="f966" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">不完全是。在图中，所有的数据点(节点)都是相互连接的。这意味着数据不再是独立的，这使得大多数标准的机器学习模型无用，因为它们的推导强烈地基于这一假设。为了克服这个问题，可以从图中提取数字数据，或者使用直接处理这类数据的模型。</p><p id="547c" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">创建直接在图上工作的模型是更可取的，因为我们获得了关于它们的结构和属性的更多信息。在本文中，我们将研究专门为这种类型的数据设计的架构之一，消息传递神经网络(MPNNs)。</p><h1 id="2215" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">多面模特</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/86c4bcf0f8a2c8dbc0f5b0648fc03d45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nMw8BeUtN1TykDz7"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安德鲁·西曼在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="1792" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">在这个模型被标准化为一个单一的MPNN框架之前，几个独立的研究人员已经发表了不同的版本。这种类型的结构在化学中特别受欢迎，有助于预测分子的性质。</p><p id="a139" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">Duvenaud等人(2015)在[1]中发表了关于这一主题的第一批工作之一。他使用消息传递架构从图形分子中提取有价值的信息，然后将其转换为单个特征向量。当时，他的工作是开创性的，因为他使架构<em class="nu">可区分。</em>事实上，这是第一个可以在图形上操作的卷积神经网络架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/64c6717df9548fb4dc3b41b8216a4d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*NxgW0QIfO-mgT1-I8WqfJQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Duvenaud等人(2015)在[1]中创建的消息传递架构。他将该模型定义为一堆可区分的层，其中每一层都是另一轮的消息传递。修改自[1]</p></figure><p id="492a" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">李等人(2016)在[2]中对此架构进行了另一次尝试。在这里，他们关注图的顺序输出，例如在图[2]中寻找最优路径。为了实现这一点，他们将GRU(门控循环单元)嵌入到他们的算法中。</p><p id="56d1" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">尽管这些算法看起来非常不同，但它们有相同的基本概念，即在图中的节点之间传递消息。我们将很快看到如何将这些模型组合成一个单一的框架。</p><h1 id="63d1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">将模型统一到MPNN框架中</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/0da189d4ac08e02a4f349de61ef5f312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*oSQyFjtUkI7_u7lJXWU68Q.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">节点<strong class="bd nx"> V1 </strong>的消息传递架构的一个非常简单的例子。在这种情况下，消息是邻居隐藏状态的总和。更新函数是消息<strong class="bd nx"> m </strong>和<strong class="bd nx"> h1 </strong>之间的平均值。作者创建的Gif</p></figure><p id="2bf5" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">毕竟，MPNN背后的想法在概念上很简单。</p><p id="3739" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">图中的每个节点都有一个隐藏状态(即特征向量)。对于每个节点<strong class="lt iu"> V </strong> t，我们将所有<em class="nu">相邻节点的隐藏状态和可能的边与节点<strong class="lt iu"> V </strong> t本身聚合在一起。然后，我们使用获得的消息和该节点的先前隐藏状态来更新节点<strong class="lt iu"> V </strong> t的隐藏状态。</em></p><p id="201d" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">有3个主要等式定义了图上的MPNN框架[3]。从相邻节点获得的消息由以下等式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/39555fc0757a79aac557b0cacd380dd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*hfY4yVjHUBVvJQuMdG74yQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从相邻节点获得消息。修改自[3]。</p></figure><p id="bedf" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">它是Mt从邻居获得的所有消息的总和。Mt是一个任意函数，它依赖于相邻节点的隐藏状态和边缘。我们可以通过保留一些输入参数来简化这个函数。在上面的例子中，我们只对不同的隐藏状态h <em class="nu"> w. </em>求和</p><p id="3917" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">然后，我们使用一个简单的等式更新节点<strong class="lt iu"> V </strong> t的隐藏状态:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/cd3e03fec8a83f93c7d5e87dfc21f396.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*Y2-JZf53PjNEs4sN3nIScA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用先前的隐藏状态和新消息更新节点的状态。修改自[3]。</p></figure><p id="13c7" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">简单地说，节点<strong class="lt iu"> V </strong> t的隐藏状态是通过用新获得的消息<strong class="lt iu"> m </strong> v更新旧的隐藏状态而获得的。在上述示例的情况下，更新函数Ut是先前隐藏状态和消息之间的平均值。</p><p id="2cc8" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">我们重复这个消息传递算法指定的次数。之后，我们到达最后的读出阶段。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/d9bea045f26af2b94ec09607b0209e2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*26UNARz9n6fcMDSz_nmcnw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将获得的隐藏状态映射到描述整个图的单个特征向量中。修改自[3]。</p></figure><p id="c503" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">在这一步中，我们提取所有新更新的隐藏状态，并创建描述整个图的最终特征向量。然后，该特征向量可以用作标准机器学习模型的输入。</p><p id="7677" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">就是这样！这些是MPNN的基础。这个框架非常健壮，因为我们可以根据我们想要实现的目标来定义不同的消息和更新功能。我建议查看[3]更多信息，MPNN模型的不同变体。</p><h1 id="3152" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">在哪里可以找到模型的实现</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/bf63cc6af721558876ed888f32cf22ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RT-6voZMLKN7bVA1"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@jannerboy62?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">尼克·费因斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="1b07" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">MPNN是作为少数深度学习库的一部分实现的。以下是我能找到的几种不同实现的列表:</p><ul class=""><li id="6381" class="lr ls it lt b lu mx lw mz ly oc ma od mc oe me mf mg mh mi bi translated"><a class="ae ky" href="https://github.com/brain-research/mpnn" rel="noopener ugc nofollow" target="_blank">原始模型的实现</a></li><li id="9041" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" href="https://github.com/deepchem/deepchem/tree/master/contrib/mpnn" rel="noopener ugc nofollow" target="_blank">深度化学实施</a></li><li id="698e" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" href="https://github.com/rusty1s/pytorch_geometric" rel="noopener ugc nofollow" target="_blank"> PyTorch几何实现</a></li></ul><h1 id="0837" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">总结</h1><p id="8c43" class="pw-post-body-paragraph mv mw it lt b lu lv ju my lw lx jx na ly of nc nd ma og nf ng mc oh ni nj me im bi translated">MPNN框架标准化了由几个研究人员独立创建的不同的消息传递模型。该框架的主要思想由消息、更新和读出功能组成，这些功能在图中的不同节点上操作。有不同的MPNN模型共享这一功能，但他们定义不同。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="a1a3" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">感谢阅读，希望对你有用！</p><h1 id="b82d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">关于我</h1><p id="96cb" class="pw-post-body-paragraph mv mw it lt b lu lv ju my lw lx jx na ly of nc nd ma og nf ng mc oh ni nj me im bi translated">我是阿姆斯特丹大学的人工智能硕士学生。在我的业余时间，你可以发现我摆弄数据或者调试我的深度学习模型(我发誓这很有效！).我也喜欢徒步旅行:)</p><p id="020f" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">如果你想了解我的最新文章和其他有用的内容，以下是我的其他社交媒体资料:</p><ul class=""><li id="9715" class="lr ls it lt b lu mx lw mz ly oc ma od mc oe me mf mg mh mi bi translated"><a class="ae ky" href="https://www.linkedin.com/in/kacperkubara/" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="7d2b" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" href="https://github.com/KacperKubara" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li></ul><h1 id="0ca1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="9ba8" class="pw-post-body-paragraph mv mw it lt b lu lv ju my lw lx jx na ly of nc nd ma og nf ng mc oh ni nj me im bi translated">[1] <a class="ae ky" href="https://arxiv.org/pdf/1509.09292.pdf" rel="noopener ugc nofollow" target="_blank">用于学习分子指纹的图上的卷积网络</a></p><p id="04a0" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">[2] <a class="ae ky" href="https://arxiv.org/abs/1511.05493" rel="noopener ugc nofollow" target="_blank">门控图序列神经网络</a></p><p id="746e" class="pw-post-body-paragraph mv mw it lt b lu mx ju my lw mz jx na ly nb nc nd ma ne nf ng mc nh ni nj me im bi translated">【3】<a class="ae ky" href="https://arxiv.org/pdf/1704.01212.pdf" rel="noopener ugc nofollow" target="_blank">量子化学的神经信息传递</a></p></div></div>    
</body>
</html>