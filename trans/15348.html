<html>
<head>
<title>K-Means Clustering and the Gap-Statistics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-均值聚类和间隙统计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-and-the-gap-statistics-4c5d414acd29?source=collection_archive---------0-----------------------#2020-10-22">https://towardsdatascience.com/k-means-clustering-and-the-gap-statistics-4c5d414acd29?source=collection_archive---------0-----------------------#2020-10-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b06f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用差距统计弥合知识差距</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/656f5b5e0c1ea81da2b02dcf61a74495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3wegFyGEfip1U5Y8tt-V-w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.freeimages.com/photographer/Suserl-44212" rel="noopener ugc nofollow" target="_blank">suser ll just me</a>在<a class="ae kv" href="https://www.freeimages.com/" rel="noopener ugc nofollow" target="_blank"> Freeimages </a>上拍摄并由我编辑</p></figure><blockquote class="kw"><p id="7455" class="kx ky iq bd kz la lb lc ld le lf lg dk translated">在引擎盖下有许多代码在运行。这就是为什么我在这篇文章的最后提供了我的Github库，并且我只展示了K-Means的一小段代码。</p></blockquote><h1 id="01ab" class="lh li iq bd lj lk ll lm ln lo lp lq lr jw ls jx lt jz lu ka lv kc lw kd lx ly bi translated">介绍</h1><p id="9ce6" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">聚类是模式分析中识别数据中不同组的一种重要技术。由于数据大多是三维以上的，所以在应用聚类技术之前，我们执行像PCA或拉普拉斯特征映射这样的降维方法。然后，这些数据可以在2D或3D中使用，这使得我们可以很好地将发现的星团可视化。<br/>尽管这是一个基本的工作流程，但情况并非总是如此。</p><p id="1d3f" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">数据通常也是无标签的。这意味着你没有明确的定义你想在这些数据中找到什么。这就是为什么聚类也是一种很好的数据探索技术，而不需要事先降维。</p><p id="bd56" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">常见的聚类算法是K-Means和Meanshift算法。在这篇文章中，我将重点介绍K-Means算法，因为这是最简单和最直接的聚类技术。此外，我们将假设数据要么直接提供有两个特征(因此2D)，要么有人对数据执行2D降维，然后将其提供给我们。因此，我们直接深入应用K-Means。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><p id="60ab" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">K-Means算法需要一个超参数，即你想要寻找的聚类数<strong class="mb ir"> K </strong>。但是，如果我们想找到集群，我们怎么知道我们需要多少集群？</p><blockquote class="ng nh ni"><p id="d437" class="lz ma nj mb b mc mu jr me mf mv ju mh nk mw mk ml nl mx mo mp nm my ms mt lg ij bi translated"><strong class="mb ir">示例</strong>:如果我们想要在数据中为我们的营销团队找到人物角色，我们可以假设我们想要找到三到四类人。</p></blockquote><p id="6890" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">在这种情况下，K的数量是固定的。但是如果<strong class="mb ir">不是</strong>呢？</p><p id="f030" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">超参数的选择称为模型选择。在<br/> K-Means的情况下，这只是我已经说过的<strong class="mb ir"> K、</strong>的数量。在这篇博文中，我将重点讲述我们如何通过对K-Means执行Tibshirani的Gap-Statistics，从统计上找出K 的最佳值。</p><blockquote class="kw"><p id="07d5" class="kx ky iq bd kz la nn no np nq nr lg dk translated">Gap-Statistics是由Robert Tibshirani于2000年在斯坦福引入的。</p></blockquote><h1 id="9c50" class="lh li iq bd lj lk ll lm ln lo lp lq lr jw ls jx lt jz lu ka lv kc lw kd lx ly bi translated">目标</h1><p id="4d0c" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">我想用这个帖子来回答这三个问题:</p><ul class=""><li id="1078" class="ns nt iq mb b mc mu mf mv mi nu mm nv mq nw lg nx ny nz oa bi translated">1)如何在K-Means中找到K 的最佳值？</li><li id="49c8" class="ns nt iq mb b mc ob mf oc mi od mm oe mq of lg nx ny nz oa bi translated">2)我们如何衡量差距统计的性能？</li><li id="333e" class="ns nt iq mb b mc ob mf oc mi od mm oe mq of lg nx ny nz oa bi translated">3)在什么条件下差距统计可能会失败？</li></ul></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="8a16" class="lh li iq bd lj lk og lm ln lo oh lq lr jw oi jx lt jz oj ka lv kc ok kd lx ly bi translated"><strong class="ak">K-Means——非常简短的介绍</strong></h1><p id="0c7b" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">K-Means执行三个步骤。但是首先你需要预先定义k的数量。这些聚类点通常被称为<strong class="mb ir">质心</strong>。</p><ul class=""><li id="ddc8" class="ns nt iq mb b mc mu mf mv mi nu mm nv mq nw lg nx ny nz oa bi translated">1)通过计算所有点到所有质心之间的欧几里德距离，将每个数据点(重新)分配到其最近的质心。</li><li id="6a29" class="ns nt iq mb b mc ob mf oc mi od mm oe mq of lg nx ny nz oa bi translated">2)基于所有相应的数据点计算每个质心的平均值，并将质心移动到所有指定数据点的中间。</li><li id="096e" class="ns nt iq mb b mc ob mf oc mi od mm oe mq of lg nx ny nz oa bi translated">3)转到1)直到满足收敛标准。在我的例子中，我计算所有点到重新分配的质心平均值之间的类内距离。在新的迭代之后，如果所有的质心一起移动小于0.01，那么基本上不再发生什么，执行收敛标准。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol om l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码1)K-表示由我从头开始编写</p></figure><p id="cb02" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">在上面的代码1中可以看到，第9行使用列表理解为每个数据点计算到每个中心点的欧几里德距离(第3行中K = 3)。Numpy.argmin然后为每个点选择最近的距离，并将其分配给该质心。第13行计算第三步，并对总的簇内距离求和。如果小于0.01，则while循环中断。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/339d2542577a034f28d1bb765226d2f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/1*BAaNLpVupzkS_JXb933VYA.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1) K=3 gif |我做的</p></figure><p id="0d92" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">我们可以看到在循环中每个<em class="nj">点的分配是如何变化的。总共需要五次循环直到收敛。</em></p><blockquote class="ng nh ni"><p id="480b" class="lz ma nj mb b mc mu jr me mf mv ju mh nk mw mk ml nl mx mo mp nm my ms mt lg ij bi translated">通知:质心的初始化对K-Means的执行有很大的影响。想象两个质心从最左边和最低的位置开始。每个数据点将被分配给质心一号，聚类是无用的。</p><p id="7ccd" class="lz ma nj mb b mc mu jr me mf mv ju mh nk mw mk ml nl mx mo mp nm my ms mt lg ij bi translated"><strong class="mb ir">注意</strong> : K-Means只是局部最优，并不能保证全局最小</p></blockquote><h2 id="0e9c" class="oo li iq bd lj op oq dn ln or os dp lr mi ot ou lt mm ov ow lv mq ox oy lx oz bi translated">进一步的调查</h2><p id="0159" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">图1显示了300个数据点的3个质心。我还没有展示数据的代码，它只是从带有<em class="nj"> make_blobs </em>的<em class="nj"> sklearn </em>中生成的。</p><p id="3ba3" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">当然，在有三个全局最优质心的情况下，这种簇内距离不会进一步减小。但是当我们给K加上越来越多的值时会发生什么呢？簇内距离将几乎单调地缩小。想象一下，如果我们用一个额外的质心将图1的三个集群中的一个分成两个。簇内距离会缩小，因为所有点现在都更靠近另一个质心，即使这显然是一个错误的<strong class="mb ir">簇分裂。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e3ceebb84fdd7300643458b766dafe29.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*tzRy1YPUJ1LDABhhcKNhmw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2) K=300 |我做的</p></figure><p id="5bb0" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">当我们选择K的数量等于质心的数量时，总的簇内距离将是0。从最小化问题的角度来看，这是理想的，但对我们来说不是。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/1d03c77509a760cd8d4f3aa35cd48855.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*DtqNACzmlMez9upCmQLNkA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3) K=11 |我做的</p></figure><p id="a9f0" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">当我们选择K的数目等于11时，我们可以看到，我们将永远不会达到小于3的总簇内距离。</p><blockquote class="ng nh ni"><p id="f6d6" class="lz ma nj mb b mc mu jr me mf mv ju mh nk mw mk ml nl mx mo mp nm my ms mt lg ij bi translated">由于图2和图3中的起点为1，所以图2的实际簇内距离为零，而图3的实际簇内距离为二。</p></blockquote><p id="8d7a" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">那么我们如何防止K被选择为等于数据点的数量呢？</p><h2 id="6a81" class="oo li iq bd lj op oq dn ln or os dp lr mi ot ou lt mm ov ow lv mq ox oy lx oz bi translated">我们如何通过初始化来强制我们的集群拥有K个非空组件？</h2><ul class=""><li id="e0e3" class="ns nt iq mb b mc md mf mg mi pc mm pd mq pe lg nx ny nz oa bi translated">1)在样本点的位置初始化每个聚类中心</li><li id="222f" class="ns nt iq mb b mc ob mf oc mi od mm oe mq of lg nx ny nz oa bi translated">2)针对局部最小值制定对策</li></ul></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="db1c" class="lh li iq bd lj lk og lm ln lo oh lq lr jw oi jx lt jz oj ka lv kc ok kd lx ly bi translated">1)如何在K-Means中找到K 的最优值？</h1><p id="3ec4" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">答案当然是<strong class="mb ir">缺口统计</strong>，但它是什么呢？</p><p id="9d37" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">它是由罗伯特·提布拉尼在20年前发明的。基本的最小化问题如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/9a1e0e0588f40b6e8b110e39339c7c46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5hZ5Po5rWcn4bCAxPnm8Eg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">公式1)估计K*的形式规则由<strong class="bd pg"> HTF </strong>统计学习的要素第<strong class="bd pg"> 519页</strong> | 14.3.11实际问题</p></figure><p id="3381" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">看起来怪怪的？没问题！K*是我们想要找到的K的最优值。我们需要定义不同的数据矩阵W_data和W_uniform。其中W_data是上图1中的300个数据点，W_uniform基本上是群集内距离的模拟平均分布，如<strong class="mb ir">进一步调查</strong>部分所述。W_uniform基本看起来像图3。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ph"><img src="../Images/85b2cb53071eaeb804e78cef21827129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KViWppNopU8UkxDK_olSPQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">公式2)缩减差距统计公式</p></figure><p id="d144" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">我们在<strong class="mb ir"> <em class="nj">中可以看到，其中</em> </strong> <em class="nj"> </em>上面的<em class="nj">，</em>表示S’(S素数)等于前面的S乘以一个小值(1.024695)。这意味着它在每个时间步长不断变大。</p><p id="c951" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">第一个时间步长的初始S等于所有均匀绘制的数据点的标准偏差，例如20次运行。可能更多，但是Tibshirani写道20次模拟已经足够了。</p><h2 id="2128" class="oo li iq bd lj op oq dn ln or os dp lr mi ot ou lt mm ov ow lv mq ox oy lx oz bi translated">这是什么意思呢？</h2><p id="3b34" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">好吧。正如我们已经指出的，我们需要一种方法，使得K的最佳数目不被选为数据点的数目。为此，引入模拟平均值<strong class="mb ir">W _ uniform from</strong>和标准差S’(K+1)。</p><p id="f4f7" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">通过检查公式1中每个时间步长的图中的最小化问题，我们取K的数目，这通过引入新的质心而使最大跳跃进入减少的总簇内距离。</p><p id="37f2" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">如果你仔细观察图3，你会发现距离减少的跳跃对于前几个质心来说特别大。这具有直观的意义，因为如果你回头看一下图1，很明显你引入的质心越多，簇内距离的变化就越小。</p><p id="8701" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">因此，差距统计的基本思想是根据均匀抽取的样本的总体行为，选择K的数量，即类内距离发生最大跳跃的地方。可能的情况是簇内距离仅发生非常轻微的减小。出于这个原因，S'(K+1)充当阈值，以挑选出太微小的变化，并从数据中去除采样噪声。只有当变化大到阈值S′(K+1)不再起作用时，才会选择K的最佳值。</p><p id="d623" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">让我为你想象一下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pi"><img src="../Images/27111645d3daeb7bd08e5f4450eb6106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EmCtR0SWfquXi7FyWQn4Xw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4)差距统计|我做的</p></figure><blockquote class="ng nh ni"><p id="f2ee" class="lz ma nj mb b mc mu jr me mf mv ju mh nk mw mk ml nl mx mo mp nm my ms mt lg ij bi translated"><strong class="mb ir">注意</strong>:类内距离曲线的值已经被最大类间距离归一化。</p></blockquote><p id="3970" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">我故意选择了图4左上角的数据，下面的两个簇非常接近。K-means几乎不可能分辨出这些是不同的聚类。</p><p id="e824" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">在左下方的图像中，我们可以看到差距统计数据。选择K=3的最佳值，因为我们在值再次收缩之前选择第一个峰值点。红线的计算方法是从右下图的W_data(蓝色)中减去W_uniform(绿色)。</p><blockquote class="ng nh ni"><p id="3c4c" class="lz ma nj mb b mc mu jr me mf mv ju mh nk mw mk ml nl mx mo mp nm my ms mt lg ij bi translated"><strong class="mb ir">退一步</strong>:右下图像显示W_data(蓝色)和W_uniform(绿色)分布。通过查看G(K)上的公式2，我们看到需要用W_data to的对数减去W_uniform的对数。</p></blockquote><p id="9e12" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">对于W_data和模拟的W_uniform，总的簇内距离如预期的那样收缩。但是在K=4时，间隙统计检测到W_data的总距离的变化不像模拟的那样。这意味着它没有像预期的那样减少。</p><p id="8477" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">即使当从W_uniform中减去标准偏差S’(K+1)时，间隙统计也将选择K的最佳值作为<strong class="mb ir"> 3 </strong>。</p><blockquote class="ng nh ni"><p id="a13c" class="lz ma nj mb b mc mu jr me mf mv ju mh nk mw mk ml nl mx mo mp nm my ms mt lg ij bi translated"><strong class="mb ir">注意</strong>:如果我们假设K=4处的间隙不存在，则低于其前一个点的下一个点在K=6处。但由于我将标准偏差标记为垂直的粗红线，我们看到，减去标准偏差后，距离的变化太小，因此K将被选择为K=6。将被选择的下一个K是K=7，因为在K=8时出现下一个大间隙。</p></blockquote><h1 id="d465" class="lh li iq bd lj lk ll lm ln lo lp lq lr jw pj jx lt jz pk ka lv kc pl kd lx ly bi translated">2)我们如何衡量差距统计的性能？</h1><p id="b50d" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">正如我已经提到的，质心的初始化高度影响K均值的优化。我们需要考虑不同的数据分布。在计算K-Means算法的第二步时，我们需要记住两种基本情况:</p><ul class=""><li id="38b8" class="ns nt iq mb b mc mu mf mv mi nu mm nv mq nw lg nx ny nz oa bi translated">1)总体聚类距离<strong class="mb ir">大</strong> - &gt;我们取平均值</li><li id="44b6" class="ns nt iq mb b mc ob mf oc mi od mm oe mq of lg nx ny nz oa bi translated">2)总体聚类距离<strong class="mb ir">小</strong> - &gt;我们取最小值</li></ul><p id="6f73" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">如果我们在计算差距统计之前考虑这两种差异，那么差距统计将更加稳健。</p><p id="eaea" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">对于我的例子，总的聚类距离很小，所以我计算了平均值，如代码1的第12行所示，这里我使用Numpy.mean。</p><h1 id="6447" class="lh li iq bd lj lk ll lm ln lo lp lq lr jw pj jx lt jz pk ka lv kc pl kd lx ly bi translated">3)在什么条件下差距统计可能会失败？</h1><p id="5025" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">可能会出现三种情况:</p><ul class=""><li id="430a" class="ns nt iq mb b mc mu mf mv mi nu mm nv mq nw lg nx ny nz oa bi translated">低估集群</li><li id="d00d" class="ns nt iq mb b mc ob mf oc mi od mm oe mq of lg nx ny nz oa bi translated">高估集群</li><li id="e4da" class="ns nt iq mb b mc ob mf oc mi od mm oe mq of lg nx ny nz oa bi translated">通常</li></ul><h2 id="c6a6" class="oo li iq bd lj op oq dn ln or os dp lr mi ot ou lt mm ov ow lv mq ox oy lx oz bi translated">低估</h2><p id="7894" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">如果两个或三个集群非常接近，而其他集群相距甚远，它往往会低估。这正是我之前向您展示的示例中所发生的情况。其中两个星系团靠得太近，所以间隙统计低估了K*。</p><h2 id="a6de" class="oo li iq bd lj op oq dn ln or os dp lr mi ot ou lt mm ov ow lv mq ox oy lx oz bi translated">高估</h2><p id="5228" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">如果所有的星团都很接近，那就高估了而不是低估了。我真的不能解释为什么，但这是我计算了几次后发生的事情。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pm"><img src="../Images/284f82dda6504ffabec7bb29585814b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e7Im9TJ82dJcoLAvyQ6TtA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5)差距统计高估|我做的</p></figure><h2 id="a27e" class="oo li iq bd lj op oq dn ln or os dp lr mi ot ou lt mm ov ow lv mq ox oy lx oz bi translated">通常</h2><p id="4c26" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">低估和高估都主要取决于随机初始化的质心。当它们中的一些由于随机的不顺利而被忽略时，类内距离的中断迫使间隙统计更早地产生最佳的类。</p><h1 id="101b" class="lh li iq bd lj lk ll lm ln lo lp lq lr jw pj jx lt jz pk ka lv kc pl kd lx ly bi translated">结论</h1><p id="4e16" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">即使间隙统计是找到合适的K的好方法，它仍然不是完美的。例如，我们需要引入一个新的超参数，即模拟W_uniform的K的个数。我们不能确定这个的理想值是多少。此外，质心的随机初始化会导致高估或低估K*。</p><p id="eb29" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">但是通过了解差距统计的所有方面，最好的方法是应用它，然后运行几次差距统计图。取差距统计的平均值可以是一个增加的评估标准。</p><p id="5eff" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">希望你喜欢这篇文章，它给了你一些差距统计的见解。</p><h1 id="eb09" class="lh li iq bd lj lk ll lm ln lo lp lq lr jw pj jx lt jz pk ka lv kc pl kd lx ly bi translated">密码</h1><p id="a3e2" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">你可以在我的Github页面<a class="ae kv" href="https://github.com/Mavengence/Pattern_Analysis_SS20_FAU/blob/master/Exercise_6.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="mb ir">这里</strong> </a>找到这个项目的完整代码。如果您对代码有任何疑问，可以给我发电子邮件。你可以在我的<a class="ae kv" href="https://timloehr.me/" rel="noopener ugc nofollow" target="_blank">网站</a>上找到我的邮箱地址。</p><h1 id="a620" class="lh li iq bd lj lk ll lm ln lo lp lq lr jw pj jx lt jz pk ka lv kc pl kd lx ly bi translated">参考</h1><p id="a2cd" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt lg ij bi translated">[1] Trevor Hastie，Robert Tibshirani和Jerome Friedman，<a class="ae kv" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">统计学习的要素:数据挖掘、推理和预测</a> (2009)，Springer <br/> [2] Trevor Hastie，Robert Tibshirani和Guenther Walther，<a class="ae kv" href="https://statweb.stanford.edu/~gwalther/gap" rel="noopener ugc nofollow" target="_blank">通过差距统计估计数据集中的聚类数</a> (2000)</p><p id="717b" class="pw-post-body-paragraph lz ma iq mb b mc mu jr me mf mv ju mh mi mw mk ml mm mx mo mp mq my ms mt lg ij bi translated">这篇博文基于从弗里德里希·亚历山大大学埃尔兰根-纽伦堡的课程模式分析中获得的知识。我使用了Christian Riess博士演讲的部分内容来说明这篇文章的例子。所以所有权利归克里斯汀·里斯博士所有。</p></div></div>    
</body>
</html>