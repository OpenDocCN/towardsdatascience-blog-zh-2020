<html>
<head>
<title>Understanding Regions with CNN features (R-CNN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解具有CNN功能的区域(R-CNN)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-regions-with-cnn-features-r-cnn-ec69c15f8ea7?source=collection_archive---------38-----------------------#2020-10-26">https://towardsdatascience.com/understanding-regions-with-cnn-features-r-cnn-ec69c15f8ea7?source=collection_archive---------38-----------------------#2020-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="58c7" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">R-CNN物体探测完全指南</h2><div class=""/><div class=""><h2 id="d3a5" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">R-CNN的架构细节以及模型设计和论文的关键要点</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/016442b3b351c05adb4d79f9fffd2b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*42RsY5qiSNeYDTse"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae le" href="https://unsplash.com/@pietrozj?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Pietro Jeng </a>拍摄</p></figure><p id="4f96" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇博文中，我解释了关于“用于精确对象检测和语义分割的丰富特征层次”这篇文章的架构细节。虽然这篇文章已经发表了一段时间，但是除了架构之外，还有很多东西需要从这篇文章中学习。我首先简要介绍了过量饮食网络，然后介绍了RNN网络。如果你对过食网一无所知，那么不用担心！！你仍然不会错过任何东西。</p><p id="adc4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">还有，博客的结构有点不一样。这更像是学生和老师之间的对话(尝试用费曼技巧学习😃).学生端的问题用粗体突出显示(以防你赶时间)。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="1d58" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated"><strong class="ak">学生</strong></h2><p id="4d26" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">我精通著名的对象分类算法，比如VGG、AlexNet、ResNet、InceptionNet、MobileNet(以及它们所有的变体)等等。我对这种方法的架构感到惊讶，并希望进一步扩展我在这一领域的知识。然而，我脑海中有一个问题，这些模型只能判断图像中是否包含物体。然而，我想研究还能判断物体在图像中的位置的模型？</p><h2 id="89e1" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated"><strong class="ak">老师</strong></h2><p id="61fb" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">你希望冒险进入的深度学习部分也已经分类如下:图像定位、对象检测和图像分割。</p><blockquote class="nf ng nh"><p id="9360" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">物体检测:</strong>预测多类多个物体的包围盒。也可以包含许多属于同一类的对象。<br/> <strong class="lh ja">图像定位:</strong>预测图像中单个类别的单个对象的边界框<br/> <strong class="lh ja">图像分割:</strong>为图像中的每个对象创建逐像素的掩模。</p></blockquote><p id="80d1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这三者中，我们将讨论对象检测。在深入研究R-CNN架构细节之前，我想让您了解一下Union上的交集的概念。两个框之间的相交面积除以两个框所占面积的并集，定义为并集上的交集。在下图中，上半部分显示了公式的可视化表示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/d9311b549d9e2f382b6ae2a16b8a62da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRD4ZO5eS1736gWXinewKw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">并集上的交集(IoU)——作者在<a class="ae le" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>的帮助下创作的图片</p></figure><p id="8076" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">设上面图像下半部的蓝色方框是物体边界方框的预测，黄色方框是另一个预测。所以，当两个盒子完全重叠时，IoU为1，当它们完全不重叠时，IoU为0。所有其他情况都在0和1之间，精确值可以通过使用边界框坐标来确定。在训练和评估性能时，其中一个框可以被标记为基础事实，如果IoU低于特定阈值，则预测可以被认为是假阳性。记住这个解释，我们一会儿会需要它。</p><p id="11c2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，网络中使用的性能测量是<a class="ae le" href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener">平均精度</a> (mAP)。平均精度的本质就像准确性一样，地图越多越好。在跳到R-CNN之前，我们先来看看R-CNN之前流行的一个模型。</p><p id="3d17" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用卷积神经网络进行对象检测的第一篇论文之一是<a class="ae le" href="https://arxiv.org/pdf/1312.6229.pdf" rel="noopener ugc nofollow" target="_blank"> OverFeat </a>。他们使用滑动窗口方法来检测图像中不同位置的物体。他们还使用了图像金字塔，以便在放大的图像中检测较小的对象，在较小的图像中检测较大的对象。下面的gif将让你对滑动窗口的使用有一个直观的了解。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5a045bb63882fa243d216e04c119176f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/1*0sW-bTOCbGM3Co2T6wlTzg.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">用于对象检测的滑动窗口方法—作者gif，照片由<a class="ae le" href="https://unsplash.com/@sonniehiles?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Sonnie Hiles </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="617a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">OverFeat的作者使用AlexNet作为他们的基础架构，并提供了两个网络，一个快速，另一个精确。他们用1 x 1卷积层取代了CNN的全连接层(这是一个好主意)，减少了计算量，提高了训练和测试的速度。它们还有一个回归层，从Pool5层获取输入，并通过两个密集层传递它们，然后最终输出有4个单元来确定边界框。</p><blockquote class="nf ng nh"><p id="9d8a" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">注意:</strong>我过度简化了对超喂纸的解释，并且有许多关于架构的微妙细节，这些细节请参考<a class="ae le" href="https://arxiv.org/pdf/1312.6229.pdf" rel="noopener ugc nofollow" target="_blank">纸</a></p></blockquote><h2 id="5ed8" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated"><strong class="ak">学生</strong></h2><p id="b6be" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">我没有时间深入研究过厚的纸张，但是根据你展示的gif，看起来是不是窗口选择的图像的许多子部分不会检测到任何对象？像这些区域将被检测器选择，通过整个CNN架构发送，并且仍然不产生任何对象并且增加计算成本？<strong class="lh ja">如果我们能以某种方式给模型一个提示，这些地方可能有一些物体，而模型只会告诉我在特定的位置是否有物体，这不是更好吗？</strong></p><h2 id="a8e2" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated"><strong class="ak">老师</strong></h2><p id="a9b7" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">你强调的问题确实是过度饮食的警告。R-CNN的论文做了一些类似于你直觉认为应该做的事情。它使用<a class="ae le" href="https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/" rel="noopener ugc nofollow" target="_blank">选择性搜索</a>算法来检测图像中某个对象的可能位置，并只将这些图像部分(大约2000个区域建议)发送到AlexNet网络。他们用SVM层替换了最后一个Softmax层，并在Pascal VOC 2007上只训练了用于分类的SVM层(关于训练的细节将在后面解释)。此外，Pascal VOC数据集有20个类，但他们为SVM训练了21个类，其中额外的类对应于没有对象的背景类。通过这种方法，他们能够获得44.7%的mAP。看看建筑吧！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/38557031244595e3f1db3da66b2248fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*P0sFsWI1vfr7nLp-zUmz0g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">R-CNN网络架构— <a class="ae le" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="1494" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated"><strong class="ak">学生</strong></h2><p id="d612" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">对于Pascal VOC上的对象检测来说，这些确实是很好的结果，因为它们比当时的当代模型好得多。然而，我有这样一个查询，<strong class="lh ja">由于ImageNet数据集与Pascal VOC数据集相比来自不同的分布，作者难道不应该对模型进行微调吗？</strong>根据我所知的文献，如果数据集分布发生变化，我们通常会对模型进行微调。此外，<strong class="lh ja">卷积网络接受固定大小的输入，例如，在AlexNet的情况下为227 x 227，那么它们如何在选择性搜索算法提出的区域上应用约束呢？</strong></p><h2 id="f708" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated"><strong class="ak">老师</strong></h2><p id="fee3" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">作者首先<strong class="lh ja">将图像扭曲</strong>到227 x 227大小，然后将它们传递给输入，这时他们在训练SVM后得到了44.7%的地图。扭曲图像时，图像中的对象可能会被<strong class="lh ja">拉伸、拉长、挤压等</strong>，它们甚至可能会妨碍性能。因此，作者首先微调整个AlexNet网络进行分类。之后，作者用SVM层替换了Softmax层，只训练这一层。由于这种微调，地图从44.7%增加到54.2%。</p><p id="ecc1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">还有一个关于用于微调和SVM训练的数据的细节。在ImageNet数据集上训练预训练网络。在微调模型时，作者使用了Pascal VOC 2007数据集。在微调过程中，即softmax层位于顶部时，包含IoU ≥ 0.5的图像被用作地面实况正框，其余图像被用作负类(背景，无对象)。然而，为了训练SVM，他们仅使用真实的图像作为训练，并带有IoU &lt; 0.3 as negative class. The remaining grey zone proposals were ignored. The authors also replaced the SVM with 21 way Softmax and the mAP dropped from 54.2% to 50.9%.</p><blockquote class="nf ng nh"><p id="2e46" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">注意:</strong>关于为Softmax和SVM选择阳性和阴性样本的详细信息在本文的附录B中提供</p></blockquote><h2 id="a107" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated"><strong class="ak">学生</strong></h2><p id="6455" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">这是我第一次遇到这样的微调方法。<strong class="lh ja">RCNN的论文还有其他特点吗？</strong></p><h2 id="4106" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated"><strong class="ak">老师</strong></h2><p id="7594" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">是的，有。作者对他们的建筑做了详尽的烧蚀研究。大部分讨论都围绕着密集层和微调网络的必要性。让我们这样来看。</p><p id="2d17" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="ni">无需微调</em> <br/> </strong>作者从网络的Pool5层提取特征，然后从fc6层，再从fc7层提取特征。对于提取的每个要素，他们分别训练SVM图层并生成地图。看看下面的图片。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi np"><img src="../Images/686cae22217588cc109661fa13238367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8NmSPKBeg23Q3hKc_4rYQg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">没有微调的地图— <a class="ae le" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">纸</a></p></figure><p id="5059" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从上图的最后一列中，我们看到删除两个FC层并没有对性能造成太大影响。此外，来自fc6层的特征比来自fc7层的特征是更好的预测器。以下是作者在观察后所说的话。</p><blockquote class="nq"><p id="dfa6" class="nr ns iq bd nt nu nv nw nx ny nz ma dk translated">这意味着CNN的29%，即大约1680万个参数可以在不降低mAP的情况下被删除。更令人惊讶的是，移除fc7和fc6产生了相当好的结果7，尽管pool5特征仅使用6%的CNN参数来计算。</p></blockquote><p id="2d3b" class="pw-post-body-paragraph lf lg iq lh b li oa ka lk ll ob kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated"><strong class="lh ja"><em class="ni"/><br/></strong>作者们现在已经用各种方式对网络进行了微调。首先，当他们直接使用Pool5的功能并将其发送到SoftMax以微调整个网络时。在这种情况下，他们发现Pool5的功能在没有微调的情况下增加了3%。接下来，他们对fc6和fc7层也做了同样的事情，分别增加到53%和54.2%。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/8082e6c03e983fa35e0e2f23c7b1e2de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KsOdBrjRPhuDQ7y5UADVA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">带微调的地图— <a class="ae le" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">纸</a></p></figure><p id="8ecd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">您可以将调优后获得的结果图与上图中共享的未调优结果图进行比较。微调大大改善了结果。但是，第四排有一些特殊之处，我们来讨论一下！！</p><p id="c25b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="ni">使用包围盒回归器</em> </strong> <br/>作者已经添加了一个包围盒回归器到池5层的输出中。这样做之后，他们发现58.5% mAP的性能有了相当大的提高。这是一个未经微调的模型在地图上几乎14%的提升。这个包围盒回归用l2损失来训练，并且用λ= 1000来惩罚。</p><blockquote class="nf ng nh"><p id="5a56" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">注:</strong>关于边界框回归器的详细信息见本文附录C。</p></blockquote><p id="a528" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作者还尝试用VGG-16代替AlexNet，发现使用边界框回归器时mAP增加了8%。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/c35207d2fe6879f2a3ce3924d1be350c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zx7v5aGo0w41Oyft0v37pA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">具有不同模型架构的地图— <a class="ae le" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="e065" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我现在建议您阅读这篇文章，以便更深入地了解这篇文章和实现细节。他们还提供了ILSVRC2013探测数据集的详细说明，并将他们的模型与其他模型进行了比较。</p><h2 id="c58f" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated"><strong class="ak">学生</strong></h2><p id="4f7e" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">这是作者做的非常详尽的分析。这篇文章可能有点旧了，但是模型架构的分析方法是值得注意的。<strong class="lh ja">这些通过实验验证层对精度提高的贡献的策略，可以在设计任何其他类型的模型时使用，甚至可以用于一些其他任务。</strong></p><h2 id="c123" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">教师</h2><p id="8bd4" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">是的，他们已经对他们的模型进行了详细而全面的分析，并与当代模型进行了比较。最后一点，出于定义的原因，像R-CNN这样的模型被称为<strong class="lh ja">两阶段</strong>模型。这种模型具有用于检测物体的可能位置的独立算法/模型(选择性搜索)和用于检测物体存在的独立模型(AlexNet)。并且这个模型(包括包围盒回归器)的训练是一个<strong class="lh ja">的三阶段过程</strong>，其中第一阶段，模型被微调，第二阶段，SVM分类器被训练，第三阶段，包围盒回归器被训练。</p><blockquote class="nf ng nh"><p id="8d18" class="lf lg ni lh b li lj ka lk ll lm kd ln nj lp lq lr nk lt lu lv nl lx ly lz ma ij bi translated"><strong class="lh ja">注意:</strong>我已经讨论了R-CNN论文中Pascal VOC 2007数据集的分数和分析。该文件还显示了对其他数据集的深入分析。</p></blockquote></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="ad62" class="oh mj iq bd mk oi oj ok mn ol om on mq kf oo kg mt ki op kj mw kl oq km mz or bi translated">参考</h1><p id="876c" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">R.Girshick，J. Donahue，T. Darrell，J. Malik，<a class="ae le" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">用于精确对象检测和语义分割的丰富特征层次，</a>计算机视觉和模式识别，2014年</p><p id="19c1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">X.张，M. Mathieu，R. Fergus，Y. LeCun，<a class="ae le" href="https://arxiv.org/pdf/1312.6229.pdf" rel="noopener ugc nofollow" target="_blank"> OverFeat:使用卷积网络的集成识别、定位和检测</a>，计算机视觉与模式识别，2014</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="os ot l"/></div></figure></div></div>    
</body>
</html>