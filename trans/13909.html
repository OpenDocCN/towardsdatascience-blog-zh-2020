<html>
<head>
<title>Bias Correction For Paid Search In Media Mix Modeling: Paper Review</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">媒体混合模型中付费搜索的偏差修正:论文综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bias-correction-for-paid-search-in-media-mix-modeling-paper-review-d3579cfaee9b?source=collection_archive---------31-----------------------#2020-09-24">https://towardsdatascience.com/bias-correction-for-paid-search-in-media-mix-modeling-paper-review-d3579cfaee9b?source=collection_archive---------31-----------------------#2020-09-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4cc8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这篇文章提供了“媒体混合建模中付费搜索的偏差纠正”的高级概述，提供了关键概念的代码和实现。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/98e835adac456ab74c518b877dbb07b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YECeOxlko9KoOJNw8RNm3A.jpeg"/></div></div></figure><p id="06a8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">媒体组合建模中付费搜索的偏差修正:</strong> <a class="ae ln" href="https://arxiv.org/pdf/1807.03292.pdf" rel="noopener ugc nofollow" target="_blank">链接论文</a></p><p id="3768" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">媒体组合建模试图仅基于观察数据来估计媒体支出对销售的因果影响。众所周知，从观测数据中估计因果关系充满了挑战。</p><p id="4fcd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随着时间的推移，出现了两种主要的、互补的框架来处理因果推理。</p><ol class=""><li id="2c5d" class="lo lp iq kt b ku kv kx ky la lq le lr li ls lm lt lu lv lw bi translated">鲁宾的潜在结果框架。</li><li id="cb88" class="lo lp iq kt b ku lx kx ly la lz le ma li mb lm lt lu lv lw bi translated">Pearl的图形框架。</li></ol><p id="814b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本文探讨了使用Pearl的图形框架来控制媒体组合建模中的选择偏差，特别是在付费搜索广告中。</p><h2 id="f949" class="mc md iq bd me mf mg dn mh mi mj dp mk la ml mm mn le mo mp mq li mr ms mt mu bi translated">问题设置</h2><p id="dac4" class="pw-post-body-paragraph kr ks iq kt b ku mv jr kw kx mw ju kz la mx lc ld le my lg lh li mz lk ll lm ij bi translated">假设我们的目标是测量搜索广告(PPC)对销售的因果影响。在一个简单的回归模型中，我们可以回归销售支出并得出一个因果估计值:</p><blockquote class="na nb nc"><p id="3c09" class="kr ks nd kt b ku kv jr kw kx ky ju kz ne lb lc ld nf lf lg lh ng lj lk ll lm ij bi translated">销售额=平均值_销售额+<strong class="kt ir">roas _估计值</strong> *搜索_花费+误差</p></blockquote><p id="22ce" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以用普通的最小二乘法(OLS)来拟合上面的模型，在一个简单的世界中，这将产生对roa的精确估计。不幸的是，世界并非如此简单，我们知道经常会有令人困惑的变量。例如，我们知道有机搜索也是销售的驱动力，有机搜索和付费搜索都有一个潜在的原因，即消费者需求。下图说明了这一点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/506bc4f9ab4e2be64e9dceaf3a6e1fe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_3gTsouswZKLQCoGsXvNg.png"/></div></div></figure><p id="cc30" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">以上，我们可以看到经济因素如何驱动消费者需求，进而驱动搜索查询，进而驱动付费搜索和有机搜索。</p><p id="d90e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当我们使用上面定义的简单模型来模拟更复杂的世界时，我们会遇到选择偏差或更广泛的内生性问题。</p><blockquote class="na nb nc"><p id="571f" class="kr ks nd kt b ku kv jr kw kx ky ju kz ne lb lc ld nf lf lg lh ng lj lk ll lm ij bi translated"><strong class="kt ir">内生性:</strong>是计量经济学中使用的术语，指与误差项相关的解释变量。</p></blockquote><p id="4587" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">简而言之，我们的模型捕捉或吸收了有机搜索在误差项和ROAS估计中的解释价值，从而产生有偏差的ROAS估计。</p><p id="39c7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了控制这一点，本文建议使用珀尔的后门标准。Pearl的后门标准是基于使用图形模型来描述因果关系的思想。图形模型是有用的，因为它们允许我们结合领域知识和来自图论的思想。</p><p id="8a4c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其中一个想法就是分离。</p><blockquote class="na nb nc"><p id="c0d6" class="kr ks nd kt b ku kv jr kw kx ky ju kz ne lb lc ld nf lf lg lh ng lj lk ll lm ij bi translated"><strong class="kt ir"> D-separation </strong>是directed-separation的缩写，它允许我们在一个图中的两个节点被一组第三节点分隔(或有条件地相互独立)时进行通信。</p></blockquote><p id="2020" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">例如，在下图中，给定x1和x2，我们可以说Z与Y是d分离的，或条件独立的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e132dc9b4a41543d9e4c3762916c24e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*aeO8RqiCdhyIsp-HOP-fmA.png"/></div></figure><p id="a0ee" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">另一个重要的想法是<strong class="kt ir">后门标准。</strong></p><blockquote class="na nb nc"><p id="27ab" class="kr ks nd kt b ku kv jr kw kx ky ju kz ne lb lc ld nf lf lg lh ng lj lk ll lm ij bi translated"><strong class="kt ir">后门准则:</strong>给定一个因果图，一组变量Z相对于图中一个有序的变量对(X，Y)满足<br/>后门准则，如果:1)Z中没有一个节点是X的后代；以及2) Z“阻塞”X和Y之间包含指向X的箭头的每条路径</p></blockquote><p id="09c8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，如果一组节点Z满足有向对的后门准则(X →Y ),那么在给定足够大的数据集的情况下，我们可以揭示X对Y的无偏估计。这也被定义为<strong class="kt ir">可识别性</strong>。</p><p id="d50c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了让自己熟悉后门标准的概念，我建议尝试下面的代码片段(尝试创建各种图形，并确定哪些节点将满足后门标准)并探索其他资源。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="6644" class="mc md iq nk b gy no np l nq nr">from causalgraphicalmodels import CausalGraphicalModel</span><span id="b72a" class="mc md iq nk b gy ns np l nq nr">simple = CausalGraphicalModel(<br/>    nodes=["x1", 'x2', 'z', 'y'],<br/>    edges=[<br/>        ("z", "x1"), <br/>        ("z", "x2"),<br/>        ("x1", "y"),<br/>        ("x2", "y"),<br/>    ]<br/>)</span><span id="4362" class="mc md iq nk b gy ns np l nq nr">simple.draw()</span><span id="38c3" class="mc md iq nk b gy ns np l nq nr">simple.is_valid_backdoor_adjustment_set("x1", "y", {"z"})</span></pre><h2 id="ef88" class="mc md iq bd me mf mg dn mh mi mj dp mk la ml mm mn le mo mp mq li mr ms mt mu bi translated">申请搜索</h2><p id="4960" class="pw-post-body-paragraph kr ks iq kt b ku mv jr kw kx mw ju kz la mx lc ld le my lg lh li mz lk ll lm ij bi translated">现在，我们已经探索了一些与因果图模型相关的基本概念，我们可以看到它们是如何应用于恢复付费搜索的无偏ROAS估计的。首先，本文阐述了环境的因果结构。(见下图)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/4bd153ccb257853e8adf57c0815fd30b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Eo0AgweOUNOuaK1O.png"/></div></div></figure><p id="75f2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上图表明我们将影响建模为:</p><blockquote class="na nb nc"><p id="7731" class="kr ks nd kt b ku kv jr kw kx ky ju kz ne lb lc ld nf lf lg lh ng lj lk ll lm ij bi translated">sales = average _ sales+<em class="iq">roas _ estimate</em>* search _ spend+error _ 0+error _ 1</p></blockquote><p id="719d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其中error_0和error_1分别吸收了消费者需求和有机搜索的影响。</p><p id="1853" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">基于我们对图形模型的了解，我们现在知道，如果我们可以恢复搜索查询的度量，我们就可以满足(搜索广告X →销售)的后门标准。为了测试这一点，我们可以使用一个叫做<a class="ae ln" href="https://github.com/ijmbarr/causalgraphicalmodels" rel="noopener ugc nofollow" target="_blank">因果图模型</a>的便利包。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="1d32" class="mc md iq nk b gy no np l nq nr">from causalgraphicalmodels import CausalGraphicalModel</span><span id="96af" class="mc md iq nk b gy ns np l nq nr">search = CausalGraphicalModel(<br/>    nodes=["economic_factors", "consumer_demand", "search_queriers", "auction", "organic_search", "paid_search", 'organic_search', 'sales'],<br/>    edges=[<br/>        ("economic_factors", "consumer_demand"), <br/>        ("consumer_demand", "sales"), <br/>        ("consumer_demand", "search_q"),<br/>        ("search_q", "auction"), <br/>        ("auction", "paid_search"),<br/>        ("search_q", "paid_search"),<br/>        ("search_q", "organic_search"), <br/>        ("organic_search", "sales"), <br/>        ("paid_search", "sales"),<br/>    ]<br/>)</span><span id="0e93" class="mc md iq nk b gy ns np l nq nr">search.is_valid_backdoor_adjustment_set("paid_search", "sales", {"search_q"})</span><span id="40f1" class="mc md iq nk b gy ns np l nq nr"># output is True </span></pre><p id="3c1b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在上面的代码中，我们定义了因果图模型(DAG ),并测试我们的控制变量是否满足后门标准(payed _ search→sales)。</p><p id="de76" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">接下来，我们生成样本数据，运行OLS回归来比较满足和不满足后门标准时的估计值。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="d3b4" class="mc md iq nk b gy no np l nq nr">from causalgraphicalmodels import StructuralCausalModel<br/>from causalgraphicalmodels import CausalGraphicalModel<br/>import numpy as np</span><span id="4733" class="mc md iq nk b gy ns np l nq nr"># create structural causal model. This let's us generate data. </span><span id="4c5d" class="mc md iq nk b gy ns np l nq nr">search_data = StructuralCausalModel({<br/>    "consumer_demand": lambda n_samples:                  np.random.normal(100,5, size=n_samples) ,<br/>    "search_q":        lambda consumer_demand, n_samples: np.random.normal(consumer_demand*.3,  1, n_samples)  ,<br/>    "organic_search":  lambda search_q, n_samples:        np.random.normal(search_q*.6, 1)                ,<br/>    "paid_search":     lambda search_q, n_samples:        np.random.normal(search_q*.1, 1)                ,<br/>    <br/>    "sales":           lambda organic_search, paid_search, n_samples: np.random.normal(75 + organic_search*.2  + paid_search*.3,  1 )<br/>})</span><span id="24c9" class="mc md iq nk b gy ns np l nq nr">data = search_data.sample(156)</span><span id="69e1" class="mc md iq nk b gy ns np l nq nr"># run OLS without backdoor criterion satisfied for paid search --&gt; sales</span><span id="4137" class="mc md iq nk b gy ns np l nq nr">X = data[['paid_search' ]].values<br/>X = sm.add_constant(X)</span><span id="87d3" class="mc md iq nk b gy ns np l nq nr">results = sm.OLS(data.sales.values, X).fit()</span><span id="c234" class="mc md iq nk b gy ns np l nq nr">print(results.summary())</span><span id="a832" class="mc md iq nk b gy ns np l nq nr"># run OLS without backdoor criterion satisfied for paid search --&gt; sales</span><span id="2084" class="mc md iq nk b gy ns np l nq nr">X = data[['paid_search' ]].values<br/>X = sm.add_constant(X)</span><span id="5a83" class="mc md iq nk b gy ns np l nq nr">results = sm.OLS(data.sales.values, X).fit()</span><span id="698c" class="mc md iq nk b gy ns np l nq nr">print(results.summary())</span><span id="df45" class="mc md iq nk b gy ns np l nq nr"># with backdoor criterion satisfied </span><span id="d50b" class="mc md iq nk b gy ns np l nq nr">X = data[['paid_search',   'search_q']].values<br/>X = sm.add_constant(X)</span><span id="9751" class="mc md iq nk b gy ns np l nq nr">results = sm.OLS(data.sales.values, X).fit()</span><span id="63bb" class="mc md iq nk b gy ns np l nq nr">print(results.summary())</span></pre><p id="d803" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">得出以下ROAS估计值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/0e5d27fc28ce0e9bc3c7f3c749f258a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lJMDeJRwSLi0FfQr5d6XWQ.png"/></div></div></figure><p id="9a0d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如我们所看到的，两种估计都捕捉到了真实的参数，无偏估计(满足后门标准)更接近真实的估计。</p><p id="d07c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，您可能已经注意到，在代码示例中，我们只采样了156个数据点，这相当于三年的每周MMM数据。这并不是大量的数据，也提出了一个重要的问题，即我们如何知道我们的样本量何时足够？</p><p id="50bd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该论文提出，当样本量足够大以允许非参数估计时，这种担心可以得到缓解，然而大样本量在MMM建模中并不常见。</p><p id="1b39" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了进一步探索这一点，我制作了下图，展示了在样本量越来越大的情况下，ROAS估计值和置信区间是如何变化的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/baaa657d2655b0cf002cad21cbfa8b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SYsrkHUhZdJir0om2U_efg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/e1643d74a70f6cc1277d31723d4b352f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-S4BeYvI9xc2KXXDgxel1g.png"/></div></div></figure><p id="b9f6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如我们所见，无偏估计量收敛于真实参数，而有偏估计量过于乐观。此外，上图强调了小样本量如何产生非常大的置信区间。如果样本量很小，需要注意一些事情。</p></div><div class="ab cl nx ny hu nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="ij ik il im in"><p id="39f5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">至此，我们已经完成了论文理论部分的主要内容，并涵盖了:</p><ul class=""><li id="202d" class="lo lp iq kt b ku kv kx ky la lq le lr li ls lm oe lu lv lw bi translated">付费搜索中的选择偏差与MMM建模</li><li id="8c86" class="lo lp iq kt b ku lx kx ly la lz le ma li mb lm oe lu lv lw bi translated">因果图形模型/珍珠框架</li><li id="45a6" class="lo lp iq kt b ku lx kx ly la lz le ma li mb lm oe lu lv lw bi translated">如何应用于简单的付费搜索场景</li><li id="98f2" class="lo lp iq kt b ku lx kx ly la lz le ma li mb lm oe lu lv lw bi translated">如何模拟数据和实现模型</li><li id="0700" class="lo lp iq kt b ku lx kx ly la lz le ma li mb lm oe lu lv lw bi translated">一些需要注意的“陷阱”</li></ul><p id="1985" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本白皮书更详细地介绍了这里涉及的主题，并继续介绍:</p><ol class=""><li id="1f30" class="lo lp iq kt b ku kv kx ky la lq le lr li ls lm lt lu lv lw bi translated">复杂场景</li><li id="a14c" class="lo lp iq kt b ku lx kx ly la lz le ma li mb lm lt lu lv lw bi translated">履行</li><li id="696c" class="lo lp iq kt b ku lx kx ly la lz le ma li mb lm lt lu lv lw bi translated">实证结果</li></ol><p id="def2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我强烈推荐感兴趣的读者查看全文以了解更多细节。</p></div></div>    
</body>
</html>